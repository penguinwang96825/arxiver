{"title": "Nile-Chat: Egyptian Language Models for Arabic and Latin Scripts", "abstract": "We introduce Nile-Chat-4B, 3x4B-A6B, and 12B, a collection of LLMs for\nEgyptian dialect, uniquely designed to understand and generate texts written in\nboth Arabic and Latin scripts. Specifically, with Nile-Chat-3x4B-A6B, we\nintroduce a novel language adaptation approach by leveraging the\nBranch-Train-MiX strategy to merge script-specialized experts, into a single\nMoE model. Our Nile-Chat models significantly outperform leading multilingual\nand Arabic LLMs, such as LLaMa, Jais, and ALLaM, on our newly introduced\nEgyptian evaluation benchmarks, which span both understanding and generative\ntasks. Notably, our 12B model yields a 14.4% performance gain over\nQwen2.5-14B-Instruct on Latin-script benchmarks. All our resources are publicly\navailable. We believe this work presents a comprehensive methodology for\nadapting LLMs to dual-script languages, addressing an often overlooked aspect\nin modern LLM development.", "published": "2025-07-06 22:53:41", "link": "http://arxiv.org/abs/2507.04569v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluating LLMs on Real-World Forecasting Against Human Superforecasters", "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across\ndiverse tasks, but their ability to forecast future events remains\nunderstudied. A year ago, large language models struggle to come close to the\naccuracy of a human crowd. I evaluate state-of-the-art LLMs on 464 forecasting\nquestions from Metaculus, comparing their performance against human\nsuperforecasters. Frontier models achieve Brier scores that ostensibly surpass\nthe human crowd but still significantly underperform a group of\nsuperforecasters.", "published": "2025-07-06 22:26:59", "link": "http://arxiv.org/abs/2507.04562v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "DP-Fusion: Token-Level Differentially Private Inference for Large Language Models", "abstract": "Large language models (LLMs) can leak sensitive information from their\ncontext through generated outputs, either accidentally or when prompted\nadversarially. Existing defenses that aim to preserve context privacy during\ninference either lack formal guarantees or suffer from a poor utility/privacy\ntrade-off. We propose DP-Fusion, a token-level Differentially Private Inference\n(DPI) mechanism that provably bounds how much an LLM's outputs reveal about\nsensitive tokens in its context. We demonstrate DPI through the task of\ndocument privatization, where the goal is to paraphrase documents so that\nsensitive content (e.g., Personally Identifiable Information, PII) cannot be\nreliably inferred, while still preserving the overall utility of the text. This\nis controlled by a parameter $\\epsilon$: $\\epsilon=0$ hides PII entirely, while\nhigher values trade off privacy for improved paraphrase quality. DP-Fusion\nworks as follows: (i) partition sensitive tokens into disjoint privacy groups,\n(ii) run the LLM once per group, and (iii) blend the output distributions so\nthat the final output remains within a fixed statistical distance of the\nbaseline distribution produced when no privacy group is revealed. This approach\nallows fine-grained control over the privacy/utility trade-off but requires\nmultiple LLM forward passes.", "published": "2025-07-06 20:49:39", "link": "http://arxiv.org/abs/2507.04531v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DOTResize: Reducing LLM Width via Discrete Optimal Transport-based Neuron Merging", "abstract": "Model compression offers a promising path to reducing the cost and\ninaccessibility of large pre-trained models, without significantly compromising\ntheir impressive performance. Large Transformer models, including large\nlanguage models (LLMs), often contain computational redundancy, which can serve\nas a target for new model compression methods. In this work, we specifically\ntarget neuron-level redundancies in model layers by combining groups of similar\nneurons into fewer neurons. We frame this width reduction as a Discrete Optimal\nTransport problem, and propose DOTResize, a novel Transformer compression\nmethod that uses optimal transport theory to transform and compress model\nweights. To ensure applicability within the Transformer architecture, we\nmotivate and incorporate entropic regularization and matrix factorization into\nthe transportation maps produced by our method. Unlike pruning-based approaches\nwhich discard neurons based on importance measures, DOTResize re-projects the\nentire neuron width, allowing the retention and redistribution of useful signal\nacross the reduced layer. Empirical results show that compared to simple or\nstate-of-the-art neuron width-pruning techniques, DOTResize can outperform\nthese methods across multiple LLM families and sizes, while achieving\nmeasurable reductions in real-world computational cost.", "published": "2025-07-06 19:49:46", "link": "http://arxiv.org/abs/2507.04517v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "AdS: Adapter-state Sharing Framework for Multimodal Sarcasm Detection", "abstract": "The growing prevalence of multimodal image-text sarcasm on social media poses\nchallenges for opinion mining, especially under resource constraints. Existing\napproaches rely on full fine-tuning of large pre-trained models, making them\nunsuitable for low-resource settings. While recent parameter-efficient\nfine-tuning (PEFT) methods offer promise, their off-the-shelf use underperforms\non complex tasks like sarcasm detection. We propose AdS (Adapter-State\nSharing), a lightweight framework built on CLIP that inserts adapters only in\nthe upper layers and introduces a novel adapter-state sharing mechanism, where\ntextual adapters guide visual ones. This design promotes efficient cross-modal\nlearning while preserving low-level unimodal representations. Experiments on\ntwo public benchmarks demonstrate that AdS achieves state-of-the-art results\nusing significantly fewer trainable parameters than existing PEFT and full\nfine-tuning approaches.", "published": "2025-07-06 18:51:00", "link": "http://arxiv.org/abs/2507.04508v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unveiling the Potential of Diffusion Large Language Model in Controllable Generation", "abstract": "Diffusion models, originally developed for image generation, have emerged as\na promising alternative to autoregressive large language models (LLMs). We\npresent a theoretical analysis comparing autoregressive and masked diffusion\nLLMs, revealing that the intrinsic bidirectional attention mechanism of\ndiffusion LLMs (dLLMs) enables superior context modeling and generation\ncontrollability. However, existing dLLM applications face significant\nchallenges in controllable generation: the native multi-step denoising process\nexhibits high sensitivity to sequence length, elevated hallucination rates, and\nprohibitive inference costs without specialized optimizations. To address these\nlimitations, we propose \\textbf{S}elf-adaptive \\textbf{S}chema\n\\textbf{S}caffolding ($S^3$), a novel framework that enables dLLMs to generate\nstructured outputs (e.g., JSON) while maintaining semantic fidelity and\naccelerating inference. Our approach injects the target schema structure into\nthe output context, reducing unnecessary computation while improving\ncontrollability. Extensive experiments demonstrate that $S^3$ achieves\nsubstantial improvements: 65\\% increase in structural adherence, 48\\%\nenhancement in content fidelity, and 17\\% reduction in hallucination rates\ncompared to baseline. These results establish both theoretical foundations and\npractical pathways for deploying diffusion models in controllable text\ngeneration tasks. Code and data will be publicly released.", "published": "2025-07-06 18:41:34", "link": "http://arxiv.org/abs/2507.04504v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A validity-guided workflow for robust large language model research in psychology", "abstract": "Large language models (LLMs) are rapidly being integrated into psychological\nresearch as research tools, evaluation targets, human simulators, and cognitive\nmodels. However, recent evidence reveals severe measurement unreliability:\nPersonality assessments collapse under factor analysis, moral preferences\nreverse with punctuation changes, and theory-of-mind accuracy varies widely\nwith trivial rephrasing. These \"measurement phantoms\"--statistical artifacts\nmasquerading as psychological phenomena--threaten the validity of a growing\nbody of research. Guided by the dual-validity framework that integrates\npsychometrics with causal inference, we present a six-stage workflow that\nscales validity requirements to research ambition--using LLMs to code text\nrequires basic reliability and accuracy, while claims about psychological\nproperties demand comprehensive construct validation. Researchers must (1)\nexplicitly define their research goal and corresponding validity requirements,\n(2) develop and validate computational instruments through psychometric\ntesting, (3) design experiments that control for computational confounds, (4)\nexecute protocols with transparency, (5) analyze data using methods appropriate\nfor non-independent observations, and (6) report findings within demonstrated\nboundaries and use results to refine theory. We illustrate the workflow through\nan example of model evaluation--\"LLM selfhood\"--showing how systematic\nvalidation can distinguish genuine computational phenomena from measurement\nartifacts. By establishing validated computational instruments and transparent\npractices, this workflow provides a path toward building a robust empirical\nfoundation for AI psychology research.", "published": "2025-07-06 18:06:12", "link": "http://arxiv.org/abs/2507.04491v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.HC"}
{"title": "Does Overnight News Explain Overnight Returns?", "abstract": "Over the past 30 years, nearly all the gains in the U.S. stock market have\nbeen earned overnight, while average intraday returns have been negative or\nflat. We find that a large part of this effect can be explained through\nfeatures of intraday and overnight news. Our analysis uses a collection of 2.4\nmillion news articles. We apply a novel technique for supervised topic analysis\nthat selects news topics based on their ability to explain contemporaneous\nmarket returns. We find that time variation in the prevalence of news topics\nand differences in the responses to news topics both contribute to the\ndifference in intraday and overnight returns. In out-of-sample tests, our\napproach forecasts which stocks will do particularly well overnight and\nparticularly poorly intraday. Our approach also helps explain patterns of\ncontinuation and reversal in intraday and overnight returns. We contrast the\neffect of news with other mechanisms proposed in the literature to explain\novernight returns.", "published": "2025-07-06 17:37:50", "link": "http://arxiv.org/abs/2507.04481v1", "categories": ["q-fin.TR", "cs.CL", "stat.ML"], "primary_category": "q-fin.TR"}
{"title": "The role of large language models in UI/UX design: A systematic literature review", "abstract": "This systematic literature review examines the role of large language models\n(LLMs) in UI/UX design, synthesizing findings from 38 peer-reviewed studies\npublished between 2022 and 2025. We identify key LLMs in use, including GPT-4,\nGemini, and PaLM, and map their integration across the design lifecycle, from\nideation to evaluation. Common practices include prompt engineering,\nhuman-in-the-loop workflows, and multimodal input. While LLMs are reshaping\ndesign processes, challenges such as hallucination, prompt instability, and\nlimited explainability persist. Our findings highlight LLMs as emerging\ncollaborators in design, and we propose directions for the ethical, inclusive,\nand effective integration of these technologies.", "published": "2025-07-06 17:18:05", "link": "http://arxiv.org/abs/2507.04469v1", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Dual Modality-Aware Gated Prompt Tuning for Few-Shot Multimodal Sarcasm Detection", "abstract": "The widespread use of multimodal content on social media has heightened the\nneed for effective sarcasm detection to improve opinion mining. However,\nexisting models rely heavily on large annotated datasets, making them less\nsuitable for real-world scenarios where labeled data is scarce. This motivates\nthe need to explore the problem in a few-shot setting. To this end, we\nintroduce DMDP (Deep Modality-Disentangled Prompt Tuning), a novel framework\nfor few-shot multimodal sarcasm detection. Unlike prior methods that use\nshallow, unified prompts across modalities, DMDP employs gated,\nmodality-specific deep prompts for text and visual encoders. These prompts are\ninjected across multiple layers to enable hierarchical feature learning and\nbetter capture diverse sarcasm types. To enhance intra-modal learning, we\nincorporate a prompt-sharing mechanism across layers, allowing the model to\naggregate both low-level and high-level semantic cues. Additionally, a\ncross-modal prompt alignment module enables nuanced interactions between image\nand text representations, improving the model's ability to detect subtle\nsarcastic intent. Experiments on two public datasets demonstrate DMDP's\nsuperior performance in both few-shot and extremely low-resource settings.\nFurther cross-dataset evaluations show that DMDP generalizes well across\ndomains, consistently outperforming baseline methods.", "published": "2025-07-06 17:16:34", "link": "http://arxiv.org/abs/2507.04468v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Think Twice Before You Judge: Mixture of Dual Reasoning Experts for Multimodal Sarcasm Detection", "abstract": "Multimodal sarcasm detection has attracted growing interest due to the rise\nof multimedia posts on social media. Understanding sarcastic image-text posts\noften requires external contextual knowledge, such as cultural references or\ncommonsense reasoning. However, existing models struggle to capture the deeper\nrationale behind sarcasm, relying mainly on shallow cues like image captions or\nobject-attribute pairs from images. To address this, we propose \\textbf{MiDRE}\n(\\textbf{Mi}xture of \\textbf{D}ual \\textbf{R}easoning \\textbf{E}xperts), which\nintegrates an internal reasoning expert for detecting incongruities within the\nimage-text pair and an external reasoning expert that utilizes structured\nrationales generated via Chain-of-Thought prompting to a Large Vision-Language\nModel. An adaptive gating mechanism dynamically weighs the two experts,\nselecting the most relevant reasoning path. Experiments on two benchmark\ndatasets show that MiDRE achieves superior performance over baselines. Various\nqualitative analyses highlight the crucial role of external rationales,\nrevealing that even when they are occasionally noisy, they provide valuable\ncues that guide the model toward a better understanding of sarcasm.", "published": "2025-07-06 16:37:21", "link": "http://arxiv.org/abs/2507.04458v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GradOT: Training-free Gradient-preserving Offsite-tuning for Large Language Models", "abstract": "The rapid growth of large language models (LLMs) with traditional centralized\nfine-tuning emerges as a key technique for adapting these models to\ndomain-specific challenges, yielding privacy risks for both model and data\nowners. One promising solution, called offsite-tuning (OT), is proposed to\naddress these challenges, where a weaker emulator is compressed from the\noriginal model and further fine-tuned with adapter to enhance privacy. However,\nthe existing OT-based methods require high computational costs and lack\ntheoretical analysis. This paper introduces a novel OT approach based on\ngradient-preserving compression, named GradOT. By analyzing the OT problem\nthrough the lens of optimization, we propose a method that selectively applies\ncompression techniques such as rank compression and channel pruning, preserving\nthe gradients of fine-tuned adapters while ensuring privacy. Extensive\nexperiments demonstrate that our approach surpasses existing OT methods, both\nin terms of privacy protection and model performance. Our method provides a\ntheoretical foundation for OT and offers a practical, training-free solution\nfor offsite-tuning of large-scale LLMs.", "published": "2025-07-06 16:27:27", "link": "http://arxiv.org/abs/2507.04455v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Linguistic Analysis of Spontaneous Thoughts: Investigating Experiences of D\u00e9j\u00e0 Vu, Unexpected Thoughts, and Involuntary Autobiographical Memories", "abstract": "The onset of spontaneous thoughts are reflective of dynamic interactions\nbetween cognition, emotion, and attention. Typically, these experiences are\nstudied through subjective appraisals that focus on their triggers,\nphenomenology, and emotional salience. In this work, we use linguistic\nsignatures to investigate Deja Vu, Involuntary Autobiographical Memories and\nUnexpected Thoughts. Specifically, we analyze the inherent characteristics of\nthe linguistic patterns in participant generated descriptions of these thought\ntypes. We show how, by positioning language as a window into spontaneous\ncognition, existing theories on these attentional states can be updated and\nreaffirmed. Our findings align with prior research, reinforcing that Deja Vu is\na metacognitive experience characterized by abstract and spatial language,\nInvoluntary Autobiographical Memories are rich in personal and emotionally\nsignificant detail, and Unexpected Thoughts are marked by unpredictability and\ncognitive disruption. This work is demonstrative of languages potential to\nreveal deeper insights into how internal spontaneous cognitive states manifest\nthrough expression.", "published": "2025-07-06 15:57:36", "link": "http://arxiv.org/abs/2507.04439v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Reconstructing Biological Pathways by Applying Selective Incremental Learning to (Very) Small Language Models", "abstract": "The use of generative artificial intelligence (AI) models is becoming\nubiquitous in many fields. Though progress continues to be made, general\npurpose large language AI models (LLM) show a tendency to deliver creative\nanswers, often called \"hallucinations\", which have slowed their application in\nthe medical and biomedical fields where accuracy is paramount. We propose that\nthe design and use of much smaller, domain and even task-specific LM may be a\nmore rational and appropriate use of this technology in biomedical research. In\nthis work we apply a very small LM by today's standards to the specialized task\nof predicting regulatory interactions between molecular components to fill gaps\nin our current understanding of intracellular pathways. Toward this we attempt\nto correctly posit known pathway-informed interactions recovered from manually\ncurated pathway databases by selecting and using only the most informative\nexamples as part of an active learning scheme. With this example we show that a\nsmall (~110 million parameters) LM based on a Bidirectional Encoder\nRepresentations from Transformers (BERT) architecture can propose molecular\ninteractions relevant to tuberculosis persistence and transmission with over\n80% accuracy using less than 25% of the ~520 regulatory relationships in\nquestion. Using information entropy as a metric for the iterative selection of\nnew tuning examples, we also find that increased accuracy is driven by favoring\nthe use of the incorrectly assigned statements with the highest certainty\n(lowest entropy). In contrast, the concurrent use of correct but least certain\nexamples contributed little and may have even been detrimental to the learning\nrate.", "published": "2025-07-06 15:35:45", "link": "http://arxiv.org/abs/2507.04432v1", "categories": ["q-bio.MN", "cs.CL", "cs.IT", "cs.LG", "cs.PF", "math.IT"], "primary_category": "q-bio.MN"}
{"title": "MedGellan: LLM-Generated Medical Guidance to Support Physicians", "abstract": "Medical decision-making is a critical task, where errors can result in\nserious, potentially life-threatening consequences. While full automation\nremains challenging, hybrid frameworks that combine machine intelligence with\nhuman oversight offer a practical alternative. In this paper, we present\nMedGellan, a lightweight, annotation-free framework that uses a Large Language\nModel (LLM) to generate clinical guidance from raw medical records, which is\nthen used by a physician to predict diagnoses. MedGellan uses a\nBayesian-inspired prompting strategy that respects the temporal order of\nclinical data. Preliminary experiments show that the guidance generated by the\nLLM with MedGellan improves diagnostic performance, particularly in recall and\n$F_1$ score.", "published": "2025-07-06 15:31:01", "link": "http://arxiv.org/abs/2507.04431v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "RAT: Bridging RNN Efficiency and Attention Accuracy in Language Modeling", "abstract": "Transformers have become the cornerstone of modern large-scale language\nmodels; however, their dependence on softmax attention poses a major\ncomputational bottleneck, particularly in long-context settings. In this work,\nrather than following prevalent approaches such as linear attention (or SSMs)\nand local attention, we introduce an intermediate design called \\rat between\nrecurrence and attention mechanisms. It partitions the input into chunks,\napplies a simple linear recurrence within each chunk to capture local\ndependencies, and then performs softmax attention across chunks to model\nlong-range interactions. By adjusting the size of the chunk, \\rat enables\nflexible trade-offs, combining the strengths of RNN and attention. Empirically,\nwith a chunk size of 16, the \\rat layer achieves a \\(7\\times\\) improvement in\ntraining speed with 100K token sequences and \\(9\\times\\) in generation at 4K\nsequence length, while maintaining similar or sometimes even better accuracy\ncompared to standard attention. We demonstrate this by training 1.3B parameter\nmodels from scratch and performing large-scale evaluations, including short-\nand long-context benchmarks, as well as supervised fine-tuning~(SFT). We\nfurther propose a hybrid architecture that interleaves \\rat with local\nattention. By combining efficient long-range modeling with strong local\ninteractions, this hybrid design not only improves inference speed and reduces\ncache memory usage compared to attention, but also consistently enhances\nperformance, for example, achieving an average 1 point gain in commonsense\nreasoning tasks, up to 4 points on code tasks, and a 1 point Rouge-L increase\nin a summarization SFT task. Code is available at\nhttps://github.com/CLAIRE-Labo/RAT", "published": "2025-07-06 15:08:49", "link": "http://arxiv.org/abs/2507.04416v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MOMENTS: A Comprehensive Multimodal Benchmark for Theory of Mind", "abstract": "Understanding Theory of Mind is essential for building socially intelligent\nmultimodal agents capable of perceiving and interpreting human behavior. We\nintroduce MOMENTS (Multimodal Mental States), a comprehensive benchmark\ndesigned to assess the ToM capabilities of multimodal large language models\n(LLMs) through realistic, narrative-rich scenarios presented in short films.\nMOMENTS includes over 2,344 multiple-choice questions spanning seven distinct\nToM categories. The benchmark features long video context windows and realistic\nsocial interactions that provide deeper insight into characters' mental states.\nWhile the visual modality generally enhances model performance, current systems\nstill struggle to integrate it effectively, underscoring the need for further\nresearch into AI's multimodal understanding of human behavior.", "published": "2025-07-06 15:06:30", "link": "http://arxiv.org/abs/2507.04415v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "THM@SimpleText 2025 -- Task 1.1: Revisiting Text Simplification based on Complex Terms for Non-Experts", "abstract": "Scientific text is complex as it contains technical terms by definition.\nSimplifying such text for non-domain experts enhances accessibility of\ninnovation and information. Politicians could be enabled to understand new\nfindings on topics on which they intend to pass a law, or family members of\nseriously ill patients could read about clinical trials. The SimpleText CLEF\nLab focuses on exactly this problem of simplification of scientific text. Task\n1.1 of the 2025 edition specifically handles the simplification of complex\nsentences, so very short texts with little context. To tackle this task we\ninvestigate the identification of complex terms in sentences which are\nrephrased using small Gemini and OpenAI large language models for non-expert\nreaders.", "published": "2025-07-06 15:05:54", "link": "http://arxiv.org/abs/2507.04414v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SpiritRAG: A Q&A System for Religion and Spirituality in the United Nations Archive", "abstract": "Religion and spirituality (R/S) are complex and highly domain-dependent\nconcepts which have long confounded researchers and policymakers. Due to their\ncontext-specificity, R/S are difficult to operationalize in conventional\narchival search strategies, particularly when datasets are very large, poorly\naccessible, and marked by information noise. As a result, considerable time\ninvestments and specialist knowledge is often needed to extract actionable\ninsights related to R/S from general archival sources, increasing reliance on\npublished literature and manual desk reviews. To address this challenge, we\npresent SpiritRAG, an interactive Question Answering (Q&A) system based on\nRetrieval-Augmented Generation (RAG). Built using 7,500 United Nations (UN)\nresolution documents related to R/S in the domains of health and education,\nSpiritRAG allows researchers and policymakers to conduct complex,\ncontext-sensitive database searches of very large datasets using an easily\naccessible, chat-based web interface. SpiritRAG is lightweight to deploy and\nleverages both UN documents and user provided documents as source material. A\npilot test and evaluation with domain experts on 100 manually composed\nquestions demonstrates the practical value and usefulness of SpiritRAG.", "published": "2025-07-06 13:54:54", "link": "http://arxiv.org/abs/2507.04395v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Does Learning Mathematical Problem-Solving Generalize to Broader Reasoning?", "abstract": "There has been a growing interest in enhancing the mathematical\nproblem-solving (MPS) capabilities of large language models. While the majority\nof research efforts concentrate on creating specialized models to solve\nmathematical problems, it remains unknown how learning mathematical\nproblem-solving generalizes to help develop other reasoning abilities. In this\npaper, we present an empirical investigation into the generalization potential\nof various MPS training approaches, such as continual pretraining, instruction\ntuning, and rule-based reinforcement learning across various data sources,\nincluding both short and long chain-of-thought (CoT) samples. Evaluation on 5\nmathematical and 8 general reasoning benchmarks show that continual pretraining\non math text is able to generalize to general reasoning tasks to some extent.\nIn constrast, instruction tuning on conventional, short MPS samples provides\nlimited benefits and, in many cases, even impairs generalization performance.\nNotably, training with long CoT responses for MPS samples and incorporating\nrule-based reinforcement learning on MPS queries exhibit distinct behavior,\nsignificantly enhancing generalization by extending the model's reasoning\nprocesses into other domains. These results suggest that traditional approaches\nto learning MPS with short reasoning chains largely fail to achieve robust\ngeneralization. However, the emerging paradigm of longer reasoning chains,\ncoupled with self-reflection, offers a promising direction for improving\ngeneralized reasoning abilities through learning from specialized domains.", "published": "2025-07-06 13:47:55", "link": "http://arxiv.org/abs/2507.04391v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Modal Semantic Parsing for the Interpretation of Tombstone Inscriptions", "abstract": "Tombstones are historically and culturally rich artifacts, encapsulating\nindividual lives, community memory, historical narratives and artistic\nexpression. Yet, many tombstones today face significant preservation\nchallenges, including physical erosion, vandalism, environmental degradation,\nand political shifts. In this paper, we introduce a novel multi-modal framework\nfor tombstones digitization, aiming to improve the interpretation, organization\nand retrieval of tombstone content. Our approach leverages vision-language\nmodels (VLMs) to translate tombstone images into structured Tombstone Meaning\nRepresentations (TMRs), capturing both image and text information. To further\nenrich semantic parsing, we incorporate retrieval-augmented generation (RAG)\nfor integrate externally dependent elements such as toponyms, occupation codes,\nand ontological concepts. Compared to traditional OCR-based pipelines, our\nmethod improves parsing accuracy from an F1 score of 36.1 to 89.5. We\nadditionally evaluate the model's robustness across diverse linguistic and\ncultural inscriptions, and simulate physical degradation through image fusion\nto assess performance under noisy or damaged conditions. Our work represents\nthe first attempt to formalize tombstone understanding using large\nvision-language models, presenting implications for heritage preservation.", "published": "2025-07-06 12:50:07", "link": "http://arxiv.org/abs/2507.04377v1", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Attention Slipping: A Mechanistic Understanding of Jailbreak Attacks and Defenses in LLMs", "abstract": "As large language models (LLMs) become more integral to society and\ntechnology, ensuring their safety becomes essential. Jailbreak attacks exploit\nvulnerabilities to bypass safety guardrails, posing a significant threat.\nHowever, the mechanisms enabling these attacks are not well understood. In this\npaper, we reveal a universal phenomenon that occurs during jailbreak attacks:\nAttention Slipping. During this phenomenon, the model gradually reduces the\nattention it allocates to unsafe requests in a user query during the attack\nprocess, ultimately causing a jailbreak. We show Attention Slipping is\nconsistent across various jailbreak methods, including gradient-based token\nreplacement, prompt-level template refinement, and in-context learning.\nAdditionally, we evaluate two defenses based on query perturbation, Token\nHighlighter and SmoothLLM, and find they indirectly mitigate Attention\nSlipping, with their effectiveness positively correlated with the degree of\nmitigation achieved. Inspired by this finding, we propose Attention Sharpening,\na new defense that directly counters Attention Slipping by sharpening the\nattention score distribution using temperature scaling. Experiments on four\nleading LLMs (Gemma2-9B-It, Llama3.1-8B-It, Qwen2.5-7B-It, Mistral-7B-It v0.2)\nshow that our method effectively resists various jailbreak attacks while\nmaintaining performance on benign tasks on AlpacaEval. Importantly, Attention\nSharpening introduces no additional computational or memory overhead, making it\nan efficient and practical solution for real-world deployment.", "published": "2025-07-06 12:19:04", "link": "http://arxiv.org/abs/2507.04365v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Large Language Models' Varying Accuracy in Recognizing Risk-Promoting and Health-Supporting Sentiments in Public Health Discourse: The Cases of HPV Vaccination and Heated Tobacco Products", "abstract": "Machine learning methods are increasingly applied to analyze health-related\npublic discourse based on large-scale data, but questions remain regarding\ntheir ability to accurately detect different types of health sentiments.\nEspecially, Large Language Models (LLMs) have gained attention as a powerful\ntechnology, yet their accuracy and feasibility in capturing different opinions\nand perspectives on health issues are largely unexplored. Thus, this research\nexamines how accurate the three prominent LLMs (GPT, Gemini, and LLAMA) are in\ndetecting risk-promoting versus health-supporting sentiments across two\ncritical public health topics: Human Papillomavirus (HPV) vaccination and\nheated tobacco products (HTPs). Drawing on data from Facebook and Twitter, we\ncurated multiple sets of messages supporting or opposing recommended health\nbehaviors, supplemented with human annotations as the gold standard for\nsentiment classification. The findings indicate that all three LLMs generally\ndemonstrate substantial accuracy in classifying risk-promoting and\nhealth-supporting sentiments, although notable discrepancies emerge by\nplatform, health issue, and model type. Specifically, models often show higher\naccuracy for risk-promoting sentiment on Facebook, whereas health-supporting\nmessages on Twitter are more accurately detected. An additional analysis also\nshows the challenges LLMs face in reliably detecting neutral messages. These\nresults highlight the importance of carefully selecting and validating language\nmodels for public health analyses, particularly given potential biases in\ntraining data that may lead LLMs to overestimate or underestimate the\nprevalence of certain perspectives.", "published": "2025-07-06 11:57:02", "link": "http://arxiv.org/abs/2507.04364v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "HatePRISM: Policies, Platforms, and Research Integration. Advancing NLP for Hate Speech Proactive Mitigation", "abstract": "Despite regulations imposed by nations and social media platforms, e.g.\n(Government of India, 2021; European Parliament and Council of the European\nUnion, 2022), inter alia, hateful content persists as a significant challenge.\nExisting approaches primarily rely on reactive measures such as blocking or\nsuspending offensive messages, with emerging strategies focusing on proactive\nmeasurements like detoxification and counterspeech. In our work, which we call\nHatePRISM, we conduct a comprehensive examination of hate speech regulations\nand strategies from three perspectives: country regulations, social platform\npolicies, and NLP research datasets. Our findings reveal significant\ninconsistencies in hate speech definitions and moderation practices across\njurisdictions and platforms, alongside a lack of alignment with research\nefforts. Based on these insights, we suggest ideas and research direction for\nfurther exploration of a unified framework for automated hate speech moderation\nincorporating diverse strategies.", "published": "2025-07-06 11:25:23", "link": "http://arxiv.org/abs/2507.04350v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SmartThinker: Learning to Compress and Preserve Reasoning by Step-Level Length Control", "abstract": "Large reasoning models (LRMs) have exhibited remarkable reasoning\ncapabilities through inference-time scaling, but this progress has also\nintroduced considerable redundancy and inefficiency into their reasoning\nprocesses, resulting in substantial computational waste. Previous work has\nattempted to mitigate this issue by penalizing the overall length of generated\nsamples during reinforcement learning (RL), with the goal of encouraging a more\nconcise chains of thought. However, we observe that such global length penalty\noften lead to excessive compression of critical reasoning steps while\npreserving unnecessary details in simpler ones, yielding a suboptimal trade-off\nbetween accuracy and efficiency. To address this issue, we propose\nSmartThinker, a two-stage learnable framework designed to enable fine-grained\ncontrol over the length of reasoning chains based on the importance of each\nindividual step. In the first stage, SmartThinker adapts a reasoning model to a\nshort-form reasoning mode through rejection sampling combined with supervised\nfine-tuning (SFT). In the second stage, SmartThinker applies Step-Level Length\nControl Policy Optimization (SCPO) to refine the model output distribution,\nwhich increases the proportion of length allocated to critical steps while\nreducing redundancy in less important ones. SCPO consists of four core\ncomponents: an online importance estimator, a step-level length control reward\nfunction, a step-level generalized advantage estimation (S-GAE) and a\ndifficulty-adaptive clipping strategy. Working in concert, these components\nenable SCPO to implement differentiated length control across reasoning steps.\nEmpirical results across multiple reasoning benchmarks and various backbone\nmodels demonstrate that SmartThinker significantly reduces redundant reasoning\nwhile achieving comparable or even superior performance to existing methods.", "published": "2025-07-06 11:21:47", "link": "http://arxiv.org/abs/2507.04348v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Computed Tomography Visual Question Answering with Cross-modal Feature Graphing", "abstract": "Visual question answering (VQA) in medical imaging aims to support clinical\ndiagnosis by automatically interpreting complex imaging data in response to\nnatural language queries. Existing studies typically rely on distinct visual\nand textual encoders to independently extract features from medical images and\nclinical questions, which are subsequently combined to generate answers.\nSpecifically, in computed tomography (CT), such approaches are similar to the\nconventional practices in medical image analysis. However, these approaches pay\nless attention to the spatial continuity and inter-slice correlations in the\nvolumetric CT data, leading to fragmented and imprecise responses. In this\npaper, we propose a novel large language model (LLM)-based framework enhanced\nby a graph representation of salient features. Different from conventional\nmultimodal encoding strategies, our approach constructs a cross-modal graph\nintegrating both visual and textual features, treating individual CT slices and\nquestion tokens as nodes within the graph. We further leverage an attentive\ngraph convolutional network to dynamically fuse information within this\nstructure. The resulting aggregated graph features then serve as a soft prompt\nto guide a large language model in generating accurate answers. Extensive\nexperiments on the M3D-VQA benchmark demonstrate that our approach consistently\noutperforms baselines across multiple evaluation metrics, offering more robust\nreasoning capabilities.", "published": "2025-07-06 10:37:16", "link": "http://arxiv.org/abs/2507.04333v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "No Language Data Left Behind: A Comparative Study of CJK Language Datasets in the Hugging Face Ecosystem", "abstract": "Recent advances in Natural Language Processing (NLP) have underscored the\ncrucial role of high-quality datasets in building large language models (LLMs).\nHowever, while extensive resources and analyses exist for English, the\nlandscape for East Asian languages - particularly Chinese, Japanese, and Korean\n(CJK) - remains fragmented and underexplored, despite these languages together\nserving over 1.6 billion speakers. To address this gap, we investigate the\nHuggingFace ecosystem from a cross-linguistic perspective, focusing on how\ncultural norms, research environments, and institutional practices shape\ndataset availability and quality. Drawing on more than 3,300 datasets, we\nemploy quantitative and qualitative methods to examine how these factors drive\ndistinct creation and curation patterns across Chinese, Japanese, and Korean\nNLP communities. Our findings highlight the large-scale and often\ninstitution-driven nature of Chinese datasets, grassroots community-led\ndevelopment in Korean NLP, and an entertainment- and subculture-focused\nemphasis on Japanese collections. By uncovering these patterns, we reveal\npractical strategies for enhancing dataset documentation, licensing clarity,\nand cross-lingual resource sharing - ultimately guiding more effective and\nculturally attuned LLM development in East Asia. We conclude by discussing best\npractices for future dataset curation and collaboration, aiming to strengthen\nresource development across all three languages.", "published": "2025-07-06 10:32:32", "link": "http://arxiv.org/abs/2507.04329v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LearnLens: LLM-Enabled Personalised, Curriculum-Grounded Feedback with Educators in the Loop", "abstract": "Effective feedback is essential for student learning but is time-intensive\nfor teachers. We present LearnLens, a modular, LLM-based system that generates\npersonalised, curriculum-aligned feedback in science education. LearnLens\ncomprises three components: (1) an error-aware assessment module that captures\nnuanced reasoning errors; (2) a curriculum-grounded generation module that uses\na structured, topic-linked memory chain rather than traditional\nsimilarity-based retrieval, improving relevance and reducing noise; and (3) an\neducator-in-the-loop interface for customisation and oversight. LearnLens\naddresses key challenges in existing systems, offering scalable, high-quality\nfeedback that empowers both teachers and students.", "published": "2025-07-06 08:39:26", "link": "http://arxiv.org/abs/2507.04295v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.CY"}
{"title": "Fairness Evaluation of Large Language Models in Academic Library Reference Services", "abstract": "As libraries explore large language models (LLMs) for use in virtual\nreference services, a key question arises: Can LLMs serve all users equitably,\nregardless of demographics or social status? While they offer great potential\nfor scalable support, LLMs may also reproduce societal biases embedded in their\ntraining data, risking the integrity of libraries' commitment to equitable\nservice. To address this concern, we evaluate whether LLMs differentiate\nresponses across user identities by prompting six state-of-the-art LLMs to\nassist patrons differing in sex, race/ethnicity, and institutional role. We\nfound no evidence of differentiation by race or ethnicity, and only minor\nevidence of stereotypical bias against women in one model. LLMs demonstrated\nnuanced accommodation of institutional roles through the use of linguistic\nchoices related to formality, politeness, and domain-specific vocabularies,\nreflecting professional norms rather than discriminatory treatment. These\nfindings suggest that current LLMs show a promising degree of readiness to\nsupport equitable and contextually appropriate communication in academic\nlibrary reference services.", "published": "2025-07-06 03:28:24", "link": "http://arxiv.org/abs/2507.04224v1", "categories": ["cs.CL", "cs.AI", "cs.DL"], "primary_category": "cs.CL"}
{"title": "Context Tuning for In-Context Optimization", "abstract": "We introduce Context Tuning, a simple and effective method to significantly\nenhance few-shot adaptation of language models (LLMs) without fine-tuning model\nparameters. While prompt-based adaptation techniques have demonstrated the\neffectiveness of lightweight adaptation methods for large language models\n(LLMs), they typically initialize a trainable prompt or prefix with irrelevant\ntokens for the task at hand. In contrast, Context Tuning initializes the\ntrainable prompt or prefix with task-specific demonstration examples,\nleveraging the model's inherent In-Context Learning (ICL) ability to extract\nrelevant information for improved few-shot learning performance. Extensive\nevaluations on benchmarks such as CrossFit, UnifiedQA, MMLU, BIG-Bench Hard,\nand ARC demonstrate that Context Tuning outperforms traditional prompt-based\nadaptation methods and achieves competitive accuracy to Test-Time Training with\nsignificantly higher training efficiency.", "published": "2025-07-06 03:23:53", "link": "http://arxiv.org/abs/2507.04221v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Lilith: Developmental Modular LLMs with Chemical Signaling", "abstract": "Current paradigms in Artificial Intelligence rely on layers of feedforward\nnetworks which model brain activity at the neuronal level. We conjecture that\nexpanding to the level of multiple brain regions with chemical signaling may be\na productive step toward understanding the emergence of consciousness. We\npropose LILITH, a novel architecture that combines developmental training of\nmodular language models with brain-inspired token-based communication\nprotocols, mirroring chemical signaling in the brain. Our approach models\ndistinct brain regions as specialized LLM modules including thinking, memory,\nsensory, and regulatory components that communicate through emergent\ntoken-based signaling protocols analogous to neurotransmitter networks. Unlike\ntraditional pre-trained systems, LILITH would employ developmental training\nwhere untrained LLM architectures learn through simulated life experiences,\ndeveloping communication pathways and cognitive abilities through environmental\ninteraction and evolutionary optimization. This framework would enable direct\nempirical investigation of consciousness emergence using Integrated Information\nTheory metrics while providing unprecedented insight into inter-module\nsignaling patterns during development. By optimizing for consciousness\nemergence rather than task performance, LILITH could provide insight into\ndifferent emergent phenomena at multiple levels of neural correlates,\ncontrasting neuronal-level processing with multi-region coordination dynamics.\nThe goal of this paper is to put the idea forward while recognizing the\nsubstantial challenges in implementing such a system.", "published": "2025-07-06 23:18:51", "link": "http://arxiv.org/abs/2507.04575v1", "categories": ["q-bio.NC", "cs.AI"], "primary_category": "q-bio.NC"}
{"title": "SPIRA: Building an Intelligent System for Respiratory Insufficiency Detection", "abstract": "Respiratory insufficiency is a medic symptom in which a person gets a reduced\namount of oxygen in the blood. This paper reports the experience of building\nSPIRA: an intelligent system for detecting respiratory insufficiency from\nvoice. It compiles challenges faced in two succeeding implementations of the\nsame architecture, summarizing lessons learned on data collection, training,\nand inference for future projects in similar systems.", "published": "2025-07-06 21:42:02", "link": "http://arxiv.org/abs/2507.04548v1", "categories": ["cs.SE", "cs.AI", "cs.LG", "D.2.11; D.2.7; I.2.7; I.5.4"], "primary_category": "cs.SE"}
{"title": "Short rainbow cycles for families of small edge sets", "abstract": "In 2019, Aharoni proposed a conjecture generalizing the Caceetta-H\\\"aggkvist\nconjecture: if an $n$-vertex graph $G$ admits an edge coloring (not necessarily\nproper) with $n$ colors such that each color class has size at least $r$, then\n$G$ contains a rainbow cycle of length at most $\\lceil n/r\\rceil$. Recent works\n\\cite{AG2023,ABCGZ2023,G2025} have shown that if a constant fraction of the\ncolor classes are non-star, then the rainbow girth is $O(\\log n)$. In this\nnote, we extend these results, and we show that even a small fraction of\nnon-star color classes suffices to ensure logarithmic rainbow girth. We also\nprove that the logarithmic bound is of the right order of magnitude. Moreover,\nwe determine the threshold fraction between the types of color classes at which\nthe rainbow girth transitions from linear to logarithmic.", "published": "2025-07-06 23:54:53", "link": "http://arxiv.org/abs/2507.04581v1", "categories": ["math.CO", "cs.DM", "05C35, 05C38, 05D40"], "primary_category": "math.CO"}
{"title": "On Modular Edge Colourings of Graphs", "abstract": "Given a graph $G$ and an integer $k\\geq 2$, let $\\chi'_k(G)$ denote the\nminimum number of colours required to colour the edges of $G$ such that, in\neach colour class, the subgraph induced by the edges of that colour has all\nnon-zero degrees congruent to $1$ modulo $k$. In 1992, Pyber proved that\n$\\chi'_2(G) \\leq 4$ for every graph $G$, and posed the question of whether\n$\\chi'_k(G)$ can be bounded solely in terms of $k$ for every $k\\geq 3$. This\nquestion was answered in 1997 by Scott, who showed that $\\chi'_k(G)\\leq5k^2\\log\nk$, and further asked whether $\\chi'_k(G) = O(k)$. Recently, Botler, Colucci,\nand Kohayakawa (2023) answered Scott's question affirmatively proving that\n$\\chi'_k(G) \\leq 198k - 101$, and conjectured that the multiplicative constant\ncould be reduced to $1$. A step towards this latter conjecture was made in 2024\nby Nweit and Yang, who improved the bound to $\\chi'_k(G) \\leq 177k - 93$. In\nthis paper, we further improve the multiplicative constant to $9$. More\nspecifically, we prove that there is a function $f\\in o(k)$ for which\n$\\chi'_k(G) \\leq 7k + f(k)$ if $k$ is odd, and $\\chi'_k(G) \\leq 9k + f(k)$ if\n$k$ is even. In doing so, we prove that $\\chi'_k(G) \\leq k + O(d)$ for every\n$d$-degenerate graph $G$, which plays a central role in our proof.", "published": "2025-07-06 06:10:41", "link": "http://arxiv.org/abs/2507.04254v1", "categories": ["math.CO", "cs.DM"], "primary_category": "math.CO"}
{"title": "Multimedia Verification Through Multi-Agent Deep Research Multimodal Large Language Models", "abstract": "This paper presents our submission to the ACMMM25 - Grand Challenge on\nMultimedia Verification. We developed a multi-agent verification system that\ncombines Multimodal Large Language Models (MLLMs) with specialized verification\ntools to detect multimedia misinformation. Our system operates through six\nstages: raw data processing, planning, information extraction, deep research,\nevidence collection, and report generation. The core Deep Researcher Agent\nemploys four tools: reverse image search, metadata analysis, fact-checking\ndatabases, and verified news processing that extracts spatial, temporal,\nattribution, and motivational context. We demonstrate our approach on a\nchallenge dataset sample involving complex multimedia content. Our system\nsuccessfully verified content authenticity, extracted precise geolocation and\ntiming information, and traced source attribution across multiple platforms,\neffectively addressing real-world multimedia verification scenarios.", "published": "2025-07-06 14:54:07", "link": "http://arxiv.org/abs/2507.04410v1", "categories": ["cs.CV", "cs.AI", "cs.IR", "I.2.10"], "primary_category": "cs.CV"}
{"title": "BiFair: A Fairness-aware Training Framework for LLM-enhanced Recommender Systems via Bi-level Optimization", "abstract": "Large Language Model-enhanced Recommender Systems (LLM-enhanced RSs) have\nemerged as a powerful approach to improving recommendation quality by\nleveraging LLMs to generate item representations. Despite these advancements,\nthe integration of LLMs raises severe fairness concerns. Existing studies\nreveal that LLM-based RSs exhibit greater unfairness than traditional RSs, yet\nfairness issues in LLM-enhanced RSs remain largely unexplored. In this paper,\nour empirical study reveals that while LLM-enhanced RSs improve fairness across\nitem groups, a significant fairness gap persists. Further enhancement remains\nchallenging due to the architectural differences and varying sources of\nunfairness inherent in LLM-enhanced RSs. To bridge this gap, we first decompose\nunfairness into i) \\textit{prior unfairness} in LLM-generated representations\nand ii) \\textit{training unfairness} in recommendation models. Then, we propose\nBiFair, a bi-level optimization-based fairness-aware training framework\ndesigned to mitigate both prior and training unfairness simultaneously. BiFair\noptimizes two sets of learnable parameters: LLM-generated representations and a\ntrainable projector in the recommendation model, using a two-level nested\noptimization process. Additionally, we introduce an adaptive inter-group\nbalancing mechanism, leveraging multi-objective optimization principles to\ndynamically balance fairness across item groups. Extensive experiments on three\nreal-world datasets demonstrate that BiFair significantly mitigates unfairness\nand outperforms previous state-of-the-art methods.", "published": "2025-07-06 08:39:26", "link": "http://arxiv.org/abs/2507.04294v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "High-Resolution Sustain Pedal Depth Estimation from Piano Audio Across Room Acoustics", "abstract": "Piano sustain pedal detection has previously been approached as a binary\non/off classification task, limiting its application in real-world piano\nperformance scenarios where pedal depth significantly influences musical\nexpression. This paper presents a novel approach for high-resolution estimation\nthat predicts continuous pedal depth values. We introduce a Transformer-based\narchitecture that not only matches state-of-the-art performance on the\ntraditional binary classification task but also achieves high accuracy in\ncontinuous pedal depth estimation. Furthermore, by estimating continuous\nvalues, our model provides musically meaningful predictions for sustain pedal\nusage, whereas baseline models struggle to capture such nuanced expressions\nwith their binary detection approach. Additionally, this paper investigates the\ninfluence of room acoustics on sustain pedal estimation using a synthetic\ndataset that includes varied acoustic conditions. We train our model with\ndifferent combinations of room settings and test it in an unseen new\nenvironment using a \"leave-one-out\" approach. Our findings show that the two\nbaseline models and ours are not robust to unseen room conditions. Statistical\nanalysis further confirms that reverberation influences model predictions and\nintroduces an overestimation bias.", "published": "2025-07-06 03:40:54", "link": "http://arxiv.org/abs/2507.04230v1", "categories": ["cs.SD", "cs.AI", "cs.IR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Digital-Twin Empowered Site-Specific Radio Resource Management in 5G Aerial Corridor", "abstract": "Base station (BS) association and beam selection in multi-cell drone corridor\nnetworks present unique challenges due to the high altitude, mobility and\nthree-dimensional movement of drones. These factors lead to frequent handovers\nand complex beam alignment issues, especially in environments with dense BS\ndeployments and varying signal conditions. To address these challenges, this\npaper proposes a channel-twin (CT) enabled resource-allocation framework for\ndrone-corridor communications, where the CT constitutes the radio-channel\ncomponent of a broader digital-twin (DT) environment. The CT supplies\nhigh-fidelity channel-state information (CSI), which drives a two-stage\noptimization procedure. In Stage 1, array-level beamforming weights at each BS\nare selected to maximize antenna gain. In Stage 2, the framework jointly\noptimizes drone-BS-beam associations at discrete corridor way-points to\nmaximize end-to-end throughput. Simulation results confirm that the CT-driven\nstrategy delivers significant throughput gains over baseline methods across\ndiverse operating scenarios, validating the effectiveness of integrating\nprecise digital-twin channel models with cross-layer resource optimization.", "published": "2025-07-06 22:42:19", "link": "http://arxiv.org/abs/2507.04566v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "LINE: Public-key encryption", "abstract": "We propose a public key encryption cryptosystem based on solutions of linear\nequation systems with predefinition of input parameters through shared secret\ncomputation for factorizable substitutions. The existence of multiple\nequivalent solutions for an underdetermined system of linear equations\ndetermines the impossibility of its resolution by a cryptanalyst in polynomial\ntime. The completion of input parameters of the equation system is implemented\nthrough secret homomorphic matrix transformation for substitutions factorized\nover the basis of a vector space of dimension m over the field F2. Encryption\nis implemented through computation of substitutions that are one-way functions\non an elementary abelian 2-group of order 2\"m. Decryption is implemented\nthrough completion of input parameters of the equation system. Homomorphic\ntransformations are constructed based on matrix computations. Matrix\ncomputations enable the implementation of high security and low computational\noverhead for homomorphic transformations.", "published": "2025-07-06 18:38:04", "link": "http://arxiv.org/abs/2507.04501v1", "categories": ["cs.CR", "cs.IT", "math.IT"], "primary_category": "cs.CR"}
{"title": "Entropy measures as indicators of connectivity paths in the human brain", "abstract": "How does the information flow between different brain regions during various\nstimuli? This is the question we aim to address by studying complex cognitive\nparadigms in terms of Information Theory. To assess creativity and the\nemergence of patterns from a Shannon perspective, we applied a range of tools,\nincluding Entropy Density, Effective Measure Complexity, and the Lempel-Ziv\ndistance. These entropic tools enable the detection of both linear and\nnon-linear dynamics without relying on pre-established parameters, models, or\nprior assumptions about the data. To identify connections between different\nbrain regions, we analyse task-based fMRI data from subjects during motor,\nworking memory, emotion recognition, and language stimuli to gain insight into\nthese complex cognitive processes. Since this method does not rely on prior\nknowledge, it is particularly well-suited for exploratory research,\nfacilitating the discovery of previously unidentified connections or patterns\nin the brain. The capacity to identify non-linear dynamics is especially\nimportant for studying brain connectivity, as the brain exhibits significant\nnon-linear interactions across multiple functional levels.", "published": "2025-07-06 16:05:38", "link": "http://arxiv.org/abs/2507.04442v1", "categories": ["q-bio.NC", "cs.IT", "math.IT"], "primary_category": "q-bio.NC"}
{"title": "High-Availability Integrity Monitoring for Multi-Constellation GNSS Navigation with Non-Gaussian Errors", "abstract": "Global navigation satellite systems (GNSS) are essential for aviation,\nrequiring strict integrity monitoring to alert users to hazardously misleading\ninformation. Conventional receiver autonomous integrity monitoring (RAIM) and\nadvanced RAIM (ARAIM) rely heavily on Gaussian models in bounding nominal\nerrors, which can be overly conservative with real-world non-Gaussian errors\nwith heavy tails, such as the satellite clock and orbit errors. This paper\nproposes an extended jackknife detector capable of detecting multiple\nsimultaneous faults with non-Gaussian nominal errors. Furthermore, an integrity\nmonitoring algorithm, jackknife ARAIM, is developed by systematically\nexploiting the properties of the jackknife detector in the range domain. A\ntight bound of the integrity risk is derived by quantifying the impacts of\nhypothetical fault vectors on the position solution. The proposed method is\nexamined in worldwide simulations, with the nominal measurement error simulated\nbased on authentic experimental data, which reveals different findings in\nexisting research. In a setting of a single Global Positioning System (GPS)\nconstellation, the proposed method reduces the 99.5 percentile vertical\nprotection level (VPL) 45m, where the VPL of the baseline ARAIM is larger than\n50m in most user locations. For dual-constellation (GPS-Galileo) settings,\nbaseline ARAIM suffers VPL inflation over 60m due to the over-conservatism\ninduced by the heavy-tailed Galileo signal-in-space range errors, whereas the\nproposed jackknife ARAIM retains VPL below 40m, achieving over 92% normal\noperations for a 35m Vertical Alert Limit. These improvements have promising\npotential to support localizer performance with vertical guidance (LPV) with a\ndecision height of 200 ft, enhancing integrity and availability for\nmulti-constellation GNSS applications.", "published": "2025-07-06 07:57:29", "link": "http://arxiv.org/abs/2507.04284v1", "categories": ["eess.SP", "cs.IT", "math.IT"], "primary_category": "eess.SP"}
{"title": "Mutual Information Bounds for Lossy Common Information", "abstract": "We show the mutual information between the targets in a Gray-Wyner Network as\na bound that separates Wyner's lossy common information and G\\'acs-K\\\"orner\nlossy common information. The results are a generalization of the lossless case\npresented by Wyner (1975).", "published": "2025-07-06 01:39:00", "link": "http://arxiv.org/abs/2507.04209v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "An Efficient Max-Min Fair Resource Optimization Algorithm for Rate-Splitting Multiple Access", "abstract": "The max-min fairness (MMF) problem in rate-splitting multiple access (RSMA)\nis known to be challenging due to its non-convex and non-smooth nature, as well\nas the coupled beamforming and common rate variables. Conventional algorithms\nto address this problem often incur high computational complexity or degraded\nMMF rate performance. To address these challenges, in this work, we propose a\nnovel optimization algorithm named extragradient-fractional programming (EG-FP)\nto address the MMF problem of downlink RSMA. The proposed algorithm first\nleverages FP to transform the original problem into a block-wise convex\nproblem. For the subproblem of precoding block, we show that its Lagrangian\ndual is equivalent to a variational inequality problem, which is then solved\nusing an extragradient-based algorithm. Additionally, we discover the optimal\nbeamforming structure of the problem and based on which, we introduce a\nlow-dimensional EG-FP algorithm with computational complexity independent of\nthe number of transmit antennas. This feature is especially beneficial in\nscenarios with a large number of transmit antennas. The proposed algorithms are\nthen extended to handle imperfect channel state information at the transmitter\n(CSIT). Numerical results demonstrate that the MMF rate achieved by our\nproposed algorithms closely matches that of the conventional successive convex\napproximation (SCA) algorithm and significantly outperforms other baseline\nschemes. Remarkably, the average CPU time of the proposed algorithms is less\nthan 10\\% of the runtime required by the SCA algorithm, showing the efficiency\nand scalability of the proposed algorithms.", "published": "2025-07-06 01:11:13", "link": "http://arxiv.org/abs/2507.04201v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Self-supervised learning of speech representations with Dutch archival data", "abstract": "This paper explores the use of Dutch archival television broadcast data for\nself-supervised learning of speech foundation models, specifically wav2vec 2.0.\nWe first study data quality assumptions for pre-training, and show how music,\nnoise and speaker overlap affect SSL convergence and downstream fine-tuning\nperformance. Secondly, we explore effectively pre-processing strategies to\nconvert the noisy broadcast dataset into a qualitative dataset for\npre-training, by using Whisper and WhisperX., Thirdly, we compare mono-lingual\nand multi-lingual pre-training with equivalent amounts of data, and show that\nmono-lingual pre-training is more robust to out-of-domain data. Lastly, we\nachieve a state-of-the-art LARGE wav2vec 2.0 model for the Dutch language, by a\ncontinuation of pre-training a wav2vec 2.0 XLS-R model checkpoint with our 55k\nhour archival dataset.", "published": "2025-07-06 22:11:22", "link": "http://arxiv.org/abs/2507.04554v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "da4ml: Distributed Arithmetic for Real-time Neural Networks on FPGAs", "abstract": "Neural networks with a latency requirement on the order of microseconds, like\nthe ones used at the CERN Large Hadron Collider, are typically deployed on\nFPGAs fully unrolled and pipelined. A bottleneck for the deployment of such\nneural networks is area utilization, which is directly related to the required\nconstant matrix-vector multiplication (CMVM) operations. In this work, we\npropose an efficient algorithm for implementing CMVM operations with\ndistributed arithmetic (DA) on FPGAs that simultaneously optimizes for area\nconsumption and latency. The algorithm achieves resource reduction similar to\nstate-of-the-art algorithms while being significantly faster to compute. The\nproposed algorithm is open-sourced and integrated into the \\texttt{hls4ml}\nlibrary, a free and open-source library for running real-time neural network\ninference on FPGAs. We show that the proposed algorithm can reduce on-chip\nresources by up to a third for realistic, highly quantized neural networks\nwhile simultaneously reducing latency, enabling the implementation of\npreviously infeasible networks.", "published": "2025-07-06 21:01:32", "link": "http://arxiv.org/abs/2507.04535v1", "categories": ["cs.AR", "cs.LG", "hep-ex", "B.2.4; B.6"], "primary_category": "cs.AR"}
{"title": "Verification of Visual Controllers via Compositional Geometric Transformations", "abstract": "Perception-based neural network controllers are increasingly used in\nautonomous systems that rely on visual inputs to operate in the real world.\nEnsuring the safety of such systems under uncertainty is challenging. Existing\nverification techniques typically focus on Lp-bounded perturbations in the\npixel space, which fails to capture the low-dimensional structure of many\nreal-world effects. In this work, we introduce a novel verification framework\nfor perception-based controllers that can generate outer-approximations of\nreachable sets through explicitly modeling uncertain observations with\ngeometric perturbations. Our approach constructs a boundable mapping from\nstates to images, enabling the use of state-based verification tools while\naccounting for uncertainty in perception. We provide theoretical guarantees on\nthe soundness of our method and demonstrate its effectiveness across benchmark\ncontrol environments. This work provides a principled framework for certifying\nthe safety of perception-driven control systems under realistic visual\nperturbations.", "published": "2025-07-06 20:22:58", "link": "http://arxiv.org/abs/2507.04523v1", "categories": ["cs.RO", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Thousand-Brains Systems: Sensorimotor Intelligence for Rapid, Robust Learning and Inference", "abstract": "Current AI systems achieve impressive performance on many tasks, yet they\nlack core attributes of biological intelligence, including rapid, continual\nlearning, representations grounded in sensorimotor interactions, and structured\nknowledge that enables efficient generalization. Neuroscience theory suggests\nthat mammals evolved flexible intelligence through the replication of a\nsemi-independent, sensorimotor module, a functional unit known as a cortical\ncolumn. To address the disparity between biological and artificial\nintelligence, thousand-brains systems were proposed as a means of mirroring the\narchitecture of cortical columns and their interactions.\n  In the current work, we evaluate the unique properties of Monty, the first\nimplementation of a thousand-brains system. We focus on 3D object perception,\nand in particular, the combined task of object recognition and pose estimation.\nUtilizing the YCB dataset of household objects, we first assess Monty's use of\nsensorimotor learning to build structured representations, finding that these\nenable robust generalization. These representations include an emphasis on\nclassifying objects by their global shape, as well as a natural ability to\ndetect object symmetries. We then explore Monty's use of model-free and\nmodel-based policies to enable rapid inference by supporting principled\nmovements. We find that such policies complement Monty's modular architecture,\na design that can accommodate communication between modules to further\naccelerate inference speed via a novel `voting' algorithm. Finally, we examine\nMonty's use of associative, Hebbian-like binding to enable rapid, continual,\nand computationally efficient learning, properties that compare favorably to\ncurrent deep learning architectures. While Monty is still in a nascent stage of\ndevelopment, these findings support thousand-brains systems as a powerful and\npromising new approach to AI.", "published": "2025-07-06 18:11:07", "link": "http://arxiv.org/abs/2507.04494v1", "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.RO"], "primary_category": "cs.AI"}
{"title": "Machine Learning-Based Prediction of Metal-Organic Framework Materials: A Comparative Analysis of Multiple Models", "abstract": "Metal-organic frameworks (MOFs) have emerged as promising materials for\nvarious applications due to their unique structural properties and versatile\nfunctionalities. This study presents a comprehensive investigation of machine\nlearning approaches for predicting MOF material properties. We employed five\ndifferent machine learning models: Random Forest, XGBoost, LightGBM, Support\nVector Machine, and Neural Network, to analyze and predict MOF characteristics\nusing a dataset from the Kaggle platform. The models were evaluated using\nmultiple performance metrics, including RMSE, R^2, MAE, and cross-validation\nscores. Results demonstrated that the Random Forest model achieved superior\nperformance with an R^2 value of 0.891 and RMSE of 0.152, significantly\noutperforming other models. LightGBM showed remarkable computational\nefficiency, completing training in 25.7 seconds while maintaining high\naccuracy. Our comparative analysis revealed that ensemble learning methods\ngenerally exhibited better performance than traditional single models in MOF\nproperty prediction. This research provides valuable insights into the\napplication of machine learning in materials science and establishes a robust\nframework for future MOF material design and property prediction.", "published": "2025-07-06 18:10:00", "link": "http://arxiv.org/abs/2507.04493v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Agentic Distributed Computing", "abstract": "The most celebrated and extensively studied model of distributed computing is\nthe {\\em message-passing model,} in which each vertex/node of the (distributed\nnetwork) graph corresponds to a static computational device that communicates\nwith other devices through passing messages. In this paper, we consider the\n{\\em agentic model} of distributed computing which extends the message-passing\nmodel in a new direction. In the agentic model, computational devices are\nmodeled as relocatable or mobile computational devices (called agents in this\npaper), i.e., each vertex/node of the graph serves as a container for the\ndevices, and hence communicating with another device requires relocating to the\nsame node. We study two fundamental graph level tasks, leader election, and\nminimum spanning tree, in the agentic model, which will enhance our\nunderstanding of distributed computation across paradigms. The objective is to\nminimize both time and memory complexities. Following the literature, we\nconsider the synchronous setting in which each agent performs its operations\nsynchronously with others, and hence the time complexity can be measured in\nrounds. In this paper, we present two deterministic algorithms for leader\nelection: one for the case of $k<n$ and another for the case of $k=n$,\nminimizing both time and memory complexities, where $k$ and $n$, respectively,\nare the number of agents and number of nodes of the graph. Using these leader\nelection results, we develop deterministic algorithms for agents to construct a\nminimum spanning tree of the graph, minimizing both time and memory\ncomplexities. To the best of our knowledge, this is the first study of\ndistributed graph level tasks in the agentic model with $k\\leq n$. Previous\nstudies only considered the case of $k=n$.", "published": "2025-07-06 16:38:27", "link": "http://arxiv.org/abs/2507.04459v1", "categories": ["cs.DC", "cs.DS", "cs.MA", "cs.RO"], "primary_category": "cs.DC"}
{"title": "MOD-X: A Modular Open Decentralized eXchange Framework proposal for Heterogeneous Interoperable Artificial Agents", "abstract": "As Artificial Intelligence systems evolve from monolithic models to\necosystems of specialized agents, the need for standardized communication\nprotocols becomes increasingly critical. This paper introduces MOD-X (Modular\nOpen Decentralized eXchange), a novel architectural framework proposal for\nagent interoperability that addresses key limitations of existing protocols.\nUnlike current approaches, MOD-X proposes a layered architecture with a\nUniversal Message Bus, thorough state management, translation capabilities, and\nblockchain-based security mechanisms. We present MOD-X's architecture, compare\nit with existing protocols, and demonstrate its application through a worked\nexample how it enables integration between heterogeneous specialist agents\n(agents with different architectures, vendors, capabilities, and knowledge\nrepresentations--including rule-based systems, neural networks, symbolic\nreasoning engines, and legacy software with agent wrappers). MOD-X's key\ninnovations include a publish-subscribe communication model, semantic\ncapability discovery, and dynamic workflow orchestration--providing a framework\nthat bridges theoretical formalism with practical implementation. This\narchitecture addresses the growing need for truly decentralized, interoperable\nagent ecosystems that can scale effectively without the need for central\ncoordination.", "published": "2025-07-06 12:46:57", "link": "http://arxiv.org/abs/2507.04376v1", "categories": ["cs.AI", "cs.DC", "cs.MA", "cs.NI"], "primary_category": "cs.AI"}
{"title": "Optimal Sizing and Control of a Grid-Connected Battery in a Stacked Revenue Model Including an Energy Community", "abstract": "Recent years have seen rapid increases in intermittent renewable generation,\nrequiring novel battery energy storage systems (BESS) solutions. One recent\ntrend is the emergence of large grid-connected batteries, that can be\ncontrolled to provide multiple storage and flexibility services, using a\nstacked revenue model. Another emerging development is renewable energy\ncommunities (REC), in which prosumers invest in their own renewable generation\ncapacity, but also requiring battery storage for flexibility. In this paper, we\nstudy settings in which energy communities rent battery capacity from a battery\noperator through a battery-as-a-service (BaaS) model. We present a methodology\nfor determining the sizing and pricing of battery capacity that can be rented,\nsuch that it provides economic benefits to both the community and the battery\noperator that participates in the energy market. We examine how sizes and\nprices vary across a number of different scenarios for different types of\ntariffs (flat, dynamic) and competing energy market uses. Second, we conduct a\nsystematic study of linear optimization models for battery control when\ndeployed to provide flexibility to energy communities. We show that existing\napproaches for battery control with daily time windows have a number of\nimportant limitations in practical deployments, and we propose a number of\nregularization functions in the optimization to address them. Finally, we\ninvestigate the proposed method using real generation, demand, tariffs, and\nbattery data, based on a practical case study from a large battery operator in\nthe Netherlands. For the settings in our case study, we find that a community\nof 200 houses with a 330 kW wind turbine can save up to 12,874 euros per year\nby renting just 280 kWh of battery capacity (after subtracting battery rental\ncosts), with the methodology applicable to a wide variety of settings and\ntariff types.", "published": "2025-07-06 11:08:27", "link": "http://arxiv.org/abs/2507.04343v1", "categories": ["eess.SY", "cs.MA", "cs.SY", "math.OC", "90C08", "I.2.1; I.2.11"], "primary_category": "eess.SY"}
{"title": "Kernels of trace operators via fine continuity", "abstract": "We study traces of elements of fractional Sobolev spaces\n$H_p^\\alpha(\\mathbb{R}^n)$ on closed subsets $\\Gamma$ of $\\mathbb{R}^n$, given\nas the supports of suitable measures $\\mu$. We prove that if these measures\nsatisfy localized upper density conditions, then quasi continuous\nrepresentatives vanish quasi everywhere on $\\Gamma$ if and only if they vanish\n$\\mu$-almost everywhere on $\\Gamma$. We use this result to characterize the\nkernel of the trace operator mapping from $H_p^\\alpha(\\mathbb{R}^n)$ into the\nspace of $\\mu$-equivalence classes of functions on $\\Gamma$ as the closure of\n$C_c^\\infty(\\mathbb{R}^n\\setminus \\Gamma)$ in $H_p^\\alpha(\\mathbb{R}^n)$. The\nmeasures do not have to satisfy a doubling condition. In particular, the set\n$\\Gamma$ may be a finite union of closed sets having different Hausdorff\ndimensions. We provide corresponding results for fractional Sobolev spaces\n$H_p^\\alpha(\\Omega)$ on domains $\\Omega\\subset \\mathbb{R}^n$ satisfying the\nmeasure density condition.", "published": "2025-07-06 21:05:11", "link": "http://arxiv.org/abs/2507.04536v1", "categories": ["math.FA", "cs.NA", "math.AP", "math.NA", "28A12, 28A75, 28A80, 31B15, 31C40, 46E35, 46N40"], "primary_category": "math.FA"}
{"title": "Energy-conserving Kansa methods for Hamiltonian wave equations", "abstract": "We introduce a fast, constrained meshfree solver designed specifically to\ninherit energy conservation (EC) in second-order time-dependent Hamiltonian\nwave equations. For discretization, we adopt the Kansa method, also known as\nthe kernel-based collocation method, combined with time-stepping. This approach\nensures that the critical structural feature of energy conservation is\nmaintained over time by embedding a quadratic constraint into the definition of\nthe numerical solution. To address the computational challenges posed by the\nnonlinearity in the Hamiltonian wave equations and the EC constraint, we\npropose a fast iterative solver based on the Newton method with successive\nlinearization. This novel solver significantly accelerates the computation,\nmaking the method highly effective for practical applications. Numerical\ncomparisons with the traditional secant methods highlight the competitive\nperformance of our scheme. These results demonstrate that our method not only\nconserves the energy but also offers a promising new direction for solving\nHamiltonian wave equations more efficiently. While we focus on the Kansa method\nand corresponding convergence theories in this study, the proposed solver is\nbased solely on linear algebra techniques and has the potential to be applied\nto EC constrained optimization problems arising from other PDE discretization\nmethods.", "published": "2025-07-06 11:49:54", "link": "http://arxiv.org/abs/2507.04361v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Entropy stable high-order discontinuous Galerkin spectral-element methods on curvilinear, hybrid meshes", "abstract": "Hyperbolic-parabolic partial differential equations are widely used for the\nmodeling of complex, multiscale problems. High-order methods such as the\ndiscontinuous Galerkin (DG) scheme are attractive candidates for their\nnumerical approximation. However, high-order methods are prone to instabilities\nin the presence of underresolved flow features. A popular counter measure to\nstabilize DG methods is the use of entropy-stable formulations based on\nsummation-by-parts (SBP) operators. The present paper aims to construct a\nrobust and efficient entropy-stable discontinuous Galerkin spectral element\nmethod (DGSEM) of arbitrary order on heterogeneous, curvilinear grids composed\nof triangular and quadrilateral elements or hexahedral, prismatic, tetrahedral\nand pyramid elements. To the author's knowledge, with the exception of\nhexahedral and quadrilateral elements, entropy-stable DGSE operators have been\nconstructed exclusively for tetrahedral and triangular meshes. The extension of\nthe DGSEM to more complex element shapes is achieved by means of a collapsed\ncoordinate transformation. Legendre--Gauss quadrature nodes are employed as\ncollocation points in conjunction with a generalized SBP operator and\nentropy-projected variables. The purely hyperbolic operator is extended to\nhyperbolic-parabolic problems by the use of a lifting procedure. To circumvent\nthe penalizing time step restriction imposed by the collapsing, modal rather\nthan nodal degrees of freedom are evolved in time, thereby relying on a\nmemory-efficient weight-adjusted approximation to the inverse of the mass\nmatrix. Essential properties of the proposed numerical scheme including\nfree-stream preservation, polynomial and grid convergence as well as entropy\nconservation / stability are verified. Finally, with the flow around the common\nresearch model, the applicability of the presented method to real-world\nproblems is demonstrated.", "published": "2025-07-06 10:42:27", "link": "http://arxiv.org/abs/2507.04334v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "The median trick does not help for fully nested scrambling", "abstract": "In randomized quasi-Monte Carlo methods for numerical integration, average\nestimators based on digital nets with fully nested and linear scrambling are\nknown to exhibit the same variance. In this note, we show that this equivalence\ndoes not extend to the median estimators. Specifically, while the median\nestimator with linear scrambling can achieve faster convergence for smooth\nintegrands, the median estimator with fully nested scrambling does not exhibit\nthis advantage.", "published": "2025-07-06 08:51:22", "link": "http://arxiv.org/abs/2507.04297v1", "categories": ["math.NA", "cs.NA", "65C05, 65D30, 65D32"], "primary_category": "math.NA"}
{"title": "Adaptive Multiquadratic Radial Basis Function-based Explicit Runge--Kutta Methods", "abstract": "Runge--Kutta (RK) methods are widely used techniques for solving a class of\ninitial value problems. In this article, we introduce an adaptive\nmultiquadratic (MQ) radial basis function (RBF)-based method to develop\nenhanced explicit RK methods. These methods achieve a higher order of\nconvergence than the corresponding classical RK methods. To improve the local\nconvergence of the numerical solution, we optimize the free parameters (shape\nfunctions) involved in the RBFs by forcing the local truncation errors to\nvanish. We also present a convergence and stability analysis of the proposed\nmethods. To demonstrate the advantages of these methods in terms of accuracy\nand convergence, we consider several numerical examples and compare the\nperformance of our methods with that of the classical RK methods. The Tables\nand Figures presented in this article clearly validate the superiority of the\nproposed methods.", "published": "2025-07-06 06:44:15", "link": "http://arxiv.org/abs/2507.04261v1", "categories": ["math.NA", "cs.NA", "65L06, 65L05, 65D12"], "primary_category": "math.NA"}
{"title": "Generalized Rellich's lemmas, uniqueness theorem and inside-out duality for scattering poles", "abstract": "Scattering poles correspond to non-trivial scattered fields in the absence of\nincident waves and play a crucial role in the study of wave phenomena. These\npoles are complex wavenumbers with negative imaginary parts. In this paper, we\nprove two generalized Rellich's lemmas for scattered fields associated with\ncomplex wavenumbers. These lemmas are then used to establish uniqueness results\nfor inverse scattering problems. We further explore the inside-out duality,\nwhich characterizes scattering poles through the linear sampling method applied\nto interior scattering problems. Notably, we demonstrate that exterior\nDirichlet/Neumann poles can be identified without prior knowledge of the actual\nsound-soft or sound-hard obstacles. Numerical examples are provided to validate\nthe theoretical results.", "published": "2025-07-06 04:38:12", "link": "http://arxiv.org/abs/2507.04242v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "AL-SPCE -- Reliability analysis for nondeterministic models using stochastic polynomial chaos expansions and active learning", "abstract": "Reliability analysis typically relies on deterministic simulators, which\nyield repeatable outputs for identical inputs. However, many real-world systems\ndisplay intrinsic randomness, requiring stochastic simulators whose outputs are\nrandom variables. This inherent variability must be accounted for in\nreliability analysis. While Monte Carlo methods can handle this, their high\ncomputational cost is often prohibitive. To address this, stochastic emulators\nhave emerged as efficient surrogate models capable of capturing the random\nresponse of simulators at reduced cost. Although promising, current methods\nstill require large training sets to produce accurate reliability estimates,\nwhich limits their practicality for expensive simulations. This work introduces\nan active learning framework to further reduce the computational burden of\nreliability analysis using stochastic emulators. We focus on stochastic\npolynomial chaos expansions (SPCE) and propose a novel learning function that\ntargets regions of high predictive uncertainty relevant to failure probability\nestimation. To quantify this uncertainty, we exploit the asymptotic normality\nof the maximum likelihood estimator. The resulting method, named active\nlearning stochastic polynomial chaos expansions (AL-SPCE), is applied to three\ntest cases. Results demonstrate that AL-SPCE maintains high accuracy in\nreliability estimates while significantly improving efficiency compared to\nconventional surrogate-based methods and direct Monte Carlo simulation. This\nconfirms the potential of active learning in enhancing the practicality of\nstochastic reliability analysis for complex, computationally expensive models.", "published": "2025-07-06 22:07:57", "link": "http://arxiv.org/abs/2507.04553v1", "categories": ["stat.ME", "stat.CO", "stat.ML"], "primary_category": "stat.ME"}
{"title": "Dealing with Uncertainty in Contextual Anomaly Detection", "abstract": "Contextual anomaly detection (CAD) aims to identify anomalies in a target\n(behavioral) variable conditioned on a set of contextual variables that\ninfluence the normalcy of the target variable but are not themselves indicators\nof anomaly. In many anomaly detection tasks, there exist contextual variables\nthat influence the normalcy of the target variable but are not themselves\nindicators of anomaly. In this work, we propose a novel framework for CAD,\nnormalcy score (NS), that explicitly models both the aleatoric and epistemic\nuncertainties. Built on heteroscedastic Gaussian process regression, our method\nregards the Z-score as a random variable, providing confidence intervals that\nreflect the reliability of the anomaly assessment. Through experiments on\nbenchmark datasets and a real-world application in cardiology, we demonstrate\nthat NS outperforms state-of-the-art CAD methods in both detection accuracy and\ninterpretability. Moreover, confidence intervals enable an adaptive,\nuncertainty-driven decision-making process, which may be very important in\ndomains such as healthcare.", "published": "2025-07-06 18:02:11", "link": "http://arxiv.org/abs/2507.04490v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Transfer Learning in Infinite Width Feature Learning Networks", "abstract": "We develop a theory of transfer learning in infinitely wide neural networks\nwhere both the pretraining (source) and downstream (target) task can operate in\na feature learning regime. We analyze both the Bayesian framework, where\nlearning is described by a posterior distribution over the weights, and\ngradient flow training of randomly initialized networks trained with weight\ndecay. Both settings track how representations evolve in both source and target\ntasks. The summary statistics of these theories are adapted feature kernels\nwhich, after transfer learning, depend on data and labels from both source and\ntarget tasks. Reuse of features during transfer learning is controlled by an\nelastic weight coupling which controls the reliance of the network on features\nlearned during training on the source task. We apply our theory to linear and\npolynomial regression tasks as well as real datasets. Our theory and\nexperiments reveal interesting interplays between elastic weight coupling,\nfeature learning strength, dataset size, and source and target task alignment\non the utility of transfer learning.", "published": "2025-07-06 16:14:43", "link": "http://arxiv.org/abs/2507.04448v1", "categories": ["cs.LG", "cond-mat.dis-nn", "stat.ML"], "primary_category": "cs.LG"}
{"title": "The Joys of Categorical Conformal Prediction", "abstract": "Conformal prediction (CP) is an Uncertainty Representation technique that\ndelivers finite-sample calibrated prediction regions for any underlying Machine\nLearning model, yet its status as an Uncertainty Quantification (UQ) tool has\nremained conceptually opaque. We adopt a category-theoretic approach to CP --\nframing it as a morphism, embedded in a commuting diagram, of two newly-defined\ncategories -- that brings us three joys. First, we show that -- under minimal\nassumptions -- CP is intrinsically a UQ mechanism, that is, its UQ capabilities\nare a structural feature of the method. Second, we demonstrate that CP bridges\n(and perhaps subsumes) the Bayesian, frequentist, and imprecise probabilistic\napproaches to predictive statistical reasoning. Finally, we show that a\nconformal prediction region (CPR) is the image of a covariant functor. This\nobservation is relevant to AI privacy: It implies that privacy noise added\nlocally does not break coverage.", "published": "2025-07-06 16:03:08", "link": "http://arxiv.org/abs/2507.04441v1", "categories": ["stat.ML", "cs.AI", "cs.LG", "math.CT", "Primary: 18D99, Secondary: 62G07, 28B20"], "primary_category": "stat.ML"}
{"title": "Quantum Algorithms for Bandits with Knapsacks with Improved Regret and Time Complexities", "abstract": "Bandits with knapsacks (BwK) constitute a fundamental model that combines\naspects of stochastic integer programming with online learning. Classical\nalgorithms for BwK with a time horizon $T$ achieve a problem-independent regret\nbound of ${O}(\\sqrt{T})$ and a problem-dependent bound of ${O}(\\log T)$. In\nthis paper, we initiate the study of the BwK model in the setting of quantum\ncomputing, where both reward and resource consumption can be accessed via\nquantum oracles. We establish both problem-independent and problem-dependent\nregret bounds for quantum BwK algorithms. For the problem-independent case, we\ndemonstrate that a quantum approach can improve the classical regret bound by a\nfactor of $(1+\\sqrt{B/\\mathrm{OPT}_\\mathrm{LP}})$, where $B$ is budget\nconstraint in BwK and $\\mathrm{OPT}_{\\mathrm{LP}}$ denotes the optimal value of\na linear programming relaxation of the BwK problem. For the problem-dependent\nsetting, we develop a quantum algorithm using an inexact quantum linear\nprogramming solver. This algorithm achieves a quadratic improvement in terms of\nthe problem-dependent parameters, as well as a polynomial speedup of time\ncomplexity on problem's dimensions compared to classical counterparts. Compared\nto previous works on quantum algorithms for multi-armed bandits, our study is\nthe first to consider bandit models with resource constraints and hence shed\nlight on operations research.", "published": "2025-07-06 15:52:37", "link": "http://arxiv.org/abs/2507.04438v1", "categories": ["quant-ph", "cs.DS", "cs.LG", "math.OC", "stat.ML"], "primary_category": "quant-ph"}
{"title": "Neural Networks for Tamed Milstein Approximation of SDEs with Additive Symmetric Jump Noise Driven by a Poisson Random Measure", "abstract": "This work aims to estimate the drift and diffusion functions in stochastic\ndifferential equations (SDEs) driven by a particular class of L\\'evy processes\nwith finite jump intensity, using neural networks. We propose a framework that\nintegrates the Tamed-Milstein scheme with neural networks employed as\nnon-parametric function approximators. Estimation is carried out in a\nnon-parametric fashion for the drift function \\( f: \\mathbb{Z} \\to \\mathbb{R}\n\\), the diffusion coefficient \\( g: \\mathbb{Z} \\to \\mathbb{R} \\). The model of\ninterest is given by \\[ dX(t) = \\xi + f(X(t))\\, dt + g(X(t))\\, dW_t + \\gamma\n\\int_{\\mathbb{Z}} z\\, N(dt,dz), \\] where \\( W_t \\) is a standard Brownian\nmotion, and \\( N(dt,dz) \\) is a Poisson random measure on \\( (~\\mathbb{R}_{+}\n~\\times ~\\mathbb{Z}~, ~\\mathcal{B}~(~\\mathbb{R}_{+}~)~\\otimes~\\mathcal{Z}~,~\n\\lambda( \\Lambda~\\otimes~v~)~) \\), with \\( \\lambda, \\gamma > 0 \\), \\( \\Lambda\n\\) being the Lebesgue measure on \\( \\mathbb{R}_{+} \\), and \\( v \\) a finite\nmeasure on the measurable space \\( (\\mathbb{Z}, \\mathcal{Z}) \\).\n  Neural networks are used as non-parametric function approximators, enabling\nthe modeling of complex nonlinear dynamics without assuming restrictive\nfunctional forms. The proposed methodology constitutes a flexible alternative\nfor inference in systems with state-dependent noise and discontinuities driven\nby L\\'evy processes.", "published": "2025-07-06 15:13:31", "link": "http://arxiv.org/abs/2507.04417v1", "categories": ["stat.ML", "cs.LG", "60H10, 68T07", "I.2.6; G.3"], "primary_category": "stat.ML"}
{"title": "Convergence and Sample Complexity of First-Order Methods for Agnostic Reinforcement Learning", "abstract": "We study reinforcement learning (RL) in the agnostic policy learning setting,\nwhere the goal is to find a policy whose performance is competitive with the\nbest policy in a given class of interest $\\Pi$ -- crucially, without assuming\nthat $\\Pi$ contains the optimal policy. We propose a general policy learning\nframework that reduces this problem to first-order optimization in a\nnon-Euclidean space, leading to new algorithms as well as shedding light on the\nconvergence properties of existing ones. Specifically, under the assumption\nthat $\\Pi$ is convex and satisfies a variational gradient dominance (VGD)\ncondition -- an assumption known to be strictly weaker than more standard\ncompleteness and coverability conditions -- we obtain sample complexity upper\nbounds for three policy learning algorithms: \\emph{(i)} Steepest Descent Policy\nOptimization, derived from a constrained steepest descent method for non-convex\noptimization; \\emph{(ii)} the classical Conservative Policy Iteration algorithm\n\\citep{kakade2002approximately} reinterpreted through the lens of the\nFrank-Wolfe method, which leads to improved convergence results; and\n\\emph{(iii)} an on-policy instantiation of the well-studied Policy Mirror\nDescent algorithm. Finally, we empirically evaluate the VGD condition across\nseveral standard environments, demonstrating the practical relevance of our key\nassumption.", "published": "2025-07-06 14:40:05", "link": "http://arxiv.org/abs/2507.04406v1", "categories": ["cs.LG", "math.OC", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Information-theoretic Quantification of High-order Feature Effects in Classification Problems", "abstract": "Understanding the contribution of individual features in predictive models\nremains a central goal in interpretable machine learning, and while many\nmodel-agnostic methods exist to estimate feature importance, they often fall\nshort in capturing high-order interactions and disentangling overlapping\ncontributions. In this work, we present an information-theoretic extension of\nthe High-order interactions for Feature importance (Hi-Fi) method, leveraging\nConditional Mutual Information (CMI) estimated via a k-Nearest Neighbor (kNN)\napproach working on mixed discrete and continuous random variables. Our\nframework decomposes feature contributions into unique, synergistic, and\nredundant components, offering a richer, model-independent understanding of\ntheir predictive roles. We validate the method using synthetic datasets with\nknown Gaussian structures, where ground truth interaction patterns are\nanalytically derived, and further test it on non-Gaussian and real-world gene\nexpression data from TCGA-BRCA. Results indicate that the proposed estimator\naccurately recovers theoretical and expected findings, providing a potential\nuse case for developing feature selection algorithms or model development based\non interaction analysis.", "published": "2025-07-06 11:50:30", "link": "http://arxiv.org/abs/2507.04362v1", "categories": ["cs.LG", "physics.data-an", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Efficient Perplexity Bound and Ratio Matching in Discrete Diffusion Language Models", "abstract": "While continuous diffusion models excel in modeling continuous distributions,\ntheir application to categorical data has been less effective. Recent work has\nshown that ratio-matching through score-entropy within a continuous-time\ndiscrete Markov chain (CTMC) framework serves as a competitive alternative to\nautoregressive models in language modeling. To enhance this framework, we first\nintroduce three new theorems concerning the KL divergence between the data and\nlearned distribution. Our results serve as the discrete counterpart to those\nestablished for continuous diffusion models and allow us to derive an improved\nupper bound of the perplexity. Second, we empirically show that ratio-matching\nperformed by minimizing the denoising cross-entropy between the clean and\ncorrupted data enables models to outperform those utilizing score-entropy with\nup to 10% lower perplexity/generative-perplexity, and 15% faster training\nsteps. To further support our findings, we introduce and evaluate a novel CTMC\ntransition-rate matrix that allows prediction refinement, and derive the\nanalytic expression for its matrix exponential which facilitates the\ncomputation of conditional ratios thus enabling efficient training and\ngeneration.", "published": "2025-07-06 10:54:37", "link": "http://arxiv.org/abs/2507.04341v1", "categories": ["stat.ML", "cs.AI", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Consistent Labeling Across Group Assignments: Variance Reduction in Conditional Average Treatment Effect Estimation", "abstract": "Numerous algorithms have been developed for Conditional Average Treatment\nEffect (CATE) estimation. In this paper, we first highlight a common issue\nwhere many algorithms exhibit inconsistent learning behavior for the same\ninstance across different group assignments. We introduce a metric to quantify\nand visualize this inconsistency. Next, we present a theoretical analysis\nshowing that this inconsistency indeed contributes to higher test errors and\ncannot be resolved through conventional machine learning techniques. To address\nthis problem, we propose a general method called \\textbf{Consistent Labeling\nAcross Group Assignments} (CLAGA), which eliminates the inconsistency and is\napplicable to any existing CATE estimation algorithm. Experiments on both\nsynthetic and real-world datasets demonstrate significant performance\nimprovements with CLAGA.", "published": "2025-07-06 10:36:39", "link": "http://arxiv.org/abs/2507.04332v1", "categories": ["cs.LG", "stat.ME", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Bandit Pareto Set Identification in a Multi-Output Linear Model", "abstract": "We study the Pareto Set Identification (PSI) problem in a structured\nmulti-output linear bandit model. In this setting, each arm is associated a\nfeature vector belonging to $\\mathbb{R}^h$, and its mean vector in\n$\\mathbb{R}^d$ linearly depends on this feature vector through a common unknown\nmatrix $\\Theta \\in \\mathbb{R}^{h \\times d}$. The goal is to identify the set of\nnon-dominated arms by adaptively collecting samples from the arms. We introduce\nand analyze the first optimal design-based algorithms for PSI, providing nearly\noptimal guarantees in both the fixed-budget and the fixed-confidence settings.\nNotably, we show that the difficulty of these tasks mainly depends on the\nsub-optimality gaps of $h$ arms only. Our theoretical results are supported by\nan extensive benchmark on synthetic and real-world datasets.", "published": "2025-07-06 06:14:43", "link": "http://arxiv.org/abs/2507.04255v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Inertial Quadratic Majorization Minimization with Application to Kernel Regularized Learning", "abstract": "First-order methods in convex optimization offer low per-iteration cost but\noften suffer from slow convergence, while second-order methods achieve fast\nlocal convergence at the expense of costly Hessian inversions. In this paper,\nwe highlight a middle ground: minimizing a quadratic majorant with fixed\ncurvature at each iteration. This strategy strikes a balance between\nper-iteration cost and convergence speed, and crucially allows the reuse of\nmatrix decompositions, such as Cholesky or spectral decompositions, across\niterations and varying regularization parameters. We introduce the Quadratic\nMajorization Minimization with Extrapolation (QMME) framework and establish its\nsequential convergence properties under standard assumptions. The new\nperspective of our analysis is to center the arguments around the induced norm\nof the curvature matrix $H$. To demonstrate practical advantages, we apply QMME\nto large-scale kernel regularized learning problems. In particular, we propose\na novel Sylvester equation modelling technique for kernel multinomial\nregression. In Julia-based experiments, QMME compares favorably against various\nestablished first- and second-order methods. Furthermore, we demonstrate that\nour algorithms complement existing kernel approximation techniques through more\nefficiently handling sketching matrices with large projection dimensions. Our\nnumerical experiments and real data analysis are available and fully\nreproducible at https://github.com/qhengncsu/QMME.jl.", "published": "2025-07-06 05:17:28", "link": "http://arxiv.org/abs/2507.04247v1", "categories": ["stat.ML", "cs.LG", "math.OC"], "primary_category": "stat.ML"}
{"title": "Structural Classification of Locally Stationary Time Series Based on Second-order Characteristics", "abstract": "Time series classification is crucial for numerous scientific and engineering\napplications. In this article, we present a numerically efficient, practically\ncompetitive, and theoretically rigorous classification method for\ndistinguishing between two classes of locally stationary time series based on\ntheir time-domain, second-order characteristics. Our approach builds on the\nautoregressive approximation for locally stationary time series, combined with\nan ensemble aggregation and a distance-based threshold for classification. It\nimposes no requirement on the training sample size, and is shown to achieve\nzero misclassification error rate asymptotically when the underlying time\nseries differ only mildly in their second-order characteristics. The new method\nis demonstrated to outperform a variety of state-of-the-art solutions,\nincluding wavelet-based, tree-based, convolution-based methods, as well as\nmodern deep learning methods, through intensive numerical simulations and a\nreal EEG data analysis for epilepsy classification.", "published": "2025-07-06 04:00:26", "link": "http://arxiv.org/abs/2507.04237v1", "categories": ["stat.ME", "stat.ML"], "primary_category": "stat.ME"}
{"title": "Normalizing Flow to Augmented Posterior: Conditional Density Estimation with Interpretable Dimension Reduction for High Dimensional Data", "abstract": "The conditional density characterizes the distribution of a response variable\n$y$ given other predictor $x$, and plays a key role in many statistical tasks,\nincluding classification and outlier detection. Although there has been\nabundant work on the problem of Conditional Density Estimation (CDE) for a\nlow-dimensional response in the presence of a high-dimensional predictor,\nlittle work has been done for a high-dimensional response such as images. The\npromising performance of normalizing flow (NF) neural networks in unconditional\ndensity estimation acts a motivating starting point. In this work, we extend NF\nneural networks when external $x$ is present. Specifically, they use the NF to\nparameterize a one-to-one transform between a high-dimensional $y$ and a latent\n$z$ that comprises two components \\([z_P,z_N]\\). The $z_P$ component is a\nlow-dimensional subvector obtained from the posterior distribution of an\nelementary predictive model for $x$, such as logistic/linear regression. The\n$z_N$ component is a high-dimensional independent Gaussian vector, which\nexplains the variations in $y$ not or less related to $x$. Unlike existing CDE\nmethods, the proposed approach, coined Augmented Posterior CDE (AP-CDE), only\nrequires a simple modification on the common normalizing flow framework, while\nsignificantly improving the interpretation of the latent component, since $z_P$\nrepresents a supervised dimension reduction. In image analytics applications,\nAP-CDE shows good separation of $x$-related variations due to factors such as\nlighting condition and subject id, from the other random variations. Further,\nthe experiments show that an unconditional NF neural network, based on an\nunsupervised model of $z$, such as Gaussian mixture, fails to generate\ninterpretable results.", "published": "2025-07-06 02:58:52", "link": "http://arxiv.org/abs/2507.04216v1", "categories": ["stat.ME", "stat.AP", "stat.ML"], "primary_category": "stat.ME"}
{"title": "Mixed-Sample SGD: an End-to-end Analysis of Supervised Transfer Learning", "abstract": "Theoretical works on supervised transfer learning (STL) -- where the learner\nhas access to labeled samples from both source and target distributions -- have\nfor the most part focused on statistical aspects of the problem, while\nefficient optimization has received less attention. We consider the problem of\ndesigning an SGD procedure for STL that alternates sampling between source and\ntarget data, while maintaining statistical transfer guarantees without prior\nknowledge of the quality of the source data. A main algorithmic difficulty is\nin understanding how to design such an adaptive sub-sampling mechanism at each\nSGD step, to automatically gain from the source when it is informative, or bias\ntowards the target and avoid negative transfer when the source is less\ninformative.\n  We show that, such a mixed-sample SGD procedure is feasible for general\nprediction tasks with convex losses, rooted in tracking an abstract sequence of\nconstrained convex programs that serve to maintain the desired transfer\nguarantees.\n  We instantiate these results in the concrete setting of linear regression\nwith square loss, and show that the procedure converges, with $1/\\sqrt{T}$\nrate, to a solution whose statistical performance on the target is adaptive to\nthe a priori unknown quality of the source. Experiments with synthetic and real\ndatasets support the theory.", "published": "2025-07-06 00:03:34", "link": "http://arxiv.org/abs/2507.04194v1", "categories": ["stat.ML", "cs.AI", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Machine Learning in Acoustics: A Review and Open-Source Repository", "abstract": "Acoustic data provide scientific and engineering insights in fields ranging\nfrom bioacoustics and communications to ocean and earth sciences. In this\nreview, we survey recent advances and the transformative potential of machine\nlearning (ML) in acoustics, including deep learning (DL). Using the Python\nhigh-level programming language, we demonstrate a broad collection of ML\ntechniques to detect and find patterns for classification, regression, and\ngeneration in acoustics data automatically. We have ML examples including\nacoustic data classification, generative modeling for spatial audio, and\nphysics-informed neural networks. This work includes AcousticsML, a set of\npractical Jupyter notebook examples on GitHub demonstrating ML benefits and\nencouraging researchers and practitioners to apply reproducible data-driven\napproaches to acoustic challenges.", "published": "2025-07-06 15:15:57", "link": "http://arxiv.org/abs/2507.04419v1", "categories": ["cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "Long-Context Modeling Networks for Monaural Speech Enhancement: A Comparative Study", "abstract": "Advanced long-context modeling backbone networks, such as Transformer,\nConformer, and Mamba, have demonstrated state-of-the-art performance in speech\nenhancement. However, a systematic and comprehensive comparative study of these\nbackbones within a unified speech enhancement framework remains lacking. In\naddition, xLSTM, a more recent and efficient variant of LSTM, has shown\npromising results in language modeling and as a general-purpose vision\nbackbone. In this paper, we investigate the capability of xLSTM in speech\nenhancement, and conduct a comprehensive comparison and analysis of the\nTransformer, Conformer, Mamba, and xLSTM backbones within a unified framework,\nconsidering both causal and noncausal configurations. Overall, xLSTM and Mamba\nachieve better performance than Transformer and Conformer. Mamba demonstrates\nsignificantly superior training and inference efficiency, particularly for long\nspeech inputs, whereas xLSTM suffers from the slowest processing speed.", "published": "2025-07-06 12:28:12", "link": "http://arxiv.org/abs/2507.04368v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "The Overview of Segmental Durations Modification Algorithms on Speech Signal Characteristics", "abstract": "This paper deeply evaluates and analyzes several mainstream algorithms that\ncan arbitrarily modify the duration of any portion of a given speech signal\nwithout changing the essential properties (e.g., pitch contour, power spectrum,\netc.) of the original signal. Arbitrary modification in this context means that\nthe duration of any region of the signal can be changed by specifying the\nstarting and ending time for modification or the target duration of the\nspecified interval, which can be either a fixed value of duration in the time\ndomain or a scaling factor of the original duration. In addition, arbitrary\nmodification also indicates any number of intervals can be modified at the same\ntime.", "published": "2025-07-06 06:50:09", "link": "http://arxiv.org/abs/2507.04264v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Context-Aware Deep Learning for Robust Channel Extrapolation in Fluid Antenna Systems", "abstract": "Fluid antenna systems (FAS) offer remarkable spatial flexibility but face\nsignificant challenges in acquiring high-resolution channel state information\n(CSI), leading to considerable overhead. To address this issue, we propose\nCANet, a robust deep learning model for channel extrapolation in FAS. CANet\ncombines context-adaptive modeling with a cross-scale attention mechanism and\nis built on a ConvNeXt v2 backbone to improve extrapolation accuracy for\nunobserved antenna ports. To further enhance robustness, we introduce a novel\nspatial amplitude perturbation strategy, inspired by frequency-domain\naugmentation techniques in image processing. This motivates the incorporation\nof a Fourier-domain loss function, capturing frequency-domain consistency,\nalongside a spectral structure consistency loss that reinforces learning\nstability under perturbations. Our simulation results demonstrate that CANet\noutperforms benchmark models across a wide range of signal-to-noise ratio (SNR)\nlevels.", "published": "2025-07-06 15:44:25", "link": "http://arxiv.org/abs/2507.04435v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Inverse Reinforcement Learning using Revealed Preferences and Passive Stochastic Optimization", "abstract": "This monograph, spanning three chapters, explores Inverse Reinforcement\nLearning (IRL). The first two chapters view inverse reinforcement learning\n(IRL) through the lens of revealed preferences from microeconomics while the\nthird chapter studies adaptive IRL via Langevin dynamics stochastic gradient\nalgorithms.\n  Chapter uses classical revealed preference theory (Afriat's theorem and\nextensions) to identify constrained utility maximizers based on observed agent\nactions. This allows for the reconstruction of set-valued estimates of an\nagent's utility. We illustrate this procedure by identifying the presence of a\ncognitive radar and reconstructing its utility function. The chapter also\naddresses the construction of a statistical detector for utility maximization\nbehavior when agent actions are corrupted by noise.\n  Chapter 2 studies Bayesian IRL. It investigates how an analyst can determine\nif an observed agent is a rationally inattentive Bayesian utility maximizer\n(i.e., simultaneously optimizing its utility and observation likelihood). The\nchapter discusses inverse stopping-time problems, focusing on reconstructing\nthe continuation and stopping costs of a Bayesian agent operating over a random\nhorizon. We then apply this IRL methodology to identify the presence of a\nBayes-optimal sequential detector. Additionally, Chapter 2 provides a concise\noverview of discrete choice models, inverse Bayesian filtering, and inverse\nstochastic gradient algorithms for adaptive IRL.\n  Finally, Chapter 3 introduces an adaptive IRL approach utilizing passive\nLangevin dynamics. This method aims to track time-varying utility functions\ngiven noisy and misspecified gradients. In essence, the adaptive IRL algorithms\npresented in Chapter 3 can be conceptualized as inverse stochastic gradient\nalgorithms, as they learn the utility function in real-time while a stochastic\ngradient algorithm is in operation.", "published": "2025-07-06 13:56:02", "link": "http://arxiv.org/abs/2507.04396v1", "categories": ["cs.LG", "eess.SP"], "primary_category": "cs.LG"}
{"title": "Near-Field ISAC for THz Wireless Systems", "abstract": "Sixth-generation (6G) wireless networks are expected not only to provide\nhigh-speed connectivity but also to support reliable sensing capabilities,\ngiving rise to the integrated sensing and communication (ISAC) paradigm. To\nenable higher data rates and more accurate sensing, terahertz (THz) systems\nempowered by extremely large multiple-input-multiple-output (XL-MIMO)\ntechnology are envisioned as key enablers for future ISAC systems. Owing to the\nsubstantial increase in both effective array aperture and carrier frequency, a\nconsiderable portion of future ISAC applications is anticipated to fall within\nthe near-field coverage region, instead of the conventional far-field. However,\nmost existing ISAC techniques are designed under the far-field planar wave\nassumption, struggling to accommodate the unique characteristics of THz\nnear-field propagation. To motivate future research into near-field ISAC\nresearch, we systematically investigate the characteristics of THz near-field\npropagation and explore its potential to facilitate ISAC systems. Specifically,\nwe analyze three fundamental characteristics of THz near-field propagation and\nreview state-of-the-art techniques that exploit these features to boost both\ncommunication and sensing performance. To further harness the angular-range\ncoupling effect, we zoom into a particularly interesting approach to near-field\nsensing based on wavenumber domain. Besides, to exploit the beam squint effect,\nan ISAC resource allocation framework is introduced to support integrated\nmulti-angle sensing and multi-user communication. Finally, we outline promising\ndirections for future research in this emerging area.", "published": "2025-07-06 08:29:13", "link": "http://arxiv.org/abs/2507.04292v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Normalized Iterative Hard Thresholding for Tensor Recovery", "abstract": "Low-rank recovery builds upon ideas from the theory of compressive sensing,\nwhich predicts that sparse signals can be accurately reconstructed from\nincomplete measurements. Iterative thresholding-type algorithms-particularly\nthe normalized iterative hard thresholding (NIHT) method-have been widely used\nin compressed sensing (CS) and applied to matrix recovery tasks. In this paper,\nwe propose a tensor extension of NIHT, referred to as TNIHT, for the recovery\nof low-rank tensors under two widely used tensor decomposition models. This\nextension enables the effective reconstruction of high-order low-rank tensors\nfrom a limited number of linear measurements by leveraging the inherent\nlow-dimensional structure of multi-way data. Specifically, we consider both the\nCANDECOMP/PARAFAC (CP) rank and the Tucker rank to characterize tensor\nlow-rankness within the TNIHT framework. At the same time, we establish a\nconvergence theorem for the proposed TNIHT method under the tensor restricted\nisometry property (TRIP), providing theoretical support for its recovery\nguarantees. Finally, we evaluate the performance of TNIHT through numerical\nexperiments on synthetic, image, and video data, and compare it with several\nstate-of-the-art algorithms.", "published": "2025-07-06 03:36:50", "link": "http://arxiv.org/abs/2507.04228v1", "categories": ["cs.LG", "eess.SP"], "primary_category": "cs.LG"}
{"title": "Adaptive Resource Management in Cognitive Radar via Deep Deterministic Policy Gradient", "abstract": "In this paper, scanning for target detection, and multi-target tracking in a\ncognitive radar system are considered, and adaptive radar resource management\nis investigated. In particular, time management for radar scanning and tracking\nof multiple maneuvering targets subject to budget constraints is studied with\nthe goal to jointly maximize the tracking and scanning performances of a\ncognitive radar. We tackle the constrained optimization problem of allocating\nthe dwell time to track individual targets by employing a deep deterministic\npolicy gradient (DDPG) based reinforcement learning approach. We propose a\nconstrained deep reinforcement learning (CDRL) algorithm that updates the DDPG\nneural networks and dual variables simultaneously. Numerical results show that\nthe radar can autonomously allocate time appropriately so as to maximize the\nreward function without exceeding the time constraint.", "published": "2025-07-06 00:10:45", "link": "http://arxiv.org/abs/2507.04195v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Self-supervised learning of speech representations with Dutch archival data", "abstract": "This paper explores the use of Dutch archival television broadcast data for\nself-supervised learning of speech foundation models, specifically wav2vec 2.0.\nWe first study data quality assumptions for pre-training, and show how music,\nnoise and speaker overlap affect SSL convergence and downstream fine-tuning\nperformance. Secondly, we explore effectively pre-processing strategies to\nconvert the noisy broadcast dataset into a qualitative dataset for\npre-training, by using Whisper and WhisperX. Thirdly, we compare mono-lingual\nand multi-lingual pre-training with equivalent amounts of data, and show that\nmono-lingual pre-training is more robust to out-of-domain data. Lastly, we\nachieve a state-of-the-art LARGE wav2vec 2.0 model for the Dutch language, by a\ncontinuation of pre-training a wav2vec 2.0 XLS-R model checkpoint with our 55k\nhour archival dataset.", "published": "2025-07-06 22:11:22", "link": "http://arxiv.org/abs/2507.04554v2", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MOD-X: A Modular Open Decentralized eXchange Framework proposal for Heterogeneous Interoperable Artificial Intelligence Agents", "abstract": "As Artificial Intelligence systems evolve from monolithic models to\necosystems of specialized agents, the need for standardized communication\nprotocols becomes increasingly critical. This paper introduces MOD-X (Modular\nOpen Decentralized eXchange), a novel architectural framework proposal for\nagent interoperability that addresses key limitations of existing protocols.\nUnlike current approaches, MOD-X proposes a layered architecture with a\nUniversal Message Bus, thorough state management, translation capabilities, and\nblockchain-based security mechanisms. We present MOD-X's architecture, compare\nit with existing protocols, and demonstrate its application through a worked\nexample how it enables integration between heterogeneous specialist agents\n(agents with different architectures, vendors, capabilities, and knowledge\nrepresentations--including rule-based systems, neural networks, symbolic\nreasoning engines, and legacy software with agent wrappers). MOD-X's key\ninnovations include a publish-subscribe communication model, semantic\ncapability discovery, and dynamic workflow orchestration--providing a framework\nthat bridges theoretical formalism with practical implementation. This\narchitecture addresses the growing need for truly decentralized, interoperable\nagent ecosystems that can scale effectively without the need for central\ncoordination.", "published": "2025-07-06 12:46:57", "link": "http://arxiv.org/abs/2507.04376v2", "categories": ["cs.AI", "cs.DC", "cs.MA", "cs.NI"], "primary_category": "cs.AI"}
{"title": "Dude, where's my utterance? Evaluating the effects of automatic segmentation and transcription on CPS detection", "abstract": "Collaborative Problem-Solving (CPS) markers capture key aspects of effective\nteamwork, such as staying on task, avoiding interruptions, and generating\nconstructive ideas. An AI system that reliably detects these markers could help\nteachers identify when a group is struggling or demonstrating productive\ncollaboration. Such a system requires an automated pipeline composed of\nmultiple components. In this work, we evaluate how CPS detection is impacted by\nautomating two critical components: transcription and speech segmentation. On\nthe public Weights Task Dataset (WTD), we find CPS detection performance with\nautomated transcription and segmentation methods is comparable to\nhuman-segmented and manually transcribed data; however, we find the automated\nsegmentation methods reduces the number of utterances by 26.5%, impacting the\nthe granularity of the data. We discuss the implications for developing\nAI-driven tools that support collaborative learning in classrooms.", "published": "2025-07-06 16:25:18", "link": "http://arxiv.org/abs/2507.04454v1", "categories": ["cs.HC", "cs.CL", "cs.CY", "eess.AS"], "primary_category": "cs.HC"}
{"title": "TTS-CtrlNet: Time varying emotion aligned text-to-speech generation with ControlNet", "abstract": "Recent advances in text-to-speech (TTS) have enabled natural speech\nsynthesis, but fine-grained, time-varying emotion control remains challenging.\nExisting methods often allow only utterance-level control and require full\nmodel fine-tuning with a large emotion speech dataset, which can degrade\nperformance. Inspired by adding conditional control to the existing model in\nControlNet (Zhang et al, 2023), we propose the first ControlNet-based approach\nfor controllable flow-matching TTS (TTS-CtrlNet), which freezes the original\nmodel and introduces a trainable copy of it to process additional conditions.\nWe show that TTS-CtrlNet can boost the pretrained large TTS model by adding\nintuitive, scalable, and time-varying emotion control while inheriting the\nability of the original model (e.g., zero-shot voice cloning & naturalness).\nFurthermore, we provide practical recipes for adding emotion control: 1)\noptimal architecture design choice with block analysis, 2) emotion-specific\nflow step, and 3) flexible control scale.\n  Experiments show that ours can effectively add an emotion controller to\nexisting TTS, and achieves state-of-the-art performance with emotion similarity\nscores: Emo-SIM and Aro-Val SIM. The project page is available at:\nhttps://curryjung.github.io/ttsctrlnet_project_page", "published": "2025-07-06 11:22:08", "link": "http://arxiv.org/abs/2507.04349v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "WSCoach: Wearable Real-time Auditory Feedback for Reducing Unwanted Words in Daily Communication", "abstract": "The rise of wearable smart devices raises unprecedented opportunities for\nself-improvement through ubiquitous behavior tracking and guidance. However,\nthe design of effective wearable behavior intervention systems remains\nrelatively unexplored. To address this gap, we conducted controlled studies\nfocusing on the reduction of unwanted words (e.g., filler words, swear words)\nin daily communication through auditory feedback using wearable technology. We\nstarted with a design space exploration, considering various factors such as\nthe type, duration, and timing of the auditory feedback. Then, we conducted\npilot studies to reduce the space of design choices and prototyped a system\ncalled WSCoach (Wearable Speech Coach), which informs users when they utter\nunwanted words in near-real-time. To evaluate WSCoach, we compared it with a\nstate-of-the-art mobile application supporting post-hoc conversation analysis.\nBoth approaches were effective in reducing the occurrence of unwanted words,\nbut WSCoach appears to be more effective in the long run. Finally, we discuss\nguidelines for the design of wearable audio-based behavior monitoring and\nintervention systems and highlight the potential of wearable technology for\nfacilitating behavior correction and improvement. For supplementary material,\nplease see the META Appendix and our OSF project at\nhttps://osf.io/6vhwn/?view_only=489498d3ac2d4703a17475fc6ca65dfa.", "published": "2025-07-06 04:00:59", "link": "http://arxiv.org/abs/2507.04238v1", "categories": ["cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
{"title": "Mutual Information Bounds for Lossy Common Information", "abstract": "We show the mutual information between the targets in a Gray-Wyner Network as\na bound that separates Wyner's lossy common information and G\\'acs-K\\\"orner\nlossy common information. The results are a generalization of the lossless case\npresented by Wyner (1975).", "published": "2025-07-06 01:39:00", "link": "http://arxiv.org/abs/2507.04209v2", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Neural Networks for Tamed Milstein Approximation of SDEs with Additive Symmetric Jump Noise Driven by a Poisson Random Measure", "abstract": "This work aims to estimate the drift and diffusion functions in stochastic\ndifferential equations (SDEs) driven by a particular class of L\\'evy processes\nwith finite jump intensity, using neural networks. We propose a framework that\nintegrates the Tamed-Milstein scheme with neural networks employed as\nnon-parametric function approximators. Estimation is carried out in a\nnon-parametric fashion for the drift function $f: \\mathbb{Z} \\to \\mathbb{R}$,\nthe diffusion coefficient $g: \\mathbb{Z} \\to \\mathbb{R}$. The model of interest\nis given by \\[ dX(t) = \\xi + f(X(t))\\, dt + g(X(t))\\, dW_t + \\gamma\n\\int_{\\mathbb{Z}} z\\, N(dt,dz), \\] where $W_t$ is a standard Brownian motion,\nand $N(dt,dz)$ is a Poisson random measure on $(\\mathbb{R}_{+} \\times\n\\mathbb{Z}$, $\\mathcal{B} (\\mathbb{R}_{+}) \\otimes \\mathcal{Z}$, $\\lambda(\n\\Lambda \\otimes v))$, with $\\lambda, \\gamma > 0$, $\\Lambda$ being the Lebesgue\nmeasure on $\\mathbb{R}_{+}$, and $v$ a finite measure on the measurable space\n$(\\mathbb{Z}, \\mathcal{Z})$. Neural networks are used as non-parametric\nfunction approximators, enabling the modeling of complex nonlinear dynamics\nwithout assuming restrictive functional forms. The proposed methodology\nconstitutes a flexible alternative for inference in systems with\nstate-dependent noise and discontinuities driven by L\\'evy processes.", "published": "2025-07-06 15:13:31", "link": "http://arxiv.org/abs/2507.04417v2", "categories": ["stat.ML", "cs.LG", "60H10, 68T07", "I.2.6; G.3"], "primary_category": "stat.ML"}
{"title": "Structural Classification of Locally Stationary Time Series Based on Second-order Characteristics", "abstract": "Time series classification is crucial for numerous scientific and engineering\napplications. In this article, we present a numerically efficient, practically\ncompetitive, and theoretically rigorous classification method for\ndistinguishing between two classes of locally stationary time series based on\ntheir time-domain, second-order characteristics. Our approach builds on the\nautoregressive approximation for locally stationary time series, combined with\nan ensemble aggregation and a distance-based threshold for classification. It\nimposes no requirement on the training sample size, and is shown to achieve\nzero misclassification error rate asymptotically when the underlying time\nseries differ only mildly in their second-order characteristics. The new method\nis demonstrated to outperform a variety of state-of-the-art solutions,\nincluding wavelet-based, tree-based, convolution-based methods, as well as\nmodern deep learning methods, through intensive numerical simulations and a\nreal EEG data analysis for epilepsy classification.", "published": "2025-07-06 04:00:26", "link": "http://arxiv.org/abs/2507.04237v2", "categories": ["stat.ME", "stat.ML"], "primary_category": "stat.ME"}
