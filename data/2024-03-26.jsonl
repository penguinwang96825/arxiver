{"title": "Automate Knowledge Concept Tagging on Math Questions with LLMs", "abstract": "Knowledge concept tagging for questions plays a crucial role in contemporary\nintelligent educational applications, including learning progress diagnosis,\npractice question recommendations, and course content organization.\nTraditionally, these annotations have been conducted manually with help from\npedagogical experts, as the task requires not only a strong semantic\nunderstanding of both question stems and knowledge definitions but also deep\ninsights into connecting question-solving logic with corresponding knowledge\nconcepts. In this paper, we explore automating the tagging task using Large\nLanguage Models (LLMs), in response to the inability of prior manual methods to\nmeet the rapidly growing demand for concept tagging in questions posed by\nadvanced educational applications. Moreover, the zero/few-shot learning\ncapability of LLMs makes them well-suited for application in educational\nscenarios, which often face challenges in collecting large-scale,\nexpertise-annotated datasets. By conducting extensive experiments with a\nvariety of representative LLMs, we demonstrate that LLMs are a promising tool\nfor concept tagging in math questions. Furthermore, through case studies\nexamining the results from different LLMs, we draw some empirical conclusions\nabout the key factors for success in applying LLMs to the automatic concept\ntagging task.", "published": "2024-03-26 00:09:38", "link": "http://arxiv.org/abs/2403.17281v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Common Ground Tracking in Multimodal Dialogue", "abstract": "Within Dialogue Modeling research in AI and NLP, considerable attention has\nbeen spent on ``dialogue state tracking'' (DST), which is the ability to update\nthe representations of the speaker's needs at each turn in the dialogue by\ntaking into account the past dialogue moves and history. Less studied but just\nas important to dialogue modeling, however, is ``common ground tracking''\n(CGT), which identifies the shared belief space held by all of the participants\nin a task-oriented dialogue: the task-relevant propositions all participants\naccept as true. In this paper we present a method for automatically identifying\nthe current set of shared beliefs and ``questions under discussion'' (QUDs) of\na group with a shared goal. We annotate a dataset of multimodal interactions in\na shared physical space with speech transcriptions, prosodic features,\ngestures, actions, and facets of collaboration, and operationalize these\nfeatures for use in a deep neural model to predict moves toward construction of\ncommon ground. Model outputs cascade into a set of formal closure rules derived\nfrom situated evidence and belief axioms and update operations. We empirically\nassess the contribution of each feature type toward successful construction of\ncommon ground relative to ground truth, establishing a benchmark in this novel,\nchallenging task.", "published": "2024-03-26 00:25:01", "link": "http://arxiv.org/abs/2403.17284v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Project MOSLA: Recording Every Moment of Second Language Acquisition", "abstract": "Second language acquisition (SLA) is a complex and dynamic process. Many SLA\nstudies that have attempted to record and analyze this process have typically\nfocused on a single modality (e.g., textual output of learners), covered only a\nshort period of time, and/or lacked control (e.g., failed to capture every\naspect of the learning process). In Project MOSLA (Moments of Second Language\nAcquisition), we have created a longitudinal, multimodal, multilingual, and\ncontrolled dataset by inviting participants to learn one of three target\nlanguages (Arabic, Spanish, and Chinese) from scratch over a span of two years,\nexclusively through online instruction, and recording every lesson using Zoom.\nThe dataset is semi-automatically annotated with speaker/language IDs and\ntranscripts by both human annotators and fine-tuned state-of-the-art speech\nmodels. Our experiments reveal linguistic insights into learners' proficiency\ndevelopment over time, as well as the potential for automatically detecting the\nareas of focus on the screen purely from the unannotated multimodal data. Our\ndataset is freely available for research purposes and can serve as a valuable\nresource for a wide range of applications, including but not limited to SLA,\nproficiency assessment, language and speech processing, pedagogy, and\nmultimodal learning analytics.", "published": "2024-03-26 01:52:59", "link": "http://arxiv.org/abs/2403.17314v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Chain-of-Action: Faithful and Multimodal Question Answering through\n  Large Language Models", "abstract": "We present a Chain-of-Action (CoA) framework for multimodal and\nretrieval-augmented Question-Answering (QA). Compared to the literature, CoA\novercomes two major challenges of current QA applications: (i) unfaithful\nhallucination that is inconsistent with real-time or domain facts and (ii) weak\nreasoning performance over compositional information. Our key contribution is a\nnovel reasoning-retrieval mechanism that decomposes a complex question into a\nreasoning chain via systematic prompting and pre-designed actions.\nMethodologically, we propose three types of domain-adaptable `Plug-and-Play'\nactions for retrieving real-time information from heterogeneous sources. We\nalso propose a multi-reference faith score (MRFS) to verify and resolve\nconflicts in the answers. Empirically, we exploit both public benchmarks and a\nWeb3 case study to demonstrate the capability of CoA over other methods.", "published": "2024-03-26 03:51:01", "link": "http://arxiv.org/abs/2403.17359v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extracting Biomedical Entities from Noisy Audio Transcripts", "abstract": "Automatic Speech Recognition (ASR) technology is fundamental in transcribing\nspoken language into text, with considerable applications in the clinical\nrealm, including streamlining medical transcription and integrating with\nElectronic Health Record (EHR) systems. Nevertheless, challenges persist,\nespecially when transcriptions contain noise, leading to significant drops in\nperformance when Natural Language Processing (NLP) models are applied. Named\nEntity Recognition (NER), an essential clinical task, is particularly affected\nby such noise, often termed the ASR-NLP gap. Prior works have primarily studied\nASR's efficiency in clean recordings, leaving a research gap concerning the\nperformance in noisy environments. This paper introduces a novel dataset,\nBioASR-NER, designed to bridge the ASR-NLP gap in the biomedical domain,\nfocusing on extracting adverse drug reactions and mentions of entities from the\nBrief Test of Adult Cognition by Telephone (BTACT) exam. Our dataset offers a\ncomprehensive collection of almost 2,000 clean and noisy recordings. In\naddressing the noise challenge, we present an innovative transcript-cleaning\nmethod using GPT4, investigating both zero-shot and few-shot methodologies. Our\nstudy further delves into an error analysis, shedding light on the types of\nerrors in transcription software, corrections by GPT4, and the challenges GPT4\nfaces. This paper aims to foster improved understanding and potential solutions\nfor the ASR-NLP gap, ultimately supporting enhanced healthcare documentation\npractices.", "published": "2024-03-26 03:58:52", "link": "http://arxiv.org/abs/2403.17363v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PCToolkit: A Unified Plug-and-Play Prompt Compression Toolkit of Large\n  Language Models", "abstract": "Prompt compression is an innovative method for efficiently condensing input\nprompts while preserving essential information. To facilitate quick-start\nservices, user-friendly interfaces, and compatibility with common datasets and\nmetrics, we present the Prompt Compression Toolkit (PCToolkit). This toolkit is\na unified plug-and-play solution for compressing prompts in Large Language\nModels (LLMs), featuring cutting-edge prompt compressors, diverse datasets, and\nmetrics for comprehensive performance evaluation. PCToolkit boasts a modular\ndesign, allowing for easy integration of new datasets and metrics through\nportable and user-friendly interfaces. In this paper, we outline the key\ncomponents and functionalities of PCToolkit. We conducted evaluations of the\ncompressors within PCToolkit across various natural language tasks, including\nreconstruction, summarization, mathematical problem-solving, question\nanswering, few-shot learning, synthetic tasks, code completion, boolean\nexpressions, multiple choice questions, and lies recognition.", "published": "2024-03-26 06:11:07", "link": "http://arxiv.org/abs/2403.17411v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LM-Combiner: A Contextual Rewriting Model for Chinese Grammatical Error\n  Correction", "abstract": "Over-correction is a critical problem in Chinese grammatical error correction\n(CGEC) task. Recent work using model ensemble methods based on voting can\neffectively mitigate over-correction and improve the precision of the GEC\nsystem. However, these methods still require the output of several GEC systems\nand inevitably lead to reduced error recall. In this light, we propose the\nLM-Combiner, a rewriting model that can directly modify the over-correction of\nGEC system outputs without a model ensemble. Specifically, we train the model\non an over-correction dataset constructed through the proposed K-fold cross\ninference method, which allows it to directly generate filtered sentences by\ncombining the original and the over-corrected text. In the inference stage, we\ndirectly take the original sentences and the output results of other systems as\ninput and then obtain the filtered sentences through LM-Combiner. Experiments\non the FCGEC dataset show that our proposed method effectively alleviates the\nover-correction of the original system (+18.2 Precision) while ensuring the\nerror recall remains unchanged. Besides, we find that LM-Combiner still has a\ngood rewriting performance even with small parameters and few training data,\nand thus can cost-effectively mitigate the over-correction of black-box GEC\nsystems (e.g., ChatGPT).", "published": "2024-03-26 06:12:21", "link": "http://arxiv.org/abs/2403.17413v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KDMCSE: Knowledge Distillation Multimodal Sentence Embeddings with\n  Adaptive Angular margin Contrastive Learning", "abstract": "Previous work on multimodal sentence embedding has proposed multimodal\ncontrastive learning and achieved promising results. However, by taking the\nrest of the batch as negative samples without reviewing when forming\ncontrastive pairs, those studies encountered many suspicious and noisy negative\nexamples, significantly affecting the methods' overall performance. In this\nwork, we propose KDMCSE (Knowledge Distillation Multimodal contrastive learning\nof Sentence Embeddings), a novel approach that enhances the discrimination and\ngeneralizability of multimodal representation and inherits the knowledge from\nthe teacher model to learn the difference between positive and negative\ninstances and via that, can detect noisy and wrong negative samples effectively\nbefore they are calculated in the contrastive objective. Furthermore, to\novercome the limitation of modeling the variation within negative pairs, we\nintroduce a new contrastive objective, AdapACSE (Adaptive Angular Margin\nSupervised Contrastive Learning for Multimodal sentence embeddings), that\nenhances the discriminative representation by strengthening the margin within\nthe angular space while capturing varying semantics within the negative.\nExperimental results on widely used Semantic Textual Similarity (STS)\nbenchmarks demonstrate the effectiveness of our approach.", "published": "2024-03-26 08:32:39", "link": "http://arxiv.org/abs/2403.17486v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DGoT: Dynamic Graph of Thoughts for Scientific Abstract Generation", "abstract": "The method of training language models based on domain datasets has obtained\nsignificant achievements in the task of generating scientific paper abstracts.\nHowever, such models face problems of generalization and expensive training\ncosts. The use of large language models (LLMs) to solve the task of generating\npaper abstracts saves the cost of model training. However, due to the\nhallucination problem of LLM, it is often necessary to improve the reliability\nof the results through multi-round query prompt approach such as Graph of\nThoughts (GoT), which also brings additional reasoning costs. In this paper, we\npropose a Dynamic Graph of Thought (DGoT). It not only inherits the advantages\nof the existing GoT prompt approach, but also dynamically adjust the graph\nstructure according to data characteristics while reducing model reasoning\ncost. Experimental results show that our method's cost-effectiveness in\nabstract generation tasks is only 43.7% to 56.4% of other multi-round query\nprompt approaches. Our code is available at https://github.com/JayceNing/DGoT.", "published": "2024-03-26 08:47:23", "link": "http://arxiv.org/abs/2403.17491v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Sentence-T5: Scalable Sentence Encoders for Multilingual\n  Applications", "abstract": "Prior work on multilingual sentence embedding has demonstrated that the\nefficient use of natural language inference (NLI) data to build\nhigh-performance models can outperform conventional methods. However, the\npotential benefits from the recent ``exponential'' growth of language models\nwith billions of parameters have not yet been fully explored. In this paper, we\nintroduce Multilingual Sentence T5 (m-ST5), as a larger model of NLI-based\nmultilingual sentence embedding, by extending Sentence T5, an existing\nmonolingual model. By employing the low-rank adaptation (LoRA) technique, we\nhave achieved a successful scaling of the model's size to 5.7 billion\nparameters. We conducted experiments to evaluate the performance of sentence\nembedding and verified that the method outperforms the NLI-based prior\napproach. Furthermore, we also have confirmed a positive correlation between\nthe size of the model and its performance. It was particularly noteworthy that\nlanguages with fewer resources or those with less linguistic similarity to\nEnglish benefited more from the parameter increase. Our model is available at\nhttps://huggingface.co/pkshatech/m-ST5.", "published": "2024-03-26 09:31:55", "link": "http://arxiv.org/abs/2403.17528v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sparse Logistic Regression with High-order Features for Automatic\n  Grammar Rule Extraction from Treebanks", "abstract": "Descriptive grammars are highly valuable, but writing them is time-consuming\nand difficult. Furthermore, while linguists typically use corpora to create\nthem, grammar descriptions often lack quantitative data. As for formal\ngrammars, they can be challenging to interpret. In this paper, we propose a new\nmethod to extract and explore significant fine-grained grammar patterns and\npotential syntactic grammar rules from treebanks, in order to create an\neasy-to-understand corpus-based grammar. More specifically, we extract\ndescriptions and rules across different languages for two linguistic phenomena,\nagreement and word order, using a large search space and paying special\nattention to the ranking order of the extracted rules. For that, we use a\nlinear classifier to extract the most salient features that predict the\nlinguistic phenomena under study. We associate statistical information to each\nrule, and we compare the ranking of the model's results to those of other\nquantitative and statistical measures. Our method captures both well-known and\nless well-known significant grammar rules in Spanish, French, and Wolof.", "published": "2024-03-26 09:39:53", "link": "http://arxiv.org/abs/2403.17534v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ILLUMINER: Instruction-tuned Large Language Models as Few-shot Intent\n  Classifier and Slot Filler", "abstract": "State-of-the-art intent classification (IC) and slot filling (SF) methods\noften rely on data-intensive deep learning models, limiting their practicality\nfor industry applications. Large language models on the other hand,\nparticularly instruction-tuned models (Instruct-LLMs), exhibit remarkable\nzero-shot performance across various natural language tasks. This study\nevaluates Instruct-LLMs on popular benchmark datasets for IC and SF,\nemphasizing their capacity to learn from fewer examples. We introduce\nILLUMINER, an approach framing IC and SF as language generation tasks for\nInstruct-LLMs, with a more efficient SF-prompting method compared to prior\nwork. A comprehensive comparison with multiple baselines shows that our\napproach, using the FLAN-T5 11B model, outperforms the state-of-the-art joint\nIC+SF method and in-context learning with GPT3.5 (175B), particularly in slot\nfilling by 11.1--32.2 percentage points. Additionally, our in-depth ablation\nstudy demonstrates that parameter-efficient fine-tuning requires less than 6%\nof training data to yield comparable performance with traditional full-weight\nfine-tuning.", "published": "2024-03-26 09:41:21", "link": "http://arxiv.org/abs/2403.17536v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models Are State-of-the-Art Evaluator for Grammatical\n  Error Correction", "abstract": "Large Language Models (LLMs) have been reported to outperform existing\nautomatic evaluation metrics in some tasks, such as text summarization and\nmachine translation. However, there has been a lack of research on LLMs as\nevaluators in grammatical error correction (GEC). In this study, we investigate\nthe performance of LLMs in GEC evaluation by employing prompts designed to\nincorporate various evaluation criteria inspired by previous research. Our\nextensive experimental results demonstrate that GPT-4 achieved Kendall's rank\ncorrelation of 0.662 with human judgments, surpassing all existing methods.\nFurthermore, in recent GEC evaluations, we have underscored the significance of\nthe LLMs scale and particularly emphasized the importance of fluency among\nevaluation criteria.", "published": "2024-03-26 09:43:15", "link": "http://arxiv.org/abs/2403.17540v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Naive Bayes-based Context Extension for Large Language Models", "abstract": "Large Language Models (LLMs) have shown promising in-context learning\nabilities. However, conventional In-Context Learning (ICL) approaches are often\nimpeded by length limitations of transformer architecture, which pose\nchallenges when attempting to effectively integrate supervision from a\nsubstantial number of demonstration examples. In this paper, we introduce a\nnovel framework, called Naive Bayes-based Context Extension (NBCE), to enable\nexisting LLMs to perform ICL with an increased number of demonstrations by\nsignificantly expanding their context size. Importantly, this expansion does\nnot require fine-tuning or dependence on particular model architectures, all\nthe while preserving linear efficiency. NBCE initially splits the context into\nequal-sized windows fitting the target LLM's maximum length. Then, it\nintroduces a voting mechanism to select the most relevant window, regarded as\nthe posterior context. Finally, it employs Bayes' theorem to generate the test\ntask. Our experimental results demonstrate that NBCE substantially enhances\nperformance, particularly as the number of demonstration examples increases,\nconsistently outperforming alternative methods. The NBCE code will be made\npublicly accessible. The code NBCE is available at:\nhttps://github.com/amurtadha/NBCE-master", "published": "2024-03-26 09:59:45", "link": "http://arxiv.org/abs/2403.17552v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RuBia: A Russian Language Bias Detection Dataset", "abstract": "Warning: this work contains upsetting or disturbing content.\n  Large language models (LLMs) tend to learn the social and cultural biases\npresent in the raw pre-training data. To test if an LLM's behavior is fair,\nfunctional datasets are employed, and due to their purpose, these datasets are\nhighly language and culture-specific. In this paper, we address a gap in the\nscope of multilingual bias evaluation by presenting a bias detection dataset\nspecifically designed for the Russian language, dubbed as RuBia. The RuBia\ndataset is divided into 4 domains: gender, nationality, socio-economic status,\nand diverse, each of the domains is further divided into multiple fine-grained\nsubdomains. Every example in the dataset consists of two sentences with the\nfirst reinforcing a potentially harmful stereotype or trope and the second\ncontradicting it. These sentence pairs were first written by volunteers and\nthen validated by native-speaking crowdsourcing workers. Overall, there are\nnearly 2,000 unique sentence pairs spread over 19 subdomains in RuBia. To\nillustrate the dataset's purpose, we conduct a diagnostic evaluation of\nstate-of-the-art or near-state-of-the-art LLMs and discuss the LLMs'\npredisposition to social biases.", "published": "2024-03-26 10:01:01", "link": "http://arxiv.org/abs/2403.17553v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Task-Oriented Paraphrase Analytics", "abstract": "Since paraphrasing is an ill-defined task, the term \"paraphrasing\" covers\ntext transformation tasks with different characteristics. Consequently,\nexisting paraphrasing studies have applied quite different (explicit and\nimplicit) criteria as to when a pair of texts is to be considered a paraphrase,\nall of which amount to postulating a certain level of semantic or lexical\nsimilarity. In this paper, we conduct a literature review and propose a\ntaxonomy to organize the 25~identified paraphrasing (sub-)tasks. Using\nclassifiers trained to identify the tasks that a given paraphrasing instance\nfits, we find that the distributions of task-specific instances in the known\nparaphrase corpora vary substantially. This means that the use of these\ncorpora, without the respective paraphrase conditions being clearly defined\n(which is the normal case), must lead to incomparable and misleading results.", "published": "2024-03-26 10:14:12", "link": "http://arxiv.org/abs/2403.17564v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"You are an expert annotator\": Automatic Best-Worst-Scaling Annotations\n  for Emotion Intensity Modeling", "abstract": "Labeling corpora constitutes a bottleneck to create models for new tasks or\ndomains. Large language models mitigate the issue with automatic corpus\nlabeling methods, particularly for categorical annotations. Some NLP tasks such\nas emotion intensity prediction, however, require text regression, but there is\nno work on automating annotations for continuous label assignments. Regression\nis considered more challenging than classification: The fact that humans\nperform worse when tasked to choose values from a rating scale lead to\ncomparative annotation methods, including best-worst scaling. This raises the\nquestion if large language model-based annotation methods show similar\npatterns, namely that they perform worse on rating scale annotation tasks than\non comparative annotation tasks. To study this, we automate emotion intensity\npredictions and compare direct rating scale predictions, pairwise comparisons\nand best-worst scaling. We find that the latter shows the highest reliability.\nA transformer regressor fine-tuned on these data performs nearly on par with a\nmodel trained on the original manual annotations.", "published": "2024-03-26 11:45:22", "link": "http://arxiv.org/abs/2403.17612v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mix-Initiative Response Generation with Dynamic Prefix Tuning", "abstract": "Mixed initiative serves as one of the key factors in controlling conversation\ndirections. For a speaker, responding passively or leading proactively would\nresult in rather different responses. However, most dialogue systems focus on\ntraining a holistic response generation model without any distinction among\ndifferent initiatives. It leads to the cross-contamination problem, where the\nmodel confuses different initiatives and generates inappropriate responses.\nMoreover, obtaining plenty of human annotations for initiative labels can be\nexpensive. To address this issue, we propose a general mix-Initiative Dynamic\nPrefix Tuning framework (IDPT) to decouple different initiatives from the\ngeneration model, which learns initiative-aware prefixes in both supervised and\nunsupervised settings. Specifically, IDPT decouples initiative factors into\ndifferent prefix parameters and uses the attention mechanism to adjust the\nselection of initiatives in guiding generation dynamically. The prefix\nparameters can be tuned towards accurate initiative prediction as well as\nmix-initiative response generation. Extensive experiments on two public\ndialogue datasets show that the proposed IDPT outperforms previous baselines on\nboth automatic metrics and human evaluations. It also manages to generate\nappropriate responses with manipulated initiatives.", "published": "2024-03-26 12:11:29", "link": "http://arxiv.org/abs/2403.17636v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "REFeREE: A REference-FREE Model-Based Metric for Text Simplification", "abstract": "Text simplification lacks a universal standard of quality, and annotated\nreference simplifications are scarce and costly. We propose to alleviate such\nlimitations by introducing REFeREE, a reference-free model-based metric with a\n3-stage curriculum. REFeREE leverages an arbitrarily scalable pretraining stage\nand can be applied to any quality standard as long as a small number of human\nannotations are available. Our experiments show that our metric outperforms\nexisting reference-based metrics in predicting overall ratings and reaches\ncompetitive and consistent performance in predicting specific ratings while\nrequiring no reference simplifications at inference time.", "published": "2024-03-26 12:21:51", "link": "http://arxiv.org/abs/2403.17640v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DANCER: Entity Description Augmented Named Entity Corrector for\n  Automatic Speech Recognition", "abstract": "End-to-end automatic speech recognition (E2E ASR) systems often suffer from\nmistranscription of domain-specific phrases, such as named entities, sometimes\nleading to catastrophic failures in downstream tasks. A family of fast and\nlightweight named entity correction (NEC) models for ASR have recently been\nproposed, which normally build on phonetic-level edit distance algorithms and\nhave shown impressive NEC performance. However, as the named entity (NE) list\ngrows, the problems of phonetic confusion in the NE list are exacerbated; for\nexample, homophone ambiguities increase substantially. In view of this, we\nproposed a novel Description Augmented Named entity CorrEctoR (dubbed DANCER),\nwhich leverages entity descriptions to provide additional information to\nfacilitate mitigation of phonetic confusion for NEC on ASR transcription. To\nthis end, an efficient entity description augmented masked language model\n(EDA-MLM) comprised of a dense retrieval model is introduced, enabling MLM to\nadapt swiftly to domain-specific entities for the NEC task. A series of\nexperiments conducted on the AISHELL-1 and Homophone datasets confirm the\neffectiveness of our modeling approach. DANCER outperforms a strong baseline,\nthe phonetic edit-distance-based NEC model (PED-NEC), by a character error rate\n(CER) reduction of about 7% relatively on AISHELL-1 for named entities. More\nnotably, when tested on Homophone that contain named entities of high phonetic\nconfusion, DANCER offers a more pronounced CER reduction of 46% relatively over\nPED-NEC for named entities.", "published": "2024-03-26 12:27:32", "link": "http://arxiv.org/abs/2403.17645v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Intrinsic Subgraph Generation for Interpretable Graph based Visual\n  Question Answering", "abstract": "The large success of deep learning based methods in Visual Question Answering\n(VQA) has concurrently increased the demand for explainable methods. Most\nmethods in Explainable Artificial Intelligence (XAI) focus on generating\npost-hoc explanations rather than taking an intrinsic approach, the latter\ncharacterizing an interpretable model. In this work, we introduce an\ninterpretable approach for graph-based VQA and demonstrate competitive\nperformance on the GQA dataset. This approach bridges the gap between\ninterpretability and performance. Our model is designed to intrinsically\nproduce a subgraph during the question-answering process as its explanation,\nproviding insight into the decision making. To evaluate the quality of these\ngenerated subgraphs, we compare them against established post-hoc\nexplainability methods for graph neural networks, and perform a human\nevaluation. Moreover, we present quantitative metrics that correlate with the\nevaluations of human assessors, acting as automatic metrics for the generated\nexplanatory subgraphs. Our implementation is available at\nhttps://github.com/DigitalPhonetics/Intrinsic-Subgraph-Generation-for-VQA.", "published": "2024-03-26 12:29:18", "link": "http://arxiv.org/abs/2403.17647v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Continual Few-shot Event Detection via Hierarchical Augmentation\n  Networks", "abstract": "Traditional continual event detection relies on abundant labeled data for\ntraining, which is often impractical to obtain in real-world applications. In\nthis paper, we introduce continual few-shot event detection (CFED), a more\ncommonly encountered scenario when a substantial number of labeled samples are\nnot accessible. The CFED task is challenging as it involves memorizing previous\nevent types and learning new event types with few-shot samples. To mitigate\nthese challenges, we propose a memory-based framework: Hierarchical\nAugmentation Networks (HANet). To memorize previous event types with limited\nmemory, we incorporate prototypical augmentation into the memory set. For the\nissue of learning new event types in few-shot scenarios, we propose a\ncontrastive augmentation module for token representations. Despite comparing\nwith previous state-of-the-art methods, we also conduct comparisons with\nChatGPT. Experiment results demonstrate that our method significantly\noutperforms all of these methods in multiple continual few-shot event detection\ntasks.", "published": "2024-03-26 14:20:42", "link": "http://arxiv.org/abs/2403.17733v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UCxn: Typologically Informed Annotation of Constructions Atop Universal\n  Dependencies", "abstract": "The Universal Dependencies (UD) project has created an invaluable collection\nof treebanks with contributions in over 140 languages. However, the UD\nannotations do not tell the full story. Grammatical constructions that convey\nmeaning through a particular combination of several morphosyntactic elements --\nfor example, interrogative sentences with special markers and/or word orders --\nare not labeled holistically. We argue for (i) augmenting UD annotations with a\n'UCxn' annotation layer for such meaning-bearing grammatical constructions, and\n(ii) approaching this in a typologically informed way so that morphosyntactic\nstrategies can be compared across languages. As a case study, we consider five\nconstruction families in ten languages, identifying instances of each\nconstruction in UD treebanks through the use of morphosyntactic patterns. In\naddition to findings regarding these particular constructions, our study yields\nimportant insights on methodology for describing and identifying constructions\nin language-general and language-particular ways, and lays the foundation for\nfuture constructional enrichment of UD treebanks.", "published": "2024-03-26 14:40:10", "link": "http://arxiv.org/abs/2403.17748v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can multiple-choice questions really be useful in detecting the\n  abilities of LLMs?", "abstract": "Multiple-choice questions (MCQs) are widely used in the evaluation of large\nlanguage models (LLMs) due to their simplicity and efficiency. However, there\nare concerns about whether MCQs can truly measure LLM's capabilities,\nparticularly in knowledge-intensive scenarios where long-form generation (LFG)\nanswers are required. The misalignment between the task and the evaluation\nmethod demands a thoughtful analysis of MCQ's efficacy, which we undertake in\nthis paper by evaluating nine LLMs on four question-answering (QA) datasets in\ntwo languages: Chinese and English. We identify a significant issue: LLMs\nexhibit an order sensitivity in bilingual MCQs, favoring answers located at\nspecific positions, i.e., the first position. We further quantify the gap\nbetween MCQs and long-form generation questions (LFGQs) by comparing their\ndirect outputs, token logits, and embeddings. Our results reveal a relatively\nlow correlation between answers from MCQs and LFGQs for identical questions.\nAdditionally, we propose two methods to quantify the consistency and confidence\nof LLMs' output, which can be generalized to other QA evaluation benchmarks.\nNotably, our analysis challenges the idea that the higher the consistency, the\ngreater the accuracy. We also find MCQs to be less reliable than LFGQs in terms\nof expected calibration error. Finally, the misalignment between MCQs and LFGQs\nis not only reflected in the evaluation performance but also in the embedding\nspace. Our code and models can be accessed at\nhttps://github.com/Meetyou-AI-Lab/Can-MC-Evaluate-LLMs.", "published": "2024-03-26 14:43:48", "link": "http://arxiv.org/abs/2403.17752v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Constructions Are So Difficult That Even Large Language Models Get Them\n  Right for the Wrong Reasons", "abstract": "In this paper, we make a contribution that can be understood from two\nperspectives: from an NLP perspective, we introduce a small challenge dataset\nfor NLI with large lexical overlap, which minimises the possibility of models\ndiscerning entailment solely based on token distinctions, and show that GPT-4\nand Llama 2 fail it with strong bias. We then create further challenging\nsub-tasks in an effort to explain this failure. From a Computational\nLinguistics perspective, we identify a group of constructions with three\nclasses of adjectives which cannot be distinguished by surface features. This\nenables us to probe for LLM's understanding of these constructions in various\nways, and we find that they fail in a variety of ways to distinguish between\nthem, suggesting that they don't adequately represent their meaning or capture\nthe lexical properties of phrasal heads.", "published": "2024-03-26 14:51:12", "link": "http://arxiv.org/abs/2403.17760v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Graph Language Model (GLM): A new graph-based approach to detect social\n  instabilities", "abstract": "This scientific report presents a novel methodology for the early prediction\nof important political events using News datasets. The methodology leverages\nnatural language processing, graph theory, clique analysis, and semantic\nrelationships to uncover hidden predictive signals within the data. Initially,\nwe designed a preliminary version of the method and tested it on a few events.\nThis analysis revealed limitations in the initial research phase. We then\nenhanced the model in two key ways: first, we added a filtration step to only\nconsider politically relevant news before further processing; second, we\nadjusted the input features to make the alert system more sensitive to\nsignificant spikes in the data. After finalizing the improved methodology, we\ntested it on eleven events including US protests, the Ukraine war, and French\nprotests. Results demonstrate the superiority of our approach compared to\nbaseline methods. Through targeted refinements, our model can now provide\nearlier and more accurate predictions of major political events based on subtle\npatterns in news data.", "published": "2024-03-26 15:53:02", "link": "http://arxiv.org/abs/2403.17816v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Verbing Weirds Language (Models): Evaluation of English Zero-Derivation\n  in Five LLMs", "abstract": "Lexical-syntactic flexibility, in the form of conversion (or zero-derivation)\nis a hallmark of English morphology. In conversion, a word with one part of\nspeech is placed in a non-prototypical context, where it is coerced to behave\nas if it had a different part of speech. However, while this process affects a\nlarge part of the English lexicon, little work has been done to establish the\ndegree to which language models capture this type of generalization. This paper\nreports the first study on the behavior of large language models with reference\nto conversion. We design a task for testing lexical-syntactic flexibility --\nthe degree to which models can generalize over words in a construction with a\nnon-prototypical part of speech. This task is situated within a natural\nlanguage inference paradigm. We test the abilities of five language models --\ntwo proprietary models (GPT-3.5 and GPT-4), three open-source models (Mistral\n7B, Falcon 40B, and Llama 2 70B). We find that GPT-4 performs best on the task,\nfollowed by GPT-3.5, but that the open source language models are also able to\nperform it and that the 7B parameter Mistral displays as little difference\nbetween its baseline performance on the natural language inference task and the\nnon-prototypical syntactic category task, as the massive GPT-4.", "published": "2024-03-26 16:45:27", "link": "http://arxiv.org/abs/2403.17856v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ChroniclingAmericaQA: A Large-scale Question Answering Dataset based on\n  Historical American Newspaper Pages", "abstract": "Question answering (QA) and Machine Reading Comprehension (MRC) tasks have\nsignificantly advanced in recent years due to the rapid development of deep\nlearning techniques and, more recently, large language models. At the same\ntime, many benchmark datasets have become available for QA and MRC tasks.\nHowever, most existing large-scale benchmark datasets have been created\npredominantly using synchronous document collections like Wikipedia or the Web.\nArchival document collections, such as historical newspapers, contain valuable\ninformation from the past that is still not widely used to train large language\nmodels. To further contribute to advancing QA and MRC tasks and to overcome the\nlimitation of previous datasets, we introduce ChroniclingAmericaQA, a\nlarge-scale temporal QA dataset with 487K question-answer pairs created based\non the historical newspaper collection Chronicling America. Our dataset is\nconstructed from a subset of the Chronicling America newspaper collection\nspanning 120 years. One of the significant challenges for utilizing digitized\nhistorical newspaper collections is the low quality of OCR text. Therefore, to\nenable realistic testing of QA models, our dataset can be used in three\ndifferent ways: answering questions from raw and noisy content, answering\nquestions from cleaner, corrected version of the content, as well as answering\nquestions from scanned images of newspaper pages. This and the fact that\nChroniclingAmericaQA spans the longest time period among available QA datasets\nmake it quite a unique and useful resource.", "published": "2024-03-26 16:48:13", "link": "http://arxiv.org/abs/2403.17859v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Illuminating Blind Spots of Language Models with Targeted\n  Agent-in-the-Loop Synthetic Data", "abstract": "Language models (LMs) have achieved impressive accuracy across a variety of\ntasks but remain vulnerable to high-confidence misclassifications, also\nreferred to as unknown unknowns (UUs). These UUs cluster into blind spots in\nthe feature space, leading to significant risks in high-stakes applications.\nThis is particularly relevant for smaller, lightweight LMs that are more\nsusceptible to such errors. While the identification of UUs has been\nextensively studied, their mitigation remains an open challenge, including how\nto use identified UUs to eliminate unseen blind spots. In this work, we propose\na novel approach to address blind spot mitigation through the use of\nintelligent agents -- either humans or large LMs -- as teachers to characterize\nUU-type errors. By leveraging the generalization capabilities of intelligent\nagents, we identify patterns in high-confidence misclassifications and use them\nto generate targeted synthetic samples to improve model robustness and reduce\nblind spots. We conduct an extensive evaluation of our method on three\nclassification tasks and demonstrate its effectiveness in reducing the number\nof UUs, all while maintaining a similar level of accuracy. We find that the\neffectiveness of human computation has a high ceiling but is highly dependent\non familiarity with the underlying task. Moreover, the cost gap between humans\nand LMs surpasses an order of magnitude, as LMs attain human-like\ngeneralization and generation performance while being more scalable.", "published": "2024-03-26 16:49:25", "link": "http://arxiv.org/abs/2403.17860v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enriching Word Usage Graphs with Cluster Definitions", "abstract": "We present a dataset of word usage graphs (WUGs), where the existing WUGs for\nmultiple languages are enriched with cluster labels functioning as sense\ndefinitions. They are generated from scratch by fine-tuned encoder-decoder\nlanguage models. The conducted human evaluation has shown that these\ndefinitions match the existing clusters in WUGs better than the definitions\nchosen from WordNet by two baseline systems. At the same time, the method is\nstraightforward to use and easy to extend to new languages. The resulting\nenriched datasets can be extremely helpful for moving on to explainable\nsemantic change modeling.", "published": "2024-03-26 18:22:05", "link": "http://arxiv.org/abs/2403.18024v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Impact of Syntactic and Semantic Proximity on Machine Translation\n  with Back-Translation", "abstract": "Unsupervised on-the-fly back-translation, in conjunction with multilingual\npretraining, is the dominant method for unsupervised neural machine\ntranslation. Theoretically, however, the method should not work in general. We\ntherefore conduct controlled experiments with artificial languages to determine\nwhat properties of languages make back-translation an effective training\nmethod, covering lexical, syntactic, and semantic properties. We find, contrary\nto popular belief, that (i) parallel word frequency distributions, (ii)\npartially shared vocabulary, and (iii) similar syntactic structure across\nlanguages are not sufficient to explain the success of back-translation. We\nshow however that even crude semantic signal (similar lexical fields across\nlanguages) does improve alignment of two languages through back-translation. We\nconjecture that rich semantic dependencies, parallel across languages, are at\nthe root of the success of unsupervised methods based on back-translation.\nOverall, the success of unsupervised machine translation was far from being\nanalytically guaranteed. Instead, it is another proof that languages of the\nworld share deep similarities, and we hope to show how to identify which of\nthese similarities can serve the development of unsupervised, cross-linguistic\ntools.", "published": "2024-03-26 18:38:14", "link": "http://arxiv.org/abs/2403.18031v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "For those who don't know (how) to ask: Building a dataset of technology\n  questions for digital newcomers", "abstract": "While the rise of large language models (LLMs) has created rich new\nopportunities to learn about digital technology, many on the margins of this\ntechnology struggle to gain and maintain competency due to lexical or\nconceptual barriers that prevent them from asking appropriate questions.\nAlthough there have been many efforts to understand factuality of LLM-created\ncontent and ability of LLMs to answer questions, it is not well understood how\nunclear or nonstandard language queries affect the model outputs. We propose\nthe creation of a dataset that captures questions of digital newcomers and\noutsiders, utilizing data we have compiled from a decade's worth of one-on-one\ntutoring. In this paper we lay out our planned efforts and some potential uses\nof this dataset.", "published": "2024-03-26 22:08:33", "link": "http://arxiv.org/abs/2403.18125v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models as Financial Data Annotators: A Study on\n  Effectiveness and Efficiency", "abstract": "Collecting labeled datasets in finance is challenging due to scarcity of\ndomain experts and higher cost of employing them. While Large Language Models\n(LLMs) have demonstrated remarkable performance in data annotation tasks on\ngeneral domain datasets, their effectiveness on domain specific datasets\nremains underexplored. To address this gap, we investigate the potential of\nLLMs as efficient data annotators for extracting relations in financial\ndocuments. We compare the annotations produced by three LLMs (GPT-4, PaLM 2,\nand MPT Instruct) against expert annotators and crowdworkers. We demonstrate\nthat the current state-of-the-art LLMs can be sufficient alternatives to\nnon-expert crowdworkers. We analyze models using various prompts and parameter\nsettings and find that customizing the prompts for each relation group by\nproviding specific examples belonging to those groups is paramount.\nFurthermore, we introduce a reliability index (LLM-RelIndex) used to identify\noutputs that may require expert attention. Finally, we perform an extensive\ntime, cost and error analysis and provide recommendations for the collection\nand usage of automated annotations in domain-specific settings.", "published": "2024-03-26 23:32:52", "link": "http://arxiv.org/abs/2403.18152v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Domain Adaptation in Intent Classification Systems: A Review", "abstract": "Dialogue agents, which perform specific tasks, are part of the long-term goal\nof NLP researchers to build intelligent agents that communicate with humans in\nnatural language. Such systems should adapt easily from one domain to another\nto assist users in completing tasks. Researchers have developed a broad range\nof techniques, objectives, and datasets for intent classification to achieve\nsuch systems. Despite the progress in developing intent classification systems\n(ICS), a systematic review of the progress from a technical perspective is yet\nto be conducted. In effect, important implementation details of intent\nclassification remain restricted and unclear, making it hard for natural\nlanguage processing (NLP) researchers to develop new methods. To fill this gap,\nwe review contemporary works in intent classification. Specifically, we conduct\na thorough technical review of the datasets, domains, tasks, and methods needed\nto train the intent classification part of dialogue systems. Our structured\nanalysis describes why intent classification is difficult and studies the\nlimitations to domain adaptation while presenting opportunities for future\nwork.", "published": "2024-03-26 15:59:05", "link": "http://arxiv.org/abs/2404.14415v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Introducing Syllable Tokenization for Low-resource Languages: A Case\n  Study with Swahili", "abstract": "Many attempts have been made in multilingual NLP to ensure that pre-trained\nlanguage models, such as mBERT or GPT2 get better and become applicable to\nlow-resource languages. To achieve multilingualism for pre-trained language\nmodels (PLMs), we need techniques to create word embeddings that capture the\nlinguistic characteristics of any language. Tokenization is one such technique\nbecause it allows for the words to be split based on characters or subwords,\ncreating word embeddings that best represent the structure of the language.\nCreating such word embeddings is essential to applying PLMs to other languages\nwhere the model was not trained, enabling multilingual NLP. However, most PLMs\nuse generic tokenization methods like BPE, wordpiece, or unigram which may not\nsuit specific languages. We hypothesize that tokenization based on syllables\nwithin the input text, which we call syllable tokenization, should facilitate\nthe development of syllable-aware language models. The syllable-aware language\nmodels make it possible to apply PLMs to languages that are rich in syllables,\nfor instance, Swahili. Previous works introduced subword tokenization. Our work\nextends such efforts. Notably, we propose a syllable tokenizer and adopt an\nexperiment-centric approach to validate the proposed tokenizer based on the\nSwahili language. We conducted text-generation experiments with GPT2 to\nevaluate the effectiveness of the syllable tokenizer. Our results show that the\nproposed syllable tokenizer generates syllable embeddings that effectively\nrepresent the Swahili language.", "published": "2024-03-26 17:26:50", "link": "http://arxiv.org/abs/2406.15358v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "InternLM2 Technical Report", "abstract": "The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has\nsparked discussions on the advent of Artificial General Intelligence (AGI).\nHowever, replicating such advancements in open-source models has been\nchallenging. This paper introduces InternLM2, an open-source LLM that\noutperforms its predecessors in comprehensive evaluations across 6 dimensions\nand 30 benchmarks, long-context modeling, and open-ended subjective evaluations\nthrough innovative pre-training and optimization techniques. The pre-training\nprocess of InternLM2 is meticulously detailed, highlighting the preparation of\ndiverse data types including text, code, and long-context data. InternLM2\nefficiently captures long-term dependencies, initially trained on 4k tokens\nbefore advancing to 32k tokens in pre-training and fine-tuning stages,\nexhibiting remarkable performance on the 200k ``Needle-in-a-Haystack\" test.\nInternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel\nConditional Online Reinforcement Learning from Human Feedback (COOL RLHF)\nstrategy that addresses conflicting human preferences and reward hacking. By\nreleasing InternLM2 models in different training stages and model sizes, we\nprovide the community with insights into the model's evolution.", "published": "2024-03-26 00:53:24", "link": "http://arxiv.org/abs/2403.17297v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Decoding Probing: Revealing Internal Linguistic Structures in Neural\n  Language Models using Minimal Pairs", "abstract": "Inspired by cognitive neuroscience studies, we introduce a novel `decoding\nprobing' method that uses minimal pairs benchmark (BLiMP) to probe internal\nlinguistic characteristics in neural language models layer by layer. By\ntreating the language model as the `brain' and its representations as `neural\nactivations', we decode grammaticality labels of minimal pairs from the\nintermediate layers' representations. This approach reveals: 1) Self-supervised\nlanguage models capture abstract linguistic structures in intermediate layers\nthat GloVe and RNN language models cannot learn. 2) Information about syntactic\ngrammaticality is robustly captured through the first third layers of GPT-2 and\nalso distributed in later layers. As sentence complexity increases, more layers\nare required for learning grammatical capabilities. 3) Morphological and\nsemantics/syntax interface-related features are harder to capture than syntax.\n4) For Transformer-based models, both embeddings and attentions capture\ngrammatical features but show distinct patterns. Different attention heads\nexhibit similar tendencies toward various linguistic phenomena, but with varied\ncontributions.", "published": "2024-03-26 00:56:06", "link": "http://arxiv.org/abs/2403.17299v1", "categories": ["cs.CL", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "JMultiWOZ: A Large-Scale Japanese Multi-Domain Task-Oriented Dialogue\n  Dataset", "abstract": "Dialogue datasets are crucial for deep learning-based task-oriented dialogue\nsystem research. While numerous English language multi-domain task-oriented\ndialogue datasets have been developed and contributed to significant\nadvancements in task-oriented dialogue systems, such a dataset does not exist\nin Japanese, and research in this area is limited compared to that in English.\nIn this study, towards the advancement of research and development of\ntask-oriented dialogue systems in Japanese, we constructed JMultiWOZ, the first\nJapanese language large-scale multi-domain task-oriented dialogue dataset.\nUsing JMultiWOZ, we evaluated the dialogue state tracking and response\ngeneration capabilities of the state-of-the-art methods on the existing major\nEnglish benchmark dataset MultiWOZ2.2 and the latest large language model\n(LLM)-based methods. Our evaluation results demonstrated that JMultiWOZ\nprovides a benchmark that is on par with MultiWOZ2.2. In addition, through\nevaluation experiments of interactive dialogues with the models and human\nparticipants, we identified limitations in the task completion capabilities of\nLLMs in Japanese.", "published": "2024-03-26 02:01:18", "link": "http://arxiv.org/abs/2403.17319v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Don't Listen To Me: Understanding and Exploring Jailbreak Prompts of\n  Large Language Models", "abstract": "Recent advancements in generative AI have enabled ubiquitous access to large\nlanguage models (LLMs). Empowered by their exceptional capabilities to\nunderstand and generate human-like text, these models are being increasingly\nintegrated into our society. At the same time, there are also concerns on the\npotential misuse of this powerful technology, prompting defensive measures from\nservice providers. To overcome such protection, jailbreaking prompts have\nrecently emerged as one of the most effective mechanisms to circumvent security\nrestrictions and elicit harmful content originally designed to be prohibited.\n  Due to the rapid development of LLMs and their ease of access via natural\nlanguages, the frontline of jailbreak prompts is largely seen in online forums\nand among hobbyists. To gain a better understanding of the threat landscape of\nsemantically meaningful jailbreak prompts, we systemized existing prompts and\nmeasured their jailbreak effectiveness empirically. Further, we conducted a\nuser study involving 92 participants with diverse backgrounds to unveil the\nprocess of manually creating jailbreak prompts. We observed that users often\nsucceeded in jailbreak prompts generation regardless of their expertise in\nLLMs. Building on the insights from the user study, we also developed a system\nusing AI as the assistant to automate the process of jailbreak prompt\ngeneration.", "published": "2024-03-26 02:47:42", "link": "http://arxiv.org/abs/2403.17336v2", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Disambiguate Entity Matching using Large Language Models through\n  Relation Discovery", "abstract": "Entity matching is a critical challenge in data integration and cleaning,\ncentral to tasks like fuzzy joins and deduplication. Traditional approaches\nhave focused on overcoming fuzzy term representations through methods such as\nedit distance, Jaccard similarity, and more recently, embeddings and deep\nneural networks, including advancements from large language models (LLMs) like\nGPT. However, the core challenge in entity matching extends beyond term\nfuzziness to the ambiguity in defining what constitutes a \"match,\" especially\nwhen integrating with external databases. This ambiguity arises due to varying\nlevels of detail and granularity among entities, complicating exact matches. We\npropose a novel approach that shifts focus from purely identifying semantic\nsimilarities to understanding and defining the \"relations\" between entities as\ncrucial for resolving ambiguities in matching. By predefining a set of\nrelations relevant to the task at hand, our method allows analysts to navigate\nthe spectrum of similarity more effectively, from exact matches to conceptually\nrelated entities.", "published": "2024-03-26 03:07:32", "link": "http://arxiv.org/abs/2403.17344v2", "categories": ["cs.DB", "cs.CL"], "primary_category": "cs.DB"}
{"title": "Bridging Textual and Tabular Worlds for Fact Verification: A\n  Lightweight, Attention-Based Model", "abstract": "FEVEROUS is a benchmark and research initiative focused on fact extraction\nand verification tasks involving unstructured text and structured tabular data.\nIn FEVEROUS, existing works often rely on extensive preprocessing and utilize\nrule-based transformations of data, leading to potential context loss or\nmisleading encodings. This paper introduces a simple yet powerful model that\nnullifies the need for modality conversion, thereby preserving the original\nevidence's context. By leveraging pre-trained models on diverse text and\ntabular datasets and by incorporating a lightweight attention-based mechanism,\nour approach efficiently exploits latent connections between different data\ntypes, thereby yielding comprehensive and reliable verdict predictions. The\nmodel's modular structure adeptly manages multi-modal information, ensuring the\nintegrity and authenticity of the original evidence are uncompromised.\nComparative analyses reveal that our approach exhibits competitive performance,\naligning itself closely with top-tier models on the FEVEROUS benchmark.", "published": "2024-03-26 03:54:25", "link": "http://arxiv.org/abs/2403.17361v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ChatGPT Rates Natural Language Explanation Quality Like Humans: But on\n  Which Scales?", "abstract": "As AI becomes more integral in our lives, the need for transparency and\nresponsibility grows. While natural language explanations (NLEs) are vital for\nclarifying the reasoning behind AI decisions, evaluating them through human\njudgments is complex and resource-intensive due to subjectivity and the need\nfor fine-grained ratings. This study explores the alignment between ChatGPT and\nhuman assessments across multiple scales (i.e., binary, ternary, and 7-Likert\nscale). We sample 300 data instances from three NLE datasets and collect 900\nhuman annotations for both informativeness and clarity scores as the text\nquality measurement. We further conduct paired comparison experiments under\ndifferent ranges of subjectivity scores, where the baseline comes from 8,346\nhuman annotations. Our results show that ChatGPT aligns better with humans in\nmore coarse-grained scales. Also, paired comparisons and dynamic prompting\n(i.e., providing semantically similar examples in the prompt) improve the\nalignment. This research advances our understanding of large language models'\ncapabilities to assess the text explanation quality in different configurations\nfor responsible AI development.", "published": "2024-03-26 04:07:08", "link": "http://arxiv.org/abs/2403.17368v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Aligning Large Language Models for Enhancing Psychiatric Interviews\n  Through Symptom Delineation and Summarization: Pilot Study", "abstract": "Background: Advancements in large language models (LLMs) have opened new\npossibilities in psychiatric interviews, an underexplored area where LLMs could\nbe valuable. This study focuses on enhancing psychiatric interviews by\nanalyzing counseling data from North Korean defectors who have experienced\ntrauma and mental health issues.\n  Objective: The study investigates whether LLMs can (1) identify parts of\nconversations that suggest psychiatric symptoms and recognize those symptoms,\nand (2) summarize stressors and symptoms based on interview transcripts.\n  Methods: LLMs are tasked with (1) extracting stressors from transcripts, (2)\nidentifying symptoms and their corresponding sections, and (3) generating\ninterview summaries using the extracted data. The transcripts were labeled by\nmental health experts for training and evaluation.\n  Results: In the zero-shot inference setting using GPT-4 Turbo, 73 out of 102\nsegments demonstrated a recall mid-token distance d < 20 in identifying\nsymptom-related sections. For recognizing specific symptoms, fine-tuning\noutperformed zero-shot inference, achieving an accuracy, precision, recall, and\nF1-score of 0.82. For the generative summarization task, LLMs using symptom and\nstressor information scored highly on G-Eval metrics: coherence (4.66),\nconsistency (4.73), fluency (2.16), and relevance (4.67). Retrieval-augmented\ngeneration showed no notable performance improvement.\n  Conclusions: LLMs, with fine-tuning or appropriate prompting, demonstrated\nstrong accuracy (over 0.8) for symptom delineation and achieved high coherence\n(4.6+) in summarization. This study highlights their potential to assist mental\nhealth practitioners in analyzing psychiatric interviews.", "published": "2024-03-26 06:50:04", "link": "http://arxiv.org/abs/2403.17428v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Robust and Scalable Model Editing for Large Language Models", "abstract": "Large language models (LLMs) can make predictions using parametric\nknowledge--knowledge encoded in the model weights--or contextual\nknowledge--knowledge presented in the context. In many scenarios, a desirable\nbehavior is that LLMs give precedence to contextual knowledge when it conflicts\nwith the parametric knowledge, and fall back to using their parametric\nknowledge when the context is irrelevant. This enables updating and correcting\nthe model's knowledge by in-context editing instead of retraining. Previous\nworks have shown that LLMs are inclined to ignore contextual knowledge and fail\nto reliably fall back to parametric knowledge when presented with irrelevant\ncontext. In this work, we discover that, with proper prompting methods,\ninstruction-finetuned LLMs can be highly controllable by contextual knowledge\nand robust to irrelevant context. Utilizing this feature, we propose EREN (Edit\nmodels by REading Notes) to improve the scalability and robustness of LLM\nediting. To better evaluate the robustness of model editors, we collect a new\ndataset, that contains irrelevant questions that are more challenging than the\nones in existing datasets. Empirical results show that our method outperforms\ncurrent state-of-the-art methods by a large margin. Unlike existing techniques,\nit can integrate knowledge from multiple edits, and correctly respond to\nsyntactically similar but semantically unrelated inputs (and vice versa). The\nsource code can be found at https://github.com/thunlp/EREN.", "published": "2024-03-26 06:57:23", "link": "http://arxiv.org/abs/2403.17431v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sharing the Cost of Success: A Game for Evaluating and Learning\n  Collaborative Multi-Agent Instruction Giving and Following Policies", "abstract": "In collaborative goal-oriented settings, the participants are not only\ninterested in achieving a successful outcome, but do also implicitly negotiate\nthe effort they put into the interaction (by adapting to each other). In this\nwork, we propose a challenging interactive reference game that requires two\nplayers to coordinate on vision and language observations. The learning signal\nin this game is a score (given after playing) that takes into account the\nachieved goal and the players' assumed efforts during the interaction. We show\nthat a standard Proximal Policy Optimization (PPO) setup achieves a high\nsuccess rate when bootstrapped with heuristic partner behaviors that implement\ninsights from the analysis of human-human interactions. And we find that a\npairing of neural partners indeed reduces the measured joint effort when\nplaying together repeatedly. However, we observe that in comparison to a\nreasonable heuristic pairing there is still room for improvement -- which\ninvites further research in the direction of cost-sharing in collaborative\ninteractions.", "published": "2024-03-26 08:58:28", "link": "http://arxiv.org/abs/2403.17497v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "MapGuide: A Simple yet Effective Method to Reconstruct Continuous\n  Language from Brain Activities", "abstract": "Decoding continuous language from brain activity is a formidable yet\npromising field of research. It is particularly significant for aiding people\nwith speech disabilities to communicate through brain signals. This field\naddresses the complex task of mapping brain signals to text. The previous best\nattempt reverse-engineered this process in an indirect way: it began by\nlearning to encode brain activity from text and then guided text generation by\naligning with predicted brain responses. In contrast, we propose a simple yet\neffective method that guides text reconstruction by directly comparing them\nwith the predicted text embeddings mapped from brain activities. Comprehensive\nexperiments reveal that our method significantly outperforms the current\nstate-of-the-art model, showing average improvements of 77% and 54% on BLEU and\nMETEOR scores. We further validate the proposed modules through detailed\nablation studies and case analyses and highlight a critical correlation: the\nmore precisely we map brain activities to text embeddings, the better the text\nreconstruction results. Such insight can simplify the task of reconstructing\nlanguage from brain activities for future work, emphasizing the importance of\nimproving brain-to-text-embedding mapping techniques.", "published": "2024-03-26 09:18:59", "link": "http://arxiv.org/abs/2403.17516v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Provably Secure Disambiguating Neural Linguistic Steganography", "abstract": "Recent research in provably secure neural linguistic steganography has\noverlooked a crucial aspect: the sender must detokenize stegotexts to avoid\nraising suspicion from the eavesdropper. The segmentation ambiguity problem,\nwhich arises when using language models based on subwords, leads to occasional\ndecoding failures in all neural language steganography implementations based on\nthese models. Current solutions to this issue involve altering the probability\ndistribution of candidate words, rendering them incompatible with provably\nsecure steganography. We propose a novel secure disambiguation method named\nSyncPool, which effectively addresses the segmentation ambiguity problem. We\ngroup all tokens with prefix relationships in the candidate pool before the\nsteganographic embedding algorithm runs to eliminate uncertainty among\nambiguous tokens. To enable the receiver to synchronize the sampling process of\nthe sender, a shared cryptographically-secure pseudorandom number generator\n(CSPRNG) is deployed to select a token from the ambiguity pool. SyncPool does\nnot change the size of the candidate pool or the distribution of tokens and\nthus is applicable to provably secure language steganography methods. We\nprovide theoretical proofs and experimentally demonstrate the applicability of\nour solution to various languages and models, showing its potential to\nsignificantly improve the reliability and security of neural linguistic\nsteganography systems.", "published": "2024-03-26 09:25:57", "link": "http://arxiv.org/abs/2403.17524v2", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "A Gaze-grounded Visual Question Answering Dataset for Clarifying\n  Ambiguous Japanese Questions", "abstract": "Situated conversations, which refer to visual information as visual question\nanswering (VQA), often contain ambiguities caused by reliance on directive\ninformation. This problem is exacerbated because some languages, such as\nJapanese, often omit subjective or objective terms. Such ambiguities in\nquestions are often clarified by the contexts in conversational situations,\nsuch as joint attention with a user or user gaze information. In this study, we\npropose the Gaze-grounded VQA dataset (GazeVQA) that clarifies ambiguous\nquestions using gaze information by focusing on a clarification process\ncomplemented by gaze information. We also propose a method that utilizes gaze\ntarget estimation results to improve the accuracy of GazeVQA tasks. Our\nexperimental results showed that the proposed method improved the performance\nin some cases of a VQA system on GazeVQA and identified some typical problems\nof GazeVQA tasks that need to be improved.", "published": "2024-03-26 09:49:35", "link": "http://arxiv.org/abs/2403.17545v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "m3P: Towards Multimodal Multilingual Translation with Multimodal Prompt", "abstract": "Multilingual translation supports multiple translation directions by\nprojecting all languages in a shared space, but the translation quality is\nundermined by the difference between languages in the text-only modality,\nespecially when the number of languages is large. To bridge this gap, we\nintroduce visual context as the universal language-independent representation\nto facilitate multilingual translation. In this paper, we propose a framework\nto leverage the multimodal prompt to guide the Multimodal Multilingual neural\nMachine Translation (m3P), which aligns the representations of different\nlanguages with the same meaning and generates the conditional vision-language\nmemory for translation. We construct a multilingual multimodal instruction\ndataset (InstrMulti102) to support 102 languages. Our method aims to minimize\nthe representation distance of different languages by regarding the image as a\ncentral language. Experimental results show that m3P outperforms previous\ntext-only baselines and multilingual multimodal methods by a large margin.\nFurthermore, the probing experiments validate the effectiveness of our method\nin enhancing translation under the low-resource and massively multilingual\nscenario.", "published": "2024-03-26 10:04:24", "link": "http://arxiv.org/abs/2403.17556v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Denoising Table-Text Retrieval for Open-Domain Question Answering", "abstract": "In table-text open-domain question answering, a retriever system retrieves\nrelevant evidence from tables and text to answer questions. Previous studies in\ntable-text open-domain question answering have two common challenges: firstly,\ntheir retrievers can be affected by false-positive labels in training datasets;\nsecondly, they may struggle to provide appropriate evidence for questions that\nrequire reasoning across the table. To address these issues, we propose\nDenoised Table-Text Retriever (DoTTeR). Our approach involves utilizing a\ndenoised training dataset with fewer false positive labels by discarding\ninstances with lower question-relevance scores measured through a false\npositive detection model. Subsequently, we integrate table-level ranking\ninformation into the retriever to assist in finding evidence for questions that\ndemand reasoning across the table. To encode this ranking information, we\nfine-tune a rank-aware column encoder to identify minimum and maximum values\nwithin a column. Experimental results demonstrate that DoTTeR significantly\noutperforms strong baselines on both retrieval recall and downstream QA tasks.\nOur code is available at https://github.com/deokhk/DoTTeR.", "published": "2024-03-26 11:44:49", "link": "http://arxiv.org/abs/2403.17611v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Language Models for Text Classification: Is In-Context Learning Enough?", "abstract": "Recent foundational language models have shown state-of-the-art performance\nin many NLP tasks in zero- and few-shot settings. An advantage of these models\nover more standard approaches based on fine-tuning is the ability to understand\ninstructions written in natural language (prompts), which helps them generalise\nbetter to different tasks and domains without the need for specific training\ndata. This makes them suitable for addressing text classification problems for\ndomains with limited amounts of annotated instances. However, existing research\nis limited in scale and lacks understanding of how text generation models\ncombined with prompting techniques compare to more established methods for text\nclassification such as fine-tuning masked language models. In this paper, we\naddress this research gap by performing a large-scale evaluation study for 16\ntext classification datasets covering binary, multiclass, and multilabel\nproblems. In particular, we compare zero- and few-shot approaches of large\nlanguage models to fine-tuning smaller language models. We also analyse the\nresults by prompt, classification type, domain, and number of labels. In\ngeneral, the results show how fine-tuning smaller and more efficient language\nmodels can still outperform few-shot approaches of larger language models,\nwhich have room for improvement when it comes to text classification.", "published": "2024-03-26 12:47:39", "link": "http://arxiv.org/abs/2403.17661v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Not All Similarities Are Created Equal: Leveraging Data-Driven Biases to\n  Inform GenAI Copyright Disputes", "abstract": "The advent of Generative Artificial Intelligence (GenAI) models, including\nGitHub Copilot, OpenAI GPT, and Stable Diffusion, has revolutionized content\ncreation, enabling non-professionals to produce high-quality content across\nvarious domains. This transformative technology has led to a surge of synthetic\ncontent and sparked legal disputes over copyright infringement. To address\nthese challenges, this paper introduces a novel approach that leverages the\nlearning capacity of GenAI models for copyright legal analysis, demonstrated\nwith GPT2 and Stable Diffusion models. Copyright law distinguishes between\noriginal expressions and generic ones (Sc\\`enes \\`a faire), protecting the\nformer and permitting reproduction of the latter. However, this distinction has\nhistorically been challenging to make consistently, leading to over-protection\nof copyrighted works. GenAI offers an unprecedented opportunity to enhance this\nlegal analysis by revealing shared patterns in preexisting works. We propose a\ndata-driven approach to identify the genericity of works created by GenAI,\nemploying \"data-driven bias\" to assess the genericity of expressive\ncompositions. This approach aids in copyright scope determination by utilizing\nthe capabilities of GenAI to identify and prioritize expressive elements and\nrank them according to their frequency in the model's dataset. The potential\nimplications of measuring expressive genericity for copyright law are profound.\nSuch scoring could assist courts in determining copyright scope during\nlitigation, inform the registration practices of Copyright Offices, allowing\nregistration of only highly original synthetic works, and help copyright owners\nsignal the value of their works and facilitate fairer licensing deals. More\ngenerally, this approach offers valuable insights to policymakers grappling\nwith adapting copyright law to the challenges posed by the era of GenAI.", "published": "2024-03-26 13:32:32", "link": "http://arxiv.org/abs/2403.17691v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "A Large Language Model Guided Topic Refinement Mechanism for Short Text\n  Modeling", "abstract": "Modeling topics effectively in short texts, such as tweets and news snippets,\nis crucial to capturing rapidly evolving social trends. Existing topic models\noften struggle to accurately capture the underlying semantic patterns of short\ntexts, primarily due to the sparse nature of such data. This nature of texts\nleads to an unavoidable lack of co-occurrence information, which hinders the\ncoherence and granularity of mined topics. This paper introduces a novel\nmodel-agnostic mechanism, termed Topic Refinement, which leverages the advanced\ntext comprehension capabilities of Large Language Models (LLMs) for short-text\ntopic modeling. Unlike traditional methods, this post-processing mechanism\nenhances the quality of topics extracted by various topic modeling methods\nthrough prompt engineering. We guide LLMs in identifying semantically intruder\nwords within the extracted topics and suggesting coherent alternatives to\nreplace these words. This process mimics human-like identification, evaluation,\nand refinement of the extracted topics. Extensive experiments on four diverse\ndatasets demonstrate that Topic Refinement boosts the topic quality and\nimproves the performance in topic-related text classification tasks.", "published": "2024-03-26 13:50:34", "link": "http://arxiv.org/abs/2403.17706v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Text-to-Image Consistency via Automatic Prompt Optimization", "abstract": "Impressive advances in text-to-image (T2I) generative models have yielded a\nplethora of high performing models which are able to generate aesthetically\nappealing, photorealistic images. Despite the progress, these models still\nstruggle to produce images that are consistent with the input prompt,\noftentimes failing to capture object quantities, relations and attributes\nproperly. Existing solutions to improve prompt-image consistency suffer from\nthe following challenges: (1) they oftentimes require model fine-tuning, (2)\nthey only focus on nearby prompt samples, and (3) they are affected by\nunfavorable trade-offs among image quality, representation diversity, and\nprompt-image consistency. In this paper, we address these challenges and\nintroduce a T2I optimization-by-prompting framework, OPT2I, which leverages a\nlarge language model (LLM) to improve prompt-image consistency in T2I models.\nOur framework starts from a user prompt and iteratively generates revised\nprompts with the goal of maximizing a consistency score. Our extensive\nvalidation on two datasets, MSCOCO and PartiPrompts, shows that OPT2I can boost\nthe initial consistency score by up to 24.9% in terms of DSG score while\npreserving the FID and increasing the recall between generated and real data.\nOur work paves the way toward building more reliable and robust T2I systems by\nharnessing the power of LLMs.", "published": "2024-03-26 15:42:01", "link": "http://arxiv.org/abs/2403.17804v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Have Faith in Faithfulness: Going Beyond Circuit Overlap When Finding\n  Model Mechanisms", "abstract": "Many recent language model (LM) interpretability studies have adopted the\ncircuits framework, which aims to find the minimal computational subgraph, or\ncircuit, that explains LM behavior on a given task. Most studies determine\nwhich edges belong in a LM's circuit by performing causal interventions on each\nedge independently, but this scales poorly with model size. Edge attribution\npatching (EAP), gradient-based approximation to interventions, has emerged as a\nscalable but imperfect solution to this problem. In this paper, we introduce a\nnew method - EAP with integrated gradients (EAP-IG) - that aims to better\nmaintain a core property of circuits: faithfulness. A circuit is faithful if\nall model edges outside the circuit can be ablated without changing the model's\nperformance on the task; faithfulness is what justifies studying circuits,\nrather than the full model. Our experiments demonstrate that circuits found\nusing EAP are less faithful than those found using EAP-IG, even though both\nhave high node overlap with circuits found previously using causal\ninterventions. We conclude more generally that when using circuits to compare\nthe mechanisms models use to solve tasks, faithfulness, not overlap, is what\nshould be measured.", "published": "2024-03-26 15:44:58", "link": "http://arxiv.org/abs/2403.17806v2", "categories": ["cs.LG", "cs.CL", "I.2.7"], "primary_category": "cs.LG"}
{"title": "Are Compressed Language Models Less Subgroup Robust?", "abstract": "To reduce the inference cost of large language models, model compression is\nincreasingly used to create smaller scalable models. However, little is known\nabout their robustness to minority subgroups defined by the labels and\nattributes of a dataset. In this paper, we investigate the effects of 18\ndifferent compression methods and settings on the subgroup robustness of BERT\nlanguage models. We show that worst-group performance does not depend on model\nsize alone, but also on the compression method used. Additionally, we find that\nmodel compression does not always worsen the performance on minority subgroups.\nAltogether, our analysis serves to further research into the subgroup\nrobustness of model compression.", "published": "2024-03-26 15:50:37", "link": "http://arxiv.org/abs/2403.17811v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "ArabicaQA: A Comprehensive Dataset for Arabic Question Answering", "abstract": "In this paper, we address the significant gap in Arabic natural language\nprocessing (NLP) resources by introducing ArabicaQA, the first large-scale\ndataset for machine reading comprehension and open-domain question answering in\nArabic. This comprehensive dataset, consisting of 89,095 answerable and 3,701\nunanswerable questions created by crowdworkers to look similar to answerable\nones, along with additional labels of open-domain questions marks a crucial\nadvancement in Arabic NLP resources. We also present AraDPR, the first dense\npassage retrieval model trained on the Arabic Wikipedia corpus, specifically\ndesigned to tackle the unique challenges of Arabic text retrieval. Furthermore,\nour study includes extensive benchmarking of large language models (LLMs) for\nArabic question answering, critically evaluating their performance in the\nArabic language context. In conclusion, ArabicaQA, AraDPR, and the benchmarking\nof LLMs in Arabic question answering offer significant advancements in the\nfield of Arabic NLP. The dataset and code are publicly accessible for further\nresearch https://github.com/DataScienceUIBK/ArabicaQA.", "published": "2024-03-26 16:37:54", "link": "http://arxiv.org/abs/2403.17848v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Using Domain Knowledge to Guide Dialog Structure Induction via Neural\n  Probabilistic Soft Logic", "abstract": "Dialog Structure Induction (DSI) is the task of inferring the latent dialog\nstructure (i.e., a set of dialog states and their temporal transitions) of a\ngiven goal-oriented dialog. It is a critical component for modern dialog system\ndesign and discourse analysis. Existing DSI approaches are often purely\ndata-driven, deploy models that infer latent states without access to domain\nknowledge, underperform when the training corpus is limited/noisy, or have\ndifficulty when test dialogs exhibit distributional shifts from the training\ndomain. This work explores a neural-symbolic approach as a potential solution\nto these problems. We introduce Neural Probabilistic Soft Logic Dialogue\nStructure Induction (NEUPSL DSI), a principled approach that injects symbolic\nknowledge into the latent space of a generative neural model. We conduct a\nthorough empirical investigation on the effect of NEUPSL DSI learning on hidden\nrepresentation quality, few-shot learning, and out-of-domain generalization\nperformance. Over three dialog structure induction datasets and across\nunsupervised and semi-supervised settings for standard and cross-domain\ngeneralization, the injection of symbolic knowledge using NEUPSL DSI provides a\nconsistent boost in performance over the canonical baselines.", "published": "2024-03-26 16:42:30", "link": "http://arxiv.org/abs/2403.17853v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DORE: A Dataset For Portuguese Definition Generation", "abstract": "Definition modelling (DM) is the task of automatically generating a\ndictionary definition for a specific word. Computational systems that are\ncapable of DM can have numerous applications benefiting a wide range of\naudiences. As DM is considered a supervised natural language generation\nproblem, these systems require large annotated datasets to train the machine\nlearning (ML) models. Several DM datasets have been released for English and\nother high-resource languages. While Portuguese is considered a\nmid/high-resource language in most natural language processing tasks and is\nspoken by more than 200 million native speakers, there is no DM dataset\navailable for Portuguese. In this research, we fill this gap by introducing\nDORE; the first dataset for Definition MOdelling for PoRtuguEse containing more\nthan 100,000 definitions. We also evaluate several deep learning based DM\nmodels on DORE and report the results. The dataset and the findings of this\npaper will facilitate research and study of Portuguese in wider contexts.", "published": "2024-03-26 18:07:10", "link": "http://arxiv.org/abs/2403.18018v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Supervisory Prompt Training", "abstract": "The performance of Large Language Models (LLMs) relies heavily on the quality\nof prompts, which are often manually engineered and task-specific, making them\ncostly and non-scalable. We propose a novel approach, Supervisory Prompt\nTraining (SPT). SPT automates the generation of highly effective prompts using\na dual LLM system. In this system, one LLM, the generator, performs a task\nwhile the other, the corrector, provides feedback and generates improved\nprompts. In contrast to earlier techniques, both the generator and corrector\ncollaboratively and continuously improve their prompts over time. We also\nintroduce the concept of \\textit{impact scores} to measure the sentence-level\neffectiveness of the prompts. Our method was tested on four benchmarks, testing\nthe level of hallucinations in LLMs. Notably, we were able to increase the\naccuracy of GPT-4 on GSM8K from 65.8\\% to 94.1\\% (28.3\\% increase). SPT\nadvances LLMs by refining prompts to enhance performance and reduce\nhallucinations, offering an efficient and scalable alternative to traditional\nmodel fine-tuning.", "published": "2024-03-26 19:08:20", "link": "http://arxiv.org/abs/2403.18051v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning", "abstract": "Remarkable progress on English instruction tuning has facilitated the\nefficacy and reliability of large language models (LLMs). However, there\nremains a noticeable gap in instruction tuning for Chinese, where the complex\nlinguistic features pose significant challenges. Existing datasets, generally\ndistilled from English-centric LLMs, are not well-aligned with Chinese users'\ninteraction patterns. To bridge this gap, we introduce COIG-CQIA, a new Chinese\ninstruction tuning dataset derived from various real-world resources and\nundergoing rigorous human verification. We conduct extensive experiments on\nCOIG-CQIA, and compare them with strong baseline models and datasets. The\nexperimental results show that models trained on COIG-CQIA achieve highly\ncompetitive performance in diverse benchmarks. Additionally, our findings offer\nseveral insights for designing effective Chinese instruction-tuning datasets\nand data-mixing strategies. Our dataset are available at\nhttps://huggingface.co/datasets/m-a-p/COIG-CQIA.", "published": "2024-03-26 19:24:18", "link": "http://arxiv.org/abs/2403.18058v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing Legal Document Retrieval: A Multi-Phase Approach with Large\n  Language Models", "abstract": "Large language models with billions of parameters, such as GPT-3.5, GPT-4,\nand LLaMA, are increasingly prevalent. Numerous studies have explored effective\nprompting techniques to harness the power of these LLMs for various research\nproblems. Retrieval, specifically in the legal data domain, poses a challenging\ntask for the direct application of Prompting techniques due to the large number\nand substantial length of legal articles. This research focuses on maximizing\nthe potential of prompting by placing it as the final phase of the retrieval\nsystem, preceded by the support of two phases: BM25 Pre-ranking and BERT-based\nRe-ranking. Experiments on the COLIEE 2023 dataset demonstrate that integrating\nprompting techniques on LLMs into the retrieval system significantly improves\nretrieval accuracy. However, error analysis reveals several existing issues in\nthe retrieval system that still need resolution.", "published": "2024-03-26 20:25:53", "link": "http://arxiv.org/abs/2403.18093v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GPTs and Language Barrier: A Cross-Lingual Legal QA Examination", "abstract": "In this paper, we explore the application of Generative Pre-trained\nTransformers (GPTs) in cross-lingual legal Question-Answering (QA) systems\nusing the COLIEE Task 4 dataset. In the COLIEE Task 4, given a statement and a\nset of related legal articles that serve as context, the objective is to\ndetermine whether the statement is legally valid, i.e., if it can be inferred\nfrom the provided contextual articles or not, which is also known as an\nentailment task. By benchmarking four different combinations of English and\nJapanese prompts and data, we provide valuable insights into GPTs' performance\nin multilingual legal QA scenarios, contributing to the development of more\nefficient and accurate cross-lingual QA solutions in the legal domain.", "published": "2024-03-26 20:47:32", "link": "http://arxiv.org/abs/2403.18098v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Language Models for Education: A Survey and Outlook", "abstract": "The advent of Large Language Models (LLMs) has brought in a new era of\npossibilities in the realm of education. This survey paper summarizes the\nvarious technologies of LLMs in educational settings from multifaceted\nperspectives, encompassing student and teacher assistance, adaptive learning,\nand commercial tools. We systematically review the technological advancements\nin each perspective, organize related datasets and benchmarks, and identify the\nrisks and challenges associated with deploying LLMs in education. Furthermore,\nwe outline future research opportunities, highlighting the potential promising\ndirections. Our survey aims to provide a comprehensive technological picture\nfor educators, researchers, and policymakers to harness the power of LLMs to\nrevolutionize educational practices and foster a more effective personalized\nlearning environment.", "published": "2024-03-26 21:04:29", "link": "http://arxiv.org/abs/2403.18105v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ChatGPT Role-play Dataset: Analysis of User Motives and Model\n  Naturalness", "abstract": "Recent advances in interactive large language models like ChatGPT have\nrevolutionized various domains; however, their behavior in natural and\nrole-play conversation settings remains underexplored. In our study, we address\nthis gap by deeply investigating how ChatGPT behaves during conversations in\ndifferent settings by analyzing its interactions in both a normal way and a\nrole-play setting. We introduce a novel dataset of broad range of human-AI\nconversations annotated with user motives and model naturalness to examine (i)\nhow humans engage with the conversational AI model, and (ii) how natural are AI\nmodel responses. Our study highlights the diversity of user motives when\ninteracting with ChatGPT and variable AI naturalness, showing not only the\nnuanced dynamics of natural conversations between humans and AI, but also\nproviding new avenues for improving the effectiveness of human-AI\ncommunication.", "published": "2024-03-26 22:01:13", "link": "http://arxiv.org/abs/2403.18121v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Juru: Legal Brazilian Large Language Model from Reputable Sources", "abstract": "The high computational cost associated with pretraining large language models\nlimits their research. Two strategies have emerged to address this issue:\ndomain specialization and pretraining with high-quality data. To explore these\nstrategies, we specialized the Sabi\\'a-2 Small model with 1.9 billion unique\ntokens from reputable Brazilian legal sources and conducted few-shot\nevaluations on legal and general knowledge exams. Our model, Juru, demonstrates\nthe benefits of domain specialization with a reduced amount of pretraining\ndata. However, this specialization comes at the expense of degrading\nperformance in other knowledge areas within the same language. This study\ncontributes to the growing body of scientific evidence showing that pretraining\ndata selection may enhance the performance of large language models, enabling\nthe exploration of these models at a lower cost.", "published": "2024-03-26 22:54:12", "link": "http://arxiv.org/abs/2403.18140v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Language Models Produce Responses Perceived to be Empathic", "abstract": "Large Language Models (LLMs) have demonstrated surprising performance on many\ntasks, including writing supportive messages that display empathy. Here, we had\nthese models generate empathic messages in response to posts describing common\nlife experiences, such as workplace situations, parenting, relationships, and\nother anxiety- and anger-eliciting situations. Across two studies (N=192, 202),\nwe showed human raters a variety of responses written by several models (GPT4\nTurbo, Llama2, and Mistral), and had people rate these responses on how\nempathic they seemed to be. We found that LLM-generated responses were\nconsistently rated as more empathic than human-written responses. Linguistic\nanalyses also show that these models write in distinct, predictable ``styles\",\nin terms of their use of punctuation, emojis, and certain words. These results\nhighlight the potential of using LLMs to enhance human peer support in contexts\nwhere empathy is important.", "published": "2024-03-26 23:14:34", "link": "http://arxiv.org/abs/2403.18148v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "NJUST-KMG at TRAC-2024 Tasks 1 and 2: Offline Harm Potential\n  Identification", "abstract": "This report provide a detailed description of the method that we proposed in\nthe TRAC-2024 Offline Harm Potential dentification which encloses two\nsub-tasks. The investigation utilized a rich dataset comprised of social media\ncomments in several Indian languages, annotated with precision by expert judges\nto capture the nuanced implications for offline context harm. The objective\nassigned to the participants was to design algorithms capable of accurately\nassessing the likelihood of harm in given situations and identifying the most\nlikely target(s) of offline harm. Our approach ranked second in two separate\ntracks, with F1 values of 0.73 and 0.96 respectively. Our method principally\ninvolved selecting pretrained models for finetuning, incorporating contrastive\nlearning techniques, and culminating in an ensemble approach for the test set.", "published": "2024-03-26 14:09:49", "link": "http://arxiv.org/abs/2403.19713v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Leveraging Large Language Models in Human-Robot Interaction: A Critical\n  Analysis of Potential and Pitfalls", "abstract": "The emergence of large language models (LLM) and, consequently, vision\nlanguage models (VLM) has ignited new imaginations among robotics researchers.\nAt this point, the range of applications to which LLM and VLM can be applied in\nhuman-robot interaction (HRI), particularly socially assistive robots (SARs),\nis unchartered territory. However, LLM and VLM present unprecedented\nopportunities and challenges for SAR integration. We aim to illuminate the\nopportunities and challenges when roboticists deploy LLM and VLM in SARs.\nFirst, we conducted a meta-study of more than 250 papers exploring 1) major\nrobots in HRI research and 2) significant applications of SARs, emphasizing\neducation, healthcare, and entertainment while addressing 3) societal norms and\nissues like trust, bias, and ethics that the robot developers must address.\nThen, we identified 4) critical components of a robot that LLM or VLM can\nreplace while addressing the 5) benefits of integrating LLM into robot designs\nand the 6) risks involved. Finally, we outline a pathway for the responsible\nand effective adoption of LLM or VLM into SARs, and we close our discussion by\noffering caution regarding this deployment.", "published": "2024-03-26 15:36:40", "link": "http://arxiv.org/abs/2405.00693v2", "categories": ["cs.RO", "cs.CL"], "primary_category": "cs.RO"}
{"title": "HILL: Hierarchy-aware Information Lossless Contrastive Learning for\n  Hierarchical Text Classification", "abstract": "Existing self-supervised methods in natural language processing (NLP),\nespecially hierarchical text classification (HTC), mainly focus on\nself-supervised contrastive learning, extremely relying on human-designed\naugmentation rules to generate contrastive samples, which can potentially\ncorrupt or distort the original information. In this paper, we tend to\ninvestigate the feasibility of a contrastive learning scheme in which the\nsemantic and syntactic information inherent in the input sample is adequately\nreserved in the contrastive samples and fused during the learning process.\nSpecifically, we propose an information lossless contrastive learning strategy\nfor HTC, namely \\textbf{H}ierarchy-aware \\textbf{I}nformation \\textbf{L}ossless\ncontrastive \\textbf{L}earning (HILL), which consists of a text encoder\nrepresenting the input document, and a structure encoder directly generating\nthe positive sample. The structure encoder takes the document embedding as\ninput, extracts the essential syntactic information inherent in the label\nhierarchy with the principle of structural entropy minimization, and injects\nthe syntactic information into the text representation via hierarchical\nrepresentation learning. Experiments on three common datasets are conducted to\nverify the superiority of HILL.", "published": "2024-03-26 01:29:17", "link": "http://arxiv.org/abs/2403.17307v1", "categories": ["cs.CL", "cs.IT", "math.IT"], "primary_category": "cs.CL"}
{"title": "Neural Multimodal Topic Modeling: A Comprehensive Evaluation", "abstract": "Neural topic models can successfully find coherent and diverse topics in\ntextual data. However, they are limited in dealing with multimodal datasets\n(e.g., images and text). This paper presents the first systematic and\ncomprehensive evaluation of multimodal topic modeling of documents containing\nboth text and images. In the process, we propose two novel topic modeling\nsolutions and two novel evaluation metrics. Overall, our evaluation on an\nunprecedented rich and diverse collection of datasets indicates that both of\nour models generate coherent and diverse topics. Nevertheless, the extent to\nwhich one method outperforms the other depends on the metrics and dataset\ncombinations, which suggests further exploration of hybrid solutions in the\nfuture. Notably, our succinct human evaluation aligns with the outcomes\ndetermined by our proposed metrics. This alignment not only reinforces the\ncredibility of our metrics but also highlights the potential for their\napplication in guiding future multimodal topic modeling endeavors.", "published": "2024-03-26 01:29:46", "link": "http://arxiv.org/abs/2403.17308v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Residual-based Language Models are Free Boosters for Biomedical Imaging", "abstract": "In this study, we uncover the unexpected efficacy of residual-based large\nlanguage models (LLMs) as part of encoders for biomedical imaging tasks, a\ndomain traditionally devoid of language or textual data. The approach diverges\nfrom established methodologies by utilizing a frozen transformer block,\nextracted from pre-trained LLMs, as an innovative encoder layer for the direct\nprocessing of visual tokens. This strategy represents a significant departure\nfrom the standard multi-modal vision-language frameworks, which typically hinge\non language-driven prompts and inputs. We found that these LLMs could boost\nperformance across a spectrum of biomedical imaging applications, including\nboth 2D and 3D visual classification tasks, serving as plug-and-play boosters.\nMore interestingly, as a byproduct, we found that the proposed framework\nachieved superior performance, setting new state-of-the-art results on\nextensive, standardized datasets in MedMNIST-2D and 3D. Through this work, we\naim to open new avenues for employing LLMs in biomedical imaging and enriching\nthe understanding of their potential in this specialized domain.", "published": "2024-03-26 03:05:20", "link": "http://arxiv.org/abs/2403.17343v3", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity\n  Recognition", "abstract": "In this work, we revisit the problem of semi-supervised named entity\nrecognition (NER) focusing on extremely light supervision, consisting of a\nlexicon containing only 10 examples per class. We introduce ELLEN, a simple,\nfully modular, neuro-symbolic method that blends fine-tuned language models\nwith linguistic rules. These rules include insights such as ''One Sense Per\nDiscourse'', using a Masked Language Model as an unsupervised NER, leveraging\npart-of-speech tags to identify and eliminate unlabeled entities as false\nnegatives, and other intuitions about classifier confidence scores in local and\nglobal context. ELLEN achieves very strong performance on the CoNLL-2003\ndataset when using the minimal supervision from the lexicon above. It also\noutperforms most existing (and considerably more complex) semi-supervised NER\nmethods under the same supervision settings commonly used in the literature\n(i.e., 5% of the training data). Further, we evaluate our CoNLL-2003 model in a\nzero-shot scenario on WNUT-17 where we find that it outperforms GPT-3.5 and\nachieves comparable performance to GPT-4. In a zero-shot setting, ELLEN also\nachieves over 75% of the performance of a strong, fully supervised model\ntrained on gold data. Our code is available at:\nhttps://github.com/hriaz17/ELLEN.", "published": "2024-03-26 05:11:51", "link": "http://arxiv.org/abs/2403.17385v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Transcribing Bengali Text with Regional Dialects to IPA using District\n  Guided Tokens", "abstract": "Accurate transcription of Bengali text to the International Phonetic Alphabet\n(IPA) is a challenging task due to the complex phonology of the language and\ncontext-dependent sound changes. This challenge is even more for regional\nBengali dialects due to unavailability of standardized spelling conventions for\nthese dialects, presence of local and foreign words popular in those regions\nand phonological diversity across different regions. This paper presents an\napproach to this sequence-to-sequence problem by introducing the District\nGuided Tokens (DGT) technique on a new dataset spanning six districts of\nBangladesh. The key idea is to provide the model with explicit information\nabout the regional dialect or \"district\" of the input text before generating\nthe IPA transcription. This is achieved by prepending a district token to the\ninput sequence, effectively guiding the model to understand the unique phonetic\npatterns associated with each district. The DGT technique is applied to\nfine-tune several transformer-based models, on this new dataset. Experimental\nresults demonstrate the effectiveness of DGT, with the ByT5 model achieving\nsuperior performance over word-based models like mT5, BanglaT5, and umT5. This\nis attributed to ByT5's ability to handle a high percentage of\nout-of-vocabulary words in the test set. The proposed approach highlights the\nimportance of incorporating regional dialect information into ubiquitous\nnatural language processing systems for languages with diverse phonological\nvariations. The following work was a result of the \"Bhashamul\" challenge, which\nis dedicated to solving the problem of Bengali text with regional dialects to\nIPA transcription https://www.kaggle.com/competitions/regipa/. The training and\ninference notebooks are available through the competition link.", "published": "2024-03-26 05:55:21", "link": "http://arxiv.org/abs/2403.17407v3", "categories": ["cs.CL", "cs.AI", "cs.LG", "F.2.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "Incorporating Exponential Smoothing into MLP: A Simple but Effective\n  Sequence Model", "abstract": "Modeling long-range dependencies in sequential data is a crucial step in\nsequence learning. A recently developed model, the Structured State Space (S4),\ndemonstrated significant effectiveness in modeling long-range sequences.\nHowever, It is unclear whether the success of S4 can be attributed to its\nintricate parameterization and HiPPO initialization or simply due to State\nSpace Models (SSMs). To further investigate the potential of the deep SSMs, we\nstart with exponential smoothing (ETS), a simple SSM, and propose a stacked\narchitecture by directly incorporating it into an element-wise MLP. We augment\nsimple ETS with additional parameters and complex field to reduce the inductive\nbias. Despite increasing less than 1\\% of parameters of element-wise MLP, our\nmodels achieve comparable results to S4 on the LRA benchmark.", "published": "2024-03-26 07:23:46", "link": "http://arxiv.org/abs/2403.17445v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Towards a Zero-Data, Controllable, Adaptive Dialog System", "abstract": "Conversational Tree Search (V\\\"ath et al., 2023) is a recent approach to\ncontrollable dialog systems, where domain experts shape the behavior of a\nReinforcement Learning agent through a dialog tree. The agent learns to\nefficiently navigate this tree, while adapting to information needs, e.g.,\ndomain familiarity, of different users. However, the need for additional\ntraining data hinders deployment in new domains. To address this, we explore\napproaches to generate this data directly from dialog trees. We improve the\noriginal approach, and show that agents trained on synthetic data can achieve\ncomparable dialog success to models trained on human data, both when using a\ncommercial Large Language Model for generation, or when using a smaller\nopen-source model, running on a single GPU. We further demonstrate the\nscalability of our approach by collecting and testing on two new datasets:\nONBOARD, a new domain helping foreign residents moving to a new city, and the\nmedical domain DIAGNOSE, a subset of Wikipedia articles related to scalp and\nhead symptoms. Finally, we perform human testing, where no statistically\nsignificant differences were found in either objective or subjective measures\nbetween models trained on human and generated data.", "published": "2024-03-26 10:45:11", "link": "http://arxiv.org/abs/2403.17582v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Coimagining the Future of Voice Assistants with Cultural Sensitivity", "abstract": "Voice assistants (VAs) are becoming a feature of our everyday life. Yet, the\nuser experience (UX) is often limited, leading to underuse, disengagement, and\nabandonment. Co-designing interactions for VAs with potential end-users can be\nuseful. Crowdsourcing this process online and anonymously may add value.\nHowever, most work has been done in the English-speaking West on dialogue data\nsets. We must be sensitive to cultural differences in language, social\ninteractions, and attitudes towards technology. Our aims were to explore the\nvalue of co-designing VAs in the non-Western context of Japan and demonstrate\nthe necessity of cultural sensitivity. We conducted an online elicitation study\n(N = 135) where Americans (n = 64) and Japanese people (n = 71) imagined\ndialogues (N = 282) and activities (N = 73) with future VAs. We discuss the\nimplications for coimagining interactions with future VAs, offer design\nguidelines for the Japanese and English-speaking US contexts, and suggest\nopportunities for cultural plurality in VA design and scholarship.", "published": "2024-03-26 11:09:58", "link": "http://arxiv.org/abs/2403.17599v1", "categories": ["cs.HC", "cs.CL", "cs.CY"], "primary_category": "cs.HC"}
{"title": "FastPerson: Enhancing Video Learning through Effective Video\n  Summarization that Preserves Linguistic and Visual Contexts", "abstract": "Quickly understanding lengthy lecture videos is essential for learners with\nlimited time and interest in various topics to improve their learning\nefficiency. To this end, video summarization has been actively researched to\nenable users to view only important scenes from a video. However, these studies\nfocus on either the visual or audio information of a video and extract\nimportant segments in the video. Therefore, there is a risk of missing\nimportant information when both the teacher's speech and visual information on\nthe blackboard or slides are important, such as in a lecture video. To tackle\nthis issue, we propose FastPerson, a video summarization approach that\nconsiders both the visual and auditory information in lecture videos.\nFastPerson creates summary videos by utilizing audio transcriptions along with\non-screen images and text, minimizing the risk of overlooking crucial\ninformation for learners. Further, it provides a feature that allows learners\nto switch between the summary and original videos for each chapter of the\nvideo, enabling them to adjust the pace of learning based on their interests\nand level of understanding. We conducted an evaluation with 40 participants to\nassess the effectiveness of our method and confirmed that it reduced viewing\ntime by 53\\% at the same level of comprehension as that when using traditional\nvideo playback methods.", "published": "2024-03-26 14:16:56", "link": "http://arxiv.org/abs/2403.17727v1", "categories": ["cs.CV", "cs.CL", "cs.HC", "cs.MM"], "primary_category": "cs.CV"}
{"title": "SciNews: From Scholarly Complexities to Public Narratives -- A Dataset\n  for Scientific News Report Generation", "abstract": "Scientific news reports serve as a bridge, adeptly translating complex\nresearch articles into reports that resonate with the broader public. The\nautomated generation of such narratives enhances the accessibility of scholarly\ninsights. In this paper, we present a new corpus to facilitate this paradigm\ndevelopment. Our corpus comprises a parallel compilation of academic\npublications and their corresponding scientific news reports across nine\ndisciplines. To demonstrate the utility and reliability of our dataset, we\nconduct an extensive analysis, highlighting the divergences in readability and\nbrevity between scientific news narratives and academic manuscripts. We\nbenchmark our dataset employing state-of-the-art text generation models. The\nevaluation process involves both automatic and human evaluation, which lays the\ngroundwork for future explorations into the automated generation of scientific\nnews reports. The dataset and code related to this work are available at\nhttps://dongqi.me/projects/SciNews.", "published": "2024-03-26 14:54:48", "link": "http://arxiv.org/abs/2403.17768v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Unreasonable Ineffectiveness of the Deeper Layers", "abstract": "How is knowledge stored in an LLM's weights? We study this via layer pruning:\nif removing a certain layer does not affect model performance in common\nquestion-answering benchmarks, then the weights in that layer are not necessary\nfor storing the knowledge needed to answer those questions. To find these\nunnecessary parameters, we identify the optimal block of layers to prune by\nconsidering similarity across layers; then, to \"heal\" the damage, we perform a\nsmall amount of finetuning. Surprisingly, with this method we find minimal\ndegradation of performance until after a large fraction (up to half) of the\nlayers are removed for some common open-weight models. From a scientific\nperspective, the robustness of these LLMs to the deletion of layers implies\neither that current pretraining methods are not properly leveraging the\nparameters in the deeper layers of the network or that the shallow layers play\na critical role in storing knowledge. For our study, we use parameter-efficient\nfinetuning (PEFT) methods, specifically quantization and Low Rank Adapters\n(QLoRA), such that each of our experiments can be performed on a single 40GB\nA100 GPU.", "published": "2024-03-26 17:20:04", "link": "http://arxiv.org/abs/2403.17887v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language\n  Model Fine-Tuning", "abstract": "The machine learning community has witnessed impressive advancements since\nlarge language models (LLMs) first appeared. Yet, their massive memory\nconsumption has become a significant roadblock to large-scale training. For\ninstance, a 7B model typically requires at least 60 GB of GPU memory with full\nparameter training, which presents challenges for researchers without access to\nhigh-resource environments. Parameter Efficient Fine-Tuning techniques such as\nLow-Rank Adaptation (LoRA) have been proposed to alleviate this problem.\nHowever, in most large-scale fine-tuning settings, their performance does not\nreach the level of full parameter training because they confine the parameter\nsearch to a low-rank subspace. Attempting to complement this deficiency, we\ninvestigate the layerwise properties of LoRA on fine-tuning tasks and observe\nan unexpected but consistent skewness of weight norms across different layers.\nUtilizing this key observation, a surprisingly simple training strategy is\ndiscovered, which outperforms both LoRA and full parameter training in a wide\nrange of settings with memory costs as low as LoRA. We name it Layerwise\nImportance Sampled AdamW (LISA), a promising alternative for LoRA, which\napplies the idea of importance sampling to different layers in LLMs and\nrandomly freezes most middle layers during optimization. Experimental results\nshow that with similar or less GPU memory consumption, LISA surpasses LoRA or\neven full parameter tuning in downstream fine-tuning tasks, where LISA\nconsistently outperforms LoRA by over 10%-35% in terms of MT-Bench score while\nachieving on-par or better performance in MMLU, AGIEval and WinoGrande. On\nlarge models, specifically LLaMA-2-70B, LISA surpasses LoRA on MT-Bench, GSM8K,\nand PubMedQA, demonstrating its effectiveness across different domains.", "published": "2024-03-26 17:55:02", "link": "http://arxiv.org/abs/2403.17919v4", "categories": ["cs.LG", "cs.AI", "cs.CL", "math.OC"], "primary_category": "cs.LG"}
{"title": "Improving Pre-trained Language Model Sensitivity via Mask Specific\n  losses: A case study on Biomedical NER", "abstract": "Adapting language models (LMs) to novel domains is often achieved through\nfine-tuning a pre-trained LM (PLM) on domain-specific data. Fine-tuning\nintroduces new knowledge into an LM, enabling it to comprehend and efficiently\nperform a target domain task. Fine-tuning can however be inadvertently\ninsensitive if it ignores the wide array of disparities (e.g in word meaning)\nbetween source and target domains. For instance, words such as chronic and\npressure may be treated lightly in social conversations, however, clinically,\nthese words are usually an expression of concern. To address insensitive\nfine-tuning, we propose Mask Specific Language Modeling (MSLM), an approach\nthat efficiently acquires target domain knowledge by appropriately weighting\nthe importance of domain-specific terms (DS-terms) during fine-tuning. MSLM\njointly masks DS-terms and generic words, then learns mask-specific losses by\nensuring LMs incur larger penalties for inaccurately predicting DS-terms\ncompared to generic words. Results of our analysis show that MSLM improves LMs\nsensitivity and detection of DS-terms. We empirically show that an optimal\nmasking rate not only depends on the LM, but also on the dataset and the length\nof sequences. Our proposed masking strategy outperforms advanced masking\nstrategies such as span- and PMI-based masking.", "published": "2024-03-26 18:23:16", "link": "http://arxiv.org/abs/2403.18025v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Don't Trust: Verify -- Grounding LLM Quantitative Reasoning with\n  Autoformalization", "abstract": "Large language models (LLM), such as Google's Minerva and OpenAI's GPT\nfamilies, are becoming increasingly capable of solving mathematical\nquantitative reasoning problems. However, they still make unjustified logical\nand computational errors in their reasoning steps and answers. In this paper,\nwe leverage the fact that if the training corpus of LLMs contained sufficiently\nmany examples of formal mathematics (e.g. in Isabelle, a formal theorem proving\nenvironment), they can be prompted to translate i.e. autoformalize informal\nmathematical statements into formal Isabelle code -- which can be verified\nautomatically for internal consistency. This provides a mechanism to\nautomatically reject solutions whose formalized versions are inconsistent\nwithin themselves or with the formalized problem statement. We evaluate our\nmethod on GSM8K, MATH and MultiArith datasets and demonstrate that our approach\nprovides a consistently better heuristic than vanilla majority voting -- the\npreviously best method to identify correct answers, by more than 12% on GSM8K.\nIn our experiments it improves results consistently across all datasets and LLM\nmodel sizes. The code can be found at https://github.com/jinpz/dtv.", "published": "2024-03-26 22:01:13", "link": "http://arxiv.org/abs/2403.18120v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Oh! We Freeze: Improving Quantized Knowledge Distillation via Signal\n  Propagation Analysis for Large Language Models", "abstract": "Large generative models such as large language models (LLMs) and diffusion\nmodels have revolutionized the fields of NLP and computer vision respectively.\nHowever, their slow inference, high computation and memory requirement makes it\nchallenging to deploy them on edge devices. In this study, we propose a\nlight-weight quantization aware fine tuning technique using knowledge\ndistillation (KD-QAT) to improve the performance of 4-bit weight quantized LLMs\nusing commonly available datasets to realize a popular language use case, on\ndevice chat applications. To improve this paradigm of finetuning, as main\ncontributions, we provide insights into stability of KD-QAT by empirically\nstudying the gradient propagation during training to better understand the\nvulnerabilities of KD-QAT based approaches to low-bit quantization errors.\nBased on our insights, we propose ov-freeze, a simple technique to stabilize\nthe KD-QAT process. Finally, we experiment with the popular 7B LLaMAv2-Chat\nmodel at 4-bit quantization level and demonstrate that ov-freeze results in\nnear floating point precision performance, i.e., less than 0.7% loss of\naccuracy on Commonsense Reasoning benchmarks.", "published": "2024-03-26 23:51:44", "link": "http://arxiv.org/abs/2403.18159v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Targeted Visualization of the Backbone of Encoder LLMs", "abstract": "Attention based Large Language Models (LLMs) are the state-of-the-art in\nnatural language processing (NLP). The two most common architectures are\nencoders such as BERT, and decoders like the GPT models. Despite the success of\nencoder models, on which we focus in this work, they also bear several risks,\nincluding issues with bias or their susceptibility for adversarial attacks,\nsignifying the necessity for explainable AI to detect such issues. While there\ndoes exist various local explainability methods focusing on the prediction of\nsingle inputs, global methods based on dimensionality reduction for\nclassification inspection, which have emerged in other domains and that go\nfurther than just using t-SNE in the embedding space, are not widely spread in\nNLP.\n  To reduce this gap, we investigate the application of DeepView, a method for\nvisualizing a part of the decision function together with a data set in two\ndimensions, to the NLP domain. While in previous work, DeepView has been used\nto inspect deep image classification models, we demonstrate how to apply it to\nBERT-based NLP classifiers and investigate its usability in this domain,\nincluding settings with adversarially perturbed input samples and pre-trained,\nfine-tuned, and multi-task models.", "published": "2024-03-26 12:51:02", "link": "http://arxiv.org/abs/2403.18872v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Decoding excellence: Mapping the demand for psychological traits of\n  operations and supply chain professionals through text mining", "abstract": "The current study proposes an innovative methodology for the profiling of\npsychological traits of Operations Management (OM) and Supply Chain Management\n(SCM) professionals. We use innovative methods and tools of text mining and\nsocial network analysis to map the demand for relevant skills from a set of job\ndescriptions, with a focus on psychological characteristics. The proposed\napproach aims to evaluate the market demand for specific traits by combining\nrelevant psychological constructs, text mining techniques, and an innovative\nmeasure, namely, the Semantic Brand Score. We apply the proposed methodology to\na dataset of job descriptions for OM and SCM professionals, with the objective\nof providing a mapping of their relevant required skills, including\npsychological characteristics. In addition, the analysis is then detailed by\nconsidering the region of the organization that issues the job description, its\norganizational size, and the seniority level of the open position in order to\nunderstand their nuances. Finally, topic modeling is used to examine key\ncomponents and their relative significance in job descriptions. By employing a\nnovel methodology and considering contextual factors, we provide an innovative\nunderstanding of the attitudinal traits that differentiate professionals. This\nresearch contributes to talent management, recruitment practices, and\nprofessional development initiatives, since it provides new figures and\nperspectives to improve the effectiveness and success of Operations Management\nand Supply Chain Management professionals.", "published": "2024-03-26 09:51:43", "link": "http://arxiv.org/abs/2403.17546v1", "categories": ["cs.CL", "cs.SI", "econ.GN", "physics.soc-ph", "q-fin.EC", "I.2.7; J.4; H.4.0"], "primary_category": "cs.CL"}
{"title": "Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot\n  Navigation", "abstract": "Recent open-vocabulary robot mapping methods enrich dense geometric maps with\npre-trained visual-language features. While these maps allow for the prediction\nof point-wise saliency maps when queried for a certain language concept,\nlarge-scale environments and abstract queries beyond the object level still\npose a considerable hurdle, ultimately limiting language-grounded robotic\nnavigation. In this work, we present HOV-SG, a hierarchical open-vocabulary 3D\nscene graph mapping approach for language-grounded robot navigation. Leveraging\nopen-vocabulary vision foundation models, we first obtain state-of-the-art\nopen-vocabulary segment-level maps in 3D and subsequently construct a 3D scene\ngraph hierarchy consisting of floor, room, and object concepts, each enriched\nwith open-vocabulary features. Our approach is able to represent multi-story\nbuildings and allows robotic traversal of those using a cross-floor Voronoi\ngraph. HOV-SG is evaluated on three distinct datasets and surpasses previous\nbaselines in open-vocabulary semantic accuracy on the object, room, and floor\nlevel while producing a 75% reduction in representation size compared to dense\nopen-vocabulary maps. In order to prove the efficacy and generalization\ncapabilities of HOV-SG, we showcase successful long-horizon\nlanguage-conditioned robot navigation within real-world multi-storage\nenvironments. We provide code and trial video data at http://hovsg.github.io/.", "published": "2024-03-26 16:36:43", "link": "http://arxiv.org/abs/2403.17846v2", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Heracles: A Hybrid SSM-Transformer Model for High-Resolution Image and\n  Time-Series Analysis", "abstract": "Transformers have revolutionized image modeling tasks with adaptations like\nDeIT, Swin, SVT, Biformer, STVit, and FDVIT. However, these models often face\nchallenges with inductive bias and high quadratic complexity, making them less\nefficient for high-resolution images. State space models (SSMs) such as Mamba,\nV-Mamba, ViM, and SiMBA offer an alternative to handle high resolution images\nin computer vision tasks. These SSMs encounter two major issues. First, they\nbecome unstable when scaled to large network sizes. Second, although they\nefficiently capture global information in images, they inherently struggle with\nhandling local information. To address these challenges, we introduce Heracles,\na novel SSM that integrates a local SSM, a global SSM, and an attention-based\ntoken interaction module. Heracles leverages a Hartely kernel-based state space\nmodel for global image information, a localized convolutional network for local\ndetails, and attention mechanisms in deeper layers for token interactions. Our\nextensive experiments demonstrate that Heracles-C-small achieves\nstate-of-the-art performance on the ImageNet dataset with 84.5\\% top-1\naccuracy. Heracles-C-Large and Heracles-C-Huge further improve accuracy to\n85.9\\% and 86.4\\%, respectively. Additionally, Heracles excels in transfer\nlearning tasks on datasets such as CIFAR-10, CIFAR-100, Oxford Flowers, and\nStanford Cars, and in instance segmentation on the MSCOCO dataset. Heracles\nalso proves its versatility by achieving state-of-the-art results on seven\ntime-series datasets, showcasing its ability to generalize across domains with\nspectral data, capturing both local and global information. The project page is\navailable at this link.\\url{https://github.com/badripatro/heracles}", "published": "2024-03-26 19:29:21", "link": "http://arxiv.org/abs/2403.18063v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Low-Latency Neural Speech Phase Prediction based on Parallel Estimation\n  Architecture and Anti-Wrapping Losses for Speech Generation Tasks", "abstract": "This paper presents a novel neural speech phase prediction model which\npredicts wrapped phase spectra directly from amplitude spectra. The proposed\nmodel is a cascade of a residual convolutional network and a parallel\nestimation architecture. The parallel estimation architecture is a core module\nfor direct wrapped phase prediction. This architecture consists of two parallel\nlinear convolutional layers and a phase calculation formula, imitating the\nprocess of calculating the phase spectra from the real and imaginary parts of\ncomplex spectra and strictly restricting the predicted phase values to the\nprincipal value interval. To avoid the error expansion issue caused by phase\nwrapping, we design anti-wrapping training losses defined between the predicted\nwrapped phase spectra and natural ones by activating the instantaneous phase\nerror, group delay error and instantaneous angular frequency error using an\nanti-wrapping function. We mathematically demonstrate that the anti-wrapping\nfunction should possess three properties, namely parity, periodicity and\nmonotonicity. We also achieve low-latency streamable phase prediction by\ncombining causal convolutions and knowledge distillation training strategies.\nFor both analysis-synthesis and specific speech generation tasks, experimental\nresults show that our proposed neural speech phase prediction model outperforms\nthe iterative phase estimation algorithms and neural network-based phase\nprediction methods in terms of phase prediction precision, efficiency and\nrobustness. Compared with HiFi-GAN-based waveform reconstruction method, our\nproposed model also shows outstanding efficiency advantages while ensuring the\nquality of synthesized speech. To the best of our knowledge, we are the first\nto directly predict speech phase spectra from amplitude spectra only via neural\nnetworks.", "published": "2024-03-26 04:53:15", "link": "http://arxiv.org/abs/2403.17378v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Infrastructure-less Localization from Indoor Environmental Sounds Based\n  on Spectral Decomposition and Spatial Likelihood Model", "abstract": "Human and/or asset tracking using an attached sensor units helps understand\ntheir activities. Most common indoor localization methods for human tracking\ntechnologies require expensive infrastructures, deployment and maintenance. To\novercome this problem, environmental sounds have been used for\ninfrastructure-free localization. While they achieve room-level classification,\nthey suffer from two problems: low signal-to-noise-ratio (SNR) condition and\nnon-uniqueness of sound over the coverage area. A microphone localization\nmethod was proposed using supervised spectral decomposition and spatial\nlikelihood to solve these problems. The proposed method was evaluated with\nactual recordings in an experimental room with a size of 12 x 30 m. The results\nshowed that the proposed method with supervised NMF was robust under low-SNR\ncondition compared to a simple feature (mel frequency cepstrum coefficient:\nMFCC). Additionally, the proposed method could be easily integrated with prior\ndistribution, which is available from other Bayesian localizations. The\nproposed method can be used to evaluate the spatial likelihood from\nenvironmental sounds.", "published": "2024-03-26 05:41:39", "link": "http://arxiv.org/abs/2403.17402v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Correlation of Fr\u00e9chet Audio Distance With Human Perception of\n  Environmental Audio Is Embedding Dependant", "abstract": "This paper explores whether considering alternative domain-specific\nembeddings to calculate the Fr\\'echet Audio Distance (FAD) metric can help the\nFAD to correlate better with perceptual ratings of environmental sounds. We\nused embeddings from VGGish, PANNs, MS-CLAP, L-CLAP, and MERT, which are\ntailored for either music or environmental sound evaluation. The FAD scores\nwere calculated for sounds from the DCASE 2023 Task 7 dataset. Using perceptual\ndata from the same task, we find that PANNs-WGM-LogMel produces the best\ncorrelation between FAD scores and perceptual ratings of both audio quality and\nperceived fit with a Spearman correlation higher than 0.5. We also find that\nmusic-specific embeddings resulted in significantly lower results.\nInterestingly, VGGish, the embedding used for the original Fr\\'echet\ncalculation, yielded a correlation below 0.1. These results underscore the\ncritical importance of the choice of embedding for the FAD metric design.", "published": "2024-03-26 09:09:59", "link": "http://arxiv.org/abs/2403.17508v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speaker Distance Estimation in Enclosures from Single-Channel Audio", "abstract": "Distance estimation from audio plays a crucial role in various applications,\nsuch as acoustic scene analysis, sound source localization, and room modeling.\nMost studies predominantly center on employing a classification approach, where\ndistances are discretized into distinct categories, enabling smoother model\ntraining and achieving higher accuracy but imposing restrictions on the\nprecision of the obtained sound source position. Towards this direction, in\nthis paper we propose a novel approach for continuous distance estimation from\naudio signals using a convolutional recurrent neural network with an attention\nmodule. The attention mechanism enables the model to focus on relevant temporal\nand spectral features, enhancing its ability to capture fine-grained\ndistance-related information. To evaluate the effectiveness of our proposed\nmethod, we conduct extensive experiments using audio recordings in controlled\nenvironments with three levels of realism (synthetic room impulse response,\nmeasured response with convolved speech, and real recordings) on four datasets\n(our synthetic dataset, QMULTIMIT, VoiceHome-2, and STARSS23). Experimental\nresults show that the model achieves an absolute error of 0.11 meters in a\nnoiseless synthetic scenario. Moreover, the results showed an absolute error of\nabout 1.30 meters in the hybrid scenario. The algorithm's performance in the\nreal scenario, where unpredictable environmental factors and noise are\nprevalent, yields an absolute error of approximately 0.50 meters. For\nreproducible research purposes we make model, code, and synthetic datasets\navailable at https://github.com/michaelneri/audio-distance-estimation.", "published": "2024-03-26 09:16:21", "link": "http://arxiv.org/abs/2403.17514v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Detection of Deepfake Environmental Audio", "abstract": "With the ever-rising quality of deep generative models, it is increasingly\nimportant to be able to discern whether the audio data at hand have been\nrecorded or synthesized. Although the detection of fake speech signals has been\nstudied extensively, this is not the case for the detection of fake\nenvironmental audio.\n  We propose a simple and efficient pipeline for detecting fake environmental\nsounds based on the CLAP audio embedding. We evaluate this detector using audio\ndata from the 2023 DCASE challenge task on Foley sound synthesis.\n  Our experiments show that fake sounds generated by 44 state-of-the-art\nsynthesizers can be detected on average with 98% accuracy. We show that using\nan audio embedding learned on environmental audio is beneficial over a standard\nVGGish one as it provides a 10% increase in detection performance. Informal\nlistening to Incorrect Negative examples demonstrates audible features of fake\nsounds missed by the detector such as distortion and implausible background\nnoise.", "published": "2024-03-26 09:35:16", "link": "http://arxiv.org/abs/2403.17529v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Accuracy enhancement method for speech emotion recognition from\n  spectrogram using temporal frequency correlation and positional information\n  learning through knowledge transfer", "abstract": "In this paper, we propose a method to improve the accuracy of speech emotion\nrecognition (SER) by using vision transformer (ViT) to attend to the\ncorrelation of frequency (y-axis) with time (x-axis) in spectrogram and\ntransferring positional information between ViT through knowledge transfer. The\nproposed method has the following originality i) We use vertically segmented\npatches of log-Mel spectrogram to analyze the correlation of frequencies over\ntime. This type of patch allows us to correlate the most relevant frequencies\nfor a particular emotion with the time they were uttered. ii) We propose the\nuse of image coordinate encoding, an absolute positional encoding suitable for\nViT. By normalizing the x, y coordinates of the image to -1 to 1 and\nconcatenating them to the image, we can effectively provide valid absolute\npositional information for ViT. iii) Through feature map matching, the locality\nand location information of the teacher network is effectively transmitted to\nthe student network. Teacher network is a ViT that contains locality of\nconvolutional stem and absolute position information through image coordinate\nencoding, and student network is a structure that lacks positional encoding in\nthe basic ViT structure. In feature map matching stage, we train through the\nmean absolute error (L1 loss) to minimize the difference between the feature\nmaps of the two networks. To validate the proposed method, three emotion\ndatasets (SAVEE, EmoDB, and CREMA-D) consisting of speech were converted into\nlog-Mel spectrograms for comparison experiments. The experimental results show\nthat the proposed method significantly outperforms the state-of-the-art methods\nin terms of weighted accuracy while requiring significantly fewer floating\npoint operations (FLOPs). Overall, the proposed method offers an promising\nsolution for SER by providing improved efficiency and performance.", "published": "2024-03-26 02:21:36", "link": "http://arxiv.org/abs/2403.17327v1", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Learning to Visually Localize Sound Sources from Mixtures without Prior\n  Source Knowledge", "abstract": "The goal of the multi-sound source localization task is to localize sound\nsources from the mixture individually. While recent multi-sound source\nlocalization methods have shown improved performance, they face challenges due\nto their reliance on prior information about the number of objects to be\nseparated. In this paper, to overcome this limitation, we present a novel\nmulti-sound source localization method that can perform localization without\nprior knowledge of the number of sound sources. To achieve this goal, we\npropose an iterative object identification (IOI) module, which can recognize\nsound-making objects in an iterative manner. After finding the regions of\nsound-making objects, we devise object similarity-aware clustering (OSC) loss\nto guide the IOI module to effectively combine regions of the same object but\nalso distinguish between different objects and backgrounds. It enables our\nmethod to perform accurate localization of sound-making objects without any\nprior knowledge. Extensive experimental results on the MUSIC and VGGSound\nbenchmarks show the significant performance improvements of the proposed method\nover the existing methods for both single and multi-source. Our code is\navailable at: https://github.com/VisualAIKHU/NoPrior_MultiSSL", "published": "2024-03-26 06:27:50", "link": "http://arxiv.org/abs/2403.17420v1", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Deep functional multiple index models with an application to SER", "abstract": "Speech Emotion Recognition (SER) plays a crucial role in advancing\nhuman-computer interaction and speech processing capabilities. We introduce a\nnovel deep-learning architecture designed specifically for the functional data\nmodel known as the multiple-index functional model. Our key innovation lies in\nintegrating adaptive basis layers and an automated data transformation search\nwithin the deep learning framework. Simulations for this new model show good\nperformances. This allows us to extract features tailored for chunk-level SER,\nbased on Mel Frequency Cepstral Coefficients (MFCCs). We demonstrate the\neffectiveness of our approach on the benchmark IEMOCAP database, achieving good\nperformance compared to existing methods.", "published": "2024-03-26 10:10:56", "link": "http://arxiv.org/abs/2403.17562v1", "categories": ["cs.SD", "eess.AS", "stat.AP"], "primary_category": "cs.SD"}
{"title": "Synthetic training set generation using text-to-audio models for\n  environmental sound classification", "abstract": "In recent years, text-to-audio models have revolutionized the field of\nautomatic audio generation. This paper investigates their application in\ngenerating synthetic datasets for training data-driven models. Specifically,\nthis study analyzes the performance of two environmental sound classification\nsystems trained with data generated from text-to-audio models. We considered\nthree scenarios: a) augmenting the training dataset with data generated by\ntext-to-audio models; b) using a mixed training dataset combining real and\nsynthetic text-driven generated data; and c) using a training dataset composed\nentirely of synthetic audio. In all cases, the performance of the\nclassification models was tested on real data. Results indicate that\ntext-to-audio models are effective for dataset augmentation, with consistent\nperformance when replacing a subset of the recorded dataset. However, the\nperformance of the audio recognition models drops when relying entirely on\ngenerated audio.", "published": "2024-03-26 16:51:21", "link": "http://arxiv.org/abs/2403.17864v3", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
