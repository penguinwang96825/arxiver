{"title": "Diffusion Models for Non-autoregressive Text Generation: A Survey", "abstract": "Non-autoregressive (NAR) text generation has attracted much attention in the\nfield of natural language processing, which greatly reduces the inference\nlatency but has to sacrifice the generation accuracy. Recently, diffusion\nmodels, a class of latent variable generative models, have been introduced into\nNAR text generation, showing an improved text generation quality. In this\nsurvey, we review the recent progress in diffusion models for NAR text\ngeneration. As the background, we first present the general definition of\ndiffusion models and the text diffusion models, and then discuss their merits\nfor NAR generation. As the core content, we further introduce two mainstream\ndiffusion models in existing work of text diffusion, and review the key designs\nof the diffusion process. Moreover, we discuss the utilization of pre-trained\nlanguage models (PLMs) for text diffusion models and introduce optimization\ntechniques for text data. Finally, we discuss several promising directions and\nconclude this paper. Our survey aims to provide researchers with a systematic\nreference of related research on text diffusion models for NAR generation. We\npresent our collection of text diffusion models at\nhttps://github.com/RUCAIBox/Awesome-Text-Diffusion-Models.", "published": "2023-03-12 05:11:09", "link": "http://arxiv.org/abs/2303.06574v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MWE as WSD: Solving Multiword Expression Identification with Word Sense\n  Disambiguation", "abstract": "Recent approaches to word sense disambiguation (WSD) utilize encodings of the\nsense gloss (definition), in addition to the input context, to improve\nperformance. In this work we demonstrate that this approach can be adapted for\nuse in multiword expression (MWE) identification by training models which use\ngloss and context information to filter MWE candidates produced by a rule-based\nextraction pipeline. Our approach substantially improves precision,\noutperforming the state-of-the-art in MWE identification on the DiMSUM dataset\nby up to 1.9 F1 points and achieving competitive results on the PARSEME 1.1\nEnglish dataset. Our models also retain most of their WSD performance, showing\nthat a single model can be used for both tasks. Finally, building on similar\napproaches using Bi-encoders for WSD, we introduce a novel Poly-encoder\narchitecture which improves MWE identification performance.", "published": "2023-03-12 09:35:42", "link": "http://arxiv.org/abs/2303.06623v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fuzzy Alignments in Directed Acyclic Graph for Non-Autoregressive\n  Machine Translation", "abstract": "Non-autoregressive translation (NAT) reduces the decoding latency but suffers\nfrom performance degradation due to the multi-modality problem. Recently, the\nstructure of directed acyclic graph has achieved great success in NAT, which\ntackles the multi-modality problem by introducing dependency between vertices.\nHowever, training it with negative log-likelihood loss implicitly requires a\nstrict alignment between reference tokens and vertices, weakening its ability\nto handle multiple translation modalities. In this paper, we hold the view that\nall paths in the graph are fuzzily aligned with the reference sentence. We do\nnot require the exact alignment but train the model to maximize a fuzzy\nalignment score between the graph and reference, which takes captured\ntranslations in all modalities into account. Extensive experiments on major WMT\nbenchmarks show that our method substantially improves translation performance\nand increases prediction confidence, setting a new state of the art for NAT on\nthe raw training data.", "published": "2023-03-12 13:51:38", "link": "http://arxiv.org/abs/2303.06662v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Compressed Heterogeneous Graph for Abstractive Multi-Document\n  Summarization", "abstract": "Multi-document summarization (MDS) aims to generate a summary for a number of\nrelated documents. We propose HGSUM, an MDS model that extends an\nencoder-decoder architecture, to incorporate a heterogeneous graph to represent\ndifferent semantic units (e.g., words and sentences) of the documents. This\ncontrasts with existing MDS models which do not consider different edge types\nof graphs and as such do not capture the diversity of relationships in the\ndocuments. To preserve only key information and relationships of the documents\nin the heterogeneous graph, HGSUM uses graph pooling to compress the input\ngraph. And to guide HGSUM to learn compression, we introduce an additional\nobjective that maximizes the similarity between the compressed graph and the\ngraph constructed from the ground-truth summary during training. HGSUM is\ntrained end-to-end with graph similarity and standard cross-entropy objectives.\nExperimental results over MULTI-NEWS, WCEP-100, and ARXIV show that HGSUM\noutperforms state-of-the-art MDS models. The code for our model and experiments\nis available at: https://github.com/oaimli/HGSum.", "published": "2023-03-12 04:23:54", "link": "http://arxiv.org/abs/2303.06565v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LUKE-Graph: A Transformer-based Approach with Gated Relational Graph\n  Attention for Cloze-style Reading Comprehension", "abstract": "Incorporating prior knowledge can improve existing pre-training models in\ncloze-style machine reading and has become a new trend in recent studies.\nNotably, most of the existing models have integrated external knowledge graphs\n(KG) and transformer-based models, such as BERT into a unified data structure.\nHowever, selecting the most relevant ambiguous entities in KG and extracting\nthe best subgraph remains a challenge. In this paper, we propose the\nLUKE-Graph, a model that builds a heterogeneous graph based on the intuitive\nrelationships between entities in a document without using any external KG. We\nthen use a Relational Graph Attention (RGAT) network to fuse the graph's\nreasoning information and the contextual representation encoded by the\npre-trained LUKE model. In this way, we can take advantage of LUKE, to derive\nan entity-aware representation; and a graph model - to exploit relation-aware\nrepresentation. Moreover, we propose Gated-RGAT by augmenting RGAT with a\ngating mechanism that regulates the question information for the graph\nconvolution operation. This is very similar to human reasoning processing\nbecause they always choose the best entity candidate based on the question\ninformation. Experimental results demonstrate that the LUKE-Graph achieves\nstate-of-the-art performance on the ReCoRD dataset with commonsense reasoning.", "published": "2023-03-12 14:31:44", "link": "http://arxiv.org/abs/2303.06675v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A comprehensive evaluation of ChatGPT's zero-shot Text-to-SQL capability", "abstract": "This paper presents the first comprehensive analysis of ChatGPT's Text-to-SQL\nability. Given the recent emergence of large-scale conversational language\nmodel ChatGPT and its impressive capabilities in both conversational abilities\nand code generation, we sought to evaluate its Text-to-SQL performance. We\nconducted experiments on 12 benchmark datasets with different languages,\nsettings, or scenarios, and the results demonstrate that ChatGPT has strong\ntext-to-SQL abilities. Although there is still a gap from the current\nstate-of-the-art (SOTA) model performance, considering that the experiment was\nconducted in a zero-shot scenario, ChatGPT's performance is still impressive.\nNotably, in the ADVETA (RPL) scenario, the zero-shot ChatGPT even outperforms\nthe SOTA model that requires fine-tuning on the Spider dataset by 4.1\\%,\ndemonstrating its potential for use in practical applications. To support\nfurther research in related fields, we have made the data generated by ChatGPT\npublicly available at https://github.com/THU-BPM/chatgpt-sql.", "published": "2023-03-12 04:22:01", "link": "http://arxiv.org/abs/2303.13547v1", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Improving the Diproche CNL through Autoformalization via Large Language\n  Models", "abstract": "The Diproche system is an automated proof checker for texts written in a\ncontrolled fragment of German, designed for didactical applications in classes\nintroducing students to proofs for the first time. The first version of the\nsystem used a controlled natural language for which a Prolog formalization\nroutine was written. In this paper, we explore the possibility of prompting\nlarge language models for autoformalization in the context of Diproche, with\nencouraging first results.", "published": "2023-03-12 20:11:25", "link": "http://arxiv.org/abs/2303.17513v3", "categories": ["cs.CL", "cs.LO"], "primary_category": "cs.CL"}
{"title": "Towards General Purpose Medical AI: Continual Learning Medical\n  Foundation Model", "abstract": "Inevitable domain and task discrepancies in real-world scenarios can impair\nthe generalization performance of the pre-trained deep models for medical data.\nTherefore, we audaciously propose that we should build a general-purpose\nmedical AI system that can be seamlessly adapted to downstream domains/tasks.\nSince the domain/task adaption procedures usually involve additional labeling\nwork for the target data, designing a data-efficient adaption algorithm is\ndesired to save the cost of transferring the learned knowledge. Our recent work\nfound that vision-language models (VLMs) are efficient learners with\nextraordinary cross-domain ability. Therefore, in this work, we further explore\nthe possibility of leveraging pre-trained VLMs as medical foundation models for\nbuilding general-purpose medical AI, where we thoroughly investigate three\nmachine-learning paradigms, i.e., domain/task-specialized learning, joint\nlearning, and continual learning, for training the VLMs and evaluate their\ngeneralization performance on cross-domain and cross-task test sets. To\nalleviate the catastrophic forgetting during sequential training, we employ\nrehearsal learning and receive a sharp boost in terms of generalization\ncapability. In a nutshell, our empirical evidence suggests that continual\nlearning may be a practical and efficient learning paradigm for the medical\nfoundation model. And we hope researchers can use our empirical evidence as\nbasement to further explore the path toward medical foundation model.", "published": "2023-03-12 05:27:22", "link": "http://arxiv.org/abs/2303.06580v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Improving the Intent Classification accuracy in Noisy Environment", "abstract": "Intent classification is a fundamental task in the spoken language\nunderstanding field that has recently gained the attention of the scientific\ncommunity, mainly because of the feasibility of approaching it with end-to-end\nneural models. In this way, avoiding using intermediate steps, i.e. automatic\nspeech recognition, is possible, thus the propagation of errors due to\nbackground noise, spontaneous speech, speaking styles of users, etc. Towards\nthe development of solutions applicable in real scenarios, it is interesting to\ninvestigate how environmental noise and related noise reduction techniques to\naddress the intent classification task with end-to-end neural models. In this\npaper, we experiment with a noisy version of the fluent speech command data\nset, combining the intent classifier with a time-domain speech enhancement\nsolution based on Wave-U-Net and considering different training strategies.\nExperimental results reveal that, for this task, the use of speech enhancement\ngreatly improves the classification accuracy in noisy conditions, in particular\nwhen the classification model is trained on enhanced signals.", "published": "2023-03-12 06:11:44", "link": "http://arxiv.org/abs/2303.06585v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "ICASSP 2023 Speech Signal Improvement Challenge", "abstract": "The ICASSP 2023 Speech Signal Improvement Challenge is intended to stimulate\nresearch in the area of improving the speech signal quality in communication\nsystems. The speech signal quality can be measured with SIG in ITU-T P.835 and\nis still a top issue in audio communication and conferencing systems. For\nexample, in the ICASSP 2022 Deep Noise Suppression challenge, the improvement\nin the background and overall quality is impressive, but the improvement in the\nspeech signal is not statistically significant. To improve the speech signal\nthe following speech impairment areas must be addressed: coloration,\ndiscontinuity, loudness, reverberation, and noise. A training and test set was\nprovided for the challenge, and the winners were determined using an extended\ncrowdsourced implementation of ITU-T P.804's listening phase. The results show\nsignificant improvement was made across all measured dimensions of speech\nquality.", "published": "2023-03-12 04:29:19", "link": "http://arxiv.org/abs/2303.06566v4", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Fine-tuning Strategies for Faster Inference using Speech Self-Supervised\n  Models: A Comparative Study", "abstract": "Self-supervised learning (SSL) has allowed substantial progress in Automatic\nSpeech Recognition (ASR) performance in low-resource settings. In this context,\nit has been demonstrated that larger self-supervised feature extractors are\ncrucial for achieving lower downstream ASR error rates. Thus, better\nperformance might be sanctioned with longer inferences. This article explores\ndifferent approaches that may be deployed during the fine-tuning to reduce the\ncomputations needed in the SSL encoder, leading to faster inferences. We adapt\na number of existing techniques to common ASR settings and benchmark them,\ndisplaying performance drops and gains in inference times. Interestingly, we\nfound that given enough downstream data, a simple downsampling of the input\nsequences outperforms the other methods with both low performance drops and\nhigh computational savings, reducing computations by 61.3% with an WER increase\nof only 0.81. Finally, we analyze the robustness of the comparison to changes\nin dataset conditions, revealing sensitivity to dataset size.", "published": "2023-03-12 19:52:34", "link": "http://arxiv.org/abs/2303.06740v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Heart Murmur and Abnormal PCG Detection via Wavelet Scattering Transform\n  & a 1D-CNN", "abstract": "Heart murmurs provide valuable information about mechanical activity of the\nheart, which aids in diagnosis of various heart valve diseases. This work does\nautomatic and accurate heart murmur detection from phonocardiogram (PCG)\nrecordings. Two public PCG datasets (CirCor Digiscope 2022 dataset and PCG 2016\ndataset) from Physionet online database are utilized to train and test three\ncustom neural networks (NN): a 1D convolutional neural network (CNN), a long\nshort-term memory (LSTM) recurrent neural network (RNN), and a convolutional\nRNN (C-RNN). We first do pre-processing which includes the following key steps:\ndenoising, segmentation, re-labeling of noise-only segments, data\nnormalization, and time-frequency analysis of the PCG segments using wavelet\nscattering transform. We then conduct four experiments, first three (E1-E3)\nusing PCG 2022 dataset, and fourth (E4) using PCG 2016 dataset. It turns out\nthat our custom 1D-CNN outperforms other two NNs (LSTM-RNN and C-RNN). Further,\nour 1D-CNN model outperforms the related work in terms of accuracy, weighted\naccuracy, F1-score and AUROC, for experiment E3 (that utilizes the cleaned and\nre-labeled PCG 2022 dataset). As for experiment E1 (that utilizes the original\nPCG 2022 dataset), our model performs quite close to the related work in terms\nof weighted accuracy and F1-score.", "published": "2023-03-12 13:49:45", "link": "http://arxiv.org/abs/2303.11423v2", "categories": ["eess.SP", "cs.LG", "eess.AS"], "primary_category": "eess.SP"}
