{"title": "Comparing Fifty Natural Languages and Twelve Genetic Languages Using\n  Word Embedding Language Divergence (WELD) as a Quantitative Measure of\n  Language Distance", "abstract": "We introduce a new measure of distance between languages based on word\nembedding, called word embedding language divergence (WELD). WELD is defined as\ndivergence between unified similarity distribution of words between languages.\nUsing such a measure, we perform language comparison for fifty natural\nlanguages and twelve genetic languages. Our natural language dataset is a\ncollection of sentence-aligned parallel corpora from bible translations for\nfifty languages spanning a variety of language families. Although we use\nparallel corpora, which guarantees having the same content in all languages,\ninterestingly in many cases languages within the same family cluster together.\nIn addition to natural languages, we perform language comparison for the coding\nregions in the genomes of 12 different organisms (4 plants, 6 animals, and two\nhuman subjects). Our result confirms a significant high-level difference in the\ngenetic language model of humans/animals versus plants. The proposed method is\na step toward defining a quantitative measure of similarity between languages,\nwith applications in languages classification, genre identification, dialect\nidentification, and evaluation of translations.", "published": "2016-04-28 19:10:47", "link": "http://arxiv.org/abs/1604.08561v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Word Ordering Without Syntax", "abstract": "Recent work on word ordering has argued that syntactic structure is\nimportant, or even required, for effectively recovering the order of a\nsentence. We find that, in fact, an n-gram language model with a simple\nheuristic gives strong results on this task. Furthermore, we show that a long\nshort-term memory (LSTM) language model is even more effective at recovering\norder, with our basic model outperforming a state-of-the-art syntactic model by\n11.5 BLEU points. Additional data and larger beams yield further gains, at the\nexpense of training and search time.", "published": "2016-04-28 22:09:49", "link": "http://arxiv.org/abs/1604.08633v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting \"Smart\" Spammers On Social Network: A Topic Model Approach", "abstract": "Spammer detection on social network is a challenging problem. The rigid\nanti-spam rules have resulted in emergence of \"smart\" spammers. They resemble\nlegitimate users who are difficult to identify. In this paper, we present a\nnovel spammer classification approach based on Latent Dirichlet\nAllocation(LDA), a topic model. Our approach extracts both the local and the\nglobal information of topic distribution patterns, which capture the essence of\nspamming. Tested on one benchmark dataset and one self-collected dataset, our\nproposed method outperforms other state-of-the-art methods in terms of averaged\nF1-score.", "published": "2016-04-28 16:36:35", "link": "http://arxiv.org/abs/1604.08504v2", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
