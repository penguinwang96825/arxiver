{"title": "AlloVera: A Multilingual Allophone Database", "abstract": "We introduce a new resource, AlloVera, which provides mappings from 218\nallophones to phonemes for 14 languages. Phonemes are contrastive phonological\nunits, and allophones are their various concrete realizations, which are\npredictable from phonological context. While phonemic representations are\nlanguage specific, phonetic representations (stated in terms of (allo)phones)\nare much closer to a universal (language-independent) transcription. AlloVera\nallows the training of speech recognition models that output phonetic\ntranscriptions in the International Phonetic Alphabet (IPA), regardless of the\ninput language. We show that a \"universal\" allophone model, Allosaurus, built\nwith AlloVera, outperforms \"universal\" phonemic models and language-specific\nmodels on a speech-transcription task. We explore the implications of this\ntechnology (and related technologies) for the documentation of endangered and\nminority languages. We further explore other applications for which AlloVera\nwill be suitable as it grows, including phonological typology.", "published": "2020-04-17 02:02:18", "link": "http://arxiv.org/abs/2004.08031v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enriching the Transformer with Linguistic Factors for Low-Resource\n  Machine Translation", "abstract": "Introducing factors, that is to say, word features such as linguistic\ninformation referring to the source tokens, is known to improve the results of\nneural machine translation systems in certain settings, typically in recurrent\narchitectures. This study proposes enhancing the current state-of-the-art\nneural machine translation architecture, the Transformer, so that it allows to\nintroduce external knowledge. In particular, our proposed modification, the\nFactored Transformer, uses linguistic factors that insert additional knowledge\ninto the machine translation system. Apart from using different kinds of\nfeatures, we study the effect of different architectural configurations.\nSpecifically, we analyze the performance of combining words and features at the\nembedding level or at the encoder level, and we experiment with two different\ncombination strategies. With the best-found configuration, we show improvements\nof 0.8 BLEU over the baseline Transformer in the IWSLT German-to-English task.\nMoreover, we experiment with the more challenging FLoRes English-to-Nepali\nbenchmark, which includes both extremely low-resourced and very distant\nlanguages, and obtain an improvement of 1.2 BLEU.", "published": "2020-04-17 03:40:13", "link": "http://arxiv.org/abs/2004.08053v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Dialogue-Based Relation Extraction", "abstract": "We present the first human-annotated dialogue-based relation extraction (RE)\ndataset DialogRE, aiming to support the prediction of relation(s) between two\narguments that appear in a dialogue. We further offer DialogRE as a platform\nfor studying cross-sentence RE as most facts span multiple sentences. We argue\nthat speaker-related information plays a critical role in the proposed task,\nbased on an analysis of similarities and differences between dialogue-based and\ntraditional RE tasks. Considering the timeliness of communication in a\ndialogue, we design a new metric to evaluate the performance of RE methods in a\nconversational setting and investigate the performance of several\nrepresentative RE methods on DialogRE. Experimental results demonstrate that a\nspeaker-aware extension on the best-performing model leads to gains in both the\nstandard and conversational evaluation settings. DialogRE is available at\nhttps://dataset.org/dialogre/.", "published": "2020-04-17 03:51:57", "link": "http://arxiv.org/abs/2004.08056v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Approaches for Data Driven Dependency Parsing in Sanskrit", "abstract": "Data-driven approaches for dependency parsing have been of great interest in\nNatural Language Processing for the past couple of decades. However, Sanskrit\nstill lacks a robust purely data-driven dependency parser, probably with an\nexception to Krishna (2019). This can primarily be attributed to the lack of\navailability of task-specific labelled data and the morphologically rich nature\nof the language. In this work, we evaluate four different data-driven machine\nlearning models, originally proposed for different languages, and compare their\nperformances on Sanskrit data. We experiment with 2 graph based and 2\ntransition based parsers. We compare the performance of each of the models in a\nlow-resource setting, with 1,500 sentences for training. Further, since our\nfocus is on the learning power of each of the models, we do not incorporate any\nSanskrit specific features explicitly into the models, and rather use the\ndefault settings in each of the paper for obtaining the feature functions. In\nthis work, we analyse the performance of the parsers using both an in-domain\nand an out-of-domain test dataset. We also investigate the impact of word\nordering in which the sentences are provided as input to these systems, by\nparsing verses and their corresponding prose order (anvaya) sentences.", "published": "2020-04-17 06:47:15", "link": "http://arxiv.org/abs/2004.08076v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fast and Accurate Deep Bidirectional Language Representations for\n  Unsupervised Learning", "abstract": "Even though BERT achieves successful performance improvements in various\nsupervised learning tasks, applying BERT for unsupervised tasks still holds a\nlimitation that it requires repetitive inference for computing contextual\nlanguage representations. To resolve the limitation, we propose a novel deep\nbidirectional language model called Transformer-based Text Autoencoder (T-TA).\nThe T-TA computes contextual language representations without repetition and\nhas benefits of the deep bidirectional architecture like BERT. In run-time\nexperiments on CPU environments, the proposed T-TA performs over six times\nfaster than the BERT-based model in the reranking task and twelve times faster\nin the semantic similarity task. Furthermore, the T-TA shows competitive or\neven better accuracies than those of BERT on the above tasks.", "published": "2020-04-17 07:43:38", "link": "http://arxiv.org/abs/2004.08097v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Probing Linguistic Features of Sentence-Level Representations in Neural\n  Relation Extraction", "abstract": "Despite the recent progress, little is known about the features captured by\nstate-of-the-art neural relation extraction (RE) models. Common methods encode\nthe source sentence, conditioned on the entity mentions, before classifying the\nrelation. However, the complexity of the task makes it difficult to understand\nhow encoder architecture and supporting linguistic knowledge affect the\nfeatures learned by the encoder. We introduce 14 probing tasks targeting\nlinguistic properties relevant to RE, and we use them to study representations\nlearned by more than 40 different encoder architecture and linguistic feature\ncombinations trained on two datasets, TACRED and SemEval 2010 Task 8. We find\nthat the bias induced by the architecture and the inclusion of linguistic\nfeatures are clearly expressed in the probing task performance. For example,\nadding contextualized word representations greatly increases performance on\nprobing tasks with a focus on named entity and part-of-speech information, and\nyields better results in RE. In contrast, entity masking improves RE, but\nconsiderably lowers performance on entity type related probing tasks.", "published": "2020-04-17 09:17:40", "link": "http://arxiv.org/abs/2004.08134v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Too Many Claims to Fact-Check: Prioritizing Political Claims Based on\n  Check-Worthiness", "abstract": "The massive amount of misinformation spreading on the Internet on a daily\nbasis has enormous negative impacts on societies. Therefore, we need automated\nsystems helping fact-checkers in the combat against misinformation. In this\npaper, we propose a model prioritizing the claims based on their\ncheck-worthiness. We use BERT model with additional features including\ndomain-specific controversial topics, word embeddings, and others. In our\nexperiments, we show that our proposed model outperforms all state-of-the-art\nmodels in both test collections of CLEF Check That! Lab in 2018 and 2019. We\nalso conduct a qualitative analysis to shed light-detecting check-worthy\nclaims. We suggest requesting rationales behind judgments are needed to\nunderstand subjective nature of the task and problematic labels.", "published": "2020-04-17 10:55:07", "link": "http://arxiv.org/abs/2004.08166v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Discovery of Implicit Gender Bias", "abstract": "Despite their prevalence in society, social biases are difficult to identify,\nprimarily because human judgements in this domain can be unreliable. We take an\nunsupervised approach to identifying gender bias against women at a comment\nlevel and present a model that can surface text likely to contain bias. Our\nmain challenge is forcing the model to focus on signs of implicit bias, rather\nthan other artifacts in the data. Thus, our methodology involves reducing the\ninfluence of confounds through propensity matching and adversarial learning.\nOur analysis shows how biased comments directed towards female politicians\ncontain mixed criticisms, while comments directed towards other female public\nfigures focus on appearance and sexualization. Ultimately, our work offers a\nway to capture subtle biases in various domains without relying on subjective\nhuman judgements.", "published": "2020-04-17 17:36:20", "link": "http://arxiv.org/abs/2004.08361v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can You Put it All Together: Evaluating Conversational Agents' Ability\n  to Blend Skills", "abstract": "Being engaging, knowledgeable, and empathetic are all desirable general\nqualities in a conversational agent. Previous work has introduced tasks and\ndatasets that aim to help agents to learn those qualities in isolation and\ngauge how well they can express them. But rather than being specialized in one\nsingle quality, a good open-domain conversational agent should be able to\nseamlessly blend them all into one cohesive conversational flow. In this work,\nwe investigate several ways to combine models trained towards isolated\ncapabilities, ranging from simple model aggregation schemes that require\nminimal additional training, to various forms of multi-task training that\nencompass several skills at all training stages. We further propose a new\ndataset, BlendedSkillTalk, to analyze how these capabilities would mesh\ntogether in a natural conversation, and compare the performance of different\narchitectures and training schemes. Our experiments show that multi-tasking\nover several tasks that focus on particular capabilities results in better\nblended conversation performance compared to models trained on a single skill,\nand that both unified or two-stage approaches perform well if they are\nconstructed to avoid unwanted bias in skill selection or are fine-tuned on our\nnew task.", "published": "2020-04-17 20:51:40", "link": "http://arxiv.org/abs/2004.08449v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SongNet: Rigid Formats Controlled Text Generation", "abstract": "Neural text generation has made tremendous progress in various tasks. One\ncommon characteristic of most of the tasks is that the texts are not restricted\nto some rigid formats when generating. However, we may confront some special\ntext paradigms such as Lyrics (assume the music score is given), Sonnet, SongCi\n(classical Chinese poetry of the Song dynasty), etc. The typical\ncharacteristics of these texts are in three folds: (1) They must comply fully\nwith the rigid predefined formats. (2) They must obey some rhyming schemes. (3)\nAlthough they are restricted to some formats, the sentence integrity must be\nguaranteed. To the best of our knowledge, text generation based on the\npredefined rigid formats has not been well investigated. Therefore, we propose\na simple and elegant framework named SongNet to tackle this problem. The\nbackbone of the framework is a Transformer-based auto-regressive language\nmodel. Sets of symbols are tailor-designed to improve the modeling performance\nespecially on format, rhyme, and sentence integrity. We improve the attention\nmechanism to impel the model to capture some future information on the format.\nA pre-training and fine-tuning framework is designed to further improve the\ngeneration quality. Extensive experiments conducted on two collected corpora\ndemonstrate that our proposed framework generates significantly better results\nin terms of both automatic metrics and the human evaluation.", "published": "2020-04-17 01:40:18", "link": "http://arxiv.org/abs/2004.08022v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Active Sentence Learning by Adversarial Uncertainty Sampling in Discrete\n  Space", "abstract": "Active learning for sentence understanding aims at discovering informative\nunlabeled data for annotation and therefore reducing the demand for labeled\ndata. We argue that the typical uncertainty sampling method for active learning\nis time-consuming and can hardly work in real-time, which may lead to\nineffective sample selection. We propose adversarial uncertainty sampling in\ndiscrete space (AUSDS) to retrieve informative unlabeled samples more\nefficiently. AUSDS maps sentences into latent space generated by the popular\npre-trained language models, and discover informative unlabeled text samples\nfor annotation via adversarial attack. The proposed approach is extremely\nefficient compared with traditional uncertainty sampling with more than 10x\nspeedup. Experimental results on five datasets show that AUSDS outperforms\nstrong baselines on effectiveness.", "published": "2020-04-17 03:12:34", "link": "http://arxiv.org/abs/2004.08046v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Transform and Tell: Entity-Aware News Image Captioning", "abstract": "We propose an end-to-end model which generates captions for images embedded\nin news articles. News images present two key challenges: they rely on\nreal-world knowledge, especially about named entities; and they typically have\nlinguistically rich captions that include uncommon words. We address the first\nchallenge by associating words in the caption with faces and objects in the\nimage, via a multi-modal, multi-head attention mechanism. We tackle the second\nchallenge with a state-of-the-art transformer language model that uses\nbyte-pair-encoding to generate captions as a sequence of word parts. On the\nGoodNews dataset, our model outperforms the previous state of the art by a\nfactor of four in CIDEr score (13 to 54). This performance gain comes from a\nunique combination of language models, word representation, image embeddings,\nface embeddings, object embeddings, and improvements in neural network design.\nWe also introduce the NYTimes800k dataset which is 70% larger than GoodNews,\nhas higher article quality, and includes the locations of images within\narticles as an additional contextual cue.", "published": "2020-04-17 05:44:37", "link": "http://arxiv.org/abs/2004.08070v2", "categories": ["cs.CV", "cs.CL", "I.4.0; I.2.7"], "primary_category": "cs.CV"}
{"title": "Batch Clustering for Multilingual News Streaming", "abstract": "Nowadays, digital news articles are widely available, published by various\neditors and often written in different languages. This large volume of diverse\nand unorganized information makes human reading very difficult or almost\nimpossible. This leads to a need for algorithms able to arrange high amount of\nmultilingual news into stories. To this purpose, we extend previous works on\nTopic Detection and Tracking, and propose a new system inspired from newsLens.\nWe process articles per batch, looking for monolingual local topics which are\nthen linked across time and languages. Here, we introduce a novel \"replaying\"\nstrategy to link monolingual local topics into stories. Besides, we propose new\nfine tuned multilingual embedding using SBERT to create crosslingual stories.\nOur system gives monolingual state-of-the-art results on dataset of Spanish and\nGerman news and crosslingual state-of-the-art results on English, Spanish and\nGerman news.", "published": "2020-04-17 08:59:13", "link": "http://arxiv.org/abs/2004.08123v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Highway Transformer: Self-Gating Enhanced Self-Attentive Networks", "abstract": "Self-attention mechanisms have made striking state-of-the-art (SOTA) progress\nin various sequence learning tasks, standing on the multi-headed dot product\nattention by attending to all the global contexts at different locations.\nThrough a pseudo information highway, we introduce a gated component\nself-dependency units (SDU) that incorporates LSTM-styled gating units to\nreplenish internal semantic importance within the multi-dimensional latent\nspace of individual representations. The subsidiary content-based SDU gates\nallow for the information flow of modulated latent embeddings through skipped\nconnections, leading to a clear margin of convergence speed with gradient\ndescent algorithms. We may unveil the role of gating mechanism to aid in the\ncontext-based Transformer modules, with hypothesizing that SDU gates,\nespecially on shallow layers, could push it faster to step towards suboptimal\npoints during the optimization process.", "published": "2020-04-17 11:25:07", "link": "http://arxiv.org/abs/2004.08178v5", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Women worry about family, men about the economy: Gender differences in\n  emotional responses to COVID-19", "abstract": "Among the critical challenges around the COVID-19 pandemic is dealing with\nthe potentially detrimental effects on people's mental health. Designing\nappropriate interventions and identifying the concerns of those most at risk\nrequires methods that can extract worries, concerns and emotional responses\nfrom text data. We examine gender differences and the effect of document length\non worries about the ongoing COVID-19 situation. Our findings suggest that i)\nshort texts do not offer as adequate insights into psychological processes as\nlonger texts. We further find ii) marked gender differences in topics\nconcerning emotional responses. Women worried more about their loved ones and\nsevere health concerns while men were more occupied with effects on the economy\nand society. This paper adds to the understanding of general gender differences\nin language found elsewhere, and shows that the current unique circumstances\nlikely amplified these effects. We close this paper with a call for more\nhigh-quality datasets due to the limitations of Tweet-sized data.", "published": "2020-04-17 12:23:46", "link": "http://arxiv.org/abs/2004.08202v2", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Towards an Interoperable Ecosystem of AI and LT Platforms: A Roadmap for\n  the Implementation of Different Levels of Interoperability", "abstract": "With regard to the wider area of AI/LT platform interoperability, we\nconcentrate on two core aspects: (1) cross-platform search and discovery of\nresources and services; (2) composition of cross-platform service workflows. We\ndevise five different levels (of increasing complexity) of platform\ninteroperability that we suggest to implement in a wider federation of AI/LT\nplatforms. We illustrate the approach using the five emerging AI/LT platforms\nAI4EU, ELG, Lynx, QURATOR and SPEAKER.", "published": "2020-04-17 17:22:52", "link": "http://arxiv.org/abs/2004.08355v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring the Combination of Contextual Word Embeddings and Knowledge\n  Graph Embeddings", "abstract": "``Classical'' word embeddings, such as Word2Vec, have been shown to capture\nthe semantics of words based on their distributional properties. However, their\nability to represent the different meanings that a word may have is limited.\nSuch approaches also do not explicitly encode relations between entities, as\ndenoted by words. Embeddings of knowledge bases (KB) capture the explicit\nrelations between entities denoted by words, but are not able to directly\ncapture the syntagmatic properties of these words. To our knowledge, recent\nresearch have focused on representation learning that augment the strengths of\none with the other. In this work, we begin exploring another approach using\ncontextual and KB embeddings jointly at the same level and propose two tasks --\nan entity typing and a relation typing task -- that evaluate the performance of\ncontextual and KB embeddings. We also evaluated a concatenated model of\ncontextual and KB embeddings with these two tasks, and obtain conclusive\nresults on the first task. We hope our work may contribute as a basis for\nmodels and datasets that develop in the direction of this approach.", "published": "2020-04-17 17:49:45", "link": "http://arxiv.org/abs/2004.08371v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Knowledge-Based Visual Question Answering in Videos", "abstract": "We propose a novel video understanding task by fusing knowledge-based and\nvideo question answering. First, we introduce KnowIT VQA, a video dataset with\n24,282 human-generated question-answer pairs about a popular sitcom. The\ndataset combines visual, textual and temporal coherence reasoning together with\nknowledge-based questions, which need of the experience obtained from the\nviewing of the series to be answered. Second, we propose a video understanding\nmodel by combining the visual and textual video content with specific knowledge\nabout the show. Our main findings are: (i) the incorporation of knowledge\nproduces outstanding improvements for VQA in video, and (ii) the performance on\nKnowIT VQA still lags well behind human accuracy, indicating its usefulness for\nstudying current video modelling limitations.", "published": "2020-04-17 02:06:26", "link": "http://arxiv.org/abs/2004.08385v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "How recurrent networks implement contextual processing in sentiment\n  analysis", "abstract": "Neural networks have a remarkable capacity for contextual processing--using\nrecent or nearby inputs to modify processing of current input. For example, in\nnatural language, contextual processing is necessary to correctly interpret\nnegation (e.g. phrases such as \"not bad\"). However, our ability to understand\nhow networks process context is limited. Here, we propose general methods for\nreverse engineering recurrent neural networks (RNNs) to identify and elucidate\ncontextual processing. We apply these methods to understand RNNs trained on\nsentiment classification. This analysis reveals inputs that induce contextual\neffects, quantifies the strength and timescale of these effects, and identifies\nsets of these inputs with similar properties. Additionally, we analyze\ncontextual effects related to differential processing of the beginning and end\nof documents. Using the insights learned from the RNNs we improve baseline\nBag-of-Words models with simple extensions that incorporate contextual\nmodification, recovering greater than 90% of the RNN's performance increase\nover the baseline. This work yields a new understanding of how RNNs process\ncontextual information, and provides tools that should provide similar insight\nmore broadly.", "published": "2020-04-17 00:58:30", "link": "http://arxiv.org/abs/2004.08013v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Show Us the Way: Learning to Manage Dialog from Demonstrations", "abstract": "We present our submission to the End-to-End Multi-Domain Dialog Challenge\nTrack of the Eighth Dialog System Technology Challenge. Our proposed dialog\nsystem adopts a pipeline architecture, with distinct components for Natural\nLanguage Understanding, Dialog State Tracking, Dialog Management and Natural\nLanguage Generation. At the core of our system is a reinforcement learning\nalgorithm which uses Deep Q-learning from Demonstrations to learn a dialog\npolicy with the help of expert examples. We find that demonstrations are\nessential to training an accurate dialog policy where both state and action\nspaces are large. Evaluation of our Dialog Management component shows that our\napproach is effective - beating supervised and reinforcement learning\nbaselines.", "published": "2020-04-17 08:41:54", "link": "http://arxiv.org/abs/2004.08114v1", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Understanding the Difficulty of Training Transformers", "abstract": "Transformers have proved effective in many NLP tasks. However, their training\nrequires non-trivial efforts regarding designing cutting-edge optimizers and\nlearning rate schedulers carefully (e.g., conventional SGD fails to train\nTransformers effectively). Our objective here is to understand $\\textit{what\ncomplicates Transformer training}$ from both empirical and theoretical\nperspectives. Our analysis reveals that unbalanced gradients are not the root\ncause of the instability of training. Instead, we identify an amplification\neffect that influences training substantially -- for each layer in a\nmulti-layer Transformer model, heavy dependency on its residual branch makes\ntraining unstable, since it amplifies small parameter perturbations (e.g.,\nparameter updates) and results in significant disturbances in the model output.\nYet we observe that a light dependency limits the model potential and leads to\ninferior trained models. Inspired by our analysis, we propose Admin\n($\\textbf{Ad}$aptive $\\textbf{m}$odel $\\textbf{in}$itialization) to stabilize\nstabilize the early stage's training and unleash its full potential in the late\nstage. Extensive experiments show that Admin is more stable, converges faster,\nand leads to better performance. Implementations are released at:\nhttps://github.com/LiyuanLucasLiu/Transforemr-Clinic.", "published": "2020-04-17 13:59:07", "link": "http://arxiv.org/abs/2004.08249v3", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "SpEx: Multi-Scale Time Domain Speaker Extraction Network", "abstract": "Speaker extraction aims to mimic humans' selective auditory attention by\nextracting a target speaker's voice from a multi-talker environment. It is\ncommon to perform the extraction in frequency-domain, and reconstruct the\ntime-domain signal from the extracted magnitude and estimated phase spectra.\nHowever, such an approach is adversely affected by the inherent difficulty of\nphase estimation. Inspired by Conv-TasNet, we propose a time-domain speaker\nextraction network (SpEx) that converts the mixture speech into multi-scale\nembedding coefficients instead of decomposing the speech signal into magnitude\nand phase spectra. In this way, we avoid phase estimation. The SpEx network\nconsists of four network components, namely speaker encoder, speech encoder,\nspeaker extractor, and speech decoder. Specifically, the speech encoder\nconverts the mixture speech into multi-scale embedding coefficients, the\nspeaker encoder learns to represent the target speaker with a speaker\nembedding. The speaker extractor takes the multi-scale embedding coefficients\nand target speaker embedding as input and estimates a receptive mask. Finally,\nthe speech decoder reconstructs the target speaker's speech from the masked\nembedding coefficients. We also propose a multi-task learning framework and a\nmulti-scale embedding implementation. Experimental results show that the\nproposed SpEx achieves 37.3%, 37.7% and 15.0% relative improvements over the\nbest baseline in terms of signal-to-distortion ratio (SDR), scale-invariant SDR\n(SI-SDR), and perceptual evaluation of speech quality (PESQ) under an open\nevaluation condition.", "published": "2020-04-17 16:13:06", "link": "http://arxiv.org/abs/2004.08326v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Natural Language Processing with Deep Learning for Medical Adverse Event\n  Detection from Free-Text Medical Narratives: A Case Study of Detecting Total\n  Hip Replacement Dislocation", "abstract": "Accurate and timely detection of medical adverse events (AEs) from free-text\nmedical narratives is challenging. Natural language processing (NLP) with deep\nlearning has already shown great potential for analyzing free-text data, but\nits application for medical AE detection has been limited. In this study we\nproposed deep learning based NLP (DL-NLP) models for efficient and accurate hip\ndislocation AE detection following total hip replacement from standard\n(radiology notes) and non-standard (follow-up telephone notes) free-text\nmedical narratives. We benchmarked these proposed models with a wide variety of\ntraditional machine learning based NLP (ML-NLP) models, and also assessed the\naccuracy of International Classification of Diseases (ICD) and Current\nProcedural Terminology (CPT) codes in capturing these hip dislocation AEs in a\nmulti-center orthopaedic registry. All DL-NLP models out-performed all of the\nML-NLP models, with a convolutional neural network (CNN) model achieving the\nbest overall performance (Kappa = 0.97 for radiology notes, and Kappa = 1.00\nfor follow-up telephone notes). On the other hand, the ICD/CPT codes of the\npatients who sustained a hip dislocation AE were only 75.24% accurate, showing\nthe potential of the proposed model to be used in largescale orthopaedic\nregistries for accurate and efficient hip dislocation AE detection to improve\nthe quality of care and patient outcome.", "published": "2020-04-17 16:25:36", "link": "http://arxiv.org/abs/2004.08333v2", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Survey of Document Grounded Dialogue Systems (DGDS)", "abstract": "Dialogue system (DS) attracts great attention from industry and academia\nbecause of its wide application prospects. Researchers usually divide the DS\naccording to the function. However, many conversations require the DS to switch\nbetween different functions. For example, movie discussion can change from\nchit-chat to QA, the conversational recommendation can transform from chit-chat\nto recommendation, etc. Therefore, classification according to functions may\nnot be enough to help us appreciate the current development trend. We classify\nthe DS based on background knowledge. Specifically, study the latest DS based\non the unstructured document(s). We define Document Grounded Dialogue System\n(DGDS) as the DS that the dialogues are centering on the given document(s). The\nDGDS can be used in scenarios such as talking over merchandise against product\nManual, commenting on news reports, etc. We believe that extracting\nunstructured document(s) information is the future trend of the DS because a\ngreat amount of human knowledge lies in these document(s). The research of the\nDGDS not only possesses a broad application prospect but also facilitates AI to\nbetter understand human knowledge and natural language. We analyze the\nclassification, architecture, datasets, models, and future development trends\nof the DGDS, hoping to help researchers in this field.", "published": "2020-04-17 03:22:28", "link": "http://arxiv.org/abs/2004.13818v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "How to Teach DNNs to Pay Attention to the Visual Modality in Speech\n  Recognition", "abstract": "Audio-Visual Speech Recognition (AVSR) seeks to model, and thereby exploit,\nthe dynamic relationship between a human voice and the corresponding mouth\nmovements. A recently proposed multimodal fusion strategy, AV Align, based on\nstate-of-the-art sequence to sequence neural networks, attempts to model this\nrelationship by explicitly aligning the acoustic and visual representations of\nspeech. This study investigates the inner workings of AV Align and visualises\nthe audio-visual alignment patterns. Our experiments are performed on two of\nthe largest publicly available AVSR datasets, TCD-TIMIT and LRS2. We find that\nAV Align learns to align acoustic and visual representations of speech at the\nframe level on TCD-TIMIT in a generally monotonic pattern. We also determine\nthe cause of initially seeing no improvement over audio-only speech recognition\non the more challenging LRS2. We propose a regularisation method which involves\npredicting lip-related Action Units from visual representations. Our\nregularisation method leads to better exploitation of the visual modality, with\nperformance improvements between 7% and 30% depending on the noise level.\nFurthermore, we show that the alternative Watch, Listen, Attend, and Spell\nnetwork is affected by the same problem as AV Align, and that our proposed\napproach can effectively help it learn visual representations. Our findings\nvalidate the suitability of the regularisation method to AVSR and encourage\nresearchers to rethink the multimodal convergence problem when having one\ndominant modality.", "published": "2020-04-17 13:59:19", "link": "http://arxiv.org/abs/2004.08250v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Beat Detection and Automatic Annotation of the Music of Bharatanatyam\n  Dance using Speech Recognition Techniques", "abstract": "Bharatanatyam, an Indian Classical Dance form, represents the rich cultural\nheritage of India. Analysis and recognition of such dance forms are critical\nfor the preservation of cultural heritage. Like in most dance forms, a\nBharatanatyam dancer performs in synchronization with structured rhythmic\nmusic, called Sollukattu, which comprises instrumental beats and vocalized\nutterances (bols) to create a rhythmic music structure. Computer analysis of\nBharatanatyam, therefore, requires a structural analysis of Sollukattus. In\nthis paper, we use speech processing techniques to recognize bols. Exploiting\nthe predefined structures of Sollukattus and the detected bols, we recognize\nthe Sollukattu. We estimate the tempo period by two methods. Finally, we\ngenerate a complete annotation of the audio signal by beat marking. For this,\nwe also use the information of beats detected from the onset envelope of a\nSollukattu signal. For training and test, we create a data set for Sollukattus\nand annotate them. We achieve 85% accuracy in bol recognition, 95% in\nSollukattu recognition, 96% in tempo period estimation, and over 90% in beat\nmarking. This is the maiden attempt to fully structurally analyze the music of\nan Indian Classical Dance form and the use of speech processing techniques for\nbeat marking.", "published": "2020-04-17 14:33:57", "link": "http://arxiv.org/abs/2004.08269v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
