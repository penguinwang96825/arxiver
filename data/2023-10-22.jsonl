{"title": "PromptCBLUE: A Chinese Prompt Tuning Benchmark for the Medical Domain", "abstract": "Biomedical language understanding benchmarks are the driving forces for\nartificial intelligence applications with large language model (LLM) back-ends.\nHowever, most current benchmarks: (a) are limited to English which makes it\nchallenging to replicate many of the successes in English for other languages,\nor (b) focus on knowledge probing of LLMs and neglect to evaluate how LLMs\napply these knowledge to perform on a wide range of bio-medical tasks, or (c)\nhave become a publicly available corpus and are leaked to LLMs during\npre-training. To facilitate the research in medical LLMs, we re-build the\nChinese Biomedical Language Understanding Evaluation (CBLUE) benchmark into a\nlarge scale prompt-tuning benchmark, PromptCBLUE. Our benchmark is a suitable\ntest-bed and an online platform for evaluating Chinese LLMs' multi-task\ncapabilities on a wide range bio-medical tasks including medical entity\nrecognition, medical text classification, medical natural language inference,\nmedical dialogue understanding and medical content/dialogue generation. To\nestablish evaluation on these tasks, we have experimented and report the\nresults with the current 9 Chinese LLMs fine-tuned with differtent fine-tuning\ntechniques.", "published": "2023-10-22 02:20:38", "link": "http://arxiv.org/abs/2310.14151v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An In-Context Schema Understanding Method for Knowledge Base Question\n  Answering", "abstract": "The Knowledge Base Question Answering (KBQA) task aims to answer natural\nlanguage questions based on a given knowledge base. Recently, Large Language\nModels (LLMs) have shown strong capabilities in language understanding and can\nbe used to solve this task. In doing so, a major challenge for LLMs is to\novercome the immensity and heterogeneity of knowledge base schemas.Existing\nmethods bypass this challenge by initially employing LLMs to generate drafts of\nlogic forms without schema-specific details.Then, an extra module is used to\ninject schema information to these drafts.In contrast, in this paper, we\npropose a simple In-Context Schema Understanding (ICSU) method that enables\nLLMs to directly understand schemas by leveraging in-context learning.\nSpecifically, ICSU provides schema information to LLMs using schema-related\nannotated examples. We investigate three example retrieval strategies based on\nraw questions, anonymized questions, and generated SPARQL queries. Experimental\nresults show that ICSU demonstrates competitive performance compared to\nbaseline methods on both the KQA Pro and WebQSP datasets.", "published": "2023-10-22 04:19:17", "link": "http://arxiv.org/abs/2310.14174v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "QA-NatVer: Question Answering for Natural Logic-based Fact Verification", "abstract": "Fact verification systems assess a claim's veracity based on evidence. An\nimportant consideration in designing them is faithfulness, i.e. generating\nexplanations that accurately reflect the reasoning of the model. Recent works\nhave focused on natural logic, which operates directly on natural language by\ncapturing the semantic relation of spans between an aligned claim with its\nevidence via set-theoretic operators. However, these approaches rely on\nsubstantial resources for training, which are only available for high-resource\nlanguages. To this end, we propose to use question answering to predict natural\nlogic operators, taking advantage of the generalization capabilities of\ninstruction-tuned language models. Thus, we obviate the need for annotated\ntraining data while still relying on a deterministic inference system. In a\nfew-shot setting on FEVER, our approach outperforms the best baseline by $4.3$\naccuracy points, including a state-of-the-art pre-trained seq2seq natural logic\nsystem, as well as a state-of-the-art prompt-based classifier. Our system\ndemonstrates its robustness and portability, achieving competitive performance\non a counterfactual dataset and surpassing all approaches without further\nannotation on a Danish verification dataset. A human evaluation indicates that\nour approach produces more plausible proofs with fewer erroneous natural logic\noperators than previous natural logic-based systems.", "published": "2023-10-22 06:27:31", "link": "http://arxiv.org/abs/2310.14198v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Customising General Large Language Models for Specialised Emotion\n  Recognition Tasks", "abstract": "The advent of large language models (LLMs) has gained tremendous attention\nover the past year. Previous studies have shown the astonishing performance of\nLLMs not only in other tasks but also in emotion recognition in terms of\naccuracy, universality, explanation, robustness, few/zero-shot learning, and\nothers. Leveraging the capability of LLMs inevitably becomes an essential\nsolution for emotion recognition. To this end, we further comprehensively\ninvestigate how LLMs perform in linguistic emotion recognition if we\nconcentrate on this specific task. Specifically, we exemplify a publicly\navailable and widely used LLM -- Chat General Language Model, and customise it\nfor our target by using two different modal adaptation techniques, i.e., deep\nprompt tuning and low-rank adaptation. The experimental results obtained on six\nwidely used datasets present that the adapted LLM can easily outperform other\nstate-of-the-art but specialised deep models. This indicates the strong\ntransferability and feasibility of LLMs in the field of emotion recognition.", "published": "2023-10-22 08:09:13", "link": "http://arxiv.org/abs/2310.14225v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Static to Dynamic: A Continual Learning Framework for Large\n  Language Models", "abstract": "The vast number of parameters in large language models (LLMs) endows them\nwith remarkable capabilities, allowing them to excel in a variety of natural\nlanguage processing tasks. However, this complexity also presents challenges,\nmaking LLMs difficult to train and inhibiting their ability to continuously\nassimilate new knowledge, which may lead to inaccuracies in their outputs. To\nmitigate these issues, this paper presents DynaMind, a novel continual learning\nframework designed for LLMs. DynaMind incorporates memory mechanisms to\nassimilate new knowledge and modular operators to enhance the model inference\nprocess with the newly assimilated knowledge, consequently improving the\naccuracies of LLMs' outputs. Benchmark experiments demonstrate DynaMind's\neffectiveness in overcoming these challenges. The code and demo of DynaMind are\navailable on GitHub: https://github.com/Elfsong/DynaMind.", "published": "2023-10-22 10:18:53", "link": "http://arxiv.org/abs/2310.14248v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Boosting Unsupervised Machine Translation with Pseudo-Parallel Data", "abstract": "Even with the latest developments in deep learning and large-scale language\nmodeling, the task of machine translation (MT) of low-resource languages\nremains a challenge. Neural MT systems can be trained in an unsupervised way\nwithout any translation resources but the quality lags behind, especially in\ntruly low-resource conditions. We propose a training strategy that relies on\npseudo-parallel sentence pairs mined from monolingual corpora in addition to\nsynthetic sentence pairs back-translated from monolingual corpora. We\nexperiment with different training schedules and reach an improvement of up to\n14.5 BLEU points (English to Ukrainian) over a baseline trained on\nback-translated data only.", "published": "2023-10-22 10:57:12", "link": "http://arxiv.org/abs/2310.14262v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CT-GAT: Cross-Task Generative Adversarial Attack based on\n  Transferability", "abstract": "Neural network models are vulnerable to adversarial examples, and adversarial\ntransferability further increases the risk of adversarial attacks. Current\nmethods based on transferability often rely on substitute models, which can be\nimpractical and costly in real-world scenarios due to the unavailability of\ntraining data and the victim model's structural details. In this paper, we\npropose a novel approach that directly constructs adversarial examples by\nextracting transferable features across various tasks. Our key insight is that\nadversarial transferability can extend across different tasks. Specifically, we\ntrain a sequence-to-sequence generative model named CT-GAT using adversarial\nsample data collected from multiple tasks to acquire universal adversarial\nfeatures and generate adversarial examples for different tasks. We conduct\nexperiments on ten distinct datasets, and the results demonstrate that our\nmethod achieves superior attack performance with small cost.", "published": "2023-10-22 11:00:04", "link": "http://arxiv.org/abs/2310.14265v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Model Unalignment: Parametric Red-Teaming to Expose Hidden\n  Harms and Biases", "abstract": "Red-teaming has been a widely adopted way to evaluate the harmfulness of\nLarge Language Models (LLMs). It aims to jailbreak a model's safety behavior to\nmake it act as a helpful agent disregarding the harmfulness of the query.\nExisting methods are primarily based on input text-based red-teaming such as\nadversarial prompts, low-resource prompts, or contextualized prompts to\ncondition the model in a way to bypass its safe behavior. Bypassing the\nguardrails uncovers hidden harmful information and biases in the model that are\nleft untreated or newly introduced by its safety training. However,\nprompt-based attacks fail to provide such a diagnosis owing to their low attack\nsuccess rate, and applicability to specific models. In this paper, we present a\nnew perspective on LLM safety research i.e., parametric red-teaming through\nUnalignment. It simply (instruction) tunes the model parameters to break model\nguardrails that are not deeply rooted in the model's behavior. Unalignment\nusing as few as 100 examples can significantly bypass commonly referred to as\nCHATGPT, to the point where it responds with an 88% success rate to harmful\nqueries on two safety benchmark datasets. On open-source models such as\nVICUNA-7B and LLAMA-2-CHAT 7B AND 13B, it shows an attack success rate of more\nthan 91%. On bias evaluations, Unalignment exposes inherent biases in\nsafety-aligned models such as CHATGPT and LLAMA- 2-CHAT where the model's\nresponses are strongly biased and opinionated 64% of the time.", "published": "2023-10-22 13:55:46", "link": "http://arxiv.org/abs/2310.14303v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Text Sanitization with Privacy Risk Indicators: An Empirical\n  Analysis", "abstract": "Text sanitization is the task of redacting a document to mask all occurrences\nof (direct or indirect) personal identifiers, with the goal of concealing the\nidentity of the individual(s) referred in it. In this paper, we consider a\ntwo-step approach to text sanitization and provide a detailed analysis of its\nempirical performance on two recently published datasets: the Text\nAnonymization Benchmark (Pil\\'an et al., 2022) and a collection of Wikipedia\nbiographies (Papadopoulou et al., 2022). The text sanitization process starts\nwith a privacy-oriented entity recognizer that seeks to determine the text\nspans expressing identifiable personal information. This privacy-oriented\nentity recognizer is trained by combining a standard named entity recognition\nmodel with a gazetteer populated by person-related terms extracted from\nWikidata. The second step of the text sanitization process consists in\nassessing the privacy risk associated with each detected text span, either\nisolated or in combination with other text spans. We present five distinct\nindicators of the re-identification risk, respectively based on language model\nprobabilities, text span classification, sequence labelling, perturbations, and\nweb search. We provide a contrastive analysis of each privacy indicator and\nhighlight their benefits and limitations, notably in relation to the available\nlabeled data.", "published": "2023-10-22 14:17:27", "link": "http://arxiv.org/abs/2310.14312v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Harmful Erotic Content Detection through Coreference-Driven\n  Contextual Analysis", "abstract": "Adult content detection still poses a great challenge for automation.\nExisting classifiers primarily focus on distinguishing between erotic and\nnon-erotic texts. However, they often need more nuance in assessing the\npotential harm. Unfortunately, the content of this nature falls beyond the\nreach of generative models due to its potentially harmful nature. Ethical\nrestrictions prohibit large language models (LLMs) from analyzing and\nclassifying harmful erotics, let alone generating them to create synthetic\ndatasets for other neural models. In such instances where data is scarce and\nchallenging, a thorough analysis of the structure of such texts rather than a\nlarge model may offer a viable solution. Especially given that harmful erotic\nnarratives, despite appearing similar to harmless ones, usually reveal their\nharmful nature first through contextual information hidden in the non-sexual\nparts of the narrative.\n  This paper introduces a hybrid neural and rule-based context-aware system\nthat leverages coreference resolution to identify harmful contextual cues in\nerotic content. Collaborating with professional moderators, we compiled a\ndataset and developed a classifier capable of distinguishing harmful from\nnon-harmful erotic content. Our hybrid model, tested on Polish text,\ndemonstrates a promising accuracy of 84% and a recall of 80%. Models based on\nRoBERTa and Longformer without explicit usage of coreference chains achieved\nsignificantly weaker results, underscoring the importance of coreference\nresolution in detecting such nuanced content as harmful erotics. This approach\nalso offers the potential for enhanced visual explainability, supporting\nmoderators in evaluating predictions and taking necessary actions to address\nharmful content.", "published": "2023-10-22 15:19:04", "link": "http://arxiv.org/abs/2310.14325v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DiFair: A Benchmark for Disentangled Assessment of Gender Knowledge and\n  Bias", "abstract": "Numerous debiasing techniques have been proposed to mitigate the gender bias\nthat is prevalent in pretrained language models. These are often evaluated on\ndatasets that check the extent to which the model is gender-neutral in its\npredictions. Importantly, this evaluation protocol overlooks the possible\nadverse impact of bias mitigation on useful gender knowledge. To fill this gap,\nwe propose DiFair, a manually curated dataset based on masked language modeling\nobjectives. DiFair allows us to introduce a unified metric, gender invariance\nscore, that not only quantifies a model's biased behavior, but also checks if\nuseful gender knowledge is preserved. We use DiFair as a benchmark for a number\nof widely-used pretained language models and debiasing techniques. Experimental\nresults corroborate previous findings on the existing gender biases, while also\ndemonstrating that although debiasing techniques ameliorate the issue of gender\nbias, this improvement usually comes at the price of lowering useful gender\nknowledge of the model.", "published": "2023-10-22 15:27:16", "link": "http://arxiv.org/abs/2310.14329v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Social Commonsense-Guided Search Query Generation for Open-Domain\n  Knowledge-Powered Conversations", "abstract": "Open-domain dialog involves generating search queries that help obtain\nrelevant knowledge for holding informative conversations. However, it can be\nchallenging to determine what information to retrieve when the user is passive\nand does not express a clear need or request. To tackle this issue, we present\na novel approach that focuses on generating internet search queries that are\nguided by social commonsense. Specifically, we leverage a commonsense dialog\nsystem to establish connections related to the conversation topic, which\nsubsequently guides our query generation. Our proposed framework addresses\npassive user interactions by integrating topic tracking, commonsense response\ngeneration and instruction-driven query generation. Through extensive\nevaluations, we show that our approach overcomes limitations of existing query\ngeneration techniques that rely solely on explicit dialog information, and\nproduces search queries that are more relevant, specific, and compelling,\nultimately resulting in more engaging responses.", "published": "2023-10-22 16:14:56", "link": "http://arxiv.org/abs/2310.14340v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Law and NLP: Bridging Disciplinary Disconnects", "abstract": "Legal practice is intrinsically rooted in the fabric of language, yet legal\npractitioners and scholars have been slow to adopt tools from natural language\nprocessing (NLP). At the same time, the legal system is experiencing an access\nto justice crisis, which could be partially alleviated with NLP. In this\nposition paper, we argue that the slow uptake of NLP in legal practice is\nexacerbated by a disconnect between the needs of the legal community and the\nfocus of NLP researchers. In a review of recent trends in the legal NLP\nliterature, we find limited overlap between the legal NLP community and legal\nacademia. Our interpretation is that some of the most popular legal NLP tasks\nfail to address the needs of legal practitioners. We discuss examples of legal\nNLP tasks that promise to bridge disciplinary disconnects and highlight\ninteresting areas for legal NLP research that remain underexplored.", "published": "2023-10-22 16:34:31", "link": "http://arxiv.org/abs/2310.14346v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Subjective Cognitive Appraisals of Emotions from Large\n  Language Models", "abstract": "The emotions we experience involve complex processes; besides physiological\naspects, research in psychology has studied cognitive appraisals where people\nassess their situations subjectively, according to their own values (Scherer,\n2005). Thus, the same situation can often result in different emotional\nexperiences. While the detection of emotion is a well-established task, there\nis very limited work so far on the automatic prediction of cognitive\nappraisals. This work fills the gap by presenting CovidET-Appraisals, the most\ncomprehensive dataset to-date that assesses 24 appraisal dimensions, each with\na natural language rationale, across 241 Reddit posts. CovidET-Appraisals\npresents an ideal testbed to evaluate the ability of large language models --\nexcelling at a wide range of NLP tasks -- to automatically assess and explain\ncognitive appraisals. We found that while the best models are performant,\nopen-sourced LLMs fall short at this task, presenting a new challenge in the\nfuture development of emotionally intelligent models. We release our dataset at\nhttps://github.com/honglizhan/CovidET-Appraisals-Public.", "published": "2023-10-22 19:12:17", "link": "http://arxiv.org/abs/2310.14389v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "REFER: An End-to-end Rationale Extraction Framework for Explanation\n  Regularization", "abstract": "Human-annotated textual explanations are becoming increasingly important in\nExplainable Natural Language Processing. Rationale extraction aims to provide\nfaithful (i.e., reflective of the behavior of the model) and plausible (i.e.,\nconvincing to humans) explanations by highlighting the inputs that had the\nlargest impact on the prediction without compromising the performance of the\ntask model. In recent works, the focus of training rationale extractors was\nprimarily on optimizing for plausibility using human highlights, while the task\nmodel was trained on jointly optimizing for task predictive accuracy and\nfaithfulness. We propose REFER, a framework that employs a differentiable\nrationale extractor that allows to back-propagate through the rationale\nextraction process. We analyze the impact of using human highlights during\ntraining by jointly training the task model and the rationale extractor. In our\nexperiments, REFER yields significantly better results in terms of\nfaithfulness, plausibility, and downstream task accuracy on both\nin-distribution and out-of-distribution data. On both e-SNLI and CoS-E, our\nbest setting produces better results in terms of composite normalized relative\ngain than the previous baselines by 11% and 3%, respectively.", "published": "2023-10-22 21:20:52", "link": "http://arxiv.org/abs/2310.14418v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models are biased to overestimate profoundness", "abstract": "Recent advancements in natural language processing by large language models\n(LLMs), such as GPT-4, have been suggested to approach Artificial General\nIntelligence. And yet, it is still under dispute whether LLMs possess similar\nreasoning abilities to humans. This study evaluates GPT-4 and various other\nLLMs in judging the profoundness of mundane, motivational, and pseudo-profound\nstatements. We found a significant statement-to-statement correlation between\nthe LLMs and humans, irrespective of the type of statements and the prompting\ntechnique used. However, LLMs systematically overestimate the profoundness of\nnonsensical statements, with the exception of Tk-instruct, which uniquely\nunderestimates the profoundness of statements. Only few-shot learning prompts,\nas opposed to chain-of-thought prompting, draw LLMs ratings closer to humans.\nFurthermore, this work provides insights into the potential biases induced by\nReinforcement Learning from Human Feedback (RLHF), inducing an increase in the\nbias to overestimate the profoundness of statements.", "published": "2023-10-22 21:33:50", "link": "http://arxiv.org/abs/2310.14422v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Domain Terminology Integration into Machine Translation: Leveraging\n  Large Language Models", "abstract": "This paper discusses the methods that we used for our submissions to the WMT\n2023 Terminology Shared Task for German-to-English (DE-EN), English-to-Czech\n(EN-CS), and Chinese-to-English (ZH-EN) language pairs. The task aims to\nadvance machine translation (MT) by challenging participants to develop systems\nthat accurately translate technical terms, ultimately enhancing communication\nand understanding in specialised domains. To this end, we conduct experiments\nthat utilise large language models (LLMs) for two purposes: generating\nsynthetic bilingual terminology-based data, and post-editing translations\ngenerated by an MT model through incorporating pre-approved terms. Our system\nemploys a four-step process: (i) using an LLM to generate bilingual synthetic\ndata based on the provided terminology, (ii) fine-tuning a generic\nencoder-decoder MT model, with a mix of the terminology-based synthetic data\ngenerated in the first step and a randomly sampled portion of the original\ngeneric training data, (iii) generating translations with the fine-tuned MT\nmodel, and (iv) finally, leveraging an LLM for terminology-constrained\nautomatic post-editing of the translations that do not include the required\nterms. The results demonstrate the effectiveness of our proposed approach in\nimproving the integration of pre-approved terms into translations. The number\nof terms incorporated into the translations of the blind dataset increases from\nan average of 36.67% with the generic model to an average of 72.88% by the end\nof the process. In other words, successful utilisation of terms nearly doubles\nacross the three language pairs.", "published": "2023-10-22 23:25:28", "link": "http://arxiv.org/abs/2310.14451v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PHD: Pixel-Based Language Modeling of Historical Documents", "abstract": "The digitisation of historical documents has provided historians with\nunprecedented research opportunities. Yet, the conventional approach to\nanalysing historical documents involves converting them from images to text\nusing OCR, a process that overlooks the potential benefits of treating them as\nimages and introduces high levels of noise. To bridge this gap, we take\nadvantage of recent advancements in pixel-based language models trained to\nreconstruct masked patches of pixels instead of predicting token distributions.\nDue to the scarcity of real historical scans, we propose a novel method for\ngenerating synthetic scans to resemble real historical documents. We then\npre-train our model, PHD, on a combination of synthetic scans and real\nhistorical newspapers from the 1700-1900 period. Through our experiments, we\ndemonstrate that PHD exhibits high proficiency in reconstructing masked image\npatches and provide evidence of our model's noteworthy language understanding\ncapabilities. Notably, we successfully apply our model to a historical QA task,\nhighlighting its usefulness in this domain.", "published": "2023-10-22 08:45:48", "link": "http://arxiv.org/abs/2310.18343v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Orthogonal Subspace Learning for Language Model Continual Learning", "abstract": "Benefiting from massive corpora and advanced hardware, large language models\n(LLMs) exhibit remarkable capabilities in language understanding and\ngeneration. However, their performance degrades in scenarios where multiple\ntasks are encountered sequentially, also known as catastrophic forgetting. In\nthis paper, we propose orthogonal low-rank adaptation (O-LoRA), a simple and\nefficient approach for continual learning in language models, effectively\nmitigating catastrophic forgetting while learning new tasks. Specifically,\nO-LoRA learns tasks in different (low-rank) vector subspaces that are kept\northogonal to each other in order to minimize interference. Our method induces\nonly marginal additional parameter costs and requires no user data storage for\nreplay. Experimental results on continual learning benchmarks show that our\nmethod outperforms state-of-the-art methods. Furthermore, compared to previous\napproaches, our method excels in preserving the generalization ability of LLMs\non unseen tasks.", "published": "2023-10-22 02:23:44", "link": "http://arxiv.org/abs/2310.14152v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Can Language Models Laugh at YouTube Short-form Videos?", "abstract": "As short-form funny videos on social networks are gaining popularity, it\nbecomes demanding for AI models to understand them for better communication\nwith humans. Unfortunately, previous video humor datasets target specific\ndomains, such as speeches or sitcoms, and mostly focus on verbal cues. We\ncurate a user-generated dataset of 10K multimodal funny videos from YouTube,\ncalled ExFunTube. Using a video filtering pipeline with GPT-3.5, we verify both\nverbal and visual elements contributing to humor. After filtering, we annotate\neach video with timestamps and text explanations for funny moments. Our\nExFunTube is unique over existing datasets in that our videos cover a wide\nrange of domains with various types of humor that necessitate a multimodal\nunderstanding of the content. Also, we develop a zero-shot video-to-text\nprompting to maximize video humor understanding of large language models\n(LLMs). With three different evaluation methods using automatic scores,\nrationale quality experiments, and human evaluations, we show that our\nprompting significantly improves LLMs' ability for humor explanation.", "published": "2023-10-22 03:01:38", "link": "http://arxiv.org/abs/2310.14159v3", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "PromptMix: A Class Boundary Augmentation Method for Large Language Model\n  Distillation", "abstract": "Data augmentation is a widely used technique to address the problem of text\nclassification when there is a limited amount of training data. Recent work\noften tackles this problem using large language models (LLMs) like GPT3 that\ncan generate new examples given already available ones. In this work, we\npropose a method to generate more helpful augmented data by utilizing the LLM's\nabilities to follow instructions and perform few-shot classifications. Our\nspecific PromptMix method consists of two steps: 1) generate challenging text\naugmentations near class boundaries; however, generating borderline examples\nincreases the risk of false positives in the dataset, so we 2) relabel the text\naugmentations using a prompting-based LLM classifier to enhance the correctness\nof labels in the generated data. We evaluate the proposed method in challenging\n2-shot and zero-shot settings on four text classification datasets: Banking77,\nTREC6, Subjectivity (SUBJ), and Twitter Complaints. Our experiments show that\ngenerating and, crucially, relabeling borderline examples facilitates the\ntransfer of knowledge of a massive LLM like GPT3.5-turbo into smaller and\ncheaper classifiers like DistilBERT$_{base}$ and BERT$_{base}$. Furthermore,\n2-shot PromptMix outperforms multiple 5-shot data augmentation methods on the\nfour datasets. Our code is available at\nhttps://github.com/ServiceNow/PromptMix-EMNLP-2023.", "published": "2023-10-22 05:43:23", "link": "http://arxiv.org/abs/2310.14192v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Manifold-Preserving Transformers are Effective for Short-Long Range\n  Encoding", "abstract": "Multi-head self-attention-based Transformers have shown promise in different\nlearning tasks. Albeit these models exhibit significant improvement in\nunderstanding short-term and long-term contexts from sequences, encoders of\nTransformers and their variants fail to preserve layer-wise contextual\ninformation. Transformers usually project tokens onto sparse manifolds and fail\nto preserve mathematical equivalence among the token representations. In this\nwork, we propose TransJect, an encoder model that guarantees a theoretical\nbound for layer-wise distance preservation between a pair of tokens. We propose\na simple alternative to dot-product attention to ensure Lipschitz continuity.\nThis allows TransJect to learn injective mappings to transform token\nrepresentations to different manifolds with similar topology and preserve\nEuclidean distance between every pair of tokens in subsequent layers.\nEvaluations across multiple benchmark short- and long-sequence classification\ntasks show maximum improvements of 6.8% and 5.9%, respectively, over the\nvariants of Transformers. Additionally, TransJect displays 79% better\nperformance than Transformer on the language modeling task. We further\nhighlight the shortcomings of multi-head self-attention from the statistical\nphysics viewpoint. Although multi-head self-attention was incepted to learn\ndifferent abstraction levels within the networks, our empirical analyses\nsuggest that different attention heads learn randomly and unorderly. In\ncontrast, TransJect adapts a mixture of experts for regularization; these\nexperts are more orderly and balanced and learn different sparse\nrepresentations from the input sequences. TransJect exhibits very low entropy\nand can be efficiently scaled to larger depths.", "published": "2023-10-22 06:58:28", "link": "http://arxiv.org/abs/2310.14206v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "4 and 7-bit Labeling for Projective and Non-Projective Dependency Trees", "abstract": "We introduce an encoding for parsing as sequence labeling that can represent\nany projective dependency tree as a sequence of 4-bit labels, one per word. The\nbits in each word's label represent (1) whether it is a right or left\ndependent, (2) whether it is the outermost (left/right) dependent of its\nparent, (3) whether it has any left children and (4) whether it has any right\nchildren. We show that this provides an injective mapping from trees to labels\nthat can be encoded and decoded in linear time. We then define a 7-bit\nextension that represents an extra plane of arcs, extending the coverage to\nalmost full non-projectivity (over 99.9% empirical arc coverage). Results on a\nset of diverse treebanks show that our 7-bit encoding obtains substantial\naccuracy gains over the previously best-performing sequence labeling encodings.", "published": "2023-10-22 14:43:53", "link": "http://arxiv.org/abs/2310.14319v1", "categories": ["cs.CL", "cs.FL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "CLMSM: A Multi-Task Learning Framework for Pre-training on Procedural\n  Text", "abstract": "In this paper, we propose CLMSM, a domain-specific, continual pre-training\nframework, that learns from a large set of procedural recipes. CLMSM uses a\nMulti-Task Learning Framework to optimize two objectives - a) Contrastive\nLearning using hard triplets to learn fine-grained differences across entities\nin the procedures, and b) a novel Mask-Step Modelling objective to learn\nstep-wise context of a procedure. We test the performance of CLMSM on the\ndownstream tasks of tracking entities and aligning actions between two\nprocedures on three datasets, one of which is an open-domain dataset not\nconforming with the pre-training dataset. We show that CLMSM not only\noutperforms baselines on recipes (in-domain) but is also able to generalize to\nopen-domain procedural NLP tasks.", "published": "2023-10-22 15:20:11", "link": "http://arxiv.org/abs/2310.14326v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "From Chaos to Clarity: Claim Normalization to Empower Fact-Checking", "abstract": "With the rise of social media, users are exposed to many misleading claims.\nHowever, the pervasive noise inherent in these posts presents a challenge in\nidentifying precise and prominent claims that require verification. Extracting\nthe important claims from such posts is arduous and time-consuming, yet it is\nan underexplored problem. Here, we aim to bridge this gap. We introduce a novel\ntask, Claim Normalization (aka ClaimNorm), which aims to decompose complex and\nnoisy social media posts into more straightforward and understandable forms,\ntermed normalized claims. We propose CACN, a pioneering approach that leverages\nchain-of-thought and claim check-worthiness estimation, mimicking human\nreasoning processes, to comprehend intricate claims. Moreover, we capitalize on\nthe in-context learning capabilities of large language models to provide\nguidance and to improve claim normalization. To evaluate the effectiveness of\nour proposed model, we meticulously compile a comprehensive real-world dataset,\nCLAN, comprising more than 6k instances of social media posts alongside their\nrespective normalized claims. Our experiments demonstrate that CACN outperforms\nseveral baselines across various evaluation measures. Finally, our rigorous\nerror analysis validates CACN's capabilities and pitfalls.", "published": "2023-10-22 16:07:06", "link": "http://arxiv.org/abs/2310.14338v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Is ChatGPT a game changer for geocoding -- a benchmark for geocoding\n  address parsing techniques", "abstract": "The remarkable success of GPT models across various tasks, including toponymy\nrecognition motivates us to assess the performance of the GPT-3 model in the\ngeocoding address parsing task. To ensure that the evaluation more accurately\nmirrors performance in real-world scenarios with diverse user input qualities\nand resolve the pressing need for a 'gold standard' evaluation dataset for\ngeocoding systems, we introduce a benchmark dataset of low-quality address\ndescriptions synthesized based on human input patterns mining from actual input\nlogs of a geocoding system in production. This dataset has 21 different input\nerrors and variations; contains over 239,000 address records that are uniquely\nselected from streets across all U.S. 50 states and D.C.; and consists of three\nsubsets to be used as training, validation, and testing sets. Building on this,\nwe train and gauge the performance of the GPT-3 model in extracting address\ncomponents, contrasting its performance with transformer-based and LSTM-based\nmodels. The evaluation results indicate that Bidirectional LSTM-CRF model has\nachieved the best performance over these transformer-based models and GPT-3\nmodel. Transformer-based models demonstrate very comparable results compared to\nthe Bidirectional LSTM-CRF model. The GPT-3 model, though trailing in\nperformance, showcases potential in the address parsing task with few-shot\nexamples, exhibiting room for improvement with additional fine-tuning. We open\nsource the code and data of this presented benchmark so that researchers can\nutilize it for future model development or extend it to evaluate similar tasks,\nsuch as document geocoding.", "published": "2023-10-22 17:03:56", "link": "http://arxiv.org/abs/2310.14360v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Bi-Encoders based Species Normalization -- Pairwise Sentence Learning to\n  Rank", "abstract": "Motivation: Biomedical named-entity normalization involves connecting\nbiomedical entities with distinct database identifiers in order to facilitate\ndata integration across various fields of biology. Existing systems for\nbiomedical named entity normalization heavily rely on dictionaries, manually\ncreated rules, and high-quality representative features such as lexical or\nmorphological characteristics. However, recent research has investigated the\nuse of neural network-based models to reduce dependence on dictionaries,\nmanually crafted rules, and features. Despite these advancements, the\nperformance of these models is still limited due to the lack of sufficiently\nlarge training datasets. These models have a tendency to overfit small training\ncorpora and exhibit poor generalization when faced with previously unseen\nentities, necessitating the redesign of rules and features. Contribution: We\npresent a novel deep learning approach for named entity normalization, treating\nit as a pair-wise learning to rank problem. Our method utilizes the widely-used\ninformation retrieval algorithm Best Matching 25 to generate candidate\nconcepts, followed by the application of bi-directional encoder representation\nfrom the encoder (BERT) to re-rank the candidate list. Notably, our approach\neliminates the need for feature-engineering or rule creation. We conduct\nexperiments on species entity types and evaluate our method against\nstate-of-the-art techniques using LINNAEUS and S800 biomedical corpora. Our\nproposed approach surpasses existing methods in linking entities to the NCBI\ntaxonomy. To the best of our knowledge, there is no existing neural\nnetwork-based approach for species normalization in the literature.", "published": "2023-10-22 17:30:16", "link": "http://arxiv.org/abs/2310.14366v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ARCOQ: Arabic Closest Opposite Questions Dataset", "abstract": "This paper presents a dataset for closest opposite questions in Arabic\nlanguage. The dataset is the first of its kind for the Arabic language. It is\nbeneficial for the assessment of systems on the aspect of antonymy detection.\nThe structure is similar to that of the Graduate Record Examination (GRE)\nclosest opposite questions dataset for the English language. The introduced\ndataset consists of 500 questions, each contains a query word for which the\nclosest opposite needs to be determined from among a set of candidate words.\nEach question is also associated with the correct answer. We publish the\ndataset publicly in addition to providing standard splits of the dataset into\ndevelopment and test sets. Moreover, the paper provides a benchmark for the\nperformance of different Arabic word embedding models on the introduced\ndataset.", "published": "2023-10-22 18:41:26", "link": "http://arxiv.org/abs/2310.14384v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Merging Generated and Retrieved Knowledge for Open-Domain QA", "abstract": "Open-domain question answering (QA) systems are often built with retrieval\nmodules. However, retrieving passages from a given source is known to suffer\nfrom insufficient knowledge coverage. Alternatively, prompting large language\nmodels (LLMs) to generate contextual passages based on their parametric\nknowledge has been shown to improve QA performance. Yet, LLMs tend to\n\"hallucinate\" content that conflicts with the retrieved knowledge. Based on the\nintuition that answers supported by both sources are more likely to be correct,\nwe propose COMBO, a Compatibility-Oriented knowledge Merging for Better\nOpen-domain QA framework, to effectively leverage the two sources of\ninformation. Concretely, we match LLM-generated passages with retrieved\ncounterparts into compatible pairs, based on discriminators trained with silver\ncompatibility labels. Then a Fusion-in-Decoder-based reader model handles\npassage pairs to arrive at the final answer. Experiments show that COMBO\noutperforms competitive baselines on three out of four tested open-domain QA\nbenchmarks. Further analysis reveals that our proposed framework demonstrates\ngreater efficacy in scenarios with a higher degree of knowledge conflicts.", "published": "2023-10-22 19:37:06", "link": "http://arxiv.org/abs/2310.14393v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "O3D: Offline Data-driven Discovery and Distillation for Sequential\n  Decision-Making with Large Language Models", "abstract": "Recent advancements in large language models (LLMs) have exhibited promising\nperformance in solving sequential decision-making problems. By imitating\nfew-shot examples provided in the prompts (i.e., in-context learning), an LLM\nagent can interact with an external environment and complete given tasks\nwithout additional training. However, such few-shot examples are often\ninsufficient to generate high-quality solutions for complex and long-horizon\ntasks, while the limited context length cannot consume larger-scale\ndemonstrations with long interaction horizons. To this end, we propose an\noffline learning framework that utilizes offline data at scale (e.g, logs of\nhuman interactions) to improve LLM-powered policies without finetuning. The\nproposed method O3D (Offline Data-driven Discovery and Distillation)\nautomatically discovers reusable skills and distills generalizable knowledge\nacross multiple tasks based on offline interaction data, advancing the\ncapability of solving downstream tasks. Empirical results under two interactive\ndecision-making benchmarks (ALFWorld and WebShop) verify that O3D can notably\nenhance the decision-making capabilities of LLMs through the offline discovery\nand distillation process, and consistently outperform baselines across various\nLLMs.", "published": "2023-10-22 20:28:33", "link": "http://arxiv.org/abs/2310.14403v5", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Be Selfish, But Wisely: Investigating the Impact of Agent Personality in\n  Mixed-Motive Human-Agent Interactions", "abstract": "A natural way to design a negotiation dialogue system is via self-play RL:\ntrain an agent that learns to maximize its performance by interacting with a\nsimulated user that has been designed to imitate human-human dialogue data.\nAlthough this procedure has been adopted in prior work, we find that it results\nin a fundamentally flawed system that fails to learn the value of compromise in\na negotiation, which can often lead to no agreements (i.e., the partner walking\naway without a deal), ultimately hurting the model's overall performance. We\ninvestigate this observation in the context of the DealOrNoDeal task, a\nmulti-issue negotiation over books, hats, and balls. Grounded in negotiation\ntheory from Economics, we modify the training procedure in two novel ways to\ndesign agents with diverse personalities and analyze their performance with\nhuman partners. We find that although both techniques show promise, a selfish\nagent, which maximizes its own performance while also avoiding walkaways,\nperforms superior to other variants by implicitly learning to generate value\nfor both itself and the negotiation partner. We discuss the implications of our\nfindings for what it means to be a successful negotiation dialogue system and\nhow these systems should be designed in the future.", "published": "2023-10-22 20:31:35", "link": "http://arxiv.org/abs/2310.14404v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Which Prompts Make The Difference? Data Prioritization For Efficient\n  Human LLM Evaluation", "abstract": "Human evaluation is increasingly critical for assessing large language\nmodels, capturing linguistic nuances, and reflecting user preferences more\naccurately than traditional automated metrics. However, the resource-intensive\nnature of this type of annotation process poses significant challenges. The key\nquestion driving our work: \"is it feasible to minimize human-in-the-loop\nfeedback by prioritizing data instances which most effectively distinguish\nbetween models?\" We evaluate several metric-based methods and find that these\nmetrics enhance the efficiency of human evaluations by minimizing the number of\nrequired annotations, thus saving time and cost, while ensuring a robust\nperformance evaluation. We show that our method is effective across widely used\nmodel families, reducing instances of indecisive (or \"tie\") outcomes by up to\n54% compared to a random sample when focusing on the top-20 percentile of\nprioritized instances. This potential reduction in required human effort\npositions our approach as a valuable strategy in future large language model\nevaluations.", "published": "2023-10-22 21:48:51", "link": "http://arxiv.org/abs/2310.14424v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Text generation for dataset augmentation in security classification\n  tasks", "abstract": "Security classifiers, designed to detect malicious content in computer\nsystems and communications, can underperform when provided with insufficient\ntraining data. In the security domain, it is often easy to find samples of the\nnegative (benign) class, and challenging to find enough samples of the positive\n(malicious) class to train an effective classifier. This study evaluates the\napplication of natural language text generators to fill this data gap in\nmultiple security-related text classification tasks. We describe a variety of\npreviously-unexamined language-model fine-tuning approaches for this purpose\nand consider in particular the impact of disproportionate class-imbalances in\nthe training set. Across our evaluation using three state-of-the-art\nclassifiers designed for offensive language detection, review fraud detection,\nand SMS spam detection, we find that models trained with GPT-3 data\naugmentation strategies outperform both models trained without augmentation and\nmodels trained using basic data augmentation strategies already in common\nusage. In particular, we find substantial benefits for GPT-3 data augmentation\nstrategies in situations with severe limitations on known positive-class\nsamples.", "published": "2023-10-22 22:25:14", "link": "http://arxiv.org/abs/2310.14429v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Retrieval-Augmented Chain-of-Thought in Semi-structured Domains", "abstract": "Applying existing question answering (QA) systems to specialized domains like\nlaw and finance presents challenges that necessitate domain expertise. Although\nlarge language models (LLMs) have shown impressive language comprehension and\nin-context learning capabilities, their inability to handle very long\ninputs/contexts is well known. Tasks specific to these domains need significant\nbackground knowledge, leading to contexts that can often exceed the maximum\nlength that existing LLMs can process. This study explores leveraging the\nsemi-structured nature of legal and financial data to efficiently retrieve\nrelevant context, enabling the use of LLMs for domain-specialized QA. The\nresulting system outperforms contemporary models and also provides useful\nexplanations for the answers, encouraging the integration of LLMs into legal\nand financial NLP systems for future research.", "published": "2023-10-22 22:45:14", "link": "http://arxiv.org/abs/2310.14435v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "UrbanCLIP: Learning Text-enhanced Urban Region Profiling with\n  Contrastive Language-Image Pretraining from the Web", "abstract": "Urban region profiling from web-sourced data is of utmost importance for\nurban planning and sustainable development. We are witnessing a rising trend of\nLLMs for various fields, especially dealing with multi-modal data research such\nas vision-language learning, where the text modality serves as a supplement\ninformation for the image. Since textual modality has never been introduced\ninto modality combinations in urban region profiling, we aim to answer two\nfundamental questions in this paper: i) Can textual modality enhance urban\nregion profiling? ii) and if so, in what ways and with regard to which aspects?\nTo answer the questions, we leverage the power of Large Language Models (LLMs)\nand introduce the first-ever LLM-enhanced framework that integrates the\nknowledge of textual modality into urban imagery profiling, named LLM-enhanced\nUrban Region Profiling with Contrastive Language-Image Pretraining (UrbanCLIP).\nSpecifically, it first generates a detailed textual description for each\nsatellite image by an open-source Image-to-Text LLM. Then, the model is trained\non the image-text pairs, seamlessly unifying natural language supervision for\nurban visual representation learning, jointly with contrastive loss and\nlanguage modeling loss. Results on predicting three urban indicators in four\nmajor Chinese metropolises demonstrate its superior performance, with an\naverage improvement of 6.1% on R^2 compared to the state-of-the-art methods.\nOur code and the image-language dataset will be released upon paper\nnotification.", "published": "2023-10-22 02:32:53", "link": "http://arxiv.org/abs/2310.18340v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CXR-LLAVA: a multimodal large language model for interpreting chest\n  X-ray images", "abstract": "Purpose: This study aimed to develop an open-source multimodal large language\nmodel (CXR-LLAVA) for interpreting chest X-ray images (CXRs), leveraging recent\nadvances in large language models (LLMs) to potentially replicate the image\ninterpretation skills of human radiologists Materials and Methods: For\ntraining, we collected 592,580 publicly available CXRs, of which 374,881 had\nlabels for certain radiographic abnormalities (Dataset 1) and 217,699 provided\nfree-text radiology reports (Dataset 2). After pre-training a vision\ntransformer with Dataset 1, we integrated it with an LLM influenced by the\nLLAVA network. Then, the model was fine-tuned, primarily using Dataset 2. The\nmodel's diagnostic performance for major pathological findings was evaluated,\nalong with the acceptability of radiologic reports by human radiologists, to\ngauge its potential for autonomous reporting. Results: The model demonstrated\nimpressive performance in test sets, achieving an average F1 score of 0.81 for\nsix major pathological findings in the MIMIC internal test set and 0.62 for\nseven major pathological findings in the external test set. The model's F1\nscores surpassed those of GPT-4-vision and Gemini-Pro-Vision in both test sets.\nIn human radiologist evaluations of the external test set, the model achieved a\n72.7% success rate in autonomous reporting, slightly below the 84.0% rate of\nground truth reports. Conclusion: This study highlights the significant\npotential of multimodal LLMs for CXR interpretation, while also acknowledging\nthe performance limitations. Despite these challenges, we believe that making\nour model open-source will catalyze further research, expanding its\neffectiveness and applicability in various clinical contexts. CXR-LLAVA is\navailable at https://github.com/ECOFRI/CXR_LLAVA.", "published": "2023-10-22 06:22:37", "link": "http://arxiv.org/abs/2310.18341v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MIRACLE: Towards Personalized Dialogue Generation with Latent-Space\n  Multiple Personal Attribute Control", "abstract": "Personalized dialogue systems aim to endow the chatbot agent with more\nanthropomorphic traits for human-like interactions. Previous approaches have\nexplored explicitly user profile modeling using text descriptions, implicit\nderivation of user embeddings, or utilizing handicraft prompts for ChatGPT-like\nmodels. However, textual personas are limited in describing multi-faceted\nattributes (\\emph{e.g.}, \\emph{language style, inner character nuances}),\nimplicit embedding suffers from personality sparsity, and handicraft prompts\nlack fine-grained and stable controllability. Hence, these approaches may\nstruggle with complex personalized dialogue generation tasks that require\ngenerating controllable responses with multiple personal attributes. To this\nend, we propose \\textbf{\\textsc{Miracle}}, a novel personalized dialogue\ngeneration method through \\textbf{M}ult\\textbf{I}ple Pe\\textbf{R}sonal\n\\textbf{A}ttributes \\textbf{C}ontrol within \\textbf{L}atent-Space\n\\textbf{E}nergy-based Models. ttributes \\textbf{C}ontrol within\n\\textbf{L}atent-Space \\textbf{E}nergy-based Models. Specifically, our approach\nfirst disentangles complex personality into multi-faceted attributes.\nSubsequently, we employ a conditional variational auto-encoder to align with\nthe dense personalized responses within a latent joint attribute space. We have\nalso tailored a dedicated energy function and customized the ordinary\ndifferential equations sampling method to offer flexible attribute composition\nand precise attribute control. Extensive experiments demonstrate that\n\\textsc{Miracle} outperforms several strong baselines in terms of personality\ncontrollability and response generation quality. Our dataset and code are\navailable at \\url{https://github.com/LZY-the-boys/MIRACLE}", "published": "2023-10-22 08:44:26", "link": "http://arxiv.org/abs/2310.18342v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Survey on Semantic Processing Techniques", "abstract": "Semantic processing is a fundamental research domain in computational\nlinguistics. In the era of powerful pre-trained language models and large\nlanguage models, the advancement of research in this domain appears to be\ndecelerating. However, the study of semantics is multi-dimensional in\nlinguistics. The research depth and breadth of computational semantic\nprocessing can be largely improved with new technologies. In this survey, we\nanalyzed five semantic processing tasks, e.g., word sense disambiguation,\nanaphora resolution, named entity recognition, concept extraction, and\nsubjectivity detection. We study relevant theoretical research in these fields,\nadvanced methods, and downstream applications. We connect the surveyed tasks\nwith downstream applications because this may inspire future scholars to fuse\nthese low-level semantic processing tasks with high-level natural language\nprocessing tasks. The review of theoretical research may also inspire new tasks\nand technologies in the semantic processing domain. Finally, we compare the\ndifferent semantic processing techniques and summarize their technical trends,\napplication trends, and future directions.", "published": "2023-10-22 15:09:51", "link": "http://arxiv.org/abs/2310.18345v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RSM-NLP at BLP-2023 Task 2: Bangla Sentiment Analysis using Weighted and\n  Majority Voted Fine-Tuned Transformers", "abstract": "This paper describes our approach to submissions made at Shared Task 2 at BLP\nWorkshop - Sentiment Analysis of Bangla Social Media Posts. Sentiment Analysis\nis an action research area in the digital age. With the rapid and constant\ngrowth of online social media sites and services and the increasing amount of\ntextual data, the application of automatic Sentiment Analysis is on the rise.\nHowever, most of the research in this domain is based on the English language.\nDespite being the world's sixth most widely spoken language, little work has\nbeen done in Bangla. This task aims to promote work on Bangla Sentiment\nAnalysis while identifying the polarity of social media content by determining\nwhether the sentiment expressed in the text is Positive, Negative, or Neutral.\nOur approach consists of experimenting and finetuning various multilingual and\npre-trained BERT-based models on our downstream tasks and using a Majority\nVoting and Weighted ensemble model that outperforms individual baseline model\nscores. Our system scored 0.711 for the multiclass classification task and\nscored 10th place among the participants on the leaderboard for the shared\ntask. Our code is available at https://github.com/ptnv-s/RSM-NLP-BLP-Task2 .", "published": "2023-10-22 10:55:56", "link": "http://arxiv.org/abs/2310.14261v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Conversational Speech Recognition by Learning Audio-textual Cross-modal\n  Contextual Representation", "abstract": "Automatic Speech Recognition (ASR) in conversational settings presents unique\nchallenges, including extracting relevant contextual information from previous\nconversational turns. Due to irrelevant content, error propagation, and\nredundancy, existing methods struggle to extract longer and more effective\ncontexts. To address this issue, we introduce a novel conversational ASR\nsystem, extending the Conformer encoder-decoder model with cross-modal\nconversational representation. Our approach leverages a cross-modal extractor\nthat combines pre-trained speech and text models through a specialized encoder\nand a modal-level mask input. This enables the extraction of richer historical\nspeech context without explicit error propagation. We also incorporate\nconditional latent variational modules to learn conversational level attributes\nsuch as role preference and topic coherence. By introducing both cross-modal\nand conversational representations into the decoder, our model retains context\nover longer sentences without information loss, achieving relative accuracy\nimprovements of 8.8% and 23% on Mandarin conversation datasets HKUST and\nMagicData-RAMC, respectively, compared to the standard Conformer model.", "published": "2023-10-22 11:57:33", "link": "http://arxiv.org/abs/2310.14278v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "NERetrieve: Dataset for Next Generation Named Entity Recognition and\n  Retrieval", "abstract": "Recognizing entities in texts is a central need in many information-seeking\nscenarios, and indeed, Named Entity Recognition (NER) is arguably one of the\nmost successful examples of a widely adopted NLP task and corresponding NLP\ntechnology. Recent advances in large language models (LLMs) appear to provide\neffective solutions (also) for NER tasks that were traditionally handled with\ndedicated models, often matching or surpassing the abilities of the dedicated\nmodels. Should NER be considered a solved problem? We argue to the contrary:\nthe capabilities provided by LLMs are not the end of NER research, but rather\nan exciting beginning. They allow taking NER to the next level, tackling\nincreasingly more useful, and increasingly more challenging, variants. We\npresent three variants of the NER task, together with a dataset to support\nthem. The first is a move towards more fine-grained -- and intersectional --\nentity types. The second is a move towards zero-shot recognition and extraction\nof these fine-grained types based on entity-type labels. The third, and most\nchallenging, is the move from the recognition setup to a novel retrieval setup,\nwhere the query is a zero-shot entity type, and the expected result is all the\nsentences from a large, pre-indexed corpus that contain entities of these\ntypes, and their corresponding spans. We show that all of these are far from\nbeing solved. We provide a large, silver-annotated corpus of 4 million\nparagraphs covering 500 entity types, to facilitate research towards all of\nthese three goals.", "published": "2023-10-22 12:23:00", "link": "http://arxiv.org/abs/2310.14282v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Computer Vision Datasets and Models Exhibit Cultural and Linguistic\n  Diversity in Perception", "abstract": "Computer vision often treats human perception as homogeneous: an implicit\nassumption that visual stimuli are perceived similarly by everyone. This\nassumption is reflected in the way researchers collect datasets and train\nvision models. By contrast, literature in cross-cultural psychology and\nlinguistics has provided evidence that people from different cultural\nbackgrounds observe vastly different concepts even when viewing the same visual\nstimuli. In this paper, we study how these differences manifest themselves in\nvision-language datasets and models, using language as a proxy for culture. By\ncomparing textual descriptions generated across 7 languages for the same\nimages, we find significant differences in the semantic content and linguistic\nexpression. When datasets are multilingual as opposed to monolingual,\ndescriptions have higher semantic coverage on average, where coverage is\nmeasured using scene graphs, model embeddings, and linguistic taxonomies. For\nexample, multilingual descriptions have on average 29.9% more objects, 24.5%\nmore relations, and 46.0% more attributes than a set of monolingual captions.\nWhen prompted to describe images in different languages, popular models (e.g.\nLLaVA) inherit this bias and describe different parts of the image. Moreover,\nfinetuning models on captions from one language performs best on corresponding\ntest data from that language, while finetuning on multilingual data performs\nconsistently well across all test data compositions. Our work points towards\nthe need to account for and embrace the diversity of human perception in the\ncomputer vision community.", "published": "2023-10-22 16:51:42", "link": "http://arxiv.org/abs/2310.14356v4", "categories": ["cs.CV", "cs.CL", "cs.CY", "cs.HC"], "primary_category": "cs.CV"}
{"title": "Right, No Matter Why: AI Fact-checking and AI Authority in\n  Health-related Inquiry Settings", "abstract": "Previous research on expert advice-taking shows that humans exhibit two\ncontradictory behaviors: on the one hand, people tend to overvalue their own\nopinions undervaluing the expert opinion, and on the other, people often defer\nto other people's advice even if the advice itself is rather obviously wrong.\nIn our study, we conduct an exploratory evaluation of users' AI-advice\naccepting behavior when evaluating the truthfulness of a health-related\nstatement in different \"advice quality\" settings. We find that even feedback\nthat is confined to just stating that \"the AI thinks that the statement is\nfalse/true\" results in more than half of people moving their statement veracity\nassessment towards the AI suggestion. The different types of advice given\ninfluence the acceptance rates, but the sheer effect of getting a suggestion is\noften bigger than the suggestion-type effect.", "published": "2023-10-22 16:59:31", "link": "http://arxiv.org/abs/2310.14358v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "TATA: Stance Detection via Topic-Agnostic and Topic-Aware Embeddings", "abstract": "Stance detection is important for understanding different attitudes and\nbeliefs on the Internet. However, given that a passage's stance toward a given\ntopic is often highly dependent on that topic, building a stance detection\nmodel that generalizes to unseen topics is difficult. In this work, we propose\nusing contrastive learning as well as an unlabeled dataset of news articles\nthat cover a variety of different topics to train topic-agnostic/TAG and\ntopic-aware/TAW embeddings for use in downstream stance detection. Combining\nthese embeddings in our full TATA model, we achieve state-of-the-art\nperformance across several public stance detection datasets (0.771 $F_1$-score\non the Zero-shot VAST dataset). We release our code and data at\nhttps://github.com/hanshanley/tata.", "published": "2023-10-22 23:23:44", "link": "http://arxiv.org/abs/2310.14450v3", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Chainpoll: A high efficacy method for LLM hallucination detection", "abstract": "Large language models (LLMs) have experienced notable advancements in\ngenerating coherent and contextually relevant responses. However,\nhallucinations - incorrect or unfounded claims - are still prevalent, prompting\nthe creation of automated metrics to detect these in LLM outputs. Our\ncontributions include: introducing ChainPoll, an innovative hallucination\ndetection method that excels compared to its counterparts, and unveiling\nRealHall, a refined collection of benchmark datasets to assess hallucination\ndetection metrics from recent studies. While creating RealHall, we assessed\ntasks and datasets from previous hallucination detection studies and observed\nthat many are not suitable for the potent LLMs currently in use. Overcoming\nthis, we opted for four datasets challenging for modern LLMs and pertinent to\nreal-world scenarios. Using RealHall, we conducted a comprehensive comparison\nof ChainPoll with numerous hallucination metrics from recent studies. Our\nfindings indicate that ChainPoll outperforms in all RealHall benchmarks,\nachieving an overall AUROC of 0.781. This surpasses the next best theoretical\nmethod by 11% and exceeds industry standards by over 23%. Additionally,\nChainPoll is cost-effective and offers greater transparency than other metrics.\nWe introduce two novel metrics to assess LLM hallucinations: Adherence and\nCorrectness. Adherence is relevant to Retrieval Augmented Generation workflows,\nevaluating an LLM's analytical capabilities within given documents and\ncontexts. In contrast, Correctness identifies logical and reasoning errors.", "published": "2023-10-22 14:45:14", "link": "http://arxiv.org/abs/2310.18344v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ITEm: Unsupervised Image-Text Embedding Learning for eCommerce", "abstract": "Product embedding serves as a cornerstone for a wide range of applications in\neCommerce. The product embedding learned from multiple modalities shows\nsignificant improvement over that from a single modality, since different\nmodalities provide complementary information. However, some modalities are more\ninformatively dominant than others. How to teach a model to learn embedding\nfrom different modalities without neglecting information from the less dominant\nmodality is challenging. We present an image-text embedding model (ITEm), an\nunsupervised learning method that is designed to better attend to image and\ntext modalities. We extend BERT by (1) learning an embedding from text and\nimage without knowing the regions of interest; (2) training a global\nrepresentation to predict masked words and to construct masked image patches\nwithout their individual representations. We evaluate the pre-trained ITEm on\ntwo tasks: the search for extremely similar products and the prediction of\nproduct categories, showing substantial gains compared to strong baseline\nmodels.", "published": "2023-10-22 15:39:44", "link": "http://arxiv.org/abs/2311.02084v2", "categories": ["cs.CV", "cs.CL", "cs.IR"], "primary_category": "cs.CV"}
{"title": "LUNA: A Model-Based Universal Analysis Framework for Large Language\n  Models", "abstract": "Over the past decade, Artificial Intelligence (AI) has had great success\nrecently and is being used in a wide range of academic and industrial fields.\nMore recently, LLMs have made rapid advancements that have propelled AI to a\nnew level, enabling even more diverse applications and industrial domains with\nintelligence, particularly in areas like software engineering and natural\nlanguage processing. Nevertheless, a number of emerging trustworthiness\nconcerns and issues exhibited in LLMs have already recently received much\nattention, without properly solving which the widespread adoption of LLMs could\nbe greatly hindered in practice. The distinctive characteristics of LLMs, such\nas the self-attention mechanism, extremely large model scale, and\nautoregressive generation schema, differ from classic AI software based on CNNs\nand RNNs and present new challenges for quality analysis. Up to the present, it\nstill lacks universal and systematic analysis techniques for LLMs despite the\nurgent industrial demand. Towards bridging this gap, we initiate an early\nexploratory study and propose a universal analysis framework for LLMs, LUNA,\ndesigned to be general and extensible, to enable versatile analysis of LLMs\nfrom multiple quality perspectives in a human-interpretable manner. In\nparticular, we first leverage the data from desired trustworthiness\nperspectives to construct an abstract model as an auxiliary analysis asset,\nwhich is empowered by various abstract model construction methods. To assess\nthe quality of the abstract model, we collect and define a number of evaluation\nmetrics, aiming at both abstract model level and the semantics level. Then, the\nsemantics, which is the degree of satisfaction of the LLM w.r.t. the\ntrustworthiness perspective, is bound to and enriches the abstract model with\nsemantics, which enables more detailed analysis applications for diverse\npurposes.", "published": "2023-10-22 07:26:21", "link": "http://arxiv.org/abs/2310.14211v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR", "cs.SE"], "primary_category": "cs.LG"}
{"title": "Modeling Intrapersonal and Interpersonal Influences for Automatic\n  Estimation of Therapist Empathy in Counseling Conversation", "abstract": "Counseling is usually conducted through spoken conversation between a\ntherapist and a client. The empathy level of therapist is a key indicator of\noutcomes. Presuming that therapist's empathy expression is shaped by their past\nbehavior and their perception of the client's behavior, we propose a model to\nestimate the therapist empathy by considering both intrapersonal and\ninterpersonal influences. These dynamic influences are captured by applying an\nattention mechanism to the therapist turn and the historical turns of both\ntherapist and client. Our findings suggest that the integration of dynamic\ninfluences enhances empathy level estimation. The influence-derived embedding\nshould constitute a minor portion of the target turn representation for optimal\nempathy estimation. The client's turns (interpersonal influence) slightly\nsurpass the therapist's own turns (intrapersonal influence) in empathy\nestimation effectiveness. It is noted that concentrating exclusively on recent\nhistorical turns can significantly impact the estimation of therapist empathy.", "published": "2023-10-22 05:00:04", "link": "http://arxiv.org/abs/2310.14178v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "A Study on Prosodic Entrainment in Relation to Therapist Empathy in\n  Counseling Conversation", "abstract": "Counseling is carried out as spoken conversation between a therapist and a\nclient. The empathy level expressed by the therapist is considered an important\nindex of the quality of counseling and often assessed by an observer or the\nclient. This research investigates the entrainment of speech prosody in\nrelation to subjectively rated empathy. Experimental results show that the\nentrainment of intensity is more influential to empathy observation than that\nof pitch or speech rate in client-therapist interaction. The observer and the\nclient have different perceptions of therapist empathy with the same entrained\nphenomena in pitch and intensity. The client's intention to make adjustment on\npitch variation and intensity of speech is considered an indicator of the\nclient's perception of counseling quality.", "published": "2023-10-22 05:09:04", "link": "http://arxiv.org/abs/2310.14181v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "First-Shot Unsupervised Anomalous Sound Detection With Unknown Anomalies\n  Estimated by Metadata-Assisted Audio Generation", "abstract": "First-shot (FS) unsupervised anomalous sound detection (ASD) is a brand-new\ntask introduced in DCASE 2023 Challenge Task 2, where the anomalous sounds for\nthe target machine types are unseen in training. Existing methods often rely on\nthe availability of normal and abnormal sound data from the target machines.\nHowever, due to the lack of anomalous sound data for the target machine types,\nit becomes challenging when adapting the existing ASD methods to the first-shot\ntask. In this paper, we propose a new framework for the first-shot unsupervised\nASD, where metadata-assisted audio generation is used to estimate unknown\nanomalies, by utilising the available machine information (i.e., metadata and\nsound data) to fine-tune a text-to-audio generation model for generating the\nanomalous sounds that contain unique acoustic characteristics accounting for\neach different machine type. We then use the method of Time-Weighted Frequency\ndomain audio Representation with Gaussian Mixture Model (TWFR-GMM) as the\nbackbone to achieve the first-shot unsupervised ASD. Our proposed FS-TWFR-GMM\nmethod achieves competitive performance amongst top systems in DCASE 2023\nChallenge Task 2, while requiring only 1% model parameters for detection, as\nvalidated in our experiments.", "published": "2023-10-22 04:15:59", "link": "http://arxiv.org/abs/2310.14173v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Diffusion-Based Adversarial Purification for Speaker Verification", "abstract": "Recently, automatic speaker verification (ASV) based on deep learning is\neasily contaminated by adversarial attacks, which is a new type of attack that\ninjects imperceptible perturbations to audio signals so as to make ASV produce\nwrong decisions. This poses a significant threat to the security and\nreliability of ASV systems. To address this issue, we propose a Diffusion-Based\nAdversarial Purification (DAP) method that enhances the robustness of ASV\nsystems against such adversarial attacks. Our method leverages a conditional\ndenoising diffusion probabilistic model to effectively purify the adversarial\nexamples and mitigate the impact of perturbations. DAP first introduces\ncontrolled noise into adversarial examples, and then performs a reverse\ndenoising process to reconstruct clean audio. Experimental results demonstrate\nthe efficacy of the proposed DAP in enhancing the security of ASV and meanwhile\nminimizing the distortion of the purified audio signals.", "published": "2023-10-22 11:34:30", "link": "http://arxiv.org/abs/2310.14270v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MFCC-GAN Codec: A New AI-based Audio Coding", "abstract": "In this paper, we proposed AI-based audio coding using MFCC features in an\nadversarial setting. We combined a conventional encoder with an adversarial\nlearning decoder to better reconstruct the original waveform. Since GAN gives\nimplicit density estimation, therefore, such models are less prone to\noverfitting. We compared our work with five well-known codecs namely AAC, AC3,\nOpus, Vorbis, and Speex, performing on bitrates from 2kbps to 128kbps.\nMFCCGAN_36k achieved the state-of-the-art result in terms of SNR despite a\nlower bitrate in comparison to AC3_128k, AAC_112k, Vorbis_48k, Opus_48k, and\nSpeex_48K. On the other hand, MFCCGAN_13k also achieved high SNR=27 which is\nequal to that of AC3_128k, and AAC_112k while having a significantly lower\nbitrate (13 kbps). MFCCGAN_36k achieved higher NISQA-MOS results compared to\nAAC_48k while having a 20% lower bitrate. Furthermore, MFCCGAN_13k obtained\nNISQAMOS= 3.9 which is much higher than AAC_24k, AAC_32k, AC3_32k, and AAC_48k.\nFor future work, we finally suggest adopting loss functions optimizing\nintelligibility and perceptual metrics in the MFCCGAN structure to improve\nquality and intelligibility simultaneously.", "published": "2023-10-22 13:44:31", "link": "http://arxiv.org/abs/2310.14300v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "An overview of text-to-speech systems and media applications", "abstract": "Producing synthetic voice, similar to human-like sound, is an emerging\nnovelty of modern interactive media systems. Text-To-Speech (TTS) systems try\nto generate synthetic and authentic voices via text input. Besides, well known\nand familiar dubbing, announcing and narrating voices, as valuable possessions\nof any media organization, can be kept forever by utilizing TTS and Voice\nConversion (VC) algorithms . The emergence of deep learning approaches has made\nsuch TTS systems more accurate and accessible. To understand TTS systems\nbetter, this paper investigates the key components of such systems including\ntext analysis, acoustic modelling and vocoding. The paper then provides details\nof important state-of-the-art TTS systems based on deep learning. Finally, a\ncomparison is made between recently released systems in term of backbone\narchitecture, type of input and conversion, vocoder used and subjective\nassessment (MOS). Accordingly, Tacotron 2, Transformer TTS, WaveNet and\nFastSpeech 1 are among the most successful TTS systems ever released. In the\ndiscussion section, some suggestions are made to develop a TTS system with\nregard to the intended application.", "published": "2023-10-22 13:52:06", "link": "http://arxiv.org/abs/2310.14301v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
