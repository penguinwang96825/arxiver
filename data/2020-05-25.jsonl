{"title": "Sentiment Analysis: Automatically Detecting Valence, Emotions, and Other\n  Affectual States from Text", "abstract": "Recent advances in machine learning have led to computer systems that are\nhuman-like in behaviour. Sentiment analysis, the automatic determination of\nemotions in text, is allowing us to capitalize on substantial previously\nunattainable opportunities in commerce, public health, government policy,\nsocial sciences, and art. Further, analysis of emotions in text, from news to\nsocial media posts, is improving our understanding of not just how people\nconvey emotions through language but also how emotions shape our behaviour.\nThis article presents a sweeping overview of sentiment analysis research that\nincludes: the origins of the field, the rich landscape of tasks, challenges, a\nsurvey of the methods and resources used, and applications. We also discuss\ndiscuss how, without careful fore-thought, sentiment analysis has the potential\nfor harmful outcomes. We outline the latest lines of research in pursuit of\nfairness in sentiment analysis.", "published": "2020-05-25 01:37:31", "link": "http://arxiv.org/abs/2005.11882v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pointwise Paraphrase Appraisal is Potentially Problematic", "abstract": "The prevailing approach for training and evaluating paraphrase identification\nmodels is constructed as a binary classification problem: the model is given a\npair of sentences, and is judged by how accurately it classifies pairs as\neither paraphrases or non-paraphrases. This pointwise-based evaluation method\ndoes not match well the objective of most real world applications, so the goal\nof our work is to understand how models which perform well under pointwise\nevaluation may fail in practice and find better methods for evaluating\nparaphrase identification models. As a first step towards that goal, we show\nthat although the standard way of fine-tuning BERT for paraphrase\nidentification by pairing two sentences as one sequence results in a model with\nstate-of-the-art performance, that model may perform poorly on simple tasks\nlike identifying pairs with two identical sentences. Moreover, we show that\nthese models may even predict a pair of randomly-selected sentences with higher\nparaphrase score than a pair of identical ones.", "published": "2020-05-25 09:27:31", "link": "http://arxiv.org/abs/2005.11996v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge Graph Simple Question Answering for Unseen Domains", "abstract": "Knowledge graph simple question answering (KGSQA), in its standard form, does\nnot take into account that human-curated question answering training data only\ncover a small subset of the relations that exist in a Knowledge Graph (KG), or\neven worse, that new domains covering unseen and rather different to existing\ndomains relations are added to the KG. In this work, we study KGSQA in a\npreviously unstudied setting where new, unseen domains are added during test\ntime. In this setting, question-answer pairs of the new domain do not appear\nduring training, thus making the task more challenging. We propose a\ndata-centric domain adaptation framework that consists of a KGSQA system that\nis applicable to new domains, and a sequence to sequence question generation\nmethod that automatically generates question-answer pairs for the new domain.\nSince the effectiveness of question generation for KGSQA can be restricted by\nthe limited lexical variety of the generated questions, we use distant\nsupervision to extract a set of keywords that express each relation of the\nunseen domain and incorporate those in the question generation method.\nExperimental results demonstrate that our framework significantly improves over\nzero-shot baselines and is robust across domains.", "published": "2020-05-25 11:34:54", "link": "http://arxiv.org/abs/2005.12040v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Happy Are Those Who Grade without Seeing: A Multi-Task Learning Approach\n  to Grade Essays Using Gaze Behaviour", "abstract": "The gaze behaviour of a reader is helpful in solving several NLP tasks such\nas automatic essay grading. However, collecting gaze behaviour from readers is\ncostly in terms of time and money. In this paper, we propose a way to improve\nautomatic essay grading using gaze behaviour, which is learnt at run time using\na multi-task learning framework. To demonstrate the efficacy of this multi-task\nlearning based approach to automatic essay grading, we collect gaze behaviour\nfor 48 essays across 4 essay sets, and learn gaze behaviour for the rest of the\nessays, numbering over 7000 essays. Using the learnt gaze behaviour, we can\nachieve a statistically significant improvement in performance over the\nstate-of-the-art system for the essay sets where we have gaze data. We also\nachieve a statistically significant improvement for 4 other essay sets,\nnumbering about 6000 essays, where we have no gaze behaviour data available.\nOur approach establishes that learning gaze behaviour improves automatic essay\ngrading.", "published": "2020-05-25 12:38:47", "link": "http://arxiv.org/abs/2005.12078v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Stable Style Transformer: Delete and Generate Approach with\n  Encoder-Decoder for Text Style Transfer", "abstract": "Text style transfer is the task that generates a sentence by preserving the\ncontent of the input sentence and transferring the style. Most existing studies\nare progressing on non-parallel datasets because parallel datasets are limited\nand hard to construct. In this work, we introduce a method that follows two\nstages in non-parallel datasets. The first stage is to delete attribute markers\nof a sentence directly through a classifier. The second stage is to generate a\ntransferred sentence by combining the content tokens and the target style. We\nexperiment on two benchmark datasets and evaluate context, style, fluency, and\nsemantic. It is difficult to select the best system using only these automatic\nmetrics, but it is possible to select stable systems. We consider only robust\nsystems in all automatic evaluation metrics to be the minimum conditions that\ncan be used in real applications. Many previous systems are difficult to use in\ncertain situations because performance is significantly lower in several\nevaluation metrics. However, our system is stable in all automatic evaluation\nmetrics and has results comparable to other models. Also, we compare the\nperformance results of our system and the unstable system through human\nevaluation. Our code and data are available at the link\n(https://github.com/rungjoo/Stable-Style-Transformer).", "published": "2020-05-25 13:04:54", "link": "http://arxiv.org/abs/2005.12086v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "K\u00f8psala: Transition-Based Graph Parsing via Efficient Training and\n  Effective Encoding", "abstract": "We present K{\\o}psala, the Copenhagen-Uppsala system for the Enhanced\nUniversal Dependencies Shared Task at IWPT 2020. Our system is a pipeline\nconsisting of off-the-shelf models for everything but enhanced graph parsing,\nand for the latter, a transition-based graph parser adapted from Che et al.\n(2019). We train a single enhanced parser model per language, using gold\nsentence splitting and tokenization for training, and rely only on tokenized\nsurface forms and multilingual BERT for encoding. While a bug introduced just\nbefore submission resulted in a severe drop in precision, its post-submission\nfix would bring us to 4th place in the official ranking, according to average\nELAS. Our parser demonstrates that a unified pipeline is effective for both\nMeaning Representation Parsing and Enhanced Universal Dependencies.", "published": "2020-05-25 13:17:09", "link": "http://arxiv.org/abs/2005.12094v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NILE : Natural Language Inference with Faithful Natural Language\n  Explanations", "abstract": "The recent growth in the popularity and success of deep learning models on\nNLP classification tasks has accompanied the need for generating some form of\nnatural language explanation of the predicted labels. Such generated natural\nlanguage (NL) explanations are expected to be faithful, i.e., they should\ncorrelate well with the model's internal decision making. In this work, we\nfocus on the task of natural language inference (NLI) and address the following\nquestion: can we build NLI systems which produce labels with high accuracy,\nwhile also generating faithful explanations of its decisions? We propose\nNatural-language Inference over Label-specific Explanations (NILE), a novel NLI\nmethod which utilizes auto-generated label-specific NL explanations to produce\nlabels along with its faithful explanation. We demonstrate NILE's effectiveness\nover previously reported methods through automated and human evaluation of the\nproduced labels and explanations. Our evaluation of NILE also supports the\nclaim that accurate systems capable of providing testable explanations of their\ndecisions can be designed. We discuss the faithfulness of NILE's explanations\nin terms of sensitivity of the decisions to the corresponding explanations. We\nargue that explicit evaluation of faithfulness, in addition to label and\nexplanation accuracy, is an important step in evaluating model's explanations.\nFurther, we demonstrate that task-specific probes are necessary to establish\nsuch sensitivity.", "published": "2020-05-25 13:56:03", "link": "http://arxiv.org/abs/2005.12116v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Audio-enriched BERT-based Framework for Spoken Multiple-choice\n  Question Answering", "abstract": "In a spoken multiple-choice question answering (SMCQA) task, given a passage,\na question, and multiple choices all in the form of speech, the machine needs\nto pick the correct choice to answer the question. While the audio could\ncontain useful cues for SMCQA, usually only the auto-transcribed text is\nutilized in system development. Thanks to the large-scaled pre-trained language\nrepresentation models, such as the bidirectional encoder representations from\ntransformers (BERT), systems with only auto-transcribed text can still achieve\na certain level of performance. However, previous studies have evidenced that\nacoustic-level statistics can offset text inaccuracies caused by the automatic\nspeech recognition systems or representation inadequacy lurking in word\nembedding generators, thereby making the SMCQA system robust. Along the line of\nresearch, this study concentrates on designing a BERT-based SMCQA framework,\nwhich not only inherits the advantages of contextualized language\nrepresentations learned by BERT, but integrates the complementary\nacoustic-level information distilled from audio with the text-level\ninformation. Consequently, an audio-enriched BERT-based SMCQA framework is\nproposed. A series of experiments demonstrates remarkable improvements in\naccuracy over selected baselines and SOTA systems on a published Chinese SMCQA\ndataset.", "published": "2020-05-25 14:41:28", "link": "http://arxiv.org/abs/2005.12142v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adapting End-to-End Speech Recognition for Readable Subtitles", "abstract": "Automatic speech recognition (ASR) systems are primarily evaluated on\ntranscription accuracy. However, in some use cases such as subtitling, verbatim\ntranscription would reduce output readability given limited screen size and\nreading time. Therefore, this work focuses on ASR with output compression, a\ntask challenging for supervised approaches due to the scarcity of training\ndata. We first investigate a cascaded system, where an unsupervised compression\nmodel is used to post-edit the transcribed speech. We then compare several\nmethods of end-to-end speech recognition under output length constraints. The\nexperiments show that with limited data far less than needed for training a\nmodel from scratch, we can adapt a Transformer-based ASR model to incorporate\nboth transcription and compression capabilities. Furthermore, the best\nperformance in terms of WER and ROUGE scores is achieved by explicitly modeling\nthe length constraints within the end-to-end ASR system.", "published": "2020-05-25 14:42:26", "link": "http://arxiv.org/abs/2005.12143v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A review of sentiment analysis research in Arabic language", "abstract": "Sentiment analysis is a task of natural language processing which has\nrecently attracted increasing attention. However, sentiment analysis research\nhas mainly been carried out for the English language. Although Arabic is\nramping up as one of the most used languages on the Internet, only a few\nstudies have focused on Arabic sentiment analysis so far. In this paper, we\ncarry out an in-depth qualitative study of the most important research works in\nthis context by presenting limits and strengths of existing approaches. In\nparticular, we survey both approaches that leverage machine translation or\ntransfer learning to adapt English resources to Arabic and approaches that stem\ndirectly from the Arabic language.", "published": "2020-05-25 17:26:02", "link": "http://arxiv.org/abs/2005.12240v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Demoting Racial Bias in Hate Speech Detection", "abstract": "In current hate speech datasets, there exists a high correlation between\nannotators' perceptions of toxicity and signals of African American English\n(AAE). This bias in annotated training data and the tendency of machine\nlearning models to amplify it cause AAE text to often be mislabeled as\nabusive/offensive/hate speech with a high false positive rate by current hate\nspeech classifiers. In this paper, we use adversarial training to mitigate this\nbias, introducing a hate speech classifier that learns to detect toxic\nsentences while demoting confounds corresponding to AAE texts. Experimental\nresults on a hate speech dataset and an AAE dataset suggest that our method is\nable to substantially reduce the false positive rate for AAE text while only\nminimally affecting the performance of hate speech classification.", "published": "2020-05-25 17:43:22", "link": "http://arxiv.org/abs/2005.12246v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Unreasonable Volatility of Neural Machine Translation Models", "abstract": "Recent works have shown that Neural Machine Translation (NMT) models achieve\nimpressive performance, however, questions about understanding the behavior of\nthese models remain unanswered. We investigate the unexpected volatility of NMT\nmodels where the input is semantically and syntactically correct. We discover\nthat with trivial modifications of source sentences, we can identify cases\nwhere \\textit{unexpected changes} happen in the translation and in the worst\ncase lead to mistranslations. This volatile behavior of translating extremely\nsimilar sentences in surprisingly different ways highlights the underlying\ngeneralization problem of current NMT models. We find that both RNN and\nTransformer models display volatile behavior in 26% and 19% of sentence\nvariations, respectively.", "published": "2020-05-25 20:54:23", "link": "http://arxiv.org/abs/2005.12398v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MaintNet: A Collaborative Open-Source Library for Predictive Maintenance\n  Language Resources", "abstract": "Maintenance record logbooks are an emerging text type in NLP. They typically\nconsist of free text documents with many domain specific technical terms,\nabbreviations, as well as non-standard spelling and grammar, which poses\ndifficulties to NLP pipelines trained on standard corpora. Analyzing and\nannotating such documents is of particular importance in the development of\npredictive maintenance systems, which aim to provide operational efficiencies,\nprevent accidents and save lives. In order to facilitate and encourage research\nin this area, we have developed MaintNet, a collaborative open-source library\nof technical and domain-specific language datasets. MaintNet provides novel\nlogbook data from the aviation, automotive, and facilities domains along with\ntools to aid in their (pre-)processing and clustering. Furthermore, it provides\na way to encourage discussion on and sharing of new datasets and tools for\nlogbook data analysis.", "published": "2020-05-25 23:44:19", "link": "http://arxiv.org/abs/2005.12443v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AutoSUM: Automating Feature Extraction and Multi-user Preference\n  Simulation for Entity Summarization", "abstract": "Withthegrowthofknowledgegraphs, entity descriptions are becoming extremely\nlengthy. Entity summarization task, aiming to generate diverse, comprehensive,\nand representative summaries for entities, has received increasing interest\nrecently. In most previous methods, features are usually extracted by the\nhandcrafted templates. Then the feature selection and multi-user preference\nsimulation take place, depending too much on human expertise. In this paper, a\nnovel integration method called AutoSUM is proposed for automatic feature\nextraction and multi-user preference simulation to overcome the drawbacks of\nprevious methods. There are two modules in AutoSUM: extractor and simulator.\nThe extractor module operates automatic feature extraction based on a BiLSTM\nwith a combined input representation including word embeddings and graph\nembeddings. Meanwhile, the simulator module automates multi-user preference\nsimulation based on a well-designed two-phase attention mechanism (i.e.,\nentity-phase attention and user-phase attention). Experimental results\ndemonstrate that AutoSUM produces state-of-the-art performance on two widely\nused datasets (i.e., DBpedia and LinkedMDB) in both F-measure and MAP.", "published": "2020-05-25 02:20:18", "link": "http://arxiv.org/abs/2005.11888v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Deep Learning Models for Automatic Summarization", "abstract": "Text summarization is an NLP task which aims to convert a textual document\ninto a shorter one while keeping as much meaning as possible. This pedagogical\narticle reviews a number of recent Deep Learning architectures that have helped\nto advance research in this field. We will discuss in particular applications\nof pointer networks, hierarchical Transformers and Reinforcement Learning. We\nassume basic knowledge of Seq2Seq architecture and Transformer networks within\nNLP.", "published": "2020-05-25 09:12:37", "link": "http://arxiv.org/abs/2005.11988v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Degree-Aware Alignment for Entities in Tail", "abstract": "Entity alignment (EA) is to discover equivalent entities in knowledge graphs\n(KGs), which bridges heterogeneous sources of information and facilitates the\nintegration of knowledge. Existing EA solutions mainly rely on structural\ninformation to align entities, typically through KG embedding. Nonetheless, in\nreal-life KGs, only a few entities are densely connected to others, and the\nrest majority possess rather sparse neighborhood structure. We refer to the\nlatter as long-tail entities, and observe that such phenomenon arguably limits\nthe use of structural information for EA. To mitigate the issue, we revisit and\ninvestigate into the conventional EA pipeline in pursuit of elegant\nperformance. For pre-alignment, we propose to amplify long-tail entities, which\nare of relatively weak structural information, with entity name information\nthat is generally available (but overlooked) in the form of concatenated power\nmean word embeddings. For alignment, under a novel complementary framework of\nconsolidating structural and name signals, we identify entity's degree as\nimportant guidance to effectively fuse two different sources of information. To\nthis end, a degree-aware co-attention network is conceived, which dynamically\nadjusts the significance of features in a degree-aware manner. For\npost-alignment, we propose to complement original KGs with facts from their\ncounterparts by using confident EA results as anchors via iterative training.\nComprehensive experimental evaluations validate the superiority of our proposed\ntechniques.", "published": "2020-05-25 14:15:49", "link": "http://arxiv.org/abs/2005.12132v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "AMR Quality Rating with a Lightweight CNN", "abstract": "Structured semantic sentence representations such as Abstract Meaning\nRepresentations (AMRs) are potentially useful in various NLP tasks. However,\nthe quality of automatic parses can vary greatly and jeopardizes their\nusefulness. This can be mitigated by models that can accurately rate AMR\nquality in the absence of costly gold data, allowing us to inform downstream\nsystems about an incorporated parse's trustworthiness or select among different\ncandidate parses.\n  In this work, we propose to transfer the AMR graph to the domain of images.\nThis allows us to create a simple convolutional neural network (CNN) that\nimitates a human judge tasked with rating graph quality. Our experiments show\nthat the method can rate quality more accurately than strong baselines, in\nseveral quality dimensions. Moreover, the method proves to be efficient and\nreduces the incurred energy consumption.", "published": "2020-05-25 15:58:00", "link": "http://arxiv.org/abs/2005.12187v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The IMS-CUBoulder System for the SIGMORPHON 2020 Shared Task on\n  Unsupervised Morphological Paradigm Completion", "abstract": "In this paper, we present the systems of the University of Stuttgart IMS and\nthe University of Colorado Boulder (IMS-CUBoulder) for SIGMORPHON 2020 Task 2\non unsupervised morphological paradigm completion (Kann et al., 2020). The task\nconsists of generating the morphological paradigms of a set of lemmas, given\nonly the lemmas themselves and unlabeled text. Our proposed system is a\nmodified version of the baseline introduced together with the task. In\nparticular, we experiment with substituting the inflection generation component\nwith an LSTM sequence-to-sequence model and an LSTM pointer-generator network.\nOur pointer-generator system obtains the best score of all seven submitted\nsystems on average over all languages, and outperforms the official baseline,\nwhich was best overall, on Bulgarian and Kannada.", "published": "2020-05-25 21:23:52", "link": "http://arxiv.org/abs/2005.12411v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An End-to-End Mispronunciation Detection System for L2 English Speech\n  Leveraging Novel Anti-Phone Modeling", "abstract": "Mispronunciation detection and diagnosis (MDD) is a core component of\ncomputer-assisted pronunciation training (CAPT). Most of the existing MDD\napproaches focus on dealing with categorical errors (viz. one canonical phone\nis substituted by another one, aside from those mispronunciations caused by\ndeletions or insertions). However, accurate detection and diagnosis of\nnon-categorial or distortion errors (viz. approximating L2 phones with L1\n(first-language) phones, or erroneous pronunciations in between) still seems\nout of reach. In view of this, we propose to conduct MDD with a novel end-\nto-end automatic speech recognition (E2E-based ASR) approach. In particular, we\nexpand the original L2 phone set with their corresponding anti-phone set,\nmaking the E2E-based MDD approach have a better capability to take in both\ncategorical and non-categorial mispronunciations, aiming to provide better\nmispronunciation detection and diagnosis feedback. Furthermore, a novel\ntransfer-learning paradigm is devised to obtain the initial model estimate of\nthe E2E-based MDD system without resource to any phonological rules. Extensive\nsets of experimental results on the L2-ARCTIC dataset show that our best system\ncan outperform the existing E2E baseline system and pronunciation scoring based\nmethod (GOP) in terms of the F1-score, by 11.05% and 27.71%, respectively.", "published": "2020-05-25 07:27:47", "link": "http://arxiv.org/abs/2005.11950v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "NENET: An Edge Learnable Network for Link Prediction in Scene Text", "abstract": "Text detection in scenes based on deep neural networks have shown promising\nresults. Instead of using word bounding box regression, recent state-of-the-art\nmethods have started focusing on character bounding box and pixel-level\nprediction. This necessitates the need to link adjacent characters, which we\npropose in this paper using a novel Graph Neural Network (GNN) architecture\nthat allows us to learn both node and edge features as opposed to only the node\nfeatures under the typical GNN. The main advantage of using GNN for link\nprediction lies in its ability to connect characters which are spatially\nseparated and have an arbitrary orientation. We show our concept on the well\nknown SynthText dataset, achieving top results as compared to state-of-the-art\nmethods.", "published": "2020-05-25 14:47:16", "link": "http://arxiv.org/abs/2005.12147v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Incidental Supervision: Moving beyond Supervised Learning", "abstract": "Machine Learning and Inference methods have become ubiquitous in our attempt\nto induce more abstract representations of natural language text, visual\nscenes, and other messy, naturally occurring data, and support decisions that\ndepend on it. However, learning models for these tasks is difficult partly\nbecause generating the necessary supervision signals for it is costly and does\nnot scale. This paper describes several learning paradigms that are designed to\nalleviate the supervision bottleneck. It will illustrate their benefit in the\ncontext of multiple problems, all pertaining to inducing various levels of\nsemantic representations from text.", "published": "2020-05-25 18:44:53", "link": "http://arxiv.org/abs/2005.12339v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "FT Speech: Danish Parliament Speech Corpus", "abstract": "This paper introduces FT Speech, a new speech corpus created from the\nrecorded meetings of the Danish Parliament, otherwise known as the Folketing\n(FT). The corpus contains over 1,800 hours of transcribed speech by a total of\n434 speakers. It is significantly larger in duration, vocabulary, and amount of\nspontaneous speech than the existing public speech corpora for Danish, which\nare largely limited to read-aloud and dictation data. We outline design\nconsiderations, including the preprocessing methods and the alignment\nprocedure. To evaluate the quality of the corpus, we train automatic speech\nrecognition systems on the new resource and compare them to the systems trained\non the Danish part of Spr\\r{a}kbanken, the largest public ASR corpus for Danish\nto date. Our baseline results show that we achieve a 14.01 WER on the new\ncorpus. A combination of FT Speech with in-domain language data provides\ncomparable results to models trained specifically on Spr\\r{a}kbanken, showing\nthat FT Speech transfers well to this data set. Interestingly, our results\ndemonstrate that the opposite is not the case. This shows that FT Speech\nprovides a valuable resource for promoting research on Danish ASR with more\nspontaneous speech.", "published": "2020-05-25 19:51:18", "link": "http://arxiv.org/abs/2005.12368v2", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Racism is a Virus: Anti-Asian Hate and Counterspeech in Social Media\n  during the COVID-19 Crisis", "abstract": "The spread of COVID-19 has sparked racism and hate on social media targeted\ntowards Asian communities. However, little is known about how racial hate\nspreads during a pandemic and the role of counterspeech in mitigating this\nspread. In this work, we study the evolution and spread of anti-Asian hate\nspeech through the lens of Twitter. We create COVID-HATE, the largest dataset\nof anti-Asian hate and counterspeech spanning 14 months, containing over 206\nmillion tweets, and a social network with over 127 million nodes. By creating a\nnovel hand-labeled dataset of 3,355 tweets, we train a text classifier to\nidentify hate and counterspeech tweets that achieves an average macro-F1 score\nof 0.832. Using this dataset, we conduct longitudinal analysis of tweets and\nusers. Analysis of the social network reveals that hateful and counterspeech\nusers interact and engage extensively with one another, instead of living in\nisolated polarized communities. We find that nodes were highly likely to become\nhateful after being exposed to hateful content. Notably, counterspeech messages\nmay discourage users from turning hateful, potentially suggesting a solution to\ncurb hate on web and social media platforms. Data and code is at\nhttp://claws.cc.gatech.edu/covid.", "published": "2020-05-25 21:58:09", "link": "http://arxiv.org/abs/2005.12423v2", "categories": ["cs.SI", "cs.CL", "cs.CY", "cs.IR", "physics.soc-ph"], "primary_category": "cs.SI"}
{"title": "Domain-Invariant Speaker Vector Projection by Model-Agnostic\n  Meta-Learning", "abstract": "Domain generalization remains a critical problem for speaker recognition,\neven with the state-of-the-art architectures based on deep neural nets. For\nexample, a model trained on reading speech may largely fail when applied to\nscenarios of singing or movie. In this paper, we propose a domain-invariant\nprojection to improve the generalizability of speaker vectors. This projection\nis a simple neural net and is trained following the Model-Agnostic\nMeta-Learning (MAML) principle, for which the objective is to classify speakers\nin one domain if it had been updated with speech data in another domain. We\ntested the proposed method on CNCeleb, a new dataset consisting of\nsingle-speaker multi-condition (SSMC) data. The results demonstrated that the\nMAML-based domain-invariant projection can produce more generalizable speaker\nvectors, and effectively improve the performance in unseen domains.", "published": "2020-05-25 03:02:09", "link": "http://arxiv.org/abs/2005.11900v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "ASR-Free Pronunciation Assessment", "abstract": "Most of the pronunciation assessment methods are based on local features\nderived from automatic speech recognition (ASR), e.g., the Goodness of\nPronunciation (GOP) score. In this paper, we investigate an ASR-free scoring\napproach that is derived from the marginal distribution of raw speech signals.\nThe hypothesis is that even if we have no knowledge of the language (so cannot\nrecognize the phones/words), we can still tell how good a pronunciation is, by\ncomparatively listening to some speech data from the target language. Our\nanalysis shows that this new scoring approach provides an interesting\ncorrection for the phone-competition problem of GOP. Experimental results on\nthe ERJ dataset demonstrated that combining the ASR-free score and GOP can\nachieve better performance than the GOP baseline.", "published": "2020-05-25 03:10:29", "link": "http://arxiv.org/abs/2005.11902v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Neural Discriminant Analysis for Deep Speaker Embedding", "abstract": "Probabilistic Linear Discriminant Analysis (PLDA) is a popular tool in\nopen-set classification/verification tasks. However, the Gaussian assumption\nunderlying PLDA prevents it from being applied to situations where the data is\nclearly non-Gaussian. In this paper, we present a novel nonlinear version of\nPLDA named as Neural Discriminant Analysis (NDA). This model employs an\ninvertible deep neural network to transform a complex distribution to a simple\nGaussian, so that the linear Gaussian model can be readily established in the\ntransformed space. We tested this NDA model on a speaker recognition task where\nthe deep speaker vectors (x-vectors) are presumably non-Gaussian. Experimental\nresults on two datasets demonstrate that NDA consistently outperforms PLDA, by\nhandling the non-Gaussian distributions of the x-vectors.", "published": "2020-05-25 03:17:12", "link": "http://arxiv.org/abs/2005.11905v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Masked Pre-trained Encoder base on Joint CTC-Transformer", "abstract": "This study (The work was accomplished during the internship in Tencent AI\nlab) addresses semi-supervised acoustic modeling, i.e. attaining high-level\nrepresentations from unsupervised audio data and fine-tuning the parameters of\npre-trained model with supervised data. The proposed approach adopts a\ntwo-stage training framework, consisting of masked pre-trained encoder (MPE)\nand Joint CTC-Transformer (JCT). In the MPE framework, part of input frames are\nmasked and reconstructed after the encoder with massive unsupervised data. In\nJCT framework, compared with original Transformer, acoustic features are\napplied as input instead of plain text. CTC loss performs as the prediction\ntarget on top of the encoder, and decoder blocks remain unchanged. This paper\npresents a comparison between two-stage training method and the fully\nsupervised JCT. In addition, this paper investigates the our approach's\nrobustness against different volumns of training data. Experiments on the\ntwo-stage training method deliver much better performance than fully supervised\nmodel. The word error rate (WER) with two-stage training which only exploits\n30\\% of WSJ labeled data achieves 17\\% reduction than which trained by 50\\% of\nWSJ in a fully supervised way. Moreover, increasing unlabeled data for MPE from\nWSJ (81h) to Librispeech (960h) attains about 22\\% WER reduction.", "published": "2020-05-25 08:41:37", "link": "http://arxiv.org/abs/2005.11978v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "InfantNet: A Deep Neural Network for Analyzing Infant Vocalizations", "abstract": "Acoustic analyses of infant vocalizations are valuable for research on speech\ndevelopment as well as applications in sound classification. Previous studies\nhave focused on measures of acoustic features based on theories of speech\nprocessing, such spectral and cepstrum-based analyses. More recently,\nend-to-end models of deep learning have been developed to take raw speech\nsignals (acoustic waveforms) as inputs and convolutional neural network layers\nto learn representations of speech sounds based on classification tasks. We\napplied a recent end-to-end model of sound classification to analyze a\nlarge-scale database of labeled infant and adult vocalizations recorded in\nnatural settings outside the lab with no control over recording conditions. The\nmodel learned basic classifications like infant versus adult vocalizations,\ninfant speech-related versus non-speech vocalizations, and canonical versus\nnon-canonical babbling. The model was trained on recordings of infants ranging\nfrom 3 to 18 months of age, and classification accuracy changed with age as\nspeech became more distinct and babbling became more speech-like. Further work\nis needed to validate and explore the model and dataset, but our results show\nhow deep learning can be used to measure and investigate speech acquisition and\ndevelopment, with potential applications in speech pathology and infant\nmonitoring.", "published": "2020-05-25 21:24:01", "link": "http://arxiv.org/abs/2005.12412v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "End-to-End Auditory Object Recognition via Inception Nucleus", "abstract": "Machine learning approaches to auditory object recognition are traditionally\nbased on engineered features such as those derived from the spectrum or\ncepstrum. More recently, end-to-end classification systems in image and\nauditory recognition systems have been developed to learn features jointly with\nclassification and result in improved classification accuracy. In this paper,\nwe propose a novel end-to-end deep neural network to map the raw waveform\ninputs to sound class labels. Our network includes an \"inception nucleus\" that\noptimizes the size of convolutional filters on the fly that results in reducing\nengineering efforts dramatically. Classification results compared favorably\nagainst current state-of-the-art approaches, besting them by 10.4 percentage\npoints on the Urbansound8k dataset. Analyses of learned representations\nrevealed that filters in the earlier hidden layers learned wavelet-like\ntransforms to extract features that were informative for classification.", "published": "2020-05-25 16:08:41", "link": "http://arxiv.org/abs/2005.12195v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Speaker and Posture Classification using Instantaneous Intraspeech\n  Breathing Features", "abstract": "Acoustic features extracted from speech are widely used in problems such as\nbiometric speaker identification and first-person activity detection. However,\nthe use of speech for such purposes raises privacy issues as the content is\naccessible to the processing party. In this work, we propose a method for\nspeaker and posture classification using intraspeech breathing sounds.\nInstantaneous magnitude features are extracted using the Hilbert-Huang\ntransform (HHT) and fed into a CNN-GRU network for classification of recordings\nfrom the open intraspeech breathing sound dataset, BreathBase, that we\ncollected for this study. Using intraspeech breathing sounds, 87% speaker\nclassification, and 98% posture classification accuracy were obtained.", "published": "2020-05-25 17:00:26", "link": "http://arxiv.org/abs/2005.12230v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
