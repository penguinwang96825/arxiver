{"title": "Word Embeddings for Entity-annotated Texts", "abstract": "Learned vector representations of words are useful tools for many information\nretrieval and natural language processing tasks due to their ability to capture\nlexical semantics. However, while many such tasks involve or even rely on named\nentities as central components, popular word embedding models have so far\nfailed to include entities as first-class citizens. While it seems intuitive\nthat annotating named entities in the training corpus should result in more\nintelligent word features for downstream tasks, performance issues arise when\npopular embedding approaches are naively applied to entity annotated corpora.\nNot only are the resulting entity embeddings less useful than expected, but one\nalso finds that the performance of the non-entity word embeddings degrades in\ncomparison to those trained on the raw, unannotated corpus. In this paper, we\ninvestigate approaches to jointly train word and entity embeddings on a large\ncorpus with automatically annotated and linked entities. We discuss two\ndistinct approaches to the generation of such embeddings, namely the training\nof state-of-the-art embeddings on raw-text and annotated versions of the\ncorpus, as well as node embeddings of a co-occurrence graph representation of\nthe annotated corpus. We compare the performance of annotated embeddings and\nclassical word embeddings on a variety of word similarity, analogy, and\nclustering evaluation tasks, and investigate their performance in\nentity-specific tasks. Our findings show that it takes more than training\npopular word embedding models on an annotated corpus to create entity\nembeddings with acceptable performance on common test cases. Based on these\nresults, we discuss how and when node embeddings of the co-occurrence graph\nrepresentation of the text can restore the performance.", "published": "2019-02-06 09:21:55", "link": "http://arxiv.org/abs/1902.02078v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extending a model for ontology-based Arabic-English machine translation", "abstract": "The acceleration in telecommunication needs leads to many groups of research,\nespecially in communication facilitating and Machine Translation fields. While\npeople contact with others having different languages and cultures, they need\nto have instant translations. However, the available instant translators are\nstill providing somewhat bad Arabic-English Translations, for instance when\ntranslating books or articles, the meaning is not totally accurate. Therefore,\nusing the semantic web techniques to deal with the homographs and homonyms\nsemantically, the aim of this research is to extend a model for the\nontology-based Arabic-English Machine Translation, named NAN, which simulate\nthe human way in translation. The experimental results show that NAN\ntranslation is approximately more similar to the Human Translation than the\nother instant translators. The resulted translation will help getting the\ntranslated texts in the target language somewhat correctly and semantically\nmore similar to human translations for the Non-Arabic Natives and the\nNon-English natives.", "published": "2019-02-06 18:42:18", "link": "http://arxiv.org/abs/1902.02326v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Compression of Recurrent Neural Networks for Efficient Language Modeling", "abstract": "Recurrent neural networks have proved to be an effective method for\nstatistical language modeling. However, in practice their memory and run-time\ncomplexity are usually too large to be implemented in real-time offline mobile\napplications. In this paper we consider several compression techniques for\nrecurrent neural networks including Long-Short Term Memory models. We make\nparticular attention to the high-dimensional output problem caused by the very\nlarge vocabulary size. We focus on effective compression methods in the context\nof their exploitation on devices: pruning, quantization, and matrix\ndecomposition approaches (low-rank factorization and tensor train\ndecomposition, in particular). For each model we investigate the trade-off\nbetween its size, suitability for fast inference and perplexity. We propose a\ngeneral pipeline for applying the most suitable methods to compress recurrent\nneural networks for language modeling. It has been shown in the experimental\nstudy with the Penn Treebank (PTB) dataset that the most efficient results in\nterms of speed and compression-perplexity balance are obtained by matrix\ndecomposition techniques.", "published": "2019-02-06 19:49:22", "link": "http://arxiv.org/abs/1902.02380v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Latent Space Cartography: Generalised Metric-Inspired Measures and\n  Measure-Based Transformations for Generative Models", "abstract": "Deep generative models are universal tools for learning data distributions on\nhigh dimensional data spaces via a mapping to lower dimensional latent spaces.\nWe provide a study of latent space geometries and extend and build upon\nprevious results on Riemannian metrics. We show how a class of heuristic\nmeasures gives more flexibility in finding meaningful, problem-specific\ndistances, and how it can be applied to diverse generator types such as\nautoregressive generators commonly used in e.g. language and other sequence\nmodeling. We further demonstrate how a diffusion-inspired transformation\npreviously studied in cartography can be used to smooth out latent spaces,\nstretching them according to a chosen measure. In addition to providing more\nmeaningful distances directly in latent space, this also provides a unique tool\nfor novel kinds of data visualizations. We believe that the proposed methods\ncan be a valuable tool for studying the structure of latent spaces and learned\ndata distributions of generative models.", "published": "2019-02-06 11:15:08", "link": "http://arxiv.org/abs/1902.02113v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "End-to-end Anchored Speech Recognition", "abstract": "Voice-controlled house-hold devices, like Amazon Echo or Google Home, face\nthe problem of performing speech recognition of device-directed speech in the\npresence of interfering background speech, i.e., background noise and\ninterfering speech from another person or media device in proximity need to be\nignored. We propose two end-to-end models to tackle this problem with\ninformation extracted from the \"anchored segment\". The anchored segment refers\nto the wake-up word part of an audio stream, which contains valuable speaker\ninformation that can be used to suppress interfering speech and background\nnoise. The first method is called \"Multi-source Attention\" where the attention\nmechanism takes both the speaker information and decoder state into\nconsideration. The second method directly learns a frame-level mask on top of\nthe encoder output. We also explore a multi-task learning setup where we use\nthe ground truth of the mask to guide the learner. Given that audio data with\ninterfering speech is rare in our training data set, we also propose a way to\nsynthesize \"noisy\" speech from \"clean\" speech to mitigate the mismatch between\ntraining and test data. Our proposed methods show up to 15% relative reduction\nin WER for Amazon Alexa live data with interfering background speech without\nsignificantly degrading on clean speech.", "published": "2019-02-06 19:50:23", "link": "http://arxiv.org/abs/1902.02383v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Unsupervised Polyglot Text To Speech", "abstract": "We present a TTS neural network that is able to produce speech in multiple\nlanguages. The proposed network is able to transfer a voice, which was\npresented as a sample in a source language, into one of several target\nlanguages. Training is done without using matching or parallel data, i.e.,\nwithout samples of the same speaker in multiple languages, making the method\nmuch more applicable. The conversion is based on learning a polyglot network\nthat has multiple per-language sub-networks and adding loss terms that preserve\nthe speaker's identity in multiple languages. We evaluate the proposed polyglot\nneural network for three languages with a total of more than 400 speakers and\ndemonstrate convincing conversion capabilities.", "published": "2019-02-06 16:28:26", "link": "http://arxiv.org/abs/1902.02263v1", "categories": ["cs.LG", "cs.CL", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Transfer Learning From Sound Representations For Anger Detection in\n  Speech", "abstract": "In this work, we train fully convolutional networks to detect anger in\nspeech. Since training these deep architectures requires large amounts of data\nand the size of emotion datasets is relatively small, we use transfer learning.\nHowever, unlike previous approaches that use speech or emotion-based tasks for\nthe source model, we instead use SoundNet, a fully convolutional neural network\ntrained multimodally on a massive video dataset to classify audio, with\nground-truth labels provided by vision-based classifiers. As a result of\ntransfer learning from SoundNet, our trained anger detection model improves\nperformance and generalizes well on a variety of acted, elicited, and natural\nemotional speech datasets. We also test the cross-lingual effectiveness of our\nmodel by evaluating our English-trained model on Mandarin Chinese speech\nemotion data. Furthermore, our proposed system has low latency suitable for\nreal-time applications, only requiring 1.2 seconds of audio to make a reliable\nclassification.", "published": "2019-02-06 11:34:44", "link": "http://arxiv.org/abs/1902.02120v1", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Centroid-based deep metric learning for speaker recognition", "abstract": "Speaker embedding models that utilize neural networks to map utterances to a\nspace where distances reflect similarity between speakers have driven recent\nprogress in the speaker recognition task. However, there is still a significant\nperformance gap between recognizing speakers in the training set and unseen\nspeakers. The latter case corresponds to the few-shot learning task, where a\ntrained model is evaluated on unseen classes. Here, we optimize a speaker\nembedding model with prototypical network loss (PNL), a state-of-the-art\napproach for the few-shot image classification task. The resulting embedding\nmodel outperforms the state-of-the-art triplet loss based models in both\nspeaker verification and identification tasks, for both seen and unseen\nspeakers.", "published": "2019-02-06 19:40:33", "link": "http://arxiv.org/abs/1902.02375v1", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
