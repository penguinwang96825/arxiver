{"title": "HASOCOne@FIRE-HASOC2020: Using BERT and Multilingual BERT models for\n  Hate Speech Detection", "abstract": "Hateful and Toxic content has become a significant concern in today's world\ndue to an exponential rise in social media. The increase in hate speech and\nharmful content motivated researchers to dedicate substantial efforts to the\nchallenging direction of hateful content identification. In this task, we\npropose an approach to automatically classify hate speech and offensive\ncontent. We have used the datasets obtained from FIRE 2019 and 2020 shared\ntasks. We perform experiments by taking advantage of transfer learning models.\nWe observed that the pre-trained BERT model and the multilingual-BERT model\ngave the best results. The code is made publically available at\nhttps://github.com/suman101112/hasoc-fire-2020.", "published": "2021-01-22 08:55:32", "link": "http://arxiv.org/abs/2101.09007v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Does a Hybrid Neural Network based Feature Selection Model Improve Text\n  Classification?", "abstract": "Text classification is a fundamental problem in the field of natural language\nprocessing. Text classification mainly focuses on giving more importance to all\nthe relevant features that help classify the textual data. Apart from these,\nthe text can have redundant or highly correlated features. These features\nincrease the complexity of the classification algorithm. Thus, many\ndimensionality reduction methods were proposed with the traditional machine\nlearning classifiers. The use of dimensionality reduction methods with machine\nlearning classifiers has achieved good results. In this paper, we propose a\nhybrid feature selection method for obtaining relevant features by combining\nvarious filter-based feature selection methods and fastText classifier. We then\npresent three ways of implementing a feature selection and neural network\npipeline. We observed a reduction in training time when feature selection\nmethods are used along with neural networks. We also observed a slight increase\nin accuracy on some datasets.", "published": "2021-01-22 09:12:19", "link": "http://arxiv.org/abs/2101.09009v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Pre-Trained Transformers and Convolutional NN\n  Classification Models for Technical Domain Identification", "abstract": "In this paper, we present a transfer learning system to perform technical\ndomain identification on multilingual text data. We have submitted two runs,\none uses the transformer model BERT, and the other uses XLM-ROBERTa with the\nCNN model for text classification. These models allowed us to identify the\ndomain of the given sentences for the ICON 2020 shared Task, TechDOfication:\nTechnical Domain Identification. Our system ranked the best for the subtasks\n1d, 1g for the given TechDOfication dataset.", "published": "2021-01-22 09:18:02", "link": "http://arxiv.org/abs/2101.09012v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Technical Domain Terms Extraction using Term Extractor", "abstract": "Terminology extraction, also known as term extraction, is a subtask of\ninformation extraction. The goal of terminology extraction is to extract\nrelevant words or phrases from a given corpus automatically. This paper focuses\non the unsupervised automated domain term extraction method that considers\nchunking, preprocessing, and ranking domain-specific terms using relevance and\ncohesion functions for ICON 2020 shared task 2: TermTraction.", "published": "2021-01-22 09:24:09", "link": "http://arxiv.org/abs/2101.09015v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluation Discrepancy Discovery: A Sentence Compression Case-study", "abstract": "Reliable evaluation protocols are of utmost importance for reproducible NLP\nresearch. In this work, we show that sometimes neither metric nor conventional\nhuman evaluation is sufficient to draw conclusions about system performance.\nUsing sentence compression as an example task, we demonstrate how a system can\ngame a well-established dataset to achieve state-of-the-art results. In\ncontrast with the results reported in previous work that showed correlation\nbetween human judgements and metric scores, our manual analysis of\nstate-of-the-art system outputs demonstrates that high metric scores may only\nindicate a better fit to the data, but not better outputs, as perceived by\nhumans.", "published": "2021-01-22 12:28:41", "link": "http://arxiv.org/abs/2101.09079v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Beyond Domain APIs: Task-oriented Conversational Modeling with\n  Unstructured Knowledge Access Track in DSTC9", "abstract": "Most prior work on task-oriented dialogue systems are restricted to a limited\ncoverage of domain APIs, while users oftentimes have domain related requests\nthat are not covered by the APIs. This challenge track aims to expand the\ncoverage of task-oriented dialogue systems by incorporating external\nunstructured knowledge sources. We define three tasks: knowledge-seeking turn\ndetection, knowledge selection, and knowledge-grounded response generation. We\nintroduce the data sets and the neural baseline models for three tasks. The\nchallenge track received a total of 105 entries from 24 participating teams. In\nthe evaluation results, the ensemble methods with different large-scale\npretrained language models achieved high performances with improved knowledge\nselection capability and better generalization into unseen data.", "published": "2021-01-22 18:57:56", "link": "http://arxiv.org/abs/2101.09276v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BERT Transformer model for Detecting Arabic GPT2 Auto-Generated Tweets", "abstract": "During the last two decades, we have progressively turned to the Internet and\nsocial media to find news, entertain conversations and share opinion. Recently,\nOpenAI has developed a ma-chine learning system called GPT-2 for Generative\nPre-trained Transformer-2, which can pro-duce deepfake texts. It can generate\nblocks of text based on brief writing prompts that look like they were written\nby humans, facilitating the spread false or auto-generated text. In line with\nthis progress, and in order to counteract potential dangers, several methods\nhave been pro-posed for detecting text written by these language models. In\nthis paper, we propose a transfer learning based model that will be able to\ndetect if an Arabic sentence is written by humans or automatically generated by\nbots. Our dataset is based on tweets from a previous work, which we have\ncrawled and extended using the Twitter API. We used GPT2-Small-Arabic to\ngenerate fake Arabic Sentences. For evaluation, we compared different recurrent\nneural network (RNN) word embeddings based baseline models, namely: LSTM,\nBI-LSTM, GRU and BI-GRU, with a transformer-based model. Our new\ntransfer-learning model has obtained an accuracy up to 98%. To the best of our\nknowledge, this work is the first study where ARABERT and GPT2 were combined to\ndetect and classify the Arabic auto-generated texts.", "published": "2021-01-22 21:50:38", "link": "http://arxiv.org/abs/2101.09345v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Effects of Pre- and Post-Processing on type-based Embeddings in Lexical\n  Semantic Change Detection", "abstract": "Lexical semantic change detection is a new and innovative research field. The\noptimal fine-tuning of models including pre- and post-processing is largely\nunclear. We optimize existing models by (i) pre-training on large corpora and\nrefining on diachronic target corpora tackling the notorious small data\nproblem, and (ii) applying post-processing transformations that have been shown\nto improve performance on synchronic tasks. Our results provide a guide for the\napplication and optimization of lexical semantic change detection models across\nvarious learning scenarios.", "published": "2021-01-22 22:34:15", "link": "http://arxiv.org/abs/2101.09368v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enriching Non-Autoregressive Transformer with Syntactic and\n  SemanticStructures for Neural Machine Translation", "abstract": "The non-autoregressive models have boosted the efficiency of neural machine\ntranslation through parallelized decoding at the cost of effectiveness when\ncomparing with the autoregressive counterparts. In this paper, we claim that\nthe syntactic and semantic structures among natural language are critical for\nnon-autoregressive machine translation and can further improve the performance.\nHowever, these structures are rarely considered in the existing\nnon-autoregressive models. Inspired by this intuition, we propose to\nincorporate the explicit syntactic and semantic structures of languages into a\nnon-autoregressive Transformer, for the task of neural machine translation.\nMoreover, we also consider the intermediate latent alignment within target\nsentences to better learn the long-term token dependencies. Experimental\nresults on two real-world datasets (i.e., WMT14 En-De and WMT16 En-Ro) show\nthat our model achieves a significantly faster speed, as well as keeps the\ntranslation quality when compared with several state-of-the-art\nnon-autoregressive models.", "published": "2021-01-22 04:12:17", "link": "http://arxiv.org/abs/2101.08942v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Knowledge Graph Completion with Text-aided Regularization", "abstract": "Knowledge Graph Completion is a task of expanding the knowledge graph/base\nthrough estimating possible entities, or proper nouns, that can be connected\nusing a set of predefined relations, or verb/predicates describing\ninterconnections of two things. Generally, we describe this problem as adding\nnew edges to a current network of vertices and edges. Traditional approaches\nmainly focus on using the existing graphical information that is intrinsic of\nthe graph and train the corresponding embeddings to describe the information;\nhowever, we think that the corpus that are related to the entities should also\ncontain information that can positively influence the embeddings to better make\npredictions. In our project, we try numerous ways of using extracted or raw\ntextual information to help existing KG embedding frameworks reach better\nprediction results, in the means of adding a similarity function to the\nregularization part in the loss function. Results have shown that we have made\ndecent improvements over baseline KG embedding methods.", "published": "2021-01-22 06:10:09", "link": "http://arxiv.org/abs/2101.08962v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "CMSAOne@Dravidian-CodeMix-FIRE2020: A Meta Embedding and Transformer\n  model for Code-Mixed Sentiment Analysis on Social Media Text", "abstract": "Code-mixing(CM) is a frequently observed phenomenon that uses multiple\nlanguages in an utterance or sentence. CM is mostly practiced on various social\nmedia platforms and in informal conversations. Sentiment analysis (SA) is a\nfundamental step in NLP and is well studied in the monolingual text.\nCode-mixing adds a challenge to sentiment analysis due to its non-standard\nrepresentations. This paper proposes a meta embedding with a transformer method\nfor sentiment analysis on the Dravidian code-mixed dataset. In our method, we\nused meta embeddings to capture rich text representations. We used the proposed\nmethod for the Task: \"Sentiment Analysis for Dravidian Languages in Code-Mixed\nText\", and it achieved an F1 score of $0.58$ and $0.66$ for the given Dravidian\ncode mixed data sets. The code is provided in the Github\nhttps://github.com/suman101112/fire-2020-Dravidian-CodeMix.", "published": "2021-01-22 08:48:27", "link": "http://arxiv.org/abs/2101.09004v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Enhanced word embeddings using multi-semantic representation through\n  lexical chains", "abstract": "The relationship between words in a sentence often tells us more about the\nunderlying semantic content of a document than its actual words, individually.\nIn this work, we propose two novel algorithms, called Flexible Lexical Chain II\nand Fixed Lexical Chain II. These algorithms combine the semantic relations\nderived from lexical chains, prior knowledge from lexical databases, and the\nrobustness of the distributional hypothesis in word embeddings as building\nblocks forming a single system. In short, our approach has three main\ncontributions: (i) a set of techniques that fully integrate word embeddings and\nlexical chains; (ii) a more robust semantic representation that considers the\nlatent relation between words in a document; and (iii) lightweight word\nembeddings models that can be extended to any natural language task. We intend\nto assess the knowledge of pre-trained models to evaluate their robustness in\nthe document classification task. The proposed techniques are tested against\nseven word embeddings algorithms using five different machine learning\nclassifiers over six scenarios in the document classification task. Our results\nshow the integration between lexical chains and word embeddings representations\nsustain state-of-the-art results, even against more complex systems.", "published": "2021-01-22 09:43:33", "link": "http://arxiv.org/abs/2101.09023v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Lexical semantic change for Ancient Greek and Latin", "abstract": "Change and its precondition, variation, are inherent in languages. Over time,\nnew words enter the lexicon, others become obsolete, and existing words acquire\nnew senses. Associating a word's correct meaning in its historical context is a\ncentral challenge in diachronic research. Historical corpora of classical\nlanguages, such as Ancient Greek and Latin, typically come with rich metadata,\nand existing models are limited by their inability to exploit contextual\ninformation beyond the document timestamp. While embedding-based methods\nfeature among the current state of the art systems, they are lacking in the\ninterpretative power. In contrast, Bayesian models provide explicit and\ninterpretable representations of semantic change phenomena. In this chapter we\nbuild on GASC, a recent computational approach to semantic change based on a\ndynamic Bayesian mixture model. In this model, the evolution of word senses\nover time is based not only on distributional information of lexical nature,\nbut also on text genres. We provide a systematic comparison of dynamic Bayesian\nmixture models for semantic change with state-of-the-art embedding-based\nmodels. On top of providing a full description of meaning change over time, we\nshow that Bayesian mixture models are highly competitive approaches to detect\nbinary semantic change in both Ancient Greek and Latin.", "published": "2021-01-22 12:04:08", "link": "http://arxiv.org/abs/2101.09069v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A shallow neural model for relation prediction", "abstract": "Knowledge graph completion refers to predicting missing triples. Most\napproaches achieve this goal by predicting entities, given an entity and a\nrelation. We predict missing triples via the relation prediction. To this end,\nwe frame the relation prediction problem as a multi-label classification\nproblem and propose a shallow neural model (SHALLOM) that accurately infers\nmissing relations from entities. SHALLOM is analogous to C-BOW as both\napproaches predict a central token (p) given surrounding tokens ((s,o)). Our\nexperiments indicate that SHALLOM outperforms state-of-the-art approaches on\nthe FB15K-237 and WN18RR with margins of up to $3\\%$ and $8\\%$ (absolute),\nrespectively, while requiring a maximum training time of 8 minutes on these\ndatasets. We ensure the reproducibility of our results by providing an\nopen-source implementation including training and evaluation scripts at\n{\\url{https://github.com/dice-group/Shallom}.}", "published": "2021-01-22 13:10:11", "link": "http://arxiv.org/abs/2101.09090v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A multi-perspective combined recall and rank framework for Chinese\n  procedure terminology normalization", "abstract": "Medical terminology normalization aims to map the clinical mention to\nterminologies come from a knowledge base, which plays an important role in\nanalyzing Electronic Health Record(EHR) and many downstream tasks. In this\npaper, we focus on Chinese procedure terminology normalization. The expression\nof terminologies are various and one medical mention may be linked to multiple\nterminologies. Previous study explores some methods such as multi-class\nclassification or learning to rank(LTR) to sort the terminologies by literature\nand semantic information. However, these information is inadequate to find the\nright terminologies, particularly in multi-implication cases. In this work, we\npropose a combined recall and rank framework to solve the above problems. This\nframework is composed of a multi-task candidate generator(MTCG), a keywords\nattentive ranker(KAR) and a fusion block(FB). MTCG is utilized to predict the\nmention implication number and recall candidates with semantic similarity. KAR\nis based on Bert with a keywords attentive mechanism which focuses on keywords\nsuch as procedure sites and procedure types. FB merges the similarity come from\nMTCG and KAR to sort the terminologies from different perspectives. Detailed\nexperimental analysis shows our proposed framework has a remarkable improvement\non both performance and efficiency.", "published": "2021-01-22 13:37:10", "link": "http://arxiv.org/abs/2101.09101v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The heads hypothesis: A unifying statistical approach towards\n  understanding multi-headed attention in BERT", "abstract": "Multi-headed attention heads are a mainstay in transformer-based models.\nDifferent methods have been proposed to classify the role of each attention\nhead based on the relations between tokens which have high pair-wise attention.\nThese roles include syntactic (tokens with some syntactic relation), local\n(nearby tokens), block (tokens in the same sentence) and delimiter (the special\n[CLS], [SEP] tokens). There are two main challenges with existing methods for\nclassification: (a) there are no standard scores across studies or across\nfunctional roles, and (b) these scores are often average quantities measured\nacross sentences without capturing statistical significance. In this work, we\nformalize a simple yet effective score that generalizes to all the roles of\nattention heads and employs hypothesis testing on this score for robust\ninference. This provides us the right lens to systematically analyze attention\nheads and confidently comment on many commonly posed questions on analyzing the\nBERT model. In particular, we comment on the co-location of multiple functional\nroles in the same attention head, the distribution of attention heads across\nlayers, and effect of fine-tuning for specific NLP tasks on these functional\nroles.", "published": "2021-01-22 14:10:59", "link": "http://arxiv.org/abs/2101.09115v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Streaming Models for Joint Speech Recognition and Translation", "abstract": "Using end-to-end models for speech translation (ST) has increasingly been the\nfocus of the ST community. These models condense the previously cascaded\nsystems by directly converting sound waves into translated text. However,\ncascaded models have the advantage of including automatic speech recognition\noutput, useful for a variety of practical ST systems that often display\ntranscripts to the user alongside the translations. To bridge this gap, recent\nwork has shown initial progress into the feasibility for end-to-end models to\nproduce both of these outputs. However, all previous work has only looked at\nthis problem from the consecutive perspective, leaving uncertainty on whether\nthese approaches are effective in the more challenging streaming setting. We\ndevelop an end-to-end streaming ST model based on a re-translation approach and\ncompare against standard cascading approaches. We also introduce a novel\ninference method for the joint case, interleaving both transcript and\ntranslation in generation and removing the need to use separate decoders. Our\nevaluation across a range of metrics capturing accuracy, latency, and\nconsistency shows that our end-to-end models are statistically similar to\ncascading models, while having half the number of parameters. We also find that\nboth systems provide strong translation quality at low latency, keeping 99% of\nconsecutive quality at a lag of just under a second.", "published": "2021-01-22 15:16:54", "link": "http://arxiv.org/abs/2101.09149v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Impact of Multiple Parallel Phrase Suggestions on Email Input and\n  Composition Behaviour of Native and Non-Native English Writers", "abstract": "We present an in-depth analysis of the impact of multi-word suggestion\nchoices from a neural language model on user behaviour regarding input and text\ncomposition in email writing. Our study for the first time compares different\nnumbers of parallel suggestions, and use by native and non-native English\nwriters, to explore a trade-off of \"efficiency vs ideation\", emerging from\nrecent literature. We built a text editor prototype with a neural language\nmodel (GPT-2), refined in a prestudy with 30 people. In an online study\n(N=156), people composed emails in four conditions (0/1/3/6 parallel\nsuggestions). Our results reveal (1) benefits for ideation, and costs for\nefficiency, when suggesting multiple phrases; (2) that non-native speakers\nbenefit more from more suggestions; and (3) further insights into behaviour\npatterns. We discuss implications for research, the design of interactive\nsuggestion systems, and the vision of supporting writers with AI instead of\nreplacing them.", "published": "2021-01-22 15:32:32", "link": "http://arxiv.org/abs/2101.09157v1", "categories": ["cs.HC", "cs.CL", "H.5.2; I.2.7"], "primary_category": "cs.HC"}
{"title": "Extracting Lifestyle Factors for Alzheimer's Disease from Clinical Notes\n  Using Deep Learning with Weak Supervision", "abstract": "Since no effective therapies exist for Alzheimer's disease (AD), prevention\nhas become more critical through lifestyle factor changes and interventions.\nAnalyzing electronic health records (EHR) of patients with AD can help us\nbetter understand lifestyle's effect on AD. However, lifestyle information is\ntypically stored in clinical narratives. Thus, the objective of the study was\nto demonstrate the feasibility of natural language processing (NLP) models to\nclassify lifestyle factors (e.g., physical activity and excessive diet) from\nclinical texts. We automatically generated labels for the training data by\nusing a rule-based NLP algorithm. We conducted weak supervision for pre-trained\nBidirectional Encoder Representations from Transformers (BERT) models on the\nweakly labeled training corpus. These models include the BERT base model,\nPubMedBERT(abstracts + full text), PubMedBERT(only abstracts), Unified Medical\nLanguage System (UMLS) BERT, Bio BERT, and Bio-clinical BERT. We performed two\ncase studies: physical activity and excessive diet, in order to validate the\neffectiveness of BERT models in classifying lifestyle factors for AD. These\nmodels were compared on the developed Gold Standard Corpus (GSC) on the two\ncase studies. The PubmedBERT(Abs) model achieved the best performance for\nphysical activity, with its precision, recall, and F-1 scores of 0.96, 0.96,\nand 0.96, respectively. Regarding classifying excessive diet, the Bio BERT\nmodel showed the highest performance with perfect precision, recall, and F-1\nscores. The proposed approach leveraging weak supervision could significantly\nincrease the sample size, which is required for training the deep learning\nmodels. The study also demonstrates the effectiveness of BERT models for\nextracting lifestyle factors for Alzheimer's disease from clinical notes.", "published": "2021-01-22 17:55:03", "link": "http://arxiv.org/abs/2101.09244v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Drug and Disease Interpretation Learning with Biomedical Entity\n  Representation Transformer", "abstract": "Concept normalization in free-form texts is a crucial step in every\ntext-mining pipeline. Neural architectures based on Bidirectional Encoder\nRepresentations from Transformers (BERT) have achieved state-of-the-art results\nin the biomedical domain. In the context of drug discovery and development,\nclinical trials are necessary to establish the efficacy and safety of drugs. We\ninvestigate the effectiveness of transferring concept normalization from the\ngeneral biomedical domain to the clinical trials domain in a zero-shot setting\nwith an absence of labeled data. We propose a simple and effective two-stage\nneural approach based on fine-tuned BERT architectures. In the first stage, we\ntrain a metric learning model that optimizes relative similarity of mentions\nand concepts via triplet loss. The model is trained on available labeled\ncorpora of scientific abstracts to obtain vector embeddings of concept names\nand entity mentions from texts. In the second stage, we find the closest\nconcept name representation in an embedding space to a given clinical mention.\nWe evaluated several models, including state-of-the-art architectures, on a\ndataset of abstracts and a real-world dataset of trial records with\ninterventions and conditions mapped to drug and disease terminologies.\nExtensive experiments validate the effectiveness of our approach in knowledge\ntransfer from the scientific literature to clinical trials.", "published": "2021-01-22 20:01:25", "link": "http://arxiv.org/abs/2101.09311v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "$k$-Neighbor Based Curriculum Sampling for Sequence Prediction", "abstract": "Multi-step ahead prediction in language models is challenging due to the\ndiscrepancy between training and test time processes. At test time, a sequence\npredictor is required to make predictions given past predictions as the input,\ninstead of the past targets that are provided during training. This difference,\nknown as exposure bias, can lead to the compounding of errors along a generated\nsequence at test time. To improve generalization in neural language models and\naddress compounding errors, we propose \\textit{Nearest-Neighbor Replacement\nSampling} -- a curriculum learning-based method that gradually changes an\ninitially deterministic teacher policy to a stochastic policy. A token at a\ngiven time-step is replaced with a sampled nearest neighbor of the past target\nwith a truncated probability proportional to the cosine similarity between the\noriginal word and its top $k$ most similar words. This allows the learner to\nexplore alternatives when the current policy provided by the teacher is\nsub-optimal or difficult to learn from. The proposed method is straightforward,\nonline and requires little additional memory requirements. We report our\nfindings on two language modelling benchmarks and find that the proposed method\nfurther improves performance when used in conjunction with scheduled sampling.", "published": "2021-01-22 20:07:29", "link": "http://arxiv.org/abs/2101.09313v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Slot Self-Attentive Dialogue State Tracking", "abstract": "An indispensable component in task-oriented dialogue systems is the dialogue\nstate tracker, which keeps track of users' intentions in the course of\nconversation. The typical approach towards this goal is to fill in multiple\npre-defined slots that are essential to complete the task. Although various\ndialogue state tracking methods have been proposed in recent years, most of\nthem predict the value of each slot separately and fail to consider the\ncorrelations among slots. In this paper, we propose a slot self-attention\nmechanism that can learn the slot correlations automatically. Specifically, a\nslot-token attention is first utilized to obtain slot-specific features from\nthe dialogue context. Then a stacked slot self-attention is applied on these\nfeatures to learn the correlations among slots. We conduct comprehensive\nexperiments on two multi-domain task-oriented dialogue datasets, including\nMultiWOZ 2.0 and MultiWOZ 2.1. The experimental results demonstrate that our\napproach achieves state-of-the-art performance on both datasets, verifying the\nnecessity and effectiveness of taking slot correlations into consideration.", "published": "2021-01-22 22:48:51", "link": "http://arxiv.org/abs/2101.09374v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Fringe News Networks: Dynamics of US News Viewership following the 2020\n  Presidential Election", "abstract": "The growing political polarization of the American electorate over the last\nseveral decades has been widely studied and documented. During the\nadministration of President Donald Trump, charges of \"fake news\" made social\nand news media not only the means but, to an unprecedented extent, the topic of\npolitical communication. Using data from before the November 3rd, 2020 US\nPresidential election, recent work has demonstrated the viability of using\nYouTube's social media ecosystem to obtain insights into the extent of US\npolitical polarization as well as the relationship between this polarization\nand the nature of the content and commentary provided by different US news\nnetworks. With that work as background, this paper looks at the sharp\ntransformation of the relationship between news consumers and here-to-fore\n\"fringe\" news media channels in the 64 days between the US presidential\nelection and the violence that took place at US Capitol on January 6th. This\npaper makes two distinct types of contributions. The first is to introduce a\nnovel methodology to analyze large social media data to study the dynamics of\nsocial political news networks and their viewers. The second is to provide\ninsights into what actually happened regarding US political social media\nchannels and their viewerships during this volatile 64 day period.", "published": "2021-01-22 03:42:36", "link": "http://arxiv.org/abs/2101.10112v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Application of Lexical Features Towards Improvement of Filipino\n  Readability Identification of Children's Literature", "abstract": "Proper identification of grade levels of children's reading materials is an\nimportant step towards effective learning. Recent studies in readability\nassessment for the English domain applied modern approaches in natural language\nprocessing (NLP) such as machine learning (ML) techniques to automate the\nprocess. There is also a need to extract the correct linguistic features when\nmodeling readability formulas. In the context of the Filipino language, limited\nwork has been done [1, 2], especially in considering the language's lexical\ncomplexity as main features. In this paper, we explore the use of lexical\nfeatures towards improving the development of readability identification of\nchildren's books written in Filipino. Results show that combining lexical\nfeatures (LEX) consisting of type-token ratio, lexical density, lexical\nvariation, foreign word count with traditional features (TRAD) used by previous\nworks such as sentence length, average syllable length, polysyllabic words,\nword, sentence, and phrase counts increased the performance of readability\nmodels by almost a 5% margin (from 42% to 47.2%). Further analysis and ranking\nof the most important features were shown to identify which features contribute\nthe most in terms of reading complexity.", "published": "2021-01-22 19:54:37", "link": "http://arxiv.org/abs/2101.10537v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SkillNER: Mining and Mapping Soft Skills from any Text", "abstract": "In today's digital world, there is an increasing focus on soft skills. On the\none hand, they facilitate innovation at companies, but on the other, they are\nunlikely to be automated soon. Researchers struggle with accurately approaching\nquantitatively the study of soft skills due to the lack of data-driven methods\nto retrieve them. This limits the possibility for psychologists and HR managers\nto understand the relation between humans and digitalisation. This paper\npresents SkillNER, a novel data-driven method for automatically extracting soft\nskills from text. It is a named entity recognition (NER) system trained with a\nsupport vector machine (SVM) on a corpus of more than 5000 scientific papers.\nWe developed this system by measuring the performance of our approach against\ndifferent training models and validating the results together with a team of\npsychologists. Finally, SkillNER was tested in a real-world case study using\nthe job descriptions of ESCO (European Skill/Competence Qualification and\nOccupation) as textual source. The system enabled the detection of communities\nof job profiles based on their shared soft skills and communities of soft\nskills based on their shared job profiles. This case study demonstrates that\nthe tool can automatically retrieve soft skills from a large corpus in an\nefficient way, proving useful for firms, institutions, and workers. The tool is\nopen and available online to foster quantitative methods for the study of soft\nskills.", "published": "2021-01-22 11:14:05", "link": "http://arxiv.org/abs/2101.11431v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Using Finite-State Machines to Automatically Scan Classical Greek\n  Hexameter", "abstract": "This paper presents a fully automatic approach to the scansion of Classical\nGreek hexameter verse. In particular, the paper describes an algorithm that\nuses deterministic finite-state automata and local linguistic rules to\nimplement a targeted search for valid spondeus patterns and, in addition, a\nweighted finite-state transducer to correct and complete partial analyses and\nto reject invalid candidates. The paper also details the results of an\nempirical evaluation of the annotation quality resulting from this approach on\nhand-annotated data. It is shown that a finite-state approach provides quick\nand linguistically sound analyses of hexameter verses as well as an efficient\nformalisation of linguistic knowledge. The project code is available (see\nhttps://github.com/anetschka/greek_scansion).", "published": "2021-01-22 09:59:46", "link": "http://arxiv.org/abs/2101.11437v1", "categories": ["cs.CL", "cs.AI", "I.2.7; J.5"], "primary_category": "cs.CL"}
{"title": "Applications of artificial intelligence in drug development using\n  real-world data", "abstract": "The US Food and Drug Administration (FDA) has been actively promoting the use\nof real-world data (RWD) in drug development. RWD can generate important\nreal-world evidence reflecting the real-world clinical environment where the\ntreatments are used. Meanwhile, artificial intelligence (AI), especially\nmachine- and deep-learning (ML/DL) methods, have been increasingly used across\nmany stages of the drug development process. Advancements in AI have also\nprovided new strategies to analyze large, multidimensional RWD. Thus, we\nconducted a rapid review of articles from the past 20 years, to provide an\noverview of the drug development studies that use both AI and RWD. We found\nthat the most popular applications were adverse event detection, trial\nrecruitment, and drug repurposing. Here, we also discuss current research gaps\nand future opportunities.", "published": "2021-01-22 01:13:54", "link": "http://arxiv.org/abs/2101.08904v2", "categories": ["cs.CY", "cs.CL", "cs.LG", "q-bio.QM"], "primary_category": "cs.CY"}
{"title": "Exploiting Beam Search Confidence for Energy-Efficient Speech\n  Recognition", "abstract": "With computers getting more and more powerful and integrated in our daily\nlives, the focus is increasingly shifting towards more human-friendly\ninterfaces, making Automatic Speech Recognition (ASR) a central player as the\nideal means of interaction with machines. Consequently, interest in speech\ntechnology has grown in the last few years, with more systems being proposed\nand higher accuracy levels being achieved, even surpassing \\textit{Human\nAccuracy}. While ASR systems become increasingly powerful, the computational\ncomplexity also increases, and the hardware support have to keep pace. In this\npaper, we propose a technique to improve the energy-efficiency and performance\nof ASR systems, focusing on low-power hardware for edge devices. We focus on\noptimizing the DNN-based Acoustic Model evaluation, as we have observed it to\nbe the main bottleneck in state-of-the-art ASR systems, by leveraging run-time\ninformation from the Beam Search. By doing so, we reduce energy and execution\ntime of the acoustic model evaluation by 25.6% and 25.9%, respectively, with\nnegligible accuracy loss.", "published": "2021-01-22 12:35:35", "link": "http://arxiv.org/abs/2101.09083v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Censorship of Online Encyclopedias: Implications for NLP Models", "abstract": "While artificial intelligence provides the backbone for many tools people use\naround the world, recent work has brought to attention that the algorithms\npowering AI are not free of politics, stereotypes, and bias. While most work in\nthis area has focused on the ways in which AI can exacerbate existing\ninequalities and discrimination, very little work has studied how governments\nactively shape training data. We describe how censorship has affected the\ndevelopment of Wikipedia corpuses, text data which are regularly used for\npre-trained inputs into NLP algorithms. We show that word embeddings trained on\nBaidu Baike, an online Chinese encyclopedia, have very different associations\nbetween adjectives and a range of concepts about democracy, freedom, collective\naction, equality, and people and historical events in China than its regularly\nblocked but uncensored counterpart - Chinese language Wikipedia. We examine the\nimplications of these discrepancies by studying their use in downstream AI\napplications. Our paper shows how government repression, censorship, and\nself-censorship may impact training data and the applications that draw from\nthem.", "published": "2021-01-22 19:09:53", "link": "http://arxiv.org/abs/2101.09294v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Study of Pre-processing Defenses against Adversarial Attacks on\n  State-of-the-art Speaker Recognition Systems", "abstract": "Adversarial examples to speaker recognition (SR) systems are generated by\nadding a carefully crafted noise to the speech signal to make the system fail\nwhile being imperceptible to humans. Such attacks pose severe security risks,\nmaking it vital to deep-dive and understand how much the state-of-the-art SR\nsystems are vulnerable to these attacks. Moreover, it is of greater importance\nto propose defenses that can protect the systems against these attacks.\nAddressing these concerns, this paper at first investigates how\nstate-of-the-art x-vector based SR systems are affected by white-box\nadversarial attacks, i.e., when the adversary has full knowledge of the system.\nx-Vector based SR systems are evaluated against white-box adversarial attacks\ncommon in the literature like fast gradient sign method (FGSM), basic iterative\nmethod (BIM)--a.k.a. iterative-FGSM--, projected gradient descent (PGD), and\nCarlini-Wagner (CW) attack. To mitigate against these attacks, the paper\nproposes four pre-processing defenses. It evaluates them against powerful\nadaptive white-box adversarial attacks, i.e., when the adversary has full\nknowledge of the system, including the defense. The four pre-processing\ndefenses--viz. randomized smoothing, DefenseGAN, variational autoencoder (VAE),\nand Parallel WaveGAN vocoder (PWG) are compared against the baseline defense of\nadversarial training. Conclusions indicate that SR systems were extremely\nvulnerable under BIM, PGD, and CW attacks. Among the proposed pre-processing\ndefenses, PWG combined with randomized smoothing offers the most protection\nagainst the attacks, with accuracy averaging 93% compared to 52% in the\nundefended system and an absolute improvement >90% for BIM attacks with\n$L_\\infty>0.001$ and CW attack.", "published": "2021-01-22 01:25:18", "link": "http://arxiv.org/abs/2101.08909v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Towards efficient models for real-time deep noise suppression", "abstract": "With recent research advancements, deep learning models are becoming\nattractive and powerful choices for speech enhancement in real-time\napplications. While state-of-the-art models can achieve outstanding results in\nterms of speech quality and background noise reduction, the main challenge is\nto obtain compact enough models, which are resource efficient during inference\ntime. An important but often neglected aspect for data-driven methods is that\nresults can be only convincing when tested on real-world data and evaluated\nwith useful metrics. In this work, we investigate reasonably small recurrent\nand convolutional-recurrent network architectures for speech enhancement,\ntrained on a large dataset considering also reverberation. We show interesting\ntradeoffs between computational complexity and the achievable speech quality,\nmeasured on real recordings using a highly accurate MOS estimator. It is shown\nthat the achievable speech quality is a function of network complexity, and\nshow which models have better tradeoffs.", "published": "2021-01-22 18:00:39", "link": "http://arxiv.org/abs/2101.09249v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Understanding the Tradeoffs in Client-side Privacy for Downstream Speech\n  Tasks", "abstract": "As users increasingly rely on cloud-based computing services, it is important\nto ensure that uploaded speech data remains private. Existing solutions rely\neither on server-side methods or focus on hiding speaker identity. While these\napproaches reduce certain security concerns, they do not give users client-side\ncontrol over whether their biometric information is sent to the server. In this\npaper, we formally define client-side privacy and discuss its three unique\ntechnical challenges: (1) direct manipulation of raw data on client devices,\n(2) adaptability with a broad range of server-side processing models, and (3)\nlow time and space complexity for compatibility with limited-bandwidth devices.\nSolving these challenges requires new models that achieve high-fidelity\nreconstruction, privacy preservation of sensitive personal attributes, and\nefficiency during training and inference. As a step towards client-side privacy\nfor speech recognition, we investigate three techniques spanning signal\nprocessing, disentangled representation learning, and adversarial training.\nThrough a series of gender and accent masking tasks, we observe that each\nmethod has its unique strengths, but none manage to effectively balance the\ntrade-offs between performance, privacy, and complexity. These insights call\nfor more research in client-side privacy to ensure a safer deployment of\ncloud-based speech processing services.", "published": "2021-01-22 02:05:19", "link": "http://arxiv.org/abs/2101.08919v2", "categories": ["eess.AS", "cs.CR", "cs.SD"], "primary_category": "eess.AS"}
