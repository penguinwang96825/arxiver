{"title": "VSEC: Transformer-based Model for Vietnamese Spelling Correction", "abstract": "Spelling error correction is one of topics which have a long history in\nnatural language processing. Although previous studies have achieved remarkable\nresults, challenges still exist. In the Vietnamese language, a state-of-the-art\nmethod for the task infers a syllable's context from its adjacent syllables.\nThe method's accuracy can be unsatisfactory, however, because the model may\nlose the context if two (or more) spelling mistakes stand near each other. In\nthis paper, we propose a novel method to correct Vietnamese spelling errors. We\ntackle the problems of mistyped errors and misspelled errors by using a deep\nlearning model. The embedding layer, in particular, is powered by the byte pair\nencoding technique. The sequence to sequence model based on the Transformer\narchitecture makes our approach different from the previous works on the same\nproblem. In the experiment, we train the model with a large synthetic dataset,\nwhich is randomly introduced spelling errors. We test the performance of the\nproposed method using a realistic dataset. This dataset contains 11,202\nhuman-made misspellings in 9,341 different Vietnamese sentences. The\nexperimental results show that our method achieves encouraging performance with\n86.8% errors detected and 81.5% errors corrected, which improves the\nstate-of-the-art approach 5.6% and 2.2%, respectively.", "published": "2021-11-01 00:55:32", "link": "http://arxiv.org/abs/2111.00640v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SADGA: Structure-Aware Dual Graph Aggregation Network for Text-to-SQL", "abstract": "The Text-to-SQL task, aiming to translate the natural language of the\nquestions into SQL queries, has drawn much attention recently. One of the most\nchallenging problems of Text-to-SQL is how to generalize the trained model to\nthe unseen database schemas, also known as the cross-domain Text-to-SQL task.\nThe key lies in the generalizability of (i) the encoding method to model the\nquestion and the database schema and (ii) the question-schema linking method to\nlearn the mapping between words in the question and tables/columns in the\ndatabase schema. Focusing on the above two key issues, we propose a\nStructure-Aware Dual Graph Aggregation Network (SADGA) for cross-domain\nText-to-SQL. In SADGA, we adopt the graph structure to provide a unified\nencoding model for both the natural language question and database schema.\nBased on the proposed unified modeling, we further devise a structure-aware\naggregation method to learn the mapping between the question-graph and\nschema-graph. The structure-aware aggregation method is featured with Global\nGraph Linking, Local Graph Linking, and Dual-Graph Aggregation Mechanism. We\nnot only study the performance of our proposal empirically but also achieved\n3rd place on the challenging Text-to-SQL benchmark Spider at the time of\nwriting.", "published": "2021-11-01 01:50:28", "link": "http://arxiv.org/abs/2111.00653v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Domain Adaptation with Adapter", "abstract": "Unsupervised domain adaptation (UDA) with pre-trained language models (PrLM)\nhas achieved promising results since these pre-trained models embed generic\nknowledge learned from various domains. However, fine-tuning all the parameters\nof the PrLM on a small domain-specific corpus distort the learned generic\nknowledge, and it is also expensive to deployment a whole fine-tuned PrLM for\neach domain. This paper explores an adapter-based fine-tuning approach for\nunsupervised domain adaptation. Specifically, several trainable adapter modules\nare inserted in a PrLM, and the embedded generic knowledge is preserved by\nfixing the parameters of the original PrLM at fine-tuning. A domain-fusion\nscheme is introduced to train these adapters using a mix-domain corpus to\nbetter capture transferable features. Elaborated experiments on two benchmark\ndatasets are carried out, and the results demonstrate that our approach is\neffective with different tasks, dataset sizes, and domain similarities.", "published": "2021-11-01 02:50:53", "link": "http://arxiv.org/abs/2111.00667v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Domain-adaptation of spherical embeddings", "abstract": "Domain adaptation of embedding models, updating a generic embedding to the\nlanguage of a specific domain, is a proven technique for domains that have\ninsufficient data to train an effective model from scratch. Chemistry\npublications is one such domain, where scientific jargon and overloaded\nterminology inhibit the performance of a general language model. The recent\nspherical embedding model (JoSE) proposed in arXiv:1911.01196 jointly learns\nword and document embeddings during training on the multi-dimensional unit\nsphere, which performs well for document classification and word correlation\ntasks. But, we show a non-convergence caused by global rotations during its\ntraining prevents it from domain adaptation. In this work, we develop methods\nto counter the global rotation of the embedding space and propose strategies to\nupdate words and documents during domain specific training. Two new document\nclassification data-sets are collated from general and chemistry scientific\njournals to compare the proposed update training strategies with benchmark\nmodels. We show that our strategies are able to reduce the performance cost of\ndomain adaptation to a level similar to Word2Vec.", "published": "2021-11-01 03:29:36", "link": "http://arxiv.org/abs/2111.00677v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Discourse Comprehension: A Question Answering Framework to Represent\n  Sentence Connections", "abstract": "While there has been substantial progress in text comprehension through\nsimple factoid question answering, more holistic comprehension of a discourse\nstill presents a major challenge (Dunietz et al., 2020). Someone critically\nreflecting on a text as they read it will pose curiosity-driven, often\nopen-ended questions, which reflect deep understanding of the content and\nrequire complex reasoning to answer (Ko et al., 2020; Westera et al., 2020). A\nkey challenge in building and evaluating models for this type of discourse\ncomprehension is the lack of annotated data, especially since collecting\nanswers to such questions requires high cognitive load for annotators. This\npaper presents a novel paradigm that enables scalable data collection targeting\nthe comprehension of news documents, viewing these questions through the lens\nof discourse. The resulting corpus, DCQA (Discourse Comprehension by Question\nAnswering), captures both discourse and semantic links between sentences in the\nform of free-form, open-ended questions. On an evaluation set that we annotated\non questions from Ko et al. (2020), we show that DCQA provides valuable\nsupervision for answering open-ended questions. We additionally design\npre-training methods utilizing existing question-answering resources, and use\nsynthetic data to accommodate unanswerable questions.", "published": "2021-11-01 04:50:26", "link": "http://arxiv.org/abs/2111.00701v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A New Tool for Efficiently Generating Quality Estimation Datasets", "abstract": "Building of data for quality estimation (QE) training is expensive and\nrequires significant human labor. In this study, we focus on a data-centric\napproach while performing QE, and subsequently propose a fully automatic\npseudo-QE dataset generation tool that generates QE datasets by receiving only\nmonolingual or parallel corpus as the input. Consequently, the QE performance\nis enhanced either by data augmentation or by encouraging multiple language\npairs to exploit the applicability of QE. Further, we intend to publicly\nrelease this user friendly QE dataset generation tool as we believe this tool\nprovides a new, inexpensive method to the community for developing QE datasets.", "published": "2021-11-01 08:37:30", "link": "http://arxiv.org/abs/2111.00767v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Discovery of Unaccusative and Unergative Verbs", "abstract": "We present an unsupervised method to detect English unergative and\nunaccusative verbs. These categories allow us to identify verbs participating\nin the causative-inchoative alternation without knowing the semantic roles of\nthe verb. The method is based on the generation of intransitive sentence\nvariants of candidate verbs and probing a language model. We obtained results\non par with similar approaches, with the added benefit of not relying on\nannotated resources.", "published": "2021-11-01 10:21:58", "link": "http://arxiv.org/abs/2111.00808v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhanced Language Representation with Label Knowledge for Span\n  Extraction", "abstract": "Span extraction, aiming to extract text spans (such as words or phrases) from\nplain texts, is a fundamental process in Information Extraction. Recent works\nintroduce the label knowledge to enhance the text representation by formalizing\nthe span extraction task into a question answering problem (QA Formalization),\nwhich achieves state-of-the-art performance. However, QA Formalization does not\nfully exploit the label knowledge and suffers from low efficiency in\ntraining/inference. To address those problems, we introduce a new paradigm to\nintegrate label knowledge and further propose a novel model to explicitly and\nefficiently integrate label knowledge into text representations. Specifically,\nit encodes texts and label annotations independently and then integrates label\nknowledge into text representation with an elaborate-designed semantics fusion\nmodule. We conduct extensive experiments on three typical span extraction\ntasks: flat NER, nested NER, and event detection. The empirical results show\nthat 1) our method achieves state-of-the-art performance on four benchmarks,\nand 2) reduces training time and inference time by 76% and 77% on average,\nrespectively, compared with the QA Formalization paradigm. Our code and data\nare available at https://github.com/Akeepers/LEAR.", "published": "2021-11-01 12:21:05", "link": "http://arxiv.org/abs/2111.00884v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-lingual Hate Speech Detection using Transformer Models", "abstract": "Hate speech detection within a cross-lingual setting represents a paramount\narea of interest for all medium and large-scale online platforms. Failing to\nproperly address this issue on a global scale has already led over time to\nmorally questionable real-life events, human deaths, and the perpetuation of\nhate itself. This paper illustrates the capabilities of fine-tuned altered\nmulti-lingual Transformer models (mBERT, XLM-RoBERTa) regarding this crucial\nsocial data science task with cross-lingual training from English to French,\nvice-versa and each language on its own, including sections about iterative\nimprovement and comparative error analysis.", "published": "2021-11-01 14:42:50", "link": "http://arxiv.org/abs/2111.00981v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Switch Point biased Self-Training: Re-purposing Pretrained Models for\n  Code-Switching", "abstract": "Code-switching (CS), a ubiquitous phenomenon due to the ease of communication\nit offers in multilingual communities still remains an understudied problem in\nlanguage processing. The primary reasons behind this are: (1) minimal efforts\nin leveraging large pretrained multilingual models, and (2) the lack of\nannotated data. The distinguishing case of low performance of multilingual\nmodels in CS is the intra-sentence mixing of languages leading to switch\npoints. We first benchmark two sequence labeling tasks -- POS and NER on 4\ndifferent language pairs with a suite of pretrained models to identify the\nproblems and select the best performing model, char-BERT, among them\n(addressing (1)). We then propose a self training method to repurpose the\nexisting pretrained models using a switch-point bias by leveraging unannotated\ndata (addressing (2)). We finally demonstrate that our approach performs well\non both tasks by reducing the gap between the switch point performance while\nretaining the overall performance on two distinct language pairs in both the\ntasks. Our code is available here:\nhttps://github.com/PC09/EMNLP2021-Switch-Point-biased-Self-Training.", "published": "2021-11-01 19:42:08", "link": "http://arxiv.org/abs/2111.01231v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparative Study of Long Document Classification", "abstract": "The amount of information stored in the form of documents on the internet has\nbeen increasing rapidly. Thus it has become a necessity to organize and\nmaintain these documents in an optimum manner. Text classification algorithms\nstudy the complex relationships between words in a text and try to interpret\nthe semantics of the document. These algorithms have evolved significantly in\nthe past few years. There has been a lot of progress from simple machine\nlearning algorithms to transformer-based architectures. However, existing\nliterature has analyzed different approaches on different data sets thus making\nit difficult to compare the performance of machine learning algorithms. In this\nwork, we revisit long document classification using standard machine learning\napproaches. We benchmark approaches ranging from simple Naive Bayes to complex\nBERT on six standard text classification datasets. We present an exhaustive\ncomparison of different algorithms on a range of long document datasets. We\nre-iterate that long document classification is a simpler task and even basic\nalgorithms perform competitively with BERT-based approaches on most of the\ndatasets. The BERT-based models perform consistently well on all the datasets\nand can be blindly used for the document classification task when the\ncomputations cost is not a concern. In the shallow model's category, we suggest\nthe usage of raw BiLSTM + Max architecture which performs decently across all\nthe datasets. Even simpler Glove + Attention bag of words model can be utilized\nfor simpler use cases. The importance of using sophisticated models is clearly\nvisible in the IMDB sentiment dataset which is a comparatively harder task.", "published": "2021-11-01 04:51:51", "link": "http://arxiv.org/abs/2111.00702v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Outlining and Filling: Hierarchical Query Graph Generation for Answering\n  Complex Questions over Knowledge Graphs", "abstract": "Query graph construction aims to construct the correct executable SPARQL on\nthe KG to answer natural language questions. Although recent methods have\nachieved good results using neural network-based query graph ranking, they\nsuffer from three new challenges when handling more complex questions: 1)\ncomplicated SPARQL syntax, 2) huge search space, and 3) locally ambiguous query\ngraphs. In this paper, we provide a new solution. As a preparation, we extend\nthe query graph by treating each SPARQL clause as a subgraph consisting of\nvertices and edges and define a unified graph grammar called AQG to describe\nthe structure of query graphs. Based on these concepts, we propose a novel\nend-to-end model that performs hierarchical autoregressive decoding to generate\nquery graphs. The high-level decoding generates an AQG as a constraint to prune\nthe search space and reduce the locally ambiguous query graph. The bottom-level\ndecoding accomplishes the query graph construction by selecting appropriate\ninstances from the preprepared candidates to fill the slots in the AQG. The\nexperimental results show that our method greatly improves the SOTA performance\non complex KGQA benchmarks. Equipped with pre-trained models, the performance\nof our method is further improved, achieving SOTA for all three datasets used.", "published": "2021-11-01 07:08:46", "link": "http://arxiv.org/abs/2111.00732v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Deep Learning Transformer Architecture for Named Entity Recognition on\n  Low Resourced Languages: State of the art results", "abstract": "This paper reports on the evaluation of Deep Learning (DL) transformer\narchitecture models for Named-Entity Recognition (NER) on ten low-resourced\nSouth African (SA) languages. In addition, these DL transformer models were\ncompared to other Neural Network and Machine Learning (ML) NER models. The\nfindings show that transformer models substantially improve performance when\napplying discrete fine-tuning parameters per language. Furthermore, fine-tuned\ntransformer models outperform other neural network and machine learning models\non NER with the low-resourced SA languages. For example, the transformer models\nobtained the highest F-scores for six of the ten SA languages and the highest\naverage F-score surpassing the Conditional Random Fields ML model. Practical\nimplications include developing high-performance NER capability with less\neffort and resource costs, potentially improving downstream NLP tasks such as\nMachine Translation (MT). Therefore, the application of DL transformer\narchitecture models for NLP NER sequence tagging tasks on low-resourced SA\nlanguages is viable. Additional research could evaluate the more recent\ntransformer architecture models on other Natural Language Processing tasks and\napplications, such as Phrase chunking, MT, and Part-of-Speech tagging.", "published": "2021-11-01 11:02:01", "link": "http://arxiv.org/abs/2111.00830v2", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Exploring Non-Autoregressive End-To-End Neural Modeling For English\n  Mispronunciation Detection And Diagnosis", "abstract": "End-to-end (E2E) neural modeling has emerged as one predominant school of\nthought to develop computer-assisted language training (CAPT) systems, showing\ncompetitive performance to conventional pronunciation-scoring based methods.\nHowever, current E2E neural methods for CAPT are faced with at least two\npivotal challenges. On one hand, most of the E2E methods operate in an\nautoregressive manner with left-to-right beam search to dictate the\npronunciations of an L2 learners. This however leads to very slow inference\nspeed, which inevitably hinders their practical use. On the other hand, E2E\nneural methods are normally data greedy and meanwhile an insufficient amount of\nnonnative training data would often reduce their efficacy on mispronunciation\ndetection and diagnosis (MD&D). In response, we put forward a novel MD&D method\nthat leverages non-autoregressive (NAR) E2E neural modeling to dramatically\nspeed up the inference time while maintaining performance in line with the\nconventional E2E neural methods. In addition, we design and develop a\npronunciation modeling network stacked on top of the NAR E2E models of our\nmethod to further boost the effectiveness of MD&D. Empirical experiments\nconducted on the L2-ARCTIC English dataset seems to validate the feasibility of\nour method, in comparison to some top-of-the-line E2E models and an iconic\npronunciation-scoring based method built on a DNN-HMM acoustic model.", "published": "2021-11-01 11:23:48", "link": "http://arxiv.org/abs/2111.00844v2", "categories": ["cs.CL", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Interpretable contrastive word mover's embedding", "abstract": "This paper shows that a popular approach to the supervised embedding of\ndocuments for classification, namely, contrastive Word Mover's Embedding, can\nbe significantly enhanced by adding interpretability. This interpretability is\nachieved by incorporating a clustering promoting mechanism into the contrastive\nloss. On several public datasets, we show that our method improves\nsignificantly upon existing baselines while providing interpretation to the\nclusters via identifying a set of keywords that are the most representative of\na particular class. Our approach was motivated in part by the need to develop\nNatural Language Processing (NLP) methods for the \\textit{novel problem of\nassessing student work for scientific writing and thinking} - a problem that is\ncentral to the area of (educational) Learning Sciences (LS). In this context,\nwe show that our approach leads to a meaningful assessment of the student work\nrelated to lab reports from a biology class and can help LS researchers gain\ninsights into student understanding and assess evidence of scientific thought\nprocesses.", "published": "2021-11-01 15:27:27", "link": "http://arxiv.org/abs/2111.01023v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Transformers for prompt-level EMA non-response prediction", "abstract": "Ecological Momentary Assessments (EMAs) are an important psychological data\nsource for measuring current cognitive states, affect, behavior, and\nenvironmental factors from participants in mobile health (mHealth) studies and\ntreatment programs. Non-response, in which participants fail to respond to EMA\nprompts, is an endemic problem. The ability to accurately predict non-response\ncould be utilized to improve EMA delivery and develop compliance interventions.\nPrior work has explored classical machine learning models for predicting\nnon-response. However, as increasingly large EMA datasets become available,\nthere is the potential to leverage deep learning models that have been\neffective in other fields. Recently, transformer models have shown\nstate-of-the-art performance in NLP and other domains. This work is the first\nto explore the use of transformers for EMA data analysis. We address three key\nquestions in applying transformers to EMA data: 1. Input representation, 2.\nencoding temporal information, 3. utility of pre-training on improving\ndownstream prediction task performance. The transformer model achieves a\nnon-response prediction AUC of 0.77 and is significantly better than classical\nML and LSTM-based deep learning models. We will make our a predictive model\ntrained on a corpus of 40K EMA samples freely-available to the research\ncommunity, in order to facilitate the development of future transformer-based\nEMA analysis works.", "published": "2021-11-01 18:38:47", "link": "http://arxiv.org/abs/2111.01193v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Comparative Explanations of Recommendations", "abstract": "As recommendation is essentially a comparative (or ranking) process, a good\nexplanation should illustrate to users why an item is believed to be better\nthan another, i.e., comparative explanations about the recommended items.\nIdeally, after reading the explanations, a user should reach the same ranking\nof items as the system's. Unfortunately, little research attention has yet been\npaid on such comparative explanations.\n  In this work, we develop an extract-and-refine architecture to explain the\nrelative comparisons among a set of ranked items from a recommender system. For\neach recommended item, we first extract one sentence from its associated\nreviews that best suits the desired comparison against a set of reference\nitems. Then this extracted sentence is further articulated with respect to the\ntarget user through a generative model to better explain why the item is\nrecommended. We design a new explanation quality metric based on BLEU to guide\nthe end-to-end training of the extraction and refinement components, which\navoids generation of generic content. Extensive offline evaluations on two\nlarge recommendation benchmark datasets and serious user studies against an\narray of state-of-the-art explainable recommendation algorithms demonstrate the\nnecessity of comparative explanations and the effectiveness of our solution.", "published": "2021-11-01 02:55:56", "link": "http://arxiv.org/abs/2111.00670v3", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Transductive Data Augmentation with Relational Path Rule Mining for\n  Knowledge Graph Embedding", "abstract": "For knowledge graph completion, two major types of prediction models exist:\none based on graph embeddings, and the other based on relation path rule\ninduction. They have different advantages and disadvantages. To take advantage\nof both types, hybrid models have been proposed recently. One of the hybrid\nmodels, UniKER, alternately augments training data by relation path rules and\ntrains an embedding model. Despite its high prediction accuracy, it does not\ntake full advantage of relation path rules, as it disregards low-confidence\nrules in order to maintain the quality of augmented data. To mitigate this\nlimitation, we propose transductive data augmentation by relation path rules\nand confidence-based weighting of augmented data. The results and analysis show\nthat our proposed method effectively improves the performance of the embedding\nmodel by augmenting data that include true answers or entities similar to them.", "published": "2021-11-01 14:35:14", "link": "http://arxiv.org/abs/2111.00974v1", "categories": ["cs.LG", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "A transfer learning based approach for pronunciation scoring", "abstract": "Phone-level pronunciation scoring is a challenging task, with performance far\nfrom that of human annotators. Standard systems generate a score for each phone\nin a phrase using models trained for automatic speech recognition (ASR) with\nnative data only. Better performance has been shown when using systems that are\ntrained specifically for the task using non-native data. Yet, such systems face\nthe challenge that datasets labelled for this task are scarce and usually\nsmall. In this paper, we present a transfer learning-based approach that\nleverages a model trained for ASR, adapting it for the task of pronunciation\nscoring. We analyze the effect of several design choices and compare the\nperformance with a state-of-the-art goodness of pronunciation (GOP) system. Our\nfinal system is 20% better than the GOP system on EpaDB, a database for\npronunciation scoring research, for a cost function that prioritizes low rates\nof unnecessary corrections.", "published": "2021-11-01 14:37:06", "link": "http://arxiv.org/abs/2111.00976v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Introspective Distillation for Robust Question Answering", "abstract": "Question answering (QA) models are well-known to exploit data bias, e.g., the\nlanguage prior in visual QA and the position bias in reading comprehension.\nRecent debiasing methods achieve good out-of-distribution (OOD)\ngeneralizability with a considerable sacrifice of the in-distribution (ID)\nperformance. Therefore, they are only applicable in domains where the test\ndistribution is known in advance. In this paper, we present a novel debiasing\nmethod called Introspective Distillation (IntroD) to make the best of both\nworlds for QA. Our key technical contribution is to blend the inductive bias of\nOOD and ID by introspecting whether a training sample fits in the factual ID\nworld or the counterfactual OOD one. Experiments on visual QA datasets VQA v2,\nVQA-CP, and reading comprehension dataset SQuAD demonstrate that our proposed\nIntroD maintains the competitive OOD performance compared to other debiasing\nmethods, while sacrificing little or even achieving better ID performance\ncompared to the non-debiasing ones.", "published": "2021-11-01 15:30:15", "link": "http://arxiv.org/abs/2111.01026v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "ASMDD: Arabic Speech Mispronunciation Detection Dataset", "abstract": "The largest dataset of Arabic speech mispronunciation detections in Egyptian\ndialogues is introduced. The dataset is composed of annotated audio files\nrepresenting the top 100 words that are most frequently used in the Arabic\nlanguage, pronounced by 100 Egyptian children (aged between 2 and 8 years old).\nThe dataset is collected and annotated on segmental pronunciation error\ndetections by expert listeners.", "published": "2021-11-01 16:50:07", "link": "http://arxiv.org/abs/2111.01136v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NI"], "primary_category": "cs.CL"}
{"title": "Evaluating robustness of You Only Hear Once(YOHO) Algorithm on noisy\n  audios in the VOICe Dataset", "abstract": "Sound event detection (SED) in machine listening entails identifying the\ndifferent sounds in an audio file and identifying the start and end time of a\nparticular sound event in the audio. SED finds use in various applications such\nas audio surveillance, speech recognition, and context-based indexing and\nretrieval of data in a multimedia database. However, in real-life scenarios,\nthe audios from various sources are seldom devoid of any interfering noise or\ndisturbance. In this paper, we test the performance of the You Only Hear Once\n(YOHO) algorithm on noisy audio data. Inspired by the You Only Look Once (YOLO)\nalgorithm in computer vision, the YOHO algorithm can match the performance of\nthe various state-of-the-art algorithms on datasets such as Music Speech\nDetection Dataset, TUT Sound Event, and Urban-SED datasets but at lower\ninference times. In this paper, we explore the performance of the YOHO\nalgorithm on the VOICe dataset containing audio files with noise at different\nsound-to-noise ratios (SNR). YOHO could outperform or at least match the best\nperforming SED algorithms reported in the VOICe dataset paper and make\ninferences in less time.", "published": "2021-11-01 18:58:50", "link": "http://arxiv.org/abs/2111.01205v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Identifying causal relations in tweets using deep learning: Use case on\n  diabetes-related tweets from 2017-2021", "abstract": "Objective: Leveraging machine learning methods, we aim to extract both\nexplicit and implicit cause-effect associations in patient-reported,\ndiabetes-related tweets and provide a tool to better understand opinion,\nfeelings and observations shared within the diabetes online community from a\ncausality perspective. Materials and Methods: More than 30 million\ndiabetes-related tweets in English were collected between April 2017 and\nJanuary 2021. Deep learning and natural language processing methods were\napplied to focus on tweets with personal and emotional content. A\ncause-effect-tweet dataset was manually labeled and used to train 1) a\nfine-tuned Bertweet model to detect causal sentences containing a causal\nassociation 2) a CRF model with BERT based features to extract possible\ncause-effect associations. Causes and effects were clustered in a\nsemi-supervised approach and visualised in an interactive cause-effect-network.\nResults: Causal sentences were detected with a recall of 68% in an imbalanced\ndataset. A CRF model with BERT based features outperformed a fine-tuned BERT\nmodel for cause-effect detection with a macro recall of 68%. This led to 96,676\nsentences with cause-effect associations. \"Diabetes\" was identified as the\ncentral cluster followed by \"Death\" and \"Insulin\". Insulin pricing related\ncauses were frequently associated with \"Death\". Conclusions: A novel\nmethodology was developed to detect causal sentences and identify both explicit\nand implicit, single and multi-word cause and corresponding effect as expressed\nin diabetes-related tweets leveraging BERT-based architectures and visualised\nas cause-effect-network. Extracting causal associations on real-life, patient\nreported outcomes in social media data provides a useful complementary source\nof information in diabetes research.", "published": "2021-11-01 19:25:35", "link": "http://arxiv.org/abs/2111.01225v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Low-Cost Algorithmic Recourse for Users With Uncertain Cost Functions", "abstract": "People affected by machine learning model decisions may benefit greatly from\naccess to recourses, i.e. suggestions about what features they could change to\nreceive a more favorable decision from the model. Current approaches try to\noptimize for the cost incurred by users when adopting a recourse, but they\nassume that all users share the same cost function. This is an unrealistic\nassumption because users might have diverse preferences about their willingness\nto change certain features. In this work, we introduce a new method for\nidentifying recourse sets for users which does not assume that users'\npreferences are known in advance. We propose an objective function, Expected\nMinimum Cost (EMC), based on two key ideas: (1) when presenting a set of\noptions to a user, there only needs to be one low-cost solution that the user\ncould adopt; (2) when we do not know the user's true cost function, we can\napproximately optimize for user satisfaction by first sampling plausible cost\nfunctions from a distribution, then finding a recourse set that achieves a good\ncost for these samples. We optimize EMC with a novel discrete optimization\nalgorithm, Cost Optimized Local Search (COLS), which is guaranteed to improve\nthe recourse set quality over iterations. Experimental evaluation on popular\nreal-world datasets with simulated users demonstrates that our method satisfies\nup to 25.89 percentage points more users compared to strong baseline methods,\nwhile, the human evaluation shows that our recourses are preferred more than\ntwice as often as the strongest baseline recourses. Finally, using standard\nfairness metrics we show that our method can provide more fair solutions across\ndemographic groups than baselines. We provide our source code at:\nhttps://github.com/prateeky2806/EMC-COLS-recourse", "published": "2021-11-01 19:49:35", "link": "http://arxiv.org/abs/2111.01235v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Recent Advances in Natural Language Processing via Large Pre-Trained\n  Language Models: A Survey", "abstract": "Large, pre-trained transformer-based language models such as BERT have\ndrastically changed the Natural Language Processing (NLP) field. We present a\nsurvey of recent work that uses these large language models to solve NLP tasks\nvia pre-training then fine-tuning, prompting, or text generation approaches. We\nalso present approaches that use pre-trained language models to generate data\nfor training augmentation or other purposes. We conclude with discussions on\nlimitations and suggested directions for future research.", "published": "2021-11-01 20:08:05", "link": "http://arxiv.org/abs/2111.01243v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sequence Transduction with Graph-based Supervision", "abstract": "The recurrent neural network transducer (RNN-T) objective plays a major role\nin building today's best automatic speech recognition (ASR) systems for\nproduction. Similarly to the connectionist temporal classification (CTC)\nobjective, the RNN-T loss uses specific rules that define how a set of\nalignments is generated to form a lattice for the full-sum training. However,\nit is yet largely unknown if these rules are optimal and do lead to the best\npossible ASR results. In this work, we present a new transducer objective\nfunction that generalizes the RNN-T loss to accept a graph representation of\nthe labels, thus providing a flexible and efficient framework to manipulate\ntraining lattices, e.g., for studying different transition rules, implementing\ndifferent transducer losses, or restricting alignments. We demonstrate that\ntransducer-based ASR with CTC-like lattice achieves better results compared to\nstandard RNN-T, while also ensuring a strictly monotonic alignment, which will\nallow better optimization of the decoding procedure. For example, the proposed\nCTC-like transducer achieves an improvement of 4.8% on the test-other condition\nof LibriSpeech relative to an equivalent RNN-T based system.", "published": "2021-11-01 21:51:42", "link": "http://arxiv.org/abs/2111.01272v2", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "ParsiNorm: A Persian Toolkit for Speech Processing Normalization", "abstract": "In general, speech processing models consist of a language model along with\nan acoustic model. Regardless of the language model's complexity and variants,\nthree critical pre-processing steps are needed in language models: cleaning,\nnormalization, and tokenization. Among mentioned steps, the normalization step\nis so essential to format unification in pure textual applications. However,\nfor embedded language models in speech processing modules, normalization is not\nlimited to format unification. Moreover, it has to convert each readable\nsymbol, number, etc., to how they are pronounced. To the best of our knowledge,\nthere is no Persian normalization toolkits for embedded language models in\nspeech processing modules, So in this paper, we propose an open-source\nnormalization toolkit for text processing in speech applications. Briefly, we\nconsider different readable Persian text like symbols (common currencies, #, @,\nURL, etc.), numbers (date, time, phone number, national code, etc.), and so on.\nComparison with other available Persian textual normalization tools indicates\nthe superiority of the proposed method in speech processing. Also, comparing\nthe model's performance for one of the proposed functions (sentence separation)\nwith other common natural language libraries such as HAZM and Parsivar\nindicates the proper performance of the proposed method. Besides, its\nevaluation of some Persian Wikipedia data confirms the proper performance of\nthe proposed method.", "published": "2021-11-01 17:41:01", "link": "http://arxiv.org/abs/2111.03470v2", "categories": ["eess.AS", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "SNRi Target Training for Joint Speech Enhancement and Recognition", "abstract": "Speech enhancement (SE) is used as a frontend in speech applications\nincluding automatic speech recognition (ASR) and telecommunication. A\ndifficulty in using the SE frontend is that the appropriate noise reduction\nlevel differs depending on applications and/or noise characteristics. In this\nstudy, we propose \"signal-to-noise ratio improvement (SNRi) target training\";\nthe SE frontend is trained to output a signal whose SNRi is controlled by an\nauxiliary scalar input. In joint training with a backend, the target SNRi value\nis estimated by an auxiliary network. By training all networks to minimize the\nbackend task loss, we can estimate the appropriate noise reduction level for\neach noisy input in a data-driven scheme. Our experiments showed that the SNRi\ntarget training enables control of the output SNRi. In addition, the proposed\njoint training relatively reduces word error rate by 4.0\\% and 5.7\\% compared\nto a Conformer-based standard ASR model and conventional SE-ASR joint training\nmodel, respectively. Furthermore, by analyzing the predicted target SNRi, we\nobserved the jointly trained network automatically controls the target SNRi\naccording to noise characteristics. Audio demos are available in our demo page:\ngoogle.github.io/df-conformer/snri_target/.", "published": "2021-11-01 08:26:12", "link": "http://arxiv.org/abs/2111.00764v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Novel 1D State Space for Efficient Music Rhythmic Analysis", "abstract": "Inferring music time structures has a broad range of applications in music\nproduction, processing and analysis. Scholars have proposed various methods to\nanalyze different aspects of time structures, such as beat, downbeat, tempo and\nmeter. Many state-of-the-art (SOFA) methods, however, are computationally\nexpensive. This makes them inapplicable in real-world industrial settings where\nthe scale of the music collections can be millions. This paper proposes a new\nstate space and a semi-Markov model for music time structure analysis. The\nproposed approach turns the commonly used 2D state spaces into a 1D model\nthrough a jump-back reward strategy. It reduces the state spaces size\ndrastically. We then utilize the proposed method for causal, joint beat,\ndownbeat, tempo, and meter tracking, and compare it against several previous\nmethods. The proposed method delivers similar performance with the SOFA joint\ncausal models with a much smaller state space and a more than 30 times speedup.", "published": "2021-11-01 04:54:01", "link": "http://arxiv.org/abs/2111.00704v2", "categories": ["cs.SD", "cs.IR", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "RefineGAN: Universally Generating Waveform Better than Ground Truth with\n  Highly Accurate Pitch and Intensity Responses", "abstract": "Most GAN(Generative Adversarial Network)-based approaches towards\nhigh-fidelity waveform generation heavily rely on discriminators to improve\ntheir performance. However, GAN methods introduce much uncertainty into the\ngeneration process and often result in mismatches of pitch and intensity, which\nis fatal when it comes to sensitive use cases such as singing voice\nsynthesis(SVS). To address this problem, we propose RefineGAN, a high-fidelity\nneural vocoder focused on the robustness, pitch and intensity accuracy, and\nhigh-speed full-band audio generation. We applyed a pitch-guided refine\narchitecture with a multi-scale spectrogram-based loss function to help\nstabilize the training process and maintain the robustness of the neural\nvocoder while using the GAN-based training method. Audio generated using this\nmethod shows a better performance in subjective tests when compared with the\nground-truth audio. This result shows that the fidelity is even improved during\nthe waveform reconstruction by eliminating defects produced by recording\nprocedures. Moreover, it shows that models trained on a specified type of data\ncan perform on totally unseen language and unseen speaker identically well.\nGenerated sample pairs are provided\nonhttps://timedomain-tech.github.io/refinegan/.", "published": "2021-11-01 14:12:54", "link": "http://arxiv.org/abs/2111.00962v3", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "With a Little Help from my Temporal Context: Multimodal Egocentric\n  Action Recognition", "abstract": "In egocentric videos, actions occur in quick succession. We capitalise on the\naction's temporal context and propose a method that learns to attend to\nsurrounding actions in order to improve recognition performance. To incorporate\nthe temporal context, we propose a transformer-based multimodal model that\ningests video and audio as input modalities, with an explicit language model\nproviding action sequence context to enhance the predictions. We test our\napproach on EPIC-KITCHENS and EGTEA datasets reporting state-of-the-art\nperformance. Our ablations showcase the advantage of utilising temporal context\nas well as incorporating audio input modality and language model to rescore\npredictions. Code and models at: https://github.com/ekazakos/MTCN.", "published": "2021-11-01 15:27:35", "link": "http://arxiv.org/abs/2111.01024v1", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Learning To Generate Piano Music With Sustain Pedals", "abstract": "Recent years have witnessed a growing interest in research related to the\ndetection of piano pedals from audio signals in the music information retrieval\ncommunity. However, to our best knowledge, recent generative models for\nsymbolic music have rarely taken piano pedals into account. In this work, we\nemploy the transcription model proposed by Kong et al. to get pedal information\nfrom the audio recordings of piano performance in the AILabs1k7 dataset, and\nthen modify the Compound Word Transformer proposed by Hsiao et al. to build a\nTransformer decoder that generates pedal-related tokens along with other\nmusical tokens. While the work is done by using inferred sustain pedal\ninformation as training data, the result shows hope for further improvement and\nthe importance of the involvement of sustain pedal in tasks of piano\nperformance generations.", "published": "2021-11-01 19:12:48", "link": "http://arxiv.org/abs/2111.01216v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
