{"title": "Query of CC: Unearthing Large Scale Domain-Specific Knowledge from\n  Public Corpora", "abstract": "Large language models have demonstrated remarkable potential in various\ntasks, however, there remains a significant scarcity of open-source models and\ndata for specific domains. Previous works have primarily focused on manually\nspecifying resources and collecting high-quality data on specific domains,\nwhich significantly consume time and effort. To address this limitation, we\npropose an efficient data collection method $\\textit{Query of CC}$ based on\nlarge language models. This method bootstraps seed information through a large\nlanguage model and retrieves related data from public corpora. It not only\ncollects knowledge-related data for specific domains but unearths the data with\npotential reasoning procedures. Through the application of this method, we have\ncurated a high-quality dataset called KNOWLEDGE PILE, encompassing four major\ndomains, including stem and humanities sciences, among others. Experimental\nresults demonstrate that KNOWLEDGE PILE significantly improves the performance\nof large language models in mathematical and knowledge-related reasoning\nability tests. To facilitate academic sharing, we open-source our dataset and\ncode, providing valuable support to the academic community.", "published": "2024-01-26 03:38:23", "link": "http://arxiv.org/abs/2401.14624v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Toward Practical Automatic Speech Recognition and Post-Processing: a\n  Call for Explainable Error Benchmark Guideline", "abstract": "Automatic speech recognition (ASR) outcomes serve as input for downstream\ntasks, substantially impacting the satisfaction level of end-users. Hence, the\ndiagnosis and enhancement of the vulnerabilities present in the ASR model bear\nsignificant importance. However, traditional evaluation methodologies of ASR\nsystems generate a singular, composite quantitative metric, which fails to\nprovide comprehensive insight into specific vulnerabilities. This lack of\ndetail extends to the post-processing stage, resulting in further obfuscation\nof potential weaknesses. Despite an ASR model's ability to recognize utterances\naccurately, subpar readability can negatively affect user satisfaction, giving\nrise to a trade-off between recognition accuracy and user-friendliness. To\neffectively address this, it is imperative to consider both the speech-level,\ncrucial for recognition accuracy, and the text-level, critical for\nuser-friendliness. Consequently, we propose the development of an Error\nExplainable Benchmark (EEB) dataset. This dataset, while considering both\nspeech- and text-level, enables a granular understanding of the model's\nshortcomings. Our proposition provides a structured pathway for a more\n`real-world-centric' evaluation, a marked shift away from abstracted,\ntraditional methods, allowing for the detection and rectification of nuanced\nsystem weaknesses, ultimately aiming for an improved user experience.", "published": "2024-01-26 03:42:45", "link": "http://arxiv.org/abs/2401.14625v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "T-Rex: Text-assisted Retrosynthesis Prediction", "abstract": "As a fundamental task in computational chemistry, retrosynthesis prediction\naims to identify a set of reactants to synthesize a target molecule. Existing\ntemplate-free approaches only consider the graph structures of the target\nmolecule, which often cannot generalize well to rare reaction types and large\nmolecules. Here, we propose T-Rex, a text-assisted retrosynthesis prediction\napproach that exploits pre-trained text language models, such as ChatGPT, to\nassist the generation of reactants. T-Rex first exploits ChatGPT to generate a\ndescription for the target molecule and rank candidate reaction centers based\nboth the description and the molecular graph. It then re-ranks these candidates\nby querying the descriptions for each reactants and examines which group of\nreactants can best synthesize the target molecule. We observed that T-Rex\nsubstantially outperformed graph-based state-of-the-art approaches on two\ndatasets, indicating the effectiveness of considering text information. We\nfurther found that T-Rex outperformed the variant that only use ChatGPT-based\ndescription without the re-ranking step, demonstrate how our framework\noutperformed a straightforward integration of ChatGPT and graph information.\nCollectively, we show that text generated by pre-trained language models can\nsubstantially improve retrosynthesis prediction, opening up new avenues for\nexploiting ChatGPT to advance computational chemistry. And the codes can be\nfound at https://github.com/lauyikfung/T-Rex.", "published": "2024-01-26 04:08:50", "link": "http://arxiv.org/abs/2401.14637v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Benchmarking Large Language Models in Complex Question Answering\n  Attribution using Knowledge Graphs", "abstract": "The attribution of question answering is to provide citations for supporting\ngenerated statements, and has attracted wide research attention. The current\nmethods for automatically evaluating the attribution, which are often based on\nLarge Language Models (LLMs), are still inadequate, particularly in recognizing\nsubtle differences between attributions, and complex relationships between\ncitations and statements. To compare these attribution evaluation methods and\ndevelop new ones, we introduce a set of fine-grained categories (i.e.,\nsupportive, insufficient, contradictory and irrelevant) for measuring the\nattribution, and develop a Complex Attributed Question Answering (CAQA)\nbenchmark by leveraging knowledge graphs (KGs) for automatically generating\nattributions of different categories to question-answer pairs. Our analysis\nreveals that existing evaluators perform poorly under fine-grained attribution\nsettings and exhibit weaknesses in complex citation-statement reasoning. Our\nCAQA benchmark, validated with human annotations, emerges as a promising tool\nfor selecting and developing LLM attribution evaluators.", "published": "2024-01-26 04:11:07", "link": "http://arxiv.org/abs/2401.14640v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Scientific Large Language Models: A Survey on Biological & Chemical\n  Domains", "abstract": "Large Language Models (LLMs) have emerged as a transformative power in\nenhancing natural language comprehension, representing a significant stride\ntoward artificial general intelligence. The application of LLMs extends beyond\nconventional linguistic boundaries, encompassing specialized linguistic systems\ndeveloped within various scientific disciplines. This growing interest has led\nto the advent of scientific LLMs, a novel subclass specifically engineered for\nfacilitating scientific discovery. As a burgeoning area in the community of AI\nfor Science, scientific LLMs warrant comprehensive exploration. However, a\nsystematic and up-to-date survey introducing them is currently lacking. In this\npaper, we endeavor to methodically delineate the concept of \"scientific\nlanguage\", whilst providing a thorough review of the latest advancements in\nscientific LLMs. Given the expansive realm of scientific disciplines, our\nanalysis adopts a focused lens, concentrating on the biological and chemical\ndomains. This includes an in-depth examination of LLMs for textual knowledge,\nsmall molecules, macromolecular proteins, genomic sequences, and their\ncombinations, analyzing them in terms of model architectures, capabilities,\ndatasets, and evaluation. Finally, we critically examine the prevailing\nchallenges and point out promising research directions along with the advances\nof LLMs. By offering a comprehensive overview of technical developments in this\nfield, this survey aspires to be an invaluable resource for researchers\nnavigating the intricate landscape of scientific LLMs.", "published": "2024-01-26 05:33:34", "link": "http://arxiv.org/abs/2401.14656v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MaLLaM -- Malaysia Large Language Model", "abstract": "Addressing the gap in Large Language Model pretrained from scratch with\nMalaysian context, We trained models with 1.1 billion, 3 billion, and 5 billion\nparameters on a substantial 349GB dataset, equivalent to 90 billion tokens\nbased on our pretrained Byte Pair Encoding (BPE) tokenizer for a single epoch.\nMaLLaM contributes to enhanced natural language understanding and generation\ntasks in the Malay language. Although trained on a smaller dataset of 90\nbillion tokens, our instruction-tuned MaLLaM models perform competitively. When\ncompared to ChatGPT3.5 and Malaysian Mistral, MaLLaM's instruction-tuned models\ndemonstrate notable proficiency, underscoring the effectiveness of our approach\nin capturing and understanding the nuances of the Malaysian language. MaLLaM\nmodels mark a significant contribution to the field, providing comprehensive\nlanguage representations grounded in Malaysian context. This endeavor aims to\npave the way for enhanced natural language understanding and generation tasks\nspecific to the linguistic nuances present in Malaysia. We discuss the training\nmethodology, dataset composition, and the potential impact of MaLLaM in\nadvancing the capabilities of large language models within the context of the\nMalay language.\n  All models released at\nhttps://huggingface.co/collections/mesolitica/mallam-6577b59d1e0b436ae75f930f", "published": "2024-01-26 06:56:05", "link": "http://arxiv.org/abs/2401.14680v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MasonTigers@LT-EDI-2024: An Ensemble Approach Towards Detecting\n  Homophobia and Transphobia in Social Media Comments", "abstract": "In this paper, we describe our approaches and results for Task 2 of the\nLT-EDI 2024 Workshop, aimed at detecting homophobia and/or transphobia across\nten languages. Our methodologies include monolingual transformers and ensemble\nmethods, capitalizing on the strengths of each to enhance the performance of\nthe models. The ensemble models worked well, placing our team, MasonTigers, in\nthe top five for eight of the ten languages, as measured by the macro F1 score.\nOur work emphasizes the efficacy of ensemble methods in multilingual scenarios,\naddressing the complexities of language-specific tasks.", "published": "2024-01-26 06:56:17", "link": "http://arxiv.org/abs/2401.14681v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Taiyi-Diffusion-XL: Advancing Bilingual Text-to-Image Generation with\n  Large Vision-Language Model Support", "abstract": "Recent advancements in text-to-image models have significantly enhanced image\ngeneration capabilities, yet a notable gap of open-source models persists in\nbilingual or Chinese language support. To address this need, we present\nTaiyi-Diffusion-XL, a new Chinese and English bilingual text-to-image model\nwhich is developed by extending the capabilities of CLIP and\nStable-Diffusion-XL through a process of bilingual continuous pre-training.\nThis approach includes the efficient expansion of vocabulary by integrating the\nmost frequently used Chinese characters into CLIP's tokenizer and embedding\nlayers, coupled with an absolute position encoding expansion. Additionally, we\nenrich text prompts by large vision-language model, leading to better images\ncaptions and possess higher visual quality. These enhancements are subsequently\napplied to downstream text-to-image models. Our empirical results indicate that\nthe developed CLIP model excels in bilingual image-text retrieval.Furthermore,\nthe bilingual image generation capabilities of Taiyi-Diffusion-XL surpass\nprevious models. This research leads to the development and open-sourcing of\nthe Taiyi-Diffusion-XL model, representing a notable advancement in the field\nof image generation, particularly for Chinese language applications. This\ncontribution is a step forward in addressing the need for more diverse language\nsupport in multimodal research. The model and demonstration are made publicly\navailable at\n\\href{https://huggingface.co/IDEA-CCNL/Taiyi-Stable-Diffusion-XL-3.5B/},\nfostering further research and collaboration in this domain.", "published": "2024-01-26 07:17:50", "link": "http://arxiv.org/abs/2401.14688v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "F-Eval: Assessing Fundamental Abilities with Refined Evaluation Methods", "abstract": "Large language models (LLMs) garner significant attention for their\nunprecedented performance, leading to an increasing number of researches\nevaluating LLMs. However, these evaluation benchmarks are limited to assessing\nthe instruction-following capabilities, overlooking the fundamental abilities\nthat emerge during the pre-training stage. Previous subjective evaluation\nmethods mainly reply on scoring by API models. However, in the absence of\nreferences, large models have shown limited ability to discern subtle\ndifferences. To bridge the gap, we propose F-Eval, a bilingual evaluation\nbenchmark to evaluate the fundamental abilities, including expression,\ncommonsense and logic. The tasks in F-Eval include multi-choice objective\ntasks, open-ended objective tasks, reference-based subjective tasks and\nreference-free subjective tasks. For reference-free subjective tasks, we devise\nnew evaluation methods, serving as alternatives to scoring by API models. We\nconduct evaluations on 13 advanced LLMs. Results show that our evaluation\nmethods show higher correlation coefficients and larger distinction than other\nevaluators. Additionally, we discuss the influence of different model sizes,\ndimensions, and normalization methods. We anticipate that F-Eval will\nfacilitate the study of LLMs' fundamental abilities.", "published": "2024-01-26 13:55:32", "link": "http://arxiv.org/abs/2401.14869v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LongFin: A Multimodal Document Understanding Model for Long Financial\n  Domain Documents", "abstract": "Document AI is a growing research field that focuses on the comprehension and\nextraction of information from scanned and digital documents to make everyday\nbusiness operations more efficient. Numerous downstream tasks and datasets have\nbeen introduced to facilitate the training of AI models capable of parsing and\nextracting information from various document types such as receipts and scanned\nforms. Despite these advancements, both existing datasets and models fail to\naddress critical challenges that arise in industrial contexts. Existing\ndatasets primarily comprise short documents consisting of a single page, while\nexisting models are constrained by a limited maximum length, often set at 512\ntokens. Consequently, the practical application of these methods in financial\nservices, where documents can span multiple pages, is severely impeded. To\novercome these challenges, we introduce LongFin, a multimodal document AI model\ncapable of encoding up to 4K tokens. We also propose the LongForms dataset, a\ncomprehensive financial dataset that encapsulates several industrial challenges\nin financial documents. Through an extensive evaluation, we demonstrate the\neffectiveness of the LongFin model on the LongForms dataset, surpassing the\nperformance of existing public models while maintaining comparable results on\nexisting single-page benchmarks.", "published": "2024-01-26 18:23:45", "link": "http://arxiv.org/abs/2401.15050v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pairing Orthographically Variant Literary Words to Standard Equivalents\n  Using Neural Edit Distance Models", "abstract": "We present a novel corpus consisting of orthographically variant words found\nin works of 19th century U.S. literature annotated with their corresponding\n\"standard\" word pair. We train a set of neural edit distance models to pair\nthese variants with their standard forms, and compare the performance of these\nmodels to the performance of a set of neural edit distance models trained on a\ncorpus of orthographic errors made by L2 English learners. Finally, we analyze\nthe relative performance of these models in the light of different negative\ntraining sample generation strategies, and offer concluding remarks on the\nunique challenge literary orthographic variation poses to string pairing\nmethodologies.", "published": "2024-01-26 18:49:34", "link": "http://arxiv.org/abs/2401.15068v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Diagnostic Accuracy through Multi-Agent Conversations: Using\n  Large Language Models to Mitigate Cognitive Bias", "abstract": "Background: Cognitive biases in clinical decision-making significantly\ncontribute to errors in diagnosis and suboptimal patient outcomes. Addressing\nthese biases presents a formidable challenge in the medical field.\n  Objective: This study explores the role of large language models (LLMs) in\nmitigating these biases through the utilization of a multi-agent framework. We\nsimulate the clinical decision-making processes through multi-agent\nconversation and evaluate its efficacy in improving diagnostic accuracy.\n  Methods: A total of 16 published and unpublished case reports where cognitive\nbiases have resulted in misdiagnoses were identified from the literature. In\nthe multi-agent framework, we leveraged GPT-4 to facilitate interactions among\nfour simulated agents to replicate clinical team dynamics. Each agent has a\ndistinct role: 1) To make the final diagnosis after considering the\ndiscussions, 2) The devil's advocate and correct confirmation and anchoring\nbias, 3) The tutor and facilitator of the discussion to reduce premature\nclosure bias, and 4) To record and summarize the findings. A total of 80\nsimulations were evaluated for the accuracy of initial diagnosis, top\ndifferential diagnosis and final two differential diagnoses.\n  Results: In a total of 80 responses evaluating both initial and final\ndiagnoses, the initial diagnosis had an accuracy of 0% (0/80), but following\nmulti-agent discussions, the accuracy for the top differential diagnosis\nincreased to 71.3% (57/80), and for the final two differential diagnoses, to\n80.0% (64/80).\n  Conclusions: The framework demonstrated an ability to re-evaluate and correct\nmisconceptions, even in scenarios with misleading initial investigations. The\nLLM-driven multi-agent conversation framework shows promise in enhancing\ndiagnostic accuracy in diagnostically challenging medical scenarios.", "published": "2024-01-26 01:35:50", "link": "http://arxiv.org/abs/2401.14589v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Alternative Speech: Complementary Method to Counter-Narrative for Better\n  Discourse", "abstract": "We introduce the concept of \"Alternative Speech\" as a new way to directly\ncombat hate speech and complement the limitations of counter-narrative. An\nalternative speech provides practical alternatives to hate speech in real-world\nscenarios by offering speech-level corrections to speakers while considering\nthe surrounding context and promoting speakers to reform. Further, an\nalternative speech can combat hate speech alongside counter-narratives,\noffering a useful tool to address social issues such as racial discrimination\nand gender inequality. We propose the new concept and provide detailed\nguidelines for constructing the necessary dataset. Through discussion, we\ndemonstrate that combining alternative speech and counter-narrative can be a\nmore effective strategy for combating hate speech by complementing specificity\nand guiding capacity of counter-narrative. This paper presents another\nperspective for dealing with hate speech, offering viable remedies to\ncomplement the constraints of current approaches to mitigating harmful bias.", "published": "2024-01-26 03:16:54", "link": "http://arxiv.org/abs/2401.14616v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Empirical Investigation of Domain Adaptation Ability for Chinese\n  Spelling Check Models", "abstract": "Chinese Spelling Check (CSC) is a meaningful task in the area of Natural\nLanguage Processing (NLP) which aims at detecting spelling errors in Chinese\ntexts and then correcting these errors. However, CSC models are based on\npretrained language models, which are trained on a general corpus.\nConsequently, their performance may drop when confronted with downstream tasks\ninvolving domain-specific terms. In this paper, we conduct a thorough\nevaluation about the domain adaption ability of various typical CSC models by\nbuilding three new datasets encompassing rich domain-specific terms from the\nfinancial, medical, and legal domains. Then we conduct empirical investigations\nin the corresponding domain-specific test datasets to ascertain the\ncross-domain adaptation ability of several typical CSC models. We also test the\nperformance of the popular large language model ChatGPT. As shown in our\nexperiments, the performances of the CSC models drop significantly in the new\ndomains.", "published": "2024-01-26 03:49:55", "link": "http://arxiv.org/abs/2401.14630v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Korean Legal Judgment Prediction Dataset for Insurance Disputes", "abstract": "This paper introduces a Korean legal judgment prediction (LJP) dataset for\ninsurance disputes. Successful LJP models on insurance disputes can benefit\ninsurance companies and their customers. It can save both sides' time and money\nby allowing them to predict how the result would come out if they proceed to\nthe dispute mediation process. As is often the case with low-resource\nlanguages, there is a limitation on the amount of data available for this\nspecific task. To mitigate this issue, we investigate how one can achieve a\ngood performance despite the limitation in data. In our experiment, we\ndemonstrate that Sentence Transformer Fine-tuning (SetFit, Tunstall et al.,\n2022) is a good alternative to standard fine-tuning when training data are\nlimited. The models fine-tuned with the SetFit approach on our data show\nsimilar performance to the Korean LJP benchmark models (Hwang et al., 2022)\ndespite the much smaller data size.", "published": "2024-01-26 05:26:27", "link": "http://arxiv.org/abs/2401.14654v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Under the Surface: Tracking the Artifactuality of LLM-Generated Data", "abstract": "This work delves into the expanding role of large language models (LLMs) in\ngenerating artificial data. LLMs are increasingly employed to create a variety\nof outputs, including annotations, preferences, instruction prompts, simulated\ndialogues, and free text. As these forms of LLM-generated data often intersect\nin their application, they exert mutual influence on each other and raise\nsignificant concerns about the quality and diversity of the artificial data\nincorporated into training cycles, leading to an artificial data ecosystem. To\nthe best of our knowledge, this is the first study to aggregate various types\nof LLM-generated text data, from more tightly constrained data like \"task\nlabels\" to more lightly constrained \"free-form text\". We then stress test the\nquality and implications of LLM-generated artificial data, comparing it with\nhuman data across various existing benchmarks. Despite artificial data's\ncapability to match human performance, this paper reveals significant hidden\ndisparities, especially in complex tasks where LLMs often miss the nuanced\nunderstanding of intrinsic human-generated content. This study critically\nexamines diverse LLM-generated data and emphasizes the need for ethical\npractices in data creation and when using LLMs. It highlights the LLMs'\nshortcomings in replicating human traits and behaviors, underscoring the\nimportance of addressing biases and artifacts produced in LLM-generated content\nfor future research and development. All data and code are available on our\nproject page.", "published": "2024-01-26 07:53:27", "link": "http://arxiv.org/abs/2401.14698v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Language Model Adaptation for Financial Sentiment Analysis", "abstract": "Natural language processing (NLP) has recently gained relevance within\nfinancial institutions by providing highly valuable insights into companies and\nmarkets' financial documents. However, the landscape of the financial domain\npresents extra challenges for NLP, due to the complexity of the texts and the\nuse of specific terminology. Generalist language models tend to fall short in\ntasks specifically tailored for finance, even when using large language models\n(LLMs) with great natural language understanding and generative capabilities.\nThis paper presents a study on LLM adaptation methods targeted at the financial\ndomain and with high emphasis on financial sentiment analysis. To this purpose,\ntwo foundation models with less than 1.5B parameters have been adapted using a\nwide range of strategies. We show that through careful fine-tuning on both\nfinancial documents and instructions, these foundation models can be adapted to\nthe target domain. Moreover, we observe that small LLMs have comparable\nperformance to larger scale models, while being more efficient in terms of\nparameters and data. In addition to the models, we show how to generate\nartificial instructions through LLMs to augment the number of samples of the\ninstruction dataset.", "published": "2024-01-26 11:04:01", "link": "http://arxiv.org/abs/2401.14777v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ChemDFM: A Large Language Foundation Model for Chemistry", "abstract": "Artificial intelligence (AI) has played an increasingly important role in\nchemical research. However, most models currently used in chemistry are\nspecialist models that require training and tuning for specific tasks. A more\ngeneric and efficient solution would be an AI model that could address many\ntasks and support free-form dialogue in the broad field of chemistry. In its\nutmost form, such a generalist AI chemist could be referred to as Chemical\nGeneral Intelligence. Large language models (LLMs) have recently logged\ntremendous success in the general domain of natural language processing,\nshowing emerging task generalization and free-form dialogue capabilities.\nHowever, domain knowledge of chemistry is largely missing when training\ngeneral-domain LLMs. The lack of such knowledge greatly hinders the performance\nof generalist LLMs in the field of chemistry. To this end, we develop ChemDFM,\na pioneering LLM for chemistry trained on 34B tokens from chemical literature\nand textbooks, and fine-tuned using 2.7M instructions. As a result, it can\nunderstand and reason with chemical knowledge in free-form dialogue.\nQuantitative evaluations show that ChemDFM significantly surpasses most\nrepresentative open-source LLMs. It outperforms GPT-4 on a great portion of\nchemical tasks, despite the substantial size difference. We have open-sourced\nthe inference codes, evaluation datasets, and model weights of ChemDFM on\nHuggingface (https://huggingface.co/OpenDFM/ChemDFM-v1.0-13B).", "published": "2024-01-26 12:45:55", "link": "http://arxiv.org/abs/2401.14818v5", "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.CL"}
{"title": "The Power of Noise: Redefining Retrieval for RAG Systems", "abstract": "Retrieval-Augmented Generation (RAG) has recently emerged as a method to\nextend beyond the pre-trained knowledge of Large Language Models by augmenting\nthe original prompt with relevant passages or documents retrieved by an\nInformation Retrieval (IR) system. RAG has become increasingly important for\nGenerative AI solutions, especially in enterprise settings or in any domain in\nwhich knowledge is constantly refreshed and cannot be memorized in the LLM. We\nargue here that the retrieval component of RAG systems, be it dense or sparse,\ndeserves increased attention from the research community, and accordingly, we\nconduct the first comprehensive and systematic examination of the retrieval\nstrategy of RAG systems. We focus, in particular, on the type of passages IR\nsystems within a RAG solution should retrieve. Our analysis considers multiple\nfactors, such as the relevance of the passages included in the prompt context,\ntheir position, and their number. One counter-intuitive finding of this work is\nthat the retriever's highest-scoring documents that are not directly relevant\nto the query (e.g., do not contain the answer) negatively impact the\neffectiveness of the LLM. Even more surprising, we discovered that adding\nrandom documents in the prompt improves the LLM accuracy by up to 35%. These\nresults highlight the need to investigate the appropriate strategies when\nintegrating retrieval with LLMs, thereby laying the groundwork for future\nresearch in this area.", "published": "2024-01-26 14:14:59", "link": "http://arxiv.org/abs/2401.14887v4", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Do LLMs Dream of Ontologies?", "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across\ndiverse natural language processing tasks, yet their ability to memorize\nstructured knowledge remains underexplored. In this paper, we investigate the\nextent to which general-purpose pre-trained LLMs retain and correctly reproduce\nconcept identifier (ID)-label associations from publicly available ontologies.\nWe conduct a systematic evaluation across multiple ontological resources,\nincluding the Gene Ontology, Uberon, Wikidata, and ICD-10, using LLMs such as\nPythia-12B, Gemini-1.5-Flash, GPT-3.5, and GPT-4. Our findings reveal that only\na small fraction of ontological concepts is accurately memorized, with GPT-4\ndemonstrating the highest performance. To understand why certain concepts are\nmemorized more effectively than others, we analyze the relationship between\nmemorization accuracy and concept popularity on the Web. Our results indicate a\nstrong correlation between the frequency of a concept's occurrence online and\nthe likelihood of accurately retrieving its ID from the label. This suggests\nthat LLMs primarily acquire such knowledge through indirect textual exposure\nrather than directly from structured ontological resources. Furthermore, we\nintroduce new metrics to quantify prediction invariance, demonstrating that the\nstability of model responses across variations in prompt language and\ntemperature settings can serve as a proxy for estimating memorization\nrobustness.", "published": "2024-01-26 15:10:23", "link": "http://arxiv.org/abs/2401.14931v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Airavata: Introducing Hindi Instruction-tuned LLM", "abstract": "We announce the initial release of \"Airavata,\" an instruction-tuned LLM for\nHindi. Airavata was created by fine-tuning OpenHathi with diverse,\ninstruction-tuning Hindi datasets to make it better suited for assistive tasks.\nAlong with the model, we also share the IndicInstruct dataset, which is a\ncollection of diverse instruction-tuning datasets to enable further research\nfor Indic LLMs. Additionally, we present evaluation benchmarks and a framework\nfor assessing LLM performance across tasks in Hindi. Currently, Airavata\nsupports Hindi, but we plan to expand this to all 22 scheduled Indic languages.\nYou can access all artifacts at https://ai4bharat.github.io/airavata.", "published": "2024-01-26 17:07:08", "link": "http://arxiv.org/abs/2401.15006v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SliceGPT: Compress Large Language Models by Deleting Rows and Columns", "abstract": "Large language models have become the cornerstone of natural language\nprocessing, but their use comes with substantial costs in terms of compute and\nmemory resources. Sparsification provides a solution to alleviate these\nresource constraints, and recent works have shown that trained models can be\nsparsified post-hoc. Existing sparsification techniques face challenges as they\nneed additional data structures and offer constrained speedup with current\nhardware. In this paper we present SliceGPT, a new post-training sparsification\nscheme which replaces each weight matrix with a smaller (dense) matrix,\nreducing the embedding dimension of the network. Through extensive\nexperimentation, we show that SliceGPT can remove up to 25% of the model\nparameters (including embeddings) for LLAMA2-70B, OPT 66B and Phi-2 models\nwhile maintaining 99%, 99% and 90% zero-shot task performance of the dense\nmodel respectively. Our sliced models run on fewer GPUs and run faster without\nany additional code optimization: on 24GB consumer GPUs we reduce the total\ncompute for inference on LLAMA2-70B to 64% of that of the dense model; on 40GB\nA100 GPUs we reduce it to 66%. We offer a new insight, computational invariance\nin transformer networks, which enables SliceGPT and we hope it will inspire and\nenable future avenues to reduce memory and computation demands for pre-trained\nmodels. Code is available at:\nhttps://github.com/microsoft/TransformerCompression", "published": "2024-01-26 17:35:45", "link": "http://arxiv.org/abs/2401.15024v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "PROXYQA: An Alternative Framework for Evaluating Long-Form Text\n  Generation with Large Language Models", "abstract": "Large Language Models (LLMs) have succeeded remarkably in understanding\nlong-form contents. However, exploring their capability for generating\nlong-form contents, such as reports and articles, has been relatively\nunexplored and inadequately assessed by existing benchmarks. The prevalent\nevaluation methods, which predominantly rely on crowdsourcing, are recognized\nfor their labor-intensive nature and lack of efficiency, whereas automated\nmetrics, such as the ROUGE score, demonstrate discordance with human judgment\ncriteria. In this paper, we propose ProxyQA, an innovative framework dedicated\nto assessing long-text generation. ProxyQA comprises in-depth human-curated\nmeta-questions spanning various domains, each accompanied by specific\nproxy-questions with pre-annotated answers. LLMs are tasked to generate\nextensive content in response to these meta-questions, by engaging an evaluator\nand incorporating the generated texts as contextual background, ProxyQA\nassesses the generated content's quality through the evaluator's accuracy in\naddressing the proxy-questions. We examine multiple LLMs, emphasizing ProxyQA's\ndemanding nature as a high-quality assessment tool. Human evaluation\ndemonstrates that the proxy-question method is notably self-consistent and\naligns closely with human evaluative standards. The dataset and leaderboard is\navailable at \\url{https://proxy-qa.com}.", "published": "2024-01-26 18:12:25", "link": "http://arxiv.org/abs/2401.15042v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Deep learning-based approach for tomato classification in complex scenes", "abstract": "Tracking ripening tomatoes is time consuming and labor intensive. Artificial\nintelligence technologies combined with those of computer vision can help users\noptimize the process of monitoring the ripening status of plants. To this end,\nwe have proposed a tomato ripening monitoring approach based on deep learning\nin complex scenes. The objective is to detect mature tomatoes and harvest them\nin a timely manner. The proposed approach is declined in two parts. Firstly,\nthe images of the scene are transmitted to the pre-processing layer. This\nprocess allows the detection of areas of interest (area of the image containing\ntomatoes). Then, these images are used as input to the maturity detection\nlayer. This layer, based on a deep neural network learning algorithm,\nclassifies the tomato thumbnails provided to it in one of the following five\ncategories: green, brittle, pink, pale red, mature red. The experiments are\nbased on images collected from the internet gathered through searches using\ntomato state across diverse languages including English, German, French, and\nSpanish. The experimental results of the maturity detection layer on a dataset\ncomposed of images of tomatoes taken under the extreme conditions, gave a good\nclassification rate.", "published": "2024-01-26 18:33:57", "link": "http://arxiv.org/abs/2401.15055v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty", "abstract": "Autoregressive decoding makes the inference of Large Language Models (LLMs)\ntime-consuming. In this paper, we reconsider speculative sampling and derive\ntwo key observations. Firstly, autoregression at the feature\n(second-to-top-layer) level is more straightforward than at the token level.\nSecondly, the inherent uncertainty in feature (second-to-top-layer) level\nautoregression constrains its performance. Based on these insights, we\nintroduce EAGLE (Extrapolation Algorithm for Greater Language-model\nEfficiency), a simple yet highly efficient speculative sampling framework. By\nincorporating a token sequence advanced by one time step, EAGLE effectively\nresolves the uncertainty, enabling precise second-to-top-layer feature\nprediction with minimal overhead. We conducted comprehensive evaluations of\nEAGLE, including all models from the Vicuna and LLaMA2-Chat series, the MoE\nmodel Mixtral 8x7B Instruct, and tasks in dialogue, code generation,\nmathematical reasoning, and instruction following. For LLaMA2-Chat 70B, EAGLE\nachieved a latency speedup ratio of 2.7x-3.5x, doubled throughput, while\nmaintaining the distribution of the generated text.", "published": "2024-01-26 18:59:01", "link": "http://arxiv.org/abs/2401.15077v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Scalable Qualitative Coding with LLMs: Chain-of-Thought Reasoning\n  Matches Human Performance in Some Hermeneutic Tasks", "abstract": "Qualitative coding, or content analysis, extracts meaning from text to\ndiscern quantitative patterns across a corpus of texts. Recently, advances in\nthe interpretive abilities of large language models (LLMs) offer potential for\nautomating the coding process (applying category labels to texts), thereby\nenabling human researchers to concentrate on more creative research aspects,\nwhile delegating these interpretive tasks to AI. Our case study comprises a set\nof socio-historical codes on dense, paragraph-long passages representative of a\nhumanistic study. We show that GPT-4 is capable of human-equivalent\ninterpretations, whereas GPT-3.5 is not. Compared to our human-derived gold\nstandard, GPT-4 delivers excellent intercoder reliability (Cohen's $\\kappa \\geq\n0.79$) for 3 of 9 codes, and substantial reliability ($\\kappa \\geq 0.6$) for 8\nof 9 codes. In contrast, GPT-3.5 greatly underperforms for all codes\n($mean(\\kappa) = 0.34$; $max(\\kappa) = 0.55$). Importantly, we find that coding\nfidelity improves considerably when the LLM is prompted to give rationale\njustifying its coding decisions (chain-of-thought reasoning). We present these\nand other findings along with a set of best practices for adapting traditional\ncodebooks for LLMs. Our results indicate that for certain codebooks,\nstate-of-the-art LLMs are already adept at large-scale content analysis.\nFurthermore, they suggest the next generation of models will likely render AI\ncoding a viable option for a majority of codebooks.", "published": "2024-01-26 19:25:43", "link": "http://arxiv.org/abs/2401.15170v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "HiFT: A Hierarchical Full Parameter Fine-Tuning Strategy", "abstract": "Full-parameter fine-tuning has become the go-to choice for adapting language\nmodels (LMs) to downstream tasks due to its excellent performance. As LMs grow\nin size, fine-tuning the full parameters of LMs requires a prohibitively large\namount of GPU memory. Existing approaches utilize zeroth-order optimizer to\nconserve GPU memory, which can potentially compromise the performance of LMs as\nnon-zero order optimizers tend to converge more readily on most downstream\ntasks. In this paper, we propose a novel optimizer-independent end-to-end\nhierarchical fine-tuning strategy, HiFT, which only updates a subset of\nparameters at each training step. HiFT can significantly reduce the amount of\ngradients and optimizer state parameters residing in GPU memory at the same\ntime, thereby reducing GPU memory usage. Our results demonstrate that: (1) HiFT\nachieves comparable performance to parameter-efficient fine-tuning and standard\nfull parameter fine-tuning. (2) HiFT supports various optimizers including\nAdamW, AdaGrad, SGD, etc. (3) HiFT can save more than 60\\% GPU memory compared\nwith standard full-parameter fine-tuning for 7B model. (4) HiFT enables\nfull-parameter fine-tuning of a 7B model on single 48G A6000 with a precision\nof 32 using the AdamW optimizer, without using any memory saving techniques.", "published": "2024-01-26 21:14:32", "link": "http://arxiv.org/abs/2401.15207v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Unlearning Traces the Influential Training Data of Language Models", "abstract": "Identifying the training datasets that influence a language model's outputs\nis essential for minimizing the generation of harmful content and enhancing its\nperformance. Ideally, we can measure the influence of each dataset by removing\nit from training; however, it is prohibitively expensive to retrain a model\nmultiple times. This paper presents UnTrac: unlearning traces the influence of\na training dataset on the model's performance. UnTrac is extremely simple; each\ntraining dataset is unlearned by gradient ascent, and we evaluate how much the\nmodel's predictions change after unlearning. Furthermore, we propose a more\nscalable approach, UnTrac-Inv, which unlearns a test dataset and evaluates the\nunlearned model on training datasets. UnTrac-Inv resembles UnTrac, while being\nefficient for massive training datasets. In the experiments, we examine if our\nmethods can assess the influence of pretraining datasets on generating toxic,\nbiased, and untruthful content. Our methods estimate their influence much more\naccurately than existing methods while requiring neither excessive memory space\nnor multiple checkpoints.", "published": "2024-01-26 23:17:31", "link": "http://arxiv.org/abs/2401.15241v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Measuring Moral Inconsistencies in Large Language Models", "abstract": "A Large Language Model (LLM) is considered consistent if semantically\nequivalent prompts produce semantically equivalent responses. Despite recent\nadvancements showcasing the impressive capabilities of LLMs in conversational\nsystems, we show that even state-of-the-art LLMs are highly inconsistent in\ntheir generations, questioning their reliability. Prior research has tried to\nmeasure this with task-specific accuracy. However, this approach is unsuitable\nfor moral scenarios, such as the trolley problem, with no \"correct\" answer. To\naddress this issue, we propose a novel information-theoretic measure called\nSemantic Graph Entropy (SGE) to measure the consistency of an LLM in moral\nscenarios. We leverage \"Rules of Thumb\" (RoTs) to explain a model's\ndecision-making strategies and further enhance our metric. Compared to existing\nconsistency metrics, SGE correlates better with human judgments across five\nLLMs. In the future, we aim to investigate the root causes of LLM\ninconsistencies and propose improvements.", "published": "2024-01-26 18:05:47", "link": "http://arxiv.org/abs/2402.01719v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "UNIT-DSR: Dysarthric Speech Reconstruction System Using Speech Unit\n  Normalization", "abstract": "Dysarthric speech reconstruction (DSR) systems aim to automatically convert\ndysarthric speech into normal-sounding speech. The technology eases\ncommunication with speakers affected by the neuromotor disorder and enhances\ntheir social inclusion. NED-based (Neural Encoder-Decoder) systems have\nsignificantly improved the intelligibility of the reconstructed speech as\ncompared with GAN-based (Generative Adversarial Network) approaches, but the\napproach is still limited by training inefficiency caused by the cascaded\npipeline and auxiliary tasks of the content encoder, which may in turn affect\nthe quality of reconstruction. Inspired by self-supervised speech\nrepresentation learning and discrete speech units, we propose a Unit-DSR\nsystem, which harnesses the powerful domain-adaptation capacity of HuBERT for\ntraining efficiency improvement and utilizes speech units to constrain the\ndysarthric content restoration in a discrete linguistic space. Compared with\nNED approaches, the Unit-DSR system only consists of a speech unit normalizer\nand a Unit HiFi-GAN vocoder, which is considerably simpler without cascaded\nsub-modules or auxiliary tasks. Results on the UASpeech corpus indicate that\nUnit-DSR outperforms competitive baselines in terms of content restoration,\nreaching a 28.2% relative average word error rate reduction when compared to\noriginal dysarthric speech, and shows robustness against speed perturbation and\nnoise.", "published": "2024-01-26 06:08:47", "link": "http://arxiv.org/abs/2401.14664v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Comparison of parameters of vowel sounds of russian and english\n  languages", "abstract": "In multilingual speech recognition systems, a situation can often arise when\nthe language is not known in advance, but the signal has already been received\nand is being processed. For such cases, some generalized model is needed that\nwill be able to respond to phonetic differences and, depending on them,\ncorrectly recog-nize speech in the desired language. To build such a model, it\nis necessary to set the values of phonetic parameters, and then compare similar\nsounds, establishing significant differences.", "published": "2024-01-26 14:15:20", "link": "http://arxiv.org/abs/2401.14890v1", "categories": ["cs.SD", "cs.CL", "eess.AS", "68T10", "H.2.8"], "primary_category": "cs.SD"}
{"title": "Embedding-based search in JetBrains IDEs", "abstract": "Most modern Integrated Development Environments (IDEs) and code editors have\na feature to search across available functionality and items in an open\nproject. In JetBrains IDEs, this feature is called Search Everywhere: it allows\nusers to search for files, actions, classes, symbols, settings, and anything\nfrom VCS history from a single entry point. However, it works with the\ncandidates obtained by algorithms that don't account for semantics, e.g.,\nsynonyms, complex word permutations, part of the speech modifications, and\ntypos. In this work, we describe the machine learning approach we implemented\nto improve the discoverability of search items. We also share the obstacles\nencountered during this process and how we overcame them.", "published": "2024-01-26 16:07:42", "link": "http://arxiv.org/abs/2401.14975v1", "categories": ["cs.SE", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.SE"}
{"title": "Health Text Simplification: An Annotated Corpus for Digestive Cancer\n  Education and Novel Strategies for Reinforcement Learning", "abstract": "Objective: The reading level of health educational materials significantly\ninfluences the understandability and accessibility of the information,\nparticularly for minoritized populations. Many patient educational resources\nsurpass the reading level and complexity of widely accepted standards. There is\na critical need for high-performing text simplification models in health\ninformation to enhance dissemination and literacy. This need is particularly\nacute in cancer education, where effective prevention and screening education\ncan substantially reduce morbidity and mortality.\n  Methods: We introduce Simplified Digestive Cancer (SimpleDC), a parallel\ncorpus of cancer education materials tailored for health text simplification\nresearch, comprising educational content from the American Cancer Society,\nCenters for Disease Control and Prevention, and National Cancer Institute.\nUtilizing SimpleDC alongside the existing Med-EASi corpus, we explore Large\nLanguage Model (LLM)-based simplification methods, including fine-tuning,\nreinforcement learning (RL), reinforcement learning with human feedback (RLHF),\ndomain adaptation, and prompt-based approaches. Our experimentation encompasses\nLlama 2 and GPT-4. A novel RLHF reward function is introduced, featuring a\nlightweight model adept at distinguishing between original and simplified\ntexts, thereby enhancing the model's effectiveness with unlabeled data.\n  Results: Fine-tuned Llama 2 models demonstrated high performance across\nvarious metrics. Our innovative RLHF reward function surpassed existing RL text\nsimplification reward functions in effectiveness. The results underscore that\nRL/RLHF can augment fine-tuning, facilitating model training on unlabeled text\nand improving performance.", "published": "2024-01-26 18:13:57", "link": "http://arxiv.org/abs/2401.15043v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluation of LLM Chatbots for OSINT-based Cyber Threat Awareness", "abstract": "Knowledge sharing about emerging threats is crucial in the rapidly advancing\nfield of cybersecurity and forms the foundation of Cyber Threat Intelligence\n(CTI). In this context, Large Language Models are becoming increasingly\nsignificant in the field of cybersecurity, presenting a wide range of\nopportunities. This study surveys the performance of ChatGPT, GPT4all, Dolly,\nStanford Alpaca, Alpaca-LoRA, Falcon, and Vicuna chatbots in binary\nclassification and Named Entity Recognition (NER) tasks performed using Open\nSource INTelligence (OSINT). We utilize well-established data collected in\nprevious research from Twitter to assess the competitiveness of these chatbots\nwhen compared to specialized models trained for those tasks. In binary\nclassification experiments, Chatbot GPT-4 as a commercial model achieved an\nacceptable F1 score of 0.94, and the open-source GPT4all model achieved an F1\nscore of 0.90. However, concerning cybersecurity entity recognition, all\nevaluated chatbots have limitations and are less effective. This study\ndemonstrates the capability of chatbots for OSINT binary classification and\nshows that they require further improvement in NER to effectively replace\nspecially trained models. Our results shed light on the limitations of the LLM\nchatbots when compared to specialized models, and can help researchers improve\nchatbots technology with the objective to reduce the required effort to\nintegrate machine learning in OSINT-based CTI tools.", "published": "2024-01-26 13:15:24", "link": "http://arxiv.org/abs/2401.15127v3", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Transfer Learning for the Prediction of Entity Modifiers in Clinical\n  Text: Application to Opioid Use Disorder Case Detection", "abstract": "Background: The semantics of entities extracted from a clinical text can be\ndramatically altered by modifiers, including entity negation, uncertainty,\nconditionality, severity, and subject. Existing models for determining\nmodifiers of clinical entities involve regular expression or features weights\nthat are trained independently for each modifier.\n  Methods: We develop and evaluate a multi-task transformer architecture design\nwhere modifiers are learned and predicted jointly using the publicly available\nSemEval 2015 Task 14 corpus and a new Opioid Use Disorder (OUD) data set that\ncontains modifiers shared with SemEval as well as novel modifiers specific for\nOUD. We evaluate the effectiveness of our multi-task learning approach versus\npreviously published systems and assess the feasibility of transfer learning\nfor clinical entity modifiers when only a portion of clinical modifiers are\nshared.\n  Results: Our approach achieved state-of-the-art results on the ShARe corpus\nfrom SemEval 2015 Task 14, showing an increase of 1.1% on weighted accuracy,\n1.7% on unweighted accuracy, and 10% on micro F1 scores.\n  Conclusions: We show that learned weights from our shared model can be\neffectively transferred to a new partially matched data set, validating the use\nof transfer learning for clinical text modifiers", "published": "2024-01-26 22:19:31", "link": "http://arxiv.org/abs/2401.15222v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Bloom-epistemic and sentiment analysis hierarchical classification in\n  course discussion forums", "abstract": "Online discussion forums are widely used for active textual interaction\nbetween lecturers and students, and to see how the students have progressed in\na learning process. The objective of this study is to compare appropriate\nmachine-learning models to assess sentiments and Bloom\\'s epistemic taxonomy\nbased on textual comments in educational discussion forums. Our proposed method\nis called the hierarchical approach of Bloom-Epistemic and Sentiment Analysis\n(BE-Sent). The research methodology consists of three main steps. The first\nstep is the data collection from the internal discussion forum and YouTube\ncomments of a Web Programming channel. The next step is text preprocessing to\nannotate the text and clear unimportant words. Furthermore, with the text\ndataset that has been successfully cleaned, sentiment analysis and epistemic\ncategorization will be done in each sentence of the text. Sentiment analysis is\ndivided into three categories: positive, negative, and neutral. Bloom\\'s\nepistemic is divided into six categories: remembering, understanding, applying,\nanalyzing, evaluating, and creating. This research has succeeded in producing a\ncourse learning subsystem that assesses opinions based on text reviews of\ndiscussion forums according to the category of sentiment and epistemic\nanalysis.", "published": "2024-01-26 08:20:13", "link": "http://arxiv.org/abs/2402.01716v1", "categories": ["cs.CY", "cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.CY"}
{"title": "From RAG to QA-RAG: Integrating Generative AI for Pharmaceutical\n  Regulatory Compliance Process", "abstract": "Regulatory compliance in the pharmaceutical industry entails navigating\nthrough complex and voluminous guidelines, often requiring significant human\nresources. To address these challenges, our study introduces a chatbot model\nthat utilizes generative AI and the Retrieval Augmented Generation (RAG)\nmethod. This chatbot is designed to search for guideline documents relevant to\nthe user inquiries and provide answers based on the retrieved guidelines.\nRecognizing the inherent need for high reliability in this domain, we propose\nthe Question and Answer Retrieval Augmented Generation (QA-RAG) model. In\ncomparative experiments, the QA-RAG model demonstrated a significant\nimprovement in accuracy, outperforming all other baselines including\nconventional RAG methods. This paper details QA-RAG's structure and performance\nevaluation, emphasizing its potential for the regulatory compliance domain in\nthe pharmaceutical industry and beyond. We have made our work publicly\navailable for further research and development.", "published": "2024-01-26 08:23:29", "link": "http://arxiv.org/abs/2402.01717v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "I.2.7; I.2.1; J.3"], "primary_category": "cs.CL"}
{"title": "Deep Learning Based Amharic Chatbot for FAQs in Universities", "abstract": "University students often spend a considerable amount of time seeking answers\nto common questions from administrators or teachers. This can become tedious\nfor both parties, leading to a need for a solution. In response, this paper\nproposes a chatbot model that utilizes natural language processing and deep\nlearning techniques to answer frequently asked questions (FAQs) in the Amharic\nlanguage. Chatbots are computer programs that simulate human conversation\nthrough the use of artificial intelligence (AI), acting as a virtual assistant\nto handle questions and other tasks. The proposed chatbot program employs\ntokenization, normalization, stop word removal, and stemming to analyze and\ncategorize Amharic input sentences. Three machine learning model algorithms\nwere used to classify tokens and retrieve appropriate responses: Support Vector\nMachine (SVM), Multinomial Na\\\"ive Bayes, and deep neural networks implemented\nthrough TensorFlow, Keras, and NLTK. The deep learning model achieved the best\nresults with 91.55% accuracy and a validation loss of 0.3548 using an Adam\noptimizer and SoftMax activation function. The chatbot model was integrated\nwith Facebook Messenger and deployed on a Heroku server for 24-hour\naccessibility. The experimental results demonstrate that the chatbot framework\nachieved its objectives and effectively addressed challenges such as Amharic\nFidel variation, morphological variation, and lexical gaps. Future research\ncould explore the integration of Amharic WordNet to narrow the lexical gap and\nsupport more complex questions.", "published": "2024-01-26 18:37:21", "link": "http://arxiv.org/abs/2402.01720v2", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "Turn-taking and Backchannel Prediction with Acoustic and Large Language\n  Model Fusion", "abstract": "We propose an approach for continuous prediction of turn-taking and\nbackchanneling locations in spoken dialogue by fusing a neural acoustic model\nwith a large language model (LLM). Experiments on the Switchboard human-human\nconversation dataset demonstrate that our approach consistently outperforms the\nbaseline models with single modality. We also develop a novel multi-task\ninstruction fine-tuning strategy to further benefit from LLM-encoded knowledge\nfor understanding the tasks and conversational contexts, leading to additional\nimprovements. Our approach demonstrates the potential of combined LLMs and\nacoustic models for a more natural and conversational interaction between\nhumans and speech-enabled AI agents.", "published": "2024-01-26 08:59:07", "link": "http://arxiv.org/abs/2401.14717v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Spatial Analysis and Synthesis Methods: Subjective and Objective\n  Evaluations Using Various Microphone Arrays in the Auralization of a Critical\n  Listening Room", "abstract": "Parametric sound field synthesis methods, such as the Spatial Decomposition\nMethod (SDM) and Higher-Order Spatial Impulse Response Rendering (HO-SIRR), are\nwidely used for the analysis and auralization of sound fields. This paper\nstudies the performances of various sound field synthesis methods in the\ncontext of the auralization of a critical listening room. The influence on the\nperceived spatial and timbral fidelity of the following factors is considered:\nthe rendering framework, direction of arrival (DOA) estimation method,\nmicrophone array structure, and use of a dedicated center reference microphone\nwith SDM. Listening tests compare the synthesized sound fields to a reference\nbinaural rendering condition. Several acoustic parameters are measured to gain\ninsights into objective differences between methods. A high-quality pressure\nmicrophone improves the SDM framework's timbral fidelity. Additionally, SDM and\nHO-SIRR show similarities in spatial fidelity. Performance variation between\nSDM configurations is influenced by the DOA estimation method and microphone\narray construction. The binaural SDM (BSDM) presentations display temporal\nartifacts impacting sound quality.", "published": "2024-01-26 17:29:47", "link": "http://arxiv.org/abs/2401.15023v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Expressivity-aware Music Performance Retrieval using Mid-level\n  Perceptual Features and Emotion Word Embeddings", "abstract": "This paper explores a specific sub-task of cross-modal music retrieval. We\nconsider the delicate task of retrieving a performance or rendition of a\nmusical piece based on a description of its style, expressive character, or\nemotion from a set of different performances of the same piece. We observe that\na general purpose cross-modal system trained to learn a common text-audio\nembedding space does not yield optimal results for this task. By introducing\ntwo changes -- one each to the text encoder and the audio encoder -- we\ndemonstrate improved performance on a dataset of piano performances and\nassociated free-text descriptions. On the text side, we use emotion-enriched\nword embeddings (EWE) and on the audio side, we extract mid-level perceptual\nfeatures instead of generic audio embeddings. Our results highlight the\neffectiveness of mid-level perceptual features learnt from music and emotion\nenriched word embeddings learnt from emotion-labelled text in capturing musical\nexpression in a cross-modal setting. Additionally, our interpretable mid-level\nfeatures provide a route for introducing explainability in the retrieval and\ndownstream recommendation processes.", "published": "2024-01-26 12:52:56", "link": "http://arxiv.org/abs/2401.14826v1", "categories": ["cs.SD", "cs.IR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Robust Dual-Modal Speech Keyword Spotting for XR Headsets", "abstract": "While speech interaction finds widespread utility within the Extended Reality\n(XR) domain, conventional vocal speech keyword spotting systems continue to\ngrapple with formidable challenges, including suboptimal performance in noisy\nenvironments, impracticality in situations requiring silence, and\nsusceptibility to inadvertent activations when others speak nearby. These\nchallenges, however, can potentially be surmounted through the cost-effective\nfusion of voice and lip movement information. Consequently, we propose a novel\nvocal-echoic dual-modal keyword spotting system designed for XR headsets. We\ndevise two different modal fusion approches and conduct experiments to test the\nsystem's performance across diverse scenarios. The results show that our\ndual-modal system not only consistently outperforms its single-modal\ncounterparts, demonstrating higher precision in both typical and noisy\nenvironments, but also excels in accurately identifying silent utterances.\nFurthermore, we have successfully applied the system in real-time\ndemonstrations, achieving promising results. The code is available at\nhttps://github.com/caizhuojiang/VE-KWS.", "published": "2024-01-26 16:09:18", "link": "http://arxiv.org/abs/2401.14978v1", "categories": ["cs.HC", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
{"title": "Enhancement of a Text-Independent Speaker Verification System by using\n  Feature Combination and Parallel-Structure Classifiers", "abstract": "Speaker Verification (SV) systems involve mainly two individual stages:\nfeature extraction and classification. In this paper, we explore these two\nmodules with the aim of improving the performance of a speaker verification\nsystem under noisy conditions. On the one hand, the choice of the most\nappropriate acoustic features is a crucial factor for performing robust speaker\nverification. The acoustic parameters used in the proposed system are: Mel\nFrequency Cepstral Coefficients (MFCC), their first and second derivatives\n(Deltas and Delta- Deltas), Bark Frequency Cepstral Coefficients (BFCC),\nPerceptual Linear Predictive (PLP), and Relative Spectral Transform -\nPerceptual Linear Predictive (RASTA-PLP). In this paper, a complete comparison\nof different combinations of the previous features is discussed. On the other\nhand, the major weakness of a conventional Support Vector Machine (SVM)\nclassifier is the use of generic traditional kernel functions to compute the\ndistances among data points. However, the kernel function of an SVM has great\ninfluence on its performance. In this work, we propose the combination of two\nSVM-based classifiers with different kernel functions: Linear kernel and\nGaussian Radial Basis Function (RBF) kernel with a Logistic Regression (LR)\nclassifier. The combination is carried out by means of a parallel structure\napproach, in which different voting rules to take the final decision are\nconsidered. Results show that significant improvement in the performance of the\nSV system is achieved by using the combined features with the combined\nclassifiers either with clean speech or in the presence of noise. Finally, to\nenhance the system more in noisy environments, the inclusion of the multiband\nnoise removal technique as a preprocessing stage is proposed.", "published": "2024-01-26 17:19:59", "link": "http://arxiv.org/abs/2401.15018v1", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "AMuSE: Adaptive Multimodal Analysis for Speaker Emotion Recognition in\n  Group Conversations", "abstract": "Analyzing individual emotions during group conversation is crucial in\ndeveloping intelligent agents capable of natural human-machine interaction.\nWhile reliable emotion recognition techniques depend on different modalities\n(text, audio, video), the inherent heterogeneity between these modalities and\nthe dynamic cross-modal interactions influenced by an individual's unique\nbehavioral patterns make the task of emotion recognition very challenging. This\ndifficulty is compounded in group settings, where the emotion and its temporal\nevolution are not only influenced by the individual but also by external\ncontexts like audience reaction and context of the ongoing conversation. To\nmeet this challenge, we propose a Multimodal Attention Network that captures\ncross-modal interactions at various levels of spatial abstraction by jointly\nlearning its interactive bunch of mode-specific Peripheral and Central\nnetworks. The proposed MAN injects cross-modal attention via its Peripheral\nkey-value pairs within each layer of a mode-specific Central query network. The\nresulting cross-attended mode-specific descriptors are then combined using an\nAdaptive Fusion technique that enables the model to integrate the\ndiscriminative and complementary mode-specific data patterns within an\ninstance-specific multimodal descriptor. Given a dialogue represented by a\nsequence of utterances, the proposed AMuSE model condenses both spatial and\ntemporal features into two dense descriptors: speaker-level and\nutterance-level. This helps not only in delivering better classification\nperformance (3-5% improvement in Weighted-F1 and 5-7% improvement in Accuracy)\nin large-scale public datasets but also helps the users in understanding the\nreasoning behind each emotion prediction made by the model via its Multimodal\nExplainability Visualization module.", "published": "2024-01-26 19:17:05", "link": "http://arxiv.org/abs/2401.15164v1", "categories": ["cs.SD", "cs.CV", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
