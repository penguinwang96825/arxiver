{"title": "Anchoring and Agreement in Syntactic Annotations", "abstract": "We present a study on two key characteristics of human syntactic annotations:\nanchoring and agreement. Anchoring is a well known cognitive bias in human\ndecision making, where judgments are drawn towards pre-existing values. We\nstudy the influence of anchoring on a standard approach to creation of\nsyntactic resources where syntactic annotations are obtained via human editing\nof tagger and parser output. Our experiments demonstrate a clear anchoring\neffect and reveal unwanted consequences, including overestimation of parsing\nperformance and lower quality of annotations in comparison with human-based\nannotations. Using sentences from the Penn Treebank WSJ, we also report\nsystematically obtained inter-annotator agreement estimates for English\ndependency parsing. Our agreement results control for parser bias, and are\nconsequential in that they are on par with state of the art parsing performance\nfor English newswire. We discuss the impact of our findings on strategies for\nfuture annotation efforts and parser evaluations.", "published": "2016-05-15 00:26:26", "link": "http://arxiv.org/abs/1605.04481v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Machine Translation Evaluation Resources and Methods: A Survey", "abstract": "We introduce the Machine Translation (MT) evaluation survey that contains\nboth manual and automatic evaluation methods. The traditional human evaluation\ncriteria mainly include the intelligibility, fidelity, fluency, adequacy,\ncomprehension, and informativeness. The advanced human assessments include\ntask-oriented measures, post-editing, segment ranking, and extended criteriea,\netc. We classify the automatic evaluation methods into two categories,\nincluding lexical similarity scenario and linguistic features application. The\nlexical similarity methods contain edit distance, precision, recall, F-measure,\nand word order. The linguistic features can be divided into syntactic features\nand semantic features respectively. The syntactic features include part of\nspeech tag, phrase types and sentence structures, and the semantic features\ninclude named entity, synonyms, textual entailment, paraphrase, semantic roles,\nand language models. The deep learning models for evaluation are very newly\nproposed. Subsequently, we also introduce the evaluation methods for MT\nevaluation including different correlation scores, and the recent quality\nestimation (QE) tasks for MT.\n  This paper differs from the existing works\n\\cite{GALEprogram2009,EuroMatrixProject2007} from several aspects, by\nintroducing some recent development of MT evaluation measures, the different\nclassifications from manual to automatic evaluation measures, the introduction\nof recent QE tasks of MT, and the concise construction of the content.\n  We hope this work will be helpful for MT researchers to easily pick up some\nmetrics that are best suitable for their specific MT model development, and\nhelp MT evaluation researchers to get a general clue of how MT evaluation\nresearch developed. Furthermore, hopefully, this work can also shine some light\non other evaluation tasks, except for translation, of NLP fields.", "published": "2016-05-15 09:41:00", "link": "http://arxiv.org/abs/1605.04515v8", "categories": ["cs.CL", "I.2.7; I.2.1"], "primary_category": "cs.CL"}
{"title": "A Proposal for Linguistic Similarity Datasets Based on Commonality Lists", "abstract": "Similarity is a core notion that is used in psychology and two branches of\nlinguistics: theoretical and computational. The similarity datasets that come\nfrom the two fields differ in design: psychological datasets are focused around\na certain topic such as fruit names, while linguistic datasets contain words\nfrom various categories. The later makes humans assign low similarity scores to\nthe words that have nothing in common and to the words that have contrast in\nmeaning, making similarity scores ambiguous. In this work we discuss the\nsimilarity collection procedure for a multi-category dataset that avoids score\nambiguity and suggest changes to the evaluation procedure to reflect the\ninsights of psychological literature for word, phrase and sentence similarity.\nWe suggest to ask humans to provide a list of commonalities and differences\ninstead of numerical similarity scores and employ the structure of human\njudgements beyond pairwise similarity for model evaluation. We believe that the\nproposed approach will give rise to datasets that test meaning representation\nmodels more thoroughly with respect to the human treatment of similarity.", "published": "2016-05-15 14:00:06", "link": "http://arxiv.org/abs/1605.04553v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Syntactically Guided Neural Machine Translation", "abstract": "We investigate the use of hierarchical phrase-based SMT lattices in\nend-to-end neural machine translation (NMT). Weight pushing transforms the\nHiero scores for complete translation hypotheses, with the full translation\ngrammar score and full n-gram language model score, into posteriors compatible\nwith NMT predictive probabilities. With a slightly modified NMT beam-search\ndecoder we find gains over both Hiero and NMT decoding alone, with practical\nadvantages in extending NMT to very large input and output vocabularies.", "published": "2016-05-15 15:53:02", "link": "http://arxiv.org/abs/1605.04569v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
