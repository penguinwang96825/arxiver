{"title": "Fast and Accurate Neural CRF Constituency Parsing", "abstract": "Estimating probability distribution is one of the core issues in the NLP\nfield. However, in both deep learning (DL) and pre-DL eras, unlike the vast\napplications of linear-chain CRF in sequence labeling tasks, very few works\nhave applied tree-structure CRF to constituency parsing, mainly due to the\ncomplexity and inefficiency of the inside-outside algorithm. This work presents\na fast and accurate neural CRF constituency parser. The key idea is to batchify\nthe inside algorithm for loss computation by direct large tensor operations on\nGPU, and meanwhile avoid the outside algorithm for gradient computation via\nefficient back-propagation. We also propose a simple two-stage\nbracketing-then-labeling parsing approach to improve efficiency further. To\nimprove the parsing performance, inspired by recent progress in dependency\nparsing, we introduce a new scoring architecture based on boundary\nrepresentation and biaffine attention, and a beneficial dropout strategy.\nExperiments on PTB, CTB5.1, and CTB7 show that our two-stage CRF parser\nachieves new state-of-the-art performance on both settings of w/o and w/ BERT,\nand can parse over 1,000 sentences per second. We release our code at\nhttps://github.com/yzhangcs/crfpar.", "published": "2020-08-09 14:38:48", "link": "http://arxiv.org/abs/2008.03736v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adversarial Training with Fast Gradient Projection Method against\n  Synonym Substitution based Text Attacks", "abstract": "Adversarial training is the most empirically successful approach in improving\nthe robustness of deep neural networks for image classification.For text\nclassification, however, existing synonym substitution based adversarial\nattacks are effective but not efficient to be incorporated into practical text\nadversarial training. Gradient-based attacks, which are very efficient for\nimages, are hard to be implemented for synonym substitution based text attacks\ndue to the lexical, grammatical and semantic constraints and the discrete text\ninput space. Thereby, we propose a fast text adversarial attack method called\nFast Gradient Projection Method (FGPM) based on synonym substitution, which is\nabout 20 times faster than existing text attack methods and could achieve\nsimilar attack performance. We then incorporate FGPM with adversarial training\nand propose a text defense method called Adversarial Training with FGPM\nenhanced by Logit pairing (ATFL). Experiments show that ATFL could\nsignificantly improve the model robustness and block the transferability of\nadversarial examples.", "published": "2020-08-09 11:02:06", "link": "http://arxiv.org/abs/2008.03709v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Distilling the Knowledge of BERT for Sequence-to-Sequence ASR", "abstract": "Attention-based sequence-to-sequence (seq2seq) models have achieved promising\nresults in automatic speech recognition (ASR). However, as these models decode\nin a left-to-right way, they do not have access to context on the right. We\nleverage both left and right context by applying BERT as an external language\nmodel to seq2seq ASR through knowledge distillation. In our proposed method,\nBERT generates soft labels to guide the training of seq2seq ASR. Furthermore,\nwe leverage context beyond the current utterance as input to BERT. Experimental\nevaluations show that our method significantly improves the ASR performance\nfrom the seq2seq baseline on the Corpus of Spontaneous Japanese (CSJ).\nKnowledge distillation from BERT outperforms that from a transformer LM that\nonly looks at left context. We also show the effectiveness of leveraging\ncontext beyond the current utterance. Our method outperforms other LM\napplication approaches such as n-best rescoring and shallow fusion, while it\ndoes not require extra inference cost.", "published": "2020-08-09 21:48:02", "link": "http://arxiv.org/abs/2008.03822v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "C1 at SemEval-2020 Task 9: SentiMix: Sentiment Analysis for Code-Mixed\n  Social Media Text using Feature Engineering", "abstract": "In today's interconnected and multilingual world, code-mixing of languages on\nsocial media is a common occurrence. While many Natural Language Processing\n(NLP) tasks like sentiment analysis are mature and well designed for\nmonolingual text, techniques to apply these tasks to code-mixed text still\nwarrant exploration. This paper describes our feature engineering approach to\nsentiment analysis in code-mixed social media text for SemEval-2020 Task 9:\nSentiMix. We tackle this problem by leveraging a set of hand-engineered\nlexical, sentiment, and metadata features to design a classifier that can\ndisambiguate between \"positive\", \"negative\" and \"neutral\" sentiment. With this\nmodel, we are able to obtain a weighted F1 score of 0.65 for the \"Hinglish\"\ntask and 0.63 for the \"Spanglish\" tasks", "published": "2020-08-09 00:46:26", "link": "http://arxiv.org/abs/2008.13549v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LRSpeech: Extremely Low-Resource Speech Synthesis and Recognition", "abstract": "Speech synthesis (text to speech, TTS) and recognition (automatic speech\nrecognition, ASR) are important speech tasks, and require a large amount of\ntext and speech pairs for model training. However, there are more than 6,000\nlanguages in the world and most languages are lack of speech training data,\nwhich poses significant challenges when building TTS and ASR systems for\nextremely low-resource languages. In this paper, we develop LRSpeech, a TTS and\nASR system under the extremely low-resource setting, which can support rare\nlanguages with low data cost. LRSpeech consists of three key techniques: 1)\npre-training on rich-resource languages and fine-tuning on low-resource\nlanguages; 2) dual transformation between TTS and ASR to iteratively boost the\naccuracy of each other; 3) knowledge distillation to customize the TTS model on\na high-quality target-speaker voice and improve the ASR model on multiple\nvoices. We conduct experiments on an experimental language (English) and a\ntruly low-resource language (Lithuanian) to verify the effectiveness of\nLRSpeech. Experimental results show that LRSpeech 1) achieves high quality for\nTTS in terms of both intelligibility (more than 98% intelligibility rate) and\nnaturalness (above 3.5 mean opinion score (MOS)) of the synthesized speech,\nwhich satisfy the requirements for industrial deployment, 2) achieves promising\nrecognition accuracy for ASR, and 3) last but not least, uses extremely\nlow-resource training data. We also conduct comprehensive analyses on LRSpeech\nwith different amounts of data resources, and provide valuable insights and\nguidances for industrial deployment. We are currently deploying LRSpeech into a\ncommercialized cloud speech service to support TTS on more rare languages.", "published": "2020-08-09 08:16:33", "link": "http://arxiv.org/abs/2008.03687v1", "categories": ["eess.AS", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Analysing the Effect of Clarifying Questions on Document Ranking in\n  Conversational Search", "abstract": "Recent research on conversational search highlights the importance of\nmixed-initiative in conversations. To enable mixed-initiative, the system\nshould be able to ask clarifying questions to the user. However, the ability of\nthe underlying ranking models (which support conversational search) to account\nfor these clarifying questions and answers has not been analysed when ranking\ndocuments, at large. To this end, we analyse the performance of a lexical\nranking model on a conversational search dataset with clarifying questions. We\ninvestigate, both quantitatively and qualitatively, how different aspects of\nclarifying questions and user answers affect the quality of ranking. We argue\nthat there needs to be some fine-grained treatment of the entire conversational\nround of clarification, based on the explicit feedback which is present in such\nmixed-initiative settings. Informed by our findings, we introduce a simple\nheuristic-based lexical baseline, that significantly outperforms the existing\nnaive baselines. Our work aims to enhance our understanding of the challenges\npresent in this particular task and inform the design of more appropriate\nconversational ranking models.", "published": "2020-08-09 12:55:16", "link": "http://arxiv.org/abs/2008.03717v2", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.IR"}
{"title": "SpeedySpeech: Efficient Neural Speech Synthesis", "abstract": "While recent neural sequence-to-sequence models have greatly improved the\nquality of speech synthesis, there has not been a system capable of fast\ntraining, fast inference and high-quality audio synthesis at the same time. We\npropose a student-teacher network capable of high-quality faster-than-real-time\nspectrogram synthesis, with low requirements on computational resources and\nfast training time. We show that self-attention layers are not necessary for\ngeneration of high quality audio. We utilize simple convolutional blocks with\nresidual connections in both student and teacher networks and use only a single\nattention layer in the teacher model. Coupled with a MelGAN vocoder, our\nmodel's voice quality was rated significantly higher than Tacotron 2. Our model\ncan be efficiently trained on a single GPU and can run in real time even on a\nCPU. We provide both our source code and audio samples in our GitHub\nrepository.", "published": "2020-08-09 20:00:57", "link": "http://arxiv.org/abs/2008.03802v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "An Overview of Voice Conversion and its Challenges: From Statistical\n  Modeling to Deep Learning", "abstract": "Speaker identity is one of the important characteristics of human speech. In\nvoice conversion, we change the speaker identity from one to another, while\nkeeping the linguistic content unchanged. Voice conversion involves multiple\nspeech processing techniques, such as speech analysis, spectral conversion,\nprosody conversion, speaker characterization, and vocoding. With the recent\nadvances in theory and practice, we are now able to produce human-like voice\nquality with high speaker similarity. In this paper, we provide a comprehensive\noverview of the state-of-the-art of voice conversion techniques and their\nperformance evaluation methods from the statistical approaches to deep\nlearning, and discuss their promise and limitations. We will also report the\nrecent Voice Conversion Challenges (VCC), the performance of the current state\nof technology, and provide a summary of the available resources for voice\nconversion research.", "published": "2020-08-09 04:31:16", "link": "http://arxiv.org/abs/2008.03648v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Cosine-Distance Virtual Adversarial Training for Semi-Supervised\n  Speaker-Discriminative Acoustic Embeddings", "abstract": "In this paper, we propose a semi-supervised learning (SSL) technique for\ntraining deep neural networks (DNNs) to generate speaker-discriminative\nacoustic embeddings (speaker embeddings). Obtaining large amounts of speaker\nrecognition train-ing data can be difficult for desired target domains,\nespecially under privacy constraints. The proposed technique reduces\nrequirements for labelled data by leveraging unlabelled data. The technique is\na variant of virtual adversarial training (VAT) [1] in the form of a loss that\nis defined as the robustness of the speaker embedding against input\nperturbations, as measured by the cosine-distance. Thus, we term the technique\ncosine-distance virtual adversarial training (CD-VAT). In comparison to many\nexisting SSL techniques, the unlabelled data does not have to come from the\nsame set of classes (here speakers) as the labelled data. The effectiveness of\nCD-VAT is shown on the 2750+ hour VoxCeleb data set, where on a speaker\nverification task it achieves a reduction in equal error rate (EER) of 11.1%\nrelative to a purely supervised baseline. This is 32.5% of the improvement that\nwould be achieved from supervised training if the speaker labels for the\nunlabelled data were available.", "published": "2020-08-09 16:09:05", "link": "http://arxiv.org/abs/2008.03756v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Deep MOS Predictor for Synthetic Speech Using Cluster-Based Modeling", "abstract": "While deep learning has made impressive progress in speech synthesis and\nvoice conversion, the assessment of the synthesized speech is still carried out\nby human participants. Several recent papers have proposed deep-learning-based\nassessment models and shown the potential to automate the speech quality\nassessment. To improve the previously proposed assessment model, MOSNet, we\npropose three models using cluster-based modeling methods: using a global\nquality token (GQT) layer, using an Encoding Layer, and using both of them. We\nperform experiments using the evaluation results of the Voice Conversion\nChallenge 2018 to predict the mean opinion score of synthesized speech and\nsimilarity score between synthesized speech and reference speech. The results\nshow that the GQT layer helps to predict human assessment better by\nautomatically learning the useful quality tokens for the task and that the\nEncoding Layer helps to utilize frame-level scores more precisely.", "published": "2020-08-09 11:14:19", "link": "http://arxiv.org/abs/2008.03710v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Disentangled Multidimensional Metric Learning for Music Similarity", "abstract": "Music similarity search is useful for a variety of creative tasks such as\nreplacing one music recording with another recording with a similar \"feel\", a\ncommon task in video editing. For this task, it is typically necessary to\ndefine a similarity metric to compare one recording to another. Music\nsimilarity, however, is hard to define and depends on multiple simultaneous\nnotions of similarity (i.e. genre, mood, instrument, tempo). While prior work\nignore this issue, we embrace this idea and introduce the concept of\nmultidimensional similarity and unify both global and specialized similarity\nmetrics into a single, semantically disentangled multidimensional similarity\nmetric. To do so, we adapt a variant of deep metric learning called conditional\nsimilarity networks to the audio domain and extend it using track-based\ninformation to control the specificity of our model. We evaluate our method and\nshow that our single, multidimensional model outperforms both specialized\nsimilarity spaces and alternative baselines. We also run a user-study and show\nthat our approach is favored by human annotators as well.", "published": "2020-08-09 13:04:25", "link": "http://arxiv.org/abs/2008.03720v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Metric Learning vs Classification for Disentangled Music Representation\n  Learning", "abstract": "Deep representation learning offers a powerful paradigm for mapping input\ndata onto an organized embedding space and is useful for many music information\nretrieval tasks. Two central methods for representation learning include deep\nmetric learning and classification, both having the same goal of learning a\nrepresentation that can generalize well across tasks. Along with\ngeneralization, the emerging concept of disentangled representations is also of\ngreat interest, where multiple semantic concepts (e.g., genre, mood,\ninstrumentation) are learned jointly but remain separable in the learned\nrepresentation space. In this paper we present a single representation learning\nframework that elucidates the relationship between metric learning,\nclassification, and disentanglement in a holistic manner. For this, we (1)\noutline past work on the relationship between metric learning and\nclassification, (2) extend this relationship to multi-label data by exploring\nthree different learning approaches and their disentangled versions, and (3)\nevaluate all models on four tasks (training time, similarity retrieval,\nauto-tagging, and triplet prediction). We find that classification-based models\nare generally advantageous for training time, similarity retrieval, and\nauto-tagging, while deep metric learning exhibits better performance for\ntriplet-prediction. Finally, we show that our proposed approach yields\nstate-of-the-art results for music auto-tagging.", "published": "2020-08-09 13:53:12", "link": "http://arxiv.org/abs/2008.03729v2", "categories": ["cs.SD", "cs.IR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Accurate Detection of Wake Word Start and End Using a CNN", "abstract": "Small footprint embedded devices require keyword spotters (KWS) with small\nmodel size and detection latency for enabling voice assistants. Such a keyword\nis often referred to as \\textit{wake word} as it is used to wake up voice\nassistant enabled devices. Together with wake word detection, accurate\nestimation of wake word endpoints (start and end) is an important task of KWS.\nIn this paper, we propose two new methods for detecting the endpoints of wake\nwords in neural KWS that use single-stage word-level neural networks. Our\nresults show that the new techniques give superior accuracy for detecting wake\nwords' endpoints of up to 50 msec standard error versus human annotations, on\npar with the conventional Acoustic Model plus HMM forced alignment. To our\nknowledge, this is the first study of wake word endpoints detection methods for\nsingle-stage neural KWS.", "published": "2020-08-09 19:02:41", "link": "http://arxiv.org/abs/2008.03790v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speaker Conditional WaveRNN: Towards Universal Neural Vocoder for Unseen\n  Speaker and Recording Conditions", "abstract": "Recent advancements in deep learning led to human-level performance in\nsingle-speaker speech synthesis. However, there are still limitations in terms\nof speech quality when generalizing those systems into multiple-speaker models\nespecially for unseen speakers and unseen recording qualities. For instance,\nconventional neural vocoders are adjusted to the training speaker and have poor\ngeneralization capabilities to unseen speakers. In this work, we propose a\nvariant of WaveRNN, referred to as speaker conditional WaveRNN (SC-WaveRNN). We\ntarget towards the development of an efficient universal vocoder even for\nunseen speakers and recording conditions. In contrast to standard WaveRNN,\nSC-WaveRNN exploits additional information given in the form of speaker\nembeddings. Using publicly-available data for training, SC-WaveRNN achieves\nsignificantly better performance over baseline WaveRNN on both subjective and\nobjective metrics. In MOS, SC-WaveRNN achieves an improvement of about 23% for\nseen speaker and seen recording condition and up to 95% for unseen speaker and\nunseen condition. Finally, we extend our work by implementing a multi-speaker\ntext-to-speech (TTS) synthesis similar to zero-shot speaker adaptation. In\nterms of performance, our system has been preferred over the baseline TTS\nsystem by 60% over 15.5% and by 60.9% over 32.6%, for seen and unseen speakers,\nrespectively.", "published": "2020-08-09 13:54:46", "link": "http://arxiv.org/abs/2008.05289v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
