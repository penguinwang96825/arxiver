{"title": "Are Character-level Translations Worth the Wait? Comparing ByT5 and mT5\n  for Machine Translation", "abstract": "Pretrained character-level and byte-level language models have been shown to\nbe competitive with popular subword models across a range of Natural Language\nProcessing (NLP) tasks. However, there has been little research on their\neffectiveness for neural machine translation (NMT), particularly within the\npopular pretrain-then-finetune paradigm. This work performs an extensive\ncomparison across multiple languages and experimental conditions of character-\nand subword-level pretrained models (ByT5 and mT5, respectively) on NMT. We\nshow the effectiveness of character-level modeling in translation, particularly\nin cases where fine-tuning data is limited. In our analysis, we show how\ncharacter models' gains in translation quality are reflected in better\ntranslations of orthographically similar words and rare words. While evaluating\nthe importance of source texts in driving model predictions, we highlight\nword-level patterns within ByT5, suggesting an ability to modulate word-level\nand character-level information during generation. We conclude by assessing the\nefficiency tradeoff of byte models, suggesting their usage in non-time-critical\nscenarios to boost translation quality.", "published": "2023-02-28 00:50:19", "link": "http://arxiv.org/abs/2302.14220v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Augmented Transformers with Adaptive n-grams Embedding for Multilingual\n  Scene Text Recognition", "abstract": "While vision transformers have been highly successful in improving the\nperformance in image-based tasks, not much work has been reported on applying\ntransformers to multilingual scene text recognition due to the complexities in\nthe visual appearance of multilingual texts. To fill the gap, this paper\nproposes an augmented transformer architecture with n-grams embedding and\ncross-language rectification (TANGER). TANGER consists of a primary transformer\nwith single patch embeddings of visual images, and a supplementary transformer\nwith adaptive n-grams embeddings that aims to flexibly explore the potential\ncorrelations between neighbouring visual patches, which is essential for\nfeature extraction from multilingual scene texts. Cross-language rectification\nis achieved with a loss function that takes into account both language\nidentification and contextual coherence scoring. Extensive comparative studies\nare conducted on four widely used benchmark datasets as well as a new\nmultilingual scene text dataset containing Indonesian, English, and Chinese\ncollected from tourism scenes in Indonesia. Our experimental results\ndemonstrate that TANGER is considerably better compared to the\nstate-of-the-art, especially in handling complex multilingual scene texts.", "published": "2023-02-28 02:37:30", "link": "http://arxiv.org/abs/2302.14261v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HugNLP: A Unified and Comprehensive Library for Natural Language\n  Processing", "abstract": "In this paper, we introduce HugNLP, a unified and comprehensive library for\nnatural language processing (NLP) with the prevalent backend of HuggingFace\nTransformers, which is designed for NLP researchers to easily utilize\noff-the-shelf algorithms and develop novel methods with user-defined models and\ntasks in real-world scenarios. HugNLP consists of a hierarchical structure\nincluding models, processors and applications that unifies the learning process\nof pre-trained language models (PLMs) on different NLP tasks. Additionally, we\npresent some featured NLP applications to show the effectiveness of HugNLP,\nsuch as knowledge-enhanced PLMs, universal information extraction, low-resource\nmining, and code understanding and generation, etc. The source code will be\nreleased on GitHub (https://github.com/wjn1996/HugNLP).", "published": "2023-02-28 03:38:26", "link": "http://arxiv.org/abs/2302.14286v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Information-Restricted Neural Language Models Reveal Different Brain\n  Regions' Sensitivity to Semantics, Syntax and Context", "abstract": "A fundamental question in neurolinguistics concerns the brain regions\ninvolved in syntactic and semantic processing during speech comprehension, both\nat the lexical (word processing) and supra-lexical levels (sentence and\ndiscourse processing). To what extent are these regions separated or\nintertwined? To address this question, we trained a lexical language model,\nGlove, and a supra-lexical language model, GPT-2, on a text corpus from which\nwe selectively removed either syntactic or semantic information. We then\nassessed to what extent these information-restricted models were able to\npredict the time-courses of fMRI signal of humans listening to naturalistic\ntext. We also manipulated the size of contextual information provided to GPT-2\nin order to determine the windows of integration of brain regions involved in\nsupra-lexical processing. Our analyses show that, while most brain regions\ninvolved in language are sensitive to both syntactic and semantic variables,\nthe relative magnitudes of these effects vary a lot across these regions.\nFurthermore, we found an asymmetry between the left and right hemispheres, with\nsemantic and syntactic processing being more dissociated in the left hemisphere\nthan in the right, and the left and right hemispheres showing respectively\ngreater sensitivity to short and long contexts. The use of\ninformation-restricted NLP models thus shed new light on the spatial\norganization of syntactic processing, semantic processing and compositionality.", "published": "2023-02-28 08:16:18", "link": "http://arxiv.org/abs/2302.14389v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Instruction Clarification Requests in Multimodal Collaborative Dialogue\n  Games: Tasks, and an Analysis of the CoDraw Dataset", "abstract": "In visual instruction-following dialogue games, players can engage in repair\nmechanisms in face of an ambiguous or underspecified instruction that cannot be\nfully mapped to actions in the world. In this work, we annotate Instruction\nClarification Requests (iCRs) in CoDraw, an existing dataset of interactions in\na multimodal collaborative dialogue game. We show that it contains lexically\nand semantically diverse iCRs being produced self-motivatedly by players\ndeciding to clarify in order to solve the task successfully. With 8.8k iCRs\nfound in 9.9k dialogues, CoDraw-iCR (v1) is a large spontaneous iCR corpus,\nmaking it a valuable resource for data-driven research on clarification in\ndialogue. We then formalise and provide baseline models for two tasks:\nDetermining when to make an iCR and how to recognise them, in order to\ninvestigate to what extent these tasks are learnable from data.", "published": "2023-02-28 08:41:53", "link": "http://arxiv.org/abs/2302.14406v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SMoA: Sparse Mixture of Adapters to Mitigate Multiple Dataset Biases", "abstract": "Recent studies reveal that various biases exist in different NLP tasks, and\nover-reliance on biases results in models' poor generalization ability and low\nadversarial robustness. To mitigate datasets biases, previous works propose\nlots of debiasing techniques to tackle specific biases, which perform well on\nrespective adversarial sets but fail to mitigate other biases. In this paper,\nwe propose a new debiasing method Sparse Mixture-of-Adapters (SMoA), which can\nmitigate multiple dataset biases effectively and efficiently. Experiments on\nNatural Language Inference and Paraphrase Identification tasks demonstrate that\nSMoA outperforms full-finetuning, adapter tuning baselines, and prior strong\ndebiasing methods. Further analysis indicates the interpretability of SMoA that\nsub-adapter can capture specific pattern from the training data and specialize\nto handle specific bias.", "published": "2023-02-28 08:47:20", "link": "http://arxiv.org/abs/2302.14413v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text classification dataset and analysis for Uzbek language", "abstract": "Text classification is an important task in Natural Language Processing\n(NLP), where the goal is to categorize text data into predefined classes. In\nthis study, we analyse the dataset creation steps and evaluation techniques of\nmulti-label news categorisation task as part of text classification. We first\npresent a newly obtained dataset for Uzbek text classification, which was\ncollected from 10 different news and press websites and covers 15 categories of\nnews, press and law texts. We also present a comprehensive evaluation of\ndifferent models, ranging from traditional bag-of-words models to deep learning\narchitectures, on this newly created dataset. Our experiments show that the\nRecurrent Neural Network (RNN) and Convolutional Neural Network (CNN) based\nmodels outperform the rule-based models. The best performance is achieved by\nthe BERTbek model, which is a transformer-based BERT model trained on the Uzbek\ncorpus. Our findings provide a good baseline for further research in Uzbek text\nclassification.", "published": "2023-02-28 11:21:24", "link": "http://arxiv.org/abs/2302.14494v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey on Long Text Modeling with Transformers", "abstract": "Modeling long texts has been an essential technique in the field of natural\nlanguage processing (NLP). With the ever-growing number of long documents, it\nis important to develop effective modeling methods that can process and analyze\nsuch texts. However, long texts pose important research challenges for existing\ntext models, with more complex semantics and special characteristics. In this\npaper, we provide an overview of the recent advances on long texts modeling\nbased on Transformer models. Firstly, we introduce the formal definition of\nlong text modeling. Then, as the core content, we discuss how to process long\ninput to satisfy the length limitation and design improved Transformer\narchitectures to effectively extend the maximum context length. Following this,\nwe discuss how to adapt Transformer models to capture the special\ncharacteristics of long texts. Finally, we describe four typical applications\ninvolving long text modeling and conclude this paper with a discussion of\nfuture directions. Our survey intends to provide researchers with a synthesis\nand pointer to related work on long text modeling.", "published": "2023-02-28 11:34:30", "link": "http://arxiv.org/abs/2302.14502v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models Are State-of-the-Art Evaluators of Translation\n  Quality", "abstract": "We describe GEMBA, a GPT-based metric for assessment of translation quality,\nwhich works both with a reference translation and without. In our evaluation,\nwe focus on zero-shot prompting, comparing four prompt variants in two modes,\nbased on the availability of the reference. We investigate nine versions of GPT\nmodels, including ChatGPT and GPT-4. We show that our method for translation\nquality assessment only works with GPT~3.5 and larger models. Comparing to\nresults from WMT22's Metrics shared task, our method achieves state-of-the-art\naccuracy in both modes when compared to MQM-based human labels. Our results are\nvalid on the system level for all three WMT22 Metrics shared task language\npairs, namely English into German, English into Russian, and Chinese into\nEnglish. This provides a first glimpse into the usefulness of pre-trained,\ngenerative large language models for quality assessment of translations. We\npublicly release all our code and prompt templates used for the experiments\ndescribed in this work, as well as all corresponding scoring results, to allow\nfor external validation and reproducibility.", "published": "2023-02-28 12:23:48", "link": "http://arxiv.org/abs/2302.14520v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Heteronym Resolution Pipeline Using RAD-TTS Aligners", "abstract": "Grapheme-to-phoneme (G2P) transduction is part of the standard text-to-speech\n(TTS) pipeline. However, G2P conversion is difficult for languages that contain\nheteronyms -- words that have one spelling but can be pronounced in multiple\nways. G2P datasets with annotated heteronyms are limited in size and expensive\nto create, as human labeling remains the primary method for heteronym\ndisambiguation. We propose a RAD-TTS Aligner-based pipeline to automatically\ndisambiguate heteronyms in datasets that contain both audio with text\ntranscripts. The best pronunciation can be chosen by generating all possible\ncandidates for each heteronym and scoring them with an Aligner model. The\nresulting labels can be used to create training datasets for use in both\nmulti-stage and end-to-end G2P systems.", "published": "2023-02-28 12:33:12", "link": "http://arxiv.org/abs/2302.14523v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "H-AES: Towards Automated Essay Scoring for Hindi", "abstract": "The use of Natural Language Processing (NLP) for Automated Essay Scoring\n(AES) has been well explored in the English language, with benchmark models\nexhibiting performance comparable to human scorers. However, AES in Hindi and\nother low-resource languages remains unexplored. In this study, we reproduce\nand compare state-of-the-art methods for AES in the Hindi domain. We employ\nclassical feature-based Machine Learning (ML) and advanced end-to-end models,\nincluding LSTM Networks and Fine-Tuned Transformer Architecture, in our\napproach and derive results comparable to those in the English language domain.\nHindi being a low-resource language, lacks a dedicated essay-scoring corpus. We\ntrain and evaluate our models using translated English essays and empirically\nmeasure their performance on our own small-scale, real-world Hindi corpus. We\nfollow this up with an in-depth analysis discussing prompt-specific behavior of\ndifferent language models implemented.", "published": "2023-02-28 15:14:15", "link": "http://arxiv.org/abs/2302.14635v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is Japanese CCGBank empirically correct? A case study of passive and\n  causative constructions", "abstract": "The Japanese CCGBank serves as training and evaluation data for developing\nJapanese CCG parsers. However, since it is automatically generated from the\nKyoto Corpus, a dependency treebank, its linguistic validity still needs to be\nsufficiently verified. In this paper, we focus on the analysis of\npassive/causative constructions in the Japanese CCGBank and show that, together\nwith the compositional semantics of ccg2lambda, a semantic parsing system, it\nyields empirically wrong predictions for the nested construction of passives\nand causatives.", "published": "2023-02-28 16:19:24", "link": "http://arxiv.org/abs/2302.14708v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatically Classifying Emotions based on Text: A Comparative\n  Exploration of Different Datasets", "abstract": "Emotion Classification based on text is a task with many applications which\nhas received growing interest in recent years. This paper presents a\npreliminary study with the goal to help researchers and practitioners gain\ninsight into relatively new datasets as well as emotion classification in\ngeneral. We focus on three datasets that were recently presented in the related\nliterature, and we explore the performance of traditional as well as\nstate-of-the-art deep learning models in the presence of different\ncharacteristics in the data. We also explore the use of data augmentation in\norder to improve performance. Our experimental work shows that state-of-the-art\nmodels such as RoBERTa perform the best for all cases. We also provide\nobservations and discussion that highlight the complexity of emotion\nclassification in these datasets and test out the applicability of the models\nto actual social media posts we collected and labeled.", "published": "2023-02-28 16:34:55", "link": "http://arxiv.org/abs/2302.14727v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Joint Representations of Text and Knowledge Graphs for Retrieval and\n  Evaluation", "abstract": "A key feature of neural models is that they can produce semantic vector\nrepresentations of objects (texts, images, speech, etc.) ensuring that similar\nobjects are close to each other in the vector space. While much work has\nfocused on learning representations for other modalities, there are no aligned\ncross-modal representations for text and knowledge base (KB) elements. One\nchallenge for learning such representations is the lack of parallel data, which\nwe use contrastive training on heuristics-based datasets and data augmentation\nto overcome, training embedding models on (KB graph, text) pairs. On WebNLG, a\ncleaner manually crafted dataset, we show that they learn aligned\nrepresentations suitable for retrieval. We then fine-tune on annotated data to\ncreate EREDAT (Ensembled Representations for Evaluation of DAta-to-Text), a\nsimilarity metric between English text and KB graphs. EREDAT outperforms or\nmatches state-of-the-art metrics in terms of correlation with human judgments\non WebNLG even though, unlike them, it does not require a reference text to\ncompare against.", "published": "2023-02-28 17:39:43", "link": "http://arxiv.org/abs/2302.14785v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Scoring of Dream Reports' Emotional Content with Large\n  Language Models", "abstract": "In the field of dream research, the study of dream content typically relies\non the analysis of verbal reports provided by dreamers upon awakening from\ntheir sleep. This task is classically performed through manual scoring provided\nby trained annotators, at a great time expense. While a consistent body of work\nsuggests that natural language processing (NLP) tools can support the automatic\nanalysis of dream reports, proposed methods lacked the ability to reason over a\nreport's full context and required extensive data pre-processing. Furthermore,\nin most cases, these methods were not validated against standard manual scoring\napproaches. In this work, we address these limitations by adopting large\nlanguage models (LLMs) to study and replicate the manual annotation of dream\nreports, using a mixture of off-the-shelf and bespoke approaches, with a focus\non references to reports' emotions. Our results show that the off-the-shelf\nmethod achieves a low performance probably in light of inherent linguistic\ndifferences between reports collected in different (groups of) individuals. On\nthe other hand, the proposed bespoke text classification method achieves a high\nperformance, which is robust against potential biases. Overall, these\nobservations indicate that our approach could find application in the analysis\nof large dream datasets and may favour reproducibility and comparability of\nresults across studies.", "published": "2023-02-28 18:23:17", "link": "http://arxiv.org/abs/2302.14828v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Weighted Sampling for Masked Language Modeling", "abstract": "Masked Language Modeling (MLM) is widely used to pretrain language models.\nThe standard random masking strategy in MLM causes the pre-trained language\nmodels (PLMs) to be biased toward high-frequency tokens. Representation\nlearning of rare tokens is poor and PLMs have limited performance on downstream\ntasks. To alleviate this frequency bias issue, we propose two simple and\neffective Weighted Sampling strategies for masking tokens based on the token\nfrequency and training loss. We apply these two strategies to BERT and obtain\nWeighted-Sampled BERT (WSBERT). Experiments on the Semantic Textual Similarity\nbenchmark (STS) show that WSBERT significantly improves sentence embeddings\nover BERT. Combining WSBERT with calibration methods and prompt learning\nfurther improves sentence embeddings. We also investigate fine-tuning WSBERT on\nthe GLUE benchmark and show that Weighted Sampling also improves the transfer\nlearning capability of the backbone PLM. We further analyze and provide\ninsights into how WSBERT improves token embeddings.", "published": "2023-02-28 01:07:39", "link": "http://arxiv.org/abs/2302.14225v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Cross-Lingual Summarization via Large Language Models", "abstract": "Given a document in a source language, cross-lingual summarization (CLS) aims\nto generate a summary in a different target language. Recently, the emergence\nof Large Language Models (LLMs), such as GPT-3.5, ChatGPT and GPT-4, has\nattracted wide attention from the computational linguistics community. However,\nit is not yet known the performance of LLMs on CLS. In this report, we\nempirically use various prompts to guide LLMs to perform zero-shot CLS from\ndifferent paradigms (i.e., end-to-end and pipeline), and provide a preliminary\nevaluation on the generated summaries. We find that ChatGPT and GPT-4\noriginally prefer to produce lengthy summaries with detailed information. These\ntwo LLMs can further balance informativeness and conciseness with the help of\nan interactive prompt, significantly improving their CLS performance.\nExperimental results on three widely-used CLS datasets show that GPT-4 achieves\nstate-of-the-art zero-shot CLS performance, and performs competitively compared\nwith the fine-tuned mBART-50. Moreover, we also find some multi-lingual and\nbilingual LLMs (i.e., BLOOMZ, ChatGLM-6B, Vicuna-13B and ChatYuan) have limited\nzero-shot CLS ability. Due to the composite nature of CLS, which requires\nmodels to perform summarization and translation simultaneously, accomplishing\nthis task in a zero-shot manner is even a challenge for LLMs. Therefore, we\nsincerely hope and recommend future LLM research could use CLS as a testbed.", "published": "2023-02-28 01:27:37", "link": "http://arxiv.org/abs/2302.14229v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GLM-Dialog: Noise-tolerant Pre-training for Knowledge-grounded Dialogue\n  Generation", "abstract": "We present GLM-Dialog, a large-scale language model (LLM) with 10B parameters\ncapable of knowledge-grounded conversation in Chinese using a search engine to\naccess the Internet knowledge. GLM-Dialog offers a series of applicable\ntechniques for exploiting various external knowledge including both helpful and\nnoisy knowledge, enabling the creation of robust knowledge-grounded dialogue\nLLMs with limited proper datasets. To evaluate the GLM-Dialog more fairly, we\nalso propose a novel evaluation method to allow humans to converse with\nmultiple deployed bots simultaneously and compare their performance implicitly\ninstead of explicitly rating using multidimensional metrics.Comprehensive\nevaluations from automatic to human perspective demonstrate the advantages of\nGLM-Dialog comparing with existing open source Chinese dialogue models. We\nrelease both the model checkpoint and source code, and also deploy it as a\nWeChat application to interact with users. We offer our evaluation platform\nonline in an effort to prompt the development of open source models and\nreliable dialogue evaluation systems. The additional easy-to-use toolkit that\nconsists of short text entity linking, query generation, and helpful knowledge\nclassification is also released to enable diverse applications. All the source\ncode is available on Github.", "published": "2023-02-28 08:35:28", "link": "http://arxiv.org/abs/2302.14401v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Spacerini: Plug-and-play Search Engines with Pyserini and Hugging Face", "abstract": "We present Spacerini, a tool that integrates the Pyserini toolkit for\nreproducible information retrieval research with Hugging Face to enable the\nseamless construction and deployment of interactive search engines. Spacerini\nmakes state-of-the-art sparse and dense retrieval models more accessible to\nnon-IR practitioners while minimizing deployment effort. This is useful for NLP\nresearchers who want to better understand and validate their research by\nperforming qualitative analyses of training corpora, for IR researchers who\nwant to demonstrate new retrieval models integrated into the growing Pyserini\necosystem, and for third parties reproducing the work of other researchers.\nSpacerini is open source and includes utilities for loading, preprocessing,\nindexing, and deploying search engines locally and remotely. We demonstrate a\nportfolio of 13 search engines created with Spacerini for different use cases.", "published": "2023-02-28 12:44:10", "link": "http://arxiv.org/abs/2302.14534v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Synthesizing Mixed-type Electronic Health Records using Diffusion Models", "abstract": "Electronic Health Records (EHRs) contain sensitive patient information, which\npresents privacy concerns when sharing such data. Synthetic data generation is\na promising solution to mitigate these risks, often relying on deep generative\nmodels such as Generative Adversarial Networks (GANs). However, recent studies\nhave shown that diffusion models offer several advantages over GANs, such as\ngeneration of more realistic synthetic data and stable training in generating\ndata modalities, including image, text, and sound. In this work, we investigate\nthe potential of diffusion models for generating realistic mixed-type tabular\nEHRs, comparing TabDDPM model with existing methods on four datasets in terms\nof data quality, utility, privacy, and augmentation. Our experiments\ndemonstrate that TabDDPM outperforms the state-of-the-art models across all\nevaluation metrics, except for privacy, which confirms the trade-off between\nprivacy and utility.", "published": "2023-02-28 15:42:30", "link": "http://arxiv.org/abs/2302.14679v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Investigating the Effectiveness of Task-Agnostic Prefix Prompt for\n  Instruction Following", "abstract": "In this paper, we present our finding that prepending a Task-Agnostic Prefix\nPrompt (TAPP) to the input improves the instruction-following ability of\nvarious Large Language Models (LLMs) during inference. TAPP is different from\ncanonical prompts for LLMs in that it is a fixed prompt prepended to the\nbeginning of every input regardless of the target task for zero-shot\ngeneralization. We observe that both base LLMs (i.e. not fine-tuned to follow\ninstructions) and instruction-tuned models benefit from TAPP, resulting in\n34.58% and 12.26% improvement on average, respectively. This implies that the\ninstruction-following ability of LLMs can be improved during inference time\nwith a fixed prompt constructed with simple heuristics. We hypothesize that\nTAPP assists language models to better estimate the output distribution by\nfocusing more on the instruction of the target task during inference. In other\nwords, such ability does not seem to be sufficiently activated in not only base\nLLMs but also many instruction-fine-tuned LLMs. All experiments are\nreproducible from https://github.com/seonghyeonye/TAPP.", "published": "2023-02-28 16:06:35", "link": "http://arxiv.org/abs/2302.14691v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Self-training through Classifier Disagreement for Cross-Domain Opinion\n  Target Extraction", "abstract": "Opinion target extraction (OTE) or aspect extraction (AE) is a fundamental\ntask in opinion mining that aims to extract the targets (or aspects) on which\nopinions have been expressed. Recent work focus on cross-domain OTE, which is\ntypically encountered in real-world scenarios, where the testing and training\ndistributions differ. Most methods use domain adversarial neural networks that\naim to reduce the domain gap between the labelled source and unlabelled target\ndomains to improve target domain performance. However, this approach only\naligns feature distributions and does not account for class-wise feature\nalignment, leading to suboptimal results. Semi-supervised learning (SSL) has\nbeen explored as a solution, but is limited by the quality of pseudo-labels\ngenerated by the model. Inspired by the theoretical foundations in domain\nadaptation [2], we propose a new SSL approach that opts for selecting target\nsamples whose model output from a domain-specific teacher and student network\ndisagree on the unlabelled target data, in an effort to boost the target domain\nperformance. Extensive experiments on benchmark cross-domain OTE datasets show\nthat this approach is effective and performs consistently well in settings with\nlarge domain shifts.", "published": "2023-02-28 16:31:17", "link": "http://arxiv.org/abs/2302.14719v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Beyond the limitations of any imaginable mechanism: large language\n  models and psycholinguistics", "abstract": "Large language models are not detailed models of human linguistic processing.\nThey are, however, extremely successful at their primary task: providing a\nmodel for language. For this reason and because there are no animal models for\nlanguage, large language models are important in psycholinguistics: they are\nuseful as a practical tool, as an illustrative comparative, and\nphilosophically, as a basis for recasting the relationship between language and\nthought.", "published": "2023-02-28 20:49:38", "link": "http://arxiv.org/abs/2303.00077v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PANACEA: An Automated Misinformation Detection System on COVID-19", "abstract": "In this demo, we introduce a web-based misinformation detection system\nPANACEA on COVID-19 related claims, which has two modules, fact-checking and\nrumour detection. Our fact-checking module, which is supported by novel natural\nlanguage inference methods with a self-attention network, outperforms\nstate-of-the-art approaches. It is also able to give automated veracity\nassessment and ranked supporting evidence with the stance towards the claim to\nbe checked. In addition, PANACEA adapts the bi-directional graph convolutional\nnetworks model, which is able to detect rumours based on comment networks of\nrelated tweets, instead of relying on the knowledge base. This rumour detection\nmodule assists by warning the users in the early stages when a knowledge base\nmay not be available.", "published": "2023-02-28 21:53:48", "link": "http://arxiv.org/abs/2303.01241v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An evaluation of Google Translate for Sanskrit to English translation\n  via sentiment and semantic analysis", "abstract": "Google Translate has been prominent for language translation; however,\nlimited work has been done in evaluating the quality of translation when\ncompared to human experts. Sanskrit one of the oldest written languages in the\nworld. In 2022, the Sanskrit language was added to the Google Translate engine.\nSanskrit is known as the mother of languages such as Hindi and an ancient\nsource of the Indo-European group of languages. Sanskrit is the original\nlanguage for sacred Hindu texts such as the Bhagavad Gita. In this study, we\npresent a framework that evaluates the Google Translate for Sanskrit using the\nBhagavad Gita. We first publish a translation of the Bhagavad Gita in Sanskrit\nusing Google Translate. Our framework then compares Google Translate version of\nBhagavad Gita with expert translations using sentiment and semantic analysis\nvia BERT-based language models. Our results indicate that in terms of sentiment\nand semantic analysis, there is low level of similarity in selected verses of\nGoogle Translate when compared to expert translations. In the qualitative\nevaluation, we find that Google translate is unsuitable for translation of\ncertain Sanskrit words and phrases due to its poetic nature, contextual\nsignificance, metaphor and imagery. The mistranslations are not surprising\nsince the Bhagavad Gita is known as a difficult text not only to translate, but\nalso to interpret since it relies on contextual, philosophical and historical\ninformation. Our framework lays the foundation for automatic evaluation of\nother languages by Google Translate", "published": "2023-02-28 04:24:55", "link": "http://arxiv.org/abs/2303.07201v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Goal Driven Discovery of Distributional Differences via Language\n  Descriptions", "abstract": "Mining large corpora can generate useful discoveries but is time-consuming\nfor humans. We formulate a new task, D5, that automatically discovers\ndifferences between two large corpora in a goal-driven way. The task input is a\nproblem comprising a research goal \"$\\textit{comparing the side effects of drug\nA and drug B}$\" and a corpus pair (two large collections of patients'\nself-reported reactions after taking each drug). The output is a language\ndescription (discovery) of how these corpora differ (patients taking drug A\n\"$\\textit{mention feelings of paranoia}$\" more often). We build a D5 system,\nand to quantitatively measure its performance, we 1) contribute a meta-dataset,\nOpenD5, aggregating 675 open-ended problems ranging across business, social\nsciences, humanities, machine learning, and health, and 2) propose a set of\nunified evaluation metrics: validity, relevance, novelty, and significance.\nWith the dataset and the unified metrics, we confirm that language models can\nuse the goals to propose more relevant, novel, and significant candidate\ndiscoveries. Finally, our system produces discoveries previously unknown to the\nauthors on a wide range of applications in OpenD5, including temporal and\ndemographic differences in discussion topics, political stances and stereotypes\nin speech, insights in commercial reviews, and error patterns in NLP models.", "published": "2023-02-28 01:32:32", "link": "http://arxiv.org/abs/2302.14233v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Linear Spaces of Meanings: Compositional Structures in Vision-Language\n  Models", "abstract": "We investigate compositional structures in data embeddings from pre-trained\nvision-language models (VLMs). Traditionally, compositionality has been\nassociated with algebraic operations on embeddings of words from a pre-existing\nvocabulary. In contrast, we seek to approximate representations from an encoder\nas combinations of a smaller set of vectors in the embedding space. These\nvectors can be seen as \"ideal words\" for generating concepts directly within\nthe embedding space of the model. We first present a framework for\nunderstanding compositional structures from a geometric perspective. We then\nexplain what these compositional structures entail probabilistically in the\ncase of VLM embeddings, providing intuitions for why they arise in practice.\nFinally, we empirically explore these structures in CLIP's embeddings and we\nevaluate their usefulness for solving different vision-language tasks such as\nclassification, debiasing, and retrieval. Our results show that simple linear\nalgebraic operations on embedding vectors can be used as compositional and\ninterpretable methods for regulating the behavior of VLMs.", "published": "2023-02-28 08:11:56", "link": "http://arxiv.org/abs/2302.14383v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "The 2022 NIST Language Recognition Evaluation", "abstract": "In 2022, the U.S. National Institute of Standards and Technology (NIST)\nconducted the latest Language Recognition Evaluation (LRE) in an ongoing series\nadministered by NIST since 1996 to foster research in language recognition and\nto measure state-of-the-art technology. Similar to previous LREs, LRE22 focused\non conversational telephone speech (CTS) and broadcast narrowband speech (BNBS)\ndata. LRE22 also introduced new evaluation features, such as an emphasis on\nAfrican languages, including low resource languages, and a test set consisting\nof segments containing between 3s and 35s of speech randomly sampled and\nextracted from longer recordings. A total of 21 research organizations, forming\n16 teams, participated in this 3-month long evaluation and made a total of 65\nvalid system submissions to be evaluated. This paper presents an overview of\nLRE22 and an analysis of system performance over different evaluation\nconditions. The evaluation results suggest that Oromo and Tigrinya are easier\nto detect while Xhosa and Zulu are more challenging. A greater confusability is\nseen for some language pairs. When speech duration increased, system\nperformance significantly increased up to a certain duration, and then a\ndiminishing return on system performance is observed afterward.", "published": "2023-02-28 15:05:33", "link": "http://arxiv.org/abs/2302.14624v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Which One Are You Referring To? Multimodal Object Identification in\n  Situated Dialogue", "abstract": "The demand for multimodal dialogue systems has been rising in various\ndomains, emphasizing the importance of interpreting multimodal inputs from\nconversational and situational contexts. We explore three methods to tackle\nthis problem and evaluate them on the largest situated dialogue dataset, SIMMC\n2.1. Our best method, scene-dialogue alignment, improves the performance by\n~20% F1-score compared to the SIMMC 2.1 baselines. We provide analysis and\ndiscussion regarding the limitation of our methods and the potential directions\nfor future works. Our code is publicly available at\nhttps://github.com/holylovenia/multimodal-object-identification.", "published": "2023-02-28 15:45:20", "link": "http://arxiv.org/abs/2302.14680v2", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "EvoPrompting: Language Models for Code-Level Neural Architecture Search", "abstract": "Given the recent impressive accomplishments of language models (LMs) for code\ngeneration, we explore the use of LMs as adaptive mutation and crossover\noperators for an evolutionary neural architecture search (NAS) algorithm. While\nNAS still proves too difficult a task for LMs to succeed at solely through\nprompting, we find that the combination of evolutionary prompt engineering with\nsoft prompt-tuning, a method we term EvoPrompting, consistently finds diverse\nand high performing models. We first demonstrate that EvoPrompting is effective\non the computationally efficient MNIST-1D dataset, where EvoPrompting produces\nconvolutional architecture variants that outperform both those designed by\nhuman experts and naive few-shot prompting in terms of accuracy and model size.\nWe then apply our method to searching for graph neural networks on the CLRS\nAlgorithmic Reasoning Benchmark, where EvoPrompting is able to design novel\narchitectures that outperform current state-of-the-art models on 21 out of 30\nalgorithmic reasoning tasks while maintaining similar model size. EvoPrompting\nis successful at designing accurate and efficient neural network architectures\nacross a variety of machine learning tasks, while also being general enough for\neasy adaptation to other tasks beyond neural network design.", "published": "2023-02-28 18:37:25", "link": "http://arxiv.org/abs/2302.14838v3", "categories": ["cs.NE", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.NE"}
{"title": "ClArTTS: An Open-Source Classical Arabic Text-to-Speech Corpus", "abstract": "At present, Text-to-speech (TTS) systems that are trained with high-quality\ntranscribed speech data using end-to-end neural models can generate speech that\nis intelligible, natural, and closely resembles human speech. These models are\ntrained with relatively large single-speaker professionally recorded audio,\ntypically extracted from audiobooks. Meanwhile, due to the scarcity of freely\navailable speech corpora of this kind, a larger gap exists in Arabic TTS\nresearch and development. Most of the existing freely available Arabic speech\ncorpora are not suitable for TTS training as they contain multi-speaker casual\nspeech with variations in recording conditions and quality, whereas the corpus\ncurated for speech synthesis are generally small in size and not suitable for\ntraining state-of-the-art end-to-end models. In a move towards filling this gap\nin resources, we present a speech corpus for Classical Arabic Text-to-Speech\n(ClArTTS) to support the development of end-to-end TTS systems for Arabic. The\nspeech is extracted from a LibriVox audiobook, which is then processed,\nsegmented, and manually transcribed and annotated. The final ClArTTS corpus\ncontains about 12 hours of speech from a single male speaker sampled at 40100\nkHz. In this paper, we describe the process of corpus creation and provide\ndetails of corpus statistics and a comparison with existing resources.\nFurthermore, we develop two TTS systems based on Grad-TTS and Glow-TTS and\nillustrate the performance of the resulting systems via subjective and\nobjective evaluations. The corpus will be made publicly available at\nwww.clartts.com for research purposes, along with the baseline TTS systems\ndemo.", "published": "2023-02-28 20:18:59", "link": "http://arxiv.org/abs/2303.00069v1", "categories": ["cs.CL", "cs.SD", "eess.AS", "none"], "primary_category": "cs.CL"}
{"title": "Deep learning for COVID-19 topic modelling via Twitter: Alpha, Delta and\n  Omicron", "abstract": "Topic modelling with innovative deep learning methods has gained interest for\na wide range of applications that includes COVID-19. Topic modelling can\nprovide, psychological, social and cultural insights for understanding human\nbehaviour in extreme events such as the COVID-19 pandemic. In this paper, we\nuse prominent deep learning-based language models for COVID-19 topic modelling\ntaking into account data from emergence (Alpha) to the Omicron variant. We\napply topic modeling to review the public behaviour across the first, second\nand third waves based on Twitter dataset from India. Our results show that the\ntopics extracted for the subsequent waves had certain overlapping themes such\nas covers governance, vaccination, and pandemic management while novel issues\naroused in political, social and economic situation during COVID-19 pandemic.\nWe also found a strong correlation of the major topics qualitatively to news\nmedia prevalent at the respective time period. Hence, our framework has the\npotential to capture major issues arising during different phases of the\nCOVID-19 pandemic which can be extended to other countries and regions.", "published": "2023-02-28 23:40:41", "link": "http://arxiv.org/abs/2303.00135v1", "categories": ["cs.LG", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "Language-Universal Adapter Learning with Knowledge Distillation for\n  End-to-End Multilingual Speech Recognition", "abstract": "In this paper, we propose a language-universal adapter learning framework\nbased on a pre-trained model for end-to-end multilingual automatic speech\nrecognition (ASR). For acoustic modeling, the wav2vec 2.0 pre-trained model is\nfine-tuned by inserting language-specific and language-universal adapters. An\nonline knowledge distillation is then used to enable the language-universal\nadapters to learn both language-specific and universal features. The linguistic\ninformation confusion is also reduced by leveraging language identifiers\n(LIDs). With LIDs we perform a position-wise modification on the multi-head\nattention outputs. In the inference procedure, the language-specific adapters\nare removed while the language-universal adapters are kept activated. The\nproposed method improves the recognition accuracy and addresses the linear\nincrease of the number of adapters' parameters with the number of languages in\ncommon multilingual ASR systems. Experiments on the BABEL dataset confirm the\neffectiveness of the proposed framework. Compared to the conventional\nmultilingual model, a 3.3% absolute error rate reduction is achieved. The code\nis available at: https://github.com/shen9712/UniversalAdapterLearning.", "published": "2023-02-28 14:43:49", "link": "http://arxiv.org/abs/2303.01249v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "UniFLG: Unified Facial Landmark Generator from Text or Speech", "abstract": "Talking face generation has been extensively investigated owing to its wide\napplicability. The two primary frameworks used for talking face generation\ncomprise a text-driven framework, which generates synchronized speech and\ntalking faces from text, and a speech-driven framework, which generates talking\nfaces from speech. To integrate these frameworks, this paper proposes a unified\nfacial landmark generator (UniFLG). The proposed system exploits end-to-end\ntext-to-speech not only for synthesizing speech but also for extracting a\nseries of latent representations that are common to text and speech, and feeds\nit to a landmark decoder to generate facial landmarks. We demonstrate that our\nsystem achieves higher naturalness in both speech synthesis and facial landmark\ngeneration compared to the state-of-the-art text-driven method. We further\ndemonstrate that our system can generate facial landmarks from speech of\nspeakers without facial video data or even speech data.", "published": "2023-02-28 06:05:43", "link": "http://arxiv.org/abs/2302.14337v2", "categories": ["cs.CV", "cs.CL", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.CV"}
{"title": "Adapter Incremental Continual Learning of Efficient Audio Spectrogram\n  Transformers", "abstract": "Continual learning involves training neural networks incrementally for new\ntasks while retaining the knowledge of previous tasks. However, efficiently\nfine-tuning the model for sequential tasks with minimal computational resources\nremains a challenge. In this paper, we propose Task Incremental Continual\nLearning (TI-CL) of audio classifiers with both parameter-efficient and\ncompute-efficient Audio Spectrogram Transformers (AST). To reduce the trainable\nparameters without performance degradation for TI-CL, we compare several\nParameter Efficient Transfer (PET) methods and propose AST with Convolutional\nAdapters for TI-CL, which has less than 5% of trainable parameters of the fully\nfine-tuned counterparts. To reduce the computational complexity, we introduce a\nnovel Frequency-Time factorized Attention (FTA) method that replaces the\ntraditional self-attention in transformers for audio spectrograms. FTA achieves\ncompetitive performance with only a factor of the computations required by\nGlobal Self-Attention (GSA). Finally, we formulate our method for TI-CL, called\nAdapter Incremental Continual Learning (AI-CL), as a combination of the\n\"parameter-efficient\" Convolutional Adapter and the \"compute-efficient\" FTA.\nExperiments on ESC-50, SpeechCommandsV2 (SCv2), and Audio-Visual Event (AVE)\nbenchmarks show that our proposed method prevents catastrophic forgetting in\nTI-CL while maintaining a lower computational budget.", "published": "2023-02-28 05:11:40", "link": "http://arxiv.org/abs/2302.14314v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Training sound event detection with soft labels from crowdsourced\n  annotations", "abstract": "In this paper, we study the use of soft labels to train a system for sound\nevent detection (SED). Soft labels can result from annotations which account\nfor human uncertainty about categories, or emerge as a natural representation\nof multiple opinions in annotation. Converting annotations to hard labels\nresults in unambiguous categories for training, at the cost of losing the\ndetails about the labels distribution. This work investigates how soft labels\ncan be used, and what benefits they bring in training a SED system. The results\nshow that the system is capable of learning information about the activity of\nthe sounds which is reflected in the soft labels and is able to detect sounds\nthat are missed in the typical binary target training setup. We also release a\nnew dataset produced through crowdsourcing, containing temporally strong labels\nfor sound events in real-life recordings, with both soft and hard labels.", "published": "2023-02-28 13:51:54", "link": "http://arxiv.org/abs/2302.14572v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "deHuBERT: Disentangling Noise in a Self-supervised Model for Robust\n  Speech Recognition", "abstract": "Existing self-supervised pre-trained speech models have offered an effective\nway to leverage massive unannotated corpora to build good automatic speech\nrecognition (ASR). However, many current models are trained on a clean corpus\nfrom a single source, which tends to do poorly when noise is present during\ntesting. Nonetheless, it is crucial to overcome the adverse influence of noise\nfor real-world applications. In this work, we propose a novel training\nframework, called deHuBERT, for noise reduction encoding inspired by H.\nBarlow's redundancy-reduction principle. The new framework improves the HuBERT\ntraining algorithm by introducing auxiliary losses that drive the self- and\ncross-correlation matrix between pairwise noise-distorted embeddings towards\nidentity matrix. This encourages the model to produce noise-agnostic speech\nrepresentations. With this method, we report improved robustness in noisy\nenvironments, including unseen noises, without impairing the performance on the\nclean set.", "published": "2023-02-28 14:33:41", "link": "http://arxiv.org/abs/2302.14597v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Incremental Learning of Acoustic Scenes and Sound Events", "abstract": "In this paper, we propose a method for incremental learning of two distinct\ntasks over time: acoustic scene classification (ASC) and audio tagging (AT). We\nuse a simple convolutional neural network (CNN) model as an incremental learner\nto solve the tasks. Generally, incremental learning methods catastrophically\nforget the previous task when sequentially trained on a new task. To alleviate\nthis problem, we propose independent learning and knowledge distillation (KD)\nbetween the timesteps in learning. Experiments are performed on TUT 2016/2017\ndataset, containing 4 acoustic scene classes and 25 sound event classes. The\nproposed incremental learner first solves the ASC task with an accuracy of\n94.0%. Next, it learns to solve the AT task with an F1 score of 54.4%. At the\nsame time, its performance on the previous ASC task decreases only by 5.1\npercentage points due to the additional learning of the AT task.", "published": "2023-02-28 18:07:41", "link": "http://arxiv.org/abs/2302.14815v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Token-Wise Beam Search Algorithm for RNN-T", "abstract": "Standard Recurrent Neural Network Transducers (RNN-T) decoding algorithms for\nspeech recognition are iterating over the time axis, such that one time step is\ndecoded before moving on to the next time step. Those algorithms result in a\nlarge number of calls to the joint network, which were shown in previous work\nto be an important factor that reduces decoding speed. We present a decoding\nbeam search algorithm that batches the joint network calls across a segment of\ntime steps, which results in 20%-96% decoding speedups consistently across all\nmodels and settings experimented with. In addition, aggregating emission\nprobabilities over a segment may be seen as a better approximation to finding\nthe most likely model output, causing our algorithm to improve oracle word\nerror rate by up to 11% relative as the segment size increases, and to slightly\nimprove general word error rate.", "published": "2023-02-28 07:20:49", "link": "http://arxiv.org/abs/2302.14357v2", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "CrossSpeech: Speaker-independent Acoustic Representation for\n  Cross-lingual Speech Synthesis", "abstract": "While recent text-to-speech (TTS) systems have made remarkable strides toward\nhuman-level quality, the performance of cross-lingual TTS lags behind that of\nintra-lingual TTS. This gap is mainly rooted from the speaker-language\nentanglement problem in cross-lingual TTS. In this paper, we propose\nCrossSpeech which improves the quality of cross-lingual speech by effectively\ndisentangling speaker and language information in the level of acoustic feature\nspace. Specifically, CrossSpeech decomposes the speech generation pipeline into\nthe speaker-independent generator (SIG) and speaker-dependent generator (SDG).\nThe SIG produces the speaker-independent acoustic representation which is not\nbiased to specific speaker distributions. On the other hand, the SDG models\nspeaker-dependent speech variation that characterizes speaker attributes. By\nhandling each information separately, CrossSpeech can obtain disentangled\nspeaker and language representations. From the experiments, we verify that\nCrossSpeech achieves significant improvements in cross-lingual TTS, especially\nin terms of speaker similarity to the target speaker.", "published": "2023-02-28 07:51:10", "link": "http://arxiv.org/abs/2302.14370v2", "categories": ["cs.SD", "cs.AI", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "Exploring Self-supervised Pre-trained ASR Models For Dysarthric and\n  Elderly Speech Recognition", "abstract": "Automatic recognition of disordered and elderly speech remains a highly\nchallenging task to date due to the difficulty in collecting such data in large\nquantities. This paper explores a series of approaches to integrate domain\nadapted SSL pre-trained models into TDNN and Conformer ASR systems for\ndysarthric and elderly speech recognition: a) input feature fusion between\nstandard acoustic frontends and domain adapted wav2vec2.0 speech\nrepresentations; b) frame-level joint decoding of TDNN systems separately\ntrained using standard acoustic features alone and with additional wav2vec2.0\nfeatures; and c) multi-pass decoding involving the TDNN/Conformer system\noutputs to be rescored using domain adapted wav2vec2.0 models. In addition,\ndomain adapted wav2vec2.0 representations are utilized in\nacoustic-to-articulatory (A2A) inversion to construct multi-modal dysarthric\nand elderly speech recognition systems. Experiments conducted on the UASpeech\ndysarthric and DementiaBank Pitt elderly speech corpora suggest TDNN and\nConformer ASR systems integrated domain adapted wav2vec2.0 models consistently\noutperform the standalone wav2vec2.0 models by statistically significant WER\nreductions of 8.22% and 3.43% absolute (26.71% and 15.88% relative) on the two\ntasks respectively. The lowest published WERs of 22.56% (52.53% on very low\nintelligibility, 39.09% on unseen words) and 18.17% are obtained on the\nUASpeech test set of 16 dysarthric speakers, and the DementiaBank Pitt test set\nrespectively.", "published": "2023-02-28 13:39:17", "link": "http://arxiv.org/abs/2302.14564v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Reducing the Prior Mismatch of Stochastic Differential Equations for\n  Diffusion-based Speech Enhancement", "abstract": "Recently, score-based generative models have been successfully employed for\nthe task of speech enhancement. A stochastic differential equation is used to\nmodel the iterative forward process, where at each step environmental noise and\nwhite Gaussian noise are added to the clean speech signal. While in limit the\nmean of the forward process ends at the noisy mixture, in practice it stops\nearlier and thus only at an approximation of the noisy mixture. This results in\na discrepancy between the terminating distribution of the forward process and\nthe prior used for solving the reverse process at inference. In this paper, we\naddress this discrepancy and propose a forward process based on a Brownian\nbridge. We show that such a process leads to a reduction of the mismatch\ncompared to previous diffusion processes. More importantly, we show that our\napproach improves in objective metrics over the baseline process with only half\nof the iteration steps and having one hyperparameter less to tune.", "published": "2023-02-28 16:45:42", "link": "http://arxiv.org/abs/2302.14748v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Audio Retrieval for Multimodal Design Documents: A New Dataset and\n  Algorithms", "abstract": "We consider and propose a new problem of retrieving audio files relevant to\nmultimodal design document inputs comprising both textual elements and visual\nimagery, e.g., birthday/greeting cards. In addition to enhancing user\nexperience, integrating audio that matches the theme/style of these inputs also\nhelps improve the accessibility of these documents (e.g., visually impaired\npeople can listen to the audio instead). While recent work in audio retrieval\nexists, these methods and datasets are targeted explicitly towards natural\nimages. However, our problem considers multimodal design documents (created by\nusers using creative software) substantially different from a naturally clicked\nphotograph. To this end, our first contribution is collecting and curating a\nnew large-scale dataset called Melodic-Design (or MELON), comprising design\ndocuments representing various styles, themes, templates, illustrations, etc.,\npaired with music audio. Given our paired image-text-audio dataset, our next\ncontribution is a novel multimodal cross-attention audio retrieval (MMCAR)\nalgorithm that enables training neural networks to learn a common shared\nfeature space across image, text, and audio dimensions. We use these learned\nfeatures to demonstrate that our method outperforms existing state-of-the-art\nmethods and produce a new reference benchmark for the research community on our\nnew dataset.", "published": "2023-02-28 16:59:13", "link": "http://arxiv.org/abs/2302.14757v1", "categories": ["cs.MM", "cs.IR", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "Practice of the conformer enhanced AUDIO-VISUAL HUBERT on Mandarin and\n  English", "abstract": "Considering the bimodal nature of human speech perception, lips, and teeth\nmovement has a pivotal role in automatic speech recognition. Benefiting from\nthe correlated and noise-invariant visual information, audio-visual recognition\nsystems enhance robustness in multiple scenarios. In previous work,\naudio-visual HuBERT appears to be the finest practice incorporating modality\nknowledge. This paper outlines a mixed methodology, named conformer enhanced\nAV-HuBERT, boosting the AV-HuBERT system's performance a step further. Compared\nwith baseline AV-HuBERT, our method in the one-phase evaluation of clean and\nnoisy conditions achieves 7% and 16% relative WER reduction on the English AVSR\nbenchmark dataset LRS3. Furthermore, we establish a novel 1000h Mandarin AVSR\ndataset CSTS. On top of the baseline AV-HuBERT, we exceed the WeNet ASR system\nby 14% and 18% relatively on MISP and CMLR by pre-training with this dataset.\nThe conformer-enhanced AV-HuBERT we proposed brings 7% on MISP and 6% CER\nreduction on CMLR, compared with the baseline AV-HuBERT system.", "published": "2023-02-28 02:10:13", "link": "http://arxiv.org/abs/2303.12187v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
