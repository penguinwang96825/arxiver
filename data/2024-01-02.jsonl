{"title": "Cheetah: Natural Language Generation for 517 African Languages", "abstract": "Low-resource African languages pose unique challenges for natural language\nprocessing (NLP) tasks, including natural language generation (NLG). In this\npaper, we develop Cheetah, a massively multilingual NLG language model for\nAfrican languages. Cheetah supports 517 African languages and language\nvarieties, allowing us to address the scarcity of NLG resources and provide a\nsolution to foster linguistic diversity. We demonstrate the effectiveness of\nCheetah through comprehensive evaluations across six generation downstream\ntasks. In five of the six tasks, Cheetah significantly outperforms other\nmodels, showcasing its remarkable performance for generating coherent and\ncontextually appropriate text in a wide range of African languages. We\nadditionally conduct a detailed human evaluation to delve deeper into the\nlinguistic capabilities of Cheetah. The introduction of Cheetah has\nfar-reaching benefits for linguistic diversity. By leveraging pretrained models\nand adapting them to specific languages, our approach facilitates the\ndevelopment of practical NLG applications for African communities. The findings\nof this study contribute to advancing NLP research in low-resource settings,\nenabling greater accessibility and inclusion for African languages in a rapidly\nexpanding digital landscape. We publicly release our models for research.", "published": "2024-01-02 06:24:13", "link": "http://arxiv.org/abs/2401.01053v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DialCLIP: Empowering CLIP as Multi-Modal Dialog Retriever", "abstract": "Recently, substantial advancements in pre-trained vision-language models have\ngreatly enhanced the capabilities of multi-modal dialog systems. These models\nhave demonstrated significant improvements by fine-tuning on downstream tasks.\nHowever, the existing pre-trained models primarily focus on effectively\ncapturing the alignment between vision and language modalities, often ignoring\nthe intricate nature of dialog context. In this paper, we propose a\nparameter-efficient prompt-tuning method named DialCLIP for multi-modal dialog\nretrieval. Specifically, our approach introduces a multi-modal context prompt\ngenerator to learn context features which are subsequently distilled into\nprompts within the pre-trained vision-language model CLIP. Besides, we\nintroduce domain prompt to mitigate the disc repancy from the downstream dialog\ndata. To facilitate various types of retrieval, we also design multiple experts\nto learn mappings from CLIP outputs to multi-modal representation space, with\neach expert being responsible to one specific retrieval type. Extensive\nexperiments show that DialCLIP achieves state-of-the-art performance on two\nwidely recognized benchmark datasets (i.e., PhotoChat and MMDialog) by tuning a\nmere 0.04% of the total parameters. These results highlight the efficacy and\nefficiency of our proposed approach, underscoring its potential to advance the\nfield of multi-modal dialog retrieval.", "published": "2024-01-02 07:40:12", "link": "http://arxiv.org/abs/2401.01076v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unveiling Comparative Sentiments in Vietnamese Product Reviews: A\n  Sequential Classification Framework", "abstract": "Comparative opinion mining is a specialized field of sentiment analysis that\naims to identify and extract sentiments expressed comparatively. To address\nthis task, we propose an approach that consists of solving three sequential\nsub-tasks: (i) identifying comparative sentence, i.e., if a sentence has a\ncomparative meaning, (ii) extracting comparative elements, i.e., what are\ncomparison subjects, objects, aspects, predicates, and (iii) classifying\ncomparison types which contribute to a deeper comprehension of user sentiments\nin Vietnamese product reviews. Our method is ranked fifth at the Vietnamese\nLanguage and Speech Processing (VLSP) 2023 challenge on Comparative Opinion\nMining (ComOM) from Vietnamese Product Reviews.", "published": "2024-01-02 08:58:01", "link": "http://arxiv.org/abs/2401.01108v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent\n  Evaluation", "abstract": "Recently, the advent of large language models (LLMs) has revolutionized\ngenerative agents. Among them, Role-Playing Conversational Agents (RPCAs)\nattract considerable attention due to their ability to emotionally engage\nusers. However, the absence of a comprehensive benchmark impedes progress in\nthis field. To bridge this gap, we introduce CharacterEval, a Chinese benchmark\nfor comprehensive RPCA assessment, complemented by a tailored high-quality\ndataset. The dataset comprises 1,785 multi-turn role-playing dialogues,\nencompassing 23,020 examples and featuring 77 characters derived from Chinese\nnovels and scripts. It was carefully constructed, beginning with initial\ndialogue extraction via GPT-4, followed by rigorous human-led quality control,\nand enhanced with in-depth character profiles sourced from Baidu Baike.\nCharacterEval employs a multifaceted evaluation approach, encompassing thirteen\ntargeted metrics on four dimensions. Comprehensive experiments on CharacterEval\ndemonstrate that Chinese LLMs exhibit more promising capabilities than GPT-4 in\nChinese role-playing conversation. Source code, data source and reward model\nwill be publicly accessible at https://github.com/morecry/CharacterEval.", "published": "2024-01-02 16:20:40", "link": "http://arxiv.org/abs/2401.01275v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Quality and Quantity of Machine Translation References for Automatic\n  Metrics", "abstract": "Automatic machine translation metrics typically rely on human translations to\ndetermine the quality of system translations. Common wisdom in the field\ndictates that the human references should be of very high quality. However,\nthere are no cost-benefit analyses that could be used to guide practitioners\nwho plan to collect references for machine translation evaluation. We find that\nhigher-quality references lead to better metric correlations with humans at the\nsegment-level. Having up to 7 references per segment and taking their average\n(or maximum) helps all metrics. Interestingly, the references from vendors of\ndifferent qualities can be mixed together and improve metric success. Higher\nquality references, however, cost more to create and we frame this as an\noptimization problem: given a specific budget, what references should be\ncollected to maximize metric success. These findings can be used by evaluators\nof shared tasks when references need to be created under a certain budget.", "published": "2024-01-02 16:51:17", "link": "http://arxiv.org/abs/2401.01283v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Comprehensive Survey of Hallucination Mitigation Techniques in Large\n  Language Models", "abstract": "As Large Language Models (LLMs) continue to advance in their ability to write\nhuman-like text, a key challenge remains around their tendency to hallucinate\ngenerating content that appears factual but is ungrounded. This issue of\nhallucination is arguably the biggest hindrance to safely deploying these\npowerful LLMs into real-world production systems that impact people's lives.\nThe journey toward widespread adoption of LLMs in practical settings heavily\nrelies on addressing and mitigating hallucinations. Unlike traditional AI\nsystems focused on limited tasks, LLMs have been exposed to vast amounts of\nonline text data during training. While this allows them to display impressive\nlanguage fluency, it also means they are capable of extrapolating information\nfrom the biases in training data, misinterpreting ambiguous prompts, or\nmodifying the information to align superficially with the input. This becomes\nhugely alarming when we rely on language generation capabilities for sensitive\napplications, such as summarizing medical records, financial analysis reports,\netc. This paper presents a comprehensive survey of over 32 techniques developed\nto mitigate hallucination in LLMs. Notable among these are Retrieval Augmented\nGeneration (Lewis et al, 2021), Knowledge Retrieval (Varshney et al,2023),\nCoNLI (Lei et al, 2023), and CoVe (Dhuliawala et al, 2023). Furthermore, we\nintroduce a detailed taxonomy categorizing these methods based on various\nparameters, such as dataset utilization, common tasks, feedback mechanisms, and\nretriever types. This classification helps distinguish the diverse approaches\nspecifically designed to tackle hallucination issues in LLMs. Additionally, we\nanalyze the challenges and limitations inherent in these techniques, providing\na solid foundation for future research in addressing hallucinations and related\nphenomena within the realm of LLMs.", "published": "2024-01-02 17:56:30", "link": "http://arxiv.org/abs/2401.01313v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine\n  Translation vs Human Translation", "abstract": "We conduct a large-scale fine-grained comparative analysis of machine\ntranslations (MT) against human translations (HT) through the lens of\nmorphosyntactic divergence. Across three language pairs and two types of\ndivergence defined as the structural difference between the source and the\ntarget, MT is consistently more conservative than HT, with less morphosyntactic\ndiversity, more convergent patterns, and more one-to-one alignments. Through\nanalysis on different decoding algorithms, we attribute this discrepancy to the\nuse of beam search that biases MT towards more convergent patterns. This bias\nis most amplified when the convergent pattern appears around 50% of the time in\ntraining data. Lastly, we show that for a majority of morphosyntactic\ndivergences, their presence in HT is correlated with decreased MT performance,\npresenting a greater challenge for MT systems.", "published": "2024-01-02 20:05:56", "link": "http://arxiv.org/abs/2401.01419v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLaMA Beyond English: An Empirical Study on Language Capability Transfer", "abstract": "In recent times, substantial advancements have been witnessed in large\nlanguage models (LLMs), exemplified by ChatGPT, showcasing remarkable\nproficiency across a range of complex tasks. However, many mainstream LLMs\n(e.g. LLaMA) are pretrained on English-dominant corpus, which limits their\nperformance in other non-English languages. In this paper, we focus on how to\neffectively transfer the capabilities of language generation and following\ninstructions to a non-English language. To answer this question, we conduct an\nextensive empirical investigation based on LLaMA, accumulating over 1440 GPU\nhours. We analyze the impact of key factors such as vocabulary extension,\nfurther pretraining, and instruction tuning on transfer. To accurately assess\nthe model's level of knowledge, we employ four widely used standardized testing\nbenchmarks: C-Eval, MMLU, AGI-Eval, and GAOKAO-Bench. Furthermore, a\ncomprehensive evaluation of the model's response quality is conducted,\nconsidering aspects such as accuracy, fluency, informativeness, logical\ncoherence, and harmlessness, based on LLM-Eval, a benchmarks consisting\ninstruction tasks from 17 diverse categories. Our evaluation results\ndemonstrate that comparable performance to state-of-the-art transfer models can\nbe achieved with less than 1% of the pretraining data, both in terms of\nknowledge alignment and response quality. Furthermore, the experimental\noutcomes across the thirteen low-resource languages also exhibit similar\ntrends. We anticipate that the conclusions revealed by the experiments will aid\nthe community in developing non-English LLMs.", "published": "2024-01-02 06:29:02", "link": "http://arxiv.org/abs/2401.01055v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Discovering Significant Topics from Legal Decisions with Selective\n  Inference", "abstract": "We propose and evaluate an automated pipeline for discovering significant\ntopics from legal decision texts by passing features synthesized with topic\nmodels through penalised regressions and post-selection significance tests. The\nmethod identifies case topics significantly correlated with outcomes,\ntopic-word distributions which can be manually-interpreted to gain insights\nabout significant topics, and case-topic weights which can be used to identify\nrepresentative cases for each topic. We demonstrate the method on a new dataset\nof domain name disputes and a canonical dataset of European Court of Human\nRights violation cases. Topic models based on latent semantic analysis as well\nas language model embeddings are evaluated. We show that topics derived by the\npipeline are consistent with legal doctrines in both areas and can be useful in\nother related legal analysis tasks.", "published": "2024-01-02 07:00:24", "link": "http://arxiv.org/abs/2401.01068v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Vietnamese Poem Generation & The Prospect Of Cross-Language Poem-To-Poem\n  Translation", "abstract": "Poetry generation has been a challenging task in the field of Natural\nLanguage Processing, as it requires the model to understand the nuances of\nlanguage, sentiment, and style. In this paper, we propose using Large Language\nModels to generate Vietnamese poems of various genres from natural language\nprompts, thereby facilitating an intuitive process with enhanced content\ncontrol. Our most efficacious model, the GPT-3 Babbage variant, achieves a\ncustom evaluation score of 0.8, specifically tailored to the \"luc bat\" genre of\nVietnamese poetry. Furthermore, we also explore the idea of paraphrasing poems\ninto normal text prompts and yield a relatively high score of 0.781 in the \"luc\nbat\" genre. This experiment presents the potential for cross-Language\npoem-to-poem translation with translated poems as the inputs while concurrently\nmaintaining complete control over the generated content.", "published": "2024-01-02 07:46:34", "link": "http://arxiv.org/abs/2401.01078v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unifying Structured Data as Graph for Data-to-Text Pre-Training", "abstract": "Data-to-text (D2T) generation aims to transform structured data into natural\nlanguage text. Data-to-text pre-training has proved to be powerful in enhancing\nD2T generation and yields impressive performances. However, previous\npre-training methods either oversimplified structured data into a sequence\nwithout considering input structures or designed training objectives tailored\nfor a specific data structure (e.g., table or knowledge graph). In this paper,\nwe unify different types of structured data (i.e., table, key-value data,\nknowledge graph) into the graph format and cast different data-to-text\ngeneration tasks as graph-to-text generation. To effectively exploit the\nstructural information of the input graph, we propose a structure-enhanced\npre-training method for D2T generation by designing a structure-enhanced\nTransformer. Concretely, we devise a position matrix for the Transformer,\nencoding relative positional information of connected nodes in the input graph.\nIn addition, we propose a new attention matrix to incorporate graph structures\ninto the original Transformer by taking the available explicit connectivity\nstructure into account. Extensive experiments on six benchmark datasets show\nthe effectiveness of our model. Our source codes are available at\nhttps://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/unid2t.", "published": "2024-01-02 12:23:49", "link": "http://arxiv.org/abs/2401.01183v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Uncertainty Resolution in Misinformation Detection", "abstract": "Misinformation poses a variety of risks, such as undermining public trust and\ndistorting factual discourse. Large Language Models (LLMs) like GPT-4 have been\nshown effective in mitigating misinformation, particularly in handling\nstatements where enough context is provided. However, they struggle to assess\nambiguous or context-deficient statements accurately. This work introduces a\nnew method to resolve uncertainty in such statements. We propose a framework to\ncategorize missing information and publish category labels for the LIAR-New\ndataset, which is adaptable to cross-domain content with missing information.\nWe then leverage this framework to generate effective user queries for missing\ncontext. Compared to baselines, our method improves the rate at which generated\nquestions are answerable by the user by 38 percentage points and classification\nperformance by over 10 percentage points macro F1. Thus, this approach may\nprovide a valuable component for future misinformation mitigation pipelines.", "published": "2024-01-02 13:01:50", "link": "http://arxiv.org/abs/2401.01197v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "VideoStudio: Generating Consistent-Content and Multi-Scene Videos", "abstract": "The recent innovations and breakthroughs in diffusion models have\nsignificantly expanded the possibilities of generating high-quality videos for\nthe given prompts. Most existing works tackle the single-scene scenario with\nonly one video event occurring in a single background. Extending to generate\nmulti-scene videos nevertheless is not trivial and necessitates to nicely\nmanage the logic in between while preserving the consistent visual appearance\nof key content across video scenes. In this paper, we propose a novel\nframework, namely VideoStudio, for consistent-content and multi-scene video\ngeneration. Technically, VideoStudio leverages Large Language Models (LLM) to\nconvert the input prompt into comprehensive multi-scene script that benefits\nfrom the logical knowledge learnt by LLM. The script for each scene includes a\nprompt describing the event, the foreground/background entities, as well as\ncamera movement. VideoStudio identifies the common entities throughout the\nscript and asks LLM to detail each entity. The resultant entity description is\nthen fed into a text-to-image model to generate a reference image for each\nentity. Finally, VideoStudio outputs a multi-scene video by generating each\nscene video via a diffusion process that takes the reference images, the\ndescriptive prompt of the event and camera movement into account. The diffusion\nmodel incorporates the reference images as the condition and alignment to\nstrengthen the content consistency of multi-scene videos. Extensive experiments\ndemonstrate that VideoStudio outperforms the SOTA video generation models in\nterms of visual quality, content consistency, and user preference. Source code\nis available at \\url{https://github.com/FuchenUSTC/VideoStudio}.", "published": "2024-01-02 15:56:48", "link": "http://arxiv.org/abs/2401.01256v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Evaluating Large Language Models on the GMAT: Implications for the\n  Future of Business Education", "abstract": "The rapid evolution of artificial intelligence (AI), especially in the domain\nof Large Language Models (LLMs) and generative AI, has opened new avenues for\napplication across various fields, yet its role in business education remains\nunderexplored. This study introduces the first benchmark to assess the\nperformance of seven major LLMs, OpenAI's models (GPT-3.5 Turbo, GPT-4, and\nGPT-4 Turbo), Google's models (PaLM 2, Gemini 1.0 Pro), and Anthropic's models\n(Claude 2 and Claude 2.1), on the GMAT, which is a key exam in the admission\nprocess for graduate business programs. Our analysis shows that most LLMs\noutperform human candidates, with GPT-4 Turbo not only outperforming the other\nmodels but also surpassing the average scores of graduate students at top\nbusiness schools. Through a case study, this research examines GPT-4 Turbo's\nability to explain answers, evaluate responses, identify errors, tailor\ninstructions, and generate alternative scenarios. The latest LLM versions,\nGPT-4 Turbo, Claude 2.1, and Gemini 1.0 Pro, show marked improvements in\nreasoning tasks compared to their predecessors, underscoring their potential\nfor complex problem-solving. While AI's promise in education, assessment, and\ntutoring is clear, challenges remain. Our study not only sheds light on LLMs'\nacademic potential but also emphasizes the need for careful development and\napplication of AI in education. As AI technology advances, it is imperative to\nestablish frameworks and protocols for AI interaction, verify the accuracy of\nAI-generated content, ensure worldwide access for diverse learners, and create\nan educational environment where AI supports human expertise. This research\nsets the stage for further exploration into the responsible use of AI to enrich\neducational experiences and improve exam preparation and assessment methods.", "published": "2024-01-02 03:54:50", "link": "http://arxiv.org/abs/2401.02985v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Identification of Regulatory Requirements Relevant to Business\n  Processes: A Comparative Study on Generative AI, Embedding-based Ranking,\n  Crowd and Expert-driven Methods", "abstract": "Organizations face the challenge of ensuring compliance with an increasing\namount of requirements from various regulatory documents. Which requirements\nare relevant depends on aspects such as the geographic location of the\norganization, its domain, size, and business processes. Considering these\ncontextual factors, as a first step, relevant documents (e.g., laws,\nregulations, directives, policies) are identified, followed by a more detailed\nanalysis of which parts of the identified documents are relevant for which step\nof a given business process. Nowadays the identification of regulatory\nrequirements relevant to business processes is mostly done manually by domain\nand legal experts, posing a tremendous effort on them, especially for a large\nnumber of regulatory documents which might frequently change. Hence, this work\nexamines how legal and domain experts can be assisted in the assessment of\nrelevant requirements. For this, we compare an embedding-based NLP ranking\nmethod, a generative AI method using GPT-4, and a crowdsourced method with the\npurely manual method of creating relevancy labels by experts. The proposed\nmethods are evaluated based on two case studies: an Australian insurance case\ncreated with domain experts and a global banking use case, adapted from SAP\nSignavio's workflow example of an international guideline. A gold standard is\ncreated for both BPMN2.0 processes and matched to real-world textual\nrequirements from multiple regulatory documents. The evaluation and discussion\nprovide insights into strengths and weaknesses of each method regarding\napplicability, automation, transparency, and reproducibility and provide\nguidelines on which method combinations will maximize benefits for given\ncharacteristics such as process usage, impact, and dynamics of an application\nscenario.", "published": "2024-01-02 12:08:31", "link": "http://arxiv.org/abs/2401.02986v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Has Your Pretrained Model Improved? A Multi-head Posterior Based\n  Approach", "abstract": "The emergence of pre-trained models has significantly impacted Natural\nLanguage Processing (NLP) and Computer Vision to relational datasets.\nTraditionally, these models are assessed through fine-tuned downstream tasks.\nHowever, this raises the question of how to evaluate these models more\nefficiently and more effectively. In this study, we explore a novel approach\nwhere we leverage the meta-features associated with each entity as a source of\nworldly knowledge and employ entity representations from the models. We propose\nusing the consistency between these representations and the meta-features as a\nmetric for evaluating pre-trained models. Our method's effectiveness is\ndemonstrated across various domains, including models with relational datasets,\nlarge language models and image models.", "published": "2024-01-02 17:08:26", "link": "http://arxiv.org/abs/2401.02987v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Detection of Machine-Generated Text: Literature Survey", "abstract": "Since language models produce fake text quickly and easily, there is an\noversupply of such content in the public domain. The degree of sophistication\nand writing style has reached a point where differentiating between human\nauthored and machine-generated content is nearly impossible. As a result, works\ngenerated by language models rather than human authors have gained significant\nmedia attention and stirred controversy.Concerns regarding the possible\ninfluence of advanced language models on society have also arisen, needing a\nfuller knowledge of these processes. Natural language generation (NLG) and\ngenerative pre-trained transformer (GPT) models have revolutionized a variety\nof sectors: the scope not only permeated throughout journalism and customer\nservice but also reached academia. To mitigate the hazardous implications that\nmay arise from the use of these models, preventative measures must be\nimplemented, such as providing human agents with the capacity to distinguish\nbetween artificially made and human composed texts utilizing automated systems\nand possibly reverse-engineered language models. Furthermore, to ensure a\nbalanced and responsible approach, it is critical to have a full grasp of the\nsocio-technological ramifications of these breakthroughs. This literature\nsurvey aims to compile and synthesize accomplishments and developments in the\naforementioned work, while also identifying future prospects. It also gives an\noverview of machine-generated text trends and explores the larger societal\nimplications. Ultimately, this survey intends to contribute to the development\nof robust and effective approaches for resolving the issues connected with the\nusage and detection of machine-generated text by exploring the interplay\nbetween the capabilities of language models and their possible implications.", "published": "2024-01-02 01:44:15", "link": "http://arxiv.org/abs/2402.01642v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Auffusion: Leveraging the Power of Diffusion and Large Language Models\n  for Text-to-Audio Generation", "abstract": "Recent advancements in diffusion models and large language models (LLMs) have\nsignificantly propelled the field of AIGC. Text-to-Audio (TTA), a burgeoning\nAIGC application designed to generate audio from natural language prompts, is\nattracting increasing attention. However, existing TTA studies often struggle\nwith generation quality and text-audio alignment, especially for complex\ntextual inputs. Drawing inspiration from state-of-the-art Text-to-Image (T2I)\ndiffusion models, we introduce Auffusion, a TTA system adapting T2I model\nframeworks to TTA task, by effectively leveraging their inherent generative\nstrengths and precise cross-modal alignment. Our objective and subjective\nevaluations demonstrate that Auffusion surpasses previous TTA approaches using\nlimited data and computational resource. Furthermore, previous studies in T2I\nrecognizes the significant impact of encoder choice on cross-modal alignment,\nlike fine-grained details and object bindings, while similar evaluation is\nlacking in prior TTA works. Through comprehensive ablation studies and\ninnovative cross-attention map visualizations, we provide insightful\nassessments of text-audio alignment in TTA. Our findings reveal Auffusion's\nsuperior capability in generating audios that accurately match textual\ndescriptions, which further demonstrated in several related tasks, such as\naudio style transfer, inpainting and other manipulations. Our implementation\nand demos are available at https://auffusion.github.io.", "published": "2024-01-02 05:42:14", "link": "http://arxiv.org/abs/2401.01044v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Quokka: An Open-source Large Language Model ChatBot for Material Science", "abstract": "This paper presents the development of a specialized chatbot for materials\nscience, leveraging the Llama-2 language model, and continuing pre-training on\nthe expansive research articles in the materials science domain from the S2ORC\ndataset. The methodology involves an initial pretraining phase on over one\nmillion domain-specific papers, followed by an instruction-tuning process to\nrefine the chatbot's capabilities. The chatbot is designed to assist\nresearchers, educators, and students by providing instant, context-aware\nresponses to queries in the field of materials science. We make the four\ntrained checkpoints (7B, 13B, with or without chat ability) freely available to\nthe research community at https://github.com/Xianjun-Yang/Quokka.", "published": "2024-01-02 08:14:48", "link": "http://arxiv.org/abs/2401.01089v1", "categories": ["cs.CL", "cs.AI", "cs.CE"], "primary_category": "cs.CL"}
{"title": "Self-Supervised Position Debiasing for Large Language Models", "abstract": "Fine-tuning has been demonstrated to be an effective method to improve the\ndomain performance of large language models (LLMs). However, LLMs might fit the\ndataset bias and shortcuts for prediction, leading to poor generation\nperformance. Previous works have proven that LLMs are prone to exhibit position\nbias, i.e., leveraging information positioned at the beginning or end, or\nspecific positional cues within the input. Existing debiasing methods for LLMs\nrequire external bias knowledge or annotated non-biased samples, which is\nlacking for position debiasing and impractical in reality. In this work, we\npropose a self-supervised position debiasing (SOD) framework to mitigate\nposition bias for LLMs. SOD leverages unsupervised responses from pre-trained\nLLMs for debiasing without relying on any external knowledge. To improve the\nquality of unsupervised responses, we propose an objective alignment (OAM)\nmodule to prune these responses. Experiments on eight datasets and five tasks\nshow that SOD consistently outperforms existing methods in mitigating three\ntypes of position biases. Besides, SOD achieves this by sacrificing only a\nsmall performance on biased samples, which is general and effective. To\nfacilitate the reproducibility of the results, we share the code of all methods\nand datasets on https://github.com/LZKSKY/SOD.", "published": "2024-01-02 14:12:41", "link": "http://arxiv.org/abs/2401.01218v3", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Fairness Certification for Natural Language Processing and Large\n  Language Models", "abstract": "Natural Language Processing (NLP) plays an important role in our daily lives,\nparticularly due to the enormous progress of Large Language Models (LLM).\nHowever, NLP has many fairness-critical use cases, e.g., as an expert system in\nrecruitment or as an LLM-based tutor in education. Since NLP is based on human\nlanguage, potentially harmful biases can diffuse into NLP systems and produce\nunfair results, discriminate against minorities or generate legal issues.\nHence, it is important to develop a fairness certification for NLP approaches.\nWe follow a qualitative research approach towards a fairness certification for\nNLP. In particular, we have reviewed a large body of literature on algorithmic\nfairness, and we have conducted semi-structured expert interviews with a wide\nrange of experts from that area. We have systematically devised six fairness\ncriteria for NLP, which can be further refined into 18 sub-categories. Our\ncriteria offer a foundation for operationalizing and testing processes to\ncertify fairness, both from the perspective of the auditor and the audited\norganization.", "published": "2024-01-02 16:09:36", "link": "http://arxiv.org/abs/2401.01262v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Large Legal Fictions: Profiling Legal Hallucinations in Large Language\n  Models", "abstract": "Do large language models (LLMs) know the law? These models are increasingly\nbeing used to augment legal practice, education, and research, yet their\nrevolutionary potential is threatened by the presence of hallucinations --\ntextual output that is not consistent with legal facts. We present the first\nsystematic evidence of these hallucinations, documenting LLMs' varying\nperformance across jurisdictions, courts, time periods, and cases. Our work\nmakes four key contributions. First, we develop a typology of legal\nhallucinations, providing a conceptual framework for future research in this\narea. Second, we find that legal hallucinations are alarmingly prevalent,\noccurring between 58% of the time with ChatGPT 4 and 88% with Llama 2, when\nthese models are asked specific, verifiable questions about random federal\ncourt cases. Third, we illustrate that LLMs often fail to correct a user's\nincorrect legal assumptions in a contra-factual question setup. Fourth, we\nprovide evidence that LLMs cannot always predict, or do not always know, when\nthey are producing legal hallucinations. Taken together, our findings caution\nagainst the rapid and unsupervised integration of popular LLMs into legal\ntasks. Even experienced lawyers must remain wary of legal hallucinations, and\nthe risks are highest for those who stand to benefit from LLMs the most -- pro\nse litigants or those without access to traditional legal resources.", "published": "2024-01-02 17:28:06", "link": "http://arxiv.org/abs/2401.01301v2", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning", "abstract": "It is well known that LLMs cannot generalize well to long contexts whose\nlengths are larger than the training sequence length. This poses challenges\nwhen employing LLMs for processing long input sequences during inference. In\nthis work, we argue that LLMs themselves have inherent capabilities to handle\nlong contexts without fine-tuning. To achieve this goal, we propose SelfExtend\nto extend the context window of LLMs by constructing bi-level attention\ninformation: the grouped attention and the neighbor attention. The grouped\nattention captures the dependencies among tokens that are far apart, while\nneighbor attention captures dependencies among adjacent tokens within a\nspecified range. The two-level attentions are computed based on the original\nmodel's self-attention mechanism during inference. With minor code\nmodification, our SelfExtend can effortlessly extend existing LLMs' context\nwindow without any fine-tuning. We conduct comprehensive experiments on\nmultiple benchmarks and the results show that our SelfExtend can effectively\nextend existing LLMs' context window length. The code can be found at\n\\url{https://github.com/datamllab/LongLM}.", "published": "2024-01-02 18:30:51", "link": "http://arxiv.org/abs/2401.01325v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Autoregressive Text-to-Graph Framework for Joint Entity and Relation\n  Extraction", "abstract": "In this paper, we propose a novel method for joint entity and relation\nextraction from unstructured text by framing it as a conditional sequence\ngeneration problem. In contrast to conventional generative information\nextraction models that are left-to-right token-level generators, our approach\nis \\textit{span-based}. It generates a linearized graph where nodes represent\ntext spans and edges represent relation triplets. Our method employs a\ntransformer encoder-decoder architecture with pointing mechanism on a dynamic\nvocabulary of spans and relation types. Our model can capture the structural\ncharacteristics and boundaries of entities and relations through span\nrepresentations while simultaneously grounding the generated output in the\noriginal text thanks to the pointing mechanism. Evaluation on benchmark\ndatasets validates the effectiveness of our approach, demonstrating competitive\nresults. Code is available at https://github.com/urchade/ATG.", "published": "2024-01-02 18:32:14", "link": "http://arxiv.org/abs/2401.01326v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TREC iKAT 2023: The Interactive Knowledge Assistance Track Overview", "abstract": "Conversational Information Seeking has evolved rapidly in the last few years\nwith the development of Large Language Models providing the basis for\ninterpreting and responding in a naturalistic manner to user requests. iKAT\nemphasizes the creation and research of conversational search agents that adapt\nresponses based on the user's prior interactions and present context. This\nmeans that the same question might yield varied answers, contingent on the\nuser's profile and preferences. The challenge lies in enabling Conversational\nSearch Agents (CSA) to incorporate personalized context to effectively guide\nusers through the relevant information to them. iKAT's first year attracted\nseven teams and a total of 24 runs. Most of the runs leveraged Large Language\nModels (LLMs) in their pipelines, with a few focusing on a\ngenerate-then-retrieve approach.", "published": "2024-01-02 18:40:03", "link": "http://arxiv.org/abs/2401.01330v2", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language\n  Models", "abstract": "Harnessing the power of human-annotated data through Supervised Fine-Tuning\n(SFT) is pivotal for advancing Large Language Models (LLMs). In this paper, we\ndelve into the prospect of growing a strong LLM out of a weak one without the\nneed for acquiring additional human-annotated data. We propose a new\nfine-tuning method called Self-Play fIne-tuNing (SPIN), which starts from a\nsupervised fine-tuned model. At the heart of SPIN lies a self-play mechanism,\nwhere the LLM refines its capability by playing against instances of itself.\nMore specifically, the LLM generates its own training data from its previous\niterations, refining its policy by discerning these self-generated responses\nfrom those obtained from human-annotated data. Our method progressively\nelevates the LLM from a nascent model to a formidable one, unlocking the full\npotential of human-annotated demonstration data for SFT. Theoretically, we\nprove that the global optimum to the training objective function of our method\nis achieved only when the LLM policy aligns with the target data distribution.\nEmpirically, we evaluate our method on several benchmark datasets including the\nHuggingFace Open LLM Leaderboard, MT-Bench, and datasets from Big-Bench. Our\nresults show that SPIN can significantly improve the LLM's performance across a\nvariety of benchmarks and even outperform models trained through direct\npreference optimization (DPO) supplemented with extra GPT-4 preference data.\nThis sheds light on the promise of self-play, enabling the achievement of\nhuman-level performance in LLMs without the need for expert opponents. Codes\nare available at https://github.com/uclaml/SPIN.", "published": "2024-01-02 18:53:13", "link": "http://arxiv.org/abs/2401.01335v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Quantifying the Uniqueness of Donald Trump in Presidential Discourse", "abstract": "Does Donald Trump speak differently from other presidents? If so, in what\nways? Are these differences confined to any single medium of communication? To\ninvestigate these questions, this paper introduces a novel metric of uniqueness\nbased on large language models, develops a new lexicon for divisive speech, and\npresents a framework for comparing the lexical features of political opponents.\nApplying these tools to a variety of corpora of presidential speeches, we find\nconsiderable evidence that Trump's speech patterns diverge from those of all\nmajor party nominees for the presidency in recent history. Some notable\nfindings include Trump's employment of particularly divisive and antagonistic\nlanguage targeting of his political opponents and his patterns of repetition\nfor emphasis. Furthermore, Trump is significantly more distinctive than his\nfellow Republicans, whose uniqueness values are comparably closer to those of\nthe Democrats. These differences hold across a variety of measurement\nstrategies, arise on both the campaign trail and in official presidential\naddresses, and do not appear to be an artifact of secular time trends.", "published": "2024-01-02 19:00:17", "link": "http://arxiv.org/abs/2401.01405v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.SI"], "primary_category": "cs.CL"}
{"title": "A Comprehensive Study of Knowledge Editing for Large Language Models", "abstract": "Large Language Models (LLMs) have shown extraordinary capabilities in\nunderstanding and generating text that closely mirrors human communication.\nHowever, a primary limitation lies in the significant computational demands\nduring training, arising from their extensive parameterization. This challenge\nis further intensified by the dynamic nature of the world, necessitating\nfrequent updates to LLMs to correct outdated information or integrate new\nknowledge, thereby ensuring their continued relevance. Note that many\napplications demand continual model adjustments post-training to address\ndeficiencies or undesirable behaviors. There is an increasing interest in\nefficient, lightweight methods for on-the-fly model modifications. To this end,\nrecent years have seen a burgeoning in the techniques of knowledge editing for\nLLMs, which aim to efficiently modify LLMs' behaviors within specific domains\nwhile preserving overall performance across various inputs. In this paper, we\nfirst define the knowledge editing problem and then provide a comprehensive\nreview of cutting-edge approaches. Drawing inspiration from educational and\ncognitive research theories, we propose a unified categorization criterion that\nclassifies knowledge editing methods into three groups: resorting to external\nknowledge, merging knowledge into the model, and editing intrinsic knowledge.\nFurthermore, we introduce a new benchmark, KnowEdit, for a comprehensive\nempirical evaluation of representative knowledge editing approaches.\nAdditionally, we provide an in-depth analysis of knowledge location, which can\ngive a deeper understanding of the knowledge structures inherent within LLMs.\nFinally, we discuss several potential applications of knowledge editing,\noutlining its broad and impactful implications.", "published": "2024-01-02 16:54:58", "link": "http://arxiv.org/abs/2401.01286v5", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Room impulse response reconstruction with physics-informed deep learning", "abstract": "A method is presented for estimating and reconstructing the sound field\nwithin a room using physics-informed neural networks. By incorporating a\nlimited set of experimental room impulse responses as training data, this\napproach combines neural network processing capabilities with the underlying\nphysics of sound propagation, as articulated by the wave equation. The\nnetwork's ability to estimate particle velocity and intensity, in addition to\nsound pressure, demonstrates its capacity to represent the flow of acoustic\nenergy and completely characterise the sound field with only a few\nmeasurements. Additionally, an investigation into the potential of this network\nas a tool for improving acoustic simulations is conducted. This is due to its\nprofficiency in offering grid-free sound field mappings with minimal inference\ntime. Furthermore, a study is carried out which encompasses comparative\nanalyses against current approaches for sound field reconstruction.\nSpecifically, the proposed approach is evaluated against both data-driven\ntechniques and elementary wave-based regression methods. The results\ndemonstrate that the physics-informed neural network stands out when\nreconstructing the early part of the room impulse response, while\nsimultaneously allowing for complete sound field characterisation in the time\ndomain.", "published": "2024-01-02 13:26:09", "link": "http://arxiv.org/abs/2401.01206v1", "categories": ["eess.AS", "J.2; G.1.8; I.6.4"], "primary_category": "eess.AS"}
{"title": "On the Parameter Estimation of Sinusoidal Models for Speech and Audio\n  Signals", "abstract": "In this paper, we examine the parameter estimation performance of three\nwell-known sinusoidal models for speech and audio. The first one is the\nstandard Sinusoidal Model (SM), which is based on the Fast Fourier Transform\n(FFT). The second is the Exponentially Damped Sinusoidal Model (EDSM) which has\nbeen proposed in the last decade, and utilizes a subspace method for parameter\nestimation, and finally the extended adaptive Quasi-Harmonic Model (eaQHM),\nwhich has been recently proposed for AM-FM decomposition, and estimates the\nsignal parameters using Least Squares on a set of basis function that are\nadaptive to the local characteristics of the signal. The parameter estimation\nof each model is briefly described and its performance is compared to the\nothers in terms of signal reconstruction accuracy versus window size on a\nvariety of synthetic signals and versus the number of sinusoids on real\nsignals. The latter include highly non stationary signals, such as singing\nvoices and guitar solos. The advantages and disadvantages of each model are\npresented via synthetic signals and then the application on real signals is\ndiscussed. Conclusively, eaQHM outperforms EDS in medium-to-large window size\nanalysis, whereas EDSM yields higher reconstruction values for smaller analysis\nwindow sizes. Thus, a future research direction appears to be the merge of\nadaptivity of the eaQHM and parameter estimation robustness of the EDSM in a\nnew paradigm for high-quality analysis and resynthesis of general audio\nsignals.", "published": "2024-01-02 15:46:19", "link": "http://arxiv.org/abs/2401.01255v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Efficient Parallel Audio Generation using Group Masked Language Modeling", "abstract": "We present a fast and high-quality codec language model for parallel audio\ngeneration. While SoundStorm, a state-of-the-art parallel audio generation\nmodel, accelerates inference speed compared to autoregressive models, it still\nsuffers from slow inference due to iterative sampling. To resolve this problem,\nwe propose Group-Masked Language Modeling~(G-MLM) and Group Iterative Parallel\nDecoding~(G-IPD) for efficient parallel audio generation. Both the training and\nsampling schemes enable the model to synthesize high-quality audio with a small\nnumber of iterations by effectively modeling the group-wise conditional\ndependencies. In addition, our model employs a cross-attention-based\narchitecture to capture the speaker style of the prompt voice and improves\ncomputational efficiency. Experimental results demonstrate that our proposed\nmodel outperforms the baselines in prompt-based audio generation.", "published": "2024-01-02 08:42:48", "link": "http://arxiv.org/abs/2401.01099v1", "categories": ["eess.AS", "cs.AI", "cs.LG"], "primary_category": "eess.AS"}
{"title": "HAAQI-Net: A Non-intrusive Neural Music Audio Quality Assessment Model\n  for Hearing Aids", "abstract": "This paper introduces HAAQI-Net, a non-intrusive deep learning-based music\naudio quality assessment model for hearing aid users. Unlike traditional\nmethods like the Hearing Aid Audio Quality Index (HAAQI) that require intrusive\nreference signal comparisons, HAAQI-Net offers a more accessible and\ncomputationally efficient alternative. By utilizing a Bidirectional Long\nShort-Term Memory (BLSTM) architecture with attention mechanisms and features\nextracted from the pre-trained BEATs model, it can predict HAAQI scores\ndirectly from music audio clips and hearing loss patterns. Experimental results\ndemonstrate HAAQI-Net's effectiveness, achieving a Linear Correlation\nCoefficient (LCC) of 0.9368 , a Spearman's Rank Correlation Coefficient (SRCC)\nof 0.9486 , and a Mean Squared Error (MSE) of 0.0064 and inference time\nsignificantly reduces from 62.52 to 2.54 seconds. To address computational\noverhead, a knowledge distillation strategy was applied, reducing parameters by\n75.85% and inference time by 96.46%, while maintaining strong performance (LCC:\n0.9071 , SRCC: 0.9307 , MSE: 0.0091 ). To expand its capabilities, HAAQI-Net\nwas adapted to predict subjective human scores like the Mean Opinion Score\n(MOS) through fine-tuning. This adaptation significantly improved prediction\naccuracy, validated through statistical analysis. Furthermore, the robustness\nof HAAQI-Net was evaluated under varying Sound Pressure Level (SPL) conditions,\nrevealing optimal performance at a reference SPL of 65 dB, with accuracy\ngradually decreasing as SPL deviated from this point. The advancements in\nsubjective score prediction, SPL robustness, and computational efficiency\nposition HAAQI-Net as a scalable solution for music audio quality assessment in\nhearing aid applications, contributing to efficient and accurate models in\naudio signal processing and hearing aid technology.", "published": "2024-01-02 10:55:01", "link": "http://arxiv.org/abs/2401.01145v5", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
