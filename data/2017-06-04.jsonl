{"title": "CRNN: A Joint Neural Network for Redundancy Detection", "abstract": "This paper proposes a novel framework for detecting redundancy in supervised\nsentence categorisation. Unlike traditional singleton neural network, our model\nincorporates character-aware convolutional neural network (Char-CNN) with\ncharacter-aware recurrent neural network (Char-RNN) to form a convolutional\nrecurrent neural network (CRNN). Our model benefits from Char-CNN in that only\nsalient features are selected and fed into the integrated Char-RNN. Char-RNN\neffectively learns long sequence semantics via sophisticated update mechanism.\nWe compare our framework against the state-of-the-art text classification\nalgorithms on four popular benchmarking corpus. For instance, our model\nachieves competing precision rate, recall ratio, and F1 score on the\nGoogle-news data-set. For twenty-news-groups data stream, our algorithm obtains\nthe optimum on precision rate, recall ratio, and F1 score. For Brown Corpus,\nour framework obtains the best F1 score and almost equivalent precision rate\nand recall ratio over the top competitor. For the question classification\ncollection, CRNN produces the optimal recall rate and F1 score and comparable\nprecision rate. We also analyse three different RNN hidden recurrent cells'\nimpact on performance and their runtime efficiency. We observe that MGU\nachieves the optimal runtime and comparable performance against GRU and LSTM.\nFor TFIDF based algorithms, we experiment with word2vec, GloVe, and sent2vec\nembeddings and report their performance differences.", "published": "2017-06-04 13:12:45", "link": "http://arxiv.org/abs/1706.01069v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Legal Information Retrieval by Distributional Composition with\n  Term Order Probabilities", "abstract": "Legal professionals worldwide are currently trying to get up-to-pace with the\nexplosive growth in legal document availability through digital means. This\ndrives a need for high efficiency Legal Information Retrieval (IR) and Question\nAnswering (QA) methods. The IR task in particular has a set of unique\nchallenges that invite the use of semantic motivated NLP techniques. In this\nwork, a two-stage method for Legal Information Retrieval is proposed, combining\nlexical statistics and distributional sentence representations in the context\nof Competition on Legal Information Extraction/Entailment (COLIEE). The\ncombination is done with the use of disambiguation rules, applied over the\nrankings obtained through n-gram statistics. After the ranking is done, its\nresults are evaluated for ambiguity, and disambiguation is done if a result is\ndecided to be unreliable for a given query. Competition and experimental\nresults indicate small gains in overall retrieval performance using the\nproposed approach. Additionally, an analysis of error and improvement cases is\npresented for a better understanding of the contributions.", "published": "2017-06-04 06:57:09", "link": "http://arxiv.org/abs/1706.01038v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Joint Text Embedding for Personalized Content-based Recommendation", "abstract": "Learning a good representation of text is key to many recommendation\napplications. Examples include news recommendation where texts to be\nrecommended are constantly published everyday. However, most existing\nrecommendation techniques, such as matrix factorization based methods, mainly\nrely on interaction histories to learn representations of items. While latent\nfactors of items can be learned effectively from user interaction data, in many\ncases, such data is not available, especially for newly emerged items.\n  In this work, we aim to address the problem of personalized recommendation\nfor completely new items with text information available. We cast the problem\nas a personalized text ranking problem and propose a general framework that\ncombines text embedding with personalized recommendation. Users and textual\ncontent are embedded into latent feature space. The text embedding function can\nbe learned end-to-end by predicting user interactions with items. To alleviate\nsparsity in interaction data, and leverage large amount of text data with\nlittle or no user interactions, we further propose a joint text embedding model\nthat incorporates unsupervised text embedding with a combination module.\nExperimental results show that our model can significantly improve the\neffectiveness of recommendation systems on real-world datasets.", "published": "2017-06-04 14:48:28", "link": "http://arxiv.org/abs/1706.01084v2", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
