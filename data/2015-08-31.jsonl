{"title": "Neural Machine Translation of Rare Words with Subword Units", "abstract": "Neural machine translation (NMT) models typically operate with a fixed\nvocabulary, but translation is an open-vocabulary problem. Previous work\naddresses the translation of out-of-vocabulary words by backing off to a\ndictionary. In this paper, we introduce a simpler and more effective approach,\nmaking the NMT model capable of open-vocabulary translation by encoding rare\nand unknown words as sequences of subword units. This is based on the intuition\nthat various word classes are translatable via smaller units than words, for\ninstance names (via character copying or transliteration), compounds (via\ncompositional translation), and cognates and loanwords (via phonological and\nmorphological transformations). We discuss the suitability of different word\nsegmentation techniques, including simple character n-gram models and a\nsegmentation based on the byte pair encoding compression algorithm, and\nempirically show that subword models improve over a back-off dictionary\nbaseline for the WMT 15 translation tasks English-German and English-Russian by\n1.1 and 1.3 BLEU, respectively.", "published": "2015-08-31 16:37:31", "link": "http://arxiv.org/abs/1508.07909v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Word Representations, Tree Models and Syntactic Functions", "abstract": "Word representations induced from models with discrete latent variables\n(e.g.\\ HMMs) have been shown to be beneficial in many NLP applications. In this\nwork, we exploit labeled syntactic dependency trees and formalize the induction\nproblem as unsupervised learning of tree-structured hidden Markov models.\nSyntactic functions are used as additional observed variables in the model,\ninfluencing both transition and emission components. Such syntactic information\ncan potentially lead to capturing more fine-grain and functional distinctions\nbetween words, which, in turn, may be desirable in many NLP applications. We\nevaluate the word representations on two tasks -- named entity recognition and\nsemantic frame identification. We observe improvements from exploiting\nsyntactic function information in both cases, and the results rivaling those of\nstate-of-the-art representation learning methods. Additionally, we revisit the\nrelationship between sequential and unlabeled-tree models and find that the\nadvantage of the latter is not self-evident.", "published": "2015-08-31 07:52:50", "link": "http://arxiv.org/abs/1508.07709v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
