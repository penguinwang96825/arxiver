{"title": "Self-Attention with Relative Position Representations", "abstract": "Relying entirely on an attention mechanism, the Transformer introduced by\nVaswani et al. (2017) achieves state-of-the-art results for machine\ntranslation. In contrast to recurrent and convolutional neural networks, it\ndoes not explicitly model relative or absolute position information in its\nstructure. Instead, it requires adding representations of absolute positions to\nits inputs. In this work we present an alternative approach, extending the\nself-attention mechanism to efficiently consider representations of the\nrelative positions, or distances between sequence elements. On the WMT 2014\nEnglish-to-German and English-to-French translation tasks, this approach yields\nimprovements of 1.3 BLEU and 0.3 BLEU over absolute position representations,\nrespectively. Notably, we observe that combining relative and absolute position\nrepresentations yields no further improvement in translation quality. We\ndescribe an efficient implementation of our method and cast it as an instance\nof relation-aware self-attention mechanisms that can generalize to arbitrary\ngraph-labeled inputs.", "published": "2018-03-06 13:13:11", "link": "http://arxiv.org/abs/1803.02155v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CliNER 2.0: Accessible and Accurate Clinical Concept Extraction", "abstract": "Clinical notes often describe important aspects of a patient's stay and are\ntherefore critical to medical research. Clinical concept extraction (CCE) of\nnamed entities - such as problems, tests, and treatments - aids in forming an\nunderstanding of notes and provides a foundation for many downstream clinical\ndecision-making tasks. Historically, this task has been posed as a standard\nnamed entity recognition (NER) sequence tagging problem, and solved with\nfeature-based methods using handengineered domain knowledge. Recent advances,\nhowever, have demonstrated the efficacy of LSTM-based models for NER tasks,\nincluding CCE. This work presents CliNER 2.0, a simple-to-install, open-source\ntool for extracting concepts from clinical text. CliNER 2.0 uses a word- and\ncharacter- level LSTM model, and achieves state-of-the-art performance. For\nease of use, the tool also includes pre-trained models available for public\nuse.", "published": "2018-03-06 15:17:40", "link": "http://arxiv.org/abs/1803.02245v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An End-to-End Goal-Oriented Dialog System with a Generative Natural\n  Language Response Generation", "abstract": "Recently advancements in deep learning allowed the development of end-to-end\ntrained goal-oriented dialog systems. Although these systems already achieve\ngood performance, some simplifications limit their usage in real-life\nscenarios.\n  In this work, we address two of these limitations: ignoring positional\ninformation and a fixed number of possible response candidates. We propose to\nuse positional encodings in the input to model the word order of the user\nutterances. Furthermore, by using a feedforward neural network, we are able to\ngenerate the output word by word and are no longer restricted to a fixed number\nof possible response candidates. Using the positional encoding, we were able to\nachieve better accuracies in the Dialog bAbI Tasks and using the feedforward\nneural network for generating the response, we were able to save computation\ntime and space consumption.", "published": "2018-03-06 16:17:18", "link": "http://arxiv.org/abs/1803.02279v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multimodal Emoji Prediction", "abstract": "Emojis are small images that are commonly included in social media text\nmessages. The combination of visual and textual content in the same message\nbuilds up a modern way of communication, that automatic systems are not used to\ndeal with. In this paper we extend recent advances in emoji prediction by\nputting forward a multimodal approach that is able to predict emojis in\nInstagram posts. Instagram posts are composed of pictures together with texts\nwhich sometimes include emojis. We show that these emojis can be predicted by\nusing the text, but also using the picture. Our main finding is that\nincorporating the two synergistic modalities, in a combined model, improves\naccuracy in an emoji prediction task. This result demonstrates that these two\nmodalities (text and images) encode different information on the use of emojis\nand therefore can complement each other.", "published": "2018-03-06 19:23:24", "link": "http://arxiv.org/abs/1803.02392v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Code Review Comments: Language Matters", "abstract": "Recent research provides evidence that effective communication in\ncollaborative software development has significant impact on the software\ndevelopment lifecycle. Although related qualitative and quantitative studies\npoint out textual characteristics of well-formed messages, the underlying\nsemantics of the intertwined linguistic structures still remain largely\nmisinterpreted or ignored. Especially, regarding quality of code reviews the\nimportance of thorough feedback, and explicit rationale is often mentioned but\nrarely linked with related linguistic features. As a first step towards\naddressing this shortcoming, we propose grounding these studies on theories of\nlinguistics. We particularly focus on linguistic structures of coherent speech\nand explain how they can be exploited in practice. We reflect on related\napproaches and examine through a preliminary study on four open source\nprojects, possible links between existing findings and the directions we\nsuggest for detecting textual features of useful code reviews.", "published": "2018-03-06 14:21:15", "link": "http://arxiv.org/abs/1803.02205v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Annotation Artifacts in Natural Language Inference Data", "abstract": "Large-scale datasets for natural language inference are created by presenting\ncrowd workers with a sentence (premise), and asking them to generate three new\nsentences (hypotheses) that it entails, contradicts, or is logically neutral\nwith respect to. We show that, in a significant portion of such data, this\nprotocol leaves clues that make it possible to identify the label by looking\nonly at the hypothesis, without observing the premise. Specifically, we show\nthat a simple text categorization model can correctly classify the hypothesis\nalone in about 67% of SNLI (Bowman et. al, 2015) and 53% of MultiNLI (Williams\net. al, 2017). Our analysis reveals that specific linguistic phenomena such as\nnegation and vagueness are highly correlated with certain inference classes.\nOur findings suggest that the success of natural language inference models to\ndate has been overestimated, and that the task remains a hard open problem.", "published": "2018-03-06 18:23:08", "link": "http://arxiv.org/abs/1803.02324v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Explain Yourself: A Natural Language Interface for Scrutable Autonomous\n  Robots", "abstract": "Autonomous systems in remote locations have a high degree of autonomy and\nthere is a need to explain what they are doing and why in order to increase\ntransparency and maintain trust. Here, we describe a natural language chat\ninterface that enables vehicle behaviour to be queried by the user. We obtain\nan interpretable model of autonomy through having an expert 'speak out-loud'\nand provide explanations during a mission. This approach is agnostic to the\ntype of autonomy model and as expert and operator are from the same user-group,\nwe predict that these explanations will align well with the operator's mental\nmodel, increase transparency and assist with operator training.", "published": "2018-03-06 10:13:29", "link": "http://arxiv.org/abs/1803.02088v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "I.2.7; H.5.2"], "primary_category": "cs.CL"}
{"title": "Precise but Natural Specification for Robot Tasks", "abstract": "We present Flipper, a natural language interface for describing high-level\ntask specifications for robots that are compiled into robot actions. Flipper\nstarts with a formal core language for task planning that allows expressing\nrich temporal specifications and uses a semantic parser to provide a natural\nlanguage interface. Flipper provides immediate visual feedback by executing an\nautomatically constructed plan of the task in a graphical user interface. This\nallows the user to resolve potentially ambiguous interpretations. Flipper\nextends itself via naturalization: its users can add definitions for\nutterances, from which Flipper induces new rules and adds them to the core\nlanguage, gradually growing a more and more natural task specification\nlanguage. Flipper improves the naturalization by generalizing the definition\nprovided by users. Unlike other task-specification systems, Flipper enables\nnatural language interactions while maintaining the expressive power and formal\nprecision of a programming language. We show through an initial user study that\nnatural language interactions and generalization can considerably ease the\ndescription of tasks. Moreover, over time, users employ more and more concepts\noutside of the initial core language. Such extensions are available to the\nFlipper community, and users can use concepts that others have defined.", "published": "2018-03-06 15:07:40", "link": "http://arxiv.org/abs/1803.02238v2", "categories": ["cs.RO", "cs.CL", "cs.SY"], "primary_category": "cs.RO"}
{"title": "Co-occurrence of the Benford-like and Zipf Laws Arising from the Texts\n  Representing Human and Artificial Languages", "abstract": "We demonstrate that large texts, representing human (English, Russian,\nUkrainian) and artificial (C++, Java) languages, display quantitative patterns\ncharacterized by the Benford-like and Zipf laws. The frequency of a word\nfollowing the Zipf law is inversely proportional to its rank, whereas the total\nnumbers of a certain word appearing in the text generate the uneven\nBenford-like distribution of leading numbers. Excluding the most popular words\nessentially improves the correlation of actual textual data with the Zipfian\ndistribution, whereas the Benford distribution of leading numbers (arising from\nthe overall amount of a certain word) is insensitive to the same elimination\nprocedure. The calculated values of the moduli of slopes of double\nlogarithmical plots for artificial languages (C++, Java) are markedly larger\nthan those for human ones.", "published": "2018-03-06 12:24:42", "link": "http://arxiv.org/abs/1803.03667v1", "categories": ["cs.CL", "physics.soc-ph", "stat.OT"], "primary_category": "cs.CL"}
{"title": "Multi-level Attention Model for Weakly Supervised Audio Classification", "abstract": "In this paper, we propose a multi-level attention model to solve the weakly\nlabelled audio classification problem. The objective of audio classification is\nto predict the presence or absence of audio events in an audio clip. Recently,\nGoogle published a large scale weakly labelled dataset called Audio Set, where\neach audio clip contains only the presence or absence of the audio events,\nwithout the onset and offset time of the audio events. Our multi-level\nattention model is an extension to the previously proposed single-level\nattention model. It consists of several attention modules applied on\nintermediate neural network layers. The output of these attention modules are\nconcatenated to a vector followed by a multi-label classifier to make the final\nprediction of each class. Experiments shown that our model achieves a mean\naverage precision (mAP) of 0.360, outperforms the state-of-the-art single-level\nattention model of 0.327 and Google baseline of 0.314.", "published": "2018-03-06 15:59:21", "link": "http://arxiv.org/abs/1803.02353v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Masked Conditional Neural Networks for Audio Classification", "abstract": "We present the ConditionaL Neural Network (CLNN) and the Masked ConditionaL\nNeural Network (MCLNN) designed for temporal signal recognition. The CLNN takes\ninto consideration the temporal nature of the sound signal and the MCLNN\nextends upon the CLNN through a binary mask to preserve the spatial locality of\nthe features and allows an automated exploration of the features combination\nanalogous to hand-crafting the most relevant features for the recognition task.\nMCLNN has achieved competitive recognition accuracies on the GTZAN and the\nISMIR2004 music datasets that surpass several state-of-the-art neural network\nbased architectures and hand-crafted methods applied on both datasets.", "published": "2018-03-06 20:54:00", "link": "http://arxiv.org/abs/1803.02421v2", "categories": ["stat.ML", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "stat.ML"}
