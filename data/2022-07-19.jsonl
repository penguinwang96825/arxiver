{"title": "Multilingual Transformer Encoders: a Word-Level Task-Agnostic Evaluation", "abstract": "Some Transformer-based models can perform cross-lingual transfer learning:\nthose models can be trained on a specific task in one language and give\nrelatively good results on the same task in another language, despite having\nbeen pre-trained on monolingual tasks only. But, there is no consensus yet on\nwhether those transformer-based models learn universal patterns across\nlanguages. We propose a word-level task-agnostic method to evaluate the\nalignment of contextualized representations built by such models. We show that\nour method provides more accurate translated word pairs than previous methods\nto evaluate word-level alignment. And our results show that some inner layers\nof multilingual Transformer-based models outperform other explicitly aligned\nrepresentations, and even more so according to a stricter definition of\nmultilingual alignment.", "published": "2022-07-19 05:23:18", "link": "http://arxiv.org/abs/2207.09076v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the cross-lingual transferability of multilingual prototypical models\n  across NLU tasks", "abstract": "Supervised deep learning-based approaches have been applied to task-oriented\ndialog and have proven to be effective for limited domain and language\napplications when a sufficient number of training examples are available. In\npractice, these approaches suffer from the drawbacks of domain-driven design\nand under-resourced languages. Domain and language models are supposed to grow\nand change as the problem space evolves. On one hand, research on transfer\nlearning has demonstrated the cross-lingual ability of multilingual\nTransformers-based models to learn semantically rich representations. On the\nother, in addition to the above approaches, meta-learning have enabled the\ndevelopment of task and language learning algorithms capable of far\ngeneralization. Through this context, this article proposes to investigate the\ncross-lingual transferability of using synergistically few-shot learning with\nprototypical neural networks and multilingual Transformers-based models.\nExperiments in natural language understanding tasks on MultiATIS++ corpus shows\nthat our approach substantially improves the observed transfer learning\nperformances between the low and the high resource languages. More generally\nour approach confirms that the meaningful latent space learned in a given\nlanguage can be can be generalized to unseen and under-resourced ones using\nmeta-learning.", "published": "2022-07-19 09:55:04", "link": "http://arxiv.org/abs/2207.09157v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Urdu Speech and Text Based Sentiment Analyzer", "abstract": "Discovering what other people think has always been a key aspect of our\ninformation-gathering strategy. People can now actively utilize information\ntechnology to seek out and comprehend the ideas of others, thanks to the\nincreased availability and popularity of opinion-rich resources such as online\nreview sites and personal blogs. Because of its crucial function in\nunderstanding people's opinions, sentiment analysis (SA) is a crucial task.\nExisting research, on the other hand, is primarily focused on the English\nlanguage, with just a small amount of study devoted to low-resource languages.\nFor sentiment analysis, this work presented a new multi-class Urdu dataset\nbased on user evaluations. The tweeter website was used to get Urdu dataset.\nOur proposed dataset includes 10,000 reviews that have been carefully\nclassified into two categories by human experts: positive, negative. The\nprimary purpose of this research is to construct a manually annotated dataset\nfor Urdu sentiment analysis and to establish the baseline result. Five\ndifferent lexicon- and rule-based algorithms including Naivebayes, Stanza,\nTextblob, Vader, and Flair are employed and the experimental results show that\nFlair with an accuracy of 70% outperforms other tested algorithms.", "published": "2022-07-19 10:11:22", "link": "http://arxiv.org/abs/2207.09163v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Big Data and Education: using big data analytics in language learning", "abstract": "Working with big data using data mining tools is rapidly becoming a trend in\neducation industry. The combination of the current capacity to collect, store,\nmanage and process data in a timely manner, and data from online educational\nplatforms represents an unprecedented opportunity for educational institutes,\nlearners, educators, and researchers. In this position paper, we consider some\nbasic concepts as well as most popular tools, methods and techniques regarding\nEducational Data Mining and Learning Analytics, and discuss big data\napplications in language learning, in particular.", "published": "2022-07-19 19:17:10", "link": "http://arxiv.org/abs/2207.10572v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PiC: A Phrase-in-Context Dataset for Phrase Understanding and Semantic\n  Search", "abstract": "While contextualized word embeddings have been a de-facto standard, learning\ncontextualized phrase embeddings is less explored and being hindered by the\nlack of a human-annotated benchmark that tests machine understanding of phrase\nsemantics given a context sentence or paragraph (instead of phrases alone). To\nfill this gap, we propose PiC -- a dataset of ~28K of noun phrases accompanied\nby their contextual Wikipedia pages and a suite of three tasks for training and\nevaluating phrase embeddings. Training on PiC improves ranking models' accuracy\nand remarkably pushes span-selection (SS) models (i.e., predicting the start\nand end index of the target phrase) near-human accuracy, which is 95% Exact\nMatch (EM) on semantic search given a query phrase and a passage.\nInterestingly, we find evidence that such impressive performance is because the\nSS models learn to better capture the common meaning of a phrase regardless of\nits actual context. SotA models perform poorly in distinguishing two senses of\nthe same phrase in two contexts (~60% EM) and in estimating the similarity\nbetween two different phrases in the same context (~70% EM).", "published": "2022-07-19 04:45:41", "link": "http://arxiv.org/abs/2207.09068v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ILASR: Privacy-Preserving Incremental Learning for Automatic Speech\n  Recognition at Production Scale", "abstract": "Incremental learning is one paradigm to enable model building and updating at\nscale with streaming data. For end-to-end automatic speech recognition (ASR)\ntasks, the absence of human annotated labels along with the need for privacy\npreserving policies for model building makes it a daunting challenge. Motivated\nby these challenges, in this paper we use a cloud based framework for\nproduction systems to demonstrate insights from privacy preserving incremental\nlearning for automatic speech recognition (ILASR). By privacy preserving, we\nmean, usage of ephemeral data which are not human annotated. This system is a\nstep forward for production levelASR models for incremental/continual learning\nthat offers near real-time test-bed for experimentation in the cloud for\nend-to-end ASR, while adhering to privacy-preserving policies. We show that the\nproposed system can improve the production models significantly(3%) over a new\ntime period of six months even in the absence of human annotated labels with\nvarying levels of weak supervision and large batch sizes in incremental\nlearning. This improvement is 20% over test sets with new words and phrases in\nthe new time period. We demonstrate the effectiveness of model building in a\nprivacy-preserving incremental fashion for ASR while further exploring the\nutility of having an effective teacher model and use of large batch sizes.", "published": "2022-07-19 05:24:13", "link": "http://arxiv.org/abs/2207.09078v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MoEC: Mixture of Expert Clusters", "abstract": "Sparsely Mixture of Experts (MoE) has received great interest due to its\npromising scaling capability with affordable computational overhead. MoE\nconverts dense layers into sparse experts, and utilizes a gated routing network\nto make experts conditionally activated. However, as the number of experts\ngrows, MoE with outrageous parameters suffers from overfitting and sparse data\nallocation. Such problems are especially severe on tasks with limited data,\nthus hindering the progress for MoE models to improve performance by scaling\nup. In this work, we propose Mixture of Expert Clusters - a general approach to\nenable expert layers to learn more diverse and appropriate knowledge by\nimposing variance-based constraints on the routing stage. We further propose a\ncluster-level expert dropout strategy specifically designed for the expert\ncluster structure. Our experiments reveal that MoEC could improve performance\non machine translation and natural language understanding tasks, and raise the\nperformance upper bound for scaling up experts under limited data. We also\nverify that MoEC plays a positive role in mitigating overfitting and sparse\ndata allocation.", "published": "2022-07-19 06:09:55", "link": "http://arxiv.org/abs/2207.09094v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Analyzing Bagging Methods for Language Models", "abstract": "Modern language models leverage increasingly large numbers of parameters to\nachieve performance on natural language understanding tasks. Ensembling these\nmodels in specific configurations for downstream tasks show even further\nperformance improvements. In this paper, we perform an analysis of bagging\nlanguage models and compare single language models to bagged ensembles that are\nroughly equivalent in terms of final model size. We explore an array of model\nbagging configurations for natural language understanding tasks with final\nensemble sizes ranging from 300M parameters to 1.5B parameters and determine\nthat our ensembling methods are at best roughly equivalent to single LM\nbaselines. We note other positive effects of bagging and pruning in specific\nscenarios according to findings in our experiments such as variance reduction\nand minor performance improvements.", "published": "2022-07-19 06:30:37", "link": "http://arxiv.org/abs/2207.09099v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On the Usability of Transformers-based models for a French\n  Question-Answering task", "abstract": "For many tasks, state-of-the-art results have been achieved with\nTransformer-based architectures, resulting in a paradigmatic shift in practices\nfrom the use of task-specific architectures to the fine-tuning of pre-trained\nlanguage models. The ongoing trend consists in training models with an\never-increasing amount of data and parameters, which requires considerable\nresources. It leads to a strong search to improve resource efficiency based on\nalgorithmic and hardware improvements evaluated only for English. This raises\nquestions about their usability when applied to small-scale learning problems,\nfor which a limited amount of training data is available, especially for\nunder-resourced languages tasks. The lack of appropriately sized corpora is a\nhindrance to applying data-driven and transfer learning-based approaches with\nstrong instability cases. In this paper, we establish a state-of-the-art of the\nefforts dedicated to the usability of Transformer-based models and propose to\nevaluate these improvements on the question-answering performances of French\nlanguage which have few resources. We address the instability relating to data\nscarcity by investigating various training strategies with data augmentation,\nhyperparameters optimization and cross-lingual transfer. We also introduce a\nnew compact model for French FrALBERT which proves to be competitive in\nlow-resource settings.", "published": "2022-07-19 09:46:15", "link": "http://arxiv.org/abs/2207.09150v1", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Benchmarking Transformers-based models on French Spoken Language\n  Understanding tasks", "abstract": "In the last five years, the rise of the self-attentional Transformer-based\narchitectures led to state-of-the-art performances over many natural language\ntasks. Although these approaches are increasingly popular, they require large\namounts of data and computational resources. There is still a substantial need\nfor benchmarking methodologies ever upwards on under-resourced languages in\ndata-scarce application conditions. Most pre-trained language models were\nmassively studied using the English language and only a few of them were\nevaluated on French. In this paper, we propose a unified benchmark, focused on\nevaluating models quality and their ecological impact on two well-known French\nspoken language understanding tasks. Especially we benchmark thirteen\nwell-established Transformer-based models on the two available spoken language\nunderstanding tasks for French: MEDIA and ATIS-FR. Within this framework, we\nshow that compact models can reach comparable results to bigger ones while\ntheir ecological impact is considerably lower. However, this assumption is\nnuanced and depends on the considered compression method.", "published": "2022-07-19 09:47:08", "link": "http://arxiv.org/abs/2207.09152v1", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "QuoteKG: A Multilingual Knowledge Graph of Quotes", "abstract": "Quotes of public figures can mark turning points in history. A quote can\nexplain its originator's actions, foreshadowing political or personal decisions\nand revealing character traits. Impactful quotes cross language barriers and\ninfluence the general population's reaction to specific stances, always facing\nthe risk of being misattributed or taken out of context. The provision of a\ncross-lingual knowledge graph of quotes that establishes the authenticity of\nquotes and their contexts is of great importance to allow the exploration of\nthe lives of important people as well as topics from the perspective of what\nwas actually said. In this paper, we present QuoteKG, the first multilingual\nknowledge graph of quotes. We propose the QuoteKG creation pipeline that\nextracts quotes from Wikiquote, a free and collaboratively created collection\nof quotes in many languages, and aligns different mentions of the same quote.\nQuoteKG includes nearly one million quotes in $55$ languages, said by more than\n$69,000$ people of public interest across a wide range of topics. QuoteKG is\npublicly available and can be accessed via a SPARQL endpoint.", "published": "2022-07-19 21:32:59", "link": "http://arxiv.org/abs/2207.09562v1", "categories": ["cs.CL", "cs.DB"], "primary_category": "cs.CL"}
{"title": "Relational Future Captioning Model for Explaining Likely Collisions in\n  Daily Tasks", "abstract": "Domestic service robots that support daily tasks are a promising solution for\nelderly or disabled people. It is crucial for domestic service robots to\nexplain the collision risk before they perform actions. In this paper, our aim\nis to generate a caption about a future event. We propose the Relational Future\nCaptioning Model (RFCM), a crossmodal language generation model for the future\ncaptioning task. The RFCM has the Relational Self-Attention Encoder to extract\nthe relationships between events more effectively than the conventional\nself-attention in transformers. We conducted comparison experiments, and the\nresults show the RFCM outperforms a baseline method on two datasets.", "published": "2022-07-19 05:42:14", "link": "http://arxiv.org/abs/2207.09083v1", "categories": ["cs.RO", "cs.CL", "cs.CV"], "primary_category": "cs.RO"}
{"title": "Can You Fool AI by Doing a 180? $\\unicode{x2013}$ A Case Study on\n  Authorship Analysis of Texts by Arata Osada", "abstract": "This paper is our attempt at answering a twofold question covering the areas\nof ethics and authorship analysis. Firstly, since the methods used for\nperforming authorship analysis imply that an author can be recognized by the\ncontent he or she creates, we were interested in finding out whether it would\nbe possible for an author identification system to correctly attribute works to\nauthors if in the course of years they have undergone a major psychological\ntransition. Secondly, and from the point of view of the evolution of an\nauthor's ethical values, we checked what it would mean if the authorship\nattribution system encounters difficulties in detecting single authorship. We\nset out to answer those questions through performing a binary authorship\nanalysis task using a text classifier based on a pre-trained transformer model\nand a baseline method relying on conventional similarity metrics. For the test\nset, we chose works of Arata Osada, a Japanese educator and specialist in the\nhistory of education, with half of them being books written before the World\nWar II and another half in the 1950s, in between which he underwent a\ntransformation in terms of political opinions. As a result, we were able to\nconfirm that in the case of texts authored by Arata Osada in a time span of\nmore than 10 years, while the classification accuracy drops by a large margin\nand is substantially lower than for texts by other non-fiction writers,\nconfidence scores of the predictions remain at a similar level as in the case\nof a shorter time span, indicating that the classifier was in many instances\ntricked into deciding that texts written over a time span of multiple years\nwere actually written by two different people, which in turn leads us to\nbelieve that such a change can affect authorship analysis, and that historical\nevents have great impact on a person's ethical outlook as expressed in their\nwritings.", "published": "2022-07-19 05:43:49", "link": "http://arxiv.org/abs/2207.09085v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Formal Algorithms for Transformers", "abstract": "This document aims to be a self-contained, mathematically precise overview of\ntransformer architectures and algorithms (*not* results). It covers what\ntransformers are, how they are trained, what they are used for, their key\narchitectural components, and a preview of the most prominent models. The\nreader is assumed to be familiar with basic ML terminology and simpler neural\nnetwork architectures such as MLPs.", "published": "2022-07-19 12:49:02", "link": "http://arxiv.org/abs/2207.09238v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NE"], "primary_category": "cs.LG"}
{"title": "ESPnet-SE++: Speech Enhancement for Robust Speech Recognition,\n  Translation, and Understanding", "abstract": "This paper presents recent progress on integrating speech separation and\nenhancement (SSE) into the ESPnet toolkit. Compared with the previous ESPnet-SE\nwork, numerous features have been added, including recent state-of-the-art\nspeech enhancement models with their respective training and evaluation\nrecipes. Importantly, a new interface has been designed to flexibly combine\nspeech enhancement front-ends with other tasks, including automatic speech\nrecognition (ASR), speech translation (ST), and spoken language understanding\n(SLU). To showcase such integration, we performed experiments on carefully\ndesigned synthetic datasets for noisy-reverberant multi-channel ST and SLU\ntasks, which can be used as benchmark corpora for future research. In addition\nto these new tasks, we also use CHiME-4 and WSJ0-2Mix to benchmark multi- and\nsingle-channel SE approaches. Results show that the integration of SE\nfront-ends with back-end tasks is a promising research direction even for tasks\nbesides ASR, especially in the multi-channel scenario. The code is available\nonline at https://github.com/ESPnet/ESPnet. The multi-channel ST and SLU\ndatasets, which are another contribution of this work, are released on\nHuggingFace.", "published": "2022-07-19 18:55:29", "link": "http://arxiv.org/abs/2207.09514v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Tip-Adapter: Training-free Adaption of CLIP for Few-shot Classification", "abstract": "Contrastive Vision-Language Pre-training, known as CLIP, has provided a new\nparadigm for learning visual representations using large-scale image-text\npairs. It shows impressive performance on downstream tasks by zero-shot\nknowledge transfer. To further enhance CLIP's adaption capability, existing\nmethods proposed to fine-tune additional learnable modules, which significantly\nimproves the few-shot performance but introduces extra training time and\ncomputational resources. In this paper, we propose a training-free adaption\nmethod for CLIP to conduct few-shot classification, termed as Tip-Adapter,\nwhich not only inherits the training-free advantage of zero-shot CLIP but also\nperforms comparably to those training-required approaches. Tip-Adapter\nconstructs the adapter via a key-value cache model from the few-shot training\nset, and updates the prior knowledge encoded in CLIP by feature retrieval. On\ntop of that, the performance of Tip-Adapter can be further boosted to be\nstate-of-the-art on ImageNet by fine-tuning the cache model for 10$\\times$\nfewer epochs than existing methods, which is both effective and efficient. We\nconduct extensive experiments of few-shot classification on 11 datasets to\ndemonstrate the superiority of our proposed methods. Code is released at\nhttps://github.com/gaopengcuhk/Tip-Adapter.", "published": "2022-07-19 19:12:11", "link": "http://arxiv.org/abs/2207.09519v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Human-guided Collaborative Problem Solving: A Natural Language based\n  Framework", "abstract": "We consider the problem of human-machine collaborative problem solving as a\nplanning task coupled with natural language communication. Our framework\nconsists of three components -- a natural language engine that parses the\nlanguage utterances to a formal representation and vice-versa, a concept\nlearner that induces generalized concepts for plans based on limited\ninteractions with the user, and an HTN planner that solves the task based on\nhuman interaction. We illustrate the ability of this framework to address the\nkey challenges of collaborative problem solving by demonstrating it on a\ncollaborative building task in a Minecraft-based blocksworld domain. The\naccompanied demo video is available at https://youtu.be/q1pWe4aahF0.", "published": "2022-07-19 21:52:37", "link": "http://arxiv.org/abs/2207.09566v1", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Target-Driven Structured Transformer Planner for Vision-Language\n  Navigation", "abstract": "Vision-language navigation is the task of directing an embodied agent to\nnavigate in 3D scenes with natural language instructions. For the agent,\ninferring the long-term navigation target from visual-linguistic clues is\ncrucial for reliable path planning, which, however, has rarely been studied\nbefore in literature. In this article, we propose a Target-Driven Structured\nTransformer Planner (TD-STP) for long-horizon goal-guided and room layout-aware\nnavigation. Specifically, we devise an Imaginary Scene Tokenization mechanism\nfor explicit estimation of the long-term target (even located in unexplored\nenvironments). In addition, we design a Structured Transformer Planner which\nelegantly incorporates the explored room layout into a neural attention\narchitecture for structured and global planning. Experimental results\ndemonstrate that our TD-STP substantially improves previous best methods'\nsuccess rate by 2% and 5% on the test set of R2R and REVERIE benchmarks,\nrespectively. Our code is available at https://github.com/YushengZhao/TD-STP .", "published": "2022-07-19 06:46:21", "link": "http://arxiv.org/abs/2207.11201v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Enhancing Collaborative Filtering Recommender with Prompt-Based\n  Sentiment Analysis", "abstract": "Collaborative Filtering(CF) recommender is a crucial application in the\nonline market and ecommerce. However, CF recommender has been proven to suffer\nfrom persistent problems related to sparsity of the user rating that will\nfurther lead to a cold-start issue. Existing methods address the data sparsity\nissue by applying token-level sentiment analysis that translate text review\ninto sentiment scores as a complement of the user rating. In this paper, we\nattempt to optimize the sentiment analysis with advanced NLP models including\nBERT and RoBERTa, and experiment on whether the CF recommender has been further\nenhanced. We build the recommenders on the Amazon US Reviews dataset, and tune\nthe pretrained BERT and RoBERTa with the traditional fine-tuned paradigm as\nwell as the new prompt-based learning paradigm. Experimental result shows that\nthe recommender enhanced with the sentiment ratings predicted by the fine-tuned\nRoBERTa has the best performance, and achieved 30.7% overall gain by comparing\nMAP, NDCG and precision at K to the baseline recommender. Prompt-based learning\nparadigm, although superior to traditional fine-tune paradigm in pure sentiment\nanalysis, fail to further improve the CF recommender.", "published": "2022-07-19 21:04:31", "link": "http://arxiv.org/abs/2207.12883v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Realistic sources, receivers and walls improve the generalisability of\n  virtually-supervised blind acoustic parameter estimators", "abstract": "Blind acoustic parameter estimation consists in inferring the acoustic\nproperties of an environment from recordings of unknown sound sources. Recent\nworks in this area have utilized deep neural networks trained either partially\nor exclusively on simulated data, due to the limited availability of real\nannotated measurements. In this paper, we study whether a model purely trained\nusing a fast image-source room impulse response simulator can generalize to\nreal data. We present an ablation study on carefully crafted simulated training\nsets that account for different levels of realism in source, receiver and wall\nresponses. The extent of realism is controlled by the sampling of wall\nabsorption coefficients and by applying measured directivity patterns to\nmicrophones and sources. A state-of-the-art model trained on these datasets is\nevaluated on the task of jointly estimating the room's volume, total surface\narea, and octave-band reverberation times from multiple, multichannel speech\nrecordings. Results reveal that every added layer of simulation realism at\ntrain time significantly improves the estimation of all quantities on real\nsignals.", "published": "2022-07-19 09:09:38", "link": "http://arxiv.org/abs/2207.09133v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "GAFX: A General Audio Feature eXtractor", "abstract": "Most machine learning models for audio tasks are dealing with a handcrafted\nfeature, the spectrogram. However, it is still unknown whether the spectrogram\ncould be replaced with deep learning based features. In this paper, we answer\nthis question by comparing the different learnable neural networks extracting\nfeatures with a successful spectrogram model and proposed a General Audio\nFeature eXtractor (GAFX) based on a dual U-Net (GAFX-U), ResNet (GAFX-R), and\nAttention (GAFX-A) modules. We design experiments to evaluate this model on the\nmusic genre classification task on the GTZAN dataset and perform a detailed\nablation study of different configurations of our framework and our model\nGAFX-U, following the Audio Spectrogram Transformer (AST) classifier achieves\ncompetitive performance.", "published": "2022-07-19 09:37:44", "link": "http://arxiv.org/abs/2207.09145v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Machine-learning applied to classify flow-induced sound parameters from\n  simulated human voice", "abstract": "Disorders of voice production have severe effects on the quality of life of\nthe affected individuals. A simulation approach is used to investigate the\ncause-effect chain in voice production showing typical characteristics of voice\nsuch as sub-glottal pressure and of functional voice disorders as glottal\nclosure insufficiency and left-right asymmetry. Therewith, 24 different voice\nconfigurations are simulated in a parameter study using a previously published\nhybrid aeroacoustic simulation model. Based on these 24 simulation\nconfigurations, selected acoustic parameters (HNR, CPP, ...) at simulation\nevaluation points are correlated with these simulation configuration details to\nderive characteristic insight in the flow-induced sound generation of human\nphonation based on simulation results. Recently, several institutions studied\nexperimental data, of flow and acoustic properties and correlated it with\nhealthy and disordered voice signals. Upon this, the study is a next step\ntowards a detailed dataset definition, the dataset is small, but the definition\nof relevant characteristics are precise based on the existing simulation\nmethodology of simVoice. The small datasets are studied by correlation\nanalysis, and a Support Vector Machine classifier with RBF kernel is used to\nclassify the representations. With the use of Linear Discriminant Analysis the\ndimensions of the individual studies are visualized. This allows to draw\ncorrelations and determine the most important features evaluated from the\nacoustic signals in front of the mouth. The GC type can be best discriminated\nbased on CPP and boxplot visualizations. Furthermore and using the\nLDA-dimensionality-reduced feature space, one can best classify subglottal\npressure with 91.7\\% accuracy, independent of healthy or disordered voice\nsimulation parameters.", "published": "2022-07-19 13:25:34", "link": "http://arxiv.org/abs/2207.09265v1", "categories": ["cs.SD", "eess.AS", "physics.bio-ph"], "primary_category": "cs.SD"}
{"title": "COVID-19 Detection from Respiratory Sounds with Hierarchical Spectrogram\n  Transformers", "abstract": "Monitoring of prevalent airborne diseases such as COVID-19 characteristically\ninvolves respiratory assessments. While auscultation is a mainstream method for\npreliminary screening of disease symptoms, its utility is hampered by the need\nfor dedicated hospital visits. Remote monitoring based on recordings of\nrespiratory sounds on portable devices is a promising alternative, which can\nassist in early assessment of COVID-19 that primarily affects the lower\nrespiratory tract. In this study, we introduce a novel deep learning approach\nto distinguish patients with COVID-19 from healthy controls given audio\nrecordings of cough or breathing sounds. The proposed approach leverages a\nnovel hierarchical spectrogram transformer (HST) on spectrogram representations\nof respiratory sounds. HST embodies self-attention mechanisms over local\nwindows in spectrograms, and window size is progressively grown over model\nstages to capture local to global context. HST is compared against\nstate-of-the-art conventional and deep-learning baselines. Demonstrations on\ncrowd-sourced multi-national datasets indicate that HST outperforms competing\nmethods, achieving over 83% area under the receiver operating characteristic\ncurve (AUC) in detecting COVID-19 cases.", "published": "2022-07-19 19:55:16", "link": "http://arxiv.org/abs/2207.09529v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
