{"title": "On the Origin of Hallucinations in Conversational Models: Is it the\n  Datasets or the Models?", "abstract": "Knowledge-grounded conversational models are known to suffer from producing\nfactually invalid statements, a phenomenon commonly called hallucination. In\nthis work, we investigate the underlying causes of this phenomenon: is\nhallucination due to the training data, or to the models? We conduct a\ncomprehensive human study on both existing knowledge-grounded conversational\nbenchmarks and several state-of-the-art models. Our study reveals that the\nstandard benchmarks consist of >60% hallucinated responses, leading to models\nthat not only hallucinate but even amplify hallucinations. Our findings raise\nimportant questions on the quality of existing datasets and models trained\nusing them. We make our annotations publicly available for future research.", "published": "2022-04-17 05:15:24", "link": "http://arxiv.org/abs/2204.07931v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Does Recommend-Revise Produce Reliable Annotations? An Analysis on\n  Missing Instances in DocRED", "abstract": "DocRED is a widely used dataset for document-level relation extraction. In\nthe large-scale annotation, a \\textit{recommend-revise} scheme is adopted to\nreduce the workload. Within this scheme, annotators are provided with candidate\nrelation instances from distant supervision, and they then manually supplement\nand remove relational facts based on the recommendations. However, when\ncomparing DocRED with a subset relabeled from scratch, we find that this scheme\nresults in a considerable amount of false negative samples and an obvious bias\ntowards popular entities and relations. Furthermore, we observe that the models\ntrained on DocRED have low recall on our relabeled dataset and inherit the same\nbias in the training data. Through the analysis of annotators' behaviors, we\nfigure out the underlying reason for the problems above: the scheme actually\ndiscourages annotators from supplementing adequate instances in the revision\nphase. We appeal to future research to take into consideration the issues with\nthe recommend-revise scheme when designing new models and annotation schemes.\nThe relabeled dataset is released at\n\\url{https://github.com/AndrewZhe/Revisit-DocRED}, to serve as a more reliable\ntest set of document RE models.", "published": "2022-04-17 11:29:01", "link": "http://arxiv.org/abs/2204.07980v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Nested Named Entity Recognition as Holistic Structure Parsing", "abstract": "As a fundamental natural language processing task and one of core knowledge\nextraction techniques, named entity recognition (NER) is widely used to extract\ninformation from texts for downstream tasks. Nested NER is a branch of NER in\nwhich the named entities (NEs) are nested with each other. However, most of the\nprevious studies on nested NER usually apply linear structure to model the\nnested NEs which are actually accommodated in a hierarchical structure. Thus in\norder to address this mismatch, this work models the full nested NEs in a\nsentence as a holistic structure, then we propose a holistic structure parsing\nalgorithm to disclose the entire NEs once for all. Besides, there is no\nresearch on applying corpus-level information to NER currently. To make up for\nthe loss of this information, we introduce Point-wise Mutual Information (PMI)\nand other frequency features from corpus-aware statistics for even better\nperformance by holistic modeling from sentence-level to corpus-level.\nExperiments show that our model yields promising results on widely-used\nbenchmarks which approach or even achieve state-of-the-art. Further empirical\nstudies show that our proposed corpus-aware features can substantially improve\nNER domain adaptation, which demonstrates the surprising advantage of our\nproposed corpus-level holistic structure modeling.", "published": "2022-04-17 12:48:20", "link": "http://arxiv.org/abs/2204.08006v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pathologies of Pre-trained Language Models in Few-shot Fine-tuning", "abstract": "Although adapting pre-trained language models with few examples has shown\npromising performance on text classification, there is a lack of understanding\nof where the performance gain comes from. In this work, we propose to answer\nthis question by interpreting the adaptation behavior using post-hoc\nexplanations from model predictions. By modeling feature statistics of\nexplanations, we discover that (1) without fine-tuning, pre-trained models\n(e.g. BERT and RoBERTa) show strong prediction bias across labels; (2) although\nfew-shot fine-tuning can mitigate the prediction bias and demonstrate promising\nprediction performance, our analysis shows models gain performance improvement\nby capturing non-task-related features (e.g. stop words) or shallow data\npatterns (e.g. lexical overlaps). These observations alert that pursuing model\nperformance with fewer examples may incur pathological prediction behavior,\nwhich requires further sanity check on model predictions and careful design in\nmodel evaluations in few-shot fine-tuning.", "published": "2022-04-17 15:55:18", "link": "http://arxiv.org/abs/2204.08039v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AfriWOZ: Corpus for Exploiting Cross-Lingual Transferability for\n  Generation of Dialogues in Low-Resource, African Languages", "abstract": "Dialogue generation is an important NLP task fraught with many challenges.\nThe challenges become more daunting for low-resource African languages. To\nenable the creation of dialogue agents for African languages, we contribute the\nfirst high-quality dialogue datasets for 6 African languages: Swahili, Wolof,\nHausa, Nigerian Pidgin English, Kinyarwanda & Yor\\`ub\\'a. These datasets\nconsist of 1,500 turns each, which we translate from a portion of the English\nmulti-domain MultiWOZ dataset. Subsequently, we investigate & analyze the\neffectiveness of modelling through transfer learning by utilziing\nstate-of-the-art (SoTA) deep monolingual models: DialoGPT and BlenderBot. We\ncompare the models with a simple seq2seq baseline using perplexity. Besides\nthis, we conduct human evaluation of single-turn conversations by using\nmajority votes and measure inter-annotator agreement (IAA). We find that the\nhypothesis that deep monolingual models learn some abstractions that generalize\nacross languages holds. We observe human-like conversations, to different\ndegrees, in 5 out of the 6 languages. The language with the most transferable\nproperties is the Nigerian Pidgin English, with a human-likeness score of\n78.1%, of which 34.4% are unanimous. We freely provide the datasets and host\nthe model checkpoints/demos on the HuggingFace hub for public access.", "published": "2022-04-17 20:23:04", "link": "http://arxiv.org/abs/2204.08083v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "kpfriends at SemEval-2022 Task 2: NEAMER -- Named Entity Augmented\n  Multi-word Expression Recognizer", "abstract": "We present NEAMER -- Named Entity Augmented Multi-word Expression Recognizer.\nThis system is inspired by non-compositionality characteristics shared between\nNamed Entity and Idiomatic Expressions. We utilize transfer learning and\nlocality features to enhance idiom classification task. This system is our\nsubmission for SemEval Task 2: Multilingual Idiomaticity Detection and Sentence\nEmbedding Subtask A OneShot shared task. We achieve SOTA with F1 0.9395 during\npost-evaluation phase. We also observe improvement in training stability.\nLastly, we experiment with non-compositionality knowledge transfer,\ncross-lingual fine-tuning and locality features, which we also introduce in\nthis paper.", "published": "2022-04-17 22:58:33", "link": "http://arxiv.org/abs/2204.08102v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ArcaneQA: Dynamic Program Induction and Contextualized Encoding for\n  Knowledge Base Question Answering", "abstract": "Question answering on knowledge bases (KBQA) poses a unique challenge for\nsemantic parsing research due to two intertwined challenges: large search space\nand ambiguities in schema linking. Conventional ranking-based KBQA models,\nwhich rely on a candidate enumeration step to reduce the search space, struggle\nwith flexibility in predicting complicated queries and have impractical running\ntime. In this paper, we present ArcaneQA, a novel generation-based model that\naddresses both the large search space and the schema linking challenges in a\nunified framework with two mutually boosting ingredients: dynamic program\ninduction for tackling the large search space and dynamic contextualized\nencoding for schema linking. Experimental results on multiple popular KBQA\ndatasets demonstrate the highly competitive performance of ArcaneQA in both\neffectiveness and efficiency.", "published": "2022-04-17 23:50:40", "link": "http://arxiv.org/abs/2204.08109v3", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Language Contamination Helps Explain the Cross-lingual Capabilities of\n  English Pretrained Models", "abstract": "English pretrained language models, which make up the backbone of many modern\nNLP systems, require huge amounts of unlabeled training data. These models are\ngenerally presented as being trained only on English text but have been found\nto transfer surprisingly well to other languages. We investigate this\nphenomenon and find that common English pretraining corpora actually contain\nsignificant amounts of non-English text: even when less than 1% of data is not\nEnglish (well within the error rate of strong language classifiers), this leads\nto hundreds of millions of foreign language tokens in large-scale datasets. We\nthen demonstrate that even these small percentages of non-English data\nfacilitate cross-lingual transfer for models trained on them, with target\nlanguage performance strongly correlated to the amount of in-language data seen\nduring pretraining. In light of these findings, we argue that no model is truly\nmonolingual when pretrained at scale, which should be considered when\nevaluating cross-lingual transfer.", "published": "2022-04-17 23:56:54", "link": "http://arxiv.org/abs/2204.08110v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledgeable Salient Span Mask for Enhancing Language Models as\n  Knowledge Base", "abstract": "Pre-trained language models (PLMs) like BERT have made significant progress\nin various downstream NLP tasks. However, by asking models to do cloze-style\ntests, recent work finds that PLMs are short in acquiring knowledge from\nunstructured text. To understand the internal behaviour of PLMs in retrieving\nknowledge, we first define knowledge-baring (K-B) tokens and knowledge-free\n(K-F) tokens for unstructured text and ask professional annotators to label\nsome samples manually. Then, we find that PLMs are more likely to give wrong\npredictions on K-B tokens and attend less attention to those tokens inside the\nself-attention module. Based on these observations, we develop two solutions to\nhelp the model learn more knowledge from unstructured text in a fully\nself-supervised manner. Experiments on knowledge-intensive tasks show the\neffectiveness of the proposed methods. To our best knowledge, we are the first\nto explore fully self-supervised learning of knowledge in continual\npre-training.", "published": "2022-04-17 12:33:34", "link": "http://arxiv.org/abs/2204.07994v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "WikiOmnia: generative QA corpus on the whole Russian Wikipedia", "abstract": "The General QA field has been developing the methodology referencing the\nStanford Question answering dataset (SQuAD) as the significant benchmark.\nHowever, compiling factual questions is accompanied by time- and\nlabour-consuming annotation, limiting the training data's potential size. We\npresent the WikiOmnia dataset, a new publicly available set of QA-pairs and\ncorresponding Russian Wikipedia article summary sections, composed with a fully\nautomated generative pipeline. The dataset includes every available article\nfrom Wikipedia for the Russian language. The WikiOmnia pipeline is available\nopen-source and is also tested for creating SQuAD-formatted QA on other\ndomains, like news texts, fiction, and social media. The resulting dataset\nincludes two parts: raw data on the whole Russian Wikipedia (7,930,873 QA pairs\nwith paragraphs for ruGPT-3 XL and 7,991,040 QA pairs with paragraphs for\nruT5-large) and cleaned data with strict automatic verification (over 160,000\nQA pairs with paragraphs for ruGPT-3 XL and over 3,400,000 QA pairs with\nparagraphs for ruT5-large).", "published": "2022-04-17 12:59:36", "link": "http://arxiv.org/abs/2204.08009v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evaluating Mixed-initiative Conversational Search Systems via User\n  Simulation", "abstract": "Clarifying the underlying user information need by asking clarifying\nquestions is an important feature of modern conversational search system.\nHowever, evaluation of such systems through answering prompted clarifying\nquestions requires significant human effort, which can be time-consuming and\nexpensive. In this paper, we propose a conversational User Simulator, called\nUSi, for automatic evaluation of such conversational search systems. Given a\ndescription of an information need, USi is capable of automatically answering\nclarifying questions about the topic throughout the search session. Through a\nset of experiments, including automated natural language generation metrics and\ncrowdsourcing studies, we show that responses generated by USi are both inline\nwith the underlying information need and comparable to human-generated answers.\nMoreover, we make the first steps towards multi-turn interactions, where\nconversational search systems asks multiple questions to the (simulated) user\nwith a goal of clarifying the user need. To this end, we expand on currently\navailable datasets for studying clarifying questions, i.e., Qulac and ClariQ,\nby performing a crowdsourcing-based multi-turn data acquisition. We show that\nour generative, GPT2-based model, is capable of providing accurate and natural\nanswers to unseen clarifying questions in the single-turn setting and discuss\ncapabilities of our model in the multi-turn setting. We provide the code, data,\nand the pre-trained model to be used for further research on the topic.", "published": "2022-04-17 16:27:33", "link": "http://arxiv.org/abs/2204.08046v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Residue-Based Natural Language Adversarial Attack Detection", "abstract": "Deep learning based systems are susceptible to adversarial attacks, where a\nsmall, imperceptible change at the input alters the model prediction. However,\nto date the majority of the approaches to detect these attacks have been\ndesigned for image processing systems. Many popular image adversarial detection\napproaches are able to identify adversarial examples from embedding feature\nspaces, whilst in the NLP domain existing state of the art detection approaches\nsolely focus on input text features, without consideration of model embedding\nspaces. This work examines what differences result when porting these image\ndesigned strategies to Natural Language Processing (NLP) tasks - these\ndetectors are found to not port over well. This is expected as NLP systems have\na very different form of input: discrete and sequential in nature, rather than\nthe continuous and fixed size inputs for images. As an equivalent model-focused\nNLP detection approach, this work proposes a simple sentence-embedding\n\"residue\" based detector to identify adversarial examples. On many tasks, it\nout-performs ported image domain detectors and recent state of the art NLP\nspecific detectors.", "published": "2022-04-17 17:47:47", "link": "http://arxiv.org/abs/2204.10192v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unsupervised Cross-Task Generalization via Retrieval Augmentation", "abstract": "Humans can perform unseen tasks by recalling relevant skills acquired\npreviously and then generalizing them to the target tasks, even if there is no\nsupervision at all. In this paper, we aim to improve this kind of cross-task\ngeneralization ability of massive multi-task language models, such as T0 and\nFLAN, in an unsupervised setting. We propose a retrieval-augmentation method\nnamed ReCross that takes a few unlabelled examples as queries to retrieve a\nsmall subset of upstream data and uses them to update the multi-task model for\nbetter generalization. ReCross is a straightforward yet effective retrieval\nmethod that combines both efficient dense retrieval and effective pair-wise\nreranking. Our results and analysis show that it significantly outperforms both\nnon-retrieval methods and other baseline methods.", "published": "2022-04-17 06:05:13", "link": "http://arxiv.org/abs/2204.07937v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Vision-Language Pre-Training for Multimodal Aspect-Based Sentiment\n  Analysis", "abstract": "As an important task in sentiment analysis, Multimodal Aspect-Based Sentiment\nAnalysis (MABSA) has attracted increasing attention in recent years. However,\nprevious approaches either (i) use separately pre-trained visual and textual\nmodels, which ignore the crossmodal alignment or (ii) use vision-language\nmodels pre-trained with general pre-training tasks, which are inadequate to\nidentify finegrained aspects, opinions, and their alignments across modalities.\nTo tackle these limitations, we propose a task-specific Vision-Language\nPre-training framework for MABSA (VLPMABSA), which is a unified multimodal\nencoder-decoder architecture for all the pretraining and downstream tasks. We\nfurther design three types of task-specific pre-training tasks from the\nlanguage, vision, and multimodal modalities, respectively. Experimental results\nshow that our approach generally outperforms the state-of-the-art approaches on\nthree MABSA subtasks. Further analysis demonstrates the effectiveness of each\npretraining task. The source code is publicly released at\nhttps://github.com/NUSTM/VLP-MABSA.", "published": "2022-04-17 08:44:00", "link": "http://arxiv.org/abs/2204.07955v2", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "A Psycho-linguistic Analysis of BitChute", "abstract": "In order to better support researchers, journalist, and practitioners in\ntheir use of the MeLa-BitChute dataset for exploration and investigative\nreporting, we provide new psycho-linguistic metadata for the videos, comments,\nand channels in the dataset using LIWC22. This paper describes that metadata\nand methods to filter the data using the metadata. In addition, we provide\nbasic analysis and comparison of the language on BitChute to other social media\nplatforms. The MeLa-BitChute dataset and LIWC metadata described in this paper\ncan be found at:\nhttps://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/KRD1VS.", "published": "2022-04-17 20:10:02", "link": "http://arxiv.org/abs/2204.08078v2", "categories": ["cs.CY", "cs.CL", "cs.SI"], "primary_category": "cs.CY"}
{"title": "Monte Carlo Tree Search for Interpreting Stress in Natural Language", "abstract": "Natural language processing can facilitate the analysis of a person's mental\nstate from text they have written. Previous studies have developed models that\ncan predict whether a person is experiencing a mental health condition from\nsocial media posts with high accuracy. Yet, these models cannot explain why the\nperson is experiencing a particular mental state. In this work, we present a\nnew method for explaining a person's mental state from text using Monte Carlo\ntree search (MCTS). Our MCTS algorithm employs trained classification models to\nguide the search for key phrases that explain the writer's mental state in a\nconcise, interpretable manner. Furthermore, our algorithm can find both\nexplanations that depend on the particular context of the text (e.g., a recent\nbreakup) and those that are context-independent. Using a dataset of Reddit\nposts that exhibit stress, we demonstrate the ability of our MCTS algorithm to\nidentify interpretable explanations for a person's feeling of stress in both a\ncontext-dependent and context-independent manner.", "published": "2022-04-17 23:06:01", "link": "http://arxiv.org/abs/2204.08105v1", "categories": ["cs.CL", "cs.IT", "math.IT"], "primary_category": "cs.CL"}
{"title": "Advances in Thunder Sound Synthesis", "abstract": "A recent comparative study evaluated all known thunder synthesis techniques\nin terms of their perceptual realness. The findings concluded that none of the\nsynthesised audio extracts seemed as realistic as the genuine phenomenon. The\nwork presented herein is motivated by those findings, and attempts to create a\nsynthesised sound effect of thunder indistinguishable from a real recording.\nThe technique supplements an existing implementation with physics-inspired,\nsignal-based design elements intended to simulate environmental occurrences. In\na listening test conducted with over 50 participants, this new implementation\nwas perceived as the most realistic synthesised sound, though still\ndistinguishable from a real recording. Further improvements to the model, based\non insights from the listening test, were also implemented and described\nherein.", "published": "2022-04-17 14:57:00", "link": "http://arxiv.org/abs/2204.08026v1", "categories": ["cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "A Data-Driven Methodology for Considering Feasibility and Pairwise\n  Likelihood in Deep Learning Based Guitar Tablature Transcription Systems", "abstract": "Guitar tablature transcription is an important but understudied problem\nwithin the field of music information retrieval. Traditional signal processing\napproaches offer only limited performance on the task, and there is little\nacoustic data with transcription labels for training machine learning models.\nHowever, guitar transcription labels alone are more widely available in the\nform of tablature, which is commonly shared among guitarists online. In this\nwork, a collection of symbolic tablature is leveraged to estimate the pairwise\nlikelihood of notes on the guitar. The output layer of a baseline tablature\ntranscription model is reformulated, such that an inhibition loss can be\nincorporated to discourage the co-activation of unlikely note pairs. This\nnaturally enforces playability constraints for guitar, and yields tablature\nwhich is more consistent with the symbolic data used to estimate pairwise\nlikelihoods. With this methodology, we show that symbolic tablature can be used\nto shape the distribution of a tablature transcription model's predictions,\neven when little acoustic data is available.", "published": "2022-04-17 22:10:37", "link": "http://arxiv.org/abs/2204.08094v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
