{"title": "A Deep Neural Network for Chinese Zero Pronoun Resolution", "abstract": "Existing approaches for Chinese zero pronoun resolution overlook semantic\ninformation. This is because zero pronouns have no descriptive information,\nwhich results in difficulty in explicitly capturing their semantic similarities\nwith antecedents. Moreover, when dealing with candidate antecedents,\ntraditional systems simply take advantage of the local information of a single\ncandidate antecedent while failing to consider the underlying information\nprovided by the other candidates from a global perspective. To address these\nweaknesses, we propose a novel zero pronoun-specific neural network, which is\ncapable of representing zero pronouns by utilizing the contextual information\nat the semantic level. In addition, when dealing with candidate antecedents, a\ntwo-level candidate encoder is employed to explicitly capture both the local\nand global information of candidate antecedents. We conduct experiments on the\nChinese portion of the OntoNotes 5.0 corpus. Experimental results show that our\napproach substantially outperforms the state-of-the-art method in various\nexperimental settings.", "published": "2016-04-20 03:03:12", "link": "http://arxiv.org/abs/1604.05800v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dialog-based Language Learning", "abstract": "A long-term goal of machine learning research is to build an intelligent\ndialog agent. Most research in natural language understanding has focused on\nlearning from fixed training sets of labeled data, with supervision either at\nthe word level (tagging, parsing tasks) or sentence level (question answering,\nmachine translation). This kind of supervision is not realistic of how humans\nlearn, where language is both learned by, and used for, communication. In this\nwork, we study dialog-based language learning, where supervision is given\nnaturally and implicitly in the response of the dialog partner during the\nconversation. We study this setup in two domains: the bAbI dataset of (Weston\net al., 2015) and large-scale question answering from (Dodge et al., 2015). We\nevaluate a set of baseline learning strategies on these tasks, and show that a\nnovel model incorporating predictive lookahead is a promising approach for\nlearning from a teacher's response. In particular, a surprising result is that\nit can learn to answer questions correctly without any reward-based supervision\nat all.", "published": "2016-04-20 18:06:49", "link": "http://arxiv.org/abs/1604.06045v7", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Speaker Cluster-Based Speaker Adaptive Training for Deep Neural Network\n  Acoustic Modeling", "abstract": "A speaker cluster-based speaker adaptive training (SAT) method under deep\nneural network-hidden Markov model (DNN-HMM) framework is presented in this\npaper. During training, speakers that are acoustically adjacent to each other\nare hierarchically clustered using an i-vector based distance metric. DNNs with\nspeaker dependent layers are then adaptively trained for each cluster of\nspeakers. Before decoding starts, an unseen speaker in test set is matched to\nthe closest speaker cluster through comparing i-vector based distances. The\npreviously trained DNN of the matched speaker cluster is used for decoding\nutterances of the test speaker. The performance of the proposed method on a\nlarge vocabulary spontaneous speech recognition task is evaluated on a training\nset of with 1500 hours of speech, and a test set of 24 speakers with 1774\nutterances. Comparing to a speaker independent DNN with a baseline word error\nrate of 11.6%, a relative 6.8% reduction in word error rate is observed from\nthe proposed method.", "published": "2016-04-20 20:10:41", "link": "http://arxiv.org/abs/1604.06113v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Distributed Entity Disambiguation with Per-Mention Learning", "abstract": "Entity disambiguation, or mapping a phrase to its canonical representation in\na knowledge base, is a fundamental step in many natural language processing\napplications. Existing techniques based on global ranking models fail to\ncapture the individual peculiarities of the words and hence, either struggle to\nmeet the accuracy requirements of many real-world applications or they are too\ncomplex to satisfy real-time constraints of applications.\n  In this paper, we propose a new disambiguation system that learns specialized\nfeatures and models for disambiguating each ambiguous phrase in the English\nlanguage. To train and validate the hundreds of thousands of learning models\nfor this purpose, we use a Wikipedia hyperlink dataset with more than 170\nmillion labelled annotations. We provide an extensive experimental evaluation\nto show that the accuracy of our approach compares favourably with respect to\nmany state-of-the-art disambiguation systems. The training required for our\napproach can be easily distributed over a cluster. Furthermore, updating our\nsystem for new entities or calibrating it for special ones is a computationally\nfast process, that does not affect the disambiguation of the other entities.", "published": "2016-04-20 09:53:42", "link": "http://arxiv.org/abs/1604.05875v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Question Answering via Integer Programming over Semi-Structured\n  Knowledge", "abstract": "Answering science questions posed in natural language is an important AI\nchallenge. Answering such questions often requires non-trivial inference and\nknowledge that goes beyond factoid retrieval. Yet, most systems for this task\nare based on relatively shallow Information Retrieval (IR) and statistical\ncorrelation techniques operating on large unstructured corpora. We propose a\nstructured inference system for this task, formulated as an Integer Linear\nProgram (ILP), that answers natural language questions using a semi-structured\nknowledge base derived from text, including questions requiring multi-step\ninference and a combination of multiple facts. On a dataset of real, unseen\nscience questions, our system significantly outperforms (+14%) the best\nprevious attempt at structured reasoning for this task, which used Markov Logic\nNetworks (MLNs). It also improves upon a previous ILP formulation by 17.7%.\nWhen combined with unstructured inference methods, the ILP system significantly\nboosts overall performance (+10%). Finally, we show our approach is\nsubstantially more robust to a simple answer perturbation compared to\nstatistical correlation methods.", "published": "2016-04-20 19:48:07", "link": "http://arxiv.org/abs/1604.06076v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "What we write about when we write about causality: Features of causal\n  statements across large-scale social discourse", "abstract": "Identifying and communicating relationships between causes and effects is\nimportant for understanding our world, but is affected by language structure,\ncognitive and emotional biases, and the properties of the communication medium.\nDespite the increasing importance of social media, much remains unknown about\ncausal statements made online. To study real-world causal attribution, we\nextract a large-scale corpus of causal statements made on the Twitter social\nnetwork platform as well as a comparable random control corpus. We compare\ncausal and control statements using statistical language and sentiment analysis\ntools. We find that causal statements have a number of significant lexical and\ngrammatical differences compared with controls and tend to be more negative in\nsentiment than controls. Causal statements made online tend to focus on news\nand current events, medicine and health, or interpersonal relationships, as\nshown by topic models. By quantifying the features and potential biases of\ncausality communication, this study improves our understanding of the accuracy\nof information and opinions found online.", "published": "2016-04-20 01:06:50", "link": "http://arxiv.org/abs/1604.05781v2", "categories": ["cs.CY", "cs.CL", "cs.SI"], "primary_category": "cs.CY"}
{"title": "A Factorization Machine Framework for Testing Bigram Embeddings in\n  Knowledgebase Completion", "abstract": "Embedding-based Knowledge Base Completion models have so far mostly combined\ndistributed representations of individual entities or relations to compute\ntruth scores of missing links. Facts can however also be represented using\npairwise embeddings, i.e. embeddings for pairs of entities and relations. In\nthis paper we explore such bigram embeddings with a flexible Factorization\nMachine model and several ablations from it. We investigate the relevance of\nvarious bigram types on the fb15k237 dataset and find relative improvements\ncompared to a compositional model.", "published": "2016-04-20 09:58:56", "link": "http://arxiv.org/abs/1604.05878v1", "categories": ["cs.CL", "cs.AI", "cs.NE", "stat.ML"], "primary_category": "cs.CL"}
