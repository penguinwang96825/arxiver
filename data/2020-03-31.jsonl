{"title": "Procedural Reading Comprehension with Attribute-Aware Context Flow", "abstract": "Procedural texts often describe processes (e.g., photosynthesis and cooking)\nthat happen over entities (e.g., light, food). In this paper, we introduce an\nalgorithm for procedural reading comprehension by translating the text into a\ngeneral formalism that represents processes as a sequence of transitions over\nentity attributes (e.g., location, temperature). Leveraging pre-trained\nlanguage models, our model obtains entity-aware and attribute-aware\nrepresentations of the text by joint prediction of entity attributes and their\ntransitions. Our model dynamically obtains contextual encodings of the\nprocedural text exploiting information that is encoded about previous and\ncurrent states to predict the transition of a certain attribute which can be\nidentified as a span of text or from a pre-defined set of classes. Moreover,\nour model achieves state of the art results on two procedural reading\ncomprehension datasets, namely ProPara and npn-cooking", "published": "2020-03-31 00:06:29", "link": "http://arxiv.org/abs/2003.13878v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MULTEXT-East", "abstract": "MULTEXT-East language resources, a multilingual dataset for language\nengineering research, focused on the morphosyntactic level of linguistic\ndescription. The MULTEXT-East dataset includes the EAGLES-based morphosyntactic\nspecifications, morphosyntactic lexicons, and an annotated multilingual\ncorpora. The parallel corpus, the novel \"1984\" by George Orwell, is sentence\naligned and contains hand-validated morphosyntactic descriptions and lemmas.\nThe resources are uniformly encoded in XML, using the Text Encoding Initiative\nGuidelines, TEI P5, and cover 16 languages: Bulgarian, Croatian, Czech,\nEnglish, Estonian, Hungarian, Macedonian, Persian, Polish, Resian, Romanian,\nRussian, Serbian, Slovak, Slovene, and Ukrainian. This dataset is extensively\ndocumented, and freely available for research purposes. This case study gives a\nhistory of the development of the MULTEXT-East resources, presents their\nencoding and components, discusses related work and gives some conclusions.", "published": "2020-03-31 08:45:52", "link": "http://arxiv.org/abs/2003.14026v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Appraisal Theories for Emotion Classification in Text", "abstract": "Automatic emotion categorization has been predominantly formulated as text\nclassification in which textual units are assigned to an emotion from a\npredefined inventory, for instance following the fundamental emotion classes\nproposed by Paul Ekman (fear, joy, anger, disgust, sadness, surprise) or Robert\nPlutchik (adding trust, anticipation). This approach ignores existing\npsychological theories to some degree, which provide explanations regarding the\nperception of events. For instance, the description that somebody discovers a\nsnake is associated with fear, based on the appraisal as being an unpleasant\nand non-controllable situation. This emotion reconstruction is even possible\nwithout having access to explicit reports of a subjective feeling (for instance\nexpressing this with the words \"I am afraid.\"). Automatic classification\napproaches therefore need to learn properties of events as latent variables\n(for instance that the uncertainty and the mental or physical effort associated\nwith the encounter of a snake leads to fear). With this paper, we propose to\nmake such interpretations of events explicit, following theories of cognitive\nappraisal of events, and show their potential for emotion classification when\nbeing encoded in classification models. Our results show that high quality\nappraisal dimension assignments in event descriptions lead to an improvement in\nthe classification of discrete emotion categories. We make our corpus of\nappraisal-annotated emotion-associated event descriptions publicly available.", "published": "2020-03-31 12:43:54", "link": "http://arxiv.org/abs/2003.14155v6", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Inherent Dependency Displacement Bias of Transition-Based Algorithms", "abstract": "A wide variety of transition-based algorithms are currently used for\ndependency parsers. Empirical studies have shown that performance varies across\ndifferent treebanks in such a way that one algorithm outperforms another on one\ntreebank and the reverse is true for a different treebank. There is often no\ndiscernible reason for what causes one algorithm to be more suitable for a\ncertain treebank and less so for another. In this paper we shed some light on\nthis by introducing the concept of an algorithm's inherent dependency\ndisplacement distribution. This characterises the bias of the algorithm in\nterms of dependency displacement, which quantify both distance and direction of\nsyntactic relations. We show that the similarity of an algorithm's inherent\ndistribution to a treebank's displacement distribution is clearly correlated to\nthe algorithm's parsing performance on that treebank, specifically with highly\nsignificant and substantial correlations for the predominant sentence lengths\nin Universal Dependency treebanks. We also obtain results which show a more\ndiscrete analysis of dependency displacement does not result in any meaningful\ncorrelations.", "published": "2020-03-31 15:11:12", "link": "http://arxiv.org/abs/2003.14282v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Amharic Machine Translation", "abstract": "Machine translation (MT) systems are now able to provide very accurate\nresults for high resource language pairs. However, for many low resource\nlanguages, MT is still under active research. In this paper, we develop and\nshare a dataset to automatically evaluate the quality of MT systems for\nAmharic. We compare two commercially available MT systems that support\ntranslation of Amharic to and from English to assess the current state of MT\nfor Amharic. The BLEU score results show that the results for Amharic\ntranslation are promising but still low. We hope that this dataset will be\nuseful to the research community both in academia and industry as a benchmark\nto evaluate Amharic MT systems.", "published": "2020-03-31 17:30:08", "link": "http://arxiv.org/abs/2003.14386v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Low Resource Neural Machine Translation: A Benchmark for Five African\n  Languages", "abstract": "Recent advents in Neural Machine Translation (NMT) have shown improvements in\nlow-resource language (LRL) translation tasks. In this work, we benchmark NMT\nbetween English and five African LRL pairs (Swahili, Amharic, Tigrigna, Oromo,\nSomali [SATOS]). We collected the available resources on the SATOS languages to\nevaluate the current state of NMT for LRLs. Our evaluation, comparing a\nbaseline single language pair NMT model against semi-supervised learning,\ntransfer learning, and multilingual modeling, shows significant performance\nimprovements both in the En-LRL and LRL-En directions. In terms of averaged\nBLEU score, the multilingual approach shows the largest gains, up to +5 points,\nin six out of ten translation directions. To demonstrate the generalization\ncapability of each model, we also report results on multi-domain test sets. We\nrelease the standardized experimental data and the test sets for future works\naddressing the challenges of NMT in under-resourced settings, in particular for\nthe SATOS languages.", "published": "2020-03-31 17:50:07", "link": "http://arxiv.org/abs/2003.14402v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Give your Text Representation Models some Love: the Case for Basque", "abstract": "Word embeddings and pre-trained language models allow to build rich\nrepresentations of text and have enabled improvements across most NLP tasks.\nUnfortunately they are very expensive to train, and many small companies and\nresearch groups tend to use models that have been pre-trained and made\navailable by third parties, rather than building their own. This is suboptimal\nas, for many languages, the models have been trained on smaller (or lower\nquality) corpora. In addition, monolingual pre-trained models for non-English\nlanguages are not always available. At best, models for those languages are\nincluded in multilingual versions, where each language shares the quota of\nsubstrings and parameters with the rest of the languages. This is particularly\ntrue for smaller languages such as Basque. In this paper we show that a number\nof monolingual models (FastText word embeddings, FLAIR and BERT language\nmodels) trained with larger Basque corpora produce much better results than\npublicly available versions in downstream NLP tasks, including topic\nclassification, sentiment classification, PoS tagging and NER. This work sets a\nnew state-of-the-art in those tasks for Basque. All benchmarks and models used\nin this work are publicly available.", "published": "2020-03-31 18:01:56", "link": "http://arxiv.org/abs/2004.00033v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Stance Detection: The Catalonia Independence Corpus", "abstract": "Stance detection aims to determine the attitude of a given text with respect\nto a specific topic or claim. While stance detection has been fairly well\nresearched in the last years, most the work has been focused on English. This\nis mainly due to the relative lack of annotated data in other languages. The\nTW-10 Referendum Dataset released at IberEval 2018 is a previous effort to\nprovide multilingual stance-annotated data in Catalan and Spanish.\nUnfortunately, the TW-10 Catalan subset is extremely imbalanced. This paper\naddresses these issues by presenting a new multilingual dataset for stance\ndetection in Twitter for the Catalan and Spanish languages, with the aim of\nfacilitating research on stance detection in multilingual and cross-lingual\nsettings. The dataset is annotated with stance towards one topic, namely, the\nindependence of Catalonia. We also provide a semi-automatic method to annotate\nthe dataset based on a categorization of Twitter users. We experiment on the\nnew corpus with a number of supervised approaches, including linear classifiers\nand deep learning methods. Comparison of our new corpus with the with the TW-1O\ndataset shows both the benefits and potential of a well balanced corpus for\nmultilingual and cross-lingual research on stance detection. Finally, we\nestablish new state-of-the-art results on the TW-10 dataset, both for Catalan\nand Spanish.", "published": "2020-03-31 18:28:36", "link": "http://arxiv.org/abs/2004.00050v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assessing Human Translations from French to Bambara for Machine\n  Learning: a Pilot Study", "abstract": "We present novel methods for assessing the quality of human-translated\naligned texts for learning machine translation models of under-resourced\nlanguages. Malian university students translated French texts, producing either\nwritten or oral translations to Bambara. Our results suggest that similar\nquality can be obtained from either written or spoken translations for certain\nkinds of texts. They also suggest specific instructions that human translators\nshould be given in order to improve the quality of their work.", "published": "2020-03-31 19:28:35", "link": "http://arxiv.org/abs/2004.00068v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Extraction of Bengali Root Verbs using Paninian Grammar", "abstract": "In this research work, we have proposed an algorithm based on supervised\nlearning methodology to extract the root forms of the Bengali verbs using the\ngrammatical rules proposed by Panini [1] in Ashtadhyayi. This methodology can\nbe applied for the languages which are derived from Sanskrit. The proposed\nsystem has been developed based on tense, person and morphological inflections\nof the verbs to find their root forms. The work has been executed in two\nphases: first, the surface level forms or inflected forms of the verbs have\nbeen classified into a certain number of groups of similar tense and person.\nFor this task, a standard pattern, available in Bengali language has been used.\nNext, a set of rules have been applied to extract the root form from the\nsurface level forms of a verb. The system has been tested on 10000 verbs\ncollected from the Bengali text corpus developed in the TDIL project of the\nGovt. of India. The accuracy of the output has been achieved 98% which is\nverified by a linguistic expert. Root verb identification is a key step in\nsemantic searching, multi-sentence search query processing, understanding the\nmeaning of a language, disambiguation of word sense, classification of the\nsentences etc.", "published": "2020-03-31 20:22:10", "link": "http://arxiv.org/abs/2004.00089v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Swiss German Dictionary: Variation in Speech and Writing", "abstract": "We introduce a dictionary containing forms of common words in various Swiss\nGerman dialects normalized into High German. As Swiss German is, for now, a\npredominantly spoken language, there is a significant variation in the written\nforms, even between speakers of the same dialect. To alleviate the uncertainty\nassociated with this diversity, we complement the pairs of Swiss German - High\nGerman words with the Swiss German phonetic transcriptions (SAMPA). This\ndictionary becomes thus the first resource to combine large-scale spontaneous\ntranslation with phonetic transcriptions. Moreover, we control for the regional\ndistribution and insure the equal representation of the major Swiss dialects.\nThe coupling of the phonetic and written Swiss German forms is powerful. We\nshow that they are sufficient to train a Transformer-based phoneme to grapheme\nmodel that generates credible novel Swiss German writings. In addition, we show\nthat the inverse mapping - from graphemes to phonemes - can be modeled with a\ntransformer trained with the novel dictionary. This generation of\npronunciations for previously unknown words is key in training extensible\nautomated speech recognition (ASR) systems, which are key beneficiaries of this\ndictionary.", "published": "2020-03-31 22:10:43", "link": "http://arxiv.org/abs/2004.00139v1", "categories": ["cs.CL", "68T50, 68T10", "A.2; I.2.7; J.5"], "primary_category": "cs.CL"}
{"title": "SPARQA: Skeleton-based Semantic Parsing for Complex Questions over\n  Knowledge Bases", "abstract": "Semantic parsing transforms a natural language question into a formal query\nover a knowledge base. Many existing methods rely on syntactic parsing like\ndependencies. However, the accuracy of producing such expressive formalisms is\nnot satisfying on long complex questions. In this paper, we propose a novel\nskeleton grammar to represent the high-level structure of a complex question.\nThis dedicated coarse-grained formalism with a BERT-based parsing algorithm\nhelps to improve the accuracy of the downstream fine-grained semantic parsing.\nBesides, to align the structure of a question with the structure of a knowledge\nbase, our multi-strategy method combines sentence-level and word-level\nsemantics. Our approach shows promising performance on several datasets.", "published": "2020-03-31 05:12:31", "link": "http://arxiv.org/abs/2003.13956v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Understanding Cross-Lingual Syntactic Transfer in Multilingual Recurrent\n  Neural Networks", "abstract": "It is now established that modern neural language models can be successfully\ntrained on multiple languages simultaneously without changes to the underlying\narchitecture. But what kind of knowledge is really shared among languages\nwithin these models? Does multilingual training mostly lead to an alignment of\nthe lexical representation spaces or does it also enable the sharing of purely\ngrammatical knowledge? In this paper we dissect different forms of\ncross-lingual transfer and look for its most determining factors, using a\nvariety of models and probing tasks. We find that exposing our LMs to a related\nlanguage does not always increase grammatical knowledge in the target language,\nand that optimal conditions for lexical-semantic transfer may not be optimal\nfor syntactic transfer.", "published": "2020-03-31 09:48:25", "link": "http://arxiv.org/abs/2003.14056v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Graph Enhanced Representation Learning for News Recommendation", "abstract": "With the explosion of online news, personalized news recommendation becomes\nincreasingly important for online news platforms to help their users find\ninteresting information. Existing news recommendation methods achieve\npersonalization by building accurate news representations from news content and\nuser representations from their direct interactions with news (e.g., click),\nwhile ignoring the high-order relatedness between users and news. Here we\npropose a news recommendation method which can enhance the representation\nlearning of users and news by modeling their relatedness in a graph setting. In\nour method, users and news are both viewed as nodes in a bipartite graph\nconstructed from historical user click behaviors. For news representations, a\ntransformer architecture is first exploited to build news semantic\nrepresentations. Then we combine it with the information from neighbor news in\nthe graph via a graph attention network. For user representations, we not only\nrepresent users from their historically clicked news, but also attentively\nincorporate the representations of their neighbor users in the graph. Improved\nperformances on a large-scale real-world dataset validate the effectiveness of\nour proposed method.", "published": "2020-03-31 15:27:31", "link": "http://arxiv.org/abs/2003.14292v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "On the Integration of LinguisticFeatures into Statistical and Neural\n  Machine Translation", "abstract": "New machine translations (MT) technologies are emerging rapidly and with\nthem, bold claims of achieving human parity such as: (i) the results produced\napproach \"accuracy achieved by average bilingual human translators\" (Wu et al.,\n2017b) or (ii) the \"translation quality is at human parity when compared to\nprofessional human translators\" (Hassan et al., 2018) have seen the light of\nday (Laubli et al., 2018). Aside from the fact that many of these papers craft\ntheir own definition of human parity, these sensational claims are often not\nsupported by a complete analysis of all aspects involved in translation.\nEstablishing the discrepancies between the strengths of statistical approaches\nto MT and the way humans translate has been the starting point of our research.\nBy looking at MT output and linguistic theory, we were able to identify some\nremaining issues. The problems range from simple number and gender agreement\nerrors to more complex phenomena such as the correct translation of aspectual\nvalues and tenses. Our experiments confirm, along with other studies\n(Bentivogli et al., 2016), that neural MT has surpassed statistical MT in many\naspects. However, some problems remain and others have emerged. We cover a\nseries of problems related to the integration of specific linguistic features\ninto statistical and neural MT, aiming to analyse and provide a solution to\nsome of them. Our work focuses on addressing three main research questions that\nrevolve around the complex relationship between linguistics and MT in general.\nWe identify linguistic information that is lacking in order for automatic\ntranslation systems to produce more accurate translations and integrate\nadditional features into the existing pipelines. We identify overgeneralization\nor 'algorithmic bias' as a potential drawback of neural MT and link it to many\nof the remaining linguistic issues.", "published": "2020-03-31 16:03:38", "link": "http://arxiv.org/abs/2003.14324v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Clustering Framework for Lexical Normalization of Roman Urdu", "abstract": "Roman Urdu is an informal form of the Urdu language written in Roman script,\nwhich is widely used in South Asia for online textual content. It lacks\nstandard spelling and hence poses several normalization challenges during\nautomatic language processing. In this article, we present a feature-based\nclustering framework for the lexical normalization of Roman Urdu corpora, which\nincludes a phonetic algorithm UrduPhone, a string matching component, a\nfeature-based similarity function, and a clustering algorithm Lex-Var.\nUrduPhone encodes Roman Urdu strings to their pronunciation-based\nrepresentations. The string matching component handles character-level\nvariations that occur when writing Urdu using Roman script.", "published": "2020-03-31 20:21:55", "link": "http://arxiv.org/abs/2004.00088v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improvement of electronic Governance and mobile Governance in\n  Multilingual Countries with Digital Etymology using Sanskrit Grammar", "abstract": "With huge improvement of digital connectivity (Wifi,3G,4G) and digital\ndevices access to internet has reached in the remotest corners now a days.\nRural people can easily access web or apps from PDAs, laptops, smartphones etc.\nThis is an opportunity of the Government to reach to the citizen in large\nnumber, get their feedback, associate them in policy decision with e governance\nwithout deploying huge man, material or resourses. But the Government of\nmultilingual countries face a lot of problem in successful implementation of\nGovernment to Citizen (G2C) and Citizen to Government (C2G) governance as the\nrural people tend and prefer to interact in their native languages. Presenting\nequal experience over web or app to different language group of speakers is a\nreal challenge. In this research we have sorted out the problems faced by Indo\nAryan speaking netizens which is in general also applicable to any language\nfamily groups or subgroups. Then we have tried to give probable solutions using\nEtymology. Etymology is used to correlate the words using their ROOT forms. In\n5th century BC Panini wrote Astadhyayi where he depicted sutras or rules -- how\na word is changed according to person,tense,gender,number etc. Later this book\nwas followed in Western countries also to derive their grammar of comparatively\nnew languages. We have trained our system for automatic root extraction from\nthe surface level or morphed form of words using Panian Gramatical rules. We\nhave tested our system over 10000 bengali Verbs and extracted the root form\nwith 98% accuracy. We are now working to extend the program to successfully\nlemmatize any words of any language and correlate them by applying those rule\nsets in Artificial Neural Network.", "published": "2020-03-31 20:58:14", "link": "http://arxiv.org/abs/2004.00104v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "DeepSumm -- Deep Code Summaries using Neural Transformer Architecture", "abstract": "Source code summarizing is a task of writing short, natural language\ndescriptions of source code behavior during run time. Such summaries are\nextremely useful for software development and maintenance but are expensive to\nmanually author,hence it is done for small fraction of the code that is\nproduced and is often ignored. Automatic code documentation can possibly solve\nthis at a low cost. This is thus an emerging research field with further\napplications to program comprehension, and software maintenance. Traditional\nmethods often relied on cognitive models that were built in the form of\ntemplates and by heuristics and had varying degree of adoption by the developer\ncommunity. But with recent advancements, end to end data-driven approaches\nbased on neural techniques have largely overtaken the traditional techniques.\nMuch of the current landscape employs neural translation based architectures\nwith recurrence and attention which is resource and time intensive training\nprocedure. In this paper, we employ neural techniques to solve the task of\nsource code summarizing and specifically compare NMT based techniques to more\nsimplified and appealing Transformer architecture on a dataset of Java methods\nand comments. We bring forth an argument to dispense the need of recurrence in\nthe training procedure. To the best of our knowledge, transformer based models\nhave not been used for the task before. With supervised samples of more than\n2.1m comments and code, we reduce the training time by more than 50% and\nachieve the BLEU score of 17.99 for the test set of examples.", "published": "2020-03-31 22:43:29", "link": "http://arxiv.org/abs/2004.00998v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Information Leakage in Embedding Models", "abstract": "Embeddings are functions that map raw input data to low-dimensional vector\nrepresentations, while preserving important semantic information about the\ninputs. Pre-training embeddings on a large amount of unlabeled data and\nfine-tuning them for downstream tasks is now a de facto standard in achieving\nstate of the art learning in many domains.\n  We demonstrate that embeddings, in addition to encoding generic semantics,\noften also present a vector that leaks sensitive information about the input\ndata. We develop three classes of attacks to systematically study information\nthat might be leaked by embeddings. First, embedding vectors can be inverted to\npartially recover some of the input data. As an example, we show that our\nattacks on popular sentence embeddings recover between 50\\%--70\\% of the input\nwords (F1 scores of 0.5--0.7). Second, embeddings may reveal sensitive\nattributes inherent in inputs and independent of the underlying semantic task\nat hand. Attributes such as authorship of text can be easily extracted by\ntraining an inference model on just a handful of labeled embedding vectors.\nThird, embedding models leak moderate amount of membership information for\ninfrequent training data inputs. We extensively evaluate our attacks on various\nstate-of-the-art embedding models in the text domain. We also propose and\nevaluate defenses that can prevent the leakage to some extent at a minor cost\nin utility.", "published": "2020-03-31 18:33:36", "link": "http://arxiv.org/abs/2004.00053v2", "categories": ["cs.LG", "cs.CL", "cs.CR", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Unification-based Reconstruction of Multi-hop Explanations for Science\n  Questions", "abstract": "This paper presents a novel framework for reconstructing multi-hop\nexplanations in science Question Answering (QA). While existing approaches for\nmulti-hop reasoning build explanations considering each question in isolation,\nwe propose a method to leverage explanatory patterns emerging in a corpus of\nscientific explanations. Specifically, the framework ranks a set of atomic\nfacts by integrating lexical relevance with the notion of unification power,\nestimated analysing explanations for similar questions in the corpus.\n  An extensive evaluation is performed on the Worldtree corpus, integrating\nk-NN clustering and Information Retrieval (IR) techniques. We present the\nfollowing conclusions: (1) The proposed method achieves results competitive\nwith Transformers, yet being orders of magnitude faster, a feature that makes\nit scalable to large explanatory corpora (2) The unification-based mechanism\nhas a key role in reducing semantic drift, contributing to the reconstruction\nof many hops explanations (6 or more facts) and the ranking of complex\ninference facts (+12.0 Mean Average Precision) (3) Crucially, the constructed\nexplanations can support downstream QA models, improving the accuracy of BERT\nby up to 10% overall.", "published": "2020-03-31 19:07:51", "link": "http://arxiv.org/abs/2004.00061v2", "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.AI"}
{"title": "AM-MobileNet1D: A Portable Model for Speaker Recognition", "abstract": "Speaker Recognition and Speaker Identification are challenging tasks with\nessential applications such as automation, authentication, and security. Deep\nlearning approaches like SincNet and AM-SincNet presented great results on\nthese tasks. The promising performance took these models to real-world\napplications that becoming fundamentally end-user driven and mostly mobile. The\nmobile computation requires applications with reduced storage size,\nnon-processing and memory intensive and efficient energy-consuming. The deep\nlearning approaches, in contrast, usually are energy expensive, demanding\nstorage, processing power, and memory. To address this demand, we propose a\nportable model called Additive Margin MobileNet1D (AM-MobileNet1D) to Speaker\nIdentification on mobile devices. We evaluated the proposed approach on TIMIT\nand MIT datasets obtaining equivalent or better performances concerning the\nbaseline methods. Additionally, the proposed model takes only 11.6 megabytes on\ndisk storage against 91.2 from SincNet and AM-SincNet architectures, making the\nmodel seven times faster, with eight times fewer parameters.", "published": "2020-03-31 21:42:59", "link": "http://arxiv.org/abs/2004.00132v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Enriching Consumer Health Vocabulary Using Enhanced GloVe Word Embedding", "abstract": "Open-Access and Collaborative Consumer Health Vocabulary (OAC CHV, or CHV for\nshort), is a collection of medical terms written in plain English. It provides\na list of simple, easy, and clear terms that laymen prefer to use rather than\nan equivalent professional medical term. The National Library of Medicine (NLM)\nhas integrated and mapped the CHV terms to their Unified Medical Language\nSystem (UMLS). These CHV terms mapped to 56000 professional concepts on the\nUMLS. We found that about 48% of these laymen's terms are still jargon and\nmatched with the professional terms on the UMLS. In this paper, we present an\nenhanced word embedding technique that generates new CHV terms from a\nconsumer-generated text. We downloaded our corpus from a healthcare social\nmedia and evaluated our new method based on iterative feedback to word\nembedding using ground truth built from the existing CHV terms. Our feedback\nalgorithm outperformed unmodified GLoVe and new CHV terms have been detected.", "published": "2020-03-31 22:50:24", "link": "http://arxiv.org/abs/2004.00150v2", "categories": ["cs.CL", "cs.IR", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Deep Learning Approach for Intelligent Named Entity Recognition of Cyber\n  Security", "abstract": "In recent years, the amount of Cyber Security data generated in the form of\nunstructured texts, for example, social media resources, blogs, articles, and\nso on has exceptionally increased. Named Entity Recognition (NER) is an initial\nstep towards converting this unstructured data into structured data which can\nbe used by a lot of applications. The existing methods on NER for Cyber\nSecurity data are based on rules and linguistic characteristics. A Deep\nLearning (DL) based approach embedded with Conditional Random Fields (CRFs) is\nproposed in this paper. Several DL architectures are evaluated to find the most\noptimal architecture. The combination of Bidirectional Gated Recurrent Unit\n(Bi-GRU), Convolutional Neural Network (CNN), and CRF performed better compared\nto various other DL frameworks on a publicly available benchmark dataset. This\nmay be due to the reason that the bidirectional structures preserve the\nfeatures related to the future and previous words in a sequence.", "published": "2020-03-31 00:36:19", "link": "http://arxiv.org/abs/2004.00502v1", "categories": ["cs.CL", "cs.CR", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Characterizing Speech Adversarial Examples Using Self-Attention U-Net\n  Enhancement", "abstract": "Recent studies have highlighted adversarial examples as ubiquitous threats to\nthe deep neural network (DNN) based speech recognition systems. In this work,\nwe present a U-Net based attention model, U-Net$_{At}$, to enhance adversarial\nspeech signals. Specifically, we evaluate the model performance by\ninterpretable speech recognition metrics and discuss the model performance by\nthe augmented adversarial training. Our experiments show that our proposed\nU-Net$_{At}$ improves the perceptual evaluation of speech quality (PESQ) from\n1.13 to 2.78, speech transmission index (STI) from 0.65 to 0.75, short-term\nobjective intelligibility (STOI) from 0.83 to 0.96 on the task of speech\nenhancement with adversarial speech examples. We conduct experiments on the\nautomatic speech recognition (ASR) task with adversarial audio attacks. We find\nthat (i) temporal features learned by the attention network are capable of\nenhancing the robustness of DNN based ASR models; (ii) the generalization power\nof DNN based ASR model could be enhanced by applying adversarial training with\nan additive adversarial data augmentation. The ASR metric on word-error-rates\n(WERs) shows that there is an absolute 2.22 $\\%$ decrease under gradient-based\nperturbation, and an absolute 2.03 $\\%$ decrease, under evolutionary-optimized\nperturbation, which suggests that our enhancement models with adversarial\ntraining can further secure a resilient ASR system.", "published": "2020-03-31 02:16:34", "link": "http://arxiv.org/abs/2003.13917v2", "categories": ["eess.AS", "cs.CL", "cs.CR", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Deep Learning Approach for Enhanced Cyber Threat Indicators in Twitter\n  Stream", "abstract": "In recent days, the amount of Cyber Security text data shared via social\nmedia resources mainly Twitter has increased. An accurate analysis of this data\ncan help to develop cyber threat situational awareness framework for a cyber\nthreat. This work proposes a deep learning based approach for tweet data\nanalysis. To convert the tweets into numerical representations, various text\nrepresentations are employed. These features are feed into deep learning\narchitecture for optimal feature extraction as well as classification. Various\nhyperparameter tuning approaches are used for identifying optimal text\nrepresentation method as well as optimal network parameters and network\nstructures for deep learning models. For comparative analysis, the classical\ntext representation method with classical machine learning algorithm is\nemployed. From the detailed analysis of experiments, we found that the deep\nlearning architecture with advanced text representation methods performed\nbetter than the classical text representation and classical machine learning\nalgorithms. The primary reason for this is that the advanced text\nrepresentation methods have the capability to learn sequential properties which\nexist among the textual data and deep learning architectures learns the optimal\nfeatures along with decreasing the feature size.", "published": "2020-03-31 00:29:42", "link": "http://arxiv.org/abs/2004.00503v1", "categories": ["cs.CL", "cs.CR", "cs.LG", "cs.NE", "cs.SI"], "primary_category": "cs.CL"}
{"title": "A Comparison of Metric Learning Loss Functions for End-To-End Speaker\n  Verification", "abstract": "Despite the growing popularity of metric learning approaches, very little\nwork has attempted to perform a fair comparison of these techniques for speaker\nverification. We try to fill this gap and compare several metric learning loss\nfunctions in a systematic manner on the VoxCeleb dataset. The first family of\nloss functions is derived from the cross entropy loss (usually used for\nsupervised classification) and includes the congenerous cosine loss, the\nadditive angular margin loss, and the center loss. The second family of loss\nfunctions focuses on the similarity between training samples and includes the\ncontrastive loss and the triplet loss. We show that the additive angular margin\nloss function outperforms all other loss functions in the study, while learning\nmore robust representations. Based on a combination of SincNet trainable\nfeatures and the x-vector architecture, the network used in this paper brings\nus a step closer to a really-end-to-end speaker verification system, when\ncombined with the additive angular margin loss, while still being competitive\nwith the x-vector baseline. In the spirit of reproducible research, we also\nrelease open source Python code for reproducing our results, and share\npretrained PyTorch models on torch.hub that can be used either directly or\nafter fine-tuning.", "published": "2020-03-31 08:36:07", "link": "http://arxiv.org/abs/2003.14021v1", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
