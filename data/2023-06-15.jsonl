{"title": "Pragmatic Inference with a CLIP Listener for Contrastive Captioning", "abstract": "We propose a simple yet effective and robust method for contrastive\ncaptioning: generating discriminative captions that distinguish target images\nfrom very similar alternative distractor images. Our approach is built on a\npragmatic inference procedure that formulates captioning as a reference game\nbetween a speaker, which produces possible captions describing the target, and\na listener, which selects the target given the caption. Unlike previous methods\nthat derive both speaker and listener distributions from a single captioning\nmodel, we leverage an off-the-shelf CLIP model to parameterize the listener.\nCompared with captioner-only pragmatic models, our method benefits from rich\nvision language alignment representations from CLIP when reasoning over\ndistractors. Like previous methods for discriminative captioning, our method\nuses a hyperparameter to control the tradeoff between the informativity (how\nlikely captions are to allow a human listener to discriminate the target image)\nand the fluency of the captions. However, we find that our method is\nsubstantially more robust to the value of this hyperparameter than past\nmethods, which allows us to automatically optimize the captions for\ninformativity - outperforming past methods for discriminative captioning by 11%\nto 15% accuracy in human evaluations", "published": "2023-06-15 02:22:28", "link": "http://arxiv.org/abs/2306.08818v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BED: Bi-Encoder-Based Detectors for Out-of-Distribution Detection", "abstract": "This paper introduces a novel method leveraging bi-encoder-based detectors\nalong with a comprehensive study comparing different out-of-distribution (OOD)\ndetection methods in NLP using different feature extractors. The feature\nextraction stage employs popular methods such as Universal Sentence Encoder\n(USE), BERT, MPNET, and GLOVE to extract informative representations from\ntextual data. The evaluation is conducted on several datasets, including\nCLINC150, ROSTD-Coarse, SNIPS, and YELLOW. Performance is assessed using\nmetrics such as F1-Score, MCC, FPR@90, FPR@95, AUPR, an AUROC. The experimental\nresults demonstrate that the proposed bi-encoder-based detectors outperform\nother methods, both those that require OOD labels in training and those that do\nnot, across all datasets, showing great potential for OOD detection in NLP. The\nsimplicity of the training process and the superior detection performance make\nthem applicable to real-world scenarios. The presented methods and benchmarking\nmetrics serve as a valuable resource for future research in OOD detection,\nenabling further advancements in this field. The code and implementation\ndetails can be found on our GitHub repository:\nhttps://github.com/yellowmessenger/ood-detection.", "published": "2023-06-15 04:41:28", "link": "http://arxiv.org/abs/2306.08852v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Interleaving Pre-Trained Language Models and Large Language Models for\n  Zero-Shot NL2SQL Generation", "abstract": "Zero-shot NL2SQL is crucial in achieving natural language to SQL that is\nadaptive to new environments (e.g., new databases, new linguistic phenomena or\nSQL structures) with zero annotated NL2SQL samples from such environments.\nExisting approaches either fine-tune pre-trained language models (PLMs) based\non annotated data or use prompts to guide fixed large language models (LLMs)\nsuch as ChatGPT. PLMs can perform well in schema alignment but struggle to\nachieve complex reasoning, while LLMs is superior in complex reasoning tasks\nbut cannot achieve precise schema alignment. In this paper, we propose a\nZeroNL2SQL framework that combines the complementary advantages of PLMs and\nLLMs for supporting zero-shot NL2SQL. ZeroNL2SQL first uses PLMs to generate an\nSQL sketch via schema alignment, then uses LLMs to fill the missing information\nvia complex reasoning. Moreover, in order to better align the generated SQL\nqueries with values in the given database instances, we design a predicate\ncalibration method to guide the LLM in completing the SQL sketches based on the\ndatabase instances and select the optimal SQL query via an execution-based\nstrategy. Comprehensive experiments show that ZeroNL2SQL can achieve the best\nzero-shot NL2SQL performance on real-world benchmarks. Specifically, ZeroNL2SQL\noutperforms the state-of-the-art PLM-based methods by 3.2% to 13% and exceeds\nLLM-based methods by 10% to 20% on execution accuracy.", "published": "2023-06-15 06:50:51", "link": "http://arxiv.org/abs/2306.08891v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MetricPrompt: Prompting Model as a Relevance Metric for Few-shot Text\n  Classification", "abstract": "Prompting methods have shown impressive performance in a variety of text\nmining tasks and applications, especially few-shot ones. Despite the promising\nprospects, the performance of prompting model largely depends on the design of\nprompt template and verbalizer. In this work, we propose MetricPrompt, which\neases verbalizer design difficulty by reformulating few-shot text\nclassification task into text pair relevance estimation task. MetricPrompt\nadopts prompting model as the relevance metric, further bridging the gap\nbetween Pre-trained Language Model's (PLM) pre-training objective and text\nclassification task, making possible PLM's smooth adaption. Taking a training\nsample and a query one simultaneously, MetricPrompt captures cross-sample\nrelevance information for accurate relevance estimation. We conduct experiments\non three widely used text classification datasets across four few-shot\nsettings. Results show that MetricPrompt outperforms manual verbalizer and\nother automatic verbalizer design methods across all few-shot settings,\nachieving new state-of-the-art (SOTA) performance.", "published": "2023-06-15 06:51:35", "link": "http://arxiv.org/abs/2306.08892v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual End to End Entity Linking", "abstract": "Entity Linking is one of the most common Natural Language Processing tasks in\npractical applications, but so far efficient end-to-end solutions with\nmultilingual coverage have been lacking, leading to complex model stacks. To\nfill this gap, we release and open source BELA, the first fully end-to-end\nmultilingual entity linking model that efficiently detects and links entities\nin texts in any of 97 languages. We provide here a detailed description of the\nmodel and report BELA's performance on four entity linking datasets covering\nhigh- and low-resource languages.", "published": "2023-06-15 07:02:00", "link": "http://arxiv.org/abs/2306.08896v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bridging the Gap between Decision and Logits in Decision-based Knowledge\n  Distillation for Pre-trained Language Models", "abstract": "Conventional knowledge distillation (KD) methods require access to the\ninternal information of teachers, e.g., logits. However, such information may\nnot always be accessible for large pre-trained language models (PLMs). In this\nwork, we focus on decision-based KD for PLMs, where only teacher decisions\n(i.e., top-1 labels) are accessible. Considering the information gap between\nlogits and decisions, we propose a novel method to estimate logits from the\ndecision distributions. Specifically, decision distributions can be both\nderived as a function of logits theoretically and estimated with test-time data\naugmentation empirically. By combining the theoretical and empirical\nestimations of the decision distributions together, the estimation of logits\ncan be successfully reduced to a simple root-finding problem. Extensive\nexperiments show that our method significantly outperforms strong baselines on\nboth natural language understanding and machine reading comprehension datasets.", "published": "2023-06-15 07:23:44", "link": "http://arxiv.org/abs/2306.08909v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rethinking Document-Level Relation Extraction: A Reality Check", "abstract": "Recently, numerous efforts have continued to push up performance boundaries\nof document-level relation extraction (DocRE) and have claimed significant\nprogress in DocRE. In this paper, we do not aim at proposing a novel model for\nDocRE. Instead, we take a closer look at the field to see if these performance\ngains are actually true. By taking a comprehensive literature review and a\nthorough examination of popular DocRE datasets, we find that these performance\ngains are achieved upon a strong or even untenable assumption in common: all\nnamed entities are perfectly localized, normalized, and typed in advance. Next,\nwe construct four types of entity mention attacks to examine the robustness of\ntypical DocRE models by behavioral probing. We also have a close check on model\nusability in a more realistic setting. Our findings reveal that most of current\nDocRE models are vulnerable to entity mention attacks and difficult to be\ndeployed in real-world end-user NLP applications. Our study calls more\nattentions for future research to stop simplifying problem setups, and to model\nDocRE in the wild rather than in an unrealistic Utopian world.", "published": "2023-06-15 08:47:42", "link": "http://arxiv.org/abs/2306.08953v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DiPlomat: A Dialogue Dataset for Situated Pragmatic Reasoning", "abstract": "Pragmatic reasoning plays a pivotal role in deciphering implicit meanings\nthat frequently arise in real-life conversations and is essential for the\ndevelopment of communicative social agents. In this paper, we introduce a novel\nchallenge, DiPlomat, aiming at benchmarking machines' capabilities on pragmatic\nreasoning and situated conversational understanding. Compared with previous\nworks that treat different figurative expressions (e.g. metaphor, sarcasm) as\nindividual tasks, DiPlomat provides a cohesive framework towards general\npragmatic understanding. Our dataset is created through the utilization of\nAmazon Mechanical Turk ( AMT ), resulting in a total of 4, 177 multi-turn\ndialogues. In conjunction with the dataset, we propose two tasks, Pragmatic\nIdentification and Reasoning (PIR) and Conversational Question Answering (CQA).\nExperimental results with state-of-the-art (SOTA) neural architectures reveal\nseveral significant findings: 1) large language models ( LLMs) exhibit poor\nperformance in tackling this subjective domain; 2) comprehensive comprehension\nof context emerges as a critical factor for establishing benign human-machine\ninteractions; 3) current models defect in the application of pragmatic\nreasoning. As a result, we call on more attention to improve the ability of\ncontext understanding, reasoning, and implied meaning modeling.", "published": "2023-06-15 10:41:23", "link": "http://arxiv.org/abs/2306.09030v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning by Analogy: Diverse Questions Generation in Math Word Problem", "abstract": "Solving math word problem (MWP) with AI techniques has recently made great\nprogress with the success of deep neural networks (DNN), but it is far from\nbeing solved. We argue that the ability of learning by analogy is essential for\nan MWP solver to better understand same problems which may typically be\nformulated in diverse ways. However most existing works exploit the shortcut\nlearning to train MWP solvers simply based on samples with a single question.\nIn lack of diverse questions, these methods merely learn shallow heuristics. In\nthis paper, we make a first attempt to solve MWPs by generating diverse yet\nconsistent questions/equations. Given a typical MWP including the scenario\ndescription, question, and equation (i.e., answer), we first generate multiple\nconsistent equations via a group of heuristic rules. We then feed them to a\nquestion generator together with the scenario to obtain the corresponding\ndiverse questions, forming a new MWP with a variety of questions and equations.\nFinally we engage a data filter to remove those unreasonable MWPs, keeping the\nhigh-quality augmented ones. To evaluate the ability of learning by analogy for\nan MWP solver, we generate a new MWP dataset (called DiverseMath23K) with\ndiverse questions by extending the current benchmark Math23K. Extensive\nexperimental results demonstrate that our proposed method can generate\nhigh-quality diverse questions with corresponding equations, further leading to\nperformance improvement on Diverse-Math23K. The code and dataset is available\nat: https://github.com/zhouzihao501/DiverseMWP", "published": "2023-06-15 11:47:07", "link": "http://arxiv.org/abs/2306.09064v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KUCST at CheckThat 2023: How good can we be with a generic model?", "abstract": "In this paper we present our method for tasks 2 and 3A at the CheckThat2023\nshared task. We make use of a generic approach that has been used to tackle a\ndiverse set of tasks, inspired by authorship attribution and profiling. We\ntrain a number of Machine Learning models and our results show that Gradient\nBoosting performs the best for both tasks. Based on the official ranking\nprovided by the shared task organizers, our model shows an average performance\ncompared to other teams.", "published": "2023-06-15 13:09:40", "link": "http://arxiv.org/abs/2306.09108v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CMMLU: Measuring massive multitask language understanding in Chinese", "abstract": "As the capabilities of large language models (LLMs) continue to advance,\nevaluating their performance becomes increasingly crucial and challenging. This\npaper aims to bridge this gap by introducing CMMLU, a comprehensive Chinese\nbenchmark that covers various subjects, including natural science, social\nsciences, engineering, and humanities. We conduct a thorough evaluation of 18\nadvanced multilingual- and Chinese-oriented LLMs, assessing their performance\nacross different subjects and settings. The results reveal that most existing\nLLMs struggle to achieve an average accuracy of 50%, even when provided with\nin-context examples and chain-of-thought prompts, whereas the random baseline\nstands at 25%. This highlights significant room for improvement in LLMs.\nAdditionally, we conduct extensive experiments to identify factors impacting\nthe models' performance and propose directions for enhancing LLMs. CMMLU fills\nthe gap in evaluating the knowledge and reasoning capabilities of large\nlanguage models within the Chinese context.", "published": "2023-06-15 15:49:51", "link": "http://arxiv.org/abs/2306.09212v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DaMuEL: A Large Multilingual Dataset for Entity Linking", "abstract": "We present DaMuEL, a large Multilingual Dataset for Entity Linking containing\ndata in 53 languages. DaMuEL consists of two components: a knowledge base that\ncontains language-agnostic information about entities, including their claims\nfrom Wikidata and named entity types (PER, ORG, LOC, EVENT, BRAND, WORK_OF_ART,\nMANUFACTURED); and Wikipedia texts with entity mentions linked to the knowledge\nbase, along with language-specific text from Wikidata such as labels, aliases,\nand descriptions, stored separately for each language. The Wikidata QID is used\nas a persistent, language-agnostic identifier, enabling the combination of the\nknowledge base with language-specific texts and information for each entity.\nWikipedia documents deliberately annotate only a single mention for every\nentity present; we further automatically detect all mentions of named entities\nlinked from each document. The dataset contains 27.9M named entities in the\nknowledge base and 12.3G tokens from Wikipedia texts. The dataset is published\nunder the CC BY-SA license at https://hdl.handle.net/11234/1-5047.", "published": "2023-06-15 17:15:52", "link": "http://arxiv.org/abs/2306.09288v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KoLA: Carefully Benchmarking World Knowledge of Large Language Models", "abstract": "The unprecedented performance of large language models (LLMs) necessitates\nimprovements in evaluations. Rather than merely exploring the breadth of LLM\nabilities, we believe meticulous and thoughtful designs are essential to\nthorough, unbiased, and applicable evaluations. Given the importance of world\nknowledge to LLMs, we construct a Knowledge-oriented LLM Assessment benchmark\n(KoLA), in which we carefully design three crucial factors: (1) For\n\\textbf{ability modeling}, we mimic human cognition to form a four-level\ntaxonomy of knowledge-related abilities, covering $19$ tasks. (2) For\n\\textbf{data}, to ensure fair comparisons, we use both Wikipedia, a corpus\nprevalently pre-trained by LLMs, along with continuously collected emerging\ncorpora, aiming to evaluate the capacity to handle unseen data and evolving\nknowledge. (3) For \\textbf{evaluation criteria}, we adopt a contrastive system,\nincluding overall standard scores for better numerical comparability across\ntasks and models and a unique self-contrast metric for automatically evaluating\nknowledge-creating ability. We evaluate $28$ open-source and commercial LLMs\nand obtain some intriguing findings. The KoLA dataset and open-participation\nleaderboard are publicly released at https://kola.xlore.cn and will be\ncontinuously updated to provide references for developing LLMs and\nknowledge-related systems.", "published": "2023-06-15 17:20:46", "link": "http://arxiv.org/abs/2306.09296v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Propagating Knowledge Updates to LMs Through Distillation", "abstract": "Modern language models have the capacity to store and use immense amounts of\nknowledge about real-world entities, but it remains unclear how to update such\nknowledge stored in model parameters. While prior methods for updating\nknowledge in LMs successfully inject atomic facts, updated LMs fail to make\ninferences based on injected facts. In this work, we demonstrate that a context\ndistillation-based approach can both impart knowledge about entities and\npropagate that knowledge to enable broader inferences. Our approach consists of\ntwo stages: transfer set generation and distillation on the transfer set. We\nfirst generate a transfer set by prompting a language model to generate\ncontinuations from the entity definition. Then, we update the model parameters\nso that the distribution of the LM (the student) matches the distribution of\nthe LM conditioned on the definition (the teacher) on the transfer set. Our\nexperiments demonstrate that this approach is more effective at propagating\nknowledge updates than fine-tuning and other gradient-based knowledge-editing\nmethods. Moreover, it does not compromise performance in other contexts, even\nwhen injecting the definitions of up to 150 entities at once.", "published": "2023-06-15 17:39:50", "link": "http://arxiv.org/abs/2306.09306v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Quality and Efficiency of Manual Annotation: Pre-annotation Bias", "abstract": "This paper presents an analysis of annotation using an automatic\npre-annotation for a mid-level annotation complexity task -- dependency syntax\nannotation. It compares the annotation efforts made by annotators using a\npre-annotated version (with a high-accuracy parser) and those made by fully\nmanual annotation. The aim of the experiment is to judge the final annotation\nquality when pre-annotation is used. In addition, it evaluates the effect of\nautomatic linguistically-based (rule-formulated) checks and another annotation\non the same data available to the annotators, and their influence on annotation\nquality and efficiency. The experiment confirmed that the pre-annotation is an\nefficient tool for faster manual syntactic annotation which increases the\nconsistency of the resulting annotation without reducing its quality.", "published": "2023-06-15 17:41:14", "link": "http://arxiv.org/abs/2306.09307v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Wikibio: a Semantic Resource for the Intersectional Analysis of\n  Biographical Events", "abstract": "Biographical event detection is a relevant task for the exploration and\ncomparison of the ways in which people's lives are told and represented. In\nthis sense, it may support several applications in digital humanities and in\nworks aimed at exploring bias about minoritized groups. Despite that, there are\nno corpora and models specifically designed for this task. In this paper we\nfill this gap by presenting a new corpus annotated for biographical event\ndetection. The corpus, which includes 20 Wikipedia biographies, was compared\nwith five existing corpora to train a model for the biographical event\ndetection task. The model was able to detect all mentions of the target-entity\nin a biography with an F-score of 0.808 and the entity-related events with an\nF-score of 0.859. Finally, the model was used for performing an analysis of\nbiases about women and non-Western people in Wikipedia biographies.", "published": "2023-06-15 20:59:37", "link": "http://arxiv.org/abs/2306.09505v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Building blocks for complex tasks: Robust generative event extraction\n  for radiology reports under domain shifts", "abstract": "This paper explores methods for extracting information from radiology reports\nthat generalize across exam modalities to reduce requirements for annotated\ndata. We demonstrate that multi-pass T5-based text-to-text generative models\nexhibit better generalization across exam modalities compared to approaches\nthat employ BERT-based task-specific classification layers. We then develop\nmethods that reduce the inference cost of the model, making large-scale corpus\nprocessing more feasible for clinical applications. Specifically, we introduce\na generative technique that decomposes complex tasks into smaller subtask\nblocks, which improves a single-pass model when combined with multitask\ntraining. In addition, we leverage target-domain contexts during inference to\nenhance domain adaptation, enabling use of smaller models. Analyses offer\ninsights into the benefits of different cost reduction strategies.", "published": "2023-06-15 23:16:58", "link": "http://arxiv.org/abs/2306.09544v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MPSA-DenseNet: A novel deep learning model for English accent\n  classification", "abstract": "This paper presents three innovative deep learning models for English accent\nclassification: Multi-DenseNet, PSA-DenseNet, and MPSE-DenseNet, that combine\nmulti-task learning and the PSA module attention mechanism with DenseNet. We\napplied these models to data collected from six dialects of English across\nnative English speaking regions (Britain, the United States, Scotland) and\nnonnative English speaking regions (China, Germany, India). Our experimental\nresults show a significant improvement in classification accuracy, particularly\nwith MPSA-DenseNet, which outperforms all other models, including DenseNet and\nEPSA models previously used for accent identification. Our findings indicate\nthat MPSA-DenseNet is a highly promising model for accurately identifying\nEnglish accents.", "published": "2023-06-15 01:03:54", "link": "http://arxiv.org/abs/2306.08798v1", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "PEACE: Cross-Platform Hate Speech Detection- A Causality-guided\n  Framework", "abstract": "Hate speech detection refers to the task of detecting hateful content that\naims at denigrating an individual or a group based on their religion, gender,\nsexual orientation, or other characteristics. Due to the different policies of\nthe platforms, different groups of people express hate in different ways.\nFurthermore, due to the lack of labeled data in some platforms it becomes\nchallenging to build hate speech detection models. To this end, we revisit if\nwe can learn a generalizable hate speech detection model for the cross platform\nsetting, where we train the model on the data from one (source) platform and\ngeneralize the model across multiple (target) platforms. Existing\ngeneralization models rely on linguistic cues or auxiliary information, making\nthem biased towards certain tags or certain kinds of words (e.g., abusive\nwords) on the source platform and thus not applicable to the target platforms.\nInspired by social and psychological theories, we endeavor to explore if there\nexist inherent causal cues that can be leveraged to learn generalizable\nrepresentations for detecting hate speech across these distribution shifts. To\nthis end, we propose a causality-guided framework, PEACE, that identifies and\nleverages two intrinsic causal cues omnipresent in hateful content: the overall\nsentiment and the aggression in the text. We conduct extensive experiments\nacross multiple platforms (representing the distribution shift) showing if\ncausal cues can help cross-platform generalization.", "published": "2023-06-15 01:18:02", "link": "http://arxiv.org/abs/2306.08804v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Description-Enhanced Label Embedding Contrastive Learning for Text\n  Classification", "abstract": "Text Classification is one of the fundamental tasks in natural language\nprocessing, which requires an agent to determine the most appropriate category\nfor input sentences. Recently, deep neural networks have achieved impressive\nperformance in this area, especially Pre-trained Language Models (PLMs).\nUsually, these methods concentrate on input sentences and corresponding\nsemantic embedding generation. However, for another essential component:\nlabels, most existing works either treat them as meaningless one-hot vectors or\nuse vanilla embedding methods to learn label representations along with model\ntraining, underestimating the semantic information and guidance that these\nlabels reveal. To alleviate this problem and better exploit label information,\nin this paper, we employ Self-Supervised Learning (SSL) in model learning\nprocess and design a novel self-supervised Relation of Relation (R2)\nclassification task for label utilization from a one-hot manner perspective.\nThen, we propose a novel Relation of Relation Learning Network (R2-Net) for\ntext classification, in which text classification and R2 classification are\ntreated as optimization targets. Meanwhile, triplet loss is employed to enhance\nthe analysis of differences and connections among labels. Moreover, considering\nthat one-hot usage is still short of exploiting label information, we\nincorporate external knowledge from WordNet to obtain multi-aspect descriptions\nfor label semantic learning and extend R2-Net to a novel Description-Enhanced\nLabel Embedding network (DELE) from a label embedding perspective. ...", "published": "2023-06-15 02:19:34", "link": "http://arxiv.org/abs/2306.08817v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Neural models for Factual Inconsistency Classification with Explanations", "abstract": "Factual consistency is one of the most important requirements when editing\nhigh quality documents. It is extremely important for automatic text generation\nsystems like summarization, question answering, dialog modeling, and language\nmodeling. Still, automated factual inconsistency detection is rather\nunder-studied. Existing work has focused on (a) finding fake news keeping a\nknowledge base in context, or (b) detecting broad contradiction (as part of\nnatural language inference literature). However, there has been no work on\ndetecting and explaining types of factual inconsistencies in text, without any\nknowledge base in context. In this paper, we leverage existing work in\nlinguistics to formally define five types of factual inconsistencies. Based on\nthis categorization, we contribute a novel dataset, FICLE (Factual\nInconsistency CLassification with Explanation), with ~8K samples where each\nsample consists of two sentences (claim and context) annotated with type and\nspan of inconsistency. When the inconsistency relates to an entity type, it is\nlabeled as well at two levels (coarse and fine-grained). Further, we leverage\nthis dataset to train a pipeline of four neural models to predict inconsistency\ntype with explanations, given a (claim, context) sentence pair. Explanations\ninclude inconsistent claim fact triple, inconsistent context span, inconsistent\nclaim component, coarse and fine-grained inconsistent entity types. The\nproposed system first predicts inconsistent spans from claim and context; and\nthen uses them to predict inconsistency types and inconsistent entity types\n(when inconsistency is due to entities). We experiment with multiple\nTransformer-based natural language classification as well as generative models,\nand find that DeBERTa performs the best. Our proposed methods provide a\nweighted F1 of ~87% for inconsistency type classification across the five\nclasses.", "published": "2023-06-15 06:06:50", "link": "http://arxiv.org/abs/2306.08872v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Linguistic Binding in Diffusion Models: Enhancing Attribute\n  Correspondence through Attention Map Alignment", "abstract": "Text-conditioned image generation models often generate incorrect\nassociations between entities and their visual attributes. This reflects an\nimpaired mapping between linguistic binding of entities and modifiers in the\nprompt and visual binding of the corresponding elements in the generated image.\nAs one notable example, a query like \"a pink sunflower and a yellow flamingo\"\nmay incorrectly produce an image of a yellow sunflower and a pink flamingo. To\nremedy this issue, we propose SynGen, an approach which first syntactically\nanalyses the prompt to identify entities and their modifiers, and then uses a\nnovel loss function that encourages the cross-attention maps to agree with the\nlinguistic binding reflected by the syntax. Specifically, we encourage large\noverlap between attention maps of entities and their modifiers, and small\noverlap with other entities and modifier words. The loss is optimized during\ninference, without retraining or fine-tuning the model. Human evaluation on\nthree datasets, including one new and challenging set, demonstrate significant\nimprovements of SynGen compared with current state of the art methods. This\nwork highlights how making use of sentence structure during inference can\nefficiently and substantially improve the faithfulness of text-to-image\ngeneration.", "published": "2023-06-15 06:21:44", "link": "http://arxiv.org/abs/2306.08877v3", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Participatory Research as a Path to Community-Informed, Gender-Fair\n  Machine Translation", "abstract": "Recent years have seen a strongly increased visibility of non-binary people\nin public discourse. Accordingly, considerations of gender-fair language go\nbeyond a binary conception of male/female. However, language technology,\nespecially machine translation (MT), still suffers from binary gender bias.\nProposing a solution for gender-fair MT beyond the binary from a purely\ntechnological perspective might fall short to accommodate different target user\ngroups and in the worst case might lead to misgendering. To address this\nchallenge, we propose a method and case study building on participatory action\nresearch to include experiential experts, i.e., queer and non-binary people,\ntranslators, and MT experts, in the MT design process. The case study focuses\non German, where central findings are the importance of context dependency to\navoid identity invalidation and a desire for customizable MT solutions.", "published": "2023-06-15 07:20:14", "link": "http://arxiv.org/abs/2306.08906v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Opinion Tree Parsing for Aspect-based Sentiment Analysis", "abstract": "Extracting sentiment elements using pre-trained generative models has\nrecently led to large improvements in aspect-based sentiment analysis\nbenchmarks. However, these models always need large-scale computing resources,\nand they also ignore explicit modeling of structure between sentiment elements.\nTo address these challenges, we propose an opinion tree parsing model, aiming\nto parse all the sentiment elements from an opinion tree, which is much faster,\nand can explicitly reveal a more comprehensive and complete aspect-level\nsentiment structure. In particular, we first introduce a novel context-free\nopinion grammar to normalize the opinion tree structure. We then employ a\nneural chart-based opinion tree parser to fully explore the correlations among\nsentiment elements and parse them into an opinion tree structure. Extensive\nexperiments show the superiority of our proposed model and the capacity of the\nopinion tree parser with the proposed context-free opinion grammar. More\nimportantly, the results also prove that our model is much faster than previous\nmodels.", "published": "2023-06-15 07:53:14", "link": "http://arxiv.org/abs/2306.08925v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DocumentNet: Bridging the Data Gap in Document Pre-Training", "abstract": "Document understanding tasks, in particular, Visually-rich Document Entity\nRetrieval (VDER), have gained significant attention in recent years thanks to\ntheir broad applications in enterprise AI. However, publicly available data\nhave been scarce for these tasks due to strict privacy constraints and high\nannotation costs. To make things worse, the non-overlapping entity spaces from\ndifferent datasets hinder the knowledge transfer between document types. In\nthis paper, we propose a method to collect massive-scale and weakly labeled\ndata from the web to benefit the training of VDER models. The collected\ndataset, named DocumentNet, does not depend on specific document types or\nentity sets, making it universally applicable to all VDER tasks. The current\nDocumentNet consists of 30M documents spanning nearly 400 document types\norganized in a four-level ontology. Experiments on a set of broadly adopted\nVDER tasks show significant improvements when DocumentNet is incorporated into\nthe pre-training for both classic and few-shot learning settings. With the\nrecent emergence of large language models (LLMs), DocumentNet provides a large\ndata source to extend their multi-modal capabilities for VDER.", "published": "2023-06-15 08:21:15", "link": "http://arxiv.org/abs/2306.08937v3", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Towards Benchmarking and Improving the Temporal Reasoning Capability of\n  Large Language Models", "abstract": "Reasoning about time is of fundamental importance. Many facts are\ntime-dependent. For example, athletes change teams from time to time, and\ndifferent government officials are elected periodically. Previous\ntime-dependent question answering (QA) datasets tend to be biased in either\ntheir coverage of time spans or question types. In this paper, we introduce a\ncomprehensive probing dataset \\tempreason to evaluate the temporal reasoning\ncapability of large language models. Our dataset includes questions of three\ntemporal reasoning levels. In addition, we also propose a novel learning\nframework to improve the temporal reasoning capability of large language\nmodels, based on temporal span extraction and time-sensitive reinforcement\nlearning. We conducted experiments in closed book QA, open book QA, and\nreasoning QA settings and demonstrated the effectiveness of our approach. Our\ncode and data are released on https://github.com/DAMO-NLP-SG/TempReason.", "published": "2023-06-15 08:44:41", "link": "http://arxiv.org/abs/2306.08952v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Voting Booklet Bias: Stance Detection in Swiss Federal Communication", "abstract": "In this study, we use recent stance detection methods to study the stance\n(for, against or neutral) of statements in official information booklets for\nvoters. Our main goal is to answer the fundamental question: are topics to be\nvoted on presented in a neutral way?\n  To this end, we first train and compare several models for stance detection\non a large dataset about Swiss politics. We find that fine-tuning an M-BERT\nmodel leads to the best accuracy. We then use our best model to analyze the\nstance of utterances extracted from the Swiss federal voting booklet concerning\nthe Swiss popular votes of September 2022, which is the main goal of this\nproject.\n  We evaluated the models in both a multilingual as well as a monolingual\ncontext for German, French, and Italian. Our analysis shows that some issues\nare heavily favored while others are more balanced, and that the results are\nlargely consistent across languages.\n  Our findings have implications for the editorial process of future voting\nbooklets and the design of better automated systems for analyzing political\ndiscourse. The data and code accompanying this paper are available at\nhttps://github.com/ZurichNLP/voting-booklet-bias.", "published": "2023-06-15 09:49:12", "link": "http://arxiv.org/abs/2306.08999v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Web of Things and Trends in Agriculture: A Systematic Literature Review", "abstract": "In the past few years, the Web of Things (WOT) became a beneficial\ngame-changing technology within the Agriculture domain as it introduces\ninnovative and promising solutions to the Internet of Things (IoT) agricultural\napplications problems by providing its services. WOT provides the support for\nintegration, interoperability for heterogeneous devices, infrastructures,\nplatforms, and the emergence of various other technologies. The main aim of\nthis study is about understanding and providing a growing and existing research\ncontent, issues, and directions for the future regarding WOT-based agriculture.\nTherefore, a systematic literature review (SLR) of research articles is\npresented by categorizing the selected studies published between 2010 and 2020\ninto the following categories: research type, approaches, and their application\ndomains. Apart from reviewing the state-of-the-art articles on WOT solutions\nfor the agriculture field, a taxonomy of WOT-base agriculture application\ndomains has also been presented in this study. A model has also presented to\nshow the picture of WOT based Smart Agriculture. Lastly, the findings of this\nSLR and the research gaps in terms of open issues have been presented to\nprovide suggestions on possible future directions for the researchers for\nfuture research.", "published": "2023-06-15 12:20:30", "link": "http://arxiv.org/abs/2306.09079v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Relational Temporal Graph Reasoning for Dual-task Dialogue Language\n  Understanding", "abstract": "Dual-task dialog language understanding aims to tackle two correlative dialog\nlanguage understanding tasks simultaneously via leveraging their inherent\ncorrelations. In this paper, we put forward a new framework, whose core is\nrelational temporal graph reasoning.We propose a speaker-aware temporal graph\n(SATG) and a dual-task relational temporal graph (DRTG) to facilitate\nrelational temporal modeling in dialog understanding and dual-task reasoning.\nBesides, different from previous works that only achieve implicit\nsemantics-level interactions, we propose to model the explicit dependencies via\nintegrating prediction-level interactions. To implement our framework, we first\npropose a novel model Dual-tAsk temporal Relational rEcurrent Reasoning network\n(DARER), which first generates the context-, speaker- and temporal-sensitive\nutterance representations through relational temporal modeling of SATG, then\nconducts recurrent dual-task relational temporal graph reasoning on DRTG, in\nwhich process the estimated label distributions act as key clues in\nprediction-level interactions. And the relational temporal modeling in DARER is\nachieved by relational convolutional networks (RGCNs). Then we further propose\nRelational Temporal Transformer (ReTeFormer), which achieves fine-grained\nrelational temporal modeling via Relation- and Structure-aware Disentangled\nMulti-head Attention. Accordingly, we propose DARER with ReTeFormer (DARER2),\nwhich adopts two variants of ReTeFormer to achieve the relational temporal\nmodeling of SATG and DTRG, respectively. The extensive experiments on different\nscenarios verify that our models outperform state-of-the-art models by a large\nmargin. Remarkably, on the dialog sentiment classification task in the Mastodon\ndataset, DARER and DARER2 gain relative improvements of about 28% and 34% over\nthe previous best model in terms of F1.", "published": "2023-06-15 13:19:08", "link": "http://arxiv.org/abs/2306.09114v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Can ChatGPT pass the Vietnamese National High School Graduation\n  Examination?", "abstract": "This research article highlights the potential of AI-powered chatbots in\neducation and presents the results of using ChatGPT, a large language model, to\ncomplete the Vietnamese National High School Graduation Examination (VNHSGE).\nThe study dataset included 30 essays in the literature test case and 1,700\nmultiple-choice questions designed for other subjects. The results showed that\nChatGPT was able to pass the examination with an average score of 6-7,\ndemonstrating the technology's potential to revolutionize the educational\nlandscape. The analysis of ChatGPT performance revealed its proficiency in a\nrange of subjects, including mathematics, English, physics, chemistry, biology,\nhistory, geography, civic education, and literature, which suggests its\npotential to provide effective support for learners. However, further research\nis needed to assess ChatGPT performance on more complex exam questions and its\npotential to support learners in different contexts. As technology continues to\nevolve and improve, we can expect to see the use of AI tools like ChatGPT\nbecome increasingly common in educational settings, ultimately enhancing the\neducational experience for both students and educators.", "published": "2023-06-15 14:47:03", "link": "http://arxiv.org/abs/2306.09170v3", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Span-Selective Linear Attention Transformers for Effective and Robust\n  Schema-Guided Dialogue State Tracking", "abstract": "In schema-guided dialogue state tracking models estimate the current state of\na conversation using natural language descriptions of the service schema for\ngeneralization to unseen services. Prior generative approaches which decode\nslot values sequentially do not generalize well to variations in schema, while\ndiscriminative approaches separately encode history and schema and fail to\naccount for inter-slot and intent-slot dependencies. We introduce SPLAT, a\nnovel architecture which achieves better generalization and efficiency than\nprior approaches by constraining outputs to a limited prediction space. At the\nsame time, our model allows for rich attention among descriptions and history\nwhile keeping computation costs constrained by incorporating linear-time\nattention. We demonstrate the effectiveness of our model on the Schema-Guided\nDialogue (SGD) and MultiWOZ datasets. Our approach significantly improves upon\nexisting models achieving 85.3 JGA on the SGD dataset. Further, we show\nincreased robustness on the SGD-X benchmark: our model outperforms the more\nthan 30$\\times$ larger D3ST-XXL model by 5.0 points.", "published": "2023-06-15 17:59:31", "link": "http://arxiv.org/abs/2306.09340v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SIGHT: A Large Annotated Dataset on Student Insights Gathered from\n  Higher Education Transcripts", "abstract": "Lectures are a learning experience for both students and teachers. Students\nlearn from teachers about the subject material, while teachers learn from\nstudents about how to refine their instruction. However, online student\nfeedback is unstructured and abundant, making it challenging for teachers to\nlearn and improve. We take a step towards tackling this challenge. First, we\ncontribute a dataset for studying this problem: SIGHT is a large dataset of 288\nmath lecture transcripts and 15,784 comments collected from the Massachusetts\nInstitute of Technology OpenCourseWare (MIT OCW) YouTube channel. Second, we\ndevelop a rubric for categorizing feedback types using qualitative analysis.\nQualitative analysis methods are powerful in uncovering domain-specific\ninsights, however they are costly to apply to large data sources. To overcome\nthis challenge, we propose a set of best practices for using large language\nmodels (LLMs) to cheaply classify the comments at scale. We observe a striking\ncorrelation between the model's and humans' annotation: Categories with\nconsistent human annotations (>$0.9$ inter-rater reliability, IRR) also display\nhigher human-model agreement (>$0.7$), while categories with less consistent\nhuman annotations ($0.7$-$0.8$ IRR) correspondingly demonstrate lower\nhuman-model agreement ($0.3$-$0.5$). These techniques uncover useful student\nfeedback from thousands of comments, costing around $\\$0.002$ per comment. We\nconclude by discussing exciting future directions on using online student\nfeedback and improving automated annotation techniques for qualitative\nresearch.", "published": "2023-06-15 17:59:47", "link": "http://arxiv.org/abs/2306.09343v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Explaining Legal Concepts with Augmented Large Language Models (GPT-4)", "abstract": "Interpreting the meaning of legal open-textured terms is a key task of legal\nprofessionals. An important source for this interpretation is how the term was\napplied in previous court cases. In this paper, we evaluate the performance of\nGPT-4 in generating factually accurate, clear and relevant explanations of\nterms in legislation. We compare the performance of a baseline setup, where\nGPT-4 is directly asked to explain a legal term, to an augmented approach,\nwhere a legal information retrieval module is used to provide relevant context\nto the model, in the form of sentences from case law. We found that the direct\napplication of GPT-4 yields explanations that appear to be of very high quality\non their surface. However, detailed analysis uncovered limitations in terms of\nthe factual accuracy of the explanations. Further, we found that the\naugmentation leads to improved quality, and appears to eliminate the issue of\nhallucination, where models invent incorrect statements. These findings open\nthe door to the building of systems that can autonomously retrieve relevant\nsentences from case law and condense them into a useful explanation for legal\nscholars, educators or practicing lawyers alike.", "published": "2023-06-15 21:58:18", "link": "http://arxiv.org/abs/2306.09525v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Block-State Transformers", "abstract": "State space models (SSMs) have shown impressive results on tasks that require\nmodeling long-range dependencies and efficiently scale to long sequences owing\nto their subquadratic runtime complexity. Originally designed for continuous\nsignals, SSMs have shown superior performance on a plethora of tasks, in vision\nand audio; however, SSMs still lag Transformer performance in Language Modeling\ntasks. In this work, we propose a hybrid layer named Block-State Transformer\n(BST), that internally combines an SSM sublayer for long-range\ncontextualization, and a Block Transformer sublayer for short-term\nrepresentation of sequences. We study three different, and completely\nparallelizable, variants that integrate SSMs and block-wise attention. We show\nthat our model outperforms similar Transformer-based architectures on language\nmodeling perplexity and generalizes to longer sequences. In addition, the\nBlock-State Transformer demonstrates more than tenfold increase in speed at the\nlayer level compared to the Block-Recurrent Transformer when model\nparallelization is employed.", "published": "2023-06-15 22:48:08", "link": "http://arxiv.org/abs/2306.09539v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Domain-specific ChatBots for Science using Embeddings", "abstract": "Large language models (LLMs) have emerged as powerful machine-learning\nsystems capable of handling a myriad of tasks. Tuned versions of these systems\nhave been turned into chatbots that can respond to user queries on a vast\ndiversity of topics, providing informative and creative replies. However, their\napplication to physical science research remains limited owing to their\nincomplete knowledge in these areas, contrasted with the needs of rigor and\nsourcing in science domains. Here, we demonstrate how existing methods and\nsoftware tools can be easily combined to yield a domain-specific chatbot. The\nsystem ingests scientific documents in existing formats, and uses text\nembedding lookup to provide the LLM with domain-specific contextual information\nwhen composing its reply. We similarly demonstrate that existing image\nembedding methods can be used for search and retrieval across publication\nfigures. These results confirm that LLMs are already suitable for use by\nphysical scientists in accelerating their research efforts.", "published": "2023-06-15 15:26:20", "link": "http://arxiv.org/abs/2306.10067v2", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "PRISMA-DFLLM: An Extension of PRISMA for Systematic Literature Reviews\n  using Domain-specific Finetuned Large Language Models", "abstract": "With the proliferation of open-sourced Large Language Models (LLMs) and\nefficient finetuning techniques, we are on the cusp of the emergence of\nnumerous domain-specific LLMs that have been finetuned for expertise across\nspecialized fields and applications for which the current general-purpose LLMs\nare unsuitable. In academia, this technology has the potential to revolutionize\nthe way we conduct systematic literature reviews (SLRs), access knowledge and\ngenerate new insights. This paper proposes an AI-enabled methodological\nframework that combines the power of LLMs with the rigorous reporting\nguidelines of the Preferred Reporting Items for Systematic Reviews and\nMeta-Analyses (PRISMA). By finetuning LLMs on domain-specific academic papers\nthat have been selected as a result of a rigorous SLR process, the proposed\nPRISMA-DFLLM (for Domain-specific Finetuned LLMs) reporting guidelines offer\nthe potential to achieve greater efficiency, reusability and scalability, while\nalso opening the potential for conducting incremental living systematic reviews\nwith the aid of LLMs. Additionally, the proposed approach for leveraging LLMs\nfor SLRs enables the dissemination of finetuned models, empowering researchers\nto accelerate advancements and democratize cutting-edge research. This paper\npresents the case for the feasibility of finetuned LLMs to support rigorous\nSLRs and the technical requirements for realizing this. This work then proposes\nthe extended PRISMA-DFLLM checklist of reporting guidelines as well as the\nadvantages, challenges, and potential implications of implementing\nPRISMA-DFLLM. Finally, a future research roadmap to develop this line of\nAI-enabled SLRs is presented, paving the way for a new era of evidence\nsynthesis and knowledge discovery.", "published": "2023-06-15 02:52:50", "link": "http://arxiv.org/abs/2306.14905v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Reading Comprehension Question Generation with Data\n  Augmentation and Overgenerate-and-rank", "abstract": "Reading comprehension is a crucial skill in many aspects of education,\nincluding language learning, cognitive development, and fostering early\nliteracy skills in children. Automated answer-aware reading comprehension\nquestion generation has significant potential to scale up learner support in\neducational activities. One key technical challenge in this setting is that\nthere can be multiple questions, sometimes very different from each other, with\nthe same answer; a trained question generation method may not necessarily know\nwhich question human educators would prefer. To address this challenge, we\npropose 1) a data augmentation method that enriches the training dataset with\ndiverse questions given the same context and answer and 2) an\novergenerate-and-rank method to select the best question from a pool of\ncandidates. We evaluate our method on the FairytaleQA dataset, showing a 5%\nabsolute improvement in ROUGE-L over the best existing method. We also\ndemonstrate the effectiveness of our method in generating harder, \"implicit\"\nquestions, where the answers are not contained in the context as text spans.", "published": "2023-06-15 04:23:25", "link": "http://arxiv.org/abs/2306.08847v1", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Pushing the Limits of Unsupervised Unit Discovery for SSL Speech\n  Representation", "abstract": "The excellent generalization ability of self-supervised learning (SSL) for\nspeech foundation models has garnered significant attention. HuBERT is a\nsuccessful example that utilizes offline clustering to convert speech features\ninto discrete units for a masked language modeling pretext task. However,\nsimply clustering features as targets by k-means does not fully inspire the\nmodel's performance. In this work, we present an unsupervised method to improve\nSSL targets. Two models are proposed, MonoBERT and PolyBERT, which leverage\ncontext-independent and context-dependent phoneme-based units for pre-training.\nOur models outperform other SSL models significantly on the LibriSpeech\nbenchmark without the need for iterative re-clustering and re-training.\nFurthermore, our models equipped with context-dependent units even outperform\ntarget-improvement models that use labeled data during pre-training. How we\nprogressively improve the unit discovery process is demonstrated through\nexperiments.", "published": "2023-06-15 07:45:12", "link": "http://arxiv.org/abs/2306.08920v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Exploring the MIT Mathematics and EECS Curriculum Using Large Language\n  Models", "abstract": "We curate a comprehensive dataset of 4,550 questions and solutions from\nproblem sets, midterm exams, and final exams across all MIT Mathematics and\nElectrical Engineering and Computer Science (EECS) courses required for\nobtaining a degree. We evaluate the ability of large language models to fulfill\nthe graduation requirements for any MIT major in Mathematics and EECS. Our\nresults demonstrate that GPT-3.5 successfully solves a third of the entire MIT\ncurriculum, while GPT-4, with prompt engineering, achieves a perfect solve rate\non a test set excluding questions based on images. We fine-tune an open-source\nlarge language model on this dataset. We employ GPT-4 to automatically grade\nmodel responses, providing a detailed performance breakdown by course,\nquestion, and answer type. By embedding questions in a low-dimensional space,\nwe explore the relationships between questions, topics, and classes and\ndiscover which questions and classes are required for solving other questions\nand classes through few-shot learning. Our analysis offers valuable insights\ninto course prerequisites and curriculum design, highlighting language models'\npotential for learning and improving Mathematics and EECS education.", "published": "2023-06-15 09:48:14", "link": "http://arxiv.org/abs/2306.08997v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mapping Researcher Activity based on Publication Data by means of\n  Transformers", "abstract": "Modern performance on several natural language processing (NLP) tasks has\nbeen enhanced thanks to the Transformer-based pre-trained language model BERT.\nWe employ this concept to investigate a local publication database. Research\npapers are encoded and clustered to form a landscape view of the scientific\ntopics, in which research is active. Authors working on similar topics can be\nidentified by calculating the similarity between their papers. Based on this,\nwe define a similarity metric between authors. Additionally we introduce the\nconcept of self-similarity to indicate the topical variety of authors.", "published": "2023-06-15 11:13:54", "link": "http://arxiv.org/abs/2306.09049v1", "categories": ["cs.CL", "cs.DL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and\n  Text Integration", "abstract": "Although instruction-tuned large language models (LLMs) have exhibited\nremarkable capabilities across various NLP tasks, their effectiveness on other\ndata modalities beyond text has not been fully studied. In this work, we\npropose Macaw-LLM, a novel multi-modal LLM that seamlessly integrates visual,\naudio, and textual information. Macaw-LLM consists of three main components: a\nmodality module for encoding multi-modal data, a cognitive module for\nharnessing pretrained LLMs, and an alignment module for harmonizing diverse\nrepresentations. Our novel alignment module seamlessly bridges multi-modal\nfeatures to textual features, simplifying the adaptation process from the\nmodality modules to the cognitive module. In addition, we construct a\nlarge-scale multi-modal instruction dataset in terms of multi-turn dialogue,\nincluding 69K image instances and 50K video instances. We have made our data,\ncode and model publicly available, which we hope can pave the way for future\nresearch in multi-modal LLMs and expand the capabilities of LLMs to handle\ndiverse data modalities and address complex real-world scenarios.", "published": "2023-06-15 12:45:25", "link": "http://arxiv.org/abs/2306.09093v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "On the $k$-Hamming and $k$-Edit Distances", "abstract": "In this paper we consider the weighted $k$-Hamming and $k$-Edit distances,\nthat are natural generalizations of the classical Hamming and Edit distances.\nAs main results of this paper we prove that for any $k\\geq 2$ the\nDECIS-$k$-Hamming problem is $\\mathbb{P}$-SPACE-complete and the DECIS-$k$-Edit\nproblem is NEXPTIME-complete.", "published": "2023-06-15 14:03:28", "link": "http://arxiv.org/abs/2306.09144v1", "categories": ["cs.CC", "cs.CL", "cs.DS"], "primary_category": "cs.CC"}
{"title": "Opportunities for Large Language Models and Discourse in Engineering\n  Design", "abstract": "In recent years, large language models have achieved breakthroughs on a wide\nrange of benchmarks in natural language processing and continue to increase in\nperformance. Recently, the advances of large language models have raised\ninterest outside the natural language processing community and could have a\nlarge impact on daily life. In this paper, we pose the question: How will large\nlanguage models and other foundation models shape the future product\ndevelopment process? We provide the reader with an overview of the subject by\nsummarizing both recent advances in natural language processing and the use of\ninformation technology in the engineering design process. We argue that\ndiscourse should be regarded as the core of engineering design processes, and\ntherefore should be represented in a digital artifact. On this basis, we\ndescribe how foundation models such as large language models could contribute\nto the design discourse by automating parts thereof that involve creativity and\nreasoning, and were previously reserved for humans. We describe how\nsimulations, experiments, topology optimizations, and other process steps can\nbe integrated into a machine-actionable, discourse-centric design process.\nFinally, we outline the future research that will be necessary for the\nimplementation of the conceptualized framework.", "published": "2023-06-15 14:46:44", "link": "http://arxiv.org/abs/2306.09169v1", "categories": ["cs.CL", "cs.AI", "cs.CE"], "primary_category": "cs.CL"}
{"title": "One Law, Many Languages: Benchmarking Multilingual Legal Reasoning for\n  Judicial Support", "abstract": "Recent strides in Large Language Models (LLMs) have saturated many Natural\nLanguage Processing (NLP) benchmarks, emphasizing the need for more challenging\nones to properly assess LLM capabilities. However, domain-specific and\nmultilingual benchmarks are rare because they require in-depth expertise to\ndevelop. Still, most public models are trained predominantly on English\ncorpora, while other languages remain understudied, particularly for practical\ndomain-specific NLP tasks. In this work, we introduce a novel NLP benchmark for\nthe legal domain that challenges LLMs in five key dimensions: processing\n\\emph{long documents} (up to 50K tokens), using \\emph{domain-specific\nknowledge} (embodied in legal texts), \\emph{multilingual} understanding\n(covering five languages), \\emph{multitasking} (comprising legal\ndocument-to-document Information Retrieval, Court View Generation, Leading\nDecision Summarization, Citation Extraction, and eight challenging Text\nClassification tasks) and \\emph{reasoning} (comprising especially Court View\nGeneration, but also the Text Classification tasks). Our benchmark contains\ndiverse datasets from the Swiss legal system, allowing for a comprehensive\nstudy of the underlying non-English, inherently multilingual legal system.\nDespite the large size of our datasets (some with hundreds of thousands of\nexamples), existing publicly available multilingual models struggle with most\ntasks, even after extensive in-domain pre-training and fine-tuning. We publish\nall resources (benchmark suite, pre-trained models, code) under permissive open\nCC BY-SA licenses.", "published": "2023-06-15 16:19:15", "link": "http://arxiv.org/abs/2306.09237v3", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2"], "primary_category": "cs.CL"}
{"title": "Can Language Models Teach Weaker Agents? Teacher Explanations Improve\n  Students via Personalization", "abstract": "A hallmark property of explainable AI models is the ability to teach other\nagents, communicating knowledge of how to perform a task. While Large Language\nModels perform complex reasoning by generating explanations for their\npredictions, it is unclear whether they also make good teachers for weaker\nagents. To address this, we consider a student-teacher framework between two\nLLM agents and study if, when, and how the teacher should intervene with\nnatural language explanations to improve the student's performance. Since\ncommunication is expensive, we define a budget such that the teacher only\ncommunicates explanations for a fraction of the data, after which the student\nshould perform well on its own. We decompose the teaching problem along four\naxes: (1) if teacher's test time intervention improve student predictions, (2)\nwhen it is worth explaining a data point, (3) how the teacher should\npersonalize explanations to better teach the student, and (4) if teacher\nexplanations also improve students on future unexplained data. We first show\nthat teacher LLMs can indeed intervene on student reasoning to improve their\nperformance. Next, inspired by the Theory of Mind abilities of effective\nteachers, we propose building two few-shot mental models of the student. The\nfirst model defines an Intervention Function that simulates the utility of an\nintervention, allowing the teacher to intervene when this utility is the\nhighest and improving student performance at lower budgets. The second model\nenables the teacher to personalize explanations for a particular student and\noutperform unpersonalized teachers. We also demonstrate that in multi-turn\ninteractions, teacher explanations generalize and learning from explained data\nimproves student performance on future unexplained data. Finally, we verify\nthat misaligned teachers can lower student performance to random chance by\nintentionally misleading them.", "published": "2023-06-15 17:27:20", "link": "http://arxiv.org/abs/2306.09299v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Matching Pairs: Attributing Fine-Tuned Models to their Pre-Trained Large\n  Language Models", "abstract": "The wide applicability and adaptability of generative large language models\n(LLMs) has enabled their rapid adoption. While the pre-trained models can\nperform many tasks, such models are often fine-tuned to improve their\nperformance on various downstream applications. However, this leads to issues\nover violation of model licenses, model theft, and copyright infringement.\nMoreover, recent advances show that generative technology is capable of\nproducing harmful content which exacerbates the problems of accountability\nwithin model supply chains. Thus, we need a method to investigate how a model\nwas trained or a piece of text was generated and what their pre-trained base\nmodel was. In this paper we take the first step to address this open problem by\ntracing back the origin of a given fine-tuned LLM to its corresponding\npre-trained base model. We consider different knowledge levels and attribution\nstrategies, and find that we can correctly trace back 8 out of the 10 fine\ntuned models with our best method.", "published": "2023-06-15 17:42:48", "link": "http://arxiv.org/abs/2306.09308v1", "categories": ["cs.CL", "cs.AI", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Semantic HELM: A Human-Readable Memory for Reinforcement Learning", "abstract": "Reinforcement learning agents deployed in the real world often have to cope\nwith partially observable environments. Therefore, most agents employ memory\nmechanisms to approximate the state of the environment. Recently, there have\nbeen impressive success stories in mastering partially observable environments,\nmostly in the realm of computer games like Dota 2, StarCraft II, or MineCraft.\nHowever, existing methods lack interpretability in the sense that it is not\ncomprehensible for humans what the agent stores in its memory. In this regard,\nwe propose a novel memory mechanism that represents past events in human\nlanguage. Our method uses CLIP to associate visual inputs with language tokens.\nThen we feed these tokens to a pretrained language model that serves the agent\nas memory and provides it with a coherent and human-readable representation of\nthe past. We train our memory mechanism on a set of partially observable\nenvironments and find that it excels on tasks that require a memory component,\nwhile mostly attaining performance on-par with strong baselines on tasks that\ndo not. On a challenging continuous recognition task, where memorizing the past\nis crucial, our memory mechanism converges two orders of magnitude faster than\nprior methods. Since our memory mechanism is human-readable, we can peek at an\nagent's memory and check whether crucial pieces of information have been\nstored. This significantly enhances troubleshooting and paves the way toward\nmore interpretable agents.", "published": "2023-06-15 17:47:31", "link": "http://arxiv.org/abs/2306.09312v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Lexical Speaker Error Correction: Leveraging Language Models for Speaker\n  Diarization Error Correction", "abstract": "Speaker diarization (SD) is typically used with an automatic speech\nrecognition (ASR) system to ascribe speaker labels to recognized words. The\nconventional approach reconciles outputs from independently optimized ASR and\nSD systems, where the SD system typically uses only acoustic information to\nidentify the speakers in the audio stream. This approach can lead to speaker\nerrors especially around speaker turns and regions of speaker overlap. In this\npaper, we propose a novel second-pass speaker error correction system using\nlexical information, leveraging the power of modern language models (LMs). Our\nexperiments across multiple telephony datasets show that our approach is both\neffective and robust. Training and tuning only on the Fisher dataset, this\nerror correction approach leads to relative word-level diarization error rate\n(WDER) reductions of 15-30% on three telephony datasets: RT03-CTS, Callhome\nAmerican English and held-out portions of Fisher.", "published": "2023-06-15 17:47:41", "link": "http://arxiv.org/abs/2306.09313v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "WizMap: Scalable Interactive Visualization for Exploring Large Machine\n  Learning Embeddings", "abstract": "Machine learning models often learn latent embedding representations that\ncapture the domain semantics of their training data. These embedding\nrepresentations are valuable for interpreting trained models, building new\nmodels, and analyzing new datasets. However, interpreting and using embeddings\ncan be challenging due to their opaqueness, high dimensionality, and the large\nsize of modern datasets. To tackle these challenges, we present WizMap, an\ninteractive visualization tool to help researchers and practitioners easily\nexplore large embeddings. With a novel multi-resolution embedding summarization\nmethod and a familiar map-like interaction design, WizMap enables users to\nnavigate and interpret embedding spaces with ease. Leveraging modern web\ntechnologies such as WebGL and Web Workers, WizMap scales to millions of\nembedding points directly in users' web browsers and computational notebooks\nwithout the need for dedicated backend servers. WizMap is open-source and\navailable at the following public demo link: https://poloclub.github.io/wizmap.", "published": "2023-06-15 17:58:04", "link": "http://arxiv.org/abs/2306.09328v1", "categories": ["cs.LG", "cs.CL", "cs.CV", "cs.HC"], "primary_category": "cs.LG"}
{"title": "PaReprop: Fast Parallelized Reversible Backpropagation", "abstract": "The growing size of datasets and deep learning models has made faster and\nmemory-efficient training crucial. Reversible transformers have recently been\nintroduced as an exciting new method for extremely memory-efficient training,\nbut they come with an additional computation overhead of activation\nre-computation in the backpropagation phase. We present PaReprop, a fast\nParallelized Reversible Backpropagation algorithm that parallelizes the\nadditional activation re-computation overhead in reversible training with the\ngradient computation itself in backpropagation phase. We demonstrate the\neffectiveness of the proposed PaReprop algorithm through extensive benchmarking\nacross model families (ViT, MViT, Swin and RoBERTa), data modalities (Vision &\nNLP), model sizes (from small to giant), and training batch sizes. Our\nempirical results show that PaReprop achieves up to 20% higher training\nthroughput than vanilla reversible training, largely mitigating the theoretical\noverhead of 25% lower throughput from activation recomputation in reversible\ntraining. Project page: https://tylerzhu.com/pareprop.", "published": "2023-06-15 17:59:32", "link": "http://arxiv.org/abs/2306.09342v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "ChatGPT for Suicide Risk Assessment on Social Media: Quantitative\n  Evaluation of Model Performance, Potentials and Limitations", "abstract": "This paper presents a novel framework for quantitatively evaluating the\ninteractive ChatGPT model in the context of suicidality assessment from social\nmedia posts, utilizing the University of Maryland Reddit suicidality dataset.\nWe conduct a technical evaluation of ChatGPT's performance on this task using\nZero-Shot and Few-Shot experiments and compare its results with those of two\nfine-tuned transformer-based models. Additionally, we investigate the impact of\ndifferent temperature parameters on ChatGPT's response generation and discuss\nthe optimal temperature based on the inconclusiveness rate of ChatGPT. Our\nresults indicate that while ChatGPT attains considerable accuracy in this task,\ntransformer-based models fine-tuned on human-annotated datasets exhibit\nsuperior performance. Moreover, our analysis sheds light on how adjusting the\nChatGPT's hyperparameters can improve its ability to assist mental health\nprofessionals in this critical task.", "published": "2023-06-15 16:01:30", "link": "http://arxiv.org/abs/2306.09390v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Explore, Establish, Exploit: Red Teaming Language Models from Scratch", "abstract": "Deploying large language models (LMs) can pose hazards from harmful outputs\nsuch as toxic or false text. Prior work has introduced automated tools that\nelicit harmful outputs to identify these risks. While this is a valuable step\ntoward securing models, these approaches rely on a pre-existing way to\nefficiently classify undesirable outputs. Using a pre-existing classifier does\nnot allow for red-teaming to be tailored to the target model. Furthermore, when\nfailures can be easily classified in advance, red-teaming has limited marginal\nvalue because problems can be avoided by simply filtering training data and/or\nmodel outputs. Here, we consider red-teaming \"from scratch,\" in which the\nadversary does not begin with a way to classify failures. Our framework\nconsists of three steps: 1) Exploring the model's range of behaviors in the\ndesired context; 2) Establishing a definition and measurement for undesired\nbehavior (e.g., a classifier trained to reflect human evaluations); and 3)\nExploiting the model's flaws using this measure to develop diverse adversarial\nprompts. We use this approach to red-team GPT-3 to discover classes of inputs\nthat elicit false statements. In doing so, we construct the CommonClaim dataset\nof 20,000 statements labeled by humans as common-knowledge-true, common\nknowledge-false, or neither. We are making code and data available.", "published": "2023-06-15 18:49:50", "link": "http://arxiv.org/abs/2306.09442v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Inverse Scaling: When Bigger Isn't Better", "abstract": "Work on scaling laws has found that large language models (LMs) show\npredictable improvements to overall loss with increased scale (model size,\ntraining data, and compute). Here, we present evidence for the claim that LMs\nmay show inverse scaling, or worse task performance with increased scale, e.g.,\ndue to flaws in the training objective and data. We present empirical evidence\nof inverse scaling on 11 datasets collected by running a public contest, the\nInverse Scaling Prize, with a substantial prize pool. Through analysis of the\ndatasets, along with other examples found in the literature, we identify four\npotential causes of inverse scaling: (i) preference to repeat memorized\nsequences over following in-context instructions, (ii) imitation of undesirable\npatterns in the training data, (iii) tasks containing an easy distractor task\nwhich LMs could focus on, rather than the harder real task, and (iv) correct\nbut misleading few-shot demonstrations of the task. We release the winning\ndatasets at https://inversescaling.com/data to allow for further investigation\nof inverse scaling. Our tasks have helped drive the discovery of U-shaped and\ninverted-U scaling trends, where an initial trend reverses, suggesting that\nscaling trends are less reliable at predicting the behavior of larger-scale\nmodels than previously understood. Overall, our results suggest that there are\ntasks for which increased model scale alone may not lead to progress, and that\nmore careful thought needs to go into the data and objectives for training\nlanguage models.", "published": "2023-06-15 20:11:23", "link": "http://arxiv.org/abs/2306.09479v2", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Relation-Aware Network with Attention-Based Loss for Few-Shot Knowledge\n  Graph Completion", "abstract": "Few-shot knowledge graph completion (FKGC) task aims to predict unseen facts\nof a relation with few-shot reference entity pairs. Current approaches randomly\nselect one negative sample for each reference entity pair to minimize a\nmargin-based ranking loss, which easily leads to a zero-loss problem if the\nnegative sample is far away from the positive sample and then out of the\nmargin. Moreover, the entity should have a different representation under a\ndifferent context. To tackle these issues, we propose a novel Relation-Aware\nNetwork with Attention-Based Loss (RANA) framework. Specifically, to better\nutilize the plentiful negative samples and alleviate the zero-loss issue, we\nstrategically select relevant negative samples and design an attention-based\nloss function to further differentiate the importance of each negative sample.\nThe intuition is that negative samples more similar to positive samples will\ncontribute more to the model. Further, we design a dynamic relation-aware\nentity encoder for learning a context-dependent entity representation.\nExperiments demonstrate that RANA outperforms the state-of-the-art models on\ntwo benchmark datasets.", "published": "2023-06-15 21:41:43", "link": "http://arxiv.org/abs/2306.09519v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Opportunities and Challenges for ChatGPT and Large Language Models in\n  Biomedicine and Health", "abstract": "ChatGPT has drawn considerable attention from both the general public and\ndomain experts with its remarkable text generation capabilities. This has\nsubsequently led to the emergence of diverse applications in the field of\nbiomedicine and health. In this work, we examine the diverse applications of\nlarge language models (LLMs), such as ChatGPT, in biomedicine and health.\nSpecifically we explore the areas of biomedical information retrieval, question\nanswering, medical text summarization, information extraction, and medical\neducation, and investigate whether LLMs possess the transformative power to\nrevolutionize these tasks or whether the distinct complexities of biomedical\ndomain presents unique challenges. Following an extensive literature survey, we\nfind that significant advances have been made in the field of text generation\ntasks, surpassing the previous state-of-the-art methods. For other\napplications, the advances have been modest. Overall, LLMs have not yet\nrevolutionized biomedicine, but recent rapid progress indicates that such\nmethods hold great potential to provide valuable means for accelerating\ndiscovery and improving health. We also find that the use of LLMs, like\nChatGPT, in the fields of biomedicine and health entails various risks and\nchallenges, including fabricated information in its generated responses, as\nwell as legal and privacy concerns associated with sensitive patient data. We\nbelieve this survey can provide a comprehensive and timely overview to\nbiomedical researchers and healthcare practitioners on the opportunities and\nchallenges associated with using ChatGPT and other LLMs for transforming\nbiomedicine and health.", "published": "2023-06-15 20:19:08", "link": "http://arxiv.org/abs/2306.10070v2", "categories": ["cs.CY", "cs.AI", "cs.CL", "q-bio.QM"], "primary_category": "cs.CY"}
{"title": "Thrilled by Your Progress! Large Language Models (GPT-4) No Longer\n  Struggle to Pass Assessments in Higher Education Programming Courses", "abstract": "This paper studies recent developments in large language models' (LLM)\nabilities to pass assessments in introductory and intermediate Python\nprogramming courses at the postsecondary level. The emergence of ChatGPT\nresulted in heated debates of its potential uses (e.g., exercise generation,\ncode explanation) as well as misuses in programming classes (e.g., cheating).\nRecent studies show that while the technology performs surprisingly well on\ndiverse sets of assessment instruments employed in typical programming classes\nthe performance is usually not sufficient to pass the courses. The release of\nGPT-4 largely emphasized notable improvements in the capabilities related to\nhandling assessments originally designed for human test-takers. This study is\nthe necessary analysis in the context of this ongoing transition towards mature\ngenerative AI systems. Specifically, we report the performance of GPT-4,\ncomparing it to the previous generations of GPT models, on three Python courses\nwith assessments ranging from simple multiple-choice questions (no code\ninvolved) to complex programming projects with code bases distributed into\nmultiple files (599 exercises overall). Additionally, we analyze the\nassessments that were not handled well by GPT-4 to understand the current\nlimitations of the model, as well as its capabilities to leverage feedback\nprovided by an auto-grader. We found that the GPT models evolved from\ncompletely failing the typical programming class' assessments (the original\nGPT-3) to confidently passing the courses with no human involvement (GPT-4).\nWhile we identified certain limitations in GPT-4's handling of MCQs and coding\nexercises, the rate of improvement across the recent generations of GPT models\nstrongly suggests their potential to handle almost any type of assessment\nwidely used in higher education programming courses. These findings could be\nleveraged by educators and institutions to adapt the design of programming\nassessments as well as to fuel the necessary discussions into how programming\nclasses should be updated to reflect the recent technological developments.\nThis study provides evidence that programming instructors need to prepare for a\nworld in which there is an easy-to-use widely accessible technology that can be\nutilized by learners to collect passing scores, with no effort whatsoever, on\nwhat today counts as viable programming knowledge and skills assessments.", "published": "2023-06-15 22:12:34", "link": "http://arxiv.org/abs/2306.10073v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.SE"], "primary_category": "cs.CY"}
{"title": "A Bayesian approach to uncertainty in word embedding bias estimation", "abstract": "Multiple measures, such as WEAT or MAC, attempt to quantify the magnitude of\nbias present in word embeddings in terms of a single-number metric. However,\nsuch metrics and the related statistical significance calculations rely on\ntreating pre-averaged data as individual data points and employing\nbootstrapping techniques with low sample sizes. We show that similar results\ncan be easily obtained using such methods even if the data are generated by a\nnull model lacking the intended bias. Consequently, we argue that this approach\ngenerates false confidence. To address this issue, we propose a Bayesian\nalternative: hierarchical Bayesian modeling, which enables a more\nuncertainty-sensitive inspection of bias in word embeddings at different levels\nof granularity. To showcase our method, we apply it to Religion, Gender, and\nRace word lists from the original research, together with our control neutral\nword lists. We deploy the method using Google, Glove, and Reddit embeddings.\nFurther, we utilize our approach to evaluate a debiasing technique applied to\nReddit word embedding. Our findings reveal a more complex landscape than\nsuggested by the proponents of single-number metrics. The datasets and source\ncode for the paper are publicly available.", "published": "2023-06-15 11:48:50", "link": "http://arxiv.org/abs/2306.09066v1", "categories": ["cs.CL", "cs.HC", "cs.LG", "stat.AP", "stat.ME"], "primary_category": "cs.CL"}
{"title": "COSA: Concatenated Sample Pretrained Vision-Language Foundation Model", "abstract": "Due to the limited scale and quality of video-text training corpus, most\nvision-language foundation models employ image-text datasets for pretraining\nand primarily focus on modeling visually semantic representations while\ndisregarding temporal semantic representations and correlations. To address\nthis issue, we propose COSA, a COncatenated SAmple pretrained vision-language\nfoundation model. COSA jointly models visual contents and event-level temporal\ncues using only image-text corpora. We achieve this by sequentially\nconcatenating multiple image-text pairs as inputs for pretraining. This\ntransformation effectively converts existing image-text corpora into a pseudo\nlong-form video-paragraph corpus, enabling richer scene transformations and\nexplicit event-description correspondence. Extensive experiments demonstrate\nthat COSA consistently improves performance across a broad range of downstream\ntasks, including long-form/short-form video-text tasks and image-text tasks\nsuch as retrieval, captioning, and question answering. Notably, COSA achieves\nstate-of-the-art results on various competitive benchmarks. Code and model are\nreleased at https://github.com/TXH-mercury/COSA.", "published": "2023-06-15 12:29:42", "link": "http://arxiv.org/abs/2306.09085v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Multi-modal Hate Speech Detection using Machine Learning", "abstract": "With the continuous growth of internet users and media content, it is very\nhard to track down hateful speech in audio and video. Converting video or audio\ninto text does not detect hate speech accurately as human sometimes uses\nhateful words as humorous or pleasant in sense and also uses different voice\ntones or show different action in the video. The state-ofthe-art hate speech\ndetection models were mostly developed on a single modality. In this research,\na combined approach of multimodal system has been proposed to detect hate\nspeech from video contents by extracting feature images, feature values\nextracted from the audio, text and used machine learning and Natural language\nprocessing.", "published": "2023-06-15 06:46:52", "link": "http://arxiv.org/abs/2307.11519v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.AI"}
{"title": "Distillation Strategies for Discriminative Speech Recognition Rescoring", "abstract": "Second-pass rescoring is employed in most state-of-the-art speech recognition\nsystems. Recently, BERT based models have gained popularity for re-ranking the\nn-best hypothesis by exploiting the knowledge from masked language model\npre-training. Further, fine-tuning with discriminative loss such as minimum\nword error rate (MWER) has shown to perform better than likelihood-based loss.\nStreaming applications with low latency requirements impose significant\nconstraints on the size of the models, thereby limiting the word error rate\n(WER) performance gains. In this paper, we propose effective strategies for\ndistilling from large models discriminatively trained with the MWER objective.\nWe experiment on Librispeech and production scale internal dataset for\nvoice-assistant. Our results demonstrate relative improvements of upto 7% WER\nover student models trained with MWER. We also show that the proposed\ndistillation can reduce the WER gap between the student and the teacher by 62%\nupto 100%.", "published": "2023-06-15 19:15:14", "link": "http://arxiv.org/abs/2306.09452v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Exploring Isolated Musical Notes as Pre-training Data for Predominant\n  Instrument Recognition in Polyphonic Music", "abstract": "With the growing amount of musical data available, automatic instrument\nrecognition, one of the essential problems in Music Information Retrieval\n(MIR), is drawing more and more attention. While automatic recognition of\nsingle instruments has been well-studied, it remains challenging for\npolyphonic, multi-instrument musical recordings. This work presents our efforts\ntoward building a robust end-to-end instrument recognition system for\npolyphonic multi-instrument music. We train our model using a pre-training and\nfine-tuning approach: we use a large amount of monophonic musical data for\npre-training and subsequently fine-tune the model for the polyphonic ensemble.\nIn pre-training, we apply data augmentation techniques to alleviate the domain\ngap between monophonic musical data and real-world music. We evaluate our\nmethod on the IRMAS testing data, a polyphonic musical dataset comprising\nprofessionally-produced commercial music recordings. Experimental results show\nthat our best model achieves a micro F1-score of 0.674 and an LRAP of 0.814,\nmeaning 10.9% and 8.9% relative improvement compared with the previous\nstate-of-the-art end-to-end approach. Also, we are able to build a lightweight\nmodel, achieving competitive performance with only 519K trainable parameters.", "published": "2023-06-15 04:27:15", "link": "http://arxiv.org/abs/2306.08850v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multichannel Active Noise Control with Exterior Radiation Suppression\n  Based on Riemannian Optimization", "abstract": "A multichannel active noise control (ANC) method with exterior radiation\nsuppression is proposed. When applying ANC in a three-dimensional space by\nusing multiple microphones and loudspeakers, the loudspeaker output can amplify\nnoise outside a region of target positions because most of current ANC methods\ndo not take into consideration the exterior radiation of secondary\nloudspeakers. We propose a normalized least mean square algorithm for\nfeedforward ANC in the frequency domain based on the Riemannian optimization to\nupdate the control filter with the exterior radiation power constrained to a\ntarget value. The advantages of the proposed method, compared with the\nalgorithm using a penalty term of exterior radiation, were validated by\nnumerical experiments: the exterior radiation power can be constrained during\nthe adaptation process and the parameter for the constraint can be determined\nin advance.", "published": "2023-06-15 04:48:10", "link": "http://arxiv.org/abs/2306.08855v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Time-Domain Wideband Image Source Method for Spherical Microphone Arrays", "abstract": "This paper presents the time-domain wideband spherical microphone array\nimpulse response generator (TDW-SMIR generator), which is a time-domain\nwideband image source method (ISM) for generating the room impulse responses\ncaptured by an open spherical microphone array. To incorporate loudspeaker\ndirectivity, the TDW-SMIR generator considers a source that emits a sequence of\nspherical wave fronts whose amplitudes are related to the loudspeaker\ndirectional impulse responses measured in the far-field. The TDW-SMIR generator\nuses geometric models to derive the time-domain signals recorded by the\nspherical microphone array. Comparisons are made with frequency-domain single\nband ISMs. Simulation results prove the results of the TDW-SMIR generator are\nsimilar to those of frequency-domain single band ISMs.", "published": "2023-06-15 13:48:51", "link": "http://arxiv.org/abs/2306.09135v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MobileASR: A resource-aware on-device learning framework for user voice\n  personalization applications on mobile phones", "abstract": "We describe a comprehensive methodology for developing user-voice\npersonalized automatic speech recognition (ASR) models by effectively training\nmodels on mobile phones, allowing user data and models to be stored and used\nlocally. To achieve this, we propose a resource-aware sub-model-based training\napproach that considers the RAM, and battery capabilities of mobile phones. By\nconsidering the evaluation metric and resource constraints of the mobile\nphones, we are able to perform efficient training and halt the process\naccordingly. To simulate real users, we use speakers with various accents. The\nentire on-device training and evaluation framework was then tested on various\nmobile phones across brands. We show that fine-tuning the models and selecting\nthe right hyperparameter values is a trade-off between the lowest achievable\nperformance metric, on-device training time, and memory consumption. Overall,\nour methodology offers a comprehensive solution for developing personalized ASR\nmodels while leveraging the capabilities of mobile phones, and balancing the\nneed for accuracy with resource constraints.", "published": "2023-06-15 13:44:45", "link": "http://arxiv.org/abs/2306.09384v2", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "Competitive and Resource Efficient Factored Hybrid HMM Systems are\n  Simpler Than You Think", "abstract": "Building competitive hybrid hidden Markov model~(HMM) systems for automatic\nspeech recognition~(ASR) requires a complex multi-stage pipeline consisting of\nseveral training criteria. The recent sequence-to-sequence models offer the\nadvantage of having simpler pipelines that can start from-scratch. We propose a\npurely neural based single-stage from-scratch pipeline for a context-dependent\nhybrid HMM that offers similar simplicity. We use an alignment from a full-sum\ntrained zero-order posterior HMM with a BLSTM encoder. We show that with this\nalignment we can build a Conformer factored hybrid that performs even better\nthan both a state-of-the-art classic hybrid and a factored hybrid trained with\nalignments taken from more complex Gaussian mixture based systems. Our finding\nis confirmed on Switchboard 300h and LibriSpeech 960h tasks with comparable\nresults to other approaches in the literature, and by additionally relying on a\nresponsible choice of available computational resources.", "published": "2023-06-15 21:34:31", "link": "http://arxiv.org/abs/2306.09517v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Unsupervised speech intelligibility assessment with utterance level\n  alignment distance between teacher and learner Wav2Vec-2.0 representations", "abstract": "Speech intelligibility is crucial in language learning for effective\ncommunication. Thus, to develop computer-assisted language learning systems,\nautomatic speech intelligibility detection (SID) is necessary. Most of the\nworks have assessed the intelligibility in a supervised manner considering\nmanual annotations, which requires cost and time; hence scalability is limited.\nTo overcome these, this work proposes an unsupervised approach for SID. The\nproposed approach considers alignment distance computed with dynamic-time\nwarping (DTW) between teacher and learner representation sequence as a measure\nto separate intelligible versus non-intelligible speech. We obtain the feature\nsequence using current state-of-the-art self-supervised representations from\nWav2Vec-2.0. We found the detection accuracies as 90.37\\%, 92.57\\% and 96.58\\%,\nrespectively, with three alignment distance measures -- mean absolute error,\nmean squared error and cosine distance (equal to one minus cosine similarity).", "published": "2023-06-15 04:18:30", "link": "http://arxiv.org/abs/2306.08845v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multi-Loss Convolutional Network with Time-Frequency Attention for\n  Speech Enhancement", "abstract": "The Dual-Path Convolution Recurrent Network (DPCRN) was proposed to\neffectively exploit time-frequency domain information. By combining the DPRNN\nmodule with Convolution Recurrent Network (CRN), the DPCRN obtained a promising\nperformance in speech separation with a limited model size. In this paper, we\nexplore self-attention in the DPCRN module and design a model called Multi-Loss\nConvolutional Network with Time-Frequency Attention(MNTFA) for speech\nenhancement. We use self-attention modules to exploit the long-time\ninformation, where the intra-chunk self-attentions are used to model the\nspectrum pattern and the inter-chunk self-attention are used to model the\ndependence between consecutive frames. Compared to DPRNN, axial self-attention\ngreatly reduces the need for memory and computation, which is more suitable for\nlong sequences of speech signals. In addition, we propose a joint training\nmethod of a multi-resolution STFT loss and a WavLM loss using a pre-trained\nWavLM network. Experiments show that with only 0.23M parameters, the proposed\nmodel achieves a better performance than DPCRN.", "published": "2023-06-15 08:48:19", "link": "http://arxiv.org/abs/2306.08956v1", "categories": ["cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Team AcieLee: Technical Report for EPIC-SOUNDS Audio-Based Interaction\n  Recognition Challenge 2023", "abstract": "In this report, we describe the technical details of our submission to the\nEPIC-SOUNDS Audio-Based Interaction Recognition Challenge 2023, by Team\n\"AcieLee\" (username: Yuqi\\_Li). The task is to classify the audio caused by\ninteractions between objects, or from events of the camera wearer. We conducted\nexhaustive experiments and found learning rate step decay, backbone frozen,\nlabel smoothing and focal loss contribute most to the performance improvement.\nAfter training, we combined multiple models from different stages and\nintegrated them into a single model by assigning fusion weights. This proposed\nmethod allowed us to achieve 3rd place in the CVPR 2023 workshop of EPIC-SOUNDS\nAudio-Based Interaction Recognition Challenge.", "published": "2023-06-15 09:49:07", "link": "http://arxiv.org/abs/2306.08998v1", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "CoverHunter: Cover Song Identification with Refined Attention and\n  Alignments", "abstract": "Abstract: Cover song identification (CSI) focuses on finding the same music\nwith different versions in reference anchors given a query track. In this\npaper, we propose a novel system named CoverHunter that overcomes the\nshortcomings of existing detection schemes by exploring richer features with\nrefined attention and alignments. CoverHunter contains three key modules: 1) A\nconvolution-augmented transformer (i.e., Conformer) structure that captures\nboth local and global feature interactions in contrast to previous methods\nmainly relying on convolutional neural networks; 2) An attention-based time\npooling module that further exploits the attention in the time dimension; 3) A\nnovel coarse-to-fine training scheme that first trains a network to roughly\nalign the song chunks and then refines the network by training on the aligned\nchunks. At the same time, we also summarize some important training tricks used\nin our system that help achieve better results. Experiments on several standard\nCSI datasets show that our method significantly improves over state-of-the-art\nmethods with an embedding size of 128 (2.3% on SHS100K-TEST and 17.7% on\nDaTacos).", "published": "2023-06-15 10:34:20", "link": "http://arxiv.org/abs/2306.09025v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Few-shot bioacoustic event detection at the DCASE 2023 challenge", "abstract": "Few-shot bioacoustic event detection consists in detecting sound events of\nspecified types, in varying soundscapes, while having access to only a few\nexamples of the class of interest. This task ran as part of the DCASE challenge\nfor the third time this year with an evaluation set expanded to include new\nanimal species, and a new rule: ensemble models were no longer allowed. The\n2023 few shot task received submissions from 6 different teams with F-scores\nreaching as high as 63% on the evaluation set. Here we describe the task,\nfocusing on describing the elements that differed from previous years. We also\ntake a look back at past editions to describe how the task has evolved. Not\nonly have the F-score results steadily improved (40% to 60% to 63%), but the\ntype of systems proposed have also become more complex. Sound event detection\nsystems are no longer simple variations of the baselines provided: multiple\nfew-shot learning methodologies are still strong contenders for the task.", "published": "2023-06-15 15:59:26", "link": "http://arxiv.org/abs/2306.09223v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Sound Demixing Challenge 2023 Music Demixing Track Technical Report:\n  TFC-TDF-UNet v3", "abstract": "In this report, we present our award-winning solutions for the Music Demixing\nTrack of Sound Demixing Challenge 2023. First, we propose TFC-TDF-UNet v3, a\ntime-efficient music source separation model that achieves state-of-the-art\nresults on the MUSDB benchmark. We then give full details regarding our\nsolutions for each Leaderboard, including a loss masking approach for\nnoise-robust training. Code for reproducing model training and final\nsubmissions is available at github.com/kuielab/sdx23.", "published": "2023-06-15 12:59:04", "link": "http://arxiv.org/abs/2306.09382v3", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Taming Diffusion Models for Music-driven Conducting Motion Generation", "abstract": "Generating the motion of orchestral conductors from a given piece of symphony\nmusic is a challenging task since it requires a model to learn semantic music\nfeatures and capture the underlying distribution of real conducting motion.\nPrior works have applied Generative Adversarial Networks (GAN) to this task,\nbut the promising diffusion model, which recently showed its advantages in\nterms of both training stability and output quality, has not been exploited in\nthis context. This paper presents Diffusion-Conductor, a novel DDIM-based\napproach for music-driven conducting motion generation, which integrates the\ndiffusion model to a two-stage learning framework. We further propose a random\nmasking strategy to improve the feature robustness, and use a pair of geometric\nloss functions to impose additional regularizations and increase motion\ndiversity. We also design several novel metrics, including Frechet Gesture\nDistance (FGD) and Beat Consistency Score (BC) for a more comprehensive\nevaluation of the generated motion. Experimental results demonstrate the\nadvantages of our model.", "published": "2023-06-15 03:49:24", "link": "http://arxiv.org/abs/2306.10065v2", "categories": ["eess.AS", "cs.AI", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Audio Tagging on an Embedded Hardware Platform", "abstract": "Convolutional neural networks (CNNs) have exhibited state-of-the-art\nperformance in various audio classification tasks. However, their real-time\ndeployment remains a challenge on resource-constrained devices like embedded\nsystems. In this paper, we analyze how the performance of large-scale\npretrained audio neural networks designed for audio pattern recognition changes\nwhen deployed on a hardware such as Raspberry Pi. We empirically study the role\nof CPU temperature, microphone quality and audio signal volume on performance.\nOur experiments reveal that the continuous CPU usage results in an increased\ntemperature that can trigger an automated slowdown mechanism in the Raspberry\nPi, impacting inference latency. The quality of a microphone, specifically with\naffordable devices like the Google AIY Voice Kit, and audio signal volume, all\naffect the system performance. In the course of our investigation, we encounter\nsubstantial complications linked to library compatibility and the unique\nprocessor architecture requirements of the Raspberry Pi, making the process\nless straightforward compared to conventional computers (PCs). Our\nobservations, while presenting challenges, pave the way for future researchers\nto develop more compact machine learning models, design heat-dissipative\nhardware, and select appropriate microphones when AI models are deployed for\nreal-time applications on edge devices. All related assets and an interactive\ndemo can be found on GitHub", "published": "2023-06-15 13:02:41", "link": "http://arxiv.org/abs/2306.09106v1", "categories": ["cs.SD", "cs.AI", "cs.SY", "eess.AS", "eess.SY"], "primary_category": "cs.SD"}
{"title": "STARSS23: An Audio-Visual Dataset of Spatial Recordings of Real Scenes\n  with Spatiotemporal Annotations of Sound Events", "abstract": "While direction of arrival (DOA) of sound events is generally estimated from\nmultichannel audio data recorded in a microphone array, sound events usually\nderive from visually perceptible source objects, e.g., sounds of footsteps come\nfrom the feet of a walker. This paper proposes an audio-visual sound event\nlocalization and detection (SELD) task, which uses multichannel audio and video\ninformation to estimate the temporal activation and DOA of target sound events.\nAudio-visual SELD systems can detect and localize sound events using signals\nfrom a microphone array and audio-visual correspondence. We also introduce an\naudio-visual dataset, Sony-TAu Realistic Spatial Soundscapes 2023 (STARSS23),\nwhich consists of multichannel audio data recorded with a microphone array,\nvideo data, and spatiotemporal annotation of sound events. Sound scenes in\nSTARSS23 are recorded with instructions, which guide recording participants to\nensure adequate activity and occurrences of sound events. STARSS23 also serves\nhuman-annotated temporal activation labels and human-confirmed DOA labels,\nwhich are based on tracking results of a motion capture system. Our benchmark\nresults demonstrate the benefits of using visual object positions in\naudio-visual SELD tasks. The data is available at\nhttps://zenodo.org/record/7880637.", "published": "2023-06-15 13:37:14", "link": "http://arxiv.org/abs/2306.09126v2", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS", "eess.IV"], "primary_category": "cs.SD"}
{"title": "Diff-TTSG: Denoising probabilistic integrated speech and gesture\n  synthesis", "abstract": "With read-aloud speech synthesis achieving high naturalness scores, there is\na growing research interest in synthesising spontaneous speech. However, human\nspontaneous face-to-face conversation has both spoken and non-verbal aspects\n(here, co-speech gestures). Only recently has research begun to explore the\nbenefits of jointly synthesising these two modalities in a single system. The\nprevious state of the art used non-probabilistic methods, which fail to capture\nthe variability of human speech and motion, and risk producing oversmoothing\nartefacts and sub-optimal synthesis quality. We present the first\ndiffusion-based probabilistic model, called Diff-TTSG, that jointly learns to\nsynthesise speech and gestures together. Our method can be trained on small\ndatasets from scratch. Furthermore, we describe a set of careful uni- and\nmulti-modal subjective tests for evaluating integrated speech and gesture\nsynthesis systems, and use them to validate our proposed approach. Please see\nhttps://shivammehta25.github.io/Diff-TTSG/ for video examples, data, and code.", "published": "2023-06-15 18:02:49", "link": "http://arxiv.org/abs/2306.09417v3", "categories": ["eess.AS", "cs.AI", "cs.CV", "cs.HC", "cs.LG", "68T07 (Primary), 68T42 (Secondary)", "I.2.7; I.2.6; G.3; H.5.5"], "primary_category": "eess.AS"}
