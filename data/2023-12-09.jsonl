{"title": "Beneath the Surface: Unveiling Harmful Memes with Multimodal Reasoning\n  Distilled from Large Language Models", "abstract": "The age of social media is rife with memes. Understanding and detecting\nharmful memes pose a significant challenge due to their implicit meaning that\nis not explicitly conveyed through the surface text and image. However,\nexisting harmful meme detection approaches only recognize superficial\nharm-indicative signals in an end-to-end classification manner but ignore\nin-depth cognition of the meme text and image. In this paper, we attempt to\ndetect harmful memes based on advanced reasoning over the interplay of\nmultimodal information in memes. Inspired by the success of Large Language\nModels (LLMs) on complex reasoning, we first conduct abductive reasoning with\nLLMs. Then we propose a novel generative framework to learn reasonable thoughts\nfrom LLMs for better multimodal fusion and lightweight fine-tuning, which\nconsists of two training stages: 1) Distill multimodal reasoning knowledge from\nLLMs; and 2) Fine-tune the generative framework to infer harmfulness. Extensive\nexperiments conducted on three meme datasets demonstrate that our proposed\napproach achieves superior performance than state-of-the-art methods on the\nharmful meme detection task.", "published": "2023-12-09 01:59:11", "link": "http://arxiv.org/abs/2312.05434v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Robustness of Foundation Model Representations under\n  Provenance-related Distribution Shifts", "abstract": "Foundation models are a current focus of attention in both industry and\nacademia. While they have shown their capabilities in a variety of tasks,\nin-depth research is required to determine their robustness to distribution\nshift when used as a basis for supervised machine learning. This is especially\nimportant in the context of clinical data, with particular limitations related\nto data accessibility, lack of pretraining materials, and limited availability\nof high-quality annotations. In this work, we examine the stability of models\nbased on representations from foundation models under distribution shift. We\nfocus on confounding by provenance, a form of distribution shift that emerges\nin the context of multi-institutional datasets when there are differences in\nsource-specific language use and class distributions. Using a sampling strategy\nthat synthetically induces varying degrees of distribution shift, we evaluate\nthe extent to which representations from foundation models result in\npredictions that are inherently robust to confounding by provenance.\nAdditionally, we examine the effectiveness of a straightforward confounding\nadjustment method inspired by Pearl's conception of backdoor adjustment.\nResults indicate that while foundation models do show some out-of-the-box\nrobustness to confounding-by-provenance related distribution shifts, this can\nbe considerably improved through adjustment. These findings suggest a need for\ndeliberate adjustment of predictive models using representations from\nfoundation models in the context of source-specific distributional differences.", "published": "2023-12-09 02:02:45", "link": "http://arxiv.org/abs/2312.05435v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Domain Adaptation of a State of the Art Text-to-SQL Model: Lessons\n  Learned and Challenges Found", "abstract": "There are many recent advanced developments for the Text-to-SQL task, where\nthe Picard model is one of the the top performing models as measured by the\nSpider dataset competition. However, bringing Text-to-SQL systems to realistic\nuse-cases through domain adaptation remains a tough challenge. We analyze how\nwell the base T5 Language Model and Picard perform on query structures\ndifferent from the Spider dataset, we fine-tuned the base model on the Spider\ndata and on independent databases (DB). To avoid accessing the DB content\nonline during inference, we also present an alternative way to disambiguate the\nvalues in an input question using a rule-based approach that relies on an\nintermediate representation of the semantic concepts of an input question. In\nour results we show in what cases T5 and Picard can deliver good performance,\nwe share the lessons learned, and discuss current domain adaptation challenges.", "published": "2023-12-09 03:30:21", "link": "http://arxiv.org/abs/2312.05448v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Textual Toxicity in Social Media: Understanding the Bangla Toxic\n  Language Expressed in Facebook Comment", "abstract": "Social Media is a repository of digital literature including user-generated\ncontent. The users of social media are expressing their opinion with diverse\nmediums such as text, emojis, memes, and also through other visual and textual\nmediums. A major portion of these media elements could be treated as harmful to\nothers and they are known by many words including Cyberbullying and Toxic\nLanguage . The goal of this research paper is to analyze a curated and\nvalue-added dataset of toxic language titled ToxLex_bn . It is an exhaustive\nwordlist that can be used as classifier material to detect toxicity in social\nmedia. The toxic language/script used by the Bengali community as\ncyberbullying, hate speech and moral policing became major trends in social\nmedia culture in Bangladesh and West Bengal. The toxicity became so high that\nthe victims has to post as a counter or release explanation video for the\nhaters. Most cases are pointed to women celebrity and their relation, dress,\nlifestyle are became trolled and toxicity flooded in comments boxes. Not only\ncelebrity bashing but also hates occurred between Hindu Muslims,\nIndia-Bangladesh, Two opponents of 1971 and these are very common for virtual\nconflict in the comment thread. Even many times facebook comment causes sue and\nlegal matters in Bangladesh and thus it requires more study. In this study, a\nBangla toxic language dataset has been analyzed which was inputted by the user\nin Bengali script & language. For this, about 1968 unique bigrams or phrases as\nwordlists have been analyzed which are derived from 2207590 comments. It is\nassumed that this analysis will reinforce the detection of Bangla's toxic\nlanguage used in social media and thus cure this virtual disease.", "published": "2023-12-09 05:04:34", "link": "http://arxiv.org/abs/2312.05467v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Teamwork Dimensions Classification Using BERT", "abstract": "Teamwork is a necessary competency for students that is often inadequately\nassessed. Towards providing a formative assessment of student teamwork, an\nautomated natural language processing approach was developed to identify\nteamwork dimensions of students' online team chat. Developments in the field of\nnatural language processing and artificial intelligence have resulted in\nadvanced deep transfer learning approaches namely the Bidirectional Encoder\nRepresentations from Transformers (BERT) model that allow for more in-depth\nunderstanding of the context of the text. While traditional machine learning\nalgorithms were used in the previous work for the automatic classification of\nchat messages into the different teamwork dimensions, our findings have shown\nthat classifiers based on the pre-trained language model BERT provides improved\nclassification performance, as well as much potential for generalizability in\nthe language use of varying team chat contexts and team member demographics.\nThis model will contribute towards an enhanced learning analytics tool for\nteamwork assessment and feedback.", "published": "2023-12-09 07:18:41", "link": "http://arxiv.org/abs/2312.05483v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "History Matters: Temporal Knowledge Editing in Large Language Model", "abstract": "The imperative task of revising or updating the knowledge stored within large\nlanguage models arises from two distinct sources: intrinsic errors inherent in\nthe model which should be corrected and outdated knowledge due to external\nshifts in the real world which should be updated. Prevailing efforts in model\nediting conflate these two distinct categories of edits arising from distinct\nreasons and directly modify the original knowledge in models into new\nknowledge. However, we argue that preserving the model's original knowledge\nremains pertinent. Specifically, if a model's knowledge becomes outdated due to\nevolving worldly dynamics, it should retain recollection of the historical\nknowledge while integrating the newfound knowledge. In this work, we introduce\nthe task of Temporal Knowledge Editing (TKE) and establish a benchmark AToKe\n(Assessment of TempOral Knowledge Editing) to evaluate current model editing\nmethods. We find that while existing model editing methods are effective at\nmaking models remember new knowledge, the edited model catastrophically forgets\nhistorical knowledge. To address this gap, we propose a simple and general\nframework termed Multi-Editing with Time Objective (METO) for enhancing\nexisting editing models, which edits both historical and new knowledge\nconcurrently and optimizes the model's prediction for the time of each fact.\nOur assessments demonstrate that while AToKe is still difficult, METO maintains\nthe effectiveness of learning new knowledge and meanwhile substantially\nimproves the performance of edited models on utilizing historical knowledge.", "published": "2023-12-09 07:51:56", "link": "http://arxiv.org/abs/2312.05497v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Augmenty: A Python Library for Structured Text Augmentation", "abstract": "Augmnety is a Python library for structured text augmentation. It is built on\ntop of spaCy and allows for augmentation of both the text and its annotations.\nAugmenty provides a wide range of augmenters which can be combined in a\nflexible manner to create complex augmentation pipelines. It also includes a\nset of primitives that can be used to create custom augmenters such as word\nreplacement augmenters. This functionality allows for augmentations within a\nrange of applications such as named entity recognition (NER), part-of-speech\ntagging, and dependency parsing.", "published": "2023-12-09 10:24:59", "link": "http://arxiv.org/abs/2312.05520v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PILLOW: Enhancing Efficient Instruction Fine-tuning via Prompt Matching", "abstract": "Instruction fine-tuning has conventionally been employed to adapt Large\nLanguage Models (LLMs) to a variety of tasks. Nonetheless, this technique often\nnecessitates substantial computational resources, making it impractical for\ndeployment by individuals or small-scale entities. Recently, Low-Rank\nAdaptation (LoRA) has become a promising alternative, offering high\ncapabilities on par with full tuning with reduced resource overhead. However,\nattaining satisfactory performance through the fine-tuning of LoRA is a\nnon-trivial challenge. In this paper, we propose PILLOW, which aims to improve\nLoRA's performance by a discrimination-based prompting method, leveraging LLMs'\nIn-Context Learning ability. PILLOW incorporates a matching network that\nselects prompts from a user-defined prompt pool, concatenates the selected\nprompts with the user instruction as input, and performs inference using the\nLoRA-fine-tuned LLMs. Trained with Reinforcement Learning, PILLOW exhibits\ncommensurate performance on various evaluation metrics compared with typical\ninstruction fine-tuning methods, utilizing only consumer-grade GPU resources\nand exhibiting a large reduction in computational costs.", "published": "2023-12-09 17:38:39", "link": "http://arxiv.org/abs/2312.05621v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding the Effect of Model Compression on Social Bias in Large\n  Language Models", "abstract": "Large Language Models (LLMs) trained with self-supervision on vast corpora of\nweb text fit to the social biases of that text. Without intervention, these\nsocial biases persist in the model's predictions in downstream tasks, leading\nto representational harm. Many strategies have been proposed to mitigate the\neffects of inappropriate social biases learned during pretraining.\nSimultaneously, methods for model compression have become increasingly popular\nto reduce the computational burden of LLMs. Despite the popularity and need for\nboth approaches, little work has been done to explore the interplay between\nthese two. We perform a carefully controlled study of the impact of model\ncompression via quantization and knowledge distillation on measures of social\nbias in LLMs. Longer pretraining and larger models led to higher social bias,\nand quantization showed a regularizer effect with its best trade-off around 20%\nof the original pretraining time.", "published": "2023-12-09 20:04:20", "link": "http://arxiv.org/abs/2312.05662v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hate Speech and Offensive Content Detection in Indo-Aryan Languages: A\n  Battle of LSTM and Transformers", "abstract": "Social media platforms serve as accessible outlets for individuals to express\ntheir thoughts and experiences, resulting in an influx of user-generated data\nspanning all age groups. While these platforms enable free expression, they\nalso present significant challenges, including the proliferation of hate speech\nand offensive content. Such objectionable language disrupts objective discourse\nand can lead to radicalization of debates, ultimately threatening democratic\nvalues. Consequently, organizations have taken steps to monitor and curb\nabusive behavior, necessitating automated methods for identifying suspicious\nposts. This paper contributes to Hate Speech and Offensive Content\nIdentification in English and Indo-Aryan Languages (HASOC) 2023 shared tasks\ntrack. We, team Z-AGI Labs, conduct a comprehensive comparative analysis of\nhate speech classification across five distinct languages: Bengali, Assamese,\nBodo, Sinhala, and Gujarati. Our study encompasses a wide range of pre-trained\nmodels, including Bert variants, XLM-R, and LSTM models, to assess their\nperformance in identifying hate speech across these languages. Results reveal\nintriguing variations in model performance. Notably, Bert Base Multilingual\nCased emerges as a strong performer across languages, achieving an F1 score of\n0.67027 for Bengali and 0.70525 for Assamese. At the same time, it\nsignificantly outperforms other models with an impressive F1 score of 0.83009\nfor Bodo. In Sinhala, XLM-R stands out with an F1 score of 0.83493, whereas for\nGujarati, a custom LSTM-based model outshined with an F1 score of 0.76601. This\nstudy offers valuable insights into the suitability of various pre-trained\nmodels for hate speech detection in multilingual settings. By considering the\nnuances of each, our research contributes to an informed model selection for\nbuilding robust hate speech detection systems.", "published": "2023-12-09 20:24:00", "link": "http://arxiv.org/abs/2312.05671v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Two Directions for Clinical Data Generation with Large Language Models:\n  Data-to-Label and Label-to-Data", "abstract": "Large language models (LLMs) can generate natural language texts for various\ndomains and tasks, but their potential for clinical text mining, a domain with\nscarce, sensitive, and imbalanced medical data, is underexplored. We\ninvestigate whether LLMs can augment clinical data for detecting Alzheimer's\nDisease (AD)-related signs and symptoms from electronic health records (EHRs),\na challenging task that requires high expertise. We create a novel pragmatic\ntaxonomy for AD sign and symptom progression based on expert knowledge, which\nguides LLMs to generate synthetic data following two different directions:\n\"data-to-label\", which labels sentences from a public EHR collection with\nAD-related signs and symptoms; and \"label-to-data\", which generates sentences\nwith AD-related signs and symptoms based on the label definition. We train a\nsystem to detect AD-related signs and symptoms from EHRs, using three datasets:\n(1) a gold dataset annotated by human experts on longitudinal EHRs of AD\npatients; (2) a silver dataset created by the data-to-label method; and (3) a\nbronze dataset created by the label-to-data method. We find that using the\nsilver and bronze datasets improves the system performance, outperforming the\nsystem using only the gold dataset. This shows that LLMs can generate synthetic\nclinical data for a complex task by incorporating expert knowledge, and our\nlabel-to-data method can produce datasets that are free of sensitive\ninformation, while maintaining acceptable quality.", "published": "2023-12-09 19:35:40", "link": "http://arxiv.org/abs/2401.06774v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-Grained Analysis of Team Collaborative Dialogue", "abstract": "Natural language analysis of human collaborative chat dialogues is an\nunderstudied domain with many unique challenges: a large number of dialogue act\nlabels, underspecified and dynamic tasks, interleaved topics, and long-range\ncontextual dependence. While prior work has studied broad metrics of team\ndialogue and associated performance using methods such as LSA, there has been\nlittle effort in generating fine-grained descriptions of team dynamics and\nindividual performance from dialogue. We describe initial work towards\ndeveloping an explainable analytics tool in the software development domain\nusing Slack chats mined from our organization, including generation of a novel,\nhierarchical labeling scheme; design of descriptive metrics based on the\nfrequency of occurrence of dialogue acts; and initial results using a\ntransformer + CRF architecture to incorporate long-range context.", "published": "2023-12-09 05:38:32", "link": "http://arxiv.org/abs/2312.05471v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Using Captum to Explain Generative Language Models", "abstract": "Captum is a comprehensive library for model explainability in PyTorch,\noffering a range of methods from the interpretability literature to enhance\nusers' understanding of PyTorch models. In this paper, we introduce new\nfeatures in Captum that are specifically designed to analyze the behavior of\ngenerative language models. We provide an overview of the available\nfunctionalities and example applications of their potential for understanding\nlearned associations within generative language models.", "published": "2023-12-09 07:35:24", "link": "http://arxiv.org/abs/2312.05491v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Enhancing Medical Specialty Assignment to Patients using NLP Techniques", "abstract": "The introduction of Large Language Models (LLMs), and the vast volume of\npublicly available medical data, amplified the application of NLP to the\nmedical domain. However, LLMs are pretrained on data that are not explicitly\nrelevant to the domain that are applied to and are often biased towards the\noriginal data they were pretrained upon. Even when pretrained on domainspecific\ndata, these models typically require time-consuming fine-tuning to achieve good\nperformance for a specific task. To address these limitations, we propose an\nalternative approach that achieves superior performance while being\ncomputationally efficient. Specifically, we utilize keywords to train a deep\nlearning architecture that outperforms a language model pretrained on a large\ncorpus of text. Our proposal does not require pretraining nor fine-tuning and\ncan be applied directly to a specific setting for performing multi-label\nclassification. Our objective is to automatically assign a new patient to the\nspecialty of the medical professional they require, using a dataset that\ncontains medical transcriptions and relevant keywords. To this end, we\nfine-tune the PubMedBERT model on this dataset, which serves as the baseline\nfor our experiments. We then twice train/fine-tune a DNN and the RoBERTa\nlanguage model, using both the keywords and the full transcriptions as input.\nWe compare the performance of these approaches using relevant metrics. Our\nresults demonstrate that utilizing keywords for text classification\nsignificantly improves classification performance, for both a basic DL\narchitecture and a large language model. Our approach represents a promising\nand efficient alternative to traditional methods for finetuning language models\non domain-specific data and has potential applications in various medical\ndomains", "published": "2023-12-09 14:13:45", "link": "http://arxiv.org/abs/2312.05585v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sim-GPT: Text Similarity via GPT Annotated Data", "abstract": "Due to the lack of a large collection of high-quality labeled sentence pairs\nwith textual similarity scores, existing approaches for Semantic Textual\nSimilarity (STS) mostly rely on unsupervised techniques or training signals\nthat are only partially correlated with textual similarity, e.g., NLI-based\ndatasets. To tackle this issue, in this paper, we propose the strategy of\nmeasuring text similarity via GPT annotated data (Sim-GPT for short). The core\nidea of Sim-GPT is to generate data with STS labels using GPT-4, based on which\nan STS model is trained. Sim-GPT framework utilizes LLMs to provide a\nsubstantial amount of reliable annotated data filling the gap of the lack of\ntraining signals for STS. Sim-GPT is trained on a one-time generated dataset\nusing BERT or RoBERTa as the backbone, which offers long-term savings in cost\nand speed compared to repeatedly invoking LLMs for each sentence pair. Trained\non the examples from GPT-4 (371K), Sim-GPT yields SOTA performances on the\nwidely-used seven STS benchmarks: +0.99 over supervised-SimCSE, and +0.42 over\nthe current SOTA PromCSE model. To encourage further advancements of the field,\nwe release both models and the 371K annotated examples from GPT-4. Code, models\nand annotated data are available at: https://github.com/ShuheWang1998/Sim-GPT.", "published": "2023-12-09 16:10:23", "link": "http://arxiv.org/abs/2312.05603v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Can Large Language Models Serve as Rational Players in Game Theory? A\n  Systematic Analysis", "abstract": "Game theory, as an analytical tool, is frequently utilized to analyze human\nbehavior in social science research. With the high alignment between the\nbehavior of Large Language Models (LLMs) and humans, a promising research\ndirection is to employ LLMs as substitutes for humans in game experiments,\nenabling social science research. However, despite numerous empirical\nresearches on the combination of LLMs and game theory, the capability\nboundaries of LLMs in game theory remain unclear. In this research, we endeavor\nto systematically analyze LLMs in the context of game theory. Specifically,\nrationality, as the fundamental principle of game theory, serves as the metric\nfor evaluating players' behavior -- building a clear desire, refining belief\nabout uncertainty, and taking optimal actions. Accordingly, we select three\nclassical games (dictator game, Rock-Paper-Scissors, and ring-network game) to\nanalyze to what extent LLMs can achieve rationality in these three aspects. The\nexperimental results indicate that even the current state-of-the-art LLM\n(GPT-4) exhibits substantial disparities compared to humans in game theory. For\ninstance, LLMs struggle to build desires based on uncommon preferences, fail to\nrefine belief from many simple patterns, and may overlook or modify refined\nbelief when taking actions. Therefore, we consider that introducing LLMs into\ngame experiments in the field of social science should be approached with\ngreater caution.", "published": "2023-12-09 07:33:26", "link": "http://arxiv.org/abs/2312.05488v2", "categories": ["cs.AI", "cs.CL", "cs.GT"], "primary_category": "cs.AI"}
{"title": "Aligner: One Global Token is Worth Millions of Parameters When Aligning\n  Large Language Models", "abstract": "We introduce Aligner, a novel Parameter-Efficient Fine-Tuning (PEFT) method\nfor aligning multi-billion-parameter-sized Large Language Models (LLMs).\nAligner employs a unique design that constructs a globally shared set of\ntunable tokens that modify the attention of every layer. Remarkably with this\nmethod, even when using one token accounting for a mere 5,000 parameters,\nAligner can still perform comparably well to state-of-the-art LLM adaptation\nmethods like LoRA that require millions of parameters. This capacity is\nsubstantiated in both instruction following and value alignment tasks. Besides\nthe multiple order-of-magnitude improvement in parameter efficiency, the\ninsight Aligner provides into the internal mechanisms of LLMs is also valuable.\nThe architectural features and efficacy of our method, in addition to our\nexperiments demonstrate that an LLM separates its internal handling of \"form\"\nand \"knowledge\" in a somewhat orthogonal manner. This finding promises to\nmotivate new research into LLM mechanism understanding and value alignment.", "published": "2023-12-09 08:25:55", "link": "http://arxiv.org/abs/2312.05503v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2; I.2.6; I.2.7"], "primary_category": "cs.CL"}
{"title": "Batched Low-Rank Adaptation of Foundation Models", "abstract": "Low-Rank Adaptation (LoRA) has recently gained attention for fine-tuning\nfoundation models by incorporating trainable low-rank matrices, thereby\nreducing the number of trainable parameters. While LoRA offers numerous\nadvantages, its applicability for real-time serving to a diverse and global\nuser base is constrained by its incapability to handle multiple task-specific\nadapters efficiently. This imposes a performance bottleneck in scenarios\nrequiring personalized, task-specific adaptations for each incoming request. To\nmitigate this constraint, we introduce Fast LoRA (FLoRA), a framework in which\neach input example in a minibatch can be associated with its unique low-rank\nadaptation weights, allowing for efficient batching of heterogeneous requests.\nWe empirically demonstrate that FLoRA retains the performance merits of LoRA,\nshowcasing competitive results on the MultiPL-E code generation benchmark\nspanning over 8 languages and a multilingual speech recognition task across 6\nlanguages.", "published": "2023-12-09 20:51:48", "link": "http://arxiv.org/abs/2312.05677v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Agile-Quant: Activation-Guided Quantization for Faster Inference of LLMs\n  on the Edge", "abstract": "Large Language Models (LLMs) stand out for their impressive performance in\nintricate language modeling tasks. However, their demanding computational and\nmemory needs pose obstacles for broad use on edge devices. Quantization is then\nintroduced to boost LLMs' on-device efficiency. Recent works show that 8-bit or\nlower weight quantization is feasible with minimal impact on end-to-end task\nperformance, while the activation is still not quantized. On the other hand,\nmainstream commodity edge devices still struggle to execute these sub-8-bit\nquantized networks effectively. In this paper, we propose Agile-Quant, an\nactivation-guided quantization framework for popular Large Language Models\n(LLMs), and implement an end-to-end accelerator on multiple edge devices for\nfaster inference. Considering the hardware profiling and activation analysis,\nwe first introduce a basic activation quantization strategy to balance the\ntrade-off of task performance and real inference speed. Then we leverage the\nactivation-aware token pruning technique to reduce the outliers and the adverse\nimpact on attentivity. Ultimately, we utilize the SIMD-based 4-bit multiplier\nand our efficient TRIP matrix multiplication to implement the accelerator for\nLLMs on the edge. We apply our framework on different scales of LLMs including\nLLaMA, OPT, and BLOOM with 4-bit or 8-bit for the activation and 4-bit for the\nweight quantization. Experiments show that Agile-Quant achieves simultaneous\nquantization of model weights and activations while maintaining task\nperformance comparable to existing weight-only quantization methods. Moreover,\nin the 8- and 4-bit scenario, Agile-Quant achieves an on-device speedup of up\nto 2.55x compared to its FP16 counterparts across multiple edge devices,\nmarking a pioneering advancement in this domain.", "published": "2023-12-09 22:12:52", "link": "http://arxiv.org/abs/2312.05693v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Steering Llama 2 via Contrastive Activation Addition", "abstract": "We introduce Contrastive Activation Addition (CAA), an innovative method for\nsteering language models by modifying their activations during forward passes.\nCAA computes \"steering vectors\" by averaging the difference in residual stream\nactivations between pairs of positive and negative examples of a particular\nbehavior, such as factual versus hallucinatory responses. During inference,\nthese steering vectors are added at all token positions after the user's prompt\nwith either a positive or negative coefficient, allowing precise control over\nthe degree of the targeted behavior. We evaluate CAA's effectiveness on Llama 2\nChat using multiple-choice behavioral question datasets and open-ended\ngeneration tasks. We demonstrate that CAA significantly alters model behavior,\nis effective over and on top of traditional methods like finetuning and system\nprompt design, and minimally reduces capabilities. Moreover, we gain deeper\ninsights into CAA's mechanisms by employing various activation space\ninterpretation methods. CAA accurately steers model outputs and sheds light on\nhow high-level concepts are represented in Large Language Models (LLMs).", "published": "2023-12-09 04:40:46", "link": "http://arxiv.org/abs/2312.06681v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Labrador: Exploring the Limits of Masked Language Modeling for\n  Laboratory Data", "abstract": "In this work we introduce Labrador, a pre-trained Transformer model for\nlaboratory data. Labrador and BERT were pre-trained on a corpus of 100 million\nlab test results from electronic health records (EHRs) and evaluated on various\ndownstream outcome prediction tasks. Both models demonstrate mastery of the\npre-training task but neither consistently outperform XGBoost on downstream\nsupervised tasks. Our ablation studies reveal that transfer learning shows\nlimited effectiveness for BERT and achieves marginal success with Labrador. We\nexplore the reasons for the failure of transfer learning and suggest that the\ndata generating process underlying each patient cannot be characterized\nsufficiently using labs alone, among other factors. We encourage future work to\nfocus on joint modeling of multiple EHR data categories and to include\ntree-based baselines in their evaluations.", "published": "2023-12-09 23:43:35", "link": "http://arxiv.org/abs/2312.11502v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Keyword spotting -- Detecting commands in speech using deep learning", "abstract": "Speech recognition has become an important task in the development of machine\nlearning and artificial intelligence. In this study, we explore the important\ntask of keyword spotting using speech recognition machine learning and deep\nlearning techniques. We implement feature engineering by converting raw\nwaveforms to Mel Frequency Cepstral Coefficients (MFCCs), which we use as\ninputs to our models. We experiment with several different algorithms such as\nHidden Markov Model with Gaussian Mixture, Convolutional Neural Networks and\nvariants of Recurrent Neural Networks including Long Short-Term Memory and the\nAttention mechanism. In our experiments, RNN with BiLSTM and Attention achieves\nthe best performance with an accuracy of 93.9 %", "published": "2023-12-09 19:04:17", "link": "http://arxiv.org/abs/2312.05640v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
{"title": "NLLG Quarterly arXiv Report 09/23: What are the most influential current\n  AI Papers?", "abstract": "Artificial Intelligence (AI) has witnessed rapid growth, especially in the\nsubfields Natural Language Processing (NLP), Machine Learning (ML) and Computer\nVision (CV). Keeping pace with this rapid progress poses a considerable\nchallenge for researchers and professionals in the field. In this arXiv report,\nthe second of its kind, which covers the period from January to September 2023,\nwe aim to provide insights and analysis that help navigate these dynamic areas\nof AI. We accomplish this by 1) identifying the top-40 most cited papers from\narXiv in the given period, comparing the current top-40 papers to the previous\nreport, which covered the period January to June; 2) analyzing dataset\ncharacteristics and keyword popularity; 3) examining the global sectoral\ndistribution of institutions to reveal differences in engagement across\ngeographical areas. Our findings highlight the continued dominance of NLP:\nwhile only 16% of all submitted papers have NLP as primary category (more than\n25% have CV and ML as primary category), 50% of the most cited papers have NLP\nas primary category, 90% of which target LLMs. Additionally, we show that i)\nthe US dominates among both top-40 and top-9k papers, followed by China; ii)\nEurope clearly lags behind and is hardly represented in the top-40 most cited\npapers; iii) US industry is largely overrepresented in the top-40 most\ninfluential papers.", "published": "2023-12-09 21:42:20", "link": "http://arxiv.org/abs/2312.05688v1", "categories": ["cs.DL", "cs.AI", "cs.CL", "cs.CV", "cs.CY", "cs.LG"], "primary_category": "cs.DL"}
