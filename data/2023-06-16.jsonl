{"title": "Reproducibility in NLP: What Have We Learned from the Checklist?", "abstract": "Scientific progress in NLP rests on the reproducibility of researchers'\nclaims. The *CL conferences created the NLP Reproducibility Checklist in 2020\nto be completed by authors at submission to remind them of key information to\ninclude. We provide the first analysis of the Checklist by examining 10,405\nanonymous responses to it. First, we find evidence of an increase in reporting\nof information on efficiency, validation performance, summary statistics, and\nhyperparameters after the Checklist's introduction. Further, we show acceptance\nrate grows for submissions with more Yes responses. We find that the 44% of\nsubmissions that gather new data are 5% less likely to be accepted than those\nthat did not; the average reviewer-rated reproducibility of these submissions\nis also 2% lower relative to the rest. We find that only 46% of submissions\nclaim to open-source their code, though submissions that do have 8% higher\nreproducibility score relative to those that do not, the most for any item. We\ndiscuss what can be inferred about the state of reproducibility in NLP, and\nprovide a set of recommendations for future conferences, including: a) allowing\nsubmitting code and appendices one week after the deadline, and b) measuring\ndataset reproducibility by a checklist of data collection practices.", "published": "2023-06-16 00:39:25", "link": "http://arxiv.org/abs/2306.09562v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Listener Model for the PhotoBook Referential Game with CLIPScores as\n  Implicit Reference Chain", "abstract": "PhotoBook is a collaborative dialogue game where two players receive private,\npartially-overlapping sets of images and resolve which images they have in\ncommon. It presents machines with a great challenge to learn how people build\ncommon ground around multimodal context to communicate effectively. Methods\ndeveloped in the literature, however, cannot be deployed to real gameplay since\nthey only tackle some subtasks of the game, and they require additional\nreference chains inputs, whose extraction process is imperfect. Therefore, we\npropose a reference chain-free listener model that directly addresses the\ngame's predictive task, i.e., deciding whether an image is shared with partner.\nOur DeBERTa-based listener model reads the full dialogue, and utilizes\nCLIPScore features to assess utterance-image relevance. We achieve >77%\naccuracy on unseen sets of images/game themes, outperforming baseline by >17\npoints.", "published": "2023-06-16 03:41:14", "link": "http://arxiv.org/abs/2306.09607v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AUGUST: an Automatic Generation Understudy for Synthesizing\n  Conversational Recommendation Datasets", "abstract": "High-quality data is essential for conversational recommendation systems and\nserves as the cornerstone of the network architecture development and training\nstrategy design. Existing works contribute heavy human efforts to manually\nlabeling or designing and extending recommender dialogue templates. However,\nthey suffer from (i) the limited number of human annotators results in that\ndatasets can hardly capture rich and large-scale cases in the real world, (ii)\nthe limited experience and knowledge of annotators account for the\nuninformative corpus and inappropriate recommendations. In this paper, we\npropose a novel automatic dataset synthesis approach that can generate both\nlarge-scale and high-quality recommendation dialogues through a data2text\ngeneration process, where unstructured recommendation conversations are\ngenerated from structured graphs based on user-item information from the real\nworld. In doing so, we comprehensively exploit: (i) rich personalized user\nprofiles from traditional recommendation datasets, (ii) rich external knowledge\nfrom knowledge graphs, and (iii) the conversation ability contained in\nhuman-to-human conversational recommendation datasets. Extensive experiments\nvalidate the benefit brought by the automatically synthesized data under\nlow-resource scenarios and demonstrate the promising potential to facilitate\nthe development of a more effective conversational recommendation system.", "published": "2023-06-16 05:27:14", "link": "http://arxiv.org/abs/2306.09631v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Class-Adaptive Self-Training for Relation Extraction with Incompletely\n  Annotated Training Data", "abstract": "Relation extraction (RE) aims to extract relations from sentences and\ndocuments. Existing relation extraction models typically rely on supervised\nmachine learning. However, recent studies showed that many RE datasets are\nincompletely annotated. This is known as the false negative problem in which\nvalid relations are falsely annotated as 'no_relation'. Models trained with\nsuch data inevitably make similar mistakes during the inference stage.\nSelf-training has been proven effective in alleviating the false negative\nproblem. However, traditional self-training is vulnerable to confirmation bias\nand exhibits poor performance in minority classes. To overcome this limitation,\nwe proposed a novel class-adaptive re-sampling self-training framework.\nSpecifically, we re-sampled the pseudo-labels for each class by precision and\nrecall scores. Our re-sampling strategy favored the pseudo-labels of classes\nwith high precision and low recall, which improved the overall recall without\nsignificantly compromising precision. We conducted experiments on\ndocument-level and biomedical relation extraction datasets, and the results\nshowed that our proposed self-training framework consistently outperforms\nexisting competitive methods on the Re-DocRED and ChemDisgene datasets when the\ntraining data are incompletely annotated. Our code is released at\nhttps://github.com/DAMO-NLP-SG/CAST.", "published": "2023-06-16 09:01:45", "link": "http://arxiv.org/abs/2306.09697v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-corpus Readability Compatibility Assessment for English Texts", "abstract": "Text readability assessment has gained significant attention from researchers\nin various domains. However, the lack of exploration into corpus compatibility\nposes a challenge as different research groups utilize different corpora. In\nthis study, we propose a novel evaluation framework, Cross-corpus text\nReadability Compatibility Assessment (CRCA), to address this issue. The\nframework encompasses three key components: (1) Corpus: CEFR, CLEC, CLOTH, NES,\nOSP, and RACE. Linguistic features, GloVe word vector representations, and\ntheir fusion features were extracted. (2) Classification models: Machine\nlearning methods (XGBoost, SVM) and deep learning methods (BiLSTM,\nAttention-BiLSTM) were employed. (3) Compatibility metrics: RJSD, RRNSS, and\nNDCG metrics. Our findings revealed: (1) Validated corpus compatibility, with\nOSP standing out as significantly different from other datasets. (2) An\nadaptation effect among corpora, feature representations, and classification\nmethods. (3) Consistent outcomes across the three metrics, validating the\nrobustness of the compatibility assessment framework. The outcomes of this\nstudy offer valuable insights into corpus selection, feature representation,\nand classification methods, and it can also serve as a beginning effort for\ncross-corpus transfer learning.", "published": "2023-06-16 09:15:39", "link": "http://arxiv.org/abs/2306.09704v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Discourse Representation Structure Parsing for Chinese", "abstract": "Previous work has predominantly focused on monolingual English semantic\nparsing. We, instead, explore the feasibility of Chinese semantic parsing in\nthe absence of labeled data for Chinese meaning representations. We describe\nthe pipeline of automatically collecting the linearized Chinese meaning\nrepresentation data for sequential-to sequential neural networks. We further\npropose a test suite designed explicitly for Chinese semantic parsing, which\nprovides fine-grained evaluation for parsing performance, where we aim to study\nChinese parsing difficulties. Our experimental results show that the difficulty\nof Chinese semantic parsing is mainly caused by adverbs. Realizing Chinese\nparsing through machine translation and an English parser yields slightly lower\nperformance than training a model directly on Chinese data.", "published": "2023-06-16 09:47:45", "link": "http://arxiv.org/abs/2306.09725v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using Natural Language Processing and Networks to Automate Structured\n  Literature Reviews: An Application to Farmers Climate Change Adaptation", "abstract": "The fast-growing number of research articles makes it problematic for\nscholars to keep track of the new findings related to their areas of expertise.\nFurthermore, linking knowledge across disciplines in rapidly developing fields\nbecomes challenging for complex topics like climate change that demand\ninterdisciplinary solutions. At the same time, the rise of Black Box types of\ntext summarization makes it difficult to understand how text relationships are\nbuilt, let alone relate to existing theories conceptualizing cause-effect\nrelationships and permitting hypothesizing. This work aims to sensibly use\nNatural Language Processing by extracting variables relations and synthesizing\ntheir findings using networks while relating to key concepts dominant in\nrelevant disciplines. As an example, we apply our methodology to the analysis\nof farmers' adaptation to climate change. For this, we perform a Natural\nLanguage Processing analysis of publications returned by Scopus in August 2022.\nResults show that the use of Natural Language Processing together with networks\nin a descriptive manner offers a fast and interpretable way to synthesize\nliterature review findings as long as researchers back up results with theory.", "published": "2023-06-16 10:05:47", "link": "http://arxiv.org/abs/2306.09737v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Full Parameter Fine-tuning for Large Language Models with Limited\n  Resources", "abstract": "Large Language Models (LLMs) have revolutionized Natural Language Processing\n(NLP) but demand massive GPU resources for training. Lowering the threshold for\nLLMs training would encourage greater participation from researchers,\nbenefiting both academia and society. While existing approaches have focused on\nparameter-efficient fine-tuning, which tunes or adds a small number of\nparameters, few have addressed the challenge of tuning the full parameters of\nLLMs with limited resources. In this work, we propose a new optimizer,\nLOw-Memory Optimization (LOMO), which fuses the gradient computation and the\nparameter update in one step to reduce memory usage. By integrating LOMO with\nexisting memory saving techniques, we reduce memory usage to 10.8% compared to\nthe standard approach (DeepSpeed solution). Consequently, our approach enables\nthe full parameter fine-tuning of a 65B model on a single machine with 8 RTX\n3090, each with 24GB memory.Code and data are available at\nhttps://github.com/OpenLMLab/LOMO.", "published": "2023-06-16 11:37:15", "link": "http://arxiv.org/abs/2306.09782v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RED$^{\\rm FM}$: a Filtered and Multilingual Relation Extraction Dataset", "abstract": "Relation Extraction (RE) is a task that identifies relationships between\nentities in a text, enabling the acquisition of relational facts and bridging\nthe gap between natural language and structured knowledge. However, current RE\nmodels often rely on small datasets with low coverage of relation types,\nparticularly when working with languages other than English. In this paper, we\naddress the above issue and provide two new resources that enable the training\nand evaluation of multilingual RE systems. First, we present SRED$^{\\rm FM}$,\nan automatically annotated dataset covering 18 languages, 400 relation types,\n13 entity types, totaling more than 40 million triplet instances. Second, we\npropose RED$^{\\rm FM}$, a smaller, human-revised dataset for seven languages\nthat allows for the evaluation of multilingual RE systems. To demonstrate the\nutility of these novel datasets, we experiment with the first end-to-end\nmultilingual RE model, mREBEL, that extracts triplets, including entity types,\nin multiple languages. We release our resources and model checkpoints at\nhttps://www.github.com/babelscape/rebel", "published": "2023-06-16 12:29:59", "link": "http://arxiv.org/abs/2306.09802v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unlocking the Potential of User Feedback: Leveraging Large Language\n  Model as User Simulator to Enhance Dialogue System", "abstract": "Dialogue systems and large language models (LLMs) have gained considerable\nattention. However, the direct utilization of LLMs as task-oriented dialogue\n(TOD) models has been found to underperform compared to smaller task-specific\nmodels. Nonetheless, it is crucial to acknowledge the significant potential of\nLLMs and explore improved approaches for leveraging their impressive abilities.\nMotivated by the goal of leveraging LLMs, we propose an alternative approach\ncalled User-Guided Response Optimization (UGRO) to combine it with a smaller\nTOD model. This approach uses LLM as annotation-free user simulator to assess\ndialogue responses, combining them with smaller fine-tuned end-to-end TOD\nmodels. By utilizing the satisfaction feedback generated by LLMs, UGRO further\noptimizes the supervised fine-tuned TOD model. Specifically, the TOD model\ntakes the dialogue history as input and, with the assistance of the user\nsimulator's feedback, generates high-satisfaction responses that meet the\nuser's requirements. Through empirical experiments on two TOD benchmarks, we\nvalidate the effectiveness of our method. The results demonstrate that our\napproach outperforms previous state-of-the-art (SOTA) results.", "published": "2023-06-16 13:04:56", "link": "http://arxiv.org/abs/2306.09821v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sheffield's Submission to the AmericasNLP Shared Task on Machine\n  Translation into Indigenous Languages", "abstract": "In this paper we describe the University of Sheffield's submission to the\nAmericasNLP 2023 Shared Task on Machine Translation into Indigenous Languages\nwhich comprises the translation from Spanish to eleven indigenous languages.\nOur approach consists of extending, training, and ensembling different\nvariations of NLLB-200. We use data provided by the organizers and data from\nvarious other sources such as constitutions, handbooks, news articles, and\nbacktranslations generated from monolingual data. On the dev set, our best\nsubmission outperforms the baseline by 11% average chrF across all languages,\nwith substantial improvements particularly for Aymara, Guarani and Quechua. On\nthe test set, we achieve the highest average chrF of all the submissions, we\nrank first in four of the eleven languages, and at least one of our submissions\nranks in the top 3 for all languages.", "published": "2023-06-16 13:15:26", "link": "http://arxiv.org/abs/2306.09830v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revealing the impact of social circumstances on the selection of cancer\n  therapy through natural language processing of social work notes", "abstract": "We aimed to investigate the impact of social circumstances on cancer therapy\nselection using natural language processing to derive insights from social\nworker documentation. We developed and employed a Bidirectional Encoder\nRepresentations from Transformers (BERT) based approach, using a hierarchical\nmulti-step BERT model (BERT-MS) to predict the prescription of targeted cancer\ntherapy to patients based solely on documentation by clinical social workers.\nOur corpus included free-text clinical social work notes, combined with\nmedication prescription information, for all patients treated for breast\ncancer. We conducted a feature importance analysis to pinpoint the specific\nsocial circumstances that impact cancer therapy selection. Using only social\nwork notes, we consistently predicted the administration of targeted therapies,\nsuggesting systematic differences in treatment selection exist due to\nnon-clinical factors. The UCSF-BERT model, pretrained on clinical text at UCSF,\noutperformed other publicly available language models with an AUROC of 0.675\nand a Macro F1 score of 0.599. The UCSF BERT-MS model, capable of leveraging\nmultiple pieces of notes, surpassed the UCSF-BERT model in both AUROC and\nMacro-F1. Our feature importance analysis identified several clinically\nintuitive social determinants of health (SDOH) that potentially contribute to\ndisparities in treatment. Our findings indicate that significant disparities\nexist among breast cancer patients receiving different types of therapies based\non social determinants of health. Social work reports play a crucial role in\nunderstanding these disparities in clinical decision-making.", "published": "2023-06-16 14:40:39", "link": "http://arxiv.org/abs/2306.09877v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ClinicalGPT: Large Language Models Finetuned with Diverse Medical Data\n  and Comprehensive Evaluation", "abstract": "Large language models have exhibited exceptional performance on various\nNatural Language Processing (NLP) tasks, leveraging techniques such as the\npre-training, and instruction fine-tuning. Despite these advances, their\neffectiveness in medical applications is limited, due to challenges such as\nfactual inaccuracies, reasoning abilities, and lack grounding in real-world\nexperience. In this study, we present ClinicalGPT, a language model explicitly\ndesigned and optimized for clinical scenarios. By incorporating extensive and\ndiverse real-world data, such as medical records, domain-specific knowledge,\nand multi-round dialogue consultations in the training process, ClinicalGPT is\nbetter prepared to handle multiple clinical task. Furthermore, we introduce a\ncomprehensive evaluation framework that includes medical knowledge\nquestion-answering, medical exams, patient consultations, and diagnostic\nanalysis of medical records. Our results demonstrate that ClinicalGPT\nsignificantly outperforms other models in these tasks, highlighting the\neffectiveness of our approach in adapting large language models to the critical\ndomain of healthcare.", "published": "2023-06-16 16:56:32", "link": "http://arxiv.org/abs/2306.09968v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data Selection for Fine-tuning Large Language Models Using Transferred\n  Shapley Values", "abstract": "Although Shapley values have been shown to be highly effective for\nidentifying harmful training instances, dataset size and model complexity\nconstraints limit the ability to apply Shapley-based data valuation to\nfine-tuning large pre-trained language models. To address this, we propose\nTS-DShapley, an algorithm that reduces computational cost of Shapley-based data\nvaluation through: 1) an efficient sampling-based method that aggregates\nShapley values computed from subsets for valuation of the entire training set,\nand 2) a value transfer method that leverages value information extracted from\na simple classifier trained using representations from the target language\nmodel. Our experiments applying TS-DShapley to select data for fine-tuning\nBERT-based language models on benchmark natural language understanding (NLU)\ndatasets show that TS-DShapley outperforms existing data selection methods.\nFurther, TS-DShapley can filter fine-tuning data to increase language model\nperformance compared to training with the full fine-tuning dataset.", "published": "2023-06-16 20:07:38", "link": "http://arxiv.org/abs/2306.10165v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Clickbait Classification and Spoiling Using Natural Language Processing", "abstract": "Clickbait is the practice of engineering titles to incentivize readers to\nclick through to articles. Such titles with sensationalized language reveal as\nlittle information as possible. Occasionally, clickbait will be intentionally\nmisleading, so natural language processing (NLP) can scan the article and\nanswer the question posed by the clickbait title, or spoil it. We tackle two\ntasks: classifying the clickbait into one of 3 types (Task 1), and spoiling the\nclickbait (Task 2). For Task 1, we propose two binary classifiers to determine\nthe final spoiler type. For Task 2, we experiment with two approaches: using a\nquestion-answering model to identify the span of text of the spoiler, and using\na large language model (LLM) to generate the spoiler. Because the spoiler is\ncontained in the article, we frame the second task as a question-answering\napproach for identifying the starting and ending positions of the spoiler. We\ncreated models for Task 1 that were better than the baselines proposed by the\ndataset authors and engineered prompts for Task 2 that did not perform as well\nas the baselines proposed by the dataset authors due to the evaluation metric\nperforming worse when the output text is from a generative model as opposed to\nan extractive model.", "published": "2023-06-16 01:45:57", "link": "http://arxiv.org/abs/2306.14907v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "How do different tokenizers perform on downstream tasks in scriptio\n  continua languages?: A case study in Japanese", "abstract": "This paper investigates the effect of tokenizers on the downstream\nperformance of pretrained language models (PLMs) in scriptio continua languages\nwhere no explicit spaces exist between words, using Japanese as a case study.\nThe tokenizer for such languages often consists of a morphological analyzer and\na subword tokenizer, requiring us to conduct a comprehensive study of all\npossible pairs. However, previous studies lack this comprehensiveness. We\ntherefore train extensive sets of tokenizers, build a PLM using each, and\nmeasure the downstream performance on a wide range of tasks. Our results\ndemonstrate that each downstream task has a different optimal morphological\nanalyzer, and that it is better to use Byte-Pair-Encoding or Unigram rather\nthan WordPiece as a subword tokenizer, regardless of the type of task.", "published": "2023-06-16 01:22:32", "link": "http://arxiv.org/abs/2306.09572v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CMLM-CSE: Based on Conditional MLM Contrastive Learning for Sentence\n  Embeddings", "abstract": "Traditional comparative learning sentence embedding directly uses the encoder\nto extract sentence features, and then passes in the comparative loss function\nfor learning. However, this method pays too much attention to the sentence body\nand ignores the influence of some words in the sentence on the sentence\nsemantics. To this end, we propose CMLM-CSE, an unsupervised contrastive\nlearning framework based on conditional MLM. On the basis of traditional\ncontrastive learning, an additional auxiliary network is added to integrate\nsentence embedding to perform MLM tasks, forcing sentence embedding to learn\nmore masked word information. Finally, when Bertbase was used as the\npretraining language model, we exceeded SimCSE by 0.55 percentage points on\naverage in textual similarity tasks, and when Robertabase was used as the\npretraining language model, we exceeded SimCSE by 0.3 percentage points on\naverage in textual similarity tasks.", "published": "2023-06-16 02:39:45", "link": "http://arxiv.org/abs/2306.09594v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Clickbait Detection via Large Language Models", "abstract": "Clickbait, which aims to induce users with some surprising and even thrilling\nheadlines for increasing click-through rates, permeates almost all online\ncontent publishers, such as news portals and social media. Recently, Large\nLanguage Models (LLMs) have emerged as a powerful instrument and achieved\ntremendous success in a series of NLP downstream tasks. However, it is not yet\nknown whether LLMs can be served as a high-quality clickbait detection system.\nIn this paper, we analyze the performance of LLMs in the few-shot and zero-shot\nscenarios on several English and Chinese benchmark datasets. Experimental\nresults show that LLMs cannot achieve the best results compared to the\nstate-of-the-art deep and fine-tuning PLMs methods. Different from human\nintuition, the experiments demonstrated that LLMs cannot make satisfied\nclickbait detection just by the headlines.", "published": "2023-06-16 02:49:20", "link": "http://arxiv.org/abs/2306.09597v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Cross-Domain Toxic Spans Detection", "abstract": "Given the dynamic nature of toxic language use, automated methods for\ndetecting toxic spans are likely to encounter distributional shift. To explore\nthis phenomenon, we evaluate three approaches for detecting toxic spans under\ncross-domain conditions: lexicon-based, rationale extraction, and fine-tuned\nlanguage models. Our findings indicate that a simple method using off-the-shelf\nlexicons performs best in the cross-domain setup. The cross-domain error\nanalysis suggests that (1) rationale extraction methods are prone to false\nnegatives, while (2) language models, despite performing best for the in-domain\ncase, recall fewer explicitly toxic words than lexicons and are prone to\ncertain types of false positives. Our code is publicly available at:\nhttps://github.com/sfschouten/toxic-cross-domain.", "published": "2023-06-16 06:10:00", "link": "http://arxiv.org/abs/2306.09642v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ReactGenie: A Development Framework for Complex Multimodal Interactions\n  Using Large Language Models", "abstract": "By combining voice and touch interactions, multimodal interfaces can surpass\nthe efficiency of either modality alone. Traditional multimodal frameworks\nrequire laborious developer work to support rich multimodal commands where the\nuser's multimodal command involves possibly exponential combinations of\nactions/function invocations. This paper presents ReactGenie, a programming\nframework that better separates multimodal input from the computational model\nto enable developers to create efficient and capable multimodal interfaces with\nease. ReactGenie translates multimodal user commands into NLPL (Natural\nLanguage Programming Language), a programming language we created, using a\nneural semantic parser based on large-language models. The ReactGenie runtime\ninterprets the parsed NLPL and composes primitives in the computational model\nto implement complex user commands. As a result, ReactGenie allows easy\nimplementation and unprecedented richness in commands for end-users of\nmultimodal apps. Our evaluation showed that 12 developers can learn and build a\nnontrivial ReactGenie application in under 2.5 hours on average. In addition,\ncompared with a traditional GUI, end-users can complete tasks faster and with\nless task load using ReactGenie apps.", "published": "2023-06-16 06:53:26", "link": "http://arxiv.org/abs/2306.09649v3", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Online Distillation for Pseudo-Relevance Feedback", "abstract": "Model distillation has emerged as a prominent technique to improve neural\nsearch models. To date, distillation taken an offline approach, wherein a new\nneural model is trained to predict relevance scores between arbitrary queries\nand documents. In this paper, we explore a departure from this offline\ndistillation strategy by investigating whether a model for a specific query can\nbe effectively distilled from neural re-ranking results (i.e., distilling in an\nonline setting). Indeed, we find that a lexical model distilled online can\nreasonably replicate the re-ranking of a neural model. More importantly, these\nmodels can be used as queries that execute efficiently on indexes. This second\nretrieval stage can enrich the pool of documents for re-ranking by identifying\ndocuments that were missed in the first retrieval stage. Empirically, we show\nthat this approach performs favourably when compared with established pseudo\nrelevance feedback techniques, dense retrieval methods, and sparse-dense\nensemble \"hybrid\" approaches.", "published": "2023-06-16 07:26:33", "link": "http://arxiv.org/abs/2306.09657v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Reducing Computational Costs in Sentiment Analysis: Tensorized Recurrent\n  Networks vs. Recurrent Networks", "abstract": "Anticipating audience reaction towards a certain text is integral to several\nfacets of society ranging from politics, research, and commercial industries.\nSentiment analysis (SA) is a useful natural language processing (NLP) technique\nthat utilizes lexical/statistical and deep learning methods to determine\nwhether different-sized texts exhibit positive, negative, or neutral emotions.\nRecurrent networks are widely used in machine-learning communities for problems\nwith sequential data. However, a drawback of models based on Long-Short Term\nMemory networks and Gated Recurrent Units is the significantly high number of\nparameters, and thus, such models are computationally expensive. This drawback\nis even more significant when the available data are limited. Also, such models\nrequire significant over-parameterization and regularization to achieve optimal\nperformance. Tensorized models represent a potential solution. In this paper,\nwe classify the sentiment of some social media posts. We compare traditional\nrecurrent models with their tensorized version, and we show that with the\ntensorized models, we reach comparable performances with respect to the\ntraditional models while using fewer resources for the training.", "published": "2023-06-16 09:18:08", "link": "http://arxiv.org/abs/2306.09705v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Pushing the Limits of ChatGPT on NLP Tasks", "abstract": "Despite the success of ChatGPT, its performances on most NLP tasks are still\nwell below the supervised baselines. In this work, we looked into the causes,\nand discovered that its subpar performance was caused by the following factors:\n(1) token limit in the prompt does not allow for the full utilization of the\nsupervised datasets; (2) mismatch between the generation nature of ChatGPT and\nNLP tasks; (3) intrinsic pitfalls of LLMs models, e.g., hallucination, overly\nfocus on certain keywords, etc.\n  In this work, we propose a collection of general modules to address these\nissues, in an attempt to push the limits of ChatGPT on NLP tasks. Our proposed\nmodules include (1) a one-input-multiple-prompts strategy that employs multiple\nprompts for one input to accommodate more demonstrations; (2) using fine-tuned\nmodels for better demonstration retrieval; (3) transforming tasks to formats\nthat are more tailored to the generation nature; (4) employing reasoning\nstrategies that are tailored to addressing the task-specific complexity; (5)\nthe self-verification strategy to address the hallucination issue of LLMs; (6)\nthe paraphrase strategy to improve the robustness of model predictions.\n  We conduct experiments on 21 datasets of 10 representative NLP tasks,\nincluding question answering, commonsense reasoning, natural language\ninference, sentiment analysis, named entity recognition, entity-relation\nextraction, event extraction, dependency parsing, semantic role labeling, and\npart-of-speech tagging. Using the proposed assemble of techniques, we are able\nto significantly boost the performance of ChatGPT on the selected NLP tasks,\nachieving performances comparable to or better than supervised baselines, or\neven existing SOTA performances.", "published": "2023-06-16 09:40:05", "link": "http://arxiv.org/abs/2306.09719v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Investigating the Utility of Surprisal from Large Language Models for\n  Speech Synthesis Prosody", "abstract": "This paper investigates the use of word surprisal, a measure of the\npredictability of a word in a given context, as a feature to aid speech\nsynthesis prosody. We explore how word surprisal extracted from large language\nmodels (LLMs) correlates with word prominence, a signal-based measure of the\nsalience of a word in a given discourse. We also examine how context length and\nLLM size affect the results, and how a speech synthesizer conditioned with\nsurprisal values compares with a baseline system. To evaluate these factors, we\nconducted experiments using a large corpus of English text and LLMs of varying\nsizes. Our results show that word surprisal and word prominence are moderately\ncorrelated, suggesting that they capture related but distinct aspects of\nlanguage use. We find that length of context and size of the LLM impact the\ncorrelations, but not in the direction anticipated, with longer contexts and\nlarger LLMs generally underpredicting prominent words in a nearly linear\nmanner. We demonstrate that, in line with these findings, a speech synthesizer\nconditioned with surprisal values provides a minimal improvement over the\nbaseline with the results suggesting a limited effect of using surprisal values\nfor eliciting appropriate prominence patterns.", "published": "2023-06-16 12:49:44", "link": "http://arxiv.org/abs/2306.09814v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Process Knowledge-infused Learning for Clinician-friendly Explanations", "abstract": "Language models have the potential to assess mental health using social media\ndata. By analyzing online posts and conversations, these models can detect\npatterns indicating mental health conditions like depression, anxiety, or\nsuicidal thoughts. They examine keywords, language markers, and sentiment to\ngain insights into an individual's mental well-being. This information is\ncrucial for early detection, intervention, and support, improving mental health\ncare and prevention strategies. However, using language models for mental\nhealth assessments from social media has two limitations: (1) They do not\ncompare posts against clinicians' diagnostic processes, and (2) It's\nchallenging to explain language model outputs using concepts that the clinician\ncan understand, i.e., clinician-friendly explanations. In this study, we\nintroduce Process Knowledge-infused Learning (PK-iL), a new learning paradigm\nthat layers clinical process knowledge structures on language model outputs,\nenabling clinician-friendly explanations of the underlying language model\npredictions. We rigorously test our methods on existing benchmark datasets,\naugmented with such clinical process knowledge, and release a new dataset for\nassessing suicidality. PK-iL performs competitively, achieving a 70% agreement\nwith users, while other XAI methods only achieve 47% agreement (average\ninter-rater agreement of 0.72). Our evaluations demonstrate that PK-iL\neffectively explains model predictions to clinicians.", "published": "2023-06-16 13:08:17", "link": "http://arxiv.org/abs/2306.09824v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Are Large Language Models Really Good Logical Reasoners? A Comprehensive\n  Evaluation and Beyond", "abstract": "Logical reasoning consistently plays a fundamental and significant role in\nthe domains of knowledge engineering and artificial intelligence. Recently,\nLarge Language Models (LLMs) have emerged as a noteworthy innovation in natural\nlanguage processing (NLP). However, the question of whether LLMs can\neffectively address the task of logical reasoning, which requires gradual\ncognitive inference similar to human intelligence, remains unanswered. To this\nend, we aim to bridge this gap and provide comprehensive evaluations in this\npaper. Firstly, to offer systematic evaluations, we select fifteen typical\nlogical reasoning datasets and organize them into deductive, inductive,\nabductive and mixed-form reasoning settings. Considering the comprehensiveness\nof evaluations, we include 3 early-era representative LLMs and 4 trending LLMs.\nSecondly, different from previous evaluations relying only on simple metrics\n(e.g., \\emph{accuracy}), we propose fine-level evaluations in objective and\nsubjective manners, covering both answers and explanations, including\n\\emph{answer correctness}, \\emph{explain correctness}, \\emph{explain\ncompleteness} and \\emph{explain redundancy}. Additionally, to uncover the\nlogical flaws of LLMs, problematic cases will be attributed to five error types\nfrom two dimensions, i.e., \\emph{evidence selection process} and\n\\emph{reasoning process}. Thirdly, to avoid the influences of knowledge bias\nand concentrate purely on benchmarking the logical reasoning capability of\nLLMs, we propose a new dataset with neutral content. Based on the in-depth\nevaluations, this paper finally forms a general evaluation scheme of logical\nreasoning capability from six dimensions (i.e., \\emph{Correct},\n\\emph{Rigorous}, \\emph{Self-aware}, \\emph{Active}, \\emph{Oriented} and \\emph{No\nhallucination}). It reflects the pros and cons of LLMs and gives guiding\ndirections for future works.", "published": "2023-06-16 13:39:35", "link": "http://arxiv.org/abs/2306.09841v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "No Strong Feelings One Way or Another: Re-operationalizing Neutrality in\n  Natural Language Inference", "abstract": "Natural Language Inference (NLI) has been a cornerstone task in evaluating\nlanguage models' inferential reasoning capabilities. However, the standard\nthree-way classification scheme used in NLI has well-known shortcomings in\nevaluating models' ability to capture the nuances of natural human reasoning.\nIn this paper, we argue that the operationalization of the neutral label in\ncurrent NLI datasets has low validity, is interpreted inconsistently, and that\nat least one important sense of neutrality is often ignored. We uncover the\ndetrimental impact of these shortcomings, which in some cases leads to\nannotation datasets that actually decrease performance on downstream tasks. We\ncompare approaches of handling annotator disagreement and identify flaws in a\nrecent NLI dataset that designs an annotator study based on a problematic\noperationalization. Our findings highlight the need for a more refined\nevaluation framework for NLI, and we hope to spark further discussion and\naction in the NLP community.", "published": "2023-06-16 15:45:08", "link": "http://arxiv.org/abs/2306.09918v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Rewriting the Script: Adapting Text Instructions for Voice Interaction", "abstract": "Voice assistants have sharply risen in popularity in recent years, but their\nuse has been limited mostly to simple applications like music, hands-free\nsearch, or control of internet-of-things devices. What would it take for voice\nassistants to guide people through more complex tasks? In our work, we study\nthe limitations of the dominant approach voice assistants take to complex task\nguidance: reading aloud written instructions. Using recipes as an example, we\nobserve twelve participants cook at home with a state-of-the-art voice\nassistant. We learn that the current approach leads to nine challenges,\nincluding obscuring the bigger picture, overwhelming users with too much\ninformation, and failing to communicate affordances. Instructions delivered by\na voice assistant are especially difficult because they cannot be skimmed as\neasily as written instructions. Alexa in particular did not surface crucial\ndetails to the user or answer questions well. We draw on our observations to\npropose eight ways in which voice assistants can ``rewrite the script'' --\nsummarizing, signposting, splitting, elaborating, volunteering, reordering,\nredistributing, and visualizing -- to transform written sources into forms that\nare readily communicated through spoken conversation. We conclude with a vision\nof how modern advancements in natural language processing can be leveraged for\nintelligent agents to guide users effectively through complex tasks.", "published": "2023-06-16 17:43:00", "link": "http://arxiv.org/abs/2306.09992v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Investigating Prompting Techniques for Zero- and Few-Shot Visual\n  Question Answering", "abstract": "In this paper, we explore effective prompting techniques to enhance zero- and\nfew-shot Visual Question Answering (VQA) performance in contemporary\nVision-Language Models (VLMs). Central to our investigation is the role of\nquestion templates in guiding VLMs to generate accurate answers. We identify\nthat specific templates significantly influence VQA outcomes, underscoring the\nneed for strategic template selection. Another pivotal aspect of our study is\naugmenting VLMs with image captions, providing them with additional visual cues\nalongside direct image features in VQA tasks. Surprisingly, this augmentation\nsignificantly improves the VLMs' performance in many cases, even though VLMs\n\"see\" the image directly! We explore chain-of-thought (CoT) reasoning and find\nthat while standard CoT reasoning causes drops in performance, advanced methods\nlike self-consistency can help recover it. Furthermore, we find that text-only\nfew-shot examples enhance VLMs' alignment with the task format, particularly\nbenefiting models prone to verbose zero-shot answers. Lastly, to mitigate the\nchallenges associated with evaluating free-form open-ended VQA responses using\nstring-matching based VQA metrics, we introduce a straightforward LLM-guided\npre-processing technique to adapt the model responses to the expected\nground-truth answer distribution. In summary, our research sheds light on the\nintricacies of prompting strategies in VLMs for VQA, emphasizing the\nsynergistic use of captions, templates, and pre-processing to enhance model\nefficacy.", "published": "2023-06-16 17:47:57", "link": "http://arxiv.org/abs/2306.09996v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Semi-supervised Relation Extraction via Data Augmentation and\n  Consistency-training", "abstract": "Due to the semantic complexity of the Relation extraction (RE) task,\nobtaining high-quality human labelled data is an expensive and noisy process.\nTo improve the sample efficiency of the models, semi-supervised learning (SSL)\nmethods aim to leverage unlabelled data in addition to learning from limited\nlabelled data points. Recently, strong data augmentation combined with\nconsistency-based semi-supervised learning methods have advanced the state of\nthe art in several SSL tasks. However, adapting these methods to the RE task\nhas been challenging due to the difficulty of data augmentation for RE. In this\nwork, we leverage the recent advances in controlled text generation to perform\nhigh quality data augmentation for the RE task. We further introduce small but\nsignificant changes to model architecture that allows for generation of more\ntraining data by interpolating different data points in their latent space.\nThese data augmentations along with consistency training result in very\ncompetitive results for semi-supervised relation extraction on four benchmark\ndatasets.", "published": "2023-06-16 19:45:42", "link": "http://arxiv.org/abs/2306.10153v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Conformal Language Modeling", "abstract": "We propose a novel approach to conformal prediction for generative language\nmodels (LMs). Standard conformal prediction produces prediction sets -- in\nplace of single predictions -- that have rigorous, statistical performance\nguarantees. LM responses are typically sampled from the model's predicted\ndistribution over the large, combinatorial output space of natural language.\nTranslating this process to conformal prediction, we calibrate a stopping rule\nfor sampling different outputs from the LM that get added to a growing set of\ncandidates until we are confident that the output set is sufficient. Since some\nsamples may be low-quality, we also simultaneously calibrate and apply a\nrejection rule for removing candidates from the output set to reduce noise.\nSimilar to conformal prediction, we prove that the sampled set returned by our\nprocedure contains at least one acceptable answer with high probability, while\nstill being empirically precise (i.e., small) on average. Furthermore, within\nthis set of candidate responses, we show that we can also accurately identify\nsubsets of individual components -- such as phrases or sentences -- that are\neach independently correct (e.g., that are not \"hallucinations\"), again with\nstatistical guarantees. We demonstrate the promise of our approach on multiple\ntasks in open-domain question answering, text summarization, and radiology\nreport generation using different LM variants.", "published": "2023-06-16 21:55:08", "link": "http://arxiv.org/abs/2306.10193v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Investigating Masking-based Data Generation in Language Models", "abstract": "The current era of natural language processing (NLP) has been defined by the\nprominence of pre-trained language models since the advent of BERT. A feature\nof BERT and models with similar architecture is the objective of masked\nlanguage modeling, in which part of the input is intentionally masked and the\nmodel is trained to predict this piece of masked information. Data augmentation\nis a data-driven technique widely used in machine learning, including research\nareas like computer vision and natural language processing, to improve model\nperformance by artificially augmenting the training data set by designated\ntechniques. Masked language models (MLM), an essential training feature of\nBERT, have introduced a novel approach to perform effective pre-training on\nTransformer based models in natural language processing tasks. Recent studies\nhave utilized masked language model to generate artificially augmented data for\nNLP downstream tasks. The experimental results show that Mask based data\naugmentation method provides a simple but efficient approach to improve the\nmodel performance. In this paper, we explore and discuss the broader\nutilization of these data augmentation methods based on MLM.", "published": "2023-06-16 16:48:27", "link": "http://arxiv.org/abs/2307.00008v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Schema-learning and rebinding as mechanisms of in-context learning and\n  emergence", "abstract": "In-context learning (ICL) is one of the most powerful and most unexpected\ncapabilities to emerge in recent transformer-based large language models\n(LLMs). Yet the mechanisms that underlie it are poorly understood. In this\npaper, we demonstrate that comparable ICL capabilities can be acquired by an\nalternative sequence prediction learning method using clone-structured causal\ngraphs (CSCGs). Moreover, a key property of CSCGs is that, unlike\ntransformer-based LLMs, they are {\\em interpretable}, which considerably\nsimplifies the task of explaining how ICL works. Specifically, we show that it\nuses a combination of (a) learning template (schema) circuits for pattern\ncompletion, (b) retrieving relevant templates in a context-sensitive manner,\nand (c) rebinding of novel tokens to appropriate slots in the templates. We go\non to marshall evidence for the hypothesis that similar mechanisms underlie ICL\nin LLMs. For example, we find that, with CSCGs as with LLMs, different\ncapabilities emerge at different levels of overparameterization, suggesting\nthat overparameterization helps in learning more complex template (schema)\ncircuits. By showing how ICL can be achieved with small models and datasets, we\nopen up a path to novel architectures, and take a vital step towards a more\ngeneral understanding of the mechanics behind this important capability.", "published": "2023-06-16 00:29:19", "link": "http://arxiv.org/abs/2307.01201v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Semi-Offline Reinforcement Learning for Optimized Text Generation", "abstract": "In reinforcement learning (RL), there are two major settings for interacting\nwith the environment: online and offline. Online methods explore the\nenvironment at significant time cost, and offline methods efficiently obtain\nreward signals by sacrificing exploration capability. We propose semi-offline\nRL, a novel paradigm that smoothly transits from offline to online settings,\nbalances exploration capability and training cost, and provides a theoretical\nfoundation for comparing different RL settings. Based on the semi-offline\nformulation, we present the RL setting that is optimal in terms of optimization\ncost, asymptotic error, and overfitting error bound. Extensive experiments show\nthat our semi-offline approach is efficient and yields comparable or often\nbetter performance compared with state-of-the-art methods.", "published": "2023-06-16 09:24:29", "link": "http://arxiv.org/abs/2306.09712v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Politeness Stereotypes and Attack Vectors: Gender Stereotypes in\n  Japanese and Korean Language Models", "abstract": "In efforts to keep up with the rapid progress and use of large language\nmodels, gender bias research is becoming more prevalent in NLP. Non-English\nbias research, however, is still in its infancy with most work focusing on\nEnglish. In our work, we study how grammatical gender bias relating to\npoliteness levels manifests in Japanese and Korean language models. Linguistic\nstudies in these languages have identified a connection between gender bias and\npoliteness levels, however it is not yet known if language models reproduce\nthese biases. We analyze relative prediction probabilities of the male and\nfemale grammatical genders using templates and find that informal polite speech\nis most indicative of the female grammatical gender, while rude and formal\nspeech is most indicative of the male grammatical gender. Further, we find\npoliteness levels to be an attack vector for allocational gender bias in\ncyberbullying detection models. Cyberbullies can evade detection through simple\ntechniques abusing politeness levels. We introduce an attack dataset to (i)\nidentify representational gender bias across politeness levels, (ii)\ndemonstrate how gender biases can be abused to bypass cyberbullying detection\nmodels and (iii) show that allocational biases can be mitigated via training on\nour proposed dataset. Through our findings we highlight the importance of bias\nresearch moving beyond its current English-centrism.", "published": "2023-06-16 10:36:18", "link": "http://arxiv.org/abs/2306.09752v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Energy-Based Cross Attention for Bayesian Context Update in\n  Text-to-Image Diffusion Models", "abstract": "Despite the remarkable performance of text-to-image diffusion models in image\ngeneration tasks, recent studies have raised the issue that generated images\nsometimes cannot capture the intended semantic contents of the text prompts,\nwhich phenomenon is often called semantic misalignment. To address this, here\nwe present a novel energy-based model (EBM) framework for adaptive context\ncontrol by modeling the posterior of context vectors. Specifically, we first\nformulate EBMs of latent image representations and text embeddings in each\ncross-attention layer of the denoising autoencoder. Then, we obtain the\ngradient of the log posterior of context vectors, which can be updated and\ntransferred to the subsequent cross-attention layer, thereby implicitly\nminimizing a nested hierarchy of energy functions. Our latent EBMs further\nallow zero-shot compositional generation as a linear combination of\ncross-attention outputs from different contexts. Using extensive experiments,\nwe demonstrate that the proposed method is highly effective in handling various\nimage generation tasks, including multi-concept generation, text-guided image\ninpainting, and real and synthetic image editing. Code:\nhttps://github.com/EnergyAttention/Energy-Based-CrossAttention.", "published": "2023-06-16 14:30:41", "link": "http://arxiv.org/abs/2306.09869v3", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Is Self-Repair a Silver Bullet for Code Generation?", "abstract": "Large language models have shown remarkable aptitude in code generation, but\nstill struggle to perform complex tasks. Self-repair -- in which the model\ndebugs and repairs its own code -- has recently become a popular way to boost\nperformance in these settings. However, despite its increasing popularity,\nexisting studies of self-repair have been limited in scope; in many settings,\nits efficacy thus remains poorly understood. In this paper, we analyze Code\nLlama, GPT-3.5 and GPT-4's ability to perform self-repair on problems taken\nfrom HumanEval and APPS. We find that when the cost of carrying out repair is\ntaken into account, performance gains are often modest, vary a lot between\nsubsets of the data, and are sometimes not present at all. We hypothesize that\nthis is because self-repair is bottlenecked by the model's ability to provide\nfeedback on its own code; using a stronger model to artificially boost the\nquality of the feedback, we observe substantially larger performance gains.\nSimilarly, a small-scale study in which we provide GPT-4 with feedback from\nhuman participants suggests that even for the strongest models, self-repair\nstill lags far behind what can be achieved with human-level debugging.", "published": "2023-06-16 15:13:17", "link": "http://arxiv.org/abs/2306.09896v5", "categories": ["cs.CL", "cs.AI", "cs.PL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Learning to Summarize and Answer Questions about a Virtual Robot's Past\n  Actions", "abstract": "When robots perform long action sequences, users will want to easily and\nreliably find out what they have done. We therefore demonstrate the task of\nlearning to summarize and answer questions about a robot agent's past actions\nusing natural language alone. A single system with a large language model at\nits core is trained to both summarize and answer questions about action\nsequences given ego-centric video frames of a virtual robot and a question\nprompt. To enable training of question answering, we develop a method to\nautomatically generate English-language questions and answers about objects,\nactions, and the temporal order in which actions occurred during episodes of\nrobot action in the virtual environment. Training one model to both summarize\nand answer questions enables zero-shot transfer of representations of objects\nlearned through question answering to improved action summarization. %\ninvolving objects not seen in training to summarize.", "published": "2023-06-16 15:47:24", "link": "http://arxiv.org/abs/2306.09922v1", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Trained Transformers Learn Linear Models In-Context", "abstract": "Attention-based neural networks such as transformers have demonstrated a\nremarkable ability to exhibit in-context learning (ICL): Given a short prompt\nsequence of tokens from an unseen task, they can formulate relevant per-token\nand next-token predictions without any parameter updates. By embedding a\nsequence of labeled training data and unlabeled test data as a prompt, this\nallows for transformers to behave like supervised learning algorithms. Indeed,\nrecent work has shown that when training transformer architectures over random\ninstances of linear regression problems, these models' predictions mimic those\nof ordinary least squares.\n  Towards understanding the mechanisms underlying this phenomenon, we\ninvestigate the dynamics of ICL in transformers with a single linear\nself-attention layer trained by gradient flow on linear regression tasks. We\nshow that despite non-convexity, gradient flow with a suitable random\ninitialization finds a global minimum of the objective function. At this global\nminimum, when given a test prompt of labeled examples from a new prediction\ntask, the transformer achieves prediction error competitive with the best\nlinear predictor over the test prompt distribution. We additionally\ncharacterize the robustness of the trained transformer to a variety of\ndistribution shifts and show that although a number of shifts are tolerated,\nshifts in the covariate distribution of the prompts are not. Motivated by this,\nwe consider a generalized ICL setting where the covariate distributions can\nvary across prompts. We show that although gradient flow succeeds at finding a\nglobal minimum in this setting, the trained transformer is still brittle under\nmild covariate shifts. We complement this finding with experiments on large,\nnonlinear transformer architectures which we show are more robust under\ncovariate shifts.", "published": "2023-06-16 15:50:03", "link": "http://arxiv.org/abs/2306.09927v3", "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
{"title": "MagicBrush: A Manually Annotated Dataset for Instruction-Guided Image\n  Editing", "abstract": "Text-guided image editing is widely needed in daily life, ranging from\npersonal use to professional applications such as Photoshop. However, existing\nmethods are either zero-shot or trained on an automatically synthesized\ndataset, which contains a high volume of noise. Thus, they still require lots\nof manual tuning to produce desirable outcomes in practice. To address this\nissue, we introduce MagicBrush (https://osu-nlp-group.github.io/MagicBrush/),\nthe first large-scale, manually annotated dataset for instruction-guided real\nimage editing that covers diverse scenarios: single-turn, multi-turn,\nmask-provided, and mask-free editing. MagicBrush comprises over 10K manually\nannotated triplets (source image, instruction, target image), which supports\ntrainining large-scale text-guided image editing models. We fine-tune\nInstructPix2Pix on MagicBrush and show that the new model can produce much\nbetter images according to human evaluation. We further conduct extensive\nexperiments to evaluate current image editing baselines from multiple\ndimensions including quantitative, qualitative, and human evaluations. The\nresults reveal the challenging nature of our dataset and the gap between\ncurrent baselines and real-world editing needs.", "published": "2023-06-16 17:58:58", "link": "http://arxiv.org/abs/2306.10012v3", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Just One Byte (per gradient): A Note on Low-Bandwidth Decentralized\n  Language Model Finetuning Using Shared Randomness", "abstract": "Language model training in distributed settings is limited by the\ncommunication cost of gradient exchanges. In this short note, we extend recent\nwork from Malladi et al. (2023), using shared randomness to perform distributed\nfine-tuning with low bandwidth. The method is a natural decentralized extension\nof memory-efficient Simultaneous Perturbation Stochastic Approximation (SPSA).\nEach iteration, each machine seeds a Random Number Generator (RNG) to perform\nlocal reproducible perturbations on model weights and calculate and exchange\nscalar projected gradients, which are then used to update each model. By using\na (machine, sample) identifier as the random seed, each model can regenerate\none another's perturbations. As machines only exchange single-byte projected\ngradients, this is highly communication efficient. There are also potential\nprivacy benefits, as projected gradients may be calculated on different\ntraining data, and models never access the other's data. Our approach not only\ndrastically reduces communication bandwidth requirements but also accommodates\ndynamic addition or removal of machines during the training process and retains\nthe memory-efficient and inference-only advantages of recent work. We perform\nproof-of-concept experiments to demonstrate the potential usefulness of this\nmethod, building off of rich literature on distributed optimization and\nmemory-efficient training.", "published": "2023-06-16 17:59:51", "link": "http://arxiv.org/abs/2306.10015v1", "categories": ["cs.LG", "cs.CL", "cs.DC"], "primary_category": "cs.LG"}
{"title": "Improving Audio Caption Fluency with Automatic Error Correction", "abstract": "Automated audio captioning (AAC) is an important cross-modality translation\ntask, aiming at generating descriptions for audio clips. However, captions\ngenerated by previous AAC models have faced ``false-repetition'' errors due to\nthe training objective. In such scenarios, we propose a new task of AAC error\ncorrection and hope to reduce such errors by post-processing AAC outputs. To\ntackle this problem, we use observation-based rules to corrupt captions without\nerrors, for pseudo grammatically-erroneous sentence generation. One pair of\ncorrupted and clean sentences can thus be used for training. We train a neural\nnetwork-based model on the synthetic error dataset and apply the model to\ncorrect real errors in AAC outputs. Results on two benchmark datasets indicate\nthat our approach significantly improves fluency while maintaining semantic\ninformation.", "published": "2023-06-16 13:37:01", "link": "http://arxiv.org/abs/2306.10090v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "AD-AutoGPT: An Autonomous GPT for Alzheimer's Disease Infodemiology", "abstract": "In this pioneering study, inspired by AutoGPT, the state-of-the-art\nopen-source application based on the GPT-4 large language model, we develop a\nnovel tool called AD-AutoGPT which can conduct data collection, processing, and\nanalysis about complex health narratives of Alzheimer's Disease in an\nautonomous manner via users' textual prompts. We collated comprehensive data\nfrom a variety of news sources, including the Alzheimer's Association, BBC,\nMayo Clinic, and the National Institute on Aging since June 2022, leading to\nthe autonomous execution of robust trend analyses, intertopic distance maps\nvisualization, and identification of salient terms pertinent to Alzheimer's\nDisease. This approach has yielded not only a quantifiable metric of relevant\ndiscourse but also valuable insights into public focus on Alzheimer's Disease.\nThis application of AD-AutoGPT in public health signifies the transformative\npotential of AI in facilitating a data-rich understanding of complex health\nnarratives like Alzheimer's Disease in an autonomous manner, setting the\ngroundwork for future AI-driven investigations in global health landscapes.", "published": "2023-06-16 16:35:59", "link": "http://arxiv.org/abs/2306.10095v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "68T01, 68T50, 92C50", "I.2.7; I.2.1; J.3"], "primary_category": "cs.CL"}
{"title": "CML-TTS A Multilingual Dataset for Speech Synthesis in Low-Resource\n  Languages", "abstract": "In this paper, we present CML-TTS, a recursive acronym for\nCML-Multi-Lingual-TTS, a new Text-to-Speech (TTS) dataset developed at the\nCenter of Excellence in Artificial Intelligence (CEIA) of the Federal\nUniversity of Goias (UFG). CML-TTS is based on Multilingual LibriSpeech (MLS)\nand adapted for training TTS models, consisting of audiobooks in seven\nlanguages: Dutch, French, German, Italian, Portuguese, Polish, and Spanish.\nAdditionally, we provide the YourTTS model, a multi-lingual TTS model, trained\nusing 3,176.13 hours from CML-TTS and also with 245.07 hours from LibriTTS, in\nEnglish. Our purpose in creating this dataset is to open up new research\npossibilities in the TTS area for multi-lingual models. The dataset is publicly\navailable under the CC-BY 4.0 license1.", "published": "2023-06-16 17:17:06", "link": "http://arxiv.org/abs/2306.10097v1", "categories": ["eess.AS", "cs.AI", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Differentiable Instruction Optimization for Cross-Task Generalization", "abstract": "Instruction tuning has been attracting much attention to achieve\ngeneralization ability across a wide variety of tasks. Although various types\nof instructions have been manually created for instruction tuning, it is still\nunclear what kind of instruction is optimal to obtain cross-task generalization\nability. This work presents instruction optimization, which optimizes training\ninstructions with respect to generalization ability. Rather than manually\ntuning instructions, we introduce learnable instructions and optimize them with\ngradient descent by leveraging bilevel optimization. Experimental results show\nthat the learned instruction enhances the diversity of instructions and\nimproves the generalization ability compared to using only manually created\ninstructions.", "published": "2023-06-16 17:49:34", "link": "http://arxiv.org/abs/2306.10098v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Meta-Personalizing Vision-Language Models to Find Named Instances in\n  Video", "abstract": "Large-scale vision-language models (VLM) have shown impressive results for\nlanguage-guided search applications. While these models allow category-level\nqueries, they currently struggle with personalized searches for moments in a\nvideo where a specific object instance such as ``My dog Biscuit'' appears. We\npresent the following three contributions to address this problem. First, we\ndescribe a method to meta-personalize a pre-trained VLM, i.e., learning how to\nlearn to personalize a VLM at test time to search in video. Our method extends\nthe VLM's token vocabulary by learning novel word embeddings specific to each\ninstance. To capture only instance-specific features, we represent each\ninstance embedding as a combination of shared and learned global category\nfeatures. Second, we propose to learn such personalization without explicit\nhuman supervision. Our approach automatically identifies moments of named\nvisual instances in video using transcripts and vision-language similarity in\nthe VLM's embedding space. Finally, we introduce This-Is-My, a personal video\ninstance retrieval benchmark. We evaluate our approach on This-Is-My and\nDeepFashion2 and show that we obtain a 15% relative improvement over the state\nof the art on the latter dataset.", "published": "2023-06-16 20:12:11", "link": "http://arxiv.org/abs/2306.10169v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Structured Thoughts Automaton: First Formalized Execution Model for\n  Auto-Regressive Language Models", "abstract": "In recent months, Language Models (LMs) have become a part of daily\ndiscourse, with focus on OpenAI and the potential of Artificial General\nIntelligence (AGI). Furthermore, the leaking of LLama's weights to the public\nhas led to an influx of innovations demonstrating the impressive capabilities\nof generative LMs. While we believe that AGI is still a distant goal, we\nrecognize the potential of LMs in solving tasks such as searching complex\ndocuments, compiling reports with basic analysis, and providing assistance in\nproblem-solving. In this paper, we propose formalizing the execution model of\nlanguage models. We investigate current execution models, to find that this\nformalism has received little attention, and present our contribution: the\nfirst formalized execution model for LMs. We introduce a new algorithm for\nsampling the predictions of LMs, which we use to build a reliable and\ninspectable execution model. We introduce a low-level language to write\n\"cognitive program\" for this execution model. We hope to shed light on the need\nfor execution models for LMs and encourage further research in this area.", "published": "2023-06-16 22:04:50", "link": "http://arxiv.org/abs/2306.10196v1", "categories": ["cs.CL", "cs.AI", "cs.FL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "GPT4 is Slightly Helpful for Peer-Review Assistance: A Pilot Study", "abstract": "In this pilot study, we investigate the use of GPT4 to assist in the\npeer-review process. Our key hypothesis was that GPT-generated reviews could\nachieve comparable helpfulness to human reviewers. By comparing reviews\ngenerated by both human reviewers and GPT models for academic papers submitted\nto a major machine learning conference, we provide initial evidence that\nartificial intelligence can contribute effectively to the peer-review process.\nWe also perform robustness experiments with inserted errors to understand which\nparts of the paper the model tends to focus on. Our findings open new avenues\nfor leveraging machine learning tools to address resource constraints in peer\nreview. The results also shed light on potential enhancements to the review\nprocess and lay the groundwork for further research on scaling oversight in a\ndomain where human-feedback is increasingly a scarce resource.", "published": "2023-06-16 23:11:06", "link": "http://arxiv.org/abs/2307.05492v1", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "MF-PAM: Accurate Pitch Estimation through Periodicity Analysis and\n  Multi-level Feature Fusion", "abstract": "We introduce Multi-level feature Fusion-based Periodicity Analysis Model\n(MF-PAM), a novel deep learning-based pitch estimation model that accurately\nestimates pitch trajectory in noisy and reverberant acoustic environments. Our\nmodel leverages the periodic characteristics of audio signals and involves two\nkey steps: extracting pitch periodicity using periodic non-periodic convolution\n(PNP-Conv) blocks and estimating pitch by aggregating multi-level features\nusing a modified bi-directional feature pyramid network (BiFPN). We evaluate\nour model on speech and music datasets and achieve superior pitch estimation\nperformance compared to state-of-the-art baselines while using fewer model\nparameters. Our model achieves 99.20 % accuracy in pitch estimation on a clean\nmusical dataset. Overall, our proposed model provides a promising solution for\naccurate pitch estimation in challenging acoustic environments and has\npotential applications in audio signal processing.", "published": "2023-06-16 06:01:04", "link": "http://arxiv.org/abs/2306.09640v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Use of a humanoid robot for auditory psychophysical testing", "abstract": "Tasks in psychophysical tests can at times be repetitive and cause\nindividuals to lose engagement during the test. To facilitate engagement, we\npropose the use of a humanoid NAO robot, named Sam, as an alternative interface\nfor conducting psychophysical tests. Specifically, we aim to evaluate the\nperformance of Sam as an auditory testing interface, given its potential\nlimitations and technical differences, in comparison to the current laptop\ninterface. We examine the results and durations of two voice perception tests,\nvoice cue sensitivity and voice gender categorisation, obtained from both the\nconventionally used laptop interface and Sam. Both tests investigate the\nperception and use of two speaker-specific voice cues, fundamental frequency\n(F0) and vocal tract length (VTL), important for characterising voice gender.\nResponses are logged on the laptop using a connected mouse, and on Sam using\nthe tactile sensors. Comparison of test results from both interfaces shows\nfunctional similarity between the interfaces and replicates findings from\nprevious studies with similar tests. Comparison of test durations shows longer\ntesting times with Sam, primarily due to longer processing times in comparison\nto the laptop, as well as other design limitations due to the implementation of\nthe test on the robot. Despite the inherent constraints of the NAO robot, such\nas in sound quality, relatively long processing and testing times, and\ndifferent methods of response logging, the NAO interface appears to facilitate\ncollecting similar data to the current laptop interface, confirming its\npotential as an alternative psychophysical test interface for auditory\nperception tests.", "published": "2023-06-16 09:27:03", "link": "http://arxiv.org/abs/2306.09714v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Knowledge Distillation for Efficient Audio-Visual Video Captioning", "abstract": "Automatically describing audio-visual content with texts, namely video\ncaptioning, has received significant attention due to its potential\napplications across diverse fields. Deep neural networks are the dominant\nmethods, offering state-of-the-art performance. However, these methods are\noften undeployable in low-power devices like smartphones due to the large size\nof the model parameters. In this paper, we propose to exploit simple pooling\nfront-end and down-sampling algorithms with knowledge distillation for audio\nand visual attributes using a reduced number of audio-visual frames. With the\nhelp of knowledge distillation from the teacher model, our proposed method\ngreatly reduces the redundant information in audio-visual streams without\nlosing critical contexts for caption generation. Extensive experimental\nevaluations on the MSR-VTT dataset demonstrate that our proposed approach\nsignificantly reduces the inference time by about 80% with a small sacrifice\n(less than 0.02%) in captioning accuracy.", "published": "2023-06-16 16:28:03", "link": "http://arxiv.org/abs/2306.09947v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Crowdsourcing and Evaluating Text-Based Audio Retrieval Relevances", "abstract": "This paper explores grading text-based audio retrieval relevances with\ncrowdsourcing assessments. Given a free-form text (e.g., a caption) as a query,\ncrowdworkers are asked to grade audio clips using numeric scores (between 0 and\n100) to indicate their judgements of how much the sound content of an audio\nclip matches the text, where 0 indicates no content match at all and 100\nindicates perfect content match. We integrate the crowdsourced relevances into\ntraining and evaluating text-based audio retrieval systems, and evaluate the\neffect of using them together with binary relevances from audio captioning.\nConventionally, these binary relevances are defined by captioning-based\naudio-caption pairs, where being positive indicates that the caption describes\nthe paired audio, and being negative applies to all other pairs. Experimental\nresults indicate that there is no clear benefit from incorporating crowdsourced\nrelevances alongside binary relevances when the crowdsourced relevances are\nbinarized for contrastive learning. Conversely, the results suggest that using\nonly binary relevances defined by captioning-based audio-caption pairs is\nsufficient for contrastive learning.", "published": "2023-06-16 13:04:21", "link": "http://arxiv.org/abs/2306.09820v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Low-Resource Text-to-Speech Using Specific Data and Noise Augmentation", "abstract": "Many neural text-to-speech architectures can synthesize nearly natural speech\nfrom text inputs. These architectures must be trained with tens of hours of\nannotated and high-quality speech data. Compiling such large databases for\nevery new voice requires a lot of time and effort. In this paper, we describe a\nmethod to extend the popular Tacotron-2 architecture and its training with data\naugmentation to enable single-speaker synthesis using a limited amount of\nspecific training data. In contrast to elaborate augmentation methods proposed\nin the literature, we use simple stationary noises for data augmentation. Our\nextension is easy to implement and adds almost no computational overhead during\ntraining and inference. Using only two hours of training data, our approach was\nrated by human listeners to be on par with the baseline Tacotron-2 trained with\n23.5 hours of LJSpeech data. In addition, we tested our model with a\nsemantically unpredictable sentences test, which showed that both models\nexhibit similar intelligibility levels.", "published": "2023-06-16 19:42:40", "link": "http://arxiv.org/abs/2306.10152v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "FALL-E: A Foley Sound Synthesis Model and Strategies", "abstract": "This paper introduces FALL-E, a foley synthesis system and its\ntraining/inference strategies. The FALL-E model employs a cascaded approach\ncomprising low-resolution spectrogram generation, spectrogram super-resolution,\nand a vocoder. We trained every sound-related model from scratch using our\nextensive datasets, and utilized a pre-trained language model. We conditioned\nthe model with dataset-specific texts, enabling it to learn sound quality and\nrecording environment based on text input. Moreover, we leveraged external\nlanguage models to improve text descriptions of our datasets and performed\nprompt engineering for quality, coherence, and diversity. FALL-E was evaluated\nby an objective measure as well as listening tests in the DCASE 2023 challenge\nTask 7. The submission achieved the second place on average, while achieving\nthe best score for diversity, second place for audio quality, and third place\nfor class fitness.", "published": "2023-06-16 12:44:10", "link": "http://arxiv.org/abs/2306.09807v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Correlation Clustering of Bird Sounds", "abstract": "Bird sound classification is the task of relating any sound recording to\nthose species of bird that can be heard in the recording. Here, we study bird\nsound clustering, the task of deciding for any pair of sound recordings whether\nthe same species of bird can be heard in both. We address this problem by first\nlearning, from a training set, probabilities of pairs of recordings being\nrelated in this way, and then inferring a maximally probable partition of a\ntest set by correlation clustering. We address the following questions: How\naccurate is this clustering, compared to a classification of the test set? How\ndo the clusters thus inferred relate to the clusters obtained by\nclassification? How accurate is this clustering when applied to recordings of\nbird species not heard during training? How effective is this clustering in\nseparating, from bird sounds, environmental noise not heard during training?", "published": "2023-06-16 15:35:09", "link": "http://arxiv.org/abs/2306.09906v1", "categories": ["cs.SD", "cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "RealImpact: A Dataset of Impact Sound Fields for Real Objects", "abstract": "Objects make unique sounds under different perturbations, environment\nconditions, and poses relative to the listener. While prior works have modeled\nimpact sounds and sound propagation in simulation, we lack a standard dataset\nof impact sound fields of real objects for audio-visual learning and\ncalibration of the sim-to-real gap. We present RealImpact, a large-scale\ndataset of real object impact sounds recorded under controlled conditions.\nRealImpact contains 150,000 recordings of impact sounds of 50 everyday objects\nwith detailed annotations, including their impact locations, microphone\nlocations, contact force profiles, material labels, and RGBD images. We make\npreliminary attempts to use our dataset as a reference to current simulation\nmethods for estimating object impact sounds that match the real world.\nMoreover, we demonstrate the usefulness of our dataset as a testbed for\nacoustic and audio-visual learning via the evaluation of two benchmark tasks,\nincluding listener location classification and visual acoustic matching.", "published": "2023-06-16 16:25:41", "link": "http://arxiv.org/abs/2306.09944v1", "categories": ["cs.SD", "cs.CV", "cs.GR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Evaluation of Speech Representations for MOS prediction", "abstract": "In this paper, we evaluate feature extraction models for predicting speech\nquality. We also propose a model architecture to compare embeddings of\nsupervised learning and self-supervised learning models with embeddings of\nspeaker verification models to predict the metric MOS. Our experiments were\nperformed on the VCC2018 dataset and a Brazilian-Portuguese dataset called\nBRSpeechMOS, which was created for this work. The results show that the Whisper\nmodel is appropriate in all scenarios: with both the VCC2018 and BRSpeech- MOS\ndatasets. Among the supervised and self-supervised learning models using\nBRSpeechMOS, Whisper-Small achieved the best linear correlation of 0.6980, and\nthe speaker verification model, SpeakerNet, had linear correlation of 0.6963.\nUsing VCC2018, the best supervised and self-supervised learning model,\nWhisper-Large, achieved linear correlation of 0.7274, and the best model\nspeaker verification, TitaNet, achieved a linear correlation of 0.6933.\nAlthough the results of the speaker verification models are slightly lower, the\nSpeakerNet model has only 5M parameters, making it suitable for real-time\napplications, and the TitaNet model produces an embedding of size 192, the\nsmallest among all the evaluated models. The experiment results are\nreproducible with publicly available source-code1 .", "published": "2023-06-16 17:21:42", "link": "http://arxiv.org/abs/2306.09979v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Acoustic Identification of Ae. aegypti Mosquitoes using Smartphone Apps\n  and Residual Convolutional Neural Networks", "abstract": "In this paper, we advocate in favor of smartphone apps as low-cost,\neasy-to-deploy solution for raising awareness among the population on the\nproliferation of Aedes aegypti mosquitoes. Nevertheless, devising such a\nsmartphone app is challenging, for many reasons, including the required\nmaturity level of techniques for identifying mosquitoes based on features that\ncan be captured using smartphone resources. In this paper, we identify a set of\n(non-exhaustive) requirements that smartphone apps must meet to become an\neffective tooling in the fight against Ae. aegypti, and advance the\nstate-of-the-art with (i) a residual convolutional neural network for\nclassifying Ae. aegypti mosquitoes from their wingbeat sound, (ii) a\nmethodology for reducing the influence of background noise in the\nclassification process, and (iii) a dataset for benchmarking solutions for\ndetecting Ae. aegypti mosquitoes from wingbeat sound recordings. From the\nanalysis of accuracy and recall, we provide evidence that convolutional neural\nnetworks have potential as a cornerstone for tracking mosquito apps for\nsmartphones.", "published": "2023-06-16 13:41:01", "link": "http://arxiv.org/abs/2306.10091v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Low-rank Matching Attention based Cross-modal Feature Fusion Method\n  for Conversational Emotion Recognition", "abstract": "Conversational emotion recognition (CER) is an important research topic in\nhuman-computer interactions. {Although recent advancements in transformer-based\ncross-modal fusion methods have shown promise in CER tasks, they tend to\noverlook the crucial intra-modal and inter-modal emotional interaction or\nsuffer from high computational complexity. To address this, we introduce a\nnovel and lightweight cross-modal feature fusion method called Low-Rank\nMatching Attention Method (LMAM). LMAM effectively captures contextual\nemotional semantic information in conversations while mitigating the quadratic\ncomplexity issue caused by the self-attention mechanism. Specifically, by\nsetting a matching weight and calculating inter-modal features attention scores\nrow by row, LMAM requires only one-third of the parameters of self-attention\nmethods. We also employ the low-rank decomposition method on the weights to\nfurther reduce the number of parameters in LMAM. As a result, LMAM offers a\nlightweight model while avoiding overfitting problems caused by a large number\nof parameters. Moreover, LMAM is able to fully exploit the intra-modal\nemotional contextual information within each modality and integrates\ncomplementary emotional semantic information across modalities by computing and\nfusing similarities of intra-modal and inter-modal features simultaneously.\nExperimental results verify the superiority of LMAM compared with other popular\ncross-modal fusion methods on the premise of being more lightweight. Also, LMAM\ncan be embedded into any existing state-of-the-art CER methods in a\nplug-and-play manner, and can be applied to other multi-modal recognition\ntasks, e.g., session recommendation and humour detection, demonstrating its\nremarkable generalization ability.", "published": "2023-06-16 16:02:44", "link": "http://arxiv.org/abs/2306.17799v2", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "CLIPSonic: Text-to-Audio Synthesis with Unlabeled Videos and Pretrained\n  Language-Vision Models", "abstract": "Recent work has studied text-to-audio synthesis using large amounts of paired\ntext-audio data. However, audio recordings with high-quality text annotations\ncan be difficult to acquire. In this work, we approach text-to-audio synthesis\nusing unlabeled videos and pretrained language-vision models. We propose to\nlearn the desired text-audio correspondence by leveraging the visual modality\nas a bridge. We train a conditional diffusion model to generate the audio track\nof a video, given a video frame encoded by a pretrained contrastive\nlanguage-image pretraining (CLIP) model. At test time, we first explore\nperforming a zero-shot modality transfer and condition the diffusion model with\na CLIP-encoded text query. However, we observe a noticeable performance drop\nwith respect to image queries. To close this gap, we further adopt a pretrained\ndiffusion prior model to generate a CLIP image embedding given a CLIP text\nembedding. Our results show the effectiveness of the proposed method, and that\nthe pretrained diffusion prior can reduce the modality transfer gap. While we\nfocus on text-to-audio synthesis, the proposed model can also generate audio\nfrom image queries, and it shows competitive performance against a\nstate-of-the-art image-to-audio synthesis model in a subjective listening test.\nThis study offers a new direction of approaching text-to-audio synthesis that\nleverages the naturally-occurring audio-visual correspondence in videos and the\npower of pretrained language-vision models.", "published": "2023-06-16 05:42:01", "link": "http://arxiv.org/abs/2306.09635v2", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
