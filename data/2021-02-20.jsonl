{"title": "Machine Translation Customization via Automatic Training Data Selection\n  from the Web", "abstract": "Machine translation (MT) systems, especially when designed for an industrial\nsetting, are trained with general parallel data derived from the Web. Thus,\ntheir style is typically driven by word/structure distribution coming from the\naverage of many domains. In contrast, MT customers want translations to be\nspecialized to their domain, for which they are typically able to provide text\nsamples. We describe an approach for customizing MT systems on specific domains\nby selecting data similar to the target customer data to train neural\ntranslation models. We build document classifiers using monolingual target\ndata, e.g., provided by the customers to select parallel training data from Web\ncrawled data. Finally, we train MT models on our automatically selected data,\nobtaining a system specialized to the target domain. We tested our approach on\nthe benchmark from WMT-18 Translation Task for News domains enabling\ncomparisons with state-of-the-art MT systems. The results show that our models\noutperform the top systems while using less data and smaller models.", "published": "2021-02-20 03:29:41", "link": "http://arxiv.org/abs/2102.10243v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Answer Sentence Reranking via Automatically Translated Data", "abstract": "We present a study on the design of multilingual Answer Sentence Selection\n(AS2) models, which are a core component of modern Question Answering (QA)\nsystems. The main idea is to transfer data, created from one resource rich\nlanguage, e.g., English, to other languages, less rich in terms of resources.\nThe main findings of this paper are: (i) the training data for AS2 translated\ninto a target language can be used to effectively fine-tune a Transformer-based\nmodel for that language; (ii) one multilingual Transformer model it is enough\nto rank answers in multiple languages; and (iii) mixed-language question/answer\npairs can be used to fine-tune models to select answers from any language,\nwhere the input question is just in one language. This highly reduces the\ncomplexity and technical requirement of a multilingual QA system. Our\nexperiments validate the findings above, showing a modest drop, at most 3%,\nwith respect to the state-of-the-art English model.", "published": "2021-02-20 03:52:08", "link": "http://arxiv.org/abs/2102.10250v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep Structured Feature Networks for Table Detection and Tabular Data\n  Extraction from Scanned Financial Document Images", "abstract": "Automatic table detection in PDF documents has achieved a great success but\ntabular data extraction are still challenging due to the integrity and noise\nissues in detected table areas. The accurate data extraction is extremely\ncrucial in finance area. Inspired by this, the aim of this research is\nproposing an automated table detection and tabular data extraction from\nfinancial PDF documents. We proposed a method that consists of three main\nprocesses, which are detecting table areas with a Faster R-CNN (Region-based\nConvolutional Neural Network) model with Feature Pyramid Network (FPN) on each\npage image, extracting contents and structures by a compounded layout\nsegmentation technique based on optical character recognition (OCR) and\nformulating regular expression rules for table header separation. The tabular\ndata extraction feature is embedded with rule-based filtering and restructuring\nfunctions that are highly scalable. We annotate a new Financial Documents\ndataset with table regions for the experiment. The excellent table detection\nperformance of the detection model is obtained from our customized dataset. The\nmain contributions of this paper are proposing the Financial Documents dataset\nwith table-area annotations, the superior detection model and the rule-based\nlayout segmentation technique for the tabular data extraction from PDF files.", "published": "2021-02-20 08:21:17", "link": "http://arxiv.org/abs/2102.10287v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contextual Argument Component Classification for Class Discussions", "abstract": "Argument mining systems often consider contextual information, i.e.\ninformation outside of an argumentative discourse unit, when trained to\naccomplish tasks such as argument component identification, classification, and\nrelation extraction. However, prior work has not carefully analyzed the utility\nof different contextual properties in context-aware models. In this work, we\nshow how two different types of contextual information, local discourse context\nand speaker context, can be incorporated into a computational model for\nclassifying argument components in multi-party classroom discussions. We find\nthat both context types can improve performance, although the improvements are\ndependent on context size and position.", "published": "2021-02-20 08:48:07", "link": "http://arxiv.org/abs/2102.10290v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Discussion Tracker: Supporting Teacher Learning about Students'\n  Collaborative Argumentation in High School Classrooms", "abstract": "Teaching collaborative argumentation is an advanced skill that many K-12\nteachers struggle to develop. To address this, we have developed Discussion\nTracker, a classroom discussion analytics system based on novel algorithms for\nclassifying argument moves, specificity, and collaboration. Results from a\nclassroom deployment indicate that teachers found the analytics useful, and\nthat the underlying classifiers perform with moderate to substantial agreement\nwith humans.", "published": "2021-02-20 09:06:57", "link": "http://arxiv.org/abs/2102.10293v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding and Enhancing the Use of Context for Machine Translation", "abstract": "To understand and infer meaning in language, neural models have to learn\ncomplicated nuances. Discovering distinctive linguistic phenomena from data is\nnot an easy task. For instance, lexical ambiguity is a fundamental feature of\nlanguage which is challenging to learn. Even more prominently, inferring the\nmeaning of rare and unseen lexical units is difficult with neural networks.\nMeaning is often determined from context. With context, languages allow meaning\nto be conveyed even when the specific words used are not known by the reader.\nTo model this learning process, a system has to learn from a few instances in\ncontext and be able to generalize well to unseen cases. The learning process is\nhindered when training data is scarce for a task. Even with sufficient data,\nlearning patterns for the long tail of the lexical distribution is challenging.\nIn this thesis, we focus on understanding certain potentials of contexts in\nneural models and design augmentation models to benefit from them. We focus on\nmachine translation as an important instance of the more general language\nunderstanding problem. To translate from a source language to a target\nlanguage, a neural model has to understand the meaning of constituents in the\nprovided context and generate constituents with the same meanings in the target\nlanguage. This task accentuates the value of capturing nuances of language and\nthe necessity of generalization from few observations. The main problem we\nstudy in this thesis is what neural machine translation models learn from data\nand how we can devise more focused contexts to enhance this learning. Looking\nmore in-depth into the role of context and the impact of data on learning\nmodels is essential to advance the NLP field. Moreover, it helps highlight the\nvulnerabilities of current neural networks and provides insights into designing\nmore robust models.", "published": "2021-02-20 20:19:27", "link": "http://arxiv.org/abs/2102.10437v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Entity Structure Within and Throughout: Modeling Mention Dependencies\n  for Document-Level Relation Extraction", "abstract": "Entities, as the essential elements in relation extraction tasks, exhibit\ncertain structure. In this work, we formulate such structure as distinctive\ndependencies between mention pairs. We then propose SSAN, which incorporates\nthese structural dependencies within the standard self-attention mechanism and\nthroughout the overall encoding stage. Specifically, we design two alternative\ntransformation modules inside each self-attention building block to produce\nattentive biases so as to adaptively regularize its attention flow. Our\nexperiments demonstrate the usefulness of the proposed entity structure and the\neffectiveness of SSAN. It significantly outperforms competitive baselines,\nachieving new state-of-the-art results on three popular document-level relation\nextraction datasets. We further provide ablation and visualization to show how\nthe entity structure guides the model for better relation extraction. Our code\nis publicly available.", "published": "2021-02-20 03:47:46", "link": "http://arxiv.org/abs/2102.10249v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "NUBOT: Embedded Knowledge Graph With RASA Framework for Generating\n  Semantic Intents Responses in Roman Urdu", "abstract": "The understanding of the human language is quantified by identifying intents\nand entities. Even though classification methods that rely on labeled\ninformation are often used for the comprehension of language understanding, it\nis incredibly time consuming and tedious process to generate high propensity\nsupervised datasets. In this paper, we present the generation of accurate\nintents for the corresponding Roman Urdu unstructured data and integrate this\ncorpus in RASA NLU module for intent classification. We embed knowledge graph\nwith RASA Framework to maintain the dialog history for semantic based natural\nlanguage mechanism for chatbot communication. We compare results of our work\nwith existing linguistic systems combined with semantic technologies. Minimum\naccuracy of intents generation is 64 percent of confidence and in the response\ngeneration part minimum accuracy is 82.1 percent and maximum accuracy gain is\n96.7 percent. All the scores refers to log precision, recall, and f1 measure\nfor each intents once summarized for all. Furthermore, it creates a confusion\nmatrix represents that which intents are ambiguously recognized by approach.", "published": "2021-02-20 18:17:21", "link": "http://arxiv.org/abs/2102.10410v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Knowledge-Base Enriched Word Embeddings for Biomedical Domain", "abstract": "Word embeddings have been shown adept at capturing the semantic and syntactic\nregularities of the natural language text, as a result of which these\nrepresentations have found their utility in a wide variety of downstream\ncontent analysis tasks. Commonly, these word embedding techniques derive the\ndistributed representation of words based on the local context information.\nHowever, such approaches ignore the rich amount of explicit information present\nin knowledge-bases. This is problematic, as it might lead to poor\nrepresentation for words with insufficient local context such as domain\nspecific words. Furthermore, the problem becomes pronounced in domain such as\nbio-medicine where the presence of these domain specific words are relatively\nhigh. Towards this end, in this project, we propose a new word embedding based\nmodel for biomedical domain that jointly leverages the information from\navailable corpora and domain knowledge in order to generate knowledge-base\npowered embeddings. Unlike existing approaches, the proposed methodology is\nsimple but adept at capturing the precise knowledge available in domain\nresources in an accurate way. Experimental results on biomedical concept\nsimilarity and relatedness task validates the effectiveness of the proposed\napproach.", "published": "2021-02-20 18:18:51", "link": "http://arxiv.org/abs/2103.00479v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Automatic Evaluation of Dialog Systems: A Model-Free Off-Policy\n  Evaluation Approach", "abstract": "Reliable automatic evaluation of dialogue systems under an interactive\nenvironment has long been overdue. An ideal environment for evaluating dialog\nsystems, also known as the Turing test, needs to involve human interaction,\nwhich is usually not affordable for large-scale experiments. Though researchers\nhave attempted to use metrics (e.g., perplexity, BLEU) in language generation\ntasks or some model-based reinforcement learning methods (e.g., self-play\nevaluation) for automatic evaluation, these methods only show a very weak\ncorrelation with the actual human evaluation in practice. To bridge such a gap,\nwe propose a new framework named ENIGMA for estimating human evaluation scores\nbased on recent advances of off-policy evaluation in reinforcement learning.\nENIGMA only requires a handful of pre-collected experience data, and therefore\ndoes not involve human interaction with the target policy during the\nevaluation, making automatic evaluations feasible. More importantly, ENIGMA is\nmodel-free and agnostic to the behavior policies for collecting the experience\ndata (see details in Section 2), which significantly alleviates the technical\ndifficulties of modeling complex dialogue environments and human behaviors. Our\nexperiments show that ENIGMA significantly outperforms existing methods in\nterms of correlation with human evaluation scores.", "published": "2021-02-20 03:29:20", "link": "http://arxiv.org/abs/2102.10242v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CDA: a Cost Efficient Content-based Multilingual Web Document Aligner", "abstract": "We introduce a Content-based Document Alignment approach (CDA), an efficient\nmethod to align multilingual web documents based on content in creating\nparallel training data for machine translation (MT) systems operating at the\nindustrial level. CDA works in two steps: (i) projecting documents of a web\ndomain to a shared multilingual space; then (ii) aligning them based on the\nsimilarity of their representations in such space. We leverage lexical\ntranslation models to build vector representations using TF-IDF. CDA achieves\nperformance comparable with state-of-the-art systems in the WMT-16 Bilingual\nDocument Alignment Shared Task benchmark while operating in multilingual space.\nBesides, we created two web-scale datasets to examine the robustness of CDA in\nan industrial setting involving up to 28 languages and millions of documents.\nThe experiments show that CDA is robust, cost-effective, and is significantly\nsuperior in (i) processing large and noisy web data and (ii) scaling to new and\nlow-resourced languages.", "published": "2021-02-20 03:37:23", "link": "http://arxiv.org/abs/2102.10246v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "An Attention Ensemble Approach for Efficient Text Classification of\n  Indian Languages", "abstract": "The recent surge of complex attention-based deep learning architectures has\nled to extraordinary results in various downstream NLP tasks in the English\nlanguage. However, such research for resource-constrained and morphologically\nrich Indian vernacular languages has been relatively limited. This paper\nproffers team SPPU\\_AKAH's solution for the TechDOfication 2020 subtask-1f:\nwhich focuses on the coarse-grained technical domain identification of short\ntext documents in Marathi, a Devanagari script-based Indian language. Availing\nthe large dataset at hand, a hybrid CNN-BiLSTM attention ensemble model is\nproposed that competently combines the intermediate sentence representations\ngenerated by the convolutional neural network and the bidirectional long\nshort-term memory, leading to efficient text classification. Experimental\nresults show that the proposed model outperforms various baseline machine\nlearning and deep learning models in the given task, giving the best validation\naccuracy of 89.57\\% and f1-score of 0.8875. Furthermore, the solution resulted\nin the best system submission for this subtask, giving a test accuracy of\n64.26\\% and f1-score of 0.6157, transcending the performances of other teams as\nwell as the baseline system given by the organizers of the shared task.", "published": "2021-02-20 07:31:38", "link": "http://arxiv.org/abs/2102.10275v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "VisualGPT: Data-efficient Adaptation of Pretrained Language Models for\n  Image Captioning", "abstract": "The ability to quickly learn from a small quantity oftraining data widens the\nrange of machine learning applications. In this paper, we propose a\ndata-efficient image captioning model, VisualGPT, which leverages the\nlinguistic knowledge from a large pretrained language model(LM). A crucial\nchallenge is to balance between the use of visual information in the image and\nprior linguistic knowledge acquired from pretraining. We designed a novel\nself-resurrecting encoder-decoder attention mechanism to quickly adapt the\npretrained LM as the language decoder ona small amount of in-domain training\ndata. The proposed self-resurrecting activation unit produces sparse\nactivations but has reduced susceptibility to zero gradients. We train the\nproposed model, VisualGPT, on 0.1%, 0.5% and 1% of MSCOCO and Conceptual\nCaptions training data. Under these conditions, we outperform the best baseline\nmodel by up to 10.8% CIDEr on MS COCO and upto 5.4% CIDEr on Conceptual\nCaptions. Further, Visual-GPT achieves the state-of-the-art result on IU X-ray,\na medical report generation dataset. To the best of our knowledge, this is the\nfirst work that improves data efficiency of image captioning by utilizing LM\npretrained on unimodal data. Our code is available at:\nhttps://github.com/Vision-CAIR/VisualGPT.", "published": "2021-02-20 18:02:42", "link": "http://arxiv.org/abs/2102.10407v5", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Model architectures to extrapolate emotional expressions in DNN-based\n  text-to-speech", "abstract": "This paper proposes architectures that facilitate the extrapolation of\nemotional expressions in deep neural network (DNN)-based text-to-speech (TTS).\nIn this study, the meaning of \"extrapolate emotional expressions\" is to borrow\nemotional expressions from others, and the collection of emotional speech\nuttered by target speakers is unnecessary. Although a DNN has potential power\nto construct DNN-based TTS with emotional expressions and some DNN-based TTS\nsystems have demonstrated satisfactory performances in the expression of the\ndiversity of human speech, it is necessary and troublesome to collect emotional\nspeech uttered by target speakers. To solve this issue, we propose\narchitectures to separately train the speaker feature and the emotional feature\nand to synthesize speech with any combined quality of speakers and emotions.\nThe architectures are parallel model (PM), serial model (SM), auxiliary input\nmodel (AIM), and hybrid models (PM&AIM and SM&AIM). These models are trained\nthrough emotional speech uttered by few speakers and neutral speech uttered by\nmany speakers. Objective evaluations demonstrate that the performances in the\nopen-emotion test provide insufficient information. They make a comparison with\nthose in the closed-emotion test, but each speaker has their own manner of\nexpressing emotion. However, subjective evaluation results indicate that the\nproposed models could convey emotional information to some extent. Notably, the\nPM can correctly convey sad and joyful emotions at a rate of >60%.", "published": "2021-02-20 13:29:48", "link": "http://arxiv.org/abs/2102.10345v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "The Accented English Speech Recognition Challenge 2020: Open Datasets,\n  Tracks, Baselines, Results and Methods", "abstract": "The variety of accents has posed a big challenge to speech recognition. The\nAccented English Speech Recognition Challenge (AESRC2020) is designed for\nproviding a common testbed and promoting accent-related research. Two tracks\nare set in the challenge -- English accent recognition (track 1) and accented\nEnglish speech recognition (track 2). A set of 160 hours of accented English\nspeech collected from 8 countries is released with labels as the training set.\nAnother 20 hours of speech without labels is later released as the test set,\nincluding two unseen accents from another two countries used to test the model\ngeneralization ability in track 2. We also provide baseline systems for the\nparticipants. This paper first reviews the released dataset, track setups,\nbaselines and then summarizes the challenge results and major techniques used\nin the submissions.", "published": "2021-02-20 02:50:44", "link": "http://arxiv.org/abs/2102.10233v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Singer Identification Using Deep Timbre Feature Learning with KNN-Net", "abstract": "In this paper, we study the issue of automatic singer identification (SID) in\npopular music recordings, which aims to recognize who sang a given piece of\nsong. The main challenge for this investigation lies in the fact that a\nsinger's singing voice changes and intertwines with the signal of background\naccompaniment in time domain. To handle this challenge, we propose the KNN-Net\nfor SID, which is a deep neural network model with the goal of learning local\ntimbre feature representation from the mixture of singer voice and background\nmusic. Unlike other deep neural networks using the softmax layer as the output\nlayer, we instead utilize the KNN as a more interpretable layer to output\ntarget singer labels. Moreover, attention mechanism is first introduced to\nhighlight crucial timbre features for SID. Experiments on the existing artist20\ndataset show that the proposed approach outperforms the state-of-the-art method\nby 4%. We also create singer32 and singer60 datasets consisting of Chinese pop\nmusic to evaluate the reliability of the proposed method. The more extensive\nexperiments additionally indicate that our proposed model achieves a\nsignificant performance improvement compared to the state-of-the-art methods.", "published": "2021-02-20 02:55:17", "link": "http://arxiv.org/abs/2102.10236v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "WARP-Q: Quality Prediction For Generative Neural Speech Codecs", "abstract": "Good speech quality has been achieved using waveform matching and parametric\nreconstruction coders. Recently developed very low bit rate generative codecs\ncan reconstruct high quality wideband speech with bit streams less than 3 kb/s.\nThese codecs use a DNN with parametric input to synthesise high quality speech\noutputs. Existing objective speech quality models (e.g., POLQA, ViSQOL) do not\naccurately predict the quality of coded speech from these generative models\nunderestimating quality due to signal differences not highlighted in subjective\nlistening tests. We present WARP-Q, a full-reference objective speech quality\nmetric that uses dynamic time warping cost for MFCC speech representations. It\nis robust to small perceptual signal changes. Evaluation using waveform\nmatching, parametric and generative neural vocoder based codecs as well as\nchannel and environmental noise shows that WARP-Q has better correlation and\ncodec quality ranking for novel codecs compared to traditional metrics in\naddition to versatility for general quality assessment scenarios.", "published": "2021-02-20 21:25:05", "link": "http://arxiv.org/abs/2102.10449v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Learnable MFCCs for Speaker Verification", "abstract": "We propose a learnable mel-frequency cepstral coefficient (MFCC) frontend\narchitecture for deep neural network (DNN) based automatic speaker\nverification. Our architecture retains the simplicity and interpretability of\nMFCC-based features while allowing the model to be adapted to data flexibly. In\npractice, we formulate data-driven versions of the four linear transforms of a\nstandard MFCC extractor -- windowing, discrete Fourier transform (DFT), mel\nfilterbank and discrete cosine transform (DCT). Results reported reach up to\n6.7\\% (VoxCeleb1) and 9.7\\% (SITW) relative improvement in term of equal error\nrate (EER) from static MFCCs, without additional tuning effort.", "published": "2021-02-20 12:16:35", "link": "http://arxiv.org/abs/2102.10322v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The Use of Voice Source Features for Sung Speech Recognition", "abstract": "In this paper, we ask whether vocal source features (pitch, shimmer, jitter,\netc) can improve the performance of automatic sung speech recognition, arguing\nthat conclusions previously drawn from spoken speech studies may not be valid\nin the sung speech domain. We first use a parallel singing/speaking corpus\n(NUS-48E) to illustrate differences in sung vs spoken voicing characteristics\nincluding pitch range, syllables duration, vibrato, jitter and shimmer. We then\nuse this analysis to inform speech recognition experiments on the sung speech\nDSing corpus, using a state of the art acoustic model and augmenting\nconventional features with various voice source parameters. Experiments are run\nwith three standard (increasingly large) training sets, DSing1 (15.1 hours),\nDSing3 (44.7 hours) and DSing30 (149.1 hours). Pitch combined with degree of\nvoicing produces a significant decrease in WER from 38.1% to 36.7% when\ntraining with DSing1 however smaller decreases in WER observed when training\nwith the larger more varied DSing3 and DSing30 sets were not seen to be\nstatistically significant. Voicing quality characteristics did not improve\nrecognition performance although analysis suggests that they do contribute to\nan improved discrimination between voiced/unvoiced phoneme pairs.", "published": "2021-02-20 15:54:26", "link": "http://arxiv.org/abs/2102.10376v2", "categories": ["eess.AS", "cs.AI", "eess.SP"], "primary_category": "eess.AS"}
