{"title": "The Fluidity of Concept Representations in Human Brain Signals", "abstract": "Cognitive theories of human language processing often distinguish between\nconcrete and abstract concepts. In this work, we analyze the discriminability\nof concrete and abstract concepts in fMRI data using a range of analysis\nmethods. We find that the distinction can be decoded from the signal with an\naccuracy significantly above chance, but it is not found to be a relevant\nstructuring factor in clustering and relational analyses. From our detailed\ncomparison, we obtain the impression that human concept representations are\nmore fluid than dichotomous categories can capture. We argue that fluid concept\nrepresentations lead to more realistic models of human language processing\nbecause they better capture the ambiguity and underspecification present in\nnatural language use.", "published": "2020-02-20 17:31:04", "link": "http://arxiv.org/abs/2002.08880v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Federated pretraining and fine tuning of BERT using clinical notes from\n  multiple silos", "abstract": "Large scale contextual representation models, such as BERT, have\nsignificantly advanced natural language processing (NLP) in recently years.\nHowever, in certain area like healthcare, accessing diverse large scale text\ndata from multiple institutions is extremely challenging due to privacy and\nregulatory reasons. In this article, we show that it is possible to both\npretrain and fine tune BERT models in a federated manner using clinical texts\nfrom different silos without moving the data.", "published": "2020-02-20 04:14:35", "link": "http://arxiv.org/abs/2002.08562v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FrameAxis: Characterizing Microframe Bias and Intensity with Word\n  Embedding", "abstract": "Framing is a process of emphasizing a certain aspect of an issue over the\nothers, nudging readers or listeners towards different positions on the issue\neven without making a biased argument. {Here, we propose FrameAxis, a method\nfor characterizing documents by identifying the most relevant semantic axes\n(\"microframes\") that are overrepresented in the text using word embedding. Our\nunsupervised approach can be readily applied to large datasets because it does\nnot require manual annotations. It can also provide nuanced insights by\nconsidering a rich set of semantic axes. FrameAxis is designed to\nquantitatively tease out two important dimensions of how microframes are used\nin the text. \\textit{Microframe bias} captures how biased the text is on a\ncertain microframe, and \\textit{microframe intensity} shows how actively a\ncertain microframe is used. Together, they offer a detailed characterization of\nthe text. We demonstrate that microframes with the highest bias and intensity\nwell align with sentiment, topic, and partisan spectrum by applying FrameAxis\nto multiple datasets from restaurant reviews to political news.} The existing\ndomain knowledge can be incorporated into FrameAxis {by using custom\nmicroframes and by using FrameAxis as an iterative exploratory analysis\ninstrument.} Additionally, we propose methods for explaining the results of\nFrameAxis at the level of individual words and documents. Our method may\naccelerate scalable and sophisticated computational analyses of framing across\ndisciplines.", "published": "2020-02-20 08:01:28", "link": "http://arxiv.org/abs/2002.08608v4", "categories": ["cs.CL", "cs.CY", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Balancing Cost and Benefit with Tied-Multi Transformers", "abstract": "We propose and evaluate a novel procedure for training multiple Transformers\nwith tied parameters which compresses multiple models into one enabling the\ndynamic choice of the number of encoder and decoder layers during decoding. In\nsequence-to-sequence modeling, typically, the output of the last layer of the\nN-layer encoder is fed to the M-layer decoder, and the output of the last\ndecoder layer is used to compute loss. Instead, our method computes a single\nloss consisting of NxM losses, where each loss is computed from the output of\none of the M decoder layers connected to one of the N encoder layers. Such a\nmodel subsumes NxM models with different number of encoder and decoder layers,\nand can be used for decoding with fewer than the maximum number of encoder and\ndecoder layers. We then propose a mechanism to choose a priori the number of\nencoder and decoder layers for faster decoding, and also explore recurrent\nstacking of layers and knowledge distillation for model compression. We present\na cost-benefit analysis of applying the proposed approaches for neural machine\ntranslation and show that they reduce decoding costs while preserving\ntranslation quality.", "published": "2020-02-20 08:20:52", "link": "http://arxiv.org/abs/2002.08614v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Guiding attention in Sequence-to-sequence models for Dialogue Act\n  prediction", "abstract": "The task of predicting dialog acts (DA) based on conversational dialog is a\nkey component in the development of conversational agents. Accurately\npredicting DAs requires a precise modeling of both the conversation and the\nglobal tag dependencies. We leverage seq2seq approaches widely adopted in\nNeural Machine Translation (NMT) to improve the modelling of tag sequentiality.\nSeq2seq models are known to learn complex global dependencies while currently\nproposed approaches using linear conditional random fields (CRF) only model\nlocal tag dependencies. In this work, we introduce a seq2seq model tailored for\nDA classification using: a hierarchical encoder, a novel guided attention\nmechanism and beam search applied to both training and inference. Compared to\nthe state of the art our model does not require handcrafted features and is\ntrained end-to-end. Furthermore, the proposed approach achieves an unmatched\naccuracy score of 85% on SwDA, and state-of-the-art accuracy score of 91.6% on\nMRDA.", "published": "2020-02-20 15:25:20", "link": "http://arxiv.org/abs/2002.08801v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Contextual Lensing of Universal Sentence Representations", "abstract": "What makes a universal sentence encoder universal? The notion of a generic\nencoder of text appears to be at odds with the inherent contextualization and\nnon-permanence of language use in a dynamic world. However, mapping sentences\ninto generic fixed-length vectors for downstream similarity and retrieval tasks\nhas been fruitful, particularly for multilingual applications. How do we manage\nthis dilemma? In this work we propose Contextual Lensing, a methodology for\ninducing context-oriented universal sentence vectors. We break the construction\nof universal sentence vectors into a core, variable length, sentence matrix\nrepresentation equipped with an adaptable `lens' from which fixed-length\nvectors can be induced as a function of the lens context. We show that it is\npossible to focus notions of language similarity into a small number of lens\nparameters given a core universal matrix representation. For example, we\ndemonstrate the ability to encode translation similarity of sentences across\nseveral languages into a single weight matrix, even when the core encoder has\nnot seen parallel data.", "published": "2020-02-20 17:06:27", "link": "http://arxiv.org/abs/2002.08866v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Measuring Social Biases in Grounded Vision and Language Embeddings", "abstract": "We generalize the notion of social biases from language embeddings to\ngrounded vision and language embeddings. Biases are present in grounded\nembeddings, and indeed seem to be equally or more significant than for\nungrounded embeddings. This is despite the fact that vision and language can\nsuffer from different biases, which one might hope could attenuate the biases\nin both. Multiple ways exist to generalize metrics measuring bias in word\nembeddings to this new setting. We introduce the space of generalizations\n(Grounded-WEAT and Grounded-SEAT) and demonstrate that three generalizations\nanswer different yet important questions about how biases, language, and vision\ninteract. These metrics are used on a new dataset, the first for grounded bias,\ncreated by augmenting extending standard linguistic bias benchmarks with 10,228\nimages from COCO, Conceptual Captions, and Google Images. Dataset construction\nis challenging because vision datasets are themselves very biased. The presence\nof these biases in systems will begin to have real-world consequences as they\nare deployed, making carefully measuring bias and then mitigating it critical\nto building a fair society.", "published": "2020-02-20 17:54:46", "link": "http://arxiv.org/abs/2002.08911v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi-Agent Reinforcement Learning as a Computational Tool for Language\n  Evolution Research: Historical Context and Future Challenges", "abstract": "Computational models of emergent communication in agent populations are\ncurrently gaining interest in the machine learning community due to recent\nadvances in Multi-Agent Reinforcement Learning (MARL). Current contributions\nare however still relatively disconnected from the earlier theoretical and\ncomputational literature aiming at understanding how language might have\nemerged from a prelinguistic substance. The goal of this paper is to position\nrecent MARL contributions within the historical context of language evolution\nresearch, as well as to extract from this theoretical and computational\nbackground a few challenges for future research.", "published": "2020-02-20 17:26:46", "link": "http://arxiv.org/abs/2002.08878v2", "categories": ["cs.MA", "cs.CL", "cs.LG"], "primary_category": "cs.MA"}
{"title": "Imputer: Sequence Modelling via Imputation and Dynamic Programming", "abstract": "This paper presents the Imputer, a neural sequence model that generates\noutput sequences iteratively via imputations. The Imputer is an iterative\ngenerative model, requiring only a constant number of generation steps\nindependent of the number of input or output tokens. The Imputer can be trained\nto approximately marginalize over all possible alignments between the input and\noutput sequences, and all possible generation orders. We present a tractable\ndynamic programming training algorithm, which yields a lower bound on the log\nmarginal likelihood. When applied to end-to-end speech recognition, the Imputer\noutperforms prior non-autoregressive models and achieves competitive results to\nautoregressive models. On LibriSpeech test-other, the Imputer achieves 11.1\nWER, outperforming CTC at 13.0 WER and seq2seq at 12.5 WER.", "published": "2020-02-20 18:21:30", "link": "http://arxiv.org/abs/2002.08926v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Aspect Term Extraction using Graph-based Semi-Supervised Learning", "abstract": "Aspect based Sentiment Analysis is a major subarea of sentiment analysis.\nMany supervised and unsupervised approaches have been proposed in the past for\ndetecting and analyzing the sentiment of aspect terms. In this paper, a\ngraph-based semi-supervised learning approach for aspect term extraction is\nproposed. In this approach, every identified token in the review document is\nclassified as aspect or non-aspect term from a small set of labeled tokens\nusing label spreading algorithm. The k-Nearest Neighbor (kNN) for graph\nsparsification is employed in the proposed approach to make it more time and\nmemory efficient. The proposed work is further extended to determine the\npolarity of the opinion words associated with the identified aspect terms in\nreview sentence to generate visual aspect-based summary of review documents.\nThe experimental study is conducted on benchmark and crawled datasets of\nrestaurant and laptop domains with varying value of labeled instances. The\nresults depict that the proposed approach could achieve good result in terms of\nPrecision, Recall and Accuracy with limited availability of labeled data.", "published": "2020-02-20 13:11:02", "link": "http://arxiv.org/abs/2003.04968v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Wavesplit: End-to-End Speech Separation by Speaker Clustering", "abstract": "We introduce Wavesplit, an end-to-end source separation system. From a single\nmixture, the model infers a representation for each source and then estimates\neach source signal given the inferred representations. The model is trained to\njointly perform both tasks from the raw waveform. Wavesplit infers a set of\nsource representations via clustering, which addresses the fundamental\npermutation problem of separation. For speech separation, our sequence-wide\nspeaker representations provide a more robust separation of long, challenging\nrecordings compared to prior work. Wavesplit redefines the state-of-the-art on\nclean mixtures of 2 or 3 speakers (WSJ0-2/3mix), as well as in noisy and\nreverberated settings (WHAM/WHAMR). We also set a new benchmark on the recent\nLibriMix dataset. Finally, we show that Wavesplit is also applicable to other\ndomains, by separating fetal and maternal heart rates from a single abdominal\nelectrocardiogram.", "published": "2020-02-20 18:30:36", "link": "http://arxiv.org/abs/2002.08933v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "A Neural Lip-Sync Framework for Synthesizing Photorealistic Virtual News\n  Anchors", "abstract": "Lip sync has emerged as a promising technique for generating mouth movements\nfrom audio signals. However, synthesizing a high-resolution and photorealistic\nvirtual news anchor is still challenging. Lack of natural appearance, visual\nconsistency, and processing efficiency are the main problems with existing\nmethods. This paper presents a novel lip-sync framework specially designed for\nproducing high-fidelity virtual news anchors. A pair of Temporal Convolutional\nNetworks are used to learn the cross-modal sequential mapping from audio\nsignals to mouth movements, followed by a neural rendering network that\ntranslates the synthetic facial map into a high-resolution and photorealistic\nappearance. This fully trainable framework provides end-to-end processing that\noutperforms traditional graphics-based methods in many low-delay applications.\nExperiments also show the framework has advantages over modern neural-based\nmethods in both visual appearance and efficiency.", "published": "2020-02-20 12:26:20", "link": "http://arxiv.org/abs/2002.08700v2", "categories": ["cs.CV", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Convergence-guaranteed Independent Positive Semidefinite Tensor Analysis\n  Based on Student's t Distribution", "abstract": "In this paper, we address a blind source separation (BSS) problem and propose\na new extended framework of independent positive semidefinite tensor analysis\n(IPSDTA). IPSDTA is a state-of-the-art BSS method that enables us to take\ninterfrequency correlations into account, but the generative model is limited\nwithin the multivariate Gaussian distribution and its parameter optimization\nalgorithm does not guarantee stable convergence. To resolve these problems,\nfirst, we propose to extend the generative model to a parametric multivariate\nStudent's t distribution that can deal with various types of signal. Secondly,\nwe derive a new parameter optimization algorithm that guarantees the monotonic\nnonincrease in the cost function, providing stable convergence. Experimental\nresults reveal that the cost function in the conventional IPSDTA does not\ndisplay monotonically nonincreasing properties. On the other hand, the proposed\nmethod guarantees the monotonic nonincrease in the cost function and\noutperforms the conventional ILRMA and IPSDTA in the source-separation\nperformance.", "published": "2020-02-20 06:24:02", "link": "http://arxiv.org/abs/2002.08582v1", "categories": ["cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "An empirical study of Conv-TasNet", "abstract": "Conv-TasNet is a recently proposed waveform-based deep neural network that\nachieves state-of-the-art performance in speech source separation. Its\narchitecture consists of a learnable encoder/decoder and a separator that\noperates on top of this learned space. Various improvements have been proposed\nto Conv-TasNet. However, they mostly focus on the separator, leaving its\nencoder/decoder as a (shallow) linear operator. In this paper, we conduct an\nempirical study of Conv-TasNet and propose an enhancement to the\nencoder/decoder that is based on a (deep) non-linear variant of it. In\naddition, we experiment with the larger and more diverse LibriTTS dataset and\ninvestigate the generalization capabilities of the studied models when trained\non a much larger dataset. We propose cross-dataset evaluation that includes\nassessing separations from the WSJ0-2mix, LibriTTS and VCTK databases. Our\nresults show that enhancements to the encoder/decoder can improve average\nSI-SNR performance by more than 1 dB. Furthermore, we offer insights into the\ngeneralization capabilities of Conv-TasNet and the potential value of\nimprovements to the encoder/decoder.", "published": "2020-02-20 11:51:43", "link": "http://arxiv.org/abs/2002.08688v2", "categories": ["eess.AS", "cs.CV", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Disentangled Speech Embeddings using Cross-modal Self-supervision", "abstract": "The objective of this paper is to learn representations of speaker identity\nwithout access to manually annotated data. To do so, we develop a\nself-supervised learning objective that exploits the natural cross-modal\nsynchrony between faces and audio in video. The key idea behind our approach is\nto tease apart--without annotation--the representations of linguistic content\nand speaker identity. We construct a two-stream architecture which: (1) shares\nlow-level features common to both representations; and (2) provides a natural\nmechanism for explicitly disentangling these factors, offering the potential\nfor greater generalisation to novel combinations of content and identity and\nultimately producing speaker identity representations that are more robust. We\ntrain our method on a large-scale audio-visual dataset of talking heads `in the\nwild', and demonstrate its efficacy by evaluating the learned speaker\nrepresentations for standard speaker recognition performance.", "published": "2020-02-20 14:13:12", "link": "http://arxiv.org/abs/2002.08742v2", "categories": ["eess.AS", "cs.CV", "cs.SD"], "primary_category": "eess.AS"}
{"title": "iSEGAN: Improved Speech Enhancement Generative Adversarial Networks", "abstract": "Popular neural network-based speech enhancement systems operate on the\nmagnitude spectrogram and ignore the phase mismatch between the noisy and clean\nspeech signals. Conditional generative adversarial networks (cGANs) show\npromise in addressing the phase mismatch problem by directly mapping the raw\nnoisy speech waveform to the underlying clean speech signal. However,\nstabilizing and training cGAN systems is difficult and they still fall short of\nthe performance achieved by the spectral enhancement approaches. This paper\ninvestigates whether different normalization strategies and one-sided label\nsmoothing can further stabilize the cGAN-based speech enhancement model. In\naddition, we propose incorporating a Gammatone-based auditory filtering layer\nand a trainable pre-emphasis layer to further improve the performance of the\ncGAN framework. Simulation results show that the proposed approaches improve\nthe speech enhancement performance of cGAN systems in addition to yielding\nimproved stability and reduced computational effort.", "published": "2020-02-20 15:19:17", "link": "http://arxiv.org/abs/2002.08796v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "A Comparative Study of Western and Chinese Classical Music based on\n  Soundscape Models", "abstract": "Whether literally or suggestively, the concept of soundscape is alluded in\nboth modern and ancient music. In this study, we examine whether we can analyze\nand compare Western and Chinese classical music based on soundscape models. We\naddressed this question through a comparative study. Specifically, corpora of\nWestern classical music excerpts (WCMED) and Chinese classical music excerpts\n(CCMED) were curated and annotated with emotional valence and arousal through a\ncrowdsourcing experiment. We used a sound event detection (SED) and soundscape\nemotion recognition (SER) models with transfer learning to predict the\nperceived emotion of WCMED and CCMED. The results show that both SER and SED\nmodels could be used to analyze Chinese and Western classical music. The fact\nthat SER and SED work better on Chinese classical music emotion recognition\nprovides evidence that certain similarities exist between Chinese classical\nmusic and soundscape recordings, which permits transferability between machine\nlearning models.", "published": "2020-02-20 21:16:27", "link": "http://arxiv.org/abs/2002.09021v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multi-label Sound Event Retrieval Using a Deep Learning-based Siamese\n  Structure with a Pairwise Presence Matrix", "abstract": "Realistic recordings of soundscapes often have multiple sound events\nco-occurring, such as car horns, engine and human voices. Sound event retrieval\nis a type of content-based search aiming at finding audio samples, similar to\nan audio query based on their acoustic or semantic content. State of the art\nsound event retrieval models have focused on single-label audio recordings,\nwith only one sound event occurring, rather than on multi-label audio\nrecordings (i.e., multiple sound events occur in one recording). To address\nthis latter problem, we propose different Deep Learning architectures with a\nSiamese-structure and a Pairwise Presence Matrix. The networks are trained and\nevaluated using the SONYC-UST dataset containing both single- and multi-label\nsoundscape recordings. The performance results show the effectiveness of our\nproposed model.", "published": "2020-02-20 21:33:07", "link": "http://arxiv.org/abs/2002.09026v1", "categories": ["eess.AS", "cs.IR", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Efficient Trainable Front-Ends for Neural Speech Enhancement", "abstract": "Many neural speech enhancement and source separation systems operate in the\ntime-frequency domain. Such models often benefit from making their Short-Time\nFourier Transform (STFT) front-ends trainable. In current literature, these are\nimplemented as large Discrete Fourier Transform matrices; which are\nprohibitively inefficient for low-compute systems. We present an efficient,\ntrainable front-end based on the butterfly mechanism to compute the Fast\nFourier Transform, and show its accuracy and efficiency benefits for\nlow-compute neural speech enhancement models. We also explore the effects of\nmaking the STFT window trainable.", "published": "2020-02-20 01:51:15", "link": "http://arxiv.org/abs/2002.09286v1", "categories": ["eess.AS", "cs.LG", "cs.NE", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
