{"title": "Smart leverage? Rethinking the role of Leveraged Exchange Traded Funds in constructing portfolios to beat a benchmark", "abstract": "Leveraged Exchange Traded Funds (LETFs), while extremely controversial in the\nliterature, remain stubbornly popular with both institutional and retail\ninvestors in practice. While the criticisms of LETFs are certainly valid, we\nargue that their potential has been underestimated in the literature due to the\nuse of very simple investment strategies involving LETFs. In this paper, we\nsystematically investigate the potential of including a broad stock market\nindex LETF in long-term, dynamically-optimal investment strategies designed to\nmaximize the outperformance over standard investment benchmarks in the sense of\nthe information ratio (IR). Our results exploit the observation that positions\nin a LETF deliver call-like payoffs, so that the addition of a LETF to a\nportfolio can be a convenient way to add inexpensive leverage while providing\ndownside protection. Under stylized assumptions, we present and analyze\nclosed-form IR-optimal investment strategies using either a LETF or\nstandard/vanilla ETF (VETF) on the same equity index, which provides the\nnecessary intuition for the potential and benefits of LETFs. In more realistic\nsettings, we use a neural network-based approach to determine the IR-optimal\nstrategies, trained on bootstrapped historical data. We find that IR-optimal\nstrategies with a broad stock market LETF are not only more likely to\noutperform the benchmark than IR-optimal strategies derived using the\ncorresponding VETF, but are able to achieve partial stochastic dominance over\nthe benchmark and VETF-based strategies in terms of terminal wealth.", "published": "2024-12-06 21:33:58", "link": "http://arxiv.org/abs/2412.05431v2", "categories": ["q-fin.CP", "q-fin.PM", "93E20, 91G, 68T07, 65N06, 35Q93"], "primary_category": "q-fin.CP"}
{"title": "Enhancing Fourier pricing with machine learning", "abstract": "Fourier pricing methods such as the Carr-Madan formula or the COS method are\nclassic tools for pricing European options for advanced models such as the\nHeston model. These methods require tuning parameters such as a damping factor,\na truncation range, a number of terms, etc. Estimating these tuning parameters\nis difficult or computationally expensive. Recently, machine learning\ntechniques have been proposed for fast pricing: they are able to learn the\nfunctional relationship between the parameters of the Heston model and the\noption price. However, machine learning techniques suffer from error control\nand require retraining for different error tolerances. In this research, we\npropose to learn the tuning parameters of the Fourier methods (instead of the\nprices) using machine learning techniques. As a result, we obtain very fast\nalgorithms with full error control: Our approach works with any error tolerance\nwithout retraining, as demonstrated in numerical experiments using the Heston\nmodel.", "published": "2024-12-06 14:27:59", "link": "http://arxiv.org/abs/2412.05070v1", "categories": ["q-fin.MF", "65T40, 91G20, 91B24, 68T05"], "primary_category": "q-fin.MF"}
{"title": "Foundation Models for Low-Resource Language Education (Vision Paper)", "abstract": "Recent studies show that large language models (LLMs) are powerful tools for\nworking with natural language, bringing advances in many areas of computational\nlinguistics. However, these models face challenges when applied to low-resource\nlanguages due to limited training data and difficulty in understanding cultural\nnuances. Research is now focusing on multilingual models to improve LLM\nperformance for these languages. Education in these languages also struggles\nwith a lack of resources and qualified teachers, particularly in underdeveloped\nregions. Here, LLMs can be transformative, supporting innovative methods like\ncommunity-driven learning and digital platforms. This paper discusses how LLMs\ncould enhance education for low-resource languages, emphasizing practical\napplications and benefits.", "published": "2024-12-06 04:34:45", "link": "http://arxiv.org/abs/2412.04774v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EXAONE 3.5: Series of Large Language Models for Real-world Use Cases", "abstract": "This technical report introduces the EXAONE 3.5 instruction-tuned language\nmodels, developed and released by LG AI Research. The EXAONE 3.5 language\nmodels are offered in three configurations: 32B, 7.8B, and 2.4B. These models\nfeature several standout capabilities: 1) exceptional instruction following\ncapabilities in real-world scenarios, achieving the highest scores across seven\nbenchmarks, 2) outstanding long-context comprehension, attaining the top\nperformance in four benchmarks, and 3) competitive results compared to\nstate-of-the-art open models of similar sizes across nine general benchmarks.\nThe EXAONE 3.5 language models are open to anyone for research purposes and can\nbe downloaded from https://huggingface.co/LGAI-EXAONE. For commercial use,\nplease reach out to the official contact point of LG AI Research:\ncontact_us@lgresearch.ai.", "published": "2024-12-06 08:53:46", "link": "http://arxiv.org/abs/2412.04862v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Building a Family of Data Augmentation Models for Low-cost LLM\n  Fine-tuning on the Cloud", "abstract": "Specializing LLMs in various domain-specific tasks has emerged as a critical\nstep towards achieving high performance. However, the construction and\nannotation of datasets in specific domains are always very costly. Apart from\nusing superior and expensive closed-source LLM APIs to construct datasets, some\nopen-source models have become strong enough to handle dataset construction in\nmany scenarios. Thus, we present a family of data augmentation models designed\nto significantly improve the efficiency for model fine-tuning. These models,\ntrained based on sufficiently small LLMs, support key functionalities with low\ninference costs: instruction expansion, instruction refinement, and\ninstruction-response pair expansion. To fulfill this goal, we first construct\nan automatic data collection system with seed datasets generated from both\npublic repositories and our in-house datasets. This system leverages powerful\nLLMs to expand, refine and re-write the instructions and responses,\nincorporating quality assessment techniques. Following this, we introduce the\ntraining process of our models, which effectively distills task-solving and\ntext synthesis abilities from teacher LLMs. Finally, we demonstrate how we\nintegrate these functionalities into a machine learning platform to support\nlow-cost LLM fine-tuning from both dataset preparation and training\nperspectives for users. Experiments and an application study prove the\neffectiveness of our approach.", "published": "2024-12-06 09:04:12", "link": "http://arxiv.org/abs/2412.04871v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models for Ingredient Substitution in Food Recipes using\n  Supervised Fine-tuning and Direct Preference Optimization", "abstract": "In this paper, we address the challenge of recipe personalization through\ningredient substitution. We make use of Large Language Models (LLMs) to build\nan ingredient substitution system designed to predict plausible substitute\ningredients within a given recipe context. Given that the use of LLMs for this\ntask has been barely done, we carry out an extensive set of experiments to\ndetermine the best LLM, prompt, and the fine-tuning setups. We further\nexperiment with methods such as multi-task learning, two-stage fine-tuning, and\nDirect Preference Optimization (DPO). The experiments are conducted using the\npublicly available Recipe1MSub corpus. The best results are produced by the\nMistral7-Base LLM after fine-tuning and DPO. This result outperforms the strong\nbaseline available for the same corpus with a Hit@1 score of 22.04. Thus we\nbelieve that this research represents a significant step towards enabling\npersonalized and creative culinary experiences by utilizing LLM-based\ningredient substitution.", "published": "2024-12-06 10:21:25", "link": "http://arxiv.org/abs/2412.04922v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "C$^2$LEVA: Toward Comprehensive and Contamination-Free Language Model\n  Evaluation", "abstract": "Recent advances in large language models (LLMs) have shown significant\npromise, yet their evaluation raises concerns, particularly regarding data\ncontamination due to the lack of access to proprietary training data. To\naddress this issue, we present C$^2$LEVA, a comprehensive bilingual benchmark\nfeaturing systematic contamination prevention. C$^2$LEVA firstly offers a\nholistic evaluation encompassing 22 tasks, each targeting a specific\napplication or ability of LLMs, and secondly a trustworthy assessment due to\nour contamination-free tasks, ensured by a systematic contamination prevention\nstrategy that fully automates test data renewal and enforces data protection\nduring benchmark data release. Our large-scale evaluation of 15 open-source and\nproprietary models demonstrates the effectiveness of C$^2$LEVA.", "published": "2024-12-06 11:07:44", "link": "http://arxiv.org/abs/2412.04947v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PETapter: Leveraging PET-style classification heads for modular few-shot\n  parameter-efficient fine-tuning", "abstract": "Few-shot learning and parameter-efficient fine-tuning (PEFT) are crucial to\novercome the challenges of data scarcity and ever growing language model sizes.\nThis applies in particular to specialized scientific domains, where researchers\nmight lack expertise and resources to fine-tune high-performing language models\nto nuanced tasks. We propose PETapter, a novel method that effectively combines\nPEFT methods with PET-style classification heads to boost few-shot learning\ncapabilities without the significant computational overhead typically\nassociated with full model training. We validate our approach on three\nestablished NLP benchmark datasets and one real-world dataset from\ncommunication research. We show that PETapter not only achieves comparable\nperformance to full few-shot fine-tuning using pattern-exploiting training\n(PET), but also provides greater reliability and higher parameter efficiency\nwhile enabling higher modularity and easy sharing of the trained modules, which\nenables more researchers to utilize high-performing NLP-methods in their\nresearch.", "published": "2024-12-06 11:49:18", "link": "http://arxiv.org/abs/2412.04975v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Steps are all you need: Rethinking STEM Education with Prompt\n  Engineering", "abstract": "Few shot and Chain-of-Thought prompting have shown promise when applied to\nPhysics Question Answering Tasks, but are limited by the lack of mathematical\nability inherent to LLMs, and are prone to hallucination. By utilizing a\nMixture of Experts (MoE) Model, along with analogical prompting, we are able to\nshow improved model performance when compared to the baseline on standard LLMs.\nWe also survey the limits of these prompting techniques and the effects they\nhave on model performance. Additionally, we propose Analogical CoT prompting, a\nprompting technique designed to allow smaller, open source models to leverage\nAnalogical prompting, something they have struggled with, possibly due to a\nlack of specialist training data.", "published": "2024-12-06 13:20:57", "link": "http://arxiv.org/abs/2412.05023v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unifying Dual-Space Embedding for Entity Alignment via Contrastive\n  Learning", "abstract": "Entity alignment aims to match identical entities across different knowledge\ngraphs (KGs). Graph neural network-based entity alignment methods have achieved\npromising results in Euclidean space. However, KGs often contain complex\nstructures, including both local and hierarchical ones, which make it\nchallenging to efficiently represent them within a single space. In this paper,\nwe proposed a novel method UniEA, which unifies dual-space embedding to\npreserve the intrinsic structure of KGs. Specifically, we learn graph structure\nembedding in both Euclidean and hyperbolic spaces simultaneously to maximize\nthe consistency between the embedding in both spaces. Moreover, we employ\ncontrastive learning to mitigate the misalignment issues caused by similar\nentities, where embedding of similar neighboring entities within the KG become\ntoo close in distance. Extensive experiments on benchmark datasets demonstrate\nthat our method achieves state-of-the-art performance in structure-based EA.\nOur code is available at https://github.com/wonderCS1213/UniEA.", "published": "2024-12-06 13:25:09", "link": "http://arxiv.org/abs/2412.05028v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Findings of the Second BabyLM Challenge: Sample-Efficient Pretraining on\n  Developmentally Plausible Corpora", "abstract": "The BabyLM Challenge is a community effort to close the data-efficiency gap\nbetween human and computational language learners. Participants compete to\noptimize language model training on a fixed language data budget of 100 million\nwords or less. This year, we released improved text corpora, as well as a\nvision-and-language corpus to facilitate research into cognitively plausible\nvision language models. Submissions were compared on evaluation tasks targeting\ngrammatical ability, (visual) question answering, pragmatic abilities, and\ngrounding, among other abilities. Participants could submit to a 10M-word\ntext-only track, a 100M-word text-only track, and/or a 100M-word and image\nmultimodal track. From 31 submissions employing diverse methods, a hybrid\ncausal-masked language model architecture outperformed other approaches. No\nsubmissions outperformed the baselines in the multimodal track. In follow-up\nanalyses, we found a strong relationship between training FLOPs and average\nperformance across tasks, and that the best-performing submissions proposed\nchanges to the training data, training objective, and model architecture. This\nyear's BabyLM Challenge shows that there is still significant room for\ninnovation in this setting, in particular for image-text modeling, but\ncommunity-driven research can yield actionable insights about effective\nstrategies for small-scale language modeling.", "published": "2024-12-06 16:06:08", "link": "http://arxiv.org/abs/2412.05149v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multimodal Fact-Checking with Vision Language Models: A Probing\n  Classifier based Solution with Embedding Strategies", "abstract": "This study evaluates the effectiveness of Vision Language Models (VLMs) in\nrepresenting and utilizing multimodal content for fact-checking. To be more\nspecific, we investigate whether incorporating multimodal content improves\nperformance compared to text-only models and how well VLMs utilize text and\nimage information to enhance misinformation detection. Furthermore we propose a\nprobing classifier based solution using VLMs. Our approach extracts embeddings\nfrom the last hidden layer of selected VLMs and inputs them into a neural\nprobing classifier for multi-class veracity classification. Through a series of\nexperiments on two fact-checking datasets, we demonstrate that while\nmultimodality can enhance performance, fusing separate embeddings from text and\nimage encoders yielded superior results compared to using VLM embeddings.\nFurthermore, the proposed neural classifier significantly outperformed KNN and\nSVM baselines in leveraging extracted embeddings, highlighting its\neffectiveness for multimodal fact-checking.", "published": "2024-12-06 16:13:19", "link": "http://arxiv.org/abs/2412.05155v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating and Aligning CodeLLMs on Human Preference", "abstract": "Code large language models (codeLLMs) have made significant strides in code\ngeneration. Most previous code-related benchmarks, which consist of various\nprogramming exercises along with the corresponding test cases, are used as a\ncommon measure to evaluate the performance and capabilities of code LLMs.\nHowever, the current code LLMs focus on synthesizing the correct code snippet,\nignoring the alignment with human preferences, where the query should be\nsampled from the practical application scenarios and the model-generated\nresponses should satisfy the human preference. To bridge the gap between the\nmodel-generated response and human preference, we present a rigorous\nhuman-curated benchmark CodeArena to emulate the complexity and diversity of\nreal-world coding tasks, where 397 high-quality samples spanning 40 categories\nand 44 programming languages, carefully curated from user queries. Further, we\npropose a diverse synthetic instruction corpus SynCode-Instruct (nearly 20B\ntokens) by scaling instructions from the website to verify the effectiveness of\nthe large-scale synthetic instruction fine-tuning, where Qwen2.5-SynCoder\ntotally trained on synthetic instruction data can achieve top-tier performance\nof open-source code LLMs. The results find performance differences between\nexecution-based benchmarks and CodeArena. Our systematic experiments of\nCodeArena on 40+ LLMs reveal a notable performance gap between open SOTA code\nLLMs (e.g. Qwen2.5-Coder) and proprietary LLMs (e.g., OpenAI o1), underscoring\nthe importance of the human preference\nalignment.\\footnote{\\url{https://codearenaeval.github.io/ }}", "published": "2024-12-06 17:40:38", "link": "http://arxiv.org/abs/2412.05210v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "100% Elimination of Hallucinations on RAGTruth for GPT-4 and GPT-3.5\n  Turbo", "abstract": "The issue of hallucinations in large language models (LLMs) remains a\ncritical barrier to the adoption of AI in enterprise and other high-stakes\napplications. Despite advancements in retrieval-augmented generation (RAG)\nsystems, current state-of-the-art methods fail to achieve more than 80%\naccuracy in generating faithful and factually correct outputs, even when\nprovided with relevant and accurate context. In this work, we introduce Acurai,\na novel systematic approach that achieves 100% hallucination-free responses in\nLLMs by reformatting queries and context data prior to input. Leveraging a deep\nunderstanding of LLM internal representations, the importance of noun-phrase\ndominance, and the role of discrete functional units (DFUs), Acurai ensures\nalignment between input context and generated output. We validate this method\nusing the RAGTruth corpus, demonstrating its ability to eliminate 100%\nhallucinations for both GPT-4 and GPT-3.5 Turbo. Acurai sets a new standard for\nachieving consistent, accurate, and faithful AI responses, marking a\nsignificant step forward in the development of trustworthy AI systems.", "published": "2024-12-06 17:54:54", "link": "http://arxiv.org/abs/2412.05223v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LIAR: Leveraging Inference Time Alignment (Best-of-N) to Jailbreak LLMs\n  in Seconds", "abstract": "Traditional jailbreaks have successfully exposed vulnerabilities in LLMs,\nprimarily relying on discrete combinatorial optimization, while more recent\nmethods focus on training LLMs to generate adversarial prompts. However, both\napproaches are computationally expensive and slow, often requiring significant\nresources to generate a single successful attack. We hypothesize that the\ninefficiency of these methods arises from an inadequate characterization of the\njailbreak problem itself. To address this gap, we approach the jailbreak\nproblem as an alignment problem, leading us to propose LIAR (Leveraging\nInference time Alignment to jailbReak), a fast and efficient best-of-N approach\ntailored for jailbreak attacks. LIAR offers several key advantages: it\neliminates the need for additional training, operates in a fully black-box\nsetting, significantly reduces computational overhead, and produces more\nhuman-readable adversarial prompts while maintaining competitive attack success\nrates. Our results demonstrate that a best-of-N approach is a simple yet highly\neffective strategy for evaluating the robustness of aligned LLMs, achieving\nattack success rates (ASR) comparable to state-of-the-art methods while\noffering a 10x improvement in perplexity and a significant speedup in\nTime-to-Attack, reducing execution time from tens of hours to seconds.\nAdditionally, We also provide sub-optimality guarantees for the proposed LIAR.\nOur work highlights the potential of efficient, alignment-based jailbreak\nstrategies for assessing and stress-testing AI safety measures.", "published": "2024-12-06 18:02:59", "link": "http://arxiv.org/abs/2412.05232v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Incremental Sentence Processing Mechanisms in Autoregressive Transformer\n  Language Models", "abstract": "Autoregressive transformer language models (LMs) possess strong syntactic\nabilities, often successfully handling phenomena from agreement to NPI\nlicensing. However, the features they use to incrementally process language\ninputs are not well understood. In this paper, we fill this gap by studying the\nmechanisms underlying garden path sentence processing in LMs. We ask: (1) Do\nLMs use syntactic features or shallow heuristics to perform incremental\nsentence processing? (2) Do LMs represent only one potential interpretation, or\nmultiple? and (3) Do LMs reanalyze or repair their initial incorrect\nrepresentations? To address these questions, we use sparse autoencoders to\nidentify interpretable features that determine which continuation - and thus\nwhich reading - of a garden path sentence the LM prefers. We find that while\nmany important features relate to syntactic structure, some reflect\nsyntactically irrelevant heuristics. Moreover, while most active features\ncorrespond to one reading of the sentence, some features correspond to the\nother, suggesting that LMs assign weight to both possibilities simultaneously.\nFinally, LMs do not re-use features from garden path sentence processing to\nanswer follow-up questions.", "published": "2024-12-06 18:54:54", "link": "http://arxiv.org/abs/2412.05353v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge Graphs are all you need: Leveraging KGs in Physics Question\n  Answering", "abstract": "This study explores the effectiveness of using knowledge graphs generated by\nlarge language models to decompose high school-level physics questions into\nsub-questions. We introduce a pipeline aimed at enhancing model response\nquality for Question Answering tasks. By employing LLMs to construct knowledge\ngraphs that capture the internal logic of the questions, these graphs then\nguide the generation of subquestions. We hypothesize that this method yields\nsub-questions that are more logically consistent with the original questions\ncompared to traditional decomposition techniques. Our results show that\nsub-questions derived from knowledge graphs exhibit significantly improved\nfidelity to the original question's logic. This approach not only enhances the\nlearning experience by providing clearer and more contextually appropriate\nsub-questions but also highlights the potential of LLMs to transform\neducational methodologies. The findings indicate a promising direction for\napplying AI to improve the quality and effectiveness of educational content.", "published": "2024-12-06 22:25:23", "link": "http://arxiv.org/abs/2412.05453v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLM-Align: Utilizing Large Language Models for Entity Alignment in\n  Knowledge Graphs", "abstract": "Entity Alignment (EA) seeks to identify and match corresponding entities\nacross different Knowledge Graphs (KGs), playing a crucial role in knowledge\nfusion and integration. Embedding-based entity alignment (EA) has recently\ngained considerable attention, resulting in the emergence of many innovative\napproaches. Initially, these approaches concentrated on learning entity\nembeddings based on the structural features of knowledge graphs (KGs) as\ndefined by relation triples. Subsequent methods have integrated entities' names\nand attributes as supplementary information to improve the embeddings used for\nEA. However, existing methods lack a deep semantic understanding of entity\nattributes and relations. In this paper, we propose a Large Language Model\n(LLM) based Entity Alignment method, LLM-Align, which explores the\ninstruction-following and zero-shot capabilities of Large Language Models to\ninfer alignments of entities. LLM-Align uses heuristic methods to select\nimportant attributes and relations of entities, and then feeds the selected\ntriples of entities to an LLM to infer the alignment results. To guarantee the\nquality of alignment results, we design a multi-round voting mechanism to\nmitigate the hallucination and positional bias issues that occur with LLMs.\nExperiments on three EA datasets, demonstrating that our approach achieves\nstate-of-the-art performance compared to existing EA methods.", "published": "2024-12-06 01:05:37", "link": "http://arxiv.org/abs/2412.04690v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "NoLoR: An ASR-Based Framework for Expedited Endangered Language\n  Documentation with Neo-Aramaic as a Case Study", "abstract": "The documentation of the Neo-Aramaic dialects before their extinction has\nbeen described as the most urgent task in all of Semitology today. The death of\nthis language will be an unfathomable loss to the descendents of the indigenous\nspeakers of Aramaic, now predominantly diasporic after forced displacement due\nto violence. This paper develops an ASR model to expedite the documentation of\nthis endangered language and generalizes the strategy in a new framework we\ncall NoLoR.", "published": "2024-12-06 02:15:53", "link": "http://arxiv.org/abs/2412.04717v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "BESSTIE: A Benchmark for Sentiment and Sarcasm Classification for\n  Varieties of English", "abstract": "Despite large language models (LLMs) being known to exhibit bias against\nnon-mainstream varieties, there are no known labeled datasets for sentiment\nanalysis of English. To address this gap, we introduce BESSTIE, a benchmark for\nsentiment and sarcasm classification for three varieties of English: Australian\n(en-AU), Indian (en-IN), and British (en-UK). Using web-based content from two\ndomains, namely, Google Place reviews and Reddit comments, we collect datasets\nfor these language varieties using two methods: location-based and topic-based\nfiltering. Native speakers of the language varieties manually annotate the\ndatasets with sentiment and sarcasm labels. To assess whether the dataset\naccurately represents these varieties, we conduct two validation steps: (a)\nmanual annotation of language varieties and (b) automatic language variety\nprediction. Subsequently, we fine-tune nine large language models (LLMs)\n(representing a range of encoder/decoder and mono/multilingual models) on these\ndatasets, and evaluate their performance on the two tasks. Our results reveal\nthat the models consistently perform better on inner-circle varieties (i.e.,\nen-AU and en-UK), with significant performance drops for en-IN, particularly in\nsarcasm detection. We also report challenges in cross-variety generalisation,\nhighlighting the need for language variety-specific datasets such as ours.\nBESSTIE promises to be a useful evaluative benchmark for future research in\nequitable LLMs, specifically in terms of language varieties. The BESSTIE\ndatasets, code, and models will be publicly available upon acceptance.", "published": "2024-12-06 02:34:40", "link": "http://arxiv.org/abs/2412.04726v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ChatNVD: Advancing Cybersecurity Vulnerability Assessment with Large\n  Language Models", "abstract": "The increasing frequency and sophistication of cybersecurity vulnerabilities\nin software systems underscore the urgent need for robust and effective methods\nof vulnerability assessment. However, existing approaches often rely on highly\ntechnical and abstract frameworks, which hinders understanding and increases\nthe likelihood of exploitation, resulting in severe cyberattacks. Given the\ngrowing adoption of Large Language Models (LLMs) across diverse domains, this\npaper explores their potential application in cybersecurity, specifically for\nenhancing the assessment of software vulnerabilities. We propose ChatNVD, an\nLLM-based cybersecurity vulnerability assessment tool leveraging the National\nVulnerability Database (NVD) to provide context-rich insights and streamline\nvulnerability analysis for cybersecurity professionals, developers, and\nnon-technical users. We develop three variants of ChatNVD, utilizing three\nprominent LLMs: GPT-4o mini by OpenAI, Llama 3 by Meta, and Gemini 1.5 Pro by\nGoogle. To evaluate their efficacy, we conduct a comparative analysis of these\nmodels using a comprehensive questionnaire comprising common security\nvulnerability questions, assessing their accuracy in identifying and analyzing\nsoftware vulnerabilities. This study provides valuable insights into the\npotential of LLMs to address critical challenges in understanding and\nmitigation of software vulnerabilities.", "published": "2024-12-06 03:45:49", "link": "http://arxiv.org/abs/2412.04756v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Ltri-LLM: Streaming Long Context Inference for LLMs with Training-Free\n  Dynamic Triangular Attention Pattern", "abstract": "The quadratic computational complexity of the attention mechanism in current\nLarge Language Models (LLMs) renders inference with long contexts prohibitively\nexpensive. To address this challenge, various approaches aim to retain critical\nportions of the context to optimally approximate Full Attention (FA) through\nKey-Value (KV) compression or Sparse Attention (SA), enabling the processing of\nvirtually unlimited text lengths in a streaming manner. However, these methods\nstruggle to achieve performance levels comparable to FA, particularly in\nretrieval tasks. In this paper, our analysis of attention head patterns reveals\nthat LLMs' attention distributions show strong local correlations, naturally\nreflecting a chunking mechanism for input context. We propose Ltri-LLM\nframework, which divides KVs into spans, stores them in an offline index, and\nretrieves the relevant KVs into memory for various queries. Experimental\nresults on popular long text benchmarks show that Ltri-LLM can achieve\nperformance close to FA while maintaining efficient, streaming-based inference.", "published": "2024-12-06 03:46:06", "link": "http://arxiv.org/abs/2412.04757v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "NLP-ADBench: NLP Anomaly Detection Benchmark", "abstract": "Anomaly detection (AD) is a critical machine learning task with diverse\napplications in web systems, including fraud detection, content moderation, and\nuser behavior analysis. Despite its significance, AD in natural language\nprocessing (NLP) remains underexplored, limiting advancements in detecting\nanomalies in text data such as harmful content, phishing attempts, or spam\nreviews. In this paper, we introduce NLP-ADBench, the most comprehensive\nbenchmark for NLP anomaly detection (NLP-AD), comprising eight curated datasets\nand evaluations of nineteen state-of-the-art algorithms. These include three\nend-to-end methods and sixteen two-step algorithms that apply traditional\nanomaly detection techniques to language embeddings generated by\nbert-base-uncased and OpenAI's text-embedding-3-large models.\n  Our results reveal critical insights and future directions for NLP-AD.\nNotably, no single model excels across all datasets, highlighting the need for\nautomated model selection. Moreover, two-step methods leveraging\ntransformer-based embeddings consistently outperform specialized end-to-end\napproaches, with OpenAI embeddings demonstrating superior performance over BERT\nembeddings. By releasing NLP-ADBench at\nhttps://github.com/USC-FORTIS/NLP-ADBench, we provide a standardized framework\nfor evaluating NLP-AD methods, fostering the development of innovative\napproaches. This work fills a crucial gap in the field and establishes a\nfoundation for advancing NLP anomaly detection, particularly in the context of\nimproving the safety and reliability of web-based systems.", "published": "2024-12-06 05:30:41", "link": "http://arxiv.org/abs/2412.04784v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Direct Quantized Training of Language Models with Stochastic Rounding", "abstract": "Although recent quantized Large Language Models (LLMs), such as BitNet, have\npaved the way for significant reduction in memory usage during deployment with\nbinary or ternary weights, training these models still demands substantial\nmemory footprints. This is partly because high-precision (i.e., unquantized)\nweight matrices required for straight-through estimation must be maintained\nthroughout the whole training process. To address this, we explore the\npotential of directly updating the quantized low-precision weight matrices\nwithout relying on the straight-through estimator during backpropagation,\nthereby saving memory usage during training. Specifically, we employ a\nstochastic rounding technique to minimize information loss caused by the use of\nlow-bit weights throughout training. Experimental results on our\nLLaMA-structured models indicate that (1) training with only low-precision\nweights is feasible even when they are constrained to ternary values, (2)\nextending the bit width to 8 bits results in only a 5% loss degradation\ncompared to BitNet b1.58 while offering the potential for reduced memory usage\nduring training, and (3) our models can also perform inference using ternary\nweights, showcasing their flexibility in deployment.", "published": "2024-12-06 05:41:11", "link": "http://arxiv.org/abs/2412.04787v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Adaptive Dropout for Pruning Conformers", "abstract": "This paper proposes a method to effectively perform joint\ntraining-and-pruning based on adaptive dropout layers with unit-wise retention\nprobabilities. The proposed method is based on the estimation of a unit-wise\nretention probability in a dropout layer. A unit that is estimated to have a\nsmall retention probability can be considered to be prunable. The retention\nprobability of the unit is estimated using back-propagation and the\nGumbel-Softmax technique. This pruning method is applied at several application\npoints in Conformers such that the effective number of parameters can be\nsignificantly reduced. Specifically, adaptive dropout layers are introduced in\nthree locations in each Conformer block: (a) the hidden layer of the\nfeed-forward-net component, (b) the query vectors and the value vectors of the\nself-attention component, and (c) the input vectors of the LConv component. The\nproposed method is evaluated by conducting a speech recognition experiment on\nthe LibriSpeech task. It was shown that this approach could simultaneously\nachieve a parameter reduction and accuracy improvement. The word error rates\nimproved by approx 1% while reducing the number of parameters by 54%.", "published": "2024-12-06 08:05:02", "link": "http://arxiv.org/abs/2412.04836v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Breaking Event Rumor Detection via Stance-Separated Multi-Agent Debate", "abstract": "The rapid spread of rumors on social media platforms during breaking events\nseverely hinders the dissemination of the truth. Previous studies reveal that\nthe lack of annotated resources hinders the direct detection of unforeseen\nbreaking events not covered in yesterday's news. Leveraging large language\nmodels (LLMs) for rumor detection holds significant promise. However, it is\nchallenging for LLMs to provide comprehensive responses to complex or\ncontroversial issues due to limited diversity. In this work, we propose the\nStance Separated Multi-Agent Debate (S2MAD) to address this issue.\nSpecifically, we firstly introduce Stance Separation, categorizing comments as\neither supporting or opposing the original claim. Subsequently, claims are\nclassified as subjective or objective, enabling agents to generate reasonable\ninitial viewpoints with different prompt strategies for each type of claim.\nDebaters then follow specific instructions through multiple rounds of debate to\nreach a consensus. If a consensus is not reached, a judge agent evaluates the\nopinions and delivers a final verdict on the claim's veracity. Extensive\nexperiments conducted on two real-world datasets demonstrate that our proposed\nmodel outperforms state-of-the-art methods in terms of performance and\neffectively improves the performance of LLMs in breaking event rumor detection.", "published": "2024-12-06 08:52:30", "link": "http://arxiv.org/abs/2412.04859v1", "categories": ["cs.CL", "cs.MA"], "primary_category": "cs.CL"}
{"title": "Who Speaks Next? Multi-party AI Discussion Leveraging the Systematics of\n  Turn-taking in Murder Mystery Games", "abstract": "Multi-agent systems utilizing large language models (LLMs) have shown great\npromise in achieving natural dialogue. However, smooth dialogue control and\nautonomous decision making among agents still remain challenges. In this study,\nwe focus on conversational norms such as adjacency pairs and turn-taking found\nin conversation analysis and propose a new framework called \"Murder Mystery\nAgents\" that applies these norms to AI agents' dialogue control. As an\nevaluation target, we employed the \"Murder Mystery\" game, a reasoning-type\ntable-top role-playing game that requires complex social reasoning and\ninformation manipulation. In this game, players need to unravel the truth of\nthe case based on fragmentary information through cooperation and bargaining.\nThe proposed framework integrates next speaker selection based on adjacency\npairs and a self-selection mechanism that takes agents' internal states into\naccount to achieve more natural and strategic dialogue. To verify the\neffectiveness of this new approach, we analyzed utterances that led to dialogue\nbreakdowns and conducted automatic evaluation using LLMs, as well as human\nevaluation using evaluation criteria developed for the Murder Mystery game.\nExperimental results showed that the implementation of the next speaker\nselection mechanism significantly reduced dialogue breakdowns and improved the\nability of agents to share information and perform logical reasoning. The\nresults of this study demonstrate that the systematics of turn-taking in human\nconversation are also effective in controlling dialogue among AI agents, and\nprovide design guidelines for more advanced multi-agent dialogue systems.", "published": "2024-12-06 10:45:54", "link": "http://arxiv.org/abs/2412.04937v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Federated Approach to Few-Shot Hate Speech Detection for Marginalized\n  Communities", "abstract": "Hate speech online remains an understudied issue for marginalized\ncommunities, and has seen rising relevance, especially in the Global South,\nwhich includes developing societies with increasing internet penetration. In\nthis paper, we aim to provide marginalized communities living in societies\nwhere the dominant language is low-resource with a privacy-preserving tool to\nprotect themselves from hate speech on the internet by filtering offensive\ncontent in their native languages. Our contribution in this paper is twofold:\n1) we release REACT (REsponsive hate speech datasets Across ConTexts), a\ncollection of high-quality, culture-specific hate speech detection datasets\ncomprising seven distinct target groups in eight low-resource languages,\ncurated by experienced data collectors; 2) we propose a solution to few-shot\nhate speech detection utilizing federated learning (FL), a privacy-preserving\nand collaborative learning approach, to continuously improve a central model\nthat exhibits robustness when tackling different target groups and languages.\nBy keeping the training local to the users' devices, we ensure the privacy of\nthe users' data while benefitting from the efficiency of federated learning.\nFurthermore, we personalize client models to target-specific training data and\nevaluate their performance. Our results indicate the effectiveness of FL across\ndifferent target groups, whereas the benefits of personalization on few-shot\nlearning are not clear.", "published": "2024-12-06 11:00:05", "link": "http://arxiv.org/abs/2412.04942v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "KaLM: Knowledge-aligned Autoregressive Language Modeling via Dual-view\n  Knowledge Graph Contrastive Learning", "abstract": "Autoregressive large language models (LLMs) pre-trained by next token\nprediction are inherently proficient in generative tasks. However, their\nperformance on knowledge-driven tasks such as factual knowledge querying\nremains unsatisfactory. Knowledge graphs (KGs), as high-quality structured\nknowledge bases, can provide reliable knowledge for LLMs, potentially\ncompensating for their knowledge deficiencies. Aligning LLMs with explicit,\nstructured knowledge from KGs has been a challenge; previous attempts either\nfailed to effectively align knowledge representations or compromised the\ngenerative capabilities of LLMs, leading to less-than-optimal outcomes. This\npaper proposes \\textbf{KaLM}, a \\textit{Knowledge-aligned Language Modeling}\napproach, which fine-tunes autoregressive LLMs to align with KG knowledge via\nthe joint objective of explicit knowledge alignment and implicit knowledge\nalignment. The explicit knowledge alignment objective aims to directly optimize\nthe knowledge representation of LLMs through dual-view knowledge graph\ncontrastive learning. The implicit knowledge alignment objective focuses on\nincorporating textual patterns of knowledge into LLMs through triple completion\nlanguage modeling. Notably, our method achieves a significant performance boost\nin evaluations of knowledge-driven tasks, specifically embedding-based\nknowledge graph completion and generation-based knowledge graph question\nanswering.", "published": "2024-12-06 11:08:24", "link": "http://arxiv.org/abs/2412.04948v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Practical Examination of AI-Generated Text Detectors for Large\n  Language Models", "abstract": "The proliferation of large language models has raised growing concerns about\ntheir misuse, particularly in cases where AI-generated text is falsely\nattributed to human authors. Machine-generated content detectors claim to\neffectively identify such text under various conditions and from any language\nmodel. This paper critically evaluates these claims by assessing several\npopular detectors (RADAR, Wild, T5Sentinel, Fast-DetectGPT, PHD, LogRank,\nBinoculars) on a range of domains, datasets, and models that these detectors\nhave not previously encountered. We employ various prompting strategies to\nsimulate practical adversarial attacks, demonstrating that even moderate\nefforts can significantly evade detection. We emphasize the importance of the\ntrue positive rate at a specific false positive rate (TPR@FPR) metric and\ndemonstrate that these detectors perform poorly in certain settings, with\nTPR@.01 as low as 0%. Our findings suggest that both trained and zero-shot\ndetectors struggle to maintain high sensitivity while achieving a reasonable\ntrue positive rate.", "published": "2024-12-06 15:56:11", "link": "http://arxiv.org/abs/2412.05139v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "QueEn: A Large Language Model for Quechua-English Translation", "abstract": "Recent studies show that large language models (LLMs) are powerful tools for\nworking with natural language, bringing advances in many areas of computational\nlinguistics. However, these models face challenges when applied to low-resource\nlanguages due to limited training data and difficulty in understanding cultural\nnuances. In this paper, we propose QueEn, a novel approach for Quechua-English\ntranslation that combines Retrieval-Augmented Generation (RAG) with\nparameter-efficient fine-tuning techniques. Our method leverages external\nlinguistic resources through RAG and uses Low-Rank Adaptation (LoRA) for\nefficient model adaptation. Experimental results show that our approach\nsubstantially exceeds baseline models, with a BLEU score of 17.6 compared to\n1.5 for standard GPT models. The integration of RAG with fine-tuning allows our\nsystem to address the challenges of low-resource language translation while\nmaintaining computational efficiency. This work contributes to the broader goal\nof preserving endangered languages through advanced language technologies.", "published": "2024-12-06 17:04:21", "link": "http://arxiv.org/abs/2412.05184v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MAmmoTH-VL: Eliciting Multimodal Reasoning with Instruction Tuning at\n  Scale", "abstract": "Open-source multimodal large language models (MLLMs) have shown significant\npotential in a broad range of multimodal tasks. However, their reasoning\ncapabilities remain constrained by existing instruction-tuning datasets, which\nwere predominately repurposed from academic datasets such as VQA, AI2D, and\nChartQA. These datasets target simplistic tasks, and only provide phrase-level\nanswers without any intermediate rationales. To address these challenges, we\nintroduce a scalable and cost-effective method to construct a large-scale\nmultimodal instruction-tuning dataset with rich intermediate rationales\ndesigned to elicit CoT reasoning. Using only open models, we create a dataset\ncontaining 12M instruction-response pairs to cover diverse, reasoning-intensive\ntasks with detailed and faithful rationales. Experiments demonstrate that\ntraining MLLMs on this dataset significantly improves reasoning capabilities,\nachieving state-of-the-art performance on benchmarks such as MathVerse (+8.1%),\nMMMU-Pro (+7%), and MuirBench (+13.3%). Additionally, the model demonstrates\nnotable improvements of up to 4% on non-reasoning-based benchmarks. Ablation\nstudies further highlight the importance of key components, such as rewriting\nand self-filtering, in the dataset construction process.", "published": "2024-12-06 18:14:24", "link": "http://arxiv.org/abs/2412.05237v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Multi-Party Supervised Fine-tuning of Language Models for Multi-Party\n  Dialogue Generation", "abstract": "Large Language Models (LLM) are usually fine-tuned to participate in dyadic\nor two-party dialogues, which can not adapt well to multi-party dialogues\n(MPD), which hinders their applications in such scenarios including\nmulti-personal meetings, discussions and daily communication. Previous\nLLM-based researches mainly focus on the multi-agent framework, while their\nbase LLMs are still pairwisely fine-tuned. In this work, we design a\nmulti-party fine-tuning framework (MuPaS) for LLMs on the multi-party dialogue\ndatasets, and prove such a straightforward framework can let the LLM align with\nthe multi-party conversation style efficiently and effectively. We also design\ntwo training strategies which can convert MuPaS into the MPD simulator.\nSubstantial experiments show that MuPaS can achieve state-of-the-art\nmulti-party response, higher accuracy of the-next-speaker prediction, higher\nhuman and automatic evaluated utterance qualities, and can even generate\nreasonably with out-of-distribution scene, topic and role descriptions. The\nMuPaS framework bridges the LLM training with more complicated multi-party\napplications, such as conversation generation, virtual rehearsal or\nmeta-universe.", "published": "2024-12-06 09:33:47", "link": "http://arxiv.org/abs/2412.05342v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Diversity Over Quantity: A Lesson From Few Shot Relation Classification", "abstract": "In few-shot relation classification (FSRC), models must generalize to novel\nrelations with only a few labeled examples. While much of the recent progress\nin NLP has focused on scaling data size, we argue that diversity in relation\ntypes is more crucial for FSRC performance. In this work, we demonstrate that\ntraining on a diverse set of relations significantly enhances a model's ability\nto generalize to unseen relations, even when the overall dataset size remains\nfixed.\n  We introduce REBEL-FS, a new FSRC benchmark that incorporates an order of\nmagnitude more relation types than existing datasets. Through systematic\nexperiments, we show that increasing the diversity of relation types in the\ntraining data leads to consistent gains in performance across various few-shot\nlearning scenarios, including high-negative settings. Our findings challenge\nthe common assumption that more data alone leads to better performance and\nsuggest that targeted data curation focused on diversity can substantially\nreduce the need for large-scale datasets in FSRC.", "published": "2024-12-06 21:41:01", "link": "http://arxiv.org/abs/2412.05434v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Effective GenAI Multi-Agent Collaboration: Design and Evaluation\n  for Enterprise Applications", "abstract": "AI agents powered by large language models (LLMs) have shown strong\ncapabilities in problem solving. Through combining many intelligent agents,\nmulti-agent collaboration has emerged as a promising approach to tackle\ncomplex, multi-faceted problems that exceed the capabilities of single AI\nagents. However, designing the collaboration protocols and evaluating the\neffectiveness of these systems remains a significant challenge, especially for\nenterprise applications. This report addresses these challenges by presenting a\ncomprehensive evaluation of coordination and routing capabilities in a novel\nmulti-agent collaboration framework. We evaluate two key operational modes: (1)\na coordination mode enabling complex task completion through parallel\ncommunication and payload referencing, and (2) a routing mode for efficient\nmessage forwarding between agents. We benchmark on a set of handcrafted\nscenarios from three enterprise domains, which are publicly released with the\nreport. For coordination capabilities, we demonstrate the effectiveness of\ninter-agent communication and payload referencing mechanisms, achieving\nend-to-end goal success rates of 90%. Our analysis yields several key findings:\nmulti-agent collaboration enhances goal success rates by up to 70% compared to\nsingle-agent approaches in our benchmarks; payload referencing improves\nperformance on code-intensive tasks by 23%; latency can be substantially\nreduced with a routing mechanism that selectively bypasses agent orchestration.\nThese findings offer valuable guidance for enterprise deployments of\nmulti-agent systems and advance the development of scalable, efficient\nmulti-agent collaboration frameworks.", "published": "2024-12-06 22:14:17", "link": "http://arxiv.org/abs/2412.05449v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing LLMs for Impression Generation in Radiology Reports through a\n  Multi-Agent System", "abstract": "This study introduces \"RadCouncil,\" a multi-agent Large Language Model (LLM)\nframework designed to enhance the generation of impressions in radiology\nreports from the finding section. RadCouncil comprises three specialized\nagents: 1) a \"Retrieval\" Agent that identifies and retrieves similar reports\nfrom a vector database, 2) a \"Radiologist\" Agent that generates impressions\nbased on the finding section of the given report plus the exemplar reports\nretrieved by the Retrieval Agent, and 3) a \"Reviewer\" Agent that evaluates the\ngenerated impressions and provides feedback. The performance of RadCouncil was\nevaluated using both quantitative metrics (BLEU, ROUGE, BERTScore) and\nqualitative criteria assessed by GPT-4, using chest X-ray as a case study.\nExperiment results show improvements in RadCouncil over the single-agent\napproach across multiple dimensions, including diagnostic accuracy, stylistic\nconcordance, and clarity. This study highlights the potential of utilizing\nmultiple interacting LLM agents, each with a dedicated task, to enhance\nperformance in specialized medical tasks and the development of more robust and\nadaptable healthcare AI solutions.", "published": "2024-12-06 21:33:03", "link": "http://arxiv.org/abs/2412.06828v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Privacy-Preserving Retrieval-Augmented Generation with Differential\n  Privacy", "abstract": "With the recent remarkable advancement of large language models (LLMs), there\nhas been a growing interest in utilizing them in the domains with highly\nsensitive data that lies outside their training data. For this purpose,\nretrieval-augmented generation (RAG) is particularly effective -- it assists\nLLMs by directly providing relevant information from the external knowledge\nsources. However, without extra privacy safeguards, RAG outputs risk leaking\nsensitive information from the external data source. In this work, we explore\nRAG under differential privacy (DP), a formal guarantee of data privacy. The\nmain challenge with differentially private RAG is how to generate long accurate\nanswers within a moderate privacy budget. We address this by proposing an\nalgorithm that smartly spends privacy budget only for the tokens that require\nthe sensitive information and uses the non-private LLM for other tokens. Our\nextensive empirical evaluations reveal that our algorithm outperforms the\nnon-RAG baseline under a reasonable privacy budget of $\\epsilon\\approx 10$\nacross different models and datasets.", "published": "2024-12-06 01:20:16", "link": "http://arxiv.org/abs/2412.04697v2", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Transformers Struggle to Learn to Search", "abstract": "Search is an ability foundational in many important tasks, and recent studies\nhave shown that large language models (LLMs) struggle to perform search\nrobustly. It is unknown whether this inability is due to a lack of data,\ninsufficient model parameters, or fundamental limitations of the transformer\narchitecture. In this work, we use the foundational graph connectivity problem\nas a testbed to generate effectively limitless high-coverage data to train\nsmall transformers and test whether they can learn to perform search. We find\nthat, when given the right training distribution, the transformer is able to\nlearn to search.\n  We analyze the algorithm that the transformer has learned through a novel\nmechanistic interpretability technique that enables us to extract the\ncomputation graph from the trained model. We find that transformers perform\nsearch at every vertex in parallel: For each vertex in the input graph,\ntransformers compute the set of vertices reachable from that vertex. Each layer\nthen progressively expands these sets, allowing the model to search over a\nnumber of vertices exponential in $n_{\\text{layers}}$.\n  However, we find that as the input graph size increases, the transformer has\ngreater difficulty in learning the task. This difficulty is not resolved even\nas the number of parameters is increased, suggesting that increasing model\nscale will not lead to robust search abilities. We also find that performing\nsearch in-context (i.e., chain-of-thought) does not resolve this inability to\nlearn to search on larger graphs.", "published": "2024-12-06 01:29:24", "link": "http://arxiv.org/abs/2412.04703v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Question Answering for Decisionmaking in Green Building Design: A\n  Multimodal Data Reasoning Method Driven by Large Language Models", "abstract": "In recent years, the critical role of green buildings in addressing energy\nconsumption and environmental issues has become widely acknowledged. Research\nindicates that over 40% of potential energy savings can be achieved during the\nearly design stage. Therefore, decision-making in green building design (DGBD),\nwhich is based on modeling and performance simulation, is crucial for reducing\nbuilding energy costs. However, the field of green building encompasses a broad\nrange of specialized knowledge, which involves significant learning costs and\nresults in low decision-making efficiency. Many studies have already applied\nartificial intelligence (AI) methods to this field. Based on previous research,\nthis study innovatively integrates large language models with DGBD, creating\nGreenQA, a question answering framework for multimodal data reasoning.\nUtilizing Retrieval Augmented Generation, Chain of Thought, and Function Call\nmethods, GreenQA enables multimodal question answering, including weather data\nanalysis and visualization, retrieval of green building cases, and knowledge\nquery. Additionally, this study conducted a user survey using the GreenQA web\nplatform. The results showed that 96% of users believed the platform helped\nimprove design efficiency. This study not only effectively supports DGBD but\nalso provides inspiration for AI-assisted design.", "published": "2024-12-06 03:02:58", "link": "http://arxiv.org/abs/2412.04741v1", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI"}
{"title": "Rethinking Time Series Forecasting with LLMs via Nearest Neighbor\n  Contrastive Learning", "abstract": "Adapting Large Language Models (LLMs) that are extensively trained on\nabundant text data, and customizing the input prompt to enable time series\nforecasting has received considerable attention. While recent work has shown\ngreat potential for adapting the learned prior of LLMs, the formulation of the\nprompt to finetune LLMs remains challenging as prompt should be aligned with\ntime series data. Additionally, current approaches do not effectively leverage\nword token embeddings which embody the rich representation space learned by\nLLMs. This emphasizes the need for a robust approach to formulate the prompt\nwhich utilizes the word token embeddings while effectively representing the\ncharacteristics of the time series. To address these challenges, we propose\nNNCL-TLLM: Nearest Neighbor Contrastive Learning for Time series forecasting\nvia LLMs. First, we generate time series compatible text prototypes such that\neach text prototype represents both word token embeddings in its neighborhood\nand time series characteristics via end-to-end finetuning. Next, we draw\ninspiration from Nearest Neighbor Contrastive Learning to formulate the prompt\nwhile obtaining the top-$k$ nearest neighbor time series compatible text\nprototypes. We then fine-tune the layer normalization and positional embeddings\nof the LLM, keeping the other layers intact, reducing the trainable parameters\nand decreasing the computational cost. Our comprehensive experiments\ndemonstrate that NNCL-TLLM outperforms in few-shot forecasting while achieving\ncompetitive or superior performance over the state-of-the-art methods in\nlong-term and short-term forecasting tasks.", "published": "2024-12-06 06:32:47", "link": "http://arxiv.org/abs/2412.04806v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "EACO: Enhancing Alignment in Multimodal LLMs via Critical Observation", "abstract": "Multimodal large language models (MLLMs) have achieved remarkable progress on\nvarious visual question answering and reasoning tasks leveraging instruction\nfine-tuning specific datasets. They can also learn from preference data\nannotated by human to enhance their reasoning ability and mitigate\nhallucinations. Most of preference data is generated from the model itself.\nHowever, existing methods require high-quality critical labels, which are\ncostly and rely on human or proprietary models like GPT-4V. In this work, we\npropose Enhancing Alignment in MLLMs via Critical Observation (EACO), which\naligns MLLMs by self-generated preference data using only 5k images\neconomically. Our approach begins with collecting and refining a Scoring\nEvaluation Instruction-tuning dataset to train a critical evaluation model,\ntermed the Critic. This Critic observes model responses across multiple\ndimensions, selecting preferred and non-preferred outputs for refined Direct\nPreference Optimization (DPO) tuning. To further enhance model performance, we\nemploy an additional supervised fine-tuning stage after preference tuning. EACO\nreduces the overall hallucinations by 65.6% on HallusionBench and improves the\nreasoning ability by 21.8% on MME-Cognition. EACO achieves an 8.5% improvement\nover LLaVA-v1.6-Mistral-7B across multiple benchmarks. Remarkably, EACO also\nshows the potential critical ability in open-source MLLMs, demonstrating that\nEACO is a viable path to boost the competence of MLLMs.", "published": "2024-12-06 09:59:47", "link": "http://arxiv.org/abs/2412.04903v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "DEMO: Reframing Dialogue Interaction with Fine-grained Element Modeling", "abstract": "Large language models (LLMs) enabled dialogue systems have become one of the\ncentral modes in human-machine interaction, which bring about vast amounts of\nconversation logs and increasing demand for dialogue generation. The dialogue's\nlife-cycle spans from $\\textit{Prelude}$ through $\\textit{Interlocution}$ to\n$\\textit{Epilogue}$, encompassing rich dialogue elements. Despite large volumes\nof dialogue-related studies, there is a lack of systematic investigation into\nthe dialogue stages to frame benchmark construction that covers comprehensive\ndialogue elements. This hinders the precise modeling, generation and assessment\nof LLMs-based dialogue systems. To bridge this gap, in this paper, we introduce\na new research task--$\\textbf{D}$ialogue $\\textbf{E}$lement\n$\\textbf{MO}$deling, including $\\textit{Element Awareness}$ and\n$\\textit{Dialogue Agent Interaction}$, and propose a novel benchmark,\n$\\textbf{DEMO}$, designed for a comprehensive dialogue modeling and assessment.\nOn this basis, we further build the DEMO agent with the adept ability to model\ndialogue elements via imitation learning. Extensive experiments on DEMO\nindicate that current representative LLMs still have considerable potential for\nenhancement, and our DEMO agent performs well in both dialogue element modeling\nand out-of-domain tasks.", "published": "2024-12-06 10:01:38", "link": "http://arxiv.org/abs/2412.04905v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Probing the contents of semantic representations from text, behavior,\n  and brain data using the psychNorms metabase", "abstract": "Semantic representations are integral to natural language processing,\npsycholinguistics, and artificial intelligence. Although often derived from\ninternet text, recent years have seen a rise in the popularity of\nbehavior-based (e.g., free associations) and brain-based (e.g., fMRI)\nrepresentations, which promise improvements in our ability to measure and model\nhuman representations. We carry out the first systematic evaluation of the\nsimilarities and differences between semantic representations derived from\ntext, behavior, and brain data. Using representational similarity analysis, we\nshow that word vectors derived from behavior and brain data encode information\nthat differs from their text-derived cousins. Furthermore, drawing on our\npsychNorms metabase, alongside an interpretability method that we call\nrepresentational content analysis, we find that, in particular, behavior\nrepresentations capture unique variance on certain affective, agentic, and\nsocio-moral dimensions. We thus establish behavior as an important complement\nto text for capturing human representations and behavior. These results are\nbroadly relevant to research aimed at learning human-aligned semantic\nrepresentations, including work on evaluating and aligning large language\nmodels.", "published": "2024-12-06 10:44:20", "link": "http://arxiv.org/abs/2412.04936v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Gla-AI4BioMed at RRG24: Visual Instruction-tuned Adaptation for\n  Radiology Report Generation", "abstract": "We introduce a radiology-focused visual language model designed to generate\nradiology reports from chest X-rays. Building on previous findings that large\nlanguage models (LLMs) can acquire multimodal capabilities when aligned with\npretrained vision encoders, we demonstrate similar potential with chest X-ray\nimages. This integration enhances the ability of model to understand and\ndescribe chest X-ray images. Our model combines an image encoder with a\nfine-tuned LLM based on the Vicuna-7B architecture, enabling it to generate\ndifferent sections of a radiology report with notable accuracy. The training\nprocess involves a two-stage approach: (i) initial alignment of chest X-ray\nfeatures with the LLM (ii) followed by fine-tuning for radiology report\ngeneration.", "published": "2024-12-06 11:14:03", "link": "http://arxiv.org/abs/2412.04954v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Explingo: Explaining AI Predictions using Large Language Models", "abstract": "Explanations of machine learning (ML) model predictions generated by\nExplainable AI (XAI) techniques such as SHAP are essential for people using ML\noutputs for decision-making. We explore the potential of Large Language Models\n(LLMs) to transform these explanations into human-readable, narrative formats\nthat align with natural communication. We address two key research questions:\n(1) Can LLMs reliably transform traditional explanations into high-quality\nnarratives? and (2) How can we effectively evaluate the quality of narrative\nexplanations? To answer these questions, we introduce Explingo, which consists\nof two LLM-based subsystems, a Narrator and Grader. The Narrator takes in ML\nexplanations and transforms them into natural-language descriptions. The Grader\nscores these narratives on a set of metrics including accuracy, completeness,\nfluency, and conciseness.\n  Our experiments demonstrate that LLMs can generate high-quality narratives\nthat achieve high scores across all metrics, particularly when guided by a\nsmall number of human-labeled and bootstrapped examples. We also identified\nareas that remain challenging, in particular for effectively scoring narratives\nin complex domains. The findings from this work have been integrated into an\nopen-source tool that makes narrative explanations available for further\napplications.", "published": "2024-12-06 16:01:30", "link": "http://arxiv.org/abs/2412.05145v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Benchmarking Open-ended Audio Dialogue Understanding for Large\n  Audio-Language Models", "abstract": "Large Audio-Language Models (LALMs) have unclocked audio dialogue\ncapabilities, where audio dialogues are a direct exchange of spoken language\nbetween LALMs and humans. Recent advances, such as GPT-4o, have enabled LALMs\nin back-and-forth audio dialogues with humans. This progression not only\nunderscores the potential of LALMs but also broadens their applicability across\na wide range of practical scenarios supported by audio dialogues. However,\ngiven these advancements, a comprehensive benchmark to evaluate the performance\nof LALMs in the open-ended audio dialogue understanding remains absent\ncurrently. To address this gap, we propose an Audio Dialogue Understanding\nBenchmark (ADU-Bench), which consists of 4 benchmark datasets. They assess the\nopen-ended audio dialogue ability for LALMs in 3 general scenarios, 12 skills,\n9 multilingual languages, and 4 categories of ambiguity handling. Notably, we\nfirstly propose the evaluation of ambiguity handling in audio dialogues that\nexpresses different intentions beyond the same literal meaning of sentences,\ne.g., \"Really!?\" with different intonations. In summary, ADU-Bench includes\nover 20,000 open-ended audio dialogues for the assessment of LALMs. Through\nextensive experiments conducted on 13 LALMs, our analysis reveals that there is\nstill considerable room for improvement in the audio dialogue understanding\nabilities of existing LALMs. In particular, they struggle with mathematical\nsymbols and formulas, understanding human behavior such as roleplay,\ncomprehending multiple languages, and handling audio dialogue ambiguities from\ndifferent phonetic elements, such as intonations, pause positions, and\nhomophones.", "published": "2024-12-06 16:34:15", "link": "http://arxiv.org/abs/2412.05167v1", "categories": ["cs.AI", "cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.AI"}
{"title": "ConQRet: Benchmarking Fine-Grained Evaluation of Retrieval Augmented\n  Argumentation with LLM Judges", "abstract": "Computational argumentation, which involves generating answers or summaries\nfor controversial topics like abortion bans and vaccination, has become\nincreasingly important in today's polarized environment. Sophisticated LLM\ncapabilities offer the potential to provide nuanced, evidence-based answers to\nsuch questions through Retrieval-Augmented Argumentation (RAArg), leveraging\nreal-world evidence for high-quality, grounded arguments. However, evaluating\nRAArg remains challenging, as human evaluation is costly and difficult for\ncomplex, lengthy answers on complicated topics. At the same time, re-using\nexisting argumentation datasets is no longer sufficient, as they lack long,\ncomplex arguments and realistic evidence from potentially misleading sources,\nlimiting holistic evaluation of retrieval effectiveness and argument quality.\nTo address these gaps, we investigate automated evaluation methods using\nmultiple fine-grained LLM judges, providing better and more interpretable\nassessments than traditional single-score metrics and even previously reported\nhuman crowdsourcing. To validate the proposed techniques, we introduce ConQRet,\na new benchmark featuring long and complex human-authored arguments on debated\ntopics, grounded in real-world websites, allowing an exhaustive evaluation\nacross retrieval effectiveness, argument quality, and groundedness. We validate\nour LLM Judges on a prior dataset and the new ConQRet benchmark. Our proposed\nLLM Judges and the ConQRet benchmark can enable rapid progress in computational\nargumentation and can be naturally extended to other complex\nretrieval-augmented generation tasks.", "published": "2024-12-06 17:35:52", "link": "http://arxiv.org/abs/2412.05206v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "I.2.7"], "primary_category": "cs.CL"}
{"title": "BEExformer: A Fast Inferencing Transformer Architecture via Binarization\n  with Multiple Early Exits", "abstract": "Large Language Models (LLMs) based on transformers achieve cutting-edge\nresults on a variety of applications. However, their enormous size and\nprocessing requirements make deployment on devices with constrained resources\nextremely difficult. Among various efficiency considerations, model\nbinarization and Early Exit (EE) are common effective solutions. However,\nbinarization may lead to performance loss due to reduced precision affecting\ngradient estimation and parameter updates. Besides, the present early-exit\nmechanisms are still in the nascent stages of research. To ameliorate these\nissues, we propose Binarized Early Exit Transformer (BEExformer), the\nfirst-ever selective learning transformer architecture to combine early exit\nwith binarization for textual inference. It improves the binarization process\nthrough a differentiable second-order approximation to the impulse function.\nThis enables gradient computation concerning both the sign as well as the\nmagnitude of the weights. In contrast to absolute threshold-based EE, the\nproposed EE mechanism hinges on fractional reduction in entropy among\nintermediate transformer blocks with soft-routing loss estimation. While\nbinarization results in 18.44 times reduction in model size, early exit reduces\nthe FLOPs during inference by 54.85% and even improves accuracy by 5.98%\nthrough resolving the \"overthinking\" problem inherent in deep networks.\nMoreover, the proposed BEExformer simplifies training by not requiring\nknowledge distillation from a full-precision LLM. Extensive evaluation on the\nGLUE dataset and comparison with the SOTA works showcase its pareto-optimal\nperformance-efficiency trade-off.", "published": "2024-12-06 17:58:14", "link": "http://arxiv.org/abs/2412.05225v1", "categories": ["cs.CL", "cs.AI", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Enhancing FKG.in: automating Indian food composition analysis", "abstract": "This paper presents a novel approach to compute food composition data for\nIndian recipes using a knowledge graph for Indian food (FKG.in) and LLMs. The\nprimary focus is to provide a broad overview of an automated food composition\nanalysis workflow and describe its core functionalities: nutrition data\naggregation, food composition analysis, and LLM-augmented information\nresolution. This workflow aims to complement FKG.in and iteratively supplement\nfood composition data from verified knowledge bases. Additionally, this paper\nhighlights the challenges of representing Indian food and accessing food\ncomposition data digitally. It also reviews three key sources of food\ncomposition data: the Indian Food Composition Tables, the Indian Nutrient\nDatabank, and the Nutritionix API. Furthermore, it briefly outlines how users\ncan interact with the workflow to obtain diet-based health recommendations and\ndetailed food composition information for numerous recipes. We then explore the\ncomplex challenges of analyzing Indian recipe information across dimensions\nsuch as structure, multilingualism, and uncertainty as well as present our\nongoing work on LLM-based solutions to address these issues. The methods\nproposed in this workshop paper for AI-driven knowledge curation and\ninformation resolution are application-agnostic, generalizable, and replicable\nfor any domain.", "published": "2024-12-06 18:27:15", "link": "http://arxiv.org/abs/2412.05248v2", "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.AI"}
{"title": "Uncertainty Quantification for Transformer Models for Dark-Pattern\n  Detection", "abstract": "The opaque nature of transformer-based models, particularly in applications\nsusceptible to unethical practices such as dark-patterns in user interfaces,\nrequires models that integrate uncertainty quantification to enhance trust in\npredictions. This study focuses on dark-pattern detection, deceptive design\nchoices that manipulate user decisions, undermining autonomy and consent. We\npropose a differential fine-tuning approach implemented at the final\nclassification head via uncertainty quantification with transformer-based\npre-trained models. Employing a dense neural network (DNN) head architecture as\na baseline, we examine two methods capable of quantifying uncertainty:\nSpectral-normalized Neural Gaussian Processes (SNGPs) and Bayesian Neural\nNetworks (BNNs). These methods are evaluated on a set of open-source\nfoundational models across multiple dimensions: model performance, variance in\ncertainty of predictions and environmental impact during training and inference\nphases. Results demonstrate that integrating uncertainty quantification\nmaintains performance while providing insights into challenging instances\nwithin the models. Moreover, the study reveals that the environmental impact\ndoes not uniformly increase with the incorporation of uncertainty\nquantification techniques. The study's findings demonstrate that uncertainty\nquantification enhances transparency and provides measurable confidence in\npredictions, improving the explainability and clarity of black-box models. This\nfacilitates informed decision-making and mitigates the influence of\ndark-patterns on user interfaces. These results highlight the importance of\nincorporating uncertainty quantification techniques in developing machine\nlearning models, particularly in domains where interpretability and\ntrustworthiness are critical.", "published": "2024-12-06 18:31:51", "link": "http://arxiv.org/abs/2412.05251v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "math.PR"], "primary_category": "cs.LG"}
{"title": "TeamCraft: A Benchmark for Multi-Modal Multi-Agent Systems in Minecraft", "abstract": "Collaboration is a cornerstone of society. In the real world, human teammates\nmake use of multi-sensory data to tackle challenging tasks in ever-changing\nenvironments. It is essential for embodied agents collaborating in\nvisually-rich environments replete with dynamic interactions to understand\nmulti-modal observations and task specifications. To evaluate the performance\nof generalizable multi-modal collaborative agents, we present TeamCraft, a\nmulti-modal multi-agent benchmark built on top of the open-world video game\nMinecraft. The benchmark features 55,000 task variants specified by multi-modal\nprompts, procedurally-generated expert demonstrations for imitation learning,\nand carefully designed protocols to evaluate model generalization capabilities.\nWe also perform extensive analyses to better understand the limitations and\nstrengths of existing approaches. Our results indicate that existing models\ncontinue to face significant challenges in generalizing to novel goals, scenes,\nand unseen numbers of agents. These findings underscore the need for further\nresearch in this area. The TeamCraft platform and dataset are publicly\navailable at https://github.com/teamcraft-bench/teamcraft.", "published": "2024-12-06 18:41:16", "link": "http://arxiv.org/abs/2412.05255v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.MA"], "primary_category": "cs.AI"}
{"title": "PyTerrier-GenRank: The PyTerrier Plugin for Reranking with Large\n  Language Models", "abstract": "Using LLMs as rerankers requires experimenting with various hyperparameters,\nsuch as prompt formats, model choice, and reformulation strategies. We\nintroduce PyTerrier-GenRank, a PyTerrier plugin to facilitate seamless\nreranking experiments with LLMs, supporting popular ranking strategies like\npointwise and listwise prompting. We validate our plugin through HuggingFace\nand OpenAI hosted endpoints.", "published": "2024-12-06 04:30:00", "link": "http://arxiv.org/abs/2412.05339v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "H.3.3"], "primary_category": "cs.IR"}
{"title": "CALICO: Conversational Agent Localization via Synthetic Data Generation", "abstract": "We present CALICO, a method to fine-tune Large Language Models (LLMs) to\nlocalize conversational agent training data from one language to another. For\nslots (named entities), CALICO supports three operations: verbatim copy,\nliteral translation, and localization, i.e. generating slot values more\nappropriate in the target language, such as city and airport names located in\ncountries where the language is spoken. Furthermore, we design an iterative\nfiltering mechanism to discard noisy generated samples, which we show boosts\nthe performance of the downstream conversational agent. To prove the\neffectiveness of CALICO, we build and release a new human-localized (HL)\nversion of the MultiATIS++ travel information test set in 8 languages. Compared\nto the original human-translated (HT) version of the test set, we show that our\nnew HL version is more challenging. We also show that CALICO out-performs\nstate-of-the-art LINGUIST (which relies on literal slot translation out of\ncontext) both on the HT case, where CALICO generates more accurate slot\ntranslations, and on the HL case, where CALICO generates localized slots which\nare closer to the HL test set.", "published": "2024-12-06 19:29:16", "link": "http://arxiv.org/abs/2412.05388v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "StableVC: Style Controllable Zero-Shot Voice Conversion with Conditional\n  Flow Matching", "abstract": "Zero-shot voice conversion (VC) aims to transfer the timbre from the source\nspeaker to an arbitrary unseen speaker while preserving the original linguistic\ncontent. Despite recent advancements in zero-shot VC using language model-based\nor diffusion-based approaches, several challenges remain: 1) current approaches\nprimarily focus on adapting timbre from unseen speakers and are unable to\ntransfer style and timbre to different unseen speakers independently; 2) these\napproaches often suffer from slower inference speeds due to the autoregressive\nmodeling methods or the need for numerous sampling steps; 3) the quality and\nsimilarity of the converted samples are still not fully satisfactory. To\naddress these challenges, we propose a style controllable zero-shot VC approach\nnamed StableVC, which aims to transfer timbre and style from source speech to\ndifferent unseen target speakers. Specifically, we decompose speech into\nlinguistic content, timbre, and style, and then employ a conditional flow\nmatching module to reconstruct the high-quality mel-spectrogram based on these\ndecomposed features. To effectively capture timbre and style in a zero-shot\nmanner, we introduce a novel dual attention mechanism with an adaptive gate,\nrather than using conventional feature concatenation. With this\nnon-autoregressive design, StableVC can efficiently capture the intricate\ntimbre and style from different unseen speakers and generate high-quality\nspeech significantly faster than real-time. Experiments demonstrate that our\nproposed StableVC outperforms state-of-the-art baseline systems in zero-shot VC\nand achieves flexible control over timbre and style from different unseen\nspeakers. Moreover, StableVC offers approximately 25x and 1.65x faster sampling\ncompared to autoregressive and diffusion-based baselines.", "published": "2024-12-06 02:31:23", "link": "http://arxiv.org/abs/2412.04724v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Perceptually Transparent Binaural Auralization of Simulated Sound Fields", "abstract": "Contrary to geometric acoustics-based simulations where the spatial\ninformation is available in a tangible form, it is not straightforward to\nauralize wave-based simulations. A variety of methods have been proposed that\ncompute the ear signals of a virtual listener with known head-related transfer\nfunctions from sampling either the sound pressure or the particle velocity (or\nboth) of the simulated sound field. The available perceptual evaluation results\nof such methods are not comprehensive so that it is unclear what number and\narrangement of sampling points is required for achieving perceptually\ntransparent auralization, i.e.~for achieving an auralization that is\nperceptually indistinguishable from the ground truth. This article presents a\nperceptual evaluation of the most common binaural auralization methods with and\nwithout intermediate ambisonic representation of volumetrically sampled sound\npressure or sound pressure and particle velocity sampled on spherical or\ncubical surfaces. Our results confirm that perceptually transparent\nauralization is possible if sound pressure and particle velocity are available\nat 289 sampling points on a spherical surface grid. Other grid geometries\nrequire considerably more points. All tested methods are available open source\nin the Chalmers Auralization Toolbox that accompanies this article.", "published": "2024-12-06 13:07:35", "link": "http://arxiv.org/abs/2412.05015v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Applying Automatic Differentiation to Optimize Differential Microphone\n  Array Designs", "abstract": "This paper introduces a novel methodology leveraging differentiable\nprogramming to design efficient, constrained adaptive non-uniform Linear\nDifferential Microphone Arrays (LDMAs) with reduced implementation costs.\nUtilizing an automatic differentiation framework, we propose a differentiable\nconvex approach that enables the adaptive design of a filter with a\ndistortionless constraint in the desired sound direction, while also imposing\nconstraints on microphone positioning to ensure consistent performance. This\napproach achieves the desired Directivity Factor (DF) over a wide frequency\nrange and facilitates effective recovery of wide-band speech signals at lower\nimplementation costs.", "published": "2024-12-06 15:29:48", "link": "http://arxiv.org/abs/2412.05123v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Diff4Steer: Steerable Diffusion Prior for Generative Music Retrieval\n  with Semantic Guidance", "abstract": "Modern music retrieval systems often rely on fixed representations of user\npreferences, limiting their ability to capture users' diverse and uncertain\nretrieval needs. To address this limitation, we introduce Diff4Steer, a novel\ngenerative retrieval framework that employs lightweight diffusion models to\nsynthesize diverse seed embeddings from user queries that represent potential\ndirections for music exploration. Unlike deterministic methods that map user\nquery to a single point in embedding space, Diff4Steer provides a statistical\nprior on the target modality (audio) for retrieval, effectively capturing the\nuncertainty and multi-faceted nature of user preferences. Furthermore,\nDiff4Steer can be steered by image or text inputs, enabling more flexible and\ncontrollable music discovery combined with nearest neighbor search. Our\nframework outperforms deterministic regression methods and LLM-based generative\nretrieval baseline in terms of retrieval and ranking metrics, demonstrating its\neffectiveness in capturing user preferences, leading to more diverse and\nrelevant recommendations. Listening examples are available at\ntinyurl.com/diff4steer.", "published": "2024-12-06 03:18:18", "link": "http://arxiv.org/abs/2412.04746v1", "categories": ["cs.SD", "cs.IR", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Continuous Speech Tokens Makes LLMs Robust Multi-Modality Learners", "abstract": "Recent advances in GPT-4o like multi-modality models have demonstrated\nremarkable progress for direct speech-to-speech conversation, with real-time\nspeech interaction experience and strong speech understanding ability. However,\ncurrent research focuses on discrete speech tokens to align with discrete text\ntokens for language modelling, which depends on an audio codec with residual\nconnections or independent group tokens, such a codec usually leverages large\nscale and diverse datasets training to ensure that the discrete speech codes\nhave good representation for varied domain, noise, style data reconstruction as\nwell as a well-designed codec quantizer and encoder-decoder architecture for\ndiscrete token language modelling. This paper introduces Flow-Omni, a\ncontinuous speech token based GPT-4o like model, capable of real-time speech\ninteraction and low streaming latency. Specifically, first, instead of\ncross-entropy loss only, we combine flow matching loss with a pretrained\nautoregressive LLM and a small MLP network to predict the probability\ndistribution of the continuous-valued speech tokens from speech prompt. second,\nwe incorporated the continuous speech tokens to Flow-Omni multi-modality\ntraining, thereby achieving robust speech-to-speech performance with discrete\ntext tokens and continuous speech tokens together. Experiments demonstrate\nthat, compared to discrete text and speech multi-modality training and its\nvariants, the continuous speech tokens mitigate robustness issues by avoiding\nthe inherent flaws of discrete speech code's representation loss for LLM.", "published": "2024-12-06 10:16:04", "link": "http://arxiv.org/abs/2412.04917v1", "categories": ["cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "pyAMPACT: A Score-Audio Alignment Toolkit for Performance Data\n  Estimation and Multi-modal Processing", "abstract": "pyAMPACT (Python-based Automatic Music Performance Analysis and Comparison\nToolkit) links symbolic and audio music representations to facilitate\nscore-informed estimation of performance data in audio as well as general\nlinking of symbolic and audio music representations with a variety of\nannotations. pyAMPACT can read a range of symbolic formats and can output\nnote-linked audio descriptors/performance data into MEI-formatted files. The\naudio analysis uses score alignment to calculate time-frequency regions of\nimportance for each note in the symbolic representation from which to estimate\na range of parameters. These include tuning-, dynamics-, and timbre-related\nperformance descriptors, with timing-related information available from the\nscore alignment. Beyond performance data estimation, pyAMPACT also facilitates\nmulti-modal investigations through its infrastructure for linking symbolic\nrepresentations and annotations to audio.", "published": "2024-12-06 21:44:33", "link": "http://arxiv.org/abs/2412.05436v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
