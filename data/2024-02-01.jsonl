{"title": "Does DetectGPT Fully Utilize Perturbation? Bridging Selective\n  Perturbation to Fine-tuned Contrastive Learning Detector would be Better", "abstract": "The burgeoning generative capabilities of large language models (LLMs) have\nraised growing concerns about abuse, demanding automatic machine-generated text\ndetectors. DetectGPT, a zero-shot metric-based detector, first introduces\nperturbation and shows great performance improvement. However, in DetectGPT,\nthe random perturbation strategy could introduce noise, and logit regression\ndepends on the threshold, harming the generalizability and applicability of\nindividual or small-batch inputs. Hence, we propose a novel fine-tuned\ndetector, Pecola, bridging metric-based and fine-tuned methods by contrastive\nlearning on selective perturbation. Selective strategy retains important tokens\nduring perturbation and weights for multi-pair contrastive learning. The\nexperiments show that Pecola outperforms the state-of-the-art (SOTA) by 1.20%\nin accuracy on average on four public datasets. And we further analyze the\neffectiveness, robustness, and generalization of the method.", "published": "2024-02-01 01:23:07", "link": "http://arxiv.org/abs/2402.00263v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Crucial Parameter for Rank-Frequency Relation in Natural Languages", "abstract": "$f \\propto r^{-\\alpha} \\cdot (r+\\gamma)^{-\\beta}$ has been empirically shown\nmore precise than a na\\\"ive power law $f\\propto r^{-\\alpha}$ to model the\nrank-frequency ($r$-$f$) relation of words in natural languages. This work\nshows that the only crucial parameter in the formulation is $\\gamma$, which\ndepicts the resistance to vocabulary growth on a corpus. A method of parameter\nestimation by searching an optimal $\\gamma$ is proposed, where a ``zeroth\nword'' is introduced technically for the calculation. The formulation and\nparameters are further discussed with several case studies.", "published": "2024-02-01 01:45:46", "link": "http://arxiv.org/abs/2402.00271v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bias in Opinion Summarisation from Pre-training to Adaptation: A Case\n  Study in Political Bias", "abstract": "Opinion summarisation aims to summarise the salient information and opinions\npresented in documents such as product reviews, discussion forums, and social\nmedia texts into short summaries that enable users to effectively understand\nthe opinions therein. Generating biased summaries has the risk of potentially\nswaying public opinion. Previous studies focused on studying bias in opinion\nsummarisation using extractive models, but limited research has paid attention\nto abstractive summarisation models. In this study, using political bias as a\ncase study, we first establish a methodology to quantify bias in abstractive\nmodels, then trace it from the pre-trained models to the task of summarising\nsocial media opinions using different models and adaptation methods. We find\nthat most models exhibit intrinsic bias. Using a social media text\nsummarisation dataset and contrasting various adaptation methods, we find that\ntuning a smaller number of parameters is less biased compared to standard\nfine-tuning; however, the diversity of topics in training data used for\nfine-tuning is critical.", "published": "2024-02-01 04:15:59", "link": "http://arxiv.org/abs/2402.00322v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IndiVec: An Exploration of Leveraging Large Language Models for Media\n  Bias Detection with Fine-Grained Bias Indicators", "abstract": "This study focuses on media bias detection, crucial in today's era of\ninfluential social media platforms shaping individual attitudes and opinions.\nIn contrast to prior work that primarily relies on training specific models\ntailored to particular datasets, resulting in limited adaptability and subpar\nperformance on out-of-domain data, we introduce a general bias detection\nframework, IndiVec, built upon large language models. IndiVec begins by\nconstructing a fine-grained media bias database, leveraging the robust\ninstruction-following capabilities of large language models and vector database\ntechniques. When confronted with new input for bias detection, our framework\nautomatically selects the most relevant indicator from the vector database and\nemploys majority voting to determine the input's bias label. IndiVec excels\ncompared to previous methods due to its adaptability (demonstrating consistent\nperformance across diverse datasets from various sources) and explainability\n(providing explicit top-k indicators to interpret bias predictions).\nExperimental results on four political bias datasets highlight IndiVec's\nsignificant superiority over baselines. Furthermore, additional experiments and\nanalysis provide profound insights into the framework's effectiveness.", "published": "2024-02-01 05:20:07", "link": "http://arxiv.org/abs/2402.00345v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Don't Hallucinate, Abstain: Identifying LLM Knowledge Gaps via Multi-LLM\n  Collaboration", "abstract": "Despite efforts to expand the knowledge of large language models (LLMs),\nknowledge gaps -- missing or outdated information in LLMs -- might always\npersist given the evolving nature of knowledge. In this work, we study\napproaches to identify LLM knowledge gaps and abstain from answering questions\nwhen knowledge gaps are present. We first adapt existing approaches to model\ncalibration or adaptation through fine-tuning/prompting and analyze their\nability to abstain from generating low-confidence outputs. Motivated by their\nfailures in self-reflection and over-reliance on held-out sets, we propose two\nnovel approaches that are based on model collaboration, i.e., LLMs probing\nother LLMs for knowledge gaps, either cooperatively or competitively. Extensive\nexperiments with three LLMs on four QA tasks featuring diverse knowledge\ndomains demonstrate that both cooperative and competitive approaches to\nunveiling LLM knowledge gaps achieve up to 19.3% improvements on abstain\naccuracy against the strongest baseline. Further analysis reveals that our\nproposed mechanisms could help identify failure cases in retrieval augmentation\nand pinpoint knowledge gaps in multi-hop reasoning.", "published": "2024-02-01 06:11:49", "link": "http://arxiv.org/abs/2402.00367v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What Does the Bot Say? Opportunities and Risks of Large Language Models\n  in Social Media Bot Detection", "abstract": "Social media bot detection has always been an arms race between advancements\nin machine learning bot detectors and adversarial bot strategies to evade\ndetection. In this work, we bring the arms race to the next level by\ninvestigating the opportunities and risks of state-of-the-art large language\nmodels (LLMs) in social bot detection. To investigate the opportunities, we\ndesign novel LLM-based bot detectors by proposing a\nmixture-of-heterogeneous-experts framework to divide and conquer diverse user\ninformation modalities. To illuminate the risks, we explore the possibility of\nLLM-guided manipulation of user textual and structured information to evade\ndetection. Extensive experiments with three LLMs on two datasets demonstrate\nthat instruction tuning on merely 1,000 annotated examples produces specialized\nLLMs that outperform state-of-the-art baselines by up to 9.1% on both datasets,\nwhile LLM-guided manipulation strategies could significantly bring down the\nperformance of existing bot detectors by up to 29.6% and harm the calibration\nand reliability of bot detection systems.", "published": "2024-02-01 06:21:19", "link": "http://arxiv.org/abs/2402.00371v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Computational Morphology and Lexicography Modeling of Modern Standard\n  Arabic Nominals", "abstract": "Modern Standard Arabic (MSA) nominals present many morphological and lexical\nmodeling challenges that have not been consistently addressed previously. This\npaper attempts to define the space of such challenges, and leverage a recently\nproposed morphological framework to build a comprehensive and extensible model\nfor MSA nominals. Our model design addresses the nominals' intricate\nmorphotactics, as well as their paradigmatic irregularities. Our implementation\nshowcases enhanced accuracy and consistency compared to a commonly used MSA\nmorphological analyzer and generator. We make our models publicly available.", "published": "2024-02-01 07:05:45", "link": "http://arxiv.org/abs/2402.00385v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Dialog Safety using Socially Aware Contrastive Learning", "abstract": "State-of-the-art conversational AI systems raise concerns due to their\npotential risks of generating unsafe, toxic, unethical, or dangerous content.\nPrevious works have developed datasets to teach conversational agents the\nappropriate social paradigms to respond effectively to specifically designed\nhazardous content. However, models trained on these adversarial datasets still\nstruggle to recognize subtle unsafe situations that appear naturally in\nconversations or introduce an inappropriate response in a casual context. To\nunderstand the extent of this problem, we study prosociality in both\nadversarial and casual dialog contexts and audit the response quality of\ngeneral-purpose language models in terms of propensity to produce unsafe\ncontent. We propose a dual-step fine-tuning process to address these issues\nusing a socially aware n-pair contrastive loss. Subsequently, we train a base\nmodel that integrates prosocial behavior by leveraging datasets like Moral\nIntegrity Corpus (MIC) and ProsocialDialog. Experimental results on several\ndialog datasets demonstrate the effectiveness of our approach in generating\nsocially appropriate responses.", "published": "2024-02-01 09:24:33", "link": "http://arxiv.org/abs/2402.00446v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Superfiltering: Weak-to-Strong Data Filtering for Fast\n  Instruction-Tuning", "abstract": "Instruction tuning is critical to improve LLMs but usually suffers from\nlow-quality and redundant data. Data filtering for instruction tuning has\nproved important in improving both the efficiency and performance of the tuning\nprocess. But it also leads to extra cost and computation due to the involvement\nof LLMs in this process. To reduce the filtering cost, we study Superfiltering:\nCan we use a smaller and weaker model to select data for finetuning a larger\nand stronger model? Despite the performance gap between weak and strong\nlanguage models, we find their highly consistent capability to perceive\ninstruction difficulty and data selection results. This enables us to use a\nmuch smaller and more efficient model to filter the instruction data used to\ntrain a larger language model. Not only does it largely speed up the data\nfiltering, but the filtered-data-finetuned LLM achieves even better performance\non standard benchmarks. Extensive experiments validate the efficacy and\nefficiency of our approach.", "published": "2024-02-01 11:57:53", "link": "http://arxiv.org/abs/2402.00530v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Chain-of-Thought Is as Strong as Its Weakest Link: A Benchmark for\n  Verifiers of Reasoning Chains", "abstract": "Prompting language models to provide step-by-step answers (e.g.,\n\"Chain-of-Thought\") is the prominent approach for complex reasoning tasks,\nwhere more accurate reasoning chains typically improve downstream task\nperformance. Recent literature discusses automatic methods to verify reasoning\nto evaluate and improve their correctness. However, no fine-grained step-level\ndatasets are available to enable thorough evaluation of such verification\nmethods, hindering progress in this direction. We introduce REVEAL: Reasoning\nVerification Evaluation, a dataset to benchmark automatic verifiers of complex\nChain-of-Thought reasoning in open-domain question-answering settings. REVEAL\nincludes comprehensive labels for the relevance, attribution to evidence\npassages, and logical correctness of each reasoning step in a language model's\nanswer, across a variety of datasets and state-of-the-art language models.\nEvaluation on REVEAL shows that verifiers struggle at verifying reasoning\nchains - in particular, verifying logical correctness and detecting\ncontradictions. Available at https://reveal-dataset.github.io/ .", "published": "2024-02-01 12:46:45", "link": "http://arxiv.org/abs/2402.00559v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Actor Identification in Discourse: A Challenge for LLMs?", "abstract": "The identification of political actors who put forward claims in public\ndebate is a crucial step in the construction of discourse networks, which are\nhelpful to analyze societal debates. Actor identification is, however, rather\nchallenging: Often, the locally mentioned speaker of a claim is only a pronoun\n(\"He proposed that [claim]\"), so recovering the canonical actor name requires\ndiscourse understanding. We compare a traditional pipeline of dedicated NLP\ncomponents (similar to those applied to the related task of coreference) with a\nLLM, which appears a good match for this generation task. Evaluating on a\ncorpus of German actors in newspaper reports, we find surprisingly that the LLM\nperforms worse. Further analysis reveals that the LLM is very good at\nidentifying the right reference, but struggles to generate the correct\ncanonical form. This points to an underlying issue in LLMs with controlling\ngenerated output. Indeed, a hybrid model combining the LLM with a classifier to\nnormalize its output substantially outperforms both initial models.", "published": "2024-02-01 14:30:39", "link": "http://arxiv.org/abs/2402.00620v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prosody in Cascade and Direct Speech-to-Text Translation: a case study\n  on Korean Wh-Phrases", "abstract": "Speech-to-Text Translation (S2TT) has typically been addressed with cascade\nsystems, where speech recognition systems generate a transcription that is\nsubsequently passed to a translation model. While there has been a growing\ninterest in developing direct speech translation systems to avoid propagating\nerrors and losing non-verbal content, prior work in direct S2TT has struggled\nto conclusively establish the advantages of integrating the acoustic signal\ndirectly into the translation process. This work proposes using contrastive\nevaluation to quantitatively measure the ability of direct S2TT systems to\ndisambiguate utterances where prosody plays a crucial role. Specifically, we\nevaluated Korean-English translation systems on a test set containing\nwh-phrases, for which prosodic features are necessary to produce translations\nwith the correct intent, whether it's a statement, a yes/no question, a\nwh-question, and more. Our results clearly demonstrate the value of direct\ntranslation systems over cascade translation models, with a notable 12.9%\nimprovement in overall accuracy in ambiguous cases, along with up to a 15.6%\nincrease in F1 scores for one of the major intent categories. To the best of\nour knowledge, this work stands as the first to provide quantitative evidence\nthat direct S2TT models can effectively leverage prosody. The code for our\nevaluation is openly accessible and freely available for review and\nutilisation.", "published": "2024-02-01 14:46:35", "link": "http://arxiv.org/abs/2402.00632v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Weak-to-Strong Generalization with Scalable Oversight and\n  Ensemble Learning", "abstract": "This paper presents a follow-up study to OpenAI's recent superalignment work\non Weak-to-Strong Generalization (W2SG). Superalignment focuses on ensuring\nthat high-level AI systems remain consistent with human values and intentions\nwhen dealing with complex, high-risk tasks. The W2SG framework has opened new\npossibilities for empirical research in this evolving field. Our study\nsimulates two phases of superalignment under the W2SG framework: the\ndevelopment of general superhuman models and the progression towards\nsuperintelligence. In the first phase, based on human supervision, the quality\nof weak supervision is enhanced through a combination of scalable oversight and\nensemble learning, reducing the capability gap between weak teachers and strong\nstudents. In the second phase, an automatic alignment evaluator is employed as\nthe weak supervisor. By recursively updating this auto aligner, the\ncapabilities of the weak teacher models are synchronously enhanced, achieving\nweak-to-strong supervision over stronger student models.We also provide an\ninitial validation of the proposed approach for the first phase. Using the SciQ\ntask as example, we explore ensemble learning for weak teacher models through\nbagging and boosting. Scalable oversight is explored through two auxiliary\nsettings: human-AI interaction and AI-AI debate. Additionally, the paper\ndiscusses the impact of improved weak supervision on enhancing weak-to-strong\ngeneralization based on in-context learning. Experiment code and dataset will\nbe released at https://github.com/ADaM-BJTU/W2SG.", "published": "2024-02-01 15:30:19", "link": "http://arxiv.org/abs/2402.00667v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Semantic Control in Discrete Latent Spaces with Transformer\n  Quantized Variational Autoencoders", "abstract": "Achieving precise semantic control over the latent spaces of Variational\nAutoEncoders (VAEs) holds significant value for downstream tasks in NLP as the\nunderlying generative mechanisms could be better localised, explained and\nimproved upon. Recent research, however, has struggled to achieve consistent\nresults, primarily due to the inevitable loss of semantic information in the\nvariational bottleneck and limited control over the decoding mechanism. To\novercome these challenges, we investigate discrete latent spaces in Vector\nQuantized Variational AutoEncoders (VQVAEs) to improve semantic control and\ngeneration in Transformer-based VAEs. In particular, We propose T5VQVAE, a\nnovel model that leverages the controllability of VQVAEs to guide the\nself-attention mechanism in T5 at the token-level, exploiting its full\ngeneralization capabilities. Experimental results indicate that T5VQVAE\noutperforms existing state-of-the-art VAE models, including Optimus, in terms\nof controllability and preservation of semantic information across different\ntasks such as auto-encoding of sentences and mathematical expressions, text\ntransfer, and inference. Moreover, T5VQVAE exhibits improved inference\ncapabilities, suggesting potential applications for downstream natural language\nand symbolic reasoning tasks.", "published": "2024-02-01 16:14:35", "link": "http://arxiv.org/abs/2402.00723v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Ethical Explanations of Large Language Models through\n  Iterative Symbolic Refinement", "abstract": "An increasing amount of research in Natural Language Inference (NLI) focuses\non the application and evaluation of Large Language Models (LLMs) and their\nreasoning capabilities. Despite their success, however, LLMs are still prone to\nfactual errors and inconsistencies in their explanations, offering limited\ncontrol and interpretability for inference in complex domains. In this paper,\nwe focus on ethical NLI, investigating how hybrid neuro-symbolic techniques can\nenhance the logical validity and alignment of ethical explanations produced by\nLLMs. Specifically, we present an abductive-deductive framework named\nLogic-Explainer, which integrates LLMs with an external backward-chaining\nsolver to refine step-wise natural language explanations and jointly verify\ntheir correctness, reduce incompleteness and minimise redundancy. An extensive\nempirical analysis demonstrates that Logic-Explainer can improve explanations\ngenerated via in-context learning methods and Chain-of-Thought (CoT) on\nchallenging ethical NLI tasks, while, at the same time, producing formal proofs\ndescribing and supporting models' reasoning. As ethical NLI requires\ncommonsense reasoning to identify underlying moral violations, our results\nsuggest the effectiveness of neuro-symbolic methods for multi-step NLI more\nbroadly, opening new opportunities to enhance the logical consistency,\nreliability, and alignment of LLMs.", "published": "2024-02-01 16:39:51", "link": "http://arxiv.org/abs/2402.00745v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Health-LLM: Personalized Retrieval-Augmented Disease Prediction System", "abstract": "Recent advancements in artificial intelligence (AI), especially large\nlanguage models (LLMs), have significantly advanced healthcare applications and\ndemonstrated potentials in intelligent medical treatment. However, there are\nconspicuous challenges such as vast data volumes and inconsistent symptom\ncharacterization standards, preventing full integration of healthcare AI\nsystems with individual patients' needs. To promote professional and\npersonalized healthcare, we propose an innovative framework, Heath-LLM, which\ncombines large-scale feature extraction and medical knowledge trade-off\nscoring. Compared to traditional health management applications, our system has\nthree main advantages: (1) It integrates health reports and medical knowledge\ninto a large model to ask relevant questions to large language model for\ndisease prediction; (2) It leverages a retrieval augmented generation (RAG)\nmechanism to enhance feature extraction; (3) It incorporates a semi-automated\nfeature updating framework that can merge and delete features to improve\naccuracy of disease prediction. We experiment on a large number of health\nreports to assess the effectiveness of Health-LLM system. The results indicate\nthat the proposed system surpasses the existing ones and has the potential to\nsignificantly advance disease prediction and personalized health management.", "published": "2024-02-01 16:40:32", "link": "http://arxiv.org/abs/2402.00746v8", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OLMo: Accelerating the Science of Language Models", "abstract": "Language models (LMs) have become ubiquitous in both NLP research and in\ncommercial product offerings. As their commercial importance has surged, the\nmost powerful models have become closed off, gated behind proprietary\ninterfaces, with important details of their training data, architectures, and\ndevelopment undisclosed. Given the importance of these details in\nscientifically studying these models, including their biases and potential\nrisks, we believe it is essential for the research community to have access to\npowerful, truly open LMs. To this end, we have built OLMo, a competitive, truly\nOpen Language Model, to enable the scientific study of language models. Unlike\nmost prior efforts that have only released model weights and inference code, we\nrelease OLMo alongside open training data and training and evaluation code. We\nhope this release will empower the open research community and inspire a new\nwave of innovation.", "published": "2024-02-01 18:28:55", "link": "http://arxiv.org/abs/2402.00838v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tiny Titans: Can Smaller Large Language Models Punch Above Their Weight\n  in the Real World for Meeting Summarization?", "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities to\nsolve a wide range of tasks without being explicitly fine-tuned on\ntask-specific datasets. However, deploying LLMs in the real world is not\ntrivial, as it requires substantial computing resources. In this paper, we\ninvestigate whether smaller, compact LLMs are a good alternative to the\ncomparatively Larger LLMs2 to address significant costs associated with\nutilizing LLMs in the real world. In this regard, we study the meeting\nsummarization task in a real-world industrial environment and conduct extensive\nexperiments by comparing the performance of fine-tuned compact LLMs (e.g.,\nFLAN-T5, TinyLLaMA, LiteLLaMA) with zero-shot larger LLMs (e.g., LLaMA-2,\nGPT-3.5, PaLM-2). We observe that most smaller LLMs, even after fine-tuning,\nfail to outperform larger zero-shot LLMs in meeting summarization datasets.\nHowever, a notable exception is FLAN-T5 (780M parameters), which performs on\npar or even better than many zero-shot Larger LLMs (from 7B to above 70B\nparameters), while being significantly smaller. This makes compact LLMs like\nFLAN-T5 a suitable cost-efficient solution for real-world industrial\ndeployment.", "published": "2024-02-01 18:31:34", "link": "http://arxiv.org/abs/2402.00841v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Efficient Exact Optimization of Language Model Alignment", "abstract": "The alignment of language models with human preferences is vital for their\napplication in real-world tasks. The problem is formulated as optimizing the\nmodel's policy to maximize the expected reward that reflects human preferences\nwith minimal deviation from the initial policy. While considered as a\nstraightforward solution, reinforcement learning (RL) suffers from high\nvariance in policy updates, which impedes efficient policy improvement.\nRecently, direct preference optimization (DPO) was proposed to directly\noptimize the policy from preference data. However, we show that DPO derived\nbased on the optimal solution of the problem leads to a compromised\nmean-seeking approximation of the optimal solution in practice. In this paper,\nwe propose efficient exact optimization (EXO) of the alignment objective. EXO\nis guaranteed to optimize in the same direction as RL algorithms asymptotically\nfor arbitrary policy parametrization. This leads to the same mode-seeking\nsolution, while enables efficient optimization by circumventing the\ncomplexities of RL. We also compare our method to DPO with both theoretical and\nempirical analyses, and further demonstrate the advantages of our method over\nexisting approaches on realistic human preference data. Code is available at\nhttps://github.com/haozheji/exact-optimization.", "published": "2024-02-01 18:51:54", "link": "http://arxiv.org/abs/2402.00856v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Large Language Models Understand Context?", "abstract": "Understanding context is key to understanding human language, an ability\nwhich Large Language Models (LLMs) have been increasingly seen to demonstrate\nto an impressive extent. However, though the evaluation of LLMs encompasses\nvarious domains within the realm of Natural Language Processing, limited\nattention has been paid to probing their linguistic capability of understanding\ncontextual features. This paper introduces a context understanding benchmark by\nadapting existing datasets to suit the evaluation of generative models. This\nbenchmark comprises of four distinct tasks and nine datasets, all featuring\nprompts designed to assess the models' ability to understand context. First, we\nevaluate the performance of LLMs under the in-context learning pretraining\nscenario. Experimental results indicate that pre-trained dense models struggle\nwith understanding more nuanced contextual features when compared to\nstate-of-the-art fine-tuned models. Second, as LLM compression holds growing\nsignificance in both research and real-world applications, we assess the\ncontext understanding of quantized models under in-context-learning settings.\nWe find that 3-bit post-training quantization leads to varying degrees of\nperformance reduction on our benchmark. We conduct an extensive analysis of\nthese scenarios to substantiate our experimental results.", "published": "2024-02-01 18:55:29", "link": "http://arxiv.org/abs/2402.00858v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Spatial Schema Intuitions in Large Language and Vision Models", "abstract": "Despite the ubiquity of large language models (LLMs) in AI research, the\nquestion of embodiment in LLMs remains underexplored, distinguishing them from\nembodied systems in robotics where sensory perception directly informs physical\naction. Our investigation navigates the intriguing terrain of whether LLMs,\ndespite their non-embodied nature, effectively capture implicit human\nintuitions about fundamental, spatial building blocks of language. We employ\ninsights from spatial cognitive foundations developed through early\nsensorimotor experiences, guiding our exploration through the reproduction of\nthree psycholinguistic experiments. Surprisingly, correlations between model\noutputs and human responses emerge, revealing adaptability without a tangible\nconnection to embodied experiences. Notable distinctions include polarized\nlanguage model responses and reduced correlations in vision language models.\nThis research contributes to a nuanced understanding of the interplay between\nlanguage, spatial experiences, and the computations made by large language\nmodels. More at https://cisnlp.github.io/Spatial_Schemas/", "published": "2024-02-01 19:25:50", "link": "http://arxiv.org/abs/2402.00956v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Graph-based Clustering for Detecting Semantic Change Across Time and\n  Languages", "abstract": "Despite the predominance of contextualized embeddings in NLP, approaches to\ndetect semantic change relying on these embeddings and clustering methods\nunderperform simpler counterparts based on static word embeddings. This stems\nfrom the poor quality of the clustering methods to produce sense clusters --\nwhich struggle to capture word senses, especially those with low frequency.\nThis issue hinders the next step in examining how changes in word senses in one\nlanguage influence another. To address this issue, we propose a graph-based\nclustering approach to capture nuanced changes in both high- and low-frequency\nword senses across time and languages, including the acquisition and loss of\nthese senses over time. Our experimental results show that our approach\nsubstantially surpasses previous approaches in the SemEval2020 binary\nclassification task across four languages. Moreover, we showcase the ability of\nour approach as a versatile visualization tool to detect semantic changes in\nboth intra-language and inter-language setups. We make our code and data\npublicly available.", "published": "2024-02-01 21:27:19", "link": "http://arxiv.org/abs/2402.01025v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Getting the most out of your tokenizer for pre-training and domain\n  adaptation", "abstract": "Tokenization is an understudied and often neglected component of modern LLMs.\nMost published works use a single tokenizer for all experiments, often borrowed\nfrom another model, without performing ablations or analysis to optimize\ntokenization. Moreover, the tokenizer is generally kept unchanged when\nfine-tuning a base model. In this paper, we show that the size,\npre-tokenization regular expression, and training data of a tokenizer can\nsignificantly impact the model's generation speed, effective context size,\nmemory usage, and downstream performance. We train specialized Byte-Pair\nEncoding code tokenizers, and conduct extensive ablations on the impact of\ntokenizer design on the performance of LLMs for code generation tasks such as\nHumanEval and MBPP, and provide recommendations for tokenizer hyper-parameters\nselection and switching the tokenizer in a pre-trained LLM. We perform our\nexperiments on models trained from scratch and from pre-trained models,\nverifying their applicability to a wide range of use-cases. We find that when\nfine-tuning on more than 50 billion tokens, we can specialize the tokenizer of\na pre-trained LLM to obtain large gains in generation speed and effective\ncontext size.", "published": "2024-02-01 21:49:34", "link": "http://arxiv.org/abs/2402.01035v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generation, Distillation and Evaluation of Motivational\n  Interviewing-Style Reflections with a Foundational Language Model", "abstract": "Large Foundational Language Models are capable of performing many tasks at a\nhigh level but are difficult to deploy in many applications because of their\nsize and proprietary ownership. Many will be motivated to distill specific\ncapabilities of foundational models into smaller models that can be owned and\ncontrolled. In the development of a therapeutic chatbot, we wish to distill a\ncapability known as reflective listening, in which a therapist produces\nreflections of client speech. These reflections either restate what a client\nhas said, or connect what was said to a relevant observation, idea or guess\nthat encourages and guides the client to continue contemplation. In this paper,\nwe present a method for distilling the generation of reflections from a\nFoundational Language Model (GPT-4) into smaller models. We first show that\nGPT-4, using zero-shot prompting, can generate reflections at near 100% success\nrate, superior to all previous methods. Using reflections generated by GPT-4,\nwe fine-tune different sizes of the GPT-2 family. The GPT-2-small model\nachieves 83% success on a hold-out test set and the GPT-2 XL achieves 90%\nsuccess. We also show that GPT-4 can help in the labor-intensive task of\nevaluating the quality of the distilled models, using it as a zero-shot\nclassifier. Using triple-human review as a guide, the classifier achieves a\nCohen-Kappa of 0.66, a substantial inter-rater reliability figure.", "published": "2024-02-01 22:54:31", "link": "http://arxiv.org/abs/2402.01051v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating Bias Representations in Llama 2 Chat via Activation\n  Steering", "abstract": "We address the challenge of societal bias in Large Language Models (LLMs),\nfocusing on the Llama 2 7B Chat model. As LLMs are increasingly integrated into\ndecision-making processes with substantial societal impact, it becomes\nimperative to ensure these models do not reinforce existing biases. Our\napproach employs activation steering to probe for and mitigate biases related\nto gender, race, and religion. This method manipulates model activations to\ndirect responses towards or away from biased outputs, utilizing steering\nvectors derived from the StereoSet dataset and custom GPT4 generated gender\nbias prompts. Our findings reveal inherent gender bias in Llama 2 7B Chat,\npersisting even after Reinforcement Learning from Human Feedback (RLHF). We\nalso observe a predictable negative correlation between bias and the model's\ntendency to refuse responses. Significantly, our study uncovers that RLHF tends\nto increase the similarity in the model's representation of different forms of\nsocietal biases, which raises questions about the model's nuanced understanding\nof different forms of bias. This work also provides valuable insights into\neffective red-teaming strategies for LLMs using activation steering,\nparticularly emphasizing the importance of integrating a refusal vector.", "published": "2024-02-01 07:48:50", "link": "http://arxiv.org/abs/2402.00402v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Hidding the Ghostwriters: An Adversarial Evaluation of AI-Generated\n  Student Essay Detection", "abstract": "Large language models (LLMs) have exhibited remarkable capabilities in text\ngeneration tasks. However, the utilization of these models carries inherent\nrisks, including but not limited to plagiarism, the dissemination of fake news,\nand issues in educational exercises. Although several detectors have been\nproposed to address these concerns, their effectiveness against adversarial\nperturbations, specifically in the context of student essay writing, remains\nlargely unexplored. This paper aims to bridge this gap by constructing\nAIG-ASAP, an AI-generated student essay dataset, employing a range of text\nperturbation methods that are expected to generate high-quality essays while\nevading detection. Through empirical experiments, we assess the performance of\ncurrent AIGC detectors on the AIG-ASAP dataset. The results reveal that the\nexisting detectors can be easily circumvented using straightforward automatic\nadversarial attacks. Specifically, we explore word substitution and sentence\nsubstitution perturbation methods that effectively evade detection while\nmaintaining the quality of the generated essays. This highlights the urgent\nneed for more accurate and robust methods to detect AI-generated student essays\nin the education domain.", "published": "2024-02-01 08:11:56", "link": "http://arxiv.org/abs/2402.00412v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Prompt-Time Symbolic Knowledge Capture with Large Language Models", "abstract": "Augmenting large language models (LLMs) with user-specific knowledge is\ncrucial for real-world applications, such as personal AI assistants. However,\nLLMs inherently lack mechanisms for prompt-driven knowledge capture. This paper\ninvestigates utilizing the existing LLM capabilities to enable prompt-driven\nknowledge capture, with a particular emphasis on knowledge graphs. We address\nthis challenge by focusing on prompt-to-triple (P2T) generation. We explore\nthree methods: zero-shot prompting, few-shot prompting, and fine-tuning, and\nthen assess their performance via a specialized synthetic dataset. Our code and\ndatasets are publicly available at https://github.com/HaltiaAI/paper-PTSKC.", "published": "2024-02-01 08:15:28", "link": "http://arxiv.org/abs/2402.00414v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Instruction Makes a Difference", "abstract": "We introduce Instruction Document Visual Question Answering (iDocVQA) dataset\nand Large Language Document (LLaDoc) model, for training Language-Vision (LV)\nmodels for document analysis and predictions on document images, respectively.\nUsually, deep neural networks for the DocVQA task are trained on datasets\nlacking instructions. We show that using instruction-following datasets\nimproves performance. We compare performance across document-related datasets\nusing the recent state-of-the-art (SotA) Large Language and Vision Assistant\n(LLaVA)1.5 as the base model. We also evaluate the performance of the derived\nmodels for object hallucination using the Polling-based Object Probing\nEvaluation (POPE) dataset. The results show that instruction-tuning performance\nranges from 11X to 32X of zero-shot performance and from 0.1% to 4.2% over\nnon-instruction (traditional task) finetuning. Despite the gains, these still\nfall short of human performance (94.36%), implying there's much room for\nimprovement.", "published": "2024-02-01 09:43:30", "link": "http://arxiv.org/abs/2402.00453v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "SA-MDKIF: A Scalable and Adaptable Medical Domain Knowledge Injection\n  Framework for Large Language Models", "abstract": "Recent advances in large language models (LLMs) have demonstrated exceptional\nperformance in various natural language processing (NLP) tasks. However, their\neffective application in the medical domain is hampered by a lack of medical\ndomain knowledge. In this study, we present SA-MDKIF, a scalable and adaptable\nframework that aims to inject medical knowledge into general-purpose LLMs\nthrough instruction tuning, thereby enabling adaptability for various\ndownstream tasks. SA-MDKIF consists of two stages: skill training and skill\nadaptation. In the first stage, we define 12 basic medical skills and use\nAdaLoRA to train these skills based on uniformly formatted instructional\ndatasets that we have constructed. In the next stage, we train the skill router\nusing task-specific downstream data and use this router to integrate the\nacquired skills with LLMs during inference. Experimental results on 9 different\nmedical tasks show that SA-MDKIF improves performance by 10-20% compared to the\noriginal LLMs. Notably, this improvement is particularly pronounced for unseen\nmedical tasks, showing an improvement of up to 30%.", "published": "2024-02-01 10:26:27", "link": "http://arxiv.org/abs/2402.00474v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning Planning-based Reasoning by Trajectories Collection and Process\n  Reward Synthesizing", "abstract": "Large Language Models (LLMs) have demonstrated significant potential in\nhandling complex reasoning tasks through step-by-step rationale generation.\nHowever, recent studies have raised concerns regarding the hallucination and\nflaws in their reasoning process. Substantial efforts are being made to improve\nthe reliability and faithfulness of the generated rationales. Some approaches\nmodel reasoning as planning, while others focus on annotating for process\nsupervision. Nevertheless, the planning-based search process often results in\nhigh latency due to the frequent assessment of intermediate reasoning states\nand the extensive exploration space. Additionally, supervising the reasoning\nprocess with human annotation is costly and challenging to scale for LLM\ntraining. To address these issues, in this paper, we propose a framework to\nlearn planning-based reasoning through Direct Preference Optimization (DPO) on\ncollected trajectories, which are ranked according to synthesized process\nrewards. Our results on challenging logical reasoning benchmarks demonstrate\nthe effectiveness of our learning framework, showing that our 7B model can\nsurpass the strong counterparts like GPT-3.5-Turbo.", "published": "2024-02-01 15:18:33", "link": "http://arxiv.org/abs/2402.00658v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Explaining Text Classifiers with Counterfactual Representations", "abstract": "One well motivated explanation method for classifiers leverages\ncounterfactuals which are hypothetical events identical to real observations in\nall aspects except for one feature. Constructing such counterfactual poses\nspecific challenges for texts, however, as some attribute values may not\nnecessarily align with plausible real-world events. In this paper we propose a\nsimple method for generating counterfactuals by intervening in the space of\ntext representations which bypasses this limitation. We argue that our\ninterventions are minimally disruptive and that they are theoretically sound as\nthey align with counterfactuals as defined in Pearl's causal inference\nframework. To validate our method, we conducted experiments first on a\nsynthetic dataset and then on a realistic dataset of counterfactuals. This\nallows for a direct comparison between classifier predictions based on ground\ntruth counterfactuals - obtained through explicit text interventions - and our\ncounterfactuals, derived through interventions in the representation space.\nEventually, we study a real world scenario where our counterfactuals can be\nleveraged both for explaining a classifier and for bias mitigation.", "published": "2024-02-01 16:06:35", "link": "http://arxiv.org/abs/2402.00711v3", "categories": ["cs.LG", "cs.CL", "62Fxx"], "primary_category": "cs.LG"}
{"title": "Transforming and Combining Rewards for Aligning Large Language Models", "abstract": "A common approach for aligning language models to human preferences is to\nfirst learn a reward model from preference data, and then use this reward model\nto update the language model. We study two closely related problems that arise\nin this approach. First, any monotone transformation of the reward model\npreserves preference ranking; is there a choice that is ``better'' than others?\nSecond, we often wish to align language models to multiple properties: how\nshould we combine multiple reward models? Using a probabilistic interpretation\nof the alignment procedure, we identify a natural choice for transformation for\n(the common case of) rewards learned from Bradley-Terry preference models. The\nderived transformation is straightforward: we apply a log-sigmoid function to\nthe centered rewards, a method we term ``LSC-transformation''\n(log-sigmoid-centered transformation). This transformation has two important\nproperties. First, it emphasizes improving poorly-performing outputs, rather\nthan outputs that already score well. This mitigates both underfitting (where\nsome prompts are not improved) and reward hacking (where the model learns to\nexploit misspecification of the reward model). Second, it enables principled\naggregation of rewards by linking summation to logical conjunction: the sum of\ntransformed rewards corresponds to the probability that the output is ``good''\nin all measured properties, in a sense we make precise. Experiments aligning\nlanguage models to be both helpful and harmless using RLHF show substantial\nimprovements over the baseline (non-transformed) approach.", "published": "2024-02-01 16:39:28", "link": "http://arxiv.org/abs/2402.00742v2", "categories": ["cs.CL", "cs.AI", "68T50", "I.2"], "primary_category": "cs.CL"}
{"title": "CroissantLLM: A Truly Bilingual French-English Language Model", "abstract": "We introduce CroissantLLM, a 1.3B language model pretrained on a set of 3T\nEnglish and French tokens, to bring to the research and industrial community a\nhigh-performance, fully open-sourced bilingual model that runs swiftly on\nconsumer-grade local hardware. To that end, we pioneer the approach of training\nan intrinsically bilingual model with a 1:1 English-to-French pretraining data\nratio, a custom tokenizer, and bilingual finetuning datasets. We release the\ntraining dataset, notably containing a French split with manually curated,\nhigh-quality, and varied data sources. To assess performance outside of\nEnglish, we craft a novel benchmark, FrenchBench, consisting of an array of\nclassification and generation tasks, covering various orthogonal aspects of\nmodel performance in the French Language. Additionally, rooted in transparency\nand to foster further Large Language Model research, we release codebases, and\ndozens of checkpoints across various model sizes, training data distributions,\nand training steps, as well as fine-tuned Chat models, and strong translation\nmodels. We evaluate our model through the FMTI framework, and validate 81 % of\nthe transparency criteria, far beyond the scores of even most open initiatives.\nThis work enriches the NLP landscape, breaking away from previous\nEnglish-centric work in order to strengthen our understanding of\nmultilinguality in language models.", "published": "2024-02-01 17:17:55", "link": "http://arxiv.org/abs/2402.00786v5", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluating Large Language Models for Generalization and Robustness via\n  Data Compression", "abstract": "Existing methods for evaluating large language models face challenges such as\ndata contamination, sensitivity to prompts, and the high cost of benchmark\ncreation. To address this, we propose a lossless data compression based\nevaluation approach that tests how models' predictive abilities generalize\nafter their training cutoff. Specifically, we collect comprehensive test data\nspanning 83 months from 2017 to 2023 and split the data into training and\ntesting periods according to models' training data cutoff. We measure: 1) the\ncompression performance on the testing period as a measure of generalization on\nunseen data; and 2) the performance gap between the training and testing period\nas a measure of robustness. Our experiments test 14 representative large\nlanguage models with various sizes on sources including Wikipedia, news\narticles, code, arXiv papers, and multi-modal data. We find that the\ncompression rate of many models reduces significantly after their cutoff date,\nbut models such as Mistral and Llama-2 demonstrate a good balance between\nperformance and robustness. Results also suggest that models struggle to\ngeneralize on news and code data, but work especially well on arXiv papers. We\nalso find the context size and tokenization implementation have a big impact of\non the overall compression performance.", "published": "2024-02-01 18:56:18", "link": "http://arxiv.org/abs/2402.00861v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "HR-MultiWOZ: A Task Oriented Dialogue (TOD) Dataset for HR LLM Agent", "abstract": "Recent advancements in Large Language Models (LLMs) have been reshaping\nNatural Language Processing (NLP) task in several domains. Their use in the\nfield of Human Resources (HR) has still room for expansions and could be\nbeneficial for several time consuming tasks. Examples such as time-off\nsubmissions, medical claims filing, and access requests are noteworthy, but\nthey are by no means the sole instances. However, the aforementioned\ndevelopments must grapple with the pivotal challenge of constructing a\nhigh-quality training dataset. On one hand, most conversation datasets are\nsolving problems for customers not employees. On the other hand, gathering\nconversations with HR could raise privacy concerns. To solve it, we introduce\nHR-Multiwoz, a fully-labeled dataset of 550 conversations spanning 10 HR\ndomains to evaluate LLM Agent. Our work has the following contributions: (1) It\nis the first labeled open-sourced conversation dataset in the HR domain for NLP\nresearch. (2) It provides a detailed recipe for the data generation procedure\nalong with data analysis and human evaluations. The data generation pipeline is\ntransferable and can be easily adapted for labeled conversation data generation\nin other domains. (3) The proposed data-collection pipeline is mostly based on\nLLMs with minimal human involvement for annotation, which is time and\ncost-efficient.", "published": "2024-02-01 21:10:44", "link": "http://arxiv.org/abs/2402.01018v1", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Executable Code Actions Elicit Better LLM Agents", "abstract": "Large Language Model (LLM) agents, capable of performing a broad range of\nactions, such as invoking tools and controlling robots, show great potential in\ntackling real-world challenges. LLM agents are typically prompted to produce\nactions by generating JSON or text in a pre-defined format, which is usually\nlimited by constrained action space (e.g., the scope of pre-defined tools) and\nrestricted flexibility (e.g., inability to compose multiple tools). This work\nproposes to use executable Python code to consolidate LLM agents' actions into\na unified action space (CodeAct). Integrated with a Python interpreter, CodeAct\ncan execute code actions and dynamically revise prior actions or emit new\nactions upon new observations through multi-turn interactions. Our extensive\nanalysis of 17 LLMs on API-Bank and a newly curated benchmark shows that\nCodeAct outperforms widely used alternatives (up to 20% higher success rate).\nThe encouraging performance of CodeAct motivates us to build an open-source LLM\nagent that interacts with environments by executing interpretable code and\ncollaborates with users using natural language. To this end, we collect an\ninstruction-tuning dataset CodeActInstruct that consists of 7k multi-turn\ninteractions using CodeAct. We show that it can be used with existing data to\nimprove models in agent-oriented tasks without compromising their general\ncapability. CodeActAgent, finetuned from Llama2 and Mistral, is integrated with\nPython interpreter and uniquely tailored to perform sophisticated tasks (e.g.,\nmodel training) using existing libraries and autonomously self-debug.", "published": "2024-02-01 21:38:58", "link": "http://arxiv.org/abs/2402.01030v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Plan-Grounded Large Language Models for Dual Goal Conversational\n  Settings", "abstract": "Training Large Language Models (LLMs) to follow user instructions has been\nshown to supply the LLM with ample capacity to converse fluently while being\naligned with humans. Yet, it is not completely clear how an LLM can lead a\nplan-grounded conversation in mixed-initiative settings where instructions flow\nin both directions of the conversation, i.e. both the LLM and the user provide\ninstructions to one another. In this paper, we tackle a dual goal\nmixed-initiative conversational setting where the LLM not only grounds the\nconversation on an arbitrary plan but also seeks to satisfy both a procedural\nplan and user instructions. The LLM is then responsible for guiding the user\nthrough the plan and, at the same time, adapting to new circumstances,\nanswering questions, and activating safety guardrails when needed. We propose a\nnovel LLM that grounds the dialogue on a procedural plan, can take the dialogue\ninitiative, and enforces guardrails on the system's behavior, while also\nimproving the LLM's responses to unexpected user behavior. Experiments in\ncontrolled settings and with real users show that the best-performing model,\nwhich we call PlanLLM, achieves a 2.1x improvement over a strong baseline.\nMoreover, experiments also show good generalization to unseen domains.", "published": "2024-02-01 22:56:39", "link": "http://arxiv.org/abs/2402.01053v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evaluation Methodology for Large Language Models for Multilingual\n  Document Question and Answer", "abstract": "With the widespread adoption of Large Language Models (LLMs), in this paper\nwe investigate the multilingual capability of these models. Our preliminary\nresults show that, translating the native language context, question and answer\ninto a high resource language produced the best results.", "published": "2024-02-01 23:46:05", "link": "http://arxiv.org/abs/2402.01065v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Redefining \"Hallucination\" in LLMs: Towards a psychology-informed\n  framework for mitigating misinformation", "abstract": "In recent years, large language models (LLMs) have become incredibly popular,\nwith ChatGPT for example being used by over a billion users. While these models\nexhibit remarkable language understanding and logical prowess, a notable\nchallenge surfaces in the form of \"hallucinations.\" This phenomenon results in\nLLMs outputting misinformation in a confident manner, which can lead to\ndevastating consequences with such a large user base. However, we question the\nappropriateness of the term \"hallucination\" in LLMs, proposing a psychological\ntaxonomy based on cognitive biases and other psychological phenomena. Our\napproach offers a more fine-grained understanding of this phenomenon, allowing\nfor targeted solutions. By leveraging insights from how humans internally\nresolve similar challenges, we aim to develop strategies to mitigate LLM\nhallucinations. This interdisciplinary approach seeks to move beyond\nconventional terminology, providing a nuanced understanding and actionable\npathways for improvement in LLM reliability.", "published": "2024-02-01 03:01:11", "link": "http://arxiv.org/abs/2402.01769v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Efficient Non-Parametric Uncertainty Quantification for Black-Box Large\n  Language Models and Decision Planning", "abstract": "Step-by-step decision planning with large language models (LLMs) is gaining\nattention in AI agent development. This paper focuses on decision planning with\nuncertainty estimation to address the hallucination problem in language models.\nExisting approaches are either white-box or computationally demanding, limiting\nuse of black-box proprietary LLMs within budgets. The paper's first\ncontribution is a non-parametric uncertainty quantification method for LLMs,\nefficiently estimating point-wise dependencies between input-decision on the\nfly with a single inference, without access to token logits. This estimator\ninforms the statistical interpretation of decision trustworthiness. The second\ncontribution outlines a systematic design for a decision-making agent,\ngenerating actions like ``turn on the bathroom light'' based on user prompts\nsuch as ``take a bath''. Users will be asked to provide preferences when more\nthan one action has high estimated point-wise dependencies. In conclusion, our\nuncertainty estimation and decision-making agent design offer a cost-efficient\napproach for AI agent development.", "published": "2024-02-01 00:23:31", "link": "http://arxiv.org/abs/2402.00251v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A Survey on Hallucination in Large Vision-Language Models", "abstract": "Recent development of Large Vision-Language Models (LVLMs) has attracted\ngrowing attention within the AI landscape for its practical implementation\npotential. However, ``hallucination'', or more specifically, the misalignment\nbetween factual visual content and corresponding textual generation, poses a\nsignificant challenge of utilizing LVLMs. In this comprehensive survey, we\ndissect LVLM-related hallucinations in an attempt to establish an overview and\nfacilitate future mitigation. Our scrutiny starts with a clarification of the\nconcept of hallucinations in LVLMs, presenting a variety of hallucination\nsymptoms and highlighting the unique challenges inherent in LVLM\nhallucinations. Subsequently, we outline the benchmarks and methodologies\ntailored specifically for evaluating hallucinations unique to LVLMs.\nAdditionally, we delve into an investigation of the root causes of these\nhallucinations, encompassing insights from the training data and model\ncomponents. We also critically review existing methods for mitigating\nhallucinations. The open questions and future directions pertaining to\nhallucinations within LVLMs are discussed to conclude this survey.", "published": "2024-02-01 00:33:21", "link": "http://arxiv.org/abs/2402.00253v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "From PARIS to LE-PARIS: Toward Patent Response Automation with\n  Recommender Systems and Collaborative Large Language Models", "abstract": "In patent prosecution, timely and effective responses to Office Actions (OAs)\nare crucial for securing patents. However, past automation and artificial\nintelligence research have largely overlooked this aspect. To bridge this gap,\nour study introduces the Patent Office Action Response Intelligence System\n(PARIS) and its advanced version, the Large Language Model (LLM) Enhanced PARIS\n(LE-PARIS). These systems are designed to enhance the efficiency of patent\nattorneys in handling OA responses through collaboration with AI. The systems'\nkey features include the construction of an OA Topics Database, development of\nResponse Templates, and implementation of Recommender Systems and LLM-based\nResponse Generation. To validate the effectiveness of the systems, we have\nemployed a multi-paradigm analysis using the USPTO Office Action database and\nlongitudinal data based on attorney interactions with our systems over six\nyears. Through five studies, we have examined the constructiveness of OA topics\n(studies 1 and 2) using topic modeling and our proposed Delphi process, the\nefficacy of our proposed hybrid LLM-based recommender system tailored for OA\nresponses (study 3), the quality of generated responses (study 4), and the\nsystems' practical value in real-world scenarios through user studies (study\n5). The results indicate that both PARIS and LE-PARIS significantly achieve key\nmetrics and have a positive impact on attorney performance.", "published": "2024-02-01 08:37:13", "link": "http://arxiv.org/abs/2402.00421v2", "categories": ["cs.CL", "cs.HC", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "EE-Tuning: An Economical yet Scalable Solution for Tuning Early-Exit\n  Large Language Models", "abstract": "This work introduces EE-Tuning, a lightweight and economical solution to\ntraining/tuning early-exit large language models (LLMs). In contrast to the\ncommon approach of full-parameter pre-training, EE-Tuning augments any\npre-trained (and possibly fine-tuned) standard LLM with additional early-exit\nlayers that are tuned in a parameter-efficient manner, which requires\nsignificantly less computational resources and training data. Our\nimplementation of EE-Tuning achieves outstanding training efficiency via\nextensive performance optimizations, as well as scalability due to its full\ncompatibility with 3D parallelism. Results of systematic experiments validate\nthe efficacy of EE-Tuning, confirming that effective early-exit LLM inference\ncan be achieved with a limited training budget. In hope of making early-exit\nLLMs accessible to the community, we release the source code of our\nimplementation of EE-Tuning at https://github.com/pan-x-c/EE-LLM.", "published": "2024-02-01 11:39:04", "link": "http://arxiv.org/abs/2402.00518v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Non-Exchangeable Conformal Language Generation with Nearest Neighbors", "abstract": "Quantifying uncertainty in automatically generated text is important for\nletting humans check potential hallucinations and making systems more reliable.\nConformal prediction is an attractive framework to provide predictions imbued\nwith statistical guarantees, however, its application to text generation is\nchallenging since any i.i.d. assumptions are not realistic. In this paper, we\nbridge this gap by leveraging recent results on non-exchangeable conformal\nprediction, which still ensures bounds on coverage. The result,\nnon-exchangeable conformal nucleus sampling, is a novel extension of the\nconformal prediction framework to generation based on nearest neighbors. Our\nmethod can be used post-hoc for an arbitrary model without extra training and\nsupplies token-level, calibrated prediction sets equipped with statistical\nguarantees. Experiments in machine translation and language modeling show\nencouraging results in generation quality. By also producing tighter prediction\nsets with good coverage, we thus give a more theoretically principled way to\nperform sampling with conformal guarantees.", "published": "2024-02-01 16:04:04", "link": "http://arxiv.org/abs/2402.00707v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Theoretical Understanding of In-Context Learning in Shallow Transformers\n  with Unstructured Data", "abstract": "Large language models (LLMs) are powerful models that can learn concepts at\nthe inference stage via in-context learning (ICL). While theoretical studies,\ne.g., \\cite{zhang2023trained}, attempt to explain the mechanism of ICL, they\nassume the input $x_i$ and the output $y_i$ of each demonstration example are\nin the same token (i.e., structured data). However, in real practice, the\nexamples are usually text input, and all words, regardless of their logic\nrelationship, are stored in different tokens (i.e., unstructured data\n\\cite{wibisono2023role}). To understand how LLMs learn from the unstructured\ndata in ICL, this paper studies the role of each component in the transformer\narchitecture and provides a theoretical understanding to explain the success of\nthe architecture. In particular, we consider a simple transformer with one/two\nattention layers and linear regression tasks for the ICL prediction. We observe\nthat (1) a transformer with two layers of (self-)attentions with a look-ahead\nattention mask can learn from the prompt in the unstructured data, and (2)\npositional encoding can match the $x_i$ and $y_i$ tokens to achieve a better\nICL performance.", "published": "2024-02-01 16:39:45", "link": "http://arxiv.org/abs/2402.00743v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "BATON: Aligning Text-to-Audio Model with Human Preference Feedback", "abstract": "With the development of AI-Generated Content (AIGC), text-to-audio models are\ngaining widespread attention. However, it is challenging for these models to\ngenerate audio aligned with human preference due to the inherent information\ndensity of natural language and limited model understanding ability. To\nalleviate this issue, we formulate the BATON, a framework designed to enhance\nthe alignment between generated audio and text prompt using human preference\nfeedback. Our BATON comprises three key stages: Firstly, we curated a dataset\ncontaining both prompts and the corresponding generated audio, which was then\nannotated based on human feedback. Secondly, we introduced a reward model using\nthe constructed dataset, which can mimic human preference by assigning rewards\nto input text-audio pairs. Finally, we employed the reward model to fine-tune\nan off-the-shelf text-to-audio model. The experiment results demonstrate that\nour BATON can significantly improve the generation quality of the original\ntext-to-audio models, concerning audio integrity, temporal relationship, and\nalignment with human preference.", "published": "2024-02-01 16:39:47", "link": "http://arxiv.org/abs/2402.00744v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ReAGent: A Model-agnostic Feature Attribution Method for Generative\n  Language Models", "abstract": "Feature attribution methods (FAs), such as gradients and attention, are\nwidely employed approaches to derive the importance of all input features to\nthe model predictions. Existing work in natural language processing has mostly\nfocused on developing and testing FAs for encoder-only language models (LMs) in\nclassification tasks. However, it is unknown if it is faithful to use these FAs\nfor decoder-only models on text generation, due to the inherent differences\nbetween model architectures and task settings respectively. Moreover, previous\nwork has demonstrated that there is no `one-wins-all' FA across models and\ntasks. This makes the selection of a FA computationally expensive for large LMs\nsince input importance derivation often requires multiple forward and backward\npasses including gradient computations that might be prohibitive even with\naccess to large compute. To address these issues, we present a model-agnostic\nFA for generative LMs called Recursive Attribution Generator (ReAGent). Our\nmethod updates the token importance distribution in a recursive manner. For\neach update, we compute the difference in the probability distribution over the\nvocabulary for predicting the next token between using the original input and\nusing a modified version where a part of the input is replaced with RoBERTa\npredictions. Our intuition is that replacing an important token in the context\nshould have resulted in a larger change in the model's confidence in predicting\nthe token than replacing an unimportant token. Our method can be universally\napplied to any generative LM without accessing internal model weights or\nadditional training and fine-tuning, as most other FAs require. We extensively\ncompare the faithfulness of ReAGent with seven popular FAs across six\ndecoder-only LMs of various sizes. The results show that our method\nconsistently provides more faithful token importance distributions.", "published": "2024-02-01 17:25:51", "link": "http://arxiv.org/abs/2402.00794v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Formal-LLM: Integrating Formal Language and Natural Language for\n  Controllable LLM-based Agents", "abstract": "Recent advancements on Large Language Models (LLMs) enable AI Agents to\nautomatically generate and execute multi-step plans to solve complex tasks.\nHowever, since LLM's content generation process is hardly controllable, current\nLLM-based agents frequently generate invalid or non-executable plans, which\njeopardizes the performance of the generated plans and corrupts users' trust in\nLLM-based agents. In response, this paper proposes a novel \"Formal-LLM\"\nframework for LLM-based agents by integrating the expressiveness of natural\nlanguage and the precision of formal language. Specifically, the framework\nallows agent developers to express their requirements or constraints for the\nplanning process as an automaton. A stack-based LLM plan generation process is\nthen conducted under the supervision of the automaton to ensure that the\ngenerated plan satisfies the constraints, making the planning process\ncontrollable. We conduct experiments on both benchmark tasks and practical\nreal-life tasks, and our framework achieves over 50% overall performance\nincrease, which validates the feasibility and effectiveness of employing\nFormal-LLM to guide the plan generation of agents, preventing the agents from\ngenerating invalid and unsuccessful plans. Further, more controllable LLM-based\nagents can facilitate the broader utilization of LLM in application scenarios\nwhere high validity of planning is essential. The source code of this work is\navailable at https://github.com/agiresearch/Formal-LLM.", "published": "2024-02-01 17:30:50", "link": "http://arxiv.org/abs/2402.00798v4", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.FL"], "primary_category": "cs.LG"}
{"title": "ALISON: Fast and Effective Stylometric Authorship Obfuscation", "abstract": "Authorship Attribution (AA) and Authorship Obfuscation (AO) are two competing\ntasks of increasing importance in privacy research. Modern AA leverages an\nauthor's consistent writing style to match a text to its author using an AA\nclassifier. AO is the corresponding adversarial task, aiming to modify a text\nin such a way that its semantics are preserved, yet an AA model cannot\ncorrectly infer its authorship. To address privacy concerns raised by\nstate-of-the-art (SOTA) AA methods, new AO methods have been proposed but\nremain largely impractical to use due to their prohibitively slow training and\nobfuscation speed, often taking hours. To this challenge, we propose a\npractical AO method, ALISON, that (1) dramatically reduces training/obfuscation\ntime, demonstrating more than 10x faster obfuscation than SOTA AO methods, (2)\nachieves better obfuscation success through attacking three transformer-based\nAA methods on two benchmark datasets, typically performing 15% better than\ncompeting methods, (3) does not require direct signals from a target AA\nclassifier during obfuscation, and (4) utilizes unique stylometric features,\nallowing sound model interpretation for explainable obfuscation. We also\ndemonstrate that ALISON can effectively prevent four SOTA AA methods from\naccurately determining the authorship of ChatGPT-generated texts, all while\nminimally changing the original text semantics. To ensure the reproducibility\nof our findings, our code and data are available at:\nhttps://github.com/EricX003/ALISON.", "published": "2024-02-01 18:22:32", "link": "http://arxiv.org/abs/2402.00835v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; I.2.0"], "primary_category": "cs.CL"}
{"title": "Institutional Platform for Secure Self-Service Large Language Model\n  Exploration", "abstract": "This paper introduces a user-friendly platform developed by the University of\nKentucky Center for Applied AI, designed to make large, customized language\nmodels (LLMs) more accessible. By capitalizing on recent advancements in\nmulti-LoRA inference, the system efficiently accommodates custom adapters for a\ndiverse range of users and projects. The paper outlines the system's\narchitecture and key features, encompassing dataset curation, model training,\nsecure inference, and text-based feature extraction.\n  We illustrate the establishment of a tenant-aware computational network using\nagent-based methods, securely utilizing islands of isolated resources as a\nunified system. The platform strives to deliver secure LLM services,\nemphasizing process and data isolation, end-to-end encryption, and role-based\nresource authentication. This contribution aligns with the overarching goal of\nenabling simplified access to cutting-edge AI models and technology in support\nof scientific discovery.", "published": "2024-02-01 10:58:10", "link": "http://arxiv.org/abs/2402.00913v3", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "SPARQL Generation with Entity Pre-trained GPT for KG Question Answering", "abstract": "Knowledge Graphs popularity has been rapidly growing in last years. All that\nknowledge is available for people to query it through the many online databases\non the internet. Though, it would be a great achievement if non-programmer\nusers could access whatever information they want to know. There has been a lot\nof effort oriented to solve this task using natural language processing tools\nand creativity encouragement by way of many challenges. Our approach focuses on\nassuming a correct entity linking on the natural language questions and\ntraining a GPT model to create SPARQL queries from them. We managed to isolate\nwhich property of the task can be the most difficult to solve at few or\nzero-shot and we proposed pre-training on all entities (under CWA) to improve\nthe performance. We obtained a 62.703% accuracy of exact SPARQL matches on\ntesting at 3-shots, a F1 of 0.809 on the entity linking challenge and a F1 of\n0.009 on the question answering challenge.", "published": "2024-02-01 19:38:32", "link": "http://arxiv.org/abs/2402.00969v1", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR", "68P20, 68T50", "H.2.3; H.3.3; I.2.7"], "primary_category": "cs.CL"}
{"title": "An Information-Theoretic Approach to Analyze NLP Classification Tasks", "abstract": "Understanding the importance of the inputs on the output is useful across\nmany tasks. This work provides an information-theoretic framework to analyse\nthe influence of inputs for text classification tasks. Natural language\nprocessing (NLP) tasks take either a single element input or multiple element\ninputs to predict an output variable, where an element is a block of text. Each\ntext element has two components: an associated semantic meaning and a\nlinguistic realization. Multiple-choice reading comprehension (MCRC) and\nsentiment classification (SC) are selected to showcase the framework. For MCRC,\nit is found that the context influence on the output compared to the question\ninfluence reduces on more challenging datasets. In particular, more challenging\ncontexts allow a greater variation in complexity of questions. Hence, test\ncreators need to carefully consider the choice of the context when designing\nmultiple-choice questions for assessment. For SC, it is found the semantic\nmeaning of the input text dominates (above 80\\% for all datasets considered)\ncompared to its linguistic realisation when determining the sentiment. The\nframework is made available at:\nhttps://github.com/WangLuran/nlp-element-influence", "published": "2024-02-01 19:49:44", "link": "http://arxiv.org/abs/2402.00978v1", "categories": ["cs.CL", "cs.AI", "cs.IT", "math.IT"], "primary_category": "cs.CL"}
{"title": "Domain-Independent Deception: A New Taxonomy and Linguistic Analysis", "abstract": "Internet-based economies and societies are drowning in deceptive attacks.\nThese attacks take many forms, such as fake news, phishing, and job scams,\nwhich we call ``domains of deception.'' Machine-learning and\nnatural-language-processing researchers have been attempting to ameliorate this\nprecarious situation by designing domain-specific detectors. Only a few recent\nworks have considered domain-independent deception. We collect these disparate\nthreads of research and investigate domain-independent deception. First, we\nprovide a new computational definition of deception and break down deception\ninto a new taxonomy. Then, we analyze the debate on linguistic cues for\ndeception and supply guidelines for systematic reviews. Finally, we investigate\ncommon linguistic features and give evidence for knowledge transfer across\ndifferent forms of deception.", "published": "2024-02-01 21:13:04", "link": "http://arxiv.org/abs/2402.01019v1", "categories": ["cs.CL", "cs.CR", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Repeat After Me: Transformers are Better than State Space Models at\n  Copying", "abstract": "Transformers are the dominant architecture for sequence modeling, but there\nis growing interest in models that use a fixed-size latent state that does not\ndepend on the sequence length, which we refer to as \"generalized state space\nmodels\" (GSSMs). In this paper we show that while GSSMs are promising in terms\nof inference-time efficiency, they are limited compared to transformer models\non tasks that require copying from the input context. We start with a\ntheoretical analysis of the simple task of string copying and prove that a two\nlayer transformer can copy strings of exponential length while GSSMs are\nfundamentally limited by their fixed-size latent state. Empirically, we find\nthat transformers outperform GSSMs in terms of efficiency and generalization on\nsynthetic tasks that require copying the context. Finally, we evaluate\npretrained large language models and find that transformer models dramatically\noutperform state space models at copying and retrieving information from\ncontext. Taken together, these results suggest a fundamental gap between\ntransformers and GSSMs on tasks of practical interest.", "published": "2024-02-01 21:44:11", "link": "http://arxiv.org/abs/2402.01032v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "HiQA: A Hierarchical Contextual Augmentation RAG for Multi-Documents QA", "abstract": "Retrieval-augmented generation (RAG) has rapidly advanced the language model\nfield, particularly in question-answering (QA) systems. By integrating external\ndocuments during the response generation phase, RAG significantly enhances the\naccuracy and reliability of language models. This method elevates the quality\nof responses and reduces the frequency of hallucinations, where the model\ngenerates incorrect or misleading information. However, these methods exhibit\nlimited retrieval accuracy when faced with numerous indistinguishable\ndocuments, presenting notable challenges in their practical application. In\nresponse to these emerging challenges, we present HiQA, an advanced\nmulti-document question-answering (MDQA) framework that integrates cascading\nmetadata into content and a multi-route retrieval mechanism. We also release a\nbenchmark called MasQA to evaluate and research in MDQA. Finally, HiQA\ndemonstrates the state-of-the-art performance in multi-document environments.", "published": "2024-02-01 02:24:15", "link": "http://arxiv.org/abs/2402.01767v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BlackMamba: Mixture of Experts for State-Space Models", "abstract": "State-space models (SSMs) have recently demonstrated competitive performance\nto transformers at large-scale language modeling benchmarks while achieving\nlinear time and memory complexity as a function of sequence length. Mamba, a\nrecently released SSM model, shows impressive performance in both language\nmodeling and long sequence processing tasks. Simultaneously, mixture-of-expert\n(MoE) models have shown remarkable performance while significantly reducing the\ncompute and latency costs of inference at the expense of a larger memory\nfootprint. In this paper, we present BlackMamba, a novel architecture that\ncombines the Mamba SSM with MoE to obtain the benefits of both. We demonstrate\nthat BlackMamba performs competitively against both Mamba and transformer\nbaselines, and outperforms in inference and training FLOPs. We fully train and\nopen-source 340M/1.5B and 630M/2.8B BlackMamba models on 300B tokens of a\ncustom dataset. We show that BlackMamba inherits and combines both of the\nbenefits of SSM and MoE architectures, combining linear-complexity generation\nfrom SSM with cheap and fast inference from MoE. We release all weights,\ncheckpoints, and inference code open-source. Inference code at:\nhttps://github.com/Zyphra/BlackMamba", "published": "2024-02-01 07:15:58", "link": "http://arxiv.org/abs/2402.01771v1", "categories": ["cs.CL", "cs.AI", "cs.DC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Disentangling the Roles of Target-Side Transfer and Regularization in\n  Multilingual Machine Translation", "abstract": "Multilingual Machine Translation (MMT) benefits from knowledge transfer\nacross different language pairs. However, improvements in one-to-many\ntranslation compared to many-to-one translation are only marginal and sometimes\neven negligible. This performance discrepancy raises the question of to what\nextent positive transfer plays a role on the target-side for one-to-many MT. In\nthis paper, we conduct a large-scale study that varies the auxiliary target\nside languages along two dimensions, i.e., linguistic similarity and corpus\nsize, to show the dynamic impact of knowledge transfer on the main language\npairs. We show that linguistically similar auxiliary target languages exhibit\nstrong ability to transfer positive knowledge. With an increasing size of\nsimilar target languages, the positive transfer is further enhanced to benefit\nthe main language pairs. Meanwhile, we find distant auxiliary target languages\ncan also unexpectedly benefit main language pairs, even with minimal positive\ntransfer ability. Apart from transfer, we show distant auxiliary target\nlanguages can act as a regularizer to benefit translation performance by\nenhancing the generalization and model inference calibration.", "published": "2024-02-01 10:55:03", "link": "http://arxiv.org/abs/2402.01772v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Design and consensus content validity of the questionnaire for\n  b-learning education: A 2-Tuple Fuzzy Linguistic Delphi based Decision\n  Support Tool", "abstract": "Classic Delphi and Fuzzy Delphi methods are used to test content validity of\ndata collection tools such as questionnaires. Fuzzy Delphi takes the opinion\nissued by judges from a linguistic perspective reducing ambiguity in opinions\nby using fuzzy numbers. We propose an extension named 2-Tuple Fuzzy Linguistic\nDelphi method to deal with scenarios in which judges show different expertise\ndegrees by using fuzzy multigranular semantics of the linguistic terms and to\nobtain intermediate and final results expressed by 2-tuple linguistic values.\nThe key idea of our proposal is to validate the full questionnaire by means of\nthe evaluation of its parts, defining the validity of each item as a Decision\nMaking problem. Taking the opinion of experts, we measure the degree of\nconsensus, the degree of consistency, and the linguistic score of each item, in\norder to detect those items that affect, positively or negatively, the quality\nof the instrument. Considering the real need to evaluate a b-learning\neducational experience with a consensual questionnaire, we present a Decision\nMaking model for questionnaire validation that solves it. Additionally, we\ncontribute to this consensus reaching problem by developing an online tool\nunder GPL v3 license. The software visualizes the collective valuations for\neach iteration and assists to determine which parts of the questionnaire should\nbe modified to reach a consensual solution.", "published": "2024-02-01 13:32:18", "link": "http://arxiv.org/abs/2402.01775v1", "categories": ["cs.CY", "cs.CL", "cs.HC"], "primary_category": "cs.CY"}
{"title": "On the Psychology of GPT-4: Moderately anxious, slightly masculine,\n  honest, and humble", "abstract": "We subject GPT-4 to a number of rigorous psychometric tests and analyze the\nresults. We find that, compared to the average human, GPT-4 tends to show more\nhonesty and humility, and less machiavellianism and narcissism. It sometimes\nexhibits ambivalent sexism, leans slightly toward masculinity, is moderately\nanxious but mostly not depressive (but not always). It shows human-average\nnumerical literacy and has cognitive reflection abilities that are above human\naverage for verbal tasks.", "published": "2024-02-01 15:58:13", "link": "http://arxiv.org/abs/2402.01777v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Introduction to speech recognition", "abstract": "This document contains lectures and practical experimentations using Matlab\nand implementing a system which is actually correctly classifying three words\n(one, two and three) with the help of a very small database. To achieve this\nperformance, it uses speech modeling specificities, powerful computer\nalgorithms (dynamic time warping and Dijktra's algorithm) and machine learning\n(nearest neighbor). This document introduces also some machine learning\nevaluation metrics.", "published": "2024-02-01 17:54:15", "link": "http://arxiv.org/abs/2402.01778v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "When Benchmarks are Targets: Revealing the Sensitivity of Large Language\n  Model Leaderboards", "abstract": "Large Language Model (LLM) leaderboards based on benchmark rankings are\nregularly used to guide practitioners in model selection. Often, the published\nleaderboard rankings are taken at face value - we show this is a (potentially\ncostly) mistake. Under existing leaderboards, the relative performance of LLMs\nis highly sensitive to (often minute) details. We show that for popular\nmultiple-choice question benchmarks (e.g., MMLU), minor perturbations to the\nbenchmark, such as changing the order of choices or the method of answer\nselection, result in changes in rankings up to 8 positions. We explain this\nphenomenon by conducting systematic experiments over three broad categories of\nbenchmark perturbations and identifying the sources of this behavior. Our\nanalysis results in several best-practice recommendations, including the\nadvantage of a hybrid scoring method for answer selection. Our study highlights\nthe dangers of relying on simple benchmark evaluations and charts the path for\nmore robust evaluation schemes on the existing benchmarks. The code for this\npaper is available at\nhttps://github.com/National-Center-for-AI-Saudi-Arabia/lm-evaluation-harness.", "published": "2024-02-01 19:12:25", "link": "http://arxiv.org/abs/2402.01781v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Hierarchical Multi-Label Classification of Online Vaccine Concerns", "abstract": "Vaccine concerns are an ever-evolving target, and can shift quickly as seen\nduring the COVID-19 pandemic. Identifying longitudinal trends in vaccine\nconcerns and misinformation might inform the healthcare space by helping public\nhealth efforts strategically allocate resources or information campaigns. We\nexplore the task of detecting vaccine concerns in online discourse using large\nlanguage models (LLMs) in a zero-shot setting without the need for expensive\ntraining datasets. Since real-time monitoring of online sources requires\nlarge-scale inference, we explore cost-accuracy trade-offs of different\nprompting strategies and offer concrete takeaways that may inform choices in\nsystem designs for current applications. An analysis of different prompting\nstrategies reveals that classifying the concerns over multiple passes through\nthe LLM, each consisting a boolean question whether the text mentions a vaccine\nconcern or not, works the best. Our results indicate that GPT-4 can strongly\noutperform crowdworker accuracy when compared to ground truth annotations\nprovided by experts on the recently introduced VaxConcerns dataset, achieving\nan overall F1 score of 78.7%.", "published": "2024-02-01 20:56:07", "link": "http://arxiv.org/abs/2402.01783v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "COA-GPT: Generative Pre-trained Transformers for Accelerated Course of\n  Action Development in Military Operations", "abstract": "The development of Courses of Action (COAs) in military operations is\ntraditionally a time-consuming and intricate process. Addressing this\nchallenge, this study introduces COA-GPT, a novel algorithm employing Large\nLanguage Models (LLMs) for rapid and efficient generation of valid COAs.\nCOA-GPT incorporates military doctrine and domain expertise to LLMs through\nin-context learning, allowing commanders to input mission information - in both\ntext and image formats - and receive strategically aligned COAs for review and\napproval. Uniquely, COA-GPT not only accelerates COA development, producing\ninitial COAs within seconds, but also facilitates real-time refinement based on\ncommander feedback. This work evaluates COA-GPT in a military-relevant scenario\nwithin a militarized version of the StarCraft II game, comparing its\nperformance against state-of-the-art reinforcement learning algorithms. Our\nresults demonstrate COA-GPT's superiority in generating strategically sound\nCOAs more swiftly, with added benefits of enhanced adaptability and alignment\nwith commander intentions. COA-GPT's capability to rapidly adapt and update\nCOAs during missions presents a transformative potential for military planning,\nparticularly in addressing planning discrepancies and capitalizing on emergent\nwindows of opportunities.", "published": "2024-02-01 21:51:09", "link": "http://arxiv.org/abs/2402.01786v2", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG", "I.2.6; I.2.7; J.7"], "primary_category": "cs.AI"}
{"title": "Efficient Exploration for LLMs", "abstract": "We present evidence of substantial benefit from efficient exploration in\ngathering human feedback to improve large language models. In our experiments,\nan agent sequentially generates queries while fitting a reward model to the\nfeedback received. Our best-performing agent generates queries using double\nThompson sampling, with uncertainty represented by an epistemic neural network.\nOur results demonstrate that efficient exploration enables high levels of\nperformance with far fewer queries. Further, both uncertainty estimation and\nthe choice of exploration scheme play critical roles.", "published": "2024-02-01 07:32:24", "link": "http://arxiv.org/abs/2402.00396v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ME", "stat.ML"], "primary_category": "cs.LG"}
{"title": "The Information of Large Language Model Geometry", "abstract": "This paper investigates the information encoded in the embeddings of large\nlanguage models (LLMs). We conduct simulations to analyze the representation\nentropy and discover a power law relationship with model sizes. Building upon\nthis observation, we propose a theory based on (conditional) entropy to\nelucidate the scaling law phenomenon. Furthermore, we delve into the\nauto-regressive structure of LLMs and examine the relationship between the last\ntoken and previous context tokens using information theory and regression\ntechniques. Specifically, we establish a theoretical connection between the\ninformation gain of new tokens and ridge regression. Additionally, we explore\nthe effectiveness of Lasso regression in selecting meaningful tokens, which\nsometimes outperforms the closely related attention weights. Finally, we\nconduct controlled experiments, and find that information is distributed across\ntokens, rather than being concentrated in specific \"meaningful\" tokens alone.", "published": "2024-02-01 12:50:43", "link": "http://arxiv.org/abs/2402.03471v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IT", "math.IT"], "primary_category": "cs.LG"}
{"title": "Real-time Stereo Speech Enhancement with Spatial-Cue Preservation based\n  on Dual-Path Structure", "abstract": "We introduce a real-time, multichannel speech enhancement algorithm which\nmaintains the spatial cues of stereo recordings including two speech sources.\nRecognizing that each source has unique spatial information, our method\nutilizes a dual-path structure, ensuring the spatial cues remain unaffected\nduring enhancement by applying source-specific common-band gain. This method\nalso seamlessly integrates pretrained monaural speech enhancement, eliminating\nthe need for retraining on stereo inputs. Source separation from stereo\nmixtures is achieved via spatial beamforming, with the steering vector for each\nsource being adaptively updated using post-enhancement output signal. This\nensures accurate tracking of the spatial information. The final stereo output\nis derived by merging the spatial images of the enhanced sources, with its\nefficacy not heavily reliant on the separation performance of the beamforming.\nThe algorithm runs in real-time on 10-ms frames with a 40 ms of look-ahead.\nEvaluations reveal its effectiveness in enhancing speech and preserving spatial\ncues in both fully and sparsely overlapped mixtures.", "published": "2024-02-01 04:47:12", "link": "http://arxiv.org/abs/2402.00337v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Deep Room Impulse Response Completion", "abstract": "Rendering immersive spatial audio in virtual reality (VR) and video games\ndemands a fast and accurate generation of room impulse responses (RIRs) to\nrecreate auditory environments plausibly. However, the conventional methods for\nsimulating or measuring long RIRs are either computationally intensive or\nchallenged by low signal-to-noise ratios. This study is propelled by the\ninsight that direct sound and early reflections encapsulate sufficient\ninformation about room geometry and absorption characteristics. Building upon\nthis premise, we propose a novel task termed \"RIR completion,\" aimed at\nsynthesizing the late reverberation given only the early portion (50 ms) of the\nresponse. To this end, we introduce DECOR, Deep Exponential Completion Of Room\nimpulse responses, a deep neural network structured as an autoencoder designed\nto predict multi-exponential decay envelopes of filtered noise sequences. The\ninterpretability of DECOR's output facilitates its integration with diverse\nrendering techniques. The proposed method is compared against an adapted\nstate-of-the-art network, and comparable performance shows promising results\nsupporting the feasibility of the RIR completion task. The RIR completion can\nbe widely adapted to enhance RIR generation tasks where fast late reverberation\napproximation is required.", "published": "2024-02-01 18:55:37", "link": "http://arxiv.org/abs/2402.00859v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "PAM: Prompting Audio-Language Models for Audio Quality Assessment", "abstract": "While audio quality is a key performance metric for various audio processing\ntasks, including generative modeling, its objective measurement remains a\nchallenge. Audio-Language Models (ALMs) are pre-trained on audio-text pairs\nthat may contain information about audio quality, the presence of artifacts, or\nnoise. Given an audio input and a text prompt related to quality, an ALM can be\nused to calculate a similarity score between the two. Here, we exploit this\ncapability and introduce PAM, a no-reference metric for assessing audio quality\nfor different audio processing tasks. Contrary to other \"reference-free\"\nmetrics, PAM does not require computing embeddings on a reference dataset nor\ntraining a task-specific model on a costly set of human listening scores. We\nextensively evaluate the reliability of PAM against established metrics and\nhuman listening scores on four tasks: text-to-audio (TTA), text-to-music\ngeneration (TTM), text-to-speech (TTS), and deep noise suppression (DNS). We\nperform multiple ablation studies with controlled distortions, in-the-wild\nsetups, and prompt choices. Our evaluation shows that PAM correlates well with\nexisting metrics and human listening scores. These results demonstrate the\npotential of ALMs for computing a general-purpose audio quality metric.", "published": "2024-02-01 02:15:59", "link": "http://arxiv.org/abs/2402.00282v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Frame-Wise Breath Detection with Self-Training: An Exploration of\n  Enhancing Breath Naturalness in Text-to-Speech", "abstract": "Developing Text-to-Speech (TTS) systems that can synthesize natural breath is\nessential for human-like voice agents but requires extensive manual annotation\nof breath positions in training data. To this end, we propose a self-training\nmethod for training a breath detection model that can automatically detect\nbreath positions in speech. Our method trains the model using a large speech\ncorpus and involves: 1) annotation of limited breath sounds utilizing a\nrule-based approach, and 2) iterative augmentation of these annotations through\npseudo-labeling based on the model's predictions. Our detection model employs\nConformer blocks with down-/up-sampling layers, enabling accurate frame-wise\nbreath detection. We investigate its effectiveness in multi-speaker TTS using\ntext transcripts with detected breath marks. The results indicate that using\nour proposed model for breath detection and breath mark insertion synthesizes\nbreath-contained speech more naturally than a baseline model.", "published": "2024-02-01 02:39:19", "link": "http://arxiv.org/abs/2402.00288v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Can you Remove the Downstream Model for Speaker Recognition with\n  Self-Supervised Speech Features?", "abstract": "Self-supervised features are typically used in place of filter-bank features\nin speaker verification models. However, these models were originally designed\nto ingest filter-bank features as inputs, and thus, training them on top of\nself-supervised features assumes that both feature types require the same\namount of learning for the task. In this work, we observe that pre-trained\nself-supervised speech features inherently include information required for\ndownstream speaker verification task, and therefore, we can simplify the\ndownstream model without sacrificing performance. To this end, we revisit the\ndesign of the downstream model for speaker verification using self-supervised\nfeatures. We show that we can simplify the model to use 97.51% fewer parameters\nwhile achieving a 29.93% average improvement in performance on SUPERB.\nConsequently, we show that the simplified downstream model is more data\nefficient compared to baseline--it achieves better performance with only 60% of\nthe training data.", "published": "2024-02-01 05:03:05", "link": "http://arxiv.org/abs/2402.00340v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "USDnet: Unsupervised Speech Dereverberation via Neural Forward Filtering", "abstract": "In reverberant conditions with a single speaker, each far-field microphone\nrecords a reverberant version of the same speaker signal at a different\nlocation. In over-determined conditions, where there are multiple microphones\nbut only one speaker, each recorded mixture signal can be leveraged as a\nconstraint to narrow down the solutions to target anechoic speech and thereby\nreduce reverberation. Equipped with this insight, we propose USDnet, a novel\ndeep neural network (DNN) approach for unsupervised speech dereverberation\n(USD). At each training step, we first feed an input mixture to USDnet to\nproduce an estimate for target speech, and then linearly filter the DNN\nestimate to approximate the multi-microphone mixture so that the constraint can\nbe satisfied at each microphone, thereby regularizing the DNN estimate to\napproximate target anechoic speech. The linear filter can be estimated based on\nthe mixture and DNN estimate via neural forward filtering algorithms such as\nforward convolutive prediction. We show that this novel methodology can promote\nunsupervised dereverberation of single-source reverberant speech.", "published": "2024-02-01 18:02:29", "link": "http://arxiv.org/abs/2402.00820v2", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Efficient Fine-tuning of Audio Spectrogram Transformers via Soft Mixture\n  of Adapters", "abstract": "Mixture of Experts (MoE) architectures have recently started burgeoning due\nto their ability to scale model's capacity while maintaining the computational\ncost affordable. Furthermore, they can be applied to both Transformers and\nState Space Models, the current state-of-the-art models in numerous fields.\nWhile MoE has been mostly investigated for the pre-training stage, its use in\nparameter-efficient transfer learning settings is under-explored. To narrow\nthis gap, this paper attempts to demystify the use of MoE for\nparameter-efficient fine-tuning of Audio Spectrogram Transformers to audio and\nspeech downstream tasks. Specifically, we propose Soft Mixture of Adapters\n(Soft-MoA). It exploits adapters as the experts and, leveraging the recent Soft\nMoE method, it relies on a soft assignment between the input tokens and experts\nto keep the computational time limited. Extensive experiments across 4\nbenchmarks demonstrate that Soft-MoA outperforms the single adapter method and\nperforms on par with the dense MoA counterpart. We finally present ablation\nstudies on key elements of Soft-MoA, showing for example that Soft-MoA achieves\nbetter scaling with more experts, as well as ensuring that all experts\ncontribute to the computation of the output tokens, thus dispensing with the\nexpert imbalance issue.", "published": "2024-02-01 18:16:04", "link": "http://arxiv.org/abs/2402.00828v2", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "An Analysis of the Variance of Diffusion-based Speech Enhancement", "abstract": "Diffusion models proved to be powerful models for generative speech\nenhancement. In recent SGMSE+ approaches, training involves a stochastic\ndifferential equation for the diffusion process, adding both Gaussian and\nenvironmental noise to the clean speech signal gradually. The speech\nenhancement performance varies depending on the choice of the stochastic\ndifferential equation that controls the evolution of the mean and the variance\nalong the diffusion processes when adding environmental and Gaussian noise. In\nthis work, we highlight that the scale of the variance is a dominant parameter\nfor speech enhancement performance and show that it controls the tradeoff\nbetween noise attenuation and speech distortions. More concretely, we show that\na larger variance increases the noise attenuation and allows for reducing the\ncomputational footprint, as fewer function evaluations for generating the\nestimate are required", "published": "2024-02-01 17:46:19", "link": "http://arxiv.org/abs/2402.00811v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Creating a Synthesizer from Schr\u00f6dinger's Equation", "abstract": "Our project offers an alternative approach to the sensory perception of the\nSchr\\\"odinger equation (an elementary model of quantum phenomena) by\ninterpreting it as a sound wave. We are building a synthesizer plugin that\nsimulates a quantum mechanical state that evolves over time. Thus, our tool\nallows the creation of unique sounds that are in motion and feel alive. These\ncan be used in professional music production without any knowledge of physics,\nwhile at the same time providing insight into a chapter of quantum mechanics.\nThe goal is to lower the threshold for entering complex theory by first\ndeveloping an intuition for the subject; but the tool can also be used purely\nas a musical instrument. The user is encouraged, but not forced, to learn more\nabout the underlying physics. Simulation parameters are adjustable in\nreal-time, allowing intuitive experimentation. Despite the approximate\ncalculations, real physical effects such as quantum tunneling can be observed\nacoustically and visually.", "published": "2024-02-01 10:56:08", "link": "http://arxiv.org/abs/2402.01773v1", "categories": ["cs.SD", "eess.AS", "quant-ph"], "primary_category": "cs.SD"}
{"title": "Room Transfer Function Reconstruction Using Complex-valued Neural\n  Networks and Irregularly Distributed Microphones", "abstract": "Reconstructing the room transfer functions needed to calculate the complex\nsound field in a room has several important real-world applications. However,\nan unpractical number of microphones is often required. Recently, in addition\nto classical signal processing methods, deep learning techniques have been\napplied to reconstruct the room transfer function starting from a very limited\nset of measurements at scattered points in the room. In this paper, we employ\ncomplex-valued neural networks to estimate room transfer functions in the\nfrequency range of the first room resonances, using a few irregularly\ndistributed microphones. To the best of our knowledge, this is the first time\nthat complex-valued neural networks are used to estimate room transfer\nfunctions. To analyze the benefits of applying complex-valued optimization to\nthe considered task, we compare the proposed technique with a state-of-the-art\nkernel-based signal processing approach for sound field reconstruction, showing\nthat the proposed technique exhibits relevant advantages in terms of phase\naccuracy and overall quality of the reconstructed sound field. For informative\npurposes, we also compare the model with a similarly-structured data-driven\napproach that, however, applies a real-valued neural network to reconstruct\nonly the magnitude of the sound field.", "published": "2024-02-01 21:16:40", "link": "http://arxiv.org/abs/2402.04866v3", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
