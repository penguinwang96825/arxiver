{"title": "Structural Guidance for Transformer Language Models", "abstract": "Transformer-based language models pre-trained on large amounts of text data\nhave proven remarkably successful in learning generic transferable linguistic\nrepresentations. Here we study whether structural guidance leads to more\nhuman-like systematic linguistic generalization in Transformer language models\nwithout resorting to pre-training on very large amounts of data. We explore two\ngeneral ideas. The \"Generative Parsing\" idea jointly models the incremental\nparse and word sequence as part of the same sequence modeling task. The\n\"Structural Scaffold\" idea guides the language model's representation via\nadditional structure loss that separately predicts the incremental constituency\nparse. We train the proposed models along with a vanilla Transformer language\nmodel baseline on a 14 million-token and a 46 million-token subset of the BLLIP\ndataset, and evaluate models' syntactic generalization performances on SG Test\nSuites and sized BLiMP. Experiment results across two benchmarks suggest\nconverging evidence that generative structural supervisions can induce more\nrobust and humanlike linguistic generalization in Transformer language models\nwithout the need for data intensive pre-training.", "published": "2021-07-30 23:14:51", "link": "http://arxiv.org/abs/2108.00104v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An automated domain-independent text reading, interpreting and\n  extracting approach for reviewing the scientific literature", "abstract": "It is presented here a machine learning-based (ML) natural language\nprocessing (NLP) approach capable to automatically recognize and extract\ncategorical and numerical parameters from a corpus of articles. The approach\n(named a.RIX) operates with a concomitant/interchangeable use of ML models such\nas neuron networks (NNs), latent semantic analysis (LSA), naive-Bayes\nclassifiers (NBC), and a pattern recognition model using regular expression\n(REGEX). A corpus of 7,873 scientific articles dealing with natural products\n(NPs) was used to demonstrate the efficiency of the a.RIX engine. The engine\nautomatically extracts categorical and numerical parameters such as (i) the\nplant species from which active molecules are extracted, (ii) the\nmicroorganisms species for which active molecules can act against, and (iii)\nthe values of minimum inhibitory concentration (MIC) against these\nmicroorganisms. The parameters are extracted without part-of-speech tagging\n(POS) and named entity recognition (NER) approaches (i.e. without the need of\ntext annotation), and the models training is performed with unsupervised\napproaches. In this way, a.RIX can be essentially used on articles from any\nscientific field. Finally, it can potentially make obsolete the current article\nreviewing process in some areas, especially those in which machine learning\nmodels capture texts structure, text semantics, and latent knowledge.", "published": "2021-07-30 14:02:52", "link": "http://arxiv.org/abs/2107.14638v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Claim Review for Climate Science via Explanation Generation", "abstract": "There is unison is the scientific community about human induced climate\nchange. Despite this, we see the web awash with claims around climate change\nscepticism, thus driving the need for fact checking them but at the same time\nproviding an explanation and justification for the fact check. Scientists and\nexperts have been trying to address it by providing manually written feedback\nfor these claims. In this paper, we try to aid them by automating generating\nexplanation for a predicted veracity label for a claim by deploying the\napproach used in open domain question answering of a fusion in decoder\naugmented with retrieved supporting passages from an external knowledge. We\nexperiment with different knowledge sources, retrievers, retriever depths and\ndemonstrate that even a small number of high quality manually written\nexplanations can help us in generating good explanations.", "published": "2021-07-30 16:37:45", "link": "http://arxiv.org/abs/2107.14740v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Universality in Multilingual Text Rewriting", "abstract": "In this work, we take the first steps towards building a universal rewriter:\na model capable of rewriting text in any language to exhibit a wide variety of\nattributes, including styles and languages, while preserving as much of the\noriginal semantics as possible. In addition to obtaining state-of-the-art\nresults on unsupervised translation, we also demonstrate the ability to do\nzero-shot sentiment transfer in non-English languages using only English\nexemplars for sentiment. We then show that our model is able to modify multiple\nattributes at once, for example adjusting both language and sentiment jointly.\nFinally, we show that our model is capable of performing zero-shot\nformality-sensitive translation.", "published": "2021-07-30 16:48:04", "link": "http://arxiv.org/abs/2107.14749v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Continual Entity Learning in Language Models for Conversational\n  Agents", "abstract": "Neural language models (LM) trained on diverse corpora are known to work well\non previously seen entities, however, updating these models with dynamically\nchanging entities such as place names, song titles and shopping items requires\nre-training from scratch and collecting full sentences containing these\nentities. We aim to address this issue, by introducing entity-aware language\nmodels (EALM), where we integrate entity models trained on catalogues of\nentities into the pre-trained LMs. Our combined language model adaptively adds\ninformation from the entity models into the pre-trained LM depending on the\nsentence context. Our entity models can be updated independently of the\npre-trained LM, enabling us to influence the distribution of entities output by\nthe final LM, without any further training of the pre-trained LM. We show\nsignificant perplexity improvements on task-oriented dialogue datasets,\nespecially on long-tailed utterances, with an ability to continually adapt to\nnew entities (to an extent).", "published": "2021-07-30 21:10:09", "link": "http://arxiv.org/abs/2108.00082v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Deep Natural Language Processing for LinkedIn Search Systems", "abstract": "Many search systems work with large amounts of natural language data, e.g.,\nsearch queries, user profiles and documents, where deep learning based natural\nlanguage processing techniques (deep NLP) can be of great help. In this paper,\nwe introduce a comprehensive study of applying deep NLP techniques to five\nrepresentative tasks in search engines. Through the model design and\nexperiments of the five tasks, readers can find answers to three important\nquestions: (1) When is deep NLP helpful/not helpful in search systems? (2) How\nto address latency challenges? (3) How to ensure model robustness? This work\nbuilds on existing efforts of LinkedIn search, and is tested at scale on a\ncommercial search engine. We believe our experiences can provide useful\ninsights for the industry and research communities.", "published": "2021-07-30 17:40:36", "link": "http://arxiv.org/abs/2108.08252v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Difficulty-Aware Machine Translation Evaluation", "abstract": "The high-quality translation results produced by machine translation (MT)\nsystems still pose a huge challenge for automatic evaluation. Current MT\nevaluation pays the same attention to each sentence component, while the\nquestions of real-world examinations (e.g., university examinations) have\ndifferent difficulties and weightings. In this paper, we propose a novel\ndifficulty-aware MT evaluation metric, expanding the evaluation dimension by\ntaking translation difficulty into consideration. A translation that fails to\nbe predicted by most MT systems will be treated as a difficult one and assigned\na large weight in the final score function, and conversely. Experimental\nresults on the WMT19 English-German Metrics shared tasks show that our proposed\nmethod outperforms commonly used MT metrics in terms of human correlation. In\nparticular, our proposed method performs well even when all the MT systems are\nvery competitive, which is when most existing metrics fail to distinguish\nbetween them. The source code is freely available at\nhttps://github.com/NLP2CT/Difficulty-Aware-MT-Evaluation.", "published": "2021-07-30 02:45:36", "link": "http://arxiv.org/abs/2107.14402v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "USC: An Open-Source Uzbek Speech Corpus and Initial Speech Recognition\n  Experiments", "abstract": "We present a freely available speech corpus for the Uzbek language and report\npreliminary automatic speech recognition (ASR) results using both the deep\nneural network hidden Markov model (DNN-HMM) and end-to-end (E2E)\narchitectures. The Uzbek speech corpus (USC) comprises 958 different speakers\nwith a total of 105 hours of transcribed audio recordings. To the best of our\nknowledge, this is the first open-source Uzbek speech corpus dedicated to the\nASR task. To ensure high quality, the USC has been manually checked by native\nspeakers. We first describe the design and development procedures of the USC,\nand then explain the conducted ASR experiments in detail. The experimental\nresults demonstrate promising results for the applicability of the USC for ASR.\nSpecifically, 18.1% and 17.4% word error rates were achieved on the validation\nand test sets, respectively. To enable experiment reproducibility, we share the\nUSC dataset, pre-trained models, and training recipes in our GitHub repository.", "published": "2021-07-30 03:39:39", "link": "http://arxiv.org/abs/2107.14419v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Investigating Disagreement in the Scientific Literature", "abstract": "Disagreement is essential to scientific progress. However, the extent of\ndisagreement in science, its evolution over time, and the fields in which it\nhappens, remains poorly understood. Leveraging a massive collection of\nEnglish-language scientific texts, we develop a cue-phrase based approach to\nidentify instances of disagreement citations across more than four million\nscientific articles. Using this method, we construct an indicator of\ndisagreement across scientific fields over the 2000-2015 period. In contrast\nwith black-box text classification methods, our framework is transparent and\neasily interpretable. We reveal a disciplinary spectrum of disagreement, with\nhigher disagreement in the social sciences and lower disagreement in physics\nand mathematics. However, detailed disciplinary analysis demonstrates\nheterogeneity across sub-fields, revealing the importance of local disciplinary\ncultures and epistemic characteristics of disagreement. Paper-level analysis\nreveals notable episodes of disagreement in science, and illustrates how\nmethodological artifacts can confound analyses of scientific texts. These\nfindings contribute to a broader understanding of disagreement and establish a\nfoundation for future research to understanding key processes underlying\nscientific progress.", "published": "2021-07-30 14:07:34", "link": "http://arxiv.org/abs/2107.14641v2", "categories": ["cs.DL", "cs.CL"], "primary_category": "cs.DL"}
{"title": "EmailSum: Abstractive Email Thread Summarization", "abstract": "Recent years have brought about an interest in the challenging task of\nsummarizing conversation threads (meetings, online discussions, etc.). Such\nsummaries help analysis of the long text to quickly catch up with the decisions\nmade and thus improve our work or communication efficiency. To spur research in\nthread summarization, we have developed an abstractive Email Thread\nSummarization (EmailSum) dataset, which contains human-annotated short (<30\nwords) and long (<100 words) summaries of 2549 email threads (each containing 3\nto 10 emails) over a wide variety of topics. We perform a comprehensive\nempirical study to explore different summarization techniques (including\nextractive and abstractive methods, single-document and hierarchical models, as\nwell as transfer and semisupervised learning) and conduct human evaluations on\nboth short and long summary generation tasks. Our results reveal the key\nchallenges of current abstractive summarization models in this task, such as\nunderstanding the sender's intent and identifying the roles of sender and\nreceiver. Furthermore, we find that widely used automatic evaluation metrics\n(ROUGE, BERTScore) are weakly correlated with human judgments on this email\nthread summarization task. Hence, we emphasize the importance of human\nevaluation and the development of better metrics by the community. Our code and\nsummary data have been made available at:\nhttps://github.com/ZhangShiyue/EmailSum", "published": "2021-07-30 15:13:14", "link": "http://arxiv.org/abs/2107.14691v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ChrEnTranslate: Cherokee-English Machine Translation Demo with Quality\n  Estimation and Corrective Feedback", "abstract": "We introduce ChrEnTranslate, an online machine translation demonstration\nsystem for translation between English and an endangered language Cherokee. It\nsupports both statistical and neural translation models as well as provides\nquality estimation to inform users of reliability, two user feedback interfaces\nfor experts and common users respectively, example inputs to collect human\ntranslations for monolingual data, word alignment visualization, and relevant\nterms from the Cherokee-English dictionary. The quantitative evaluation\ndemonstrates that our backbone translation models achieve state-of-the-art\ntranslation performance and our quality estimation well correlates with both\nBLEU and human judgment. By analyzing 216 pieces of expert feedback, we find\nthat NMT is preferable because it copies less than SMT, and, in general,\ncurrent models can translate fragments of the source sentence but make major\nmistakes. When we add these 216 expert-corrected parallel texts back into the\ntraining set and retrain models, equal or slightly better performance is\nobserved, which indicates the potential of human-in-the-loop learning. Our\nonline demo is at https://chren.cs.unc.edu/ , our code is open-sourced at\nhttps://github.com/ZhangShiyue/ChrEnTranslate , and our data is available at\nhttps://github.com/ZhangShiyue/ChrEn", "published": "2021-07-30 17:58:54", "link": "http://arxiv.org/abs/2107.14800v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "WLV-RIT at GermEval 2021: Multitask Learning with Transformers to Detect\n  Toxic, Engaging, and Fact-Claiming Comments", "abstract": "This paper addresses the identification of toxic, engaging, and fact-claiming\ncomments on social media. We used the dataset made available by the organizers\nof the GermEval-2021 shared task containing over 3,000 manually annotated\nFacebook comments in German. Considering the relatedness of the three tasks, we\napproached the problem using large pre-trained transformer models and multitask\nlearning. Our results indicate that multitask learning achieves performance\nsuperior to the more common single task learning approach in all three tasks.\nWe submit our best systems to GermEval-2021 under the team name WLV-RIT.", "published": "2021-07-30 19:49:16", "link": "http://arxiv.org/abs/2108.00057v1", "categories": ["cs.CL", "cs.AI", "cs.NE", "cs.SI"], "primary_category": "cs.CL"}
{"title": "MTVR: Multilingual Moment Retrieval in Videos", "abstract": "We introduce mTVR, a large-scale multilingual video moment retrieval dataset,\ncontaining 218K English and Chinese queries from 21.8K TV show video clips. The\ndataset is collected by extending the popular TVR dataset (in English) with\npaired Chinese queries and subtitles. Compared to existing moment retrieval\ndatasets, mTVR is multilingual, larger, and comes with diverse annotations. We\nfurther propose mXML, a multilingual moment retrieval model that learns and\noperates on data from both languages, via encoder parameter sharing and\nlanguage neighborhood constraints. We demonstrate the effectiveness of mXML on\nthe newly collected MTVR dataset, where mXML outperforms strong monolingual\nbaselines while using fewer parameters. In addition, we also provide detailed\ndataset analyses and model ablations. Data and code are publicly available at\nhttps://github.com/jayleicn/mTVRetrieval", "published": "2021-07-30 20:01:03", "link": "http://arxiv.org/abs/2108.00061v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "The History of Speech Recognition to the Year 2030", "abstract": "The decade from 2010 to 2020 saw remarkable improvements in automatic speech\nrecognition. Many people now use speech recognition on a daily basis, for\nexample to perform voice search queries, send text messages, and interact with\nvoice assistants like Amazon Alexa and Siri by Apple. Before 2010 most people\nrarely used speech recognition. Given the remarkable changes in the state of\nspeech recognition over the previous decade, what can we expect over the coming\ndecade? I attempt to forecast the state of speech recognition research and\napplications by the year 2030. While the changes to general speech recognition\naccuracy will not be as dramatic as in the previous decade, I suggest we have\nan exciting decade of progress in speech technology ahead of us.", "published": "2021-07-30 21:19:33", "link": "http://arxiv.org/abs/2108.00084v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Perceiver IO: A General Architecture for Structured Inputs & Outputs", "abstract": "A central goal of machine learning is the development of systems that can\nsolve many problems in as many data domains as possible. Current architectures,\nhowever, cannot be applied beyond a small set of stereotyped settings, as they\nbake in domain & task assumptions or scale poorly to large inputs or outputs.\nIn this work, we propose Perceiver IO, a general-purpose architecture that\nhandles data from arbitrary settings while scaling linearly with the size of\ninputs and outputs. Our model augments the Perceiver with a flexible querying\nmechanism that enables outputs of various sizes and semantics, doing away with\nthe need for task-specific architecture engineering. The same architecture\nachieves strong results on tasks spanning natural language and visual\nunderstanding, multi-task and multi-modal reasoning, and StarCraft II. As\nhighlights, Perceiver IO outperforms a Transformer-based BERT baseline on the\nGLUE language benchmark despite removing input tokenization and achieves\nstate-of-the-art performance on Sintel optical flow estimation with no explicit\nmechanisms for multiscale correspondence.", "published": "2021-07-30 17:53:34", "link": "http://arxiv.org/abs/2107.14795v3", "categories": ["cs.LG", "cs.CL", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "On-Line Audio-to-Lyrics Alignment Based on a Reference Performance", "abstract": "Audio-to-lyrics alignment has become an increasingly active research task in\nMIR, supported by the emergence of several open-source datasets of audio\nrecordings with word-level lyrics annotations. However, there are still a\nnumber of open problems, such as a lack of robustness in the face of severe\nduration mismatches between audio and lyrics representation; a certain degree\nof language-specificity caused by acoustic differences across languages; and\nthe fact that most successful methods in the field are not suited to work in\nreal-time. Real-time lyrics alignment (tracking) would have many useful\napplications, such as fully automated subtitle display in live concerts and\nopera. In this work, we describe the first real-time-capable audio-to-lyrics\nalignment pipeline that is able to robustly track the lyrics of different\nlanguages, without additional language information. The proposed model\npredicts, for each audio frame, a probability vector over (European) phoneme\nclasses, using a very small temporal context, and aligns this vector with a\nphoneme posteriogram matrix computed beforehand from another recording of the\nsame work, which serves as a reference and a proxy to the written-out lyrics.\nWe evaluate our system's tracking accuracy on the challenging genre of\nclassical opera. Finally, robustness to out-of-training languages is\ndemonstrated in an experiment on Jingju (Beijing opera).", "published": "2021-07-30 08:57:13", "link": "http://arxiv.org/abs/2107.14496v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Speeding Up Permutation Invariant Training for Source Separation", "abstract": "Permutation invariant training (PIT) is a widely used training criterion for\nneural network-based source separation, used for both utterance-level\nseparation with utterance-level PIT (uPIT) and separation of long recordings\nwith the recently proposed Graph-PIT. When implemented naively, both suffer\nfrom an exponential complexity in the number of utterances to separate,\nrendering them unusable for large numbers of speakers or long realistic\nrecordings. We present a decomposition of the PIT criterion into the\ncomputation of a matrix and a strictly monotonously increasing function so that\nthe permutation or assignment problem can be solved efficiently with several\nsearch algorithms. The Hungarian algorithm can be used for uPIT and we\nintroduce various algorithms for the Graph-PIT assignment problem to reduce the\ncomplexity to be polynomial in the number of utterances.", "published": "2021-07-30 06:31:21", "link": "http://arxiv.org/abs/2107.14445v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Graph-PIT: Generalized permutation invariant training for continuous\n  separation of arbitrary numbers of speakers", "abstract": "Automatic transcription of meetings requires handling of overlapped speech,\nwhich calls for continuous speech separation (CSS) systems. The uPIT criterion\nwas proposed for utterance-level separation with neural networks and introduces\nthe constraint that the total number of speakers must not exceed the number of\noutput channels. When processing meeting-like data in a segment-wise manner,\ni.e., by separating overlapping segments independently and stitching adjacent\nsegments to continuous output streams, this constraint has to be fulfilled for\nany segment. In this contribution, we show that this constraint can be\nsignificantly relaxed. We propose a novel graph-based PIT criterion, which\ncasts the assignment of utterances to output channels in a graph coloring\nproblem. It only requires that the number of concurrently active speakers must\nnot exceed the number of output channels. As a consequence, the system can\nprocess an arbitrary number of speakers and arbitrarily long segments and thus\ncan handle more diverse scenarios. Further, the stitching algorithm for\nobtaining a consistent output order in neighboring segments is of less\nimportance and can even be eliminated completely, not the least reducing the\ncomputational effort. Experiments on meeting-style WSJ data show improvements\nin recognition performance over using the uPIT criterion.", "published": "2021-07-30 06:32:40", "link": "http://arxiv.org/abs/2107.14446v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Artist Similarity with Graph Neural Networks", "abstract": "Artist similarity plays an important role in organizing, understanding, and\nsubsequently, facilitating discovery in large collections of music. In this\npaper, we present a hybrid approach to computing similarity between artists\nusing graph neural networks trained with triplet loss. The novelty of using a\ngraph neural network architecture is to combine the topology of a graph of\nartist connections with content features to embed artists into a vector space\nthat encodes similarity. To evaluate the proposed method, we compile the new\nOLGA dataset, which contains artist similarities from AllMusic, together with\ncontent features from AcousticBrainz. With 17,673 artists, this is the largest\nacademic artist similarity dataset that includes content-based features to\ndate. Moreover, we also showcase the scalability of our approach by\nexperimenting with a much larger proprietary dataset. Results show the\nsuperiority of the proposed approach over current state-of-the-art methods for\nmusic similarity. Finally, we hope that the OLGA dataset will facilitate\nresearch on data-driven models for artist similarity.", "published": "2021-07-30 10:44:31", "link": "http://arxiv.org/abs/2107.14541v1", "categories": ["cs.IR", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.IR"}
{"title": "Evaluating the COVID-19 Identification ResNet (CIdeR) on the INTERSPEECH\n  COVID-19 from Audio Challenges", "abstract": "We report on cross-running the recent COVID-19 Identification ResNet (CIdeR)\non the two Interspeech 2021 COVID-19 diagnosis from cough and speech audio\nchallenges: ComParE and DiCOVA. CIdeR is an end-to-end deep learning neural\nnetwork originally designed to classify whether an individual is COVID-positive\nor COVID-negative based on coughing and breathing audio recordings from a\npublished crowdsourced dataset. In the current study, we demonstrate the\npotential of CIdeR at binary COVID-19 diagnosis from both the COVID-19 Cough\nand Speech Sub-Challenges of INTERSPEECH 2021, ComParE and DiCOVA. CIdeR\nachieves significant improvements over several baselines.", "published": "2021-07-30 10:59:08", "link": "http://arxiv.org/abs/2107.14549v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "TASK3 DCASE2021 Challenge: Sound event localization and detection using\n  squeeze-excitation residual CNNs", "abstract": "Sound event localisation and detection (SELD) is a problem in the field of\nautomatic listening that aims at the temporal detection and localisation\n(direction of arrival estimation) of sound events within an audio clip, usually\nof long duration. Due to the amount of data present in the datasets related to\nthis problem, solutions based on deep learning have positioned themselves at\nthe top of the state of the art. Most solutions are based on 2D representations\nof the audio (different spectrograms) that are processed by a\nconvolutional-recurrent network. The motivation of this submission is to study\nthe squeeze-excitation technique in the convolutional part of the network and\nhow it improves the performance of the system. This study is based on the one\ncarried out by the same team last year. This year, it has been decided to study\nhow this technique improves each of the datasets (last year only the MIC\ndataset was studied). This modification shows an improvement in the performance\nof the system compared to the baseline using MIC dataset.", "published": "2021-07-30 11:34:15", "link": "http://arxiv.org/abs/2107.14561v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Practical Attacks on Voice Spoofing Countermeasures", "abstract": "Voice authentication has become an integral part in security-critical\noperations, such as bank transactions and call center conversations. The\nvulnerability of automatic speaker verification systems (ASVs) to spoofing\nattacks instigated the development of countermeasures (CMs), whose task is to\ntell apart bonafide and spoofed speech. Together, ASVs and CMs form today's\nvoice authentication platforms, advertised as an impregnable access control\nmechanism. We develop the first practical attack on CMs, and show how a\nmalicious actor may efficiently craft audio samples to bypass voice\nauthentication in its strictest form. Previous works have primarily focused on\nnon-proactive attacks or adversarial strategies against ASVs that do not\nproduce speech in the victim's voice. The repercussions of our attacks are far\nmore severe, as the samples we generate sound like the victim, eliminating any\nchance of plausible deniability. Moreover, the few existing adversarial attacks\nagainst CMs mistakenly optimize spoofed speech in the feature space and do not\ntake into account the existence of ASVs, resulting in inferior synthetic audio\nthat fails in realistic settings. We eliminate these obstacles through our key\ntechnical contribution: a novel joint loss function that enables mounting\nadvanced adversarial attacks against combined ASV/CM deployments directly in\nthe time domain. Our adversarials achieve concerning black-box success rates\nagainst state-of-the-art authentication platforms (up to 93.57\\%). Finally, we\nperform the first targeted, over-telephony-network attack on CMs, bypassing\nseveral challenges and enabling various potential threats, given the increased\nuse of voice biometrics in call centers. Our results call into question the\nsecurity of modern voice authentication systems in light of the real threat of\nattackers bypassing these measures to gain access to users' most valuable\nresources.", "published": "2021-07-30 14:07:49", "link": "http://arxiv.org/abs/2107.14642v1", "categories": ["cs.CR", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
{"title": "DadaGP: A Dataset of Tokenized GuitarPro Songs for Sequence Models", "abstract": "Originating in the Renaissance and burgeoning in the digital era, tablatures\nare a commonly used music notation system which provides explicit\nrepresentations of instrument fingerings rather than pitches. GuitarPro has\nestablished itself as a widely used tablature format and software enabling\nmusicians to edit and share songs for musical practice, learning, and\ncomposition. In this work, we present DadaGP, a new symbolic music dataset\ncomprising 26,181 song scores in the GuitarPro format covering 739 musical\ngenres, along with an accompanying tokenized format well-suited for generative\nsequence models such as the Transformer. The tokenized format is inspired by\nevent-based MIDI encodings, often used in symbolic music generation models. The\ndataset is released with an encoder/decoder which converts GuitarPro files to\ntokens and back. We present results of a use case in which DadaGP is used to\ntrain a Transformer-based model to generate new songs in GuitarPro format. We\ndiscuss other relevant use cases for the dataset (guitar-bass transcription,\nmusic style transfer and artist/genre classification) as well as ethical\nimplications. DadaGP opens up the possibility to train GuitarPro score\ngenerators, fine-tune models on custom data, create new styles of music,\nAI-powered songwriting apps, and human-AI improvisation.", "published": "2021-07-30 14:21:36", "link": "http://arxiv.org/abs/2107.14653v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Task 1A DCASE 2021: Acoustic Scene Classification with mismatch-devices\n  using squeeze-excitation technique and low-complexity constraint", "abstract": "Acoustic scene classification (ASC) is one of the most popular problems in\nthe field of machine listening. The objective of this problem is to classify an\naudio clip into one of the predefined scenes using only the audio data. This\nproblem has considerably progressed over the years in the different editions of\nDCASE. It usually has several subtasks that allow to tackle this problem with\ndifferent approaches. The subtask presented in this report corresponds to a ASC\nproblem that is constrained by the complexity of the model as well as having\naudio recorded from different devices, known as mismatch devices (real and\nsimulated). The work presented in this report follows the research line carried\nout by the team in previous years. Specifically, a system based on two steps is\nproposed: a two-dimensional representation of the audio using the Gamamtone\nfilter bank and a convolutional neural network using squeeze-excitation\ntechniques. The presented system outperforms the baseline by about 17\npercentage points.", "published": "2021-07-30 14:24:45", "link": "http://arxiv.org/abs/2107.14658v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Multi-Head Relevance Weighting Framework For Learning Raw Waveform\n  Audio Representations", "abstract": "In this work, we propose a multi-head relevance weighting framework to learn\naudio representations from raw waveforms. The audio waveform, split into\nwindows of short duration, are processed with a 1-D convolutional layer of\ncosine modulated Gaussian filters acting as a learnable filterbank. The key\nnovelty of the proposed framework is the introduction of multi-head relevance\non the learnt filterbank representations. Each head of the relevance network is\nmodelled as a separate sub-network. These heads perform representation\nenhancement by generating weight masks for different parts of the\ntime-frequency representation learnt by the parametric acoustic filterbank\nlayer. The relevance weighted representations are fed to a neural classifier\nand the whole system is trained jointly for the audio classification objective.\nExperiments are performed on the DCASE2020 Task 1A challenge as well as the\nUrban Sound Classification (USC) tasks. In these experiments, the proposed\napproach yields relative improvements of 10% and 23% respectively for the\nDCASE2020 and USC datasets over the mel-spectrogram baseline. Also, the\nanalysis of multi-head relevance weights provides insights on the learned\nrepresentations.", "published": "2021-07-30 17:51:32", "link": "http://arxiv.org/abs/2107.14793v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
