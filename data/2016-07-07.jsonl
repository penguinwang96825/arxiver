{"title": "Neural Name Translation Improves Neural Machine Translation", "abstract": "In order to control computational complexity, neural machine translation\n(NMT) systems convert all rare words outside the vocabulary into a single unk\nsymbol. Previous solution (Luong et al., 2015) resorts to use multiple numbered\nunks to learn the correspondence between source and target rare words. However,\ntesting words unseen in the training corpus cannot be handled by this method.\nAnd it also suffers from the noisy word alignment. In this paper, we focus on a\nmajor type of rare words -- named entity (NE), and propose to translate them\nwith character level sequence to sequence model. The NE translation model is\nfurther used to derive high quality NE alignment in the bilingual training\ncorpus. With the integration of NE translation and alignment modules, our NMT\nsystem is able to surpass the baseline system by 2.9 BLEU points on the Chinese\nto English task.", "published": "2016-07-07 02:25:57", "link": "http://arxiv.org/abs/1607.01856v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Maturity Model for Public Administration as Open Translation Data\n  Providers", "abstract": "Any public administration that produces translation data can be a provider of\nuseful reusable data to meet its own translation needs and the ones of other\npublic organizations and private companies that work with texts of the same\ndomain. These data can also be crucial to produce domain-tuned Machine\nTranslation systems. The organization's management of the translation process,\nthe characteristics of the archives of the generated resources and of the\ninfrastructure available to support them determine the efficiency and the\neffectiveness with which the materials produced can be converted into reusable\ndata. However, it is of utmost importance that the organizations themselves\nfirst become aware of the goods they are producing and, second, adapt their\ninternal processes to become optimal providers. In this article, we propose a\nMaturity Model to help these organizations to achieve it by identifying the\ndifferent stages of the management of translation data that determine the path\nto the aforementioned goal.", "published": "2016-07-07 12:35:31", "link": "http://arxiv.org/abs/1607.01990v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Representing Verbs with Rich Contexts: an Evaluation on Verb Similarity", "abstract": "Several studies on sentence processing suggest that the mental lexicon keeps\ntrack of the mutual expectations between words. Current DSMs, however,\nrepresent context words as separate features, thereby loosing important\ninformation for word expectations, such as word interrelations. In this paper,\nwe present a DSM that addresses this issue by defining verb contexts as joint\nsyntactic dependencies. We test our representation in a verb similarity task on\ntwo datasets, showing that joint contexts achieve performances comparable to\nsingle dependencies or even better. Moreover, they are able to overcome the\ndata sparsity problem of joint feature spaces, in spite of the limited size of\nour training corpus.", "published": "2016-07-07 16:00:33", "link": "http://arxiv.org/abs/1607.02061v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Scalable Semantic Matching of Queries to Ads in Sponsored Search\n  Advertising", "abstract": "Sponsored search represents a major source of revenue for web search engines.\nThis popular advertising model brings a unique possibility for advertisers to\ntarget users' immediate intent communicated through a search query, usually by\ndisplaying their ads alongside organic search results for queries deemed\nrelevant to their products or services. However, due to a large number of\nunique queries it is challenging for advertisers to identify all such relevant\nqueries. For this reason search engines often provide a service of advanced\nmatching, which automatically finds additional relevant queries for advertisers\nto bid on. We present a novel advanced matching approach based on the idea of\nsemantic embeddings of queries and ads. The embeddings were learned using a\nlarge data set of user search sessions, consisting of search queries, clicked\nads and search links, while utilizing contextual information such as dwell time\nand skipped ads. To address the large-scale nature of our problem, both in\nterms of data and vocabulary size, we propose a novel distributed algorithm for\ntraining of the embeddings. Finally, we present an approach for overcoming a\ncold-start problem associated with new ads and queries. We report results of\neditorial evaluation and online tests on actual search traffic. The results\nshow that our approach significantly outperforms baselines in terms of\nrelevance, coverage, and incremental revenue. Lastly, we open-source learned\nquery embeddings to be used by researchers in computational advertising and\nrelated fields.", "published": "2016-07-07 03:43:12", "link": "http://arxiv.org/abs/1607.01869v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Stock trend prediction using news sentiment analysis", "abstract": "Efficient Market Hypothesis is the popular theory about stock prediction.\nWith its failure much research has been carried in the area of prediction of\nstocks. This project is about taking non quantifiable data such as financial\nnews articles about a company and predicting its future stock trend with news\nsentiment classification. Assuming that news articles have impact on stock\nmarket, this is an attempt to study relationship between news and stock trend.\nTo show this, we created three different classification models which depict\npolarity of news articles being positive or negative. Observations show that RF\nand SVM perform well in all types of testing. Na\\\"ive Bayes gives good result\nbut not compared to the other two. Experiments are conducted to evaluate\nvarious aspects of the proposed model and encouraging results are obtained in\nall of the experiments. The accuracy of the prediction model is more than 80%\nand in comparison with news random labeling with 50% of accuracy; the model has\nincreased the accuracy by 30%.", "published": "2016-07-07 10:48:34", "link": "http://arxiv.org/abs/1607.01958v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sequence Training and Adaptation of Highway Deep Neural Networks", "abstract": "Highway deep neural network (HDNN) is a type of depth-gated feedforward\nneural network, which has shown to be easier to train with more hidden layers\nand also generalise better compared to conventional plain deep neural networks\n(DNNs). Previously, we investigated a structured HDNN architecture for speech\nrecognition, in which the two gate functions were tied across all the hidden\nlayers, and we were able to train a much smaller model without sacrificing the\nrecognition accuracy. In this paper, we carry on the study of this architecture\nwith sequence-discriminative training criterion and speaker adaptation\ntechniques on the AMI meeting speech recognition corpus. We show that these two\ntechniques improve speech recognition accuracy on top of the model trained with\nthe cross entropy criterion. Furthermore, we demonstrate that the two gate\nfunctions that are tied across all the hidden layers are able to control the\ninformation flow over the whole network, and we can achieve considerable\nimprovements by only updating these gate functions in both sequence training\nand adaptation experiments.", "published": "2016-07-07 11:24:51", "link": "http://arxiv.org/abs/1607.01963v5", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Predicting and Understanding Law-Making with Word Vectors and an\n  Ensemble Model", "abstract": "Out of nearly 70,000 bills introduced in the U.S. Congress from 2001 to 2015,\nonly 2,513 were enacted. We developed a machine learning approach to\nforecasting the probability that any bill will become law. Starting in 2001\nwith the 107th Congress, we trained models on data from previous Congresses,\npredicted all bills in the current Congress, and repeated until the 113th\nCongress served as the test. For prediction we scored each sentence of a bill\nwith a language model that embeds legislative vocabulary into a\nhigh-dimensional, semantic-laden vector space. This language representation\nenables our investigation into which words increase the probability of\nenactment for any topic. To test the relative importance of text and context,\nwe compared the text model to a context-only model that uses variables such as\nwhether the bill's sponsor is in the majority party. To test the effect of\nchanges to bills after their introduction on our ability to predict their final\noutcome, we compared using the bill text and meta-data available at the time of\nintroduction with using the most recent data. At the time of introduction\ncontext-only predictions outperform text-only, and with the newest data\ntext-only outperforms context-only. Combining text and context always performs\nbest. We conducted a global sensitivity analysis on the combined model to\ndetermine important variables predicting enactment.", "published": "2016-07-07 18:08:59", "link": "http://arxiv.org/abs/1607.02109v2", "categories": ["cs.CL", "physics.soc-ph", "stat.AP", "stat.ML"], "primary_category": "cs.CL"}
