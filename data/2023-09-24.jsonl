{"title": "MentaLLaMA: Interpretable Mental Health Analysis on Social Media with\n  Large Language Models", "abstract": "With the development of web technology, social media texts are becoming a\nrich source for automatic mental health analysis. As traditional discriminative\nmethods bear the problem of low interpretability, the recent large language\nmodels have been explored for interpretable mental health analysis on social\nmedia, which aims to provide detailed explanations along with predictions. The\nresults show that ChatGPT can generate approaching-human explanations for its\ncorrect classifications. However, LLMs still achieve unsatisfactory\nclassification performance in a zero-shot/few-shot manner. Domain-specific\nfinetuning is an effective solution, but faces 2 challenges: 1) lack of\nhigh-quality training data. 2) no open-source LLMs for interpretable mental\nhealth analysis were released to lower the finetuning cost. To alleviate these\nproblems, we build the first multi-task and multi-source interpretable mental\nhealth instruction (IMHI) dataset on social media, with 105K data samples. The\nraw social media data are collected from 10 existing sources covering 8 mental\nhealth analysis tasks. We use expert-written few-shot prompts and collected\nlabels to prompt ChatGPT and obtain explanations from its responses. To ensure\nthe reliability of the explanations, we perform strict automatic and human\nevaluations on the correctness, consistency, and quality of generated data.\nBased on the IMHI dataset and LLaMA2 foundation models, we train MentalLLaMA,\nthe first open-source LLM series for interpretable mental health analysis with\ninstruction-following capability. We also evaluate the performance of\nMentalLLaMA on the IMHI evaluation benchmark with 10 test sets, where their\ncorrectness for making predictions and the quality of explanations are\nexamined. The results show that MentalLLaMA approaches state-of-the-art\ndiscriminative methods in correctness and generates high-quality explanations.", "published": "2023-09-24 06:46:08", "link": "http://arxiv.org/abs/2309.13567v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Does the \"most sinfully decadent cake ever\" taste good? Answering Yes/No\n  Questions from Figurative Contexts", "abstract": "Figurative language is commonplace in natural language, and while making\ncommunication memorable and creative, can be difficult to understand. In this\nwork, we investigate the robustness of Question Answering (QA) models on\nfigurative text. Yes/no questions, in particular, are a useful probe of\nfigurative language understanding capabilities of large language models. We\npropose FigurativeQA, a set of 1000 yes/no questions with figurative and\nnon-figurative contexts, extracted from the domains of restaurant and product\nreviews. We show that state-of-the-art BERT-based QA models exhibit an average\nperformance drop of up to 15\\% points when answering questions from figurative\ncontexts, as compared to non-figurative ones. While models like GPT-3 and\nChatGPT are better at handling figurative texts, we show that further\nperformance gains can be achieved by automatically simplifying the figurative\ncontexts into their non-figurative (literal) counterparts. We find that the\nbest overall model is ChatGPT with chain-of-thought prompting to generate\nnon-figurative contexts. Our work provides a promising direction for building\nmore robust QA models with figurative language understanding capabilities.", "published": "2023-09-24 20:38:48", "link": "http://arxiv.org/abs/2309.13748v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text Classification: A Perspective of Deep Learning Methods", "abstract": "In recent years, with the rapid development of information on the Internet,\nthe number of complex texts and documents has increased exponentially, which\nrequires a deeper understanding of deep learning methods in order to accurately\nclassify texts using deep learning techniques, and thus deep learning methods\nhave become increasingly important in text classification. Text classification\nis a class of tasks that automatically classifies a set of documents into\nmultiple predefined categories based on their content and subject matter. Thus,\nthe main goal of text classification is to enable users to extract information\nfrom textual resources and process processes such as retrieval, classification,\nand machine learning techniques together in order to classify different\ncategories. Many new techniques of deep learning have already achieved\nexcellent results in natural language processing. The success of these learning\nalgorithms relies on their ability to understand complex models and non-linear\nrelationships in data. However, finding the right structure, architecture, and\ntechniques for text classification is a challenge for researchers. This paper\nintroduces deep learning-based text classification algorithms, including\nimportant steps required for text classification tasks such as feature\nextraction, feature reduction, and evaluation strategies and methods. At the\nend of the article, different deep learning text classification methods are\ncompared and summarized.", "published": "2023-09-24 21:49:51", "link": "http://arxiv.org/abs/2309.13761v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Study of Perceptual Training of Chinese Mandarin Tones for\n  Monolingual Speakers of English Using Adaptive Computer Based Training\n  Software", "abstract": "The study explored a new technique of phonetic tone training, which may have\na positive impact on second language learning and tone training.", "published": "2023-09-24 00:20:16", "link": "http://arxiv.org/abs/2309.13513v1", "categories": ["cs.HC", "cs.CL", "H.4.m"], "primary_category": "cs.HC"}
{"title": "Substituting Data Annotation with Balanced Updates and Collective Loss\n  in Multi-label Text Classification", "abstract": "Multi-label text classification (MLTC) is the task of assigning multiple\nlabels to a given text, and has a wide range of application domains. Most\nexisting approaches require an enormous amount of annotated data to learn a\nclassifier and/or a set of well-defined constraints on the label space\nstructure, such as hierarchical relations which may be complicated to provide\nas the number of labels increases. In this paper, we study the MLTC problem in\nannotation-free and scarce-annotation settings in which the magnitude of\navailable supervision signals is linear to the number of labels. Our method\nfollows three steps, (1) mapping input text into a set of preliminary label\nlikelihoods by natural language inference using a pre-trained language model,\n(2) calculating a signed label dependency graph by label descriptions, and (3)\nupdating the preliminary label likelihoods with message passing along the label\ndependency graph, driven with a collective loss function that injects the\ninformation of expected label frequency and average multi-label cardinality of\npredictions. The experiments show that the proposed framework achieves\neffective performance under low supervision settings with almost imperceptible\ncomputational and memory overheads added to the usage of pre-trained language\nmodel outperforming its initial performance by 70\\% in terms of example-based\nF1 score.", "published": "2023-09-24 04:12:52", "link": "http://arxiv.org/abs/2309.13543v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cordyceps@LT-EDI: Patching Language-Specific Homophobia/Transphobia\n  Classifiers with a Multilingual Understanding", "abstract": "Detecting transphobia, homophobia, and various other forms of hate speech is\ndifficult. Signals can vary depending on factors such as language, culture,\ngeographical region, and the particular online platform. Here, we present a\njoint multilingual (M-L) and language-specific (L-S) approach to homophobia and\ntransphobic hate speech detection (HSD). M-L models are needed to catch words,\nphrases, and concepts that are less common or missing in a particular language\nand subsequently overlooked by L-S models. Nonetheless, L-S models are better\nsituated to understand the cultural and linguistic context of the users who\ntypically write in a particular language. Here we construct a simple and\nsuccessful way to merge the M-L and L-S approaches through simple weight\ninterpolation in such a way that is interpretable and data-driven. We\ndemonstrate our system on task A of the 'Shared Task on Homophobia/Transphobia\nDetection in social media comments' dataset for homophobia and transphobic HSD.\nOur system achieves the best results in three of five languages and achieves a\n0.997 macro average F1-score on Malayalam texts.", "published": "2023-09-24 06:37:54", "link": "http://arxiv.org/abs/2309.13561v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Keeping in Time: Adding Temporal Context to Sentiment Analysis Models", "abstract": "This paper presents a state-of-the-art solution to the LongEval CLEF 2023 Lab\nTask 2: LongEval-Classification. The goal of this task is to improve and\npreserve the performance of sentiment analysis models across shorter and longer\ntime periods. Our framework feeds date-prefixed textual inputs to a pre-trained\nlanguage model, where the timestamp is included in the text. We show\ndate-prefixed samples better conditions model outputs on the temporal context\nof the respective texts. Moreover, we further boost performance by performing\nself-labeling on unlabeled data to train a student model. We augment the\nself-labeling process using a novel augmentation strategy leveraging the\ndate-prefixed formatting of our samples. We demonstrate concrete performance\ngains on the LongEval-Classification evaluation set over non-augmented\nself-labeling. Our framework achieves a 2nd place ranking with an overall score\nof 0.6923 and reports the best Relative Performance Drop (RPD) of -0.0656 over\nthe short evaluation set.", "published": "2023-09-24 06:38:21", "link": "http://arxiv.org/abs/2309.13562v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Embers of Autoregression: Understanding Large Language Models Through\n  the Problem They are Trained to Solve", "abstract": "The widespread adoption of large language models (LLMs) makes it important to\nrecognize their strengths and limitations. We argue that in order to develop a\nholistic understanding of these systems we need to consider the problem that\nthey were trained to solve: next-word prediction over Internet text. By\nrecognizing the pressures that this task exerts we can make predictions about\nthe strategies that LLMs will adopt, allowing us to reason about when they will\nsucceed or fail. This approach - which we call the teleological approach -\nleads us to identify three factors that we hypothesize will influence LLM\naccuracy: the probability of the task to be performed, the probability of the\ntarget output, and the probability of the provided input. We predict that LLMs\nwill achieve higher accuracy when these probabilities are high than when they\nare low - even in deterministic settings where probability should not matter.\nTo test our predictions, we evaluate two LLMs (GPT-3.5 and GPT-4) on eleven\ntasks, and we find robust evidence that LLMs are influenced by probability in\nthe ways that we have hypothesized. In many cases, the experiments reveal\nsurprising failure modes. For instance, GPT-4's accuracy at decoding a simple\ncipher is 51% when the output is a high-probability word sequence but only 13%\nwhen it is low-probability. These results show that AI practitioners should be\ncareful about using LLMs in low-probability situations. More broadly, we\nconclude that we should not evaluate LLMs as if they are humans but should\ninstead treat them as a distinct type of system - one that has been shaped by\nits own particular set of pressures.", "published": "2023-09-24 13:35:28", "link": "http://arxiv.org/abs/2309.13638v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Skill Check: Some Considerations on the Evaluation of Gamemastering\n  Models for Role-playing Games", "abstract": "In role-playing games a Game Master (GM) is the player in charge of the game,\nwho must design the challenges the players face and narrate the outcomes of\ntheir actions. In this work we discuss some challenges to model GMs from an\nInteractive Storytelling and Natural Language Processing perspective. Following\nthose challenges we propose three test categories to evaluate such dialogue\nsystems, and we use them to test ChatGPT, Bard and OpenAssistant as\nout-of-the-box GMs.", "published": "2023-09-24 17:19:36", "link": "http://arxiv.org/abs/2309.13702v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multiple Relations Classification using Imbalanced Predictions\n  Adaptation", "abstract": "The relation classification task assigns the proper semantic relation to a\npair of subject and object entities; the task plays a crucial role in various\ntext mining applications, such as knowledge graph construction and entities\ninteraction discovery in biomedical text. Current relation classification\nmodels employ additional procedures to identify multiple relations in a single\nsentence. Furthermore, they overlook the imbalanced predictions pattern. The\npattern arises from the presence of a few valid relations that need positive\nlabeling in a relatively large predefined relations set. We propose a multiple\nrelations classification model that tackles these issues through a customized\noutput architecture and by exploiting additional input features. Our findings\nsuggest that handling the imbalanced predictions leads to significant\nimprovements, even on a modest training design. The results demonstrate\nsuperiority performance on benchmark datasets commonly used in relation\nclassification. To the best of our knowledge, this work is the first that\nrecognizes the imbalanced predictions within the relation classification task.", "published": "2023-09-24 18:36:22", "link": "http://arxiv.org/abs/2309.13718v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Arabic Sentiment Analysis with Noisy Deep Explainable Model", "abstract": "Sentiment Analysis (SA) is an indispensable task for many real-world\napplications. Compared to limited resourced languages (i.e., Arabic, Bengali),\nmost of the research on SA are conducted for high resourced languages (i.e.,\nEnglish, Chinese). Moreover, the reasons behind any prediction of the Arabic\nsentiment analysis methods exploiting advanced artificial intelligence\n(AI)-based approaches are like black-box - quite difficult to understand. This\npaper proposes an explainable sentiment classification framework for the Arabic\nlanguage by introducing a noise layer on Bi-Directional Long Short-Term Memory\n(BiLSTM) and Convolutional Neural Networks (CNN)-BiLSTM models that overcome\nover-fitting problem. The proposed framework can explain specific predictions\nby training a local surrogate explainable model to understand why a particular\nsentiment (positive or negative) is being predicted. We carried out experiments\non public benchmark Arabic SA datasets. The results concluded that adding noise\nlayers improves the performance in sentiment analysis for the Arabic language\nby reducing overfitting and our method outperformed some known state-of-the-art\nmethods. In addition, the introduced explainability with noise layer could make\nthe model more transparent and accountable and hence help adopting AI-enabled\nsystem in practice.", "published": "2023-09-24 19:26:53", "link": "http://arxiv.org/abs/2309.13731v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Prompting and Fine-Tuning Open-Sourced Large Language Models for Stance\n  Classification", "abstract": "Stance classification, the task of predicting the viewpoint of an author on a\nsubject of interest, has long been a focal point of research in domains ranging\nfrom social science to machine learning. Current stance detection methods rely\npredominantly on manual annotation of sentences, followed by training a\nsupervised machine learning model. However, this manual annotation process\nrequires laborious annotation effort, and thus hampers its potential to\ngeneralize across different contexts. In this work, we investigate the use of\nLarge Language Models (LLMs) as a stance detection methodology that can reduce\nor even eliminate the need for manual annotations. We investigate 10\nopen-source models and 7 prompting schemes, finding that LLMs are competitive\nwith in-domain supervised models but are not necessarily consistent in their\nperformance. We also fine-tuned the LLMs, but discovered that fine-tuning\nprocess does not necessarily lead to better performance. In general, we\ndiscover that LLMs do not routinely outperform their smaller supervised machine\nlearning models, and thus call for stance detection to be a benchmark for which\nLLMs also optimize for. The code used in this study is available at\n\\url{https://github.com/ijcruic/LLM-Stance-Labeling}", "published": "2023-09-24 19:36:17", "link": "http://arxiv.org/abs/2309.13734v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Survey of Social Bias in Vision-Language Models", "abstract": "In recent years, the rapid advancement of machine learning (ML) models,\nparticularly transformer-based pre-trained models, has revolutionized Natural\nLanguage Processing (NLP) and Computer Vision (CV) fields. However, researchers\nhave discovered that these models can inadvertently capture and reinforce\nsocial biases present in their training datasets, leading to potential social\nharms, such as uneven resource allocation and unfair representation of specific\nsocial groups. Addressing these biases and ensuring fairness in artificial\nintelligence (AI) systems has become a critical concern in the ML community.\n  The recent introduction of pre-trained vision-and-language (VL) models in the\nemerging multimodal field demands attention to the potential social biases\npresent in these models as well. Although VL models are susceptible to social\nbias, there is a limited understanding compared to the extensive discussions on\nbias in NLP and CV. This survey aims to provide researchers with a high-level\ninsight into the similarities and differences of social bias studies in\npre-trained models across NLP, CV, and VL. By examining these perspectives, the\nsurvey aims to offer valuable guidelines on how to approach and mitigate social\nbias in both unimodal and multimodal settings. The findings and recommendations\npresented here can benefit the ML community, fostering the development of\nfairer and non-biased AI models in various applications and research endeavors.", "published": "2023-09-24 15:34:56", "link": "http://arxiv.org/abs/2309.14381v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Agree To Disagree", "abstract": "How frequently do individuals thoroughly review terms and conditions before\nproceeding to register for a service, install software, or access a website?\nThe majority of internet users do not engage in this practice. This trend is\nnot surprising, given that terms and conditions typically consist of lengthy\ndocuments replete with intricate legal terminology and convoluted sentences. In\nthis paper, we introduce a Machine Learning-powered approach designed to\nautomatically parse and summarize critical information in a user-friendly\nmanner. This technology focuses on distilling the pertinent details that users\nshould contemplate before committing to an agreement.", "published": "2023-09-24 18:06:45", "link": "http://arxiv.org/abs/2309.14382v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Cordyceps@LT-EDI: Depression Detection with Reddit and Self-training", "abstract": "Depression is debilitating, and not uncommon. Indeed, studies of excessive\nsocial media users show correlations with depression, ADHD, and other mental\nhealth concerns. Given that there is a large number of people with excessive\nsocial media usage, then there is a significant population of potentially\nundiagnosed users and posts that they create. In this paper, we propose a\ndepression severity detection system using a semi-supervised learning technique\nto predict if a post is from a user who is experiencing severe, moderate, or\nlow (non-diagnostic) levels of depression. Namely, we use a trained model to\nclassify a large number of unlabelled social media posts from Reddit, then use\nthese generated labels to train a more powerful classifier. We demonstrate our\nframework on Detecting Signs of Depression from Social Media Text -\nLT-EDI@RANLP 2023 shared task, where our framework ranks 3rd overall.", "published": "2023-09-24 01:14:49", "link": "http://arxiv.org/abs/2310.01418v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "EvalLM: Interactive Evaluation of Large Language Model Prompts on\n  User-Defined Criteria", "abstract": "By simply composing prompts, developers can prototype novel generative\napplications with Large Language Models (LLMs). To refine prototypes into\nproducts, however, developers must iteratively revise prompts by evaluating\noutputs to diagnose weaknesses. Formative interviews (N=8) revealed that\ndevelopers invest significant effort in manually evaluating outputs as they\nassess context-specific and subjective criteria. We present EvalLM, an\ninteractive system for iteratively refining prompts by evaluating multiple\noutputs on user-defined criteria. By describing criteria in natural language,\nusers can employ the system's LLM-based evaluator to get an overview of where\nprompts excel or fail, and improve these based on the evaluator's feedback. A\ncomparative study (N=12) showed that EvalLM, when compared to manual\nevaluation, helped participants compose more diverse criteria, examine twice as\nmany outputs, and reach satisfactory prompts with 59% fewer revisions. Beyond\nprompts, our work can be extended to augment model evaluation and alignment in\nspecific application contexts.", "published": "2023-09-24 13:19:38", "link": "http://arxiv.org/abs/2309.13633v2", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "ALLURE: Auditing and Improving LLM-based Evaluation of Text using\n  Iterative In-Context-Learning", "abstract": "From grading papers to summarizing medical documents, large language models\n(LLMs) are evermore used for evaluation of text generated by humans and AI\nalike. However, despite their extensive utility, LLMs exhibit distinct failure\nmodes, necessitating a thorough audit and improvement of their text evaluation\ncapabilities. Here we introduce ALLURE, a systematic approach to Auditing Large\nLanguage Models Understanding and Reasoning Errors. ALLURE involves comparing\nLLM-generated evaluations with annotated data, and iteratively incorporating\ninstances of significant deviation into the evaluator, which leverages\nin-context learning (ICL) to enhance and improve robust evaluation of text by\nLLMs. Through this iterative process, we refine the performance of the\nevaluator LLM, ultimately reducing reliance on human annotators in the\nevaluation process. We anticipate ALLURE to serve diverse applications of LLMs\nin various domains related to evaluation of textual data, such as medical\nsummarization, education, and and productivity.", "published": "2023-09-24 17:15:58", "link": "http://arxiv.org/abs/2309.13701v2", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "A Text Classification-Based Approach for Evaluating and Enhancing the\n  Machine Interpretability of Building Codes", "abstract": "Interpreting regulatory documents or building codes into computer-processable\nformats is essential for the intelligent design and construction of buildings\nand infrastructures. Although automated rule interpretation (ARI) methods have\nbeen investigated for years, most of them highly depend on the early and manual\nfiltering of interpretable clauses from a building code. While few of them\nconsidered machine interpretability, which represents the potential to be\ntransformed into a computer-processable format, from both clause- and\ndocument-level. Therefore, this research aims to propose a novel approach to\nautomatically evaluate and enhance the machine interpretability of single\nclause and building codes. First, a few categories are introduced to classify\neach clause in a building code considering the requirements for rule\ninterpretation, and a dataset is developed for model training. Then, an\nefficient text classification model is developed based on a pretrained\ndomain-specific language model and transfer learning techniques. Finally, a\nquantitative evaluation method is proposed to assess the overall\ninterpretability of building codes. Experiments show that the proposed text\nclassification algorithm outperforms the existing CNN- or RNN-based methods,\nimproving the F1-score from 72.16% to 93.60%. It is also illustrated that the\nproposed classification method can enhance downstream ARI methods with an\nimprovement of 4%. Furthermore, analyzing the results of more than 150 building\ncodes in China showed that their average interpretability is 34.40%, which\nimplies that it is still hard to fully transform the entire regulatory document\ninto computer-processable formats. It is also argued that the interpretability\nof building codes should be further improved both from the human side and the\nmachine side.", "published": "2023-09-24 11:36:21", "link": "http://arxiv.org/abs/2309.14374v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Machine-assisted quantitizing designs: augmenting humanities and social\n  sciences with artificial intelligence", "abstract": "The increasing capacities of large language models (LLMs) have been shown to\npresent an unprecedented opportunity to scale up data analytics in the\nhumanities and social sciences, by automating complex qualitative tasks\notherwise typically carried out by human researchers. While numerous\nbenchmarking studies have assessed the analytic prowess of LLMs, there is less\nfocus on operationalizing this capacity for inference and hypothesis testing.\nAddressing this challenge, a systematic framework is argued for here, building\non mixed methods quantitizing and converting design principles, and feature\nanalysis from linguistics, to transparently integrate human expertise and\nmachine scalability. Replicability and statistical robustness are discussed,\nincluding how to incorporate machine annotator error rates in subsequent\ninference. The approach is discussed and demonstrated in over a dozen\nLLM-assisted case studies, covering 9 diverse languages, multiple disciplines\nand tasks, including analysis of themes, stances, ideas, and genre\ncompositions; linguistic and semantic annotation, interviews, text mining and\nevent cause inference in noisy historical data, literary social network\nconstruction, metadata imputation, and multimodal visual cultural analytics.\nUsing hypothesis-driven topic classification instead of \"distant reading\" is\ndiscussed. The replications among the experiments also illustrate how tasks\npreviously requiring protracted team effort or complex computational pipelines\ncan now be accomplished by an LLM-assisted scholar in a fraction of the time.\nImportantly, the approach is not intended to replace, but to augment and scale\nresearcher expertise and analytic practices. With these opportunities in sight,\nqualitative skills and the ability to pose insightful questions have arguably\nnever been more critical.", "published": "2023-09-24 14:21:50", "link": "http://arxiv.org/abs/2309.14379v2", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Natural Language based Context Modeling and Reasoning for Ubiquitous\n  Computing with Large Language Models: A Tutorial", "abstract": "Large language models (LLMs) have become phenomenally surging, since\n2018--two decades after introducing context-awareness into computing systems.\nThrough taking into account the situations of ubiquitous devices, users and the\nsocieties, context-aware computing has enabled a wide spectrum of innovative\napplications, such as assisted living, location-based social network services\nand so on. To recognize contexts and make decisions for actions accordingly,\nvarious artificial intelligence technologies, such as Ontology and OWL, have\nbeen adopted as representations for context modeling and reasoning. Recently,\nwith the rise of LLMs and their improved natural language understanding and\nreasoning capabilities, it has become feasible to model contexts using natural\nlanguage and perform context reasoning by interacting with LLMs such as ChatGPT\nand GPT-4. In this tutorial, we demonstrate the use of texts, prompts, and\nautonomous agents (AutoAgents) that enable LLMs to perform context modeling and\nreasoning without requiring fine-tuning of the model. We organize and introduce\nworks in the related field, and name this computing paradigm as the LLM-driven\nContext-aware Computing (LCaC). In the LCaC paradigm, users' requests, sensors\nreading data, and the command to actuators are supposed to be represented as\ntexts. Given the text of users' request and sensor data, the AutoAgent models\nthe context by prompting and sends to the LLM for context reasoning. LLM\ngenerates a plan of actions and responds to the AutoAgent, which later follows\nthe action plan to foster context-awareness. To prove the concepts, we use two\nshowcases--(1) operating a mobile z-arm in an apartment for assisted living,\nand (2) planning a trip and scheduling the itinerary in a context-aware and\npersonalized manner.", "published": "2023-09-24 00:15:39", "link": "http://arxiv.org/abs/2309.15074v2", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.NI"], "primary_category": "cs.CL"}
{"title": "VoiceLDM: Text-to-Speech with Environmental Context", "abstract": "This paper presents VoiceLDM, a model designed to produce audio that\naccurately follows two distinct natural language text prompts: the description\nprompt and the content prompt. The former provides information about the\noverall environmental context of the audio, while the latter conveys the\nlinguistic content. To achieve this, we adopt a text-to-audio (TTA) model based\non latent diffusion models and extend its functionality to incorporate an\nadditional content prompt as a conditional input. By utilizing pretrained\ncontrastive language-audio pretraining (CLAP) and Whisper, VoiceLDM is trained\non large amounts of real-world audio without manual annotations or\ntranscriptions. Additionally, we employ dual classifier-free guidance to\nfurther enhance the controllability of VoiceLDM. Experimental results\ndemonstrate that VoiceLDM is capable of generating plausible audio that aligns\nwell with both input conditions, even surpassing the speech intelligibility of\nthe ground truth audio on the AudioCaps test set. Furthermore, we explore the\ntext-to-speech (TTS) and zero-shot text-to-audio capabilities of VoiceLDM and\nshow that it achieves competitive results. Demos and code are available at\nhttps://voiceldm.github.io.", "published": "2023-09-24 15:20:59", "link": "http://arxiv.org/abs/2309.13664v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Human Transcription Quality Improvement", "abstract": "High quality transcription data is crucial for training automatic speech\nrecognition (ASR) systems. However, the existing industry-level data collection\npipelines are expensive to researchers, while the quality of crowdsourced\ntranscription is low. In this paper, we propose a reliable method to collect\nspeech transcriptions. We introduce two mechanisms to improve transcription\nquality: confidence estimation based reprocessing at labeling stage, and\nautomatic word error correction at post-labeling stage. We collect and release\nLibriCrowd - a large-scale crowdsourced dataset of audio transcriptions on 100\nhours of English speech. Experiment shows the Transcription WER is reduced by\nover 50%. We further investigate the impact of transcription error on ASR model\nperformance and found a strong correlation. The transcription quality\nimprovement provides over 10% relative WER reduction for ASR models. We release\nthe dataset and code to benefit the research community.", "published": "2023-09-24 03:39:43", "link": "http://arxiv.org/abs/2309.14372v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Coco-Nut: Corpus of Japanese Utterance and Voice Characteristics\n  Description for Prompt-based Control", "abstract": "In text-to-speech, controlling voice characteristics is important in\nachieving various-purpose speech synthesis. Considering the success of\ntext-conditioned generation, such as text-to-image, free-form text instruction\nshould be useful for intuitive and complicated control of voice\ncharacteristics. A sufficiently large corpus of high-quality and diverse voice\nsamples with corresponding free-form descriptions can advance such control\nresearch. However, neither an open corpus nor a scalable method is currently\navailable. To this end, we develop Coco-Nut, a new corpus including diverse\nJapanese utterances, along with text transcriptions and free-form voice\ncharacteristics descriptions. Our methodology to construct this corpus consists\nof 1) automatic collection of voice-related audio data from the Internet, 2)\nquality assurance, and 3) manual annotation using crowdsourcing. Additionally,\nwe benchmark our corpus on the prompt embedding model trained by contrastive\nspeech-text learning.", "published": "2023-09-24 00:15:31", "link": "http://arxiv.org/abs/2309.13509v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The second multi-channel multi-party meeting transcription challenge\n  (M2MeT) 2.0): A benchmark for speaker-attributed ASR", "abstract": "With the success of the first Multi-channel Multi-party Meeting Transcription\nchallenge (M2MeT), the second M2MeT challenge (M2MeT 2.0) held in ASRU2023\nparticularly aims to tackle the complex task of \\emph{speaker-attributed ASR\n(SA-ASR)}, which directly addresses the practical and challenging problem of\n``who spoke what at when\" at typical meeting scenario. We particularly\nestablished two sub-tracks. The fixed training condition sub-track, where the\ntraining data is constrained to predetermined datasets, but participants can\nuse any open-source pre-trained model. The open training condition sub-track,\nwhich allows for the use of all available data and models without limitation.\nIn addition, we release a new 10-hour test set for challenge ranking. This\npaper provides an overview of the dataset, track settings, results, and\nanalysis of submitted systems, as a benchmark to show the current state of\nspeaker-attributed ASR.", "published": "2023-09-24 07:51:52", "link": "http://arxiv.org/abs/2309.13573v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Efficient Black-Box Speaker Verification Model Adaptation with\n  Reprogramming and Backend Learning", "abstract": "The development of deep neural networks (DNN) has significantly enhanced the\nperformance of speaker verification (SV) systems in recent years. However, a\ncritical issue that persists when applying DNN-based SV systems in practical\napplications is domain mismatch. To mitigate the performance degradation caused\nby the mismatch, domain adaptation becomes necessary. This paper introduces an\napproach to adapt DNN-based SV models by manipulating the learnable model\ninputs, inspired by the concept of adversarial reprogramming. The pre-trained\nSV model remains fixed and functions solely in the forward process, resembling\na black-box model. A lightweight network is utilized to estimate the gradients\nfor the learnable parameters at the input, which bypasses the gradient\nbackpropagation through the black-box model. The reprogrammed output is\nprocessed by a two-layer backend learning module as the final adapted speaker\nembedding. The number of parameters involved in the gradient calculation is\nsmall in our design. With few additional parameters, the proposed method\nachieves both memory and parameter efficiency. The experiments are conducted in\nlanguage mismatch scenarios. Using much less computation cost, the proposed\nmethod obtains close or superior performance to the fully finetuned models in\nour experiments, which demonstrates its effectiveness.", "published": "2023-09-24 10:50:31", "link": "http://arxiv.org/abs/2309.13605v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Cross-modal Alignment with Optimal Transport for CTC-based ASR", "abstract": "Temporal connectionist temporal classification (CTC)-based automatic speech\nrecognition (ASR) is one of the most successful end to end (E2E) ASR\nframeworks. However, due to the token independence assumption in decoding, an\nexternal language model (LM) is required which destroys its fast parallel\ndecoding property. Several studies have been proposed to transfer linguistic\nknowledge from a pretrained LM (PLM) to the CTC based ASR. Since the PLM is\nbuilt from text while the acoustic model is trained with speech, a cross-modal\nalignment is required in order to transfer the context dependent linguistic\nknowledge from the PLM to acoustic encoding. In this study, we propose a novel\ncross-modal alignment algorithm based on optimal transport (OT). In the\nalignment process, a transport coupling matrix is obtained using OT, which is\nthen utilized to transform a latent acoustic representation for matching the\ncontext-dependent linguistic features encoded by the PLM. Based on the\nalignment, the latent acoustic feature is forced to encode context dependent\nlinguistic information. We integrate this latent acoustic feature to build\nconformer encoder-based CTC ASR system. On the AISHELL-1 data corpus, our\nsystem achieved 3.96% and 4.27% character error rate (CER) for dev and test\nsets, respectively, which corresponds to relative improvements of 28.39% and\n29.42% compared to the baseline conformer CTC ASR system without cross-modal\nknowledge transfer.", "published": "2023-09-24 14:34:20", "link": "http://arxiv.org/abs/2309.13650v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speech enhancement with frequency domain auto-regressive modeling", "abstract": "Speech applications in far-field real world settings often deal with signals\nthat are corrupted by reverberation. The task of dereverberation constitutes an\nimportant step to improve the audible quality and to reduce the error rates in\napplications like automatic speech recognition (ASR). We propose a unified\nframework of speech dereverberation for improving the speech quality and the\nASR performance using the approach of envelope-carrier decomposition provided\nby an autoregressive (AR) model. The AR model is applied in the frequency\ndomain of the sub-band speech signals to separate the envelope and carrier\nparts. A novel neural architecture based on dual path long short term memory\n(DPLSTM) model is proposed, which jointly enhances the sub-band envelope and\ncarrier components. The dereverberated envelope-carrier signals are modulated\nand the sub-band signals are synthesized to reconstruct the audio signal back.\nThe DPLSTM model for dereverberation of envelope and carrier components also\nallows the joint learning of the network weights for the down stream ASR task.\nIn the ASR tasks on the REVERB challenge dataset as well as on the VOiCES\ndataset, we illustrate that the joint learning of speech dereverberation\nnetwork and the E2E ASR model yields significant performance improvements over\nthe baseline ASR system trained on log-mel spectrogram as well as other\nbenchmarks for dereverberation (average relative improvements of 10-24% over\nthe baseline system). The speech quality improvements, evaluated using\nsubjective listening tests, further highlight the improved quality of the\nreconstructed audio.", "published": "2023-09-24 03:25:51", "link": "http://arxiv.org/abs/2309.13537v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Towards using Cough for Respiratory Disease Diagnosis by leveraging\n  Artificial Intelligence: A Survey", "abstract": "Cough acoustics contain multitudes of vital information about\npathomorphological alterations in the respiratory system. Reliable and accurate\ndetection of cough events by investigating the underlying cough latent features\nand disease diagnosis can play an indispensable role in revitalizing the\nhealthcare practices. The recent application of Artificial Intelligence (AI)\nand advances of ubiquitous computing for respiratory disease prediction has\ncreated an auspicious trend and myriad of future possibilities in the medical\ndomain. In particular, there is an expeditiously emerging trend of Machine\nlearning (ML) and Deep Learning (DL)-based diagnostic algorithms exploiting\ncough signatures. The enormous body of literature on cough-based AI algorithms\ndemonstrate that these models can play a significant role for detecting the\nonset of a specific respiratory disease. However, it is pertinent to collect\nthe information from all relevant studies in an exhaustive manner for the\nmedical experts and AI scientists to analyze the decisive role of AI/ML. This\nsurvey offers a comprehensive overview of the cough data-driven ML/DL detection\nand preliminary diagnosis frameworks, along with a detailed list of significant\nfeatures. We investigate the mechanism that causes cough and the latent cough\nfeatures of the respiratory modalities. We also analyze the customized cough\nmonitoring application, and their AI-powered recognition algorithms. Challenges\nand prospective future research directions to develop practical, robust, and\nubiquitous solutions are also discussed in detail.", "published": "2023-09-24 19:03:46", "link": "http://arxiv.org/abs/2309.14383v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Related Rhythms: Recommendation System To Discover Music You May Like", "abstract": "Machine Learning models are being utilized extensively to drive recommender\nsystems, which is a widely explored topic today. This is especially true of the\nmusic industry, where we are witnessing a surge in growth. Besides a large\nchunk of active users, these systems are fueled by massive amounts of data.\nThese large-scale systems yield applications that aim to provide a better user\nexperience and to keep customers actively engaged. In this paper, a distributed\nMachine Learning (ML) pipeline is delineated, which is capable of taking a\nsubset of songs as input and producing a new subset of songs identified as\nbeing similar to the inputted subset. The publicly accessible Million Songs\nDataset (MSD) enables researchers to develop and explore reasonably efficient\nsystems for audio track analysis and recommendations, without having to access\na commercialized music platform. The objective of the proposed application is\nto leverage an ML system trained to optimally recommend songs that a user might\nlike.", "published": "2023-09-24 04:18:40", "link": "http://arxiv.org/abs/2309.13544v1", "categories": ["cs.IR", "cs.AI", "cs.LG", "cs.SD", "eess.AS", "I.2.6; H.3.3"], "primary_category": "cs.IR"}
