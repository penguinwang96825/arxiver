{"title": "Too good to be true? Predicting author profiles from abusive language", "abstract": "The problem of online threats and abuse could potentially be mitigated with a\ncomputational approach, where sources of abuse are better understood or\nidentified through author profiling. However, abusive language constitutes a\nspecific domain of language for which it has not yet been tested whether\ndifferences emerge based on a text author's personality, age, or gender. This\nstudy examines statistical relationships between author demographics and\nabusive vs normal language, and performs prediction experiments for\npersonality, age, and gender. Although some statistical relationships were\nestablished between author characteristics and language use, these patterns did\nnot translate to high prediction performance. Personality traits were predicted\nwithin 15% of their actual value, age was predicted with an error margin of 10\nyears, and gender was classified correctly in 70% of the cases. These results\nare poor when compared to previous research on author profiling, therefore we\nurge caution in applying this within the context of abusive language and threat\nassessment.", "published": "2020-09-02 15:05:43", "link": "http://arxiv.org/abs/2009.01126v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An exploratory study of L1-specific non-words", "abstract": "In this paper, we explore L1-specific non-words, i.e. non-words in a target\nlanguage (in this case Swedish) that are re-ranked by a different-language\nlanguage model. We surmise that speakers of a certain L1 will react different\nto L1-specific non-words than to general non-words. We present the results from\ntwo small case studies exploring whether re-ranking non-words with different\nlanguage models leads to a perceived difference in `Swedishness' (pilot study\n1) and whether German and English native speakers have longer reaction times in\na lexical decision task when presented with their respective L1-specific\nnon-words (pilot study 2). Tentative results seem to indicate that L1-specific\nnon-words are processed second-slowest, after purely Swedish-looking non-words.", "published": "2020-09-02 15:18:55", "link": "http://arxiv.org/abs/2009.01134v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Garain at SemEval-2020 Task 12: Sequence based Deep Learning for\n  Categorizing Offensive Language in Social Media", "abstract": "SemEval-2020 Task 12 was OffenseEval: Multilingual Offensive Language\nIdentification in Social Media (Zampieri et al., 2020). The task was subdivided\ninto multiple languages and datasets were provided for each one. The task was\nfurther divided into three sub-tasks: offensive language identification,\nautomatic categorization of offense types, and offense target identification. I\nhave participated in the task-C, that is, offense target identification. For\npreparing the proposed system, I have made use of Deep Learning networks like\nLSTMs and frameworks like Keras which combine the bag of words model with\nautomatically generated sequence based features and manually extracted features\nfrom the given dataset. My system on training on 25% of the whole dataset\nachieves macro averaged f1 score of 47.763%.", "published": "2020-09-02 17:09:29", "link": "http://arxiv.org/abs/2009.01195v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparative Evaluation of Pretrained Transfer Learning Models on\n  Automatic Short Answer Grading", "abstract": "Automatic Short Answer Grading (ASAG) is the process of grading the student\nanswers by computational approaches given a question and the desired answer.\nPrevious works implemented the methods of concept mapping, facet mapping, and\nsome used the conventional word embeddings for extracting semantic features.\nThey extracted multiple features manually to train on the corresponding\ndatasets. We use pretrained embeddings of the transfer learning models, ELMo,\nBERT, GPT, and GPT-2 to assess their efficiency on this task. We train with a\nsingle feature, cosine similarity, extracted from the embeddings of these\nmodels. We compare the RMSE scores and correlation measurements of the four\nmodels with previous works on Mohler dataset. Our work demonstrates that ELMo\noutperformed the other three models. We also, briefly describe the four\ntransfer learning models and conclude with the possible causes of poor results\nof transfer learning models.", "published": "2020-09-02 19:07:34", "link": "http://arxiv.org/abs/2009.01303v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "A Simple Global Neural Discourse Parser", "abstract": "Discourse parsing is largely dominated by greedy parsers with\nmanually-designed features, while global parsing is rare due to its\ncomputational expense. In this paper, we propose a simple chart-based neural\ndiscourse parser that does not require any manually-crafted features and is\nbased on learned span representations only. To overcome the computational\nchallenge, we propose an independence assumption between the label assigned to\na node in the tree and the splitting point that separates its children, which\nresults in tractable decoding. We empirically demonstrate that our model\nachieves the best performance among global parsers, and comparable performance\nto state-of-art greedy parsers, using only learned span representations.", "published": "2020-09-02 19:28:40", "link": "http://arxiv.org/abs/2009.01312v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automated Storytelling via Causal, Commonsense Plot Ordering", "abstract": "Automated story plot generation is the task of generating a coherent sequence\nof plot events. Causal relations between plot events are believed to increase\nthe perception of story and plot coherence. In this work, we introduce the\nconcept of soft causal relations as causal relations inferred from commonsense\nreasoning. We demonstrate C2PO, an approach to narrative generation that\noperationalizes this concept through Causal, Commonsense Plot Ordering. Using\nhuman-participant protocols, we evaluate our system against baseline systems\nwith different commonsense reasoning reasoning and inductive biases to\ndetermine the role of soft causal relations in perceived story quality. Through\nthese studies we also probe the interplay of how changes in commonsense norms\nacross storytelling genres affect perceptions of story quality.", "published": "2020-09-02 05:37:03", "link": "http://arxiv.org/abs/2009.00829v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Practical Chinese Dependency Parser Based on A Large-scale Dataset", "abstract": "Dependency parsing is a longstanding natural language processing task, with\nits outputs crucial to various downstream tasks. Recently, neural network based\n(NN-based) dependency parsing has achieved significant progress and obtained\nthe state-of-the-art results. As we all know, NN-based approaches require\nmassive amounts of labeled training data, which is very expensive because it\nrequires human annotation by experts. Thus few industrial-oriented dependency\nparser tools are publicly available. In this report, we present Baidu\nDependency Parser (DDParser), a new Chinese dependency parser trained on a\nlarge-scale manually labeled dataset called Baidu Chinese Treebank (DuCTB).\nDuCTB consists of about one million annotated sentences from multiple sources\nincluding search logs, Chinese newswire, various forum discourses, and\nconversation programs. DDParser is extended on the graph-based biaffine parser\nto accommodate to the characteristics of Chinese dataset. We conduct\nexperiments on two test sets: the standard test set with the same distribution\nas the training set and the random test set sampled from other sources, and the\nlabeled attachment scores (LAS) of them are 92.9% and 86.9% respectively.\nDDParser achieves the state-of-the-art results, and is released at\nhttps://github.com/baidu/DDParser.", "published": "2020-09-02 08:41:46", "link": "http://arxiv.org/abs/2009.00901v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Revisiting the Open-Domain Question Answering Pipeline", "abstract": "Open-domain question answering (QA) is the tasl of identifying answers to\nnatural questions from a large corpus of documents. The typical open-domain QA\nsystem starts with information retrieval to select a subset of documents from\nthe corpus, which are then processed by a machine reader to select the answer\nspans. This paper describes Mindstone, an open-domain QA system that consists\nof a new multi-stage pipeline that employs a traditional BM25-based information\nretriever, RM3-based neural relevance feedback, neural ranker, and a machine\nreading comprehension stage. This paper establishes a new baseline for\nend-to-end performance on question answering for Wikipedia/SQuAD dataset\n(EM=58.1, F1=65.8), with substantial gains over the previous state of the art\n(Yang et al., 2019b). We also show how the new pipeline enables the use of\nlow-resolution labels, and can be easily tuned to meet various timing\nrequirements.", "published": "2020-09-02 09:34:14", "link": "http://arxiv.org/abs/2009.00914v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PGST: a Polyglot Gender Style Transfer method", "abstract": "Recent developments in Text Style Transfer have led this field to be more\nhighlighted than ever. The task of transferring an input's style to another is\naccompanied by plenty of challenges (e.g., fluency and content preservation)\nthat need to be taken care of. In this research, we introduce PGST, a novel\npolyglot text style transfer approach in the gender domain, composed of\ndifferent constitutive elements. In contrast to prior studies, it is feasible\nto apply a style transfer method in multiple languages by fulfilling our\nmethod's predefined elements. We have proceeded with a pre-trained word\nembedding for token replacement purposes, a character-based token classifier\nfor gender exchange purposes, and a beam search algorithm for extracting the\nmost fluent combination. Since different approaches are introduced in our\nresearch, we determine a trade-off value for evaluating different models'\nsuccess in faking our gender identification model with transferred text. To\ndemonstrate our method's multilingual applicability, we applied our method on\nboth English and Persian corpora and ended up defeating our proposed gender\nidentification model by 45.6% and 39.2%, respectively. While this research's\nfocus is not limited to a specific language, our obtained evaluation results\nare highly competitive in an analogy among English state of the art methods.", "published": "2020-09-02 13:15:11", "link": "http://arxiv.org/abs/2009.01040v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ASTRAL: Adversarial Trained LSTM-CNN for Named Entity Recognition", "abstract": "Named Entity Recognition (NER) is a challenging task that extracts named\nentities from unstructured text data, including news, articles, social\ncomments, etc. The NER system has been studied for decades. Recently, the\ndevelopment of Deep Neural Networks and the progress of pre-trained word\nembedding have become a driving force for NER. Under such circumstances, how to\nmake full use of the information extracted by word embedding requires more\nin-depth research. In this paper, we propose an Adversarial Trained LSTM-CNN\n(ASTRAL) system to improve the current NER method from both the model structure\nand the training process. In order to make use of the spatial information\nbetween adjacent words, Gated-CNN is introduced to fuse the information of\nadjacent words. Besides, a specific Adversarial training method is proposed to\ndeal with the overfitting problem in NER. We add perturbation to variables in\nthe network during the training process, making the variables more diverse,\nimproving the generalization and robustness of the model. Our model is\nevaluated on three benchmarks, CoNLL-03, OntoNotes 5.0, and WNUT-17, achieving\nstate-of-the-art results. Ablation study and case study also show that our\nsystem can converge faster and is less prone to overfitting.", "published": "2020-09-02 13:15:25", "link": "http://arxiv.org/abs/2009.01041v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SRQA: Synthetic Reader for Factoid Question Answering", "abstract": "The question answering system can answer questions from various fields and\nforms with deep neural networks, but it still lacks effective ways when facing\nmultiple evidences. We introduce a new model called SRQA, which means Synthetic\nReader for Factoid Question Answering. This model enhances the question\nanswering system in the multi-document scenario from three aspects: model\nstructure, optimization goal, and training method, corresponding to Multilayer\nAttention (MA), Cross Evidence (CE), and Adversarial Training (AT)\nrespectively. First, we propose a multilayer attention network to obtain a\nbetter representation of the evidences. The multilayer attention mechanism\nconducts interaction between the question and the passage within each layer,\nmaking the token representation of evidences in each layer takes the\nrequirement of the question into account. Second, we design a cross evidence\nstrategy to choose the answer span within more evidences. We improve the\noptimization goal, considering all the answers' locations in multiple evidences\nas training targets, which leads the model to reason among multiple evidences.\nThird, adversarial training is employed to high-level variables besides the\nword embedding in our model. A new normalization method is also proposed for\nadversarial perturbations so that we can jointly add perturbations to several\ntarget variables. As an effective regularization method, adversarial training\nenhances the model's ability to process noisy data. Combining these three\nstrategies, we enhance the contextual representation and locating ability of\nour model, which could synthetically extract the answer span from several\nevidences. We perform SRQA on the WebQA dataset, and experiments show that our\nmodel outperforms the state-of-the-art models (the best fuzzy score of our\nmodel is up to 78.56%, with an improvement of about 2%).", "published": "2020-09-02 13:16:24", "link": "http://arxiv.org/abs/2009.01630v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Identifying Documents In-Scope of a Collection from Web Archives", "abstract": "Web archive data usually contains high-quality documents that are very useful\nfor creating specialized collections of documents, e.g., scientific digital\nlibraries and repositories of technical reports. In doing so, there is a\nsubstantial need for automatic approaches that can distinguish the documents of\ninterest for a collection out of the huge number of documents collected by web\narchiving institutions. In this paper, we explore different learning models and\nfeature representations to determine the best performing ones for identifying\nthe documents of interest from the web archived data. Specifically, we study\nboth machine learning and deep learning models and \"bag of words\" (BoW)\nfeatures extracted from the entire document or from specific portions of the\ndocument, as well as structural features that capture the structure of\ndocuments. We focus our evaluation on three datasets that we created from three\ndifferent Web archives. Our experimental results show that the BoW classifiers\nthat focus only on specific portions of the documents (rather than the full\ntext) outperform all compared methods on all three datasets.", "published": "2020-09-02 16:22:23", "link": "http://arxiv.org/abs/2009.00611v1", "categories": ["cs.IR", "cs.CL", "cs.DL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Learning to summarize from human feedback", "abstract": "As language models become more powerful, training and evaluation are\nincreasingly bottlenecked by the data and metrics used for a particular task.\nFor example, summarization models are often trained to predict human reference\nsummaries and evaluated using ROUGE, but both of these metrics are rough\nproxies for what we really care about -- summary quality. In this work, we show\nthat it is possible to significantly improve summary quality by training a\nmodel to optimize for human preferences. We collect a large, high-quality\ndataset of human comparisons between summaries, train a model to predict the\nhuman-preferred summary, and use that model as a reward function to fine-tune a\nsummarization policy using reinforcement learning. We apply our method to a\nversion of the TL;DR dataset of Reddit posts and find that our models\nsignificantly outperform both human reference summaries and much larger models\nfine-tuned with supervised learning alone. Our models also transfer to CNN/DM\nnews articles, producing summaries nearly as good as the human reference\nwithout any news-specific fine-tuning. We conduct extensive analyses to\nunderstand our human feedback dataset and fine-tuned models We establish that\nour reward model generalizes to new datasets, and that optimizing our reward\nmodel results in better summaries than optimizing ROUGE according to humans. We\nhope the evidence from our paper motivates machine learning researchers to pay\ncloser attention to how their training loss affects the model behavior they\nactually want.", "published": "2020-09-02 19:54:41", "link": "http://arxiv.org/abs/2009.01325v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Combining Determinism and Indeterminism", "abstract": "Our goal is to construct mathematical operations that combine indeterminism\nmeasured from quantum randomness with computational determinism so that\nnon-mechanistic behavior is preserved in the computation. Formally, some\nresults about operations applied to computably enumerable (c.e.) and bi-immune\nsets are proven here, where the objective is for the operations to preserve\nbi-immunity. While developing rearrangement operations on the natural numbers,\nwe discovered that the bi-immune rearrangements generate an uncountable\nsubgroup of the infinite symmetric group (Sym$(\\mathbb{N})$) on the natural\nnumbers $\\mathbb{N}$.\n  This new uncountable subgroup is called the bi-immune symmetric group. We\nshow that the bi-immune symmetric group contains the finitary symmetric group\non the natural numbers, and consequently is highly transitive. Furthermore, the\nbi-immune symmetric group is dense in Sym$(\\mathbb{N})$ with respect to the\npointwise convergence topology. The complete structure of the bi-immune\nsymmetric group and its subgroups generated by one or more bi-immune\nrearrangements is unknown.", "published": "2020-09-02 01:30:00", "link": "http://arxiv.org/abs/2009.03996v4", "categories": ["math.LO", "cs.CC", "cs.CL", "math.GR", "20B07", "F.1.1"], "primary_category": "math.LO"}
{"title": "On SkipGram Word Embedding Models with Negative Sampling: Unified\n  Framework and Impact of Noise Distributions", "abstract": "SkipGram word embedding models with negative sampling, or SGN in short, is an\nelegant family of word embedding models. In this paper, we formulate a\nframework for word embedding, referred to as Word-Context Classification (WCC),\nthat generalizes SGN to a wide family of models. The framework, utilizing some\n\"noise examples\", is justified through a theoretical analysis. The impact of\nnoise distribution on the learning of the WCC embedding models is studied\nexperimentally, suggesting that the best noise distribution is in fact the data\ndistribution, in terms of both the embedding performance and the speed of\nconvergence during training. Along our way, we discover several novel embedding\nmodels that outperform the existing WCC models.", "published": "2020-09-02 02:11:51", "link": "http://arxiv.org/abs/2009.04413v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "WNTRAC: AI Assisted Tracking of Non-pharmaceutical Interventions\n  Implemented Worldwide for COVID-19", "abstract": "The Coronavirus disease 2019 (COVID-19) global pandemic has transformed\nalmost every facet of human society throughout the world. Against an emerging,\nhighly transmissible disease with no definitive treatment or vaccine,\ngovernments worldwide have implemented non-pharmaceutical intervention (NPI) to\nslow the spread of the virus. Examples of such interventions include community\nactions (e.g. school closures, restrictions on mass gatherings), individual\nactions (e.g. mask wearing, self-quarantine), and environmental actions (e.g.\npublic facility cleaning). We present the Worldwide Non-pharmaceutical\nInterventions Tracker for COVID-19 (WNTRAC), a comprehensive dataset consisting\nof over 6,000 NPIs implemented worldwide since the start of the pandemic.\nWNTRAC covers NPIs implemented across 261 countries and territories, and\nclassifies NPI measures into a taxonomy of sixteen NPI types. NPI measures are\nautomatically extracted daily from Wikipedia articles using natural language\nprocessing techniques and manually validated to ensure accuracy and veracity.\nWe hope that the dataset is valuable for policymakers, public health leaders,\nand researchers in modeling and analysis efforts for controlling the spread of\nCOVID-19.", "published": "2020-09-02 18:06:20", "link": "http://arxiv.org/abs/2009.07057v4", "categories": ["cs.CY", "cs.CL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "Seeing wake words: Audio-visual Keyword Spotting", "abstract": "The goal of this work is to automatically determine whether and when a word\nof interest is spoken by a talking face, with or without the audio. We propose\na zero-shot method suitable for in the wild videos. Our key contributions are:\n(1) a novel convolutional architecture, KWS-Net, that uses a similarity map\nintermediate representation to separate the task into (i) sequence matching,\nand (ii) pattern detection, to decide whether the word is there and when; (2)\nwe demonstrate that if audio is available, visual keyword spotting improves the\nperformance both for a clean and noisy audio signal. Finally, (3) we show that\nour method generalises to other languages, specifically French and German, and\nachieves a comparable performance to English with less language specific data,\nby fine-tuning the network pre-trained on English. The method exceeds the\nperformance of the previous state-of-the-art visual keyword spotting\narchitecture when trained and tested on the same benchmark, and also that of a\nstate-of-the-art lip reading method.", "published": "2020-09-02 17:57:38", "link": "http://arxiv.org/abs/2009.01225v1", "categories": ["cs.CV", "eess.AS"], "primary_category": "cs.CV"}
{"title": "SAGRNN: Self-Attentive Gated RNN for Binaural Speaker Separation with\n  Interaural Cue Preservation", "abstract": "Most existing deep learning based binaural speaker separation systems focus\non producing a monaural estimate for each of the target speakers, and thus do\nnot preserve the interaural cues, which are crucial for human listeners to\nperform sound localization and lateralization. In this study, we address\ntalker-independent binaural speaker separation with interaural cues preserved\nin the estimated binaural signals. Specifically, we extend a newly-developed\ngated recurrent neural network for monaural separation by additionally\nincorporating self-attention mechanisms and dense connectivity. We develop an\nend-to-end multiple-input multiple-output system, which directly maps from the\nbinaural waveform of the mixture to those of the speech signals. The\nexperimental results show that our proposed approach achieves significantly\nbetter separation performance than a recent binaural separation approach. In\naddition, our approach effectively preserves the interaural cues, which\nimproves the accuracy of sound localization.", "published": "2020-09-02 23:07:17", "link": "http://arxiv.org/abs/2009.01381v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Degradation effects of water immersion on earbud audio quality", "abstract": "Earbuds are subjected to constant use and scenarios that may degrade sound\nquality. Indeed, a common fate of earbuds is being forgotten in pockets and\nfaced with a laundry cycle (LC). Manufacturers' accounts of the extent to which\nLCs affect earbud sound quality are vague at best, leaving users to their own\ndevices in assessing the damage caused. This paper offers a systematic,\nempirical approach to measure the effects of laundering earbuds on sound\nquality. Three earbud pairs were subjected to LCs spaced 24 hours apart. After\neach LC, a professional microphone as well as a mid-market smartphone were used\nto record i) a test tone ii) a frequency sweep and iii) a music signal played\nthrough the earbuds. We deployed mixed effects models and found significant\ndegradation in terms of RMS noise loudness, Total Harmonic Distortion (THD), as\nwell as measures of change in the frequency responses of the earbuds. All\ntransducers showed degradation already after the first cycle, and no\ntransducers produced a measurable signal after the sixth LC. The degradation\neffects were detectable in both, the professional microphone as well as the\nsmartphone recordings. We hope that the present work is a first step in\nestablishing a practical, and ecologically valid method for everyday users to\nassess the degree of degradation of their personal earbuds.", "published": "2020-09-02 03:16:56", "link": "http://arxiv.org/abs/2009.02151v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "WaveGrad: Estimating Gradients for Waveform Generation", "abstract": "This paper introduces WaveGrad, a conditional model for waveform generation\nwhich estimates gradients of the data density. The model is built on prior work\non score matching and diffusion probabilistic models. It starts from a Gaussian\nwhite noise signal and iteratively refines the signal via a gradient-based\nsampler conditioned on the mel-spectrogram. WaveGrad offers a natural way to\ntrade inference speed for sample quality by adjusting the number of refinement\nsteps, and bridges the gap between non-autoregressive and autoregressive models\nin terms of audio quality. We find that it can generate high fidelity audio\nsamples using as few as six iterations. Experiments reveal WaveGrad to generate\nhigh fidelity audio, outperforming adversarial non-autoregressive baselines and\nmatching a strong likelihood-based autoregressive baseline using fewer\nsequential operations. Audio samples are available at\nhttps://wavegrad.github.io/.", "published": "2020-09-02 17:44:10", "link": "http://arxiv.org/abs/2009.00713v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Speaker Representation Learning using Global Context Guided Channel and\n  Time-Frequency Transformations", "abstract": "In this study, we propose the global context guided channel and\ntime-frequency transformations to model the long-range, non-local\ntime-frequency dependencies and channel variances in speaker representations.\nWe use the global context information to enhance important channels and\nrecalibrate salient time-frequency locations by computing the similarity\nbetween the global context and local features. The proposed modules, together\nwith a popular ResNet based model, are evaluated on the VoxCeleb1 dataset,\nwhich is a large scale speaker verification corpus collected in the wild. This\nlightweight block can be easily incorporated into a CNN model with little\nadditional computational costs and effectively improves the speaker\nverification performance compared to the baseline ResNet-LDE model and the\nSqueeze&Excitation block by a large margin. Detailed ablation studies are also\nperformed to analyze various factors that may impact the performance of the\nproposed modules. We find that by employing the proposed L2-tf-GTFC\ntransformation block, the Equal Error Rate decreases from 4.56% to 3.07%, a\nrelative 32.68% reduction, and a relative 27.28% improvement in terms of the\nDCF score. The results indicate that our proposed global context guided\ntransformation modules can efficiently improve the learned speaker\nrepresentations by achieving time-frequency and channel-wise feature\nrecalibration.", "published": "2020-09-02 01:07:29", "link": "http://arxiv.org/abs/2009.00768v2", "categories": ["eess.AS", "cs.LG", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Detecting Parkinson's Disease From an Online Speech-task", "abstract": "In this paper, we envision a web-based framework that can help anyone,\nanywhere around the world record a short speech task, and analyze the recorded\ndata to screen for Parkinson's disease (PD). We collected data from 726 unique\nparticipants (262 PD, 38% female; 464 non-PD, 65% female; average age: 61) --\nfrom all over the US and beyond. A small portion of the data was collected in a\nlab setting to compare quality. The participants were instructed to utter a\npopular pangram containing all the letters in the English alphabet \"the quick\nbrown fox jumps over the lazy dog..\". We extracted both standard acoustic\nfeatures (Mel Frequency Cepstral Coefficients (MFCC), jitter and shimmer\nvariants) and deep learning based features from the speech data. Using these\nfeatures, we trained several machine learning algorithms. We achieved 0.75 AUC\n(Area Under The Curve) performance on determining presence of self-reported\nParkinson's disease by modeling the standard acoustic features through the\nXGBoost -- a gradient-boosted decision tree model. Further analysis reveal that\nthe widely used MFCC features and a subset of previously validated dysphonia\nfeatures designed for detecting Parkinson's from verbal phonation task\n(pronouncing 'ahh') contains the most distinct information. Our model performed\nequally well on data collected in controlled lab environment as well as 'in the\nwild' across different gender and age groups. Using this tool, we can collect\ndata from almost anyone anywhere with a video/audio enabled device,\ncontributing to equity and access in neurological care.", "published": "2020-09-02 21:16:24", "link": "http://arxiv.org/abs/2009.01231v4", "categories": ["eess.AS", "cs.CY", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Convolutional Speech Recognition with Pitch and Voice Quality Features", "abstract": "The effects of adding pitch and voice quality features such as jitter and\nshimmer to a state-of-the-art CNN model for Automatic Speech Recognition are\nstudied in this work. Pitch features have been previously used for improving\nclassical HMM and DNN baselines, while jitter and shimmer parameters have\nproven to be useful for tasks like speaker or emotion recognition. Up to our\nknowledge, this is the first work combining such pitch and voice quality\nfeatures with modern convolutional architectures, showing improvements up to 7%\nand 3% relative WER points, for the publicly available Spanish Common Voice and\nLibriSpeech 100h datasets, respectively. Particularly, our work combines these\nfeatures with mel-frequency spectral coefficients (MFSCs) to train a\nconvolutional architecture with Gated Linear Units (Conv GLUs). Such models\nhave shown to yield small word error rates, while being very suitable for\nparallel processing for online streaming recognition use cases. We have added\npitch and voice quality functionality to Facebook's wav2letter speech\nrecognition framework, and we provide with such code and recipes to the\ncommunity, to carry on with further experiments. Besides, to the best of our\nknowledge, our Spanish Common Voice recipe is the first public Spanish recipe\nfor wav2letter.", "published": "2020-09-02 19:25:50", "link": "http://arxiv.org/abs/2009.01309v2", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
