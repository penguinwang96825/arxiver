{"title": "Abstractive Text Summarization Using Sequence-to-Sequence RNNs and\n  Beyond", "abstract": "In this work, we model abstractive text summarization using Attentional\nEncoder-Decoder Recurrent Neural Networks, and show that they achieve\nstate-of-the-art performance on two different corpora. We propose several novel\nmodels that address critical problems in summarization that are not adequately\nmodeled by the basic architecture, such as modeling key-words, capturing the\nhierarchy of sentence-to-word structure, and emitting words that are rare or\nunseen at training time. Our work shows that many of our proposed models\ncontribute to further improvement in performance. We also propose a new dataset\nconsisting of multi-sentence summaries, and establish performance benchmarks\nfor further research.", "published": "2016-02-19 02:04:18", "link": "http://arxiv.org/abs/1602.06023v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On Training Bi-directional Neural Network Language Model with Noise\n  Contrastive Estimation", "abstract": "We propose to train bi-directional neural network language model(NNLM) with\nnoise contrastive estimation(NCE). Experiments are conducted on a rescore task\non the PTB data set. It is shown that NCE-trained bi-directional NNLM\noutperformed the one trained by conventional maximum likelihood training. But\nstill(regretfully), it did not out-perform the baseline uni-directional NNLM.", "published": "2016-02-19 07:27:49", "link": "http://arxiv.org/abs/1602.06064v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to SMILE(S)", "abstract": "This paper shows how one can directly apply natural language processing (NLP)\nmethods to classification problems in cheminformatics. Connection between these\nseemingly separate fields is shown by considering standard textual\nrepresentation of compound, SMILES. The problem of activity prediction against\na target protein is considered, which is a crucial part of computer aided drug\ndesign process. Conducted experiments show that this way one can not only\noutrank state of the art results of hand crafted representations but also gets\ndirect structural insights into the way decisions are made.", "published": "2016-02-19 20:48:19", "link": "http://arxiv.org/abs/1602.06289v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contextual LSTM (CLSTM) models for Large scale NLP tasks", "abstract": "Documents exhibit sequential structure at multiple levels of abstraction\n(e.g., sentences, paragraphs, sections). These abstractions constitute a\nnatural hierarchy for representing the context in which to infer the meaning of\nwords and larger fragments of text. In this paper, we present CLSTM (Contextual\nLSTM), an extension of the recurrent neural network LSTM (Long-Short Term\nMemory) model, where we incorporate contextual features (e.g., topics) into the\nmodel. We evaluate CLSTM on three specific NLP tasks: word prediction, next\nsentence selection, and sentence topic prediction. Results from experiments run\non two corpora, English documents in Wikipedia and a subset of articles from a\nrecent snapshot of English Google News, indicate that using both words and\ntopics as features improves performance of the CLSTM models over baseline LSTM\nmodels for these tasks. For example on the next sentence selection task, we get\nrelative accuracy improvements of 21% for the Wikipedia dataset and 18% for the\nGoogle News dataset. This clearly demonstrates the significant benefit of using\ncontext appropriately in natural language (NL) tasks. This has implications for\na wide variety of NL applications like question answering, sentence completion,\nparaphrase generation, and next utterance prediction in dialog systems.", "published": "2016-02-19 20:52:08", "link": "http://arxiv.org/abs/1602.06291v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Spectral Learning for Supervised Topic Models", "abstract": "Supervised topic models simultaneously model the latent topic structure of\nlarge collections of documents and a response variable associated with each\ndocument. Existing inference methods are based on variational approximation or\nMonte Carlo sampling, which often suffers from the local minimum defect.\nSpectral methods have been applied to learn unsupervised topic models, such as\nlatent Dirichlet allocation (LDA), with provable guarantees. This paper\ninvestigates the possibility of applying spectral methods to recover the\nparameters of supervised LDA (sLDA). We first present a two-stage spectral\nmethod, which recovers the parameters of LDA followed by a power update method\nto recover the regression model parameters. Then, we further present a\nsingle-phase spectral algorithm to jointly recover the topic distribution\nmatrix as well as the regression weights. Our spectral algorithms are provably\ncorrect and computationally efficient. We prove a sample complexity bound for\neach algorithm and subsequently derive a sufficient condition for the\nidentifiability of sLDA. Thorough experiments on synthetic and real-world\ndatasets verify the theory and demonstrate the practical effectiveness of the\nspectral algorithms. In fact, our results on a large-scale review rating\ndataset demonstrate that our single-phase spectral algorithm alone gets\ncomparable or even better performance than state-of-the-art methods, while\nprevious work on spectral methods has rarely reported such promising\nperformance.", "published": "2016-02-19 02:07:20", "link": "http://arxiv.org/abs/1602.06025v1", "categories": ["cs.LG", "cs.CL", "cs.IR", "stat.ML"], "primary_category": "cs.LG"}
