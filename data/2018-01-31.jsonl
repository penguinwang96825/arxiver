{"title": "Paraphrase-Supervised Models of Compositionality", "abstract": "Compositional vector space models of meaning promise new solutions to\nstubborn language understanding problems. This paper makes two contributions\ntoward this end: (i) it uses automatically-extracted paraphrase examples as a\nsource of supervision for training compositional models, replacing previous\nwork which relied on manual annotations used for the same purpose, and (ii)\ndevelops a context-aware model for scoring phrasal compositionality.\nExperimental results indicate that these multiple sources of information can be\nused to learn partial semantic supervision that matches previous techniques in\nintrinsic evaluation tasks. Our approaches are also evaluated for their impact\non a machine translation system where we show improvements in translation\nquality, demonstrating that compositionality in interpretation correlates with\ncompositionality in translation.", "published": "2018-01-31 04:14:11", "link": "http://arxiv.org/abs/1801.10293v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reinforced Self-Attention Network: a Hybrid of Hard and Soft Attention\n  for Sequence Modeling", "abstract": "Many natural language processing tasks solely rely on sparse dependencies\nbetween a few tokens in a sentence. Soft attention mechanisms show promising\nperformance in modeling local/global dependencies by soft probabilities between\nevery two tokens, but they are not effective and efficient when applied to long\nsentences. By contrast, hard attention mechanisms directly select a subset of\ntokens but are difficult and inefficient to train due to their combinatorial\nnature. In this paper, we integrate both soft and hard attention into one\ncontext fusion model, \"reinforced self-attention (ReSA)\", for the mutual\nbenefit of each other. In ReSA, a hard attention trims a sequence for a soft\nself-attention to process, while the soft attention feeds reward signals back\nto facilitate the training of the hard one. For this purpose, we develop a\nnovel hard attention called \"reinforced sequence sampling (RSS)\", selecting\ntokens in parallel and trained via policy gradient. Using two RSS modules, ReSA\nefficiently extracts the sparse dependencies between each pair of selected\ntokens. We finally propose an RNN/CNN-free sentence-encoding model, \"reinforced\nself-attention network (ReSAN)\", solely based on ReSA. It achieves\nstate-of-the-art performance on both Stanford Natural Language Inference (SNLI)\nand Sentences Involving Compositional Knowledge (SICK) datasets.", "published": "2018-01-31 04:30:32", "link": "http://arxiv.org/abs/1801.10296v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Complex Sequential Question Answering: Towards Learning to Converse Over\n  Linked Question Answer Pairs with a Knowledge Graph", "abstract": "While conversing with chatbots, humans typically tend to ask many questions,\na significant portion of which can be answered by referring to large-scale\nknowledge graphs (KG). While Question Answering (QA) and dialog systems have\nbeen studied independently, there is a need to study them closely to evaluate\nsuch real-world scenarios faced by bots involving both these tasks. Towards\nthis end, we introduce the task of Complex Sequential QA which combines the two\ntasks of (i) answering factual questions through complex inferencing over a\nrealistic-sized KG of millions of entities, and (ii) learning to converse\nthrough a series of coherently linked QA pairs. Through a labor intensive\nsemi-automatic process, involving in-house and crowdsourced workers, we created\na dataset containing around 200K dialogs with a total of 1.6M turns. Further,\nunlike existing large scale QA datasets which contain simple questions that can\nbe answered from a single tuple, the questions in our dialogs require a larger\nsubgraph of the KG. Specifically, our dataset has questions which require\nlogical, quantitative, and comparative reasoning as well as their combinations.\nThis calls for models which can: (i) parse complex natural language questions,\n(ii) use conversation context to resolve coreferences and ellipsis in\nutterances, (iii) ask for clarifications for ambiguous queries, and finally\n(iv) retrieve relevant subgraphs of the KG to answer such questions. However,\nour experiments with a combination of state of the art dialog and QA models\nshow that they clearly do not achieve the above objectives and are inadequate\nfor dealing with such complex real world settings. We believe that this new\ndataset coupled with the limitations of existing models as reported in this\npaper should encourage further research in Complex Sequential QA.", "published": "2018-01-31 06:40:40", "link": "http://arxiv.org/abs/1801.10314v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Technical Report: Adjudication of Coreference Annotations via Answer Set\n  Optimization", "abstract": "We describe the first automatic approach for merging coreference annotations\nobtained from multiple annotators into a single gold standard. This merging is\nsubject to certain linguistic hard constraints and optimization criteria that\nprefer solutions with minimal divergence from annotators. The representation\ninvolves an equivalence relation over a large number of elements. We use Answer\nSet Programming to describe two representations of the problem and four\nobjective functions suitable for different datasets. We provide two\nstructurally different real-world benchmark datasets based on the METU-Sabanci\nTurkish Treebank and we report our experiences in using the Gringo, Clasp, and\nWasp tools for computing optimal adjudication results on these datasets.", "published": "2018-01-31 19:41:19", "link": "http://arxiv.org/abs/1802.00033v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Nested LSTMs", "abstract": "We propose Nested LSTMs (NLSTM), a novel RNN architecture with multiple\nlevels of memory. Nested LSTMs add depth to LSTMs via nesting as opposed to\nstacking. The value of a memory cell in an NLSTM is computed by an LSTM cell,\nwhich has its own inner memory cell. Specifically, instead of computing the\nvalue of the (outer) memory cell as $c^{outer}_t = f_t \\odot c_{t-1} + i_t\n\\odot g_t$, NLSTM memory cells use the concatenation $(f_t \\odot c_{t-1}, i_t\n\\odot g_t)$ as input to an inner LSTM (or NLSTM) memory cell, and set\n$c^{outer}_t$ = $h^{inner}_t$. Nested LSTMs outperform both stacked and\nsingle-layer LSTMs with similar numbers of parameters in our experiments on\nvarious character-level language modeling tasks, and the inner memories of an\nLSTM learn longer term dependencies compared with the higher-level units of a\nstacked LSTM.", "published": "2018-01-31 05:52:08", "link": "http://arxiv.org/abs/1801.10308v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Comparing approaches for mitigating intergroup variability in\n  personality recognition", "abstract": "Personality have been found to predict many life outcomes, and there have\nbeen huge interests on automatic personality recognition from a speaker's\nutterance. Previously, we achieved accuracies between 37%-44% for three-way\nclassification of high, medium or low for each of the Big Five personality\ntraits (Openness to Experience, Conscientiousness, Extraversion, Agreeableness,\nNeuroticism). We show here that we can improve performance on this task by\naccounting for heterogeneity of gender and L1 in our data, which has English\nspeech from female and male native speakers of Chinese and Standard American\nEnglish (SAE). We experiment with personalizing models by L1 and gender and\nnormalizing features by speaker, L1 group, and/or gender.", "published": "2018-01-31 22:18:56", "link": "http://arxiv.org/abs/1802.01405v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Interactive Grounded Language Acquisition and Generalization in a 2D\n  World", "abstract": "We build a virtual agent for learning language in a 2D maze-like world. The\nagent sees images of the surrounding environment, listens to a virtual teacher,\nand takes actions to receive rewards. It interactively learns the teacher's\nlanguage from scratch based on two language use cases: sentence-directed\nnavigation and question answering. It learns simultaneously the visual\nrepresentations of the world, the language, and the action control. By\ndisentangling language grounding from other computational routines and sharing\na concept detection function between language grounding and prediction, the\nagent reliably interpolates and extrapolates to interpret sentences that\ncontain new word combinations or new words missing from training sentences. The\nnew words are transferred from the answers of language prediction. Such a\nlanguage ability is trained and evaluated on a population of over 1.6 million\ndistinct sentences consisting of 119 object words, 8 color words, 9\nspatial-relation words, and 50 grammatical words. The proposed model\nsignificantly outperforms five comparison methods for interpreting zero-shot\nsentences. In addition, we demonstrate human-interpretable intermediate outputs\nof the model in the appendix.", "published": "2018-01-31 01:35:46", "link": "http://arxiv.org/abs/1802.01433v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Deep Predictive Models in Interactive Music", "abstract": "Musical performance requires prediction to operate instruments, to perform in\ngroups and to improvise. In this paper, we investigate how a number of digital\nmusical instruments (DMIs), including two of our own, have applied predictive\nmachine learning models that assist users by predicting unknown states of\nmusical processes. We characterise these predictions as focussed within a\nmusical instrument, at the level of individual performers, and between members\nof an ensemble. These models can connect to existing frameworks for DMI design\nand have parallels in the cognitive predictions of human musicians.\n  We discuss how recent advances in deep learning highlight the role of\nprediction in DMIs, by allowing data-driven predictive models with a long\nmemory of past states. The systems we review are used to motivate musical\nuse-cases where prediction is a necessary component, and to highlight a number\nof challenges for DMI designers seeking to apply deep predictive models in\ninteractive music systems of the future.", "published": "2018-01-31 15:30:32", "link": "http://arxiv.org/abs/1801.10492v3", "categories": ["cs.SD", "cs.AI", "cs.HC", "cs.NE", "eess.AS"], "primary_category": "cs.SD"}
