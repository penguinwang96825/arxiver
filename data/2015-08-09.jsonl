{"title": "Bidirectional LSTM-CRF Models for Sequence Tagging", "abstract": "In this paper, we propose a variety of Long Short-Term Memory (LSTM) based\nmodels for sequence tagging. These models include LSTM networks, bidirectional\nLSTM (BI-LSTM) networks, LSTM with a Conditional Random Field (CRF) layer\n(LSTM-CRF) and bidirectional LSTM with a CRF layer (BI-LSTM-CRF). Our work is\nthe first to apply a bidirectional LSTM CRF (denoted as BI-LSTM-CRF) model to\nNLP benchmark sequence tagging data sets. We show that the BI-LSTM-CRF model\ncan efficiently use both past and future input features thanks to a\nbidirectional LSTM component. It can also use sentence level tag information\nthanks to a CRF layer. The BI-LSTM-CRF model can produce state of the art (or\nclose to) accuracy on POS, chunking and NER data sets. In addition, it is\nrobust and has less dependence on word embedding as compared to previous\nobservations.", "published": "2015-08-09 06:32:47", "link": "http://arxiv.org/abs/1508.01991v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Automatic Machine Translation Evaluation Metric Based on Dependency\n  Parsing Model", "abstract": "Most of the syntax-based metrics obtain the similarity by comparing the\nsub-structures extracted from the trees of hypothesis and reference. These\nsub-structures are defined by human and can't express all the information in\nthe trees because of the limited length of sub-structures. In addition, the\noverlapped parts between these sub-structures are computed repeatedly. To avoid\nthese problems, we propose a novel automatic evaluation metric based on\ndependency parsing model, with no need to define sub-structures by human.\nFirst, we train a dependency parsing model by the reference dependency tree.\nThen we generate the hypothesis dependency tree and the corresponding\nprobability by the dependency parsing model. The quality of the hypothesis can\nbe judged by this probability. In order to obtain the lexicon similarity, we\nalso introduce the unigram F-score to the new metric. Experiment results show\nthat the new metric gets the state-of-the-art performance on system level, and\nis comparable with METEOR on sentence level.", "published": "2015-08-09 07:55:51", "link": "http://arxiv.org/abs/1508.01996v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Finding Function in Form: Compositional Character Models for Open\n  Vocabulary Word Representation", "abstract": "We introduce a model for constructing vector representations of words by\ncomposing characters using bidirectional LSTMs. Relative to traditional word\nrepresentation models that have independent vectors for each word type, our\nmodel requires only a single vector per character type and a fixed set of\nparameters for the compositional model. Despite the compactness of this model\nand, more importantly, the arbitrary nature of the form-function relationship\nin language, our \"composed\" word representations yield state-of-the-art results\nin language modeling and part-of-speech tagging. Benefits over traditional\nbaselines are particularly pronounced in morphologically rich languages (e.g.,\nTurkish).", "published": "2015-08-09 23:41:38", "link": "http://arxiv.org/abs/1508.02096v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Image Representations and New Domains in Neural Image Captioning", "abstract": "We examine the possibility that recent promising results in automatic caption\ngeneration are due primarily to language models. By varying image\nrepresentation quality produced by a convolutional neural network, we find that\na state-of-the-art neural captioning algorithm is able to produce quality\ncaptions even when provided with surprisingly poor image representations. We\nreplicate this result in a new, fine-grained, transfer learned captioning\ndomain, consisting of 66K recipe image/title pairs. We also provide some\nexperiments regarding the appropriateness of datasets for automatic captioning,\nand find that having multiple captions per image is beneficial, but not an\nabsolute requirement.", "published": "2015-08-09 22:52:10", "link": "http://arxiv.org/abs/1508.02091v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Improving Decision Analytics with Deep Learning: The Case of Financial\n  Disclosures", "abstract": "Decision analytics commonly focuses on the text mining of financial news\nsources in order to provide managerial decision support and to predict stock\nmarket movements. Existing predictive frameworks almost exclusively apply\ntraditional machine learning methods, whereas recent research indicates that\ntraditional machine learning methods are not sufficiently capable of extracting\nsuitable features and capturing the non-linear nature of complex tasks. As a\nremedy, novel deep learning models aim to overcome this issue by extending\ntraditional neural network models with additional hidden layers. Indeed, deep\nlearning has been shown to outperform traditional methods in terms of\npredictive performance. In this paper, we adapt the novel deep learning\ntechnique to financial decision support. In this instance, we aim to predict\nthe direction of stock movements following financial disclosures. As a result,\nwe show how deep learning can outperform the accuracy of random forests as a\nbenchmark for machine learning by 5.66%.", "published": "2015-08-09 07:39:24", "link": "http://arxiv.org/abs/1508.01993v2", "categories": ["stat.ML", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
