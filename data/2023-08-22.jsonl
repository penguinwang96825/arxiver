{"title": "HopPG: Self-Iterative Program Generation for Multi-Hop Question\n  Answering over Heterogeneous Knowledge", "abstract": "The semantic parsing-based method is an important research branch for\nknowledge-based question answering. It usually generates executable programs\nlean upon the question and then conduct them to reason answers over a knowledge\nbase. Benefit from this inherent mechanism, it has advantages in the\nperformance and the interpretability. However, traditional semantic parsing\nmethods usually generate a complete program before executing it, which\nstruggles with multi-hop question answering over heterogeneous knowledge. On\none hand, generating a complete multi-hop program relies on multiple\nheterogeneous supporting facts, and it is difficult for generators to\nunderstand these facts simultaneously. On the other hand, this way ignores the\nsemantic information of the intermediate answers at each hop, which is\nbeneficial for subsequent generation. To alleviate these challenges, we propose\na self-iterative framework for multi-hop program generation (HopPG) over\nheterogeneous knowledge, which leverages the previous execution results to\nretrieve supporting facts and generate subsequent programs hop by hop. We\nevaluate our model on MMQA-T^2, and the experimental results show that HopPG\noutperforms existing semantic-parsing-based baselines, especially on the\nmulti-hop questions.", "published": "2023-08-22 08:00:50", "link": "http://arxiv.org/abs/2308.11257v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extracting Relational Triples Based on Graph Recursive Neural Network\n  via Dynamic Feedback Forest Algorithm", "abstract": "Extracting relational triples (subject, predicate, object) from text enables\nthe transformation of unstructured text data into structured knowledge. The\nnamed entity recognition (NER) and the relation extraction (RE) are two\nfoundational subtasks in this knowledge generation pipeline. The integration of\nsubtasks poses a considerable challenge due to their disparate nature. This\npaper presents a novel approach that converts the triple extraction task into a\ngraph labeling problem, capitalizing on the structural information of\ndependency parsing and graph recursive neural networks (GRNNs). To integrate\nsubtasks, this paper proposes a dynamic feedback forest algorithm that connects\nthe representations of subtasks by inference operations during model training.\nExperimental results demonstrate the effectiveness of the proposed method.", "published": "2023-08-22 13:00:13", "link": "http://arxiv.org/abs/2308.11411v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SONAR: Sentence-Level Multimodal and Language-Agnostic Representations", "abstract": "We introduce SONAR, a new multilingual and multimodal fixed-size sentence\nembedding space. Our single text encoder, covering 200 languages, substantially\noutperforms existing sentence embeddings such as LASER3 and LabSE on the xsim\nand xsim++ multilingual similarity search tasks. Speech segments can be\nembedded in the same SONAR embedding space using language-specific speech\nencoders trained in a teacher-student setting on speech transcription data. Our\nencoders outperform existing speech encoders on similarity search tasks. We\nalso provide a text decoder for 200 languages, which allows us to perform\ntext-to-text and speech-to-text machine translation, including for zero-shot\nlanguage and modality combinations. Our text-to-text results are competitive\ncompared to the state-of-the-art NLLB~1B model, despite the fixed-size\nbottleneck representation. Our zero-shot speech-to-text translation results\ncompare favorably with strong supervised baselines such as Whisper.", "published": "2023-08-22 14:25:15", "link": "http://arxiv.org/abs/2308.11466v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Empowering Refugee Claimants and their Lawyers: Using Machine Learning\n  to Examine Decision-Making in Refugee Law", "abstract": "Our project aims at helping and supporting stakeholders in refugee status\nadjudications, such as lawyers, judges, governing bodies, and claimants, in\norder to make better decisions through data-driven intelligence and increase\nthe understanding and transparency of the refugee application process for all\ninvolved parties. This PhD project has two primary objectives: (1) to retrieve\npast cases, and (2) to analyze legal decision-making processes on a dataset of\nCanadian cases. In this paper, we present the current state of our work, which\nincludes a completed experiment on part (1) and ongoing efforts related to part\n(2). We believe that NLP-based solutions are well-suited to address these\nchallenges, and we investigate the feasibility of automating all steps\ninvolved. In addition, we introduce a novel benchmark for future NLP research\nin refugee law. Our methodology aims to be inclusive to all end-users and\nstakeholders, with expected benefits including reduced time-to-decision, fairer\nand more transparent outcomes, and improved decision quality.", "published": "2023-08-22 15:59:21", "link": "http://arxiv.org/abs/2308.11531v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BELB: a Biomedical Entity Linking Benchmark", "abstract": "Biomedical entity linking (BEL) is the task of grounding entity mentions to a\nknowledge base. It plays a vital role in information extraction pipelines for\nthe life sciences literature. We review recent work in the field and find that,\nas the task is absent from existing benchmarks for biomedical text mining,\ndifferent studies adopt different experimental setups making comparisons based\non published numbers problematic. Furthermore, neural systems are tested\nprimarily on instances linked to the broad coverage knowledge base UMLS,\nleaving their performance to more specialized ones, e.g. genes or variants,\nunderstudied. We therefore developed BELB, a Biomedical Entity Linking\nBenchmark, providing access in a unified format to 11 corpora linked to 7\nknowledge bases and spanning six entity types: gene, disease, chemical,\nspecies, cell line and variant. BELB greatly reduces preprocessing overhead in\ntesting BEL systems on multiple corpora offering a standardized testbed for\nreproducible experiments. Using BELB we perform an extensive evaluation of six\nrule-based entity-specific systems and three recent neural approaches\nleveraging pre-trained language models. Our results reveal a mixed picture\nshowing that neural approaches fail to perform consistently across entity\ntypes, highlighting the need of further studies towards entity-agnostic models.", "published": "2023-08-22 16:05:18", "link": "http://arxiv.org/abs/2308.11537v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using ChatGPT as a CAT tool in Easy Language translation", "abstract": "This study sets out to investigate the feasibility of using ChatGPT to\ntranslate citizen-oriented administrative texts into German Easy Language, a\nsimplified, controlled language variety that is adapted to the needs of people\nwith reading impairments. We use ChatGPT to translate selected texts from\nwebsites of German public authorities using two strategies, i.e. linguistic and\nholistic. We analyse the quality of the generated texts based on different\ncriteria, such as correctness, readability, and syntactic complexity. The\nresults indicated that the generated texts are easier than the standard texts,\nbut that they still do not fully meet the established Easy Language standards.\nAdditionally, the content is not always rendered correctly.", "published": "2023-08-22 16:59:31", "link": "http://arxiv.org/abs/2308.11563v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SeamlessM4T: Massively Multilingual & Multimodal Machine Translation", "abstract": "What does it take to create the Babel Fish, a tool that can help individuals\ntranslate speech between any two languages? While recent breakthroughs in\ntext-based models have pushed machine translation coverage beyond 200\nlanguages, unified speech-to-speech translation models have yet to achieve\nsimilar strides. More specifically, conventional speech-to-speech translation\nsystems rely on cascaded systems that perform translation progressively,\nputting high-performing unified systems out of reach. To address these gaps, we\nintroduce SeamlessM4T, a single model that supports speech-to-speech\ntranslation, speech-to-text translation, text-to-speech translation,\ntext-to-text translation, and automatic speech recognition for up to 100\nlanguages. To build this, we used 1 million hours of open speech audio data to\nlearn self-supervised speech representations with w2v-BERT 2.0. Subsequently,\nwe created a multimodal corpus of automatically aligned speech translations.\nFiltered and combined with human-labeled and pseudo-labeled data, we developed\nthe first multilingual system capable of translating from and into English for\nboth speech and text. On FLEURS, SeamlessM4T sets a new standard for\ntranslations into multiple target languages, achieving an improvement of 20%\nBLEU over the previous SOTA in direct speech-to-text translation. Compared to\nstrong cascaded models, SeamlessM4T improves the quality of into-English\ntranslation by 1.3 BLEU points in speech-to-text and by 2.6 ASR-BLEU points in\nspeech-to-speech. Tested for robustness, our system performs better against\nbackground noises and speaker variations in speech-to-text tasks compared to\nthe current SOTA model. Critically, we evaluated SeamlessM4T on gender bias and\nadded toxicity to assess translation safety. Finally, all contributions in this\nwork are open-sourced and accessible at\nhttps://github.com/facebookresearch/seamless_communication", "published": "2023-08-22 17:44:18", "link": "http://arxiv.org/abs/2308.11596v3", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Learning to generate and corr- uh I mean repair language in real-time", "abstract": "In conversation, speakers produce language incrementally, word by word, while\ncontinuously monitoring the appropriateness of their own contribution in the\ndynamically unfolding context of the conversation; and this often leads them to\nrepair their own utterance on the fly. This real-time language processing\ncapacity is furthermore crucial to the development of fluent and natural\nconversational AI. In this paper, we use a previously learned Dynamic Syntax\ngrammar and the CHILDES corpus to develop, train and evaluate a probabilistic\nmodel for incremental generation where input to the model is a purely semantic\ngeneration goal concept in Type Theory with Records (TTR). We show that the\nmodel's output exactly matches the gold candidate in 78% of cases with a\nROUGE-l score of 0.86. We further do a zero-shot evaluation of the ability of\nthe same model to generate self-repairs when the generation goal changes\nmid-utterance. Automatic evaluation shows that the model can generate\nself-repairs correctly in 85% of cases. A small human evaluation confirms the\nnaturalness and grammaticality of the generated self-repairs. Overall, these\nresults further highlight the generalisation power of grammar-based models and\nlay the foundations for more controllable, and naturally interactive\nconversational AI systems.", "published": "2023-08-22 15:09:55", "link": "http://arxiv.org/abs/2308.11683v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards an On-device Agent for Text Rewriting", "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities for\ntext rewriting. Nonetheless, the large sizes of these models make them\nimpractical for on-device inference, which would otherwise allow for enhanced\nprivacy and economical inference. Creating a smaller yet potent language model\nfor text rewriting presents a formidable challenge because it requires\nbalancing the need for a small size with the need to retain the emergent\ncapabilities of the LLM, that requires costly data collection. To address the\nabove challenge, we introduce a new instruction tuning approach for building a\nmobile-centric text rewriting model. Our strategies enable the generation of\nhigh quality training data without any human labeling. In addition, we propose\na heuristic reinforcement learning framework which substantially enhances\nperformance without requiring preference data. To further bridge the\nperformance gap with the larger server-side model, we propose an effective\napproach that combines the mobile rewrite agent with the server model using a\ncascade. To tailor the text rewriting tasks to mobile scenarios, we introduce\nMessageRewriteEval, a benchmark that focuses on text rewriting for messages\nthrough natural language instructions. Through empirical experiments, we\ndemonstrate that our on-device model surpasses the current state-of-the-art\nLLMs in text rewriting while maintaining a significantly reduced model size.\nNotably, we show that our proposed cascading approach improves model\nperformance.", "published": "2023-08-22 22:18:38", "link": "http://arxiv.org/abs/2308.11807v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ViCo: Engaging Video Comment Generation with Human Preference Rewards", "abstract": "Engaging video comments play an important role in video social media, as they\nare the carrier of feelings, thoughts, or humor of the audience. Preliminary\nworks have made initial exploration for video comment generation by adopting\ncaption-style encoder-decoder models. However, comment generation presents some\nunique challenges distinct from caption generation, which makes these methods\nsomewhat less effective at generating engaging comments. In contrast to the\nobjective and descriptive nature of captions, comments tend to be inherently\nsubjective, making it hard to quantify and evaluate the engagement of comments.\nFurthermore, the scarcity of truly engaging comments brings difficulty to\ncollecting enough high-quality training examples. In this paper, we propose\nViCo with three novel designs to tackle the above challenges for generating\nengaging Video Comments. Firstly, to quantify the engagement of comments, we\nutilize the number of \"likes\" each comment receives as a proxy of human\npreference after an appropriate debiasing procedure. Secondly, to automatically\nevaluate the engagement of comments, we train a reward model to align its\njudgment to the above proxy. Our user studies indicate that this reward model\neffectively aligns with human judgments. Lastly, to alleviate the scarcity of\nhigh-quality comments, an initial generator is trained on readily available but\nnoisy data to generate comments. Then the reward model is employed to offer\nfeedback on the generated comments, thus optimizing the initial generator. To\nfacilitate the research of video commenting, we collect a large video\ncomment-dataset (ViCo-20k) with rich metadata from a popular video website.\nExperiments on ViCo-20k show that the comments generated by our ViCo model\nexhibit the best performance in terms of both quantitative and qualitative\nresults, particularly when engagement is considered.", "published": "2023-08-22 04:01:01", "link": "http://arxiv.org/abs/2308.11171v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Evaluating Large Language Models on Graphs: Performance Insights and\n  Comparative Analysis", "abstract": "Large Language Models (LLMs) have garnered considerable interest within both\nacademic and industrial. Yet, the application of LLMs to graph data remains\nunder-explored. In this study, we evaluate the capabilities of four LLMs in\naddressing several analytical problems with graph data. We employ four distinct\nevaluation metrics: Comprehension, Correctness, Fidelity, and Rectification.\nOur results show that: 1) LLMs effectively comprehend graph data in natural\nlanguage and reason with graph topology. 2) GPT models can generate logical and\ncoherent results, outperforming alternatives in correctness. 3) All examined\nLLMs face challenges in structural reasoning, with techniques like zero-shot\nchain-of-thought and few-shot prompting showing diminished efficacy. 4) GPT\nmodels often produce erroneous answers in multi-answer tasks, raising concerns\nin fidelity. 5) GPT models exhibit elevated confidence in their outputs,\npotentially hindering their rectification capacities. Notably, GPT-4 has\ndemonstrated the capacity to rectify responses from GPT-3.5-turbo and its own\nprevious iterations. The code is available at:\nhttps://github.com/Ayame1006/LLMtoGraph.", "published": "2023-08-22 06:32:07", "link": "http://arxiv.org/abs/2308.11224v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "LEAP: Efficient and Automated Test Method for NLP Software", "abstract": "The widespread adoption of DNNs in NLP software has highlighted the need for\nrobustness. Researchers proposed various automatic testing techniques for\nadversarial test cases. However, existing methods suffer from two limitations:\nweak error-discovering capabilities, with success rates ranging from 0% to\n24.6% for BERT-based NLP software, and time inefficiency, taking 177.8s to\n205.28s per test case, making them challenging for time-constrained scenarios.\nTo address these issues, this paper proposes LEAP, an automated test method\nthat uses LEvy flight-based Adaptive Particle swarm optimization integrated\nwith textual features to generate adversarial test cases. Specifically, we\nadopt Levy flight for population initialization to increase the diversity of\ngenerated test cases. We also design an inertial weight adaptive update\noperator to improve the efficiency of LEAP's global optimization of\nhigh-dimensional text examples and a mutation operator based on the greedy\nstrategy to reduce the search time. We conducted a series of experiments to\nvalidate LEAP's ability to test NLP software and found that the average success\nrate of LEAP in generating adversarial test cases is 79.1%, which is 6.1%\nhigher than the next best approach (PSOattack). While ensuring high success\nrates, LEAP significantly reduces time overhead by up to 147.6s compared to\nother heuristic-based methods. Additionally, the experimental results\ndemonstrate that LEAP can generate more transferable test cases and\nsignificantly enhance the robustness of DNN-based systems.", "published": "2023-08-22 08:51:10", "link": "http://arxiv.org/abs/2308.11284v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "MMAPS: End-to-End Multi-Grained Multi-Modal Attribute-Aware Product\n  Summarization", "abstract": "Given the long textual product information and the product image, Multi-modal\nProduct Summarization (MPS) aims to increase customers' desire to purchase by\nhighlighting product characteristics with a short textual summary. Existing MPS\nmethods can produce promising results. Nevertheless, they still 1) lack\nend-to-end product summarization, 2) lack multi-grained multi-modal modeling,\nand 3) lack multi-modal attribute modeling. To improve MPS, we propose an\nend-to-end multi-grained multi-modal attribute-aware product summarization\nmethod (MMAPS) for generating high-quality product summaries in e-commerce.\nMMAPS jointly models product attributes and generates product summaries. We\ndesign several multi-grained multi-modal tasks to better guide the multi-modal\nlearning of MMAPS. Furthermore, we model product attributes based on both text\nand image modalities so that multi-modal product characteristics can be\nmanifested in the generated summaries. Extensive experiments on a real\nlarge-scale Chinese e-commence dataset demonstrate that our model outperforms\nstate-of-the-art product summarization methods w.r.t. several summarization\nmetrics. Our code is publicly available at: https://github.com/KDEGroup/MMAPS.", "published": "2023-08-22 11:00:09", "link": "http://arxiv.org/abs/2308.11351v2", "categories": ["cs.MM", "cs.CL"], "primary_category": "cs.MM"}
{"title": "A Survey on Large Language Model based Autonomous Agents", "abstract": "Autonomous agents have long been a prominent research focus in both academic\nand industry communities. Previous research in this field often focuses on\ntraining agents with limited knowledge within isolated environments, which\ndiverges significantly from human learning processes, and thus makes the agents\nhard to achieve human-like decisions. Recently, through the acquisition of vast\namounts of web knowledge, large language models (LLMs) have demonstrated\nremarkable potential in achieving human-level intelligence. This has sparked an\nupsurge in studies investigating LLM-based autonomous agents. In this paper, we\npresent a comprehensive survey of these studies, delivering a systematic review\nof the field of LLM-based autonomous agents from a holistic perspective. More\nspecifically, we first discuss the construction of LLM-based autonomous agents,\nfor which we propose a unified framework that encompasses a majority of the\nprevious work. Then, we present a comprehensive overview of the diverse\napplications of LLM-based autonomous agents in the fields of social science,\nnatural science, and engineering. Finally, we delve into the evaluation\nstrategies commonly used for LLM-based autonomous agents. Based on the previous\nstudies, we also present several challenges and future directions in this\nfield. To keep track of this field and continuously update our survey, we\nmaintain a repository of relevant references at\nhttps://github.com/Paitesanshi/LLM-Agent-Survey.", "published": "2023-08-22 13:30:37", "link": "http://arxiv.org/abs/2308.11432v7", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Aspect-oriented Opinion Alignment Network for Aspect-Based Sentiment\n  Classification", "abstract": "Aspect-based sentiment classification is a crucial problem in fine-grained\nsentiment analysis, which aims to predict the sentiment polarity of the given\naspect according to its context. Previous works have made remarkable progress\nin leveraging attention mechanism to extract opinion words for different\naspects. However, a persistent challenge is the effective management of\nsemantic mismatches, which stem from attention mechanisms that fall short in\nadequately aligning opinions words with their corresponding aspect in\nmulti-aspect sentences. To address this issue, we propose a novel\nAspect-oriented Opinion Alignment Network (AOAN) to capture the contextual\nassociation between opinion words and the corresponding aspect. Specifically,\nwe first introduce a neighboring span enhanced module which highlights various\ncompositions of neighboring words and given aspects. In addition, we design a\nmulti-perspective attention mechanism that align relevant opinion information\nwith respect to the given aspect. Extensive experiments on three benchmark\ndatasets demonstrate that our model achieves state-of-the-art results. The\nsource code is available at https://github.com/AONE-NLP/ABSA-AOAN.", "published": "2023-08-22 13:55:36", "link": "http://arxiv.org/abs/2308.11447v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Can Authorship Representation Learning Capture Stylistic Features?", "abstract": "Automatically disentangling an author's style from the content of their\nwriting is a longstanding and possibly insurmountable problem in computational\nlinguistics. At the same time, the availability of large text corpora furnished\nwith author labels has recently enabled learning authorship representations in\na purely data-driven manner for authorship attribution, a task that ostensibly\ndepends to a greater extent on encoding writing style than encoding content.\nHowever, success on this surrogate task does not ensure that such\nrepresentations capture writing style since authorship could also be correlated\nwith other latent variables, such as topic. In an effort to better understand\nthe nature of the information these representations convey, and specifically to\nvalidate the hypothesis that they chiefly encode writing style, we\nsystematically probe these representations through a series of targeted\nexperiments. The results of these experiments suggest that representations\nlearned for the surrogate authorship prediction task are indeed sensitive to\nwriting style. As a consequence, authorship representations may be expected to\nbe robust to certain kinds of data shift, such as topic drift over time.\nAdditionally, our findings may open the door to downstream applications that\nrequire stylistic representations, such as style transfer.", "published": "2023-08-22 15:10:45", "link": "http://arxiv.org/abs/2308.11490v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unsupervised Prototype Adapter for Vision-Language Models", "abstract": "Recently, large-scale pre-trained vision-language models (e.g. CLIP and\nALIGN) have demonstrated remarkable effectiveness in acquiring transferable\nvisual representations. To leverage the valuable knowledge encoded within these\nmodels for downstream tasks, several fine-tuning approaches, including prompt\ntuning methods and adapter-based methods, have been developed to adapt\nvision-language models effectively with supervision. However, these methods\nrely on the availability of annotated samples, which can be labor-intensive and\ntime-consuming to acquire, thus limiting scalability. To address this issue, in\nthis work, we design an unsupervised fine-tuning approach for vision-language\nmodels called Unsupervised Prototype Adapter (UP-Adapter). Specifically, for\nthe unannotated target datasets, we leverage the text-image aligning capability\nof CLIP to automatically select the most confident samples for each class.\nUtilizing these selected samples, we generate class prototypes, which serve as\nthe initialization for the learnable prototype model. After fine-tuning, the\nprototype model prediction is combined with the original CLIP's prediction by a\nresidual connection to perform downstream recognition tasks. Our extensive\nexperimental results on image recognition and domain generalization show that\nthe proposed unsupervised method outperforms 8-shot CoOp, 8-shot Tip-Adapter,\nand also the state-of-the-art UPL method by large margins.", "published": "2023-08-22 15:28:49", "link": "http://arxiv.org/abs/2308.11507v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "StoryBench: A Multifaceted Benchmark for Continuous Story Visualization", "abstract": "Generating video stories from text prompts is a complex task. In addition to\nhaving high visual quality, videos need to realistically adhere to a sequence\nof text prompts whilst being consistent throughout the frames. Creating a\nbenchmark for video generation requires data annotated over time, which\ncontrasts with the single caption used often in video datasets. To fill this\ngap, we collect comprehensive human annotations on three existing datasets, and\nintroduce StoryBench: a new, challenging multi-task benchmark to reliably\nevaluate forthcoming text-to-video models. Our benchmark includes three video\ngeneration tasks of increasing difficulty: action execution, where the next\naction must be generated starting from a conditioning video; story\ncontinuation, where a sequence of actions must be executed starting from a\nconditioning video; and story generation, where a video must be generated from\nonly text prompts. We evaluate small yet strong text-to-video baselines, and\nshow the benefits of training on story-like data algorithmically generated from\nexisting video captions. Finally, we establish guidelines for human evaluation\nof video stories, and reaffirm the need of better automatic metrics for video\ngeneration. StoryBench aims at encouraging future research efforts in this\nexciting new area.", "published": "2023-08-22 17:53:55", "link": "http://arxiv.org/abs/2308.11606v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Halo: Estimation and Reduction of Hallucinations in Open-Source Weak\n  Large Language Models", "abstract": "Large Language Models (LLMs) have revolutionized Natural Language Processing\n(NLP). Although convenient for research and practical applications, open-source\nLLMs with fewer parameters often suffer from severe hallucinations compared to\ntheir larger counterparts. This paper focuses on measuring and reducing\nhallucinations in BLOOM 7B, a representative of such weaker open-source LLMs\nthat are publicly available for research and commercial applications. We\nintroduce HaloCheck, a lightweight BlackBox knowledge-free framework designed\nto quantify the severity of hallucinations in LLMs. Additionally, we explore\ntechniques like knowledge injection and teacher-student approaches to alleviate\nhallucinations in low-parameter LLMs. Our experiments effectively demonstrate\nthe reduction of hallucinations in challenging domains for these LLMs.", "published": "2023-08-22 20:12:49", "link": "http://arxiv.org/abs/2308.11764v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Few-shot Anomaly Detection in Text with Deviation Learning", "abstract": "Most current methods for detecting anomalies in text concentrate on\nconstructing models solely relying on unlabeled data. These models operate on\nthe presumption that no labeled anomalous examples are available, which\nprevents them from utilizing prior knowledge of anomalies that are typically\npresent in small numbers in many real-world applications. Furthermore, these\nmodels prioritize learning feature embeddings rather than optimizing anomaly\nscores directly, which could lead to suboptimal anomaly scoring and inefficient\nuse of data during the learning process. In this paper, we introduce FATE, a\ndeep few-shot learning-based framework that leverages limited anomaly examples\nand learns anomaly scores explicitly in an end-to-end method using deviation\nlearning. In this approach, the anomaly scores of normal examples are adjusted\nto closely resemble reference scores obtained from a prior distribution.\nConversely, anomaly samples are forced to have anomalous scores that\nconsiderably deviate from the reference score in the upper tail of the prior.\nAdditionally, our model is optimized to learn the distinct behavior of\nanomalies by utilizing a multi-head self-attention layer and multiple instance\nlearning approaches. Comprehensive experiments on several benchmark datasets\ndemonstrate that our proposed approach attains a new level of state-of-the-art\nperformance.", "published": "2023-08-22 20:40:21", "link": "http://arxiv.org/abs/2308.11780v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Anonymity at Risk? Assessing Re-Identification Capabilities of Large\n  Language Models", "abstract": "Anonymity of both natural and legal persons in court rulings is a critical\naspect of privacy protection in the European Union and Switzerland. With the\nadvent of LLMs, concerns about large-scale re-identification of anonymized\npersons are growing. In accordance with the Federal Supreme Court of\nSwitzerland, we explore the potential of LLMs to re-identify individuals in\ncourt rulings by constructing a proof-of-concept using actual legal data from\nthe Swiss federal supreme court. Following the initial experiment, we\nconstructed an anonymized Wikipedia dataset as a more rigorous testing ground\nto further investigate the findings. With the introduction and application of\nthe new task of re-identifying people in texts, we also introduce new metrics\nto measure performance. We systematically analyze the factors that influence\nsuccessful re-identifications, identifying model size, input length, and\ninstruction tuning among the most critical determinants. Despite high\nre-identification rates on Wikipedia, even the best LLMs struggled with court\ndecisions. The complexity is attributed to the lack of test datasets, the\nnecessity for substantial training resources, and data sparsity in the\ninformation used for re-identification. In conclusion, this study demonstrates\nthat re-identification using LLMs may not be feasible for now, but as the\nproof-of-concept on Wikipedia showed, it might become possible in the future.\nWe hope that our system can help enhance the confidence in the security of\nanonymized decisions, thus leading to the courts being more confident to\npublish decisions.", "published": "2023-08-22 00:57:36", "link": "http://arxiv.org/abs/2308.11103v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "68T50", "I.2"], "primary_category": "cs.CL"}
{"title": "NLP-based detection of systematic anomalies among the narratives of\n  consumer complaints", "abstract": "We develop an NLP-based procedure for detecting systematic nonmeritorious\nconsumer complaints, simply called systematic anomalies, among complaint\nnarratives. While classification algorithms are used to detect pronounced\nanomalies, in the case of smaller and frequent systematic anomalies, the\nalgorithms may falter due to a variety of reasons, including technical ones as\nwell as natural limitations of human analysts. Therefore, as the next step\nafter classification, we convert the complaint narratives into quantitative\ndata, which are then analyzed using an algorithm for detecting systematic\nanomalies. We illustrate the entire procedure using complaint narratives from\nthe Consumer Complaint Database of the Consumer Financial Protection Bureau.", "published": "2023-08-22 02:39:42", "link": "http://arxiv.org/abs/2308.11138v3", "categories": ["stat.ME", "cs.CL", "q-fin.RM", "stat.ML"], "primary_category": "stat.ME"}
{"title": "LLaMA-Reviewer: Advancing Code Review Automation with Large Language\n  Models through Parameter-Efficient Fine-Tuning", "abstract": "The automation of code review activities, a long-standing pursuit in software\nengineering, has been primarily addressed by numerous domain-specific\npre-trained models. Despite their success, these models frequently demand\nextensive resources for pre-training from scratch. In contrast, Large Language\nModels (LLMs) provide an intriguing alternative, given their remarkable\ncapabilities when supplemented with domain-specific knowledge. However, their\npotential for automating code review tasks remains largely unexplored.\n  In response to this research gap, we present LLaMA-Reviewer, an innovative\nframework that leverages the capabilities of LLaMA, a popular LLM, in the realm\nof code review. Mindful of resource constraints, this framework employs\nparameter-efficient fine-tuning (PEFT) methods, delivering high performance\nwhile using less than 1% of trainable parameters.\n  An extensive evaluation of LLaMA-Reviewer is conducted on two diverse,\npublicly available datasets. Notably, even with the smallest LLaMA base model\nconsisting of 6.7B parameters and a limited number of tuning epochs,\nLLaMA-Reviewer equals the performance of existing code-review-focused models.\n  The ablation experiments provide insights into the influence of various\nfine-tuning process components, including input representation, instruction\ntuning, and different PEFT methods. To foster continuous progress in this\nfield, the code and all PEFT-weight plugins have been made open-source.", "published": "2023-08-22 03:10:40", "link": "http://arxiv.org/abs/2308.11148v2", "categories": ["cs.SE", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "Diversity Measures: Domain-Independent Proxies for Failure in Language\n  Model Queries", "abstract": "Error prediction in large language models often relies on domain-specific\ninformation. In this paper, we present measures for quantification of error in\nthe response of a large language model based on the diversity of responses to a\ngiven prompt - hence independent of the underlying application. We describe how\nthree such measures - based on entropy, Gini impurity, and centroid distance -\ncan be employed. We perform a suite of experiments on multiple datasets and\ntemperature settings to demonstrate that these measures strongly correlate with\nthe probability of failure. Additionally, we present empirical results\ndemonstrating how these measures can be applied to few-shot prompting,\nchain-of-thought reasoning, and error detection.", "published": "2023-08-22 04:49:23", "link": "http://arxiv.org/abs/2308.11189v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Convoifilter: A case study of doing cocktail party speech recognition", "abstract": "This paper presents an end-to-end model designed to improve automatic speech\nrecognition (ASR) for a particular speaker in a crowded, noisy environment. The\nmodel utilizes a single-channel speech enhancement module that isolates the\nspeaker's voice from background noise (ConVoiFilter) and an ASR module. The\nmodel can decrease ASR's word error rate (WER) from 80% to 26.4% through this\napproach. Typically, these two components are adjusted independently due to\nvariations in data requirements. However, speech enhancement can create\nanomalies that decrease ASR efficiency. By implementing a joint fine-tuning\nstrategy, the model can reduce the WER from 26.4% in separate tuning to 14.5%\nin joint tuning. We openly share our pre-trained model to foster further\nresearch hf.co/nguyenvulebinh/voice-filter.", "published": "2023-08-22 12:09:30", "link": "http://arxiv.org/abs/2308.11380v3", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Large Language Models Sensitivity to The Order of Options in\n  Multiple-Choice Questions", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious NLP tasks. However, previous works have shown these models are\nsensitive towards prompt wording, and few-shot demonstrations and their order,\nposing challenges to fair assessment of these models. As these models become\nmore powerful, it becomes imperative to understand and address these\nlimitations. In this paper, we focus on LLMs robustness on the task of\nmultiple-choice questions -- commonly adopted task to study reasoning and\nfact-retrieving capability of LLMs. Investigating the sensitivity of LLMs\ntowards the order of options in multiple-choice questions, we demonstrate a\nconsiderable performance gap of approximately 13% to 75% in LLMs on different\nbenchmarks, when answer options are reordered, even when using demonstrations\nin a few-shot setting. Through a detailed analysis, we conjecture that this\nsensitivity arises when LLMs are uncertain about the prediction between the\ntop-2/3 choices, and specific options placements may favor certain prediction\nbetween those top choices depending on the question caused by positional bias.\nWe also identify patterns in top-2 choices that amplify or mitigate the model's\nbias toward option placement. We found that for amplifying bias, the optimal\nstrategy involves positioning the top two choices as the first and last\noptions. Conversely, to mitigate bias, we recommend placing these choices among\nthe adjacent options. To validate our conjecture, we conduct various\nexperiments and adopt two approaches to calibrate LLMs' predictions, leading to\nup to 8 percentage points improvement across different models and benchmarks.", "published": "2023-08-22 14:54:59", "link": "http://arxiv.org/abs/2308.11483v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Tryage: Real-time, intelligent Routing of User Prompts to Large Language\n  Models", "abstract": "The introduction of the transformer architecture and the self-attention\nmechanism has led to an explosive production of language models trained on\nspecific downstream tasks and data domains. With over 200, 000 models in the\nHugging Face ecosystem, users grapple with selecting and optimizing models to\nsuit multifaceted workflows and data domains while addressing computational,\nsecurity, and recency concerns. There is an urgent need for machine learning\nframeworks that can eliminate the burden of model selection and customization\nand unleash the incredible power of the vast emerging model library for end\nusers. Here, we propose a context-aware routing system, Tryage, that leverages\na language model router for optimal selection of expert models from a model\nlibrary based on analysis of individual input prompts. Inspired by the thalamic\nrouter in the brain, Tryage employs a perceptive router to predict down-stream\nmodel performance on prompts and, then, makes a routing decision using an\nobjective function that integrates performance predictions with user goals and\nconstraints that are incorporated through flags (e.g., model size, model\nrecency). Tryage allows users to explore a Pareto front and automatically\ntrade-off between task accuracy and secondary goals including minimization of\nmodel size, recency, security, verbosity, and readability. Across heterogeneous\ndata sets that include code, text, clinical data, and patents, the Tryage\nframework surpasses Gorilla and GPT3.5 turbo in dynamic model selection\nidentifying the optimal model with an accuracy of 50.9% , compared to 23.6% by\nGPT 3.5 Turbo and 10.8% by Gorilla. Conceptually, Tryage demonstrates how\nrouting models can be applied to program and control the behavior of\nmulti-model LLM systems to maximize efficient use of the expanding and evolving\nlanguage model ecosystem.", "published": "2023-08-22 17:48:24", "link": "http://arxiv.org/abs/2308.11601v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.LG"}
{"title": "Efficient Benchmarking of Language Models", "abstract": "The increasing versatility of language models (LMs) has given rise to a new\nclass of benchmarks that comprehensively assess a broad range of capabilities.\nSuch benchmarks are associated with massive computational costs, extending to\nthousands of GPU hours per model. However, the efficiency aspect of these\nevaluation efforts had raised little discussion in the literature. In this\nwork, we present the problem of Efficient Benchmarking, namely, intelligently\nreducing the computation costs of LM evaluation without compromising\nreliability. Using the HELM benchmark as a test case, we investigate how\ndifferent benchmark design choices affect the computation-reliability\ntrade-off. We propose to evaluate the reliability of such decisions, by using a\nnew measure -- Decision Impact on Reliability, DIoR for short. We find, for\nexample, that a benchmark leader may change by merely removing a low-ranked\nmodel from the benchmark, and observe that a correct benchmark ranking can be\nobtained by considering only a fraction of the evaluation examples. Based on\nour findings, we outline a set of concrete recommendations for efficient\nbenchmark design and utilization practices. To take a step further, we use our\nfindings to propose an evaluation algorithm, that, when applied to the HELM\nbenchmark, leads to dramatic cost savings with minimal loss of benchmark\nreliability, often reducing computation by x100 or more.", "published": "2023-08-22 17:59:30", "link": "http://arxiv.org/abs/2308.11696v5", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Knowledge Graph Prompting for Multi-Document Question Answering", "abstract": "The `pre-train, prompt, predict' paradigm of large language models (LLMs) has\nachieved remarkable success in open-domain question answering (OD-QA). However,\nfew works explore this paradigm in the scenario of multi-document question\nanswering (MD-QA), a task demanding a thorough understanding of the logical\nassociations among the contents and structures of different documents. To fill\nthis crucial gap, we propose a Knowledge Graph Prompting (KGP) method to\nformulate the right context in prompting LLMs for MD-QA, which consists of a\ngraph construction module and a graph traversal module. For graph construction,\nwe create a knowledge graph (KG) over multiple documents with nodes symbolizing\npassages or document structures (e.g., pages/tables), and edges denoting the\nsemantic/lexical similarity between passages or intra-document structural\nrelations. For graph traversal, we design an LLM-based graph traversal agent\nthat navigates across nodes and gathers supporting passages assisting LLMs in\nMD-QA. The constructed graph serves as the global ruler that regulates the\ntransitional space among passages and reduces retrieval latency. Concurrently,\nthe graph traversal agent acts as a local navigator that gathers pertinent\ncontext to progressively approach the question and guarantee retrieval quality.\nExtensive experiments underscore the efficacy of KGP for MD-QA, signifying the\npotential of leveraging graphs in enhancing the prompt design for LLMs. Our\ncode: https://github.com/YuWVandy/KG-LLM-MDQA.", "published": "2023-08-22 18:41:31", "link": "http://arxiv.org/abs/2308.11730v3", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exploring the Effectiveness of GPT Models in Test-Taking: A Case Study\n  of the Driver's License Knowledge Test", "abstract": "Large language models such as Open AI's Generative Pre-trained Transformer\n(GPT) models are proficient at answering questions, but their knowledge is\nconfined to the information present in their training data. This limitation\nrenders them ineffective when confronted with questions about recent\ndevelopments or non-public documents. Our research proposes a method that\nenables GPT models to answer questions by employing context from an information\nsource not previously included in their training data. The methodology includes\npreprocessing of contextual information, the embedding of contexts and queries,\nconstructing prompt through the integration of context embeddings, and\ngenerating answers using GPT models. We applied this method in a controlled\ntest scenario using the California Driver's Handbook as the information source.\nThe GPT-3 model achieved a 96% passing score on a set of 50 sample driving\nknowledge test questions. In contrast, without context, the model's passing\nscore fell to 82%. However, the model still fails to answer some questions\ncorrectly even with providing library of context, highlighting room for\nimprovement. The research also examined the impact of prompt length and context\nformat, on the model's performance. Overall, the study provides insights into\nthe limitations and potential improvements for GPT models in question-answering\ntasks.", "published": "2023-08-22 23:18:53", "link": "http://arxiv.org/abs/2308.11827v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Inferring gender from name: a large scale performance evaluation study", "abstract": "A person's gender is a crucial piece of information when performing research\nacross a wide range of scientific disciplines, such as medicine, sociology,\npolitical science, and economics, to name a few. However, in increasing\ninstances, especially given the proliferation of big data, gender information\nis not readily available. In such cases researchers need to infer gender from\nreadily available information, primarily from persons' names. While inferring\ngender from name may raise some ethical questions, the lack of viable\nalternatives means that researchers have to resort to such approaches when the\ngoal justifies the means - in the majority of such studies the goal is to\nexamine patterns and determinants of gender disparities. The necessity of\nname-to-gender inference has generated an ever-growing domain of algorithmic\napproaches and software products. These approaches have been used throughout\nthe world in academia, industry, governmental and non-governmental\norganizations. Nevertheless, the existing approaches have yet to be\nsystematically evaluated and compared, making it challenging to determine the\noptimal approach for future research. In this work, we conducted a large scale\nperformance evaluation of existing approaches for name-to-gender inference.\nAnalysis are performed using a variety of large annotated datasets of names. We\nfurther propose two new hybrid approaches that achieve better performance than\nany single existing approach.", "published": "2023-08-22 13:38:45", "link": "http://arxiv.org/abs/2308.12381v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Music Understanding LLaMA: Advancing Text-to-Music Generation with\n  Question Answering and Captioning", "abstract": "Text-to-music generation (T2M-Gen) faces a major obstacle due to the scarcity\nof large-scale publicly available music datasets with natural language\ncaptions. To address this, we propose the Music Understanding LLaMA (MU-LLaMA),\ncapable of answering music-related questions and generating captions for music\nfiles. Our model utilizes audio representations from a pretrained MERT model to\nextract music features. However, obtaining a suitable dataset for training the\nMU-LLaMA model remains challenging, as existing publicly accessible audio\nquestion answering datasets lack the necessary depth for open-ended music\nquestion answering. To fill this gap, we present a methodology for generating\nquestion-answer pairs from existing audio captioning datasets and introduce the\nMusicQA Dataset designed for answering open-ended music-related questions. The\nexperiments demonstrate that the proposed MU-LLaMA model, trained on our\ndesigned MusicQA dataset, achieves outstanding performance in both music\nquestion answering and music caption generation across various metrics,\noutperforming current state-of-the-art (SOTA) models in both fields and\noffering a promising advancement in the T2M-Gen research field.", "published": "2023-08-22 08:43:33", "link": "http://arxiv.org/abs/2308.11276v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Identifying depression-related topics in smartphone-collected\n  free-response speech recordings using an automatic speech recognition system\n  and a deep learning topic model", "abstract": "Language use has been shown to correlate with depression, but large-scale\nvalidation is needed. Traditional methods like clinic studies are expensive.\nSo, natural language processing has been employed on social media to predict\ndepression, but limitations remain-lack of validated labels, biased user\nsamples, and no context. Our study identified 29 topics in 3919\nsmartphone-collected speech recordings from 265 participants using the Whisper\ntool and BERTopic model. Six topics with a median PHQ-8 greater than or equal\nto 10 were regarded as risk topics for depression: No Expectations, Sleep,\nMental Therapy, Haircut, Studying, and Coursework. To elucidate the topic\nemergence and associations with depression, we compared behavioral (from\nwearables) and linguistic characteristics across identified topics. The\ncorrelation between topic shifts and changes in depression severity over time\nwas also investigated, indicating the importance of longitudinally monitoring\nlanguage use. We also tested the BERTopic model on a similar smaller dataset\n(356 speech recordings from 57 participants), obtaining some consistent\nresults. In summary, our findings demonstrate specific speech topics may\nindicate depression severity. The presented data-driven workflow provides a\npractical approach to collecting and analyzing large-scale speech data from\nreal-world settings for digital health research.", "published": "2023-08-22 20:30:59", "link": "http://arxiv.org/abs/2308.11773v2", "categories": ["cs.CL", "cs.CY", "cs.SD", "eess.AS", "q-bio.QM"], "primary_category": "cs.CL"}
{"title": "Evaluation of the Speech Resynthesis Capabilities of the VoicePrivacy\n  Challenge Baseline B1", "abstract": "Speaker anonymization systems continue to improve their ability to obfuscate\nthe original speaker characteristics in a speech signal, but often create\nprocessing artifacts and unnatural sounding voices as a tradeoff. Many of those\nsystems stem from the VoicePrivacy Challenge (VPC) Baseline B1, using a neural\nvocoder to synthesize speech from an F0, x-vectors and bottleneck\nfeatures-based speech representation. Inspired by this, we investigate the\nreproduction capabilities of the aforementioned baseline, to assess how\nsuccessful the shared methodology is in synthesizing human-like speech. We use\nfour objective metrics to measure speech quality, waveform similarity, and F0\nsimilarity. Our findings indicate that both the speech representation and the\nvocoder introduces artifacts, causing an unnatural perception. A MUSHRA-like\nlistening test on 18 subjects corroborate our findings, motivating further\nresearch on the analysis and synthesis components of the VPC Baseline B1.", "published": "2023-08-22 10:32:21", "link": "http://arxiv.org/abs/2308.11337v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Deep learning-based denoising streamed from mobile phones improves\n  speech-in-noise understanding for hearing aid users", "abstract": "The hearing loss of almost half a billion people is commonly treated with\nhearing aids. However, current hearing aids often do not work well in\nreal-world noisy environments. We present a deep learning based denoising\nsystem that runs in real time on iPhone 7 and Samsung Galaxy S10 (25ms\nalgorithmic latency). The denoised audio is streamed to the hearing aid,\nresulting in a total delay of around 75ms. In tests with hearing aid users\nhaving moderate to severe hearing loss, our denoising system improves audio\nacross three tests: 1) listening for subjective audio ratings, 2) listening for\nobjective speech intelligibility, and 3) live conversations in a noisy\nenvironment for subjective ratings. Subjective ratings increase by more than\n40%, for both the listening test and the live conversation compared to a fitted\nhearing aid as a baseline. Speech reception thresholds, measuring speech\nunderstanding in noise, improve by 1.6 dB SRT. Ours is the first denoising\nsystem that is implemented on a mobile device, streamed directly to users'\nhearing aids using only a single channel as audio input while improving user\nsatisfaction on all tested aspects, including speech intelligibility. This\nincludes overall preference of the denoised and streamed signal over the\nhearing aid, thereby accepting the higher latency for the significant\nimprovement in speech understanding.", "published": "2023-08-22 14:05:43", "link": "http://arxiv.org/abs/2308.11456v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "An Effective Transformer-based Contextual Model and Temporal Gate\n  Pooling for Speaker Identification", "abstract": "Wav2vec2 has achieved success in applying Transformer architecture and\nself-supervised learning to speech recognition. Recently, these have come to be\nused not only for speech recognition but also for the entire speech processing.\nThis paper introduces an effective end-to-end speaker identification model\napplied Transformer-based contextual model. We explored the relationship\nbetween the hyper-parameters and the performance in order to discern the\nstructure of an effective model. Furthermore, we propose a pooling method,\nTemporal Gate Pooling, with powerful learning ability for speaker\nidentification. We applied Conformer as encoder and BEST-RQ for pre-training\nand conducted an evaluation utilizing the speaker identification of VoxCeleb1.\nThe proposed method has achieved an accuracy of 87.1% with 28.5M parameters,\ndemonstrating comparable precision to wav2vec2 with 317.7M parameters. Code is\navailable at https://github.com/HarunoriKawano/speaker-identification-with-tgp.", "published": "2023-08-22 07:34:07", "link": "http://arxiv.org/abs/2308.11241v2", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Leveraging Language Model Capabilities for Sound Event Detection", "abstract": "Large language models reveal deep comprehension and fluent generation in the\nfield of multi-modality. Although significant advancements have been achieved\nin audio multi-modality, existing methods are rarely leverage language model\nfor sound event detection (SED). In this work, we propose an end-to-end\nframework for understanding audio features while simultaneously generating\nsound event and their temporal location. Specifically, we employ pretrained\nacoustic models to capture discriminative features across different categories\nand language models for autoregressive text generation. Conventional methods\ngenerally struggle to obtain features in pure audio domain for classification.\nIn contrast, our framework utilizes the language model to flexibly understand\nabundant semantic context aligned with the acoustic representation. The\nexperimental results showcase the effectiveness of proposed method in enhancing\ntimestamps precision and event classification.", "published": "2023-08-22 15:59:06", "link": "http://arxiv.org/abs/2308.11530v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Complex-valued neural networks for voice anti-spoofing", "abstract": "Current anti-spoofing and audio deepfake detection systems use either\nmagnitude spectrogram-based features (such as CQT or Melspectrograms) or raw\naudio processed through convolution or sinc-layers. Both methods have\ndrawbacks: magnitude spectrograms discard phase information, which affects\naudio naturalness, and raw-feature-based models cannot use traditional\nexplainable AI methods. This paper proposes a new approach that combines the\nbenefits of both methods by using complex-valued neural networks to process the\ncomplex-valued, CQT frequency-domain representation of the input audio. This\nmethod retains phase information and allows for explainable AI methods. Results\nshow that this approach outperforms previous methods on the \"In-the-Wild\"\nanti-spoofing dataset and enables interpretation of the results through\nexplainable AI. Ablation studies confirm that the model has learned to use\nphase information to detect voice spoofing.", "published": "2023-08-22 21:49:38", "link": "http://arxiv.org/abs/2308.11800v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Modeling Bends in Popular Music Guitar Tablatures", "abstract": "Tablature notation is widely used in popular music to transcribe and share\nguitar musical content. As a complement to standard score notation, tablatures\ntranscribe performance gesture information including finger positions and a\nvariety of guitar-specific playing techniques such as slides,\nhammer-on/pull-off or bends.This paper focuses on bends, which enable to\nprogressively shift the pitch of a note, therefore circumventing physical\nlimitations of the discrete fretted fingerboard. In this paper, we propose a\nset of 25 high-level features, computed for each note of the tablature, to\nstudy how bend occurrences can be predicted from their past and future\nshort-term context. Experiments are performed on a corpus of 932 lead guitar\ntablatures of popular music and show that a decision tree successfully predicts\nbend occurrences with an F1 score of 0.71 anda limited amount of false positive\npredictions, demonstrating promising applications to assist the arrangement of\nnon-guitar music into guitar tablatures.", "published": "2023-08-22 07:50:58", "link": "http://arxiv.org/abs/2308.12307v1", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
