{"title": "Beyond Trend Following: Deep Learning for Market Trend Prediction", "abstract": "Trend following and momentum investing are common strategies employed by\nasset managers. Even though they can be helpful in the proper situations, they\nare limited in the sense that they work just by looking at past, as if we were\ndriving with our focus on the rearview mirror. In this paper, we advocate for\nthe use of Artificial Intelligence and Machine Learning techniques to predict\nfuture market trends. These predictions, when done properly, can improve the\nperformance of asset managers by increasing returns and reducing drawdowns.", "published": "2024-06-10 11:42:30", "link": "http://arxiv.org/abs/2407.13685v1", "categories": ["q-fin.TR", "cs.AI", "cs.LG", "q-fin.CP"], "primary_category": "q-fin.TR"}
{"title": "ThaiCoref: Thai Coreference Resolution Dataset", "abstract": "While coreference resolution is a well-established research area in Natural\nLanguage Processing (NLP), research focusing on Thai language remains limited\ndue to the lack of large annotated corpora. In this work, we introduce\nThaiCoref, a dataset for Thai coreference resolution. Our dataset comprises\n777,271 tokens, 44,082 mentions and 10,429 entities across four text genres:\nuniversity essays, newspapers, speeches, and Wikipedia. Our annotation scheme\nis built upon the OntoNotes benchmark with adjustments to address Thai-specific\nphenomena. Utilizing ThaiCoref, we train models employing a multilingual\nencoder and cross-lingual transfer techniques, achieving a best F1 score of\n67.88\\% on the test set. Error analysis reveals challenges posed by Thai's\nunique linguistic features. To benefit the NLP community, we make the dataset\nand the model publicly available at http://www.github.com/nlp-chula/thai-coref .", "published": "2024-06-10 03:47:24", "link": "http://arxiv.org/abs/2406.06000v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Shoulders of Giants: A Look at the Degree and Utility of Openness in NLP\n  Research", "abstract": "We analysed a sample of NLP research papers archived in ACL Anthology as an\nattempt to quantify the degree of openness and the benefit of such an open\nculture in the NLP community. We observe that papers published in different NLP\nvenues show different patterns related to artefact reuse. We also note that\nmore than 30% of the papers we analysed do not release their artefacts\npublicly, despite promising to do so. Further, we observe a wide language-wise\ndisparity in publicly available NLP-related artefacts.", "published": "2024-06-10 04:47:27", "link": "http://arxiv.org/abs/2406.06021v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HOLMES: Hyper-Relational Knowledge Graphs for Multi-hop Question\n  Answering using LLMs", "abstract": "Given unstructured text, Large Language Models (LLMs) are adept at answering\nsimple (single-hop) questions. However, as the complexity of the questions\nincrease, the performance of LLMs degrade. We believe this is due to the\noverhead associated with understanding the complex question followed by\nfiltering and aggregating unstructured information in the raw text. Recent\nmethods try to reduce this burden by integrating structured knowledge triples\ninto the raw text, aiming to provide a structured overview that simplifies\ninformation processing. However, this simplistic approach is query-agnostic and\nthe extracted facts are ambiguous as they lack context. To address these\ndrawbacks and to enable LLMs to answer complex (multi-hop) questions with ease,\nwe propose to use a knowledge graph (KG) that is context-aware and is distilled\nto contain query-relevant information. The use of our compressed distilled KG\nas input to the LLM results in our method utilizing up to $67\\%$ fewer tokens\nto represent the query relevant information present in the supporting\ndocuments, compared to the state-of-the-art (SoTA) method. Our experiments show\nconsistent improvements over the SoTA across several metrics (EM, F1,\nBERTScore, and Human Eval) on two popular benchmark datasets (HotpotQA and\nMuSiQue).", "published": "2024-06-10 05:22:49", "link": "http://arxiv.org/abs/2406.06027v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Curse of Popularity: Popular Entities have Catastrophic Side Effects\n  when Deleting Knowledge from Language Models", "abstract": "Language models (LMs) encode world knowledge in their internal parameters\nthrough training. However, LMs may learn personal and confidential information\nfrom the training data, leading to privacy concerns such as data leakage.\nTherefore, research on knowledge deletion from LMs is essential. This study\nfocuses on the knowledge stored in LMs and analyzes the relationship between\nthe side effects of knowledge deletion and the entities related to the\nknowledge. Our findings reveal that deleting knowledge related to popular\nentities can have catastrophic side effects. Furthermore, this research is the\nfirst to analyze knowledge deletion in models trained on synthetic knowledge\ngraphs, indicating a new direction for controlled experiments.", "published": "2024-06-10 05:50:23", "link": "http://arxiv.org/abs/2406.06032v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Multidimensional Framework for Evaluating Lexical Semantic Change with\n  Social Science Applications", "abstract": "Historical linguists have identified multiple forms of lexical semantic\nchange. We present a three-dimensional framework for integrating these forms\nand a unified computational methodology for evaluating them concurrently. The\ndimensions represent increases or decreases in semantic 1) sentiment, 2)\nbreadth, and 3) intensity. These dimensions can be complemented by the\nevaluation of shifts in the frequency of the target words and the thematic\ncontent of its collocates. This framework enables lexical semantic change to be\nmapped economically and systematically and has applications in computational\nsocial science. We present an illustrative analysis of semantic shifts in\nmental health and mental illness in two corpora, demonstrating patterns of\nsemantic change that illuminate contemporary concerns about pathologization,\nstigma, and concept creep.", "published": "2024-06-10 06:46:09", "link": "http://arxiv.org/abs/2406.06052v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Synth-SBDH: A Synthetic Dataset of Social and Behavioral Determinants of\n  Health for Clinical Text", "abstract": "Social and behavioral determinants of health (SBDH) play a crucial role in\nhealth outcomes and are frequently documented in clinical text. Automatically\nextracting SBDH information from clinical text relies on publicly available\ngood-quality datasets. However, existing SBDH datasets exhibit substantial\nlimitations in their availability and coverage. In this study, we introduce\nSynth-SBDH, a novel synthetic dataset with detailed SBDH annotations,\nencompassing status, temporal information, and rationale across 15 SBDH\ncategories. We showcase the utility of Synth-SBDH on three tasks using\nreal-world clinical datasets from two distinct hospital settings, highlighting\nits versatility, generalizability, and distillation capabilities. Models\ntrained on Synth-SBDH consistently outperform counterparts with no Synth-SBDH\ntraining, achieving up to 63.75% macro-F improvements. Additionally, Synth-SBDH\nproves effective for rare SBDH categories and under-resource constraints while\nbeing substantially cheaper than expert-annotated real-world data. Human\nevaluation reveals a 71.06% Human-LLM alignment and uncovers areas for future\nrefinements.", "published": "2024-06-10 07:03:36", "link": "http://arxiv.org/abs/2406.06056v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient k-Nearest-Neighbor Machine Translation with Dynamic Retrieval", "abstract": "To achieve non-parametric NMT domain adaptation, $k$-Nearest-Neighbor Machine\nTranslation ($k$NN-MT) constructs an external datastore to store\ndomain-specific translation knowledge, which derives a $k$NN distribution to\ninterpolate the prediction distribution of the NMT model via a linear\ninterpolation coefficient $\\lambda$. Despite its success, $k$NN retrieval at\neach timestep leads to substantial time overhead. To address this issue,\ndominant studies resort to $k$NN-MT with adaptive retrieval ($k$NN-MT-AR),\nwhich dynamically estimates $\\lambda$ and skips $k$NN retrieval if $\\lambda$ is\nless than a fixed threshold. Unfortunately, $k$NN-MT-AR does not yield\nsatisfactory results. In this paper, we first conduct a preliminary study to\nreveal two key limitations of $k$NN-MT-AR: 1) the optimization gap leads to\ninaccurate estimation of $\\lambda$ for determining $k$NN retrieval skipping,\nand 2) using a fixed threshold fails to accommodate the dynamic demands for\n$k$NN retrieval at different timesteps. To mitigate these limitations, we then\npropose $k$NN-MT with dynamic retrieval ($k$NN-MT-DR) that significantly\nextends vanilla $k$NN-MT in two aspects. Firstly, we equip $k$NN-MT with a\nMLP-based classifier for determining whether to skip $k$NN retrieval at each\ntimestep. Particularly, we explore several carefully-designed scalar features\nto fully exert the potential of the classifier. Secondly, we propose a\ntimestep-aware threshold adjustment method to dynamically generate the\nthreshold, which further improves the efficiency of our model. Experimental\nresults on the widely-used datasets demonstrate the effectiveness and\ngenerality of our model.\\footnote{Our code is available at\n\\url{https://github.com/DeepLearnXMU/knn-mt-dr}.", "published": "2024-06-10 07:36:55", "link": "http://arxiv.org/abs/2406.06073v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Verifiable Generation with Subsentence-Level Fine-Grained Citations", "abstract": "Verifiable generation requires large language models (LLMs) to cite source\ndocuments supporting their outputs, thereby improve output transparency and\ntrustworthiness. Yet, previous work mainly targets the generation of\nsentence-level citations, lacking specificity about which parts of a sentence\nare backed by the cited sources. This work studies verifiable generation with\nsubsentence-level fine-grained citations for more precise location of generated\ncontent supported by the cited sources. We first present a dataset, SCiFi,\ncomprising 10K Wikipedia paragraphs with subsentence-level citations. Each\nparagraph is paired with a set of candidate source documents for citation and a\nquery that triggers the generation of the paragraph content. On SCiFi, we\nevaluate the performance of state-of-the-art LLMs and strategies for processing\nlong documents designed for these models. Our experiment results reveals key\nfactors that could enhance the quality of citations, including the expansion of\nthe source documents' context accessible to the models and the implementation\nof specialized model tuning.", "published": "2024-06-10 09:32:37", "link": "http://arxiv.org/abs/2406.06125v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Building Bridges: A Dataset for Evaluating Gender-Fair Machine\n  Translation into German", "abstract": "The translation of gender-neutral person-referring terms (e.g., the students)\nis often non-trivial. Translating from English into German poses an interesting\ncase -- in German, person-referring nouns are usually gender-specific, and if\nthe gender of the referent(s) is unknown or diverse, the generic masculine (die\nStudenten (m.)) is commonly used. This solution, however, reduces the\nvisibility of other genders, such as women and non-binary people. To counteract\ngender discrimination, a societal movement towards using gender-fair language\nexists (e.g., by adopting neosystems). However, gender-fair German is currently\nbarely supported in machine translation (MT), requiring post-editing or manual\ntranslations. We address this research gap by studying gender-fair language in\nEnglish-to-German MT. Concretely, we enrich a community-created gender-fair\nlanguage dictionary and sample multi-sentence test instances from encyclopedic\ntext and parliamentary speeches. Using these novel resources, we conduct the\nfirst benchmark study involving two commercial systems and six neural MT models\nfor translating words in isolation and natural contexts across two domains. Our\nfindings show that most systems produce mainly masculine forms and rarely\ngender-neutral variants, highlighting the need for future research. We release\ncode and data at\nhttps://github.com/g8a9/building-bridges-gender-fair-german-mt.", "published": "2024-06-10 09:39:19", "link": "http://arxiv.org/abs/2406.06131v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LINGOLY: A Benchmark of Olympiad-Level Linguistic Reasoning Puzzles in\n  Low-Resource and Extinct Languages", "abstract": "In this paper, we present the LingOly benchmark, a novel benchmark for\nadvanced reasoning abilities in large language models. Using challenging\nLinguistic Olympiad puzzles, we evaluate (i) capabilities for in-context\nidentification and generalisation of linguistic patterns in very low-resource\nor extinct languages, and (ii) abilities to follow complex task instructions.\nThe LingOly benchmark covers more than 90 mostly low-resource languages,\nminimising issues of data contamination, and contains 1,133 problems across 6\nformats and 5 levels of human difficulty. We assess performance with both\ndirect accuracy and comparison to a no-context baseline to penalise\nmemorisation. Scores from 11 state-of-the-art LLMs demonstrate the benchmark to\nbe challenging, and models perform poorly on the higher difficulty problems. On\nharder problems, even the top model only achieved 38.7% accuracy, a 24.7%\nimprovement over the no-context baseline. Large closed models typically\noutperform open models, and in general, the higher resource the language, the\nbetter the scores. These results indicate, in absence of memorisation, true\nmulti-step out-of-domain reasoning remains a challenge for current language\nmodels.", "published": "2024-06-10 11:50:29", "link": "http://arxiv.org/abs/2406.06196v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MaskLID: Code-Switching Language Identification through Iterative\n  Masking", "abstract": "We present MaskLID, a simple, yet effective, code-switching (CS) language\nidentification (LID) method. MaskLID does not require any training and is\ndesigned to complement current high-performance sentence-level LIDs.\nSentence-level LIDs are classifiers trained on monolingual texts to provide\nsingle labels, typically using a softmax layer to turn scores into\nprobabilities. However, in cases where a sentence is composed in both L1 and L2\nlanguages, the LID classifier often only returns the dominant label L1. To\naddress this limitation, MaskLID employs a strategy to mask text features\nassociated with L1, allowing the LID to classify the text as L2 in the next\nround. This method uses the LID itself to identify the features that require\nmasking and does not rely on any external resource. In this work, we explore\nthe use of MaskLID for two open-source LIDs (GlotLID and OpenLID), that are\nboth based on the FastText architecture. Code and demo are available at\nhttps://github.com/cisnlp/MaskLID.", "published": "2024-06-10 13:44:29", "link": "http://arxiv.org/abs/2406.06263v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Prompting Decoder Helps Better Language Understanding", "abstract": "Recent Pre-trained Language Models (PLMs) usually only provide users with the\ninference APIs, namely the emerging Model-as-a-Service (MaaS) setting. To adapt\nMaaS PLMs to downstream tasks without accessing their parameters and gradients,\nsome existing methods focus on the output-side adaptation of PLMs, viewing the\nPLM as an encoder and then optimizing a task-specific decoder for decoding the\noutput hidden states and class scores of the PLM. Despite the effectiveness of\nthese methods, they only use a single prompt to query PLMs for decoding,\nleading to a heavy reliance on the quality of the adopted prompt. In this\npaper, we propose a simple yet effective Multi-Prompting Decoder (MPD)\nframework for MaaS adaptation. The core idea is to query PLMs with multiple\ndifferent prompts for each sample, thereby obtaining multiple output hidden\nstates and class scores for subsequent decoding. Such multi-prompting decoding\nparadigm can simultaneously mitigate reliance on the quality of a single\nprompt, alleviate the issue of data scarcity under the few-shot setting, and\nprovide richer knowledge extracted from PLMs. Specifically, we propose two\ndecoding strategies: multi-prompting decoding with optimal transport for hidden\nstates and calibrated decoding for class scores. Extensive experiments\ndemonstrate that our method achieves new state-of-the-art results on multiple\nnatural language understanding datasets under the few-shot setting.", "published": "2024-06-10 13:58:46", "link": "http://arxiv.org/abs/2406.06279v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Tuning: Instructing LLMs to Effectively Acquire New Knowledge\n  through Self-Teaching", "abstract": "Large language models (LLMs) often struggle to provide up-to-date information\ndue to their one-time training and the constantly evolving nature of the world.\nTo keep LLMs current, existing approaches typically involve continued\npre-training on new documents. However, they frequently face difficulties in\nextracting stored knowledge. Motivated by the remarkable success of the Feynman\nTechnique in efficient human learning, we introduce Self-Tuning, a learning\nframework aimed at improving an LLM's ability to effectively acquire new\nknowledge from unseen raw documents through self-teaching. Specifically, we\ndevelop a Self-Teaching strategy that augments the documents with a set of\nknowledge-intensive tasks created in a self-supervised manner, focusing on\nthree crucial aspects: memorization, comprehension, and self-reflection.\nAdditionally, we introduce three Wiki-Newpages-2023-QA datasets to facilitate\nan in-depth analysis of an LLM's knowledge acquisition ability concerning\nmemorization, extraction, and reasoning. Extensive experimental results on\nvarious models, e.g., Llama2-7B reveal that Self-Tuning consistently exhibits\nsuperior performance across all knowledge acquisition tasks and excels in\npreserving previous knowledge.", "published": "2024-06-10 14:42:20", "link": "http://arxiv.org/abs/2406.06326v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sustained Vowels for Pre- vs Post-Treatment COPD Classification", "abstract": "Chronic obstructive pulmonary disease (COPD) is a serious inflammatory lung\ndisease affecting millions of people around the world. Due to an obstructed\nairflow from the lungs, it also becomes manifest in patients' vocal behaviour.\nOf particular importance is the detection of an exacerbation episode, which\nmarks an acute phase and often requires hospitalisation and treatment. Previous\nwork has shown that it is possible to distinguish between a pre- and a\npost-treatment state using automatic analysis of read speech. In this\ncontribution, we examine whether sustained vowels can provide a complementary\nlens for telling apart these two states. Using a cohort of 50 patients, we show\nthat the inclusion of sustained vowels can improve performance to up to 79\\%\nunweighted average recall, from a 71\\% baseline using read speech. We further\nidentify and interpret the most important acoustic features that characterise\nthe manifestation of COPD in sustained vowels.", "published": "2024-06-10 15:17:17", "link": "http://arxiv.org/abs/2406.06355v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Symmetric Dot-Product Attention for Efficient Training of BERT Language\n  Models", "abstract": "Initially introduced as a machine translation model, the Transformer\narchitecture has now become the foundation for modern deep learning\narchitecture, with applications in a wide range of fields, from computer vision\nto natural language processing. Nowadays, to tackle increasingly more complex\ntasks, Transformer-based models are stretched to enormous sizes, requiring\nincreasingly larger training datasets, and unsustainable amount of compute\nresources. The ubiquitous nature of the Transformer and its core component, the\nattention mechanism, are thus prime targets for efficiency research. In this\nwork, we propose an alternative compatibility function for the self-attention\nmechanism introduced by the Transformer architecture. This compatibility\nfunction exploits an overlap in the learned representation of the traditional\nscaled dot-product attention, leading to a symmetric with pairwise coefficient\ndot-product attention. When applied to the pre-training of BERT-like models,\nthis new symmetric attention mechanism reaches a score of 79.36 on the GLUE\nbenchmark against 78.74 for the traditional implementation, leads to a\nreduction of 6% in the number of trainable parameters, and reduces the number\nof training steps required before convergence by half.", "published": "2024-06-10 15:24:15", "link": "http://arxiv.org/abs/2406.06366v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Annotation alignment: Comparing LLM and human annotations of\n  conversational safety", "abstract": "Do LLMs align with human perceptions of safety? We study this question via\nannotation alignment, the extent to which LLMs and humans agree when annotating\nthe safety of user-chatbot conversations. We leverage the recent DICES dataset\n(Aroyo et al., 2023), in which 350 conversations are each rated for safety by\n112 annotators spanning 10 race-gender groups. GPT-4 achieves a Pearson\ncorrelation of $r = 0.59$ with the average annotator rating, \\textit{higher}\nthan the median annotator's correlation with the average ($r=0.51$). We show\nthat larger datasets are needed to resolve whether LLMs exhibit disparities in\nhow well they correlate with different demographic groups. Also, there is\nsubstantial idiosyncratic variation in correlation within groups, suggesting\nthat race & gender do not fully capture differences in alignment. Finally, we\nfind that GPT-4 cannot predict when one demographic group finds a conversation\nmore unsafe than another.", "published": "2024-06-10 15:30:13", "link": "http://arxiv.org/abs/2406.06369v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "INTERSPEECH 2009 Emotion Challenge Revisited: Benchmarking 15 Years of\n  Progress in Speech Emotion Recognition", "abstract": "We revisit the INTERSPEECH 2009 Emotion Challenge -- the first ever speech\nemotion recognition (SER) challenge -- and evaluate a series of deep learning\nmodels that are representative of the major advances in SER research in the\ntime since then. We start by training each model using a fixed set of\nhyperparameters, and further fine-tune the best-performing models of that\ninitial setup with a grid search. Results are always reported on the official\ntest set with a separate validation set only used for early stopping. Most\nmodels score below or close to the official baseline, while they marginally\noutperform the original challenge winners after hyperparameter tuning. Our work\nillustrates that, despite recent progress, FAU-AIBO remains a very challenging\nbenchmark. An interesting corollary is that newer methods do not consistently\noutperform older ones, showing that progress towards `solving' SER is not\nnecessarily monotonic.", "published": "2024-06-10 15:55:06", "link": "http://arxiv.org/abs/2406.06401v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning\n  Strategies", "abstract": "A diverse array of reasoning strategies has been proposed to elicit the\ncapabilities of large language models. However, in this paper, we point out\nthat traditional evaluations which focus solely on performance metrics miss a\nkey factor: the increased effectiveness due to additional compute. By\noverlooking this aspect, a skewed view of strategy efficiency is often\npresented. This paper introduces a framework that incorporates the compute\nbudget into the evaluation, providing a more informative comparison that takes\ninto account both performance metrics and computational cost. In this\nbudget-aware perspective, we find that complex reasoning strategies often don't\nsurpass simpler baselines purely due to algorithmic ingenuity, but rather due\nto the larger computational resources allocated. When we provide a simple\nbaseline like chain-of-thought self-consistency with comparable compute\nresources, it frequently outperforms reasoning strategies proposed in the\nliterature. In this scale-aware perspective, we find that unlike\nself-consistency, certain strategies such as multi-agent debate or Reflexion\ncan become worse if more compute budget is utilized.", "published": "2024-06-10 16:55:08", "link": "http://arxiv.org/abs/2406.06461v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enrolment-based personalisation for improving individual-level fairness\n  in speech emotion recognition", "abstract": "The expression of emotion is highly individualistic. However, contemporary\nspeech emotion recognition (SER) systems typically rely on population-level\nmodels that adopt a `one-size-fits-all' approach for predicting emotion.\nMoreover, standard evaluation practices measure performance also on the\npopulation level, thus failing to characterise how models work across different\nspeakers. In the present contribution, we present a new method for capitalising\non individual differences to adapt an SER model to each new speaker using a\nminimal set of enrolment utterances. In addition, we present novel evaluation\nschemes for measuring fairness across different speakers. Our findings show\nthat aggregated evaluation metrics may obfuscate fairness issues on the\nindividual-level, which are uncovered by our evaluation, and that our proposed\nmethod can improve performance both in aggregated and disaggregated terms.", "published": "2024-06-10 16:01:05", "link": "http://arxiv.org/abs/2406.06665v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AGB-DE: A Corpus for the Automated Legal Assessment of Clauses in German\n  Consumer Contracts", "abstract": "Legal tasks and datasets are often used as benchmarks for the capabilities of\nlanguage models. However, openly available annotated datasets are rare. In this\npaper, we introduce AGB-DE, a corpus of 3,764 clauses from German consumer\ncontracts that have been annotated and legally assessed by legal experts.\nTogether with the data, we present a first baseline for the task of detecting\npotentially void clauses, comparing the performance of an SVM baseline with\nthree fine-tuned open language models and the performance of GPT-3.5. Our\nresults show the challenging nature of the task, with no approach exceeding an\nF1-score of 0.54. While the fine-tuned models often performed better with\nregard to precision, GPT-3.5 outperformed the other approaches with regard to\nrecall. An analysis of the errors indicates that one of the main challenges\ncould be the correct interpretation of complex clauses, rather than the\ndecision boundaries of what is permissible and what is not.", "published": "2024-06-10 21:27:13", "link": "http://arxiv.org/abs/2406.06809v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EAVE: Efficient Product Attribute Value Extraction via Lightweight\n  Sparse-layer Interaction", "abstract": "Product attribute value extraction involves identifying the specific values\nassociated with various attributes from a product profile. While existing\nmethods often prioritize the development of effective models to improve\nextraction performance, there has been limited emphasis on extraction\nefficiency. However, in real-world scenarios, products are typically associated\nwith multiple attributes, necessitating multiple extractions to obtain all\ncorresponding values. In this work, we propose an Efficient product Attribute\nValue Extraction (EAVE) approach via lightweight sparse-layer interaction.\nSpecifically, we employ a heavy encoder to separately encode the product\ncontext and attribute. The resulting non-interacting heavy representations of\nthe context can be cached and reused for all attributes. Additionally, we\nintroduce a light encoder to jointly encode the context and the attribute,\nfacilitating lightweight interactions between them. To enrich the interaction\nwithin the lightweight encoder, we design a sparse-layer interaction module to\nfuse the non-interacting heavy representation into the lightweight encoder.\nComprehensive evaluation on two benchmarks demonstrate that our method achieves\nsignificant efficiency gains with neutral or marginal loss in performance when\nthe context is long and number of attributes is large. Our code is available\n\\href{https://anonymous.4open.science/r/EAVE-EA18}{here}.", "published": "2024-06-10 23:06:38", "link": "http://arxiv.org/abs/2406.06839v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Curating Grounded Synthetic Data with Global Perspectives for Equitable\n  AI", "abstract": "The development of robust AI models relies heavily on the quality and variety\nof training data available. In fields where data scarcity is prevalent,\nsynthetic data generation offers a vital solution. In this paper, we introduce\na novel approach to creating synthetic datasets, grounded in real-world\ndiversity and enriched through strategic diversification. We synthesize data\nusing a comprehensive collection of news articles spanning 12 languages and\noriginating from 125 countries, to ensure a breadth of linguistic and cultural\nrepresentations. Through enforced topic diversification, translation, and\nsummarization, the resulting dataset accurately mirrors real-world complexities\nand addresses the issue of underrepresentation in traditional datasets. This\nmethodology, applied initially to Named Entity Recognition (NER), serves as a\nmodel for numerous AI disciplines where data diversification is critical for\ngeneralizability. Preliminary results demonstrate substantial improvements in\nperformance on traditional NER benchmarks, by up to 7.3%, highlighting the\neffectiveness of our synthetic data in mimicking the rich, varied nuances of\nglobal data sources. This paper outlines the strategies employed for\nsynthesizing diverse datasets and provides such a curated dataset for NER.", "published": "2024-06-10 17:59:11", "link": "http://arxiv.org/abs/2406.10258v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated\n  Parameters", "abstract": "Exploiting activation sparsity is a promising approach to significantly\naccelerating the inference process of large language models (LLMs) without\ncompromising performance. However, activation sparsity is determined by\nactivation functions, and commonly used ones like SwiGLU and GeGLU exhibit\nlimited sparsity. Simply replacing these functions with ReLU fails to achieve\nsufficient sparsity. Moreover, inadequate training data can further increase\nthe risk of performance degradation. To address these challenges, we propose a\nnovel dReLU function, which is designed to improve LLM activation sparsity,\nalong with a high-quality training data mixture ratio to facilitate effective\nsparsification. Additionally, we leverage sparse activation patterns within the\nFeed-Forward Network (FFN) experts of Mixture-of-Experts (MoE) models to\nfurther boost efficiency. By applying our neuron sparsification method to the\nMistral and Mixtral models, only 2.5 billion and 4.3 billion parameters are\nactivated per inference iteration, respectively, while achieving even more\npowerful model performance. Evaluation results demonstrate that this sparsity\nachieves a 2-5x decoding speedup. Remarkably, on mobile phones, our\nTurboSparse-Mixtral-47B achieves an inference speed of 11 tokens per second.\nOur models are available at \\url{https://huggingface.co/PowerInfer}", "published": "2024-06-10 01:21:59", "link": "http://arxiv.org/abs/2406.05955v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Prompting Large Language Models with Audio for General-Purpose Speech\n  Summarization", "abstract": "In this work, we introduce a framework for speech summarization that\nleverages the processing and reasoning capabilities of large language models\n(LLMs). We propose an end-to-end system that combines an instruction-tuned LLM\nwith an audio encoder that converts speech into token representations that the\nLLM can interpret. Using a dataset with paired speech-text data, the overall\nsystem is trained to generate consistent responses to prompts with the same\nsemantic information regardless of the input modality. The resulting framework\nallows the LLM to process speech inputs in the same way as text, enabling\nspeech summarization by simply prompting the LLM. Unlike prior approaches, our\nmethod is able to summarize spoken content from any arbitrary domain, and it\ncan produce summaries in different styles by varying the LLM prompting\nstrategy. Experiments demonstrate that our approach outperforms a cascade\nbaseline of speech recognition followed by LLM text processing.", "published": "2024-06-10 02:04:28", "link": "http://arxiv.org/abs/2406.05968v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "MATES: Model-Aware Data Selection for Efficient Pretraining with Data\n  Influence Models", "abstract": "Pretraining data selection has the potential to improve language model\npretraining efficiency by utilizing higher-quality data from massive web data\ncorpora. Current data selection methods, which rely on either hand-crafted\nrules or larger reference models, are conducted statically and do not capture\nthe evolving data preferences during pretraining. In this paper, we introduce\nmodel-aware data selection with data influence models (MATES), where a data\ninfluence model continuously adapts to the evolving data preferences of the\npretraining model and then selects the data most effective for the current\npretraining progress. Specifically, we collect oracle data influence by locally\nprobing the pretraining model and fine-tune a small data influence model to\napproximate it accurately. The data influence model then predicts data\ninfluence over the whole pretraining corpus and selects the most influential\ndata for the next pretraining stage. Experiments of pretraining 410M and 1B\nmodels on the C4 dataset demonstrate that MATES significantly outperforms\nrandom data selection on extensive downstream tasks. It doubles the gains\nachieved by the state-of-the-art data selection approach that leverages larger\nreference models and reduces the total FLOPs required to reach certain\nperformances by half. Further analyses validate the effectiveness of the\nlocally probed oracle data influence and the approximation with data influence\nmodels. Our code is open-sourced at https://github.com/cxcscmu/MATES.", "published": "2024-06-10 06:27:42", "link": "http://arxiv.org/abs/2406.06046v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Recurrent Context Compression: Efficiently Expanding the Context Window\n  of LLM", "abstract": "To extend the context length of Transformer-based large language models\n(LLMs) and improve comprehension capabilities, we often face limitations due to\ncomputational resources and bounded memory storage capacity. This work\nintroduces a method called Recurrent Context Compression (RCC), designed to\nefficiently expand the context window length of LLMs within constrained storage\nspace. We also investigate the issue of poor model responses when both\ninstructions and context are compressed in downstream tasks, and propose an\ninstruction reconstruction method to mitigate this problem. We validated the\neffectiveness of our approach on multiple tasks, achieving a compression rate\nof up to 32x on text reconstruction tasks with a BLEU4 score close to 0.95, and\nnearly 100\\% accuracy on a passkey retrieval task with a sequence length of 1M.\nFinally, our method demonstrated competitive performance in long-text\nquestion-answering tasks compared to non-compressed methods, while\nsignificantly saving storage resources in long-text inference tasks. Our code,\nmodels, and demo are available at https://github.com/WUHU-G/RCC_Transformer", "published": "2024-06-10 08:50:59", "link": "http://arxiv.org/abs/2406.06110v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing Long-Term Memory using Hierarchical Aggregate Tree for\n  Retrieval Augmented Generation", "abstract": "Large language models have limited context capacity, hindering reasoning over\nlong conversations. We propose the Hierarchical Aggregate Tree memory structure\nto recursively aggregate relevant dialogue context through conditional tree\ntraversals. HAT encapsulates information from children nodes, enabling broad\ncoverage with depth control. We formulate finding best context as optimal tree\ntraversal. Experiments show HAT improves dialog coherence and summary quality\nover baseline contexts, demonstrating the techniques effectiveness for multi\nturn reasoning without exponential parameter growth. This memory augmentation\nenables more consistent, grounded longform conversations from LLMs", "published": "2024-06-10 09:29:08", "link": "http://arxiv.org/abs/2406.06124v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Comparing Data Augmentation Methods for End-to-End Task-Oriented Dialog\n  Systems", "abstract": "Creating effective and reliable task-oriented dialog systems (ToDSs) is\nchallenging, not only because of the complex structure of these systems, but\nalso due to the scarcity of training data, especially when several modules need\nto be trained separately, each one with its own input/output training examples.\nData augmentation (DA), whereby synthetic training examples are added to the\ntraining data, has been successful in other NLP systems, but has not been\nexplored as extensively in ToDSs. We empirically evaluate the effectiveness of\nDA methods in an end-to-end ToDS setting, where a single system is trained to\nhandle all processing stages, from user inputs to system outputs. We experiment\nwith two ToDSs (UBAR, GALAXY) on two datasets (MultiWOZ, KVRET). We consider\nthree types of DA methods (word-level, sentence-level, dialog-level), comparing\neight DA methods that have shown promising results in ToDSs and other NLP\nsystems. We show that all DA methods considered are beneficial, and we\nhighlight the best ones, also providing advice to practitioners. We also\nintroduce a more challenging few-shot cross-domain ToDS setting, reaching\nsimilar conclusions.", "published": "2024-06-10 09:36:05", "link": "http://arxiv.org/abs/2406.06127v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Can I understand what I create? Self-Knowledge Evaluation of Large\n  Language Models", "abstract": "Large language models (LLMs) have achieved remarkable progress in linguistic\ntasks, necessitating robust evaluation frameworks to understand their\ncapabilities and limitations. Inspired by Feynman's principle of understanding\nthrough creation, we introduce a self-knowledge evaluation framework that is\neasy to implement, evaluating models on their ability to comprehend and respond\nto self-generated questions. Our findings, based on testing multiple models\nacross diverse tasks, reveal significant gaps in the model's self-knowledge\nability. Further analysis indicates these gaps may be due to misalignment with\nhuman attention mechanisms. Additionally, fine-tuning on self-generated math\ntask may enhance the model's math performance, highlighting the potential of\nthe framework for efficient and insightful model evaluation and may also\ncontribute to the improvement of LLMs.", "published": "2024-06-10 09:53:54", "link": "http://arxiv.org/abs/2406.06140v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Language Models Resist Alignment: Evidence From Data Compression", "abstract": "Large language models (LLMs) may exhibit unintended or undesirable behaviors.\nRecent works have concentrated on aligning LLMs to mitigate harmful outputs.\nDespite these efforts, some anomalies indicate that even a well-conducted\nalignment process can be easily circumvented, whether intentionally or\naccidentally. Does alignment fine-tuning yield have robust effects on models,\nor are its impacts merely superficial? In this work, we make the first\nexploration of this phenomenon from both theoretical and empirical\nperspectives. Empirically, we demonstrate the elasticity of post-alignment\nmodels, i.e., the tendency to revert to the behavior distribution formed during\nthe pre-training phase upon further fine-tuning. Leveraging compression theory,\nwe formally deduce that fine-tuning disproportionately undermines alignment\nrelative to pre-training, potentially by orders of magnitude. We validate the\npresence of elasticity through experiments on models of varying types and\nscales. Specifically, we find that model performance declines rapidly before\nreverting to the pre-training distribution, after which the rate of decline\ndrops significantly. Furthermore, we further reveal that elasticity positively\ncorrelates with the increased model size and the expansion of pre-training\ndata. Our findings underscore the need to address the inherent elasticity of\nLLMs to mitigate their resistance to alignment.", "published": "2024-06-10 10:03:16", "link": "http://arxiv.org/abs/2406.06144v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning Fine-Grained Controllability on Speech Generation via Efficient\n  Fine-Tuning", "abstract": "As the scale of generative models continues to grow, efficient reuse and\nadaptation of pre-trained models have become crucial considerations. In this\nwork, we propose Voicebox Adapter, a novel approach that integrates\nfine-grained conditions into a pre-trained Voicebox speech generation model\nusing a cross-attention module. To ensure a smooth integration of newly added\nmodules with pre-trained ones, we explore various efficient fine-tuning\napproaches. Our experiment shows that the LoRA with bias-tuning configuration\nyields the best performance, enhancing controllability without compromising\nspeech quality. Across three fine-grained conditional generation tasks, we\ndemonstrate the effectiveness and resource efficiency of Voicebox Adapter.\nFollow-up experiments further highlight the robustness of Voicebox Adapter\nacross diverse data setups.", "published": "2024-06-10 13:31:18", "link": "http://arxiv.org/abs/2406.06251v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Combining Embeddings and Domain Knowledge for Job Posting Duplicate\n  Detection", "abstract": "Job descriptions are posted on many online channels, including company\nwebsites, job boards or social media platforms. These descriptions are usually\npublished with varying text for the same job, due to the requirements of each\nplatform or to target different audiences. However, for the purpose of\nautomated recruitment and assistance of people working with these texts, it is\nhelpful to aggregate job postings across platforms and thus detect duplicate\ndescriptions that refer to the same job. In this work, we propose an approach\nfor detecting duplicates in job descriptions. We show that combining\noverlap-based character similarity with text embedding and keyword matching\nmethods lead to convincing results. In particular, we show that although no\napproach individually achieves satisfying performance, a combination of string\ncomparison, deep textual embeddings, and the use of curated weighted lookup\nlists for specific skills leads to a significant boost in overall performance.\nA tool based on our approach is being used in production and feedback from\nreal-life use confirms our evaluation.", "published": "2024-06-10 13:38:15", "link": "http://arxiv.org/abs/2406.06257v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "A Parameter-efficient Language Extension Framework for Multilingual ASR", "abstract": "Covering all languages with a multilingual speech recognition model (MASR) is\nvery difficult. Performing language extension on top of an existing MASR is a\ndesirable choice. In this study, the MASR continual learning problem is\nprobabilistically decomposed into language identity prediction (LP) and\ncross-lingual adaptation (XLA) sub-problems. Based on this, we propose an\narchitecture-based framework for language extension that can fundamentally\nsolve catastrophic forgetting, debudded as PELE. PELE is designed to be\nparameter-efficient, incrementally incorporating an add-on module to adapt to a\nnew language. Specifically, different parameter-efficient fine-tuning (PEFT)\nmodules and their variants are explored as potential candidates to perform XLA.\nExperiments are carried out on 5 new languages with a wide range of\nlow-resourced data sizes. The best-performing PEFT candidate can achieve\nsatisfactory performance across all languages and demonstrates superiority in\nthree of five languages over the continual joint learning setting. Notably,\nPEFT methods focusing on weight parameters or input features are revealed to be\nlimited in performance, showing significantly inferior extension capabilities\ncompared to inserting a lightweight module in between layers such as an\nAdapter.", "published": "2024-06-10 14:46:07", "link": "http://arxiv.org/abs/2406.06329v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "MedExQA: Medical Question Answering Benchmark with Multiple Explanations", "abstract": "This paper introduces MedExQA, a novel benchmark in medical\nquestion-answering, to evaluate large language models' (LLMs) understanding of\nmedical knowledge through explanations. By constructing datasets across five\ndistinct medical specialties that are underrepresented in current datasets and\nfurther incorporating multiple explanations for each question-answer pair, we\naddress a major gap in current medical QA benchmarks which is the absence of\ncomprehensive assessments of LLMs' ability to generate nuanced medical\nexplanations. Our work highlights the importance of explainability in medical\nLLMs, proposes an effective methodology for evaluating models beyond\nclassification accuracy, and sheds light on one specific domain, speech\nlanguage pathology, where current LLMs including GPT4 lack good understanding.\nOur results show generation evaluation with multiple explanations aligns better\nwith human assessment, highlighting an opportunity for a more robust automated\ncomprehension assessment for LLMs. To diversify open-source medical LLMs\n(currently mostly based on Llama2), this work also proposes a new medical\nmodel, MedPhi-2, based on Phi-2 (2.7B). The model outperformed medical LLMs\nbased on Llama2-70B in generating explanations, showing its effectiveness in\nthe resource-constrained medical domain. We will share our benchmark datasets\nand the trained model.", "published": "2024-06-10 14:47:04", "link": "http://arxiv.org/abs/2406.06331v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MASSW: A New Dataset and Benchmark Tasks for AI-Assisted Scientific\n  Workflows", "abstract": "Scientific innovation relies on detailed workflows, which include critical\nsteps such as analyzing literature, generating ideas, validating these ideas,\ninterpreting results, and inspiring follow-up research. However, scientific\npublications that document these workflows are extensive and unstructured. This\nmakes it difficult for both human researchers and AI systems to effectively\nnavigate and explore the space of scientific innovation. To address this issue,\nwe introduce MASSW, a comprehensive text dataset on Multi-Aspect Summarization\nof Scientific Workflows. MASSW includes more than 152,000 peer-reviewed\npublications from 17 leading computer science conferences spanning the past 50\nyears. Using Large Language Models (LLMs), we automatically extract five core\naspects from these publications -- context, key idea, method, outcome, and\nprojected impact -- which correspond to five key steps in the research\nworkflow. These structured summaries facilitate a variety of downstream tasks\nand analyses. The quality of the LLM-extracted summaries is validated by\ncomparing them with human annotations. We demonstrate the utility of MASSW\nthrough multiple novel machine-learning tasks that can be benchmarked using\nthis new dataset, which make various types of predictions and recommendations\nalong the scientific workflow. MASSW holds significant potential for\nresearchers to create and benchmark new AI methods for optimizing scientific\nworkflows and fostering scientific innovation in the field. Our dataset is\nopenly available at \\url{https://github.com/xingjian-zhang/massw}.", "published": "2024-06-10 15:19:09", "link": "http://arxiv.org/abs/2406.06357v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Lifelong Learning of Large Language Models: A Survey", "abstract": "As the applications of large language models (LLMs) expand across diverse\nfields, the ability of these models to adapt to ongoing changes in data, tasks,\nand user preferences becomes crucial. Traditional training methods, relying on\nstatic datasets, are increasingly inadequate for coping with the dynamic nature\nof real-world information. Lifelong learning, also known as continual or\nincremental learning, addresses this challenge by enabling LLMs to learn\ncontinuously and adaptively over their operational lifetime, integrating new\nknowledge while retaining previously learned information and preventing\ncatastrophic forgetting. This survey delves into the sophisticated landscape of\nlifelong learning, categorizing strategies into two primary groups: Internal\nKnowledge and External Knowledge. Internal Knowledge includes continual\npretraining and continual finetuning, each enhancing the adaptability of LLMs\nin various scenarios. External Knowledge encompasses retrieval-based and\ntool-based lifelong learning, leveraging external data sources and\ncomputational tools to extend the model's capabilities without modifying core\nparameters. The key contributions of our survey are: (1) Introducing a novel\ntaxonomy categorizing the extensive literature of lifelong learning into 12\nscenarios; (2) Identifying common techniques across all lifelong learning\nscenarios and classifying existing literature into various technique groups\nwithin each scenario; (3) Highlighting emerging techniques such as model\nexpansion and data selection, which were less explored in the pre-LLM era.\nThrough a detailed examination of these groups and their respective categories,\nthis survey aims to enhance the adaptability, reliability, and overall\nperformance of LLMs in real-world applications.", "published": "2024-06-10 15:46:25", "link": "http://arxiv.org/abs/2406.06391v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Should We Fine-Tune or RAG? Evaluating Different Techniques to Adapt\n  LLMs for Dialogue", "abstract": "We study the limitations of Large Language Models (LLMs) for the task of\nresponse generation in human-machine dialogue. Several techniques have been\nproposed in the literature for different dialogue types (e.g., Open-Domain).\nHowever, the evaluations of these techniques have been limited in terms of base\nLLMs, dialogue types and evaluation metrics. In this work, we extensively\nanalyze different LLM adaptation techniques when applied to different dialogue\ntypes. We have selected two base LLMs, Llama-2 and Mistral, and four dialogue\ntypes Open-Domain, Knowledge-Grounded, Task-Oriented, and Question Answering.\nWe evaluate the performance of in-context learning and fine-tuning techniques\nacross datasets selected for each dialogue type. We assess the impact of\nincorporating external knowledge to ground the generation in both scenarios of\nRetrieval-Augmented Generation (RAG) and gold knowledge. We adopt consistent\nevaluation and explainability criteria for automatic metrics and human\nevaluation protocols. Our analysis shows that there is no universal\nbest-technique for adapting large language models as the efficacy of each\ntechnique depends on both the base LLM and the specific type of dialogue. Last\nbut not least, the assessment of the best adaptation technique should include\nhuman evaluation to avoid false expectations and outcomes derived from\nautomatic metrics.", "published": "2024-06-10 15:52:49", "link": "http://arxiv.org/abs/2406.06399v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Language Models are Alignable Decision-Makers: Dataset and Application\n  to the Medical Triage Domain", "abstract": "In difficult decision-making scenarios, it is common to have conflicting\nopinions among expert human decision-makers as there may not be a single right\nanswer. Such decisions may be guided by different attributes that can be used\nto characterize an individual's decision. We introduce a novel dataset for\nmedical triage decision-making, labeled with a set of decision-maker attributes\n(DMAs). This dataset consists of 62 scenarios, covering six different DMAs,\nincluding ethical principles such as fairness and moral desert. We present a\nnovel software framework for human-aligned decision-making by utilizing these\nDMAs, paving the way for trustworthy AI with better guardrails. Specifically,\nwe demonstrate how large language models (LLMs) can serve as ethical\ndecision-makers, and how their decisions can be aligned to different DMAs using\nzero-shot prompting. Our experiments focus on different open-source models with\nvarying sizes and training techniques, such as Falcon, Mistral, and Llama 2.\nFinally, we also introduce a new form of weighted self-consistency that\nimproves the overall quantified performance. Our results provide new research\ndirections in the use of LLMs as alignable decision-makers. The dataset and\nopen-source software are publicly available at:\nhttps://github.com/ITM-Kitware/llm-alignable-dm.", "published": "2024-06-10 16:25:23", "link": "http://arxiv.org/abs/2406.06435v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Interpretability of Language Models via Task Spaces", "abstract": "The usual way to interpret language models (LMs) is to test their performance\non different benchmarks and subsequently infer their internal processes. In\nthis paper, we present an alternative approach, concentrating on the quality of\nLM processing, with a focus on their language abilities. To this end, we\nconstruct 'linguistic task spaces' -- representations of an LM's language\nconceptualisation -- that shed light on the connections LMs draw between\nlanguage phenomena. Task spaces are based on the interactions of the learning\nsignals from different linguistic phenomena, which we assess via a method we\ncall 'similarity probing'. To disentangle the learning signals of linguistic\nphenomena, we further introduce a method called 'fine-tuning via gradient\ndifferentials' (FTGD). We apply our methods to language models of three\ndifferent scales and find that larger models generalise better to overarching\ngeneral concepts for linguistic tasks, making better use of their shared\nstructure. Further, the distributedness of linguistic processing increases with\npre-training through increased parameter sharing between related linguistic\ntasks. The overall generalisation patterns are mostly stable throughout\ntraining and not marked by incisive stages, potentially explaining the lack of\nsuccessful curriculum strategies for LMs.", "published": "2024-06-10 16:34:30", "link": "http://arxiv.org/abs/2406.06441v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Large Language Model Pipeline for Breast Cancer Oncology", "abstract": "Large language models (LLMs) have demonstrated potential in the innovation of\nmany disciplines. However, how they can best be developed for oncology remains\nunderdeveloped. State-of-the-art OpenAI models were fine-tuned on a clinical\ndataset and clinical guidelines text corpus for two important cancer treatment\nfactors, adjuvant radiation therapy and chemotherapy, using a novel Langchain\nprompt engineering pipeline. A high accuracy (0.85+) was achieved in the\nclassification of adjuvant radiation therapy and chemotherapy for breast cancer\npatients. Furthermore, a confidence interval was formed from observational data\non the quality of treatment from human oncologists to estimate the proportion\nof scenarios in which the model must outperform the original oncologist in its\ntreatment prediction to be a better solution overall as 8.2% to 13.3%. Due to\nindeterminacy in the outcomes of cancer treatment decisions, future\ninvestigation, potentially a clinical trial, would be required to determine if\nthis threshold was met by the models. Nevertheless, with 85% of U.S. cancer\npatients receiving treatment at local community facilities, these kinds of\nmodels could play an important part in expanding access to quality care with\noutcomes that lie, at minimum, close to a human oncologist.", "published": "2024-06-10 16:44:48", "link": "http://arxiv.org/abs/2406.06455v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Evaluating the Retrieval Component in LLM-Based Question Answering\n  Systems", "abstract": "Question answering systems (QA) utilizing Large Language Models (LLMs)\nheavily depend on the retrieval component to provide them with domain-specific\ninformation and reduce the risk of generating inaccurate responses or\nhallucinations. Although the evaluation of retrievers dates back to the early\nresearch in Information Retrieval, assessing their performance within LLM-based\nchatbots remains a challenge.\n  This study proposes a straightforward baseline for evaluating retrievers in\nRetrieval-Augmented Generation (RAG)-based chatbots. Our findings demonstrate\nthat this evaluation framework provides a better image of how the retriever\nperforms and is more aligned with the overall performance of the QA system.\nAlthough conventional metrics such as precision, recall, and F1 score may not\nfully capture LLMs' capabilities - as they can yield accurate responses despite\nimperfect retrievers - our method considers LLMs' strengths to ignore\nirrelevant contexts, as well as potential errors and hallucinations in their\nresponses.", "published": "2024-06-10 16:46:22", "link": "http://arxiv.org/abs/2406.06458v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Transforming Wearable Data into Health Insights using Large Language\n  Model Agents", "abstract": "Despite the proliferation of wearable health trackers and the importance of\nsleep and exercise to health, deriving actionable personalized insights from\nwearable data remains a challenge because doing so requires non-trivial\nopen-ended analysis of these data. The recent rise of large language model\n(LLM) agents, which can use tools to reason about and interact with the world,\npresents a promising opportunity to enable such personalized analysis at scale.\nYet, the application of LLM agents in analyzing personal health is still\nlargely untapped. In this paper, we introduce the Personal Health Insights\nAgent (PHIA), an agent system that leverages state-of-the-art code generation\nand information retrieval tools to analyze and interpret behavioral health data\nfrom wearables. We curate two benchmark question-answering datasets of over\n4000 health insights questions. Based on 650 hours of human and expert\nevaluation we find that PHIA can accurately address over 84% of factual\nnumerical questions and more than 83% of crowd-sourced open-ended questions.\nThis work has implications for advancing behavioral health across the\npopulation, potentially enabling individuals to interpret their own wearable\ndata, and paving the way for a new era of accessible, personalized wellness\nregimens that are informed by data-driven insights.", "published": "2024-06-10 17:00:54", "link": "http://arxiv.org/abs/2406.06464v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Towards a Personal Health Large Language Model", "abstract": "In health, most large language model (LLM) research has focused on clinical\ntasks. However, mobile and wearable devices, which are rarely integrated into\nsuch tasks, provide rich, longitudinal data for personal health monitoring.\nHere we present Personal Health Large Language Model (PH-LLM), fine-tuned from\nGemini for understanding and reasoning over numerical time-series personal\nhealth data. We created and curated three datasets that test 1) production of\npersonalized insights and recommendations from sleep patterns, physical\nactivity, and physiological responses, 2) expert domain knowledge, and 3)\nprediction of self-reported sleep outcomes. For the first task we designed 857\ncase studies in collaboration with domain experts to assess real-world\nscenarios in sleep and fitness. Through comprehensive evaluation of\ndomain-specific rubrics, we observed that Gemini Ultra 1.0 and PH-LLM are not\nstatistically different from expert performance in fitness and, while experts\nremain superior for sleep, fine-tuning PH-LLM provided significant improvements\nin using relevant domain knowledge and personalizing information for sleep\ninsights. We evaluated PH-LLM domain knowledge using multiple choice sleep\nmedicine and fitness examinations. PH-LLM achieved 79% on sleep and 88% on\nfitness, exceeding average scores from a sample of human experts. Finally, we\ntrained PH-LLM to predict self-reported sleep quality outcomes from textual and\nmultimodal encoding representations of wearable data, and demonstrate that\nmultimodal encoding is required to match performance of specialized\ndiscriminative models. Although further development and evaluation are\nnecessary in the safety-critical personal health domain, these results\ndemonstrate both the broad knowledge and capabilities of Gemini models and the\nbenefit of contextualizing physiological data for personal health applications\nas done with PH-LLM.", "published": "2024-06-10 17:16:49", "link": "http://arxiv.org/abs/2406.06474v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Parallelizing Linear Transformers with the Delta Rule over Sequence\n  Length", "abstract": "Transformers with linear attention (i.e., linear transformers) and\nstate-space models have recently been suggested as a viable linear-time\nalternative to transformers with softmax attention. However, these models still\nunderperform transformers especially on tasks that require in-context\nretrieval. While more expressive variants of linear transformers which replace\nthe additive update in linear transformers with the delta rule (DeltaNet) have\nbeen found to be more effective at associative recall, existing algorithms for\ntraining such models do not parallelize over sequence length and are thus\ninefficient to train on modern hardware. This work describes a\nhardware-efficient algorithm for training linear transformers with the delta\nrule, which exploits a memory-efficient representation for computing products\nof Householder matrices. This algorithm allows us to scale up DeltaNet to\nstandard language modeling settings. We train a 1.3B model for 100B tokens and\nfind that it outperforms recent linear-time baselines such as Mamba and GLA in\nterms of perplexity and zero-shot performance on downstream tasks. We also\nexperiment with two hybrid models which combine DeltaNet layers with (1)\nsliding-window attention layers every other layer or (2) two global attention\nlayers, and find that these hybrids outperform strong transformer baselines.", "published": "2024-06-10 17:24:42", "link": "http://arxiv.org/abs/2406.06484v6", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Can Language Models Serve as Text-Based World Simulators?", "abstract": "Virtual environments play a key role in benchmarking advances in complex\nplanning and decision-making tasks but are expensive and complicated to build\nby hand. Can current language models themselves serve as world simulators,\ncorrectly predicting how actions change different world states, thus bypassing\nthe need for extensive manual coding? Our goal is to answer this question in\nthe context of text-based simulators. Our approach is to build and use a new\nbenchmark, called ByteSized32-State-Prediction, containing a dataset of text\ngame state transitions and accompanying game tasks. We use this to directly\nquantify, for the first time, how well LLMs can serve as text-based world\nsimulators. We test GPT-4 on this dataset and find that, despite its impressive\nperformance, it is still an unreliable world simulator without further\ninnovations. This work thus contributes both new insights into current LLM's\ncapabilities and weaknesses, as well as a novel benchmark to track future\nprogress as new models appear.", "published": "2024-06-10 17:24:44", "link": "http://arxiv.org/abs/2406.06485v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "In-Context Learning and Fine-Tuning GPT for Argument Mining", "abstract": "Large Language Models (LLMs) have become ubiquitous in NLP and deep learning.\nIn-Context Learning (ICL) has been suggested as a bridging paradigm between the\ntraining-free and fine-tuning LLMs settings. In ICL, an LLM is conditioned to\nsolve tasks by means of a few solved demonstration examples included as prompt.\nArgument Mining (AM) aims to extract the complex argumentative structure of a\ntext, and Argument Type Classification (ATC) is an essential sub-task of AM. We\nintroduce an ICL strategy for ATC combining kNN-based examples selection and\nmajority vote ensembling. In the training-free ICL setting, we show that GPT-4\nis able to leverage relevant information from only a few demonstration examples\nand achieve very competitive classification accuracy on ATC. We further set up\na fine-tuning strategy incorporating well-crafted structural features given\ndirectly in textual form. In this setting, GPT-3.5 achieves state-of-the-art\nperformance on ATC. Overall, these results emphasize the emergent ability of\nLLMs to grasp global discursive flow in raw text in both off-the-shelf and\nfine-tuned setups.", "published": "2024-06-10 18:01:55", "link": "http://arxiv.org/abs/2406.06699v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Leveraging Large Language Models for Knowledge-free Weak Supervision in\n  Clinical Natural Language Processing", "abstract": "The performance of deep learning-based natural language processing systems is\nbased on large amounts of labeled training data which, in the clinical domain,\nare not easily available or affordable. Weak supervision and in-context\nlearning offer partial solutions to this issue, particularly using large\nlanguage models (LLMs), but their performance still trails traditional\nsupervised methods with moderate amounts of gold-standard data. In particular,\ninferencing with LLMs is computationally heavy. We propose an approach\nleveraging fine-tuning LLMs and weak supervision with virtually no domain\nknowledge that still achieves consistently dominant performance. Using a\nprompt-based approach, the LLM is used to generate weakly-labeled data for\ntraining a downstream BERT model. The weakly supervised model is then further\nfine-tuned on small amounts of gold standard data. We evaluate this approach\nusing Llama2 on three different n2c2 datasets. With no more than 10 gold\nstandard notes, our final BERT models weakly supervised by fine-tuned\nLlama2-13B consistently outperformed out-of-the-box PubMedBERT by 4.7% to 47.9%\nin F1 scores. With only 50 gold standard notes, our models achieved close\nperformance to fully fine-tuned systems.", "published": "2024-06-10 18:34:48", "link": "http://arxiv.org/abs/2406.06723v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Raccoon: Prompt Extraction Benchmark of LLM-Integrated Applications", "abstract": "With the proliferation of LLM-integrated applications such as GPT-s, millions\nare deployed, offering valuable services through proprietary instruction\nprompts. These systems, however, are prone to prompt extraction attacks through\nmeticulously designed queries. To help mitigate this problem, we introduce the\nRaccoon benchmark which comprehensively evaluates a model's susceptibility to\nprompt extraction attacks. Our novel evaluation method assesses models under\nboth defenseless and defended scenarios, employing a dual approach to evaluate\nthe effectiveness of existing defenses and the resilience of the models. The\nbenchmark encompasses 14 categories of prompt extraction attacks, with\nadditional compounded attacks that closely mimic the strategies of potential\nattackers, alongside a diverse collection of defense templates. This array is,\nto our knowledge, the most extensive compilation of prompt theft attacks and\ndefense mechanisms to date. Our findings highlight universal susceptibility to\nprompt theft in the absence of defenses, with OpenAI models demonstrating\nnotable resilience when protected. This paper aims to establish a more\nsystematic benchmark for assessing LLM robustness against prompt extraction\nattacks, offering insights into their causes and potential countermeasures.\nResources of Raccoon are publicly available at\nhttps://github.com/M0gician/RaccoonBench.", "published": "2024-06-10 18:57:22", "link": "http://arxiv.org/abs/2406.06737v2", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "DISCOVERYWORLD: A Virtual Environment for Developing and Evaluating\n  Automated Scientific Discovery Agents", "abstract": "Automated scientific discovery promises to accelerate progress across\nscientific domains. However, developing and evaluating an AI agent's capacity\nfor end-to-end scientific reasoning is challenging as running real-world\nexperiments is often prohibitively expensive or infeasible. In this work we\nintroduce DISCOVERYWORLD, the first virtual environment for developing and\nbenchmarking an agent's ability to perform complete cycles of novel scientific\ndiscovery. DISCOVERYWORLD contains a variety of different challenges, covering\ntopics as diverse as radioisotope dating, rocket science, and proteomics, to\nencourage development of general discovery skills rather than task-specific\nsolutions. DISCOVERYWORLD itself is an inexpensive, simulated, text-based\nenvironment (with optional 2D visual overlay). It includes 120 different\nchallenge tasks, spanning eight topics each with three levels of difficulty and\nseveral parametric variations. Each task requires an agent to form hypotheses,\ndesign and run experiments, analyze results, and act on conclusions.\nDISCOVERYWORLD further provides three automatic metrics for evaluating\nperformance, based on (a) task completion, (b) task-relevant actions taken, and\n(c) the discovered explanatory knowledge. We find that strong baseline agents,\nthat perform well in prior published environments, struggle on most\nDISCOVERYWORLD tasks, suggesting that DISCOVERYWORLD captures some of the novel\nchallenges of discovery, and thus that DISCOVERYWORLD may help accelerate\nnear-term development and assessment of scientific discovery competency in\nagents. Code available at: www.github.com/allenai/discoveryworld", "published": "2024-06-10 20:08:44", "link": "http://arxiv.org/abs/2406.06769v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Evaluating Zero-Shot Long-Context LLM Compression", "abstract": "This study evaluates the effectiveness of zero-shot compression techniques on\nlarge language models (LLMs) under long-context. We identify the tendency for\ncomputational errors to increase under long-context when employing certain\ncompression methods. We propose a hypothesis to explain the varied behavior of\ndifferent LLM compression techniques and explore remedies to mitigate the\nperformance decline observed in some techniques under long-context. This is a\ncourse report for COS 598D Machine Learning and Systems by Prof. Kai Li at\nPrinceton University. Due to limited computational resources, our experiments\nwere conducted only on LLaMA-2-7B-32K.", "published": "2024-06-10 20:19:55", "link": "http://arxiv.org/abs/2406.06773v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLM-dCache: Improving Tool-Augmented LLMs with GPT-Driven Localized Data\n  Caching", "abstract": "As Large Language Models (LLMs) broaden their capabilities to manage\nthousands of API calls, they are confronted with complex data operations across\nvast datasets with significant overhead to the underlying system. In this work,\nwe introduce LLM-dCache to optimize data accesses by treating cache operations\nas callable API functions exposed to the tool-augmented agent. We grant LLMs\nthe autonomy to manage cache decisions via prompting, seamlessly integrating\nwith existing function-calling mechanisms. Tested on an industry-scale\nmassively parallel platform that spans hundreds of GPT endpoints and terabytes\nof imagery, our method improves Copilot times by an average of 1.24x across\nvarious LLMs and prompting techniques.", "published": "2024-06-10 21:08:39", "link": "http://arxiv.org/abs/2406.06799v2", "categories": ["cs.DC", "cs.CL"], "primary_category": "cs.DC"}
{"title": "Silent Signals, Loud Impact: LLMs for Word-Sense Disambiguation of Coded\n  Dog Whistles", "abstract": "A dog whistle is a form of coded communication that carries a secondary\nmeaning to specific audiences and is often weaponized for racial and\nsocioeconomic discrimination. Dog whistling historically originated from United\nStates politics, but in recent years has taken root in social media as a means\nof evading hate speech detection systems and maintaining plausible deniability.\nIn this paper, we present an approach for word-sense disambiguation of dog\nwhistles from standard speech using Large Language Models (LLMs), and leverage\nthis technique to create a dataset of 16,550 high-confidence coded examples of\ndog whistles used in formal and informal communication. Silent Signals is the\nlargest dataset of disambiguated dog whistle usage, created for applications in\nhate speech detection, neology, and political science.\n  The dataset can be found at\nhttps://huggingface.co/datasets/SALT-NLP/silent_signals.", "published": "2024-06-10 23:09:19", "link": "http://arxiv.org/abs/2406.06840v2", "categories": ["cs.CL", "cs.LG", "J.4; K.4.1; K.4.2"], "primary_category": "cs.CL"}
{"title": "SciRIFF: A Resource to Enhance Language Model Instruction-Following over\n  Scientific Literature", "abstract": "We present SciRIFF (Scientific Resource for Instruction-Following and\nFinetuning), a dataset of 137K instruction-following demonstrations for 54\ntasks covering five essential scientific literature understanding capabilities:\ninformation extraction, summarization, question answering, claim verification,\nand classification. SciRIFF demonstrations are notable for their long input\ncontexts, detailed task specifications, and complex structured outputs. While\ninstruction-following resources are available in specific domains such as\nclinical medicine and chemistry, SciRIFF is the first dataset focused on\nextracting and synthesizing information from research literature across a wide\nrange of scientific fields. To demonstrate the utility of SciRIFF, we develop a\nsample-efficient strategy to adapt a general instruction-following model for\nscience by performing additional finetuning on a mix of general-domain and\nSciRIFF demonstrations. In evaluations on nine held-out scientific tasks, our\nmodel -- called SciTulu -- improves over a strong LLM baseline by 28.1% and\n6.5% at the 7B and 70B scales respectively, while maintaining general\ninstruction-following performance within 2% of the baseline. We are optimistic\nthat SciRIFF will facilitate the development and evaluation of LLMs to help\nresearchers navigate the ever-growing body of scientific literature. We release\nour dataset, model checkpoints, and data processing and evaluation code to\nenable further research.", "published": "2024-06-10 21:22:08", "link": "http://arxiv.org/abs/2406.07835v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "WarCov -- Large multilabel and multimodal dataset from social platform", "abstract": "In the classification tasks, from raw data acquisition to the curation of a\ndataset suitable for use in evaluating machine learning models, a series of\nsteps - often associated with high costs - are necessary. In the case of\nNatural Language Processing, initial cleaning and conversion can be performed\nautomatically, but obtaining labels still requires the rationalized input of\nhuman experts. As a result, even though many articles often state that \"the\nworld is filled with data\", data scientists suffer from its shortage. It is\ncrucial in the case of natural language applications, which is constantly\nevolving and must adapt to new concepts or events. For example, the topic of\nthe COVID-19 pandemic and the vocabulary related to it would have been mostly\nunrecognizable before 2019. For this reason, creating new datasets, also in\nlanguages other than English, is still essential. This work presents a\ncollection of 3~187~105 posts in Polish about the pandemic and the war in\nUkraine published on popular social media platforms in 2022. The collection\nincludes not only preprocessed texts but also images so it can be used also for\nmultimodal recognition tasks. The labels define posts' topics and were created\nusing hashtags accompanying the posts. The work presents the process of\ncurating a dataset from acquisition to sample pattern recognition experiments.", "published": "2024-06-10 14:36:44", "link": "http://arxiv.org/abs/2406.10255v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Optimal synthesis embeddings", "abstract": "In this paper we introduce a word embedding composition method based on the\nintuitive idea that a fair embedding representation for a given set of words\nshould satisfy that the new vector will be at the same distance of the vector\nrepresentation of each of its constituents, and this distance should be\nminimized. The embedding composition method can work with static and\ncontextualized word representations, it can be applied to create\nrepresentations of sentences and learn also representations of sets of words\nthat are not necessarily organized as a sequence. We theoretically characterize\nthe conditions for the existence of this type of representation and derive the\nsolution. We evaluate the method in data augmentation and sentence\nclassification tasks, investigating several design choices of embeddings and\ncomposition methods. We show that our approach excels in solving probing tasks\ndesigned to capture simple linguistic features of sentences.", "published": "2024-06-10 18:06:33", "link": "http://arxiv.org/abs/2406.10259v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CVQA: Culturally-diverse Multilingual Visual Question Answering\n  Benchmark", "abstract": "Visual Question Answering (VQA) is an important task in multimodal AI, and it\nis often used to test the ability of vision-language models to understand and\nreason on knowledge present in both visual and textual data. However, most of\nthe current VQA models use datasets that are primarily focused on English and a\nfew major world languages, with images that are typically Western-centric.\nWhile recent efforts have tried to increase the number of languages covered on\nVQA datasets, they still lack diversity in low-resource languages. More\nimportantly, although these datasets often extend their linguistic range via\ntranslation or some other approaches, they usually keep images the same,\nresulting in narrow cultural representation. To address these limitations, we\nconstruct CVQA, a new Culturally-diverse multilingual Visual Question Answering\nbenchmark, designed to cover a rich set of languages and cultures, where we\nengage native speakers and cultural experts in the data collection process. As\na result, CVQA includes culturally-driven images and questions from across 30\ncountries on four continents, covering 31 languages with 13 scripts, providing\na total of 10k questions. We then benchmark several Multimodal Large Language\nModels (MLLMs) on CVQA, and show that the dataset is challenging for the\ncurrent state-of-the-art models. This benchmark can serve as a probing\nevaluation suite for assessing the cultural capability and bias of multimodal\nmodels and hopefully encourage more research efforts toward increasing cultural\nawareness and linguistic diversity in this field.", "published": "2024-06-10 01:59:00", "link": "http://arxiv.org/abs/2406.05967v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "ShiftAddLLM: Accelerating Pretrained LLMs via Post-Training\n  Multiplication-Less Reparameterization", "abstract": "Large language models (LLMs) have shown impressive performance on language\ntasks but face challenges when deployed on resource-constrained devices due to\ntheir extensive parameters and reliance on dense multiplications, resulting in\nhigh memory demands and latency bottlenecks. Shift-and-add reparameterization\noffers a promising solution by replacing costly multiplications with\nhardware-friendly primitives in both the attention and multi-layer perceptron\n(MLP) layers of an LLM. However, current reparameterization techniques require\ntraining from scratch or full parameter fine-tuning to restore accuracy, which\nis resource-intensive for LLMs. To address this, we propose accelerating\npretrained LLMs through post-training shift-and-add reparameterization,\ncreating efficient multiplication-free models, dubbed ShiftAddLLM.\nSpecifically, we quantize each weight matrix into binary matrices paired with\ngroup-wise scaling factors. The associated multiplications are reparameterized\ninto (1) shifts between activations and scaling factors and (2) queries and\nadds according to the binary matrices. To reduce accuracy loss, we present a\nmulti-objective optimization method to minimize both weight and output\nactivation reparameterization errors. Additionally, based on varying\nsensitivity across layers to reparameterization, we develop an automated bit\nallocation strategy to further reduce memory usage and latency. Experiments on\nfive LLM families and eight tasks consistently validate the effectiveness of\nShiftAddLLM, achieving average perplexity improvements of 5.6 and 22.7 points\nat comparable or lower latency compared to the most competitive quantized LLMs\nat 3 and 2 bits, respectively, and more than 80% memory and energy reductions\nover the original LLMs. Codes and models are available at\nhttps://github.com/GATECH-EIC/ShiftAddLLM.", "published": "2024-06-10 02:47:55", "link": "http://arxiv.org/abs/2406.05981v4", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A Dual-View Approach to Classifying Radiology Reports by Co-Training", "abstract": "Radiology report analysis provides valuable information that can aid with\npublic health initiatives, and has been attracting increasing attention from\nthe research community. In this work, we present a novel insight that the\nstructure of a radiology report (namely, the Findings and Impression sections)\noffers different views of a radiology scan. Based on this intuition, we further\npropose a co-training approach, where two machine learning models are built\nupon the Findings and Impression sections, respectively, and use each other's\ninformation to boost performance with massive unlabeled data in a\nsemi-supervised manner. We conducted experiments in a public health\nsurveillance study, and results show that our co-training approach is able to\nimprove performance using the dual views and surpass competing supervised and\nsemi-supervised methods.", "published": "2024-06-10 03:29:23", "link": "http://arxiv.org/abs/2406.05995v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FLEUR: An Explainable Reference-Free Evaluation Metric for Image\n  Captioning Using a Large Multimodal Model", "abstract": "Most existing image captioning evaluation metrics focus on assigning a single\nnumerical score to a caption by comparing it with reference captions. However,\nthese methods do not provide an explanation for the assigned score. Moreover,\nreference captions are expensive to acquire. In this paper, we propose FLEUR,\nan explainable reference-free metric to introduce explainability into image\ncaptioning evaluation metrics. By leveraging a large multimodal model, FLEUR\ncan evaluate the caption against the image without the need for reference\ncaptions, and provide the explanation for the assigned score. We introduce\nscore smoothing to align as closely as possible with human judgment and to be\nrobust to user-defined grading criteria. FLEUR achieves high correlations with\nhuman judgment across various image captioning evaluation benchmarks and\nreaches state-of-the-art results on Flickr8k-CF, COMPOSITE, and Pascal-50S\nwithin the domain of reference-free evaluation metrics. Our source code and\nresults are publicly available at: https://github.com/Yebin46/FLEUR.", "published": "2024-06-10 03:57:39", "link": "http://arxiv.org/abs/2406.06004v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "CARES: A Comprehensive Benchmark of Trustworthiness in Medical Vision\n  Language Models", "abstract": "Artificial intelligence has significantly impacted medical applications,\nparticularly with the advent of Medical Large Vision Language Models\n(Med-LVLMs), sparking optimism for the future of automated and personalized\nhealthcare. However, the trustworthiness of Med-LVLMs remains unverified,\nposing significant risks for future model deployment. In this paper, we\nintroduce CARES and aim to comprehensively evaluate the Trustworthiness of\nMed-LVLMs across the medical domain. We assess the trustworthiness of Med-LVLMs\nacross five dimensions, including trustfulness, fairness, safety, privacy, and\nrobustness. CARES comprises about 41K question-answer pairs in both closed and\nopen-ended formats, covering 16 medical image modalities and 27 anatomical\nregions. Our analysis reveals that the models consistently exhibit concerns\nregarding trustworthiness, often displaying factual inaccuracies and failing to\nmaintain fairness across different demographic groups. Furthermore, they are\nvulnerable to attacks and demonstrate a lack of privacy awareness. We publicly\nrelease our benchmark and code in https://cares-ai.github.io/.", "published": "2024-06-10 04:07:09", "link": "http://arxiv.org/abs/2406.06007v3", "categories": ["cs.LG", "cs.CL", "cs.CV", "cs.CY"], "primary_category": "cs.LG"}
{"title": "RepoQA: Evaluating Long Context Code Understanding", "abstract": "Recent advances have been improving the context windows of Large Language\nModels (LLMs). To quantify the real long-context capabilities of LLMs,\nevaluators such as the popular Needle in a Haystack have been developed to test\nLLMs over a large chunk of raw texts. While effective, current evaluations\noverlook the insight of how LLMs work with long-context code, i.e.,\nrepositories. To this end, we initiate the RepoQA benchmark to evaluate LLMs on\nlong-context code understanding. Traditional needle testers ask LLMs to\ndirectly retrieve the answer from the context without necessary deep\nunderstanding. In RepoQA, we built our initial task, namely Searching Needle\nFunction (SNF), which exercises LLMs to search functions given their\nnatural-language description, i.e., LLMs cannot find the desired function if\nthey cannot understand the description and code. RepoQA is multilingual and\ncomprehensive: it includes 500 code search tasks gathered from 50 popular\nrepositories across 5 modern programming languages. By evaluating 26 general\nand code-specific LLMs on RepoQA, we show (i) there is still a small gap\nbetween the best open and proprietary models; (ii) different models are good at\ndifferent languages; and (iii) models may understand code better without\ncomments.", "published": "2024-06-10 05:15:30", "link": "http://arxiv.org/abs/2406.06025v1", "categories": ["cs.SE", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "StreamAtt: Direct Streaming Speech-to-Text Translation with\n  Attention-based Audio History Selection", "abstract": "Streaming speech-to-text translation (StreamST) is the task of automatically\ntranslating speech while incrementally receiving an audio stream. Unlike\nsimultaneous ST (SimulST), which deals with pre-segmented speech, StreamST\nfaces the challenges of handling continuous and unbounded audio streams. This\nrequires additional decisions about what to retain of the previous history,\nwhich is impractical to keep entirely due to latency and computational\nconstraints. Despite the real-world demand for real-time ST, research on\nstreaming translation remains limited, with existing works solely focusing on\nSimulST. To fill this gap, we introduce StreamAtt, the first StreamST policy,\nand propose StreamLAAL, the first StreamST latency metric designed to be\ncomparable with existing metrics for SimulST. Extensive experiments across all\n8 languages of MuST-C v1.0 show the effectiveness of StreamAtt compared to a\nnaive streaming baseline and the related state-of-the-art SimulST policy,\nproviding a first step in StreamST research.", "published": "2024-06-10 08:27:58", "link": "http://arxiv.org/abs/2406.06097v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Thunder : Unified Regression-Diffusion Speech Enhancement with a Single\n  Reverse Step using Brownian Bridge", "abstract": "Diffusion-based speech enhancement has shown promising results, but can\nsuffer from a slower inference time. Initializing the diffusion process with\nthe enhanced audio generated by a regression-based model can be used to reduce\nthe computational steps required. However, these approaches often necessitate a\nregression model, further increasing the system's complexity. We propose\nThunder, a unified regression-diffusion model that utilizes the Brownian bridge\nprocess which can allow the model to act in both modes. The regression mode can\nbe accessed by setting the diffusion time step closed to 1. However, the\nstandard score-based diffusion modeling does not perform well in this setup due\nto gradient instability. To mitigate this problem, we modify the diffusion\nmodel to predict the clean speech instead of the score function, achieving\ncompetitive performance with a more compact model size and fewer reverse steps.", "published": "2024-06-10 09:52:25", "link": "http://arxiv.org/abs/2406.06139v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Tx-LLM: A Large Language Model for Therapeutics", "abstract": "Developing therapeutics is a lengthy and expensive process that requires the\nsatisfaction of many different criteria, and AI models capable of expediting\nthe process would be invaluable. However, the majority of current AI approaches\naddress only a narrowly defined set of tasks, often circumscribed within a\nparticular domain. To bridge this gap, we introduce Tx-LLM, a generalist large\nlanguage model (LLM) fine-tuned from PaLM-2 which encodes knowledge about\ndiverse therapeutic modalities. Tx-LLM is trained using a collection of 709\ndatasets that target 66 tasks spanning various stages of the drug discovery\npipeline. Using a single set of weights, Tx-LLM simultaneously processes a wide\nvariety of chemical or biological entities(small molecules, proteins, nucleic\nacids, cell lines, diseases) interleaved with free-text, allowing it to predict\na broad range of associated properties, achieving competitive with\nstate-of-the-art (SOTA) performance on 43 out of 66 tasks and exceeding SOTA on\n22. Among these, Tx-LLM is particularly powerful and exceeds best-in-class\nperformance on average for tasks combining molecular SMILES representations\nwith text such as cell line names or disease names, likely due to context\nlearned during pretraining. We observe evidence of positive transfer between\ntasks with diverse drug types (e.g.,tasks involving small molecules and tasks\ninvolving proteins), and we study the impact of model size, domain finetuning,\nand prompting strategies on performance. We believe Tx-LLM represents an\nimportant step towards LLMs encoding biochemical knowledge and could have a\nfuture role as an end-to-end tool across the drug discovery development\npipeline.", "published": "2024-06-10 14:33:02", "link": "http://arxiv.org/abs/2406.06316v1", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.LG"], "primary_category": "cs.CL"}
{"title": "mHuBERT-147: A Compact Multilingual HuBERT Model", "abstract": "We present mHuBERT-147, the first general-purpose massively multilingual\nHuBERT speech representation model trained on 90K hours of clean, open-license\ndata. To scale up the multi-iteration HuBERT approach, we use faiss-based\nclustering, achieving 5.2x faster label assignment than the original method. We\nalso apply a new multilingual batching up-sampling strategy, leveraging both\nlanguage and dataset diversity. After 3 training iterations, our compact 95M\nparameter mHuBERT-147 outperforms larger models trained on substantially more\ndata. We rank second and first on the ML-SUPERB 10min and 1h leaderboards, with\nSOTA scores for 3 tasks. Across ASR/LID tasks, our model consistently surpasses\nXLS-R (300M params; 436K hours) and demonstrates strong competitiveness against\nthe much larger MMS (1B params; 491K hours). Our findings indicate that\nmHuBERT-147 is a promising model for multilingual speech tasks, offering an\nunprecedented balance between high performance and parameter efficiency.", "published": "2024-06-10 15:32:42", "link": "http://arxiv.org/abs/2406.06371v5", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Diffusion-RPO: Aligning Diffusion Models through Relative Preference\n  Optimization", "abstract": "Aligning large language models with human preferences has emerged as a\ncritical focus in language modeling research. Yet, integrating preference\nlearning into Text-to-Image (T2I) generative models is still relatively\nuncharted territory. The Diffusion-DPO technique made initial strides by\nemploying pairwise preference learning in diffusion models tailored for\nspecific text prompts. We introduce Diffusion-RPO, a new method designed to\nalign diffusion-based T2I models with human preferences more effectively. This\napproach leverages both prompt-image pairs with identical prompts and those\nwith semantically related content across various modalities. Furthermore, we\nhave developed a new evaluation metric, style alignment, aimed at overcoming\nthe challenges of high costs, low reproducibility, and limited interpretability\nprevalent in current evaluations of human preference alignment. Our findings\ndemonstrate that Diffusion-RPO outperforms established methods such as\nSupervised Fine-Tuning and Diffusion-DPO in tuning Stable Diffusion versions\n1.5 and XL-1.0, achieving superior results in both automated evaluations of\nhuman preferences and style alignment. Our code is available at\nhttps://github.com/yigu1008/Diffusion-RPO", "published": "2024-06-10 15:42:03", "link": "http://arxiv.org/abs/2406.06382v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Low-Rank Quantization-Aware Training for LLMs", "abstract": "Large language models (LLMs) are omnipresent, however their practical\ndeployment is challenging due to their ever increasing computational and memory\ndemands. Quantization is one of the most effective ways to make them more\ncompute and memory efficient. Quantization-aware training (QAT) methods,\ngenerally produce the best quantized performance, however it comes at the cost\nof potentially long training time and excessive memory usage, making it\nimpractical when applying for LLMs. Inspired by parameter-efficient fine-tuning\n(PEFT) and low-rank adaptation (LoRA) literature, we propose LR-QAT -- a\nlightweight and memory-efficient QAT algorithm for LLMs. LR-QAT employs several\ncomponents to save memory without sacrificing predictive performance: (a)\nlow-rank auxiliary weights that are aware of the quantization grid; (b) a\ndowncasting operator using fixed-point or double-packed integers and (c)\ncheckpointing. Unlike most related work, our method (i) is inference-efficient,\nleading to no additional overhead compared to traditional PTQ; (ii) can be seen\nas a general extended pretraining framework, meaning that the resulting model\ncan still be utilized for any downstream task afterwards; (iii) can be applied\nacross a wide range of quantization settings, such as different choices\nquantization granularity, activation quantization, and seamlessly combined with\nmany PTQ techniques. We apply LR-QAT to LLaMA-1/2/3 and Mistral model families\nand validate its effectiveness on several downstream tasks. Our method\noutperforms common post-training quantization (PTQ) approaches and reaches the\nsame model performance as full-model QAT at the fraction of its memory usage.\nSpecifically, we can train a 7B LLM on a single consumer grade GPU with 24GB of\nmemory. Our source code is available at\nhttps://github.com/qualcomm-ai-research/LR-QAT", "published": "2024-06-10 15:44:22", "link": "http://arxiv.org/abs/2406.06385v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "STimage-1K4M: A histopathology image-gene expression dataset for spatial\n  transcriptomics", "abstract": "Recent advances in multi-modal algorithms have driven and been driven by the\nincreasing availability of large image-text datasets, leading to significant\nstrides in various fields, including computational pathology. However, in most\nexisting medical image-text datasets, the text typically provides high-level\nsummaries that may not sufficiently describe sub-tile regions within a large\npathology image. For example, an image might cover an extensive tissue area\ncontaining cancerous and healthy regions, but the accompanying text might only\nspecify that this image is a cancer slide, lacking the nuanced details needed\nfor in-depth analysis. In this study, we introduce STimage-1K4M, a novel\ndataset designed to bridge this gap by providing genomic features for sub-tile\nimages. STimage-1K4M contains 1,149 images derived from spatial transcriptomics\ndata, which captures gene expression information at the level of individual\nspatial spots within a pathology image. Specifically, each image in the dataset\nis broken down into smaller sub-image tiles, with each tile paired with\n15,000-30,000 dimensional gene expressions. With 4,293,195 pairs of sub-tile\nimages and gene expressions, STimage-1K4M offers unprecedented granularity,\npaving the way for a wide range of advanced research in multi-modal data\nanalysis an innovative applications in computational pathology, and beyond.", "published": "2024-06-10 15:48:07", "link": "http://arxiv.org/abs/2406.06393v2", "categories": ["cs.CV", "cs.CL", "q-bio.GN", "I.4.10; I.2.10"], "primary_category": "cs.CV"}
{"title": "Meta Learning Text-to-Speech Synthesis in over 7000 Languages", "abstract": "In this work, we take on the challenging task of building a single\ntext-to-speech synthesis system that is capable of generating speech in over\n7000 languages, many of which lack sufficient data for traditional TTS\ndevelopment. By leveraging a novel integration of massively multilingual\npretraining and meta learning to approximate language representations, our\napproach enables zero-shot speech synthesis in languages without any available\ndata. We validate our system's performance through objective measures and human\nevaluation across a diverse linguistic landscape. By releasing our code and\nmodels publicly, we aim to empower communities with limited linguistic\nresources and foster further innovation in the field of speech technology.", "published": "2024-06-10 15:56:52", "link": "http://arxiv.org/abs/2406.06403v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Controlling Emotion in Text-to-Speech with Natural Language Prompts", "abstract": "In recent years, prompting has quickly become one of the standard ways of\nsteering the outputs of generative machine learning models, due to its\nintuitive use of natural language. In this work, we propose a system\nconditioned on embeddings derived from an emotionally rich text that serves as\nprompt. Thereby, a joint representation of speaker and prompt embeddings is\nintegrated at several points within a transformer-based architecture. Our\napproach is trained on merged emotional speech and text datasets and varies\nprompts in each training iteration to increase the generalization capabilities\nof the model. Objective and subjective evaluation results demonstrate the\nability of the conditioned synthesis system to accurately transfer the emotions\npresent in a prompt to speech. At the same time, precise tractability of\nspeaker identities as well as overall high speech quality and intelligibility\nare maintained.", "published": "2024-06-10 15:58:42", "link": "http://arxiv.org/abs/2406.06406v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "LLM Dataset Inference: Did you train on my dataset?", "abstract": "The proliferation of large language models (LLMs) in the real world has come\nwith a rise in copyright cases against companies for training their models on\nunlicensed data from the internet. Recent works have presented methods to\nidentify if individual text sequences were members of the model's training\ndata, known as membership inference attacks (MIAs). We demonstrate that the\napparent success of these MIAs is confounded by selecting non-members (text\nsequences not used for training) belonging to a different distribution from the\nmembers (e.g., temporally shifted recent Wikipedia articles compared with ones\nused to train the model). This distribution shift makes membership inference\nappear successful. However, most MIA methods perform no better than random\nguessing when discriminating between members and non-members from the same\ndistribution (e.g., in this case, the same period of time). Even when MIAs\nwork, we find that different MIAs succeed at inferring membership of samples\nfrom different distributions. Instead, we propose a new dataset inference\nmethod to accurately identify the datasets used to train large language models.\nThis paradigm sits realistically in the modern-day copyright landscape, where\nauthors claim that an LLM is trained over multiple documents (such as a book)\nwritten by them, rather than one particular paragraph. While dataset inference\nshares many of the challenges of membership inference, we solve it by\nselectively combining the MIAs that provide positive signal for a given\ndistribution, and aggregating them to perform a statistical test on a given\ndataset. Our approach successfully distinguishes the train and test sets of\ndifferent subsets of the Pile with statistically significant p-values < 0.1,\nwithout any false positives.", "published": "2024-06-10 16:34:43", "link": "http://arxiv.org/abs/2406.06443v1", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Husky: A Unified, Open-Source Language Agent for Multi-Step Reasoning", "abstract": "Language agents perform complex tasks by using tools to execute each step\nprecisely. However, most existing agents are based on proprietary models or\ndesigned to target specific tasks, such as mathematics or multi-hop question\nanswering. We introduce Husky, a holistic, open-source language agent that\nlearns to reason over a unified action space to address a diverse set of\ncomplex tasks involving numerical, tabular, and knowledge-based reasoning.\nHusky iterates between two stages: 1) generating the next action to take\ntowards solving a given task and 2) executing the action using expert models\nand updating the current solution state. We identify a thorough ontology of\nactions for addressing complex tasks and curate high-quality data to train\nexpert models for executing these actions. Our experiments show that Husky\noutperforms prior language agents across 14 evaluation datasets. Moreover, we\nintroduce HuskyQA, a new evaluation set which stress tests language agents for\nmixed-tool reasoning, with a focus on retrieving missing knowledge and\nperforming numerical reasoning. Despite using 7B models, Husky matches or even\nexceeds frontier LMs such as GPT-4 on these tasks, showcasing the efficacy of\nour holistic approach in addressing complex reasoning problems. Our code and\nmodels are available at https://github.com/agent-husky/Husky-v1.", "published": "2024-06-10 17:07:25", "link": "http://arxiv.org/abs/2406.06469v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Direct Preference Optimization for Suppressing Hallucinated Prior Exams\n  in Radiology Report Generation", "abstract": "Recent advances in generative vision-language models (VLMs) have exciting\npotential implications for AI in radiology, yet VLMs are also known to produce\nhallucinations, nonsensical text, and other unwanted behaviors that can waste\nclinicians' time and cause patient harm. Drawing on recent work on direct\npreference optimization (DPO), we propose a simple method for modifying the\nbehavior of pretrained VLMs performing radiology report generation by\nsuppressing unwanted types of generations. We apply our method to the\nprevention of hallucinations of prior exams, addressing a long-established\nproblem behavior in models performing chest X-ray report generation. Across our\nexperiments, we find that DPO fine-tuning achieves a 3.2-4.8x reduction in\nlines hallucinating prior exams while maintaining model performance on clinical\naccuracy metrics. Our work is, to the best of our knowledge, the first work to\napply DPO to medical VLMs, providing a data- and compute- efficient way to\nsuppress problem behaviors while maintaining overall clinical accuracy.", "published": "2024-06-10 17:31:36", "link": "http://arxiv.org/abs/2406.06496v2", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "SignBLEU: Automatic Evaluation of Multi-channel Sign Language\n  Translation", "abstract": "Sign languages are multi-channel languages that communicate information\nthrough not just the hands (manual signals) but also facial expressions and\nupper body movements (non-manual signals). However, since automatic sign\nlanguage translation is usually performed by generating a single sequence of\nglosses, researchers eschew non-manual and co-occurring manual signals in favor\nof a simplified list of manual glosses. This can lead to significant\ninformation loss and ambiguity. In this paper, we introduce a new task named\nmulti-channel sign language translation (MCSLT) and present a novel metric,\nSignBLEU, designed to capture multiple signal channels. We validated SignBLEU\non a system-level task using three sign language corpora with varied linguistic\nstructures and transcription methodologies and examined its correlation with\nhuman judgment through two segment-level tasks. We found that SignBLEU\nconsistently correlates better with human judgment than competing metrics. To\nfacilitate further MCSLT research, we report benchmark scores for the three\nsign language corpora and release the source code for SignBLEU at\nhttps://github.com/eq4all-projects/SignBLEU.", "published": "2024-06-10 05:01:26", "link": "http://arxiv.org/abs/2406.06648v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Harnessing AI for efficient analysis of complex policy documents: a case\n  study of Executive Order 14110", "abstract": "Policy documents, such as legislation, regulations, and executive orders, are\ncrucial in shaping society. However, their length and complexity make\ninterpretation and application challenging and time-consuming. Artificial\nintelligence (AI), particularly large language models (LLMs), has the potential\nto automate the process of analyzing these documents, improving accuracy and\nefficiency. This study aims to evaluate the potential of AI in streamlining\npolicy analysis and to identify the strengths and limitations of current AI\napproaches. The research focuses on question answering and tasks involving\ncontent extraction from policy documents. A case study was conducted using\nExecutive Order 14110 on \"Safe, Secure, and Trustworthy Development and Use of\nArtificial Intelligence\" as a test case. Four commercial AI systems were used\nto analyze the document and answer a set of representative policy questions.\nThe performance of the AI systems was compared to manual analysis conducted by\nhuman experts. The study found that two AI systems, Gemini 1.5 Pro and Claude 3\nOpus, demonstrated significant potential for supporting policy analysis,\nproviding accurate and reliable information extraction from complex documents.\nThey performed comparably to human analysts but with significantly higher\nefficiency. However, achieving reproducibility remains a challenge,\nnecessitating further research and development.", "published": "2024-06-10 11:19:28", "link": "http://arxiv.org/abs/2406.06657v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "SecureNet: A Comparative Study of DeBERTa and Large Language Models for\n  Phishing Detection", "abstract": "Phishing, whether through email, SMS, or malicious websites, poses a major\nthreat to organizations by using social engineering to trick users into\nrevealing sensitive information. It not only compromises company's data\nsecurity but also incurs significant financial losses. In this paper, we\ninvestigate whether the remarkable performance of Large Language Models (LLMs)\ncan be leveraged for particular task like text classification, particularly\ndetecting malicious content and compare its results with state-of-the-art\nDeberta V3 (DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled\nEmbedding Sharing) model. We systematically assess the potential and\nlimitations of both approaches using comprehensive public datasets comprising\ndiverse data sources such as email, HTML, URL, SMS, and synthetic data\ngeneration. Additionally, we demonstrate how LLMs can generate convincing\nphishing emails, making it harder to spot scams and evaluate the performance of\nboth models in this context. Our study delves further into the challenges\nencountered by DeBERTa V3 during its training phases, fine-tuning methodology\nand transfer learning processes. Similarly, we examine the challenges\nassociated with LLMs and assess their respective performance. Among our\nexperimental approaches, the transformer-based DeBERTa method emerged as the\nmost effective, achieving a test dataset (HuggingFace phishing dataset) recall\n(sensitivity) of 95.17% closely followed by GPT-4 providing a recall of 91.04%.\nWe performed additional experiments with other datasets on the trained DeBERTa\nV3 model and LLMs like GPT 4 and Gemini 1.5. Based on our findings, we provide\nvaluable insights into the effectiveness and robustness of these advanced\nlanguage models, offering a detailed comparative analysis that can inform\nfuture research efforts in strengthening cybersecurity measures for detecting\nand mitigating phishing threats.", "published": "2024-06-10 13:13:39", "link": "http://arxiv.org/abs/2406.06663v1", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Synthetic Query Generation using Large Language Models for Virtual\n  Assistants", "abstract": "Virtual Assistants (VAs) are important Information Retrieval platforms that\nhelp users accomplish various tasks through spoken commands. The speech\nrecognition system (speech-to-text) uses query priors, trained solely on text,\nto distinguish between phonetically confusing alternatives. Hence, the\ngeneration of synthetic queries that are similar to existing VA usage can\ngreatly improve upon the VA's abilities -- especially for use-cases that do not\n(yet) occur in paired audio/text data.\n  In this paper, we provide a preliminary exploration of the use of Large\nLanguage Models (LLMs) to generate synthetic queries that are complementary to\ntemplate-based methods. We investigate whether the methods (a) generate queries\nthat are similar to randomly sampled, representative, and anonymized user\nqueries from a popular VA, and (b) whether the generated queries are specific.\n  We find that LLMs generate more verbose queries, compared to template-based\nmethods, and reference aspects specific to the entity. The generated queries\nare similar to VA user queries, and are specific enough to retrieve the\nrelevant entity. We conclude that queries generated by LLMs and templates are\ncomplementary.", "published": "2024-06-10 18:50:57", "link": "http://arxiv.org/abs/2406.06729v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Scaling the Vocabulary of Non-autoregressive Models for Efficient\n  Generative Retrieval", "abstract": "Generative Retrieval introduces a new approach to Information Retrieval by\nreframing it as a constrained generation task, leveraging recent advancements\nin Autoregressive (AR) language models. However, AR-based Generative Retrieval\nmethods suffer from high inference latency and cost compared to traditional\ndense retrieval techniques, limiting their practical applicability. This paper\ninvestigates fully Non-autoregressive (NAR) language models as a more efficient\nalternative for generative retrieval. While standard NAR models alleviate\nlatency and cost concerns, they exhibit a significant drop in retrieval\nperformance (compared to AR models) due to their inability to capture\ndependencies between target tokens. To address this, we question the\nconventional choice of limiting the target token space to solely words or\nsub-words. We propose PIXAR, a novel approach that expands the target\nvocabulary of NAR models to include multi-word entities and common phrases (up\nto 5 million tokens), thereby reducing token dependencies. PIXAR employs\ninference optimization strategies to maintain low inference latency despite the\nsignificantly larger vocabulary. Our results demonstrate that PIXAR achieves a\nrelative improvement of 31.0% in MRR@10 on MS MARCO and 23.2% in Hits@5 on\nNatural Questions compared to standard NAR models with similar latency and\ncost. Furthermore, online A/B experiments on a large commercial search engine\nshow that PIXAR increases ad clicks by 5.08% and revenue by 4.02%.", "published": "2024-06-10 19:01:15", "link": "http://arxiv.org/abs/2406.06739v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "$Classi|Q\\rangle$ Towards a Translation Framework To Bridge The\n  Classical-Quantum Programming Gap", "abstract": "Quantum computing, albeit readily available as hardware or emulated on the\ncloud, is still far from being available in general regarding complex\nprogramming paradigms and learning curves. This vision paper introduces\n$Classi|Q\\rangle$, a translation framework idea to bridge Classical and Quantum\nComputing by translating high-level programming languages, e.g., Python or C++,\ninto a low-level language, e.g., Quantum Assembly. Our idea paper serves as a\nblueprint for ongoing efforts in quantum software engineering, offering a\nroadmap for further $Classi|Q\\rangle$ development to meet the diverse needs of\nresearchers and practitioners. $Classi|Q\\rangle$ is designed to empower\nresearchers and practitioners with no prior quantum experience to harness the\npotential of hybrid quantum computation. We also discuss future enhancements to\n$Classi|Q\\rangle$, including support for additional quantum languages, improved\noptimization strategies, and integration with emerging quantum computing\nplatforms.", "published": "2024-06-10 19:50:16", "link": "http://arxiv.org/abs/2406.06764v3", "categories": ["cs.SE", "cs.CL", "cs.ET", "cs.PL"], "primary_category": "cs.SE"}
{"title": "A Survey of Recent Backdoor Attacks and Defenses in Large Language\n  Models", "abstract": "Large Language Models (LLMs), which bridge the gap between human language\nunderstanding and complex problem-solving, achieve state-of-the-art performance\non several NLP tasks, particularly in few-shot and zero-shot settings. Despite\nthe demonstrable efficacy of LLMs, due to constraints on computational\nresources, users have to engage with open-source language models or outsource\nthe entire training process to third-party platforms. However, research has\ndemonstrated that language models are susceptible to potential security\nvulnerabilities, particularly in backdoor attacks. Backdoor attacks are\ndesigned to introduce targeted vulnerabilities into language models by\npoisoning training samples or model weights, allowing attackers to manipulate\nmodel responses through malicious triggers. While existing surveys on backdoor\nattacks provide a comprehensive overview, they lack an in-depth examination of\nbackdoor attacks specifically targeting LLMs. To bridge this gap and grasp the\nlatest trends in the field, this paper presents a novel perspective on backdoor\nattacks for LLMs by focusing on fine-tuning methods. Specifically, we\nsystematically classify backdoor attacks into three categories: full-parameter\nfine-tuning, parameter-efficient fine-tuning, and no fine-tuning Based on\ninsights from a substantial review, we also discuss crucial issues for future\nresearch on backdoor attacks, such as further exploring attack algorithms that\ndo not require fine-tuning, or developing more covert attack algorithms.", "published": "2024-06-10 23:54:21", "link": "http://arxiv.org/abs/2406.06852v5", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "BrainChat: Decoding Semantic Information from fMRI using Vision-language\n  Pretrained Models", "abstract": "Semantic information is vital for human interaction, and decoding it from\nbrain activity enables non-invasive clinical augmentative and alternative\ncommunication. While there has been significant progress in reconstructing\nvisual images, few studies have focused on the language aspect. To address this\ngap, leveraging the powerful capabilities of the decoder-based vision-language\npretrained model CoCa, this paper proposes BrainChat, a simple yet effective\ngenerative framework aimed at rapidly accomplishing semantic information\ndecoding tasks from brain activity, including fMRI question answering and fMRI\ncaptioning. BrainChat employs the self-supervised approach of Masked Brain\nModeling to encode sparse fMRI data, obtaining a more compact embedding\nrepresentation in the latent space. Subsequently, BrainChat bridges the gap\nbetween modalities by applying contrastive loss, resulting in aligned\nrepresentations of fMRI, image, and text embeddings. Furthermore, the fMRI\nembeddings are mapped to the generative Brain Decoder via cross-attention\nlayers, where they guide the generation of textual content about fMRI in a\nregressive manner by minimizing caption loss. Empirically, BrainChat exceeds\nthe performance of existing state-of-the-art methods in the fMRI captioning\ntask and, for the first time, implements fMRI question answering. Additionally,\nBrainChat is highly flexible and can achieve high performance without image\ndata, making it better suited for real-world scenarios with limited data.", "published": "2024-06-10 12:06:15", "link": "http://arxiv.org/abs/2406.07584v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "The Impact of Quantization on Retrieval-Augmented Generation: An\n  Analysis of Small LLMs", "abstract": "Post-training quantization reduces the computational demand of Large Language\nModels (LLMs) but can weaken some of their capabilities. Since LLM abilities\nemerge with scale, smaller LLMs are more sensitive to quantization. In this\npaper, we explore how quantization affects smaller LLMs' ability to perform\nretrieval-augmented generation (RAG), specifically in longer contexts. We chose\npersonalization for evaluation because it is a challenging domain to perform\nusing RAG as it requires long-context reasoning over multiple documents. We\ncompare the original FP16 and the quantized INT4 performance of multiple 7B and\n8B LLMs on two tasks while progressively increasing the number of retrieved\ndocuments to test how quantized models fare against longer contexts. To better\nunderstand the effect of retrieval, we evaluate three retrieval models in our\nexperiments. Our findings reveal that if a 7B LLM performs the task well,\nquantization does not impair its performance and long-context reasoning\ncapabilities. We conclude that it is possible to utilize RAG with quantized\nsmaller LLMs.", "published": "2024-06-10 08:23:52", "link": "http://arxiv.org/abs/2406.10251v3", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "AutoSurvey: Large Language Models Can Automatically Write Surveys", "abstract": "This paper introduces AutoSurvey, a speedy and well-organized methodology for\nautomating the creation of comprehensive literature surveys in rapidly evolving\nfields like artificial intelligence. Traditional survey paper creation faces\nchallenges due to the vast volume and complexity of information, prompting the\nneed for efficient survey methods. While large language models (LLMs) offer\npromise in automating this process, challenges such as context window\nlimitations, parametric knowledge constraints, and the lack of evaluation\nbenchmarks remain. AutoSurvey addresses these challenges through a systematic\napproach that involves initial retrieval and outline generation, subsection\ndrafting by specialized LLMs, integration and refinement, and rigorous\nevaluation and iteration. Our contributions include a comprehensive solution to\nthe survey problem, a reliable evaluation method, and experimental validation\ndemonstrating AutoSurvey's effectiveness.We open our resources at\n\\url{https://github.com/AutoSurveys/AutoSurvey}.", "published": "2024-06-10 12:56:06", "link": "http://arxiv.org/abs/2406.10252v2", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "D\u00e9veloppement automatique de lexiques pour les concepts \u00e9mergents :\n  une exploration m\u00e9thodologique", "abstract": "This paper presents the development of a lexicon centered on emerging\nconcepts, focusing on non-technological innovation. It introduces a four-step\nmethodology that combines human expertise, statistical analysis, and machine\nlearning techniques to establish a model that can be generalized across\nmultiple domains. This process includes the creation of a thematic corpus, the\ndevelopment of a Gold Standard Lexicon, annotation and preparation of a\ntraining corpus, and finally, the implementation of learning models to identify\nnew terms. The results demonstrate the robustness and relevance of our\napproach, highlighting its adaptability to various contexts and its\ncontribution to lexical research. The developed methodology promises\napplicability in conceptual fields.", "published": "2024-06-10 12:58:56", "link": "http://arxiv.org/abs/2406.10253v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Explicit Word Density Estimation for Language Modelling", "abstract": "Language Modelling has been a central part of Natural Language Processing for\na very long time and in the past few years LSTM-based language models have been\nthe go-to method for commercial language modeling. Recently, it has been shown\nthat when looking at language modelling from a matrix factorization point of\nview, the final Softmax layer limits the expressiveness of the model, by\nputting an upper bound on the rank of the resulting matrix. Additionally, a new\nfamily of neural networks based called NeuralODEs, has been introduced as a\ncontinuous alternative to Residual Networks. Moreover, it has been shown that\nthere is a connection between these models and Normalizing Flows. In this work\nwe propose a new family of language models based on NeuralODEs and the\ncontinuous analogue of Normalizing Flows and manage to improve on some of the\nbaselines.", "published": "2024-06-10 15:21:33", "link": "http://arxiv.org/abs/2406.10256v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Label-Looping: Highly Efficient Decoding for Transducers", "abstract": "This paper introduces a highly efficient greedy decoding algorithm for\nTransducer-based speech recognition models. We redesign the standard\nnested-loop design for RNN-T decoding, swapping loops over frames and labels:\nthe outer loop iterates over labels, while the inner loop iterates over frames\nsearching for the next non-blank symbol. Additionally, we represent partial\nhypotheses in a special structure using CUDA tensors, supporting parallelized\nhypotheses manipulations. Experiments show that the label-looping algorithm is\nup to 2.0X faster than conventional batched decoding when using batch size 32.\nIt can be further combined with other compiler or GPU call-related techniques\nto achieve even more speedup. Our algorithm is general-purpose and can work\nwith both conventional Transducers and Token-and-Duration Transducers. We\nopen-source our implementation to benefit the research community.", "published": "2024-06-10 12:34:38", "link": "http://arxiv.org/abs/2406.06220v2", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "AID: Adapting Image2Video Diffusion Models for Instruction-guided Video\n  Prediction", "abstract": "Text-guided video prediction (TVP) involves predicting the motion of future\nframes from the initial frame according to an instruction, which has wide\napplications in virtual reality, robotics, and content creation. Previous TVP\nmethods make significant breakthroughs by adapting Stable Diffusion for this\ntask. However, they struggle with frame consistency and temporal stability\nprimarily due to the limited scale of video datasets. We observe that\npretrained Image2Video diffusion models possess good priors for video dynamics\nbut they lack textual control. Hence, transferring Image2Video models to\nleverage their video dynamic priors while injecting instruction control to\ngenerate controllable videos is both a meaningful and challenging task. To\nachieve this, we introduce the Multi-Modal Large Language Model (MLLM) to\npredict future video states based on initial frames and text instructions. More\nspecifically, we design a dual query transformer (DQFormer) architecture, which\nintegrates the instructions and frames into the conditional embeddings for\nfuture frame prediction. Additionally, we develop Long-Short Term Temporal\nAdapters and Spatial Adapters that can quickly transfer general video diffusion\nmodels to specific scenarios with minimal training costs. Experimental results\nshow that our method significantly outperforms state-of-the-art techniques on\nfour datasets: Something Something V2, Epic Kitchen-100, Bridge Data, and\nUCF-101. Notably, AID achieves 91.2% and 55.5% FVD improvements on Bridge and\nSSv2 respectively, demonstrating its effectiveness in various domains. More\nexamples can be found at our website https://chenhsing.github.io/AID.", "published": "2024-06-10 17:02:08", "link": "http://arxiv.org/abs/2406.06465v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Towards Signal Processing In Large Language Models", "abstract": "This paper introduces the idea of applying signal processing inside a Large\nLanguage Model (LLM). With the recent explosion of generative AI, our work can\nhelp bridge two fields together, namely the field of signal processing and\nlarge language models. We draw parallels between classical Fourier-Transforms\nand Fourier Transform-like learnable time-frequency representations for every\nintermediate activation signal of an LLM. Once we decompose every activation\nsignal across tokens into a time-frequency representation, we learn how to\nfilter and reconstruct them, with all components learned from scratch, to\npredict the next token given the previous context. We show that for GPT-like\narchitectures, our work achieves faster convergence and significantly increases\nperformance by adding a minuscule number of extra parameters when trained for\nthe same epochs. We hope this work paves the way for algorithms exploring\nsignal processing inside the signals found in neural architectures like LLMs\nand beyond.", "published": "2024-06-10 13:51:52", "link": "http://arxiv.org/abs/2406.10254v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Multimodal Contextualized Semantic Parsing from Speech", "abstract": "We introduce Semantic Parsing in Contextual Environments (SPICE), a task\ndesigned to enhance artificial agents' contextual awareness by integrating\nmultimodal inputs with prior contexts. SPICE goes beyond traditional semantic\nparsing by offering a structured, interpretable framework for dynamically\nupdating an agent's knowledge with new information, mirroring the complexity of\nhuman communication. We develop the VG-SPICE dataset, crafted to challenge\nagents with visual scene graph construction from spoken conversational\nexchanges, highlighting speech and visual data integration. We also present the\nAudio-Vision Dialogue Scene Parser (AViD-SP) developed for use on VG-SPICE.\nThese innovations aim to improve multimodal information processing and\nintegration. Both the VG-SPICE dataset and the AViD-SP model are publicly\navailable.", "published": "2024-06-10 16:31:34", "link": "http://arxiv.org/abs/2406.06438v1", "categories": ["cs.CL", "cs.CV", "cs.HC", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Accent Conversion with Articulatory Representations", "abstract": "Conversion of non-native accented speech to native (American) English has a\nwide range of applications such as improving intelligibility of non-native\nspeech. Previous work on this domain has used phonetic posteriograms as the\ntarget speech representation to train an acoustic model which is then used to\nextract a compact representation of input speech for accent conversion. In this\nwork, we introduce the idea of using an effective articulatory speech\nrepresentation, extracted from an acoustic-to-articulatory speech inversion\nsystem, to improve the acoustic model used in accent conversion. The idea to\nincorporate articulatory representations originates from their ability to well\ncharacterize accents in speech. To incorporate articulatory representations\nwith conventional phonetic posteriograms, a multi-task learning based acoustic\nmodel is proposed. Objective and subjective evaluations show that the use of\narticulatory representations can improve the effectiveness of accent\nconversion.", "published": "2024-06-10 00:35:23", "link": "http://arxiv.org/abs/2406.05947v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "BS-PLCNet 2: Two-stage Band-split Packet Loss Concealment Network with\n  Intra-model Knowledge Distillation", "abstract": "Audio packet loss is an inevitable problem in real-time speech communication.\nA band-split packet loss concealment network (BS-PLCNet) targeting full-band\nsignals was recently proposed. Although it performs superiorly in the ICASSP\n2024 PLC Challenge, BS-PLCNet is a large model with high computational\ncomplexity of 8.95G FLOPS. This paper presents its updated version, BS-PLCNet\n2, to reduce computational complexity and improve performance further.\nSpecifically, to compensate for the missing future information, in the\nwide-band module, we design a dual-path encoder structure (with non-causal and\ncausal path) and leverage an intra-model knowledge distillation strategy to\ndistill the future information from the non-causal teacher to the casual\nstudent. Moreover, we introduce a lightweight post-processing module after\npacket loss restoration to recover speech distortions and remove residual noise\nin the audio signal. With only 40% of original parameters in BS-PLCNet,\nBS-PLCNet 2 brings 0.18 PLCMOS improvement on the ICASSP 2024 PLC challenge\nblind set, achieving state-of-the-art performance on this dataset.", "published": "2024-06-10 01:40:21", "link": "http://arxiv.org/abs/2406.05961v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Separate and Reconstruct: Asymmetric Encoder-Decoder for Speech\n  Separation", "abstract": "In speech separation, time-domain approaches have successfully replaced the\ntime-frequency domain with latent sequence feature from a learnable encoder.\nConventionally, the feature is separated into speaker-specific ones at the\nfinal stage of the network. Instead, we propose a more intuitive strategy that\nseparates features earlier by expanding the feature sequence to the number of\nspeakers as an extra dimension. To achieve this, an asymmetric strategy is\npresented in which the encoder and decoder are partitioned to perform distinct\nprocessing in separation tasks. The encoder analyzes features, and the output\nof the encoder is split into the number of speakers to be separated. The\nseparated sequences are then reconstructed by the weight-shared decoder, which\nalso performs cross-speaker processing. Without relying on speaker information,\nthe weight-shared network in the decoder directly learns to discriminate\nfeatures using a separation objective. In addition, to improve performance,\ntraditional methods have extended the sequence length, leading to the adoption\nof dual-path models, which handle the much longer sequence effectively by\nsegmenting it into chunks. To address this, we introduce global and local\nTransformer blocks that can directly handle long sequences more efficiently\nwithout chunking and dual-path processing. The experimental results\ndemonstrated that this asymmetric structure is effective and that the\ncombination of proposed global and local Transformer can sufficiently replace\nthe role of inter- and intra-chunk processing in dual-path structure. Finally,\nthe presented model combining both of these achieved state-of-the-art\nperformance with much less computation in various benchmark datasets.", "published": "2024-06-10 02:50:54", "link": "http://arxiv.org/abs/2406.05983v4", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "The Effect of Training Dataset Size on Discriminative and\n  Diffusion-Based Speech Enhancement Systems", "abstract": "The performance of deep neural network-based speech enhancement systems\ntypically increases with the training dataset size. However, studies that\ninvestigated the effect of training dataset size on speech enhancement\nperformance did not consider recent approaches, such as diffusion-based\ngenerative models. Diffusion models are typically trained with massive datasets\nfor image generation tasks, but whether this is also required for speech\nenhancement is unknown. Moreover, studies that investigated the effect of\ntraining dataset size did not control for the data diversity. It is thus\nunclear whether the performance improvement was due to the increased dataset\nsize or diversity. Therefore, we systematically investigate the effect of\ntraining dataset size on the performance of popular state-of-the-art\ndiscriminative and diffusion-based speech enhancement systems in matched\nconditions. We control for the data diversity by using a fixed set of speech\nutterances, noise segments and binaural room impulse responses to generate\ndatasets of different sizes. We find that the diffusion-based systems perform\nthe best relative to the discriminative systems in terms of objective metrics\nwith datasets of 10 h or less. However, their objective metrics performance\ndoes not improve when increasing the training dataset size as much as the\ndiscriminative systems, and they are outperformed by the discriminative systems\nwith datasets of 100 h or more.", "published": "2024-06-10 10:46:44", "link": "http://arxiv.org/abs/2406.06160v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "MakeSinger: A Semi-Supervised Training Method for Data-Efficient Singing\n  Voice Synthesis via Classifier-free Diffusion Guidance", "abstract": "In this paper, we propose MakeSinger, a semi-supervised training method for\nsinging voice synthesis (SVS) via classifier-free diffusion guidance. The\nchallenge in SVS lies in the costly process of gathering aligned sets of text,\npitch, and audio data. MakeSinger enables the training of the diffusion-based\nSVS model from any speech and singing voice data regardless of its labeling,\nthereby enhancing the quality of generated voices with large amount of\nunlabeled data. At inference, our novel dual guiding mechanism gives text and\npitch guidance on the reverse diffusion step by estimating the score of masked\ninput. Experimental results show that the model trained in a semi-supervised\nmanner outperforms other baselines trained only on the labeled data in terms of\npronunciation, pitch accuracy and overall quality. Furthermore, we demonstrate\nthat by adding Text-to-Speech (TTS) data in training, the model can synthesize\nthe singing voices of TTS speakers even without their singing voices.", "published": "2024-06-10 01:47:52", "link": "http://arxiv.org/abs/2406.05965v1", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "RawBMamba: End-to-End Bidirectional State Space Model for Audio Deepfake\n  Detection", "abstract": "Fake artefacts for discriminating between bonafide and fake audio can exist\nin both short- and long-range segments. Therefore, combining local and global\nfeature information can effectively discriminate between bonafide and fake\naudio. This paper proposes an end-to-end bidirectional state space model, named\nRawBMamba, to capture both short- and long-range discriminative information for\naudio deepfake detection. Specifically, we use sinc Layer and multiple\nconvolutional layers to capture short-range features, and then design a\nbidirectional Mamba to address Mamba's unidirectional modelling problem and\nfurther capture long-range feature information. Moreover, we develop a\nbidirectional fusion module to integrate embeddings, enhancing audio context\nrepresentation and combining short- and long-range information. The results\nshow that our proposed RawBMamba achieves a 34.1\\% improvement over Rawformer\non ASVspoof2021 LA dataset, and demonstrates competitive performance on other\ndatasets.", "published": "2024-06-10 08:13:42", "link": "http://arxiv.org/abs/2406.06086v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Quantifying the effect of speech pathology on automatic and human\n  speaker verification", "abstract": "This study investigates how surgical intervention for speech pathology\n(specifically, as a result of oral cancer surgery) impacts the performance of\nan automatic speaker verification (ASV) system. Using two recently collected\nDutch datasets with parallel pre and post-surgery audio from the same speaker,\nNKI-OC-VC and SPOKE, we assess the extent to which speech pathology influences\nASV performance, and whether objective/subjective measures of speech severity\nare correlated with the performance. Finally, we carry out a perceptual study\nto compare judgements of ASV and human listeners. Our findings reveal that\npathological speech negatively affects ASV performance, and the severity of the\nspeech is negatively correlated with the performance. There is a moderate\nagreement in perceptual and objective scores of speaker similarity and\nseverity, however, we could not clearly establish in the perceptual study,\nwhether the same phenomenon also exists in human perception.", "published": "2024-06-10 12:16:28", "link": "http://arxiv.org/abs/2406.06208v1", "categories": ["cs.SD", "eess.AS", "I.2.7"], "primary_category": "cs.SD"}
{"title": "Zero-Shot Audio Captioning Using Soft and Hard Prompts", "abstract": "In traditional audio captioning methods, a model is usually trained in a\nfully supervised manner using a human-annotated dataset containing audio-text\npairs and then evaluated on the test sets from the same dataset. Such methods\nhave two limitations. First, these methods are often data-hungry and require\ntime-consuming and expensive human annotations to obtain audio-text pairs.\nSecond, these models often suffer from performance degradation in cross-domain\nscenarios, i.e., when the input audio comes from a different domain than the\ntraining set, which, however, has received little attention. We propose an\neffective audio captioning method based on the contrastive language-audio\npre-training (CLAP) model to address these issues. Our proposed method requires\nonly textual data for training, enabling the model to generate text from the\ntextual feature in the cross-modal semantic space.In the inference stage, the\nmodel generates the descriptive text for the given audio from the audio feature\nby leveraging the audio-text alignment from CLAP.We devise two strategies to\nmitigate the discrepancy between text and audio embeddings: a\nmixed-augmentation-based soft prompt and a retrieval-based acoustic-aware hard\nprompt. These approaches are designed to enhance the generalization performance\nof our proposed model, facilitating the model to generate captions more\nrobustly and accurately. Extensive experiments on AudioCaps and Clotho\nbenchmarks show the effectiveness of our proposed method, which outperforms\nother zero-shot audio captioning approaches for in-domain scenarios and\noutperforms the compared methods for cross-domain scenarios, underscoring the\ngeneralization ability of our method.", "published": "2024-06-10 14:16:28", "link": "http://arxiv.org/abs/2406.06295v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Unsupervised Improved MVDR Beamforming for Sound Enhancement", "abstract": "Neural networks have recently become the dominant approach to sound\nseparation. Their good performance relies on large datasets of isolated\nrecordings. For speech and music, isolated single channel data are readily\navailable; however the same does not hold in the multi-channel case, and with\nmost other sound classes. Multi-channel methods have the potential to\noutperform single channel approaches as they can exploit both spatial and\nspectral features, but the lack of training data remains a challenge. We\npropose unsupervised improved minimum variation distortionless response\n(UIMVDR), which enables multi-channel separation to leverage in-the-wild\nsingle-channel data through unsupervised training and beamforming. Results show\nthat UIMVDR generalizes well and improves separation performance compared to\nsupervised models, particularly in cases with limited supervised data. By using\ndata available online, it also reduces the effort required to gather data for\nmulti-channel approaches.", "published": "2024-06-10 14:26:09", "link": "http://arxiv.org/abs/2406.06310v4", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "An automatic analysis of ultrasound vocalisations for the prediction of\n  interaction context in captive Egyptian fruit bats", "abstract": "Prior work in computational bioacoustics has mostly focused on the detection\nof animal presence in a particular habitat. However, animal sounds contain much\nricher information than mere presence; among others, they encapsulate the\ninteractions of those animals with other members of their species. Studying\nthese interactions is almost impossible in a naturalistic setting, as the\nground truth is often lacking. The use of animals in captivity instead offers a\nviable alternative pathway. However, most prior works follow a traditional,\nstatistics-based approach to analysing interactions. In the present work, we go\nbeyond this standard framework by attempting to predict the underlying context\nin interactions between captive \\emph{Rousettus Aegyptiacus} using deep neural\nnetworks. We reach an unweighted average recall of over 30\\% -- more than\nthrice the chance level -- and show error patterns that differ from our\nstatistical analysis. This work thus represents an important step towards the\nautomatic analysis of states in animals from sound.", "published": "2024-06-10 14:50:32", "link": "http://arxiv.org/abs/2406.06332v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio-based Step-count Estimation for Running -- Windowing and Neural\n  Network Baselines", "abstract": "In recent decades, running has become an increasingly popular pastime\nactivity due to its accessibility, ease of practice, and anticipated health\nbenefits. However, the risk of running-related injuries is substantial for\nrunners of different experience levels. Several common forms of injuries result\nfrom overuse -- extending beyond the recommended running time and intensity.\nRecently, audio-based tracking has emerged as yet another modality for\nmonitoring running behaviour and performance, with previous studies largely\nconcentrating on predicting runner fatigue. In this work, we investigate\naudio-based step count estimation during outdoor running, achieving a mean\nabsolute error of 1.098 in window-based step-count differences and a Pearson\ncorrelation coefficient of 0.479 when predicting the number of steps in a\n5-second window of audio. Our work thus showcases the feasibility of\naudio-based monitoring for estimating important physiological variables and\nlays the foundations for further utilising audio sensors for a more thorough\ncharacterisation of runner behaviour.", "published": "2024-06-10 14:59:01", "link": "http://arxiv.org/abs/2406.06339v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Emotion-Aware Speech Self-Supervised Representation Learning with\n  Intensity Knowledge", "abstract": "Speech Self-Supervised Learning (SSL) has demonstrated considerable efficacy\nin various downstream tasks. Nevertheless, prevailing self-supervised models\noften overlook the incorporation of emotion-related prior information, thereby\nneglecting the potential enhancement of emotion task comprehension through\nemotion prior knowledge in speech. In this paper, we propose an emotion-aware\nspeech representation learning with intensity knowledge. Specifically, we\nextract frame-level emotion intensities using an established speech-emotion\nunderstanding model. Subsequently, we propose a novel emotional masking\nstrategy (EMS) to incorporate emotion intensities into the masking process. We\nselected two representative models based on Transformer and CNN, namely\nMockingJay and Non-autoregressive Predictive Coding (NPC), and conducted\nexperiments on IEMOCAP dataset. Experiments have demonstrated that the\nrepresentations derived from our proposed method outperform the original model\nin SER task.", "published": "2024-06-10 02:08:25", "link": "http://arxiv.org/abs/2406.06646v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "ComFeAT: Combination of Neural and Spectral Features for Improved\n  Depression Detection", "abstract": "In this work, we focus on the detection of depression through speech\nanalysis. Previous research has widely explored features extracted from\npre-trained models (PTMs) primarily trained for paralinguistic tasks. Although\nthese features have led to sufficient advances in speech-based depression\ndetection, their performance declines in real-world settings. To address this,\nin this paper, we introduce ComFeAT, an application that employs a CNN model\ntrained on a combination of features extracted from PTMs, a.k.a. neural\nfeatures and spectral features to enhance depression detection. Spectral\nfeatures are robust to domain variations, but, they are not as good as neural\nfeatures in performance, suprisingly, combining them shows complementary\nbehavior and improves over both neural and spectral features individually. The\nproposed method also improves over previous state-of-the-art (SOTA) works on\nE-DAIC benchmark.", "published": "2024-06-10 20:23:04", "link": "http://arxiv.org/abs/2406.06774v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "PERSONA: An Application for Emotion Recognition, Gender Recognition and\n  Age Estimation", "abstract": "Emotion Recognition (ER), Gender Recognition (GR), and Age Estimation (AE)\nconstitute paralinguistic tasks that rely not on the spoken content but\nprimarily on speech characteristics such as pitch and tone. While previous\nresearch has made significant strides in developing models for each task\nindividually, there has been comparatively less emphasis on concurrently\nlearning these tasks, despite their inherent interconnectedness. As such in\nthis demonstration, we present PERSONA, an application for predicting ER, GR,\nand AE with a single model in the backend. One notable point is we show that\nrepresentations from speaker recognition pre-trained model (PTM) is better\nsuited for such a multi-task learning format than the state-of-the-art (SOTA)\nself-supervised (SSL) PTM by carrying out a comparative study. Our methodology\nobviates the need for deploying separate models for each task and can\npotentially conserve resources and time during the training and deployment\nphases.", "published": "2024-06-10 20:38:48", "link": "http://arxiv.org/abs/2406.06781v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The Reasonable Effectiveness of Speaker Embeddings for Violence\n  Detection", "abstract": "In this paper, we focus on audio violence detection (AVD). AVD is necessary\nfor several reasons, especially in the context of maintaining safety,\npreventing harm, and ensuring security in various environments. This calls for\naccurate AVD systems. Like many related applications in audio processing, the\nmost common approach for improving the performance, would be by leveraging\nself-supervised (SSL) pre-trained models (PTMs). However, as these SSL models\nare very large models with million of parameters and this can hinder real-world\ndeployment especially in compute-constraint environment. To resolve this, we\npropose the usage of speaker recognition models which are much smaller compared\nto the SSL models. Experimentation with speaker recognition model embeddings\nwith SVM & Random Forest as classifiers, we show that speaker recognition model\nembeddings perform the best in comparison to state-of-the-art (SOTA) SSL models\nand achieve SOTA results.", "published": "2024-06-10 21:08:31", "link": "http://arxiv.org/abs/2406.06798v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "JenGAN: Stacked Shifted Filters in GAN-Based Speech Synthesis", "abstract": "Non-autoregressive GAN-based neural vocoders are widely used due to their\nfast inference speed and high perceptual quality. However, they often suffer\nfrom audible artifacts such as tonal artifacts in their generated results.\nTherefore, we propose JenGAN, a new training strategy that involves stacking\nshifted low-pass filters to ensure the shift-equivariant property. This method\nhelps prevent aliasing and reduce artifacts while preserving the model\nstructure used during inference. In our experimental evaluation, JenGAN\nconsistently enhances the performance of vocoder models, yielding significantly\nsuperior scores across the majority of evaluation metrics.", "published": "2024-06-10 08:51:04", "link": "http://arxiv.org/abs/2406.06111v1", "categories": ["eess.AS", "cs.AI", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "EARS: An Anechoic Fullband Speech Dataset Benchmarked for Speech\n  Enhancement and Dereverberation", "abstract": "We release the EARS (Expressive Anechoic Recordings of Speech) dataset, a\nhigh-quality speech dataset comprising 107 speakers from diverse backgrounds,\ntotaling in 100 hours of clean, anechoic speech data. The dataset covers a\nlarge range of different speaking styles, including emotional speech, different\nreading styles, non-verbal sounds, and conversational freeform speech. We\nbenchmark various methods for speech enhancement and dereverberation on the\ndataset and evaluate their performance through a set of instrumental metrics.\nIn addition, we conduct a listening test with 20 participants for the speech\nenhancement task, where a generative method is preferred. We introduce a blind\ntest set that allows for automatic online evaluation of uploaded data. Dataset\ndownload links and automatic evaluation server can be found online.", "published": "2024-06-10 11:28:29", "link": "http://arxiv.org/abs/2406.06185v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Sample Rate Independent Recurrent Neural Networks for Audio Effects\n  Processing", "abstract": "In recent years, machine learning approaches to modelling guitar amplifiers\nand effects pedals have been widely investigated and have become standard\npractice in some consumer products. In particular, recurrent neural networks\n(RNNs) are a popular choice for modelling non-linear devices such as vacuum\ntube amplifiers and distortion circuitry. One limitation of such models is that\nthey are trained on audio at a specific sample rate and therefore give\nunreliable results when operating at another rate. Here, we investigate several\nmethods of modifying RNN structures to make them approximately sample rate\nindependent, with a focus on oversampling. In the case of integer oversampling,\nwe demonstrate that a previously proposed delay-based approach provides high\nfidelity sample rate conversion whilst additionally reducing aliasing. For\nnon-integer sample rate adjustment, we propose two novel methods and show that\none of these, based on cubic Lagrange interpolation of a delay-line, provides a\nsignificant improvement over existing methods. To our knowledge, this work\nprovides the first in-depth study into this problem.", "published": "2024-06-10 14:14:23", "link": "http://arxiv.org/abs/2406.06293v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Predicting Heart Activity from Speech using Data-driven and\n  Knowledge-based features", "abstract": "Accurately predicting heart activity and other biological signals is crucial\nfor diagnosis and monitoring. Given that speech is an outcome of multiple\nphysiological systems, a significant body of work studied the acoustic\ncorrelates of heart activity. Recently, self-supervised models have excelled in\nspeech-related tasks compared to traditional acoustic methods. However, the\nrobustness of data-driven representations in predicting heart activity remained\nunexplored. In this study, we demonstrate that self-supervised speech models\noutperform acoustic features in predicting heart activity parameters. We also\nemphasize the impact of individual variability on model generalizability. These\nfindings underscore the value of data-driven representations in such tasks and\nthe need for more speech-based physiological data to mitigate speaker-related\nchallenges.", "published": "2024-06-10 15:01:46", "link": "http://arxiv.org/abs/2406.06341v1", "categories": ["cs.SD", "cs.AI", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "MOSA: Music Motion with Semantic Annotation Dataset for Cross-Modal\n  Music Processing", "abstract": "In cross-modal music processing, translation between visual, auditory, and\nsemantic content opens up new possibilities as well as challenges. The\nconstruction of such a transformative scheme depends upon a benchmark corpus\nwith a comprehensive data infrastructure. In particular, the assembly of a\nlarge-scale cross-modal dataset presents major challenges. In this paper, we\npresent the MOSA (Music mOtion with Semantic Annotation) dataset, which\ncontains high quality 3-D motion capture data, aligned audio recordings, and\nnote-by-note semantic annotations of pitch, beat, phrase, dynamic,\narticulation, and harmony for 742 professional music performances by 23\nprofessional musicians, comprising more than 30 hours and 570 K notes of data.\nTo our knowledge, this is the largest cross-modal music dataset with note-level\nannotations to date. To demonstrate the usage of the MOSA dataset, we present\nseveral innovative cross-modal music information retrieval (MIR) and musical\ncontent generation tasks, including the detection of beats, downbeats, phrase,\nand expressive contents from audio, video and motion data, and the generation\nof musicians' body motion from given music audio. The dataset and codes are\navailable alongside this publication\n(https://github.com/yufenhuang/MOSA-Music-mOtion-and-Semantic-Annotation-dataset).", "published": "2024-06-10 15:37:46", "link": "http://arxiv.org/abs/2406.06375v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ASTRA: Aligning Speech and Text Representations for Asr without Sampling", "abstract": "This paper introduces ASTRA, a novel method for improving Automatic Speech\nRecognition (ASR) through text injection.Unlike prevailing techniques, ASTRA\neliminates the need for sampling to match sequence lengths between speech and\ntext modalities. Instead, it leverages the inherent alignments learned within\nCTC/RNNT models. This approach offers the following two advantages, namely,\navoiding potential misalignment between speech and text features that could\narise from upsampling and eliminating the need for models to accurately predict\nduration of sub-word tokens. This novel formulation of modality (length)\nmatching as a weighted RNNT objective matches the performance of the\nstate-of-the-art duration-based methods on the FLEURS benchmark, while opening\nup other avenues of research in speech processing.", "published": "2024-06-10 15:39:04", "link": "http://arxiv.org/abs/2406.06664v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "BTS: Bridging Text and Sound Modalities for Metadata-Aided Respiratory\n  Sound Classification", "abstract": "Respiratory sound classification (RSC) is challenging due to varied acoustic\nsignatures, primarily influenced by patient demographics and recording\nenvironments. To address this issue, we introduce a text-audio multimodal model\nthat utilizes metadata of respiratory sounds, which provides useful\ncomplementary information for RSC. Specifically, we fine-tune a pretrained\ntext-audio multimodal model using free-text descriptions derived from the sound\nsamples' metadata which includes the gender and age of patients, type of\nrecording devices, and recording location on the patient's body. Our method\nachieves state-of-the-art performance on the ICBHI dataset, surpassing the\nprevious best result by a notable margin of 1.17%. This result validates the\neffectiveness of leveraging metadata and respiratory sound samples in enhancing\nRSC performance. Additionally, we investigate the model performance in the case\nwhere metadata is partially unavailable, which may occur in real-world clinical\nsetting.", "published": "2024-06-10 20:49:54", "link": "http://arxiv.org/abs/2406.06786v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
