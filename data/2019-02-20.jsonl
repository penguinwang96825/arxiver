{"title": "Phoneme Level Language Models for Sequence Based Low Resource ASR", "abstract": "Building multilingual and crosslingual models help bring different languages\ntogether in a language universal space. It allows models to share parameters\nand transfer knowledge across languages, enabling faster and better adaptation\nto a new language. These approaches are particularly useful for low resource\nlanguages. In this paper, we propose a phoneme-level language model that can be\nused multilingually and for crosslingual adaptation to a target language. We\nshow that our model performs almost as well as the monolingual models by using\nsix times fewer parameters, and is capable of better adaptation to languages\nnot seen during training in a low resource scenario. We show that these\nphoneme-level language models can be used to decode sequence based\nConnectionist Temporal Classification (CTC) acoustic model outputs to obtain\ncomparable word error rates with Weighted Finite State Transducer (WFST) based\ndecoding in Babel languages. We also show that these phoneme-level language\nmodels outperform WFST decoding in various low-resource conditions like\nadapting to a new language and domain mismatch between training and testing\ndata.", "published": "2019-02-20 16:00:12", "link": "http://arxiv.org/abs/1902.07613v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ScispaCy: Fast and Robust Models for Biomedical Natural Language\n  Processing", "abstract": "Despite recent advances in natural language processing, many statistical\nmodels for processing text perform extremely poorly under domain shift.\nProcessing biomedical and clinical text is a critically important application\narea of natural language processing, for which there are few robust, practical,\npublicly available models. This paper describes scispaCy, a new tool for\npractical biomedical/scientific text processing, which heavily leverages the\nspaCy library. We detail the performance of two packages of models released in\nscispaCy and demonstrate their robustness on several tasks and datasets. Models\nand code are available at https://allenai.github.io/scispacy/", "published": "2019-02-20 17:28:51", "link": "http://arxiv.org/abs/1902.07669v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Dual Retrieval Module for Semi-supervised Relation Extraction", "abstract": "Relation extraction is an important task in structuring content of text data,\nand becomes especially challenging when learning with weak supervision---where\nonly a limited number of labeled sentences are given and a large number of\nunlabeled sentences are available. Most existing work exploits unlabeled data\nbased on the ideas of self-training (i.e., bootstrapping a model) and\nmulti-view learning (e.g., ensembling multiple model variants). However, these\nmethods either suffer from the issue of semantic drift, or do not fully capture\nthe problem characteristics of relation extraction. In this paper, we leverage\na key insight that retrieving sentences expressing a relation is a dual task of\npredicting relation label for a given sentence---two tasks are complementary to\neach other and can be optimized jointly for mutual enhancement. To model this\nintuition, we propose DualRE, a principled framework that introduces a\nretrieval module which is jointly trained with the original relation prediction\nmodule. In this way, high-quality samples selected by retrieval module from\nunlabeled data can be used to improve prediction module, and vice versa.\nExperimental results\\footnote{\\small Code and data can be found at\n\\url{https://github.com/INK-USC/DualRE}.} on two public datasets as well as\ncase studies demonstrate the effectiveness of the DualRE approach.", "published": "2019-02-20 23:56:21", "link": "http://arxiv.org/abs/1902.07814v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mixture Models for Diverse Machine Translation: Tricks of the Trade", "abstract": "Mixture models trained via EM are among the simplest, most widely used and\nwell understood latent variable models in the machine learning literature.\nSurprisingly, these models have been hardly explored in text generation\napplications such as machine translation. In principle, they provide a latent\nvariable to control generation and produce a diverse set of hypotheses. In\npractice, however, mixture models are prone to degeneracies---often only one\ncomponent gets trained or the latent variable is simply ignored. We find that\ndisabling dropout noise in responsibility computation is critical to successful\ntraining. In addition, the design choices of parameterization, prior\ndistribution, hard versus soft EM and online versus offline assignment can\ndramatically affect model performance. We develop an evaluation protocol to\nassess both quality and diversity of generations against multiple references,\nand provide an extensive empirical study of several mixture model variants. Our\nanalysis shows that certain types of mixture models are more robust and offer\nthe best trade-off between translation quality and diversity compared to\nvariational models and diverse decoding approaches.\\footnote{Code to reproduce\nthe results in this paper is available at\n\\url{https://github.com/pytorch/fairseq}}", "published": "2019-02-20 23:57:35", "link": "http://arxiv.org/abs/1902.07816v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Emergence of order in random languages", "abstract": "We consider languages generated by weighted context-free grammars. It is\nshown that the behaviour of large texts is controlled by saddle-point equations\nfor an appropriate generating function. We then consider ensembles of grammars,\nin particular the Random Language Model of E. DeGiuli, Phys. Rev. Lett., 122,\n128301, 2019. This model is solved in the replica-symmetric ansatz, which is\nvalid in the high-temperature, disordered phase. It is shown that in the phase\nin which languages carry information, the replica symmetry must be broken.", "published": "2019-02-20 11:30:17", "link": "http://arxiv.org/abs/1902.07516v3", "categories": ["cond-mat.dis-nn", "cs.CL", "cs.FL"], "primary_category": "cond-mat.dis-nn"}
{"title": "Audio-Linguistic Embeddings for Spoken Sentences", "abstract": "We propose spoken sentence embeddings which capture both acoustic and\nlinguistic content. While existing works operate at the character, phoneme, or\nword level, our method learns long-term dependencies by modeling speech at the\nsentence level. Formulated as an audio-linguistic multitask learning problem,\nour encoder-decoder model simultaneously reconstructs acoustic and natural\nlanguage features from audio. Our results show that spoken sentence embeddings\noutperform phoneme and word-level baselines on speech recognition and emotion\nrecognition tasks. Ablation studies show that our embeddings can better model\nhigh-level acoustic concepts while retaining linguistic content. Overall, our\nwork illustrates the viability of generic, multi-modal sentence embeddings for\nspoken language understanding.", "published": "2019-02-20 23:58:29", "link": "http://arxiv.org/abs/1902.07817v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Utterance-level end-to-end language identification using attention-based\n  CNN-BLSTM", "abstract": "In this paper, we present an end-to-end language identification framework,\nthe attention-based Convolutional Neural Network-Bidirectional Long-short Term\nMemory (CNN-BLSTM). The model is performed on the utterance level, which means\nthe utterance-level decision can be directly obtained from the output of the\nneural network. To handle speech utterances with entire arbitrary and\npotentially long duration, we combine CNN-BLSTM model with a self-attentive\npooling layer together. The front-end CNN-BLSTM module plays a role as local\npattern extractor for the variable-length inputs, and the following\nself-attentive pooling layer is built on top to get the fixed-dimensional\nutterance-level representation. We conducted experiments on NIST LRE07\nclosed-set task, and the results reveal that the proposed attention-based\nCNN-BLSTM model achieves comparable error reduction with other state-of-the-art\nutterance-level neural network approaches for all 3 seconds, 10 seconds, 30\nseconds duration tasks.", "published": "2019-02-20 02:14:35", "link": "http://arxiv.org/abs/1902.07374v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
