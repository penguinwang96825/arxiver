{"title": "Ethical Considerations for Machine Translation of Indigenous Languages:\n  Giving a Voice to the Speakers", "abstract": "In recent years machine translation has become very successful for\nhigh-resource language pairs. This has also sparked new interest in research on\nthe automatic translation of low-resource languages, including Indigenous\nlanguages. However, the latter are deeply related to the ethnic and cultural\ngroups that speak (or used to speak) them. The data collection, modeling and\ndeploying machine translation systems thus result in new ethical questions that\nmust be addressed. Motivated by this, we first survey the existing literature\non ethical considerations for the documentation, translation, and general\nnatural language processing for Indigenous languages. Afterward, we conduct and\nanalyze an interview study to shed light on the positions of community leaders,\nteachers, and language activists regarding ethical concerns for the automatic\ntranslation of their languages. Our results show that the inclusion, at\ndifferent degrees, of native speakers and community members is vital to\nperforming better and more ethical research on Indigenous languages.", "published": "2023-05-31 01:04:20", "link": "http://arxiv.org/abs/2305.19474v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Flow Graph Prediction of Open-Domain Procedural Texts", "abstract": "Machine comprehension of procedural texts is essential for reasoning about\nthe steps and automating the procedures. However, this requires identifying\nentities within a text and resolving the relationships between the entities.\nPrevious work focused on the cooking domain and proposed a framework to convert\na recipe text into a flow graph (FG) representation. In this work, we propose a\nframework based on the recipe FG for flow graph prediction of open-domain\nprocedural texts. To investigate flow graph prediction performance in\nnon-cooking domains, we introduce the wikiHow-FG corpus from articles on\nwikiHow, a website of how-to instruction articles. In experiments, we consider\nusing the existing recipe corpus and performing domain adaptation from the\ncooking to the target domain. Experimental results show that the domain\nadaptation models achieve higher performance than those trained only on the\ncooking or target domain data.", "published": "2023-05-31 02:15:15", "link": "http://arxiv.org/abs/2305.19497v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Lottery Prompts for Pre-trained Language Models", "abstract": "Consistently scaling pre-trained language models (PLMs) imposes substantial\nburdens on model adaptation, necessitating more efficient alternatives to\nconventional fine-tuning. Given the advantage of prompting in the zero-shot\nsetting and the observed performance fluctuation among different prompts, we\nexplore the instance-level prompt and their generalizability. By searching\nthrough the prompt space, we first validate the assumption that for every\ninstance, there is almost always a lottery prompt that induces the correct\nprediction from the PLM, and such prompt can be obtained at a low cost thanks\nto the inherent ability of PLMs. Meanwhile, we find that some strong lottery\nprompts have high performance over the whole training set, and they are\nequipped with distinguishable linguistic features. Lastly, we attempt to\ngeneralize the searched strong lottery prompts to unseen data with prompt\nensembling method without any parameter tuning. Experiments are conducted on\nvarious types of NLP classification tasks and demonstrate that the proposed\nmethod can achieve comparable results with other gradient-free and\noptimization-free baselines.", "published": "2023-05-31 02:17:04", "link": "http://arxiv.org/abs/2305.19500v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Accurate and Structured Pruning for Efficient Automatic Speech\n  Recognition", "abstract": "Automatic Speech Recognition (ASR) has seen remarkable advancements with deep\nneural networks, such as Transformer and Conformer. However, these models\ntypically have large model sizes and high inference costs, posing a challenge\nto deploy on resource-limited devices. In this paper, we propose a novel\ncompression strategy that leverages structured pruning and knowledge\ndistillation to reduce the model size and inference cost of the Conformer model\nwhile preserving high recognition performance. Our approach utilizes a set of\nbinary masks to indicate whether to retain or prune each Conformer module, and\nemploys L0 regularization to learn the optimal mask values. To further enhance\npruning performance, we use a layerwise distillation strategy to transfer\nknowledge from unpruned to pruned models. Our method outperforms all pruning\nbaselines on the widely used LibriSpeech benchmark, achieving a 50% reduction\nin model size and a 28% reduction in inference cost with minimal performance\nloss.", "published": "2023-05-31 04:31:16", "link": "http://arxiv.org/abs/2305.19549v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SLABERT Talk Pretty One Day: Modeling Second Language Acquisition with\n  BERT", "abstract": "Second language acquisition (SLA) research has extensively studied\ncross-linguistic transfer, the influence of linguistic structure of a speaker's\nnative language [L1] on the successful acquisition of a foreign language [L2].\nEffects of such transfer can be positive (facilitating acquisition) or negative\n(impeding acquisition). We find that NLP literature has not given enough\nattention to the phenomenon of negative transfer. To understand patterns of\nboth positive and negative transfer between L1 and L2, we model sequential\nsecond language acquisition in LMs. Further, we build a Mutlilingual Age\nOrdered CHILDES (MAO-CHILDES) -- a dataset consisting of 5 typologically\ndiverse languages, i.e., German, French, Polish, Indonesian, and Japanese -- to\nunderstand the degree to which native Child-Directed Speech (CDS) [L1] can help\nor conflict with English language acquisition [L2]. To examine the impact of\nnative CDS, we use the TILT-based cross lingual transfer learning approach\nestablished by Papadimitriou and Jurafsky (2020) and find that, as in human\nSLA, language family distance predicts more negative transfer. Additionally, we\nfind that conversational speech data shows greater facilitation for language\nacquisition than scripted speech data. Our findings call for further research\nusing our novel Transformer-based SLA models and we would like to encourage it\nby releasing our code, data, and models.", "published": "2023-05-31 06:22:07", "link": "http://arxiv.org/abs/2305.19589v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adverbs, Surprisingly", "abstract": "This paper begins with the premise that adverbs are neglected in\ncomputational linguistics. This view derives from two analyses: a literature\nreview and a novel adverb dataset to probe a state-of-the-art language model,\nthereby uncovering systematic gaps in accounts for adverb meaning. We suggest\nthat using Frame Semantics for characterizing word meaning, as in FrameNet,\nprovides a promising approach to adverb analysis, given its ability to describe\nambiguity, semantic roles, and null instantiation.", "published": "2023-05-31 08:30:08", "link": "http://arxiv.org/abs/2305.19650v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assessing Word Importance Using Models Trained for Semantic Tasks", "abstract": "Many NLP tasks require to automatically identify the most significant words\nin a text. In this work, we derive word significance from models trained to\nsolve semantic task: Natural Language Inference and Paraphrase Identification.\nUsing an attribution method aimed to explain the predictions of these models,\nwe derive importance scores for each input token. We evaluate their relevance\nusing a so-called cross-task evaluation: Analyzing the performance of one model\non an input masked according to the other model's weight, we show that our\nmethod is robust with respect to the choice of the initial task. Additionally,\nwe investigate the scores from the syntax point of view and observe interesting\npatterns, e.g. words closer to the root of a syntactic tree receive higher\nimportance scores. Altogether, these observations suggest that our method can\nbe used to identify important words in sentences without any explicit word\nimportance labeling in training.", "published": "2023-05-31 09:34:26", "link": "http://arxiv.org/abs/2305.19689v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Building Extractive Question Answering System to Support Human-AI Health\n  Coaching Model for Sleep Domain", "abstract": "Non-communicable diseases (NCDs) are a leading cause of global deaths,\nnecessitating a focus on primary prevention and lifestyle behavior change.\nHealth coaching, coupled with Question Answering (QA) systems, has the\npotential to transform preventive healthcare. This paper presents a\nhuman-Artificial Intelligence (AI) health coaching model incorporating a\ndomain-specific extractive QA system. A sleep-focused dataset, SleepQA, was\nmanually assembled and used to fine-tune domain-specific BERT models. The QA\nsystem was evaluated using automatic and human methods. A data-centric\nframework enhanced the system's performance by improving passage retrieval and\nquestion reformulation. Although the system did not outperform the baseline in\nautomatic evaluation, it excelled in the human evaluation of real-world\nquestions. Integration into a Human-AI health coaching model was tested in a\npilot Randomized Controlled Trial (RCT).", "published": "2023-05-31 10:03:18", "link": "http://arxiv.org/abs/2305.19707v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analyzing Text Representations by Measuring Task Alignment", "abstract": "Textual representations based on pre-trained language models are key,\nespecially in few-shot learning scenarios. What makes a representation good for\ntext classification? Is it due to the geometric properties of the space or\nbecause it is well aligned with the task? We hypothesize the second claim. To\ntest it, we develop a task alignment score based on hierarchical clustering\nthat measures alignment at different levels of granularity. Our experiments on\ntext classification validate our hypothesis by showing that task alignment can\nexplain the classification performance of a given representation.", "published": "2023-05-31 11:20:48", "link": "http://arxiv.org/abs/2305.19747v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UKP-SQuARE: An Interactive Tool for Teaching Question Answering", "abstract": "The exponential growth of question answering (QA) has made it an\nindispensable topic in any Natural Language Processing (NLP) course.\nAdditionally, the breadth of QA derived from this exponential growth makes it\nan ideal scenario for teaching related NLP topics such as information\nretrieval, explainability, and adversarial attacks among others. In this paper,\nwe introduce UKP-SQuARE as a platform for QA education. This platform provides\nan interactive environment where students can run, compare, and analyze various\nQA models from different perspectives, such as general behavior,\nexplainability, and robustness. Therefore, students can get a first-hand\nexperience in different QA techniques during the class. Thanks to this, we\npropose a learner-centered approach for QA education in which students\nproactively learn theoretical concepts and acquire problem-solving skills\nthrough interactive exploration, experimentation, and practical assignments,\nrather than solely relying on traditional lectures. To evaluate the\neffectiveness of UKP-SQuARE in teaching scenarios, we adopted it in a\npostgraduate NLP course and surveyed the students after the course. Their\npositive feedback shows the platform's effectiveness in their course and\ninvites a wider adoption.", "published": "2023-05-31 11:29:04", "link": "http://arxiv.org/abs/2305.19748v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentence Simplification Using Paraphrase Corpus for Initialization", "abstract": "Neural sentence simplification method based on sequence-to-sequence framework\nhas become the mainstream method for sentence simplification (SS) task.\nUnfortunately, these methods are currently limited by the scarcity of parallel\nSS corpus. In this paper, we focus on how to reduce the dependence on parallel\ncorpus by leveraging a careful initialization for neural SS methods from\nparaphrase corpus. Our work is motivated by the following two findings: (1)\nParaphrase corpus includes a large proportion of sentence pairs belonging to SS\ncorpus. (2) We can construct large-scale pseudo parallel SS data by keeping\nthese sentence pairs with a higher complexity difference. Therefore, we propose\ntwo strategies to initialize neural SS methods using paraphrase corpus. We\ntrain three different neural SS methods with our initialization, which can\nobtain substantial improvements on the available WikiLarge data compared with\nthemselves without initialization.", "published": "2023-05-31 11:39:10", "link": "http://arxiv.org/abs/2305.19754v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Discrimination of Human and Neural Machine Translation in\n  Multilingual Scenarios", "abstract": "We tackle the task of automatically discriminating between human and machine\ntranslations. As opposed to most previous work, we perform experiments in a\nmultilingual setting, considering multiple languages and multilingual\npretrained language models. We show that a classifier trained on parallel data\nwith a single source language (in our case German-English) can still perform\nwell on English translations that come from different source languages, even\nwhen the machine translations were produced by other systems than the one it\nwas trained on. Additionally, we demonstrate that incorporating the source text\nin the input of a multilingual classifier improves (i) its accuracy and (ii)\nits robustness on cross-system evaluation, compared to a monolingual\nclassifier. Furthermore, we find that using training data from multiple source\nlanguages (German, Russian, and Chinese) tends to improve the accuracy of both\nmonolingual and multilingual classifiers. Finally, we show that bilingual\nclassifiers and classifiers trained on multiple source languages benefit from\nbeing trained on longer text sequences, rather than on sentences.", "published": "2023-05-31 11:41:24", "link": "http://arxiv.org/abs/2305.19757v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IDAS: Intent Discovery with Abstractive Summarization", "abstract": "Intent discovery is the task of inferring latent intents from a set of\nunlabeled utterances, and is a useful step towards the efficient creation of\nnew conversational agents. We show that recent competitive methods in intent\ndiscovery can be outperformed by clustering utterances based on abstractive\nsummaries, i.e., \"labels\", that retain the core elements while removing\nnon-essential information. We contribute the IDAS approach, which collects a\nset of descriptive utterance labels by prompting a Large Language Model,\nstarting from a well-chosen seed set of prototypical utterances, to bootstrap\nan In-Context Learning procedure to generate labels for non-prototypical\nutterances. The utterances and their resulting noisy labels are then encoded by\na frozen pre-trained encoder, and subsequently clustered to recover the latent\nintents. For the unsupervised task (without any intent labels) IDAS outperforms\nthe state-of-the-art by up to +7.42% in standard cluster metrics for the\nBanking, StackOverflow, and Transport datasets. For the semi-supervised task\n(with labels for a subset of intents) IDAS surpasses 2 recent methods on the\nCLINC benchmark without even using labeled data.", "published": "2023-05-31 12:19:40", "link": "http://arxiv.org/abs/2305.19783v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Guiding Computational Stance Detection with Expanded Stance Triangle\n  Framework", "abstract": "Stance detection determines whether the author of a piece of text is in favor\nof, against, or neutral towards a specified target, and can be used to gain\nvaluable insights into social media. The ubiquitous indirect referral of\ntargets makes this task challenging, as it requires computational solutions to\nmodel semantic features and infer the corresponding implications from a literal\nstatement. Moreover, the limited amount of available training data leads to\nsubpar performance in out-of-domain and cross-target scenarios, as data-driven\napproaches are prone to rely on superficial and domain-specific features. In\nthis work, we decompose the stance detection task from a linguistic\nperspective, and investigate key components and inference paths in this task.\nThe stance triangle is a generic linguistic framework previously proposed to\ndescribe the fundamental ways people express their stance. We further expand it\nby characterizing the relationship between explicit and implicit objects. We\nthen use the framework to extend one single training corpus with additional\nannotation. Experimental results show that strategically-enriched data can\nsignificantly improve the performance on out-of-domain and cross-target\nevaluation.", "published": "2023-05-31 13:33:29", "link": "http://arxiv.org/abs/2305.19845v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TPDM: Selectively Removing Positional Information for Zero-shot\n  Translation via Token-Level Position Disentangle Module", "abstract": "Due to Multilingual Neural Machine Translation's (MNMT) capability of\nzero-shot translation, many works have been carried out to fully exploit the\npotential of MNMT in zero-shot translation. It is often hypothesized that\npositional information may hinder the MNMT from outputting a robust encoded\nrepresentation for decoding. However, previous approaches treat all the\npositional information equally and thus are unable to selectively remove\ncertain positional information. In sharp contrast, this paper investigates how\nto learn to selectively preserve useful positional information.\n  We describe the specific mechanism of positional information influencing MNMT\nfrom the perspective of linguistics at the token level. We design a token-level\nposition disentangle module (TPDM) framework to disentangle positional\ninformation at the token level based on the explanation. Our experiments\ndemonstrate that our framework improves zero-shot translation by a large margin\nwhile reducing the performance loss in the supervised direction compared to\nprevious works.", "published": "2023-05-31 13:48:45", "link": "http://arxiv.org/abs/2305.19857v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AQE: Argument Quadruplet Extraction via a Quad-Tagging Augmented\n  Generative Approach", "abstract": "Argument mining involves multiple sub-tasks that automatically identify\nargumentative elements, such as claim detection, evidence extraction, stance\nclassification, etc. However, each subtask alone is insufficient for a thorough\nunderstanding of the argumentative structure and reasoning process. To learn a\ncomplete view of an argument essay and capture the interdependence among\nargumentative components, we need to know what opinions people hold (i.e.,\nclaims), why those opinions are valid (i.e., supporting evidence), which source\nthe evidence comes from (i.e., evidence type), and how those claims react to\nthe debating topic (i.e., stance). In this work, we for the first time propose\na challenging argument quadruplet extraction task (AQE), which can provide an\nall-in-one extraction of four argumentative components, i.e., claims, evidence,\nevidence types, and stances. To support this task, we construct a large-scale\nand challenging dataset. However, there is no existing method that can solve\nthe argument quadruplet extraction. To fill this gap, we propose a novel\nquad-tagging augmented generative approach, which leverages a quadruplet\ntagging module to augment the training of the generative framework. The\nexperimental results on our dataset demonstrate the empirical superiority of\nour proposed approach over several strong baselines.", "published": "2023-05-31 14:35:53", "link": "http://arxiv.org/abs/2305.19902v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How to Plant Trees in Language Models: Data and Architectural Effects on\n  the Emergence of Syntactic Inductive Biases", "abstract": "Accurate syntactic representations are essential for robust generalization in\nnatural language. Recent work has found that pre-training can teach language\nmodels to rely on hierarchical syntactic features - as opposed to incorrect\nlinear features - when performing tasks after fine-tuning. We test what aspects\nof pre-training are important for endowing encoder-decoder Transformers with an\ninductive bias that favors hierarchical syntactic generalizations. We focus on\narchitectural features (depth, width, and number of parameters), as well as the\ngenre and size of the pre-training corpus, diagnosing inductive biases using\ntwo syntactic transformation tasks: question formation and passivization, both\nin English. We find that the number of parameters alone does not explain\nhierarchical generalization: model depth plays greater role than model width.\nWe also find that pre-training on simpler language, such as child-directed\nspeech, induces a hierarchical bias using an order-of-magnitude less data than\npre-training on more typical datasets based on web text or Wikipedia; this\nsuggests that in cognitively plausible language acquisition settings, neural\nlanguage models may be more data-efficient than previously thought.", "published": "2023-05-31 14:38:14", "link": "http://arxiv.org/abs/2305.19905v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting the Reliability of Psychological Scales on Large Language\n  Models", "abstract": "Recent research has focused on examining Large Language Models' (LLMs)\ncharacteristics from a psychological standpoint, acknowledging the necessity of\nunderstanding their behavioral characteristics. The administration of\npersonality tests to LLMs has emerged as a noteworthy area in this context.\nHowever, the suitability of employing psychological scales, initially devised\nfor humans, on LLMs is a matter of ongoing debate. Our study aims to determine\nthe reliability of applying personality assessments to LLMs, explicitly\ninvestigating whether LLMs demonstrate consistent personality traits. Analysis\nof 2,500 settings per model, including GPT-3.5, GPT-4, Gemini-Pro, and\nLLaMA-3.1, reveals that various LLMs show consistency in responses to the Big\nFive Inventory, indicating a satisfactory level of reliability. Furthermore,\nour research explores the potential of GPT-3.5 to emulate diverse personalities\nand represent various groups-a capability increasingly sought after in social\nsciences for substituting human participants with LLMs to reduce costs. Our\nfindings reveal that LLMs have the potential to represent different\npersonalities with specific prompt instructions.", "published": "2023-05-31 15:03:28", "link": "http://arxiv.org/abs/2305.19926v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Supplementary Features of BiLSTM for Enhanced Sequence Labeling", "abstract": "Sequence labeling tasks require the computation of sentence representations\nfor each word within a given sentence. A prevalent method incorporates a\nBi-directional Long Short-Term Memory (BiLSTM) layer to enhance the sequence\nstructure information. However, empirical evidence Li (2020) suggests that the\ncapacity of BiLSTM to produce sentence representations for sequence labeling\ntasks is inherently limited. This limitation primarily results from the\nintegration of fragments from past and future sentence representations to\nformulate a complete sentence representation. In this study, we observed that\nthe entire sentence representation, found in both the first and last cells of\nBiLSTM, can supplement each the individual sentence representation of each\ncell. Accordingly, we devised a global context mechanism to integrate entire\nfuture and past sentence representations into each cell's sentence\nrepresentation within the BiLSTM framework. By incorporating the BERT model\nwithin BiLSTM as a demonstration, and conducting exhaustive experiments on nine\ndatasets for sequence labeling tasks, including named entity recognition (NER),\npart of speech (POS) tagging, and End-to-End Aspect-Based sentiment analysis\n(E2E-ABSA). We noted significant improvements in F1 scores and accuracy across\nall examined datasets.", "published": "2023-05-31 15:05:25", "link": "http://arxiv.org/abs/2305.19928v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Correcting Semantic Parses with Natural Language through Dynamic Schema\n  Encoding", "abstract": "In addressing the task of converting natural language to SQL queries, there\nare several semantic and syntactic challenges. It becomes increasingly\nimportant to understand and remedy the points of failure as the performance of\nsemantic parsing systems improve. We explore semantic parse correction with\nnatural language feedback, proposing a new solution built on the success of\nautoregressive decoders in text-to-SQL tasks. By separating the semantic and\nsyntactic difficulties of the task, we show that the accuracy of text-to-SQL\nparsers can be boosted by up to 26% with only one turn of correction with\nnatural language. Additionally, we show that a T5-base model is capable of\ncorrecting the errors of a T5-large model in a zero-shot, cross-parser setting.", "published": "2023-05-31 16:01:57", "link": "http://arxiv.org/abs/2305.19974v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MedNgage: A Dataset for Understanding Engagement in Patient-Nurse\n  Conversations", "abstract": "Patients who effectively manage their symptoms often demonstrate higher\nlevels of engagement in conversations and interventions with healthcare\npractitioners. This engagement is multifaceted, encompassing cognitive and\nsocio-affective dimensions. Consequently, it is crucial for AI systems to\nunderstand the engagement in natural conversations between patients and\npractitioners to better contribute toward patient care. In this paper, we\npresent a novel dataset (MedNgage), which consists of patient-nurse\nconversations about cancer symptom management. We manually annotate the dataset\nwith a novel framework of categories of patient engagement from two different\nangles, namely: i) socio-affective (3.1K spans), and ii) cognitive use of\nlanguage (1.8K spans). Through statistical analysis of the data that is\nannotated using our framework, we show a positive correlation between patient\nsymptom management outcomes and their engagement in conversations.\nAdditionally, we demonstrate that pre-trained transformer models fine-tuned on\nour dataset can reliably predict engagement classes in patient-nurse\nconversations. Lastly, we use LIME (Ribeiro et al., 2016) to analyze the\nunderlying challenges of the tasks that state-of-the-art transformer models\nencounter. The de-identified data is available for research purposes upon\nrequest.", "published": "2023-05-31 16:06:07", "link": "http://arxiv.org/abs/2305.19981v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assessing Language Disorders using Artificial Intelligence: a Paradigm\n  Shift", "abstract": "Speech, language, and communication deficits are present in most\nneurodegenerative syndromes. They enable the early detection, diagnosis,\ntreatment planning, and monitoring of neurocognitive disease progression as\npart of traditional neurological assessment. Nevertheless, standard speech and\nlanguage evaluation is time-consuming and resource-intensive for clinicians. We\nargue that using machine learning methodologies, natural language processing,\nand modern artificial intelligence (AI) for Language Assessment is an\nimprovement over conventional manual assessment. Using these methodologies,\nComputational Language Assessment (CLA) accomplishes three goals: (i) provides\na neuro-cognitive evaluation of speech, language, and communication in elderly\nand high-risk individuals for dementia; (ii) facilitates the diagnosis,\nprognosis, and therapy efficacy in at-risk and language-impaired populations;\nand (iii) allows easier extensibility to assess patients from a wide range of\nlanguages. By employing AI models, CLA may inform neurocognitive theory on the\nrelationship between language symptoms and their neural bases. Finally, it\nsignals a paradigm shift by significantly advancing our ability to optimize the\nprevention and treatment of elderly individuals with communication disorders,\nallowing them to age gracefully with social engagement.", "published": "2023-05-31 17:20:45", "link": "http://arxiv.org/abs/2305.20046v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Findings of the VarDial Evaluation Campaign 2023", "abstract": "This report presents the results of the shared tasks organized as part of the\nVarDial Evaluation Campaign 2023. The campaign is part of the tenth workshop on\nNatural Language Processing (NLP) for Similar Languages, Varieties and Dialects\n(VarDial), co-located with EACL 2023. Three separate shared tasks were included\nthis year: Slot and intent detection for low-resource language varieties\n(SID4LR), Discriminating Between Similar Languages -- True Labels (DSL-TL), and\nDiscriminating Between Similar Languages -- Speech (DSL-S). All three tasks\nwere organized for the first time this year.", "published": "2023-05-31 17:55:21", "link": "http://arxiv.org/abs/2305.20080v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MetaXLR -- Mixed Language Meta Representation Transformation for\n  Low-resource Cross-lingual Learning based on Multi-Armed Bandit", "abstract": "Transfer learning for extremely low resource languages is a challenging task\nas there is no large scale monolingual corpora for pre training or sufficient\nannotated data for fine tuning. We follow the work of MetaXL which suggests\nusing meta learning for transfer learning from a single source language to an\nextremely low resource one. We propose an enhanced approach which uses multiple\nsource languages chosen in a data driven manner. In addition, we introduce a\nsample selection strategy for utilizing the languages in training by using a\nmulti armed bandit algorithm. Using both of these improvements we managed to\nachieve state of the art results on the NER task for the extremely low resource\nlanguages while using the same amount of data, making the representations\nbetter generalized. Also, due to the method ability to use multiple languages\nit allows the framework to use much larger amounts of data, while still having\nsuperior results over the former MetaXL method even with the same amounts of\ndata.", "published": "2023-05-31 18:22:33", "link": "http://arxiv.org/abs/2306.00100v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Multi-Figurative Language Detection", "abstract": "Figures of speech help people express abstract concepts and evoke stronger\nemotions than literal expressions, thereby making texts more creative and\nengaging. Due to its pervasive and fundamental character, figurative language\nunderstanding has been addressed in Natural Language Processing, but it's\nhighly understudied in a multilingual setting and when considering more than\none figure of speech at the same time. To bridge this gap, we introduce\nmultilingual multi-figurative language modelling, and provide a benchmark for\nsentence-level figurative language detection, covering three common figures of\nspeech and seven languages. Specifically, we develop a framework for figurative\nlanguage detection based on template-based prompt learning. In so doing, we\nunify multiple detection tasks that are interrelated across multiple figures of\nspeech and languages, without requiring task- or language-specific modules.\nExperimental results show that our framework outperforms several strong\nbaselines and may serve as a blueprint for the joint modelling of other\ninterrelated tasks.", "published": "2023-05-31 18:52:41", "link": "http://arxiv.org/abs/2306.00121v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pre-Trained Language-Meaning Models for Multilingual Parsing and\n  Generation", "abstract": "Pre-trained language models (PLMs) have achieved great success in NLP and\nhave recently been used for tasks in computational semantics. However, these\ntasks do not fully benefit from PLMs since meaning representations are not\nexplicitly included in the pre-training stage. We introduce multilingual\npre-trained language-meaning models based on Discourse Representation\nStructures (DRSs), including meaning representations besides natural language\ntexts in the same model, and design a new strategy to reduce the gap between\nthe pre-training and fine-tuning objectives. Since DRSs are language neutral,\ncross-lingual transfer learning is adopted to further improve the performance\nof non-English tasks. Automatic evaluation results show that our approach\nachieves the best performance on both the multilingual DRS parsing and\nDRS-to-text generation tasks. Correlation analysis between automatic metrics\nand human judgements on the generation task further validates the effectiveness\nof our model. Human inspection reveals that out-of-vocabulary tokens are the\nmain cause of erroneous results.", "published": "2023-05-31 19:00:33", "link": "http://arxiv.org/abs/2306.00124v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Sequence-to-Sequence&Set Model for Text-to-Table Generation", "abstract": "Recently, the text-to-table generation task has attracted increasing\nattention due to its wide applications. In this aspect, the dominant model\nformalizes this task as a sequence-to-sequence generation task and serializes\neach table into a token sequence during training by concatenating all rows in a\ntop-down order. However, it suffers from two serious defects: 1) the predefined\norder introduces a wrong bias during training, which highly penalizes shifts in\nthe order between rows; 2) the error propagation problem becomes serious when\nthe model outputs a long token sequence. In this paper, we first conduct a\npreliminary study to demonstrate the generation of most rows is\norder-insensitive. Furthermore, we propose a novel sequence-to-sequence&set\ntext-to-table generation model. Specifically, in addition to a text encoder\nencoding the input text, our model is equipped with a table header generator to\nfirst output a table header, i.e., the first row of the table, in the manner of\nsequence generation. Then we use a table body generator with learnable row\nembeddings and column embeddings to generate a set of table body rows in\nparallel. Particularly, to deal with the issue that there is no correspondence\nbetween each generated table body row and target during training, we propose a\ntarget assignment strategy based on the bipartite matching between the first\ncells of generated table body rows and targets. Experiment results show that\nour model significantly surpasses the baselines, achieving state-of-the-art\nperformance on commonly-used datasets.", "published": "2023-05-31 19:28:00", "link": "http://arxiv.org/abs/2306.00137v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Measuring the Robustness of NLP Models to Domain Shifts", "abstract": "Existing research on Domain Robustness (DR) suffers from disparate setups,\nlimited task variety, and scarce research on recent capabilities such as\nin-context learning. Furthermore, the common practice of measuring DR might not\nbe fully accurate. Current research focuses on challenge sets and relies solely\non the Source Drop (SD): Using the source in-domain performance as a reference\npoint for degradation. However, we argue that the Target Drop (TD), which\nmeasures degradation from the target in-domain performance, should be used as a\ncomplementary point of view. To address these issues, we first curated a DR\nbenchmark comprised of 7 diverse NLP tasks, which enabled us to measure both\nthe SD and the TD. We then conducted a comprehensive large-scale DR study\ninvolving over 14,000 domain shifts across 21 fine-tuned models and few-shot\nLLMs. We found that both model types suffer from drops upon domain shifts.\nWhile fine-tuned models excel in-domain, few-shot LLMs often surpass them\ncross-domain, showing better robustness. In addition, we found that a large SD\ncan often be explained by shifting to a harder domain rather than by a genuine\nDR challenge, and this highlights the importance of TD as a complementary\nmetric. We hope our study will shed light on the current DR state of NLP models\nand promote improved evaluation practices toward more robust models.", "published": "2023-05-31 20:25:08", "link": "http://arxiv.org/abs/2306.00168v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contrastive Hierarchical Discourse Graph for Scientific Document\n  Summarization", "abstract": "The extended structural context has made scientific paper summarization a\nchallenging task. This paper proposes CHANGES, a contrastive hierarchical graph\nneural network for extractive scientific paper summarization. CHANGES\nrepresents a scientific paper with a hierarchical discourse graph and learns\neffective sentence representations with dedicated designed hierarchical graph\ninformation aggregation. We also propose a graph contrastive learning module to\nlearn global theme-aware sentence representations. Extensive experiments on the\nPubMed and arXiv benchmark datasets prove the effectiveness of CHANGES and the\nimportance of capturing hierarchical structure information in modeling\nscientific papers.", "published": "2023-05-31 20:54:43", "link": "http://arxiv.org/abs/2306.00177v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Factually Consistent Summarization via Reinforcement Learning with\n  Textual Entailment Feedback", "abstract": "Despite the seeming success of contemporary grounded text generation systems,\nthey often tend to generate factually inconsistent text with respect to their\ninput. This phenomenon is emphasized in tasks like summarization, in which the\ngenerated summaries should be corroborated by their source article. In this\nwork, we leverage recent progress on textual entailment models to directly\naddress this problem for abstractive summarization systems. We use\nreinforcement learning with reference-free, textual entailment rewards to\noptimize for factual consistency and explore the ensuing trade-offs, as\nimproved consistency may come at the cost of less informative or more\nextractive summaries. Our results, according to both automatic metrics and\nhuman evaluation, show that our method considerably improves the faithfulness,\nsalience, and conciseness of the generated summaries.", "published": "2023-05-31 21:04:04", "link": "http://arxiv.org/abs/2306.00186v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FEED PETs: Further Experimentation and Expansion on the Disambiguation\n  of Potentially Euphemistic Terms", "abstract": "Transformers have been shown to work well for the task of English euphemism\ndisambiguation, in which a potentially euphemistic term (PET) is classified as\neuphemistic or non-euphemistic in a particular context. In this study, we\nexpand on the task in two ways. First, we annotate PETs for vagueness, a\nlinguistic property associated with euphemisms, and find that transformers are\ngenerally better at classifying vague PETs, suggesting linguistic differences\nin the data that impact performance. Second, we present novel euphemism corpora\nin three different languages: Yoruba, Spanish, and Mandarin Chinese. We perform\neuphemism disambiguation experiments in each language using multilingual\ntransformer models mBERT and XLM-RoBERTa, establishing preliminary results from\nwhich to launch future work.", "published": "2023-05-31 22:23:20", "link": "http://arxiv.org/abs/2306.00217v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models Are Not Strong Abstract Reasoners", "abstract": "Large Language Models have shown tremendous performance on a large variety of\nnatural language processing tasks, ranging from text comprehension to common\nsense reasoning. However, the mechanisms responsible for this success remain\nopaque, and it is unclear whether LLMs can achieve human-like cognitive\ncapabilities or whether these models are still fundamentally circumscribed.\nAbstract reasoning is a fundamental task for cognition, consisting of finding\nand applying a general pattern from few data. Evaluating deep neural\narchitectures on this task could give insight into their potential limitations\nregarding reasoning and their broad generalisation abilities, yet this is\ncurrently an under-explored area. In this paper, we introduce a new benchmark\nfor evaluating language models beyond memorization on abstract reasoning tasks.\nWe perform extensive evaluations of state-of-the-art LLMs, showing that they\ncurrently achieve very limited performance in contrast with other natural\nlanguage tasks, even when applying techniques that have been shown to improve\nperformance on other NLP tasks. We argue that guiding LLM generation to follow\ncausal paths could help improve the generalisation and reasoning abilities of\nLLMs.", "published": "2023-05-31 04:50:29", "link": "http://arxiv.org/abs/2305.19555v3", "categories": ["cs.CL", "cs.LG", "I.2.2; I.2.3; I.2.7; I.5.1"], "primary_category": "cs.CL"}
{"title": "The Tag-Team Approach: Leveraging CLS and Language Tagging for Enhancing\n  Multilingual ASR", "abstract": "Building a multilingual Automated Speech Recognition (ASR) system in a\nlinguistically diverse country like India can be a challenging task due to the\ndifferences in scripts and the limited availability of speech data. This\nproblem can be solved by exploiting the fact that many of these languages are\nphonetically similar. These languages can be converted into a Common Label Set\n(CLS) by mapping similar sounds to common labels. In this paper, new approaches\nare explored and compared to improve the performance of CLS based multilingual\nASR model. Specific language information is infused in the ASR model by giving\nLanguage ID or using CLS to Native script converter on top of the CLS\nMultilingual model. These methods give a significant improvement in Word Error\nRate (WER) compared to the CLS baseline. These methods are further tried on\nout-of-distribution data to check their robustness.", "published": "2023-05-31 06:09:11", "link": "http://arxiv.org/abs/2305.19584v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "LAIT: Efficient Multi-Segment Encoding in Transformers with\n  Layer-Adjustable Interaction", "abstract": "Transformer encoders contextualize token representations by attending to all\nother tokens at each layer, leading to quadratic increase in compute effort\nwith the input length. In practice, however, the input text of many NLP tasks\ncan be seen as a sequence of related segments (e.g., the sequence of sentences\nwithin a passage, or the hypothesis and premise in NLI). While attending across\nthese segments is highly beneficial for many tasks, we hypothesize that this\ninteraction can be delayed until later encoding stages.\n  To this end, we introduce Layer-Adjustable Interactions in Transformers\n(LAIT). Within LAIT, segmented inputs are first encoded independently, and then\njointly. This partial two-tower architecture bridges the gap between a Dual\nEncoder's ability to pre-compute representations for segments and a fully\nself-attentive Transformer's capacity to model cross-segment attention. The\nLAIT framework effectively leverages existing pretrained Transformers and\nconverts them into the hybrid of the two aforementioned architectures, allowing\nfor easy and intuitive control over the performance-efficiency tradeoff.\nExperimenting on a wide range of NLP tasks, we find LAIT able to reduce 30-50%\nof the attention FLOPs on many tasks, while preserving high accuracy; in some\npractical settings, LAIT could reduce actual latency by orders of magnitude.", "published": "2023-05-31 06:09:59", "link": "http://arxiv.org/abs/2305.19585v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "What does the Failure to Reason with \"Respectively\" in Zero/Few-Shot\n  Settings Tell Us about Language Models?", "abstract": "Humans can effortlessly understand the coordinate structure of sentences such\nas \"Niels Bohr and Kurt Cobain were born in Copenhagen and Seattle,\nrespectively\". In the context of natural language inference (NLI), we examine\nhow language models (LMs) reason with respective readings (Gawron and Kehler,\n2004) from two perspectives: syntactic-semantic and commonsense-world\nknowledge. We propose a controlled synthetic dataset WikiResNLI and a naturally\noccurring dataset NatResNLI to encompass various explicit and implicit\nrealizations of \"respectively\". We show that fine-tuned NLI models struggle\nwith understanding such readings without explicit supervision. While few-shot\nlearning is easy in the presence of explicit cues, longer training is required\nwhen the reading is evoked implicitly, leaving models to rely on common sense\ninferences. Furthermore, our fine-grained analysis indicates models fail to\ngeneralize across different constructions. To conclude, we demonstrate that LMs\nstill lag behind humans in generalizing to the long tail of linguistic\nconstructions.", "published": "2023-05-31 06:45:09", "link": "http://arxiv.org/abs/2305.19597v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Adversarial Clean Label Backdoor Attacks and Defenses on Text\n  Classification Systems", "abstract": "Clean-label (CL) attack is a form of data poisoning attack where an adversary\nmodifies only the textual input of the training data, without requiring access\nto the labeling function. CL attacks are relatively unexplored in NLP, as\ncompared to label flipping (LF) attacks, where the latter additionally requires\naccess to the labeling function as well. While CL attacks are more resilient to\ndata sanitization and manual relabeling methods than LF attacks, they often\ndemand as high as ten times the poisoning budget than LF attacks. In this work,\nwe first introduce an Adversarial Clean Label attack which can adversarially\nperturb in-class training examples for poisoning the training set. We then show\nthat an adversary can significantly bring down the data requirements for a CL\nattack, using the aforementioned approach, to as low as 20% of the data\notherwise required. We then systematically benchmark and analyze a number of\ndefense methods, for both LF and CL attacks, some previously employed solely\nfor LF attacks in the textual domain and others adapted from computer vision.\nWe find that text-specific defenses greatly vary in their effectiveness\ndepending on their properties.", "published": "2023-05-31 07:23:46", "link": "http://arxiv.org/abs/2305.19607v1", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Red Teaming Language Model Detectors with Language Models", "abstract": "The prevalence and strong capability of large language models (LLMs) present\nsignificant safety and ethical risks if exploited by malicious users. To\nprevent the potentially deceptive usage of LLMs, recent works have proposed\nalgorithms to detect LLM-generated text and protect LLMs. In this paper, we\ninvestigate the robustness and reliability of these LLM detectors under\nadversarial attacks. We study two types of attack strategies: 1) replacing\ncertain words in an LLM's output with their synonyms given the context; 2)\nautomatically searching for an instructional prompt to alter the writing style\nof the generation. In both strategies, we leverage an auxiliary LLM to generate\nthe word replacements or the instructional prompt. Different from previous\nworks, we consider a challenging setting where the auxiliary LLM can also be\nprotected by a detector. Experiments reveal that our attacks effectively\ncompromise the performance of all detectors in the study with plausible\ngenerations, underscoring the urgent need to improve the robustness of\nLLM-generated text detection systems.", "published": "2023-05-31 10:08:37", "link": "http://arxiv.org/abs/2305.19713v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Simple yet Effective Code-Switching Language Identification with\n  Multitask Pre-Training and Transfer Learning", "abstract": "Code-switching, also called code-mixing, is the linguistics phenomenon where\nin casual settings, multilingual speakers mix words from different languages in\none utterance. Due to its spontaneous nature, code-switching is extremely\nlow-resource, which makes it a challenging problem for language and speech\nprocessing tasks. In such contexts, Code-Switching Language Identification\n(CSLID) becomes a difficult but necessary task if we want to maximally leverage\nexisting monolingual tools for other tasks. In this work, we propose two novel\napproaches toward improving language identification accuracy on an\nEnglish-Mandarin child-directed speech dataset. Our methods include a stacked\nResidual CNN+GRU model and a multitask pre-training approach to use Automatic\nSpeech Recognition (ASR) as an auxiliary task for CSLID. Due to the\nlow-resource nature of code-switching, we also employ careful silver data\ncreation using monolingual corpora in both languages and up-sampling as data\naugmentation. We focus on English-Mandarin code-switched data, but our method\nworks on any language pair. Our best model achieves a balanced accuracy of\n0.781 on a real English-Mandarin code-switching child-directed speech corpus\nand outperforms the previous baseline by 55.3%.", "published": "2023-05-31 11:43:16", "link": "http://arxiv.org/abs/2305.19759v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "LMCap: Few-shot Multilingual Image Captioning by Retrieval Augmented\n  Language Model Prompting", "abstract": "Multilingual image captioning has recently been tackled by training with\nlarge-scale machine translated data, which is an expensive, noisy, and\ntime-consuming process. Without requiring any multilingual caption data, we\npropose LMCap, an image-blind few-shot multilingual captioning model that works\nby prompting a language model with retrieved captions. Specifically, instead of\nfollowing the standard encoder-decoder paradigm, given an image, LMCap first\nretrieves the captions of similar images using a multilingual CLIP encoder.\nThese captions are then combined into a prompt for an XGLM decoder, in order to\ngenerate captions in the desired language. In other words, the generation model\ndoes not directly process the image, instead processing retrieved captions.\nExperiments on the XM3600 dataset of geographically diverse images show that\nour model is competitive with fully-supervised multilingual captioning models,\nwithout requiring any supervised training on any captioning data.", "published": "2023-05-31 13:03:17", "link": "http://arxiv.org/abs/2305.19821v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Deliberate then Generate: Enhanced Prompting Framework for Text\n  Generation", "abstract": "Large language models (LLMs) have shown remarkable success across a wide\nrange of natural language generation tasks, where proper prompt designs make\ngreat impacts. While existing prompting methods are normally restricted to\nproviding correct information, in this paper, we encourage the model to\ndeliberate by proposing a novel Deliberate then Generate (DTG) prompting\nframework, which consists of error detection instructions and candidates that\nmay contain errors. DTG is a simple yet effective technique that can be applied\nto various text generation tasks with minimal modifications. We conduct\nextensive experiments on 20+ datasets across 7 text generation tasks, including\nsummarization, translation, dialogue, and more. We show that DTG consistently\noutperforms existing prompting methods and achieves state-of-the-art\nperformance on multiple text generation tasks. We also provide in-depth\nanalyses to reveal the underlying mechanisms of DTG, which may inspire future\nresearch on prompting for LLMs.", "published": "2023-05-31 13:23:04", "link": "http://arxiv.org/abs/2305.19835v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "How Does Pretraining Improve Discourse-Aware Translation?", "abstract": "Pretrained language models (PLMs) have produced substantial improvements in\ndiscourse-aware neural machine translation (NMT), for example, improved\ncoherence in spoken language translation. However, the underlying reasons for\ntheir strong performance have not been well explained. To bridge this gap, we\nintroduce a probing task to interpret the ability of PLMs to capture discourse\nrelation knowledge. We validate three state-of-the-art PLMs across encoder-,\ndecoder-, and encoder-decoder-based models. The analysis shows that (1) the\nability of PLMs on discourse modelling varies from architecture and layer; (2)\ndiscourse elements in a text lead to different learning difficulties for PLMs.\nBesides, we investigate the effects of different PLMs on spoken language\ntranslation. Through experiments on IWSLT2017 Chinese-English dataset, we\nempirically reveal that NMT models initialized from different layers of PLMs\nexhibit the same trends with the probing task. Our findings are instructive to\nunderstand how and when discourse knowledge in PLMs should work for downstream\ntasks.", "published": "2023-05-31 13:36:51", "link": "http://arxiv.org/abs/2305.19847v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Neuron to Graph: Interpreting Language Model Neurons at Scale", "abstract": "Advances in Large Language Models (LLMs) have led to remarkable capabilities,\nyet their inner mechanisms remain largely unknown. To understand these models,\nwe need to unravel the functions of individual neurons and their contribution\nto the network. This paper introduces a novel automated approach designed to\nscale interpretability techniques across a vast array of neurons within LLMs,\nto make them more interpretable and ultimately safe. Conventional methods\nrequire examination of examples with strong neuron activation and manual\nidentification of patterns to decipher the concepts a neuron responds to. We\npropose Neuron to Graph (N2G), an innovative tool that automatically extracts a\nneuron's behaviour from the dataset it was trained on and translates it into an\ninterpretable graph. N2G uses truncation and saliency methods to emphasise only\nthe most pertinent tokens to a neuron while enriching dataset examples with\ndiverse samples to better encompass the full spectrum of neuron behaviour.\nThese graphs can be visualised to aid researchers' manual interpretation, and\ncan generate token activations on text for automatic validation by comparison\nwith the neuron's ground truth activations, which we use to show that the model\nis better at predicting neuron activation than two baseline methods. We also\ndemonstrate how the generated graph representations can be flexibly used to\nfacilitate further automation of interpretability research, by searching for\nneurons with particular properties, or programmatically comparing neurons to\neach other to identify similar neurons. Our method easily scales to build graph\nrepresentations for all neurons in a 6-layer Transformer model using a single\nTesla T4 GPU, allowing for wide usability. We release the code and instructions\nfor use at https://github.com/alexjfoote/Neuron2Graph.", "published": "2023-05-31 14:44:33", "link": "http://arxiv.org/abs/2305.19911v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Metropolis-Hastings algorithm in joint-attention naming game:\n  Experimental semiotics study", "abstract": "In this study, we explore the emergence of symbols during interactions\nbetween individuals through an experimental semiotic study. Previous studies\ninvestigate how humans organize symbol systems through communication using\nartificially designed subjective experiments. In this study, we have focused on\na joint attention-naming game (JA-NG) in which participants independently\ncategorize objects and assign names while assuming their joint attention.\n  In the theory of the Metropolis-Hastings naming game (MHNG), listeners accept\nprovided names according to the acceptance probability computed using the\nMetropolis-Hastings (MH) algorithm. The theory of MHNG suggests that symbols\nemerge as an approximate decentralized Bayesian inference of signs, which is\nrepresented as a shared prior variable if the conditions of MHNG are satisfied.\n  This study examines whether human participants exhibit behavior consistent\nwith MHNG theory when playing JA-NG. By comparing human acceptance decisions of\na partner's naming with acceptance probabilities computed in the MHNG, we\ntested whether human behavior is consistent with the MHNG theory. The main\ncontributions of this study are twofold. First, we reject the null hypothesis\nthat humans make acceptance judgments with a constant probability, regardless\nof the acceptance probability calculated by the MH algorithm. This result\nsuggests that people followed the acceptance probability computed by the MH\nalgorithm to some extent. Second, the MH-based model predicted human\nacceptance/rejection behavior more accurately than the other four models:\nConstant, Numerator, Subtraction, and Binary. This result indicates that symbol\nemergence in JA-NG can be explained using MHNG and is considered an approximate\ndecentralized Bayesian inference.", "published": "2023-05-31 15:20:54", "link": "http://arxiv.org/abs/2305.19936v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Efficient Shapley Values Estimation by Amortization for Text\n  Classification", "abstract": "Despite the popularity of Shapley Values in explaining neural text\nclassification models, computing them is prohibitive for large pretrained\nmodels due to a large number of model evaluations. In practice, Shapley Values\nare often estimated with a small number of stochastic model evaluations.\nHowever, we show that the estimated Shapley Values are sensitive to random seed\nchoices -- the top-ranked features often have little overlap across different\nseeds, especially on examples with longer input texts. This can only be\nmitigated by aggregating thousands of model evaluations, which on the other\nhand, induces substantial computational overheads. To mitigate the trade-off\nbetween stability and efficiency, we develop an amortized model that directly\npredicts each input feature's Shapley Value without additional model\nevaluations. It is trained on a set of examples whose Shapley Values are\nestimated from a large number of model evaluations to ensure stability.\nExperimental results on two text classification datasets demonstrate that our\namortized model estimates Shapley Values accurately with up to 60 times speedup\ncompared to traditional methods. Furthermore, the estimated values are stable\nas the inference is deterministic. We release our code at\nhttps://github.com/yangalan123/Amortized-Interpretability.", "published": "2023-05-31 16:19:13", "link": "http://arxiv.org/abs/2305.19998v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Scalable Learning of Latent Language Structure With Logical Offline\n  Cycle Consistency", "abstract": "We introduce Logical Offline Cycle Consistency Optimization (LOCCO), a\nscalable, semi-supervised method for training a neural semantic parser.\nConceptually, LOCCO can be viewed as a form of self-learning where the semantic\nparser being trained is used to generate annotations for unlabeled text that\nare then used as new supervision. To increase the quality of annotations, our\nmethod utilizes a count-based prior over valid formal meaning representations\nand a cycle-consistency score produced by a neural text generation model as\nadditional signals. Both the prior and semantic parser are updated in an\nalternate fashion from full passes over the training data, which can be seen as\napproximating the marginalization of latent structures through stochastic\nvariational inference. The use of a count-based prior, frozen text generation\nmodel, and offline annotation process yields an approach with negligible\ncomplexity and latency increases as compared to conventional self-learning. As\nan added bonus, the annotations produced by LOCCO can be trivially repurposed\nto train a neural text generation model. We demonstrate the utility of LOCCO on\nthe well-known WebNLG benchmark where we obtain an improvement of 2 points\nagainst a self-learning parser under equivalent conditions, an improvement of\n1.3 points against the previous state-of-the-art parser, and competitive text\ngeneration performance in terms of BLEU score.", "published": "2023-05-31 16:47:20", "link": "http://arxiv.org/abs/2305.20018v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ActiveAED: A Human in the Loop Improves Annotation Error Detection", "abstract": "Manually annotated datasets are crucial for training and evaluating Natural\nLanguage Processing models. However, recent work has discovered that even\nwidely-used benchmark datasets contain a substantial number of erroneous\nannotations. This problem has been addressed with Annotation Error Detection\n(AED) models, which can flag such errors for human re-annotation. However, even\nthough many of these AED methods assume a final curation step in which a human\nannotator decides whether the annotation is erroneous, they have been developed\nas static models without any human-in-the-loop component. In this work, we\npropose ActiveAED, an AED method that can detect errors more accurately by\nrepeatedly querying a human for error corrections in its prediction loop. We\nevaluate ActiveAED on eight datasets spanning five different tasks and find\nthat it leads to improvements over the state of the art on seven of them, with\ngains of up to six percentage points in average precision.", "published": "2023-05-31 17:18:47", "link": "http://arxiv.org/abs/2305.20045v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Decision-Oriented Dialogue for Human-AI Collaboration", "abstract": "We describe a class of tasks called decision-oriented dialogues, in which AI\nassistants such as large language models (LMs) must collaborate with one or\nmore humans via natural language to help them make complex decisions. We\nformalize three domains in which users face everyday decisions: (1) choosing an\nassignment of reviewers to conference papers, (2) planning a multi-step\nitinerary in a city, and (3) negotiating travel plans for a group of friends.\nIn each of these settings, AI assistants and users have disparate abilities\nthat they must combine to arrive at the best decision: assistants can access\nand process large amounts of information, while users have preferences and\nconstraints external to the system. For each task, we build a dialogue\nenvironment where agents receive a reward based on the quality of the final\ndecision they reach. We evaluate LMs in self-play and in collaboration with\nhumans and find that they fall short compared to human assistants, achieving\nmuch lower rewards despite engaging in longer dialogues. We highlight a number\nof challenges models face in decision-oriented dialogues, ranging from\ngoal-directed behavior to reasoning and optimization, and release our\nenvironments as a testbed for future work.", "published": "2023-05-31 17:50:02", "link": "http://arxiv.org/abs/2305.20076v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Automated Annotation with Generative AI Requires Validation", "abstract": "Generative large language models (LLMs) can be a powerful tool for augmenting\ntext annotation procedures, but their performance varies across annotation\ntasks due to prompt quality, text data idiosyncrasies, and conceptual\ndifficulty. Because these challenges will persist even as LLM technology\nimproves, we argue that any automated annotation process using an LLM must\nvalidate the LLM's performance against labels generated by humans. To this end,\nwe outline a workflow to harness the annotation potential of LLMs in a\nprincipled, efficient way. Using GPT-4, we validate this approach by\nreplicating 27 annotation tasks across 11 datasets from recent social science\narticles in high-impact journals. We find that LLM performance for text\nannotation is promising but highly contingent on both the dataset and the type\nof annotation task, which reinforces the necessity to validate on a\ntask-by-task basis. We make available easy-to-use software designed to\nimplement our workflow and streamline the deployment of LLMs for automated\nannotation.", "published": "2023-05-31 20:50:45", "link": "http://arxiv.org/abs/2306.00176v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Invariant Learning Characterization of Controlled Text Generation", "abstract": "Controlled generation refers to the problem of creating text that contains\nstylistic or semantic attributes of interest. Many approaches reduce this\nproblem to training a predictor of the desired attribute. For example,\nresearchers hoping to deploy a large language model to produce non-toxic\ncontent may use a toxicity classifier to filter generated text. In practice,\nthe generated text to classify, which is determined by user prompts, may come\nfrom a wide range of distributions. In this paper, we show that the performance\nof controlled generation may be poor if the distributions of text in response\nto user prompts differ from the distribution the predictor was trained on. To\naddress this problem, we cast controlled generation under distribution shift as\nan invariant learning problem: the most effective predictor should be invariant\nacross multiple text environments. We then discuss a natural solution that\narises from this characterization and propose heuristics for selecting natural\nenvironments. We study this characterization and the proposed method\nempirically using both synthetic and real data. Experiments demonstrate both\nthe challenge of distribution shift in controlled generation and the potential\nof invariance methods in this setting.", "published": "2023-05-31 21:35:08", "link": "http://arxiv.org/abs/2306.00198v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AoM: Detecting Aspect-oriented Information for Multimodal Aspect-Based\n  Sentiment Analysis", "abstract": "Multimodal aspect-based sentiment analysis (MABSA) aims to extract aspects\nfrom text-image pairs and recognize their sentiments. Existing methods make\ngreat efforts to align the whole image to corresponding aspects. However,\ndifferent regions of the image may relate to different aspects in the same\nsentence, and coarsely establishing image-aspect alignment will introduce noise\nto aspect-based sentiment analysis (i.e., visual noise). Besides, the sentiment\nof a specific aspect can also be interfered by descriptions of other aspects\n(i.e., textual noise). Considering the aforementioned noises, this paper\nproposes an Aspect-oriented Method (AoM) to detect aspect-relevant semantic and\nsentiment information. Specifically, an aspect-aware attention module is\ndesigned to simultaneously select textual tokens and image blocks that are\nsemantically related to the aspects. To accurately aggregate sentiment\ninformation, we explicitly introduce sentiment embedding into AoM, and use a\ngraph convolutional network to model the vision-text and text-text interaction.\nExtensive experiments demonstrate the superiority of AoM to existing methods.\nThe source code is publicly released at https://github.com/SilyRab/AoM.", "published": "2023-05-31 11:50:43", "link": "http://arxiv.org/abs/2306.01004v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Scaling Evidence-based Instructional Design Expertise through Large\n  Language Models", "abstract": "This paper presents a comprehensive exploration of leveraging Large Language\nModels (LLMs), specifically GPT-4, in the field of instructional design. With a\nfocus on scaling evidence-based instructional design expertise, our research\naims to bridge the gap between theoretical educational studies and practical\nimplementation. We discuss the benefits and limitations of AI-driven content\ngeneration, emphasizing the necessity of human oversight in ensuring the\nquality of educational materials. This work is elucidated through two detailed\ncase studies where we applied GPT-4 in creating complex higher-order\nassessments and active learning components for different courses. From our\nexperiences, we provide best practices for effectively using LLMs in\ninstructional design tasks, such as utilizing templates, fine-tuning, handling\nunexpected output, implementing LLM chains, citing references, evaluating\noutput, creating rubrics, grading, and generating distractors. We also share\nour vision of a future recommendation system, where a customized GPT-4 extracts\ninstructional design principles from educational studies and creates\npersonalized, evidence-supported strategies for users' unique educational\ncontexts. Our research contributes to understanding and optimally harnessing\nthe potential of AI-driven language models in enhancing educational outcomes.", "published": "2023-05-31 17:54:07", "link": "http://arxiv.org/abs/2306.01006v2", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "The Impact of Positional Encoding on Length Generalization in\n  Transformers", "abstract": "Length generalization, the ability to generalize from small training context\nsizes to larger ones, is a critical challenge in the development of\nTransformer-based language models. Positional encoding (PE) has been identified\nas a major factor influencing length generalization, but the exact impact of\ndifferent PE schemes on extrapolation in downstream tasks remains unclear. In\nthis paper, we conduct a systematic empirical study comparing the length\ngeneralization performance of decoder-only Transformers with five different\nposition encoding approaches including Absolute Position Embedding (APE), T5's\nRelative PE, ALiBi, and Rotary, in addition to Transformers without positional\nencoding (NoPE). Our evaluation encompasses a battery of reasoning and\nmathematical tasks. Our findings reveal that the most commonly used positional\nencoding methods, such as ALiBi, Rotary, and APE, are not well suited for\nlength generalization in downstream tasks. More importantly, NoPE outperforms\nother explicit positional encoding methods while requiring no additional\ncomputation. We theoretically demonstrate that NoPE can represent both absolute\nand relative PEs, but when trained with SGD, it mostly resembles T5's relative\nPE attention patterns. Finally, we find that scratchpad is not always helpful\nto solve length generalization and its format highly impacts the model's\nperformance. Overall, our work suggests that explicit position embeddings are\nnot essential for decoder-only Transformers to generalize well to longer\nsequences.", "published": "2023-05-31 00:29:55", "link": "http://arxiv.org/abs/2305.19466v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PlaSma: Making Small Language Models Better Procedural Knowledge Models\n  for (Counterfactual) Planning", "abstract": "Procedural planning, which entails decomposing a high-level goal into a\nsequence of temporally ordered steps, is an important yet intricate task for\nmachines. It involves integrating common-sense knowledge to reason about\ncomplex and often contextualized situations, e.g. ``scheduling a doctor's\nappointment without a phone''. While current approaches show encouraging\nresults using large language models (LLMs), they are hindered by drawbacks such\nas costly API calls and reproducibility issues. In this paper, we advocate\nplanning using smaller language models. We present PlaSma, a novel two-pronged\napproach to endow small language models with procedural knowledge and\n(constrained) language planning capabilities. More concretely, we develop\nsymbolic procedural knowledge distillation to enhance the commonsense knowledge\nin small language models and an inference-time algorithm to facilitate more\nstructured and accurate reasoning. In addition, we introduce a new related\ntask, Replanning, that requires a revision of a plan to cope with a constrained\nsituation. In both the planning and replanning settings, we show that\norders-of-magnitude smaller models (770M-11B parameters) can compete and often\nsurpass their larger teacher models' capabilities. Finally, we showcase\nsuccessful application of PlaSma in an embodied environment, VirtualHome.", "published": "2023-05-31 00:55:40", "link": "http://arxiv.org/abs/2305.19472v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Fine-grained Text Style Transfer with Diffusion-Based Language Models", "abstract": "Diffusion probabilistic models have shown great success in generating\nhigh-quality images controllably, and researchers have tried to utilize this\ncontrollability into text generation domain. Previous works on diffusion-based\nlanguage models have shown that they can be trained without external knowledge\n(such as pre-trained weights) and still achieve stable performance and\ncontrollability. In this paper, we trained a diffusion-based model on StylePTB\ndataset, the standard benchmark for fine-grained text style transfers. The\ntasks in StylePTB requires much more refined control over the output text\ncompared to tasks evaluated in previous works, and our model was able to\nachieve state-of-the-art performance on StylePTB on both individual and\ncompositional transfers. Moreover, our model, trained on limited data from\nStylePTB without external knowledge, outperforms previous works that utilized\npretrained weights, embeddings, and external grammar parsers, and this may\nindicate that diffusion-based language models have great potential under\nlow-resource settings.", "published": "2023-05-31 02:51:26", "link": "http://arxiv.org/abs/2305.19512v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Automatic Pronunciation Assessment", "abstract": "Automatic Pronunciation Assessment (APA) is vital for computer-assisted\nlanguage learning. Prior methods rely on annotated speech-text data to train\nAutomatic Speech Recognition (ASR) models or speech-score data to train\nregression models. In this work, we propose a novel zero-shot APA method based\non the pre-trained acoustic model, HuBERT. Our method involves encoding speech\ninput and corrupting them via a masking module. We then employ the Transformer\nencoder and apply k-means clustering to obtain token sequences. Finally, a\nscoring module is designed to measure the number of wrongly recovered tokens.\nExperimental results on speechocean762 demonstrate that the proposed method\nachieves comparable performance to supervised regression baselines and\noutperforms non-regression baselines in terms of Pearson Correlation\nCoefficient (PCC). Additionally, we analyze how masking strategies affect the\nperformance of APA.", "published": "2023-05-31 05:17:17", "link": "http://arxiv.org/abs/2305.19563v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DC CoMix TTS: An End-to-End Expressive TTS with Discrete Code\n  Collaborated with Mixer", "abstract": "Despite the huge successes made in neutral TTS, content-leakage remains a\nchallenge. In this paper, we propose a new input representation and simple\narchitecture to achieve improved prosody modeling. Inspired by the recent\nsuccess in the use of discrete code in TTS, we introduce discrete code to the\ninput of the reference encoder. Specifically, we leverage the vector quantizer\nfrom the audio compression model to exploit the diverse acoustic information it\nhas already been trained on. In addition, we apply the modified MLP-Mixer to\nthe reference encoder, making the architecture lighter. As a result, we train\nthe prosody transfer TTS in an end-to-end manner. We prove the effectiveness of\nour method through both subjective and objective evaluations. We demonstrate\nthat the reference encoder learns better speaker-independent prosody when\ndiscrete code is utilized as input in the experiments. In addition, we obtain\ncomparable results even when fewer parameters are inputted.", "published": "2023-05-31 05:36:22", "link": "http://arxiv.org/abs/2305.19567v4", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Unveiling Cross Modality Bias in Visual Question Answering: A Causal\n  View with Possible Worlds VQA", "abstract": "To increase the generalization capability of VQA systems, many recent studies\nhave tried to de-bias spurious language or vision associations that shortcut\nthe question or image to the answer. Despite these efforts, the literature\nfails to address the confounding effect of vision and language simultaneously.\nAs a result, when they reduce bias learned from one modality, they usually\nincrease bias from the other. In this paper, we first model a confounding\neffect that causes language and vision bias simultaneously, then propose a\ncounterfactual inference to remove the influence of this effect. The model\ntrained in this strategy can concurrently and efficiently reduce vision and\nlanguage bias. To the best of our knowledge, this is the first work to reduce\nbiases resulting from confounding effects of vision and language in VQA,\nleveraging causal explain-away relations. We accompany our method with an\nexplain-away strategy, pushing the accuracy of the questions with numerical\nanswers results compared to existing methods that have been an open problem.\nThe proposed method outperforms the state-of-the-art methods in VQA-CP v2\ndatasets.", "published": "2023-05-31 09:02:58", "link": "http://arxiv.org/abs/2305.19664v1", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "XPhoneBERT: A Pre-trained Multilingual Model for Phoneme Representations\n  for Text-to-Speech", "abstract": "We present XPhoneBERT, the first multilingual model pre-trained to learn\nphoneme representations for the downstream text-to-speech (TTS) task. Our\nXPhoneBERT has the same model architecture as BERT-base, trained using the\nRoBERTa pre-training approach on 330M phoneme-level sentences from nearly 100\nlanguages and locales. Experimental results show that employing XPhoneBERT as\nan input phoneme encoder significantly boosts the performance of a strong\nneural TTS model in terms of naturalness and prosody and also helps produce\nfairly high-quality speech with limited training data. We publicly release our\npre-trained XPhoneBERT with the hope that it would facilitate future research\nand downstream TTS applications for multiple languages. Our XPhoneBERT model is\navailable at https://github.com/VinAIResearch/XPhoneBERT", "published": "2023-05-31 10:05:33", "link": "http://arxiv.org/abs/2305.19709v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Knowledge Base Question Answering for Space Debris Queries", "abstract": "Space agencies execute complex satellite operations that need to be supported\nby the technical knowledge contained in their extensive information systems.\nKnowledge bases (KB) are an effective way of storing and accessing such\ninformation at scale. In this work we present a system, developed for the\nEuropean Space Agency (ESA), that can answer complex natural language queries,\nto support engineers in accessing the information contained in a KB that models\nthe orbital space debris environment. Our system is based on a pipeline which\nfirst generates a sequence of basic database operations, called a %program\nsketch, from a natural language question, then specializes the sketch into a\nconcrete query program with mentions of entities, attributes and relations, and\nfinally executes the program against the database. This pipeline decomposition\napproach enables us to train the system by leveraging out-of-domain data and\nsemi-synthetic data generated by GPT-3, thus reducing overfitting and shortcut\nlearning even with limited amount of in-domain training data. Our code can be\nfound at \\url{https://github.com/PaulDrm/DISCOSQA}.", "published": "2023-05-31 10:55:41", "link": "http://arxiv.org/abs/2305.19734v1", "categories": ["cs.AI", "cs.CL", "cs.DB", "I.2.7"], "primary_category": "cs.AI"}
{"title": "Text-to-Speech Pipeline for Swiss German -- A comparison", "abstract": "In this work, we studied the synthesis of Swiss German speech using different\nText-to-Speech (TTS) models. We evaluated the TTS models on three corpora, and\nwe found, that VITS models performed best, hence, using them for further\ntesting. We also introduce a new method to evaluate TTS models by letting the\ndiscriminator of a trained vocoder GAN model predict whether a given waveform\nis human or synthesized. In summary, our best model delivers speech synthesis\nfor different Swiss German dialects with previously unachieved quality.", "published": "2023-05-31 11:33:18", "link": "http://arxiv.org/abs/2305.19750v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Recursive Metropolis-Hastings Naming Game: Symbol Emergence in a\n  Multi-agent System based on Probabilistic Generative Models", "abstract": "In the studies on symbol emergence and emergent communication in a population\nof agents, a computational model was employed in which agents participate in\nvarious language games. Among these, the Metropolis-Hastings naming game (MHNG)\npossesses a notable mathematical property: symbol emergence through MHNG is\nproven to be a decentralized Bayesian inference of representations shared by\nthe agents. However, the previously proposed MHNG is limited to a two-agent\nscenario. This paper extends MHNG to an N-agent scenario. The main\ncontributions of this paper are twofold: (1) we propose the recursive\nMetropolis-Hastings naming game (RMHNG) as an N-agent version of MHNG and\ndemonstrate that RMHNG is an approximate Bayesian inference method for the\nposterior distribution over a latent variable shared by agents, similar to\nMHNG; and (2) we empirically evaluate the performance of RMHNG on synthetic and\nreal image data, enabling multiple agents to develop and share a symbol system.\nFurthermore, we introduce two types of approximations -- one-sample and\nlimited-length -- to reduce computational complexity while maintaining the\nability to explain communication in a population of agents. The experimental\nfindings showcased the efficacy of RMHNG as a decentralized Bayesian inference\nfor approximating the posterior distribution concerning latent variables, which\nare jointly shared among agents, akin to MHNG. Moreover, the utilization of\nRMHNG elucidated the agents' capacity to exchange symbols. Furthermore, the\nstudy discovered that even the computationally simplified version of RMHNG\ncould enable symbols to emerge among the agents.", "published": "2023-05-31 11:46:13", "link": "http://arxiv.org/abs/2305.19761v1", "categories": ["cs.CL", "cs.LG", "cs.MA"], "primary_category": "cs.CL"}
{"title": "Attention-Based Methods For Audio Question Answering", "abstract": "Audio question answering (AQA) is the task of producing natural language\nanswers when a system is provided with audio and natural language questions. In\nthis paper, we propose neural network architectures based on self-attention and\ncross-attention for the AQA task. The self-attention layers extract powerful\naudio and textual representations. The cross-attention maps audio features that\nare relevant to the textual features to produce answers. All our models are\ntrained on the recently proposed Clotho-AQA dataset for both binary yes/no\nquestions and single-word answer questions. Our results clearly show\nimprovement over the reference method reported in the original paper. On the\nyes/no binary classification task, our proposed model achieves an accuracy of\n68.3% compared to 62.7% in the reference model. For the single-word answers\nmulticlass classifier, our model produces a top-1 and top-5 accuracy of 57.9%\nand 99.8% compared to 54.2% and 93.7% in the reference model respectively. We\nfurther discuss some of the challenges in the Clotho-AQA dataset such as the\npresence of the same answer word in multiple tenses, singular and plural forms,\nand the presence of specific and generic answers to the same question. We\naddress these issues and present a revised version of the dataset.", "published": "2023-05-31 12:00:51", "link": "http://arxiv.org/abs/2305.19769v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish\n  Language", "abstract": "The BEIR dataset is a large, heterogeneous benchmark for Information\nRetrieval (IR) in zero-shot settings, garnering considerable attention within\nthe research community. However, BEIR and analogous datasets are predominantly\nrestricted to the English language. Our objective is to establish extensive\nlarge-scale resources for IR in the Polish language, thereby advancing the\nresearch in this NLP area. In this work, inspired by mMARCO and Mr.~TyDi\ndatasets, we translated all accessible open IR datasets into Polish, and we\nintroduced the BEIR-PL benchmark -- a new benchmark which comprises 13\ndatasets, facilitating further development, training and evaluation of modern\nPolish language models for IR tasks. We executed an evaluation and comparison\nof numerous IR models on the newly introduced BEIR-PL benchmark. Furthermore,\nwe publish pre-trained open IR models for Polish language,d marking a\npioneering development in this field. Additionally, the evaluation revealed\nthat BM25 achieved significantly lower scores for Polish than for English,\nwhich can be attributed to high inflection and intricate morphological\nstructure of the Polish language. Finally, we trained various re-ranking models\nto enhance the BM25 retrieval, and we compared their performance to identify\ntheir unique characteristic features. To ensure accurate model comparisons, it\nis necessary to scrutinise individual results rather than to average across the\nentire benchmark. Thus, we thoroughly analysed the outcomes of IR models in\nrelation to each individual data subset encompassed by the BEIR benchmark. The\nbenchmark data is available at URL {\\bf https://huggingface.co/clarin-knext}.", "published": "2023-05-31 13:29:07", "link": "http://arxiv.org/abs/2305.19840v2", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Med-UniC: Unifying Cross-Lingual Medical Vision-Language Pre-Training by\n  Diminishing Bias", "abstract": "The scarcity of data presents a critical obstacle to the efficacy of medical\nvisionlanguage pre-training (VLP). A potential solution lies in the combination\nof datasets from various language communities. Nevertheless, the main challenge\nstems from the complexity of integrating diverse syntax and semantics,\nlanguage-specific medical terminology, and culture-specific implicit knowledge.\nTherefore, one crucial aspect to consider is the presence of community bias\ncaused by different languages. This paper presents a novel framework named\nUnifying Cross-Lingual Medical Vision-Language Pre-Training (Med-UniC),\ndesigned to integrate multimodal medical data from the two most prevalent\nlanguages, English and Spanish. Specifically, we propose Cross-lingual Text\nAlignment Regularization (CTR) to explicitly unify cross-lingual semantic\nrepresentations of medical reports originating from diverse language\ncommunities. CTR is optimized through latent language disentanglement,\nrendering our optimization objective to not depend on negative samples, thereby\nsignificantly mitigating the bias from determining positive-negative sample\npairs within analogous medical reports. Furthermore, it ensures that the\ncross-lingual representation is not biased toward any specific language\ncommunity. Med-UniC reaches superior performance across 5 medical image tasks\nand 10 datasets encompassing over 30 diseases, offering a versatile framework\nfor unifying multi-modal medical data within diverse linguistic communities.\nThe experimental outcomes highlight the presence of community bias in\ncross-lingual VLP. Reducing this bias enhances the performance not only in\nvision-language tasks but also in uni-modal visual tasks.", "published": "2023-05-31 14:28:19", "link": "http://arxiv.org/abs/2305.19894v3", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Source Code Data Augmentation for Deep Learning: A Survey", "abstract": "The increasingly popular adoption of deep learning models in many critical\nsource code tasks motivates the development of data augmentation (DA)\ntechniques to enhance training data and improve various capabilities (e.g.,\nrobustness and generalizability) of these models. Although a series of DA\nmethods have been proposed and tailored for source code models, there lacks a\ncomprehensive survey and examination to understand their effectiveness and\nimplications. This paper fills this gap by conducting a comprehensive and\nintegrative survey of data augmentation for source code, wherein we\nsystematically compile and encapsulate existing literature to provide a\ncomprehensive overview of the field. We start with an introduction of data\naugmentation in source code and then provide a discussion on major\nrepresentative approaches. Next, we highlight the general strategies and\ntechniques to optimize the DA quality. Subsequently, we underscore techniques\nuseful in real-world source code scenarios and downstream tasks. Finally, we\noutline the prevailing challenges and potential opportunities for future\nresearch. In essence, we aim to demystify the corpus of existing literature on\nsource code DA for deep learning, and foster further exploration in this\nsphere. Complementing this, we present a continually updated GitHub repository\nthat hosts a list of update-to-date papers on DA for source code modeling,\naccessible at \\url{https://github.com/terryyz/DataAug4Code}.", "published": "2023-05-31 14:47:44", "link": "http://arxiv.org/abs/2305.19915v4", "categories": ["cs.CL", "cs.AI", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Speaking the Language of Your Listener: Audience-Aware Adaptation via\n  Plug-and-Play Theory of Mind", "abstract": "Dialogue participants may have varying levels of knowledge about the topic\nunder discussion. In such cases, it is essential for speakers to adapt their\nutterances by taking their audience into account. Yet, it is an open question\nhow such adaptation can be modelled in computational agents. In this paper, we\nmodel a visually grounded referential game between a knowledgeable speaker and\na listener with more limited visual and linguistic experience. Inspired by\npsycholinguistic theories, we endow our speaker with the ability to adapt its\nreferring expressions via a simulation module that monitors the effectiveness\nof planned utterances from the listener's perspective. We propose an adaptation\nmechanism building on plug-and-play approaches to controlled language\ngeneration, where utterance generation is steered on the fly by the simulator\nwithout finetuning the speaker's underlying language model. Our results and\nanalyses show that our approach is effective: the speaker's utterances become\ncloser to the listener's domain of expertise, which leads to higher\ncommunicative success.", "published": "2023-05-31 15:17:28", "link": "http://arxiv.org/abs/2305.19933v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "VILAS: Exploring the Effects of Vision and Language Context in Automatic\n  Speech Recognition", "abstract": "Enhancing automatic speech recognition (ASR) performance by leveraging\nadditional multimodal information has shown promising results in previous\nstudies. However, most of these works have primarily focused on utilizing\nvisual cues derived from human lip motions. In fact, context-dependent visual\nand linguistic cues can also benefit in many scenarios. In this paper, we first\npropose ViLaS (Vision and Language into Automatic Speech Recognition), a novel\nmultimodal ASR model based on the continuous integrate-and-fire (CIF)\nmechanism, which can integrate visual and textual context simultaneously or\nseparately, to facilitate speech recognition. Next, we introduce an effective\ntraining strategy that improves performance in modal-incomplete test scenarios.\nThen, to explore the effects of integrating vision and language, we create\nVSDial, a multimodal ASR dataset with multimodal context cues in both Chinese\nand English versions. Finally, empirical results are reported on the public\nFlickr8K and self-constructed VSDial datasets. We explore various cross-modal\nfusion schemes, analyze fine-grained crossmodal alignment on VSDial, and\nprovide insights into the effects of integrating multimodal information on\nspeech recognition.", "published": "2023-05-31 16:01:20", "link": "http://arxiv.org/abs/2305.19972v2", "categories": ["eess.AS", "cs.AI", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Beam Tree Recursive Cells", "abstract": "We propose Beam Tree Recursive Cell (BT-Cell) - a backpropagation-friendly\nframework to extend Recursive Neural Networks (RvNNs) with beam search for\nlatent structure induction. We further extend this framework by proposing a\nrelaxation of the hard top-k operators in beam search for better propagation of\ngradient signals. We evaluate our proposed models in different\nout-of-distribution splits in both synthetic and realistic data. Our\nexperiments show that BTCell achieves near-perfect performance on several\nchallenging structure-sensitive synthetic tasks like ListOps and logical\ninference while maintaining comparable performance in realistic data against\nother RvNN-based models. Additionally, we identify a previously unknown failure\ncase for neural models in generalization to unseen number of arguments in\nListOps. The code is available at:\nhttps://github.com/JRC1995/BeamTreeRecursiveCells.", "published": "2023-05-31 16:20:04", "link": "http://arxiv.org/abs/2305.19999v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Human or Not? A Gamified Approach to the Turing Test", "abstract": "We present \"Human or Not?\", an online game inspired by the Turing test, that\nmeasures the capability of AI chatbots to mimic humans in dialog, and of humans\nto tell bots from other humans. Over the course of a month, the game was played\nby over 1.5 million users who engaged in anonymous two-minute chat sessions\nwith either another human or an AI language model which was prompted to behave\nlike humans. The task of the players was to correctly guess whether they spoke\nto a person or to an AI. This largest scale Turing-style test conducted to date\nrevealed some interesting facts. For example, overall users guessed the\nidentity of their partners correctly in only 68% of the games. In the subset of\nthe games in which users faced an AI bot, users had even lower correct guess\nrates of 60% (that is, not much higher than chance). This white paper details\nthe development, deployment, and results of this unique experiment. While this\nexperiment calls for many extensions and refinements, these findings already\nbegin to shed light on the inevitable near future which will commingle humans\nand AI.", "published": "2023-05-31 16:32:22", "link": "http://arxiv.org/abs/2305.20010v1", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.HC", "68T50", "I.2.7"], "primary_category": "cs.AI"}
{"title": "Monotonic Location Attention for Length Generalization", "abstract": "We explore different ways to utilize position-based cross-attention in\nseq2seq networks to enable length generalization in algorithmic tasks. We show\nthat a simple approach of interpolating the original and reversed encoded\nrepresentations combined with relative attention allows near-perfect length\ngeneralization for both forward and reverse lookup tasks or copy tasks that had\nbeen generally hard to tackle. We also devise harder diagnostic tasks where the\nrelative distance of the ideal attention position varies with timestep. In such\nsettings, the simple interpolation trick with relative attention is not\nsufficient. We introduce novel variants of location attention building on top\nof Dubois et al. (2020) to address the new diagnostic tasks. We also show the\nbenefits of our approaches for length generalization in SCAN (Lake & Baroni,\n2018) and CFQ (Keysers et al., 2020). Our code is available on GitHub.", "published": "2023-05-31 16:48:06", "link": "http://arxiv.org/abs/2305.20019v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Let's Verify Step by Step", "abstract": "In recent years, large language models have greatly improved in their ability\nto perform complex multi-step reasoning. However, even state-of-the-art models\nstill regularly produce logical mistakes. To train more reliable models, we can\nturn either to outcome supervision, which provides feedback for a final result,\nor process supervision, which provides feedback for each intermediate reasoning\nstep. Given the importance of training reliable models, and given the high cost\nof human feedback, it is important to carefully compare the both methods.\nRecent work has already begun this comparison, but many questions still remain.\nWe conduct our own investigation, finding that process supervision\nsignificantly outperforms outcome supervision for training models to solve\nproblems from the challenging MATH dataset. Our process-supervised model solves\n78% of problems from a representative subset of the MATH test set.\nAdditionally, we show that active learning significantly improves the efficacy\nof process supervision. To support related research, we also release PRM800K,\nthe complete dataset of 800,000 step-level human feedback labels used to train\nour best reward model.", "published": "2023-05-31 17:24:00", "link": "http://arxiv.org/abs/2305.20050v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Improving CLIP Training with Language Rewrites", "abstract": "Contrastive Language-Image Pre-training (CLIP) stands as one of the most\neffective and scalable methods for training transferable vision models using\npaired image and text data. CLIP models are trained using contrastive loss,\nwhich typically relies on data augmentations to prevent overfitting and\nshortcuts. However, in the CLIP training paradigm, data augmentations are\nexclusively applied to image inputs, while language inputs remain unchanged\nthroughout the entire training process, limiting the exposure of diverse texts\nto the same image. In this paper, we introduce Language augmented CLIP\n(LaCLIP), a simple yet highly effective approach to enhance CLIP training\nthrough language rewrites. Leveraging the in-context learning capability of\nlarge language models, we rewrite the text descriptions associated with each\nimage. These rewritten texts exhibit diversity in sentence structure and\nvocabulary while preserving the original key concepts and meanings. During\ntraining, LaCLIP randomly selects either the original texts or the rewritten\nversions as text augmentations for each image. Extensive experiments on CC3M,\nCC12M, RedCaps and LAION-400M datasets show that CLIP pre-training with\nlanguage rewrites significantly improves the transfer performance without\ncomputation or memory overhead during training. Specifically for ImageNet\nzero-shot accuracy, LaCLIP outperforms CLIP by 8.2% on CC12M and 2.4% on\nLAION-400M. Code is available at https://github.com/LijieFan/LaCLIP.", "published": "2023-05-31 17:59:04", "link": "http://arxiv.org/abs/2305.20088v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "ManagerTower: Aggregating the Insights of Uni-Modal Experts for\n  Vision-Language Representation Learning", "abstract": "Two-Tower Vision-Language (VL) models have shown promising improvements on\nvarious downstream VL tasks. Although the most advanced work improves\nperformance by building bridges between encoders, it suffers from ineffective\nlayer-by-layer utilization of uni-modal representations and cannot flexibly\nexploit different levels of uni-modal semantic knowledge. In this work, we\npropose ManagerTower, a novel VL model architecture that gathers and combines\nthe insights of pre-trained uni-modal experts at different levels. The managers\nintroduced in each cross-modal layer can adaptively aggregate uni-modal\nsemantic knowledge to facilitate more comprehensive cross-modal alignment and\nfusion. ManagerTower outperforms previous strong baselines both with and\nwithout Vision-Language Pre-training (VLP). With only 4M VLP data, ManagerTower\nachieves superior performances on various downstream VL tasks, especially\n79.15% accuracy on VQAv2 Test-Std, 86.56% IR@1 and 95.64% TR@1 on Flickr30K.\nCode and checkpoints are available at https://github.com/LooperXX/ManagerTower.", "published": "2023-05-31 18:23:57", "link": "http://arxiv.org/abs/2306.00103v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Strategies for improving low resource speech to text translation relying\n  on pre-trained ASR models", "abstract": "This paper presents techniques and findings for improving the performance of\nlow-resource speech to text translation (ST). We conducted experiments on both\nsimulated and real-low resource setups, on language pairs English - Portuguese,\nand Tamasheq - French respectively. Using the encoder-decoder framework for ST,\nour results show that a multilingual automatic speech recognition system acts\nas a good initialization under low-resource scenarios. Furthermore, using the\nCTC as an additional objective for translation during training and decoding\nhelps to reorder the internal representations and improves the final\ntranslation. Through our experiments, we try to identify various factors\n(initializations, objectives, and hyper-parameters) that contribute the most\nfor improvements in low-resource setups. With only 300 hours of pre-training\ndata, our model achieved 7.3 BLEU score on Tamasheq - French data,\noutperforming prior published works from IWSLT 2022 by 1.6 points.", "published": "2023-05-31 21:58:07", "link": "http://arxiv.org/abs/2306.00208v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Diffusion Brush: A Latent Diffusion Model-based Editing Tool for\n  AI-generated Images", "abstract": "Text-to-image generative models have made remarkable advancements in\ngenerating high-quality images. However, generated images often contain\nundesirable artifacts or other errors due to model limitations. Existing\ntechniques to fine-tune generated images are time-consuming (manual editing),\nproduce poorly-integrated results (inpainting), or result in unexpected changes\nacross the entire image (variation selection and prompt fine-tuning). In this\nwork, we present Diffusion Brush, a Latent Diffusion Model-based (LDM) tool to\nefficiently fine-tune desired regions within an AI-synthesized image. Our\nmethod introduces new random noise patterns at targeted regions during the\nreverse diffusion process, enabling the model to efficiently make changes to\nthe specified regions while preserving the original context for the rest of the\nimage. We evaluate our method's usability and effectiveness through a user\nstudy with artists, comparing our technique against other state-of-the-art\nimage inpainting techniques and editing software for fine-tuning AI-generated\nimagery.", "published": "2023-05-31 22:27:21", "link": "http://arxiv.org/abs/2306.00219v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Using Visual Cropping to Enhance Fine-Detail Question Answering of\n  BLIP-Family Models", "abstract": "Visual Question Answering is a challenging task, as it requires seamless\ninteraction between perceptual, linguistic, and background knowledge systems.\nWhile the recent progress of visual and natural language models like BLIP has\nled to improved performance on this task, we lack understanding of the ability\nof such models to perform on different kinds of questions and reasoning types.\nAs our initial analysis of BLIP-family models revealed difficulty with\nanswering fine-detail questions, we investigate the following question: Can\nvisual cropping be employed to improve the performance of state-of-the-art\nvisual question answering models on fine-detail questions? Given the recent\nsuccess of the BLIP-family models, we study a zero-shot and a fine-tuned BLIP\nmodel. We define three controlled subsets of the popular VQA-v2 benchmark to\nmeasure whether cropping can help model performance. Besides human cropping, we\ndevise two automatic cropping strategies based on multi-modal embedding by CLIP\nand BLIP visual QA model gradients. Our experiments demonstrate that the\nperformance of BLIP model variants can be significantly improved through human\ncropping, and automatic cropping methods can produce comparable benefits. A\ndeeper dive into our findings indicates that the performance enhancement is\nmore pronounced in zero-shot models than in fine-tuned models and more salient\nwith smaller bounding boxes than larger ones. We perform case studies to\nconnect quantitative differences with qualitative observations across question\ntypes and datasets. Finally, we see that the cropping enhancement is robust, as\nwe gain an improvement of 4.59% (absolute) in the general VQA-random task by\nsimply inputting a concatenation of the original and gradient-based cropped\nimages. We make our code available to facilitate further innovation on visual\ncropping methods for question answering.", "published": "2023-05-31 22:48:27", "link": "http://arxiv.org/abs/2306.00228v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "From Pixels to UI Actions: Learning to Follow Instructions via Graphical\n  User Interfaces", "abstract": "Much of the previous work towards digital agents for graphical user\ninterfaces (GUIs) has relied on text-based representations (derived from HTML\nor other structured data sources), which are not always readily available.\nThese input representations have been often coupled with custom, task-specific\naction spaces. This paper focuses on creating agents that interact with the\ndigital world using the same conceptual interface that humans commonly use --\nvia pixel-based screenshots and a generic action space corresponding to\nkeyboard and mouse actions. Building upon recent progress in pixel-based\npretraining, we show, for the first time, that it is possible for such agents\nto outperform human crowdworkers on the MiniWob++ benchmark of GUI-based\ninstruction following tasks.", "published": "2023-05-31 23:39:18", "link": "http://arxiv.org/abs/2306.00245v2", "categories": ["cs.LG", "cs.CL", "cs.CV", "cs.HC"], "primary_category": "cs.LG"}
{"title": "Examining the Emergence of Deductive Reasoning in Generative Language\n  Models", "abstract": "We conduct a preliminary inquiry into the ability of generative transformer\nmodels to deductively reason from premises provided. We observe notable\ndifferences in the performance of models coming from different training setups\nand find that the deductive reasoning ability increases with scale. Further, we\ndiscover that the performance generally does not decrease with the length of\nthe deductive chain needed to reach the conclusion, with the exception of\nOpenAI GPT-3 and GPT-3.5 models. Our study considers a wide variety of\ntransformer-decoder models, ranging from 117 million to 175 billion parameters\nin size.", "published": "2023-05-31 21:29:49", "link": "http://arxiv.org/abs/2306.01009v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Edit Distance based RL for RNNT decoding", "abstract": "RNN-T is currently considered the industry standard in ASR due to its\nexceptional WERs in various benchmark tests and its ability to support seamless\nstreaming and longform transcription. However, its biggest drawback lies in the\nsignificant discrepancy between its training and inference objectives. During\ntraining, RNN-T maximizes all alignment probabilities by teacher forcing, while\nduring inference, it uses beam search which may not necessarily find the\nmaximum probable alignment. Additionally, RNN-T's inability to experience\nmistakes during teacher forcing training makes it more problematic when a\nmistake occurs in inference. To address this issue, this paper proposes a\nReinforcement Learning method that minimizes the gap between training and\ninference time. Our Edit Distance based RL (EDRL) approach computes rewards\nbased on the edit distance, and trains the network at every action level. The\nproposed approach yielded SoTA WERs on LibriSpeech for the 600M Conformer RNN-T\nmodel.", "published": "2023-05-31 16:53:23", "link": "http://arxiv.org/abs/2306.01789v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MERT: Acoustic Music Understanding Model with Large-Scale\n  Self-supervised Training", "abstract": "Self-supervised learning (SSL) has recently emerged as a promising paradigm\nfor training generalisable models on large-scale data in the fields of vision,\ntext, and speech. Although SSL has been proven effective in speech and audio,\nits application to music audio has yet to be thoroughly explored. This is\npartially due to the distinctive challenges associated with modelling musical\nknowledge, particularly tonal and pitched characteristics of music. To address\nthis research gap, we propose an acoustic Music undERstanding model with\nlarge-scale self-supervised Training (MERT), which incorporates teacher models\nto provide pseudo labels in the masked language modelling (MLM) style acoustic\npre-training. In our exploration, we identified an effective combination of\nteacher models, which outperforms conventional speech and audio approaches in\nterms of performance. This combination includes an acoustic teacher based on\nResidual Vector Quantisation - Variational AutoEncoder (RVQ-VAE) and a musical\nteacher based on the Constant-Q Transform (CQT). Furthermore, we explore a wide\nrange of settings to overcome the instability in acoustic language model\npre-training, which allows our designed paradigm to scale from 95M to 330M\nparameters. Experimental results indicate that our model can generalise and\nperform well on 14 music understanding tasks and attain state-of-the-art (SOTA)\noverall scores.", "published": "2023-05-31 18:27:43", "link": "http://arxiv.org/abs/2306.00107v5", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MuseCoco: Generating Symbolic Music from Text", "abstract": "Generating music from text descriptions is a user-friendly mode since the\ntext is a relatively easy interface for user engagement. While some approaches\nutilize texts to control music audio generation, editing musical elements in\ngenerated audio is challenging for users. In contrast, symbolic music offers\nease of editing, making it more accessible for users to manipulate specific\nmusical elements. In this paper, we propose MuseCoco, which generates symbolic\nmusic from text descriptions with musical attributes as the bridge to break\ndown the task into text-to-attribute understanding and attribute-to-music\ngeneration stages. MuseCoCo stands for Music Composition Copilot that empowers\nmusicians to generate music directly from given text descriptions, offering a\nsignificant improvement in efficiency compared to creating music entirely from\nscratch. The system has two main advantages: Firstly, it is data efficient. In\nthe attribute-to-music generation stage, the attributes can be directly\nextracted from music sequences, making the model training self-supervised. In\nthe text-to-attribute understanding stage, the text is synthesized and refined\nby ChatGPT based on the defined attribute templates. Secondly, the system can\nachieve precise control with specific attributes in text descriptions and\noffers multiple control options through attribute-conditioned or\ntext-conditioned approaches. MuseCoco outperforms baseline systems in terms of\nmusicality, controllability, and overall score by at least 1.27, 1.08, and 1.32\nrespectively. Besides, there is a notable enhancement of about 20% in objective\ncontrol accuracy. In addition, we have developed a robust large-scale model\nwith 1.2 billion parameters, showcasing exceptional controllability and\nmusicality.", "published": "2023-05-31 18:34:16", "link": "http://arxiv.org/abs/2306.00110v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MERLIon CCS Challenge Evaluation Plan", "abstract": "This paper introduces the inaugural Multilingual Everyday Recordings-\nLanguage Identification on Code-Switched Child-Directed Speech (MERLIon CCS)\nChallenge, focused on developing robust language identification and language\ndiarization systems that are reliable for non-standard, accented, spontaneous\ncode-switched, child-directed speech collected via Zoom. Aligning closely with\nInterspeech 2023 theme, the main objectives of this inaugural challenge are to\npresent a unique first-of-its-kind Zoom videocall dataset featuring\nEnglish-Mandarin spontaneous code-switched child-directed speech, benchmark the\ncurrent and novel language identification and language diarization systems in a\ncode-switching scenario including extremely short utterances, and test the\nrobustness of such systems under accented speech. The MERLIon CCS challenge\nfeatures two task: language identification (Task 1) and language diarization\n(Task 2). Two tracks, open and closed, are available for each task, differing\nby the volume of data systems can be trained on. This paper describes the\ndataset, dataset annotation protocol, challenge tasks, open and closed tracks,\nevaluation metrics, and evaluation protocol.", "published": "2023-05-31 02:05:05", "link": "http://arxiv.org/abs/2305.19493v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Few-shot Class-incremental Audio Classification Using Dynamically\n  Expanded Classifier with Self-attention Modified Prototypes", "abstract": "Most existing methods for audio classification assume that the vocabulary of\naudio classes to be classified is fixed. When novel (unseen) audio classes\nappear, audio classification systems need to be retrained with abundant labeled\nsamples of all audio classes for recognizing base (initial) and novel audio\nclasses. If novel audio classes continue to appear, the existing methods for\naudio classification will be inefficient and even infeasible. In this work, we\npropose a method for few-shot class-incremental audio classification, which can\ncontinually recognize novel audio classes without forgetting old ones. The\nframework of our method mainly consists of two parts: an embedding extractor\nand a classifier, and their constructions are decoupled. The embedding\nextractor is the backbone of a ResNet based network, which is frozen after\nconstruction by a training strategy using only samples of base audio classes.\nHowever, the classifier consisting of prototypes is expanded by a prototype\nadaptation network with few samples of novel audio classes in incremental\nsessions. Labeled support samples and unlabeled query samples are used to train\nthe prototype adaptation network and update the classifier, since they are\ninformative for audio classification. Three audio datasets, named NSynth-100,\nFSC-89 and LS-100 are built by choosing samples from audio corpora of NSynth,\nFSD-MIX-CLIP and LibriSpeech, respectively. Results show that our method\nexceeds baseline methods in average accuracy and performance dropping rate. In\naddition, it is competitive compared to baseline methods in computational\ncomplexity and memory requirement. The code for our method is given at\nhttps://github.com/vinceasvp/FCAC.", "published": "2023-05-31 03:59:47", "link": "http://arxiv.org/abs/2305.19539v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Few-Shot Speaker Identification Using Lightweight Prototypical Network\n  with Feature Grouping and Interaction", "abstract": "Existing methods for few-shot speaker identification (FSSI) obtain high\naccuracy, but their computational complexities and model sizes need to be\nreduced for lightweight applications. In this work, we propose a FSSI method\nusing a lightweight prototypical network with the final goal to implement the\nFSSI on intelligent terminals with limited resources, such as smart watches and\nsmart speakers. In the proposed prototypical network, an embedding module is\ndesigned to perform feature grouping for reducing the memory requirement and\ncomputational complexity, and feature interaction for enhancing the\nrepresentational ability of the learned speaker embedding. In the proposed\nembedding module, audio feature of each speech sample is split into several\nlow-dimensional feature subsets that are transformed by a recurrent\nconvolutional block in parallel. Then, the operations of averaging, addition,\nconcatenation, element-wise summation and statistics pooling are sequentially\nexecuted to learn a speaker embedding for each speech sample. The recurrent\nconvolutional block consists of a block of bidirectional long short-term\nmemory, and a block of de-redundancy convolution in which feature grouping and\ninteraction are conducted too. Our method is compared to baseline methods on\nthree datasets that are selected from three public speech corpora (VoxCeleb1,\nVoxCeleb2, and LibriSpeech). The results show that our method obtains higher\naccuracy under several conditions, and has advantages over all baseline methods\nin computational complexity and model size.", "published": "2023-05-31 04:09:50", "link": "http://arxiv.org/abs/2305.19541v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "FN-SSL: Full-Band and Narrow-Band Fusion for Sound Source Localization", "abstract": "Extracting direct-path spatial features is critical for sound source\nlocalization in adverse acoustic environments. This paper proposes a full-band\nand narrow-band fusion network for estimating direct-path inter-channel phase\ndifference (DP-IPD) from microphone signals. The alternating full-band and\nnarrow-band layers are responsible for learning the full-band correlation and\nnarrow-band extraction of DP-IPD, respectively. Experiments show that the\nproposed network noticeably outperforms other advanced methods on both\nsimulated and real-world data.", "published": "2023-05-31 07:27:03", "link": "http://arxiv.org/abs/2305.19610v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Speaker-independent Speech Inversion for Estimation of Nasalance", "abstract": "The velopharyngeal (VP) valve regulates the opening between the nasal and\noral cavities. This valve opens and closes through a coordinated motion of the\nvelum and pharyngeal walls. Nasalance is an objective measure derived from the\noral and nasal acoustic signals that correlate with nasality. In this work, we\nevaluate the degree to which the nasalance measure reflects fine-grained\npatterns of VP movement by comparison with simultaneously collected direct\nmeasures of VP opening using high-speed nasopharyngoscopy (HSN). We show that\nnasalance is significantly correlated with the HSN signal, and that both match\nexpected patterns of nasality. We then train a temporal convolution-based\nspeech inversion system in a speaker-independent fashion to estimate VP\nmovement for nasality, using nasalance as the ground truth. In further\nexperiments, we also show the importance of incorporating source features (from\nglottal activity) to improve nasality prediction.", "published": "2023-05-31 21:47:26", "link": "http://arxiv.org/abs/2306.00203v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "PromptStyle: Controllable Style Transfer for Text-to-Speech with Natural\n  Language Descriptions", "abstract": "Style transfer TTS has shown impressive performance in recent years. However,\nstyle control is often restricted to systems built on expressive speech\nrecordings with discrete style categories. In practical situations, users may\nbe interested in transferring style by typing text descriptions of desired\nstyles, without the reference speech in the target style. The text-guided\ncontent generation techniques have drawn wide attention recently. In this work,\nwe explore the possibility of controllable style transfer with natural language\ndescriptions. To this end, we propose PromptStyle, a text prompt-guided\ncross-speaker style transfer system. Specifically, PromptStyle consists of an\nimproved VITS and a cross-modal style encoder. The cross-modal style encoder\nconstructs a shared space of stylistic and semantic representation through a\ntwo-stage training process. Experiments show that PromptStyle can achieve\nproper style transfer with text prompts while maintaining relatively high\nstability and speaker similarity. Audio samples are available in our demo page.", "published": "2023-05-31 03:16:59", "link": "http://arxiv.org/abs/2305.19522v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SVVAD: Personal Voice Activity Detection for Speaker Verification", "abstract": "Voice activity detection (VAD) improves the performance of speaker\nverification (SV) by preserving speech segments and attenuating the effects of\nnon-speech. However, this scheme is not ideal: (1) it fails in noisy\nenvironments or multi-speaker conversations; (2) it is trained based on\ninaccurate non-SV sensitive labels. To address this, we propose a speaker\nverification-based voice activity detection (SVVAD) framework that can adapt\nthe speech features according to which are most informative for SV. To achieve\nthis, we introduce a label-free training method with triplet-like losses that\ncompletely avoids the performance degradation of SV due to incorrect labeling.\nExtensive experiments show that SVVAD significantly outperforms the baseline in\nterms of equal error rate (EER) under conditions where other speakers are mixed\nat different ratios. Moreover, the decision boundaries reveal the importance of\nthe different parts of speech, which are largely consistent with human\njudgments.", "published": "2023-05-31 05:59:33", "link": "http://arxiv.org/abs/2305.19581v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Learning Music Sequence Representation from Text Supervision", "abstract": "Music representation learning is notoriously difficult for its complex\nhuman-related concepts contained in the sequence of numerical signals. To\nexcavate better MUsic SEquence Representation from labeled audio, we propose a\nnovel text-supervision pre-training method, namely MUSER. MUSER adopts an\naudio-spectrum-text tri-modal contrastive learning framework, where the text\ninput could be any form of meta-data with the help of text templates while the\nspectrum is derived from an audio sequence. Our experiments reveal that MUSER\ncould be more flexibly adapted to downstream tasks compared with the current\ndata-hungry pre-training method, and it only requires 0.056% of pre-training\ndata to achieve the state-of-the-art performance.", "published": "2023-05-31 07:15:06", "link": "http://arxiv.org/abs/2305.19602v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Intelligible Lip-to-Speech Synthesis with Speech Units", "abstract": "In this paper, we propose a novel Lip-to-Speech synthesis (L2S) framework,\nfor synthesizing intelligible speech from a silent lip movement video.\nSpecifically, to complement the insufficient supervisory signal of the previous\nL2S model, we propose to use quantized self-supervised speech representations,\nnamed speech units, as an additional prediction target for the L2S model.\nTherefore, the proposed L2S model is trained to generate multiple targets,\nmel-spectrogram and speech units. As the speech units are discrete while\nmel-spectrogram is continuous, the proposed multi-target L2S model can be\ntrained with strong content supervision, without using text-labeled data.\nMoreover, to accurately convert the synthesized mel-spectrogram into a\nwaveform, we introduce a multi-input vocoder that can generate a clear waveform\neven from blurry and noisy mel-spectrogram by referring to the speech units.\nExtensive experimental results confirm the effectiveness of the proposed method\nin L2S.", "published": "2023-05-31 07:17:32", "link": "http://arxiv.org/abs/2305.19603v1", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Underwater-Art: Expanding Information Perspectives With Text Templates\n  For Underwater Acoustic Target Recognition", "abstract": "Underwater acoustic target recognition is an intractable task due to the\ncomplex acoustic source characteristics and sound propagation patterns. Limited\nby insufficient data and narrow information perspective, recognition models\nbased on deep learning seem far from satisfactory in practical underwater\nscenarios. Although underwater acoustic signals are severely influenced by\ndistance, channel depth, or other factors, annotations of relevant information\nare often non-uniform, incomplete, and hard to use. In our work, we propose to\nimplement Underwater Acoustic Recognition based on Templates made up of rich\nrelevant information (hereinafter called \"UART\"). We design templates to\nintegrate relevant information from different perspectives into descriptive\nnatural language. UART adopts an audio-spectrogram-text tri-modal contrastive\nlearning framework, which endows UART with the ability to guide the learning of\nacoustic representations by descriptive natural language. Our experiments\nreveal that UART has better recognition capability and generalization\nperformance than traditional paradigms. Furthermore, the pre-trained UART model\ncould provide superior prior knowledge for the recognition model in the\nscenario without any auxiliary annotation.", "published": "2023-05-31 07:28:37", "link": "http://arxiv.org/abs/2305.19612v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multi-Dataset Co-Training with Sharpness-Aware Optimization for Audio\n  Anti-spoofing", "abstract": "Audio anti-spoofing for automatic speaker verification aims to safeguard\nusers' identities from spoofing attacks. Although state-of-the-art spoofing\ncountermeasure(CM) models perform well on specific datasets, they lack\ngeneralization when evaluated with different datasets. To address this\nlimitation, previous studies have explored large pre-trained models, which\nrequire significant resources and time. We aim to develop a compact but\nwell-generalizing CM model that can compete with large pre-trained models. Our\napproach involves multi-dataset co-training and sharpness-aware minimization,\nwhich has not been investigated in this domain. Extensive experiments reveal\nthat proposed method yield competitive results across various datasets while\nutilizing 4,000 times less parameters than the large pre-trained models.", "published": "2023-05-31 15:37:48", "link": "http://arxiv.org/abs/2305.19953v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "UNSSOR: Unsupervised Neural Speech Separation by Leveraging\n  Over-determined Training Mixtures", "abstract": "In reverberant conditions with multiple concurrent speakers, each microphone\nacquires a mixture signal of multiple speakers at a different location. In\nover-determined conditions where the microphones out-number speakers, we can\nnarrow down the solutions to speaker images and realize unsupervised speech\nseparation by leveraging each mixture signal as a constraint (i.e., the\nestimated speaker images at a microphone should add up to the mixture).\nEquipped with this insight, we propose UNSSOR, an algorithm for\n$\\textbf{u}$nsupervised $\\textbf{n}$eural $\\textbf{s}$peech\n$\\textbf{s}$eparation by leveraging $\\textbf{o}$ver-determined training\nmixtu$\\textbf{r}$es. At each training step, we feed an input mixture to a deep\nneural network (DNN) to produce an intermediate estimate for each speaker,\nlinearly filter the estimates, and optimize a loss so that, at each microphone,\nthe filtered estimates of all the speakers can add up to the mixture to satisfy\nthe above constraint. We show that this loss can promote unsupervised\nseparation of speakers. The linear filters are computed in each sub-band based\non the mixture and DNN estimates through the forward convolutive prediction\n(FCP) algorithm. To address the frequency permutation problem incurred by using\nsub-band FCP, a loss term based on minimizing intra-source magnitude scattering\nis proposed. Although UNSSOR requires over-determined training mixtures, we can\ntrain DNNs to achieve under-determined separation (e.g., unsupervised monaural\nspeech separation). Evaluation results on two-speaker separation in reverberant\nconditions show the effectiveness and potential of UNSSOR.", "published": "2023-05-31 17:28:02", "link": "http://arxiv.org/abs/2305.20054v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "How to Construct Perfect and Worse-than-Coin-Flip Spoofing\n  Countermeasures: A Word of Warning on Shortcut Learning", "abstract": "Shortcut learning, or `Clever Hans effect` refers to situations where a\nlearning agent (e.g., deep neural networks) learns spurious correlations\npresent in data, resulting in biased models. We focus on finding shortcuts in\ndeep learning based spoofing countermeasures (CMs) that predict whether a given\nutterance is spoofed or not. While prior work has addressed specific data\nartifacts, such as silence, no general normative framework has been explored\nfor analyzing shortcut learning in CMs. In this study, we propose a generic\napproach to identifying shortcuts by introducing systematic interventions on\nthe training and test sides, including the boundary cases of `near-perfect` and\n`worse than coin flip` (label flip). By using three different models, ranging\nfrom classic to state-of-the-art, we demonstrate the presence of shortcut\nlearning in five simulated conditions. We analyze the results using a\nregression model to understand how biases affect the class-conditional score\nstatistics.", "published": "2023-05-31 15:58:37", "link": "http://arxiv.org/abs/2306.00044v1", "categories": ["cs.LG", "cs.CR", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Audio-Visual Speech Separation in Noisy Environments with a Lightweight\n  Iterative Model", "abstract": "We propose Audio-Visual Lightweight ITerative model (AVLIT), an effective and\nlightweight neural network that uses Progressive Learning (PL) to perform\naudio-visual speech separation in noisy environments. To this end, we adopt the\nAsynchronous Fully Recurrent Convolutional Neural Network (A-FRCNN), which has\nshown successful results in audio-only speech separation. Our architecture\nconsists of an audio branch and a video branch, with iterative A-FRCNN blocks\nsharing weights for each modality. We evaluated our model in a controlled\nenvironment using the NTCD-TIMIT dataset and in-the-wild using a synthetic\ndataset that combines LRS3 and WHAM!. The experiments demonstrate the\nsuperiority of our model in both settings with respect to various audio-only\nand audio-visual baselines. Furthermore, the reduced footprint of our model\nmakes it suitable for low resource applications.", "published": "2023-05-31 20:09:50", "link": "http://arxiv.org/abs/2306.00160v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Adaptive ship-radiated noise recognition with learnable fine-grained\n  wavelet transform", "abstract": "Analyzing the ocean acoustic environment is a tricky task. Background noise\nand variable channel transmission environment make it complicated to implement\naccurate ship-radiated noise recognition. Existing recognition systems are weak\nin addressing the variable underwater environment, thus leading to\ndisappointing performance in practical application. In order to keep the\nrecognition system robust in various underwater environments, this work\nproposes an adaptive generalized recognition system - AGNet (Adaptive\nGeneralized Network). By converting fixed wavelet parameters into fine-grained\nlearnable parameters, AGNet learns the characteristics of underwater sound at\ndifferent frequencies. Its flexible and fine-grained design is conducive to\ncapturing more background acoustic information (e.g., background noise,\nunderwater transmission channel). To utilize the implicit information in\nwavelet spectrograms, AGNet adopts the convolutional neural network with\nparallel convolution attention modules as the classifier. Experiments reveal\nthat our AGNet outperforms all baseline methods on several underwater acoustic\ndatasets, and AGNet could benefit more from transfer learning. Moreover, AGNet\nshows robust performance against various interference factors.", "published": "2023-05-31 06:56:01", "link": "http://arxiv.org/abs/2306.01002v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Exploring Phonetic Context-Aware Lip-Sync For Talking Face Generation", "abstract": "Talking face generation is the challenging task of synthesizing a natural and\nrealistic face that requires accurate synchronization with a given audio. Due\nto co-articulation, where an isolated phone is influenced by the preceding or\nfollowing phones, the articulation of a phone varies upon the phonetic context.\nTherefore, modeling lip motion with the phonetic context can generate more\nspatio-temporally aligned lip movement. In this respect, we investigate the\nphonetic context in generating lip motion for talking face generation. We\npropose Context-Aware Lip-Sync framework (CALS), which explicitly leverages\nphonetic context to generate lip movement of the target face. CALS is comprised\nof an Audio-to-Lip module and a Lip-to-Face module. The former is pretrained\nbased on masked learning to map each phone to a contextualized lip motion unit.\nThe contextualized lip motion unit then guides the latter in synthesizing a\ntarget identity with context-aware lip motion. From extensive experiments, we\nverify that simply exploiting the phonetic context in the proposed CALS\nframework effectively enhances spatio-temporal alignment. We also demonstrate\nthe extent to which the phonetic context assists in lip synchronization and\nfind the effective window size for lip generation to be approximately 1.2\nseconds.", "published": "2023-05-31 04:50:32", "link": "http://arxiv.org/abs/2305.19556v3", "categories": ["cs.CV", "cs.AI", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.CV"}
