{"title": "Preparing graph states forbidding a vertex-minor", "abstract": "Measurement based quantum computing is preformed by adding non-Clifford\nmeasurements to a prepared stabilizer states. Entangling gates like CZ are\nlikely to have lower fidelities due to the nature of interacting qubits, so\nwhen preparing a stabilizer state, we wish to minimize the number of required\nentangling states. This naturally introduces the notion of CZ-distance.\n  Every stabilizer state is local-Clifford equivalent to a graph state, so we\nmay focus on graph states $\\left\\vert G \\right\\rangle$. As a lower bound for\ngeneral graphs, there exist $n$-vertex graphs $G$ such that the CZ-distance of\n$\\left\\vert G \\right\\rangle$ is $\\Omega(n^2 / \\log n)$. We obtain significantly\nimproved bounds when $G$ is contained within certain proper classes of graphs.\nFor instance, we prove that if $G$ is a $n$-vertex circle graph with clique\nnumber $\\omega$, then $\\left\\vert G \\right\\rangle$ has CZ-distance at most $4n\n\\log \\omega + 7n$. We prove that if $G$ is an $n$-vertex graph of rank-width at\nmost $k$, then $\\left\\vert G \\right\\rangle$ has CZ-distance at most\n$(2^{2^{k+1}} + 1) n$. More generally, this is obtained via a bound of $(k+2)n$\nthat we prove for graphs of twin-width at most $k$.\n  We also study how bounded-rank perturbations and low-rank cuts affect the\nCZ-distance. As a consequence, we prove that Geelen's Weak Structural\nConjecture for vertex-minors implies that if $G$ is an $n$-vertex graph\ncontained in some fixed proper vertex-minor-closed class of graphs, then\n$\\left\\vert G \\right\\rangle$ has CZ-distance at most $O(n\\log n)$. Since graph\nstates of locally equivalent graphs are local Clifford equivalent, proper\nvertex-minor-closed classes of graphs are natural and very general in this\nsetting.", "published": "2025-03-31 23:25:35", "link": "http://arxiv.org/abs/2504.00291v1", "categories": ["quant-ph", "cs.DM", "math.CO"], "primary_category": "quant-ph"}
{"title": "Reconstructing graphs with subgraph compositions", "abstract": "We generalize the problem of reconstructing strings from their substring\ncompositions first introduced by Acharya et al. in 2015 motivated by\npolymer-based advanced data storage systems utilizing mass spectrometry.\nNamely, we see strings as labeled path graphs, and as such try to reconstruct\nlabeled graphs. For a given integer t, the subgraph compositions contain either\nvectors of labels for each connected subgraph of order t\n(t-multiset-compositions) or the sum of all labels of all connected subgraphs\nof order t (t-sum-composition). We ask whether, given a graph of which we know\nthe structure and an oracle whom you can query for compositions, we can\nreconstruct the labeling of the graph. If it is possible, then the graph is\nreconstructable; otherwise, it is confusable, and two labeled graphs with the\nsame compositions are called equicomposable. We prove that reconstructing\nthrough a brute-force algorithm is wildly inefficient, before giving methods\nfor reconstructing several graph classes using as few compositions as possible.\nWe also give negative results, finding the smallest confusable graphs and\ntrees, as well as families with a large number of equicomposable non-isomorphic\ngraphs. An interesting result occurs when twinning one leaf of a path: some\npaths are confusable, creating a twin out of a leaf sees the graph alternating\nbetween reconstructable and confusable depending on the parity of the path, and\ncreating a false twin out of a leaf makes the graph reconstructable using only\nsum-compositions in all cases.", "published": "2025-03-31 19:25:17", "link": "http://arxiv.org/abs/2504.00169v1", "categories": ["math.CO", "cs.DM", "cs.IT", "math.IT"], "primary_category": "math.CO"}
{"title": "$\\mathsf{P}$-completeness of Graph Local Complementation", "abstract": "Local complementation of a graph $G$ on vertex $v$ is an operation that\nresults in a new graph $G*v$, where the neighborhood of $v$ is complemented.\nThis operation has been widely studied in graph theory and quantum computing.\n  This article introduces the Local Complementation Problem, a decision problem\nthat captures the complexity of applying a sequence of local complementations.\nGiven a graph $G$, a sequence of vertices $s$, and a pair of vertices $u,v$,\nthe problem asks whether the edge $(u,v)$ is present in the graph obtained\nafter applying local complementations according to $s$. The main contribution\nof this work is proving that this problem is $\\mathsf{P}$-complete, implying\nthat computing a sequence of local complementation is unlikely to be\nefficiently parallelizable. The proof is based on a reduction from the Circuit\nValue Problem, a well-known $\\mathsf{P}$-complete problem, by simulating\ncircuits through local complementations.\n  Aditionally, the complexity of this problem is analyzed under different\nrestrictions. In particular, it is shown that for complete and star graphs, the\nproblem belongs to $\\mathsf{LOGSPACE}$. Finally, it is conjectured that the\nproblem remains $\\mathsf{P}$-complete for the class of circle graphs.", "published": "2025-03-31 14:27:46", "link": "http://arxiv.org/abs/2503.24144v1", "categories": ["cs.CC", "cs.DM"], "primary_category": "cs.CC"}
{"title": "Directed treewidth is closed under taking butterfly minors", "abstract": "Butterfly minors are a generalisation of the minor containment relation for\nundirected graphs to directed graphs. Many results in directed structural graph\ntheory use this notion as a central tool next to directed treewidth, a\ngeneralisation of the width measure treewidth to directed graphs. Adler\n[JCTB'07] showed that the directed treewidth is not closed under taking\nbutterfly minors. Over the years, many alternative definitions for directed\ntreewidth appeared throughout the literature, equivalent to the original\ndefinition up to small functions. In this paper, we consider the major ones and\nshow that not all of them share the problem identified by Adler.", "published": "2025-03-31 11:40:22", "link": "http://arxiv.org/abs/2503.23977v1", "categories": ["math.CO", "cs.DM"], "primary_category": "math.CO"}
{"title": "SciReplicate-Bench: Benchmarking LLMs in Agent-driven Algorithmic Reproduction from Research Papers", "abstract": "This study evaluates large language models (LLMs) in generating code from\nalgorithm descriptions from recent NLP papers. The task requires two key\ncompetencies: (1) algorithm comprehension: synthesizing information from papers\nand academic literature to understand implementation logic, and (2) coding\nexpertise: identifying dependencies and correctly implementing necessary APIs.\nTo facilitate rigorous evaluation, we introduce SciReplicate-Bench, a benchmark\nof 100 tasks from 36 NLP papers published in 2024, featuring detailed\nannotations and comprehensive test cases. Building on SciReplicate-Bench, we\npropose Sci-Reproducer, a multi-agent framework consisting of a Paper Agent\nthat interprets algorithmic concepts from literature and a Code Agent that\nretrieves dependencies from repositories and implement solutions. To assess\nalgorithm understanding, we introduce reasoning graph accuracy, which\nquantifies similarity between generated and reference reasoning graphs derived\nfrom code comments and structure. For evaluating implementation quality, we\nemploy execution accuracy, CodeBLEU, and repository dependency/API recall\nmetrics. In our experiments, we evaluate various powerful Non-Reasoning LLMs\nand Reasoning LLMs as foundational models. The best-performing LLM using\nSci-Reproducer achieves only 39% execution accuracy, highlighting the\nbenchmark's difficulty.Our analysis identifies missing or inconsistent\nalgorithm descriptions as key barriers to successful reproduction. We will\nopen-source our benchmark, and code at\nhttps://github.com/xyzCS/SciReplicate-Bench.", "published": "2025-03-31 22:02:24", "link": "http://arxiv.org/abs/2504.00255v1", "categories": ["cs.CL", "cs.AI", "cs.MA", "cs.SE"], "primary_category": "cs.CL"}
{"title": "$\\textit{Agents Under Siege}$: Breaking Pragmatic Multi-Agent LLM Systems with Optimized Prompt Attacks", "abstract": "Most discussions about Large Language Model (LLM) safety have focused on\nsingle-agent settings but multi-agent LLM systems now create novel adversarial\nrisks because their behavior depends on communication between agents and\ndecentralized reasoning. In this work, we innovatively focus on attacking\npragmatic systems that have constrains such as limited token bandwidth, latency\nbetween message delivery, and defense mechanisms. We design a\n$\\textit{permutation-invariant adversarial attack}$ that optimizes prompt\ndistribution across latency and bandwidth-constraint network topologies to\nbypass distributed safety mechanisms within the system. Formulating the attack\npath as a problem of $\\textit{maximum-flow minimum-cost}$, coupled with the\nnovel $\\textit{Permutation-Invariant Evasion Loss (PIEL)}$, we leverage\ngraph-based optimization to maximize attack success rate while minimizing\ndetection risk. Evaluating across models including $\\texttt{Llama}$,\n$\\texttt{Mistral}$, $\\texttt{Gemma}$, $\\texttt{DeepSeek}$ and other variants on\nvarious datasets like $\\texttt{JailBreakBench}$ and\n$\\texttt{AdversarialBench}$, our method outperforms conventional attacks by up\nto $7\\times$, exposing critical vulnerabilities in multi-agent systems.\nMoreover, we demonstrate that existing defenses, including variants of\n$\\texttt{Llama-Guard}$ and $\\texttt{PromptGuard}$, fail to prohibit our attack,\nemphasizing the urgent need for multi-agent specific safety mechanisms.", "published": "2025-03-31 20:43:56", "link": "http://arxiv.org/abs/2504.00218v1", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.MA"}
{"title": "UniOcc: A Unified Benchmark for Occupancy Forecasting and Prediction in Autonomous Driving", "abstract": "We introduce UniOcc, a comprehensive, unified benchmark for occupancy\nforecasting (i.e., predicting future occupancies based on historical\ninformation) and current-frame occupancy prediction from camera images. UniOcc\nunifies data from multiple real-world datasets (i.e., nuScenes, Waymo) and\nhigh-fidelity driving simulators (i.e., CARLA, OpenCOOD), which provides 2D/3D\noccupancy labels with per-voxel flow annotations and support for cooperative\nautonomous driving. In terms of evaluation, unlike existing studies that rely\non suboptimal pseudo labels for evaluation, UniOcc incorporates novel metrics\nthat do not depend on ground-truth occupancy, enabling robust assessment of\nadditional aspects of occupancy quality. Through extensive experiments on\nstate-of-the-art models, we demonstrate that large-scale, diverse training data\nand explicit flow information significantly enhance occupancy prediction and\nforecasting performance.", "published": "2025-03-31 17:59:24", "link": "http://arxiv.org/abs/2503.24381v1", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MA", "cs.RO"], "primary_category": "cs.CV"}
{"title": "Pro-Routing: Proactive Routing of Autonomous Multi-Capacity Robots for Pickup-and-Delivery Tasks", "abstract": "We consider a multi-robot setting, where we have a fleet of multi-capacity\nautonomous robots that must service spatially distributed pickup-and-delivery\nrequests with fixed maximum wait times. Requests can be either scheduled ahead\nof time or they can enter the system in real-time. In this setting, stability\nfor a routing policy is defined as the cost of the policy being uniformly\nbounded over time. Most previous work either solve the problem offline to\ntheoretically maintain stability or they consider dynamically arriving requests\nat the expense of the theoretical guarantees on stability. In this paper, we\naim to bridge this gap by proposing a novel proactive rollout-based routing\nframework that adapts to real-time demand while still provably maintaining the\nstability of the learned routing policy. We derive provable stability\nguarantees for our method by proposing a fleet sizing algorithm that obtains a\nsufficiently large fleet that ensures stability by construction. To validate\nour theoretical results, we consider a case study on real ride requests for\nHarvard's evening Van System. We also evaluate the performance of our framework\nusing the currently deployed smaller fleet size. In this smaller setup, we\ncompare against the currently deployed routing algorithm, greedy heuristics,\nand Monte-Carlo-Tree-Search-based algorithms. Our empirical results show that\nour framework maintains stability when we use the sufficiently large fleet size\nfound in our theoretical results. For the smaller currently deployed fleet\nsize, our method services 6% more requests than the closest baseline while\nreducing median passenger wait times by 33%.", "published": "2025-03-31 17:14:07", "link": "http://arxiv.org/abs/2503.24325v1", "categories": ["cs.RO", "cs.AI", "cs.MA"], "primary_category": "cs.RO"}
{"title": "PAARS: Persona Aligned Agentic Retail Shoppers", "abstract": "In e-commerce, behavioral data is collected for decision making which can be\ncostly and slow. Simulation with LLM powered agents is emerging as a promising\nalternative for representing human population behavior. However, LLMs are known\nto exhibit certain biases, such as brand bias, review rating bias and limited\nrepresentation of certain groups in the population, hence they need to be\ncarefully benchmarked and aligned to user behavior. Ultimately, our goal is to\nsynthesise an agent population and verify that it collectively approximates a\nreal sample of humans. To this end, we propose a framework that: (i) creates\nsynthetic shopping agents by automatically mining personas from anonymised\nhistorical shopping data, (ii) equips agents with retail-specific tools to\nsynthesise shopping sessions and (iii) introduces a novel alignment suite\nmeasuring distributional differences between humans and shopping agents at the\ngroup (i.e. population) level rather than the traditional \"individual\" level.\nExperimental results demonstrate that using personas improves performance on\nthe alignment suite, though a gap remains to human behaviour. We showcase an\ninitial application of our framework for automated agentic A/B testing and\ncompare the findings to human results. Finally, we discuss applications,\nlimitations and challenges setting the stage for impactful future work.", "published": "2025-03-31 15:41:51", "link": "http://arxiv.org/abs/2503.24228v1", "categories": ["cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.AI"}
{"title": "Ride-Sourcing Vehicle Rebalancing with Service Accessibility Guarantees via Constrained Mean-Field Reinforcement Learning", "abstract": "The rapid expansion of ride-sourcing services such as Uber, Lyft, and Didi\nChuxing has fundamentally reshaped urban transportation by offering flexible,\non-demand mobility via mobile applications. Despite their convenience, these\nplatforms confront significant operational challenges, particularly vehicle\nrebalancing - the strategic repositioning of thousands of vehicles to address\nspatiotemporal mismatches in supply and demand. Inadequate rebalancing results\nin prolonged rider waiting times, inefficient vehicle utilization, and\ninequitable distribution of services, leading to disparities in driver\navailability and income.\n  To tackle these complexities, we introduce scalable continuous-state\nmean-field control (MFC) and reinforcement learning (MFRL) models that\nexplicitly represent each vehicle's precise location and employ continuous\nrepositioning actions guided by the distribution of other vehicles. To ensure\nequitable service distribution, an accessibility constraint is integrated\nwithin our optimal control formulation, balancing operational efficiency with\nequitable access to the service across geographic regions. Our approach\nacknowledges realistic conditions, including inherent stochasticity in\ntransitions, the simultaneous occurrence of vehicle-rider matching, vehicles'\nrebalancing and cruising, and variability in rider behaviors. Crucially, we\nrelax the traditional mean-field assumption of equal supply-demand volume,\nbetter reflecting practical scenarios. Extensive empirical evaluation using\nreal-world data-driven simulation of Shenzhen demonstrates the real-time\nefficiency and robustness of our approach at the scale of tens of thousands of\nvehicles.\n  The code is available at\nhttps://github.com/mjusup1501/mf-vehicle-rebalancing.", "published": "2025-03-31 15:00:11", "link": "http://arxiv.org/abs/2503.24183v1", "categories": ["cs.LG", "cs.MA"], "primary_category": "cs.LG"}
{"title": "Towards Scientific Intelligence: A Survey of LLM-based Scientific Agents", "abstract": "As scientific research becomes increasingly complex, innovative tools are\nneeded to manage vast data, facilitate interdisciplinary collaboration, and\naccelerate discovery. Large language models (LLMs) are now evolving into\nLLM-based scientific agents that automate critical tasks, ranging from\nhypothesis generation and experiment design to data analysis and simulation.\nUnlike general-purpose LLMs, these specialized agents integrate domain-specific\nknowledge, advanced tool sets, and robust validation mechanisms, enabling them\nto handle complex data types, ensure reproducibility, and drive scientific\nbreakthroughs. This survey provides a focused review of the architectures,\ndesign, benchmarks, applications, and ethical considerations surrounding\nLLM-based scientific agents. We highlight why they differ from general agents\nand the ways in which they advance research across various scientific fields.\nBy examining their development and challenges, this survey offers a\ncomprehensive roadmap for researchers and practitioners to harness these agents\nfor more efficient, reliable, and ethically sound scientific discovery.", "published": "2025-03-31 13:11:28", "link": "http://arxiv.org/abs/2503.24047v2", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI"}
{"title": "Consensus on Open Multi-Agent Systems Over Graphs Sampled from Graphons", "abstract": "We show how graphons can be used to model and analyze open multi-agent\nsystems, which are multi-agent systems subject to arrivals and departures, in\nthe specific case of linear consensus. First, we analyze the case of\nreplacements, where under the assumption of a deterministic interval between\ntwo replacements, we derive an upper bound for the disagreement in expectation.\nThen, we study the case of arrivals and departures, where we define a process\nfor the evolution of the number of agents that guarantees a minimum and a\nmaximum number of agents. Next, we derive an upper bound for the disagreement\nin expectation, and we establish a link with the spectrum of the expected graph\nused to generate the graph topologies. Finally, for stochastic block model\n(SBM) graphons, we prove that the computation of the spectrum of the expected\ngraph can be performed based on a matrix whose dimension depends only on the\ngraphon and it is independent of the number of agents.", "published": "2025-03-31 12:50:40", "link": "http://arxiv.org/abs/2503.24025v1", "categories": ["eess.SY", "cs.MA", "cs.SY", "math.OC"], "primary_category": "eess.SY"}
{"title": "GenSwarm: Scalable Multi-Robot Code-Policy Generation and Deployment via Language Models", "abstract": "The development of control policies for multi-robot systems traditionally\nfollows a complex and labor-intensive process, often lacking the flexibility to\nadapt to dynamic tasks. This has motivated research on methods to automatically\ncreate control policies. However, these methods require iterative processes of\nmanually crafting and refining objective functions, thereby prolonging the\ndevelopment cycle. This work introduces \\textit{GenSwarm}, an end-to-end system\nthat leverages large language models to automatically generate and deploy\ncontrol policies for multi-robot tasks based on simple user instructions in\nnatural language. As a multi-language-agent system, GenSwarm achieves zero-shot\nlearning, enabling rapid adaptation to altered or unseen tasks. The white-box\nnature of the code policies ensures strong reproducibility and\ninterpretability. With its scalable software and hardware architectures,\nGenSwarm supports efficient policy deployment on both simulated and real-world\nmulti-robot systems, realizing an instruction-to-execution end-to-end\nfunctionality that could prove valuable for robotics specialists and\nnon-specialists alike.The code of the proposed GenSwarm system is available\nonline: https://github.com/WindyLab/GenSwarm.", "published": "2025-03-31 09:26:34", "link": "http://arxiv.org/abs/2503.23875v1", "categories": ["cs.RO", "cs.AI", "cs.MA"], "primary_category": "cs.RO"}
{"title": "Get the Agents Drunk: Memory Perturbations in Autonomous Agent-based Recommender Systems", "abstract": "Large language model-based agents are increasingly used in recommender\nsystems (Agent4RSs) to achieve personalized behavior modeling. Specifically,\nAgent4RSs introduces memory mechanisms that enable the agents to autonomously\nlearn and self-evolve from real-world interactions. However, to the best of our\nknowledge, how robust Agent4RSs are remains unexplored. As such, in this paper,\nwe propose the first work to attack Agent4RSs by perturbing agents' memories,\nnot only to uncover their limitations but also to enhance their security and\nrobustness, ensuring the development of safer and more reliable AI agents.\n  Given the security and privacy concerns, it is more practical to launch\nattacks under a black-box setting, where the accurate knowledge of the victim\nmodels cannot be easily obtained. Moreover, the practical attacks are often\nstealthy to maximize the impact. To this end, we propose a novel practical\nattack framework named DrunkAgent. DrunkAgent consists of a generation module,\na strategy module, and a surrogate module. The generation module aims to\nproduce effective and coherent adversarial textual triggers, which can be used\nto achieve attack objectives such as promoting the target items. The strategy\nmodule is designed to `get the target agents drunk' so that their memories\ncannot be effectively updated during the interaction process. As such, the\ntriggers can play the best role. Both of the modules are optimized on the\nsurrogate module to improve the transferability and imperceptibility of the\nattacks. By identifying and analyzing the vulnerabilities, our work provides\ncritical insights that pave the way for building safer and more resilient\nAgent4RSs. Extensive experiments across various real-world datasets demonstrate\nthe effectiveness of DrunkAgent.", "published": "2025-03-31 07:35:40", "link": "http://arxiv.org/abs/2503.23804v1", "categories": ["cs.CR", "cs.CL", "cs.IR", "cs.MA"], "primary_category": "cs.CR"}
{"title": "Robust No-Arbitrage under Projective Determinacy", "abstract": "Drawing from set theory, this article contributes to a deeper understanding\nof the no-arbitrage principle in multiple-priors settings and its application\nin mathematical finance.\n  In the quasi-sure discrete-time frictionless market framework of Bouchard and\nNutz, the equivalence between the quasi-sure no-arbitrage condition and the\nexistence of a probability measure for which the local one-prior no-arbitrage\ncondition holds and the affine hull of the support is equal to the quasi-sure\nsupport, all of this in a quasi-sure sense, was established by Blanchard and\nCarassus. We aim to extend this result to the projective setup introduced by\nCarassus and Ferhoune.\n  This setup allows for standardised measurability assumptions, in contrast to\nthe framework of Bouchard and Nutz, where prices are assumed to be Borel\nmeasurable, strategies and stochastic kernels universally measurable, and the\ngraphs of one-step priors analytic sets.\n  To achieve this, we assume the classical axioms of Zermelo-Fraenkel set\ntheory, including the axiom of choice (ZFC), supplemented by the Projective\nDeterminacy (PD) axiom. In ZFC+PD the existence of such probability measures\nwas assumed by Carassus and Ferhoune to prove the existence of solutions in a\nquasi-sure nonconcave utility maximisation problem. The equivalence with the\nquasi-sure no-arbitrage was only conjectured.", "published": "2025-03-31 19:13:33", "link": "http://arxiv.org/abs/2504.00158v1", "categories": ["q-fin.MF", "math.LO"], "primary_category": "q-fin.MF"}
{"title": "Mathematical foundations of information economics", "abstract": "The state of economic theory and accumulated facts from the different\nbranches of the economic science require to analyze the concept of the\ndescription of economy systems. The economic reality generates the problems the\nsolution of that is only possible by a new paradigm of the description of\neconomy system. The classical mathematical economics is based on a notion of\nthe rational consumer choice generated by a certain preference relation on some\nset of goods a consumer wanted and the concept of maximization of the firm\nprofit. The sense of the notion of the ratio- nal consumer choice is that it is\ndetermined by a certain utility function, defining the choice of a consumer by\nmaximization of it on a certain budget set of goods. More- over, choices of\nconsumers are independent. In the reality choices of consumers are not\nindependent because they depend on the firms supply. Except the firms supply,\nthe consumer choice is also determined by information about the state of the\neconomy system that the consumer has and respectively eval- uates at the moment\nof the choice. In turn, the firms supply is made on the basis of needs of the\nconsumers and their buying power. By information about the state of the economy\nsystem we understand a certain information about the equilibrium price vector\nand productive processes realized in the economy system under the equilibrium\nprice vector.", "published": "2025-03-31 16:05:39", "link": "http://arxiv.org/abs/2503.24257v1", "categories": ["q-fin.MF"], "primary_category": "q-fin.MF"}
{"title": "Asymmetry in Distributions of Accumulated Gains and Losses in Stock Returns", "abstract": "We study decades-long historic distributions of accumulated S\\&P500 returns,\nfrom daily returns to those over several weeks. The time series of the returns\nemphasize major upheavals in the markets -- Black Monday, Tech Bubble,\nFinancial Crisis and Covid Pandemic -- which are reflected in the tail ends of\nthe distributions. De-trending the overall gain, we concentrate on comparing\ndistributions of gains and losses. Specifically, we compare the tails of the\ndistributions, which are believed to exhibit power-law behavior and possibly\ncontain outliers. Towards this end we find confidence intervals of the linear\nfits of the tails of the complementary cumulative distribution functions on a\nlog-log scale, as well as conduct a statistical U-test in order to detect\noutliers. We also study probability density functions of the full distributions\nof the returns with the emphasis on their asymmetry. The key empirical\nobservations are that the mean of de-trended distributions increases\nnear-linearly with the number of days of accumulation while the overall skew is\nnegative -- consistent with the heavier tails of losses -- and depends little\non the number of days of accumulation. At the same time the variance of the\ndistributions exhibits near-perfect linear dependence on the number of days of\naccumulation, that is it remains constant if scaled to the latter. Finally, we\ndiscuss the theoretical framework for understanding accumulated returns. Our\nmain conclusion is that the current state of theory, which predicts symmetric\nor near-symmetric distributions of returns cannot explain the aggregate of\nempirical results.", "published": "2025-03-31 15:54:04", "link": "http://arxiv.org/abs/2503.24241v1", "categories": ["q-fin.ST", "econ.TH", "physics.data-an"], "primary_category": "q-fin.ST"}
{"title": "A cost of capital approach to determining the LGD discount rate", "abstract": "Loss Given Default (LGD) is a key risk parameter in determining a bank's\nregulatory capital. During LGD-estimation, realised recovery cash flows are to\nbe discounted at an appropriate rate. Regulatory guidance mandates that this\nrate should allow for the time value of money, as well as include a risk\npremium that reflects the \"undiversifiable risk\" within these recoveries.\nHaving extensively reviewed earlier methods of determining this rate, we\npropose a new approach that is inspired by the cost of capital approach from\nthe Solvency II regulatory regime. Our method involves estimating a\nmarket-consistent price for a portfolio of defaulted loans, from which an\nassociated discount rate may be inferred. We apply this method to mortgage and\npersonal loans data from a large South African bank. The results reveal the\nmain drivers of the discount rate to be the mean and variance of these\nrecoveries, as well as the bank's cost of capital in excess of the risk-free\nrate. Our method therefore produces a discount rate that reflects both the\nundiversifiable risk of recovery recoveries and the time value of money,\nthereby satisfying regulatory requirements. This work can subsequently enhance\nthe LGD-component within the modelling of both regulatory and economic capital.", "published": "2025-03-31 12:09:21", "link": "http://arxiv.org/abs/2503.23992v1", "categories": ["q-fin.RM", "q-fin.ST"], "primary_category": "q-fin.RM"}
{"title": "Exploring In-Context Learning Capabilities of ChatGPT for Pathological Speech Detection", "abstract": "Automatic pathological speech detection approaches have shown promising\nresults, gaining attention as potential diagnostic tools alongside costly\ntraditional methods. While these approaches can achieve high accuracy, their\nlack of interpretability limits their applicability in clinical practice. In\nthis paper, we investigate the use of multimodal Large Language Models (LLMs),\nspecifically ChatGPT-4o, for automatic pathological speech detection in a\nfew-shot in-context learning setting. Experimental results show that this\napproach not only delivers promising performance but also provides explanations\nfor its decisions, enhancing model interpretability. To further understand its\neffectiveness, we conduct an ablation study to analyze the impact of different\nfactors, such as input type and system prompts, on the final results. Our\nfindings highlight the potential of multimodal LLMs for further exploration and\nadvancement in automatic pathological speech detection.", "published": "2025-03-31 09:23:52", "link": "http://arxiv.org/abs/2503.23873v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Aud-Sur: An Audio Analyzer Assistant for Audio Surveillance Applications", "abstract": "In this paper, we present an audio analyzer assistant tool designed for a\nwide range of audio-based surveillance applications (This work is a part of our\nDEFAME FAKES and EUCINF projects). The proposed tool, refered to as Aud-Sur,\ncomprises two main phases Audio Analysis and Audio Retrieval, respectively. In\nthe first phase, multiple open-source audio models are leveraged to extract\ninformation from input audio recording uploaded by a user. In the second phase,\nusers interact with the Aud-Sur tool via a natural question-and-answer manner,\npowered by a large language model (LLM), to retrieve the information extracted\nfrom the processed audio file. The Aud-Sur tool was deployed using Docker on a\nmicroservices-based architecture design. By leveraging open-source audio models\nfor information extraction, LLM for audio information retrieval, and a\nmicroservices-based deployment approach, the proposed Aud-Sur tool offers a\nhighly extensible and adaptable framework that can integrate more audio tasks,\nand be widely shared within the audio community for further development.", "published": "2025-03-31 08:21:11", "link": "http://arxiv.org/abs/2503.23827v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "UniSep: Universal Target Audio Separation with Language Models at Scale", "abstract": "We propose Universal target audio Separation (UniSep), addressing the\nseparation task on arbitrary mixtures of different types of audio.\nDistinguished from previous studies, UniSep is performed on unlimited source\ndomains and unlimited source numbers. We formulate the separation task as a\nsequence-to-sequence problem, and a large language model (LLM) is used to model\nthe audio sequence in the discrete latent space, leveraging the power of LLM in\nhandling complex mixture audios with large-scale data. Moreover, a novel\npre-training strategy is proposed to utilize audio-only data, which reduces the\nefforts of large-scale data simulation and enhances the ability of LLMs to\nunderstand the consistency and correlation of information within audio\nsequences. We also demonstrate the effectiveness of scaling datasets in an\naudio separation task: we use large-scale data (36.5k hours), including speech,\nmusic, and sound, to train a universal target audio separation model that is\nnot limited to a specific domain. Experiments show that UniSep achieves\ncompetitive subjective and objective evaluation results compared with\nsingle-task models.", "published": "2025-03-31 06:27:37", "link": "http://arxiv.org/abs/2503.23762v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
