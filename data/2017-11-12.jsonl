{"title": "Syntax-Directed Attention for Neural Machine Translation", "abstract": "Attention mechanism, including global attention and local attention, plays a\nkey role in neural machine translation (NMT). Global attention attends to all\nsource words for word prediction. In comparison, local attention selectively\nlooks at fixed-window source words. However, alignment weights for the current\ntarget word often decrease to the left and right by linear distance centering\non the aligned source position and neglect syntax-directed distance\nconstraints. In this paper, we extend local attention with syntax-distance\nconstraint, to focus on syntactically related source words with the predicted\ntarget word, thus learning a more effective context vector for word prediction.\nMoreover, we further propose a double context NMT architecture, which consists\nof a global context vector and a syntax-directed context vector over the global\nattention, to provide more translation performance for NMT from source\nrepresentation. The experiments on the large-scale Chinese-to-English and\nEnglish-to-Germen translation tasks show that the proposed approach achieves a\nsubstantial and significant improvement over the baseline system.", "published": "2017-11-12 03:35:48", "link": "http://arxiv.org/abs/1711.04231v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Natural Language Inference Models Enhanced with External\n  Knowledge", "abstract": "Modeling natural language inference is a very challenging task. With the\navailability of large annotated data, it has recently become feasible to train\ncomplex models such as neural-network-based inference models, which have shown\nto achieve the state-of-the-art performance. Although there exist relatively\nlarge annotated data, can machines learn all knowledge needed to perform\nnatural language inference (NLI) from these data? If not, how can\nneural-network-based NLI models benefit from external knowledge and how to\nbuild NLI models to leverage it? In this paper, we enrich the state-of-the-art\nneural natural language inference models with external knowledge. We\ndemonstrate that the proposed models improve neural NLI models to achieve the\nstate-of-the-art performance on the SNLI and MultiNLI datasets.", "published": "2017-11-12 13:12:19", "link": "http://arxiv.org/abs/1711.04289v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fast Reading Comprehension with ConvNets", "abstract": "State-of-the-art deep reading comprehension models are dominated by recurrent\nneural nets. Their sequential nature is a natural fit for language, but it also\nprecludes parallelization within an instances and often becomes the bottleneck\nfor deploying such models to latency critical scenarios. This is particularly\nproblematic for longer texts. Here we present a convolutional architecture as\nan alternative to these recurrent architectures. Using simple dilated\nconvolutional units in place of recurrent ones, we achieve results comparable\nto the state of the art on two question answering tasks, while at the same time\nachieving up to two orders of magnitude speedups for question answering.", "published": "2017-11-12 20:40:25", "link": "http://arxiv.org/abs/1711.04352v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep Networks tag the location of bird vocalisations on audio\n  spectrograms", "abstract": "This work focuses on reliable detection and segmentation of bird\nvocalizations as recorded in the open field. Acoustic detection of avian sounds\ncan be used for the automatized monitoring of multiple bird taxa and querying\nin long-term recordings for species of interest. These tasks are tackled in\nthis work, by suggesting two approaches: A) First, DenseNets are applied to\nweekly labeled data to infer the attention map of the dataset (i.e. Salience\nand CAM). We push further this idea by directing attention maps to the YOLO v2\nDeepnet-based, detection framework to localize bird vocalizations. B) A deep\nautoencoder, namely the U-net, maps the audio spectrogram of bird vocalizations\nto its corresponding binary mask that encircles the spectral blobs of\nvocalizations while suppressing other audio sources. We focus solely on\nprocedures requiring minimum human attendance, suitable to scan massive volumes\nof data, in order to analyze them, evaluate insights and hypotheses and\nidentify patterns of bird activity. Hopefully, this approach will be valuable\nto researchers, conservation practitioners, and decision makers that need to\ndesign policies on biodiversity issues.", "published": "2017-11-12 19:50:45", "link": "http://arxiv.org/abs/1711.04347v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Automatic detection of alarm sounds in a noisy hospital environment\n  using model and non-model based approaches", "abstract": "In the noisy acoustic environment of a Neonatal Intensive Care Unit (NICU)\nthere is a variety of alarms, which are frequently triggered by the biomedical\nequipment. In this paper different approaches for automatic detection of those\nsound alarms are presented and compared: 1) a non-model-based approach that\nemploys signal processing techniques; 2) a model-based approach based on neural\nnetworks; and 3) an approach that combines both non-model and model-based\napproaches. The performance of the developed detection systems that follow each\nof those approaches is assessed, analysed and compared both at the frame level\nand at the event level by using an audio database recorded in a real-world\nhospital environment.", "published": "2017-11-12 20:37:44", "link": "http://arxiv.org/abs/1711.04351v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
