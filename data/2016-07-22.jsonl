{"title": "Syntax-based Attention Model for Natural Language Inference", "abstract": "Introducing attentional mechanism in neural network is a powerful concept,\nand has achieved impressive results in many natural language processing tasks.\nHowever, most of the existing models impose attentional distribution on a flat\ntopology, namely the entire input representation sequence. Clearly, any\nwell-formed sentence has its accompanying syntactic tree structure, which is a\nmuch rich topology. Applying attention to such topology not only exploits the\nunderlying syntax, but also makes attention more interpretable. In this paper,\nwe explore this direction in the context of natural language inference. The\nresults demonstrate its efficacy. We also perform extensive qualitative\nanalysis, deriving insights and intuitions of why and how our model works.", "published": "2016-07-22 04:21:54", "link": "http://arxiv.org/abs/1607.06556v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CFGs-2-NLU: Sequence-to-Sequence Learning for Mapping Utterances to\n  Semantics and Pragmatics", "abstract": "In this paper, we present a novel approach to natural language understanding\nthat utilizes context-free grammars (CFGs) in conjunction with\nsequence-to-sequence (seq2seq) deep learning. Specifically, we take a CFG\nauthored to generate dialogue for our target application for NLU, a videogame,\nand train a long short-term memory (LSTM) recurrent neural network (RNN) to map\nthe surface utterances that it produces to traces of the grammatical expansions\nthat yielded them. Critically, this CFG was authored using a tool we have\ndeveloped that supports arbitrary annotation of the nonterminal symbols in the\ngrammar. Because we already annotated the symbols in this grammar for the\nsemantic and pragmatic considerations that our game's dialogue manager operates\nover, we can use the grammatical trace associated with any surface utterance to\ninfer such information. During gameplay, we translate player utterances into\ngrammatical traces (using our RNN), collect the mark-up attributed to the\nsymbols included in that trace, and pass this information to the dialogue\nmanager, which updates the conversation state accordingly. From an offline\nevaluation task, we demonstrate that our trained RNN translates surface\nutterances to grammatical traces with great accuracy. To our knowledge, this is\nthe first usage of seq2seq learning for conversational agents (our game's\ncharacters) who explicitly reason over semantic and pragmatic considerations.", "published": "2016-07-22 22:05:20", "link": "http://arxiv.org/abs/1607.06852v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automated Prediction of Temporal Relations", "abstract": "Background: There has been growing research interest in automated answering\nof questions or generation of summary of free form text such as news article.\nIn order to implement this task, the computer should be able to identify the\nsequence of events, duration of events, time at which event occurred and the\nrelationship type between event pairs, time pairs or event-time pairs. Specific\nProblem: It is important to accurately identify the relationship type between\ncombinations of event and time before the temporal ordering of events can be\ndefined. The machine learning approach taken in Mani et. al (2006) provides an\naccuracy of only 62.5 on the baseline data from TimeBank. The researchers used\nmaximum entropy classifier in their methodology. TimeML uses the TLINK\nannotation to tag a relationship type between events and time. The time\ncomplexity is quadratic when it comes to tagging documents with TLINK using\nhuman annotation. This research proposes using decision tree and parsing to\nimprove the relationship type tagging. This research attempts to solve the gaps\nin human annotation by automating the task of relationship type tagging in an\nattempt to improve the accuracy of event and time relationship in annotated\ndocuments. Scope information: The documents from the domain of news will be\nused. The tagging will be performed within the same document and not across\ndocuments. The relationship types will be identified only for a pair of event\nand time and not a chain of events. The research focuses on documents tagged\nusing the TimeML specification which contains tags such as EVENT, TLINK, and\nTIMEX. Each tag has attributes such as identifier, relation, POS, time etc.", "published": "2016-07-22 05:38:37", "link": "http://arxiv.org/abs/1607.06560v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Novel Word Embedding and Translation-based Language Modeling for\n  Extractive Speech Summarization", "abstract": "Word embedding methods revolve around learning continuous distributed vector\nrepresentations of words with neural networks, which can capture semantic\nand/or syntactic cues, and in turn be used to induce similarity measures among\nwords, sentences and documents in context. Celebrated methods can be\ncategorized as prediction-based and count-based methods according to the\ntraining objectives and model architectures. Their pros and cons have been\nextensively analyzed and evaluated in recent studies, but there is relatively\nless work continuing the line of research to develop an enhanced learning\nmethod that brings together the advantages of the two model families. In\naddition, the interpretation of the learned word representations still remains\nsomewhat opaque. Motivated by the observations and considering the pressing\nneed, this paper presents a novel method for learning the word representations,\nwhich not only inherits the advantages of classic word embedding methods but\nalso offers a clearer and more rigorous interpretation of the learned word\nrepresentations. Built upon the proposed word embedding method, we further\nformulate a translation-based language modeling framework for the extractive\nspeech summarization task. A series of empirical evaluations demonstrate the\neffectiveness of the proposed word representation learning and language\nmodeling techniques in extractive speech summarization.", "published": "2016-07-22 00:20:09", "link": "http://arxiv.org/abs/1607.06532v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.MM"], "primary_category": "cs.CL"}
