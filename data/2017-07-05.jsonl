{"title": "Multiple Range-Restricted Bidirectional Gated Recurrent Units with\n  Attention for Relation Classification", "abstract": "Most of neural approaches to relation classification have focused on finding\nshort patterns that represent the semantic relation using Convolutional Neural\nNetworks (CNNs) and those approaches have generally achieved better\nperformances than using Recurrent Neural Networks (RNNs). In a similar\nintuition to the CNN models, we propose a novel RNN-based model that strongly\nfocuses on only important parts of a sentence using multiple range-restricted\nbidirectional layers and attention for relation classification. Experimental\nresults on the SemEval-2010 relation classification task show that our model is\ncomparable to the state-of-the-art CNN-based and RNN-based models that use\nadditional linguistic information.", "published": "2017-07-05 08:55:28", "link": "http://arxiv.org/abs/1707.01265v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Influence of Feature Representation of Text on the Performance of\n  Document Classification", "abstract": "In this paper we perform a comparative analysis of three models for feature\nrepresentation of text documents in the context of document classification. In\nparticular, we consider the most often used family of models bag-of-words,\nrecently proposed continuous space models word2vec and doc2vec, and the model\nbased on the representation of text documents as language networks. While the\nbag-of-word models have been extensively used for the document classification\ntask, the performance of the other two models for the same task have not been\nwell understood. This is especially true for the network-based model that have\nbeen rarely considered for representation of text documents for classification.\nIn this study, we measure the performance of the document classifiers trained\nusing the method of random forests for features generated the three models and\ntheir variants. The results of the empirical comparison show that the commonly\nused bag-of-words model has performance comparable to the one obtained by the\nemerging continuous-space model of doc2vec. In particular, the low-dimensional\nvariants of doc2vec generating up to 75 features are among the top-performing\ndocument representation models. The results finally point out that doc2vec\nshows a superior performance in the tasks of classifying large documents.", "published": "2017-07-05 11:09:31", "link": "http://arxiv.org/abs/1707.01321v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Align and Copy: UZH at SIGMORPHON 2017 Shared Task for Morphological\n  Reinflection", "abstract": "This paper presents the submissions by the University of Zurich to the\nSIGMORPHON 2017 shared task on morphological reinflection. The task is to\npredict the inflected form given a lemma and a set of morpho-syntactic\nfeatures. We focus on neural network approaches that can tackle the task in a\nlimited-resource setting. As the transduction of the lemma into the inflected\nform is dominated by copying over lemma characters, we propose two recurrent\nneural network architectures with hard monotonic attention that are strong at\ncopying and, yet, substantially different in how they achieve this. The first\napproach is an encoder-decoder model with a copy mechanism. The second approach\nis a neural state-transition system over a set of explicit edit actions,\nincluding a designated COPY action. We experiment with character alignment and\nfind that naive, greedy alignment consistently produces strong results for some\nlanguages. Our best system combination is the overall winner of the SIGMORPHON\n2017 Shared Task 1 without external resources. At a setting with 100 training\nsamples, both our approaches, as ensembles of models, outperform the next best\ncompetitor.", "published": "2017-07-05 12:24:55", "link": "http://arxiv.org/abs/1707.01355v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Attention Mechanism for Answer Selection Using a Combined Global and\n  Local View", "abstract": "We propose a new attention mechanism for neural based question answering,\nwhich depends on varying granularities of the input. Previous work focused on\naugmenting recurrent neural networks with simple attention mechanisms which are\na function of the similarity between a question embedding and an answer\nembeddings across time. We extend this by making the attention mechanism\ndependent on a global embedding of the answer attained using a separate\nnetwork.\n  We evaluate our system on InsuranceQA, a large question answering dataset.\nOur model outperforms current state-of-the-art results on InsuranceQA. Further,\nwe visualize which sections of text our attention mechanism focuses on, and\nexplore its performance across different parameter settings.", "published": "2017-07-05 13:08:03", "link": "http://arxiv.org/abs/1707.01378v4", "categories": ["cs.CL", "68T50", "I.2.7; I.2.6"], "primary_category": "cs.CL"}
{"title": "Context Aware Document Embedding", "abstract": "Recently, doc2vec has achieved excellent results in different tasks. In this\npaper, we present a context aware variant of doc2vec. We introduce a novel\nweight estimating mechanism that generates weights for each word occurrence\naccording to its contribution in the context, using deep neural networks. Our\ncontext aware model can achieve similar results compared to doc2vec initialized\nbyWikipedia trained vectors, while being much more efficient and free from\nheavy external corpus. Analysis of context aware weights shows they are a kind\nof enhanced IDF weights that capture sub-topic level keywords in documents.\nThey might result from deep neural networks that learn hidden representations\nwith the least entropy.", "published": "2017-07-05 18:18:37", "link": "http://arxiv.org/abs/1707.01521v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "The Complex Negotiation Dialogue Game", "abstract": "This position paper formalises an abstract model for complex negotiation\ndialogue. This model is to be used for the benchmark of optimisation algorithms\nranging from Reinforcement Learning to Stochastic Games, through Transfer\nLearning, One-Shot Learning or others.", "published": "2017-07-05 16:11:31", "link": "http://arxiv.org/abs/1707.01450v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Determining sentiment in citation text and analyzing its impact on the\n  proposed ranking index", "abstract": "Whenever human beings interact with each other, they exchange or express\nopinions, emotions, and sentiments. These opinions can be expressed in text,\nspeech or images. Analysis of these sentiments is one of the popular research\nareas of present day researchers. Sentiment analysis, also known as opinion\nmining tries to identify or classify these sentiments or opinions into two\nbroad categories - positive and negative. In recent years, the scientific\ncommunity has taken a lot of interest in analyzing sentiment in textual data\navailable in various social media platforms. Much work has been done on social\nmedia conversations, blog posts, newspaper articles and various narrative\ntexts. However, when it comes to identifying emotions from scientific papers,\nresearchers have faced some difficulties due to the implicit and hidden nature\nof opinion. By default, citation instances are considered inherently positive\nin emotion. Popular ranking and indexing paradigms often neglect the opinion\npresent while citing. In this paper, we have tried to achieve three objectives.\nFirst, we try to identify the major sentiment in the citation text and assign a\nscore to the instance. We have used a statistical classifier for this purpose.\nSecondly, we have proposed a new index (we shall refer to it hereafter as\nM-index) which takes into account both the quantitative and qualitative factors\nwhile scoring a paper. Thirdly, we developed a ranking of research papers based\non the M-index. We also try to explain how the M-index impacts the ranking of\nscientific papers.", "published": "2017-07-05 15:03:30", "link": "http://arxiv.org/abs/1707.01425v1", "categories": ["cs.IR", "cs.CL", "cs.DL"], "primary_category": "cs.IR"}
{"title": "Like trainer, like bot? Inheritance of bias in algorithmic content\n  moderation", "abstract": "The internet has become a central medium through which `networked publics'\nexpress their opinions and engage in debate. Offensive comments and personal\nattacks can inhibit participation in these spaces. Automated content moderation\naims to overcome this problem using machine learning classifiers trained on\nlarge corpora of texts manually annotated for offence. While such systems could\nhelp encourage more civil debate, they must navigate inherently normatively\ncontestable boundaries, and are subject to the idiosyncratic norms of the human\nraters who provide the training data. An important objective for platforms\nimplementing such measures might be to ensure that they are not unduly biased\ntowards or against particular norms of offence. This paper provides some\nexploratory methods by which the normative biases of algorithmic content\nmoderation systems can be measured, by way of a case study using an existing\ndataset of comments labelled for offence. We train classifiers on comments\nlabelled by different demographic subsets (men and women) to understand how\ndifferences in conceptions of offence between these groups might affect the\nperformance of the resulting models on various test sets. We conclude by\ndiscussing some of the ethical choices facing the implementers of algorithmic\nmoderation systems, given various desired levels of diversity of viewpoints\namongst discussion participants.", "published": "2017-07-05 17:19:45", "link": "http://arxiv.org/abs/1707.01477v1", "categories": ["cs.CY", "cs.CL", "cs.LG", "H.1.2; I.2.6; I.2.1; J.7; J.4; K.4.1; K.4.3; K.5.2; I.2.7; K.4.2"], "primary_category": "cs.CY"}
{"title": "A Deep Network with Visual Text Composition Behavior", "abstract": "While natural languages are compositional, how state-of-the-art neural models\nachieve compositionality is still unclear. We propose a deep network, which not\nonly achieves competitive accuracy for text classification, but also exhibits\ncompositional behavior. That is, while creating hierarchical representations of\na piece of text, such as a sentence, the lower layers of the network distribute\ntheir layer-specific attention weights to individual words. In contrast, the\nhigher layers compose meaningful phrases and clauses, whose lengths increase as\nthe networks get deeper until fully composing the sentence.", "published": "2017-07-05 19:37:23", "link": "http://arxiv.org/abs/1707.01555v1", "categories": ["cs.CL", "cs.AI", "cs.NE"], "primary_category": "cs.CL"}
