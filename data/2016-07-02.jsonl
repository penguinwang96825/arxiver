{"title": "Text comparison using word vector representations and dimensionality\n  reduction", "abstract": "This paper describes a technique to compare large text sources using word\nvector representations (word2vec) and dimensionality reduction (t-SNE) and how\nit can be implemented using Python. The technique provides a bird's-eye view of\ntext sources, e.g. text summaries and their source material, and enables users\nto explore text sources like a geographical map. Word vector representations\ncapture many linguistic properties such as gender, tense, plurality and even\nsemantic concepts like \"capital city of\". Using dimensionality reduction, a 2D\nmap can be computed where semantically similar words are close to each other.\nThe technique uses the word2vec model from the gensim Python library and t-SNE\nfrom scikit-learn.", "published": "2016-07-02 17:17:22", "link": "http://arxiv.org/abs/1607.00534v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Representation learning for very short texts using weighted word\n  embedding aggregation", "abstract": "Short text messages such as tweets are very noisy and sparse in their use of\nvocabulary. Traditional textual representations, such as tf-idf, have\ndifficulty grasping the semantic meaning of such texts, which is important in\napplications such as event detection, opinion mining, news recommendation, etc.\nWe constructed a method based on semantic word embeddings and frequency\ninformation to arrive at low-dimensional representations for short texts\ndesigned to capture semantic similarity. For this purpose we designed a\nweight-based model and a learning procedure based on a novel median-based loss\nfunction. This paper discusses the details of our model and the optimization\nmethods, together with the experimental results on both Wikipedia and Twitter\ndata. We find that our method outperforms the baseline approaches in the\nexperiments, and that it generalizes well on different word embeddings without\nretraining. Our method is therefore capable of retaining most of the semantic\ninformation in the text, and is applicable out-of-the-box.", "published": "2016-07-02 23:10:09", "link": "http://arxiv.org/abs/1607.00570v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
