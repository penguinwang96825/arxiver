{"title": "Do Transformers Encode a Foundational Ontology? Probing Abstract Classes\n  in Natural Language", "abstract": "With the methodological support of probing (or diagnostic classification),\nrecent studies have demonstrated that Transformers encode syntactic and\nsemantic information to some extent. Following this line of research, this\npaper aims at taking semantic probing to an abstraction extreme with the goal\nof answering the following research question: can contemporary\nTransformer-based models reflect an underlying Foundational Ontology? To this\nend, we present a systematic Foundational Ontology (FO) probing methodology to\ninvestigate whether Transformers-based models encode abstract semantic\ninformation. Following different pre-training and fine-tuning regimes, we\npresent an extensive evaluation of a diverse set of large-scale language models\nover three distinct and complementary FO tagging experiments. Specifically, we\npresent and discuss the following conclusions: (1) The probing results indicate\nthat Transformer-based models incidentally encode information related to\nFoundational Ontologies during the pre-training pro-cess; (2) Robust FO taggers\n(accuracy of 90 percent)can be efficiently built leveraging on this knowledge.", "published": "2022-01-25 12:11:46", "link": "http://arxiv.org/abs/2201.10262v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-channel Attentive Graph Convolutional Network With Sentiment\n  Fusion For Multimodal Sentiment Analysis", "abstract": "Nowadays, with the explosive growth of multimodal reviews on social media\nplatforms, multimodal sentiment analysis has recently gained popularity because\nof its high relevance to these social media posts. Although most previous\nstudies design various fusion frameworks for learning an interactive\nrepresentation of multiple modalities, they fail to incorporate sentimental\nknowledge into inter-modality learning. This paper proposes a Multi-channel\nAttentive Graph Convolutional Network (MAGCN), consisting of two main\ncomponents: cross-modality interactive learning and sentimental feature fusion.\nFor cross-modality interactive learning, we exploit the self-attention\nmechanism combined with densely connected graph convolutional networks to learn\ninter-modality dynamics. For sentimental feature fusion, we utilize multi-head\nself-attention to merge sentimental knowledge into inter-modality feature\nrepresentations. Extensive experiments are conducted on three widely-used\ndatasets. The experimental results demonstrate that the proposed model achieves\ncompetitive performance on accuracy and F1 scores compared to several\nstate-of-the-art approaches.", "published": "2022-01-25 12:38:33", "link": "http://arxiv.org/abs/2201.10274v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling Multi-level Context for Informational Bias Detection by\n  Contrastive Learning and Sentential Graph Network", "abstract": "Informational bias is widely present in news articles. It refers to providing\none-sided, selective or suggestive information of specific aspects of certain\nentity to guide a specific interpretation, thereby biasing the reader's\nopinion. Sentence-level informational bias detection is a very challenging task\nin a way that such bias can only be revealed together with the context,\nexamples include collecting information from various sources or analyzing the\nentire article in combination with the background. In this paper, we integrate\nthree levels of context to detect the sentence-level informational bias in\nEnglish news articles: adjacent sentences, whole article, and articles from\nother news outlets describing the same event. Our model, MultiCTX (Multi-level\nConTeXt), uses contrastive learning and sentence graphs together with Graph\nAttention Network (GAT) to encode these three degrees of context at different\nstages by tactically composing contrastive triplets and constructing sentence\ngraphs within events. Our experiments proved that contrastive learning together\nwith sentence graphs effectively incorporates context in varying degrees and\nsignificantly outperforms the current SOTA model sentence-wise in informational\nbias detection.", "published": "2022-01-25 15:07:09", "link": "http://arxiv.org/abs/2201.10376v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Quantitative and Qualitative Analysis of Schizophrenia Language", "abstract": "Schizophrenia is one of the most disabling mental health conditions to live\nwith. Approximately one percent of the population has schizophrenia which makes\nit fairly common, and it affects many people and their families. Patients with\nschizophrenia suffer different symptoms: formal thought disorder (FTD),\ndelusions, and emotional flatness. In this paper, we quantitatively and\nqualitatively analyze the language of patients with schizophrenia measuring\nvarious linguistic features in two modalities: speech and written text. We\nexamine the following features: coherence and cohesion of thoughts, emotions,\nspecificity, level of committed belief (LCB), and personality traits. Our\nresults show that patients with schizophrenia score high in fear and\nneuroticism compared to healthy controls. In addition, they are more committed\nto their beliefs, and their writing lacks details. They score lower in most of\nthe linguistic features of cohesion with significant p-values.", "published": "2022-01-25 16:25:58", "link": "http://arxiv.org/abs/2201.10430v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Suicidal Ideation Detection on Social Media: A Review of Machine\n  Learning Methods", "abstract": "Social media platforms have transformed traditional communication methods by\nallowing users worldwide to communicate instantly, openly, and frequently.\nPeople use social media to express their opinion and share their personal\nstories and struggles. Negative feelings that express hardship, thoughts of\ndeath, and self-harm are widespread in social media, especially among young\ngenerations. Therefore, using social media to detect and identify suicidal\nideation will help provide proper intervention that will eventually dissuade\nothers from self-harming and committing suicide and prevent the spread of\nsuicidal ideations on social media. Many studies have been carried out to\nidentify suicidal ideation and behaviors in social media. This paper presents a\ncomprehensive summary of current research efforts to detect suicidal ideation\nusing machine learning algorithms on social media. This review 24 studies\ninvestigating the feasibility of social media usage for suicidal ideation\ndetection is intended to facilitate further research in the field and will be a\nbeneficial resource for researchers engaged in suicidal text classification.", "published": "2022-01-25 18:23:47", "link": "http://arxiv.org/abs/2201.10515v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Convex Polytope Modelling for Unsupervised Derivation of Semantic\n  Structure for Data-efficient Natural Language Understanding", "abstract": "Popular approaches for Natural Language Understanding (NLU) usually rely on a\nhuge amount of annotated data or handcrafted rules, which is laborious and not\nadaptive to domain extension. We recently proposed a\nConvex-Polytopic-Model-based framework that shows great potential in\nautomatically extracting semantic patterns by exploiting the raw dialog corpus.\nThe extracted semantic patterns can be used to generate semantic frames, which\nis essential in assisting NLU tasks. This paper further studies the CPM model\nin depth and visualizes its high interpretability and transparency at various\nlevels. We show that this framework can exploit\n  semantic-frame-related features in the corpus, reveal the underlying semantic\nstructure of the utterances, and boost the performance of the state-of-the-art\nNLU model with minimal supervision. We conduct our experiments on the ATIS (Air\nTravel Information System) corpus.", "published": "2022-01-25 19:12:44", "link": "http://arxiv.org/abs/2201.10588v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DOM-LM: Learning Generalizable Representations for HTML Documents", "abstract": "HTML documents are an important medium for disseminating information on the\nWeb for human consumption. An HTML document presents information in multiple\ntext formats including unstructured text, structured key-value pairs, and\ntables. Effective representation of these documents is essential for machine\nunderstanding to enable a wide range of applications, such as Question\nAnswering, Web Search, and Personalization. Existing work has either\nrepresented these documents using visual features extracted by rendering them\nin a browser, which is typically computationally expensive, or has simply\ntreated them as plain text documents, thereby failing to capture useful\ninformation presented in their HTML structure. We argue that the text and HTML\nstructure together convey important semantics of the content and therefore\nwarrant a special treatment for their representation learning. In this paper,\nwe introduce a novel representation learning approach for web pages, dubbed\nDOM-LM, which addresses the limitations of existing approaches by encoding both\ntext and DOM tree structure with a transformer-based encoder and learning\ngeneralizable representations for HTML documents via self-supervised\npre-training. We evaluate DOM-LM on a variety of webpage understanding tasks,\nincluding Attribute Extraction, Open Information Extraction, and Question\nAnswering. Our extensive experiments show that DOM-LM consistently outperforms\nall baselines designed for these tasks. In particular, DOM-LM demonstrates\nbetter generalization performance both in few-shot and zero-shot settings,\nmaking it attractive for making it suitable for real-world application settings\nwith limited labeled data.", "published": "2022-01-25 20:10:32", "link": "http://arxiv.org/abs/2201.10608v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The ABBE Corpus: Animate Beings Being Emotional", "abstract": "Emotion detection is an established NLP task of demonstrated utility for text\nunderstanding. However, basic emotion detection leaves out key information,\nnamely, who is experiencing the emotion in question. For example, it may be the\nauthor, the narrator, or a character; or the emotion may correspond to\nsomething the audience is supposed to feel, or even be unattributable to a\nspecific being, e.g., when emotions are being discussed per se. We provide the\nABBE corpus -- Animate Beings Being Emotional -- a new double-annotated corpus\nof texts that captures this key information for one class of emotion\nexperiencer, namely, animate beings in the world described by the text. Such a\ncorpus is useful for developing systems that seek to model or understand this\nspecific type of expressed emotion. Our corpus contains 30 chapters, comprising\n134,513 words, drawn from the Corpus of English Novels, and contains 2,010\nunique emotion expressions attributable to 2,227 animate beings. The emotion\nexpressions are categorized according to Plutchik's 8-category emotion model,\nand the overall inter-annotator agreement for the annotations was 0.83 Cohen's\nKappa, indicating excellent agreement. We describe in detail our annotation\nscheme and procedure, and also release the corpus for use by other researchers.", "published": "2022-01-25 20:35:52", "link": "http://arxiv.org/abs/2201.10618v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Documenting Geographically and Contextually Diverse Data Sources: The\n  BigScience Catalogue of Language Data and Resources", "abstract": "In recent years, large-scale data collection efforts have prioritized the\namount of data collected in order to improve the modeling capabilities of large\nlanguage models. This prioritization, however, has resulted in concerns with\nrespect to the rights of data subjects represented in data collections,\nparticularly when considering the difficulty in interrogating these collections\ndue to insufficient documentation and tools for analysis. Mindful of these\npitfalls, we present our methodology for a documentation-first, human-centered\ndata collection project as part of the BigScience initiative. We identified a\ngeographically diverse set of target language groups (Arabic, Basque, Chinese,\nCatalan, English, French, Indic languages, Indonesian, Niger-Congo languages,\nPortuguese, Spanish, and Vietnamese, as well as programming languages) for\nwhich to collect metadata on potential data sources. To structure this effort,\nwe developed our online catalogue as a supporting tool for gathering metadata\nthrough organized public hackathons. We present our development process;\nanalyses of the resulting resource metadata, including distributions over\nlanguages, regions, and resource types; and our lessons learned in this\nendeavor.", "published": "2022-01-25 03:05:23", "link": "http://arxiv.org/abs/2201.10066v1", "categories": ["cs.CL", "cs.DB"], "primary_category": "cs.CL"}
{"title": "Multimodal data matters: language model pre-training over structured and\n  unstructured electronic health records", "abstract": "As two important textual modalities in electronic health records (EHR), both\nstructured data (clinical codes) and unstructured data (clinical narratives)\nhave recently been increasingly applied to the healthcare domain. Most existing\nEHR-oriented studies, however, either focus on a particular modality or\nintegrate data from different modalities in a straightforward manner, which\nusually treats structured and unstructured data as two independent sources of\ninformation about patient admission and ignore the intrinsic interactions\nbetween them. In fact, the two modalities are documented during the same\nencounter where structured data inform the documentation of unstructured data\nand vice versa. In this paper, we proposed a Medical Multimodal Pre-trained\nLanguage Model, named MedM-PLM, to learn enhanced EHR representations over\nstructured and unstructured data and explore the interaction of two modalities.\nIn MedM-PLM, two Transformer-based neural network components are firstly\nadopted to learn representative characteristics from each modality. A\ncross-modal module is then introduced to model their interactions. We\npre-trained MedM-PLM on the MIMIC-III dataset and verified the effectiveness of\nthe model on three downstream clinical tasks, i.e., medication recommendation,\n30-day readmission prediction and ICD coding. Extensive experiments demonstrate\nthe power of MedM-PLM compared with state-of-the-art methods. Further analyses\nand visualizations show the robustness of our model, which could potentially\nprovide more comprehensive interpretations for clinical decision-making.", "published": "2022-01-25 06:14:49", "link": "http://arxiv.org/abs/2201.10113v7", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Language Generation for Broad-Coverage, Explainable Cognitive Systems", "abstract": "This paper describes recent progress on natural language generation (NLG) for\nlanguage-endowed intelligent agents (LEIAs) developed within the OntoAgent\ncognitive architecture. The approach draws heavily from past work on natural\nlanguage understanding in this paradigm: it uses the same knowledge bases,\ntheory of computational linguistics, agent architecture, and methodology of\ndeveloping broad-coverage capabilities over time while still supporting\nnear-term applications.", "published": "2022-01-25 16:09:19", "link": "http://arxiv.org/abs/2201.10422v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Distantly supervised end-to-end medical entity extraction from\n  electronic health records with human-level quality", "abstract": "Medical entity extraction (EE) is a standard procedure used as a first stage\nin medical texts processing. Usually Medical EE is a two-step process: named\nentity recognition (NER) and named entity normalization (NEN). We propose a\nnovel method of doing medical EE from electronic health records (EHR) as a\nsingle-step multi-label classification task by fine-tuning a transformer model\npretrained on a large EHR dataset. Our model is trained end-to-end in an\ndistantly supervised manner using targets automatically extracted from medical\nknowledge base. We show that our model learns to generalize for entities that\nare present frequently enough, achieving human-level classification quality for\nmost frequent entities. Our work demonstrates that medical entity extraction\ncan be done end-to-end without human supervision and with human quality given\nthe availability of a large enough amount of unlabeled EHR and a medical\nknowledge base.", "published": "2022-01-25 17:04:46", "link": "http://arxiv.org/abs/2201.10463v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Whose Language Counts as High Quality? Measuring Language Ideologies in\n  Text Data Selection", "abstract": "Language models increasingly rely on massive web dumps for diverse text data.\nHowever, these sources are rife with undesirable content. As such, resources\nlike Wikipedia, books, and newswire often serve as anchors for automatically\nselecting web text most suitable for language modeling, a process typically\nreferred to as quality filtering. Using a new dataset of U.S. high school\nnewspaper articles -- written by students from across the country -- we\ninvestigate whose language is preferred by the quality filter used for GPT-3.\nWe find that newspapers from larger schools, located in wealthier, educated,\nand urban ZIP codes are more likely to be classified as high quality. We then\ndemonstrate that the filter's measurement of quality is unaligned with other\nsensible metrics, such as factuality or literary acclaim. We argue that\nprivileging any corpus as high quality entails a language ideology, and more\ncare is needed to construct training corpora for language models, with better\ntransparency and justification for the inclusion or exclusion of various texts.", "published": "2022-01-25 17:20:04", "link": "http://arxiv.org/abs/2201.10474v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Text Anonymization Benchmark (TAB): A Dedicated Corpus and\n  Evaluation Framework for Text Anonymization", "abstract": "We present a novel benchmark and associated evaluation metrics for assessing\nthe performance of text anonymization methods. Text anonymization, defined as\nthe task of editing a text document to prevent the disclosure of personal\ninformation, currently suffers from a shortage of privacy-oriented annotated\ntext resources, making it difficult to properly evaluate the level of privacy\nprotection offered by various anonymization methods. This paper presents TAB\n(Text Anonymization Benchmark), a new, open-source annotated corpus developed\nto address this shortage. The corpus comprises 1,268 English-language court\ncases from the European Court of Human Rights (ECHR) enriched with\ncomprehensive annotations about the personal information appearing in each\ndocument, including their semantic category, identifier type, confidential\nattributes, and co-reference relations. Compared to previous work, the TAB\ncorpus is designed to go beyond traditional de-identification (which is limited\nto the detection of predefined semantic categories), and explicitly marks which\ntext spans ought to be masked in order to conceal the identity of the person to\nbe protected. Along with presenting the corpus and its annotation layers, we\nalso propose a set of evaluation metrics that are specifically tailored towards\nmeasuring the performance of text anonymization, both in terms of privacy\nprotection and utility preservation. We illustrate the use of the benchmark and\nthe proposed metrics by assessing the empirical performance of several baseline\ntext anonymization models. The full corpus along with its privacy-oriented\nannotation guidelines, evaluation scripts and baseline models are available on:\nhttps://github.com/NorskRegnesentral/text-anonymisation-benchmark", "published": "2022-01-25 14:34:42", "link": "http://arxiv.org/abs/2202.00443v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SPIRAL: Self-supervised Perturbation-Invariant Representation Learning\n  for Speech Pre-Training", "abstract": "We introduce a new approach for speech pre-training named SPIRAL which works\nby learning denoising representation of perturbed data in a teacher-student\nframework. Specifically, given a speech utterance, we first feed the utterance\nto a teacher network to obtain corresponding representation. Then the same\nutterance is perturbed and fed to a student network. The student network is\ntrained to output representation resembling that of the teacher. At the same\ntime, the teacher network is updated as moving average of student's weights\nover training steps. In order to prevent representation collapse, we apply an\nin-utterance contrastive loss as pre-training objective and impose position\nrandomization on the input to the teacher. SPIRAL achieves competitive or\nbetter results compared to state-of-the-art speech pre-training method wav2vec\n2.0, with significant reduction of training cost (80% for BASE model, 65% for\nLARGE model). Furthermore, we address the problem of noise-robustness that is\ncritical to real-world speech applications. We propose multi-condition\npre-training by perturbing the student's input with various types of additive\nnoise. We demonstrate that multi-condition pre-trained SPIRAL models are more\nrobust to noisy speech (9.0% - 13.3% relative word error rate reduction on real\nnoisy test data), compared to applying multi-condition training solely in the\nfine-tuning stage. Source code is available at\nhttps://github.com/huawei-noah/Speech-Backbones/tree/main/SPIRAL.", "published": "2022-01-25 09:53:36", "link": "http://arxiv.org/abs/2201.10207v3", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Explanatory Learning: Beyond Empiricism in Neural Networks", "abstract": "We introduce Explanatory Learning (EL), a framework to let machines use\nexisting knowledge buried in symbolic sequences -- e.g. explanations written in\nhieroglyphic -- by autonomously learning to interpret them. In EL, the burden\nof interpreting symbols is not left to humans or rigid human-coded compilers,\nas done in Program Synthesis. Rather, EL calls for a learned interpreter, built\nupon a limited collection of symbolic sequences paired with observations of\nseveral phenomena. This interpreter can be used to make predictions on a novel\nphenomenon given its explanation, and even to find that explanation using only\na handful of observations, like human scientists do. We formulate the EL\nproblem as a simple binary classification task, so that common end-to-end\napproaches aligned with the dominant empiricist view of machine learning could,\nin principle, solve it. To these models, we oppose Critical Rationalist\nNetworks (CRNs), which instead embrace a rationalist view on the acquisition of\nknowledge. CRNs express several desired properties by construction, they are\ntruly explainable, can adjust their processing at test-time for harder\ninferences, and can offer strong confidence guarantees on their predictions. As\na final contribution, we introduce Odeen, a basic EL environment that simulates\na small flatland-style universe full of phenomena to explain. Using Odeen as a\ntestbed, we show how CRNs outperform empiricist end-to-end approaches of\nsimilar size and architecture (Transformers) in discovering explanations for\nnovel phenomena.", "published": "2022-01-25 10:21:53", "link": "http://arxiv.org/abs/2201.10222v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "physics.hist-ph"], "primary_category": "cs.LG"}
{"title": "Improving the fusion of acoustic and text representations in RNN-T", "abstract": "The recurrent neural network transducer (RNN-T) has recently become the\nmainstream end-to-end approach for streaming automatic speech recognition\n(ASR). To estimate the output distributions over subword units, RNN-T uses a\nfully connected layer as the joint network to fuse the acoustic representations\nextracted using the acoustic encoder with the text representations obtained\nusing the prediction network based on the previous subword units. In this\npaper, we propose to use gating, bilinear pooling, and a combination of them in\nthe joint network to produce more expressive representations to feed into the\noutput layer. A regularisation method is also proposed to enable better\nacoustic encoder training by reducing the gradients back-propagated into the\nprediction network at the beginning of RNN-T training. Experimental results on\na multilingual ASR setting for voice search over nine languages show that the\njoint use of the proposed methods can result in 4%--5% relative word error rate\nreductions with only a few million extra parameters.", "published": "2022-01-25 11:20:50", "link": "http://arxiv.org/abs/2201.10240v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Zero-Shot Long-Form Voice Cloning with Dynamic Convolution Attention", "abstract": "With recent advancements in voice cloning, the performance of speech\nsynthesis for a target speaker has been rendered similar to the human level.\nHowever, autoregressive voice cloning systems still suffer from text alignment\nfailures, resulting in an inability to synthesize long sentences. In this work,\nwe propose a variant of attention-based text-to-speech system that can\nreproduce a target voice from a few seconds of reference speech and generalize\nto very long utterances as well. The proposed system is based on three\nindependently trained components: a speaker encoder, synthesizer and universal\nvocoder. Generalization to long utterances is realized using an energy-based\nattention mechanism known as Dynamic Convolution Attention, in combination with\na set of modifications proposed for the synthesizer based on Tacotron 2.\nMoreover, effective zero-shot speaker adaptation is achieved by conditioning\nboth the synthesizer and vocoder on a speaker encoder that has been pretrained\non a large corpus of diverse data. We compare several implementations of voice\ncloning systems in terms of speech naturalness, speaker similarity, alignment\nconsistency and ability to synthesize long utterances, and conclude that the\nproposed model can produce intelligible synthetic speech for extremely long\nutterances, while preserving a high extent of naturalness and similarity for\nshort texts.", "published": "2022-01-25 15:06:07", "link": "http://arxiv.org/abs/2201.10375v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD", "I.2.7"], "primary_category": "eess.AS"}
{"title": "Jointly Learning Knowledge Embedding and Neighborhood Consensus with\n  Relational Knowledge Distillation for Entity Alignment", "abstract": "Entity alignment aims at integrating heterogeneous knowledge from different\nknowledge graphs. Recent studies employ embedding-based methods by first\nlearning the representation of Knowledge Graphs and then performing entity\nalignment via measuring the similarity between entity embeddings. However, they\nfailed to make good use of the relation semantic information due to the\ntrade-off problem caused by the different objectives of learning knowledge\nembedding and neighborhood consensus. To address this problem, we propose\nRelational Knowledge Distillation for Entity Alignment (RKDEA), a Graph\nConvolutional Network (GCN) based model equipped with knowledge distillation\nfor entity alignment. We adopt GCN-based models to learn the representation of\nentities by considering the graph structure and incorporating the relation\nsemantic information into GCN via knowledge distillation. Then, we introduce a\nnovel adaptive mechanism to transfer relational knowledge so as to jointly\nlearn entity embedding and neighborhood consensus. Experimental results on\nseveral benchmarking datasets demonstrate the effectiveness of our proposed\nmodel.", "published": "2022-01-25 02:47:14", "link": "http://arxiv.org/abs/2201.11249v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Improving non-autoregressive end-to-end speech recognition with\n  pre-trained acoustic and language models", "abstract": "While Transformers have achieved promising results in end-to-end (E2E)\nautomatic speech recognition (ASR), their autoregressive (AR) structure becomes\na bottleneck for speeding up the decoding process. For real-world deployment,\nASR systems are desired to be highly accurate while achieving fast inference.\nNon-autoregressive (NAR) models have become a popular alternative due to their\nfast inference speed, but they still fall behind AR systems in recognition\naccuracy. To fulfill the two demands, in this paper, we propose a NAR\nCTC/attention model utilizing both pre-trained acoustic and language models:\nwav2vec2.0 and BERT. To bridge the modality gap between speech and text\nrepresentations obtained from the pre-trained models, we design a novel\nmodality conversion mechanism, which is more suitable for logographic\nlanguages. During inference, we employ a CTC branch to generate a target\nlength, which enables the BERT to predict tokens in parallel. We also design a\ncache-based CTC/attention joint decoding method to improve the recognition\naccuracy while keeping the decoding speed fast. Experimental results show that\nthe proposed NAR model greatly outperforms our strong wav2vec2.0 CTC baseline\n(15.1% relative CER reduction on AISHELL-1). The proposed NAR model\nsignificantly surpasses previous NAR systems on the AISHELL-1 benchmark and\nshows a potential for English tasks.", "published": "2022-01-25 05:40:55", "link": "http://arxiv.org/abs/2201.10103v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Run-and-back stitch search: novel block synchronous decoding for\n  streaming encoder-decoder ASR", "abstract": "A streaming style inference of encoder-decoder automatic speech recognition\n(ASR) system is important for reducing latency, which is essential for\ninteractive use cases. To this end, we propose a novel blockwise synchronous\ndecoding algorithm with a hybrid approach that combines endpoint prediction and\nendpoint post-determination. In the endpoint prediction, we compute the\nexpectation of the number of tokens that are yet to be emitted in the encoder\nfeatures of the current blocks using the CTC posterior. Based on the\nexpectation value, the decoder predicts the endpoint to realize continuous\nblock synchronization, as a running stitch. Meanwhile, endpoint\npost-determination probabilistically detects backward jump of the source-target\nattention, which is caused by the misprediction of endpoints. Then it resumes\ndecoding by discarding those hypotheses, as back stitch. We combine these\nmethods into a hybrid approach, namely run-and-back stitch search, which\nreduces the computational cost and latency. Evaluations of various ASR tasks\nshow the efficiency of our proposed decoding algorithm, which achieves a\nlatency reduction, for instance in the Librispeech test set from 1487 ms to 821\nms at the 90th percentile, while maintaining a high recognition accuracy.", "published": "2022-01-25 09:08:14", "link": "http://arxiv.org/abs/2201.10190v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improved Mispronunciation detection system using a hybrid CTC-ATT based\n  approach for L2 English speakers", "abstract": "This report proposes state-of-the-art research in the field of Computer\nAssisted Language Learning (CALL). Mispronunciation detection is one of the\ncore components of Computer Assisted Pronunciation Training (CAPT) systems\nwhich is a subset of CALL. Studies on automated pronunciation error detection\nbegan in the 1990s, but the development of fullfledged CAPTs has only\naccelerated in the last decade due to an increase in computing power and\navailability of mobile devices for recording speech required for pronunciation\nanalysis. Detecting Pronunciation errors is a hard problem to solve as there is\nno formal definition of correct and incorrect pronunciation. As a result,\ntypically prosodic and phoneme errors such as phoneme substitution, insertion,\nand deletion are detected. Also, it has been agreed upon that learning\npronunciation should focus on speaker intelligibility rather than sounding like\nan L1 English speaker. Initially, methods were developed on posterior\nlikelihood called Good of Pronunciation using Gaussian Mixture Model-Hidden\nMarkov Model and Deep Neural Network-Hidden Markov Model approaches. These are\ncomplex systems to implement when compared with the recently proposed ASR based\nEnd-to-End mispronunciations detection systems. The purpose of this research is\nto create End-to-End (E2E) models using Connectionist Temporal Classification\n(CTC) and Attention-based sequence decoder. Recently, E2E models have shown\nconsiderable improvement in mispronunciation detection accuracy. This research\nwill draw comparison amongst baseline models CNN-RNN-CTC, CNN-RNN-CTC with\ncharacter sequence-based attention decoder, and CNN-RNN-CTC with phoneme-based\ndecoder systems. This study will help us in deciding a better approach towards\ndeveloping an efficient mispronunciation detection system.", "published": "2022-01-25 09:28:54", "link": "http://arxiv.org/abs/2201.10198v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improving Adversarial Waveform Generation based Singing Voice Conversion\n  with Harmonic Signals", "abstract": "Adversarial waveform generation has been a popular approach as the backend of\nsinging voice conversion (SVC) to generate high-quality singing audio. However,\nthe instability of GAN also leads to other problems, such as pitch jitters and\nU/V errors. It affects the smoothness and continuity of harmonics, hence\ndegrades the conversion quality seriously. This paper proposes to feed harmonic\nsignals to the SVC model in advance to enhance audio generation. We extract the\nsine excitation from the pitch, and filter it with a linear time-varying (LTV)\nfilter estimated by a neural network. Both these two harmonic signals are\nadopted as the inputs to generate the singing waveform. In our experiments, two\nmainstream models, MelGAN and ParallelWaveGAN, are investigated to validate the\neffectiveness of the proposed approach. We conduct a MOS test on clean and\nnoisy test sets. The result shows that both signals significantly improve SVC\nin fidelity and timbre similarity. Besides, the case analysis further validates\nthat this method enhances the smoothness and continuity of harmonics in the\ngenerated audio, and the filtered excitation better matches the target audio.", "published": "2022-01-25 07:06:43", "link": "http://arxiv.org/abs/2201.10130v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SASV Challenge 2022: A Spoofing Aware Speaker Verification Challenge\n  Evaluation Plan", "abstract": "ASV (automatic speaker verification) systems are intrinsically required to\nreject both non-target (e.g., voice uttered by different speaker) and spoofed\n(e.g., synthesised or converted) inputs. However, there is little consideration\nfor how ASV systems themselves should be adapted when they are expected to\nencounter spoofing attacks, nor when they operate in tandem with CMs (spoofing\ncountermeasures), much less how both systems should be jointly optimised.\n  The goal of the first SASV (spoofing-aware speaker verification) challenge, a\nspecial sesscion in ISCA INTERSPEECH 2022, is to promote development of\nintegrated systems that can perform ASV and CM simultaneously.", "published": "2022-01-25 12:53:34", "link": "http://arxiv.org/abs/2201.10283v2", "categories": ["cs.SD", "cs.CR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Transformer-Based Video Front-Ends for Audio-Visual Speech Recognition\n  for Single and Multi-Person Video", "abstract": "Audio-visual automatic speech recognition (AV-ASR) extends speech recognition\nby introducing the video modality as an additional source of information. In\nthis work, the information contained in the motion of the speaker's mouth is\nused to augment the audio features. The video modality is traditionally\nprocessed with a 3D convolutional neural network (e.g. 3D version of VGG).\nRecently, image transformer networks arXiv:2010.11929 demonstrated the ability\nto extract rich visual features for image classification tasks. Here, we\npropose to replace the 3D convolution with a video transformer to extract\nvisual features. We train our baselines and the proposed model on a large scale\ncorpus of YouTube videos. The performance of our approach is evaluated on a\nlabeled subset of YouTube videos as well as on the LRS3-TED public corpus. Our\nbest video-only model obtains 31.4% WER on YTDEV18 and 17.0% on LRS3-TED, a 10%\nand 15% relative improvements over our convolutional baseline. We achieve the\nstate of the art performance of the audio-visual recognition on the LRS3-TED\nafter fine-tuning our model (1.6% WER). In addition, in a series of experiments\non multi-person AV-ASR, we obtained an average relative reduction of 2% over\nour convolutional video frontend.", "published": "2022-01-25 16:35:54", "link": "http://arxiv.org/abs/2201.10439v3", "categories": ["cs.CV", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Prediction of Neonatal Respiratory Distress in Term Babies at Birth from\n  Digital Stethoscope Recorded Chest Sounds", "abstract": "Neonatal respiratory distress is a common condition that if left untreated,\ncan lead to short- and long-term complications. This paper investigates the\nusage of digital stethoscope recorded chest sounds taken within 1min\npost-delivery, to enable early detection and prediction of neonatal respiratory\ndistress. Fifty-one term newborns were included in this study, 9 of whom\ndeveloped respiratory distress. For each newborn, 1min anterior and posterior\nrecordings were taken. These recordings were pre-processed to remove noisy\nsegments and obtain high-quality heart and lung sounds. The random\nundersampling boosting (RUSBoost) classifier was then trained on a variety of\nfeatures, such as power and vital sign features extracted from the heart and\nlung sounds. The RUSBoost algorithm produced specificity, sensitivity, and\naccuracy results of 85.0%, 66.7% and 81.8%, respectively.", "published": "2022-01-25 05:46:52", "link": "http://arxiv.org/abs/2201.10105v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP", "q-bio.QM"], "primary_category": "eess.AS"}
