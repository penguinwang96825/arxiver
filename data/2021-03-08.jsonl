{"title": "\"Sharks are not the threat humans are\": Argument Component Segmentation\n  in School Student Essays", "abstract": "Argument mining is often addressed by a pipeline method where segmentation of\ntext into argumentative units is conducted first and proceeded by an argument\ncomponent identification task. In this research, we apply a token-level\nclassification to identify claim and premise tokens from a new corpus of\nargumentative essays written by middle school students. To this end, we compare\na variety of state-of-the-art models such as discrete features and deep\nlearning architectures (e.g., BiLSTM networks and BERT-based architectures) to\nidentify the argument components. We demonstrate that a BERT-based multi-task\nlearning architecture (i.e., token and sentence level classification)\nadaptively pretrained on a relevant unlabeled dataset obtains the best results", "published": "2021-03-08 02:40:07", "link": "http://arxiv.org/abs/2103.04518v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MCR-Net: A Multi-Step Co-Interactive Relation Network for Unanswerable\n  Questions on Machine Reading Comprehension", "abstract": "Question answering systems usually use keyword searches to retrieve potential\npassages related to a question, and then extract the answer from passages with\nthe machine reading comprehension methods. However, many questions tend to be\nunanswerable in the real world. In this case, it is significant and challenging\nhow the model determines when no answer is supported by the passage and\nabstains from answering. Most of the existing systems design a simple\nclassifier to determine answerability implicitly without explicitly modeling\nmutual interaction and relation between the question and passage, leading to\nthe poor performance for determining the unanswerable questions. To tackle this\nproblem, we propose a Multi-Step Co-Interactive Relation Network (MCR-Net) to\nexplicitly model the mutual interaction and locate key clues from coarse to\nfine by introducing a co-interactive relation module. The co-interactive\nrelation module contains a stack of interaction and fusion blocks to\ncontinuously integrate and fuse history-guided and current-query-guided clues\nin an explicit way. Experiments on the SQuAD 2.0 and DuReader datasets show\nthat our model achieves a remarkable improvement, outperforming the BERT-style\nbaselines in literature. Visualization analysis also verifies the importance of\nthe mutual interaction between the question and passage.", "published": "2021-03-08 06:38:14", "link": "http://arxiv.org/abs/2103.04567v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "InFillmore: Frame-Guided Language Generation with Bidirectional Context", "abstract": "We propose a structured extension to bidirectional-context conditional\nlanguage generation, or \"infilling,\" inspired by Frame Semantic theory\n(Fillmore, 1976). Guidance is provided through two approaches: (1) model\nfine-tuning, conditioning directly on observed symbolic frames, and (2) a novel\nextension to disjunctive lexically constrained decoding that leverages frame\nsemantic lexical units. Automatic and human evaluations confirm that\nframe-guided generation allows for explicit manipulation of intended infill\nsemantics, with minimal loss in distinguishability from human-generated text.\nOur methods flexibly apply to a variety of use scenarios, and we provide a\ncodebase and interactive demo available from\nhttps://nlp.jhu.edu/demos/infillmore.", "published": "2021-03-08 17:59:41", "link": "http://arxiv.org/abs/2103.04941v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fast and Effective Biomedical Entity Linking Using a Dual Encoder", "abstract": "Biomedical entity linking is the task of identifying mentions of biomedical\nconcepts in text documents and mapping them to canonical entities in a target\nthesaurus. Recent advancements in entity linking using BERT-based models follow\na retrieve and rerank paradigm, where the candidate entities are first selected\nusing a retriever model, and then the retrieved candidates are ranked by a\nreranker model. While this paradigm produces state-of-the-art results, they are\nslow both at training and test time as they can process only one mention at a\ntime. To mitigate these issues, we propose a BERT-based dual encoder model that\nresolves multiple mentions in a document in one shot. We show that our proposed\nmodel is multiple times faster than existing BERT-based models while being\ncompetitive in accuracy for biomedical entity linking. Additionally, we modify\nour dual encoder model for end-to-end biomedical entity linking that performs\nboth mention span detection and entity disambiguation and out-performs two\nrecently proposed models.", "published": "2021-03-08 19:32:28", "link": "http://arxiv.org/abs/2103.05028v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Domain Controlled Title Generation with Human Evaluation", "abstract": "We study automatic title generation and present a method for generating\ndomain-controlled titles for scientific articles. A good title allows you to\nget the attention that your research deserves. A title can be interpreted as a\nhigh-compression description of a document containing information on the\nimplemented process. For domain-controlled titles, we used the pre-trained\ntext-to-text transformer model and the additional token technique. Title tokens\nare sampled from a local distribution (which is a subset of global vocabulary)\nof the domain-specific vocabulary and not global vocabulary, thereby generating\na catchy title and closely linking it to its corresponding abstract. Generated\ntitles looked realistic, convincing, and very close to the ground truth. We\nhave performed automated evaluation using ROUGE metric and human evaluation\nusing five parameters to make a comparison between human and machine-generated\ntitles. The titles produced were considered acceptable with higher metric\nratings in contrast to the original titles. Thus we concluded that our research\nproposes a promising method for domain-controlled title generation.", "published": "2021-03-08 20:55:55", "link": "http://arxiv.org/abs/2103.05069v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Few-Shot Learning of an Interleaved Text Summarization Model by\n  Pretraining with Synthetic Data", "abstract": "Interleaved texts, where posts belonging to different threads occur in a\nsequence, commonly occur in online chat posts, so that it can be time-consuming\nto quickly obtain an overview of the discussions. Existing systems first\ndisentangle the posts by threads and then extract summaries from those threads.\nA major issue with such systems is error propagation from the disentanglement\ncomponent. While end-to-end trainable summarization system could obviate\nexplicit disentanglement, such systems require a large amount of labeled data.\nTo address this, we propose to pretrain an end-to-end trainable hierarchical\nencoder-decoder system using synthetic interleaved texts. We show that by\nfine-tuning on a real-world meeting dataset (AMI), such a system out-performs a\ntraditional two-step system by 22%. We also compare against transformer models\nand observed that pretraining with synthetic data both the encoder and decoder\noutperforms the BertSumExtAbs transformer model which pretrains only the\nencoder on a large dataset.", "published": "2021-03-08 22:58:13", "link": "http://arxiv.org/abs/2103.05131v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Topological Approach to Compare Document Semantics Based on a New\n  Variant of Syntactic N-grams", "abstract": "This paper delivers a new perspective of thinking and utilizing syntactic\nn-grams (sn-grams). Sn-grams are a type of non-linear n-grams which have been\nplaying a critical role in many NLP tasks. Introducing sn-grams to comparing\ndocument semantics thus is an appealing application, and few studies have\nreported progress at this. However, when proceeding on this application, we\nfound three major issues of sn-grams: lack of significance, being sensitive to\nword orders and failing on capture indirect syntactic relations. To address\nthese issues, we propose a new variant of sn-grams named generalized phrases\n(GPs). Then based on GPs we propose a topological approach, named DSCoH, to\ncompute document semantic similarities. DSCoH has been extensively tested on\nthe document semantics comparison and the document clustering tasks. The\nexperimental results show that DSCoH can outperform state-of-the-art\nembedding-based methods.", "published": "2021-03-08 23:16:59", "link": "http://arxiv.org/abs/2103.05135v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AfriVEC: Word Embedding Models for African Languages. Case Study of Fon\n  and Nobiin", "abstract": "From Word2Vec to GloVe, word embedding models have played key roles in the\ncurrent state-of-the-art results achieved in Natural Language Processing.\nDesigned to give significant and unique vectorized representations of words and\nentities, those models have proven to efficiently extract similarities and\nestablish relationships reflecting semantic and contextual meaning among words\nand entities. African Languages, representing more than 31% of the worldwide\nspoken languages, have recently been subject to lots of research. However, to\nthe best of our knowledge, there are currently very few to none word embedding\nmodels for those languages words and entities, and none for the languages under\nstudy in this paper. After describing Glove, Word2Vec, and Poincar\\'e\nembeddings functionalities, we build Word2Vec and Poincar\\'e word embedding\nmodels for Fon and Nobiin, which show promising results. We test the\napplicability of transfer learning between these models as a landmark for\nAfrican Languages to jointly involve in mitigating the scarcity of their\nresources, and attempt to provide linguistic and social interpretations of our\nresults. Our main contribution is to arouse more interest in creating word\nembedding models proper to African Languages, ready for use, and that can\nsignificantly improve the performances of Natural Language Processing\ndownstream tasks on them. The official repository and implementation is at\nhttps://github.com/bonaventuredossou/afrivec", "published": "2021-03-08 22:58:20", "link": "http://arxiv.org/abs/2103.05132v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Pre-trained Language Models Contain Human-like Biases of What is\n  Right and Wrong to Do", "abstract": "Artificial writing is permeating our lives due to recent advances in\nlarge-scale, transformer-based language models (LMs) such as BERT, its\nvariants, GPT-2/3, and others. Using them as pre-trained models and fine-tuning\nthem for specific tasks, researchers have extended state of the art for many\nNLP tasks and shown that they capture not only linguistic knowledge but also\nretain general knowledge implicitly present in the data. Unfortunately, LMs\ntrained on unfiltered text corpora suffer from degenerated and biased\nbehaviour. While this is well established, we show that recent LMs also contain\nhuman-like biases of what is right and wrong to do, some form of ethical and\nmoral norms of the society -- they bring a \"moral direction\" to surface. That\nis, we show that these norms can be captured geometrically by a direction,\nwhich can be computed, e.g., by a PCA, in the embedding space, reflecting well\nthe agreement of phrases to social norms implicitly expressed in the training\ntexts and providing a path for attenuating or even preventing toxic\ndegeneration in LMs. Being able to rate the (non-)normativity of arbitrary\nphrases without explicitly training the LM for this task, we demonstrate the\ncapabilities of the \"moral direction\" for guiding (even other) LMs towards\nproducing normative text and showcase it on RealToxicityPrompts testbed,\npreventing the neural toxic degeneration in GPT-2.", "published": "2021-03-08 16:59:52", "link": "http://arxiv.org/abs/2103.11790v3", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Semiotically-grounded distant viewing of diagrams: insights from two\n  multimodal corpora", "abstract": "In this article, we bring together theories of multimodal communication and\ncomputational methods to study how primary school science diagrams combine\nmultiple expressive resources. We position our work within the field of digital\nhumanities, and show how annotations informed by multimodality research, which\ntarget expressive resources and discourse structure, allow imposing structure\non the output of computational methods. We illustrate our approach by analysing\ntwo multimodal diagram corpora: the first corpus is intended to support\nresearch on automatic diagram processing, whereas the second is oriented\ntowards studying diagrams as a mode of communication. Our results show that\nmultimodally-informed annotations can bring out structural patterns in the\ndiagrams, which also extend across diagrams that deal with different topics.", "published": "2021-03-08 12:04:06", "link": "http://arxiv.org/abs/2103.04692v1", "categories": ["cs.CL", "cs.CV", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Text Simplification by Tagging", "abstract": "Edit-based approaches have recently shown promising results on multiple\nmonolingual sequence transduction tasks. In contrast to conventional\nsequence-to-sequence (Seq2Seq) models, which learn to generate text from\nscratch as they are trained on parallel corpora, these methods have proven to\nbe much more effective since they are able to learn to make fast and accurate\ntransformations while leveraging powerful pre-trained language models. Inspired\nby these ideas, we present TST, a simple and efficient Text Simplification\nsystem based on sequence Tagging, leveraging pre-trained Transformer-based\nencoders. Our system makes simplistic data augmentations and tweaks in training\nand inference on a pre-existing system, which makes it less reliant on large\namounts of parallel training data, provides more control over the outputs and\nenables faster inference speeds. Our best model achieves near state-of-the-art\nperformance on benchmark test datasets for the task. Since it is fully\nnon-autoregressive, it achieves faster inference speeds by over 11 times than\nthe current state-of-the-art text simplification system.", "published": "2021-03-08 20:57:55", "link": "http://arxiv.org/abs/2103.05070v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Parallelizable Lattice Rescoring Strategy with Neural Language Models", "abstract": "This paper proposes a parallel computation strategy and a posterior-based\nlattice expansion algorithm for efficient lattice rescoring with neural\nlanguage models (LMs) for automatic speech recognition. First, lattices from\nfirst-pass decoding are expanded by the proposed posterior-based lattice\nexpansion algorithm. Second, each expanded lattice is converted into a minimal\nlist of hypotheses that covers every arc. Each hypothesis is constrained to be\nthe best path for at least one arc it includes. For each lattice, the neural LM\nscores of the minimal list are computed in parallel and are then integrated\nback to the lattice in the rescoring stage. Experiments on the Switchboard\ndataset show that the proposed rescoring strategy obtains comparable\nrecognition performance and generates more compact lattices than a competitive\nbaseline method. Furthermore, the parallel rescoring method offers more\nflexibility by simplifying the integration of PyTorch-trained neural LMs for\nlattice rescoring with Kaldi.", "published": "2021-03-08 21:23:12", "link": "http://arxiv.org/abs/2103.05081v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "CUHK-EE Voice Cloning System for ICASSP 2021 M2VoC Challenge", "abstract": "This paper presents the CUHK-EE voice cloning system for ICASSP 2021 M2VoC\nchallenge. The challenge provides two Mandarin speech corpora: the AIShell-3\ncorpus of 218 speakers with noise and reverberation and the MST corpus\nincluding high-quality speech of one male and one female speakers. 100 and 5\nutterances of 3 target speakers in different voice and style are provided in\ntrack 1 and 2 respectively, and the participants are required to synthesize\nspeech in target speaker's voice and style. We take part in the track 1 and\ncarry out voice cloning based on 100 utterances of target speakers. An\nend-to-end voicing cloning system is developed to accomplish the task, which\nincludes: 1. a text and speech front-end module with the help of forced\nalignment, 2. an acoustic model combining Tacotron2 and DurIAN to predict\nmelspectrogram, 3. a Hifigan vocoder for waveform generation. Our system\ncomprises three stages: multi-speaker training stage, target speaker adaption\nstage and target speaker synthesis stage. Our team is identified as T17. The\nsubjective evaluation results provided by the challenge organizer demonstrate\nthe effectiveness of our system. Audio samples are available at our demo page:\nhttps://daxintan-cuhk.github.io/CUHK-EE-system-M2VoC-challenge/ .", "published": "2021-03-08 12:19:58", "link": "http://arxiv.org/abs/2103.04699v5", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "An Ultra-low Power RNN Classifier for Always-On Voice Wake-Up Detection\n  Robust to Real-World Scenarios", "abstract": "We present in this paper an ultra-low power (ULP) Recurrent Neural Network\n(RNN) based classifier for an always-on voice Wake-Up Sensor (WUS) with\nperformances suitable for real-world applications. The purpose of our sensor is\nto bring down by at least a factor 100 the power consumption in background\nnoise of always-on speech processing algorithms such as Automatic Speech\nRecognition, Keyword Spotting, Speaker Verification, etc. Unlike the other\npublished approaches, we designed our wake-up sensor to be robust to unseen\nreal-world noises for realistic levels of speech and noise by carefully\ndesigning the dataset and the loss function. We also specifically trained it to\nmark only the speech start rather than adopting a traditional Voice Activity\nDetection (VAD) approach. We achieve less than 3% No Trigger Rate (NTR) for a\nduty cycle less than 1% in challenging background noises pooled using a model\nof an analogue front-end. We demonstrate the superiority of RNNs on this task\ncompared to the other tested approaches, with an estimated power consumption of\n45 nW for the RNN itself in 65nm CMOS and a minimal memory footprint of 0.52\nkB.", "published": "2021-03-08 14:38:06", "link": "http://arxiv.org/abs/2103.04792v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
