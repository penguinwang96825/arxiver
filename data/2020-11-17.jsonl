{"title": "MVP-BERT: Redesigning Vocabularies for Chinese BERT and Multi-Vocab\n  Pretraining", "abstract": "Despite the development of pre-trained language models (PLMs) significantly\nraise the performances of various Chinese natural language processing (NLP)\ntasks, the vocabulary for these Chinese PLMs remain to be the one provided by\nGoogle Chinese Bert \\cite{devlin2018bert}, which is based on Chinese\ncharacters. Second, the masked language model pre-training is based on a single\nvocabulary, which limits its downstream task performances. In this work, we\nfirst propose a novel method, \\emph{seg\\_tok}, to form the vocabulary of\nChinese BERT, with the help of Chinese word segmentation (CWS) and subword\ntokenization. Then we propose three versions of multi-vocabulary pretraining\n(MVP) to improve the models expressiveness. Experiments show that: (a) compared\nwith char based vocabulary, \\emph{seg\\_tok} does not only improves the\nperformances of Chinese PLMs on sentence level tasks, it can also improve\nefficiency; (b) MVP improves PLMs' downstream performance, especially it can\nimprove \\emph{seg\\_tok}'s performances on sequence labeling tasks.", "published": "2020-11-17 10:15:36", "link": "http://arxiv.org/abs/2011.08539v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Semi-supervised Learning for Text Classification Under\n  Large-Scale Pretraining", "abstract": "The goal of semi-supervised learning is to utilize the unlabeled, in-domain\ndataset U to improve models trained on the labeled dataset D. Under the context\nof large-scale language-model (LM) pretraining, how we can make the best use of\nU is poorly understood: is semi-supervised learning still beneficial with the\npresence of large-scale pretraining? should U be used for in-domain LM\npretraining or pseudo-label generation? how should the pseudo-label based\nsemi-supervised model be actually implemented? how different semi-supervised\nstrategies affect performances regarding D of different sizes, U of different\nsizes, etc. In this paper, we conduct comprehensive studies on semi-supervised\nlearning in the task of text classification under the context of large-scale LM\npretraining. Our studies shed important lights on the behavior of\nsemi-supervised learning methods: (1) with the presence of in-domain\npretraining LM on U, open-domain LM pretraining is unnecessary; (2) both the\nin-domain pretraining strategy and the pseudo-label based strategy introduce\nsignificant performance boosts, with the former performing better with larger\nU, the latter performing better with smaller U, and the combination leading to\nthe largest performance boost; (3) self-training (pretraining first on pseudo\nlabels D' and then fine-tuning on D) yields better performances when D is\nsmall, while joint training on the combination of pseudo labels D' and the\noriginal dataset D yields better performances when D is large. Using\nsemi-supervised learning strategies, we are able to achieve a performance of\naround 93.8% accuracy with only 50 training data points on the IMDB dataset,\nand a competitive performance of 96.6% with the full IMDB dataset. Our work\nmarks an initial step in understanding the behavior of semi-supervised learning\nmodels under the context of large-scale pretraining.", "published": "2020-11-17 13:39:05", "link": "http://arxiv.org/abs/2011.08626v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Toward Understanding Clinical Context of Medication Change Events in\n  Clinical Narratives", "abstract": "Understanding medication events in clinical narratives is essential to\nachieving a complete picture of a patient's medication history. While prior\nresearch has explored classification of medication changes from clinical notes,\nstudies to date have not considered the necessary clinical context needed for\ntheir use in real-world applications, such as medication timeline generation\nand medication reconciliation. In this paper, we present the Contextualized\nMedication Event Dataset (CMED), a dataset for capturing relevant context of\nmedication changes documented in clinical notes, which was developed using a\nnovel conceptual framework that organizes context for clinical events into\nvarious orthogonal dimensions. In this process, we define specific contextual\naspects pertinent to medication change events, characterize the dataset, and\nreport the results of preliminary experiments. CMED consists of 9,013\nmedication mentions annotated over 500 clinical notes, and will be released to\nthe community as a shared task in 2021.", "published": "2020-11-17 18:55:00", "link": "http://arxiv.org/abs/2011.08835v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Olfactory Information Extraction from Text: A Case Study on\n  Detecting Smell Experiences in Novels", "abstract": "Environmental factors determine the smells we perceive, but societal factors\nfactors shape the importance, sentiment and biases we give to them.\nDescriptions of smells in text, or as we call them `smell experiences', offer a\nwindow into these factors, but they must first be identified. To the best of\nour knowledge, no tool exists to extract references to smell experiences from\ntext. In this paper, we present two variations on a semi-supervised approach to\nidentify smell experiences in English literature. The combined set of patterns\nfrom both implementations offer significantly better performance than a\nkeyword-based baseline.", "published": "2020-11-17 19:41:02", "link": "http://arxiv.org/abs/2011.08903v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Exploring Neural Entity Representations for Semantic Information", "abstract": "Neural methods for embedding entities are typically extrinsically evaluated\non downstream tasks and, more recently, intrinsically using probing tasks.\nDownstream task-based comparisons are often difficult to interpret due to\ndifferences in task structure, while probing task evaluations often look at\nonly a few attributes and models. We address both of these issues by evaluating\na diverse set of eight neural entity embedding methods on a set of simple\nprobing tasks, demonstrating which methods are able to remember words used to\ndescribe entities, learn type, relationship and factual information, and\nidentify how frequently an entity is mentioned. We also compare these methods\nin a unified framework on two entity linking tasks and discuss how they\ngeneralize to different model architectures and datasets.", "published": "2020-11-17 21:21:37", "link": "http://arxiv.org/abs/2011.08951v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-supervised Document Clustering Based on BERT with Data Augment", "abstract": "Contrastive learning is a promising approach to unsupervised learning, as it\ninherits the advantages of well-studied deep models without a dedicated and\ncomplex model design. In this paper, based on bidirectional encoder\nrepresentations from transformers, we propose self-supervised contrastive\nlearning (SCL) as well as few-shot contrastive learning (FCL) with unsupervised\ndata augmentation (UDA) for text clustering. SCL outperforms state-of-the-art\nunsupervised clustering approaches for short texts and those for long texts in\nterms of several clustering evaluation measures. FCL achieves performance close\nto supervised learning, and FCL with UDA further improves the performance for\nshort texts.", "published": "2020-11-17 09:18:47", "link": "http://arxiv.org/abs/2011.08523v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On the Transferability of Adversarial Attacksagainst Neural Text\n  Classifier", "abstract": "Deep neural networks are vulnerable to adversarial attacks, where a small\nperturbation to an input alters the model prediction. In many cases, malicious\ninputs intentionally crafted for one model can fool another model. In this\npaper, we present the first study to systematically investigate the\ntransferability of adversarial examples for text classification models and\nexplore how various factors, including network architecture, tokenization\nscheme, word embedding, and model capacity, affect the transferability of\nadversarial examples. Based on these studies, we propose a genetic algorithm to\nfind an ensemble of models that can be used to induce adversarial examples to\nfool almost all existing models. Such adversarial examples reflect the defects\nof the learning process and the data bias in the training set. Finally, we\nderive word replacement rules that can be used for model diagnostics from these\nadversarial examples.", "published": "2020-11-17 10:45:05", "link": "http://arxiv.org/abs/2011.08558v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Curriculum CycleGAN for Textual Sentiment Domain Adaptation with\n  Multiple Sources", "abstract": "Sentiment analysis of user-generated reviews or comments on products and\nservices in social networks can help enterprises to analyze the feedback from\ncustomers and take corresponding actions for improvement. To mitigate\nlarge-scale annotations on the target domain, domain adaptation (DA) provides\nan alternate solution by learning a transferable model from other labeled\nsource domains. Existing multi-source domain adaptation (MDA) methods either\nfail to extract some discriminative features in the target domain that are\nrelated to sentiment, neglect the correlations of different sources and the\ndistribution difference among different sub-domains even in the same source, or\ncannot reflect the varying optimal weighting during different training stages.\nIn this paper, we propose a novel instance-level MDA framework, named\ncurriculum cycle-consistent generative adversarial network (C-CycleGAN), to\naddress the above issues. Specifically, C-CycleGAN consists of three\ncomponents: (1) pre-trained text encoder which encodes textual input from\ndifferent domains into a continuous representation space, (2) intermediate\ndomain generator with curriculum instance-level adaptation which bridges the\ngap across source and target domains, and (3) task classifier trained on the\nintermediate domain for final sentiment classification. C-CycleGAN transfers\nsource samples at instance-level to an intermediate domain that is closer to\nthe target domain with sentiment semantics preserved and without losing\ndiscriminative features. Further, our dynamic instance-level weighting\nmechanisms can assign the optimal weights to different source samples in each\ntraining stage. We conduct extensive experiments on three benchmark datasets\nand achieve substantial gains over state-of-the-art DA approaches. Our source\ncode is released at: https://github.com/WArushrush/Curriculum-CycleGAN.", "published": "2020-11-17 14:50:55", "link": "http://arxiv.org/abs/2011.08678v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "KddRES: A Multi-level Knowledge-driven Dialogue Dataset for Restaurant\n  Towards Customized Dialogue System", "abstract": "Compared with CrossWOZ (Chinese) and MultiWOZ (English) dataset which have\ncoarse-grained information, there is no dataset which handle fine-grained and\nhierarchical level information properly. In this paper, we publish a first\nCantonese knowledge-driven Dialogue Dataset for REStaurant (KddRES) in Hong\nKong, which grounds the information in multi-turn conversations to one specific\nrestaurant. Our corpus contains 0.8k conversations which derive from 10\nrestaurants with various styles in different regions. In addition to that, we\ndesigned fine-grained slots and intents to better capture semantic information.\nThe benchmark experiments and data statistic analysis show the diversity and\nrich annotations of our dataset. We believe the publish of KddRES can be a\nnecessary supplement of current dialogue datasets and more suitable and\nvaluable for small and middle enterprises (SMEs) of society, such as build a\ncustomized dialogue system for each restaurant. The corpus and benchmark models\nare publicly available.", "published": "2020-11-17 16:57:41", "link": "http://arxiv.org/abs/2011.08772v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Gunrock 2.0: A User Adaptive Social Conversational System", "abstract": "Gunrock 2.0 is built on top of Gunrock with an emphasis on user adaptation.\nGunrock 2.0 combines various neural natural language understanding modules,\nincluding named entity detection, linking, and dialog act prediction, to\nimprove user understanding. Its dialog management is a hierarchical model that\nhandles various topics, such as movies, music, and sports. The system-level\ndialog manager can handle question detection, acknowledgment, error handling,\nand additional functions, making downstream modules much easier to design and\nimplement. The dialog manager also adapts its topic selection to accommodate\ndifferent users' profile information, such as inferred gender and personality.\nThe generation model is a mix of templates and neural generation models.\nGunrock 2.0 is able to achieve an average rating of 3.73 at its latest build\nfrom May 29th to June 4th.", "published": "2020-11-17 19:52:32", "link": "http://arxiv.org/abs/2011.08906v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Argumentative Topology: Finding Loop(holes) in Logic", "abstract": "Advances in natural language processing have resulted in increased\ncapabilities with respect to multiple tasks. One of the possible causes of the\nobserved performance gains is the introduction of increasingly sophisticated\ntext representations. While many of the new word embedding techniques can be\nshown to capture particular notions of sentiment or associative structures, we\nexplore the ability of two different word embeddings to uncover or capture the\nnotion of logical shape in text. To this end we present a novel framework that\nwe call Topological Word Embeddings which leverages mathematical techniques in\ndynamical system analysis and data driven shape extraction (i.e. topological\ndata analysis). In this preliminary work we show that using a topological delay\nembedding we are able to capture and extract a different, shape-based notion of\nlogic aimed at answering the question \"Can we find a circle in a circular\nargument?\"", "published": "2020-11-17 21:23:58", "link": "http://arxiv.org/abs/2011.08952v1", "categories": ["cs.CL", "cs.CG"], "primary_category": "cs.CL"}
{"title": "Refining Automatic Speech Recognition System for older adults", "abstract": "Building a high quality automatic speech recognition (ASR) system with\nlimited training data has been a challenging task particularly for a narrow\ntarget population. Open-sourced ASR systems, trained on sufficient data from\nadults, are susceptible on seniors' speech due to acoustic mismatch between\nadults and seniors. With 12 hours of training data, we attempt to develop an\nASR system for socially isolated seniors (80+ years old) with possible\ncognitive impairments. We experimentally identify that ASR for the adult\npopulation performs poorly on our target population and transfer learning (TL)\ncan boost the system's performance. Standing on the fundamental idea of TL,\ntuning model parameters, we further improve the system by leveraging an\nattention mechanism to utilize the model's intermediate information. Our\napproach achieves 1.58% absolute improvements over the TL model.", "published": "2020-11-17 00:00:45", "link": "http://arxiv.org/abs/2011.08346v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Cascade RNN-Transducer: Syllable Based Streaming On-device Mandarin\n  Speech Recognition with a Syllable-to-Character Converter", "abstract": "End-to-end models are favored in automatic speech recognition (ASR) because\nof its simplified system structure and superior performance. Among these\nmodels, recurrent neural network transducer (RNN-T) has achieved significant\nprogress in streaming on-device speech recognition because of its high-accuracy\nand low-latency. RNN-T adopts a prediction network to enhance language\ninformation, but its language modeling ability is limited because it still\nneeds paired speech-text data to train. Further strengthening the language\nmodeling ability through extra text data, such as shallow fusion with an\nexternal language model, only brings a small performance gain. In view of the\nfact that Mandarin Chinese is a character-based language and each character is\npronounced as a tonal syllable, this paper proposes a novel cascade RNN-T\napproach to improve the language modeling ability of RNN-T. Our approach\nfirstly uses an RNN-T to transform acoustic feature into syllable sequence, and\nthen converts the syllable sequence into character sequence through an\nRNN-T-based syllable-to-character converter. Thus a rich text repository can be\neasily used to strengthen the language model ability. By introducing several\nimportant tricks, the cascade RNN-T approach surpasses the character-based\nRNN-T by a large margin on several Mandarin test sets, with much higher\nrecognition quality and similar latency.", "published": "2020-11-17 06:42:47", "link": "http://arxiv.org/abs/2011.08469v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Structural and Functional Decomposition for Personality Image Captioning\n  in a Communication Game", "abstract": "Personality image captioning (PIC) aims to describe an image with a natural\nlanguage caption given a personality trait. In this work, we introduce a novel\nformulation for PIC based on a communication game between a speaker and a\nlistener. The speaker attempts to generate natural language captions while the\nlistener encourages the generated captions to contain discriminative\ninformation about the input images and personality traits. In this way, we\nexpect that the generated captions can be improved to naturally represent the\nimages and express the traits. In addition, we propose to adapt the language\nmodel GPT2 to perform caption generation for PIC. This enables the speaker and\nlistener to benefit from the language encoding capacity of GPT2. Our\nexperiments show that the proposed model achieves the state-of-the-art\nperformance for PIC.", "published": "2020-11-17 10:19:27", "link": "http://arxiv.org/abs/2011.08543v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Measuring the Novelty of Natural Language Text Using the Conjunctive\n  Clauses of a Tsetlin Machine Text Classifier", "abstract": "Most supervised text classification approaches assume a closed world,\ncounting on all classes being present in the data at training time. This\nassumption can lead to unpredictable behaviour during operation, whenever\nnovel, previously unseen, classes appear. Although deep learning-based methods\nhave recently been used for novelty detection, they are challenging to\ninterpret due to their black-box nature. This paper addresses\n\\emph{interpretable} open-world text classification, where the trained\nclassifier must deal with novel classes during operation. To this end, we\nextend the recently introduced Tsetlin machine (TM) with a novelty scoring\nmechanism. The mechanism uses the conjunctive clauses of the TM to measure to\nwhat degree a text matches the classes covered by the training data. We\ndemonstrate that the clauses provide a succinct interpretable description of\nknown topics, and that our scoring mechanism makes it possible to discern novel\ntopics from the known ones. Empirically, our TM-based approach outperforms\nseven other novelty detection schemes on three out of five datasets, and\nperforms second and third best on the remaining, with the added benefit of an\ninterpretable propositional logic-based representation.", "published": "2020-11-17 16:35:21", "link": "http://arxiv.org/abs/2011.08755v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2; I.5; I.7"], "primary_category": "cs.CL"}
{"title": "SHIELD: Defending Textual Neural Networks against Multiple Black-Box\n  Adversarial Attacks with Stochastic Multi-Expert Patcher", "abstract": "Even though several methods have proposed to defend textual neural network\n(NN) models against black-box adversarial attacks, they often defend against a\nspecific text perturbation strategy and/or require re-training the models from\nscratch. This leads to a lack of generalization in practice and redundant\ncomputation. In particular, the state-of-the-art transformer models (e.g.,\nBERT, RoBERTa) require great time and computation resources. By borrowing an\nidea from software engineering, in order to address these limitations, we\npropose a novel algorithm, SHIELD, which modifies and re-trains only the last\nlayer of a textual NN, and thus it \"patches\" and \"transforms\" the NN into a\nstochastic weighted ensemble of multi-expert prediction heads. Considering that\nmost of current black-box attacks rely on iterative search mechanisms to\noptimize their adversarial perturbations, SHIELD confuses the attackers by\nautomatically utilizing different weighted ensembles of predictors depending on\nthe input. In other words, SHIELD breaks a fundamental assumption of the\nattack, which is a victim NN model remains constant during an attack. By\nconducting comprehensive experiments, we demonstrate that all of CNN, RNN,\nBERT, and RoBERTa-based textual NNs, once patched by SHIELD, exhibit a relative\nenhancement of 15%--70% in accuracy on average against 14 different black-box\nattacks, outperforming 6 defensive baselines across 3 public datasets. All\ncodes are to be released.", "published": "2020-11-17 19:58:03", "link": "http://arxiv.org/abs/2011.08908v2", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Generating Natural Questions from Images for Multimodal Assistants", "abstract": "Generating natural, diverse, and meaningful questions from images is an\nessential task for multimodal assistants as it confirms whether they have\nunderstood the object and scene in the images properly. The research in visual\nquestion answering (VQA) and visual question generation (VQG) is a great step.\nHowever, this research does not capture questions that a visually-abled person\nwould ask multimodal assistants. Recently published datasets such as KB-VQA,\nFVQA, and OK-VQA try to collect questions that look for external knowledge\nwhich makes them appropriate for multimodal assistants. However, they still\ncontain many obvious and common-sense questions that humans would not usually\nask a digital assistant. In this paper, we provide a new benchmark dataset that\ncontains questions generated by human annotators keeping in mind what they\nwould ask multimodal digital assistants. Large scale annotations for several\nhundred thousand images are expensive and time-consuming, so we also present an\neffective way of automatically generating questions from unseen images. In this\npaper, we present an approach for generating diverse and meaningful questions\nthat consider image content and metadata of image (e.g., location, associated\nkeyword). We evaluate our approach using standard evaluation metrics such as\nBLEU, METEOR, ROUGE, and CIDEr to show the relevance of generated questions\nwith human-provided questions. We also measure the diversity of generated\nquestions using generative strength and inventiveness metrics. We report new\nstate-of-the-art results on the public and our datasets.", "published": "2020-11-17 19:12:23", "link": "http://arxiv.org/abs/2012.03678v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Ultra-Lightweight Speech Separation via Group Communication", "abstract": "Model size and complexity remain the biggest challenges in the deployment of\nspeech enhancement and separation systems on low-resource devices such as\nearphones and hearing aids. Although methods such as compression, distillation\nand quantization can be applied to large models, they often come with a cost on\nthe model performance. In this paper, we provide a simple model design paradigm\nthat explicitly designs ultra-lightweight models without sacrificing the\nperformance. Motivated by the sub-band frequency-LSTM (F-LSTM) architectures,\nwe introduce the group communication (GroupComm), where a feature vector is\nsplit into smaller groups and a small processing block is used to perform\ninter-group communication. Unlike standard F-LSTM models where the sub-band\noutputs are concatenated, an ultra-small module is applied on all the groups in\nparallel, which allows a significant decrease on the model size. Experiment\nresults show that comparing with a strong baseline model which is already\nlightweight, GroupComm can achieve on par performance with 35.6 times fewer\nparameters and 2.3 times fewer operations.", "published": "2020-11-17 03:23:29", "link": "http://arxiv.org/abs/2011.08397v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Rethinking the Separation Layers in Speech Separation Networks", "abstract": "Modules in all existing speech separation networks can be categorized into\nsingle-input-multi-output (SIMO) modules and single-input-single-output (SISO)\nmodules. SIMO modules generate more outputs than input, and SISO modules keep\nthe numbers of input and output the same. While the majority of separation\nmodels only contain SIMO architectures, it has also been shown that certain\ntwo-stage separation systems integrated with a post-enhancement SISO module can\nimprove the separation quality. Why performance improvements can be achieved by\nincorporating the SISO modules? Are SIMO modules always necessary? In this\npaper, we empirically examine those questions by designing models with varying\nconfigurations in the SIMO and SISO modules. We show that comparing with the\nstandard SIMO-only design, a mixed SIMO-SISO design with a same model size is\nable to improve the separation performance especially under low-overlap\nconditions. We further validate the necessity of SIMO modules and show that\nSISO-only models are still able to perform separation without sacrificing the\nperformance. The observations allow us to rethink the model design paradigm and\npresent different views on how the separation is performed.", "published": "2020-11-17 03:26:19", "link": "http://arxiv.org/abs/2011.08400v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Implicit Filter-and-sum Network for Multi-channel Speech Separation", "abstract": "Various neural network architectures have been proposed in recent years for\nthe task of multi-channel speech separation. Among them, the filter-and-sum\nnetwork (FaSNet) performs end-to-end time-domain filter-and-sum beamforming and\nhas shown effective in both ad-hoc and fixed microphone array geometries. In\nthis paper, we investigate multiple ways to improve the performance of FaSNet.\nFrom the problem formulation perspective, we change the explicit time-domain\nfilter-and-sum operation which involves all the microphones into an implicit\nfilter-and-sum operation in the latent space of only the reference microphone.\nThe filter-and-sum operation is applied on a context around the frame to be\nseparated. This allows the problem formulation to better match the objective of\nend-to-end separation. From the feature extraction perspective, we modify the\ncalculation of sample-level normalized cross correlation (NCC) features into\nfeature-level NCC (fNCC) features. This makes the model better matches the\nimplicit filter-and-sum formulation. Experiment results on both ad-hoc and\nfixed microphone array geometries show that the proposed modification to the\nFaSNet, which we refer to as iFaSNet, is able to significantly outperform the\nbenchmark FaSNet across all conditions with an on par model complexity.", "published": "2020-11-17 03:30:18", "link": "http://arxiv.org/abs/2011.08401v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Learn2Sing: Target Speaker Singing Voice Synthesis by learning from a\n  Singing Teacher", "abstract": "Singing voice synthesis has been paid rising attention with the rapid\ndevelopment of speech synthesis area. In general, a studio-level singing corpus\nis usually necessary to produce a natural singing voice from lyrics and\nmusic-related transcription. However, such a corpus is difficult to collect\nsince it's hard for many of us to sing like a professional singer. In this\npaper, we propose an approach -- Learn2Sing that only needs a singing teacher\nto generate the target speakers' singing voice without their singing voice\ndata. In our approach, a teacher's singing corpus and speech from multiple\ntarget speakers are trained in a frame-level auto-regressive acoustic model\nwhere singing and speaking share the common speaker embedding and style tag\nembedding. Meanwhile, since there is no music-related transcription for the\ntarget speaker, we use log-scale fundamental frequency (LF0) as an auxiliary\nfeature as the inputs of the acoustic model for building a unified input\nrepresentation. In order to enable the target speaker to sing without singing\nreference audio in the inference stage, a duration model and an LF0 prediction\nmodel are also trained. Particularly, we employ domain adversarial training\n(DAT) in the acoustic model, which aims to enhance the singing performance of\ntarget speakers by disentangling style from acoustic features of singing and\nspeaking data. Our experiments indicate that the proposed approach is capable\nof synthesizing singing voice for target speaker given only their speech\nsamples.", "published": "2020-11-17 06:35:39", "link": "http://arxiv.org/abs/2011.08467v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Fine-grained Emotion Strength Transfer, Control and Prediction for\n  Emotional Speech Synthesis", "abstract": "This paper proposes a unified model to conduct emotion transfer, control and\nprediction for sequence-to-sequence based fine-grained emotional speech\nsynthesis. Conventional emotional speech synthesis often needs manual labels or\nreference audio to determine the emotional expressions of synthesized speech.\nSuch coarse labels cannot control the details of speech emotion, often\nresulting in an averaged emotion expression delivery, and it is also hard to\nchoose suitable reference audio during inference. To conduct fine-grained\nemotion expression generation, we introduce phoneme-level emotion strength\nrepresentations through a learned ranking function to describe the local\nemotion details, and the sentence-level emotion category is adopted to render\nthe global emotions of synthesized speech. With the global render and local\ndescriptors of emotions, we can obtain fine-grained emotion expressions from\nreference audio via its emotion descriptors (for transfer) or directly from\nphoneme-level manual labels (for control). As for the emotional speech\nsynthesis with arbitrary text inputs, the proposed model can also predict\nphoneme-level emotion expressions from texts, which does not require any\nreference audio or manual label.", "published": "2020-11-17 07:07:22", "link": "http://arxiv.org/abs/2011.08477v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "s-Transformer: Segment-Transformer for Robust Neural Speech Synthesis", "abstract": "Neural end-to-end text-to-speech (TTS) , which adopts either a recurrent\nmodel, e.g. Tacotron, or an attention one, e.g. Transformer, to characterize a\nspeech utterance, has achieved significant improvement of speech synthesis.\nHowever, it is still very challenging to deal with different sentence lengths,\nparticularly, for long sentences where sequence model has limitation of the\neffective context length. We propose a novel segment-Transformer\n(s-Transformer), which models speech at segment level where recurrence is\nreused via cached memories for both the encoder and decoder. Long-range\ncontexts can be captured by the extended memory, meanwhile, the encoder-decoder\nattention on segment which is much easier to handle. In addition, we employ a\nmodified relative positional self attention to generalize sequence length\nbeyond a period possibly unseen in the training data. By comparing the proposed\ns-Transformer with the standard Transformer, on short sentences, both achieve\nthe same MOS scores of 4.29, which is very close to 4.32 by the recordings;\nsimilar scores of 4.22 vs 4.2 on long sentences, and significantly better for\nextra-long sentences with a gain of 0.2 in MOS. Since the cached memory is\nupdated with time, the s-Transformer generates rather natural and coherent\nspeech for a long period of time.", "published": "2020-11-17 07:24:04", "link": "http://arxiv.org/abs/2011.08480v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Optimizing voice conversion network with cycle consistency loss of\n  speaker identity", "abstract": "We propose a novel training scheme to optimize voice conversion network with\na speaker identity loss function. The training scheme not only minimizes\nframe-level spectral loss, but also speaker identity loss. We introduce a cycle\nconsistency loss that constrains the converted speech to maintain the same\nspeaker identity as reference speech at utterance level. While the proposed\ntraining scheme is applicable to any voice conversion networks, we formulate\nthe study under the average model voice conversion framework in this paper.\nExperiments conducted on CMU-ARCTIC and CSTR-VCTK corpus confirm that the\nproposed method outperforms baseline methods in terms of speaker similarity.", "published": "2020-11-17 10:27:39", "link": "http://arxiv.org/abs/2011.08548v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Accent and Speaker Disentanglement in Many-to-many Voice Conversion", "abstract": "This paper proposes an interesting voice and accent joint conversion\napproach, which can convert an arbitrary source speaker's voice to a target\nspeaker with non-native accent. This problem is challenging as each target\nspeaker only has training data in native accent and we need to disentangle\naccent and speaker information in the conversion model training and re-combine\nthem in the conversion stage. In our recognition-synthesis conversion\nframework, we manage to solve this problem by two proposed tricks. First, we\nuse accent-dependent speech recognizers to obtain bottleneck features for\ndifferent accented speakers. This aims to wipe out other factors beyond the\nlinguistic information in the BN features for conversion model training.\nSecond, we propose to use adversarial training to better disentangle the\nspeaker and accent information in our encoder-decoder based conversion model.\nSpecifically, we plug an auxiliary speaker classifier to the encoder, trained\nwith an adversarial loss to wipe out speaker information from the encoder\noutput. Experiments show that our approach is superior to the baseline. The\nproposed tricks are quite effective in improving accentedness and audio quality\nand speaker similarity are well maintained.", "published": "2020-11-17 13:07:51", "link": "http://arxiv.org/abs/2011.08609v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Adversarial Training for Multi-domain Speaker Recognition", "abstract": "In real-life applications, the performance of speaker recognition systems\nalways degrades when there is a mismatch between training and evaluation data.\nMany domain adaptation methods have been successfully used for eliminating the\ndomain mismatches in speaker recognition. However, usually both training and\nevaluation data themselves can be composed of several subsets. These inner\nvariances of each dataset can also be considered as different domains.\nDifferent distributed subsets in source or target domain dataset can also cause\nmulti-domain mismatches, which are influential to speaker recognition\nperformance. In this study, we propose to use adversarial training for\nmulti-domain speaker recognition to solve the domain mismatch and the dataset\nvariance problems. By adopting the proposed method, we are able to obtain both\nmulti-domain-invariant and speaker-discriminative speech representations for\nspeaker recognition. Experimental results on DAC13 dataset indicate that the\nproposed method is not only effective to solve the multi-domain mismatch\nproblem, but also outperforms the compared unsupervised domain adaptation\nmethods.", "published": "2020-11-17 13:25:29", "link": "http://arxiv.org/abs/2011.08623v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Controllable Emotion Transfer For End-to-End Speech Synthesis", "abstract": "Emotion embedding space learned from references is a straightforward approach\nfor emotion transfer in encoder-decoder structured emotional text to speech\n(TTS) systems. However, the transferred emotion in the synthetic speech is not\naccurate and expressive enough with emotion category confusions. Moreover, it\nis hard to select an appropriate reference to deliver desired emotion strength.\nTo solve these problems, we propose a novel approach based on Tacotron. First,\nwe plug two emotion classifiers -- one after the reference encoder, one after\nthe decoder output -- to enhance the emotion-discriminative ability of the\nemotion embedding and the predicted mel-spectrum. Second, we adopt style loss\nto measure the difference between the generated and reference mel-spectrum. The\nemotion strength in the synthetic speech can be controlled by adjusting the\nvalue of the emotion embedding as the emotion embedding can be viewed as the\nfeature map of the mel-spectrum. Experiments on emotion transfer and strength\ncontrol have shown that the synthetic speech of the proposed method is more\naccurate and expressive with less emotion category confusions and the control\nof emotion strength is more salient to listeners.", "published": "2020-11-17 14:54:46", "link": "http://arxiv.org/abs/2011.08679v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "FoolHD: Fooling speaker identification by Highly imperceptible\n  adversarial Disturbances", "abstract": "Speaker identification models are vulnerable to carefully designed\nadversarial perturbations of their input signals that induce misclassification.\nIn this work, we propose a white-box steganography-inspired adversarial attack\nthat generates imperceptible adversarial perturbations against a speaker\nidentification model. Our approach, FoolHD, uses a Gated Convolutional\nAutoencoder that operates in the DCT domain and is trained with a\nmulti-objective loss function, in order to generate and conceal the adversarial\nperturbation within the original audio files. In addition to hindering speaker\nidentification performance, this multi-objective loss accounts for human\nperception through a frame-wise cosine similarity between MFCC feature vectors\nextracted from the original and adversarial audio files. We validate the\neffectiveness of FoolHD with a 250-speaker identification x-vector network,\ntrained using VoxCeleb, in terms of accuracy, success rate, and\nimperceptibility. Our results show that FoolHD generates highly imperceptible\nadversarial audio files (average PESQ scores above 4.30), while achieving a\nsuccess rate of 99.6% and 99.2% in misleading the speaker identification model,\nfor untargeted and targeted settings, respectively.", "published": "2020-11-17 07:38:26", "link": "http://arxiv.org/abs/2011.08483v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Deep Networks for Direction-of-Arrival Estimation in Low SNR", "abstract": "In this work, we consider direction-of-arrival (DoA) estimation in the\npresence of extreme noise using Deep Learning (DL). In particular, we introduce\na Convolutional Neural Network (CNN) that is trained from mutli-channel data of\nthe true array manifold matrix and is able to predict angular directions using\nthe sample covariance estimate. We model the problem as a multi-label\nclassification task and train a CNN in the low-SNR regime to predict DoAs\nacross all SNRs. The proposed architecture demonstrates enhanced robustness in\nthe presence of noise, and resilience to a small number of snapshots. Moreover,\nit is able to resolve angles within the grid resolution. Experimental results\ndemonstrate significant performance gains in the low-SNR regime compared to\nstate-of-the-art methods and without the requirement of any parameter tuning.\nWe relax the assumption that the number of sources is known a priori and\npresent a training method, where the CNN learns to infer the number of sources\njointly with the DoAs. Simulation results demonstrate that the proposed CNN can\naccurately estimate off-grid angles in low SNR, while at the same time the\nnumber of sources is successfully inferred for a sufficient number of\nsnapshots. Our robust solution can be applied in several fields, ranging from\nwireless array sensors to acoustic microphones or sonars.", "published": "2020-11-17 12:52:18", "link": "http://arxiv.org/abs/2011.08848v1", "categories": ["eess.SP", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
