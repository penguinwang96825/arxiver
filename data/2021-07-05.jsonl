{"title": "Doing Good or Doing Right? Exploring the Weakness of Commonsense Causal\n  Reasoning Models", "abstract": "Pretrained language models (PLM) achieve surprising performance on the Choice\nof Plausible Alternatives (COPA) task. However, whether PLMs have truly\nacquired the ability of causal reasoning remains a question. In this paper, we\ninvestigate the problem of semantic similarity bias and reveal the\nvulnerability of current COPA models by certain attacks. Previous solutions\nthat tackle the superficial cues of unbalanced token distribution still\nencounter the same problem of semantic bias, even more seriously due to the\nutilization of more training data. We mitigate this problem by simply adding a\nregularization loss and experimental results show that this solution not only\nimproves the model's generalization ability, but also assists the models to\nperform more robustly on a challenging dataset, BCOPA-CE, which has unbiased\ntoken distribution and is more difficult for models to distinguish cause and\neffect.", "published": "2021-07-05 05:08:30", "link": "http://arxiv.org/abs/2107.01791v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The DCU-EPFL Enhanced Dependency Parser at the IWPT 2021 Shared Task", "abstract": "We describe the DCU-EPFL submission to the IWPT 2021 Shared Task on Parsing\ninto Enhanced Universal Dependencies. The task involves parsing Enhanced UD\ngraphs, which are an extension of the basic dependency trees designed to be\nmore facilitative towards representing semantic structure. Evaluation is\ncarried out on 29 treebanks in 17 languages and participants are required to\nparse the data from each language starting from raw strings. Our approach uses\nthe Stanza pipeline to preprocess the text files, XLMRoBERTa to obtain\ncontextualized token representations, and an edge-scoring and labeling model to\npredict the enhanced graph. Finally, we run a post-processing script to ensure\nall of our outputs are valid Enhanced UD graphs. Our system places 6th out of 9\nparticipants with a coarse Enhanced Labeled Attachment Score (ELAS) of 83.57.\nWe carry out additional post-deadline experiments which include using Trankit\nfor pre-processing, XLM-RoBERTa-LARGE, treebank concatenation, and multitask\nlearning between a basic and an enhanced dependency parser. All of these\nmodifications improve our initial score and our final system has a coarse ELAS\nof 88.04.", "published": "2021-07-05 12:42:59", "link": "http://arxiv.org/abs/2107.01982v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contradiction Detection in Persian Text", "abstract": "Detection of semantic contradictory sentences is one of the most challenging\nand fundamental issues for NLP applications such as recognition of textual\nentailments. Contradiction in this study includes different types of semantic\nconfrontation, such as conflict and antonymy. Due to lack of sufficient data to\napply precise machine learning and specifically deep learning methods to\nPersian and other low resource languages, rule-based approaches that can\nfunction similarly to these systems will be of a great interest. Also recently,\nemergence of new methods such as transfer learning, has opened up the\npossibility of deep learning for low-resource languages. Considering two above\npoints, in this study, along with a simple rule-base baseline, a novel\nrule-base system for identifying semantic contradiction along with a Bert base\ndeep contradiction detection system for Persian texts have been introduced. The\nrule base system has used frequent rule mining method to extract appropriate\ncontradiction rules using a development set. Extracted rules are tested for\ndifferent categories of contradictory sentences. In this system the maximum\nf-measure among contradiction categories is obtained for negation about 90% and\nthe average F-measure of system for all classes is about 76% which outperforms\nother algorithms on Persian texts. On the other hand, because of medium\nperformance of rule base system for some categories of contradiction, we use a\nBert base deep learning system using our translated dataset; with average\nF-measure of 73. Our hybrid system has f-measure of about 80.", "published": "2021-07-05 12:51:56", "link": "http://arxiv.org/abs/2107.01987v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey on Deep Learning Event Extraction: Approaches and Applications", "abstract": "Event extraction (EE) is a crucial research task for promptly apprehending\nevent information from massive textual data. With the rapid development of deep\nlearning, EE based on deep learning technology has become a research hotspot.\nNumerous methods, datasets, and evaluation metrics have been proposed in the\nliterature, raising the need for a comprehensive and updated survey. This\narticle fills the research gap by reviewing the state-of-the-art approaches,\nespecially focusing on the general domain EE based on deep learning models. We\nintroduce a new literature classification of current general domain EE research\naccording to the task definition. Afterward, we summarize the paradigm and\nmodels of EE approaches, and then discuss each of them in detail. As an\nimportant aspect, we summarize the benchmarks that support tests of predictions\nand evaluation metrics. A comprehensive comparison among different approaches\nis also provided in this survey. Finally, we conclude by summarizing future\nresearch directions facing the research area.", "published": "2021-07-05 16:32:45", "link": "http://arxiv.org/abs/2107.02126v6", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language\n  Understanding and Generation", "abstract": "Pre-trained models have achieved state-of-the-art results in various Natural\nLanguage Processing (NLP) tasks. Recent works such as T5 and GPT-3 have shown\nthat scaling up pre-trained language models can improve their generalization\nabilities. Particularly, the GPT-3 model with 175 billion parameters shows its\nstrong task-agnostic zero-shot/few-shot learning capabilities. Despite their\nsuccess, these large-scale models are trained on plain texts without\nintroducing knowledge such as linguistic knowledge and world knowledge. In\naddition, most large-scale models are trained in an auto-regressive way. As a\nresult, this kind of traditional fine-tuning approach demonstrates relatively\nweak performance when solving downstream language understanding tasks. In order\nto solve the above problems, we propose a unified framework named ERNIE 3.0 for\npre-training large-scale knowledge enhanced models. It fuses auto-regressive\nnetwork and auto-encoding network, so that the trained model can be easily\ntailored for both natural language understanding and generation tasks with\nzero-shot learning, few-shot learning or fine-tuning. We trained the model with\n10 billion parameters on a 4TB corpus consisting of plain texts and a\nlarge-scale knowledge graph. Empirical results show that the model outperforms\nthe state-of-the-art models on 54 Chinese NLP tasks, and its English version\nachieves the first place on the SuperGLUE benchmark (July 3, 2021), surpassing\nthe human performance by +0.8% (90.6% vs. 89.8%).", "published": "2021-07-05 16:54:59", "link": "http://arxiv.org/abs/2107.02137v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Experiments with adversarial attacks on text genres", "abstract": "Neural models based on pre-trained transformers, such as BERT or XLM-RoBERTa,\ndemonstrate SOTA results in many NLP tasks, including non-topical\nclassification, such as genre identification. However, often these approaches\nexhibit low reliability to minor alterations of the test texts. A related\nprobelm concerns topical biases in the training corpus, for example, the\nprevalence of words on a specific topic in a specific genre can trick the genre\nclassifier to recognise any text on this topic in this genre. In order to\nmitigate the reliability problem, this paper investigates techniques for\nattacking genre classifiers to understand the limitations of the transformer\nmodels and to improve their performance. While simple text attacks, such as\nthose based on word replacement using keywords extracted by tf-idf, are not\ncapable of deceiving powerful models like XLM-RoBERTa, we show that\nembedding-based algorithms which can replace some of the most ``significant''\nwords with words similar to them, for example, TextFooler, have the ability to\ninfluence model predictions in a significant proportion of cases.", "published": "2021-07-05 19:37:59", "link": "http://arxiv.org/abs/2107.02246v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Injecting Knowledge Base Information into End-to-End Joint Entity and\n  Relation Extraction and Coreference Resolution", "abstract": "We consider a joint information extraction (IE) model, solving named entity\nrecognition, coreference resolution and relation extraction jointly over the\nwhole document. In particular, we study how to inject information from a\nknowledge base (KB) in such IE model, based on unsupervised entity linking. The\nused KB entity representations are learned from either (i) hyperlinked text\ndocuments (Wikipedia), or (ii) a knowledge graph (Wikidata), and appear\ncomplementary in raising IE performance. Representations of corresponding\nentity linking (EL) candidates are added to text span representations of the\ninput document, and we experiment with (i) taking a weighted average of the EL\ncandidate representations based on their prior (in Wikipedia), and (ii) using\nan attention scheme over the EL candidate list. Results demonstrate an increase\nof up to 5% F1-score for the evaluated IE tasks on two datasets. Despite a\nstrong performance of the prior-based model, our quantitative and qualitative\nanalysis reveals the advantage of using the attention-based approach.", "published": "2021-07-05 21:49:02", "link": "http://arxiv.org/abs/2107.02286v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What Helps Transformers Recognize Conversational Structure? Importance\n  of Context, Punctuation, and Labels in Dialog Act Recognition", "abstract": "Dialog acts can be interpreted as the atomic units of a conversation, more\nfine-grained than utterances, characterized by a specific communicative\nfunction. The ability to structure a conversational transcript as a sequence of\ndialog acts -- dialog act recognition, including the segmentation -- is\ncritical for understanding dialog. We apply two pre-trained transformer models,\nXLNet and Longformer, to this task in English and achieve strong results on\nSwitchboard Dialog Act and Meeting Recorder Dialog Act corpora with dialog act\nsegmentation error rates (DSER) of 8.4% and 14.2%. To understand the key\nfactors affecting dialog act recognition, we perform a comparative analysis of\nmodels trained under different conditions. We find that the inclusion of a\nbroader conversational context helps disambiguate many dialog act classes,\nespecially those infrequent in the training data. The presence of punctuation\nin the transcripts has a massive effect on the models' performance, and a\ndetailed analysis reveals specific segmentation patterns observed in its\nabsence. Finally, we find that the label set specificity does not affect dialog\nact segmentation performance. These findings have significant practical\nimplications for spoken language understanding applications that depend heavily\non a good-quality segmentation being available.", "published": "2021-07-05 21:56:00", "link": "http://arxiv.org/abs/2107.02294v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Knowledge-based Approach for Answering Complex Questions in Persian", "abstract": "Research on open-domain question answering (QA) has a long tradition. A\nchallenge in this domain is answering complex questions (CQA) that require\ncomplex inference methods and large amounts of knowledge. In low resource\nlanguages, such as Persian, there are not many datasets for open-domain complex\nquestions and also the language processing toolkits are not very accurate. In\nthis paper, we propose a knowledge-based approach for answering Persian complex\nquestions using Farsbase; the Persian knowledge graph, exploiting PeCoQ; the\nnewly created complex Persian question dataset. In this work, we handle\nmulti-constraint and multi-hop questions by building their set of possible\ncorresponding logical forms. Then Multilingual-BERT is used to select the\nlogical form that best describes the input complex question syntactically and\nsemantically. The answer to the question is built from the answer to the\nlogical form, extracted from the knowledge graph. Experiments show that our\napproach outperforms other approaches in Persian CQA.", "published": "2021-07-05 14:01:43", "link": "http://arxiv.org/abs/2107.02040v1", "categories": ["cs.CL", "cs.AI", "I.2.7, I.2.4"], "primary_category": "cs.CL"}
{"title": "Training Adaptive Computation for Open-Domain Question Answering with\n  Computational Constraints", "abstract": "Adaptive Computation (AC) has been shown to be effective in improving the\nefficiency of Open-Domain Question Answering (ODQA) systems. However, current\nAC approaches require tuning of all model parameters, and training\nstate-of-the-art ODQA models requires significant computational resources that\nmay not be available for most researchers. We propose Adaptive Passage Encoder,\nan AC method that can be applied to an existing ODQA model and can be trained\nefficiently on a single GPU. It keeps the parameters of the base ODQA model\nfixed, but it overrides the default layer-by-layer computation of the encoder\nwith an AC policy that is trained to optimise the computational efficiency of\nthe model. Our experimental results show that our method improves upon a\nstate-of-the-art model on two datasets, and is also more accurate than previous\nAC methods due to the stronger base ODQA model. All source code and datasets\nare available at https://github.com/uclnlp/APE.", "published": "2021-07-05 15:48:14", "link": "http://arxiv.org/abs/2107.02102v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FaVIQ: FAct Verification from Information-seeking Questions", "abstract": "Despite significant interest in developing general purpose fact checking\nmodels, it is challenging to construct a large-scale fact verification dataset\nwith realistic real-world claims. Existing claims are either authored by\ncrowdworkers, thereby introducing subtle biases that are difficult to control\nfor, or manually verified by professional fact checkers, causing them to be\nexpensive and limited in scale. In this paper, we construct a large-scale\nchallenging fact verification dataset called FAVIQ, consisting of 188k claims\nderived from an existing corpus of ambiguous information-seeking questions. The\nambiguities in the questions enable automatically constructing true and false\nclaims that reflect user confusions (e.g., the year of the movie being filmed\nvs. being released). Claims in FAVIQ are verified to be natural, contain little\nlexical bias, and require a complete understanding of the evidence for\nverification. Our experiments show that the state-of-the-art models are far\nfrom solving our new task. Moreover, training on our data helps in professional\nfact-checking, outperforming models trained on the widely used dataset FEVER or\nin-domain data by up to 17% absolute. Altogether, our data will serve as a\nchallenging benchmark for natural language understanding and support future\nprogress in professional fact checking.", "published": "2021-07-05 17:31:44", "link": "http://arxiv.org/abs/2107.02153v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Is Automated Topic Model Evaluation Broken?: The Incoherence of\n  Coherence", "abstract": "Topic model evaluation, like evaluation of other unsupervised methods, can be\ncontentious. However, the field has coalesced around automated estimates of\ntopic coherence, which rely on the frequency of word co-occurrences in a\nreference corpus. Contemporary neural topic models surpass classical ones\naccording to these metrics. At the same time, topic model evaluation suffers\nfrom a validation gap: automated coherence, developed for classical models, has\nnot been validated using human experimentation for neural models. In addition,\na meta-analysis of topic modeling literature reveals a substantial\nstandardization gap in automated topic modeling benchmarks. To address the\nvalidation gap, we compare automated coherence with the two most widely\naccepted human judgment tasks: topic rating and word intrusion. To address the\nstandardization gap, we systematically evaluate a dominant classical model and\ntwo state-of-the-art neural models on two commonly used datasets. Automated\nevaluations declare a winning model when corresponding human evaluations do\nnot, calling into question the validity of fully automatic evaluations\nindependent of human judgments.", "published": "2021-07-05 17:58:52", "link": "http://arxiv.org/abs/2107.02173v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Instant One-Shot Word-Learning for Context-Specific Neural\n  Sequence-to-Sequence Speech Recognition", "abstract": "Neural sequence-to-sequence systems deliver state-of-the-art performance for\nautomatic speech recognition (ASR). When using appropriate modeling units,\ne.g., byte-pair encoded characters, these systems are in principal open\nvocabulary systems. In practice, however, they often fail to recognize words\nnot seen during training, e.g., named entities, numbers or technical terms. To\nalleviate this problem we supplement an end-to-end ASR system with a\nword/phrase memory and a mechanism to access this memory to recognize the words\nand phrases correctly. After the training of the ASR system, and when it has\nalready been deployed, a relevant word can be added or subtracted instantly\nwithout the need for further training. In this paper we demonstrate that\nthrough this mechanism our system is able to recognize more than 85% of newly\nadded words that it previously failed to recognize compared to a strong\nbaseline.", "published": "2021-07-05 21:08:34", "link": "http://arxiv.org/abs/2107.02268v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sarcasm Detection: A Comparative Study", "abstract": "Sarcasm detection is the task of identifying irony containing utterances in\nsentiment-bearing text. However, the figurative and creative nature of sarcasm\nposes a great challenge for affective computing systems performing sentiment\nanalysis. This article compiles and reviews the salient work in the literature\nof automatic sarcasm detection. Thus far, three main paradigm shifts have\noccurred in the way researchers have approached this task: 1) semi-supervised\npattern extraction to identify implicit sentiment, 2) use of hashtag-based\nsupervision, and 3) incorporation of context beyond target text. In this\narticle, we provide a comprehensive review of the datasets, approaches, trends,\nand issues in sarcasm and irony detection.", "published": "2021-07-05 21:20:29", "link": "http://arxiv.org/abs/2107.02276v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Weakly Supervised Named Entity Tagging with Learnable Logical Rules", "abstract": "We study the problem of building entity tagging systems by using a few rules\nas weak supervision. Previous methods mostly focus on disambiguation entity\ntypes based on contexts and expert-provided rules, while assuming entity spans\nare given. In this work, we propose a novel method TALLOR that bootstraps\nhigh-quality logical rules to train a neural tagger in a fully automated\nmanner. Specifically, we introduce compound rules that are composed from simple\nrules to increase the precision of boundary detection and generate more diverse\npseudo labels. We further design a dynamic label selection strategy to ensure\npseudo label quality and therefore avoid overfitting the neural tagger.\nExperiments on three datasets demonstrate that our method outperforms other\nweakly supervised methods and even rivals a state-of-the-art distantly\nsupervised tagger with a lexicon of over 2,000 terms when starting from only 20\nsimple rules. Our method can serve as a tool for rapidly building taggers in\nemerging domains and tasks. Case studies show that learned rules can\npotentially explain the predicted entities.", "published": "2021-07-05 21:32:19", "link": "http://arxiv.org/abs/2107.02282v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evaluation of Audio-Visual Alignments in Visually Grounded Speech Models", "abstract": "Systems that can find correspondences between multiple modalities, such as\nbetween speech and images, have great potential to solve different recognition\nand data analysis tasks in an unsupervised manner. This work studies multimodal\nlearning in the context of visually grounded speech (VGS) models, and focuses\non their recently demonstrated capability to extract spatiotemporal alignments\nbetween spoken words and the corresponding visual objects without ever been\nexplicitly trained for object localization or word recognition. As the main\ncontributions, we formalize the alignment problem in terms of an audiovisual\nalignment tensor that is based on earlier VGS work, introduce systematic\nmetrics for evaluating model performance in aligning visual objects and spoken\nwords, and propose a new VGS model variant for the alignment task utilizing\ncross-modal attention layer. We test our model and a previously proposed model\nin the alignment task using SPEECH-COCO captions coupled with MSCOCO images. We\ncompare the alignment performance using our proposed evaluation metrics to the\nsemantic retrieval task commonly used to evaluate VGS models. We show that\ncross-modal attention layer not only helps the model to achieve higher semantic\ncross-modal retrieval performance, but also leads to substantial improvements\nin the alignment performance between image object and spoken words.", "published": "2021-07-05 12:54:05", "link": "http://arxiv.org/abs/2108.02562v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Long-Short Transformer: Efficient Transformers for Language and Vision", "abstract": "Transformers have achieved success in both language and vision domains.\nHowever, it is prohibitively expensive to scale them to long sequences such as\nlong documents or high-resolution images, because self-attention mechanism has\nquadratic time and memory complexities with respect to the input sequence\nlength. In this paper, we propose Long-Short Transformer (Transformer-LS), an\nefficient self-attention mechanism for modeling long sequences with linear\ncomplexity for both language and vision tasks. It aggregates a novel long-range\nattention with dynamic projection to model distant correlations and a\nshort-term attention to capture fine-grained local correlations. We propose a\ndual normalization strategy to account for the scale mismatch between the two\nattention mechanisms. Transformer-LS can be applied to both autoregressive and\nbidirectional models without additional complexity. Our method outperforms the\nstate-of-the-art models on multiple tasks in language and vision domains,\nincluding the Long Range Arena benchmark, autoregressive language modeling, and\nImageNet classification. For instance, Transformer-LS achieves 0.97 test BPC on\nenwik8 using half the number of parameters than previous method, while being\nfaster and is able to handle 3x as long sequences compared to its\nfull-attention version on the same hardware. On ImageNet, it can obtain the\nstate-of-the-art results (e.g., a moderate size of 55.8M model solely trained\non 224x224 ImageNet-1K can obtain Top-1 accuracy 84.1%), while being more\nscalable on high-resolution images. The source code and models are released at\nhttps://github.com/NVIDIA/transformer-ls .", "published": "2021-07-05 18:00:14", "link": "http://arxiv.org/abs/2107.02192v3", "categories": ["cs.CV", "cs.CL", "cs.LG", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Oriental Language Recognition (OLR) 2020: Summary and Analysis", "abstract": "The fifth Oriental Language Recognition (OLR) Challenge focuses on language\nrecognition in a variety of complex environments to promote its development.\nThe OLR 2020 Challenge includes three tasks: (1) cross-channel language\nidentification, (2) dialect identification, and (3) noisy language\nidentification. We choose Cavg as the principle evaluation metric, and the\nEqual Error Rate (EER) as the secondary metric. There were 58 teams\nparticipating in this challenge and one third of the teams submitted valid\nresults. Compared with the best baseline, the Cavg values of Top 1 system for\nthe three tasks were relatively reduced by 82%, 62% and 48%, respectively. This\npaper describes the three tasks, the database profile, and the final results.\nWe also outline the novel approaches that improve the performance of language\nrecognition systems most significantly, such as the utilization of auxiliary\ninformation.", "published": "2021-07-05 12:42:40", "link": "http://arxiv.org/abs/2107.05365v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DeepRapper: Neural Rap Generation with Rhyme and Rhythm Modeling", "abstract": "Rap generation, which aims to produce lyrics and corresponding singing beats,\nneeds to model both rhymes and rhythms. Previous works for rap generation\nfocused on rhyming lyrics but ignored rhythmic beats, which are important for\nrap performance. In this paper, we develop DeepRapper, a Transformer-based rap\ngeneration system that can model both rhymes and rhythms. Since there is no\navailable rap dataset with rhythmic beats, we develop a data mining pipeline to\ncollect a large-scale rap dataset, which includes a large number of rap songs\nwith aligned lyrics and rhythmic beats. Second, we design a Transformer-based\nautoregressive language model which carefully models rhymes and rhythms.\nSpecifically, we generate lyrics in the reverse order with rhyme representation\nand constraint for rhyme enhancement and insert a beat symbol into lyrics for\nrhythm/beat modeling. To our knowledge, DeepRapper is the first system to\ngenerate rap with both rhymes and rhythms. Both objective and subjective\nevaluations demonstrate that DeepRapper generates creative and high-quality\nraps with rhymes and rhythms. Code will be released on GitHub.", "published": "2021-07-05 09:01:46", "link": "http://arxiv.org/abs/2107.01875v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A comparative study of eight human auditory models of monaural\n  processing", "abstract": "A number of auditory models have been developed using diverging approaches,\neither physiological or perceptual, but they share comparable stages of signal\nprocessing, as they are inspired by the same constitutive parts of the auditory\nsystem. We compare eight monaural models that are openly accessible in the\nAuditory Modelling Toolbox. We discuss the considerations required to make the\nmodel outputs comparable to each other, as well as the results for the\nfollowing model processing stages or their equivalents: Outer and middle ear,\ncochlear filter bank, inner hair cell, auditory nerve synapse, cochlear\nnucleus, and inferior colliculus. The discussion includes a list of\nrecommendations for future applications of auditory models.", "published": "2021-07-05 01:05:49", "link": "http://arxiv.org/abs/2107.01753v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Investigation of Practical Aspects of Single Channel Speech Separation\n  for ASR", "abstract": "Speech separation has been successfully applied as a frontend processing\nmodule of conversation transcription systems thanks to its ability to handle\noverlapped speech and its flexibility to combine with downstream tasks such as\nautomatic speech recognition (ASR). However, a speech separation model often\nintroduces target speech distortion, resulting in a sub-optimum word error rate\n(WER). In this paper, we describe our efforts to improve the performance of a\nsingle channel speech separation system. Specifically, we investigate a\ntwo-stage training scheme that firstly applies a feature level optimization\ncriterion for pretraining, followed by an ASR-oriented optimization criterion\nusing an end-to-end (E2E) speech recognition model. Meanwhile, to keep the\nmodel light-weight, we introduce a modified teacher-student learning technique\nfor model compression. By combining those approaches, we achieve a absolute\naverage WER improvement of 2.70% and 0.77% using models with less than 10M\nparameters compared with the previous state-of-the-art results on the LibriCSS\ndataset for utterance-wise evaluation and continuous evaluation, respectively", "published": "2021-07-05 10:35:35", "link": "http://arxiv.org/abs/2107.01922v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speech Synthesis from Text and Ultrasound Tongue Image-based\n  Articulatory Input", "abstract": "Articulatory information has been shown to be effective in improving the\nperformance of HMM-based and DNN-based text-to-speech synthesis. Speech\nsynthesis research focuses traditionally on text-to-speech conversion, when the\ninput is text or an estimated linguistic representation, and the target is\nsynthesized speech. However, a research field that has risen in the last decade\nis articulation-to-speech synthesis (with a target application of a Silent\nSpeech Interface, SSI), when the goal is to synthesize speech from some\nrepresentation of the movement of the articulatory organs. In this paper, we\nextend traditional (vocoder-based) DNN-TTS with articulatory input, estimated\nfrom ultrasound tongue images. We compare text-only, ultrasound-only, and\ncombined inputs. Using data from eight speakers, we show that that the combined\ntext and articulatory input can have advantages in limited-data scenarios,\nnamely, it may increase the naturalness of synthesized speech compared to\nsingle text input. Besides, we analyze the ultrasound tongue recordings of\nseveral speakers, and show that misalignments in the ultrasound transducer\npositioning can have a negative effect on the final synthesis performance.", "published": "2021-07-05 13:21:04", "link": "http://arxiv.org/abs/2107.02003v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
