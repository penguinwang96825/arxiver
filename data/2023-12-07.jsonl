{"title": "Multimodal Misinformation Detection in a South African Social Media\n  Environment", "abstract": "With the constant spread of misinformation on social media networks, a need\nhas arisen to continuously assess the veracity of digital content. This need\nhas inspired numerous research efforts on the development of misinformation\ndetection (MD) models. However, many models do not use all information\navailable to them and existing research contains a lack of relevant datasets to\ntrain the models, specifically within the South African social media\nenvironment. The aim of this paper is to investigate the transferability of\nknowledge of a MD model between different contextual environments. This\nresearch contributes a multimodal MD model capable of functioning in the South\nAfrican social media environment, as well as introduces a South African\nmisinformation dataset. The model makes use of multiple sources of information\nfor misinformation detection, namely: textual and visual elements. It uses\nbidirectional encoder representations from transformers (BERT) as the textual\nencoder and a residual network (ResNet) as the visual encoder. The model is\ntrained and evaluated on the Fakeddit dataset and a South African\nmisinformation dataset. Results show that using South African samples in the\ntraining of the model increases model performance, in a South African\ncontextual environment, and that a multimodal model retains significantly more\nknowledge than both the textual and visual unimodal models. Our study suggests\nthat the performance of a misinformation detection model is influenced by the\ncultural nuances of its operating environment and multimodal models assist in\nthe transferability of knowledge between different contextual environments.\nTherefore, local data should be incorporated into the training process of a\nmisinformation detection model in order to optimize model performance.", "published": "2023-12-07 05:20:15", "link": "http://arxiv.org/abs/2312.04052v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparing Large Language Model AI and Human-Generated Coaching Messages\n  for Behavioral Weight Loss", "abstract": "Automated coaching messages for weight control can save time and costs, but\ntheir repetitive, generic nature may limit their effectiveness compared to\nhuman coaching. Large language model (LLM) based artificial intelligence (AI)\nchatbots, like ChatGPT, could offer more personalized and novel messages to\naddress repetition with their data-processing abilities. While LLM AI\ndemonstrates promise to encourage healthier lifestyles, studies have yet to\nexamine the feasibility and acceptability of LLM-based BWL coaching. 87 adults\nin a weight-loss trial rated ten coaching messages' helpfulness (five\nhuman-written, five ChatGPT-generated) using a 5-point Likert scale, providing\nadditional open-ended feedback to justify their ratings. Participants also\nidentified which messages they believed were AI-generated. The evaluation\noccurred in two phases: messages in Phase 1 were perceived as impersonal and\nnegative, prompting revisions for Phase 2 messages. In Phase 1, AI-generated\nmessages were rated less helpful than human-written ones, with 66 percent\nreceiving a helpfulness rating of 3 or higher. However, in Phase 2, the AI\nmessages matched the human-written ones regarding helpfulness, with 82% scoring\nthree or above. Additionally, 50% were misidentified as human-written,\nsuggesting AI's sophistication in mimicking human-generated content. A thematic\nanalysis of open-ended feedback revealed that participants appreciated AI's\nempathy and personalized suggestions but found them more formulaic, less\nauthentic, and too data-focused. This study reveals the preliminary feasibility\nand acceptability of LLM AIs, like ChatGPT, in crafting potentially effective\nweight control coaching messages. Our findings also underscore areas for future\nenhancement.", "published": "2023-12-07 05:45:24", "link": "http://arxiv.org/abs/2312.04059v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analyzing the Inherent Response Tendency of LLMs: Real-World\n  Instructions-Driven Jailbreak", "abstract": "Extensive work has been devoted to improving the safety mechanism of Large\nLanguage Models (LLMs). However, LLMs still tend to generate harmful responses\nwhen faced with malicious instructions, a phenomenon referred to as \"Jailbreak\nAttack\". In our research, we introduce a novel automatic jailbreak method\nRADIAL, which bypasses the security mechanism by amplifying the potential of\nLLMs to generate affirmation responses. The jailbreak idea of our method is\n\"Inherent Response Tendency Analysis\" which identifies real-world instructions\nthat can inherently induce LLMs to generate affirmation responses and the\ncorresponding jailbreak strategy is \"Real-World Instructions-Driven Jailbreak\"\nwhich involves strategically splicing real-world instructions identified\nthrough the above analysis around the malicious instruction. Our method\nachieves excellent attack performance on English malicious instructions with\nfive open-source advanced LLMs while maintaining robust attack performance in\nexecuting cross-language attacks against Chinese malicious instructions. We\nconduct experiments to verify the effectiveness of our jailbreak idea and the\nrationality of our jailbreak strategy design. Notably, our method designed a\nsemantically coherent attack prompt, highlighting the potential risks of LLMs.\nOur study provides detailed insights into jailbreak attacks, establishing a\nfoundation for the development of safer LLMs.", "published": "2023-12-07 08:29:58", "link": "http://arxiv.org/abs/2312.04127v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is Bigger and Deeper Always Better? Probing LLaMA Across Scales and\n  Layers", "abstract": "This paper presents an in-depth analysis of Large Language Models (LLMs),\nfocusing on LLaMA, a prominent open-source foundational model in natural\nlanguage processing. Instead of assessing LLaMA through its generative output,\nwe design multiple-choice tasks to probe its intrinsic understanding in\nhigh-order tasks such as reasoning and computation. We examine the model\nhorizontally, comparing different sizes, and vertically, assessing different\nlayers. We unveil several key and uncommon findings based on the designed\nprobing tasks: (1) Horizontally, enlarging model sizes almost could not\nautomatically impart additional knowledge or computational prowess. Instead, it\ncan enhance reasoning abilities, especially in math problem solving, and helps\nreduce hallucinations, but only beyond certain size thresholds; (2) In vertical\nanalysis, the lower layers of LLaMA lack substantial arithmetic and factual\nknowledge, showcasing logical thinking, multilingual and recognitive abilities,\nwith top layers housing most computational power and real-world knowledge.", "published": "2023-12-07 14:50:41", "link": "http://arxiv.org/abs/2312.04333v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OpenAsp: A Benchmark for Multi-document Open Aspect-based Summarization", "abstract": "The performance of automatic summarization models has improved dramatically\nin recent years. Yet, there is still a gap in meeting specific information\nneeds of users in real-world scenarios, particularly when a targeted summary is\nsought, such as in the useful aspect-based summarization setting targeted in\nthis paper. Previous datasets and studies for this setting have predominantly\nconcentrated on a limited set of pre-defined aspects, focused solely on single\ndocument inputs, or relied on synthetic data. To advance research on more\nrealistic scenarios, we introduce OpenAsp, a benchmark for multi-document\n\\textit{open} aspect-based summarization. This benchmark is created using a\nnovel and cost-effective annotation protocol, by which an open aspect dataset\nis derived from existing generic multi-document summarization datasets. We\nanalyze the properties of OpenAsp showcasing its high-quality content. Further,\nwe show that the realistic open-aspect setting realized in OpenAsp poses a\nchallenge for current state-of-the-art summarization models, as well as for\nlarge language models.", "published": "2023-12-07 17:06:20", "link": "http://arxiv.org/abs/2312.04440v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An LLM Compiler for Parallel Function Calling", "abstract": "The reasoning capabilities of the recent LLMs enable them to execute external\nfunction calls to overcome their inherent limitations, such as knowledge\ncutoffs, poor arithmetic skills, or lack of access to private data. This\ndevelopment has allowed LLMs to select and coordinate multiple functions based\non the context to tackle more complex problems. However, current methods for\nfunction calling often require sequential reasoning and acting for each\nfunction which can result in high latency, cost, and sometimes inaccurate\nbehavior. To address this, we introduce LLMCompiler, which executes functions\nin parallel to efficiently orchestrate multiple function calls. Drawing\ninspiration from the principles of classical compilers, LLMCompiler enables\nparallel function calling with three components: (i) a Function Calling\nPlanner, formulating execution plans for function calling; (ii) a Task Fetching\nUnit, dispatching function calling tasks; and (iii) an Executor, executing\nthese tasks in parallel. LLMCompiler automatically generates an optimized\norchestration for the function calls and can be used with both open-source and\nclosed-source models. We have benchmarked LLMCompiler on a range of tasks with\ndifferent patterns of function calling. We observe consistent latency speedup\nof up to 3.7x, cost savings of up to 6.7x, and accuracy improvement of up to\n~9% compared to ReAct. Our code is available at\nhttps://github.com/SqueezeAILab/LLMCompiler.", "published": "2023-12-07 18:32:04", "link": "http://arxiv.org/abs/2312.04511v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Monotonic Multihead Attention", "abstract": "We introduce the Efficient Monotonic Multihead Attention (EMMA), a\nstate-of-the-art simultaneous translation model with numerically-stable and\nunbiased monotonic alignment estimation. In addition, we present improved\ntraining and inference strategies, including simultaneous fine-tuning from an\noffline translation model and reduction of monotonic alignment variance. The\nexperimental results demonstrate that the proposed model attains\nstate-of-the-art performance in simultaneous speech-to-text translation on the\nSpanish and English translation task.", "published": "2023-12-07 18:34:57", "link": "http://arxiv.org/abs/2312.04515v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PyThaiNLP: Thai Natural Language Processing in Python", "abstract": "We present PyThaiNLP, a free and open-source natural language processing\n(NLP) library for Thai language implemented in Python. It provides a wide range\nof software, models, and datasets for Thai language. We first provide a brief\nhistorical context of tools for Thai language prior to the development of\nPyThaiNLP. We then outline the functionalities it provided as well as datasets\nand pre-trained language models. We later summarize its development milestones\nand discuss our experience during its development. We conclude by demonstrating\nhow industrial and research communities utilize PyThaiNLP in their work. The\nlibrary is freely available at https://github.com/pythainlp/pythainlp.", "published": "2023-12-07 19:19:43", "link": "http://arxiv.org/abs/2312.04649v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Cost-Effective In-Context Learning for Entity Resolution: A Design Space\n  Exploration", "abstract": "Entity resolution (ER) is an important data integration task with a wide\nspectrum of applications. The state-of-the-art solutions on ER rely on\npre-trained language models (PLMs), which require fine-tuning on a lot of\nlabeled matching/non-matching entity pairs. Recently, large languages models\n(LLMs), such as GPT-4, have shown the ability to perform many tasks without\ntuning model parameters, which is known as in-context learning (ICL) that\nfacilitates effective learning from a few labeled input context demonstrations.\nHowever, existing ICL approaches to ER typically necessitate providing a task\ndescription and a set of demonstrations for each entity pair and thus have\nlimitations on the monetary cost of interfacing LLMs. To address the problem,\nin this paper, we provide a comprehensive study to investigate how to develop a\ncost-effective batch prompting approach to ER. We introduce a framework BATCHER\nconsisting of demonstration selection and question batching and explore\ndifferent design choices that support batch prompting for ER. We also devise a\ncovering-based demonstration selection strategy that achieves an effective\nbalance between matching accuracy and monetary cost. We conduct a thorough\nevaluation to explore the design space and evaluate our proposed strategies.\nThrough extensive experiments, we find that batch prompting is very\ncost-effective for ER, compared with not only PLM-based methods fine-tuned with\nextensive labeled data but also LLM-based methods with manually designed\nprompting. We also provide guidance for selecting appropriate design choices\nfor batch prompting.", "published": "2023-12-07 02:09:27", "link": "http://arxiv.org/abs/2312.03987v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RoAST: Robustifying Language Models via Adversarial Perturbation with\n  Selective Training", "abstract": "Fine-tuning pre-trained language models (LMs) has become the de facto\nstandard in many NLP tasks. Nevertheless, fine-tuned LMs are still prone to\nrobustness issues, such as adversarial robustness and model calibration.\nSeveral perspectives of robustness for LMs have been studied independently, but\nlacking a unified consideration in multiple perspectives. In this paper, we\npropose Robustifying LMs via Adversarial perturbation with Selective Training\n(RoAST), a simple yet effective fine-tuning technique to enhance the\nmulti-perspective robustness of LMs in a unified way. RoAST effectively\nincorporates two important sources for the model robustness, robustness on the\nperturbed inputs and generalizable knowledge in pre-trained LMs. To be\nspecific, RoAST introduces adversarial perturbation during fine-tuning while\nthe model parameters are selectively updated upon their relative importance to\nminimize unnecessary deviation. Under a unified evaluation of fine-tuned LMs by\nincorporating four representative perspectives of model robustness, we\ndemonstrate the effectiveness of RoAST compared to state-of-the-art fine-tuning\nmethods on six different types of LMs, which indicates its usefulness in\npractice.", "published": "2023-12-07 04:23:36", "link": "http://arxiv.org/abs/2312.04032v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Using a Large Language Model to generate a Design Structure Matrix", "abstract": "The Design Structure Matrix (DSM) is an established method used in dependency\nmodelling, especially in the design of complex engineering systems. The\ngeneration of DSM is traditionally carried out through manual means and can\ninvolve interviewing experts to elicit critical system elements and the\nrelationships between them. Such manual approaches can be time-consuming and\ncostly. This paper presents a workflow that uses a Large Language Model (LLM)\nto support the generation of DSM and improve productivity. A prototype of the\nworkflow was developed in this work and applied on a diesel engine DSM\npublished previously. It was found that the prototype could reproduce 357 out\nof 462 DSM entries published (i.e. 77.3%), suggesting that the work can aid DSM\ngeneration. A no-code version of the prototype is made available online to\nsupport future research.", "published": "2023-12-07 08:48:54", "link": "http://arxiv.org/abs/2312.04134v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Swap distance minimization in SOV languages. Cognitive and mathematical\n  foundations", "abstract": "Distance minimization is a general principle of language. A special case of\nthis principle in the domain of word order is swap distance minimization. This\nprinciple predicts that variations from a canonical order that are reached by\nfewer swaps of adjacent constituents are lest costly and thus more likely. Here\nwe investigate the principle in the context of the triple formed by subject\n(S), object (O) and verb (V). We introduce the concept of word order rotation\nas a cognitive underpinning of that prediction. When the canonical order of a\nlanguage is SOV, the principle predicts SOV < SVO, OSV < VSO, OVS < VOS, in\norder of increasing cognitive cost. We test the prediction in three flexible\norder SOV languages: Korean (Koreanic), Malayalam (Dravidian), and Sinhalese\n(Indo-European). Evidence of swap distance minimization is found in all three\nlanguages, but it is weaker in Sinhalese. Swap distance minimization is\nstronger than a preference for the canonical order in Korean and especially\nMalayalam.", "published": "2023-12-07 11:10:28", "link": "http://arxiv.org/abs/2312.04219v1", "categories": ["cs.CL", "physics.soc-ph"], "primary_category": "cs.CL"}
{"title": "PsyChat: A Client-Centric Dialogue System for Mental Health Support", "abstract": "Dialogue systems are increasingly integrated into mental health support to\nhelp clients facilitate exploration, gain insight, take action, and ultimately\nheal themselves. A practical and user-friendly dialogue system should be\nclient-centric, focusing on the client's behaviors. However, existing dialogue\nsystems publicly available for mental health support often concentrate solely\non the counselor's strategies rather than the behaviors expressed by clients.\nThis can lead to unreasonable or inappropriate counseling strategies and\ncorresponding responses generated by the dialogue system. To address this\nissue, we propose PsyChat, a client-centric dialogue system that provides\npsychological support through online chat. The client-centric dialogue system\ncomprises five modules: client behavior recognition, counselor strategy\nselection, input packer, response generator, and response selection. Both\nautomatic and human evaluations demonstrate the effectiveness and practicality\nof our proposed dialogue system for real-life mental health support.\nFurthermore, the case study demonstrates that the dialogue system can predict\nthe client's behaviors, select appropriate counselor strategies, and generate\naccurate and suitable responses.", "published": "2023-12-07 12:40:00", "link": "http://arxiv.org/abs/2312.04262v2", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Prompt Highlighter: Interactive Control for Multi-Modal LLMs", "abstract": "This study targets a critical aspect of multi-modal LLMs' (LLMs&VLMs)\ninference: explicit controllable text generation. Multi-modal LLMs empower\nmulti-modality understanding with the capability of semantic generation yet\nbring less explainability and heavier reliance on prompt contents due to their\nautoregressive generative nature. While manipulating prompt formats could\nimprove outputs, designing specific and precise prompts per task can be\nchallenging and ineffective. To tackle this issue, we introduce a novel\ninference method, Prompt Highlighter, which enables users to highlight specific\nprompt spans to interactively control the focus during generation. Motivated by\nthe classifier-free diffusion guidance, we form regular and unconditional\ncontext pairs based on highlighted tokens, demonstrating that the\nautoregressive generation in models can be guided in a classifier-free way.\nNotably, we find that, during inference, guiding the models with highlighted\ntokens through the attention weights leads to more desired outputs. Our\napproach is compatible with current LLMs and VLMs, achieving impressive\ncustomized generation results without training. Experiments confirm its\neffectiveness in focusing on input contexts and generating reliable content.\nWithout tuning on LLaVA-v1.5, our method secured 70.7 in the MMBench test and\n1552.5 in MME-perception. The code is available at:\nhttps://github.com/dvlab-research/Prompt-Highlighter/", "published": "2023-12-07 13:53:29", "link": "http://arxiv.org/abs/2312.04302v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "nerblackbox: A High-level Library for Named Entity Recognition in Python", "abstract": "We present nerblackbox, a python library to facilitate the use of\nstate-of-the-art transformer-based models for named entity recognition. It\nprovides simple-to-use yet powerful methods to access data and models from a\nwide range of sources, for fully automated model training and evaluation as\nwell as versatile model inference. While many technical challenges are solved\nand hidden from the user by default, nerblackbox also offers fine-grained\ncontrol and a rich set of customizable features. It is thus targeted both at\napplication-oriented developers as well as machine learning experts and\nresearchers.", "published": "2023-12-07 14:04:15", "link": "http://arxiv.org/abs/2312.04306v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Merging by Matching Models in Task Parameter Subspaces", "abstract": "Model merging aims to cheaply combine individual task-specific models into a\nsingle multitask model. In this work, we view past merging methods as\nleveraging different notions of a ''task parameter subspace'' in which models\nare matched before being merged. We connect the task parameter subspace of a\ngiven model to its loss landscape and formalize how this approach to model\nmerging can be seen as solving a linear system of equations. While past work\nhas generally been limited to linear systems that have a closed-form solution,\nwe consider using the conjugate gradient method to find a solution. We show\nthat using the conjugate gradient method can outperform closed-form solutions,\nenables merging via linear systems that are otherwise intractable to solve, and\nflexibly allows choosing from a wide variety of initializations and estimates\nfor the ''task parameter subspace''. We ultimately demonstrate that our merging\nframework called ''Matching Models in their Task Parameter Subspace'' (MaTS)\nachieves state-of-the-art results in multitask and intermediate-task model\nmerging. We release all of the code and checkpoints used in our work at\nhttps://github.com/r-three/mats.", "published": "2023-12-07 14:59:15", "link": "http://arxiv.org/abs/2312.04339v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "PCoQA: Persian Conversational Question Answering Dataset", "abstract": "Humans seek information regarding a specific topic through performing a\nconversation containing a series of questions and answers. In the pursuit of\nconversational question answering research, we introduce the PCoQA, the first\n\\textbf{P}ersian \\textbf{Co}nversational \\textbf{Q}uestion \\textbf{A}nswering\ndataset, a resource comprising information-seeking dialogs encompassing a total\nof 9,026 contextually-driven questions. Each dialog involves a questioner, a\nresponder, and a document from the Wikipedia; The questioner asks several\ninter-connected questions from the text and the responder provides a span of\nthe document as the answer for each question. PCoQA is designed to present\nnovel challenges compared to previous question answering datasets including\nhaving more open-ended non-factual answers, longer answers, and fewer lexical\noverlaps. This paper not only presents the comprehensive PCoQA dataset but also\nreports the performance of various benchmark models. Our models include\nbaseline models and pre-trained models, which are leveraged to boost the\nperformance of the model. The dataset and benchmarks are available at our\nGithub page.", "published": "2023-12-07 15:29:34", "link": "http://arxiv.org/abs/2312.04362v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LaMPilot: An Open Benchmark Dataset for Autonomous Driving with Language\n  Model Programs", "abstract": "Autonomous driving (AD) has made significant strides in recent years.\nHowever, existing frameworks struggle to interpret and execute spontaneous user\ninstructions, such as \"overtake the car ahead.\" Large Language Models (LLMs)\nhave demonstrated impressive reasoning capabilities showing potential to bridge\nthis gap. In this paper, we present LaMPilot, a novel framework that integrates\nLLMs into AD systems, enabling them to follow user instructions by generating\ncode that leverages established functional primitives. We also introduce\nLaMPilot-Bench, the first benchmark dataset specifically designed to\nquantitatively evaluate the efficacy of language model programs in AD. Adopting\nthe LaMPilot framework, we conduct extensive experiments to assess the\nperformance of off-the-shelf LLMs on LaMPilot-Bench. Our results demonstrate\nthe potential of LLMs in handling diverse driving scenarios and following user\ninstructions in driving. To facilitate further research in this area, we\nrelease our code and data at https://github.com/PurdueDigitalTwin/LaMPilot.", "published": "2023-12-07 15:43:52", "link": "http://arxiv.org/abs/2312.04372v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Block Metropolis-Hastings Sampler for Controllable Energy-based Text\n  Generation", "abstract": "Recent work has shown that energy-based language modeling is an effective\nframework for controllable text generation because it enables flexible\nintegration of arbitrary discriminators. However, because energy-based LMs are\nglobally normalized, approximate techniques like Metropolis-Hastings (MH) are\nrequired for inference. Past work has largely explored simple proposal\ndistributions that modify a single token at a time, like in Gibbs sampling. In\nthis paper, we develop a novel MH sampler that, in contrast, proposes re-writes\nof the entire sequence in each step via iterative prompting of a large language\nmodel. Our new sampler (a) allows for more efficient and accurate sampling from\na target distribution and (b) allows generation length to be determined through\nthe sampling procedure rather than fixed in advance, as past work has required.\nWe perform experiments on two controlled generation tasks, showing both\ndownstream performance gains and more accurate target distribution sampling in\ncomparison with single-token proposal techniques.", "published": "2023-12-07 18:30:15", "link": "http://arxiv.org/abs/2312.04510v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On Sarcasm Detection with OpenAI GPT-based Models", "abstract": "Sarcasm is a form of irony that requires readers or listeners to interpret\nits intended meaning by considering context and social cues. Machine learning\nclassification models have long had difficulty detecting sarcasm due to its\nsocial complexity and contradictory nature.\n  This paper explores the applications of the Generative Pretrained Transformer\n(GPT) models, including GPT-3, InstructGPT, GPT-3.5, and GPT-4, in detecting\nsarcasm in natural language. It tests fine-tuned and zero-shot models of\ndifferent sizes and releases.\n  The GPT models were tested on the political and balanced (pol-bal) portion of\nthe popular Self-Annotated Reddit Corpus (SARC 2.0) sarcasm dataset. In the\nfine-tuning case, the largest fine-tuned GPT-3 model achieves accuracy and\n$F_1$-score of 0.81, outperforming prior models. In the zero-shot case, one of\nGPT-4 models yields an accuracy of 0.70 and $F_1$-score of 0.75. Other models\nscore lower. Additionally, a model's performance may improve or deteriorate\nwith each release, highlighting the need to reassess performance after each\nrelease.", "published": "2023-12-07 19:00:56", "link": "http://arxiv.org/abs/2312.04642v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Self-Supervised Behavior Cloned Transformers are Path Crawlers for Text\n  Games", "abstract": "In this work, we introduce a self-supervised behavior cloning transformer for\ntext games, which are challenging benchmarks for multi-step reasoning in\nvirtual environments. Traditionally, Behavior Cloning Transformers excel in\nsuch tasks but rely on supervised training data. Our approach auto-generates\ntraining data by exploring trajectories (defined by common macro-action\nsequences) that lead to reward within the games, while determining the\ngenerality and utility of these trajectories by rapidly training small models\nthen evaluating their performance on unseen development games. Through\nempirical analysis, we show our method consistently uncovers generalizable\ntraining data, achieving about 90\\% performance of supervised systems across\nthree benchmark text games.", "published": "2023-12-07 19:39:11", "link": "http://arxiv.org/abs/2312.04657v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LaRS: Latent Reasoning Skills for Chain-of-Thought Reasoning", "abstract": "Chain-of-thought (CoT) prompting is a popular in-context learning (ICL)\napproach for large language models (LLMs), especially when tackling complex\nreasoning tasks. Traditional ICL approaches construct prompts using examples\nthat contain questions similar to the input question. However, CoT prompting,\nwhich includes crucial intermediate reasoning steps (rationales) within its\nexamples, necessitates selecting examples based on these rationales rather than\nthe questions themselves. Existing methods require human experts or pre-trained\nLLMs to describe the skill, a high-level abstraction of rationales, to guide\nthe selection. These methods, however, are often costly and difficult to scale.\nInstead, this paper introduces a new approach named Latent Reasoning Skills\n(LaRS) that employs unsupervised learning to create a latent space\nrepresentation of rationales, with a latent variable called a reasoning skill.\nConcurrently, LaRS learns a reasoning policy to determine the required\nreasoning skill for a given question. Then the ICL examples are selected by\naligning the reasoning skills between past examples and the question. This\napproach is theoretically grounded and compute-efficient, eliminating the need\nfor auxiliary LLM inference or manual prompt design. Empirical results\ndemonstrate that LaRS consistently outperforms SOTA skill-based selection\nmethods, processing example banks four times faster, reducing LLM inferences\nduring the selection stage by half, and showing greater robustness to\nsub-optimal example banks.", "published": "2023-12-07 20:36:10", "link": "http://arxiv.org/abs/2312.04684v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Simul-LLM: A Framework for Exploring High-Quality Simultaneous\n  Translation with Large Language Models", "abstract": "Large language models (LLMs) with billions of parameters and pretrained on\nmassive amounts of data are now capable of near or better than state-of-the-art\nperformance in a variety of downstream natural language processing tasks.\nNeural machine translation (NMT) is one such task that LLMs have been applied\nto with great success. However, little research has focused on applying LLMs to\nthe more difficult subset of NMT called simultaneous translation (SimulMT),\nwhere translation begins before the entire source context is available to the\nmodel. In this paper, we address key challenges facing LLMs fine-tuned for\nSimulMT, validate classical SimulMT concepts and practices in the context of\nLLMs, explore adapting LLMs that are fine-tuned for NMT to the task of SimulMT,\nand introduce Simul-LLM, the first open-source fine-tuning and evaluation\npipeline development framework for LLMs focused on SimulMT.", "published": "2023-12-07 20:42:05", "link": "http://arxiv.org/abs/2312.04691v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Is Feedback All You Need? Leveraging Natural Language Feedback in\n  Goal-Conditioned Reinforcement Learning", "abstract": "Despite numerous successes, the field of reinforcement learning (RL) remains\nfar from matching the impressive generalisation power of human behaviour\nlearning. One possible way to help bridge this gap be to provide RL agents with\nricher, more human-like feedback expressed in natural language. To investigate\nthis idea, we first extend BabyAI to automatically generate language feedback\nfrom the environment dynamics and goal condition success. Then, we modify the\nDecision Transformer architecture to take advantage of this additional signal.\nWe find that training with language feedback either in place of or in addition\nto the return-to-go or goal descriptions improves agents' generalisation\nperformance, and that agents can benefit from feedback even when this is only\navailable during training, but not at inference.", "published": "2023-12-07 22:33:34", "link": "http://arxiv.org/abs/2312.04736v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations", "abstract": "We introduce Llama Guard, an LLM-based input-output safeguard model geared\ntowards Human-AI conversation use cases. Our model incorporates a safety risk\ntaxonomy, a valuable tool for categorizing a specific set of safety risks found\nin LLM prompts (i.e., prompt classification). This taxonomy is also\ninstrumental in classifying the responses generated by LLMs to these prompts, a\nprocess we refer to as response classification. For the purpose of both prompt\nand response classification, we have meticulously gathered a dataset of high\nquality. Llama Guard, a Llama2-7b model that is instruction-tuned on our\ncollected dataset, albeit low in volume, demonstrates strong performance on\nexisting benchmarks such as the OpenAI Moderation Evaluation dataset and\nToxicChat, where its performance matches or exceeds that of currently available\ncontent moderation tools. Llama Guard functions as a language model, carrying\nout multi-class classification and generating binary decision scores.\nFurthermore, the instruction fine-tuning of Llama Guard allows for the\ncustomization of tasks and the adaptation of output formats. This feature\nenhances the model's capabilities, such as enabling the adjustment of taxonomy\ncategories to align with specific use cases, and facilitating zero-shot or\nfew-shot prompting with diverse taxonomies at the input. We are making Llama\nGuard model weights available and we encourage researchers to further develop\nand adapt them to meet the evolving needs of the community for AI safety.", "published": "2023-12-07 19:40:50", "link": "http://arxiv.org/abs/2312.06674v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Language Models for Intent-Driven Session Recommendations", "abstract": "Intent-aware session recommendation (ISR) is pivotal in discerning user\nintents within sessions for precise predictions. Traditional approaches,\nhowever, face limitations due to their presumption of a uniform number of\nintents across all sessions. This assumption overlooks the dynamic nature of\nuser sessions, where the number and type of intentions can significantly vary.\nIn addition, these methods typically operate in latent spaces, thus hinder the\nmodel's transparency.Addressing these challenges, we introduce a novel ISR\napproach, utilizing the advanced reasoning capabilities of large language\nmodels (LLMs). First, this approach begins by generating an initial prompt that\nguides LLMs to predict the next item in a session, based on the varied intents\nmanifested in user sessions. Then, to refine this process, we introduce an\ninnovative prompt optimization mechanism that iteratively self-reflects and\nadjusts prompts. Furthermore, our prompt selection module, built upon the LLMs'\nbroad adaptability, swiftly selects the most optimized prompts across diverse\ndomains. This new paradigm empowers LLMs to discern diverse user intents at a\nsemantic level, leading to more accurate and interpretable session\nrecommendations. Our extensive experiments on three real-world datasets\ndemonstrate the effectiveness of our method, marking a significant advancement\nin ISR systems.", "published": "2023-12-07 02:25:14", "link": "http://arxiv.org/abs/2312.07552v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Hijacking Context in Large Multi-modal Models", "abstract": "Recently, Large Multi-modal Models (LMMs) have demonstrated their ability to\nunderstand the visual contents of images given the instructions regarding the\nimages. Built upon the Large Language Models (LLMs), LMMs also inherit their\nabilities and characteristics such as in-context learning where a coherent\nsequence of images and texts are given as the input prompt. However, we\nidentify a new limitation of off-the-shelf LMMs where a small fraction of\nincoherent images or text descriptions mislead LMMs to only generate biased\noutput about the hijacked context, not the originally intended context. To\naddress this, we propose a pre-filtering method that removes irrelevant\ncontexts via GPT-4V, based on its robustness towards distribution shift within\nthe contexts. We further investigate whether replacing the hijacked visual and\ntextual contexts with the correlated ones via GPT-4V and text-to-image models\ncan help yield coherent responses.", "published": "2023-12-07 11:23:29", "link": "http://arxiv.org/abs/2312.07553v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "A Study on the Calibration of In-context Learning", "abstract": "Accurate uncertainty quantification is crucial for the safe deployment of\nmachine learning models, and prior research has demonstrated improvements in\nthe calibration of modern language models (LMs). We study in-context learning\n(ICL), a prevalent method for adapting static LMs through tailored prompts, and\nexamine the balance between performance and calibration across a broad spectrum\nof natural language understanding and reasoning tasks. Through comprehensive\nexperiments, we observe that, with an increasing number of ICL examples, models\ninitially exhibit increased miscalibration before achieving better calibration\nand miscalibration tends to arise in low-shot settings. Moreover, we find that\nmethods aimed at improving usability, such as fine-tuning and chain-of-thought\n(CoT) prompting, can lead to miscalibration and unreliable natural language\nexplanations. Furthermore, we explore recalibration techniques and find that a\nscaling-binning calibrator can reduce calibration errors consistently.", "published": "2023-12-07 03:37:39", "link": "http://arxiv.org/abs/2312.04021v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Making Translators Privacy-aware on the User's Side", "abstract": "We propose PRISM to enable users of machine translation systems to preserve\nthe privacy of data on their own initiative. There is a growing demand to apply\nmachine translation systems to data that require privacy protection. While\nseveral machine translation engines claim to prioritize privacy, the extent and\nspecifics of such protection are largely ambiguous. First, there is often a\nlack of clarity on how and to what degree the data is protected. Even if\nservice providers believe they have sufficient safeguards in place,\nsophisticated adversaries might still extract sensitive information. Second,\nvulnerabilities may exist outside of these protective measures, such as within\ncommunication channels, potentially leading to data leakage. As a result, users\nare hesitant to utilize machine translation engines for data demanding high\nlevels of privacy protection, thereby missing out on their benefits. PRISM\nresolves this problem. Instead of relying on the translation service to keep\ndata safe, PRISM provides the means to protect data on the user's side. This\napproach ensures that even machine translation engines with inadequate privacy\nmeasures can be used securely. For platforms already equipped with privacy\nsafeguards, PRISM acts as an additional protection layer, reinforcing their\nsecurity furthermore. PRISM adds these privacy features without significantly\ncompromising translation accuracy. Our experiments demonstrate the\neffectiveness of PRISM using real-world translators, T5 and ChatGPT\n(GPT-3.5-turbo), and the datasets with two languages. PRISM effectively\nbalances privacy protection with translation accuracy.", "published": "2023-12-07 06:23:17", "link": "http://arxiv.org/abs/2312.04068v1", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Enhancing the Rationale-Input Alignment for Self-explaining\n  Rationalization", "abstract": "Rationalization empowers deep learning models with self-explaining\ncapabilities through a cooperative game, where a generator selects a\nsemantically consistent subset of the input as a rationale, and a subsequent\npredictor makes predictions based on the selected rationale. In this paper, we\ndiscover that rationalization is prone to a problem named \\emph{rationale\nshift}, which arises from the algorithmic bias of the cooperative game.\nRationale shift refers to a situation where the semantics of the selected\nrationale may deviate from the original input, but the predictor still produces\naccurate predictions based on the deviation, resulting in a compromised\ngenerator with misleading feedback.\n  To address this issue, we first demonstrate the importance of the alignment\nbetween the rationale and the full input through both empirical observations\nand theoretical analysis. Subsequently, we introduce a novel approach called\nDAR (\\textbf{D}iscriminatively \\textbf{A}ligned \\textbf{R}ationalization),\nwhich utilizes an auxiliary module pretrained on the full input to\ndiscriminatively align the selected rationale and the original input. We\ntheoretically illustrate how DAR accomplishes the desired alignment, thereby\novercoming the rationale shift problem. The experiments on two widely used\nreal-world benchmarks show that the proposed method significantly improves the\nexplanation quality (measured by the overlap between the model-selected\nexplanation and the human-annotated rationale) as compared to state-of-the-art\ntechniques. Additionally, results on two synthetic settings further validate\nthe effectiveness of DAR in addressing the rationale shift problem.", "published": "2023-12-07 07:37:15", "link": "http://arxiv.org/abs/2312.04103v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Language Model Knowledge Distillation for Efficient Question Answering\n  in Spanish", "abstract": "Recent advances in the development of pre-trained Spanish language models has\nled to significant progress in many Natural Language Processing (NLP) tasks,\nsuch as question answering. However, the lack of efficient models imposes a\nbarrier for the adoption of such models in resource-constrained environments.\nTherefore, smaller distilled models for the Spanish language could be proven to\nbe highly scalable and facilitate their further adoption on a variety of tasks\nand scenarios. In this work, we take one step in this direction by developing\nSpanishTinyRoBERTa, a compressed language model based on RoBERTa for efficient\nquestion answering in Spanish. To achieve this, we employ knowledge\ndistillation from a large model onto a lighter model that allows for a wider\nimplementation, even in areas with limited computational resources, whilst\nattaining negligible performance sacrifice. Our experiments show that the dense\ndistilled model can still preserve the performance of its larger counterpart,\nwhile significantly increasing inference speedup. This work serves as a\nstarting point for further research and investigation of model compression\nefforts for Spanish language models across various NLP tasks.", "published": "2023-12-07 10:21:22", "link": "http://arxiv.org/abs/2312.04193v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Enhancing Medical Task Performance in GPT-4V: A Comprehensive Study on\n  Prompt Engineering Strategies", "abstract": "OpenAI's latest large vision-language model (LVLM), GPT-4V(ision), has piqued\nconsiderable interest for its potential in medical applications. Despite its\npromise, recent studies and internal reviews highlight its underperformance in\nspecialized medical tasks. This paper explores the boundary of GPT-4V's\ncapabilities in medicine, particularly in processing complex imaging data from\nendoscopies, CT scans, and MRIs etc. Leveraging open-source datasets, we\nassessed its foundational competencies, identifying substantial areas for\nenhancement. Our research emphasizes prompt engineering, an often-underutilized\nstrategy for improving AI responsiveness. Through iterative testing, we refined\nthe model's prompts, significantly improving its interpretative accuracy and\nrelevance in medical imaging. From our comprehensive evaluations, we distilled\n10 effective prompt engineering techniques, each fortifying GPT-4V's medical\nacumen. These methodical enhancements facilitate more reliable, precise, and\nclinically valuable insights from GPT-4V, advancing its operability in critical\nhealthcare environments. Our findings are pivotal for those employing AI in\nmedicine, providing clear, actionable guidance on harnessing GPT-4V's full\ndiagnostic potential.", "published": "2023-12-07 15:05:59", "link": "http://arxiv.org/abs/2312.04344v2", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "When Input Integers are Given in the Unary Numeral Representation", "abstract": "Many NP-complete problems take integers as part of their input instances.\nThese input integers are generally binarized, that is, provided in the form of\nthe \"binary\" numeral representation, and the lengths of such binary forms are\nused as a basis unit to measure the computational complexity of the problems.\nIn sharp contrast, the \"unarization\" (or the \"unary\" numeral representation) of\nnumbers has been known to bring a remarkably different effect onto the\ncomputational complexity of the problems. When no computational-complexity\ndifference is observed between binarization and unarization of instances, on\nthe contrary, the problems are said to be strong NP-complete. This work\nattempts to spotlight an issue of how the unarization of instances affects the\ncomputational complexity of various combinatorial problems. We present numerous\nNP-complete (or even NP-hard) problems, which turn out to be easily solvable\nwhen input integers are represented in unary. We then discuss the computational\ncomplexities of such problems when taking unary-form integer inputs. We hope\nthat a list of such problems signifies the structural differences between\nstrong NP-completeness and non-strong NP-completeness.", "published": "2023-12-07 15:09:24", "link": "http://arxiv.org/abs/2312.04348v1", "categories": ["cs.CC", "cs.CL", "cs.FL"], "primary_category": "cs.CC"}
{"title": "CLadder: Assessing Causal Reasoning in Language Models", "abstract": "The ability to perform causal reasoning is widely considered a core feature\nof intelligence. In this work, we investigate whether large language models\n(LLMs) can coherently reason about causality. Much of the existing work in\nnatural language processing (NLP) focuses on evaluating commonsense causal\nreasoning in LLMs, thus failing to assess whether a model can perform causal\ninference in accordance with a set of well-defined formal rules. To address\nthis, we propose a new NLP task, causal inference in natural language, inspired\nby the \"causal inference engine\" postulated by Judea Pearl et al. We compose a\nlarge dataset, CLadder, with 10K samples: based on a collection of causal\ngraphs and queries (associational, interventional, and counterfactual), we\nobtain symbolic questions and ground-truth answers, through an oracle causal\ninference engine. These are then translated into natural language. We evaluate\nmultiple LLMs on our dataset, and we introduce and evaluate a bespoke\nchain-of-thought prompting strategy, CausalCoT. We show that our task is highly\nchallenging for LLMs, and we conduct an in-depth analysis to gain deeper\ninsights into the causal reasoning abilities of LLMs. Our data is open-sourced\nat https://huggingface.co/datasets/causalNLP/cladder, and our code can be found\nat https://github.com/causalNLP/cladder.", "published": "2023-12-07 15:12:12", "link": "http://arxiv.org/abs/2312.04350v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Fortify the Shortest Stave in Attention: Enhancing Context Awareness of\n  Large Language Models for Effective Tool Use", "abstract": "In this paper, we demonstrate that an inherent waveform pattern in the\nattention allocation of large language models (LLMs) significantly affects\ntheir performance in tasks demanding a high degree of context awareness, such\nas utilizing LLMs for tool-use. Specifically, the crucial information in the\ncontext will be potentially overlooked by model when it is positioned in the\ntrough zone of the attention waveform, leading to decreased performance. To\naddress this issue, we propose a novel inference method named Attention\nBuckets. It allows LLMs to process their input through multiple parallel\nprocesses. Each process utilizes a distinct base angle for the rotary position\nembedding, thereby creating a unique attention waveform. By compensating an\nattention trough of a particular process with an attention peak of another\nprocess, our approach enhances LLM's awareness to various contextual positions,\nthus mitigating the risk of overlooking crucial information. In the largest\ntool-use benchmark, our method elevates a 7B model to achieve state-of-the-art\nperformance, comparable to that of GPT-4. On other benchmarks and some RAG\ntasks, which also demand a thorough understanding of contextual content,\nAttention Buckets also exhibited notable enhancements in performance.", "published": "2023-12-07 17:24:51", "link": "http://arxiv.org/abs/2312.04455v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On the Learnability of Watermarks for Language Models", "abstract": "Watermarking of language model outputs enables statistical detection of\nmodel-generated text, which can mitigate harms and misuses of language models.\nExisting watermarking strategies operate by altering the decoder of an existing\nlanguage model. In this paper, we ask whether language models can directly\nlearn to generate watermarked text, which would have significant implications\nfor the real-world deployment of watermarks. First, learned watermarks could be\nused to build open models that naturally generate watermarked text, enabling\nwatermarking for open models, where users can control the decoding procedure.\nSecond, if watermarking is used to determine the provenance of generated text,\nan adversary can hurt the reputation of a victim model by spoofing its\nwatermark and generating damaging watermarked text. To investigate the\nlearnability of watermarks, we propose watermark distillation, which trains a\nstudent model to behave like a teacher model that uses decoding-based\nwatermarking. We test our approach on three decoding-based watermarking\nstrategies and various hyperparameter settings, finding that models can learn\nto generate watermarked text with high detectability. We also find limitations\nto learnability, including the loss of watermarking capabilities under\nfine-tuning on normal text and high sample complexity when learning\nlow-distortion watermarks.", "published": "2023-12-07 17:41:44", "link": "http://arxiv.org/abs/2312.04469v3", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Chain of Code: Reasoning with a Language Model-Augmented Code Emulator", "abstract": "Code provides a general syntactic structure to build complex programs and\nperform precise computations when paired with a code interpreter - we\nhypothesize that language models (LMs) can leverage code-writing to improve\nChain of Thought reasoning not only for logic and arithmetic tasks, but also\nfor semantic ones (and in particular, those that are a mix of both). For\nexample, consider prompting an LM to write code that counts the number of times\nit detects sarcasm in an essay: the LM may struggle to write an implementation\nfor \"detect_sarcasm(string)\" that can be executed by the interpreter (handling\nthe edge cases would be insurmountable). However, LMs may still produce a valid\nsolution if they not only write code, but also selectively \"emulate\" the\ninterpreter by generating the expected output of \"detect_sarcasm(string)\". In\nthis work, we propose Chain of Code (CoC), a simple yet surprisingly effective\nextension that improves LM code-driven reasoning. The key idea is to encourage\nLMs to format semantic sub-tasks in a program as flexible pseudocode that the\ninterpreter can explicitly catch undefined behaviors and hand off to simulate\nwith an LM (as an \"LMulator\"). Experiments demonstrate that Chain of Code\noutperforms Chain of Thought and other baselines across a variety of\nbenchmarks; on BIG-Bench Hard, Chain of Code achieves 84%, a gain of 12% over\nChain of Thought. In a nutshell, CoC broadens the scope of reasoning questions\nthat LMs can answer by \"thinking in code\".", "published": "2023-12-07 17:51:43", "link": "http://arxiv.org/abs/2312.04474v4", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.RO"], "primary_category": "cs.CL"}
{"title": "Improved Visual Grounding through Self-Consistent Explanations", "abstract": "Vision-and-language models trained to match images with text can be combined\nwith visual explanation methods to point to the locations of specific objects\nin an image. Our work shows that the localization --\"grounding\"-- abilities of\nthese models can be further improved by finetuning for self-consistent visual\nexplanations. We propose a strategy for augmenting existing text-image datasets\nwith paraphrases using a large language model, and SelfEQ, a weakly-supervised\nstrategy on visual explanation maps for paraphrases that encourages\nself-consistency. Specifically, for an input textual phrase, we attempt to\ngenerate a paraphrase and finetune the model so that the phrase and paraphrase\nmap to the same region in the image. We posit that this both expands the\nvocabulary that the model is able to handle, and improves the quality of the\nobject locations highlighted by gradient-based visual explanation methods (e.g.\nGradCAM). We demonstrate that SelfEQ improves performance on Flickr30k,\nReferIt, and RefCOCO+ over a strong baseline method and several prior works.\nParticularly, comparing to other methods that do not use any type of box\nannotations, we obtain 84.07% on Flickr30k (an absolute improvement of 4.69%),\n67.40% on ReferIt (an absolute improvement of 7.68%), and 75.10%, 55.49% on\nRefCOCO+ test sets A and B respectively (an absolute improvement of 3.74% on\naverage).", "published": "2023-12-07 18:59:22", "link": "http://arxiv.org/abs/2312.04554v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Large Language Models for Mathematicians", "abstract": "Large language models (LLMs) such as ChatGPT have received immense interest\nfor their general-purpose language understanding and, in particular, their\nability to generate high-quality text or computer code. For many professions,\nLLMs represent an invaluable tool that can speed up and improve the quality of\nwork. In this note, we discuss to what extent they can aid professional\nmathematicians. We first provide a mathematical description of the transformer\nmodel used in all modern language models. Based on recent studies, we then\noutline best practices and potential issues and report on the mathematical\nabilities of language models. Finally, we shed light on the potential of LLMs\nto change how mathematicians work.", "published": "2023-12-07 18:59:29", "link": "http://arxiv.org/abs/2312.04556v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "math.HO"], "primary_category": "cs.CL"}
{"title": "TOD-Flow: Modeling the Structure of Task-Oriented Dialogues", "abstract": "Task-Oriented Dialogue (TOD) systems have become crucial components in\ninteractive artificial intelligence applications. While recent advances have\ncapitalized on pre-trained language models (PLMs), they exhibit limitations\nregarding transparency and controllability. To address these challenges, we\npropose a novel approach focusing on inferring the TOD-Flow graph from dialogue\ndata annotated with dialog acts, uncovering the underlying task structure in\nthe form of a graph. The inferred TOD-Flow graph can be easily integrated with\nany dialogue model to improve its prediction performance, transparency, and\ncontrollability. Our TOD-Flow graph learns what a model can, should, and should\nnot predict, effectively reducing the search space and providing a rationale\nfor the model's prediction. We show that the proposed TOD-Flow graph better\nresembles human-annotated graphs compared to prior approaches. Furthermore,\nwhen combined with several dialogue policies and end-to-end dialogue models, we\ndemonstrate that our approach significantly improves dialog act classification\nand end-to-end response generation performance in the MultiWOZ and SGD\nbenchmarks. Code available at: https://github.com/srsohn/TOD-Flow", "published": "2023-12-07 20:06:23", "link": "http://arxiv.org/abs/2312.04668v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Deep Emotions Across Languages: A Novel Approach for Sentiment\n  Propagation in Multilingual WordNets", "abstract": "Sentiment analysis involves using WordNets enriched with emotional metadata,\nwhich are valuable resources. However, manual annotation is time-consuming and\nexpensive, resulting in only a few WordNet Lexical Units being annotated. This\npaper introduces two new techniques for automatically propagating sentiment\nannotations from a partially annotated WordNet to its entirety and to a WordNet\nin a different language: Multilingual Structured Synset Embeddings (MSSE) and\nCross-Lingual Deep Neural Sentiment Propagation (CLDNS). We evaluated the\nproposed MSSE+CLDNS method extensively using Princeton WordNet and Polish\nWordNet, which have many inter-lingual relations. Our results show that the\nMSSE+CLDNS method outperforms existing propagation methods, indicating its\neffectiveness in enriching WordNets with emotional metadata across multiple\nlanguages. This work provides a solid foundation for large-scale, multilingual\nsentiment analysis and is valuable for academic research and practical\napplications.", "published": "2023-12-07 21:44:14", "link": "http://arxiv.org/abs/2312.04715v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "From Big to Small Without Losing It All: Text Augmentation with ChatGPT\n  for Efficient Sentiment Analysis", "abstract": "In the era of artificial intelligence, data is gold but costly to annotate.\nThe paper demonstrates a groundbreaking solution to this dilemma using ChatGPT\nfor text augmentation in sentiment analysis. We leverage ChatGPT's generative\ncapabilities to create synthetic training data that significantly improves the\nperformance of smaller models, making them competitive with, or even\noutperforming, their larger counterparts. This innovation enables models to be\nboth efficient and effective, thereby reducing computational cost, inference\ntime, and memory usage without compromising on quality. Our work marks a key\nadvancement in the cost-effective development and deployment of robust\nsentiment analysis models.", "published": "2023-12-07 21:58:37", "link": "http://arxiv.org/abs/2312.04720v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Efficient End-to-end Language Model Fine-tuning on Graphs", "abstract": "Learning from Text-Attributed Graphs (TAGs) has attracted significant\nattention due to its wide range of real-world applications. The rapid evolution\nof language models (LMs) has revolutionized the way we process textual data,\nwhich indicates a strong potential to replace shallow text embedding generally\nused in Graph Neural Networks (GNNs). However, we find that existing LM\napproaches that exploit text information in graphs suffer from inferior\ncomputation and data efficiency. In this study, we introduce LEADING, a novel\nand efficient approach for end-to-end fine-tuning of language models on TAGs.\nTo enhance data efficiency, LEADING efficiently transfers rich knowledge from\nLMs to downstream graph learning tasks with limited labeled data by employing\nend-to-end training of LMs and GNNs in a semi-supervised learning setting. To\naddress associated computation efficiency issues, it introduces two techniques:\nneighbor decoupling targeting LMs and implicit graph modeling targeting GNNs,\nrespectively. Our proposed approach demonstrates superior performance,\nachieving state-of-the-art (SOTA) results on the ogbn-arxiv leaderboard, while\nmaintaining computation cost and memory overhead comparable to graph-less\nfine-tuning of LMs. Through comprehensive experiments, we showcase its superior\ncomputation and data efficiency, presenting a promising solution for various\nLMs and graph learning tasks on TAGs.", "published": "2023-12-07 22:35:16", "link": "http://arxiv.org/abs/2312.04737v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Quilt-LLaVA: Visual Instruction Tuning by Extracting Localized\n  Narratives from Open-Source Histopathology Videos", "abstract": "Diagnosis in histopathology requires a global whole slide images (WSIs)\nanalysis, requiring pathologists to compound evidence from different WSI\npatches. The gigapixel scale of WSIs poses a challenge for histopathology\nmulti-modal models. Training multi-model models for histopathology requires\ninstruction tuning datasets, which currently contain information for individual\nimage patches, without a spatial grounding of the concepts within each patch\nand without a wider view of the WSI. Therefore, they lack sufficient diagnostic\ncapacity for histopathology. To bridge this gap, we introduce Quilt-Instruct, a\nlarge-scale dataset of 107,131 histopathology-specific instruction\nquestion/answer pairs, grounded within diagnostically relevant image patches\nthat make up the WSI. Our dataset is collected by leveraging educational\nhistopathology videos from YouTube, which provides spatial localization of\nnarrations by automatically extracting the narrators' cursor positions.\nQuilt-Instruct supports contextual reasoning by extracting diagnosis and\nsupporting facts from the entire WSI. Using Quilt-Instruct, we train\nQuilt-LLaVA, which can reason beyond the given single image patch, enabling\ndiagnostic reasoning across patches. To evaluate Quilt-LLaVA, we propose a\ncomprehensive evaluation dataset created from 985 images and 1283\nhuman-generated question-answers. We also thoroughly evaluate Quilt-LLaVA using\npublic histopathology datasets, where Quilt-LLaVA significantly outperforms\nSOTA by over 10% on relative GPT-4 score and 4% and 9% on open and closed set\nVQA. Our code, data, and model are publicly accessible at\nquilt-llava.github.io.", "published": "2023-12-07 23:16:37", "link": "http://arxiv.org/abs/2312.04746v3", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Forcing Generative Models to Degenerate Ones: The Power of Data\n  Poisoning Attacks", "abstract": "Growing applications of large language models (LLMs) trained by a third party\nraise serious concerns on the security vulnerability of LLMs.It has been\ndemonstrated that malicious actors can covertly exploit these vulnerabilities\nin LLMs through poisoning attacks aimed at generating undesirable outputs.\nWhile poisoning attacks have received significant attention in the image domain\n(e.g., object detection), and classification tasks, their implications for\ngenerative models, particularly in the realm of natural language generation\n(NLG) tasks, remain poorly understood. To bridge this gap, we perform a\ncomprehensive exploration of various poisoning techniques to assess their\neffectiveness across a range of generative tasks. Furthermore, we introduce a\nrange of metrics designed to quantify the success and stealthiness of poisoning\nattacks specifically tailored to NLG tasks. Through extensive experiments on\nmultiple NLG tasks, LLMs and datasets, we show that it is possible to\nsuccessfully poison an LLM during the fine-tuning stage using as little as 1\\%\nof the total tuning data samples. Our paper presents the first systematic\napproach to comprehend poisoning attacks targeting NLG tasks considering a wide\nrange of triggers and attack settings. We hope our findings will assist the AI\nsecurity community in devising appropriate defenses against such threats.", "published": "2023-12-07 23:26:06", "link": "http://arxiv.org/abs/2312.04748v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Inversion-Free Image Editing with Natural Language", "abstract": "Despite recent advances in inversion-based editing, text-guided image\nmanipulation remains challenging for diffusion models. The primary bottlenecks\ninclude 1) the time-consuming nature of the inversion process; 2) the struggle\nto balance consistency with accuracy; 3) the lack of compatibility with\nefficient consistency sampling methods used in consistency models. To address\nthe above issues, we start by asking ourselves if the inversion process can be\neliminated for editing. We show that when the initial sample is known, a\nspecial variance schedule reduces the denoising step to the same form as the\nmulti-step consistency sampling. We name this Denoising Diffusion Consistent\nModel (DDCM), and note that it implies a virtual inversion strategy without\nexplicit inversion in sampling. We further unify the attention control\nmechanisms in a tuning-free framework for text-guided editing. Combining them,\nwe present inversion-free editing (InfEdit), which allows for consistent and\nfaithful editing for both rigid and non-rigid semantic changes, catering to\nintricate modifications without compromising on the image's integrity and\nexplicit inversion. Through extensive experiments, InfEdit shows strong\nperformance in various editing tasks and also maintains a seamless workflow\n(less than 3 seconds on one single A40), demonstrating the potential for\nreal-time applications. Project Page: https://sled-group.github.io/InfEdit/", "published": "2023-12-07 18:58:27", "link": "http://arxiv.org/abs/2312.04965v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Joint Training or Not: An Exploration of Pre-trained Speech Models in\n  Audio-Visual Speaker Diarization", "abstract": "The scarcity of labeled audio-visual datasets is a constraint for training\nsuperior audio-visual speaker diarization systems. To improve the performance\nof audio-visual speaker diarization, we leverage pre-trained supervised and\nself-supervised speech models for audio-visual speaker diarization.\nSpecifically, we adopt supervised~(ResNet and ECAPA-TDNN) and self-supervised\npre-trained models~(WavLM and HuBERT) as the speaker and audio embedding\nextractors in an end-to-end audio-visual speaker diarization~(AVSD) system.\nThen we explore the effectiveness of different frameworks, including\nTransformer, Conformer, and cross-attention mechanism, in the audio-visual\ndecoder. To mitigate the degradation of performance caused by separate\ntraining, we jointly train the audio encoder, speaker encoder, and audio-visual\ndecoder in the AVSD system. Experiments on the MISP dataset demonstrate that\nthe proposed method achieves superior performance and obtained third place in\nMISP Challenge 2022.", "published": "2023-12-07 08:40:37", "link": "http://arxiv.org/abs/2312.04131v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DiaPer: End-to-End Neural Diarization with Perceiver-Based Attractors", "abstract": "Until recently, the field of speaker diarization was dominated by cascaded\nsystems. Due to their limitations, mainly regarding overlapped speech and\ncumbersome pipelines, end-to-end models have gained great popularity lately.\nOne of the most successful models is end-to-end neural diarization with\nencoder-decoder based attractors (EEND-EDA). In this work, we replace the EDA\nmodule with a Perceiver-based one and show its advantages over EEND-EDA; namely\nobtaining better performance on the largely studied Callhome dataset, finding\nthe quantity of speakers in a conversation more accurately, and faster\ninference time. Furthermore, when exhaustively compared with other methods, our\nmodel, DiaPer, reaches remarkable performance with a very lightweight design.\nBesides, we perform comparisons with other works and a cascaded baseline across\nmore than ten public wide-band datasets. Together with this publication, we\nrelease the code of DiaPer as well as models trained on public and free data.", "published": "2023-12-07 14:33:27", "link": "http://arxiv.org/abs/2312.04324v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Investigating the Design Space of Diffusion Models for Speech\n  Enhancement", "abstract": "Diffusion models are a new class of generative models that have shown\noutstanding performance in image generation literature. As a consequence,\nstudies have attempted to apply diffusion models to other tasks, such as speech\nenhancement. A popular approach in adapting diffusion models to speech\nenhancement consists in modelling a progressive transformation between the\nclean and noisy speech signals. However, one popular diffusion model framework\npreviously laid in image generation literature did not account for such a\ntransformation towards the system input, which prevents from relating the\nexisting diffusion-based speech enhancement systems with the aforementioned\ndiffusion model framework. To address this, we extend this framework to account\nfor the progressive transformation between the clean and noisy speech signals.\nThis allows us to apply recent developments from image generation literature,\nand to systematically investigate design aspects of diffusion models that\nremain largely unexplored for speech enhancement, such as the neural network\npreconditioning, the training loss weighting, the stochastic differential\nequation (SDE), or the amount of stochasticity injected in the reverse process.\nWe show that the performance of previous diffusion-based speech enhancement\nsystems cannot be attributed to the progressive transformation between the\nclean and noisy speech signals. Moreover, we show that a proper choice of\npreconditioning, training loss weighting, SDE and sampler allows to outperform\na popular diffusion-based speech enhancement system while using fewer sampling\nsteps, thus reducing the computational cost by a factor of four.", "published": "2023-12-07 15:40:55", "link": "http://arxiv.org/abs/2312.04370v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SynthScribe: Deep Multimodal Tools for Synthesizer Sound Retrieval and\n  Exploration", "abstract": "Synthesizers are powerful tools that allow musicians to create dynamic and\noriginal sounds. Existing commercial interfaces for synthesizers typically\nrequire musicians to interact with complex low-level parameters or to manage\nlarge libraries of premade sounds. To address these challenges, we implement\nSynthScribe -- a fullstack system that uses multimodal deep learning to let\nusers express their intentions at a much higher level. We implement features\nwhich address a number of difficulties, namely 1) searching through existing\nsounds, 2) creating completely new sounds, 3) making meaningful modifications\nto a given sound. This is achieved with three main features: a multimodal\nsearch engine for a large library of synthesizer sounds; a user centered\ngenetic algorithm by which completely new sounds can be created and selected\ngiven the users preferences; a sound editing support feature which highlights\nand gives examples for key control parameters with respect to a text or audio\nbased query. The results of our user studies show SynthScribe is capable of\nreliably retrieving and modifying sounds while also affording the ability to\ncreate completely new sounds that expand a musicians creative horizon.", "published": "2023-12-07 20:40:36", "link": "http://arxiv.org/abs/2312.04690v2", "categories": ["cs.HC", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
