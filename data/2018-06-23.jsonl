{"title": "Emotion Representation Mapping for Automatic Lexicon Construction\n  (Mostly) Performs on Human Level", "abstract": "Emotion Representation Mapping (ERM) has the goal to convert existing emotion\nratings from one representation format into another one, e.g., mapping\nValence-Arousal-Dominance annotations for words or sentences into Ekman's Basic\nEmotions and vice versa. ERM can thus not only be considered as an alternative\nto Word Emotion Induction (WEI) techniques for automatic emotion lexicon\nconstruction but may also help mitigate problems that come from the\nproliferation of emotion representation formats in recent years. We propose a\nnew neural network approach to ERM that not only outperforms the previous\nstate-of-the-art. Equally important, we present a refined evaluation\nmethodology and gather strong evidence that our model yields results which are\n(almost) as reliable as human annotations, even in cross-lingual settings.\nBased on these results we generate new emotion ratings for 13 typologically\ndiverse languages and claim that they have near-gold quality, at least.", "published": "2018-06-23 02:00:13", "link": "http://arxiv.org/abs/1806.08890v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On Adversarial Examples for Character-Level Neural Machine Translation", "abstract": "Evaluating on adversarial examples has become a standard procedure to measure\nrobustness of deep learning models. Due to the difficulty of creating white-box\nadversarial examples for discrete text input, most analyses of the robustness\nof NLP models have been done through black-box adversarial examples. We\ninvestigate adversarial examples for character-level neural machine translation\n(NMT), and contrast black-box adversaries with a novel white-box adversary,\nwhich employs differentiable string-edit operations to rank adversarial\nchanges. We propose two novel types of attacks which aim to remove or change a\nword in a translation, rather than simply break the NMT. We demonstrate that\nwhite-box adversarial examples are significantly stronger than their black-box\ncounterparts in different attack scenarios, which show more serious\nvulnerabilities than previously known. In addition, after performing\nadversarial training, which takes only 3 times longer than regular training, we\ncan improve the model's robustness significantly.", "published": "2018-06-23 20:08:56", "link": "http://arxiv.org/abs/1806.09030v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evaluating Gammatone Frequency Cepstral Coefficients with Neural\n  Networks for Emotion Recognition from Speech", "abstract": "Current approaches to speech emotion recognition focus on speech features\nthat can capture the emotional content of a speech signal. Mel Frequency\nCepstral Coefficients (MFCCs) are one of the most commonly used representations\nfor audio speech recognition and classification. This paper proposes Gammatone\nFrequency Cepstral Coefficients (GFCCs) as a potentially better representation\nof speech signals for emotion recognition. The effectiveness of MFCC and GFCC\nrepresentations are compared and evaluated over emotion and intensity\nclassification tasks with fully connected and recurrent neural network\narchitectures. The results provide evidence that GFCCs outperform MFCCs in\nspeech emotion recognition.", "published": "2018-06-23 17:42:32", "link": "http://arxiv.org/abs/1806.09010v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improving Text-to-SQL Evaluation Methodology", "abstract": "To be informative, an evaluation must measure how well systems generalize to\nrealistic unseen data. We identify limitations of and propose improvements to\ncurrent evaluations of text-to-SQL systems. First, we compare human-generated\nand automatically generated questions, characterizing properties of queries\nnecessary for real-world applications. To facilitate evaluation on multiple\ndatasets, we release standardized and improved versions of seven existing\ndatasets and one new text-to-SQL dataset. Second, we show that the current\ndivision of data into training and test sets measures robustness to variations\nin the way questions are asked, but only partially tests how well systems\ngeneralize to new queries; therefore, we propose a complementary dataset split\nfor evaluation of future work. Finally, we demonstrate how the common practice\nof anonymizing variables during evaluation removes an important challenge of\nthe task. Our observations highlight key difficulties, and our methodology\nenables effective measurement of future development.", "published": "2018-06-23 20:02:55", "link": "http://arxiv.org/abs/1806.09029v1", "categories": ["cs.CL", "cs.AI", "cs.DB", "I.2.7; I.2.1; H.2.3; H.3.4"], "primary_category": "cs.CL"}
