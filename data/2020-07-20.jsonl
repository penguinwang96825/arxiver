{"title": "Multimodal Dialogue State Tracking By QA Approach with Data Augmentation", "abstract": "Recently, a more challenging state tracking task, Audio-Video Scene-Aware\nDialogue (AVSD), is catching an increasing amount of attention among\nresearchers. Different from purely text-based dialogue state tracking, the\ndialogue in AVSD contains a sequence of question-answer pairs about a video and\nthe final answer to the given question requires additional understanding of the\nvideo. This paper interprets the AVSD task from an open-domain Question\nAnswering (QA) point of view and proposes a multimodal open-domain QA system to\ndeal with the problem. The proposed QA system uses common encoder-decoder\nframework with multimodal fusion and attention. Teacher forcing is applied to\ntrain a natural language generator. We also propose a new data augmentation\napproach specifically under QA assumption. Our experiments show that our model\nand techniques bring significant improvements over the baseline model on the\nDSTC7-AVSD dataset and demonstrate the potentials of our data augmentation\ntechniques.", "published": "2020-07-20 06:23:18", "link": "http://arxiv.org/abs/2007.09903v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Frustratingly Hard Evidence Retrieval for QA Over Books", "abstract": "A lot of progress has been made to improve question answering (QA) in recent\nyears, but the special problem of QA over narrative book stories has not been\nexplored in-depth. We formulate BookQA as an open-domain QA task given its\nsimilar dependency on evidence retrieval. We further investigate how\nstate-of-the-art open-domain QA approaches can help BookQA. Besides achieving\nstate-of-the-art on the NarrativeQA benchmark, our study also reveals the\ndifficulty of evidence retrieval in books with a wealth of experiments and\nanalysis - which necessitates future effort on novel solutions for evidence\nretrieval in BookQA.", "published": "2020-07-20 04:10:08", "link": "http://arxiv.org/abs/2007.09878v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "How are you? Introducing stress-based text tailoring", "abstract": "Can stress affect not only your life but also how you read and interpret a\ntext? Healthcare has shown evidence of such dynamics and in this short paper we\ndiscuss customising texts based on user stress level, as it could represent a\ncritical factor when it comes to user engagement and behavioural change. We\nfirst show a real-world example in which user behaviour is influenced by\nstress, then, after discussing which tools can be employed to assess and\nmeasure it, we propose an initial method for tailoring the document by\nexploiting complexity reduction and affect enforcement. The result is a short\nand encouraging text which requires less commitment to be read and understood.\nWe believe this work in progress can raise some interesting questions on a\ntopic that is often overlooked in NLG.", "published": "2020-07-20 09:43:11", "link": "http://arxiv.org/abs/2007.09970v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Voice@SRIB at SemEval-2020 Task 9 and 12: Stacked Ensembling method for\n  Sentiment and Offensiveness detection in Social Media", "abstract": "In social-media platforms such as Twitter, Facebook, and Reddit, people\nprefer to use code-mixed language such as Spanish-English, Hindi-English to\nexpress their opinions. In this paper, we describe different models we used,\nusing the external dataset to train embeddings, ensembling methods for\nSentimix, and OffensEval tasks. The use of pre-trained embeddings usually helps\nin multiple tasks such as sentence classification, and machine translation. In\nthis experiment, we haveused our trained code-mixed embeddings and twitter\npre-trained embeddings to SemEval tasks. We evaluate our models on macro\nF1-score, precision, accuracy, and recall on the datasets. We intend to show\nthat hyper-parameter tuning and data pre-processing steps help a lot in\nimproving the scores. In our experiments, we are able to achieve 0.886 F1-Macro\non OffenEval Greek language subtask post-evaluation, whereas the highest is\n0.852 during the Evaluation Period. We stood third in Spanglish competition\nwith our best F1-score of 0.756. Codalab username is asking28.", "published": "2020-07-20 11:54:43", "link": "http://arxiv.org/abs/2007.10021v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Knowledge Graph Extraction from Videos", "abstract": "Nearly all existing techniques for automated video annotation (or captioning)\ndescribe videos using natural language sentences. However, this has several\nshortcomings: (i) it is very hard to then further use the generated natural\nlanguage annotations in automated data processing, (ii) generating natural\nlanguage annotations requires to solve the hard subtask of generating\nsemantically precise and syntactically correct natural language sentences,\nwhich is actually unrelated to the task of video annotation, (iii) it is\ndifficult to quantitatively measure performance, as standard metrics (e.g.,\naccuracy and F1-score) are inapplicable, and (iv) annotations are\nlanguage-specific. In this paper, we propose the new task of knowledge graph\nextraction from videos, i.e., producing a description in the form of a\nknowledge graph of the contents of a given video. Since no datasets exist for\nthis task, we also include a method to automatically generate them, starting\nfrom datasets where videos are annotated with natural language. We then\ndescribe an initial deep-learning model for knowledge graph extraction from\nvideos, and report results on MSVD* and MSR-VTT*, two datasets obtained from\nMSVD and MSR-VTT using our method.", "published": "2020-07-20 12:23:39", "link": "http://arxiv.org/abs/2007.10040v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Morphological Skip-Gram: Using morphological knowledge to improve word\n  representation", "abstract": "Natural language processing models have attracted much interest in the deep\nlearning community. This branch of study is composed of some applications such\nas machine translation, sentiment analysis, named entity recognition, question\nand answer, and others. Word embeddings are continuous word representations,\nthey are an essential module for those applications and are generally used as\ninput word representation to the deep learning models. Word2Vec and GloVe are\ntwo popular methods to learn word embeddings. They achieve good word\nrepresentations, however, they learn representations with limited information\nbecause they ignore the morphological information of the words and consider\nonly one representation vector for each word. This approach implies that\nWord2Vec and GloVe are unaware of the word inner structure. To mitigate this\nproblem, the FastText model represents each word as a bag of characters\nn-grams. Hence, each n-gram has a continuous vector representation, and the\nfinal word representation is the sum of its characters n-grams vectors.\nNevertheless, the use of all n-grams character of a word is a poor approach\nsince some n-grams have no semantic relation with their words and increase the\namount of potentially useless information. This approach also increases the\ntraining phase time. In this work, we propose a new method for training word\nembeddings, and its goal is to replace the FastText bag of character n-grams\nfor a bag of word morphemes through the morphological analysis of the word.\nThus, words with similar context and morphemes are represented by vectors close\nto each other. To evaluate our new approach, we performed intrinsic evaluations\nconsidering 15 different tasks, and the results show a competitive performance\ncompared to FastText.", "published": "2020-07-20 12:47:36", "link": "http://arxiv.org/abs/2007.10055v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Characterizing drug mentions in COVID-19 Twitter Chatter", "abstract": "Since the classification of COVID-19 as a global pandemic, there have been\nmany attempts to treat and contain the virus. Although there is no specific\nantiviral treatment recommended for COVID-19, there are several drugs that can\npotentially help with symptoms. In this work, we mined a large twitter dataset\nof 424 million tweets of COVID-19 chatter to identify discourse around drug\nmentions. While seemingly a straightforward task, due to the informal nature of\nlanguage use in Twitter, we demonstrate the need of machine learning alongside\ntraditional automated methods to aid in this task. By applying these\ncomplementary methods, we are able to recover almost 15% additional data,\nmaking misspelling handling a needed task as a pre-processing step when dealing\nwith social media data.", "published": "2020-07-20 16:56:46", "link": "http://arxiv.org/abs/2007.10276v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Coinduction Plain and Simple", "abstract": "Coinduction refers to both a technique for the definition of infinite\nstreams, so-called codata, and a technique for proving the equality of\ncoinductively specified codata. This article first reviews coinduction in\ndeclarative programming. Second, it reviews and slightly extends the formalism\ncommonly used for specifying codata. Third, it generalizes the coinduction\nproof principle, which has been originally specified for the equality predicate\nonly, to other predicates. This generalization makes the coinduction proof\nprinciple more intuitive and stresses its closeness with structural induction.\nThe article finally suggests in its conclusion extensions of functional and\nlogic programming with limited and decidable forms of the generalized\ncoinduction proof principle.", "published": "2020-07-20 06:52:54", "link": "http://arxiv.org/abs/2007.09909v2", "categories": ["cs.PL", "cs.CL", "cs.LO", "F.3.2"], "primary_category": "cs.PL"}
{"title": "CoVoST 2 and Massively Multilingual Speech-to-Text Translation", "abstract": "Speech translation has recently become an increasingly popular topic of\nresearch, partly due to the development of benchmark datasets. Nevertheless,\ncurrent datasets cover a limited number of languages. With the aim to foster\nresearch in massive multilingual speech translation and speech translation for\nlow resource language pairs, we release CoVoST 2, a large-scale multilingual\nspeech translation corpus covering translations from 21 languages into English\nand from English into 15 languages. This represents the largest open dataset\navailable to date from total volume and language coverage perspective. Data\nsanity checks provide evidence about the quality of the data, which is released\nunder CC0 license. We also provide extensive speech recognition, bilingual and\nmultilingual machine translation and speech translation baselines with\nopen-source implementation.", "published": "2020-07-20 17:53:35", "link": "http://arxiv.org/abs/2007.10310v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Conformer-Kernel with Query Term Independence for Document Retrieval", "abstract": "The Transformer-Kernel (TK) model has demonstrated strong reranking\nperformance on the TREC Deep Learning benchmark---and can be considered to be\nan efficient (but slightly less effective) alternative to BERT-based ranking\nmodels. In this work, we extend the TK architecture to the full retrieval\nsetting by incorporating the query term independence assumption. Furthermore,\nto reduce the memory complexity of the Transformer layers with respect to the\ninput sequence length, we propose a new Conformer layer. We show that the\nConformer's GPU memory requirement scales linearly with input sequence length,\nmaking it a more viable option when ranking long documents. Finally, we\ndemonstrate that incorporating explicit term matching signal into the model can\nbe particularly useful in the full retrieval setting. We present preliminary\nresults from our work in this paper.", "published": "2020-07-20 19:47:28", "link": "http://arxiv.org/abs/2007.10434v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Neural Machine Translation model for University Email Application", "abstract": "Machine translation has many applications such as news translation, email\ntranslation, official letter translation etc. Commercial translators, e.g.\nGoogle Translation lags in regional vocabulary and are unable to learn the\nbilingual text in the source and target languages within the input. In this\npaper, a regional vocabulary-based application-oriented Neural Machine\nTranslation (NMT) model is proposed over the data set of emails used at the\nUniversity for communication over a period of three years. A state-of-the-art\nSequence-to-Sequence Neural Network for ML -> EN and EN -> ML translations is\ncompared with Google Translate using Gated Recurrent Unit Recurrent Neural\nNetwork machine translation model with attention decoder. The low BLEU score of\nGoogle Translation in comparison to our model indicates that the application\nbased regional models are better. The low BLEU score of EN -> ML of our model\nand Google Translation indicates that the Malay Language has complex language\nfeatures corresponding to English.", "published": "2020-07-20 15:05:16", "link": "http://arxiv.org/abs/2007.16011v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Identification, Tracking and Impact: Understanding the trade secret of\n  catchphrases", "abstract": "Understanding the topical evolution in industrial innovation is a challenging\nproblem. With the advancement in the digital repositories in the form of patent\ndocuments, it is becoming increasingly more feasible to understand the\ninnovation secrets -- \"catchphrases\" of organizations. However, searching and\nunderstanding this enormous textual information is a natural bottleneck. In\nthis paper, we propose an unsupervised method for the extraction of\ncatchphrases from the abstracts of patents granted by the U.S. Patent and\nTrademark Office over the years. Our proposed system achieves substantial\nimprovement, both in terms of precision and recall, against state-of-the-art\ntechniques. As a second objective, we conduct an extensive empirical study to\nunderstand the temporal evolution of the catchphrases across various\norganizations. We also show how the overall innovation evolution in the form of\nintroduction of newer catchphrases in an organization's patents correlates with\nthe future citations received by the patents filed by that organization. Our\ncode and data sets will be placed in the public domain soon.", "published": "2020-07-20 06:11:25", "link": "http://arxiv.org/abs/2007.13520v1", "categories": ["cs.DL", "cs.CL", "cs.IR", "cs.LG", "cs.SI"], "primary_category": "cs.DL"}
{"title": "Sep-Stereo: Visually Guided Stereophonic Audio Generation by Associating\n  Source Separation", "abstract": "Stereophonic audio is an indispensable ingredient to enhance human auditory\nexperience. Recent research has explored the usage of visual information as\nguidance to generate binaural or ambisonic audio from mono ones with stereo\nsupervision. However, this fully supervised paradigm suffers from an inherent\ndrawback: the recording of stereophonic audio usually requires delicate devices\nthat are expensive for wide accessibility. To overcome this challenge, we\npropose to leverage the vastly available mono data to facilitate the generation\nof stereophonic audio. Our key observation is that the task of visually\nindicated audio separation also maps independent audios to their corresponding\nvisual positions, which shares a similar objective with stereophonic audio\ngeneration. We integrate both stereo generation and source separation into a\nunified framework, Sep-Stereo, by considering source separation as a particular\ntype of audio spatialization. Specifically, a novel associative pyramid network\narchitecture is carefully designed for audio-visual feature fusion. Extensive\nexperiments demonstrate that our framework can improve the stereophonic audio\ngeneration results while performing accurate sound separation with a shared\nbackbone.", "published": "2020-07-20 06:20:26", "link": "http://arxiv.org/abs/2007.09902v1", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "wav2shape: Hearing the Shape of a Drum Machine", "abstract": "Disentangling and recovering physical attributes, such as shape and material,\nfrom a few waveform examples is a challenging inverse problem in audio signal\nprocessing, with numerous applications in musical acoustics as well as\nstructural engineering. We propose to address this problem via a combination of\ntime--frequency analysis and supervised machine learning. We start by\nsynthesizing a dataset of sounds using the functional transformation method.\nThen, we represent each percussive sound in terms of its time-invariant\nscattering transform coefficients and formulate the parametric estimation of\nthe resonator as multidimensional regression with a deep convolutional neural\nnetwork. We interpolate scattering coefficients over the surface of the drum as\na surrogate for potentially missing data, and study the response of the neural\nnetwork to interpolated samples. Lastly, we resynthesize drum sounds from\nscattering coefficients, therefore paving the way towards a deep generative\nmodel of drum sounds whose latent variables are physically interpretable.", "published": "2020-07-20 17:35:24", "link": "http://arxiv.org/abs/2007.10299v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Acoustic Neighbor Embeddings", "abstract": "This paper proposes a novel acoustic word embedding called Acoustic Neighbor\nEmbeddings where speech or text of arbitrary length are mapped to a vector\nspace of fixed, reduced dimensions by adapting stochastic neighbor embedding\n(SNE) to sequential inputs. The Euclidean distance between coordinates in the\nembedding space reflects the phonetic confusability between their corresponding\nsequences. Two encoder neural networks are trained: an acoustic encoder that\naccepts speech signals in the form of frame-wise subword posterior\nprobabilities obtained from an acoustic model and a text encoder that accepts\ntext in the form of subword transcriptions. Compared to a triplet loss\ncriterion, the proposed method is shown to have more effective gradients for\nneural network training. Experimentally, it also gives more accurate results\nwith low-dimensional embeddings when the two encoder networks are used in\ntandem in a word (name) recognition task, and when the text encoder network is\nused standalone in an approximate phonetic matching task. In particular, in an\nisolated name recognition task depending solely on Euclidean nearest-neighbor\nsearch between the proposed embedding vectors, the recognition accuracy is\nidentical to that of conventional finite state transducer(FST)-based decoding\nusing test data with up to 1 million names in the vocabulary and 40 dimensions\nin the embeddings.", "published": "2020-07-20 05:33:07", "link": "http://arxiv.org/abs/2007.10329v5", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
