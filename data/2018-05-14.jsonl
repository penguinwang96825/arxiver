{"title": "RETURNN as a Generic Flexible Neural Toolkit with Application to Translation and Speech Recognition", "abstract": "We compare the fast training and decoding speed of RETURNN of attention models for translation, due to fast CUDA LSTM kernels, and a fast pure TensorFlow beam search decoder. We show that a layer-wise pretraining scheme for recurrent attention models gives over 1% BLEU improvement absolute and it allows to train deeper recurrent encoder networks. Promising preliminary results on max. expected BLEU training are presented. We are able to train state-of-the-art models for translation and end-to-end models for speech recognition and show results on WMT 2017 and Switchboard. The flexibility of RETURNN allows a fast research feedback loop to experiment with alternative architectures, and its generality allows to use it on a wide range of applications.", "published": "2018-05-14 15:23:40", "link": "http://arxiv.org/abs/1805.05225v2", "categories": ["cs.NE", "cs.AI", "cs.CL"], "primary_category": "cs.NE"}
{"title": "Faithfully Explaining Rankings in a News Recommender System", "abstract": "There is an increasing demand for algorithms to explain their outcomes. So far, there is no method that explains the rankings produced by a ranking algorithm. To address this gap we propose LISTEN, a LISTwise ExplaiNer, to explain rankings produced by a ranking algorithm. To efficiently use LISTEN in production, we train a neural network to learn the underlying explanation space created by LISTEN; we call this model Q-LISTEN. We show that LISTEN produces faithful explanations and that Q-LISTEN is able to learn these explanations. Moreover, we show that LISTEN is safe to use in a real world environment: users of a news recommendation system do not behave significantly differently when they are exposed to explanations generated by LISTEN instead of manually generated explanations.", "published": "2018-05-14 21:13:04", "link": "http://arxiv.org/abs/1805.05447v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Hu-Fu: Hardware and Software Collaborative Attack Framework against Neural Networks", "abstract": "Recently, Deep Learning (DL), especially Convolutional Neural Network (CNN), develops rapidly and is applied to many tasks, such as image classification, face recognition, image segmentation, and human detection. Due to its superior performance, DL-based models have a wide range of application in many areas, some of which are extremely safety-critical, e.g. intelligent surveillance and autonomous driving. Due to the latency and privacy problem of cloud computing, embedded accelerators are popular in these safety-critical areas. However, the robustness of the embedded DL system might be harmed by inserting hardware/software Trojans into the accelerator and the neural network model, since the accelerator and deploy tool (or neural network model) are usually provided by third-party companies. Fortunately, inserting hardware Trojans can only achieve inflexible attack, which means that hardware Trojans can easily break down the whole system or exchange two outputs, but can't make CNN recognize unknown pictures as targets. Though inserting software Trojans has more freedom of attack, it often requires tampering input images, which is not easy for attackers. So, in this paper, we propose a hardware-software collaborative attack framework to inject hidden neural network Trojans, which works as a back-door without requiring manipulating input images and is flexible for different scenarios. We test our attack framework for image classification and face recognition tasks, and get attack success rate of 92.6% and 100% on CIFAR10 and YouTube Faces, respectively, while keeping almost the same accuracy as the unattacked model in the normal mode. In addition, we show a specific attack scenario in which a face recognition system is attacked and gives a specific wrong answer.", "published": "2018-05-14 10:15:29", "link": "http://arxiv.org/abs/1805.05098v2", "categories": ["cs.CR", "cs.CV"], "primary_category": "cs.CR"}
{"title": "Square-free graphs with no six-vertex induced path", "abstract": "We elucidate the structure of $(P_6,C_4)$-free graphs by showing that every such graph either has a clique cutset, or a universal vertex, or belongs to several special classes of graphs. Using this result, we show that for any $(P_6,C_4)$-free graph $G$, $\\lceil\\frac{5\u03c9(G)}{4}\\rceil$ and $\\lceil\\frac{\u0394(G) + \u03c9(G) +1}{2}\\rceil$ are tight upper bounds for the chromatic number of $G$. Moreover, our structural results imply that every ($P_6$,$C_4$)-free graph with no clique cutset has bounded clique-width, and thus the existence of a polynomial-time algorithm that computes the chromatic number (or stability number) of any $(P_6,C_4)$-free graph.", "published": "2018-05-14 04:14:49", "link": "http://arxiv.org/abs/1805.05007v2", "categories": ["cs.DM", "math.CO"], "primary_category": "cs.DM"}
{"title": "A Deep Learning Approach with an Attention Mechanism for Automatic Sleep Stage Classification", "abstract": "Automatic sleep staging is a challenging problem and state-of-the-art algorithms have not yet reached satisfactory performance to be used instead of manual scoring by a sleep technician. Much research has been done to find good feature representations that extract the useful information to correctly classify each epoch into the correct sleep stage. While many useful features have been discovered, the amount of features have grown to an extent that a feature reduction step is necessary in order to avoid the curse of dimensionality. One reason for the need of such a large feature set is that many features are good for discriminating only one of the sleep stages and are less informative during other stages. This paper explores how a second feature representation over a large set of pre-defined features can be learned using an auto-encoder with a selective attention for the current sleep stage in the training batch. This selective attention allows the model to learn feature representations that focuses on the more relevant inputs without having to perform any dimensionality reduction of the input data. The performance of the proposed algorithm is evaluated on a large data set of polysomnography (PSG) night recordings of patients with sleep-disordered breathing. The performance of the auto-encoder with selective attention is compared with a regular auto-encoder and previous works using a deep belief network (DBN).", "published": "2018-05-14 07:36:26", "link": "http://arxiv.org/abs/1805.05036v1", "categories": ["cs.LG", "q-bio.NC", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Index Set Fourier Series Features for Approximating Multi-dimensional Periodic Kernels", "abstract": "Periodicity is often studied in timeseries modelling with autoregressive methods but is less popular in the kernel literature, particularly for higher dimensional problems such as in textures, crystallography, and quantum mechanics. Large datasets often make modelling periodicity untenable for otherwise powerful non-parametric methods like Gaussian Processes (GPs) which typically incur an $\\mathcal{O}(N^3)$ computational burden and, consequently, are unable to scale to larger datasets. To this end we introduce a method termed \\emph{Index Set Fourier Series Features} to tractably exploit multivariate Fourier series and efficiently decompose periodic kernels on higher-dimensional data into a series of basis functions. We show that our approximation produces significantly less predictive error than alternative approaches such as those based on random Fourier features and achieves better generalisation on regression problems with periodic data.", "published": "2018-05-14 01:52:02", "link": "http://arxiv.org/abs/1805.04982v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Self and turbo iterations for MIMO receivers and large-scale systems", "abstract": "We investigate a turbo soft detector based on the expectation propagation (EP) algorithm for large-scale multiple-input multiple-output (MIMO) systems. Optimal detection in MIMO systems becomes computationally unfeasible for high-order modulations and/or large number of antennas. In this situation, the linear minimum mean square error (LMMSE) exhibits a low-complexity with a good performance, however far from optimal. To improve the performance, the EP algorithm can be used. In this paper, we review previous EP-based detectors and enhance their estimation in terms of complexity and performance. Specifically, we improve the convergence of the self-iterated EP stage by replacing the uniform prior by a non-uniform one, which better characterizes the information returned by the decoder once the turbo procedure starts. We also review the EP parameters to avoid instabilities when using high-order modulations and to reduce the computational complexity. Simulation results illustrate the robustness and enhanced performance of this novel detector in comparison with previous approaches found in the literature. Results also show that the proposed detector is robust in the presence of imperfect channel state information (CSI).", "published": "2018-05-14 08:49:24", "link": "http://arxiv.org/abs/1805.05065v3", "categories": ["eess.SP"], "primary_category": "eess.SP"}
