{"title": "Word learning and the acquisition of syntactic--semantic overhypotheses", "abstract": "Children learning their first language face multiple problems of induction:\nhow to learn the meanings of words, and how to build meaningful phrases from\nthose words according to syntactic rules. We consider how children might solve\nthese problems efficiently by solving them jointly, via a computational model\nthat learns the syntax and semantics of multi-word utterances in a grounded\nreference game. We select a well-studied empirical case in which children are\naware of patterns linking the syntactic and semantic properties of words ---\nthat the properties picked out by base nouns tend to be related to shape, while\nprenominal adjectives tend to refer to other properties such as color. We show\nthat children applying such inductive biases are accurately reflecting the\nstatistics of child-directed speech, and that inducing similar biases in our\ncomputational model captures children's behavior in a classic adjective\nlearning experiment. Our model incorporating such biases also demonstrates a\nclear data efficiency in learning, relative to a baseline model that learns\nwithout forming syntax-sensitive overhypotheses of word meaning. Thus solving a\nmore complex joint inference problem may make the full problem of language\nacquisition easier, not harder.", "published": "2018-05-14 02:20:07", "link": "http://arxiv.org/abs/1805.04988v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Discourse Coherence in the Wild: A Dataset, Evaluation and Methods", "abstract": "To date there has been very little work on assessing discourse coherence\nmethods on real-world data. To address this, we present a new corpus of\nreal-world texts (GCDC) as well as the first large-scale evaluation of leading\ndiscourse coherence algorithms. We show that neural models, including two that\nwe introduce here (SentAvg and ParSeq), tend to perform best. We analyze these\nperformance differences and discuss patterns we observed in low coherence texts\nin four domains.", "published": "2018-05-14 02:48:29", "link": "http://arxiv.org/abs/1805.04993v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Parser Training with Heterogeneous Treebanks", "abstract": "How to make the most of multiple heterogeneous treebanks when training a\nmonolingual dependency parser is an open question. We start by investigating\npreviously suggested, but little evaluated, strategies for exploiting multiple\ntreebanks based on concatenating training sets, with or without fine-tuning. We\ngo on to propose a new method based on treebank embeddings. We perform\nexperiments for several languages and show that in many cases fine-tuning and\ntreebank embeddings lead to substantial improvements over single treebanks or\nconcatenation, with average gains of 2.0--3.5 LAS points. We argue that\ntreebank embeddings should be preferred due to their conceptual simplicity,\nflexibility and extensibility.", "published": "2018-05-14 09:52:27", "link": "http://arxiv.org/abs/1805.05089v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Spot the Difference corpus: a multi-modal corpus of spontaneous task\n  oriented spoken interactions", "abstract": "This paper describes the Spot the Difference Corpus which contains 54\ninteractions between pairs of subjects interacting to find differences in two\nvery similar scenes. The setup used, the participants' metadata and details\nabout collection are described. We are releasing this corpus of task-oriented\nspontaneous dialogues. This release includes rich transcriptions, annotations,\naudio and video. We believe that this dataset constitutes a valuable resource\nto study several dimensions of human communication that go from turn-taking to\nthe study of referring expressions. In our preliminary analyses we have looked\nat task success (how many differences were found out of the total number of\ndifferences) and how it evolves over time. In addition we have looked at scene\ncomplexity provided by the RGB components' entropy and how it could relate to\nspeech overlaps, interruptions and the expression of uncertainty. We found\nthere is a tendency that more complex scenes have more competitive\ninterruptions.", "published": "2018-05-14 09:57:47", "link": "http://arxiv.org/abs/1805.05091v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bianet: A Parallel News Corpus in Turkish, Kurdish and English", "abstract": "We present a new open-source parallel corpus consisting of news articles\ncollected from the Bianet magazine, an online newspaper that publishes Turkish\nnews, often along with their translations in English and Kurdish. In this\npaper, we describe the collection process of the corpus and its statistical\nproperties. We validate the benefit of using the Bianet corpus by evaluating\nbilingual and multilingual neural machine translation models in English-Turkish\nand English-Kurdish directions.", "published": "2018-05-14 10:07:51", "link": "http://arxiv.org/abs/1805.05095v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unpaired Sentiment-to-Sentiment Translation: A Cycled Reinforcement\n  Learning Approach", "abstract": "The goal of sentiment-to-sentiment \"translation\" is to change the underlying\nsentiment of a sentence while keeping its content. The main challenge is the\nlack of parallel data. To solve this problem, we propose a cycled reinforcement\nlearning method that enables training on unpaired data by collaboration between\na neutralization module and an emotionalization module. We evaluate our\napproach on two review datasets, Yelp and Amazon. Experimental results show\nthat our approach significantly outperforms the state-of-the-art systems.\nEspecially, the proposed method substantially improves the content preservation\nperformance. The BLEU score is improved from 1.64 to 22.46 and from 0.56 to\n14.06 on the two datasets, respectively.", "published": "2018-05-14 14:17:03", "link": "http://arxiv.org/abs/1805.05181v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Dynamic Oracle for Linear-Time 2-Planar Dependency Parsing", "abstract": "We propose an efficient dynamic oracle for training the 2-Planar\ntransition-based parser, a linear-time parser with over 99% coverage on\nnon-projective syntactic corpora. This novel approach outperforms the static\ntraining strategy in the vast majority of languages tested and scored better on\nmost datasets than the arc-hybrid parser enhanced with the SWAP transition,\nwhich can handle unrestricted non-projectivity.", "published": "2018-05-14 14:46:18", "link": "http://arxiv.org/abs/1805.05202v2", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Effects of Word Embeddings on Neural Network-based Pitch Accent\n  Detection", "abstract": "Pitch accent detection often makes use of both acoustic and lexical features\nbased on the fact that pitch accents tend to correlate with certain words. In\nthis paper, we extend a pitch accent detector that involves a convolutional\nneural network to include word embeddings, which are state-of-the-art vector\nrepresentations of words. We examine the effect these features have on\nwithin-corpus and cross-corpus experiments on three English datasets. The\nresults show that while word embeddings can improve the performance in\ncorpus-dependent experiments, they also have the potential to make\ngeneralization to unseen data more challenging.", "published": "2018-05-14 15:40:31", "link": "http://arxiv.org/abs/1805.05237v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Abstractive Meeting Summarization with Multi-Sentence\n  Compression and Budgeted Submodular Maximization", "abstract": "We introduce a novel graph-based framework for abstractive meeting speech\nsummarization that is fully unsupervised and does not rely on any annotations.\nOur work combines the strengths of multiple recent approaches while addressing\ntheir weaknesses. Moreover, we leverage recent advances in word embeddings and\ngraph degeneracy applied to NLP to take exterior semantic knowledge into\naccount, and to design custom diversity and informativeness measures.\nExperiments on the AMI and ICSI corpus show that our system improves on the\nstate-of-the-art. Code and data are publicly available, and our system can be\ninteractively tested.", "published": "2018-05-14 16:36:11", "link": "http://arxiv.org/abs/1805.05271v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AMR Parsing as Graph Prediction with Latent Alignment", "abstract": "Abstract meaning representations (AMRs) are broad-coverage sentence-level\nsemantic representations. AMRs represent sentences as rooted labeled directed\nacyclic graphs. AMR parsing is challenging partly due to the lack of annotated\nalignments between nodes in the graphs and words in the corresponding\nsentences. We introduce a neural parser which treats alignments as latent\nvariables within a joint probabilistic model of concepts, relations and\nalignments. As exact inference requires marginalizing over alignments and is\ninfeasible, we use the variational auto-encoding framework and a continuous\nrelaxation of the discrete alignments. We show that joint modeling is\npreferable to using a pipeline of align and parse. The parser achieves the best\nreported results on the standard benchmark (74.4% on LDC2016E25).", "published": "2018-05-14 16:56:36", "link": "http://arxiv.org/abs/1805.05286v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AMORE-UPF at SemEval-2018 Task 4: BiLSTM with Entity Library", "abstract": "This paper describes our winning contribution to SemEval 2018 Task 4:\nCharacter Identification on Multiparty Dialogues. It is a simple, standard\nmodel with one key innovation, an entity library. Our results show that this\ninnovation greatly facilitates the identification of infrequent characters.\nBecause of the generic nature of our model, this finding is potentially\nrelevant to any task that requires effective learning from sparse or unbalanced\ndata.", "published": "2018-05-14 18:17:12", "link": "http://arxiv.org/abs/1805.05370v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Token-level and sequence-level loss smoothing for RNN language models", "abstract": "Despite the effectiveness of recurrent neural network language models, their\nmaximum likelihood estimation suffers from two limitations. It treats all\nsentences that do not match the ground truth as equally poor, ignoring the\nstructure of the output space. Second, it suffers from \"exposure bias\": during\ntraining tokens are predicted given ground-truth sequences, while at test time\nprediction is conditioned on generated output sequences. To overcome these\nlimitations we build upon the recent reward augmented maximum likelihood\napproach \\ie sequence-level smoothing that encourages the model to predict\nsentences close to the ground truth according to a given performance metric. We\nextend this approach to token-level loss smoothing, and propose improvements to\nthe sequence-level smoothing approach. Our experiments on two different tasks,\nimage captioning and machine translation, show that token-level and\nsequence-level loss smoothing are complementary, and significantly improve\nresults.", "published": "2018-05-14 08:37:50", "link": "http://arxiv.org/abs/1805.05062v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Constructing Narrative Event Evolutionary Graph for Script Event\n  Prediction", "abstract": "Script event prediction requires a model to predict the subsequent event\ngiven an existing event context. Previous models based on event pairs or event\nchains cannot make full use of dense event connections, which may limit their\ncapability of event prediction. To remedy this, we propose constructing an\nevent graph to better utilize the event network information for script event\nprediction. In particular, we first extract narrative event chains from large\nquantities of news corpus, and then construct a narrative event evolutionary\ngraph (NEEG) based on the extracted chains. NEEG can be seen as a knowledge\nbase that describes event evolutionary principles and patterns. To solve the\ninference problem on NEEG, we present a scaled graph neural network (SGNN) to\nmodel event interactions and learn better event representations. Instead of\ncomputing the representations on the whole graph, SGNN processes only the\nconcerned nodes each time, which makes our model feasible to large-scale\ngraphs. By comparing the similarity between input context event representations\nand candidate event representations, we can choose the most reasonable\nsubsequent event. Experimental results on widely used New York Times corpus\ndemonstrate that our model significantly outperforms state-of-the-art baseline\nmethods, by using standard multiple choice narrative cloze evaluation.", "published": "2018-05-14 09:23:26", "link": "http://arxiv.org/abs/1805.05081v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Large-Scale QA-SRL Parsing", "abstract": "We present a new large-scale corpus of Question-Answer driven Semantic Role\nLabeling (QA-SRL) annotations, and the first high-quality QA-SRL parser. Our\ncorpus, QA-SRL Bank 2.0, consists of over 250,000 question-answer pairs for\nover 64,000 sentences across 3 domains and was gathered with a new\ncrowd-sourcing scheme that we show has high precision and good recall at modest\ncost. We also present neural models for two QA-SRL subtasks: detecting argument\nspans for a predicate and generating questions to label the semantic\nrelationship. The best models achieve question accuracy of 82.6% and span-level\naccuracy of 77.6% (under human evaluation) on the full pipelined QA-SRL\nprediction task. They can also, as we show, be used to gather additional\nannotations at low cost.", "published": "2018-05-14 18:50:11", "link": "http://arxiv.org/abs/1805.05377v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A La Carte Embedding: Cheap but Effective Induction of Semantic Feature\n  Vectors", "abstract": "Motivations like domain adaptation, transfer learning, and feature learning\nhave fueled interest in inducing embeddings for rare or unseen words, n-grams,\nsynsets, and other textual features. This paper introduces a la carte\nembedding, a simple and general alternative to the usual word2vec-based\napproaches for building such representations that is based upon recent\ntheoretical results for GloVe-like embeddings. Our method relies mainly on a\nlinear transformation that is efficiently learnable using pretrained word\nvectors and linear regression. This transform is applicable on the fly in the\nfuture when a new text feature or rare word is encountered, even if only a\nsingle usage example is available. We introduce a new dataset showing how the a\nla carte method requires fewer examples of words in context to learn\nhigh-quality embeddings and we obtain state-of-the-art results on a nonce task\nand some unsupervised document classification tasks.", "published": "2018-05-14 19:11:33", "link": "http://arxiv.org/abs/1805.05388v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Did the Model Understand the Question?", "abstract": "We analyze state-of-the-art deep learning models for three tasks: question\nanswering on (1) images, (2) tables, and (3) passages of text. Using the notion\nof \\emph{attribution} (word importance), we find that these deep networks often\nignore important question terms. Leveraging such behavior, we perturb questions\nto craft a variety of adversarial examples. Our strongest attacks drop the\naccuracy of a visual question answering model from $61.1\\%$ to $19\\%$, and that\nof a tabular question answering model from $33.5\\%$ to $3.3\\%$. Additionally,\nwe show how attributions can strengthen attacks proposed by Jia and Liang\n(2017) on paragraph comprehension models. Our results demonstrate that\nattributions can augment standard measures of accuracy and empower\ninvestigation of model performance. When a model is accurate but for the wrong\nreasons, attributions can surface erroneous logic in the model that indicates\ninadequacies in the test data.", "published": "2018-05-14 23:10:28", "link": "http://arxiv.org/abs/1805.05492v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RETURNN as a Generic Flexible Neural Toolkit with Application to\n  Translation and Speech Recognition", "abstract": "We compare the fast training and decoding speed of RETURNN of attention\nmodels for translation, due to fast CUDA LSTM kernels, and a fast pure\nTensorFlow beam search decoder. We show that a layer-wise pretraining scheme\nfor recurrent attention models gives over 1% BLEU improvement absolute and it\nallows to train deeper recurrent encoder networks. Promising preliminary\nresults on max. expected BLEU training are presented. We are able to train\nstate-of-the-art models for translation and end-to-end models for speech\nrecognition and show results on WMT 2017 and Switchboard. The flexibility of\nRETURNN allows a fast research feedback loop to experiment with alternative\narchitectures, and its generality allows to use it on a wide range of\napplications.", "published": "2018-05-14 15:23:40", "link": "http://arxiv.org/abs/1805.05225v2", "categories": ["cs.NE", "cs.AI", "cs.CL"], "primary_category": "cs.NE"}
{"title": "NASH: Toward End-to-End Neural Architecture for Generative Semantic\n  Hashing", "abstract": "Semantic hashing has become a powerful paradigm for fast similarity search in\nmany information retrieval systems. While fairly successful, previous\ntechniques generally require two-stage training, and the binary constraints are\nhandled ad-hoc. In this paper, we present an end-to-end Neural Architecture for\nSemantic Hashing (NASH), where the binary hashing codes are treated as\nBernoulli latent variables. A neural variational inference framework is\nproposed for training, where gradients are directly back-propagated through the\ndiscrete latent variable to optimize the hash function. We also draw\nconnections between proposed method and rate-distortion theory, which provides\na theoretical foundation for the effectiveness of the proposed framework.\nExperimental results on three public datasets demonstrate that our method\nsignificantly outperforms several state-of-the-art models on both unsupervised\nand supervised scenarios.", "published": "2018-05-14 18:04:28", "link": "http://arxiv.org/abs/1805.05361v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Crowdbreaks: Tracking Health Trends using Public Social Media Data and\n  Crowdsourcing", "abstract": "In the past decade, tracking health trends using social media data has shown\ngreat promise, due to a powerful combination of massive adoption of social\nmedia around the world, and increasingly potent hardware and software that\nenables us to work with these new big data streams. At the same time, many\nchallenging problems have been identified. First, there is often a mismatch\nbetween how rapidly online data can change, and how rapidly algorithms are\nupdated, which means that there is limited reusability for algorithms trained\non past data as their performance decreases over time. Second, much of the work\nis focusing on specific issues during a specific past period in time, even\nthough public health institutions would need flexible tools to assess multiple\nevolving situations in real time. Third, most tools providing such capabilities\nare proprietary systems with little algorithmic or data transparency, and thus\nlittle buy-in from the global public health and research community. Here, we\nintroduce Crowdbreaks, an open platform which allows tracking of health trends\nby making use of continuous crowdsourced labelling of public social media\ncontent. The system is built in a way which automatizes the typical workflow\nfrom data collection, filtering, labelling and training of machine learning\nclassifiers and therefore can greatly accelerate the research process in the\npublic health domain. This work introduces the technical aspects of the\nplatform and explores its future use cases.", "published": "2018-05-14 22:59:56", "link": "http://arxiv.org/abs/1805.05491v1", "categories": ["cs.CY", "cs.CL", "cs.SI", "stat.ML"], "primary_category": "cs.CY"}
{"title": "Conversations Gone Awry: Detecting Early Signs of Conversational Failure", "abstract": "One of the main challenges online social systems face is the prevalence of\nantisocial behavior, such as harassment and personal attacks. In this work, we\nintroduce the task of predicting from the very start of a conversation whether\nit will get out of hand. As opposed to detecting undesirable behavior after the\nfact, this task aims to enable early, actionable prediction at a time when the\nconversation might still be salvaged.\n  To this end, we develop a framework for capturing pragmatic devices---such as\npoliteness strategies and rhetorical prompts---used to start a conversation,\nand analyze their relation to its future trajectory. Applying this framework in\na controlled setting, we demonstrate the feasibility of detecting early warning\nsigns of antisocial behavior in online discussions.", "published": "2018-05-14 18:00:03", "link": "http://arxiv.org/abs/1805.05345v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "physics.soc-ph"], "primary_category": "cs.CL"}
