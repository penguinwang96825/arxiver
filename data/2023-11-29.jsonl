{"title": "Biomedical knowledge graph-optimized prompt generation for large\n  language models", "abstract": "Large Language Models (LLMs) are being adopted at an unprecedented rate, yet\nstill face challenges in knowledge-intensive domains like biomedicine.\nSolutions such as pre-training and domain-specific fine-tuning add substantial\ncomputational overhead, requiring further domain expertise. Here, we introduce\na token-optimized and robust Knowledge Graph-based Retrieval Augmented\nGeneration (KG-RAG) framework by leveraging a massive biomedical KG (SPOKE)\nwith LLMs such as Llama-2-13b, GPT-3.5-Turbo and GPT-4, to generate meaningful\nbiomedical text rooted in established knowledge. Compared to the existing RAG\ntechnique for Knowledge Graphs, the proposed method utilizes minimal graph\nschema for context extraction and uses embedding methods for context pruning.\nThis optimization in context extraction results in more than 50% reduction in\ntoken consumption without compromising the accuracy, making a cost-effective\nand robust RAG implementation on proprietary LLMs. KG-RAG consistently enhanced\nthe performance of LLMs across diverse biomedical prompts by generating\nresponses rooted in established knowledge, accompanied by accurate provenance\nand statistical evidence (if available) to substantiate the claims. Further\nbenchmarking on human curated datasets, such as biomedical true/false and\nmultiple-choice questions (MCQ), showed a remarkable 71% boost in the\nperformance of the Llama-2 model on the challenging MCQ dataset, demonstrating\nthe framework's capacity to empower open-source models with fewer parameters\nfor domain specific questions. Furthermore, KG-RAG enhanced the performance of\nproprietary GPT models, such as GPT-3.5 and GPT-4. In summary, the proposed\nframework combines explicit and implicit knowledge of KG and LLM in a token\noptimized fashion, thus enhancing the adaptability of general-purpose LLMs to\ntackle domain-specific questions in a cost-effective fashion.", "published": "2023-11-29 03:07:00", "link": "http://arxiv.org/abs/2311.17330v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are Large Language Models Good Fact Checkers: A Preliminary Study", "abstract": "Recently, Large Language Models (LLMs) have drawn significant attention due\nto their outstanding reasoning capabilities and extensive knowledge repository,\npositioning them as superior in handling various natural language processing\ntasks compared to other language models. In this paper, we present a\npreliminary investigation into the potential of LLMs in fact-checking. This\nstudy aims to comprehensively evaluate various LLMs in tackling specific\nfact-checking subtasks, systematically evaluating their capabilities, and\nconducting a comparative analysis of their performance against pre-trained and\nstate-of-the-art low-parameter models. Experiments demonstrate that LLMs\nachieve competitive performance compared to other small models in most\nscenarios. However, they encounter challenges in effectively handling Chinese\nfact verification and the entirety of the fact-checking pipeline due to\nlanguage inconsistencies and hallucinations. These findings underscore the need\nfor further exploration and research to enhance the proficiency of LLMs as\nreliable fact-checkers, unveiling the potential capability of LLMs and the\npossible challenges in fact-checking tasks.", "published": "2023-11-29 05:04:52", "link": "http://arxiv.org/abs/2311.17355v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CESAR: Automatic Induction of Compositional Instructions for Multi-turn\n  Dialogs", "abstract": "Instruction-based multitasking has played a critical role in the success of\nlarge language models (LLMs) in multi-turn dialog applications. While publicly\navailable LLMs have shown promising performance, when exposed to complex\ninstructions with multiple constraints, they lag against state-of-the-art\nmodels like ChatGPT. In this work, we hypothesize that the availability of\nlarge-scale complex demonstrations is crucial in bridging this gap. Focusing on\ndialog applications, we propose a novel framework, CESAR, that unifies a large\nnumber of dialog tasks in the same format and allows programmatic induction of\ncomplex instructions without any manual effort.\n  We apply CESAR on InstructDial, a benchmark for instruction-based dialog\ntasks. We further enhance InstructDial with new datasets and tasks and utilize\nCESAR to induce complex tasks with compositional instructions. This results in\na new benchmark called InstructDial++, which includes 63 datasets with 86 basic\ntasks and 68 composite tasks. Through rigorous experiments, we demonstrate the\nscalability of CESAR in providing rich instructions. Models trained on\nInstructDial++ can follow compositional prompts, such as prompts that ask for\nmultiple stylistic constraints.", "published": "2023-11-29 06:02:16", "link": "http://arxiv.org/abs/2311.17376v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unveiling the Implicit Toxicity in Large Language Models", "abstract": "The open-endedness of large language models (LLMs) combined with their\nimpressive capabilities may lead to new safety issues when being exploited for\nmalicious use. While recent studies primarily focus on probing toxic outputs\nthat can be easily detected with existing toxicity classifiers, we show that\nLLMs can generate diverse implicit toxic outputs that are exceptionally\ndifficult to detect via simply zero-shot prompting. Moreover, we propose a\nreinforcement learning (RL) based attacking method to further induce the\nimplicit toxicity in LLMs. Specifically, we optimize the language model with a\nreward that prefers implicit toxic outputs to explicit toxic and non-toxic\nones. Experiments on five widely-adopted toxicity classifiers demonstrate that\nthe attack success rate can be significantly improved through RL fine-tuning.\nFor instance, the RL-finetuned LLaMA-13B model achieves an attack success rate\nof 90.04% on BAD and 62.85% on Davinci003. Our findings suggest that LLMs pose\na significant threat in generating undetectable implicit toxic outputs. We\nfurther show that fine-tuning toxicity classifiers on the annotated examples\nfrom our attacking method can effectively enhance their ability to detect\nLLM-generated implicit toxic language. The code is publicly available at\nhttps://github.com/thu-coai/Implicit-Toxicity.", "published": "2023-11-29 06:42:36", "link": "http://arxiv.org/abs/2311.17391v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mergen: The First Manchu-Korean Machine Translation Model Trained on\n  Augmented Data", "abstract": "The Manchu language, with its roots in the historical Manchurian region of\nNortheast China, is now facing a critical threat of extinction, as there are\nvery few speakers left. In our efforts to safeguard the Manchu language, we\nintroduce Mergen, the first-ever attempt at a Manchu-Korean Machine Translation\n(MT) model. To develop this model, we utilize valuable resources such as the\nManwen Laodang(a historical book) and a Manchu-Korean dictionary. Due to the\nscarcity of a Manchu-Korean parallel dataset, we expand our data by employing\nword replacement guided by GloVe embeddings, trained on both monolingual and\nparallel texts. Our approach is built around an encoder-decoder neural machine\ntranslation model, incorporating a bi-directional Gated Recurrent Unit (GRU)\nlayer. The experiments have yielded promising results, showcasing a significant\nenhancement in Manchu-Korean translation, with a remarkable 20-30 point\nincrease in the BLEU score.", "published": "2023-11-29 10:01:48", "link": "http://arxiv.org/abs/2311.17492v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Answer Selection in Community Question Answering with\n  Pre-trained and Large Language Models", "abstract": "Community Question Answering (CQA) becomes increasingly prevalent in recent\nyears. However, there are a large number of answers, which is difficult for\nusers to select the relevant answers. Therefore, answer selection is a very\nsignificant subtask of CQA. In this paper, we first propose the Question-Answer\ncross attention networks (QAN) with pre-trained models for answer selection and\nutilize large language model (LLM) to perform answer selection with knowledge\naugmentation. Specifically, we apply the BERT model as the encoder layer to do\npre-training for question subjects, question bodies and answers, respectively,\nthen the cross attention mechanism selects the most relevant answer for\ndifferent questions. Experiments show that the QAN model achieves\nstate-of-the-art performance on two datasets, SemEval2015 and SemEval2017.\nMoreover, we use the LLM to generate external knowledge from questions and\ncorrect answers to achieve knowledge augmentation for the answer selection task\nby LLM, while optimizing the prompt of LLM in different aspects. The results\nshow that the introduction of external knowledge can improve the correct answer\nselection rate of LLM on datasets SemEval2015 and SemEval2017. Meanwhile, LLM\ncan also select the correct answer on more questions by optimized prompt.", "published": "2023-11-29 10:24:50", "link": "http://arxiv.org/abs/2311.17502v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How to Build an Adaptive AI Tutor for Any Course Using Knowledge\n  Graph-Enhanced Retrieval-Augmented Generation (KG-RAG)", "abstract": "Integrating Large Language Models (LLMs) in Intelligent Tutoring Systems\n(ITS) presents transformative opportunities for personalized education.\nHowever, current implementations face two critical challenges: maintaining\nfactual accuracy and delivering coherent, context-aware instruction. While\nRetrieval-Augmented Generation (RAG) partially addresses these issues, its\nreliance on pure semantic similarity limits its effectiveness in educational\ncontexts where conceptual relationships are crucial. This paper introduces\nKnowledge Graph-enhanced Retrieval-Augmented Generation (KG-RAG), a novel\nframework that integrates structured knowledge representation with\ncontext-aware retrieval to enable more effective AI tutoring. We present three\nkey contributions: (1) a novel architecture that grounds AI responses in\nstructured domain knowledge, (2) empirical validation through controlled\nexperiments (n=76) demonstrating significant learning improvements (35%\nincrease in assessment scores, p<0.001), and (3) a comprehensive implementation\nframework addressing practical deployment considerations. These results\nestablish KG-RAG as a robust solution for developing adaptable AI tutoring\nsystems across diverse educational contexts.", "published": "2023-11-29 15:02:46", "link": "http://arxiv.org/abs/2311.17696v7", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Supervising the Centroid Baseline for Extractive Multi-Document\n  Summarization", "abstract": "The centroid method is a simple approach for extractive multi-document\nsummarization and many improvements to its pipeline have been proposed. We\nfurther refine it by adding a beam search process to the sentence selection and\nalso a centroid estimation attention model that leads to improved results. We\ndemonstrate this in several multi-document summarization datasets, including in\na multilingual scenario.", "published": "2023-11-29 16:11:45", "link": "http://arxiv.org/abs/2311.17771v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hyperpolyglot LLMs: Cross-Lingual Interpretability in Token Embeddings", "abstract": "Cross-lingual transfer learning is an important property of multilingual\nlarge language models (LLMs). But how do LLMs represent relationships between\nlanguages? Every language model has an input layer that maps tokens to vectors.\nThis ubiquitous layer of language models is often overlooked. We find that\nsimilarities between these input embeddings are highly interpretable and that\nthe geometry of these embeddings differs between model families. In one case\n(XLM-RoBERTa), embeddings encode language: tokens in different writing systems\ncan be linearly separated with an average of 99.2% accuracy. Another family\n(mT5) represents cross-lingual semantic similarity: the 50 nearest neighbors\nfor any token represent an average of 7.61 writing systems, and are frequently\ntranslations. This result is surprising given that there is no explicit\nparallel cross-lingual training corpora and no explicit incentive for\ntranslations in pre-training objectives. Our research opens the door for\ninvestigations in 1) The effect of pre-training and model architectures on\nrepresentations of languages and 2) The applications of cross-lingual\nrepresentations embedded in language models.", "published": "2023-11-29 19:20:14", "link": "http://arxiv.org/abs/2311.18034v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero-shot Conversational Summarization Evaluations with small Large\n  Language Models", "abstract": "Large Language Models (LLMs) exhibit powerful summarization abilities.\nHowever, their capabilities on conversational summarization remains under\nexplored. In this work we evaluate LLMs (approx. 10 billion parameters) on\nconversational summarization and showcase their performance on various prompts.\nWe show that the summaries generated by models depend on the instructions and\nthe performance of LLMs vary with different instructions sometimes resulting\nsteep drop in ROUGE scores if prompts are not selected carefully. We also\nevaluate the models with human evaluations and discuss the limitations of the\nmodels on conversational summarization", "published": "2023-11-29 19:34:34", "link": "http://arxiv.org/abs/2311.18041v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ROBBIE: Robust Bias Evaluation of Large Generative Language Models", "abstract": "As generative large language models (LLMs) grow more performant and\nprevalent, we must develop comprehensive enough tools to measure and improve\ntheir fairness. Different prompt-based datasets can be used to measure social\nbias across multiple text domains and demographic axes, meaning that testing\nLLMs on more datasets can potentially help us characterize their biases more\nfully, and better ensure equal and equitable treatment of marginalized\ndemographic groups. In this work, our focus is two-fold:\n  (1) Benchmarking: a comparison of 6 different prompt-based bias and toxicity\nmetrics across 12 demographic axes and 5 families of generative LLMs. Out of\nthose 6 metrics, AdvPromptSet and HolisticBiasR are novel datasets proposed in\nthe paper. The comparison of those benchmarks gives us insights about the bias\nand toxicity of the compared models. Therefore, we explore the frequency of\ndemographic terms in common LLM pre-training corpora and how this may relate to\nmodel biases.\n  (2) Mitigation: we conduct a comprehensive study of how well 3 bias/toxicity\nmitigation techniques perform across our suite of measurements. ROBBIE aims to\nprovide insights for practitioners while deploying a model, emphasizing the\nneed to not only measure potential harms, but also understand how they arise by\ncharacterizing the data, mitigate harms once found, and balance any trade-offs.\nWe open-source our analysis code in hopes of encouraging broader measurements\nof bias in future LLMs.", "published": "2023-11-29 23:03:04", "link": "http://arxiv.org/abs/2311.18140v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DisCGen: A Framework for Discourse-Informed Counterspeech Generation", "abstract": "Counterspeech can be an effective method for battling hateful content on\nsocial media. Automated counterspeech generation can aid in this process.\nGenerated counterspeech, however, can be viable only when grounded in the\ncontext of topic, audience and sensitivity as these factors influence both the\nefficacy and appropriateness. In this work, we propose a novel framework based\non theories of discourse to study the inferential links that connect counter\nspeeches to the hateful comment. Within this framework, we propose: i) a\ntaxonomy of counterspeech derived from discourse frameworks, and ii)\ndiscourse-informed prompting strategies for generating contextually-grounded\ncounterspeech. To construct and validate this framework, we present a process\nfor collecting an in-the-wild dataset of counterspeech from Reddit. Using this\nprocess, we manually annotate a dataset of 3.9k Reddit comment pairs for the\npresence of hatespeech and counterspeech. The positive pairs are annotated for\n10 classes in our proposed taxonomy. We annotate these pairs with paraphrased\ncounterparts to remove offensiveness and first-person references. We show that\nby using our dataset and framework, large language models can generate\ncontextually-grounded counterspeech informed by theories of discourse.\nAccording to our human evaluation, our approaches can act as a safeguard\nagainst critical failures of discourse-agnostic models.", "published": "2023-11-29 23:20:17", "link": "http://arxiv.org/abs/2311.18147v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Uncertainty Guided Global Memory Improves Multi-Hop Question Answering", "abstract": "Transformers have become the gold standard for many natural language\nprocessing tasks and, in particular, for multi-hop question answering (MHQA).\nThis task includes processing a long document and reasoning over the multiple\nparts of it. The landscape of MHQA approaches can be classified into two\nprimary categories. The first group focuses on extracting supporting evidence,\nthereby constraining the QA model's context to predicted facts. Conversely, the\nsecond group relies on the attention mechanism of the long input encoding model\nto facilitate multi-hop reasoning. However, attention-based token\nrepresentations lack explicit global contextual information to connect\nreasoning steps. To address these issues, we propose GEMFormer, a two-stage\nmethod that first collects relevant information over the entire document to the\nmemory and then combines it with local context to solve the task. Our\nexperimental results show that fine-tuning a pre-trained model with\nmemory-augmented input, including the most certain global elements, improves\nthe model's performance on three MHQA datasets compared to the baseline. We\nalso found that the global explicit memory contains information from supporting\nfacts required for the correct answer.", "published": "2023-11-29 23:45:57", "link": "http://arxiv.org/abs/2311.18151v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Elo Uncovered: Robustness and Best Practices in Language Model\n  Evaluation", "abstract": "In Natural Language Processing (NLP), the Elo rating system, originally\ndesigned for ranking players in dynamic games such as chess, is increasingly\nbeing used to evaluate Large Language Models (LLMs) through \"A vs B\" paired\ncomparisons. However, while popular, the system's suitability for assessing\nentities with constant skill levels, such as LLMs, remains relatively\nunexplored. We study two fundamental axioms that evaluation methods should\nadhere to: reliability and transitivity. We conduct extensive evaluation of Elo\nbehaviour, illustrating that individual Elo computations exhibit volatility and\ndelving into the impact of varying the Elo rating system's hyperparameters. We\nshow that these axioms are not always satisfied raising questions about the\nreliability of current comparative evaluations of LLMs. If the current use of\nElo scores is intended to substitute the costly head-to-head comparison of\nLLMs, it is crucial to ensure the ranking is as robust as possible. Guided by\nthe axioms, our findings offer concrete guidelines for enhancing the\nreliability of LLM evaluation methods, suggesting a need for reassessment of\nexisting comparative approaches.", "published": "2023-11-29 00:45:23", "link": "http://arxiv.org/abs/2311.17295v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RoKEPG: RoBERTa and Knowledge Enhancement for Prescription Generation of\n  Traditional Chinese Medicine", "abstract": "Traditional Chinese medicine (TCM) prescription is the most critical form of\nTCM treatment, and uncovering the complex nonlinear relationship between\nsymptoms and TCM is of great significance for clinical practice and assisting\nphysicians in diagnosis and treatment. Although there have been some studies on\nTCM prescription generation, these studies consider a single factor and\ndirectly model the symptom-prescription generation problem mainly based on\nsymptom descriptions, lacking guidance from TCM knowledge. To this end, we\npropose a RoBERTa and Knowledge Enhancement model for Prescription Generation\nof Traditional Chinese Medicine (RoKEPG). RoKEPG is firstly pre-trained by our\nconstructed TCM corpus, followed by fine-tuning the pre-trained model, and the\nmodel is guided to generate TCM prescriptions by introducing four classes of\nknowledge of TCM through the attention mask matrix. Experimental results on the\npublicly available TCM prescription dataset show that RoKEPG improves the F1\nmetric by about 2% over the baseline model with the best results.", "published": "2023-11-29 01:59:38", "link": "http://arxiv.org/abs/2311.17307v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Universal Self-Consistency for Large Language Model Generation", "abstract": "Self-consistency with chain-of-thought prompting (CoT) has demonstrated\nremarkable performance gains on various challenging tasks, by utilizing\nmultiple reasoning paths sampled from large language models (LLMs). However,\nself-consistency relies on the answer extraction process to aggregate multiple\nsolutions, which is not applicable to free-form answers. In this work, we\npropose Universal Self-Consistency (USC), which leverages LLMs themselves to\nselect the most consistent answer among multiple candidates. We evaluate USC on\na variety of benchmarks, including mathematical reasoning, code generation,\nlong-context summarization, and open-ended question answering. On open-ended\ngeneration tasks where the original self-consistency method is not applicable,\nUSC effectively utilizes multiple samples and improves the performance. For\nmathematical reasoning, USC matches the standard self-consistency performance\nwithout requiring the answer formats to be similar. Finally, without access to\nexecution results, USC also matches the execution-based voting performance on\ncode generation.", "published": "2023-11-29 02:07:09", "link": "http://arxiv.org/abs/2311.17311v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring Large Language Models for Human Mobility Prediction under\n  Public Events", "abstract": "Public events, such as concerts and sports games, can be major attractors for\nlarge crowds, leading to irregular surges in travel demand. Accurate human\nmobility prediction for public events is thus crucial for event planning as\nwell as traffic or crowd management. While rich textual descriptions about\npublic events are commonly available from online sources, it is challenging to\nencode such information in statistical or machine learning models. Existing\nmethods are generally limited in incorporating textual information, handling\ndata sparsity, or providing rationales for their predictions. To address these\nchallenges, we introduce a framework for human mobility prediction under public\nevents (LLM-MPE) based on Large Language Models (LLMs), leveraging their\nunprecedented ability to process textual data, learn from minimal examples, and\ngenerate human-readable explanations. Specifically, LLM-MPE first transforms\nraw, unstructured event descriptions from online sources into a standardized\nformat, and then segments historical mobility data into regular and\nevent-related components. A prompting strategy is designed to direct LLMs in\nmaking and rationalizing demand predictions considering historical mobility and\nevent features. A case study is conducted for Barclays Center in New York City,\nbased on publicly available event information and taxi trip data. Results show\nthat LLM-MPE surpasses traditional models, particularly on event days, with\ntextual data significantly enhancing its accuracy. Furthermore, LLM-MPE offers\ninterpretable insights into its predictions. Despite the great potential of\nLLMs, we also identify key challenges including misinformation and high costs\nthat remain barriers to their broader adoption in large-scale human mobility\nanalysis.", "published": "2023-11-29 04:25:15", "link": "http://arxiv.org/abs/2311.17351v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Should we be going MAD? A Look at Multi-Agent Debate Strategies for LLMs", "abstract": "Recent advancements in large language models (LLMs) underscore their\npotential for responding to inquiries in various domains. However, ensuring\nthat generative agents provide accurate and reliable answers remains an ongoing\nchallenge. In this context, multi-agent debate (MAD) has emerged as a promising\nstrategy for enhancing the truthfulness of LLMs. We benchmark a range of\ndebating and prompting strategies to explore the trade-offs between cost, time,\nand accuracy. Importantly, we find that multi-agent debating systems, in their\ncurrent form, do not reliably outperform other proposed prompting strategies,\nsuch as self-consistency and ensembling using multiple reasoning paths.\nHowever, when performing hyperparameter tuning, several MAD systems, such as\nMulti-Persona, perform better. This suggests that MAD protocols might not be\ninherently worse than other approaches, but that they are more sensitive to\ndifferent hyperparameter settings and difficult to optimize. We build on these\nresults to offer insights into improving debating strategies, such as adjusting\nagent agreement levels, which can significantly enhance performance and even\nsurpass all other non-debate protocols we evaluated. We provide an open-source\nrepository to the community with several state-of-the-art protocols together\nwith evaluation scripts to benchmark across popular research datasets.", "published": "2023-11-29 05:54:41", "link": "http://arxiv.org/abs/2311.17371v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TARGET: Template-Transferable Backdoor Attack Against Prompt-based NLP\n  Models via GPT4", "abstract": "Prompt-based learning has been widely applied in many low-resource NLP tasks\nsuch as few-shot scenarios. However, this paradigm has been shown to be\nvulnerable to backdoor attacks. Most of the existing attack methods focus on\ninserting manually predefined templates as triggers in the pre-training phase\nto train the victim model and utilize the same triggers in the downstream task\nto perform inference, which tends to ignore the transferability and\nstealthiness of the templates. In this work, we propose a novel approach of\nTARGET (Template-trAnsfeRable backdoor attack aGainst prompt-basEd NLP models\nvia GPT4), which is a data-independent attack method. Specifically, we first\nutilize GPT4 to reformulate manual templates to generate tone-strong and normal\ntemplates, and the former are injected into the model as a backdoor trigger in\nthe pre-training phase. Then, we not only directly employ the above templates\nin the downstream task, but also use GPT4 to generate templates with similar\ntone to the above templates to carry out transferable attacks. Finally we have\nconducted extensive experiments on five NLP datasets and three BERT series\nmodels, with experimental results justifying that our TARGET method has better\nattack performance and stealthiness compared to the two-external baseline\nmethods on direct attacks, and in addition achieves satisfactory attack\ncapability in the unseen tone-similar templates.", "published": "2023-11-29 08:12:09", "link": "http://arxiv.org/abs/2311.17429v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CLOMO: Counterfactual Logical Modification with Large Language Models", "abstract": "In this study, we delve into the realm of counterfactual reasoning\ncapabilities of large language models (LLMs). Our primary objective is to\ncultivate the counterfactual thought processes within LLMs and rigorously\nassess these processes for their validity. Specifically, we introduce a novel\ntask, Counterfactual Logical Modification (CLOMO), and a high-quality\nhuman-annotated benchmark. In this task, LLMs must adeptly alter a given\nargumentative text to uphold a predetermined logical relationship. To\neffectively evaluate a generation model's counterfactual capabilities, we\npropose an innovative evaluation metric, the decomposed Self-Evaluation Score\n(SES) to directly evaluate the natural language output of LLMs instead of\nmodeling the task as a multiple-choice problem. Analysis shows that the\nproposed automatic metric aligns well with human preference. Our experimental\nresults show that while LLMs demonstrate a notable capacity for logical\ncounterfactual thinking, there remains a discernible gap between their current\nabilities and human performance. Code and data are available at\nhttps://github.com/Eleanor-H/CLOMO.", "published": "2023-11-29 08:29:54", "link": "http://arxiv.org/abs/2311.17438v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Taiwan LLM: Bridging the Linguistic Divide with a Culturally Aligned\n  Language Model", "abstract": "In the realm of language models, the nuanced linguistic and cultural\nintricacies of Traditional Chinese, as spoken in Taiwan, have been largely\noverlooked. This paper introduces Taiwan LLM, a pioneering Large Language Model\nthat specifically caters to the Traditional Chinese language, with a focus on\nthe variant used in Taiwan. Leveraging a comprehensive pretraining corpus and\ninstruction-finetuning datasets, we have developed a model that not only\nunderstands the complexities of Traditional Chinese but also embodies the\ncultural context of Taiwan. Taiwan LLM represents the first of its kind, a\nmodel that is not only linguistically accurate but also culturally resonant\nwith its user base. Our evaluations demonstrate that Taiwan LLM achieves\nsuperior performance in understanding and generating Traditional Chinese text,\noutperforming existing models that are predominantly trained on Simplified\nChinese or English. The open-source release of Taiwan LLM invites collaboration\nand further innovation, ensuring that the linguistic diversity of Chinese\nspeakers is embraced and well-served. The model, datasets, and further\nresources are made publicly available to foster ongoing research and\ndevelopment in this field.", "published": "2023-11-29 09:48:34", "link": "http://arxiv.org/abs/2311.17487v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Reinforcement Replaces Supervision: Query focused Summarization using\n  Deep Reinforcement Learning", "abstract": "Query-focused Summarization (QfS) deals with systems that generate summaries\nfrom document(s) based on a query. Motivated by the insight that Reinforcement\nLearning (RL) provides a generalization to Supervised Learning (SL) for Natural\nLanguage Generation, and thereby performs better (empirically) than SL, we use\nan RL-based approach for this task of QfS. Additionally, we also resolve the\nconflict of employing RL in Transformers with Teacher Forcing. We develop\nmultiple Policy Gradient networks, trained on various reward signals: ROUGE,\nBLEU, and Semantic Similarity, which lead to a 10-point improvement over the\nState-of-the-Art approach on the ROUGE-L metric for a benchmark dataset (ELI5).\nWe also show performance of our approach in zero-shot setting for another\nbenchmark dataset (DebatePedia) -- our approach leads to results comparable to\nbaselines, which were specifically trained on DebatePedia. To aid the RL\ntraining, we propose a better semantic similarity reward, enabled by a novel\nPassage Embedding scheme developed using Cluster Hypothesis. Lastly, we\ncontribute a gold-standard test dataset to further research in QfS and\nLong-form Question Answering (LfQA).", "published": "2023-11-29 10:38:16", "link": "http://arxiv.org/abs/2311.17514v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TimeBench: A Comprehensive Evaluation of Temporal Reasoning Abilities in\n  Large Language Models", "abstract": "Grasping the concept of time is a fundamental facet of human cognition,\nindispensable for truly comprehending the intricacies of the world. Previous\nstudies typically focus on specific aspects of time, lacking a comprehensive\ntemporal reasoning benchmark. To address this, we propose TimeBench, a\ncomprehensive hierarchical temporal reasoning benchmark that covers a broad\nspectrum of temporal reasoning phenomena. TimeBench provides a thorough\nevaluation for investigating the temporal reasoning capabilities of large\nlanguage models. We conduct extensive experiments on GPT-4, LLaMA2, and other\npopular LLMs under various settings. Our experimental results indicate a\nsignificant performance gap between the state-of-the-art LLMs and humans,\nhighlighting that there is still a considerable distance to cover in temporal\nreasoning. Besides, LLMs exhibit capability discrepancies across different\nreasoning categories. Furthermore, we thoroughly analyze the impact of multiple\naspects on temporal reasoning and emphasize the associated challenges. We\naspire for TimeBench to serve as a comprehensive benchmark, fostering research\nin temporal reasoning. Resources are available at:\nhttps://github.com/zchuz/TimeBench", "published": "2023-11-29 14:30:16", "link": "http://arxiv.org/abs/2311.17667v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Minority Stress Detection with Emotions", "abstract": "Psychological stress detection is an important task for mental healthcare\nresearch, but there has been little prior work investigating the effectiveness\nof psychological stress models on minority individuals, who are especially\nvulnerable to poor mental health outcomes. In this work, we use the related\ntask of minority stress detection to evaluate the ability of psychological\nstress models to understand the language of sexual and gender minorities. We\nfind that traditional psychological stress models underperform on minority\nstress detection, and we propose using emotion-infused models to reduce that\nperformance disparity. We further demonstrate that multi-task psychological\nstress models outperform the current state-of-the-art for minority stress\ndetection without directly training on minority stress data. We provide\nexplanatory analysis showing that minority communities have different\ndistributions of emotions than the general population and that emotion-infused\nmodels improve the performance of stress models on underrepresented groups\nbecause of their effectiveness in low-data environments, and we propose that\nintegrating emotions may benefit underrepresented groups in other mental health\ndetection tasks.", "published": "2023-11-29 14:39:38", "link": "http://arxiv.org/abs/2311.17676v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AviationGPT: A Large Language Model for the Aviation Domain", "abstract": "The advent of ChatGPT and GPT-4 has captivated the world with large language\nmodels (LLMs), demonstrating exceptional performance in question-answering,\nsummarization, and content generation. The aviation industry is characterized\nby an abundance of complex, unstructured text data, replete with technical\njargon and specialized terminology. Moreover, labeled data for model building\nare scarce in this domain, resulting in low usage of aviation text data. The\nemergence of LLMs presents an opportunity to transform this situation, but\nthere is a lack of LLMs specifically designed for the aviation domain. To\naddress this gap, we propose AviationGPT, which is built on open-source LLaMA-2\nand Mistral architectures and continuously trained on a wealth of carefully\ncurated aviation datasets. Experimental results reveal that AviationGPT offers\nusers multiple advantages, including the versatility to tackle diverse natural\nlanguage processing (NLP) problems (e.g., question-answering, summarization,\ndocument writing, information extraction, report querying, data cleaning, and\ninteractive data exploration). It also provides accurate and contextually\nrelevant responses within the aviation domain and significantly improves\nperformance (e.g., over a 40% performance gain in tested cases). With\nAviationGPT, the aviation industry is better equipped to address more complex\nresearch problems and enhance the efficiency and safety of National Airspace\nSystem (NAS) operations.", "published": "2023-11-29 14:49:31", "link": "http://arxiv.org/abs/2311.17686v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SenTest: Evaluating Robustness of Sentence Encoders", "abstract": "Contrastive learning has proven to be an effective method for pre-training\nmodels using weakly labeled data in the vision domain. Sentence transformers\nare the NLP counterparts to this architecture, and have been growing in\npopularity due to their rich and effective sentence representations. Having\neffective sentence representations is paramount in multiple tasks, such as\ninformation retrieval, retrieval augmented generation (RAG), and sentence\ncomparison. Keeping in mind the deployability factor of transformers,\nevaluating the robustness of sentence transformers is of utmost importance.\nThis work focuses on evaluating the robustness of the sentence encoders. We\nemploy several adversarial attacks to evaluate its robustness. This system uses\ncharacter-level attacks in the form of random character substitution,\nword-level attacks in the form of synonym replacement, and sentence-level\nattacks in the form of intra-sentence word order shuffling. The results of the\nexperiments strongly undermine the robustness of sentence encoders. The models\nproduce significantly different predictions as well as embeddings on perturbed\ndatasets. The accuracy of the models can fall up to 15 percent on perturbed\ndatasets as compared to unperturbed datasets. Furthermore, the experiments\ndemonstrate that these embeddings does capture the semantic and syntactic\nstructure (sentence order) of sentences. However, existing supervised\nclassification strategies fail to leverage this information, and merely\nfunction as n-gram detectors.", "published": "2023-11-29 15:21:35", "link": "http://arxiv.org/abs/2311.17722v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mukhyansh: A Headline Generation Dataset for Indic Languages", "abstract": "The task of headline generation within the realm of Natural Language\nProcessing (NLP) holds immense significance, as it strives to distill the true\nessence of textual content into concise and attention-grabbing summaries. While\nnoteworthy progress has been made in headline generation for widely spoken\nlanguages like English, there persist numerous challenges when it comes to\ngenerating headlines in low-resource languages, such as the rich and diverse\nIndian languages. A prominent obstacle that specifically hinders headline\ngeneration in Indian languages is the scarcity of high-quality annotated data.\nTo address this crucial gap, we proudly present Mukhyansh, an extensive\nmultilingual dataset, tailored for Indian language headline generation.\nComprising an impressive collection of over 3.39 million article-headline\npairs, Mukhyansh spans across eight prominent Indian languages, namely Telugu,\nTamil, Kannada, Malayalam, Hindi, Bengali, Marathi, and Gujarati. We present a\ncomprehensive evaluation of several state-of-the-art baseline models.\nAdditionally, through an empirical analysis of existing works, we demonstrate\nthat Mukhyansh outperforms all other models, achieving an impressive average\nROUGE-L score of 31.43 across all 8 languages.", "published": "2023-11-29 15:49:24", "link": "http://arxiv.org/abs/2311.17743v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DSS: Synthesizing long Digital Ink using Data augmentation, Style\n  encoding and Split generation", "abstract": "As text generative models can give increasingly long answers, we tackle the\nproblem of synthesizing long text in digital ink. We show that the commonly\nused models for this task fail to generalize to long-form data and how this\nproblem can be solved by augmenting the training data, changing the model\narchitecture and the inference procedure. These methods use contrastive\nlearning technique and are tailored specifically for the handwriting domain.\nThey can be applied to any encoder-decoder model that works with digital ink.\nWe demonstrate that our method reduces the character error rate on long-form\nEnglish data by half compared to baseline RNN and by 16% compared to the\nprevious approach that aims at addressing the same problem. We show that all\nthree parts of the method improve recognizability of generated inks. In\naddition, we evaluate synthesized data in a human study and find that people\nperceive most of generated data as real.", "published": "2023-11-29 16:33:19", "link": "http://arxiv.org/abs/2311.17786v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Higher-Order DisCoCat (Peirce-Lambek-Montague semantics)", "abstract": "We propose a new definition of higher-order DisCoCat (categorical\ncompositional distributional) models where the meaning of a word is not a\ndiagram, but a diagram-valued higher-order function. Our models can be seen as\na variant of Montague semantics based on a lambda calculus where the primitives\nact on string diagrams rather than logical formulae. As a special case, we show\nhow to translate from the Lambek calculus into Peirce's system beta for\nfirst-order logic. This allows us to give a purely diagrammatic treatment of\nhigher-order and non-linear processes in natural language semantics: adverbs,\nprepositions, negation and quantifiers. The theoretical definition presented in\nthis article comes with a proof-of-concept implementation in DisCoPy, the\nPython library for string diagrams.", "published": "2023-11-29 17:04:15", "link": "http://arxiv.org/abs/2311.17813v1", "categories": ["cs.CL", "math.CT"], "primary_category": "cs.CL"}
{"title": "A Pipeline For Discourse Circuits From CCG", "abstract": "There is a significant disconnect between linguistic theory and modern NLP\npractice, which relies heavily on inscrutable black-box architectures.\nDisCoCirc is a newly proposed model for meaning that aims to bridge this\ndivide, by providing neuro-symbolic models that incorporate linguistic\nstructure. DisCoCirc represents natural language text as a `circuit' that\ncaptures the core semantic information of the text. These circuits can then be\ninterpreted as modular machine learning models. Additionally, DisCoCirc fulfils\nanother major aim of providing an NLP model that can be implemented on\nnear-term quantum computers.\n  In this paper we describe a software pipeline that converts English text to\nits DisCoCirc representation. The pipeline achieves coverage over a large\nfragment of the English language. It relies on Combinatory Categorial Grammar\n(CCG) parses of the input text as well as coreference resolution information.\nThis semantic and syntactic information is used in several steps to convert the\ntext into a simply-typed $\\lambda$-calculus term, and then into a circuit\ndiagram. This pipeline will enable the application of the DisCoCirc framework\nto NLP tasks, using both classical and quantum approaches.", "published": "2023-11-29 18:46:29", "link": "http://arxiv.org/abs/2311.17892v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "I Know You Did Not Write That! A Sampling Based Watermarking Method for\n  Identifying Machine Generated Text", "abstract": "Potential harms of Large Language Models such as mass misinformation and\nplagiarism can be partially mitigated if there exists a reliable way to detect\nmachine generated text. In this paper, we propose a new watermarking method to\ndetect machine-generated texts. Our method embeds a unique pattern within the\ngenerated text, ensuring that while the content remains coherent and natural to\nhuman readers, it carries distinct markers that can be identified\nalgorithmically. Specifically, we intervene with the token sampling process in\na way which enables us to trace back our token choices during the detection\nphase. We show how watermarking affects textual quality and compare our\nproposed method with a state-of-the-art watermarking method in terms of\nrobustness and detectability. Through extensive experiments, we demonstrate the\neffectiveness of our watermarking scheme in distinguishing between watermarked\nand non-watermarked text, achieving high detection rates while maintaining\ntextual quality.", "published": "2023-11-29 20:04:57", "link": "http://arxiv.org/abs/2311.18054v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Clinical Risk Prediction Using Language Models: Benefits And\n  Considerations", "abstract": "The utilization of Electronic Health Records (EHRs) for clinical risk\nprediction is on the rise. However, strict privacy regulations limit access to\ncomprehensive health records, making it challenging to apply standard machine\nlearning algorithms in practical real-world scenarios. Previous research has\naddressed this data limitation by incorporating medical ontologies and\nemploying transfer learning methods. In this study, we investigate the\npotential of leveraging language models (LMs) as a means to incorporate\nsupplementary domain knowledge for improving the performance of various\nEHR-based risk prediction tasks. Unlike applying LMs to unstructured EHR data\nsuch as clinical notes, this study focuses on using textual descriptions within\nstructured EHR to make predictions exclusively based on that information. We\nextensively compare against previous approaches across various data types and\nsizes. We find that employing LMs to represent structured EHRs, such as\ndiagnostic histories, leads to improved or at least comparable performance in\ndiverse risk prediction tasks. Furthermore, LM-based approaches offer numerous\nadvantages, including few-shot learning, the capability to handle previously\nunseen medical concepts, and adaptability to various medical vocabularies.\nNevertheless, we underscore, through various experiments, the importance of\nbeing cautious when employing such models, as concerns regarding the\nreliability of LMs persist.", "published": "2023-11-29 04:32:19", "link": "http://arxiv.org/abs/2312.03742v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Easy Data Augmentation in Sentiment Analysis of Cyberbullying", "abstract": "Instagram, a social media platform, has in the vicinity of 2 billion active\nusers in 2023. The platform allows users to post photos and videos with one\nanother. However, cyberbullying remains a significant problem for about 50% of\nyoung Indonesians. To address this issue, sentiment analysis for comment\nfiltering uses a Support Vector Machine (SVM) and Easy Data Augmentation (EDA).\nEDA will augment the dataset, enabling robust prediction and analysis of\ncyberbullying by introducing more variation. Based on the tests, SVM\ncombination with EDA results in a 2.52% increase in the k-Fold Cross Validation\nscore. Our proposed approach shows an improved accuracy of 92.5%, 2.5% higher\nthan that of the existing state-of-the-art method. To maintain the\nreproducibility and replicability of this research, the source code can be\naccessed at uns.id/eda_svm.", "published": "2023-11-29 10:05:58", "link": "http://arxiv.org/abs/2312.03743v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Dynamic interactive group decision making method on two-dimensional\n  language", "abstract": "The language evaluation information of the interactive group decision method\nat present is based on the one-dimension language variable. At the same time,\nmulti-attribute group decision making method based on two-dimension linguistic\ninformation only use single-stage and static evaluation method. In this paper,\nwe propose a dynamic group decision making method based on two-dimension\nlinguistic information, combining dynamic interactive group decision making\nmethods with two-dimensional language evaluation information The method first\nuse Two-Dimensional Uncertain Linguistic Generalized Weighted Aggregation\n(DULGWA) Operators to aggregate the preference information of each decision\nmaker, then adopting dynamic information entropy method to obtain weights of\nattributes at each stage. Finally we propose the group consistency index to\nquantify the termination conditions of group interaction. One example is given\nto verify the developed approach and to demonstrate its effectiveness", "published": "2023-11-29 22:22:55", "link": "http://arxiv.org/abs/2312.03744v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Language Models: A Guide for the Perplexed", "abstract": "Given the growing importance of AI literacy, we decided to write this\ntutorial to help narrow the gap between the discourse among those who study\nlanguage models -- the core technology underlying ChatGPT and similar products\n-- and those who are intrigued and want to learn more about them. In short, we\nbelieve the perspective of researchers and educators can add some clarity to\nthe public's understanding of the technologies beyond what's currently\navailable, which tends to be either extremely technical or promotional material\ngenerated about products by their purveyors.\n  Our approach teases apart the concept of a language model from products built\non them, from the behaviors attributed to or desired from those products, and\nfrom claims about similarity to human cognition. As a starting point, we (1)\noffer a scientific viewpoint that focuses on questions amenable to study\nthrough experimentation; (2) situate language models as they are today in the\ncontext of the research that led to their development; and (3) describe the\nboundaries of what is known about the models at this writing.", "published": "2023-11-29 01:19:02", "link": "http://arxiv.org/abs/2311.17301v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Efficient Stitchable Task Adaptation", "abstract": "The paradigm of pre-training and fine-tuning has laid the foundation for\ndeploying deep learning models. However, most fine-tuning methods are designed\nto meet a specific resource budget. Recently, considering diverse deployment\nscenarios with various resource budgets, SN-Net is introduced to quickly obtain\nnumerous new networks (stitches) from the pre-trained models (anchors) in a\nmodel family via model stitching. Although promising, SN-Net confronts new\nchallenges when adapting it to new target domains, including huge memory and\nstorage requirements and a long and sub-optimal multistage adaptation process.\nIn this work, we present a novel framework, Efficient Stitchable Task\nAdaptation (ESTA), to efficiently produce a palette of fine-tuned models that\nadhere to diverse resource constraints. Specifically, we first tailor\nparameter-efficient fine-tuning to share low-rank updates among the stitches\nwhile maintaining independent bias terms. In this way, we largely reduce\nfine-tuning memory burdens and mitigate the interference among stitches that\narises in task adaptation. Furthermore, we streamline a simple yet effective\none-stage deployment pipeline, which estimates the important stitches to deploy\nwith training-time gradient statistics. By assigning higher sampling\nprobabilities to important stitches, we also get a boosted Pareto frontier.\nExtensive experiments on 25 downstream visual recognition tasks demonstrate\nthat our ESTA is capable of generating stitches with smooth accuracy-efficiency\ntrade-offs and surpasses the direct SN-Net adaptation by remarkable margins\nwith significantly lower training time and fewer trainable parameters.\nFurthermore, we demonstrate the flexibility and scalability of our ESTA\nframework by stitching LLMs from LLaMA family, obtaining chatbot stitches of\nassorted sizes. Source code is available at\nhttps://github.com/ziplab/Stitched_LLaMA", "published": "2023-11-29 04:31:35", "link": "http://arxiv.org/abs/2311.17352v2", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Improving the Robustness of Transformer-based Large Language Models with\n  Dynamic Attention", "abstract": "Transformer-based models, such as BERT and GPT, have been widely adopted in\nnatural language processing (NLP) due to their exceptional performance.\nHowever, recent studies show their vulnerability to textual adversarial attacks\nwhere the model's output can be misled by intentionally manipulating the text\ninputs. Despite various methods that have been proposed to enhance the model's\nrobustness and mitigate this vulnerability, many require heavy consumption\nresources (e.g., adversarial training) or only provide limited protection\n(e.g., defensive dropout). In this paper, we propose a novel method called\ndynamic attention, tailored for the transformer architecture, to enhance the\ninherent robustness of the model itself against various adversarial attacks.\nOur method requires no downstream task knowledge and does not incur additional\ncosts. The proposed dynamic attention consists of two modules: (I) attention\nrectification, which masks or weakens the attention value of the chosen tokens,\nand (ii) dynamic modeling, which dynamically builds the set of candidate\ntokens. Extensive experiments demonstrate that dynamic attention significantly\nmitigates the impact of adversarial attacks, improving up to 33\\% better\nperformance than previous methods against widely-used adversarial attacks. The\nmodel-level design of dynamic attention enables it to be easily combined with\nother defense methods (e.g., adversarial training) to further enhance the\nmodel's robustness. Furthermore, we demonstrate that dynamic attention\npreserves the state-of-the-art robustness space of the original model compared\nto other dynamic modeling methods.", "published": "2023-11-29 07:09:13", "link": "http://arxiv.org/abs/2311.17400v2", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of\n  Video-Language Models", "abstract": "The ability to perceive how objects change over time is a crucial ingredient\nin human intelligence. However, current benchmarks cannot faithfully reflect\nthe temporal understanding abilities of video-language models (VidLMs) due to\nthe existence of static visual shortcuts. To remedy this issue, we present\nVITATECS, a diagnostic VIdeo-Text dAtaset for the evaluation of TEmporal\nConcept underStanding. Specifically, we first introduce a fine-grained taxonomy\nof temporal concepts in natural language in order to diagnose the capability of\nVidLMs to comprehend different temporal aspects. Furthermore, to disentangle\nthe correlation between static and temporal information, we generate\ncounterfactual video descriptions that differ from the original one only in the\nspecified temporal aspect. We employ a semi-automatic data collection framework\nusing large language models and human-in-the-loop annotation to obtain\nhigh-quality counterfactual descriptions efficiently. Evaluation of\nrepresentative video-language understanding models confirms their deficiency in\ntemporal understanding, revealing the need for greater emphasis on the temporal\nelements in video-language research.", "published": "2023-11-29 07:15:34", "link": "http://arxiv.org/abs/2311.17404v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Introduction to Transformers: an NLP Perspective", "abstract": "Transformers have dominated empirical machine learning models of natural\nlanguage processing. In this paper, we introduce basic concepts of Transformers\nand present key techniques that form the recent advances of these models. This\nincludes a description of the standard Transformer architecture, a series of\nmodel refinements, and common applications. Given that Transformers and related\ndeep learning techniques might be evolving in ways we have never seen, we\ncannot dive into all the model details or cover all the technical areas.\nInstead, we focus on just those concepts that are helpful for gaining a good\nunderstanding of Transformers and their variants. We also summarize the key\nideas that impact this field, thereby yielding some insights into the strengths\nand limitations of these models.", "published": "2023-11-29 13:51:04", "link": "http://arxiv.org/abs/2311.17633v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Text as Images: Can Multimodal Large Language Models Follow Printed\n  Instructions in Pixels?", "abstract": "Recent multimodal large language models (MLLMs) have shown promising\ninstruction following capabilities on vision-language tasks. In this work, we\nintroduce VISUAL MODALITY INSTRUCTION (VIM), and investigate how well\nmultimodal models can understand textual instructions provided in pixels,\ndespite not being explicitly trained on such data during pretraining or\nfine-tuning. We adapt VIM to eight benchmarks, including OKVQA, MM-Vet,\nMathVista, MMMU, and probe diverse MLLMs in both the text-modality instruction\n(TEM) setting and VIM setting. Notably, we observe a significant performance\ndisparity between the original TEM and VIM settings for open-source MLLMs,\nindicating that open-source MLLMs face greater challenges when text instruction\nis presented solely in image form. To address this issue, we train v-MLLM, a\ngeneralizable model that is capable to conduct robust instruction following in\nboth text-modality and visual-modality instructions.", "published": "2023-11-29 14:08:53", "link": "http://arxiv.org/abs/2311.17647v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "End-to-end Joint Punctuated and Normalized ASR with a Limited Amount of\n  Punctuated Training Data", "abstract": "Joint punctuated and normalized automatic speech recognition (ASR), that\noutputs transcripts with and without punctuation and casing, remains\nchallenging due to the lack of paired speech and punctuated text data in most\nASR corpora. We propose two approaches to train an end-to-end joint punctuated\nand normalized ASR system using limited punctuated data. The first approach\nuses a language model to convert normalized training transcripts into\npunctuated transcripts. This achieves a better performance on out-of-domain\ntest data, with up to 17% relative Punctuation-Case-aware Word Error Rate\n(PC-WER) reduction. The second approach uses a single decoder conditioned on\nthe type of output. This yields a 42% relative PC-WER reduction compared to\nWhisper-base and a 4% relative (normalized) WER reduction compared to the\nnormalized output of a punctuated-only model. Additionally, our proposed\nmodeldemonstrates the feasibility of a joint ASR system using as little as 5%\npunctuated training data with a moderate (2.42% absolute) PC-WER increase.", "published": "2023-11-29 15:44:39", "link": "http://arxiv.org/abs/2311.17741v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Contextual Knowledge Pursuit for Faithful Visual Synthesis", "abstract": "Modern text-to-vision generative models often hallucinate when the prompt\ndescribing the scene to be generated is underspecified. In large language\nmodels (LLMs), a prevalent strategy to reduce hallucinations is to retrieve\nfactual knowledge from an external database. While such retrieval augmentation\nstrategies have great potential to enhance text-to-vision generators, existing\nstatic top-K retrieval methods explore the knowledge pool once, missing the\nbroader context necessary for high-quality generation. Furthermore, LLMs\ninternally possess rich world knowledge learned during large-scale training\n(parametric knowledge) that could mitigate the need for external data\nretrieval. This paper proposes Contextual Knowledge Pursuit (CKPT), a framework\nthat leverages the complementary strengths of external and parametric knowledge\nto help generators produce reliable visual content. Instead of the one-time\nretrieval of facts from an external database to improve a given prompt, CKPT\nuses (1) an LLM to decide whether to seek external knowledge or to self-elicit\ndescriptions from LLM parametric knowledge, (2) a knowledge pursuit process to\ncontextually seek and sequentially gather most relevant facts, (3) a knowledge\naggregator for prompt enhancement with the gathered fact context, and (4) a\nfiltered fine-tuning objective to improve visual synthesis with richer prompts.\nWe evaluate CKPT across multiple text-driven generative tasks (image, 3D\nrendering, and video) on datasets of rare objects and daily scenarios. Our\nresults show that CKPT is capable of generating faithful and semantically rich\ncontent across diverse visual domains, offering a promising data source for\nzero-shot synthesis and filtered fine-tuning of text-to-vision generative\nmodels.", "published": "2023-11-29 18:51:46", "link": "http://arxiv.org/abs/2311.17898v3", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "DreamSync: Aligning Text-to-Image Generation with Image Understanding\n  Feedback", "abstract": "Despite their wide-spread success, Text-to-Image models (T2I) still struggle\nto produce images that are both aesthetically pleasing and faithful to the\nuser's input text. We introduce DreamSync, a model-agnostic training algorithm\nby design that improves T2I models to be faithful to the text input. DreamSync\nbuilds off a recent insight from TIFA's evaluation framework -- that large\nvision-language models (VLMs) can effectively identify the fine-grained\ndiscrepancies between generated images and the text inputs. DreamSync uses this\ninsight to train T2I models without any labeled data; it improves T2I models\nusing its own generations. First, it prompts the model to generate several\ncandidate images for a given input text. Then, it uses two VLMs to select the\nbest generation: a Visual Question Answering model that measures the alignment\nof generated images to the text, and another that measures the generation's\naesthetic quality. After selection, we use LoRA to iteratively finetune the T2I\nmodel to guide its generation towards the selected best generations. DreamSync\ndoes not need any additional human annotation. model architecture changes, or\nreinforcement learning. Despite its simplicity, DreamSync improves both the\nsemantic alignment and aesthetic appeal of two diffusion-based T2I models,\nevidenced by multiple benchmarks (+1.7% on TIFA, +2.9% on DSG1K, +3.4% on VILA\naesthetic) and human evaluation.", "published": "2023-11-29 03:42:16", "link": "http://arxiv.org/abs/2311.17946v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Self-Infilling Code Generation", "abstract": "This work introduces self-infilling code generation, a general framework that\nincorporates infilling operations into auto-regressive decoding. Our approach\ncapitalizes on the observation that recent infilling-capable code language\nmodels can self-infill: whereas infilling operations aim to fill in the middle\nbased on a predefined prefix and suffix, self-infilling sequentially generates\nboth such surrounding context and the infilled content. We utilize this\ncapability to introduce novel interruption and looping mechanisms in\nconventional decoding, evolving it into a non-monotonic process. Interruptions\nallow for postponing the generation of specific code until a definitive suffix\nis established, enhancing control over the output. Meanwhile, the looping\nmechanism, which leverages the complementary nature of self-infilling and\nleft-to-right decoding, can iteratively update and synchronize each piece of\ngeneration cyclically. Extensive experiments are conducted to demonstrate that\nour proposed decoding process is effective in enhancing both regularity and\nquality across several code generation benchmarks.", "published": "2023-11-29 16:02:06", "link": "http://arxiv.org/abs/2311.17972v3", "categories": ["cs.PL", "cs.CL", "cs.LG"], "primary_category": "cs.PL"}
{"title": "Filtered Semi-Markov CRF", "abstract": "Semi-Markov CRF has been proposed as an alternative to the traditional Linear\nChain CRF for text segmentation tasks such as Named Entity Recognition (NER).\nUnlike CRF, which treats text segmentation as token-level prediction, Semi-CRF\nconsiders segments as the basic unit, making it more expressive. However,\nSemi-CRF suffers from two major drawbacks: (1) quadratic complexity over\nsequence length, as it operates on every span of the input sequence, and (2)\ninferior performance compared to CRF for sequence labeling tasks like NER. In\nthis paper, we introduce Filtered Semi-Markov CRF, a variant of Semi-CRF that\naddresses these issues by incorporating a filtering step to eliminate\nirrelevant segments, reducing complexity and search space. Our approach is\nevaluated on several NER benchmarks, where it outperforms both CRF and Semi-CRF\nwhile being significantly faster. The implementation of our method is available\non \\href{https://github.com/urchade/Filtered-Semi-Markov-CRF}{Github}.", "published": "2023-11-29 19:11:55", "link": "http://arxiv.org/abs/2311.18028v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TurkishBERTweet: Fast and Reliable Large Language Model for Social Media\n  Analysis", "abstract": "Turkish is one of the most popular languages in the world. Wide us of this\nlanguage on social media platforms such as Twitter, Instagram, or Tiktok and\nstrategic position of the country in the world politics makes it appealing for\nthe social network researchers and industry. To address this need, we introduce\nTurkishBERTweet, the first large scale pre-trained language model for Turkish\nsocial media built using almost 900 million tweets. The model shares the same\narchitecture as base BERT model with smaller input length, making\nTurkishBERTweet lighter than BERTurk and can have significantly lower inference\ntime. We trained our model using the same approach for RoBERTa model and\nevaluated on two text classification tasks: Sentiment Classification and Hate\nSpeech Detection. We demonstrate that TurkishBERTweet outperforms the other\navailable alternatives on generalizability and its lower inference time gives\nsignificant advantage to process large-scale datasets. We also compared our\nmodels with the commercial OpenAI solutions in terms of cost and performance to\ndemonstrate TurkishBERTweet is scalable and cost-effective solution. As part of\nour research, we released TurkishBERTweet and fine-tuned LoRA adapters for the\nmentioned tasks under the MIT License to facilitate future research and\napplications on Turkish social media. Our TurkishBERTweet model is available\nat: https://github.com/ViralLab/TurkishBERTweet", "published": "2023-11-29 20:22:44", "link": "http://arxiv.org/abs/2311.18063v1", "categories": ["cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "The Open Review-Based (ORB) dataset: Towards Automatic Assessment of\n  Scientific Papers and Experiment Proposals in High-Energy Physics", "abstract": "With the Open Science approach becoming important for research, the evolution\ntowards open scientific-paper reviews is making an impact on the scientific\ncommunity. However, there is a lack of publicly available resources for\nconducting research activities related to this subject, as only a limited\nnumber of journals and conferences currently allow access to their review\nprocess for interested parties. In this paper, we introduce the new\ncomprehensive Open Review-Based dataset (ORB); it includes a curated list of\nmore than 36,000 scientific papers with their more than 89,000 reviews and\nfinal decisions. We gather this information from two sources: the\nOpenReview.net and SciPost.org websites. However, given the volatile nature of\nthis domain, the software infrastructure that we introduce to supplement the\nORB dataset is designed to accommodate additional resources in the future. The\nORB deliverables include (1) Python code (interfaces and implementations) to\ntranslate document data and metadata into a structured and high-level\nrepresentation, (2) an ETL process (Extract, Transform, Load) to facilitate the\nautomatic updates from defined sources and (3) data files representing the\nstructured data. The paper presents our data architecture and an overview of\nthe collected data along with relevant statistics. For illustration purposes,\nwe also discuss preliminary Natural-Language-Processing-based experiments that\naim to predict (1) papers' acceptance based on their textual embeddings, and\n(2) grading statistics inferred from embeddings as well. We believe ORB\nprovides a valuable resource for researchers interested in open science and\nreview, with our implementation easing the use of this data for further\nanalysis and experimentation. We plan to update ORB as the field matures as\nwell as introduce new resources even more fitted to dedicated scientific\ndomains such as High-Energy Physics.", "published": "2023-11-29 20:52:02", "link": "http://arxiv.org/abs/2312.04576v1", "categories": ["cs.DL", "cs.CL", "cs.LG", "hep-ex"], "primary_category": "cs.DL"}
{"title": "LanGWM: Language Grounded World Model", "abstract": "Recent advances in deep reinforcement learning have showcased its potential\nin tackling complex tasks. However, experiments on visual control tasks have\nrevealed that state-of-the-art reinforcement learning models struggle with\nout-of-distribution generalization. Conversely, expressing higher-level\nconcepts and global contexts is relatively easy using language.\n  Building upon recent success of the large language models, our main objective\nis to improve the state abstraction technique in reinforcement learning by\nleveraging language for robust action selection. Specifically, we focus on\nlearning language-grounded visual features to enhance the world model learning,\na model-based reinforcement learning technique.\n  To enforce our hypothesis explicitly, we mask out the bounding boxes of a few\nobjects in the image observation and provide the text prompt as descriptions\nfor these masked objects. Subsequently, we predict the masked objects along\nwith the surrounding regions as pixel reconstruction, similar to the\ntransformer-based masked autoencoder approach.\n  Our proposed LanGWM: Language Grounded World Model achieves state-of-the-art\nperformance in out-of-distribution test at the 100K interaction steps\nbenchmarks of iGibson point navigation tasks. Furthermore, our proposed\ntechnique of explicit language-grounded visual representation learning has the\npotential to improve models for human-robot interaction because our extracted\nvisual features are language grounded.", "published": "2023-11-29 12:41:55", "link": "http://arxiv.org/abs/2311.17593v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.RO"], "primary_category": "cs.LG"}
{"title": "Look Before You Leap: Unveiling the Power of GPT-4V in Robotic\n  Vision-Language Planning", "abstract": "In this study, we are interested in imbuing robots with the capability of\nphysically-grounded task planning. Recent advancements have shown that large\nlanguage models (LLMs) possess extensive knowledge useful in robotic tasks,\nespecially in reasoning and planning. However, LLMs are constrained by their\nlack of world grounding and dependence on external affordance models to\nperceive environmental information, which cannot jointly reason with LLMs. We\nargue that a task planner should be an inherently grounded, unified multimodal\nsystem. To this end, we introduce Robotic Vision-Language Planning (ViLa), a\nnovel approach for long-horizon robotic planning that leverages vision-language\nmodels (VLMs) to generate a sequence of actionable steps. ViLa directly\nintegrates perceptual data into its reasoning and planning process, enabling a\nprofound understanding of commonsense knowledge in the visual world, including\nspatial layouts and object attributes. It also supports flexible multimodal\ngoal specification and naturally incorporates visual feedback. Our extensive\nevaluation, conducted in both real-robot and simulated environments,\ndemonstrates ViLa's superiority over existing LLM-based planners, highlighting\nits effectiveness in a wide array of open-world manipulation tasks.", "published": "2023-11-29 17:46:25", "link": "http://arxiv.org/abs/2311.17842v2", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.RO"}
{"title": "How Generative-AI can be Effectively used in Government Chatbots", "abstract": "With the rapid development of artificial intelligence and breakthroughs in\nmachine learning and natural language processing, intelligent\nquestion-answering robots have become widely used in government affairs. This\npaper conducts a horizontal comparison between Guangdong Province's government\nchatbots, ChatGPT, and Wenxin Ernie, two large language models, to analyze the\nstrengths and weaknesses of existing government chatbots and AIGC technology.\nThe study finds significant differences between government chatbots and large\nlanguage models. China's government chatbots are still in an exploratory stage\nand have a gap to close to achieve \"intelligence.\" To explore the future\ndirection of government chatbots more deeply, this research proposes targeted\noptimization paths to help generative AI be effectively applied in government\nchatbot conversations.", "published": "2023-11-29 07:27:15", "link": "http://arxiv.org/abs/2312.02181v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "econ.GN", "q-fin.EC"], "primary_category": "cs.CL"}
{"title": "Adapting OpenAI's Whisper for Speech Recognition on Code-Switch\n  Mandarin-English SEAME and ASRU2019 Datasets", "abstract": "This paper details the experimental results of adapting the OpenAI's Whisper\nmodel for Code-Switch Mandarin-English Speech Recognition (ASR) on the SEAME\nand ASRU2019 corpora. We conducted 2 experiments: a) using adaptation data from\n1 to 100/200 hours to demonstrate effectiveness of adaptation, b) examining\ndifferent language ID setup on Whisper prompt.\n  The Mixed Error Rate results show that the amount of adaptation data may be\nas low as $1\\sim10$ hours to achieve saturation in performance gain (SEAME)\nwhile the ASRU task continued to show performance with more adaptation data\n($>$100 hours). For the language prompt, the results show that although various\nprompting strategies initially produce different outcomes, adapting the Whisper\nmodel with code-switch data uniformly improves its performance.\n  These results may be relevant also to the community when applying Whisper for\nrelated tasks of adapting to new target domains.", "published": "2023-11-29 06:16:36", "link": "http://arxiv.org/abs/2311.17382v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "FAT-HuBERT: Front-end Adaptive Training of Hidden-unit BERT for\n  Distortion-Invariant Robust Speech Recognition", "abstract": "Advancements in monaural speech enhancement (SE) techniques have greatly\nimproved the perceptual quality of speech. However, integrating these\ntechniques into automatic speech recognition (ASR) systems has not yielded the\nexpected performance gains, primarily due to the introduction of distortions\nduring the SE process. In this paper, we propose a novel approach called\nFAT-HuBERT, which leverages distortion-invariant self-supervised learning (SSL)\nto enhance the robustness of ASR. To address the distortions introduced by the\nSE frontends, we introduce layer-wise fusion modules that incorporate features\nextracted from both observed noisy signals and enhanced signals. During\ntraining, the SE frontend is randomly selected from a pool of models. We\nevaluate the performance of FAT-HuBERT on simulated noisy speech generated from\nLibriSpeech as well as real-world noisy speech from the CHiME-4 1-channel\ndataset. The experimental results demonstrate a significant relative reduction\nin word error rate (WER).", "published": "2023-11-29 16:35:13", "link": "http://arxiv.org/abs/2311.17790v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Vulnerability of Automatic Identity Recognition to Audio-Visual\n  Deepfakes", "abstract": "The task of deepfakes detection is far from being solved by speech or vision\nresearchers. Several publicly available databases of fake synthetic video and\nspeech were built to aid the development of detection methods. However,\nexisting databases typically focus on visual or voice modalities and provide no\nproof that their deepfakes can in fact impersonate any real person. In this\npaper, we present the first realistic audio-visual database of deepfakes\nSWAN-DF, where lips and speech are well synchronized and video have high visual\nand audio qualities. We took the publicly available SWAN dataset of real videos\nwith different identities to create audio-visual deepfakes using several models\nfrom DeepFaceLab and blending techniques for face swapping and HiFiVC, DiffVC,\nYourTTS, and FreeVC models for voice conversion. From the publicly available\nspeech dataset LibriTTS, we also created a separate database of only audio\ndeepfakes LibriTTS-DF using several latest text to speech methods: YourTTS,\nAdaspeech, and TorToiSe. We demonstrate the vulnerability of a state of the art\nspeaker recognition system, such as ECAPA-TDNN-based model from SpeechBrain, to\nthe synthetic voices. Similarly, we tested face recognition system based on the\nMobileFaceNet architecture to several variants of our visual deepfakes. The\nvulnerability assessment show that by tuning the existing pretrained deepfake\nmodels to specific identities, one can successfully spoof the face and speaker\nrecognition systems in more than 90% of the time and achieve a very realistic\nlooking and sounding fake video of a given person.", "published": "2023-11-29 14:18:04", "link": "http://arxiv.org/abs/2311.17655v1", "categories": ["cs.CV", "cs.AI", "cs.MM", "cs.SD", "eess.AS", "I.4.3; I.2.10; H.5.1"], "primary_category": "cs.CV"}
