{"title": "From Values to Opinions: Predicting Human Behaviors and Stances Using\n  Value-Injected Large Language Models", "abstract": "Being able to predict people's opinions on issues and behaviors in realistic\nscenarios can be helpful in various domains, such as politics and marketing.\nHowever, conducting large-scale surveys like the European Social Survey to\nsolicit people's opinions on individual issues can incur prohibitive costs.\nLeveraging prior research showing influence of core human values on individual\ndecisions and actions, we propose to use value-injected large language models\n(LLM) to predict opinions and behaviors. To this end, we present Value\nInjection Method (VIM), a collection of two methods -- argument generation and\nquestion answering -- designed to inject targeted value distributions into LLMs\nvia fine-tuning. We then conduct a series of experiments on four tasks to test\nthe effectiveness of VIM and the possibility of using value-injected LLMs to\npredict opinions and behaviors of people. We find that LLMs value-injected with\nvariations of VIM substantially outperform the baselines. Also, the results\nsuggest that opinions and behaviors can be better predicted using\nvalue-injected LLMs than the baseline approaches.", "published": "2023-10-27 02:18:10", "link": "http://arxiv.org/abs/2310.17857v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TarGEN: Targeted Data Generation with Large Language Models", "abstract": "The rapid advancement of large language models (LLMs) has sparked interest in\ndata synthesis techniques, aiming to generate diverse and high-quality\nsynthetic datasets. However, these synthetic datasets often suffer from a lack\nof diversity and added noise. In this paper, we present TarGEN, a multi-step\nprompting strategy for generating high-quality synthetic datasets utilizing a\nLLM. An advantage of TarGEN is its seedless nature; it does not require\nspecific task instances, broadening its applicability beyond task replication.\nWe augment TarGEN with a method known as self-correction empowering LLMs to\nrectify inaccurately labeled instances during dataset creation, ensuring\nreliable labels. To assess our technique's effectiveness, we emulate 8 tasks\nfrom the SuperGLUE benchmark and finetune various language models, including\nencoder-only, encoder-decoder, and decoder-only models on both synthetic and\noriginal training sets. Evaluation on the original test set reveals that models\ntrained on datasets generated by TarGEN perform approximately 1-2% points\nbetter than those trained on original datasets (82.84% via syn. vs. 81.12% on\nog. using Flan-T5). When incorporating instruction tuning, the performance\nincreases to 84.54% on synthetic data vs. 81.49% on original data by Flan-T5. A\ncomprehensive analysis of the synthetic dataset compared to the original\ndataset reveals that the synthetic dataset demonstrates similar or higher\nlevels of dataset complexity and diversity. Furthermore, the synthetic dataset\ndisplays a bias level that aligns closely with the original dataset. Finally,\nwhen pre-finetuned on our synthetic SuperGLUE dataset, T5-3B yields impressive\nresults on the OpenLLM leaderboard, surpassing the model trained on the\nSelf-Instruct dataset by 4.14% points. We hope that TarGEN can be helpful for\nquality data generation and reducing the human efforts to create complex\nbenchmarks.", "published": "2023-10-27 03:32:17", "link": "http://arxiv.org/abs/2310.17876v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SOUL: Towards Sentiment and Opinion Understanding of Language", "abstract": "Sentiment analysis is a well-established natural language processing task,\nwith sentiment polarity classification being one of its most popular and\nrepresentative tasks. However, despite the success of pre-trained language\nmodels in this area, they often fall short of capturing the broader\ncomplexities of sentiment analysis. To address this issue, we propose a new\ntask called Sentiment and Opinion Understanding of Language (SOUL). SOUL aims\nto evaluate sentiment understanding through two subtasks: Review Comprehension\n(RC) and Justification Generation (JG). RC seeks to validate statements that\nfocus on subjective information based on a review text, while JG requires\nmodels to provide explanations for their sentiment predictions. To enable\ncomprehensive evaluation, we annotate a new dataset comprising 15,028\nstatements from 3,638 reviews. Experimental results indicate that SOUL is a\nchallenging task for both small and large language models, with a performance\ngap of up to 27% when compared to human performance. Furthermore, evaluations\nconducted with both human experts and GPT-4 highlight the limitations of the\nsmall language model in generating reasoning-based justifications. These\nfindings underscore the challenging nature of the SOUL task for existing\nmodels, emphasizing the need for further advancements in sentiment analysis to\naddress its complexities. The new dataset and code are available at\nhttps://github.com/DAMO-NLP-SG/SOUL.", "published": "2023-10-27 06:48:48", "link": "http://arxiv.org/abs/2310.17924v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "InCharacter: Evaluating Personality Fidelity in Role-Playing Agents\n  through Psychological Interviews", "abstract": "Role-playing agents (RPAs), powered by large language models, have emerged as\na flourishing field of applications. However, a key challenge lies in assessing\nwhether RPAs accurately reproduce the personas of target characters, namely\ntheir character fidelity. Existing methods mainly focus on the knowledge and\nlinguistic patterns of characters. This paper, instead, introduces a novel\nperspective to evaluate the personality fidelity of RPAs with psychological\nscales. Overcoming drawbacks of previous self-report assessments on RPAs, we\npropose InCharacter, namely Interviewing Character agents for personality\ntests. Experiments include various types of RPAs and LLMs, covering 32 distinct\ncharacters on 14 widely used psychological scales. The results validate the\neffectiveness of InCharacter in measuring RPA personalities. Then, with\nInCharacter, we show that state-of-the-art RPAs exhibit personalities highly\naligned with the human-perceived personalities of the characters, achieving an\naccuracy up to 80.7%.", "published": "2023-10-27 08:42:18", "link": "http://arxiv.org/abs/2310.17976v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NLP Evaluation in trouble: On the Need to Measure LLM Data Contamination\n  for each Benchmark", "abstract": "In this position paper, we argue that the classical evaluation on Natural\nLanguage Processing (NLP) tasks using annotated benchmarks is in trouble. The\nworst kind of data contamination happens when a Large Language Model (LLM) is\ntrained on the test split of a benchmark, and then evaluated in the same\nbenchmark. The extent of the problem is unknown, as it is not straightforward\nto measure. Contamination causes an overestimation of the performance of a\ncontaminated model in a target benchmark and associated task with respect to\ntheir non-contaminated counterparts. The consequences can be very harmful, with\nwrong scientific conclusions being published while other correct ones are\ndiscarded. This position paper defines different levels of data contamination\nand argues for a community effort, including the development of automatic and\nsemi-automatic measures to detect when data from a benchmark was exposed to a\nmodel, and suggestions for flagging papers with conclusions that are\ncompromised by data contamination.", "published": "2023-10-27 09:48:29", "link": "http://arxiv.org/abs/2310.18018v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SentMix-3L: A Bangla-English-Hindi Code-Mixed Dataset for Sentiment\n  Analysis", "abstract": "Code-mixing is a well-studied linguistic phenomenon when two or more\nlanguages are mixed in text or speech. Several datasets have been build with\nthe goal of training computational models for code-mixing. Although it is very\ncommon to observe code-mixing with multiple languages, most datasets available\ncontain code-mixed between only two languages. In this paper, we introduce\nSentMix-3L, a novel dataset for sentiment analysis containing code-mixed data\nbetween three languages Bangla, English, and Hindi. We carry out a\ncomprehensive evaluation using SentMix-3L. We show that zero-shot prompting\nwith GPT-3.5 outperforms all transformer-based models on SentMix-3L.", "published": "2023-10-27 09:59:24", "link": "http://arxiv.org/abs/2310.18023v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-grained Evidence Inference for Multi-choice Reading Comprehension", "abstract": "Multi-choice Machine Reading Comprehension (MRC) is a major and challenging\ntask for machines to answer questions according to provided options. Answers in\nmulti-choice MRC cannot be directly extracted in the given passages, and\nessentially require machines capable of reasoning from accurate extracted\nevidence. However, the critical evidence may be as simple as just one word or\nphrase, while it is hidden in the given redundant, noisy passage with multiple\nlinguistic hierarchies from phrase, fragment, sentence until the entire\npassage. We thus propose a novel general-purpose model enhancement which\nintegrates multi-grained evidence comprehensively, named Multi-grained evidence\ninferencer (Mugen), to make up for the inability. Mugen extracts three\ndifferent granularities of evidence: coarse-, middle- and fine-grained\nevidence, and integrates evidence with the original passages, achieving\nsignificant and consistent performance improvement on four multi-choice MRC\nbenchmarks.", "published": "2023-10-27 11:36:18", "link": "http://arxiv.org/abs/2310.18070v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Scalable Framework for Table of Contents Extraction from Complex ESG\n  Annual Reports", "abstract": "Table of contents (ToC) extraction centres on structuring documents in a\nhierarchical manner. In this paper, we propose a new dataset, ESGDoc,\ncomprising 1,093 ESG annual reports from 563 companies spanning from 2001 to\n2022. These reports pose significant challenges due to their diverse structures\nand extensive length. To address these challenges, we propose a new framework\nfor Toc extraction, consisting of three steps: (1) Constructing an initial tree\nof text blocks based on reading order and font sizes; (2) Modelling each tree\nnode (or text block) independently by considering its contextual information\ncaptured in node-centric subtree; (3) Modifying the original tree by taking\nappropriate action on each tree node (Keep, Delete, or Move). This\nconstruction-modelling-modification (CMM) process offers several benefits. It\neliminates the need for pairwise modelling of section headings as in previous\napproaches, making document segmentation practically feasible. By incorporating\nstructured information, each section heading can leverage both local and\nlong-distance context relevant to itself. Experimental results show that our\napproach outperforms the previous state-of-the-art baseline with a fraction of\nrunning time. Our framework proves its scalability by effectively handling\ndocuments of any length.", "published": "2023-10-27 11:40:32", "link": "http://arxiv.org/abs/2310.18073v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mind the Gap: Automated Corpus Creation for Enthymeme Detection and\n  Reconstruction in Learner Arguments", "abstract": "Writing strong arguments can be challenging for learners. It requires to\nselect and arrange multiple argumentative discourse units (ADUs) in a logical\nand coherent way as well as to decide which ADUs to leave implicit, so called\nenthymemes. However, when important ADUs are missing, readers might not be able\nto follow the reasoning or understand the argument's main point. This paper\nintroduces two new tasks for learner arguments: to identify gaps in arguments\n(enthymeme detection) and to fill such gaps (enthymeme reconstruction).\nApproaches to both tasks may help learners improve their argument quality. We\nstudy how corpora for these tasks can be created automatically by deleting ADUs\nfrom an argumentative text that are central to the argument and its quality,\nwhile maintaining the text's naturalness. Based on the ICLEv3 corpus of\nargumentative learner essays, we create 40,089 argument instances for enthymeme\ndetection and reconstruction. Through manual studies, we provide evidence that\nthe proposed corpus creation process leads to the desired quality reduction,\nand results in arguments that are similarly natural to those written by\nlearners. Finally, first baseline approaches to enthymeme detection and\nreconstruction demonstrate the corpus' usefulness.", "published": "2023-10-27 12:33:40", "link": "http://arxiv.org/abs/2310.18098v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Elevating Code-mixed Text Handling through Auditory Information of Words", "abstract": "With the growing popularity of code-mixed data, there is an increasing need\nfor better handling of this type of data, which poses a number of challenges,\nsuch as dealing with spelling variations, multiple languages, different\nscripts, and a lack of resources. Current language models face difficulty in\neffectively handling code-mixed data as they primarily focus on the semantic\nrepresentation of words and ignore the auditory phonetic features. This leads\nto difficulties in handling spelling variations in code-mixed text. In this\npaper, we propose an effective approach for creating language models for\nhandling code-mixed textual data using auditory information of words from\nSOUNDEX. Our approach includes a pre-training step based on\nmasked-language-modelling, which includes SOUNDEX representations (SAMLM) and a\nnew method of providing input data to the pre-trained model. Through\nexperimentation on various code-mixed datasets (of different languages) for\nsentiment, offensive and aggression classification tasks, we establish that our\nnovel language modeling approach (SAMLM) results in improved robustness towards\nadversarial attacks on code-mixed classification tasks. Additionally, our SAMLM\nbased approach also results in better classification results over the popular\nbaselines for code-mixed tasks. We use the explainability technique, SHAP\n(SHapley Additive exPlanations) to explain how the auditory features\nincorporated through SAMLM assist the model to handle the code-mixed text\neffectively and increase robustness against adversarial attacks\n\\footnote{Source code has been made available on\n\\url{https://github.com/20118/DefenseWithPhonetics},\n\\url{https://www.iitp.ac.in/~ai-nlp-ml/resources.html\\#Phonetics}}.", "published": "2023-10-27 14:03:30", "link": "http://arxiv.org/abs/2310.18155v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MPrompt: Exploring Multi-level Prompt Tuning for Machine Reading\n  Comprehension", "abstract": "The large language models have achieved superior performance on various\nnatural language tasks. One major drawback of such approaches is they are\nresource-intensive in fine-tuning new datasets. Soft-prompt tuning presents a\nresource-efficient solution to fine-tune the pre-trained language models (PLMs)\nwhile keeping their weight frozen. Existing soft prompt methods mainly focus on\ndesigning the input-independent prompts that steer the model to fit the domain\nof the new dataset. Those methods often ignore the fine-grained information\nabout the task and context of the text. In this paper, we propose a multi-level\nprompt tuning (MPrompt) method for machine reading comprehension. It utilizes\nprompts at task-specific, domain-specific, and context-specific levels to\nenhance the comprehension of input semantics at different granularities. We\nalso propose an independence constraint to steer each domain-specific prompt to\nfocus on information within its domain to avoid redundancy. Moreover, we\npresent a prompt generator that incorporates context-related knowledge in the\nprompt generation to enhance contextual relevancy. We conducted extensive\nexperiments on 12 benchmarks of various QA formats and achieved an average\nimprovement of 1.94\\% over the state-of-the-art methods.", "published": "2023-10-27 14:24:06", "link": "http://arxiv.org/abs/2310.18167v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lost in Translation, Found in Spans: Identifying Claims in Multilingual\n  Social Media", "abstract": "Claim span identification (CSI) is an important step in fact-checking\npipelines, aiming to identify text segments that contain a checkworthy claim or\nassertion in a social media post. Despite its importance to journalists and\nhuman fact-checkers, it remains a severely understudied problem, and the scarce\nresearch on this topic so far has only focused on English. Here we aim to\nbridge this gap by creating a novel dataset, X-CLAIM, consisting of 7K\nreal-world claims collected from numerous social media platforms in five Indian\nlanguages and English. We report strong baselines with state-of-the-art\nencoder-only language models (e.g., XLM-R) and we demonstrate the benefits of\ntraining on multiple languages over alternative cross-lingual transfer methods\nsuch as zero-shot transfer, or training on translated data, from a\nhigh-resource language such as English. We evaluate generative large language\nmodels from the GPT series using prompting methods on the X-CLAIM dataset and\nwe find that they underperform the smaller encoder-only language models for\nlow-resource languages.", "published": "2023-10-27 15:28:12", "link": "http://arxiv.org/abs/2310.18205v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "INA: An Integrative Approach for Enhancing Negotiation Strategies with\n  Reward-Based Dialogue System", "abstract": "In this paper, we propose a novel negotiation dialogue agent designed for the\nonline marketplace. Our agent is integrative in nature i.e, it possesses the\ncapability to negotiate on price as well as other factors, such as the addition\nor removal of items from a deal bundle, thereby offering a more flexible and\ncomprehensive negotiation experience. We create a new dataset called\nIntegrative Negotiation Dataset (IND) to enable this functionality. For this\ndataset creation, we introduce a new semi-automated data creation method, which\ncombines defining negotiation intents, actions, and intent-action simulation\nbetween users and the agent to generate potential dialogue flows. Finally, the\nprompting of GPT-J, a state-of-the-art language model, is done to generate\ndialogues for a given intent, with a human-in-the-loop process for post-editing\nand refining minor errors to ensure high data quality. We employ a set of novel\nrewards, specifically tailored for the negotiation task to train our\nNegotiation Agent, termed as the Integrative Negotiation Agent (INA). These\nrewards incentivize the chatbot to learn effective negotiation strategies that\ncan adapt to various contextual requirements and price proposals. By leveraging\nthe IND, we train our model and conduct experiments to evaluate the\neffectiveness of our reward-based dialogue system for negotiation. Our results\ndemonstrate that the proposed approach and reward system significantly enhance\nthe agent's negotiation capabilities. The INA successfully engages in\nintegrative negotiations, displaying the ability to dynamically adjust prices\nand negotiate the inclusion or exclusion of items in a bundle deal", "published": "2023-10-27 15:31:16", "link": "http://arxiv.org/abs/2310.18207v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revising with a Backward Glance: Regressions and Skips during Reading as\n  Cognitive Signals for Revision Policies in Incremental Processing", "abstract": "In NLP, incremental processors produce output in instalments, based on\nincoming prefixes of the linguistic input. Some tokens trigger revisions,\ncausing edits to the output hypothesis, but little is known about why models\nrevise when they revise. A policy that detects the time steps where revisions\nshould happen can improve efficiency. Still, retrieving a suitable signal to\ntrain a revision policy is an open problem, since it is not naturally available\nin datasets. In this work, we investigate the appropriateness of regressions\nand skips in human reading eye-tracking data as signals to inform revision\npolicies in incremental sequence labelling. Using generalised mixed-effects\nmodels, we find that the probability of regressions and skips by humans can\npotentially serve as useful predictors for revisions in BiLSTMs and Transformer\nmodels, with consistent results for various languages.", "published": "2023-10-27 16:08:15", "link": "http://arxiv.org/abs/2310.18229v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Approach to Automatically generating Riddles aiding Concept\n  Attainment", "abstract": "One of the primary challenges in online learning environments, is to retain\nlearner engagement. Several different instructional strategies are proposed\nboth in online and offline environments to enhance learner engagement. The\nConcept Attainment Model is one such instructional strategy that focuses on\nlearners acquiring a deeper understanding of a concept rather than just its\ndictionary definition. This is done by searching and listing the properties\nused to distinguish examples from non-examples of various concepts. Our work\nattempts to apply the Concept Attainment Model to build conceptual riddles, to\ndeploy over online learning environments. The approach involves creating\nfactual triples from learning resources, classifying them based on their\nuniqueness to a concept into `Topic Markers' and `Common', followed by\ngenerating riddles based on the Concept Attainment Model's format and capturing\nall possible solutions to those riddles. The results obtained from the human\nevaluation of riddles prove encouraging.", "published": "2023-10-27 17:28:23", "link": "http://arxiv.org/abs/2310.18290v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Teacher Perception of Automatically Extracted Grammar Concepts for L2\n  Language Learning", "abstract": "One of the challenges in language teaching is how best to organize rules\nregarding syntax, semantics, or phonology in a meaningful manner. This not only\nrequires content creators to have pedagogical skills, but also have that\nlanguage's deep understanding. While comprehensive materials to develop such\ncurricula are available in English and some broadly spoken languages, for many\nother languages, teachers need to manually create them in response to their\nstudents' needs. This is challenging because i) it requires that such experts\nbe accessible and have the necessary resources, and ii) describing all the\nintricacies of a language is time-consuming and prone to omission. In this\nwork, we aim to facilitate this process by automatically discovering and\nvisualizing grammar descriptions. We extract descriptions from a natural text\ncorpus that answer questions about morphosyntax (learning of word order,\nagreement, case marking, or word formation) and semantics (learning of\nvocabulary). We apply this method for teaching two Indian languages, Kannada\nand Marathi, which, unlike English, do not have well-developed resources for\nsecond language learning. To assess the perceived utility of the extracted\nmaterial, we enlist the help of language educators from schools in North\nAmerica to perform a manual evaluation, who find the materials have potential\nto be used for their lesson preparation and learner evaluation.", "published": "2023-10-27 18:17:29", "link": "http://arxiv.org/abs/2310.18417v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SDOH-NLI: a Dataset for Inferring Social Determinants of Health from\n  Clinical Notes", "abstract": "Social and behavioral determinants of health (SDOH) play a significant role\nin shaping health outcomes, and extracting these determinants from clinical\nnotes is a first step to help healthcare providers systematically identify\nopportunities to provide appropriate care and address disparities. Progress on\nusing NLP methods for this task has been hindered by the lack of high-quality\npublicly available labeled data, largely due to the privacy and regulatory\nconstraints on the use of real patients' information. This paper introduces a\nnew dataset, SDOH-NLI, that is based on publicly available notes and which we\nrelease publicly. We formulate SDOH extraction as a natural language inference\n(NLI) task, and provide binary textual entailment labels obtained from human\nraters for a cross product of a set of social history snippets as premises and\nSDOH factors as hypotheses. Our dataset differs from standard NLI benchmarks in\nthat our premises and hypotheses are obtained independently. We evaluate both\n\"off-the-shelf\" entailment models as well as models fine-tuned on our data, and\nhighlight the ways in which our dataset appears more challenging than commonly\nused NLI datasets.", "published": "2023-10-27 19:09:30", "link": "http://arxiv.org/abs/2310.18431v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Expanding the Set of Pragmatic Considerations in Conversational AI", "abstract": "Despite considerable performance improvements, current conversational AI\nsystems often fail to meet user expectations. We discuss several pragmatic\nlimitations of current conversational AI systems. We illustrate pragmatic\nlimitations with examples that are syntactically appropriate, but have clear\npragmatic deficiencies. We label our complaints as \"Turing Test Triggers\"\n(TTTs) as they indicate where current conversational AI systems fall short\ncompared to human behavior. We develop a taxonomy of pragmatic considerations\nintended to identify what pragmatic competencies a conversational AI system\nrequires and discuss implications for the design and evaluation of\nconversational AI systems.", "published": "2023-10-27 19:21:50", "link": "http://arxiv.org/abs/2310.18435v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling Legal Reasoning: LM Annotation at the Edge of Human Agreement", "abstract": "Generative language models (LMs) are increasingly used for document\nclass-prediction tasks and promise enormous improvements in cost and\nefficiency. Existing research often examines simple classification tasks, but\nthe capability of LMs to classify on complex or specialized tasks is less well\nunderstood. We consider a highly complex task that is challenging even for\nhumans: the classification of legal reasoning according to jurisprudential\nphilosophy. Using a novel dataset of historical United States Supreme Court\nopinions annotated by a team of domain experts, we systematically test the\nperformance of a variety of LMs. We find that generative models perform poorly\nwhen given instructions (i.e. prompts) equal to the instructions presented to\nhuman annotators through our codebook. Our strongest results derive from\nfine-tuning models on the annotated dataset; the best performing model is an\nin-domain model, LEGAL-BERT. We apply predictions from this fine-tuned model to\nstudy historical trends in jurisprudence, an exercise that both aligns with\nprominent qualitative historical accounts and points to areas of possible\nrefinement in those accounts. Our findings generally sound a note of caution in\nthe use of generative LMs on complex tasks without fine-tuning and point to the\ncontinued relevance of human annotation-intensive classification methods.", "published": "2023-10-27 19:27:59", "link": "http://arxiv.org/abs/2310.18440v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Benchingmaking Large Langage Models in Biomedical Triple Extraction", "abstract": "Biomedical triple extraction systems aim to automatically extract biomedical\nentities and relations between entities. The exploration of applying large\nlanguage models (LLM) to triple extraction is still relatively unexplored. In\nthis work, we mainly focus on sentence-level biomedical triple extraction.\nFurthermore, the absence of a high-quality biomedical triple extraction dataset\nimpedes the progress in developing robust triple extraction systems. To address\nthese challenges, initially, we compare the performance of various large\nlanguage models. Additionally, we present GIT, an expert-annotated biomedical\ntriple extraction dataset that covers a wider range of relation types.", "published": "2023-10-27 20:15:23", "link": "http://arxiv.org/abs/2310.18463v6", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Automatic Generation and Simplification of Children's Stories", "abstract": "With recent advances in large language models (LLMs), the concept of\nautomatically generating children's educational materials has become\nincreasingly realistic. Working toward the goal of age-appropriate simplicity\nin generated educational texts, we first examine the ability of several popular\nLLMs to generate stories with properly adjusted lexical and readability levels.\nWe find that, in spite of the growing capabilities of LLMs, they do not yet\npossess the ability to limit their vocabulary to levels appropriate for younger\nage groups. As a second experiment, we explore the ability of state-of-the-art\nlexical simplification models to generalize to the domain of children's stories\nand, thus, create an efficient pipeline for their automatic generation. In\norder to test these models, we develop a dataset of child-directed lexical\nsimplification instances, with examples taken from the LLM-generated stories in\nour first experiment. We find that, while the strongest-performing current\nlexical simplification models do not perform as well on material designed for\nchildren due to their reliance on large language models behind the scenes, some\nmodels that still achieve fairly strong results on general data can mimic or\neven improve their performance on children-directed data with proper\nfine-tuning, which we conduct using our newly created child-directed\nsimplification dataset.", "published": "2023-10-27 21:31:34", "link": "http://arxiv.org/abs/2310.18502v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Natural Language Interfaces for Tabular Data Querying and Visualization:\n  A Survey", "abstract": "The emergence of natural language processing has revolutionized the way users\ninteract with tabular data, enabling a shift from traditional query languages\nand manual plotting to more intuitive, language-based interfaces. The rise of\nlarge language models (LLMs) such as ChatGPT and its successors has further\nadvanced this field, opening new avenues for natural language processing\ntechniques. This survey presents a comprehensive overview of natural language\ninterfaces for tabular data querying and visualization, which allow users to\ninteract with data using natural language queries. We introduce the fundamental\nconcepts and techniques underlying these interfaces with a particular emphasis\non semantic parsing, the key technology facilitating the translation from\nnatural language to SQL queries or data visualization commands. We then delve\ninto the recent advancements in Text-to-SQL and Text-to-Vis problems from the\nperspectives of datasets, methodologies, metrics, and system designs. This\nincludes a deep dive into the influence of LLMs, highlighting their strengths,\nlimitations, and potential for future improvements. Through this survey, we aim\nto provide a roadmap for researchers and practitioners interested in developing\nand applying natural language interfaces for data interaction in the era of\nlarge language models.", "published": "2023-10-27 05:01:20", "link": "http://arxiv.org/abs/2310.17894v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "3D-Aware Visual Question Answering about Parts, Poses and Occlusions", "abstract": "Despite rapid progress in Visual question answering (VQA), existing datasets\nand models mainly focus on testing reasoning in 2D. However, it is important\nthat VQA models also understand the 3D structure of visual scenes, for example\nto support tasks like navigation or manipulation. This includes an\nunderstanding of the 3D object pose, their parts and occlusions. In this work,\nwe introduce the task of 3D-aware VQA, which focuses on challenging questions\nthat require a compositional reasoning over the 3D structure of visual scenes.\nWe address 3D-aware VQA from both the dataset and the model perspective. First,\nwe introduce Super-CLEVR-3D, a compositional reasoning dataset that contains\nquestions about object parts, their 3D poses, and occlusions. Second, we\npropose PO3D-VQA, a 3D-aware VQA model that marries two powerful ideas:\nprobabilistic neural symbolic program execution for reasoning and deep neural\nnetworks with 3D generative representations of objects for robust visual\nrecognition. Our experimental results show our model PO3D-VQA outperforms\nexisting methods significantly, but we still observe a significant performance\ngap compared to 2D VQA benchmarks, indicating that 3D-aware VQA remains an\nimportant open research area.", "published": "2023-10-27 06:15:30", "link": "http://arxiv.org/abs/2310.17914v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Knowing What LLMs DO NOT Know: A Simple Yet Effective Self-Detection\n  Method", "abstract": "Large Language Models (LLMs) have shown great potential in Natural Language\nProcessing (NLP) tasks. However, recent literature reveals that LLMs generate\nnonfactual responses intermittently, which impedes the LLMs' reliability for\nfurther utilization. In this paper, we propose a novel self-detection method to\ndetect which questions that a LLM does not know that are prone to generate\nnonfactual results. Specifically, we first diversify the textual expressions\nfor a given question and collect the corresponding answers. Then we examine the\ndivergencies between the generated answers to identify the questions that the\nmodel may generate falsehoods. All of the above steps can be accomplished by\nprompting the LLMs themselves without referring to any other external\nresources. We conduct comprehensive experiments and demonstrate the\neffectiveness of our method on recently released LLMs, e.g., Vicuna, ChatGPT,\nand GPT-4.", "published": "2023-10-27 06:22:14", "link": "http://arxiv.org/abs/2310.17918v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large language models for aspect-based sentiment analysis", "abstract": "Large language models (LLMs) offer unprecedented text completion\ncapabilities. As general models, they can fulfill a wide range of roles,\nincluding those of more specialized models. We assess the performance of GPT-4\nand GPT-3.5 in zero shot, few shot and fine-tuned settings on the aspect-based\nsentiment analysis (ABSA) task. Fine-tuned GPT-3.5 achieves a state-of-the-art\nF1 score of 83.8 on the joint aspect term extraction and polarity\nclassification task of the SemEval-2014 Task 4, improving upon InstructABSA\n[@scaria_instructabsa_2023] by 5.7%. However, this comes at the price of 1000\ntimes more model parameters and thus increased inference cost. We discuss the\nthe cost-performance trade-offs of different models, and analyze the typical\nerrors that they make. Our results also indicate that detailed prompts improve\nperformance in zero-shot and few-shot settings but are not necessary for\nfine-tuned models. This evidence is relevant for practioners that are faced\nwith the choice of prompt engineering versus fine-tuning when using LLMs for\nABSA.", "published": "2023-10-27 10:03:21", "link": "http://arxiv.org/abs/2310.18025v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On General Language Understanding", "abstract": "Natural Language Processing prides itself to be an empirically-minded, if not\noutright empiricist field, and yet lately it seems to get itself into\nessentialist debates on issues of meaning and measurement (\"Do Large Language\nModels Understand Language, And If So, How Much?\"). This is not by accident:\nHere, as everywhere, the evidence underspecifies the understanding. As a\nremedy, this paper sketches the outlines of a model of understanding, which can\nground questions of the adequacy of current methods of measurement of model\nquality. The paper makes three claims: A) That different language use situation\ntypes have different characteristics, B) That language understanding is a\nmultifaceted phenomenon, bringing together individualistic and social\nprocesses, and C) That the choice of Understanding Indicator marks the limits\nof benchmarking, and the beginnings of considerations of the ethics of NLP use.", "published": "2023-10-27 10:36:54", "link": "http://arxiv.org/abs/2310.18038v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "ViCLEVR: A Visual Reasoning Dataset and Hybrid Multimodal Fusion Model\n  for Visual Question Answering in Vietnamese", "abstract": "In recent years, Visual Question Answering (VQA) has gained significant\nattention for its diverse applications, including intelligent car assistance,\naiding visually impaired individuals, and document image information retrieval\nusing natural language queries. VQA requires effective integration of\ninformation from questions and images to generate accurate answers. Neural\nmodels for VQA have made remarkable progress on large-scale datasets, with a\nprimary focus on resource-rich languages like English. To address this, we\nintroduce the ViCLEVR dataset, a pioneering collection for evaluating various\nvisual reasoning capabilities in Vietnamese while mitigating biases. The\ndataset comprises over 26,000 images and 30,000 question-answer pairs (QAs),\neach question annotated to specify the type of reasoning involved. Leveraging\nthis dataset, we conduct a comprehensive analysis of contemporary visual\nreasoning systems, offering valuable insights into their strengths and\nlimitations. Furthermore, we present PhoVIT, a comprehensive multimodal fusion\nthat identifies objects in images based on questions. The architecture\neffectively employs transformers to enable simultaneous reasoning over textual\nand visual data, merging both modalities at an early model stage. The\nexperimental findings demonstrate that our proposed model achieves\nstate-of-the-art performance across four evaluation metrics. The accompanying\ncode and dataset have been made publicly accessible at\n\\url{https://github.com/kvt0012/ViCLEVR}. This provision seeks to stimulate\nadvancements within the research community, fostering the development of more\nmultimodal fusion algorithms, specifically tailored to address the nuances of\nlow-resource languages, exemplified by Vietnamese.", "published": "2023-10-27 10:44:50", "link": "http://arxiv.org/abs/2310.18046v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "\"Honey, Tell Me What's Wrong\", Global Explanation of Textual\n  Discriminative Models through Cooperative Generation", "abstract": "The ubiquity of complex machine learning has raised the importance of\nmodel-agnostic explanation algorithms. These methods create artificial\ninstances by slightly perturbing real instances, capturing shifts in model\ndecisions. However, such methods rely on initial data and only provide\nexplanations of the decision for these. To tackle these problems, we propose\nTherapy, the first global and model-agnostic explanation method adapted to text\nwhich requires no input dataset. Therapy generates texts following the\ndistribution learned by a classifier through cooperative generation. Because it\ndoes not rely on initial samples, it allows to generate explanations even when\ndata is absent (e.g., for confidentiality reasons). Moreover, conversely to\nexisting methods that combine multiple local explanations into a global one,\nTherapy offers a global overview of the model behavior on the input space. Our\nexperiments show that although using no input data to generate samples, Therapy\nprovides insightful information about features used by the classifier that is\ncompetitive with the ones from methods relying on input samples and outperforms\nthem when input samples are not specific to the studied model.", "published": "2023-10-27 11:26:27", "link": "http://arxiv.org/abs/2310.18063v1", "categories": ["cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "DUMA: a Dual-Mind Conversational Agent with Fast and Slow Thinking", "abstract": "Inspired by the dual-process theory of human cognition, we introduce DUMA, a\nnovel conversational agent framework that embodies a dual-mind mechanism\nthrough the utilization of two generative Large Language Models (LLMs)\ndedicated to fast and slow thinking respectively. The fast thinking model\nserves as the primary interface for external interactions and initial response\ngeneration, evaluating the necessity for engaging the slow thinking model based\non the complexity of the complete response. When invoked, the slow thinking\nmodel takes over the conversation, engaging in meticulous planning, reasoning,\nand tool utilization to provide a well-analyzed response. This dual-mind\nconfiguration allows for a seamless transition between intuitive responses and\ndeliberate problem-solving processes based on the situation. We have\nconstructed a conversational agent to handle online inquiries in the real\nestate industry. The experiment proves that our method balances effectiveness\nand efficiency, and has a significant improvement compared to the baseline.", "published": "2023-10-27 11:43:46", "link": "http://arxiv.org/abs/2310.18075v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Knowledge Corpus Error in Question Answering", "abstract": "Recent works in open-domain question answering (QA) have explored generating\ncontext passages from large language models (LLMs), replacing the traditional\nretrieval step in the QA pipeline. However, it is not well understood why\ngenerated passages can be more effective than retrieved ones. This study\nrevisits the conventional formulation of QA and introduces the concept of\nknowledge corpus error. This error arises when the knowledge corpus used for\nretrieval is only a subset of the entire string space, potentially excluding\nmore helpful passages that exist outside the corpus. LLMs may mitigate this\nshortcoming by generating passages in a larger space. We come up with an\nexperiment of paraphrasing human-annotated gold context using LLMs to observe\nknowledge corpus error empirically. Our results across three QA benchmarks\nreveal an increased performance (10% - 13%) when using paraphrased passage,\nindicating a signal for the existence of knowledge corpus error. Our code is\navailable at https://github.com/xfactlab/emnlp2023-knowledge-corpus-error", "published": "2023-10-27 11:44:06", "link": "http://arxiv.org/abs/2310.18076v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Detrimental Contexts in Open-Domain Question Answering", "abstract": "For knowledge intensive NLP tasks, it has been widely accepted that accessing\nmore information is a contributing factor to improvements in the model's\nend-to-end performance. However, counter-intuitively, too much context can have\na negative impact on the model when evaluated on common question answering (QA)\ndatasets. In this paper, we analyze how passages can have a detrimental effect\non retrieve-then-read architectures used in question answering. Our empirical\nevidence indicates that the current read architecture does not fully leverage\nthe retrieved passages and significantly degrades its performance when using\nthe whole passages compared to utilizing subsets of them. Our findings\ndemonstrate that model accuracy can be improved by 10% on two popular QA\ndatasets by filtering out detrimental passages. Additionally, these outcomes\nare attained by utilizing existing retrieval methods without further training\nor data. We further highlight the challenges associated with identifying the\ndetrimental passages. First, even with the correct context, the model can make\nan incorrect prediction, posing a challenge in determining which passages are\nmost influential. Second, evaluation typically considers lexical matching,\nwhich is not robust to variations of correct answers. Despite these\nlimitations, our experimental results underscore the pivotal role of\nidentifying and removing these detrimental passages for the context-efficient\nretrieve-then-read pipeline. Code and data are available at\nhttps://github.com/xfactlab/emnlp2023-damaging-retrieval", "published": "2023-10-27 11:45:16", "link": "http://arxiv.org/abs/2310.18077v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards a Unified Conversational Recommendation System: Multi-task\n  Learning via Contextualized Knowledge Distillation", "abstract": "In Conversational Recommendation System (CRS), an agent is asked to recommend\na set of items to users within natural language conversations. To address the\nneed for both conversational capability and personalized recommendations, prior\nworks have utilized separate recommendation and dialogue modules. However, such\napproach inevitably results in a discrepancy between recommendation results and\ngenerated responses. To bridge the gap, we propose a multi-task learning for a\nunified CRS, where a single model jointly learns both tasks via Contextualized\nKnowledge Distillation (ConKD). We introduce two versions of ConKD: hard gate\nand soft gate. The former selectively gates between two task-specific teachers,\nwhile the latter integrates knowledge from both teachers. Our gates are\ncomputed on-the-fly in a context-specific manner, facilitating flexible\nintegration of relevant knowledge. Extensive experiments demonstrate that our\nsingle model significantly improves recommendation performance while enhancing\nfluency, and achieves comparable results in terms of diversity.", "published": "2023-10-27 13:06:24", "link": "http://arxiv.org/abs/2310.18119v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "OpinSummEval: Revisiting Automated Evaluation for Opinion Summarization", "abstract": "Opinion summarization sets itself apart from other types of summarization\ntasks due to its distinctive focus on aspects and sentiments. Although certain\nautomated evaluation methods like ROUGE have gained popularity, we have found\nthem to be unreliable measures for assessing the quality of opinion summaries.\nIn this paper, we present OpinSummEval, a dataset comprising human judgments\nand outputs from 14 opinion summarization models. We further explore the\ncorrelation between 24 automatic metrics and human ratings across four\ndimensions. Our findings indicate that metrics based on neural networks\ngenerally outperform non-neural ones. However, even metrics built on powerful\nbackbones, such as BART and GPT-3/3.5, do not consistently correlate well\nacross all dimensions, highlighting the need for advancements in automated\nevaluation methods for opinion summarization. The code and data are publicly\navailable at https://github.com/A-Chicharito-S/OpinSummEval/tree/main.", "published": "2023-10-27 13:09:54", "link": "http://arxiv.org/abs/2310.18122v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DELPHI: Data for Evaluating LLMs' Performance in Handling Controversial\n  Issues", "abstract": "Controversy is a reflection of our zeitgeist, and an important aspect to any\ndiscourse. The rise of large language models (LLMs) as conversational systems\nhas increased public reliance on these systems for answers to their various\nquestions. Consequently, it is crucial to systematically examine how these\nmodels respond to questions that pertaining to ongoing debates. However, few\nsuch datasets exist in providing human-annotated labels reflecting the\ncontemporary discussions. To foster research in this area, we propose a novel\nconstruction of a controversial questions dataset, expanding upon the publicly\nreleased Quora Question Pairs Dataset. This dataset presents challenges\nconcerning knowledge recency, safety, fairness, and bias. We evaluate different\nLLMs using a subset of this dataset, illuminating how they handle controversial\nissues and the stances they adopt. This research ultimately contributes to our\nunderstanding of LLMs' interaction with controversial issues, paving the way\nfor improvements in their comprehension and handling of complex societal\ndebates.", "published": "2023-10-27 13:23:02", "link": "http://arxiv.org/abs/2310.18130v2", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Disentangled Representation Learning with Large Language Models for\n  Text-Attributed Graphs", "abstract": "Text-attributed graphs (TAGs) are prevalent on the web and research over TAGs\nsuch as citation networks, e-commerce networks and social networks has\nattracted considerable attention in the web community. Recently, large language\nmodels (LLMs) have demonstrated exceptional capabilities across a wide range of\ntasks. However, the existing works focus on harnessing the potential of LLMs\nsolely relying on prompts to convey graph structure information to LLMs, thus\nsuffering from insufficient understanding of the complex structural\nrelationships within TAGs. To address this problem, in this paper we present\nthe Disentangled Graph-Text Learner (DGTL) model, which is able to enhance the\nreasoning and predicting capabilities of LLMs for TAGs. Our proposed DGTL model\nincorporates graph structure information through tailored disentangled graph\nneural network (GNN) layers, enabling LLMs to capture the intricate\nrelationships hidden in text-attributed graphs from multiple structural\nfactors. Furthermore, DGTL operates with frozen pre-trained LLMs, reducing\ncomputational costs and allowing much more flexibility in combining with\ndifferent LLM models. Experimental evaluations demonstrate the effectiveness of\nthe proposed DGTL model on achieving superior or comparable performance over\nstate-of-the-art baselines. Additionally, we also demonstrate that our DGTL\nmodel can offer natural language explanations for predictions, thereby\nsignificantly enhancing model interpretability.", "published": "2023-10-27 14:00:04", "link": "http://arxiv.org/abs/2310.18152v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ArcheType: A Novel Framework for Open-Source Column Type Annotation\n  using Large Language Models", "abstract": "Existing deep-learning approaches to semantic column type annotation (CTA)\nhave important shortcomings: they rely on semantic types which are fixed at\ntraining time; require a large number of training samples per type and incur\nlarge run-time inference costs; and their performance can degrade when\nevaluated on novel datasets, even when types remain constant. Large language\nmodels have exhibited strong zero-shot classification performance on a wide\nrange of tasks and in this paper we explore their use for CTA. We introduce\nArcheType, a simple, practical method for context sampling, prompt\nserialization, model querying, and label remapping, which enables large\nlanguage models to solve CTA problems in a fully zero-shot manner. We ablate\neach component of our method separately, and establish that improvements to\ncontext sampling and label remapping provide the most consistent gains.\nArcheType establishes a new state-of-the-art performance on zero-shot CTA\nbenchmarks (including three new domain-specific benchmarks which we release\nalong with this paper), and when used in conjunction with classical CTA\ntechniques, it outperforms a SOTA DoDuo model on the fine-tuned SOTAB\nbenchmark. Our code is available at https://github.com/penfever/ArcheType.", "published": "2023-10-27 15:31:22", "link": "http://arxiv.org/abs/2310.18208v3", "categories": ["cs.CL", "cs.LG", "H.3.3; H.3; I.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "MalFake: A Multimodal Fake News Identification for Malayalam using\n  Recurrent Neural Networks and VGG-16", "abstract": "The amount of news being consumed online has substantially expanded in recent\nyears. Fake news has become increasingly common, especially in regional\nlanguages like Malayalam, due to the rapid publication and lack of editorial\nstandards on some online sites. Fake news may have a terrible effect on\nsociety, causing people to make bad judgments, lose faith in authorities, and\neven engage in violent behavior. When we take into the context of India, there\nare many regional languages, and fake news is spreading in every language.\nTherefore, providing efficient techniques for identifying false information in\nregional tongues is crucial. Until now, little to no work has been done in\nMalayalam, extracting features from multiple modalities to classify fake news.\nMultimodal approaches are more accurate in detecting fake news, as features\nfrom multiple modalities are extracted to build the deep learning\nclassification model. As far as we know, this is the first piece of work in\nMalayalam that uses multimodal deep learning to tackle false information.\nModels trained with more than one modality typically outperform models taught\nwith only one modality. Our study in the Malayalam language utilizing\nmultimodal deep learning is a significant step toward more effective\nmisinformation detection and mitigation.", "published": "2023-10-27 16:51:29", "link": "http://arxiv.org/abs/2310.18263v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "FP8-LM: Training FP8 Large Language Models", "abstract": "In this paper, we explore FP8 low-bit data formats for efficient training of\nlarge language models (LLMs). Our key insight is that most variables, such as\ngradients and optimizer states, in LLM training can employ low-precision data\nformats without compromising model accuracy and requiring no changes to\nhyper-parameters. Specifically, we propose a new FP8 automatic mixed-precision\nframework for training LLMs. This framework offers three levels of FP8\nutilization to streamline mixed-precision and distributed parallel training for\nLLMs. It gradually incorporates 8-bit gradients, optimizer states, and\ndistributed learning in an incremental manner. Experiment results show that,\nduring the training of GPT-175B model on H100 GPU platform, our FP8\nmixed-precision training framework not only achieved a remarkable 39% reduction\nin real memory usage but also ran 75% faster than the widely adopted BF16\nframework (i.e., Megatron-LM), surpassing the speed of Nvidia Transformer\nEngine by 37%. This largely reduces the training costs for large foundation\nmodels. Furthermore, our FP8 mixed-precision training methodology is generic.\nIt can be seamlessly applied to other tasks such as LLM instruction tuning and\nreinforcement learning with human feedback, offering savings in fine-tuning\nexpenses. Our FP8 low-precision training framework is open-sourced at\n{https://github.com/Azure/MS-AMP}{aka.ms/MS.AMP}.", "published": "2023-10-27 17:59:51", "link": "http://arxiv.org/abs/2310.18313v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "SQLformer: Deep Auto-Regressive Query Graph Generation for Text-to-SQL\n  Translation", "abstract": "In recent years, the task of text-to-SQL translation, which converts natural\nlanguage questions into executable SQL queries, has gained significant\nattention for its potential to democratize data access. Despite its promise,\nchallenges such as adapting to unseen databases and aligning natural language\nwith SQL syntax have hindered widespread adoption. To overcome these issues, we\nintroduce SQLformer, a novel Transformer architecture specifically crafted to\nperform text-to-SQL translation tasks. Our model predicts SQL queries as\nabstract syntax trees (ASTs) in an autoregressive way, incorporating structural\ninductive bias in the encoder and decoder layers. This bias, guided by database\ntable and column selection, aids the decoder in generating SQL query ASTs\nrepresented as graphs in a Breadth-First Search canonical order. Our\nexperiments demonstrate that SQLformer achieves state-of-the-art performance\nacross six prominent text-to-SQL benchmarks.", "published": "2023-10-27 00:13:59", "link": "http://arxiv.org/abs/2310.18376v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "OffMix-3L: A Novel Code-Mixed Dataset in Bangla-English-Hindi for\n  Offensive Language Identification", "abstract": "Code-mixing is a well-studied linguistic phenomenon when two or more\nlanguages are mixed in text or speech. Several works have been conducted on\nbuilding datasets and performing downstream NLP tasks on code-mixed data.\nAlthough it is not uncommon to observe code-mixing of three or more languages,\nmost available datasets in this domain contain code-mixed data from only two\nlanguages. In this paper, we introduce OffMix-3L, a novel offensive language\nidentification dataset containing code-mixed data from three different\nlanguages. We experiment with several models on this dataset and observe that\nBanglishBERT outperforms other transformer-based models and GPT-3.5.", "published": "2023-10-27 09:59:35", "link": "http://arxiv.org/abs/2310.18387v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "T5 meets Tybalt: Author Attribution in Early Modern English Drama Using\n  Large Language Models", "abstract": "Large language models have shown breakthrough potential in many NLP domains.\nHere we consider their use for stylometry, specifically authorship\nidentification in Early Modern English drama. We find both promising and\nconcerning results; LLMs are able to accurately predict the author of\nsurprisingly short passages but are also prone to confidently misattribute\ntexts to specific authors. A fine-tuned t5-large model outperforms all tested\nbaselines, including logistic regression, SVM with a linear kernel, and cosine\ndelta, at attributing small passages. However, we see indications that the\npresence of certain authors in the model's pre-training data affects predictive\nresults in ways that are difficult to assess.", "published": "2023-10-27 20:04:57", "link": "http://arxiv.org/abs/2310.18454v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Do Not Harm Protected Groups in Debiasing Language Representation Models", "abstract": "Language Representation Models (LRMs) trained with real-world data may\ncapture and exacerbate undesired bias and cause unfair treatment of people in\nvarious demographic groups. Several techniques have been investigated for\napplying interventions to LRMs to remove bias in benchmark evaluations on, for\nexample, word embeddings. However, the negative side effects of debiasing\ninterventions are usually not revealed in the downstream tasks. We propose\nxGAP-DEBIAS, a set of evaluations on assessing the fairness of debiasing. In\nthis work, We examine four debiasing techniques on a real-world text\nclassification task and show that reducing biasing is at the cost of degrading\nperformance for all demographic groups, including those the debiasing\ntechniques aim to protect. We advocate that a debiasing technique should have\ngood downstream performance with the constraint of ensuring no harm to the\nprotected group.", "published": "2023-10-27 20:11:38", "link": "http://arxiv.org/abs/2310.18458v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "ASPIRO: Any-shot Structured Parsing-error-Induced ReprOmpting for\n  Consistent Data-to-Text Generation", "abstract": "We present ASPIRO, an approach for structured data verbalisation into short\ntemplate sentences in zero to few-shot settings. Unlike previous methods, our\napproach prompts large language models (LLMs) to directly produce\nentity-agnostic templates, rather than relying on LLMs to faithfully copy the\ngiven example entities, or validating/crafting the templates manually. We\nincorporate LLM re-prompting, triggered by algorithmic parsing checks, as well\nas the PARENT metric induced consistency validation to identify and rectify\ntemplate generation problems in real-time. ASPIRO, compared to direct LLM\noutput, averages 66\\% parsing error rate reduction in generated verbalisations\nof RDF triples on the DART dataset. Our best 5-shot text-davinci-003 setup,\nscoring BLEU of 50.62, METEOR of 45.16, BLEURT of 0.82, NUBIA of 0.87, and\nPARENT of 0.8962 on the Rel2Text dataset, competes effectively with recent\nfine-tuned pre-trained language models.", "published": "2023-10-27 03:39:51", "link": "http://arxiv.org/abs/2310.17877v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Can LLMs Keep a Secret? Testing Privacy Implications of Language Models\n  via Contextual Integrity Theory", "abstract": "The interactive use of large language models (LLMs) in AI assistants (at\nwork, home, etc.) introduces a new set of inference-time privacy risks: LLMs\nare fed different types of information from multiple sources in their inputs\nand are expected to reason about what to share in their outputs, for what\npurpose and with whom, within a given context. In this work, we draw attention\nto the highly critical yet overlooked notion of contextual privacy by proposing\nConfAIde, a benchmark designed to identify critical weaknesses in the privacy\nreasoning capabilities of instruction-tuned LLMs. Our experiments show that\neven the most capable models such as GPT-4 and ChatGPT reveal private\ninformation in contexts that humans would not, 39% and 57% of the time,\nrespectively. This leakage persists even when we employ privacy-inducing\nprompts or chain-of-thought reasoning. Our work underscores the immediate need\nto explore novel inference-time privacy-preserving approaches, based on\nreasoning and theory of mind.", "published": "2023-10-27 04:15:30", "link": "http://arxiv.org/abs/2310.17884v2", "categories": ["cs.AI", "cs.CL", "cs.CR"], "primary_category": "cs.AI"}
{"title": "Transformers as Graph-to-Graph Models", "abstract": "We argue that Transformers are essentially graph-to-graph models, with\nsequences just being a special case. Attention weights are functionally\nequivalent to graph edges. Our Graph-to-Graph Transformer architecture makes\nthis ability explicit, by inputting graph edges into the attention weight\ncomputations and predicting graph edges with attention-like functions, thereby\nintegrating explicit graphs into the latent graphs learned by pretrained\nTransformers. Adding iterative graph refinement provides a joint embedding of\ninput, output, and latent graphs, allowing non-autoregressive graph prediction\nto optimise the complete graph without any bespoke pipeline or decoding\nstrategy. Empirical results show that this architecture achieves\nstate-of-the-art accuracies for modelling a variety of linguistic structures,\nintegrating very effectively with the latent linguistic representations learned\nby pretraining.", "published": "2023-10-27 07:21:37", "link": "http://arxiv.org/abs/2310.17936v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unified Segment-to-Segment Framework for Simultaneous Sequence\n  Generation", "abstract": "Simultaneous sequence generation is a pivotal task for real-time scenarios,\nsuch as streaming speech recognition, simultaneous machine translation and\nsimultaneous speech translation, where the target sequence is generated while\nreceiving the source sequence. The crux of achieving high-quality generation\nwith low latency lies in identifying the optimal moments for generating,\naccomplished by learning a mapping between the source and target sequences.\nHowever, existing methods often rely on task-specific heuristics for different\nsequence types, limiting the model's capacity to adaptively learn the\nsource-target mapping and hindering the exploration of multi-task learning for\nvarious simultaneous tasks. In this paper, we propose a unified\nsegment-to-segment framework (Seg2Seg) for simultaneous sequence generation,\nwhich learns the mapping in an adaptive and unified manner. During the process\nof simultaneous generation, the model alternates between waiting for a source\nsegment and generating a target segment, making the segment serve as the\nnatural bridge between the source and target. To accomplish this, Seg2Seg\nintroduces a latent segment as the pivot between source to target and explores\nall potential source-target mappings via the proposed expectation training,\nthereby learning the optimal moments for generating. Experiments on multiple\nsimultaneous generation tasks demonstrate that Seg2Seg achieves\nstate-of-the-art performance and exhibits better generality across various\ntasks.", "published": "2023-10-27 07:34:51", "link": "http://arxiv.org/abs/2310.17940v4", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Developing a Multilingual Dataset and Evaluation Metrics for\n  Code-Switching: A Focus on Hong Kong's Polylingual Dynamics", "abstract": "The existing audio datasets are predominantly tailored towards single\nlanguages, overlooking the complex linguistic behaviors of multilingual\ncommunities that engage in code-switching. This practice, where individuals\nfrequently mix two or more languages in their daily interactions, is\nparticularly prevalent in multilingual regions such as Hong Kong, China. To\nbridge this gap, we have developed a 34.8-hour dataset of Mixed Cantonese and\nEnglish (MCE) audio using our Multi-Agent Data Generation Framework (MADGF). We\nfine-tuned the open-source multilingual Automatic Speech Recognition (ASR)\nmodel, Whisper, with the MCE dataset, leading to impressive zero-shot\nperformance. The traditional metrics overlook important factors such as latency\nin real-world applications and code-switching scenarios. We have introduced a\nnovel evaluation metric called Fidelity to the Original Audio, Accuracy, and\nLatency (FAL). This metric aims to overcome the limitations of traditional\nmetrics used to assess ASR systems.", "published": "2023-10-27 08:01:55", "link": "http://arxiv.org/abs/2310.17953v4", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Qilin-Med-VL: Towards Chinese Large Vision-Language Model for General\n  Healthcare", "abstract": "Large Language Models (LLMs) have introduced a new era of proficiency in\ncomprehending complex healthcare and biomedical topics. However, there is a\nnoticeable lack of models in languages other than English and models that can\ninterpret multi-modal input, which is crucial for global healthcare\naccessibility. In response, this study introduces Qilin-Med-VL, the first\nChinese large vision-language model designed to integrate the analysis of\ntextual and visual data. Qilin-Med-VL combines a pre-trained Vision Transformer\n(ViT) with a foundational LLM. It undergoes a thorough two-stage curriculum\ntraining process that includes feature alignment and instruction tuning. This\nmethod enhances the model's ability to generate medical captions and answer\ncomplex medical queries. We also release ChiMed-VL, a dataset consisting of\nmore than 1M image-text pairs. This dataset has been carefully curated to\nenable detailed and comprehensive interpretation of medical data using various\ntypes of images.", "published": "2023-10-27 08:05:21", "link": "http://arxiv.org/abs/2310.17956v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Lost in Translation -- Multilingual Misinformation and its Evolution", "abstract": "Misinformation and disinformation are growing threats in the digital age,\nspreading rapidly across languages and borders. This paper investigates the\nprevalence and dynamics of multilingual misinformation through an analysis of\nover 250,000 unique fact-checks spanning 95 languages. First, we find that\nwhile the majority of misinformation claims are only fact-checked once, 11.7%,\ncorresponding to more than 21,000 claims, are checked multiple times. Using\nfact-checks as a proxy for the spread of misinformation, we find 33% of\nrepeated claims cross linguistic boundaries, suggesting that some\nmisinformation permeates language barriers. However, spreading patterns exhibit\nstrong homophily, with misinformation more likely to spread within the same\nlanguage. To study the evolution of claims over time and mutations across\nlanguages, we represent fact-checks with multilingual sentence embeddings and\ncluster semantically similar claims. We analyze the connected components and\nshortest paths connecting different versions of a claim finding that claims\ngradually drift over time and undergo greater alteration when traversing\nlanguages. Overall, this novel investigation of multilingual misinformation\nprovides key insights. It quantifies redundant fact-checking efforts,\nestablishes that some claims diffuse across languages, measures linguistic\nhomophily, and models the temporal and cross-lingual evolution of claims. The\nfindings advocate for expanded information sharing between fact-checkers\nglobally while underscoring the importance of localized verification.", "published": "2023-10-27 12:21:55", "link": "http://arxiv.org/abs/2310.18089v1", "categories": ["cs.CL", "cs.CY", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Ask more, know better: Reinforce-Learned Prompt Questions for Decision\n  Making with Large Language Models", "abstract": "Large language models (LLMs) demonstrate their promise in tackling\ncomplicated practical challenges by combining action-based policies with chain\nof thought (CoT) reasoning. Having high-quality prompts on hand, however, is\nvital to the framework's effectiveness. Currently, these prompts are\nhandcrafted utilising extensive human labor, resulting in CoT policies that\nfrequently fail to generalise. Human intervention is also required to develop\ngrounding functions that ensure low-level controllers appropriately process CoT\nreasoning. In this paper, we propose a comprehensive training framework for\ncomplex task-solving, incorporating human prior knowledge into the learning of\naction policies. To that purpose, we offer a new leader-follower bilevel\nframework that is capable of learning to ask relevant questions (prompts) and\nsubsequently undertaking reasoning to guide the learning of actions. The prompt\npolicy is employed to make introspective revisions based on historical\nfindings, leading the CoT process to consider the anticipated goals and\ngenerate outputs that lead to decisive, high-performing actions. The action\npolicy subsequently learns to comprehend and integrate the CoT outputs to take\nactions. Our empirical data reveal that our framework outperforms leading\nmethods in $5$ decision-making tasks such as Overcooked and FourRoom.", "published": "2023-10-27 13:19:19", "link": "http://arxiv.org/abs/2310.18127v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Personas as a Way to Model Truthfulness in Language Models", "abstract": "Large language models (LLMs) are trained on vast amounts of text from the\ninternet, which contains both factual and misleading information about the\nworld. While unintuitive from a classic view of LMs, recent work has shown that\nthe truth value of a statement can be elicited from the model's\nrepresentations. This paper presents an explanation for why LMs appear to know\nthe truth despite not being trained with truth labels. We hypothesize that the\npretraining data is generated by groups of (un)truthful agents whose outputs\nshare common features, and they form a (un)truthful persona. By training on\nthis data, LMs can infer and represent the persona in its activation space.\nThis allows the model to separate truth from falsehoods and controls the\ntruthfulness of its generation. We show evidence for the persona hypothesis via\ntwo observations: (1) we can probe whether a model's answer will be truthful\nbefore it is generated; (2) finetuning a model on a set of facts improves its\ntruthfulness on unseen topics. Next, using arithmetics as a synthetic\nenvironment, we show that structures of the pretraining data are crucial for\nthe model to infer the truthful persona. Overall, our findings suggest that\nmodels can exploit hierarchical structures in the data to learn abstract\nconcepts like truthfulness.", "published": "2023-10-27 14:27:43", "link": "http://arxiv.org/abs/2310.18168v5", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Style Description based Text-to-Speech with Conditional Prosodic Layer\n  Normalization based Diffusion GAN", "abstract": "In this paper, we present a Diffusion GAN based approach (Prosodic Diff-TTS)\nto generate the corresponding high-fidelity speech based on the style\ndescription and content text as an input to generate speech samples within only\n4 denoising steps. It leverages the novel conditional prosodic layer\nnormalization to incorporate the style embeddings into the multi head attention\nbased phoneme encoder and mel spectrogram decoder based generator architecture\nto generate the speech. The style embedding is generated by fine tuning the\npretrained BERT model on auxiliary tasks such as pitch, speaking speed,\nemotion,gender classifications. We demonstrate the efficacy of our proposed\narchitecture on multi-speaker LibriTTS and PromptSpeech datasets, using\nmultiple quantitative metrics that measure generated accuracy and MOS.", "published": "2023-10-27 14:28:41", "link": "http://arxiv.org/abs/2310.18169v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Davidsonian Scene Graph: Improving Reliability in Fine-grained\n  Evaluation for Text-to-Image Generation", "abstract": "Evaluating text-to-image models is notoriously difficult. A strong recent\napproach for assessing text-image faithfulness is based on QG/A (question\ngeneration and answering), which uses pre-trained foundational models to\nautomatically generate a set of questions and answers from the prompt, and\noutput images are scored based on whether these answers extracted with a visual\nquestion answering model are consistent with the prompt-based answers. This\nkind of evaluation is naturally dependent on the quality of the underlying QG\nand VQA models. We identify and address several reliability challenges in\nexisting QG/A work: (a) QG questions should respect the prompt (avoiding\nhallucinations, duplications, and omissions) and (b) VQA answers should be\nconsistent (not asserting that there is no motorcycle in an image while also\nclaiming the motorcycle is blue). We address these issues with Davidsonian\nScene Graph (DSG), an empirically grounded evaluation framework inspired by\nformal semantics, which is adaptable to any QG/A frameworks. DSG produces\natomic and unique questions organized in dependency graphs, which (i) ensure\nappropriate semantic coverage and (ii) sidestep inconsistent answers. With\nextensive experimentation and human evaluation on a range of model\nconfigurations (LLM, VQA, and T2I), we empirically demonstrate that DSG\naddresses the challenges noted above. Finally, we present DSG-1k, an\nopen-sourced evaluation benchmark that includes 1,060 prompts, covering a wide\nrange of fine-grained semantic categories with a balanced distribution. We\nrelease the DSG-1k prompts and the corresponding DSG questions.", "published": "2023-10-27 16:20:10", "link": "http://arxiv.org/abs/2310.18235v4", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Fine-Tuning Language Models Using Formal Methods Feedback", "abstract": "Although pre-trained language models encode generic knowledge beneficial for\nplanning and control, they may fail to generate appropriate control policies\nfor domain-specific tasks. Existing fine-tuning methods use human feedback to\naddress this limitation, however, sourcing human feedback is labor intensive\nand costly. We present a fully automated approach to fine-tune pre-trained\nlanguage models for applications in autonomous systems, bridging the gap\nbetween generic knowledge and domain-specific requirements while reducing cost.\nThe method synthesizes automaton-based controllers from pre-trained models\nguided by natural language task descriptions. These controllers are verifiable\nagainst independently provided specifications within a world model, which can\nbe abstract or obtained from a high-fidelity simulator. Controllers with high\ncompliance with the desired specifications receive higher ranks, guiding the\niterative fine-tuning process. We provide quantitative evidences, primarily in\nautonomous driving, to demonstrate the method's effectiveness across multiple\ntasks. The results indicate an improvement in percentage of specifications\nsatisfied by the controller from 60% to 90%.", "published": "2023-10-27 16:24:24", "link": "http://arxiv.org/abs/2310.18239v1", "categories": ["cs.AI", "cs.CL", "cs.FL", "cs.RO"], "primary_category": "cs.AI"}
{"title": "Matching of Descriptive Labels to Glossary Descriptions", "abstract": "Semantic text similarity plays an important role in software engineering\ntasks in which engineers are requested to clarify the semantics of descriptive\nlabels (e.g., business terms, table column names) that are often consists of\ntoo short or too generic words and appears in their IT systems. We formulate\nthis type of problem as a task of matching descriptive labels to glossary\ndescriptions. We then propose a framework to leverage an existing semantic text\nsimilarity measurement (STS) and augment it using semantic label enrichment and\nset-based collective contextualization where the former is a method to retrieve\nsentences relevant to a given label and the latter is a method to compute\nsimilarity between two contexts each of which is derived from a set of texts\n(e.g., column names in the same table). We performed an experiment on two\ndatasets derived from publicly available data sources. The result indicated\nthat the proposed methods helped the underlying STS correctly match more\ndescriptive labels with the descriptions.", "published": "2023-10-27 07:09:04", "link": "http://arxiv.org/abs/2310.18385v1", "categories": ["cs.CL", "cs.AI", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Publicly-Detectable Watermarking for Language Models", "abstract": "We present a publicly-detectable watermarking scheme for LMs: the detection\nalgorithm contains no secret information, and it is executable by anyone. We\nembed a publicly-verifiable cryptographic signature into LM output using\nrejection sampling and prove that this produces unforgeable and distortion-free\n(i.e., undetectable without access to the public key) text output. We make use\nof error-correction to overcome periods of low entropy, a barrier for all prior\nwatermarking schemes. We implement our scheme and find that our formal claims\nare met in practice.", "published": "2023-10-27 21:08:51", "link": "http://arxiv.org/abs/2310.18491v4", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Evaluating Cross-Domain Text-to-SQL Models and Benchmarks", "abstract": "Text-to-SQL benchmarks play a crucial role in evaluating the progress made in\nthe field and the ranking of different models. However, accurately matching a\nmodel-generated SQL query to a reference SQL query in a benchmark fails for\nvarious reasons, such as underspecified natural language queries, inherent\nassumptions in both model-generated and reference queries, and the\nnon-deterministic nature of SQL output under certain conditions. In this paper,\nwe conduct an extensive study of several prominent cross-domain text-to-SQL\nbenchmarks and re-evaluate some of the top-performing models within these\nbenchmarks, by both manually evaluating the SQL queries and rewriting them in\nequivalent expressions. Our evaluation reveals that attaining a perfect\nperformance on these benchmarks is unfeasible due to the multiple\ninterpretations that can be derived from the provided samples. Furthermore, we\nfind that the true performance of the models is underestimated and their\nrelative performance changes after a re-evaluation. Most notably, our\nevaluation reveals a surprising discovery: a recent GPT4-based model surpasses\nthe gold standard reference queries in the Spider benchmark in our human\nevaluation. This finding highlights the importance of interpreting benchmark\nevaluations cautiously, while also acknowledging the critical role of\nadditional independent evaluations in driving advancements in the field.", "published": "2023-10-27 23:36:14", "link": "http://arxiv.org/abs/2310.18538v1", "categories": ["cs.CL", "cs.DB", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Relative Transfer Function Vector Estimation for Acoustic Sensor\n  Networks Exploiting Covariance Matrix Structure", "abstract": "In many multi-microphone algorithms for noise reduction, an estimate of the\nrelative transfer function (RTF) vector of the target speaker is required. The\nstate-of-the-art covariance whitening (CW) method estimates the RTF vector as\nthe principal eigenvector of the whitened noisy covariance matrix, where\nwhitening is performed using an estimate of the noise covariance matrix. In\nthis paper, we consider an acoustic sensor network consisting of multiple\nmicrophone nodes. Assuming uncorrelated noise between the nodes but not within\nthe nodes, we propose two RTF vector estimation methods that leverage the\nblock-diagonal structure of the noise covariance matrix. The first method\nmodifies the CW method by considering only the diagonal blocks of the estimated\nnoise covariance matrix. In contrast, the second method only considers the\noff-diagonal blocks of the noisy covariance matrix, but cannot be solved using\na simple eigenvalue decomposition. When applying the estimated RTF vector in a\nminimum variance distortionless response beamformer, simulation results for\nreal-world recordings in a reverberant environment with multiple noise sources\nshow that the modified CW method performs slightly better than the CW method in\nterms of SNR improvement, while the off-diagonal selection method outperforms a\nbiased RTF vector estimate obtained as the principal eigenvector of the noisy\ncovariance matrix.", "published": "2023-10-27 15:18:20", "link": "http://arxiv.org/abs/2310.18199v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "MixRep: Hidden Representation Mixup for Low-Resource Speech Recognition", "abstract": "In this paper, we present MixRep, a simple and effective data augmentation\nstrategy based on mixup for low-resource ASR. MixRep interpolates the feature\ndimensions of hidden representations in the neural network that can be applied\nto both the acoustic feature input and the output of each layer, which\ngeneralizes the previous MixSpeech method. Further, we propose to combine the\nmixup with a regularization along the time axis of the input, which is shown as\ncomplementary. We apply MixRep to a Conformer encoder of an E2E LAS\narchitecture trained with a joint CTC loss. We experiment on the WSJ dataset\nand subsets of the SWB dataset, covering reading and telephony conversational\nspeech. Experimental results show that MixRep consistently outperforms other\nregularization methods for low-resource ASR. Compared to a strong SpecAugment\nbaseline, MixRep achieves a +6.5\\% and a +6.7\\% relative WER reduction on the\neval92 set and the Callhome part of the eval'2000 set.", "published": "2023-10-27 19:48:00", "link": "http://arxiv.org/abs/2310.18450v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "TorchAudio 2.1: Advancing speech recognition, self-supervised learning,\n  and audio processing components for PyTorch", "abstract": "TorchAudio is an open-source audio and speech processing library built for\nPyTorch. It aims to accelerate the research and development of audio and speech\ntechnologies by providing well-designed, easy-to-use, and performant PyTorch\ncomponents. Its contributors routinely engage with users to understand their\nneeds and fulfill them by developing impactful features. Here, we survey\nTorchAudio's development principles and contents and highlight key features we\ninclude in its latest version (2.1): self-supervised learning pre-trained\npipelines and training recipes, high-performance CTC decoders, speech\nrecognition models and training recipes, advanced media I/O capabilities, and\ntools for performing forced alignment, multi-channel speech enhancement, and\nreference-less speech assessment. For a selection of these features, through\nempirical studies, we demonstrate their efficacy and show that they achieve\ncompetitive or state-of-the-art performance.", "published": "2023-10-27 03:00:51", "link": "http://arxiv.org/abs/2310.17864v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improved Lossless Coding for Storage and Transmission of Multichannel\n  Immersive Audio", "abstract": "In this paper, techniques for improving multichannel lossless coding are\nexamined. A method is proposed for the simultaneous coding of two or more\ndifferent renderings (mixes) of the same content. The signal model uses both\npast samples of the upmix, and the current time samples of downmix samples to\npredict the upmix. Model parameters are optimized via a general linear solver,\nand the prediction residual is Rice coded. Additionally, the use of an SVD\nprojection prior to residual coding is proposed. A comparison is made against\nvarious baselines, including FLAC. The proposed methods show improved\ncompression ratios for the storage and transmission of immersive audio.", "published": "2023-10-27 20:14:00", "link": "http://arxiv.org/abs/2310.18461v1", "categories": ["eess.AS", "cs.MM"], "primary_category": "eess.AS"}
{"title": "Enabling Acoustic Audience Feedback in Large Virtual Events", "abstract": "The COVID-19 pandemic shifted many events in our daily lives into the virtual\ndomain. While virtual conference systems provide an alternative to physical\nmeetings, larger events require a muted audience to avoid an accumulation of\nbackground noise and distorted audio. However, performing artists strongly rely\non the feedback of their audience. We propose a concept for a virtual audience\nframework which supports all participants with the ambience of a real audience.\nAudience feedback is collected locally, allowing users to express enthusiasm or\ndiscontent by selecting means such as clapping, whistling, booing, and\nlaughter. This feedback is sent as abstract information to a virtual audience\nserver. We broadcast the combined virtual audience feedback information to all\nparticipants, which can be synthesized as a single acoustic feedback by the\nclient. The synthesis can be done by turning the collective audience feedback\ninto a prompt that is fed to state-of-the-art models such as AudioGen. This\nway, each user hears a single acoustic feedback sound of the entire virtual\nevent, without requiring to unmute or risk hearing distorted, unsynchronized\nfeedback.", "published": "2023-10-27 12:34:50", "link": "http://arxiv.org/abs/2310.18099v1", "categories": ["cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
