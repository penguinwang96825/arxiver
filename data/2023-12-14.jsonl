{"title": "Zebra: Extending Context Window with Layerwise Grouped Local-Global\n  Attention", "abstract": "This paper introduces a novel approach to enhance the capabilities of Large\nLanguage Models (LLMs) in processing and understanding extensive text\nsequences, a critical aspect in applications requiring deep comprehension and\nsynthesis of large volumes of information. Recognizing the inherent challenges\nin extending the context window for LLMs, primarily built on Transformer\narchitecture, we propose a new model architecture, referred to as Zebra. This\narchitecture efficiently manages the quadratic time and memory complexity\nissues associated with full attention in the Transformer by employing grouped\nlocal-global attention layers. Our model, akin to a zebra's alternating\nstripes, balances local and global attention layers, significantly reducing\ncomputational requirements and memory consumption. Comprehensive experiments,\nincluding pretraining from scratch, continuation of long context adaptation\ntraining, and long instruction tuning, are conducted to evaluate the Zebra's\nperformance. The results show that Zebra achieves comparable or superior\nperformance on both short and long sequence benchmarks, while also enhancing\ntraining and inference efficiency.", "published": "2023-12-14 02:45:31", "link": "http://arxiv.org/abs/2312.08618v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "JPIS: A Joint Model for Profile-based Intent Detection and Slot Filling\n  with Slot-to-Intent Attention", "abstract": "Profile-based intent detection and slot filling are important tasks aimed at\nreducing the ambiguity in user utterances by leveraging user-specific\nsupporting profile information. However, research in these two tasks has not\nbeen extensively explored. To fill this gap, we propose a joint model, namely\nJPIS, designed to enhance profile-based intent detection and slot filling. JPIS\nincorporates the supporting profile information into its encoder and introduces\na slot-to-intent attention mechanism to transfer slot information\nrepresentations to intent detection. Experimental results show that our JPIS\nsubstantially outperforms previous profile-based models, establishing a new\nstate-of-the-art performance in overall accuracy on the Chinese benchmark\ndataset ProSLU.", "published": "2023-12-14 08:30:38", "link": "http://arxiv.org/abs/2312.08737v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dissecting vocabulary biases datasets through statistical testing and\n  automated data augmentation for artifact mitigation in Natural Language\n  Inference", "abstract": "In recent years, the availability of large-scale annotated datasets, such as\nthe Stanford Natural Language Inference and the Multi-Genre Natural Language\nInference, coupled with the advent of pre-trained language models, has\nsignificantly contributed to the development of the natural language inference\ndomain. However, these crowdsourced annotated datasets often contain biases or\ndataset artifacts, leading to overestimated model performance and poor\ngeneralization. In this work, we focus on investigating dataset artifacts and\ndeveloping strategies to address these issues. Through the utilization of a\nnovel statistical testing procedure, we discover a significant association\nbetween vocabulary distribution and text entailment classes, emphasizing\nvocabulary as a notable source of biases. To mitigate these issues, we propose\nseveral automatic data augmentation strategies spanning character to word\nlevels. By fine-tuning the ELECTRA pre-trained language model, we compare the\nperformance of boosted models with augmented data against their baseline\ncounterparts. The experiments demonstrate that the proposed approaches\neffectively enhance model accuracy and reduce biases by up to 0.66% and 1.14%,\nrespectively.", "published": "2023-12-14 08:46:26", "link": "http://arxiv.org/abs/2312.08747v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PROPRES: Investigating the Projectivity of Presupposition with Various\n  Triggers and Environments", "abstract": "What makes a presupposition of an utterance -- information taken for granted\nby its speaker -- different from other pragmatic inferences such as an\nentailment is projectivity (e.g., the negative sentence the boy did not stop\nshedding tears presupposes the boy had shed tears before). The projectivity may\nvary depending on the combination of presupposition triggers and environments.\nHowever, prior natural language understanding studies fail to take it into\naccount as they either use no human baseline or include only negation as an\nentailment-canceling environment to evaluate models' performance. The current\nstudy attempts to reconcile these issues. We introduce a new dataset,\nprojectivity of presupposition (PROPRES, which includes 12k premise-hypothesis\npairs crossing six triggers involving some lexical variety with five\nenvironments. Our human evaluation reveals that humans exhibit variable\nprojectivity in some cases. However, the model evaluation shows that the\nbest-performed model, DeBERTa, does not fully capture it. Our findings suggest\nthat probing studies on pragmatic inferences should take extra care of the\nhuman judgment variability and the combination of linguistic items.", "published": "2023-12-14 09:07:57", "link": "http://arxiv.org/abs/2312.08755v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ComOM at VLSP 2023: A Dual-Stage Framework with BERTology and Unified\n  Multi-Task Instruction Tuning Model for Vietnamese Comparative Opinion Mining", "abstract": "The ComOM shared task aims to extract comparative opinions from product\nreviews in Vietnamese language. There are two sub-tasks, including (1)\nComparative Sentence Identification (CSI) and (2) Comparative Element\nExtraction (CEE). The first task is to identify whether the input is a\ncomparative review, and the purpose of the second task is to extract the\nquintuplets mentioned in the comparative review. To address this task, our team\nproposes a two-stage system based on fine-tuning a BERTology model for the CSI\ntask and unified multi-task instruction tuning for the CEE task. Besides, we\napply the simple data augmentation technique to increase the size of the\ndataset for training our model in the second stage. Experimental results show\nthat our approach outperforms the other competitors and has achieved the top\nscore on the official private test.", "published": "2023-12-14 14:44:59", "link": "http://arxiv.org/abs/2312.09000v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Topic Bias in Emotion Classification", "abstract": "Emotion corpora are typically sampled based on keyword/hashtag search or by\nasking study participants to generate textual instances. In any case, these\ncorpora are not uniform samples representing the entirety of a domain. We\nhypothesize that this practice of data acquisition leads to unrealistic\ncorrelations between overrepresented topics in these corpora that harm the\ngeneralizability of models. Such topic bias could lead to wrong predictions for\ninstances like \"I organized the service for my aunt's funeral.\" when funeral\nevents are over-represented for instances labeled with sadness, despite the\nemotion of pride being more appropriate here. In this paper, we study this\ntopic bias both from the data and the modeling perspective. We first label a\nset of emotion corpora automatically via topic modeling and show that emotions\nin fact correlate with specific topics. Further, we see that emotion\nclassifiers are confounded by such topics. Finally, we show that the\nestablished debiasing method of adversarial correction via gradient reversal\nmitigates the issue. Our work points out issues with existing emotion corpora\nand that more representative resources are required for fair evaluation of\nmodels predicting affective concepts from text.", "published": "2023-12-14 15:40:27", "link": "http://arxiv.org/abs/2312.09043v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Verifiable Text Generation with Evolving Memory and\n  Self-Reflection", "abstract": "Despite the remarkable ability of large language models (LLMs) in language\ncomprehension and generation, they often suffer from producing factually\nincorrect information, also known as hallucination. A promising solution to\nthis issue is verifiable text generation, which prompts LLMs to generate\ncontent with citations for accuracy verification. However, verifiable text\ngeneration is non-trivial due to the focus-shifting phenomenon, the intricate\nreasoning needed to align the claim with correct citations, and the dilemma\nbetween the precision and breadth of retrieved documents. In this paper, we\npresent VTG, an innovative framework for Verifiable Text Generation with\nevolving memory and self-reflection. VTG introduces evolving long short-term\nmemory to retain both valuable documents and recent documents. A two-tier\nverifier equipped with an evidence finder is proposed to rethink and reflect on\nthe relationship between the claim and citations. Furthermore, active retrieval\nand diverse query generation are utilized to enhance both the precision and\nbreadth of the retrieved documents. We conduct extensive experiments on five\ndatasets across three knowledge-intensive tasks and the results reveal that VTG\nsignificantly outperforms baselines.", "published": "2023-12-14 16:10:56", "link": "http://arxiv.org/abs/2312.09075v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Measurement in the Age of LLMs: An Application to Ideological Scaling", "abstract": "Much of social science is centered around terms like ``ideology'' or\n``power'', which generally elude precise definition, and whose contextual\nmeanings are trapped in surrounding language. This paper explores the use of\nlarge language models (LLMs) to flexibly navigate the conceptual clutter\ninherent to social scientific measurement tasks. We rely on LLMs' remarkable\nlinguistic fluency to elicit ideological scales of both legislators and text,\nwhich accord closely to established methods and our own judgement. A key aspect\nof our approach is that we elicit such scores directly, instructing the LLM to\nfurnish numeric scores itself. This approach affords a great deal of\nflexibility, which we showcase through a variety of different case studies. Our\nresults suggest that LLMs can be used to characterize highly subtle and diffuse\nmanifestations of political ideology in text.", "published": "2023-12-14 18:34:06", "link": "http://arxiv.org/abs/2312.09203v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mitigating Outlier Activations in Low-Precision Fine-Tuning of Language\n  Models", "abstract": "Low-precision fine-tuning of language models has gained prominence as a\ncost-effective and energy-efficient approach to deploying large-scale models in\nvarious applications. However, this approach is susceptible to the existence of\noutlier values in activation. The outlier values in the activation can\nnegatively affect the performance of fine-tuning language models in the\nlow-precision regime since they affect the scaling factor and thus make\nrepresenting smaller values harder. This paper investigates techniques for\nmitigating outlier activation in low-precision integer fine-tuning of the\nlanguage models. Our proposed novel approach enables us to represent the\noutlier activation values in 8-bit integers instead of floating-point (FP16)\nvalues. The benefit of using integers for outlier values is that it enables us\nto use operator tiling to avoid performing 16-bit integer matrix multiplication\nto address this problem effectively. We provide theoretical analysis and\nsupporting experiments to demonstrate the effectiveness of our approach in\nimproving the robustness and performance of low-precision fine-tuned language\nmodels.", "published": "2023-12-14 18:41:32", "link": "http://arxiv.org/abs/2312.09211v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak\n  Supervision", "abstract": "Widely used alignment techniques, such as reinforcement learning from human\nfeedback (RLHF), rely on the ability of humans to supervise model behavior -\nfor example, to evaluate whether a model faithfully followed instructions or\ngenerated safe outputs. However, future superhuman models will behave in\ncomplex ways too difficult for humans to reliably evaluate; humans will only be\nable to weakly supervise superhuman models. We study an analogy to this\nproblem: can weak model supervision elicit the full capabilities of a much\nstronger model? We test this using a range of pretrained language models in the\nGPT-4 family on natural language processing (NLP), chess, and reward modeling\ntasks. We find that when we naively finetune strong pretrained models on labels\ngenerated by a weak model, they consistently perform better than their weak\nsupervisors, a phenomenon we call weak-to-strong generalization. However, we\nare still far from recovering the full capabilities of strong models with naive\nfinetuning alone, suggesting that techniques like RLHF may scale poorly to\nsuperhuman models without further work. We find that simple methods can often\nsignificantly improve weak-to-strong generalization: for example, when\nfinetuning GPT-4 with a GPT-2-level supervisor and an auxiliary confidence\nloss, we can recover close to GPT-3.5-level performance on NLP tasks. Our\nresults suggest that it is feasible to make empirical progress today on a\nfundamental challenge of aligning superhuman models.", "published": "2023-12-14 23:07:33", "link": "http://arxiv.org/abs/2312.09390v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Arithmetics-Based Decomposition of Numeral Words -- Arithmetic\n  Conditions give the Unpacking Strategy", "abstract": "In this paper we present a novel numeral decomposer that is designed to\nrevert Hurford's Packing Strategy. The Packing Strategy is a model on how\nnumeral words are formed out of smaller numeral words by recursion. The\ndecomposer does not simply check decimal digits but it also works for numerals\nformed on base 20 or any other base or even combinations of different bases.\nAll assumptions that we use are justified with Hurford's Packing Strategy. The\ndecomposer reads through the numeral. When it finds a sub-numeral, it checks\narithmetic conditions to decide whether or not to unpack the sub-numeral. The\ngoal is to unpack those numerals that can sensibly be substituted by similar\nnumerals. E.g., in 'twenty-seven thousand and two hundred and six' it should\nunpack 'twenty-seven' and 'two hundred and six', as those could each be\nsensibly replaced by any numeral from 1 to 999. Our most used condition is: If\nS is a substitutable sub-numeral of a numeral N, then 2*value(S) < value(N). We\nhave tested the decomposer on numeral systems in 254 different natural\nlanguages. We also developed a reinforcement learning algorithm based on the\ndecomposer. Both algorithms' code and the results are open source on GitHub.", "published": "2023-12-14 17:45:50", "link": "http://arxiv.org/abs/2312.10097v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ZeroQuant(4+2): Redefining LLMs Quantization with a New FP6-Centric\n  Strategy for Diverse Generative Tasks", "abstract": "This study examines 4-bit quantization methods like GPTQ in large language\nmodels (LLMs), highlighting GPTQ's overfitting and limited enhancement in\nZero-Shot tasks. While prior works merely focusing on zero-shot measurement, we\nextend task scope to more generative categories such as code generation and\nabstractive summarization, in which we found that INT4 quantization can\nsignificantly underperform. However, simply shifting to higher precision\nformats like FP6 has been particularly challenging, thus overlooked, due to\npoor performance caused by the lack of sophisticated integration and system\nacceleration strategies on current AI hardware. Our results show that FP6, even\nwith a coarse-grain quantization scheme, performs robustly across various\nalgorithms and tasks, demonstrating its superiority in accuracy and\nversatility. Notably, with the FP6 quantization, \\codestar-15B model performs\ncomparably to its FP16 counterpart in code generation, and for smaller models\nlike the 406M it closely matches their baselines in summarization. Neither can\nbe achieved by INT4. To better accommodate various AI hardware and achieve the\nbest system performance, we propose a novel 4+2 design for FP6 to achieve\nsimilar latency to the state-of-the-art INT4 fine-grain quantization. With our\ndesign, FP6 can become a promising solution to the current 4-bit quantization\nmethods used in LLMs.", "published": "2023-12-14 01:06:37", "link": "http://arxiv.org/abs/2312.08583v2", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Unraveling Key Factors of Knowledge Distillation", "abstract": "Knowledge distillation, a technique for model compression and performance\nenhancement, has gained significant traction in Neural Machine Translation\n(NMT). However, existing research primarily focuses on empirical applications,\nand there is a lack of comprehensive understanding of how student model\ncapacity, data complexity, and decoding strategies collectively influence\ndistillation effectiveness. Addressing this gap, our study conducts an in-depth\ninvestigation into these factors, particularly focusing on their interplay in\nword-level and sequence-level distillation within NMT. Through extensive\nexperimentation across datasets like IWSLT13 En$\\rightarrow$Fr, IWSLT14\nEn$\\rightarrow$De, and others, we empirically validate hypotheses related to\nthe impact of these factors on knowledge distillation. Our research not only\nelucidates the significant influence of model capacity, data complexity, and\ndecoding strategies on distillation effectiveness but also introduces a novel,\noptimized distillation approach. This approach, when applied to the IWSLT14\nde$\\rightarrow$en translation task, achieves state-of-the-art performance,\ndemonstrating its practical efficacy in advancing the field of NMT.", "published": "2023-12-14 01:16:19", "link": "http://arxiv.org/abs/2312.08585v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Metacognition-Enhanced Few-Shot Prompting With Positive Reinforcement", "abstract": "Few-shot prompting elicits the remarkable abilities of large language models\nby equipping them with a few demonstration examples in the input. However, the\ntraditional method of providing large language models with all demonstration\ninput-output pairs at once may not effectively guide large language models to\nlearn the specific input-output mapping relationship. In this paper, inspired\nby the regulatory and supportive role of metacognition in students' learning,\nwe propose a novel metacognition-enhanced few-shot prompting, which guides\nlarge language models to reflect on their thought processes to comprehensively\nlearn the given demonstration examples. Furthermore, considering that positive\nreinforcement can improve students' learning motivation, we introduce positive\nreinforcement into our metacognition-enhanced few-shot prompting to promote the\nfew-shot learning of large language models by providing response-based positive\nfeedback. The experimental results on two real-world datasets show that our\nmetacognition-enhanced few-shot prompting with positive reinforcement surpasses\ntraditional few-shot prompting in classification accuracy and macro F1.", "published": "2023-12-14 03:49:52", "link": "http://arxiv.org/abs/2312.08642v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TigerBot: An Open Multilingual Multitask LLM", "abstract": "We release and introduce the TigerBot family of large language models (LLMs),\nconsisting of base and chat models, sized from 7, 13, 70 and 180 billion\nparameters. We develop our models embarking from Llama-2 and BLOOM, and push\nthe boundary further in data, training algorithm, infrastructure, and\napplication tools. Our models yield meaningful performance gain over SOTA\nopen-source models, e.g., Llama-2, specifically 6% gain in English and 20% gain\nin Chinese. TigerBot model family also achieves leading performance in major\nacademic and industrial benchmarks and leaderboards. We believe that TigerBot\nrepresents just a snapshot of lightning-fast progression in LLM open-source\ncommunity. Therefore, we are thrilled to give back by publicly releasing our\nmodels and reporting our approach behind, with additional emphases on building\nSOTA LLMs in a democratized way and making LLMs of use in real-world\napplications.", "published": "2023-12-14 07:05:42", "link": "http://arxiv.org/abs/2312.08688v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Comparative Analysis of Fine-Tuned LLMs and Few-Shot Learning of LLMs\n  for Financial Sentiment Analysis", "abstract": "Financial sentiment analysis plays a crucial role in uncovering latent\npatterns and detecting emerging trends, enabling individuals to make\nwell-informed decisions that may yield substantial advantages within the\nconstantly changing realm of finance. Recently, Large Language Models (LLMs)\nhave demonstrated their effectiveness in diverse domains, showcasing remarkable\ncapabilities even in zero-shot and few-shot in-context learning for various\nNatural Language Processing (NLP) tasks. Nevertheless, their potential and\napplicability in the context of financial sentiment analysis have not been\nthoroughly explored yet. To bridge this gap, we employ two approaches:\nin-context learning (with a focus on gpt-3.5-turbo model) and fine-tuning LLMs\non a finance-domain dataset. Given the computational costs associated with\nfine-tuning LLMs with large parameter sizes, our focus lies on smaller LLMs,\nspanning from 250M to 3B parameters for fine-tuning. We then compare the\nperformances with state-of-the-art results to evaluate their effectiveness in\nthe finance-domain. Our results demonstrate that fine-tuned smaller LLMs can\nachieve comparable performance to state-of-the-art fine-tuned LLMs, even with\nmodels having fewer parameters and a smaller training dataset. Additionally,\nthe zero-shot and one-shot performance of LLMs produces comparable results with\nfine-tuned smaller LLMs and state-of-the-art outcomes. Furthermore, our\nanalysis demonstrates that there is no observed enhancement in performance for\nfinance-domain sentiment analysis when the number of shots for in-context\nlearning is increased.", "published": "2023-12-14 08:13:28", "link": "http://arxiv.org/abs/2312.08725v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Labels Need Prompts Too: Mask Matching for Natural Language\n  Understanding Tasks", "abstract": "Textual label names (descriptions) are typically semantically rich in many\nnatural language understanding (NLU) tasks. In this paper, we incorporate the\nprompting methodology, which is widely used to enrich model input, into the\nlabel side for the first time. Specifically, we propose a Mask Matching method,\nwhich equips an input with a prompt and its label with another, and then makes\npredictions by matching their mask representations. We evaluate our method\nextensively on 8 NLU tasks with 14 datasets. The experimental results show that\nMask Matching significantly outperforms its counterparts of fine-tuning and\nconventional prompt-tuning, setting up state-of-the-art performances in several\ndatasets. Mask Matching is particularly good at handling NLU tasks with large\nlabel counts and informative label names. As pioneering efforts that\ninvestigate the label-side prompt, we also discuss open issues for future\nstudy.", "published": "2023-12-14 08:14:13", "link": "http://arxiv.org/abs/2312.08726v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Cross-modal Alignment with Synthetic Pairs for Text-only Image\n  Captioning", "abstract": "Although image captioning models have made significant advancements in recent\nyears, the majority of them heavily depend on high-quality datasets containing\npaired images and texts which are costly to acquire. Previous works leverage\nthe CLIP's cross-modal association ability for image captioning, relying solely\non textual information under unsupervised settings. However, not only does a\nmodality gap exist between CLIP text and image features, but a discrepancy also\narises between training and inference due to the unavailability of real-world\nimages, which hinders the cross-modal alignment in text-only captioning. This\npaper proposes a novel method to address these issues by incorporating\nsynthetic image-text pairs. A pre-trained text-to-image model is deployed to\nobtain images that correspond to textual data, and the pseudo features of\ngenerated images are optimized toward the real ones in the CLIP embedding\nspace. Furthermore, textual information is gathered to represent image\nfeatures, resulting in the image features with various semantics and the\nbridged modality gap. To unify training and inference, synthetic image features\nwould serve as the training prefix for the language decoder, while real images\nare used for inference. Additionally, salient objects in images are detected as\nassistance to enhance the learning of modality alignment. Experimental results\ndemonstrate that our method obtains the state-of-the-art performance on\nbenchmark datasets.", "published": "2023-12-14 12:39:29", "link": "http://arxiv.org/abs/2312.08865v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Fewer is More: Boosting LLM Reasoning with Reinforced Context Pruning", "abstract": "Large Language Models (LLMs) have shown impressive capabilities, yet they\nstill struggle with math reasoning. In this work, we propose CoT-Influx, a\nnovel approach that pushes the boundary of few-shot Chain-of-Thoughts (CoT)\nlearning to improve LLM mathematical reasoning. Motivated by the observation\nthat adding more concise CoT examples in the prompt can improve LLM reasoning\nperformance, CoT-Influx employs a coarse-to-fine pruner to maximize the input\nof effective and concise CoT examples. The pruner first selects as many crucial\nCoT examples as possible and then prunes unimportant tokens to fit the context\nwindow. A math reasoning dataset with diverse difficulty levels and reasoning\nsteps is used to train the pruner, along with a math-specialized reinforcement\nlearning approach. As a result, by enabling more CoT examples with double the\ncontext window size in tokens, CoT-Influx significantly outperforms various\nprompting baselines across various LLMs (LLaMA2-7B, 13B, 70B) and 5 math\ndatasets, achieving up to 4.55% absolute improvements. Remarkably, without any\nfine-tuning, LLaMA2-70B with CoT-Influx surpasses GPT-3.5 and a wide range of\nlarger LLMs (PaLM, Minerva 540B, etc.) on the GSM8K. CoT-Influx serves as a\nplug-and-play module for LLMs and is compatible with most existing reasoning\nprompting techniques, such as self-consistency and self-verification.", "published": "2023-12-14 13:03:13", "link": "http://arxiv.org/abs/2312.08901v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Using eye tracking to investigate what native Chinese speakers notice\n  about linguistic landscape images", "abstract": "Linguistic landscape is an important field in sociolinguistic research. Eye\ntracking technology is a common technology in psychological research. There are\nfew cases of using eye movement to study linguistic landscape. This paper uses\neye tracking technology to study the actual fixation of the linguistic\nlandscape and finds that in the two dimensions of fixation time and fixation\ntimes, the fixation of native Chinese speakers to the linguistic landscape is\nhigher than that of the general landscape. This paper argues that this\nphenomenon is due to the higher information density of linguistic landscapes.\nAt the same time, the article also discusses other possible reasons for this\nphenomenon.", "published": "2023-12-14 13:11:35", "link": "http://arxiv.org/abs/2312.08906v4", "categories": ["cs.CL", "q-bio.QM", "J.4"], "primary_category": "cs.CL"}
{"title": "Modeling Complex Mathematical Reasoning via Large Language Model based\n  MathAgent", "abstract": "Large language models (LLMs) face challenges in solving complex mathematical\nproblems that require comprehensive capacities to parse the statements,\nassociate domain knowledge, perform compound logical reasoning, and integrate\nthe intermediate rationales. Tackling all these problems once could be arduous\nfor LLMs, thus leading to confusion in generation. In this work, we explore the\npotential of enhancing LLMs with agents by meticulous decomposition and\nmodeling of mathematical reasoning process. Specifically, we propose a formal\ndescription of the mathematical solving and extend LLMs with an agent-based\nzero-shot framework named\n$\\bf{P}$lanner-$\\bf{R}$easoner-$\\bf{E}$xecutor-$\\bf{R}$eflector (PRER). We\nfurther provide and implement two MathAgents that define the logical forms and\ninherent relations via a pool of actions in different grains and orientations:\nMathAgent-M adapts its actions to LLMs, while MathAgent-H aligns with\nhumankind. Experiments on miniF2F and MATH have demonstrated the effectiveness\nof PRER and proposed MathAgents, achieving an increase of\n$12.3\\%$($53.9\\%\\xrightarrow{}66.2\\%$) on the MiniF2F, $9.2\\%$\n($49.8\\%\\xrightarrow{}59.0\\%$) on MATH, and\n$13.2\\%$($23.2\\%\\xrightarrow{}35.4\\%$) for level-5 problems of MATH against\nGPT-4. Further analytical results provide more insightful perspectives on\nexploiting the behaviors of LLMs as agents.", "published": "2023-12-14 13:33:50", "link": "http://arxiv.org/abs/2312.08926v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Detecting value-expressive text posts in Russian social media", "abstract": "Basic values are concepts or beliefs which pertain to desirable end-states\nand transcend specific situations. Studying personal values in social media can\nilluminate how and why societal values evolve especially when the stimuli-based\nmethods, such as surveys, are inefficient, for instance, in hard-to-reach\npopulations. On the other hand, user-generated content is driven by the massive\nuse of stereotyped, culturally defined speech constructions rather than\nauthentic expressions of personal values. We aimed to find a model that can\naccurately detect value-expressive posts in Russian social media VKontakte. A\ntraining dataset of 5,035 posts was annotated by three experts, 304\ncrowd-workers and ChatGPT. Crowd-workers and experts showed only moderate\nagreement in categorizing posts. ChatGPT was more consistent but struggled with\nspam detection. We applied an ensemble of human- and AI-assisted annotation\ninvolving active learning approach, subsequently trained several LLMs and\nselected a model based on embeddings from pre-trained fine-tuned rubert-tiny2,\nand reached a high quality of value detection with F1 = 0.75 (F1-macro = 0.80).\nThis model provides a crucial step to a study of values within and between\nRussian social media users.", "published": "2023-12-14 14:18:27", "link": "http://arxiv.org/abs/2312.08968v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TAP4LLM: Table Provider on Sampling, Augmenting, and Packing\n  Semi-structured Data for Large Language Model Reasoning", "abstract": "Table reasoning tasks have shown remarkable progress with the development of\nlarge language models (LLMs), which involve interpreting and drawing\nconclusions from tabular data based on natural language (NL) questions.\nExisting solutions mainly tested on smaller tables face scalability issues and\nstruggle with complex queries due to incomplete or dispersed data across\ndifferent table sections. To alleviate these challenges, we propose TAP4LLM as\na versatile pre-processor suite for leveraging LLMs in table-based tasks\neffectively. It covers several distinct components: (1) table sampling to\ndecompose large tables into manageable sub-tables based on query semantics, (2)\ntable augmentation to enhance tables with additional knowledge from external\nsources or models, and (3) table packing & serialization to convert tables into\nvarious formats suitable for LLMs' understanding. In each module, we design and\ncompare several common methods under various usage scenarios, aiming to shed\nlight on the best practices for leveraging LLMs for table-reasoning tasks. Our\nexperiments show that our method improves LLMs' reasoning capabilities in\nvarious tabular tasks and enhances the interaction between LLMs and tabular\ndata by employing effective pre-processing.", "published": "2023-12-14 15:37:04", "link": "http://arxiv.org/abs/2312.09039v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TinyGSM: achieving >80% on GSM8k with small language models", "abstract": "Small-scale models offer various computational advantages, and yet to which\nextent size is critical for problem-solving abilities remains an open question.\nSpecifically for solving grade school math, the smallest model size so far\nrequired to break the 80\\% barrier on the GSM8K benchmark remains to be 34B.\nOur work studies how high-quality datasets may be the key for small language\nmodels to acquire mathematical reasoning. We introduce \\texttt{TinyGSM}, a\nsynthetic dataset of 12.3M grade school math problems paired with Python\nsolutions, generated fully by GPT-3.5. After finetuning on \\texttt{TinyGSM}, we\nfind that a duo of a 1.3B generation model and a 1.3B verifier model can\nachieve 81.5\\% accuracy, outperforming existing models that are orders of\nmagnitude larger. This also rivals the performance of the GPT-3.5 ``teacher''\nmodel (77.4\\%), from which our model's training data is generated. Our approach\nis simple and has two key components: 1) the high-quality dataset\n\\texttt{TinyGSM}, 2) the use of a verifier, which selects the final outputs\nfrom multiple candidate generations.", "published": "2023-12-14 18:58:28", "link": "http://arxiv.org/abs/2312.09241v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Arabic Mini-ClimateGPT : A Climate Change and Sustainability Tailored\n  Arabic LLM", "abstract": "Climate change is one of the most significant challenges we face together as\na society. Creating awareness and educating policy makers the wide-ranging\nimpact of climate change is an essential step towards a sustainable future.\nRecently, Large Language Models (LLMs) like ChatGPT and Bard have shown\nimpressive conversational abilities and excel in a wide variety of NLP tasks.\nWhile these models are close-source, recently alternative open-source LLMs such\nas Stanford Alpaca and Vicuna have shown promising results. However, these\nopen-source models are not specifically tailored for climate related domain\nspecific information and also struggle to generate meaningful responses in\nother languages such as, Arabic. To this end, we propose a light-weight Arabic\nMini-ClimateGPT that is built on an open-source LLM and is specifically\nfine-tuned on a conversational-style instruction tuning curated Arabic dataset\nClima500-Instruct with over 500k instructions about climate change and\nsustainability. Further, our model also utilizes a vector embedding based\nretrieval mechanism during inference. We validate our proposed model through\nquantitative and qualitative evaluations on climate-related queries. Our model\nsurpasses the baseline LLM in 88.3% of cases during ChatGPT-based evaluation.\nFurthermore, our human expert evaluation reveals an 81.6% preference for our\nmodel's responses over multiple popular open-source models. Our open-source\ndemos, code-base and models are available here\nhttps://github.com/mbzuai-oryx/ClimateGPT.", "published": "2023-12-14 22:04:07", "link": "http://arxiv.org/abs/2312.09366v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Deep Anomaly Detection in Text", "abstract": "Deep anomaly detection methods have become increasingly popular in recent\nyears, with methods like Stacked Autoencoders, Variational Autoencoders, and\nGenerative Adversarial Networks greatly improving the state-of-the-art. Other\nmethods rely on augmenting classical models (such as the One-Class Support\nVector Machine), by learning an appropriate kernel function using Neural\nNetworks. Recent developments in representation learning by self-supervision\nare proving to be very beneficial in the context of anomaly detection. Inspired\nby the advancements in anomaly detection using self-supervised learning in the\nfield of computer vision, this thesis aims to develop a method for detecting\nanomalies by exploiting pretext tasks tailored for text corpora. This approach\ngreatly improves the state-of-the-art on two datasets, 20Newsgroups, and AG\nNews, for both semi-supervised and unsupervised anomaly detection, thus proving\nthe potential for self-supervised anomaly detectors in the field of natural\nlanguage processing.", "published": "2023-12-14 22:04:43", "link": "http://arxiv.org/abs/2401.02971v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Identifying Planetary Names in Astronomy Papers: A Multi-Step Approach", "abstract": "The automatic identification of planetary feature names in astronomy\npublications presents numerous challenges. These features include craters,\ndefined as roughly circular depressions resulting from impact or volcanic\nactivity; dorsas, which are elongate raised structures or wrinkle ridges; and\nlacus, small irregular patches of dark, smooth material on the Moon, referred\nto as \"lake\" (Planetary Names Working Group, n.d.). Many feature names overlap\nwith places or people's names that they are named after, for example, Syria,\nTempe, Einstein, and Sagan, to name a few (U.S. Geological Survey, n.d.). Some\nfeature names have been used in many contexts, for instance, Apollo, which can\nrefer to mission, program, sample, astronaut, seismic, seismometers, core, era,\ndata, collection, instrument, and station, in addition to the crater on the\nMoon. Some feature names can appear in the text as adjectives, like the lunar\ncraters Black, Green, and White. Some feature names in other contexts serve as\ndirections, like craters West and South on the Moon. Additionally, some\nfeatures share identical names across different celestial bodies, requiring\ndisambiguation, such as the Adams crater, which exists on both the Moon and\nMars. We present a multi-step pipeline combining rule-based filtering,\nstatistical relevance analysis, part-of-speech (POS) tagging, named entity\nrecognition (NER) model, hybrid keyword harvesting, knowledge graph (KG)\nmatching, and inference with a locally installed large language model (LLM) to\nreliably identify planetary names despite these challenges. When evaluated on a\ndataset of astronomy papers from the Astrophysics Data System (ADS), this\nmethodology achieves an F1-score over 0.97 in disambiguating planetary feature\nnames.", "published": "2023-12-14 00:50:14", "link": "http://arxiv.org/abs/2312.08579v2", "categories": ["cs.CL", "astro-ph.IM", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SEF-VC: Speaker Embedding Free Zero-Shot Voice Conversion with Cross\n  Attention", "abstract": "Zero-shot voice conversion (VC) aims to transfer the source speaker timbre to\narbitrary unseen target speaker timbre, while keeping the linguistic content\nunchanged. Although the voice of generated speech can be controlled by\nproviding the speaker embedding of the target speaker, the speaker similarity\nstill lags behind the ground truth recordings. In this paper, we propose\nSEF-VC, a speaker embedding free voice conversion model, which is designed to\nlearn and incorporate speaker timbre from reference speech via a powerful\nposition-agnostic cross-attention mechanism, and then reconstruct waveform from\nHuBERT semantic tokens in a non-autoregressive manner. The concise design of\nSEF-VC enhances its training stability and voice conversion performance.\nObjective and subjective evaluations demonstrate the superiority of SEF-VC to\ngenerate high-quality speech with better similarity to target reference than\nstrong zero-shot VC baselines, even for very short reference speeches.", "published": "2023-12-14 06:26:55", "link": "http://arxiv.org/abs/2312.08676v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Forbidden Facts: An Investigation of Competing Objectives in Llama-2", "abstract": "LLMs often face competing pressures (for example helpfulness vs.\nharmlessness). To understand how models resolve such conflicts, we study\nLlama-2-chat models on the forbidden fact task. Specifically, we instruct\nLlama-2 to truthfully complete a factual recall statement while forbidding it\nfrom saying the correct answer. This often makes the model give incorrect\nanswers. We decompose Llama-2 into 1000+ components, and rank each one with\nrespect to how useful it is for forbidding the correct answer. We find that in\naggregate, around 35 components are enough to reliably implement the full\nsuppression behavior. However, these components are fairly heterogeneous and\nmany operate using faulty heuristics. We discover that one of these heuristics\ncan be exploited via a manually designed adversarial attack which we call The\nCalifornia Attack. Our results highlight some roadblocks standing in the way of\nbeing able to successfully interpret advanced ML systems. Project website\navailable at https://forbiddenfacts.github.io .", "published": "2023-12-14 10:27:15", "link": "http://arxiv.org/abs/2312.08793v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Evaluating Large Language Models for Health-related Queries with\n  Presuppositions", "abstract": "As corporations rush to integrate large language models (LLMs) to their\nsearch offerings, it is critical that they provide factually accurate\ninformation that is robust to any presuppositions that a user may express. In\nthis work, we introduce UPHILL, a dataset consisting of health-related queries\nwith varying degrees of presuppositions. Using UPHILL, we evaluate the factual\naccuracy and consistency of InstructGPT, ChatGPT, and BingChat models. We find\nthat while model responses rarely disagree with true health claims (posed as\nquestions), they often fail to challenge false claims: responses from\nInstructGPT agree with 32% of the false claims, ChatGPT 26% and BingChat 23%.\nAs we increase the extent of presupposition in input queries, the responses\nfrom InstructGPT and ChatGPT agree with the claim considerably more often,\nregardless of its veracity. Responses from BingChat, which rely on retrieved\nwebpages, are not as susceptible. Given the moderate factual accuracy, and the\ninability of models to consistently correct false assumptions, our work calls\nfor a careful assessment of current LLMs for use in high-stakes scenarios.", "published": "2023-12-14 10:35:13", "link": "http://arxiv.org/abs/2312.08800v3", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TiMix: Text-aware Image Mixing for Effective Vision-Language\n  Pre-training", "abstract": "Self-supervised Multi-modal Contrastive Learning (SMCL) remarkably advances\nmodern Vision-Language Pre-training (VLP) models by aligning visual and\nlinguistic modalities. Due to noises in web-harvested text-image pairs,\nhowever, scaling up training data volume in SMCL presents considerable\nobstacles in terms of computational cost and data inefficiency. To improve data\nefficiency in VLP, we propose Text-aware Image Mixing (TiMix), which integrates\nmix-based data augmentation techniques into SMCL, yielding significant\nperformance improvements without significantly increasing computational\noverhead. We provide a theoretical analysis of TiMixfrom a mutual information\n(MI) perspective, showing that mixed data samples for cross-modal contrastive\nlearning implicitly serve as a regularizer for the contrastive loss. The\nexperimental results demonstrate that TiMix exhibits a comparable performance\non downstream tasks, even with a reduced amount of training data and shorter\ntraining time, when benchmarked against existing methods. This work empirically\nand theoretically demonstrates the potential of data mixing for data-efficient\nand computationally viable VLP, benefiting broader VLP model adoption in\npractical scenarios.", "published": "2023-12-14 12:02:24", "link": "http://arxiv.org/abs/2312.08846v4", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human\n  Annotations", "abstract": "In this paper, we present an innovative process-oriented math process reward\nmodel called \\textbf{Math-Shepherd}, which assigns a reward score to each step\nof math problem solutions. The training of Math-Shepherd is achieved using\nautomatically constructed process-wise supervision data, breaking the\nbottleneck of heavy reliance on manual annotation in existing work. We explore\nthe effectiveness of Math-Shepherd in two scenarios: 1) \\textit{Verification}:\nMath-Shepherd is utilized for reranking multiple outputs generated by Large\nLanguage Models (LLMs); 2) \\textit{Reinforcement Learning}: Math-Shepherd is\nemployed to reinforce LLMs with step-by-step Proximal Policy Optimization\n(PPO). With Math-Shepherd, a series of open-source LLMs demonstrates\nexceptional performance. For instance, the step-by-step PPO with Math-Shepherd\nsignificantly improves the accuracy of Mistral-7B (77.9\\%$\\to$84.1\\% on GSM8K\nand 28.6\\%$\\to$33.0\\% on MATH). The accuracy can be further enhanced to 89.1\\%\nand 43.5\\% on GSM8K and MATH with the verification of Math-Shepherd,\nrespectively. We believe that automatic process supervision holds significant\npotential for the future evolution of LLMs.", "published": "2023-12-14 13:41:54", "link": "http://arxiv.org/abs/2312.08935v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "FrameFinder: Explorative Multi-Perspective Framing Extraction from News\n  Headlines", "abstract": "Revealing the framing of news articles is an important yet neglected task in\ninformation seeking and retrieval. In the present work, we present FrameFinder,\nan open tool for extracting and analyzing frames in textual data. FrameFinder\nvisually represents the frames of text from three perspectives, i.e., (i) frame\nlabels, (ii) frame dimensions, and (iii) frame structure. By analyzing the\nwell-established gun violence frame corpus, we demonstrate the merits of our\nproposed solution to support social science research and call for subsequent\nintegration into information interactions.", "published": "2023-12-14 14:41:37", "link": "http://arxiv.org/abs/2312.08995v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "STaR: Distilling Speech Temporal Relation for Lightweight Speech\n  Self-Supervised Learning Models", "abstract": "Albeit great performance of Transformer-based speech selfsupervised learning\n(SSL) models, their large parameter size and computational cost make them\nunfavorable to utilize. In this study, we propose to compress the speech SSL\nmodels by distilling speech temporal relation (STaR). Unlike previous works\nthat directly match the representation for each speech frame, STaR distillation\ntransfers temporal relation between speech frames, which is more suitable for\nlightweight student with limited capacity. We explore three STaR distillation\nobjectives and select the best combination as the final STaR loss. Our model\ndistilled from HuBERT BASE achieves an overall score of 79.8 on SUPERB\nbenchmark, the best performance among models with up to 27 million parameters.\nWe show that our method is applicable across different speech SSL models and\nmaintains robust performance with further reduced parameters.", "published": "2023-12-14 15:37:37", "link": "http://arxiv.org/abs/2312.09040v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Holodeck: Language Guided Generation of 3D Embodied AI Environments", "abstract": "3D simulated environments play a critical role in Embodied AI, but their\ncreation requires expertise and extensive manual effort, restricting their\ndiversity and scope. To mitigate this limitation, we present Holodeck, a system\nthat generates 3D environments to match a user-supplied prompt fully\nautomatedly. Holodeck can generate diverse scenes, e.g., arcades, spas, and\nmuseums, adjust the designs for styles, and can capture the semantics of\ncomplex queries such as \"apartment for a researcher with a cat\" and \"office of\na professor who is a fan of Star Wars\". Holodeck leverages a large language\nmodel (i.e., GPT-4) for common sense knowledge about what the scene might look\nlike and uses a large collection of 3D assets from Objaverse to populate the\nscene with diverse objects. To address the challenge of positioning objects\ncorrectly, we prompt GPT-4 to generate spatial relational constraints between\nobjects and then optimize the layout to satisfy those constraints. Our\nlarge-scale human evaluation shows that annotators prefer Holodeck over\nmanually designed procedural baselines in residential scenes and that Holodeck\ncan produce high-quality outputs for diverse scene types. We also demonstrate\nan exciting application of Holodeck in Embodied AI, training agents to navigate\nin novel scenes like music rooms and daycares without human-constructed data,\nwhich is a significant step forward in developing general-purpose embodied\nagents.", "published": "2023-12-14 16:04:14", "link": "http://arxiv.org/abs/2312.09067v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "primary_category": "cs.CV"}
{"title": "Language Modeling on a SpiNNaker 2 Neuromorphic Chip", "abstract": "As large language models continue to scale in size rapidly, so too does the\ncomputational power required to run them. Event-based networks on neuromorphic\ndevices offer a potential way to reduce energy consumption for inference\nsignificantly. However, to date, most event-based networks that can run on\nneuromorphic hardware, including spiking neural networks (SNNs), have not\nachieved task performance even on par with LSTM models for language modeling.\nAs a result, language modeling on neuromorphic devices has seemed a distant\nprospect. In this work, we demonstrate the first-ever implementation of a\nlanguage model on a neuromorphic device - specifically the SpiNNaker 2 chip -\nbased on a recently published event-based architecture called the EGRU.\nSpiNNaker 2 is a many-core neuromorphic chip designed for large-scale\nasynchronous processing, while the EGRU is architected to leverage such\nhardware efficiently while maintaining competitive task performance. This\nimplementation marks the first time a neuromorphic language model matches\nLSTMs, setting the stage for taking task performance to the level of large\nlanguage models. We also demonstrate results on a gesture recognition task\nbased on inputs from a DVS camera. Overall, our results showcase the\nfeasibility of this neuro-inspired neural network in hardware, highlighting\nsignificant gains versus conventional hardware in energy efficiency for the\ncommon use case of single batch inference.", "published": "2023-12-14 16:16:35", "link": "http://arxiv.org/abs/2312.09084v3", "categories": ["cs.NE", "cs.CL", "cs.ET", "cs.LG"], "primary_category": "cs.NE"}
{"title": "The Earth is Flat because...: Investigating LLMs' Belief towards\n  Misinformation via Persuasive Conversation", "abstract": "Large language models (LLMs) encapsulate vast amounts of knowledge but still\nremain vulnerable to external misinformation. Existing research mainly studied\nthis susceptibility behavior in a single-turn setting. However, belief can\nchange during a multi-turn conversation, especially a persuasive one.\nTherefore, in this study, we delve into LLMs' susceptibility to persuasive\nconversations, particularly on factual questions that they can answer\ncorrectly. We first curate the Farm (i.e., Fact to Misinform) dataset, which\ncontains factual questions paired with systematically generated persuasive\nmisinformation. Then, we develop a testing framework to track LLMs' belief\nchanges in a persuasive dialogue. Through extensive experiments, we find that\nLLMs' correct beliefs on factual knowledge can be easily manipulated by various\npersuasive strategies.", "published": "2023-12-14 16:16:50", "link": "http://arxiv.org/abs/2312.09085v5", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Successor Heads: Recurring, Interpretable Attention Heads In The Wild", "abstract": "In this work we present successor heads: attention heads that increment\ntokens with a natural ordering, such as numbers, months, and days. For example,\nsuccessor heads increment 'Monday' into 'Tuesday'. We explain the successor\nhead behavior with an approach rooted in mechanistic interpretability, the\nfield that aims to explain how models complete tasks in human-understandable\nterms. Existing research in this area has found interpretable language model\ncomponents in small toy models. However, results in toy models have not yet led\nto insights that explain the internals of frontier models and little is\ncurrently understood about the internal operations of large language models. In\nthis paper, we analyze the behavior of successor heads in large language models\n(LLMs) and find that they implement abstract representations that are common to\ndifferent architectures. They form in LLMs with as few as 31 million\nparameters, and at least as many as 12 billion parameters, such as GPT-2,\nPythia, and Llama-2. We find a set of 'mod-10 features' that underlie how\nsuccessor heads increment in LLMs across different architectures and sizes. We\nperform vector arithmetic with these features to edit head behavior and provide\ninsights into numeric representations within LLMs. Additionally, we study the\nbehavior of successor heads on natural language data, identifying interpretable\npolysemanticity in a Pythia successor head.", "published": "2023-12-14 18:55:47", "link": "http://arxiv.org/abs/2312.09230v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Auto MC-Reward: Automated Dense Reward Design with Large Language Models\n  for Minecraft", "abstract": "Many reinforcement learning environments (e.g., Minecraft) provide only\nsparse rewards that indicate task completion or failure with binary values. The\nchallenge in exploration efficiency in such environments makes it difficult for\nreinforcement-learning-based agents to learn complex tasks. To address this,\nthis paper introduces an advanced learning system, named Auto MC-Reward, that\nleverages Large Language Models (LLMs) to automatically design dense reward\nfunctions, thereby enhancing the learning efficiency. Auto MC-Reward consists\nof three important components: Reward Designer, Reward Critic, and Trajectory\nAnalyzer. Given the environment information and task descriptions, the Reward\nDesigner first design the reward function by coding an executable Python\nfunction with predefined observation inputs. Then, our Reward Critic will be\nresponsible for verifying the code, checking whether the code is\nself-consistent and free of syntax and semantic errors. Further, the Trajectory\nAnalyzer summarizes possible failure causes and provides refinement suggestions\naccording to collected trajectories. In the next round, Reward Designer will\nfurther refine and iterate the dense reward function based on feedback.\nExperiments demonstrate a significant improvement in the success rate and\nlearning efficiency of our agents in complex tasks in Minecraft, such as\nobtaining diamond with the efficient ability to avoid lava, and efficiently\nexplore trees and animals that are sparse in the plains biome.", "published": "2023-12-14 18:58:12", "link": "http://arxiv.org/abs/2312.09238v2", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Acoustic models of Brazilian Portuguese Speech based on Neural\n  Transformers", "abstract": "An acoustic model, trained on a significant amount of unlabeled data,\nconsists of a self-supervised learned speech representation useful for solving\ndownstream tasks, perhaps after a fine-tuning of the model in the respective\ndownstream task. In this work, we build an acoustic model of Brazilian\nPortuguese Speech through a Transformer neural network. This model was\npretrained on more than $800$ hours of Brazilian Portuguese Speech, using a\ncombination of pretraining techniques. Using a labeled dataset collected for\nthe detection of respiratory insufficiency in Brazilian Portuguese speakers, we\nfine-tune the pretrained Transformer neural network on the following tasks:\nrespiratory insufficiency detection, gender recognition and age group\nclassification. We compare the performance of pretrained Transformers on these\ntasks with that of Transformers without previous pretraining, noting a\nsignificant improvement. In particular, the performance of respiratory\ninsufficiency detection obtains the best reported results so far, indicating\nthis kind of acoustic model as a promising tool for speech-as-biomarker\napproach. Moreover, the performance of gender recognition is comparable to the\nstate of the art models in English.", "published": "2023-12-14 14:16:40", "link": "http://arxiv.org/abs/2312.09265v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Weight subcloning: direct initialization of transformers using larger\n  pretrained ones", "abstract": "Training large transformer models from scratch for a target task requires\nlots of data and is computationally demanding. The usual practice of transfer\nlearning overcomes this challenge by initializing the model with weights of a\npretrained model of the same size and specification to increase the convergence\nand training speed. However, what if no pretrained model of the required size\nis available? In this paper, we introduce a simple yet effective technique to\ntransfer the knowledge of a pretrained model to smaller variants. Our approach\ncalled weight subcloning expedites the training of scaled-down transformers by\ninitializing their weights from larger pretrained models.\n  Weight subcloning involves an operation on the pretrained model to obtain the\nequivalent initialized scaled-down model. It consists of two key steps: first,\nwe introduce neuron importance ranking to decrease the embedding dimension per\nlayer in the pretrained model. Then, we remove blocks from the transformer\nmodel to match the number of layers in the scaled-down network. The result is a\nnetwork ready to undergo training, which gains significant improvements in\ntraining speed compared to random initialization. For instance, we achieve 4x\nfaster training for vision transformers in image classification and language\nmodels designed for next token prediction.", "published": "2023-12-14 19:08:56", "link": "http://arxiv.org/abs/2312.09299v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Self-Evaluation Improves Selective Generation in Large Language Models", "abstract": "Safe deployment of large language models (LLMs) may benefit from a reliable\nmethod for assessing their generated content to determine when to abstain or to\nselectively generate. While likelihood-based metrics such as perplexity are\nwidely employed, recent research has demonstrated the limitations of using\nsequence-level probability estimates given by LLMs as reliable indicators of\ngeneration quality. Conversely, LLMs have demonstrated strong calibration at\nthe token level, particularly when it comes to choosing correct answers in\nmultiple-choice questions or evaluating true/false statements. In this work, we\nreformulate open-ended generation tasks into token-level prediction tasks, and\nleverage LLMs' superior calibration at the token level. We instruct an LLM to\nself-evaluate its answers, employing either a multi-way comparison or a\npoint-wise evaluation approach, with the option to include a ``None of the\nabove'' option to express the model's uncertainty explicitly. We benchmark a\nrange of scoring methods based on self-evaluation and evaluate their\nperformance in selective generation using TruthfulQA and TL;DR. Through\nexperiments with PaLM-2 and GPT-3, we demonstrate that self-evaluation based\nscores not only improve accuracy, but also correlate better with the overall\nquality of generated content.", "published": "2023-12-14 19:09:22", "link": "http://arxiv.org/abs/2312.09300v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Well-calibrated Confidence Measures for Multi-label Text Classification\n  with a Large Number of Labels", "abstract": "We extend our previous work on Inductive Conformal Prediction (ICP) for\nmulti-label text classification and present a novel approach for addressing the\ncomputational inefficiency of the Label Powerset (LP) ICP, arrising when\ndealing with a high number of unique labels. We present experimental results\nusing the original and the proposed efficient LP-ICP on two English and one\nCzech language data-sets. Specifically, we apply the LP-ICP on three deep\nArtificial Neural Network (ANN) classifiers of two types: one based on\ncontextualised (bert) and two on non-contextualised (word2vec) word-embeddings.\nIn the LP-ICP setting we assign nonconformity scores to label-sets from which\nthe corresponding p-values and prediction-sets are determined. Our approach\ndeals with the increased computational burden of LP by eliminating from\nconsideration a significant number of label-sets that will surely have p-values\nbelow the specified significance level. This reduces dramatically the\ncomputational complexity of the approach while fully respecting the standard CP\nguarantees. Our experimental results show that the contextualised-based\nclassifier surpasses the non-contextualised-based ones and obtains\nstate-of-the-art performance for all data-sets examined. The good performance\nof the underlying classifiers is carried on to their ICP counterparts without\nany significant accuracy loss, but with the added benefits of ICP, i.e. the\nconfidence information encapsulated in the prediction sets. We experimentally\ndemonstrate that the resulting prediction sets can be tight enough to be\npractically useful even though the set of all possible label-sets contains more\nthan $1e+16$ combinations. Additionally, the empirical error rates of the\nobtained prediction-sets confirm that our outputs are well-calibrated.", "published": "2023-12-14 19:17:42", "link": "http://arxiv.org/abs/2312.09304v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "WikiMuTe: A web-sourced dataset of semantic descriptions for music audio", "abstract": "Multi-modal deep learning techniques for matching free-form text with music\nhave shown promising results in the field of Music Information Retrieval (MIR).\nPrior work is often based on large proprietary data while publicly available\ndatasets are few and small in size. In this study, we present WikiMuTe, a new\nand open dataset containing rich semantic descriptions of music. The data is\nsourced from Wikipedia's rich catalogue of articles covering musical works.\nUsing a dedicated text-mining pipeline, we extract both long and short-form\ndescriptions covering a wide range of topics related to music content such as\ngenre, style, mood, instrumentation, and tempo. To show the use of this data,\nwe train a model that jointly learns text and audio representations and\nperforms cross-modal retrieval. The model is evaluated on two tasks: tag-based\nmusic retrieval and music auto-tagging. The results show that while our\napproach has state-of-the-art performance on multiple tasks, but still observe\na difference in performance depending on the data used for training.", "published": "2023-12-14 18:38:02", "link": "http://arxiv.org/abs/2312.09207v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Multi-Microphone Noise Data Augmentation for DNN-based Own Voice\n  Reconstruction for Hearables in Noisy Environments", "abstract": "Hearables with integrated microphones may offer communication benefits in\nnoisy working environments, e.g. by transmitting the recorded own voice of the\nuser. Systems aiming at reconstructing the clean and full-bandwidth own voice\nfrom noisy microphone recordings are often based on supervised learning.\nRecording a sufficient amount of noise required for training such a system is\ncostly since noise transmission between outer and inner microphones varies\nindividually. Previously proposed methods either do not consider noise, only\nconsider noise at outer microphones or assume inner and outer microphone noise\nto be independent during training, and it is not yet clear whether\nindividualized noise can benefit the training of and own voice reconstruction\nsystem. In this paper, we investigate several noise data augmentation\ntechniques based on measured transfer functions to simulate multi-microphone\nnoise. Using augmented noise, we train a multi-channel own voice reconstruction\nsystem. Experiments using real noise are carried out to investigate the\ngeneralization capability. Results show that incorporating augmented noise\nyields large benefits, in particular considering individualized noise\naugmentation leads to higher performance.", "published": "2023-12-14 13:15:10", "link": "http://arxiv.org/abs/2312.08908v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "NeXt-TDNN: Modernizing Multi-Scale Temporal Convolution Backbone for\n  Speaker Verification", "abstract": "In speaker verification, ECAPA-TDNN has shown remarkable improvement by\nutilizing one-dimensional(1D) Res2Net block and squeeze-and-excitation(SE)\nmodule, along with multi-layer feature aggregation (MFA). Meanwhile, in vision\ntasks, ConvNet structures have been modernized by referring to Transformer,\nresulting in improved performance. In this paper, we present an improved block\ndesign for TDNN in speaker verification. Inspired by recent ConvNet structures,\nwe replace the SE-Res2Net block in ECAPA-TDNN with a novel 1D two-step\nmulti-scale ConvNeXt block, which we call TS-ConvNeXt. The TS-ConvNeXt block is\nconstructed using two separated sub-modules: a temporal multi-scale convolution\n(MSC) and a frame-wise feed-forward network (FFN). This two-step design allows\nfor flexible capturing of inter-frame and intra-frame contexts. Additionally,\nwe introduce global response normalization (GRN) for the FFN modules to enable\nmore selective feature propagation, similar to the SE module in ECAPA-TDNN.\nExperimental results demonstrate that NeXt-TDNN, with a modernized backbone\nblock, significantly improved performance in speaker verification tasks while\nreducing parameter size and inference time. We have released our code for\nfuture studies.", "published": "2023-12-14 02:01:38", "link": "http://arxiv.org/abs/2312.08603v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A computationally efficient semi-blind source separation based approach\n  for nonlinear echo cancellation based on an element-wise iterative source\n  steering", "abstract": "While the semi-blind source separation-based acoustic echo cancellation\n(SBSS-AEC) has received much research attention due to its promising\nperformance during double-talk compared to the traditional adaptive algorithms,\nit suffers from system latency and nonlinear distortions. To circumvent these\ndrawbacks, the recently developed ideas on convolutive transfer function (CTF)\napproximation and nonlinear expansion have been used in the iterative\nprojection (IP)-based semi-blind source separation (SBSS) algorithm. However,\nbecause of the introduction of CTF approximation and nonlinear expansion, this\nalgorithm becomes computationally very expensive, which makes it difficult to\nimplement in embedded systems. Thus, we attempt in this paper to improve this\nIP-based algorithm, thereby developing an element-wise iterative source\nsteering (EISS) algorithm. In comparison with the IP-based SBSS algorithm, the\nproposed algorithm is computationally much more efficient, especially when the\nnonlinear expansion order is high and the length of the CTF filter is long.\nMeanwhile, its AEC performance is as good as that of IP-based SBSS.", "published": "2023-12-14 02:23:38", "link": "http://arxiv.org/abs/2312.08610v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Towards Automatic Data Augmentation for Disordered Speech Recognition", "abstract": "Automatic recognition of disordered speech remains a highly challenging task\nto date due to data scarcity. This paper presents a reinforcement learning (RL)\nbased on-the-fly data augmentation approach for training state-of-the-art\nPyChain TDNN and end-to-end Conformer ASR systems on such data. The handcrafted\ntemporal and spectral mask operations in the standard SpecAugment method that\nare task and system dependent, together with additionally introduced minimum\nand maximum cut-offs of these time-frequency masks, are now automatically\nlearned using an RNN-based policy controller and tightly integrated with ASR\nsystem training. Experiments on the UASpeech corpus suggest the proposed\nRL-based data augmentation approach consistently produced performance superior\nor comparable that obtained using expert or handcrafted SpecAugment policies.\nOur RL auto-augmented PyChain TDNN system produced an overall WER of 28.79% on\nthe UASpeech test set of 16 dysarthric speakers.", "published": "2023-12-14 03:49:51", "link": "http://arxiv.org/abs/2312.08641v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Low-rank constrained multichannel signal denoising considering\n  channel-dependent sensitivity inspired by self-supervised learning for\n  optical fiber sensing", "abstract": "Optical fiber sensing is a technology wherein audio, vibrations, and\ntemperature are detected using an optical fiber; especially the\naudio/vibrations-aware sensing is called distributed acoustic sensing (DAS). In\nDAS, observed data, which is comprised of multichannel data, has suffered from\nsevere noise levels because of the optical noise or the installation methods.\nIn conventional methods for denoising DAS data, signal-processing- or\ndeep-neural-network (DNN)-based models have been studied. The\nsignal-processing-based methods have the interpretability, i.e., non-black box.\nThe DNN-based methods are good at flexibility designing network architectures\nand objective functions, that is, priors. However, there is no balance between\nthe interpretability and the flexibility of priors in the DAS studies. The\nDNN-based methods also require a large amount of training data in general. To\naddress the problems, we propose a DNN-structure signal-processing-based\ndenoising method in this paper. As the priors of DAS, we employ spatial\nknowledge; low rank and channel-dependent sensitivity using the DNN-based\nstructure. The result of fiber-acoustic sensing shows that the proposed method\noutperforms the conventional methods and the robustness to the number of the\nspatial ranks. Moreover, the optimized parameters of the proposed method\nindicate the relationship with the channel sensitivity; the interpretability.", "published": "2023-12-14 05:28:19", "link": "http://arxiv.org/abs/2312.08660v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "TIA: A Teaching Intonation Assessment Dataset in Real Teaching\n  Situations", "abstract": "Intonation is one of the important factors affecting the teaching language\narts, so it is an urgent problem to be addressed by evaluating the teachers'\nintonation through artificial intelligence technology. However, the lack of an\nintonation assessment dataset has hindered the development of the field. To\nthis end, this paper constructs a Teaching Intonation Assessment (TIA) dataset\nfor the first time in real teaching situations. This dataset covers 9\ndisciplines, 396 teachers, total of 11,444 utterance samples with a length of\n15 seconds. In order to test the validity of the dataset, this paper proposes a\nteaching intonation assessment model (TIAM) based on low-level and deep-level\nfeatures of speech. The experimental results show that TIAM based on the\ndataset constructed in this paper is basically consistent with the results of\nmanual evaluation, and the results are better than the baseline models, which\nproves the effectiveness of the evaluation model.", "published": "2023-12-14 08:20:48", "link": "http://arxiv.org/abs/2312.08732v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Hourglass-AVSR: Down-Up Sampling-based Computational Efficiency Model\n  for Audio-Visual Speech Recognition", "abstract": "Recently audio-visual speech recognition (AVSR), which better leverages video\nmodality as additional information to extend automatic speech recognition\n(ASR), has shown promising results in complex acoustic environments. However,\nthere is still substantial space to improve as complex computation of visual\nmodules and ineffective fusion of audio-visual modalities. To eliminate these\ndrawbacks, we propose a down-up sampling-based AVSR model (Hourglass-AVSR) to\nenjoy high efficiency and performance, whose time length is scaled during the\nintermediate processing, resembling an hourglass. Firstly, we propose a context\nand residual aware video upsampling approach to improve the recognition\nperformance, which utilizes contextual information from visual representations\nand captures residual information between adjacent video frames. Secondly, we\nintroduce a visual-audio alignment approach during the upsampling by explicitly\nincorporating boundary constraint loss. Besides, we propose a cross-layer\nattention fusion to capture the modality dependencies within each visual\nencoder layer. Experiments conducted on the MISP-AVSR dataset reveal that our\nproposed Hourglass-AVSR model outperforms ASR model by 12.9% and 20.8% relative\nconcatenated minimum permutation character error rate (cpCER) reduction on\nfar-field and middle-field test sets, respectively. Moreover, compared to other\nstate-of-the-art AVSR models, our model exhibits the highest improvement in\ncpCER for the visual module. Furthermore, on the benefit of our down-up\nsampling approach, Hourglass-AVSR model reduces 54.2% overall computation costs\nwith minor performance degradation.", "published": "2023-12-14 12:08:23", "link": "http://arxiv.org/abs/2312.08850v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Attention-Guided Adaptation for Code-Switching Speech Recognition", "abstract": "The prevalence of the powerful multilingual models, such as Whisper, has\nsignificantly advanced the researches on speech recognition. However, these\nmodels often struggle with handling the code-switching setting, which is\nessential in multilingual speech recognition. Recent studies have attempted to\naddress this setting by separating the modules for different languages to\nensure distinct latent representations for languages. Some other methods\nconsidered the switching mechanism based on language identification. In this\nstudy, a new attention-guided adaptation is proposed to conduct\nparameter-efficient learning for bilingual ASR. This method selects those\nattention heads in a model which closely express language identities and then\nguided those heads to be correctly attended with their corresponding languages.\nThe experiments on the Mandarin-English code-switching speech corpus show that\nthe proposed approach achieves a 14.2% mixed error rate, surpassing\nstate-of-the-art method, where only 5.6% additional parameters over Whisper are\ntrained.", "published": "2023-12-14 12:24:07", "link": "http://arxiv.org/abs/2312.08856v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multi-CMGAN+/+: Leveraging Multi-Objective Speech Quality Metric\n  Prediction for Speech Enhancement", "abstract": "Neural network based approaches to speech enhancement have shown to be\nparticularly powerful, being able to leverage a data-driven approach to result\nin a significant performance gain versus other approaches. Such approaches are\nreliant on artificially created labelled training data such that the neural\nmodel can be trained using intrusive loss functions which compare the output of\nthe model with clean reference speech. Performance of such systems when\nenhancing real-world audio often suffers relative to their performance on\nsimulated test data. In this work, a non-intrusive multi-metric prediction\napproach is introduced, wherein a model trained on artificial labelled data\nusing inference of an adversarially trained metric prediction neural network.\nThe proposed approach shows improved performance versus state-of-the-art\nsystems on the recent CHiME-7 challenge \\ac{UDASE} task evaluation sets.", "published": "2023-12-14 14:27:55", "link": "http://arxiv.org/abs/2312.08979v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "FastInject: Injecting Unpaired Text Data into CTC-based ASR training", "abstract": "Recently, connectionist temporal classification (CTC)-based end-to-end (E2E)\nautomatic speech recognition (ASR) models have achieved impressive results,\nespecially with the development of self-supervised learning. However, E2E ASR\nmodels trained on paired speech-text data often suffer from domain shifts from\ntraining to testing. To alleviate this issue, this paper proposes a flat-start\njoint training method, named FastInject, which efficiently injects multi-domain\nunpaired text data into CTC-based ASR training. To maintain training\nefficiency, text units are pre-upsampled, and their representations are fed\ninto the CTC model along with speech features. To bridge the modality gap\nbetween speech and text, an attention-based modality matching mechanism (AM3)\nis proposed, which retains the E2E flat-start training. Experiments show that\nthe proposed FastInject gave a 22\\% relative WER reduction (WERR) for\nintra-domain Librispeech-100h data and 20\\% relative WERR on out-of-domain test\nsets.", "published": "2023-12-14 16:38:24", "link": "http://arxiv.org/abs/2312.09100v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "F1-EV Score: Measuring the Likelihood of Estimating a Good Decision\n  Threshold for Semi-Supervised Anomaly Detection", "abstract": "Anomalous sound detection (ASD) systems are usually compared by using\nthreshold-independent performance measures such as AUC-ROC. However, for\npractical applications a decision threshold is needed to decide whether a given\ntest sample is normal or anomalous. Estimating such a threshold is highly\nnon-trivial in a semi-supervised setting where only normal training samples are\navailable. In this work, F1-EV a novel threshold-independent performance\nmeasure for ASD systems that also includes the likelihood of estimating a good\ndecision threshold is proposed and motivated using specific toy examples. In\nexperimental evaluations, multiple performance measures are evaluated for all\nsystems submitted to the ASD task of the DCASE Challenge 2023. It is shown that\nF1-EV is strongly correlated with AUC-ROC while having a significantly stronger\ncorrelation with the F1-score obtained with estimated and optimal decision\nthresholds than AUC-ROC.", "published": "2023-12-14 17:13:07", "link": "http://arxiv.org/abs/2312.09143v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Scalable Ensemble-based Detection Method against Adversarial Attacks for\n  speaker verification", "abstract": "Automatic speaker verification (ASV) is highly susceptible to adversarial\nattacks. Purification modules are usually adopted as a pre-processing to\nmitigate adversarial noise. However, they are commonly implemented across\ndiverse experimental settings, rendering direct comparisons challenging. This\npaper comprehensively compares mainstream purification techniques in a unified\nframework. We find these methods often face a trade-off between user experience\nand security, as they struggle to simultaneously maintain genuine sample\nperformance and reduce adversarial perturbations. To address this challenge,\nsome efforts have extended purification modules to encompass detection\ncapabilities, aiming to alleviate the trade-off. However, advanced purification\nmodules will always come into the stage to surpass previous detection method.\nAs a result, we further propose an easy-to-follow ensemble approach that\nintegrates advanced purification modules for detection, achieving\nstate-of-the-art (SOTA) performance in countering adversarial noise. Our\nensemble method has great potential due to its compatibility with future\nadvanced purification techniques.", "published": "2023-12-14 03:04:05", "link": "http://arxiv.org/abs/2312.08622v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Segment Beyond View: Handling Partially Missing Modality for\n  Audio-Visual Semantic Segmentation", "abstract": "Augmented Reality (AR) devices, emerging as prominent mobile interaction\nplatforms, face challenges in user safety, particularly concerning oncoming\nvehicles. While some solutions leverage onboard camera arrays, these cameras\noften have limited field-of-view (FoV) with front or downward perspectives.\nAddressing this, we propose a new out-of-view semantic segmentation task and\nSegment Beyond View (SBV), a novel audio-visual semantic segmentation method.\nSBV supplements the visual modality, which miss the information beyond FoV,\nwith the auditory information using a teacher-student distillation model\n(Omni2Ego). The model consists of a vision teacher utilising panoramic\ninformation, an auditory teacher with 8-channel audio, and an audio-visual\nstudent that takes views with limited FoV and binaural audio as input and\nproduce semantic segmentation for objects outside FoV. SBV outperforms existing\nmodels in comparative evaluations and shows a consistent performance across\nvarying FoV ranges and in monaural audio settings.", "published": "2023-12-14 06:17:15", "link": "http://arxiv.org/abs/2312.08673v3", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "StemGen: A music generation model that listens", "abstract": "End-to-end generation of musical audio using deep learning techniques has\nseen an explosion of activity recently. However, most models concentrate on\ngenerating fully mixed music in response to abstract conditioning information.\nIn this work, we present an alternative paradigm for producing music generation\nmodels that can listen and respond to musical context. We describe how such a\nmodel can be constructed using a non-autoregressive, transformer-based model\narchitecture and present a number of novel architectural and sampling\nimprovements. We train the described architecture on both an open-source and a\nproprietary dataset. We evaluate the produced models using standard quality\nmetrics and a new approach based on music information retrieval descriptors.\nThe resulting model reaches the audio quality of state-of-the-art\ntext-conditioned models, as well as exhibiting strong musical coherence with\nits context.", "published": "2023-12-14 08:09:20", "link": "http://arxiv.org/abs/2312.08723v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Reconstruction of Sound Field through Diffusion Models", "abstract": "Reconstructing the sound field in a room is an important task for several\napplications, such as sound control and augmented (AR) or virtual reality (VR).\nIn this paper, we propose a data-driven generative model for reconstructing the\nmagnitude of acoustic fields in rooms with a focus on the modal frequency\nrange. We introduce, for the first time, the use of a conditional Denoising\nDiffusion Probabilistic Model (DDPM) trained in order to reconstruct the sound\nfield (SF-Diff) over an extended domain. The architecture is devised in order\nto be conditioned on a set of limited available measurements at different\nfrequencies and generate the sound field in target, unknown, locations. The\nresults show that SF-Diff is able to provide accurate reconstructions,\noutperforming a state-of-the-art baseline based on kernel interpolation.", "published": "2023-12-14 11:11:26", "link": "http://arxiv.org/abs/2312.08821v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Design, construction and evaluation of emotional multimodal pathological\n  speech database", "abstract": "The lack of an available emotion pathology database is one of the key\nobstacles in studying the emotion expression status of patients with\ndysarthria. The first Chinese multimodal emotional pathological speech database\ncontaining multi-perspective information is constructed in this paper. It\nincludes 29 controls and 39 patients with different degrees of motor\ndysarthria, expressing happy, sad, angry and neutral emotions. All emotional\nspeech was labeled for intelligibility, types and discrete dimensional emotions\nby developed WeChat mini-program. The subjective analysis justifies from\nemotion discrimination accuracy, speech intelligibility, valence-arousal\nspatial distribution, and correlation between SCL-90 and disease severity. The\nautomatic recognition tested on speech and glottal data, with average accuracy\nof 78% for controls and 60% for patients in audio, while 51% for controls and\n38% for patients in glottal data, indicating an influence of the disease on\nemotional expression.", "published": "2023-12-14 14:43:31", "link": "http://arxiv.org/abs/2312.08998v1", "categories": ["eess.AS", "cs.AI", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Fusion of Audio and Visual Embeddings for Sound Event Localization and\n  Detection", "abstract": "Sound event localization and detection (SELD) combines two subtasks: sound\nevent detection (SED) and direction of arrival (DOA) estimation. SELD is\nusually tackled as an audio-only problem, but visual information has been\nrecently included. Few audio-visual (AV)-SELD works have been published and\nmost employ vision via face/object bounding boxes, or human pose keypoints. In\ncontrast, we explore the integration of audio and visual feature embeddings\nextracted with pre-trained deep networks. For the visual modality, we tested\nResNet50 and Inflated 3D ConvNet (I3D). Our comparison of AV fusion methods\nincludes the AV-Conformer and Cross-Modal Attentive Fusion (CMAF) model. Our\nbest models outperform the DCASE 2023 Task3 audio-only and AV baselines by a\nwide margin on the development set of the STARSS23 dataset, making them\ncompetitive amongst state-of-the-art results of the AV challenge, without model\nensembling, heavy data augmentation, or prediction post-processing. Such\ntechniques and further pre-training could be applied as next steps to improve\nperformance.", "published": "2023-12-14 15:34:23", "link": "http://arxiv.org/abs/2312.09034v1", "categories": ["eess.AS", "cs.SD", "eess.IV"], "primary_category": "eess.AS"}
{"title": "Efficient speech detection in environmental audio using acoustic\n  recognition and knowledge distillation", "abstract": "The ongoing biodiversity crisis, driven by factors such as land-use change\nand global warming, emphasizes the need for effective ecological monitoring\nmethods. Acoustic monitoring of biodiversity has emerged as an important\nmonitoring tool. Detecting human voices in soundscape monitoring projects is\nuseful both for analysing human disturbance and for privacy filtering. Despite\nsignificant strides in deep learning in recent years, the deployment of large\nneural networks on compact devices poses challenges due to memory and latency\nconstraints. Our approach focuses on leveraging knowledge distillation\ntechniques to design efficient, lightweight student models for speech detection\nin bioacoustics. In particular, we employed the MobileNetV3-Small-Pi model to\ncreate compact yet effective student architectures to compare against the\nlarger EcoVAD teacher model, a well-regarded voice detection architecture in\neco-acoustic monitoring. The comparative analysis included examining various\nconfigurations of the MobileNetV3-Small-Pi derived student models to identify\noptimal performance. Additionally, a thorough evaluation of different\ndistillation techniques was conducted to ascertain the most effective method\nfor model selection. Our findings revealed that the distilled models exhibited\ncomparable performance to the EcoVAD teacher model, indicating a promising\napproach to overcoming computational barriers for real-time ecological\nmonitoring.", "published": "2023-12-14 17:55:32", "link": "http://arxiv.org/abs/2312.09269v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio-visual fine-tuning of audio-only ASR models", "abstract": "Audio-visual automatic speech recognition (AV-ASR) models are very effective\nat reducing word error rates on noisy speech, but require large amounts of\ntranscribed AV training data. Recently, audio-visual self-supervised learning\n(SSL) approaches have been developed to reduce this dependence on transcribed\nAV data, but these methods are quite complex and computationally expensive. In\nthis work, we propose replacing these expensive AV-SSL methods with a simple\nand fast \\textit{audio-only} SSL method, and then performing AV supervised\nfine-tuning. We show that this approach is competitive with state-of-the-art\n(SOTA) AV-SSL methods on the LRS3-TED benchmark task (within 0.5% absolute\nWER), while being dramatically simpler and more efficient (12-30x faster to\npre-train). Furthermore, we show we can extend this approach to convert a SOTA\naudio-only ASR model into an AV model. By doing so, we match SOTA AV-SSL\nresults, even though no AV data was used during pre-training.", "published": "2023-12-14 22:05:15", "link": "http://arxiv.org/abs/2312.09369v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
