{"title": "The Microsoft Toolkit of Multi-Task Deep Neural Networks for Natural\n  Language Understanding", "abstract": "We present MT-DNN, an open-source natural language understanding (NLU)\ntoolkit that makes it easy for researchers and developers to train customized\ndeep learning models. Built upon PyTorch and Transformers, MT-DNN is designed\nto facilitate rapid customization for a broad spectrum of NLU tasks, using a\nvariety of objectives (classification, regression, structured prediction) and\ntext encoders (e.g., RNNs, BERT, RoBERTa, UniLM). A unique feature of MT-DNN is\nits built-in support for robust and transferable learning using the adversarial\nmulti-task learning paradigm. To enable efficient production deployment, MT-DNN\nsupports multi-task knowledge distillation, which can substantially compress a\ndeep neural model without significant performance drop. We demonstrate the\neffectiveness of MT-DNN on a wide range of NLU applications across general and\nbiomedical domains. The software and pre-trained models will be publicly\navailable at https://github.com/namisan/mt-dnn.", "published": "2020-02-19 03:05:28", "link": "http://arxiv.org/abs/2002.07972v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Making the Most of Context in Neural Machine Translation", "abstract": "Document-level machine translation manages to outperform sentence level\nmodels by a small margin, but have failed to be widely adopted. We argue that\nprevious research did not make a clear use of the global context, and propose a\nnew document-level NMT framework that deliberately models the local context of\neach sentence with the awareness of the global context of the document in both\nsource and target languages. We specifically design the model to be able to\ndeal with documents containing any number of sentences, including single\nsentences. This unified approach allows our model to be trained elegantly on\nstandard datasets without needing to train on sentence and document level data\nseparately. Experimental results demonstrate that our model outperforms\nTransformer baselines and previous document-level NMT models with substantial\nmargins of up to 2.1 BLEU on state-of-the-art baselines. We also provide\nanalyses which show the benefit of context far beyond the neighboring two or\nthree sentences, which previous studies have typically incorporated.", "published": "2020-02-19 03:30:00", "link": "http://arxiv.org/abs/2002.07982v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LAMBERT: Layout-Aware (Language) Modeling for information extraction", "abstract": "We introduce a simple new approach to the problem of understanding documents\nwhere non-trivial layout influences the local semantics. To this end, we modify\nthe Transformer encoder architecture in a way that allows it to use layout\nfeatures obtained from an OCR system, without the need to re-learn language\nsemantics from scratch. We only augment the input of the model with the\ncoordinates of token bounding boxes, avoiding, in this way, the use of raw\nimages. This leads to a layout-aware language model which can then be\nfine-tuned on downstream tasks.\n  The model is evaluated on an end-to-end information extraction task using\nfour publicly available datasets: Kleister NDA, Kleister Charity, SROIE and\nCORD. We show that our model achieves superior performance on datasets\nconsisting of visually rich documents, while also outperforming the baseline\nRoBERTa on documents with flat layout (NDA \\(F_{1}\\) increase from 78.50 to\n80.42). Our solution ranked first on the public leaderboard for the Key\nInformation Extraction from the SROIE dataset, improving the SOTA\n\\(F_{1}\\)-score from 97.81 to 98.17.", "published": "2020-02-19 09:48:39", "link": "http://arxiv.org/abs/2002.08087v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Compressing BERT: Studying the Effects of Weight Pruning on Transfer\n  Learning", "abstract": "Pre-trained universal feature extractors, such as BERT for natural language\nprocessing and VGG for computer vision, have become effective methods for\nimproving deep learning models without requiring more labeled data. While\neffective, feature extractors like BERT may be prohibitively large for some\ndeployment scenarios. We explore weight pruning for BERT and ask: how does\ncompression during pre-training affect transfer learning? We find that pruning\naffects transfer learning in three broad regimes. Low levels of pruning\n(30-40%) do not affect pre-training loss or transfer to downstream tasks at\nall. Medium levels of pruning increase the pre-training loss and prevent useful\npre-training information from being transferred to downstream tasks. High\nlevels of pruning additionally prevent models from fitting downstream datasets,\nleading to further degradation. Finally, we observe that fine-tuning BERT on a\nspecific task does not improve its prunability. We conclude that BERT can be\npruned once during pre-training rather than separately for each task without\naffecting performance.", "published": "2020-02-19 17:40:57", "link": "http://arxiv.org/abs/2002.08307v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tree-structured Attention with Hierarchical Accumulation", "abstract": "Incorporating hierarchical structures like constituency trees has been shown\nto be effective for various natural language processing (NLP) tasks. However,\nit is evident that state-of-the-art (SOTA) sequence-based models like the\nTransformer struggle to encode such structures inherently. On the other hand,\ndedicated models like the Tree-LSTM, while explicitly modeling hierarchical\nstructures, do not perform as efficiently as the Transformer. In this paper, we\nattempt to bridge this gap with \"Hierarchical Accumulation\" to encode parse\ntree structures into self-attention at constant time complexity. Our approach\noutperforms SOTA methods in four IWSLT translation tasks and the WMT'14\nEnglish-German translation task. It also yields improvements over Transformer\nand Tree-LSTM on three text classification tasks. We further demonstrate that\nusing hierarchical priors can compensate for data shortage, and that our model\nprefers phrase-level attentions over token-level attentions.", "published": "2020-02-19 08:17:00", "link": "http://arxiv.org/abs/2002.08046v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A Systematic Comparison of Architectures for Document-Level Sentiment\n  Classification", "abstract": "Documents are composed of smaller pieces - paragraphs, sentences, and tokens\n- that have complex relationships between one another. Sentiment classification\nmodels that take into account the structure inherent in these documents have a\ntheoretical advantage over those that do not. At the same time, transfer\nlearning models based on language model pretraining have shown promise for\ndocument classification. However, these two paradigms have not been\nsystematically compared and it is not clear under which circumstances one\napproach is better than the other. In this work we empirically compare\nhierarchical models and transfer learning for document-level sentiment\nclassification. We show that non-trivial hierarchical models outperform\nprevious baselines and transfer learning on document-level sentiment\nclassification in five languages.", "published": "2020-02-19 12:22:46", "link": "http://arxiv.org/abs/2002.08131v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CodeBERT: A Pre-Trained Model for Programming and Natural Languages", "abstract": "We present CodeBERT, a bimodal pre-trained model for programming language\n(PL) and nat-ural language (NL). CodeBERT learns general-purpose\nrepresentations that support downstream NL-PL applications such as natural\nlanguage codesearch, code documentation generation, etc. We develop CodeBERT\nwith Transformer-based neural architecture, and train it with a hybrid\nobjective function that incorporates the pre-training task of replaced token\ndetection, which is to detect plausible alternatives sampled from generators.\nThis enables us to utilize both bimodal data of NL-PL pairs and unimodal data,\nwhere the former provides input tokens for model training while the latter\nhelps to learn better generators. We evaluate CodeBERT on two NL-PL\napplications by fine-tuning model parameters. Results show that CodeBERT\nachieves state-of-the-art performance on both natural language code search and\ncode documentation generation tasks. Furthermore, to investigate what type of\nknowledge is learned in CodeBERT, we construct a dataset for NL-PL probing, and\nevaluate in a zero-shot setting where parameters of pre-trained models are\nfixed. Results show that CodeBERT performs better than previous pre-trained\nmodels on NL-PL probing.", "published": "2020-02-19 13:09:07", "link": "http://arxiv.org/abs/2002.08155v4", "categories": ["cs.CL", "cs.PL"], "primary_category": "cs.CL"}
{"title": "VQA-LOL: Visual Question Answering under the Lens of Logic", "abstract": "Logical connectives and their implications on the meaning of a natural\nlanguage sentence are a fundamental aspect of understanding. In this paper, we\ninvestigate whether visual question answering (VQA) systems trained to answer a\nquestion about an image, are able to answer the logical composition of multiple\nsuch questions. When put under this \\textit{Lens of Logic}, state-of-the-art\nVQA models have difficulty in correctly answering these logically composed\nquestions. We construct an augmentation of the VQA dataset as a benchmark, with\nquestions containing logical compositions and linguistic transformations\n(negation, disjunction, conjunction, and antonyms). We propose our {Lens of\nLogic (LOL)} model which uses question-attention and logic-attention to\nunderstand logical connectives in the question, and a novel\nFr\\'echet-Compatibility Loss, which ensures that the answers of the component\nquestions and the composed question are consistent with the inferred logical\noperation. Our model shows substantial improvement in learning logical\ncompositions while retaining performance on VQA. We suggest this work as a move\ntowards robustness by embedding logical connectives in visual understanding.", "published": "2020-02-19 17:57:46", "link": "http://arxiv.org/abs/2002.08325v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Attacking Neural Text Detectors", "abstract": "Machine learning based language models have recently made significant\nprogress, which introduces a danger to spread misinformation. To combat this\npotential danger, several methods have been proposed for detecting text written\nby these language models. This paper presents two classes of black-box attacks\non these detectors, one which randomly replaces characters with homoglyphs, and\nthe other a simple scheme to purposefully misspell words. The homoglyph and\nmisspelling attacks decrease a popular neural text detector's recall on neural\ntext from 97.44% to 0.26% and 22.68%, respectively. Results also indicate that\nthe attacks are transferable to other neural text detectors.", "published": "2020-02-19 04:18:45", "link": "http://arxiv.org/abs/2002.11768v4", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Non-Autoregressive Dialog State Tracking", "abstract": "Recent efforts in Dialogue State Tracking (DST) for task-oriented dialogues\nhave progressed toward open-vocabulary or generation-based approaches where the\nmodels can generate slot value candidates from the dialogue history itself.\nThese approaches have shown good performance gain, especially in complicated\ndialogue domains with dynamic slot values. However, they fall short in two\naspects: (1) they do not allow models to explicitly learn signals across\ndomains and slots to detect potential dependencies among (domain, slot) pairs;\nand (2) existing models follow auto-regressive approaches which incur high time\ncost when the dialogue evolves over multiple domains and multiple turns. In\nthis paper, we propose a novel framework of Non-Autoregressive Dialog State\nTracking (NADST) which can factor in potential dependencies among domains and\nslots to optimize the models towards better prediction of dialogue states as a\ncomplete set rather than separate slots. In particular, the non-autoregressive\nnature of our method not only enables decoding in parallel to significantly\nreduce the latency of DST for real-time dialogue response generation, but also\ndetect dependencies among slots at token level in addition to slot and domain\nlevel. Our empirical results show that our model achieves the state-of-the-art\njoint accuracy across all domains on the MultiWOZ 2.1 corpus, and the latency\nof our model is an order of magnitude lower than the previous state of the art\nas the dialogue history extends over time.", "published": "2020-02-19 06:39:26", "link": "http://arxiv.org/abs/2002.08024v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Rnn-transducer with language bias for end-to-end Mandarin-English\n  code-switching speech recognition", "abstract": "Recently, language identity information has been utilized to improve the\nperformance of end-to-end code-switching (CS) speech recognition. However,\nprevious works use an additional language identification (LID) model as an\nauxiliary module, which causes the system complex. In this work, we propose an\nimproved recurrent neural network transducer (RNN-T) model with language bias\nto alleviate the problem. We use the language identities to bias the model to\npredict the CS points. This promotes the model to learn the language identity\ninformation directly from transcription, and no additional LID model is needed.\nWe evaluate the approach on a Mandarin-English CS corpus SEAME. Compared to our\nRNN-T baseline, the proposed method can achieve 16.2% and 12.9% relative error\nreduction on two test sets, respectively.", "published": "2020-02-19 12:01:33", "link": "http://arxiv.org/abs/2002.08126v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A Differential-form Pullback Programming Language for Higher-order\n  Reverse-mode Automatic Differentiation", "abstract": "Building on the observation that reverse-mode automatic differentiation (AD)\n-- a generalisation of backpropagation -- can naturally be expressed as\npullbacks of differential 1-forms, we design a simple higher-order programming\nlanguage with a first-class differential operator, and present a reduction\nstrategy which exactly simulates reverse-mode AD. We justify our reduction\nstrategy by interpreting our language in any differential $\\lambda$-category\nthat satisfies the Hahn-Banach Separation Theorem, and show that the reduction\nstrategy precisely captures reverse-mode AD in a truly higher-order setting.", "published": "2020-02-19 15:38:03", "link": "http://arxiv.org/abs/2002.08241v1", "categories": ["cs.PL", "cs.CL", "cs.LO"], "primary_category": "cs.PL"}
{"title": "Multilogue-Net: A Context Aware RNN for Multi-modal Emotion Detection\n  and Sentiment Analysis in Conversation", "abstract": "Sentiment Analysis and Emotion Detection in conversation is key in several\nreal-world applications, with an increase in modalities available aiding a\nbetter understanding of the underlying emotions. Multi-modal Emotion Detection\nand Sentiment Analysis can be particularly useful, as applications will be able\nto use specific subsets of available modalities, as per the available data.\nCurrent systems dealing with Multi-modal functionality fail to leverage and\ncapture - the context of the conversation through all modalities, the\ndependency between the listener(s) and speaker emotional states, and the\nrelevance and relationship between the available modalities. In this paper, we\npropose an end to end RNN architecture that attempts to take into account all\nthe mentioned drawbacks. Our proposed model, at the time of writing,\nout-performs the state of the art on a benchmark dataset on a variety of\naccuracy and regression metrics.", "published": "2020-02-19 16:21:00", "link": "http://arxiv.org/abs/2002.08267v3", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Interactive Natural Language-based Person Search", "abstract": "In this work, we consider the problem of searching people in an unconstrained\nenvironment, with natural language descriptions. Specifically, we study how to\nsystematically design an algorithm to effectively acquire descriptions from\nhumans. An algorithm is proposed by adapting models, used for visual and\nlanguage understanding, to search a person of interest (POI) in a principled\nway, achieving promising results without the need to re-design another\ncomplicated model. We then investigate an iterative question-answering (QA)\nstrategy that enable robots to request additional information about the POI's\nappearance from the user. To this end, we introduce a greedy algorithm to rank\nquestions in terms of their significance, and equip the algorithm with the\ncapability to dynamically adjust the length of human-robot interaction\naccording to model's uncertainty. Our approach is validated not only on\nbenchmark datasets but on a mobile robot, moving in a dynamic and crowded\nenvironment.", "published": "2020-02-19 20:42:19", "link": "http://arxiv.org/abs/2002.08434v1", "categories": ["cs.RO", "cs.CL", "cs.CV", "cs.HC"], "primary_category": "cs.RO"}
{"title": "How To Avoid Being Eaten By a Grue: Exploration Strategies for\n  Text-Adventure Agents", "abstract": "Text-based games -- in which an agent interacts with the world through\ntextual natural language -- present us with the problem of\ncombinatorially-sized action-spaces. Most current reinforcement learning\nalgorithms are not capable of effectively handling such a large number of\npossible actions per turn. Poor sample efficiency, consequently, results in\nagents that are unable to pass bottleneck states, where they are unable to\nproceed because they do not see the right action sequence to pass the\nbottleneck enough times to be sufficiently reinforced. Building on prior work\nusing knowledge graphs in reinforcement learning, we introduce two new game\nstate exploration strategies. We compare our exploration strategies against\nstrong baselines on the classic text-adventure game, Zork1, where prior agent\nhave been unable to get past a bottleneck where the agent is eaten by a Grue.", "published": "2020-02-19 17:18:20", "link": "http://arxiv.org/abs/2002.08795v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "SAFE: Similarity-Aware Multi-Modal Fake News Detection", "abstract": "Effective detection of fake news has recently attracted significant\nattention. Current studies have made significant contributions to predicting\nfake news with less focus on exploiting the relationship (similarity) between\nthe textual and visual information in news articles. Attaching importance to\nsuch similarity helps identify fake news stories that, for example, attempt to\nuse irrelevant images to attract readers' attention. In this work, we propose a\n$\\mathsf{S}$imilarity-$\\mathsf{A}$ware $\\mathsf{F}$ak$\\mathsf{E}$ news\ndetection method ($\\mathsf{SAFE}$) which investigates multi-modal (textual and\nvisual) information of news articles. First, neural networks are adopted to\nseparately extract textual and visual features for news representation. We\nfurther investigate the relationship between the extracted features across\nmodalities. Such representations of news textual and visual information along\nwith their relationship are jointly learned and used to predict fake news. The\nproposed method facilitates recognizing the falsity of news articles based on\ntheir text, images, or their \"mismatches.\" We conduct extensive experiments on\nlarge-scale real-world data, which demonstrate the effectiveness of the\nproposed method.", "published": "2020-02-19 02:51:04", "link": "http://arxiv.org/abs/2003.04981v1", "categories": ["cs.CL", "cs.CV", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Gradient-Adjusted Neuron Activation Profiles for Comprehensive\n  Introspection of Convolutional Speech Recognition Models", "abstract": "Deep Learning based Automatic Speech Recognition (ASR) models are very\nsuccessful, but hard to interpret. To gain better understanding of how\nArtificial Neural Networks (ANNs) accomplish their tasks, introspection methods\nhave been proposed. Adapting such techniques from computer vision to speech\nrecognition is not straight-forward, because speech data is more complex and\nless interpretable than image data. In this work, we introduce\nGradient-adjusted Neuron Activation Profiles (GradNAPs) as means to interpret\nfeatures and representations in Deep Neural Networks. GradNAPs are\ncharacteristic responses of ANNs to particular groups of inputs, which\nincorporate the relevance of neurons for prediction. We show how to utilize\nGradNAPs to gain insight about how data is processed in ANNs. This includes\ndifferent ways of visualizing features and clustering of GradNAPs to compare\nembeddings of different groups of inputs in any layer of a given network. We\ndemonstrate our proposed techniques using a fully-convolutional ASR model.", "published": "2020-02-19 11:59:36", "link": "http://arxiv.org/abs/2002.08125v1", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "RTMobile: Beyond Real-Time Mobile Acceleration of RNNs for Speech\n  Recognition", "abstract": "Recurrent neural networks (RNNs) based automatic speech recognition has\nnowadays become prevalent on mobile devices such as smart phones. However,\nprevious RNN compression techniques either suffer from hardware performance\noverhead due to irregularity or significant accuracy loss due to the preserved\nregularity for hardware friendliness. In this work, we propose RTMobile that\nleverages both a novel block-based pruning approach and compiler optimizations\nto accelerate RNN inference on mobile devices. Our proposed RTMobile is the\nfirst work that can achieve real-time RNN inference on mobile platforms.\nExperimental results demonstrate that RTMobile can significantly outperform\nexisting RNN hardware acceleration methods in terms of inference accuracy and\ntime. Compared with prior work on FPGA, RTMobile using Adreno 640 embedded GPU\non GRU can improve the energy-efficiency by about 40$\\times$ while maintaining\nthe same inference time.", "published": "2020-02-19 00:07:32", "link": "http://arxiv.org/abs/2002.11474v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
