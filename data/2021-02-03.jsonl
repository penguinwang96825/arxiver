{"title": "A Computational Framework for Slang Generation", "abstract": "Slang is a common type of informal language, but its flexible nature and\npaucity of data resources present challenges for existing natural language\nsystems. We take an initial step toward machine generation of slang by\ndeveloping a framework that models the speaker's word choice in slang context.\nOur framework encodes novel slang meaning by relating the conventional and\nslang senses of a word while incorporating syntactic and contextual knowledge\nin slang usage. We construct the framework using a combination of probabilistic\ninference and neural contrastive learning. We perform rigorous evaluations on\nthree slang dictionaries and show that our approach not only outperforms\nstate-of-the-art language models, but also better predicts the historical\nemergence of slang word usages from 1960s to 2000s. We interpret the proposed\nmodels and find that the contrastively learned semantic space is sensitive to\nthe similarities between slang and conventional senses of words. Our work\ncreates opportunities for the automated generation and interpretation of\ninformal language.", "published": "2021-02-03 01:19:07", "link": "http://arxiv.org/abs/2102.01826v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Investigation Between Schema Linking and Text-to-SQL Performance", "abstract": "Text-to-SQL is a crucial task toward developing methods for understanding\nnatural language by computers. Recent neural approaches deliver excellent\nperformance; however, models that are difficult to interpret inhibit future\ndevelopments. Hence, this study aims to provide a better approach toward the\ninterpretation of neural models. We hypothesize that the internal behavior of\nmodels at hand becomes much easier to analyze if we identify the detailed\nperformance of schema linking simultaneously as the additional information of\nthe text-to-SQL performance. We provide the ground-truth annotation of schema\nlinking information onto the Spider dataset. We demonstrate the usefulness of\nthe annotated data and how to analyze the current state-of-the-art neural\nmodels.", "published": "2021-02-03 02:50:10", "link": "http://arxiv.org/abs/2102.01847v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HeBERT & HebEMO: a Hebrew BERT Model and a Tool for Polarity Analysis\n  and Emotion Recognition", "abstract": "This paper introduces HeBERT and HebEMO. HeBERT is a Transformer-based model\nfor modern Hebrew text, which relies on a BERT (Bidirectional Encoder\nRepresentations for Transformers) architecture. BERT has been shown to\noutperform alternative architectures in sentiment analysis, and is suggested to\nbe particularly appropriate for MRLs. Analyzing multiple BERT specifications,\nwe find that while model complexity correlates with high performance on\nlanguage tasks that aim to understand terms in a sentence, a more-parsimonious\nmodel better captures the sentiment of entire sentence. Either way, out\nBERT-based language model outperforms all existing Hebrew alternatives on all\ncommon language tasks. HebEMO is a tool that uses HeBERT to detect polarity and\nextract emotions from Hebrew UGC. HebEMO is trained on a unique\nCovid-19-related UGC dataset that we collected and annotated for this study.\nData collection and annotation followed an active learning procedure that aimed\nto maximize predictability. We show that HebEMO yields a high F1-score of 0.96\nfor polarity classification. Emotion detection reaches F1-scores of 0.78-0.97\nfor various target emotions, with the exception of surprise, which the model\nfailed to capture (F1 = 0.41). These results are better than the best-reported\nperformance, even among English-language models of emotion detection.", "published": "2021-02-03 06:59:59", "link": "http://arxiv.org/abs/2102.01909v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Top-down Discourse Parsing via Sequence Labelling", "abstract": "We introduce a top-down approach to discourse parsing that is conceptually\nsimpler than its predecessors (Kobayashi et al., 2020; Zhang et al., 2020). By\nframing the task as a sequence labelling problem where the goal is to\niteratively segment a document into individual discourse units, we are able to\neliminate the decoder and reduce the search space for splitting points. We\nexplore both traditional recurrent models and modern pre-trained transformer\nmodels for the task, and additionally introduce a novel dynamic oracle for\ntop-down parsing. Based on the Full metric, our proposed LSTM model sets a new\nstate-of-the-art for RST parsing.", "published": "2021-02-03 14:30:21", "link": "http://arxiv.org/abs/2102.02080v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Select External Knowledge with Multi-Scale Negative Sampling", "abstract": "The Track-1 of DSTC9 aims to effectively answer user requests or questions\nduring task-oriented dialogues, which are out of the scope of APIs/DB. By\nleveraging external knowledge resources, relevant information can be retrieved\nand encoded into the response generation for these out-of-API-coverage queries.\nIn this work, we have explored several advanced techniques to enhance the\nutilization of external knowledge and boost the quality of response generation,\nincluding schema guided knowledge decision, negatives enhanced knowledge\nselection, and knowledge grounded response generation. To evaluate the\nperformance of our proposed method, comprehensive experiments have been carried\nout on the publicly available dataset. Our approach was ranked as the best in\nhuman evaluation of DSTC9 Track-1.", "published": "2021-02-03 14:59:35", "link": "http://arxiv.org/abs/2102.02096v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Match Mathematical Statements with Proofs", "abstract": "We introduce a novel task consisting in assigning a proof to a given\nmathematical statement. The task is designed to improve the processing of\nresearch-level mathematical texts. Applying Natural Language Processing (NLP)\ntools to research level mathematical articles is both challenging, since it is\na highly specialized domain which mixes natural language and mathematical\nformulae. It is also an important requirement for developing tools for\nmathematical information retrieval and computer-assisted theorem proving. We\nrelease a dataset for the task, consisting of over 180k statement-proof pairs\nextracted from mathematical research articles. We carry out preliminary\nexperiments to assess the difficulty of the task. We first experiment with two\nbag-of-words baselines. We show that considering the assignment problem\nglobally and using weighted bipartite matching algorithms helps a lot in\ntackling the task. Finally, we introduce a self-attention-based model that can\nbe trained either locally or globally and outperforms baselines by a wide\nmargin.", "published": "2021-02-03 15:38:54", "link": "http://arxiv.org/abs/2102.02110v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Bias in Transfer Learning Approaches for Text Classification", "abstract": "Classification is an essential and fundamental task in machine learning,\nplaying a cardinal role in the field of natural language processing (NLP) and\ncomputer vision (CV). In a supervised learning setting, labels are always\nneeded for the classification task. Especially for deep neural models, a large\namount of high-quality labeled data are required for training. However, when a\nnew domain comes out, it is usually hard or expensive to acquire the labels.\nTransfer learning could be an option to transfer the knowledge from a source\ndomain to a target domain. A challenge is that these two domains can be\ndifferent, either on the feature distribution, or the class distribution for\nthe nature of the samples. In this work, we evaluate some existing transfer\nlearning approaches on detecting the bias of imbalanced classes including\ntraditional and deep models. Besides, we propose an approach to bridge the gap\nof the domain class imbalance issue.", "published": "2021-02-03 15:48:21", "link": "http://arxiv.org/abs/2102.02114v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Disambiguatory Signals are Stronger in Word-initial Positions", "abstract": "Psycholinguistic studies of human word processing and lexical access provide\nample evidence of the preferred nature of word-initial versus word-final\nsegments, e.g., in terms of attention paid by listeners (greater) or the\nlikelihood of reduction by speakers (lower). This has led to the conjecture --\nas in Wedel et al. (2019b), but common elsewhere -- that languages have evolved\nto provide more information earlier in words than later. Information-theoretic\nmethods to establish such tendencies in lexicons have suffered from several\nmethodological shortcomings that leave open the question of whether this high\nword-initial informativeness is actually a property of the lexicon or simply an\nartefact of the incremental nature of recognition. In this paper, we point out\nthe confounds in existing methods for comparing the informativeness of segments\nearly in the word versus later in the word, and present several new measures\nthat avoid these confounds. When controlling for these confounds, we still find\nevidence across hundreds of languages that indeed there is a cross-linguistic\ntendency to front-load information in words.", "published": "2021-02-03 18:19:16", "link": "http://arxiv.org/abs/2102.02183v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DiSCoL: Toward Engaging Dialogue Systems through Conversational Line\n  Guided Response Generation", "abstract": "Having engaging and informative conversations with users is the utmost goal\nfor open-domain conversational systems. Recent advances in transformer-based\nlanguage models and their applications to dialogue systems have succeeded to\ngenerate fluent and human-like responses. However, they still lack control over\nthe generation process towards producing contentful responses and achieving\nengaging conversations. To achieve this goal, we present \\textbf{DiSCoL}\n(\\textbf{Di}alogue \\textbf{S}ystems through \\textbf{Co}versational\n\\textbf{L}ine guided response generation). DiSCoL is an open-domain dialogue\nsystem that leverages conversational lines (briefly \\textbf{convlines}) as\ncontrollable and informative content-planning elements to guide the generation\nmodel produce engaging and informative responses. Two primary modules in\nDiSCoL's pipeline are conditional generators trained for 1) predicting relevant\nand informative convlines for dialogue contexts and 2) generating high-quality\nresponses conditioned on the predicted convlines. Users can also change the\nreturned convlines to \\textit{control} the direction of the conversations\ntowards topics that are more interesting for them. Through automatic and human\nevaluations, we demonstrate the efficiency of the convlines in producing\nengaging conversations.", "published": "2021-02-03 18:36:58", "link": "http://arxiv.org/abs/2102.02191v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Memorization vs. Generalization: Quantifying Data Leakage in NLP\n  Performance Evaluation", "abstract": "Public datasets are often used to evaluate the efficacy and generalizability\nof state-of-the-art methods for many tasks in natural language processing\n(NLP). However, the presence of overlap between the train and test datasets can\nlead to inflated results, inadvertently evaluating the model's ability to\nmemorize and interpreting it as the ability to generalize. In addition, such\ndata sets may not provide an effective indicator of the performance of these\nmethods in real world scenarios. We identify leakage of training data into test\ndata on several publicly available datasets used to evaluate NLP tasks,\nincluding named entity recognition and relation extraction, and study them to\nassess the impact of that leakage on the model's ability to memorize versus\ngeneralize.", "published": "2021-02-03 00:58:45", "link": "http://arxiv.org/abs/2102.01818v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "L2C: Describing Visual Differences Needs Semantic Understanding of\n  Individuals", "abstract": "Recent advances in language and vision push forward the research of\ncaptioning a single image to describing visual differences between image pairs.\nSuppose there are two images, I_1 and I_2, and the task is to generate a\ndescription W_{1,2} comparing them, existing methods directly model { I_1, I_2\n} -> W_{1,2} mapping without the semantic understanding of individuals. In this\npaper, we introduce a Learning-to-Compare (L2C) model, which learns to\nunderstand the semantic structures of these two images and compare them while\nlearning to describe each one. We demonstrate that L2C benefits from a\ncomparison between explicit semantic representations and single-image captions,\nand generalizes better on the new testing image pairs. It outperforms the\nbaseline on both automatic evaluation and human evaluation for the\nBirds-to-Words dataset.", "published": "2021-02-03 03:44:42", "link": "http://arxiv.org/abs/2102.01860v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Mind the Gap: Assessing Temporal Generalization in Neural Language\n  Models", "abstract": "Our world is open-ended, non-stationary, and constantly evolving; thus what\nwe talk about and how we talk about it change over time. This inherent dynamic\nnature of language contrasts with the current static language modelling\nparadigm, which trains and evaluates models on utterances from overlapping time\nperiods. Despite impressive recent progress, we demonstrate that Transformer-XL\nlanguage models perform worse in the realistic setup of predicting future\nutterances from beyond their training period, and that model performance\nbecomes increasingly worse with time. We find that, while increasing model size\nalone -- a key driver behind recent progress -- does not solve this problem,\nhaving models that continually update their knowledge with new information can\nindeed mitigate this performance degradation over time. Hence, given the\ncompilation of ever-larger language modelling datasets, combined with the\ngrowing list of language-model-based NLP applications that require up-to-date\nfactual knowledge about the world, we argue that now is the right time to\nrethink the static way in which we currently train and evaluate our language\nmodels, and develop adaptive language models that can remain up-to-date with\nrespect to our ever-changing and non-stationary world. We publicly release our\ndynamic, streaming language modelling benchmarks for WMT and arXiv to\nfacilitate language model evaluation that takes temporal dynamics into account.", "published": "2021-02-03 09:01:49", "link": "http://arxiv.org/abs/2102.01951v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Introduction to Neural Transfer Learning with Transformers for Social\n  Science Text Analysis", "abstract": "Transformer-based models for transfer learning have the potential to achieve\nhigh prediction accuracies on text-based supervised learning tasks with\nrelatively few training data instances. These models are thus likely to benefit\nsocial scientists that seek to have as accurate as possible text-based measures\nbut only have limited resources for annotating training data. To enable social\nscientists to leverage these potential benefits for their research, this paper\nexplains how these methods work, why they might be advantageous, and what their\nlimitations are. Additionally, three Transformer-based models for transfer\nlearning, BERT (Devlin et al. 2019), RoBERTa (Liu et al. 2019), and the\nLongformer (Beltagy et al. 2020), are compared to conventional machine learning\nalgorithms on three applications. Across all evaluated tasks, textual styles,\nand training data set sizes, the conventional models are consistently\noutperformed by transfer learning with Transformers, thereby demonstrating the\nbenefits these models can bring to text-based social science research.", "published": "2021-02-03 15:41:20", "link": "http://arxiv.org/abs/2102.02111v2", "categories": ["cs.CL", "stat.AP", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Bootstrapping Multilingual AMR with Contextual Word Alignments", "abstract": "We develop high performance multilingualAbstract Meaning Representation (AMR)\nsys-tems by projecting English AMR annotationsto other languages with weak\nsupervision. Weachieve this goal by bootstrapping transformer-based\nmultilingual word embeddings, in partic-ular those from cross-lingual RoBERTa\n(XLM-R large). We develop a novel technique forforeign-text-to-English AMR\nalignment, usingthe contextual word alignment between En-glish and foreign\nlanguage tokens. This wordalignment is weakly supervised and relies onthe\ncontextualized XLM-R word embeddings.We achieve a highly competitive\nperformancethat surpasses the best published results forGerman, Italian,\nSpanish and Chinese.", "published": "2021-02-03 18:35:55", "link": "http://arxiv.org/abs/2102.02189v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "When Can Models Learn From Explanations? A Formal Framework for\n  Understanding the Roles of Explanation Data", "abstract": "Many methods now exist for conditioning model outputs on task instructions,\nretrieved documents, and user-provided explanations and feedback. Rather than\nrelying solely on examples of task inputs and outputs, these approaches use\nvaluable additional data for improving model correctness and aligning learned\nmodels with human priors. Meanwhile, a growing body of evidence suggests that\nsome language models can (1) store a large amount of knowledge in their\nparameters, and (2) perform inference over tasks in textual inputs at test\ntime. These results raise the possibility that, for some tasks, humans cannot\nexplain to a model any more about the task than it already knows or could infer\non its own. In this paper, we study the circumstances under which explanations\nof individual data points can (or cannot) improve modeling performance. In\norder to carefully control important properties of the data and explanations,\nwe introduce a synthetic dataset for experiments, and we also make use of three\nexisting datasets with explanations: e-SNLI, TACRED, and SemEval. We first give\na formal framework for the available modeling approaches, in which explanation\ndata can be used as model inputs, as targets, or as a prior. After arguing that\nthe most promising role for explanation data is as model inputs, we propose to\nuse a retrieval-based method and show that it solves our synthetic task with\naccuracies upwards of 95%, while baselines without explanation data achieve\nbelow 65% accuracy. We then identify properties of datasets for which\nretrieval-based modeling fails. With the three existing datasets, we find no\nimprovements from explanation retrieval. Drawing on findings from our synthetic\ntask, we suggest that at least one of six preconditions for successful modeling\nfails to hold with these datasets. Our code is publicly available at\nhttps://github.com/peterbhase/ExplanationRoles", "published": "2021-02-03 18:57:08", "link": "http://arxiv.org/abs/2102.02201v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Confusion2vec 2.0: Enriching Ambiguous Spoken Language Representations\n  with Subwords", "abstract": "Word vector representations enable machines to encode human language for\nspoken language understanding and processing. Confusion2vec, motivated from\nhuman speech production and perception, is a word vector representation which\nencodes ambiguities present in human spoken language in addition to semantics\nand syntactic information. Confusion2vec provides a robust spoken language\nrepresentation by considering inherent human language ambiguities. In this\npaper, we propose a novel word vector space estimation by unsupervised learning\non lattices output by an automatic speech recognition (ASR) system. We encode\neach word in confusion2vec vector space by its constituent subword character\nn-grams. We show the subword encoding helps better represent the acoustic\nperceptual ambiguities in human spoken language via information modeled on\nlattice structured ASR output. The usefulness of the proposed Confusion2vec\nrepresentation is evaluated using semantic, syntactic and acoustic analogy and\nword similarity tasks. We also show the benefits of subword modeling for\nacoustic ambiguity representation on the task of spoken language intent\ndetection. The results significantly outperform existing word vector\nrepresentations when evaluated on erroneous ASR outputs. We demonstrate that\nConfusion2vec subword modeling eliminates the need for retraining/adapting the\nnatural language understanding models on ASR transcripts.", "published": "2021-02-03 20:03:50", "link": "http://arxiv.org/abs/2102.02270v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Self-Supervised Claim Identification for Automated Fact Checking", "abstract": "We propose a novel, attention-based self-supervised approach to identify\n\"claim-worthy\" sentences in a fake news article, an important first step in\nautomated fact-checking. We leverage \"aboutness\" of headline and content using\nattention mechanism for this task. The identified claims can be used for\ndownstream task of claim verification for which we are releasing a benchmark\ndataset of manually selected compelling articles with veracity labels and\nassociated evidence. This work goes beyond stylistic analysis to identifying\ncontent that influences reader belief. Experiments with three datasets show the\nstrength of our model. Data and code available at\nhttps://github.com/architapathak/Self-Supervised-ClaimIdentification", "published": "2021-02-03 23:37:09", "link": "http://arxiv.org/abs/2102.02335v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "MUFASA: Multimodal Fusion Architecture Search for Electronic Health\n  Records", "abstract": "One important challenge of applying deep learning to electronic health\nrecords (EHR) is the complexity of their multimodal structure. EHR usually\ncontains a mixture of structured (codes) and unstructured (free-text) data with\nsparse and irregular longitudinal features -- all of which doctors utilize when\nmaking decisions. In the deep learning regime, determining how different\nmodality representations should be fused together is a difficult problem, which\nis often addressed by handcrafted modeling and intuition. In this work, we\nextend state-of-the-art neural architecture search (NAS) methods and propose\nMUltimodal Fusion Architecture SeArch (MUFASA) to simultaneously search across\nmultimodal fusion strategies and modality-specific architectures for the first\ntime. We demonstrate empirically that our MUFASA method outperforms established\nunimodal NAS on public EHR data with comparable computation costs. In addition,\nMUFASA produces architectures that outperform Transformer and Evolved\nTransformer. Compared with these baselines on CCS diagnosis code prediction,\nour discovered models improve top-5 recall from 0.88 to 0.91 and demonstrate\nthe ability to generalize to other EHR tasks. Studying our top architecture in\ndepth, we provide empirical evidence that MUFASA's improvements are derived\nfrom its ability to both customize modeling for each data modality and find\neffective fusion strategies.", "published": "2021-02-03 23:48:54", "link": "http://arxiv.org/abs/2102.02340v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Impact of Sound Duration and Inactive Frames on Sound Event Detection\n  Performance", "abstract": "In many methods of sound event detection (SED), a segmented time frame is\nregarded as one data sample to model training. The durations of sound events\ngreatly depend on the sound event class, e.g., the sound event \"fan\" has a long\nduration, whereas the sound event \"mouse clicking\" is instantaneous. Thus, the\ndifference in the duration between sound event classes results in a serious\ndata imbalance in SED. Moreover, most sound events tend to occur occasionally;\ntherefore, there are many more inactive time frames of sound events than active\nframes. This also causes a severe data imbalance between active and inactive\nframes. In this paper, we investigate the impact of sound duration and inactive\nframes on SED performance by introducing four loss functions, such as simple\nreweighting loss, inverse frequency loss, asymmetric focal loss, and focal\nbatch Tversky loss. Then, we provide insights into how we tackle this imbalance\nproblem.", "published": "2021-02-03 08:00:43", "link": "http://arxiv.org/abs/2102.01927v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Global-local Attention Framework for Weakly Labelled Audio Tagging", "abstract": "Weakly labelled audio tagging aims to predict the classes of sound events\nwithin an audio clip, where the onset and offset times of the sound events are\nnot provided. Previous works have used the multiple instance learning (MIL)\nframework, and exploited the information of the whole audio clip by MIL pooling\nfunctions. However, the detailed information of sound events such as their\ndurations may not be considered under this framework. To address this issue, we\npropose a novel two-stream framework for audio tagging by exploiting the global\nand local information of sound events. The global stream aims to analyze the\nwhole audio clip in order to capture the local clips that need to be attended\nusing a class-wise selection module. These clips are then fed to the local\nstream to exploit the detailed information for a better decision. Experimental\nresults on the AudioSet show that our proposed method can significantly improve\nthe performance of audio tagging under different baseline network\narchitectures.", "published": "2021-02-03 08:13:47", "link": "http://arxiv.org/abs/2102.01931v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speech Emotion Recognition with Multiscale Area Attention and Data\n  Augmentation", "abstract": "In Speech Emotion Recognition (SER), emotional characteristics often appear\nin diverse forms of energy patterns in spectrograms. Typical attention neural\nnetwork classifiers of SER are usually optimized on a fixed attention\ngranularity. In this paper, we apply multiscale area attention in a deep\nconvolutional neural network to attend emotional characteristics with varied\ngranularities and therefore the classifier can benefit from an ensemble of\nattentions with different scales. To deal with data sparsity, we conduct data\naugmentation with vocal tract length perturbation (VTLP) to improve the\ngeneralization capability of the classifier. Experiments are carried out on the\nInteractive Emotional Dyadic Motion Capture (IEMOCAP) dataset. We achieved\n79.34% weighted accuracy (WA) and 77.54% unweighted accuracy (UA), which, to\nthe best of our knowledge, is the state of the art on this dataset.", "published": "2021-02-03 00:39:09", "link": "http://arxiv.org/abs/2102.01813v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "General-Purpose Speech Representation Learning through a Self-Supervised\n  Multi-Granularity Framework", "abstract": "This paper presents a self-supervised learning framework, named MGF, for\ngeneral-purpose speech representation learning. In the design of MGF, speech\nhierarchy is taken into consideration. Specifically, we propose to use\ngenerative learning approaches to capture fine-grained information at small\ntime scales and use discriminative learning approaches to distill\ncoarse-grained or semantic information at large time scales. For phoneme-scale\nlearning, we borrow idea from the masked language model but tailor it for the\ncontinuous speech signal by replacing classification loss with a contrastive\nloss. We corroborate our design by evaluating MGF representation on various\ndownstream tasks, including phoneme classification, speaker classification,\nspeech recognition, and emotion classification. Experiments verify that\ntraining at different time scales needs different training targets and loss\nfunctions, which in general complement each other and lead to a better\nperformance.", "published": "2021-02-03 08:13:21", "link": "http://arxiv.org/abs/2102.01930v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Towards Natural and Controllable Cross-Lingual Voice Conversion Based on\n  Neural TTS Model and Phonetic Posteriorgram", "abstract": "Cross-lingual voice conversion (VC) is an important and challenging problem\ndue to significant mismatches of the phonetic set and the speech prosody of\ndifferent languages. In this paper, we build upon the neural text-to-speech\n(TTS) model, i.e., FastSpeech, and LPCNet neural vocoder to design a new\ncross-lingual VC framework named FastSpeech-VC. We address the mismatches of\nthe phonetic set and the speech prosody by applying Phonetic PosteriorGrams\n(PPGs), which have been proved to bridge across speaker and language\nboundaries. Moreover, we add normalized logarithm-scale fundamental frequency\n(Log-F0) to further compensate for the prosodic mismatches and significantly\nimprove naturalness. Our experiments on English and Mandarin languages\ndemonstrate that with only mono-lingual corpus, the proposed FastSpeech-VC can\nachieve high quality converted speech with mean opinion score (MOS) close to\nthe professional records while maintaining good speaker similarity. Compared to\nthe baselines using Tacotron2 and Transformer TTS models, the FastSpeech-VC can\nachieve controllable converted speech rate and much faster inference speed.\nMore importantly, the FastSpeech-VC can easily be adapted to a speaker with\nlimited training utterances.", "published": "2021-02-03 10:28:07", "link": "http://arxiv.org/abs/2102.01991v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Monaural Speech Enhancement with Complex Convolutional Block Attention\n  Module and Joint Time Frequency Losses", "abstract": "Deep complex U-Net structure and convolutional recurrent network (CRN)\nstructure achieve state-of-the-art performance for monaural speech enhancement.\nBoth deep complex U-Net and CRN are encoder and decoder structures with skip\nconnections, which heavily rely on the representation power of the\ncomplex-valued convolutional layers. In this paper, we propose a complex\nconvolutional block attention module (CCBAM) to boost the representation power\nof the complex-valued convolutional layers by constructing more informative\nfeatures. The CCBAM is a lightweight and general module which can be easily\nintegrated into any complex-valued convolutional layers. We integrate CCBAM\nwith the deep complex U-Net and CRN to enhance their performance for speech\nenhancement. We further propose a mixed loss function to jointly optimize the\ncomplex models in both time-frequency (TF) domain and time domain. By\nintegrating CCBAM and the mixed loss, we form a new end-to-end (E2E) complex\nspeech enhancement framework. Ablation experiments and objective evaluations\nshow the superior performance of the proposed approaches\n(https://github.com/modelscope/ClearerVoice-Studio).", "published": "2021-02-03 10:30:52", "link": "http://arxiv.org/abs/2102.01993v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Music source separation conditioned on 3D point clouds", "abstract": "Recently, significant progress has been made in audio source separation by\nthe application of deep learning techniques. Current methods that combine both\naudio and visual information use 2D representations such as images to guide the\nseparation process. However, in order to (re)-create acoustically correct\nscenes for 3D virtual/augmented reality applications from recordings of real\nmusic ensembles, detailed information about each sound source in the 3D\nenvironment is required. This demand, together with the proliferation of 3D\nvisual acquisition systems like LiDAR or rgb-depth cameras, stimulates the\ncreation of models that can guide the audio separation using 3D visual\ninformation. This paper proposes a multi-modal deep learning model to perform\nmusic source separation conditioned on 3D point clouds of music performance\nrecordings. This model extracts visual features using 3D sparse convolutions,\nwhile audio features are extracted using dense convolutions. A fusion module\ncombines the extracted features to finally perform the audio source separation.\nIt is shown, that the presented model can distinguish the musical instruments\nfrom a single 3D point cloud frame, and perform source separation qualitatively\nsimilar to a reference case, where manually assigned instrument labels are\nprovided.", "published": "2021-02-03 12:18:35", "link": "http://arxiv.org/abs/2102.02028v1", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Data Generation Using Pass-phrase-dependent Deep Auto-encoders for\n  Text-Dependent Speaker Verification", "abstract": "In this paper, we propose a novel method that trains pass-phrase specific\ndeep neural network (PP-DNN) based auto-encoders for creating augmented data\nfor text-dependent speaker verification (TD-SV). Each PP-DNN auto-encoder is\ntrained using the utterances of a particular pass-phrase available in the\ntarget enrollment set with two methods: (i) transfer learning and (ii) training\nfrom scratch. Next, feature vectors of a given utterance are fed to the PP-DNNs\nand the output from each PP-DNN at frame-level is considered one new set of\ngenerated data. The generated data from each PP-DNN is then used for building a\nTD-SV system in contrast to the conventional method that considers only the\nevaluation data available. The proposed approach can be considered as the\ntransformation of data to the pass-phrase specific space using a non-linear\ntransformation learned by each PP-DNN. The method develops several TD-SV\nsystems with the number equal to the number of PP-DNNs separately trained for\neach pass-phrases for the evaluation. Finally, the scores of the different\nTD-SV systems are fused for decision making. Experiments are conducted on the\nRedDots challenge 2016 database for TD-SV using short utterances. Results show\nthat the proposed method improves the performance for both conventional\ncepstral feature and deep bottleneck feature using both Gaussian mixture model\n- universal background model (GMM-UBM) and i-vector framework.", "published": "2021-02-03 14:06:29", "link": "http://arxiv.org/abs/2102.02074v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Downbeat Tracking with Tempo-Invariant Convolutional Neural Networks", "abstract": "The human ability to track musical downbeats is robust to changes in tempo,\nand it extends to tempi never previously encountered. We propose a\ndeterministic time-warping operation that enables this skill in a convolutional\nneural network (CNN) by allowing the network to learn rhythmic patterns\nindependently of tempo. Unlike conventional deep learning approaches, which\nlearn rhythmic patterns at the tempi present in the training dataset, the\npatterns learned in our model are tempo-invariant, leading to better tempo\ngeneralisation and more efficient usage of the network capacity. We test the\ngeneralisation property on a synthetic dataset created by rendering the Groove\nMIDI Dataset using FluidSynth, split into a training set containing the\noriginal performances and a test set containing tempo-scaled versions rendered\nwith different SoundFonts (test-time augmentation). The proposed model\ngeneralises nearly perfectly to unseen tempi (F-measure of 0.89 on both\ntraining and test sets), whereas a comparable conventional CNN achieves similar\naccuracy only for the training set (0.89) and drops to 0.54 on the test set.\nThe generalisation advantage of the proposed model extends to real music, as\nshown by results on the GTZAN and Ballroom datasets.", "published": "2021-02-03 20:25:36", "link": "http://arxiv.org/abs/2102.02282v1", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Data-Driven Approach to Violin Making", "abstract": "Of all the characteristics of a violin, those that concern its shape are\nprobably the most important ones, as the violin maker has complete control over\nthem. Contemporary violin making, however, is still based more on tradition\nthan understanding, and a definitive scientific study of the specific relations\nthat exist between shape and vibrational properties is yet to come and sorely\nmissed. In this article, using standard statistical learning tools, we show\nthat the modal frequencies of violin tops can, in fact, be predicted from\ngeometric parameters, and that artificial intelligence can be successfully\napplied to traditional violin making. We also study how modal frequencies vary\nwith the thicknesses of the plate (a process often referred to as {\\em plate\ntuning}) and discuss the complexity of this dependency. Finally, we propose a\npredictive tool for plate tuning, which takes into account material and\ngeometric parameters.", "published": "2021-02-03 00:42:08", "link": "http://arxiv.org/abs/2102.04254v1", "categories": ["cs.CE", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CE"}
