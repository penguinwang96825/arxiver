{"title": "Annotation Inconsistency and Entity Bias in MultiWOZ", "abstract": "MultiWOZ is one of the most popular multi-domain task-oriented dialog\ndatasets, containing 10K+ annotated dialogs covering eight domains. It has been\nwidely accepted as a benchmark for various dialog tasks, e.g., dialog state\ntracking (DST), natural language generation (NLG), and end-to-end (E2E) dialog\nmodeling. In this work, we identify an overlooked issue with dialog state\nannotation inconsistencies in the dataset, where a slot type is tagged\ninconsistently across similar dialogs leading to confusion for DST modeling. We\npropose an automated correction for this issue, which is present in a whopping\n70% of the dialogs. Additionally, we notice that there is significant entity\nbias in the dataset (e.g., \"cambridge\" appears in 50% of the destination cities\nin the train domain). The entity bias can potentially lead to named entity\nmemorization in generative models, which may go unnoticed as the test set\nsuffers from a similar entity bias as well. We release a new test set with all\nentities replaced with unseen entities. Finally, we benchmark joint goal\naccuracy (JGA) of the state-of-the-art DST baselines on these modified versions\nof the data. Our experiments show that the annotation inconsistency corrections\nlead to 7-10% improvement in JGA. On the other hand, we observe a 29% drop in\nJGA when models are evaluated on the new test set with unseen entities.", "published": "2021-05-29 00:09:06", "link": "http://arxiv.org/abs/2105.14150v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Label Few-Shot Learning for Aspect Category Detection", "abstract": "Aspect category detection (ACD) in sentiment analysis aims to identify the\naspect categories mentioned in a sentence. In this paper, we formulate ACD in\nthe few-shot learning scenario. However, existing few-shot learning approaches\nmainly focus on single-label predictions. These methods can not work well for\nthe ACD task since a sentence may contain multiple aspect categories.\nTherefore, we propose a multi-label few-shot learning method based on the\nprototypical network. To alleviate the noise, we design two effective attention\nmechanisms. The support-set attention aims to extract better prototypes by\nremoving irrelevant aspects. The query-set attention computes multiple\nprototype-specific representations for each query instance, which are then used\nto compute accurate distances with the corresponding prototypes. To achieve\nmulti-label inference, we further learn a dynamic threshold per instance by a\npolicy network. Extensive experimental results on three datasets demonstrate\nthat the proposed method significantly outperforms strong baselines.", "published": "2021-05-29 01:56:11", "link": "http://arxiv.org/abs/2105.14174v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Grammatical Error Correction as GAN-like Sequence Labeling", "abstract": "In Grammatical Error Correction (GEC), sequence labeling models enjoy fast\ninference compared to sequence-to-sequence models; however, inference in\nsequence labeling GEC models is an iterative process, as sentences are passed\nto the model for multiple rounds of correction, which exposes the model to\nsentences with progressively fewer errors at each round. Traditional GEC models\nlearn from sentences with fixed error rates. Coupling this with the iterative\ncorrection process causes a mismatch between training and inference that\naffects final performance. In order to address this mismatch, we propose a\nGAN-like sequence labeling model, which consists of a grammatical error\ndetector as a discriminator and a grammatical error labeler with Gumbel-Softmax\nsampling as a generator. By sampling from real error distributions, our errors\nare more genuine compared to traditional synthesized GEC errors, thus\nalleviating the aforementioned mismatch and allowing for better training. Our\nresults on several evaluation benchmarks demonstrate that our proposed approach\nis effective and improves the previous state-of-the-art baseline.", "published": "2021-05-29 04:39:40", "link": "http://arxiv.org/abs/2105.14209v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Demoting the Lead Bias in News Summarization via Alternating Adversarial\n  Learning", "abstract": "In news articles the lead bias is a common phenomenon that usually dominates\nthe learning signals for neural extractive summarizers, severely limiting their\nperformance on data with different or even no bias. In this paper, we introduce\na novel technique to demote lead bias and make the summarizer focus more on the\ncontent semantics. Experiments on two news corpora with different degrees of\nlead bias show that our method can effectively demote the model's learned lead\nbias and improve its generality on out-of-distribution data, with little to no\nperformance loss on in-distribution data.", "published": "2021-05-29 07:40:59", "link": "http://arxiv.org/abs/2105.14241v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CommitBERT: Commit Message Generation Using Pre-Trained Programming\n  Language Model", "abstract": "Commit message is a document that summarizes source code changes in natural\nlanguage. A good commit message clearly shows the source code changes, so this\nenhances collaboration between developers. Therefore, our work is to develop a\nmodel that automatically writes the commit message.\n  To this end, we release 345K datasets consisting of code modification and\ncommit messages in six programming languages (Python, PHP, Go, Java,\nJavaScript, and Ruby). Similar to the neural machine translation (NMT) model,\nusing our dataset, we feed the code modification to the encoder input and the\ncommit message to the decoder input and measure the result of the generated\ncommit message with BLEU-4.\n  Also, we propose the following two training methods to improve the result of\ngenerating the commit message: (1) A method of preprocessing the input to feed\nthe code modification to the encoder input. (2) A method that uses an initial\nweight suitable for the code domain to reduce the gap in contextual\nrepresentation between programming language (PL) and natural language (NL).\nTraining code, dataset, and pre-trained weights are available at\nhttps://github.com/graykode/commit-autosuggestions", "published": "2021-05-29 07:48:28", "link": "http://arxiv.org/abs/2105.14242v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Korean-English Machine Translation with Multiple Tokenization Strategy", "abstract": "This work was conducted to find out how tokenization methods affect the\ntraining results of machine translation models. In this work, alphabet\ntokenization, morpheme tokenization, and BPE tokenization were applied to\nKorean as the source language and English as the target language respectively,\nand the comparison experiment was conducted by repeating 50,000 epochs of each\n9 models using the Transformer neural network. As a result of measuring the\nBLEU scores of the experimental models, the model that applied BPE tokenization\nto Korean and morpheme tokenization to English recorded 35.73, showing the best\nperformance.", "published": "2021-05-29 11:31:59", "link": "http://arxiv.org/abs/2105.14274v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Grammar Accuracy Evaluation (GAE): Quantifiable Quantitative Evaluation\n  of Machine Translation Models", "abstract": "Natural Language Generation (NLG) refers to the operation of expressing the\ncalculation results of a system in human language. Since the quality of\ngenerated sentences from an NLG model cannot be fully represented using only\nquantitative evaluation, they are evaluated using qualitative evaluation by\nhumans in which the meaning or grammar of a sentence is scored according to a\nsubjective criterion. Nevertheless, the existing evaluation methods have a\nproblem as a large score deviation occurs depending on the criteria of\nevaluators. In this paper, we propose Grammar Accuracy Evaluation (GAE) that\ncan provide the specific evaluating criteria. As a result of analyzing the\nquality of machine translation by BLEU and GAE, it was confirmed that the BLEU\nscore does not represent the absolute performance of machine translation models\nand GAE compensates for the shortcomings of BLEU with flexible evaluation of\nalternative synonyms and changes in sentence structure.", "published": "2021-05-29 11:40:51", "link": "http://arxiv.org/abs/2105.14277v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling Discriminative Representations for Out-of-Domain Detection with\n  Supervised Contrastive Learning", "abstract": "Detecting Out-of-Domain (OOD) or unknown intents from user queries is\nessential in a task-oriented dialog system. A key challenge of OOD detection is\nto learn discriminative semantic features. Traditional cross-entropy loss only\nfocuses on whether a sample is correctly classified, and does not explicitly\ndistinguish the margins between categories. In this paper, we propose a\nsupervised contrastive learning objective to minimize intra-class variance by\npulling together in-domain intents belonging to the same class and maximize\ninter-class variance by pushing apart samples from different classes. Besides,\nwe employ an adversarial augmentation mechanism to obtain pseudo diverse views\nof a sample in the latent space. Experiments on two public datasets prove the\neffectiveness of our method capturing discriminative representations for OOD\ndetection.", "published": "2021-05-29 12:54:22", "link": "http://arxiv.org/abs/2105.14289v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Simple Voting Mechanism for Online Sexist Content Identification", "abstract": "This paper presents the participation of the MiniTrue team in the EXIST 2021\nChallenge on the sexism detection in social media task for English and Spanish.\nOur approach combines the language models with a simple voting mechanism for\nthe sexist label prediction. For this, three BERT based models and a voting\nfunction are used. Experimental results show that our final model with the\nvoting function has achieved the best results among our four models, which\nmeans that our voting mechanism brings an extra benefit to our system.\nNevertheless, we also observe that our system is robust to data sources and\nlanguages.", "published": "2021-05-29 14:25:20", "link": "http://arxiv.org/abs/2105.14309v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Novel Slot Detection: A Benchmark for Discovering Unknown Slot Types in\n  the Task-Oriented Dialogue System", "abstract": "Existing slot filling models can only recognize pre-defined in-domain slot\ntypes from a limited slot set. In the practical application, a reliable\ndialogue system should know what it does not know. In this paper, we introduce\na new task, Novel Slot Detection (NSD), in the task-oriented dialogue system.\nNSD aims to discover unknown or out-of-domain slot types to strengthen the\ncapability of a dialogue system based on in-domain training data. Besides, we\nconstruct two public NSD datasets, propose several strong NSD baselines, and\nestablish a benchmark for future work. Finally, we conduct exhaustive\nexperiments and qualitative analysis to comprehend key challenges and provide\nnew guidance for future directions.", "published": "2021-05-29 14:46:38", "link": "http://arxiv.org/abs/2105.14313v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NeuralLog: Natural Language Inference with Joint Neural and Logical\n  Reasoning", "abstract": "Deep learning (DL) based language models achieve high performance on various\nbenchmarks for Natural Language Inference (NLI). And at this time, symbolic\napproaches to NLI are receiving less attention. Both approaches (symbolic and\nDL) have their advantages and weaknesses. However, currently, no method\ncombines them in a system to solve the task of NLI. To merge symbolic and deep\nlearning methods, we propose an inference framework called NeuralLog, which\nutilizes both a monotonicity-based logical inference engine and a neural\nnetwork language model for phrase alignment. Our framework models the NLI task\nas a classic search problem and uses the beam search algorithm to search for\noptimal inference paths. Experiments show that our joint logic and neural\ninference system improves accuracy on the NLI task and can achieve state-of-art\naccuracy on the SICK and MED datasets.", "published": "2021-05-29 01:02:40", "link": "http://arxiv.org/abs/2105.14167v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Maintaining Common Ground in Dynamic Environments", "abstract": "Common grounding is the process of creating and maintaining mutual\nunderstandings, which is a critical aspect of sophisticated human\ncommunication. While various task settings have been proposed in existing\nliterature, they mostly focus on creating common ground under static context\nand ignore the aspect of maintaining them overtime under dynamic context. In\nthis work, we propose a novel task setting to study the ability of both\ncreating and maintaining common ground in dynamic environments. Based on our\nminimal task formulation, we collected a large-scale dataset of 5,617 dialogues\nto enable fine-grained evaluation and analysis of various dialogue systems.\nThrough our dataset analyses, we highlight novel challenges introduced in our\nsetting, such as the usage of complex spatio-temporal expressions to create and\nmaintain common ground. Finally, we conduct extensive experiments to assess the\ncapabilities of our baseline dialogue system and discuss future prospects of\nour research.", "published": "2021-05-29 04:14:29", "link": "http://arxiv.org/abs/2105.14207v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploiting Position Bias for Robust Aspect Sentiment Classification", "abstract": "Aspect sentiment classification (ASC) aims at determining sentiments\nexpressed towards different aspects in a sentence. While state-of-the-art ASC\nmodels have achieved remarkable performance, they are recently shown to suffer\nfrom the issue of robustness. Particularly in two common scenarios: when\ndomains of test and training data are different (out-of-domain scenario) or\ntest data is adversarially perturbed (adversarial scenario), ASC models may\nattend to irrelevant words and neglect opinion expressions that truly describe\ndiverse aspects. To tackle the challenge, in this paper, we hypothesize that\nposition bias (i.e., the words closer to a concerning aspect would carry a\nhigher degree of importance) is crucial for building more robust ASC models by\nreducing the probability of mis-attending. Accordingly, we propose two\nmechanisms for capturing position bias, namely position-biased weight and\nposition-biased dropout, which can be flexibly injected into existing models to\nenhance representations for classification. Experiments conducted on\nout-of-domain and adversarial datasets demonstrate that our proposed approaches\nlargely improve the robustness and effectiveness of current models.", "published": "2021-05-29 04:41:09", "link": "http://arxiv.org/abs/2105.14210v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Predictive Representation Learning for Language Modeling", "abstract": "To effectively perform the task of next-word prediction, long short-term\nmemory networks (LSTMs) must keep track of many types of information. Some\ninformation is directly related to the next word's identity, but some is more\nsecondary (e.g. discourse-level features or features of downstream words).\nCorrelates of secondary information appear in LSTM representations even though\nthey are not part of an \\emph{explicitly} supervised prediction task. In\ncontrast, in reinforcement learning (RL), techniques that explicitly supervise\nrepresentations to predict secondary information have been shown to be\nbeneficial. Inspired by that success, we propose Predictive Representation\nLearning (PRL), which explicitly constrains LSTMs to encode specific\npredictions, like those that might need to be learned implicitly. We show that\nPRL 1) significantly improves two strong language modeling methods, 2)\nconverges more quickly, and 3) performs better when data is limited. Our work\nshows that explicitly encoding a simple predictive task facilitates the search\nfor a more effective language model.", "published": "2021-05-29 05:03:47", "link": "http://arxiv.org/abs/2105.14214v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CoDesc: A Large Code-Description Parallel Dataset", "abstract": "Translation between natural language and source code can help software\ndevelopment by enabling developers to comprehend, ideate, search, and write\ncomputer programs in natural language. Despite growing interest from the\nindustry and the research community, this task is often difficult due to the\nlack of large standard datasets suitable for training deep neural models,\nstandard noise removal methods, and evaluation benchmarks. This leaves\nresearchers to collect new small-scale datasets, resulting in inconsistencies\nacross published works. In this study, we present CoDesc -- a large parallel\ndataset composed of 4.2 million Java methods and natural language descriptions.\nWith extensive analysis, we identify and remove prevailing noise patterns from\nthe dataset. We demonstrate the proficiency of CoDesc in two complementary\ntasks for code-description pairs: code summarization and code search. We show\nthat the dataset helps improve code search by up to 22\\% and achieves the new\nstate-of-the-art in code summarization. Furthermore, we show CoDesc's\neffectiveness in pre-training--fine-tuning setup, opening possibilities in\nbuilding pretrained language models for Java. To facilitate future research, we\nrelease the dataset, a data processing tool, and a benchmark at\n\\url{https://github.com/csebuetnlp/CoDesc}.", "published": "2021-05-29 05:40:08", "link": "http://arxiv.org/abs/2105.14220v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LPF: A Language-Prior Feedback Objective Function for De-biased Visual\n  Question Answering", "abstract": "Most existing Visual Question Answering (VQA) systems tend to overly rely on\nlanguage bias and hence fail to reason from the visual clue. To address this\nissue, we propose a novel Language-Prior Feedback (LPF) objective function, to\nre-balance the proportion of each answer's loss value in the total VQA loss.\nThe LPF firstly calculates a modulating factor to determine the language bias\nusing a question-only branch. Then, the LPF assigns a self-adaptive weight to\neach training sample in the training process. With this reweighting mechanism,\nthe LPF ensures that the total VQA loss can be reshaped to a more balanced\nform. By this means, the samples that require certain visual information to\npredict will be efficiently used during training. Our method is simple to\nimplement, model-agnostic, and end-to-end trainable. We conduct extensive\nexperiments and the results show that the LPF (1) brings a significant\nimprovement over various VQA models, (2) achieves competitive performance on\nthe bias-sensitive VQA-CP v2 benchmark.", "published": "2021-05-29 13:48:11", "link": "http://arxiv.org/abs/2105.14300v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Is Sluice Resolution really just Question Answering?", "abstract": "Sluice resolution is a problem where a system needs to output the\ncorresponding antecedents of wh-ellipses. The antecedents are elided contents\nbehind the wh-words but are implicitly referred to using contexts. Previous\nwork frames sluice resolution as question answering where this setting\noutperforms all its preceding works by large margins. Ellipsis and questions\nare referentially dependent expressions (anaphoras) and retrieving the\ncorresponding antecedents are like answering questions to output pieces of\nclarifying information. However, the task is not fully solved. Therefore, we\nwant to further investigate what makes sluice resolution differ to question\nanswering and fill in the error gaps. We also present some results using recent\nstate-of-the-art question answering systems which improve the previous work\n(86.01 to 90.39 F1).", "published": "2021-05-29 17:51:02", "link": "http://arxiv.org/abs/2105.14347v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Quotation Recommendation and Interpretation Based on Transformation from\n  Queries to Quotations", "abstract": "To help individuals express themselves better, quotation recommendation is\nreceiving growing attention. Nevertheless, most prior efforts focus on modeling\nquotations and queries separately and ignore the relationship between the\nquotations and the queries. In this work, we introduce a transformation matrix\nthat directly maps the query representations to quotation representations. To\nbetter learn the mapping relationship, we employ a mapping loss that minimizes\nthe distance of two semantic spaces (one for quotation and another for\nmapped-query). Furthermore, we explore using the words in history queries to\ninterpret the figurative language of quotations, where quotation-aware\nattention is applied on top of history queries to highlight the indicator\nwords. Experiments on two datasets in English and Chinese show that our model\noutperforms previous state-of-the-art models.", "published": "2021-05-29 07:25:59", "link": "http://arxiv.org/abs/2105.14189v2", "categories": ["cs.CL", "cs.AI", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Correcting public opinion trends through Bayesian data assimilation", "abstract": "Measuring public opinion is a key focus during democratic elections, enabling\ncandidates to gauge their popularity and alter their campaign strategies\naccordingly. Traditional survey polling remains the most popular estimation\ntechnique, despite its cost and time intensity, measurement errors, lack of\nreal-time capabilities and lagged representation of public opinion. In recent\nyears, Twitter opinion mining has attempted to combat these issues. Despite\nachieving promising results, it experiences its own set of shortcomings such as\nan unrepresentative sample population and a lack of long term stability. This\npaper aims to merge data from both these techniques using Bayesian data\nassimilation to arrive at a more accurate estimate of true public opinion for\nthe Brexit referendum. This paper demonstrates the effectiveness of the\nproposed approach using Twitter opinion data and survey data from trusted\npollsters. Firstly, the possible existence of a time gap of 16 days between the\ntwo data sets is identified. This gap is subsequently incorporated into a\nproposed assimilation architecture. This method was found to adequately\nincorporate information from both sources and measure a strong upward trend in\nLeave support leading up to the Brexit referendum. The proposed technique\nprovides useful estimates of true opinion, which is essential to future opinion\nmeasurement and forecasting research.", "published": "2021-05-29 11:39:56", "link": "http://arxiv.org/abs/2105.14276v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "Constructing Flow Graphs from Procedural Cybersecurity Texts", "abstract": "Following procedural texts written in natural languages is challenging. We\nmust read the whole text to identify the relevant information or identify the\ninstruction flows to complete a task, which is prone to failures. If such texts\nare structured, we can readily visualize instruction-flows, reason or infer a\nparticular step, or even build automated systems to help novice agents achieve\na goal. However, this structure recovery task is a challenge because of such\ntexts' diverse nature. This paper proposes to identify relevant information\nfrom such texts and generate information flows between sentences. We built a\nlarge annotated procedural text dataset (CTFW) in the cybersecurity domain\n(3154 documents). This dataset contains valuable instructions regarding\nsoftware vulnerability analysis experiences. We performed extensive experiments\non CTFW with our LM-GNN model variants in multiple settings. To show the\ngeneralizability of both this task and our method, we also experimented with\nprocedural texts from two other domains (Maintenance Manual and Cooking), which\nare substantially different from cybersecurity. Our experiments show that Graph\nConvolution Network with BERT sentence embeddings outperforms BERT in all three\ndomains", "published": "2021-05-29 19:06:35", "link": "http://arxiv.org/abs/2105.14357v1", "categories": ["cs.CL", "cs.AI", "cs.CR"], "primary_category": "cs.CL"}
{"title": "DPLM: A Deep Perceptual Spatial-Audio Localization Metric", "abstract": "Subjective evaluations are critical for assessing the perceptual realism of\nsounds in audio-synthesis driven technologies like augmented and virtual\nreality. However, they are challenging to set up, fatiguing for users, and\nexpensive. In this work, we tackle the problem of capturing the perceptual\ncharacteristics of localizing sounds. Specifically, we propose a framework for\nbuilding a general purpose quality metric to assess spatial localization\ndifferences between two binaural recordings. We model localization similarity\nby utilizing activation-level distances from deep networks trained for\ndirection of arrival (DOA) estimation. Our proposed metric (DPLM) outperforms\nbaseline metrics on correlation with subjective ratings on a diverse set of\ndatasets, even without the benefit of any human-labeled training data.", "published": "2021-05-29 02:38:31", "link": "http://arxiv.org/abs/2105.14180v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
