{"title": "ART: Automatic multi-step reasoning and tool-use for large language\n  models", "abstract": "Large language models (LLMs) can perform complex reasoning in few- and\nzero-shot settings by generating intermediate chain of thought (CoT) reasoning\nsteps. Further, each reasoning step can rely on external tools to support\ncomputation beyond the core LLM capabilities (e.g. search/running code). Prior\nwork on CoT prompting and tool use typically requires hand-crafting\ntask-specific demonstrations and carefully scripted interleaving of model\ngenerations with tool use. We introduce Automatic Reasoning and Tool-use (ART),\na framework that uses frozen LLMs to automatically generate intermediate\nreasoning steps as a program. Given a new task to solve, ART selects\ndemonstrations of multi-step reasoning and tool use from a task library. At\ntest time, ART seamlessly pauses generation whenever external tools are called,\nand integrates their output before resuming generation. ART achieves a\nsubstantial improvement over few-shot prompting and automatic CoT on unseen\ntasks in the BigBench and MMLU benchmarks, and matches performance of\nhand-crafted CoT prompts on a majority of these tasks. ART is also extensible,\nand makes it easy for humans to improve performance by correcting errors in\ntask-specific programs or incorporating new tools, which we demonstrate by\ndrastically improving performance on select tasks with minimal human\nintervention.", "published": "2023-03-16 01:04:45", "link": "http://arxiv.org/abs/2303.09014v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Secret-Keeping in Question Answering", "abstract": "Existing question-answering research focuses on unanswerable questions in the\ncontext of always providing an answer when a system can\\dots but what about\ncases where a system {\\bf should not} answer a question. This can either be to\nprotect sensitive users or sensitive information. Many models expose sensitive\ninformation under interrogation by an adversarial user. We seek to determine if\nit is possible to teach a question-answering system to keep a specific fact\nsecret. We design and implement a proof-of-concept architecture and through our\nevaluation determine that while possible, there are numerous directions for\nfuture research to reduce system paranoia (false positives), information\nleakage (false negatives) and extend the implementation of the work to more\ncomplex problems with preserving secrecy in the presence of information\naggregation.", "published": "2023-03-16 03:56:17", "link": "http://arxiv.org/abs/2303.09067v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Text Generation with Cooperative Training", "abstract": "Recently, there has been a surge in the use of generated data to enhance the\nperformance of downstream models, largely due to the advancements in\npre-trained language models. However, most prevailing methods trained\ngenerative and discriminative models in isolation, which left them unable to\nadapt to changes in each other. These approaches lead to generative models that\nare prone to deviating from the true data distribution and providing limited\nbenefits to discriminative models. While some works have proposed jointly\ntraining generative and discriminative language models, their methods remain\nchallenging due to the non-differentiable nature of discrete data. To overcome\nthese issues, we introduce a \\textit{self-consistent learning} framework in the\ntext field that involves training a discriminator and generator cooperatively\nin a closed-loop manner until a scoring consensus is reached. By learning\ndirectly from selected samples, our framework are able to mitigate training\ninstabilities such as mode collapse and non-convergence. Extensive experiments\non four downstream benchmarks, including AFQMC, CHIP-STS, QQP, and MRPC,\ndemonstrate the efficacy of the proposed framework.", "published": "2023-03-16 04:21:19", "link": "http://arxiv.org/abs/2303.09075v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Challenges to Evaluating the Generalization of Coreference Resolution\n  Models: A Measurement Modeling Perspective", "abstract": "It is increasingly common to evaluate the same coreference resolution (CR)\nmodel on multiple datasets. Do these multi-dataset evaluations allow us to draw\nmeaningful conclusions about model generalization? Or, do they rather reflect\nthe idiosyncrasies of a particular experimental setup (e.g., the specific\ndatasets used)? To study this, we view evaluation through the lens of\nmeasurement modeling, a framework commonly used in the social sciences for\nanalyzing the validity of measurements. By taking this perspective, we show how\nmulti-dataset evaluations risk conflating different factors concerning what,\nprecisely, is being measured. This in turn makes it difficult to draw more\ngeneralizable conclusions from these evaluations. For instance, we show that\nacross seven datasets, measurements intended to reflect CR model generalization\nare often correlated with differences in both how coreference is defined and\nhow it is operationalized; this limits our ability to draw conclusions\nregarding the ability of CR models to generalize across any singular dimension.\nWe believe the measurement modeling framework provides the needed vocabulary\nfor discussing challenges surrounding what is actually being measured by CR\nevaluations.", "published": "2023-03-16 05:32:02", "link": "http://arxiv.org/abs/2303.09092v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GLEN: General-Purpose Event Detection for Thousands of Types", "abstract": "The progress of event extraction research has been hindered by the absence of\nwide-coverage, large-scale datasets. To make event extraction systems more\naccessible, we build a general-purpose event detection dataset GLEN, which\ncovers 205K event mentions with 3,465 different types, making it more than 20x\nlarger in ontology than today's largest event dataset. GLEN is created by\nutilizing the DWD Overlay, which provides a mapping between Wikidata Qnodes and\nPropBank rolesets. This enables us to use the abundant existing annotation for\nPropBank as distant supervision. In addition, we also propose a new multi-stage\nevent detection model CEDAR specifically designed to handle the large ontology\nsize in GLEN. We show that our model exhibits superior performance compared to\na range of baselines including InstructGPT. Finally, we perform error analysis\nand show that label noise is still the largest challenge for improving\nperformance for this new dataset. Our dataset, code, and models are released at\n\\url{https://github.com/ZQS1943/GLEN}.}", "published": "2023-03-16 05:36:38", "link": "http://arxiv.org/abs/2303.09093v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Short Survey of Viewing Large Language Models in Legal Aspect", "abstract": "Large language models (LLMs) have transformed many fields, including natural\nlanguage processing, computer vision, and reinforcement learning. These models\nhave also made a significant impact in the field of law, where they are being\nincreasingly utilized to automate various legal tasks, such as legal judgement\nprediction, legal document analysis, and legal document writing. However, the\nintegration of LLMs into the legal field has also raised several legal\nproblems, including privacy concerns, bias, and explainability. In this survey,\nwe explore the integration of LLMs into the field of law. We discuss the\nvarious applications of LLMs in legal tasks, examine the legal challenges that\narise from their use, and explore the data resources that can be used to\nspecialize LLMs in the legal domain. Finally, we discuss several promising\ndirections and conclude this paper. By doing so, we hope to provide an overview\nof the current state of LLMs in law and highlight the potential benefits and\nchallenges of their integration.", "published": "2023-03-16 08:01:22", "link": "http://arxiv.org/abs/2303.09136v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tollywood Emotions: Annotation of Valence-Arousal in Telugu Song Lyrics", "abstract": "Emotion recognition from a given music track has heavily relied on acoustic\nfeatures, social tags, and metadata but is seldom focused on lyrics. There are\nno datasets of Indian language songs that contain both valence and arousal\nmanual ratings of lyrics. We present a new manually annotated dataset of Telugu\nsongs' lyrics collected from Spotify with valence and arousal annotated on a\ndiscrete scale. A fairly high inter-annotator agreement was observed for both\nvalence and arousal. Subsequently, we create two music emotion recognition\nmodels by using two classification techniques to identify valence, arousal and\nrespective emotion quadrant from lyrics. Support vector machine (SVM) with term\nfrequency-inverse document frequency (TF-IDF) features and fine-tuning the\npre-trained XLMRoBERTa (XLM-R) model were used for valence, arousal and\nquadrant classification tasks. Fine-tuned XLMRoBERTa performs better than the\nSVM by improving macro-averaged F1-scores of 54.69%, 67.61%, 34.13% to 77.90%,\n80.71% and 58.33% for valence, arousal and quadrant classifications,\nrespectively, on 10-fold cross-validation. In addition, we compare our lyrics\nannotations with Spotify's annotations of valence and energy (same as arousal),\nwhich are based on entire music tracks. The implications of our findings are\ndiscussed. Finally, we make the dataset publicly available with lyrics,\nannotations and Spotify IDs.", "published": "2023-03-16 14:47:52", "link": "http://arxiv.org/abs/2303.09364v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SheffieldVeraAI at SemEval-2023 Task 3: Mono and multilingual approaches\n  for news genre, topic and persuasion technique classification", "abstract": "This paper describes our approach for SemEval-2023 Task 3: Detecting the\ncategory, the framing, and the persuasion techniques in online news in a\nmulti-lingual setup. For Subtask 1 (News Genre), we propose an ensemble of\nfully trained and adapter mBERT models which was ranked joint-first for German,\nand had the highest mean rank of multi-language teams. For Subtask 2 (Framing),\nwe achieved first place in 3 languages, and the best average rank across all\nthe languages, by using two separate ensembles: a monolingual\nRoBERTa-MUPPETLARGE and an ensemble of XLM-RoBERTaLARGE with adapters and task\nadaptive pretraining. For Subtask 3 (Persuasion Techniques), we train a\nmonolingual RoBERTa-Base model for English and a multilingual mBERT model for\nthe remaining languages, which achieved top 10 for all languages, including 2nd\nfor English. For each subtask, we compared monolingual and multilingual\napproaches, and considered class imbalance techniques.", "published": "2023-03-16 15:54:23", "link": "http://arxiv.org/abs/2303.09421v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Jump to Conclusions: Short-Cutting Transformers With Linear\n  Transformations", "abstract": "Transformer-based language models create hidden representations of their\ninputs at every layer, but only use final-layer representations for prediction.\nThis obscures the internal decision-making process of the model and the utility\nof its intermediate representations. One way to elucidate this is to cast the\nhidden representations as final representations, bypassing the transformer\ncomputation in-between. In this work, we suggest a simple method for such\ncasting, using linear transformations. This approximation far exceeds the\nprevailing practice of inspecting hidden representations from all layers, in\nthe space of the final layer. Moreover, in the context of language modeling,\nour method produces more accurate predictions from hidden layers, across\nvarious model scales, architectures, and data distributions. This allows\n\"peeking\" into intermediate representations, showing that GPT-2 and BERT often\npredict the final output already in early layers. We then demonstrate the\npracticality of our method to recent early exit strategies, showing that when\naiming, for example, at retention of 95% accuracy, our approach saves\nadditional 7.9% layers for GPT-2 and 5.4% layers for BERT. Last, we extend our\nmethod to linearly approximate sub-modules, finding that attention is most\ntolerant to this change. Our code and learned mappings are publicly available\nat https://github.com/sashayd/mat.", "published": "2023-03-16 16:10:16", "link": "http://arxiv.org/abs/2303.09435v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Architecture Search for Effective Teacher-Student Knowledge\n  Transfer in Language Models", "abstract": "Large pretrained language models have achieved state-of-the-art results on a\nvariety of downstream tasks. Knowledge Distillation (KD) into a smaller student\nmodel addresses their inefficiency, allowing for deployment in\nresource-constrained environments. However, KD can be ineffective when the\nstudent is manually selected from a set of existing options, since it can be a\nsub-optimal choice within the space of all possible student architectures. We\ndevelop multilingual KD-NAS, the use of Neural Architecture Search (NAS) guided\nby KD to find the optimal student architecture for task agnostic distillation\nfrom a multilingual teacher. In each episode of the search process, a NAS\ncontroller predicts a reward based on the distillation loss and latency of\ninference. The top candidate architectures are then distilled from the teacher\non a small proxy set. Finally the architecture(s) with the highest reward is\nselected, and distilled on the full training corpus. KD-NAS can automatically\ntrade off efficiency and effectiveness, and recommends architectures suitable\nto various latency budgets. Using our multi-layer hidden state distillation\nprocess, our KD-NAS student model achieves a 7x speedup on CPU inference (2x on\nGPU) compared to a XLM-Roberta Base Teacher, while maintaining 90% performance,\nand has been deployed in 3 software offerings requiring large throughput, low\nlatency and deployment on CPU.", "published": "2023-03-16 20:39:44", "link": "http://arxiv.org/abs/2303.09639v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Block-wise Bit-Compression of Transformer-based Models", "abstract": "With the popularity of the recent Transformer-based models represented by\nBERT, GPT-3 and ChatGPT, there has been state-of-the-art performance in a range\nof natural language processing tasks. However, the massive computations, huge\nmemory footprint, and thus high latency of Transformer-based models is an\ninevitable challenge for the cloud with high real-time requirement. To tackle\nthe issue, we propose BBCT, a method of block-wise bit-compression for\ntransformer without retraining. Our method achieves more fine-grained\ncompression of the whole transformer, including embedding, matrix\nmultiplication, GELU, softmax, layer normalization, and all the intermediate\nresults. As a case, we compress an efficient BERT with the method of BBCT. Our\nbenchmark test results on General Language Understanding Evaluation (GLUE) show\nthat BBCT can achieve less than 1% accuracy drop in most tasks.", "published": "2023-03-16 09:53:57", "link": "http://arxiv.org/abs/2303.09184v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BanglaCoNER: Towards Robust Bangla Complex Named Entity Recognition", "abstract": "Named Entity Recognition (NER) is a fundamental task in natural language\nprocessing that involves identifying and classifying named entities in text.\nBut much work hasn't been done for complex named entity recognition in Bangla,\ndespite being the seventh most spoken language globally. CNER is a more\nchallenging task than traditional NER as it involves identifying and\nclassifying complex and compound entities, which are not common in Bangla\nlanguage. In this paper, we present the winning solution of Bangla Complex\nNamed Entity Recognition Challenge - addressing the CNER task on BanglaCoNER\ndataset using two different approaches, namely Conditional Random Fields (CRF)\nand finetuning transformer based Deep Learning models such as BanglaBERT.\n  The dataset consisted of 15300 sentences for training and 800 sentences for\nvalidation, in the .conll format. Exploratory Data Analysis (EDA) on the\ndataset revealed that the dataset had 7 different NER tags, with notable\npresence of English words, suggesting that the dataset is synthetic and likely\na product of translation.\n  We experimented with a variety of feature combinations including Part of\nSpeech (POS) tags, word suffixes, Gazetteers, and cluster information from\nembeddings, while also finetuning the BanglaBERT (large) model for NER. We\nfound that not all linguistic patterns are immediately apparent or even\nintuitive to humans, which is why Deep Learning based models has proved to be\nthe more effective model in NLP, including CNER task. Our fine tuned BanglaBERT\n(large) model achieves an F1 Score of 0.79 on the validation set. Overall, our\nstudy highlights the importance of Bangla Complex Named Entity Recognition,\nparticularly in the context of synthetic datasets. Our findings also\ndemonstrate the efficacy of Deep Learning models such as BanglaBERT for NER in\nBangla language.", "published": "2023-03-16 13:31:31", "link": "http://arxiv.org/abs/2303.09306v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Can Generative Pre-trained Transformers (GPT) Pass Assessments in Higher\n  Education Programming Courses?", "abstract": "We evaluated the capability of generative pre-trained transformers (GPT), to\npass assessments in introductory and intermediate Python programming courses at\nthe postsecondary level. Discussions of potential uses (e.g., exercise\ngeneration, code explanation) and misuses (e.g., cheating) of this emerging\ntechnology in programming education have intensified, but to date there has not\nbeen a rigorous analysis of the models' capabilities in the realistic context\nof a full-fledged programming course with diverse set of assessment\ninstruments. We evaluated GPT on three Python courses that employ assessments\nranging from simple multiple-choice questions (no code involved) to complex\nprogramming projects with code bases distributed into multiple files (599\nexercises overall). Further, we studied if and how successfully GPT models\nleverage feedback provided by an auto-grader. We found that the current models\nare not capable of passing the full spectrum of assessments typically involved\nin a Python programming course (<70% on even entry-level modules). Yet, it is\nclear that a straightforward application of these easily accessible models\ncould enable a learner to obtain a non-trivial portion of the overall available\nscore (>55%) in introductory and intermediate courses alike. While the models\nexhibit remarkable capabilities, including correcting solutions based on\nauto-grader's feedback, some limitations exist (e.g., poor handling of\nexercises requiring complex chains of reasoning steps). These findings can be\nleveraged by instructors wishing to adapt their assessments so that GPT becomes\na valuable assistant for a learner as opposed to an end-to-end solution.", "published": "2023-03-16 13:58:45", "link": "http://arxiv.org/abs/2303.09325v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "The Scope of In-Context Learning for the Extraction of Medical Temporal\n  Constraints", "abstract": "Medications often impose temporal constraints on everyday patient activity.\nViolations of such medical temporal constraints (MTCs) lead to a lack of\ntreatment adherence, in addition to poor health outcomes and increased\nhealthcare expenses. These MTCs are found in drug usage guidelines (DUGs) in\nboth patient education materials and clinical texts. Computationally\nrepresenting MTCs in DUGs will advance patient-centric healthcare applications\nby helping to define safe patient activity patterns. We define a novel taxonomy\nof MTCs found in DUGs and develop a novel context-free grammar (CFG) based\nmodel to computationally represent MTCs from unstructured DUGs. Additionally,\nwe release three new datasets with a combined total of N = 836 DUGs labeled\nwith normalized MTCs. We develop an in-context learning (ICL) solution for\nautomatically extracting and normalizing MTCs found in DUGs, achieving an\naverage F1 score of 0.62 across all datasets. Finally, we rigorously\ninvestigate ICL model performance against a baseline model, across datasets and\nMTC types, and through in-depth error analysis.", "published": "2023-03-16 14:51:44", "link": "http://arxiv.org/abs/2303.09366v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Revealing Weaknesses of Vietnamese Language Models Through Unanswerable\n  Questions in Machine Reading Comprehension", "abstract": "Although the curse of multilinguality significantly restricts the language\nabilities of multilingual models in monolingual settings, researchers now still\nhave to rely on multilingual models to develop state-of-the-art systems in\nVietnamese Machine Reading Comprehension. This difficulty in researching is\nbecause of the limited number of high-quality works in developing Vietnamese\nlanguage models. In order to encourage more work in this research field, we\npresent a comprehensive analysis of language weaknesses and strengths of\ncurrent Vietnamese monolingual models using the downstream task of Machine\nReading Comprehension. From the analysis results, we suggest new directions for\ndeveloping Vietnamese language models. Besides this main contribution, we also\nsuccessfully reveal the existence of artifacts in Vietnamese Machine Reading\nComprehension benchmarks and suggest an urgent need for new high-quality\nbenchmarks to track the progress of Vietnamese Machine Reading Comprehension.\nMoreover, we also introduced a minor but valuable modification to the process\nof annotating unanswerable questions for Machine Reading Comprehension from\nprevious work. Our proposed modification helps improve the quality of\nunanswerable questions to a higher level of difficulty for Machine Reading\nComprehension systems to solve.", "published": "2023-03-16 20:32:58", "link": "http://arxiv.org/abs/2303.13355v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards the Scalable Evaluation of Cooperativeness in Language Models", "abstract": "It is likely that AI systems driven by pre-trained language models (PLMs)\nwill increasingly be used to assist humans in high-stakes interactions with\nother agents, such as negotiation or conflict resolution. Consistent with the\ngoals of Cooperative AI \\citep{dafoe_open_2020}, we wish to understand and\nshape the multi-agent behaviors of PLMs in a pro-social manner. An important\nfirst step is the evaluation of model behaviour across diverse cooperation\nproblems. Since desired behaviour in an interaction depends upon precise\ngame-theoretic structure, we focus on generating scenarios with particular\nstructures with both crowdworkers and a language model. Our work proceeds as\nfollows. First, we discuss key methodological issues in the generation of\nscenarios corresponding to particular game-theoretic structures. Second, we\nemploy both crowdworkers and a language model to generate such scenarios. We\nfind that the quality of generations tends to be mediocre in both cases. We\nadditionally get both crowdworkers and a language model to judge whether given\nscenarios align with their intended game-theoretic structure, finding mixed\nresults depending on the game. Third, we provide a dataset of scenario based on\nour data generated. We provide both quantitative and qualitative evaluations of\nUnifiedQA and GPT-3 on this dataset. We find that instruct-tuned models tend to\nact in a way that could be perceived as cooperative when scaled up, while other\nmodels seemed to have flat scaling trends.", "published": "2023-03-16 15:34:23", "link": "http://arxiv.org/abs/2303.13360v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "How well do Large Language Models perform in Arithmetic tasks?", "abstract": "Large language models have emerged abilities including chain-of-thought to\nanswer math word problems step by step. Solving math word problems not only\nrequires abilities to disassemble problems via chain-of-thought but also needs\nto calculate arithmetic expressions correctly for each step. To the best of our\nknowledge, there is no work to focus on evaluating the arithmetic ability of\nlarge language models. In this work, we propose an arithmetic dataset MATH 401\nto test the latest large language models including GPT-4, ChatGPT, InstrctGPT,\nGalactica, and LLaMA with various arithmetic expressions and provide a detailed\nanalysis of the ability of large language models. MATH 401 and evaluation codes\nare released at \\url{https://github.com/GanjinZero/math401-llm}.", "published": "2023-03-16 09:28:15", "link": "http://arxiv.org/abs/2304.02015v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Picture is Worth a Thousand Words: Language Models Plan from Pixels", "abstract": "Planning is an important capability of artificial agents that perform\nlong-horizon tasks in real-world environments. In this work, we explore the use\nof pre-trained language models (PLMs) to reason about plan sequences from text\ninstructions in embodied visual environments. Prior PLM based approaches for\nplanning either assume observations are available in the form of text (e.g.,\nprovided by a captioning model), reason about plans from the instruction alone,\nor incorporate information about the visual environment in limited ways (such\nas a pre-trained affordance function). In contrast, we show that PLMs can\naccurately plan even when observations are directly encoded as input prompts\nfor the PLM. We show that this simple approach outperforms prior approaches in\nexperiments on the ALFWorld and VirtualHome benchmarks.", "published": "2023-03-16 02:02:18", "link": "http://arxiv.org/abs/2303.09031v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Translating Radiology Reports into Plain Language using ChatGPT and\n  GPT-4 with Prompt Learning: Promising Results, Limitations, and Potential", "abstract": "The large language model called ChatGPT has drawn extensively attention\nbecause of its human-like expression and reasoning abilities. In this study, we\ninvestigate the feasibility of using ChatGPT in experiments on using ChatGPT to\ntranslate radiology reports into plain language for patients and healthcare\nproviders so that they are educated for improved healthcare. Radiology reports\nfrom 62 low-dose chest CT lung cancer screening scans and 76 brain MRI\nmetastases screening scans were collected in the first half of February for\nthis study. According to the evaluation by radiologists, ChatGPT can\nsuccessfully translate radiology reports into plain language with an average\nscore of 4.27 in the five-point system with 0.08 places of information missing\nand 0.07 places of misinformation. In terms of the suggestions provided by\nChatGPT, they are general relevant such as keeping following-up with doctors\nand closely monitoring any symptoms, and for about 37% of 138 cases in total\nChatGPT offers specific suggestions based on findings in the report. ChatGPT\nalso presents some randomness in its responses with occasionally\nover-simplified or neglected information, which can be mitigated using a more\ndetailed prompt. Furthermore, ChatGPT results are compared with a newly\nreleased large model GPT-4, showing that GPT-4 can significantly improve the\nquality of translated reports. Our results show that it is feasible to utilize\nlarge language models in clinical education, and further efforts are needed to\naddress limitations and maximize their potential.", "published": "2023-03-16 02:21:39", "link": "http://arxiv.org/abs/2303.09038v3", "categories": ["cs.CL", "cs.AI", "physics.med-ph"], "primary_category": "cs.CL"}
{"title": "Patch-Prompt Aligned Bayesian Prompt Tuning for Vision-Language Models", "abstract": "For downstream applications of vision-language pre-trained models, there has\nbeen significant interest in constructing effective prompts. Existing works on\nprompt engineering, which either require laborious manual designs or optimize\nthe prompt tuning as a point estimation problem, may fail to describe diverse\ncharacteristics of categories and limit their applications. We introduce a\nBayesian probabilistic resolution to prompt tuning, where the label-specific\nstochastic prompts are generated hierarchically by first sampling a latent\nvector from an underlying distribution and then employing a lightweight\ngenerative model. Importantly, we semantically regularize the tuning process by\nminimizing the statistical distance between the visual patches and linguistic\nprompts, which pushes the stochastic label representations to faithfully\ncapture diverse visual concepts, instead of overfitting the training\ncategories. We evaluate the effectiveness of our approach on four tasks:\nfew-shot image recognition, base-to-new generalization, dataset transfer\nlearning, and domain shifts. Extensive results over 15 datasets show promising\ntransferability and generalization performance of our proposed model, both\nquantitatively and qualitatively.", "published": "2023-03-16 06:09:15", "link": "http://arxiv.org/abs/2303.09100v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Exploring Distributional Shifts in Large Language Models for Code\n  Analysis", "abstract": "We systematically study how three large language models with code\ncapabilities - CodeT5, Codex, and ChatGPT - generalize to out-of-domain data.\nWe consider two fundamental applications - code summarization, and code\ngeneration. We split data into domains following its natural boundaries - by an\norganization, by a project, and by a module within the software project. We\nestablish that samples from each new domain present all the models with a\nsignificant challenge of distribution shift. We study how established methods\nadapt models to better generalize to new domains. Our experiments show that\nwhile multitask learning alone is a reasonable baseline, combining it with\nfew-shot finetuning on examples retrieved from training data can achieve very\nstrong performance. Moreover, this solution can outperform direct finetuning\nfor very low-data scenarios. Finally, we consider variations of this approach\nto create a more broadly applicable method to adapt to multiple domains at\nonce. We find that for code generation, a model adapted to multiple domains\nsimultaneously performs on par with those adapted to a single domain", "published": "2023-03-16 07:45:46", "link": "http://arxiv.org/abs/2303.09128v2", "categories": ["cs.CL", "cs.LG", "cs.SE"], "primary_category": "cs.CL"}
{"title": "SmartBERT: A Promotion of Dynamic Early Exiting Mechanism for\n  Accelerating BERT Inference", "abstract": "Dynamic early exiting has been proven to improve the inference speed of the\npre-trained language model like BERT. However, all samples must go through all\nconsecutive layers before early exiting and more complex samples usually go\nthrough more layers, which still exists redundant computation. In this paper,\nwe propose a novel dynamic early exiting combined with layer skipping for BERT\ninference named SmartBERT, which adds a skipping gate and an exiting operator\ninto each layer of BERT. SmartBERT can adaptively skip some layers and\nadaptively choose whether to exit. Besides, we propose cross-layer contrastive\nlearning and combine it into our training phases to boost the intermediate\nlayers and classifiers which would be beneficial for early exiting. To keep the\nconsistent usage of skipping gates between training and inference phases, we\npropose a hard weight mechanism during training phase. We conduct experiments\non eight classification datasets of the GLUE benchmark. Experimental results\nshow that SmartBERT achieves 2-3x computation reduction with minimal accuracy\ndrops compared with BERT and our method outperforms previous methods in both\nefficiency and accuracy. Moreover, in some complex datasets like RTE and WNLI,\nwe prove that the early exiting based on entropy hardly works, and the skipping\nmechanism is essential for reducing computation.", "published": "2023-03-16 12:44:16", "link": "http://arxiv.org/abs/2303.09266v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Trustera: A Live Conversation Redaction System", "abstract": "Trustera, the first functional system that redacts personally identifiable\ninformation (PII) in real-time spoken conversations to remove agents' need to\nhear sensitive information while preserving the naturalness of live\ncustomer-agent conversations. As opposed to post-call redaction, audio masking\nstarts as soon as the customer begins speaking to a PII entity. This\nsignificantly reduces the risk of PII being intercepted or stored in insecure\ndata storage. Trustera's architecture consists of a pipeline of automatic\nspeech recognition, natural language understanding, and a live audio redactor\nmodule. The system's goal is three-fold: redact entities that are PII, mask the\naudio that goes to the agent, and at the same time capture the entity, so that\nthe captured PII can be used for a payment transaction or caller\nidentification. Trustera is currently being used by thousands of agents to\nsecure customers' sensitive information.", "published": "2023-03-16 16:13:36", "link": "http://arxiv.org/abs/2303.09438v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "P+: Extended Textual Conditioning in Text-to-Image Generation", "abstract": "We introduce an Extended Textual Conditioning space in text-to-image models,\nreferred to as $P+$. This space consists of multiple textual conditions,\nderived from per-layer prompts, each corresponding to a layer of the denoising\nU-net of the diffusion model.\n  We show that the extended space provides greater disentangling and control\nover image synthesis. We further introduce Extended Textual Inversion (XTI),\nwhere the images are inverted into $P+$, and represented by per-layer tokens.\n  We show that XTI is more expressive and precise, and converges faster than\nthe original Textual Inversion (TI) space. The extended inversion method does\nnot involve any noticeable trade-off between reconstruction and editability and\ninduces more regular inversions.\n  We conduct a series of extensive experiments to analyze and understand the\nproperties of the new space, and to showcase the effectiveness of our method\nfor personalizing text-to-image models. Furthermore, we utilize the unique\nproperties of this space to achieve previously unattainable results in\nobject-style mixing using text-to-image models. Project page:\nhttps://prompt-plus.github.io", "published": "2023-03-16 17:38:15", "link": "http://arxiv.org/abs/2303.09522v3", "categories": ["cs.CV", "cs.CL", "cs.GR", "cs.LG"], "primary_category": "cs.CV"}
{"title": "MultiModal Bias: Introducing a Framework for Stereotypical Bias\n  Assessment beyond Gender and Race in Vision Language Models", "abstract": "Recent breakthroughs in self supervised training have led to a new class of\npretrained vision language models. While there have been investigations of bias\nin multimodal models, they have mostly focused on gender and racial bias,\ngiving much less attention to other relevant groups, such as minorities with\nregard to religion, nationality, sexual orientation, or disabilities. This is\nmainly due to lack of suitable benchmarks for such groups. We seek to address\nthis gap by providing a visual and textual bias benchmark called MMBias,\nconsisting of around 3,800 images and phrases covering 14 population subgroups.\nWe utilize this dataset to assess bias in several prominent self supervised\nmultimodal models, including CLIP, ALBEF, and ViLT. Our results show that these\nmodels demonstrate meaningful bias favoring certain groups. Finally, we\nintroduce a debiasing method designed specifically for such large pre-trained\nmodels that can be applied as a post-processing step to mitigate bias, while\npreserving the remaining accuracy of the model.", "published": "2023-03-16 17:36:37", "link": "http://arxiv.org/abs/2303.12734v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Psychotherapy AI Companion with Reinforcement Learning Recommendations\n  and Interpretable Policy Dynamics", "abstract": "We introduce a Reinforcement Learning Psychotherapy AI Companion that\ngenerates topic recommendations for therapists based on patient responses. The\nsystem uses Deep Reinforcement Learning (DRL) to generate multi-objective\npolicies for four different psychiatric conditions: anxiety, depression,\nschizophrenia, and suicidal cases. We present our experimental results on the\naccuracy of recommended topics using three different scales of working alliance\nratings: task, bond, and goal. We show that the system is able to capture the\nreal data (historical topics discussed by the therapists) relatively well, and\nthat the best performing models vary by disorder and rating scale. To gain\ninterpretable insights into the learned policies, we visualize policy\ntrajectories in a 2D principal component analysis space and transition\nmatrices. These visualizations reveal distinct patterns in the policies trained\nwith different reward signals and trained on different clinical diagnoses. Our\nsystem's success in generating DIsorder-Specific Multi-Objective Policies\n(DISMOP) and interpretable policy dynamics demonstrates the potential of DRL in\nproviding personalized and efficient therapeutic recommendations.", "published": "2023-03-16 19:01:29", "link": "http://arxiv.org/abs/2303.09601v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.HC", "q-bio.NC"], "primary_category": "cs.LG"}
{"title": "HIVE: Harnessing Human Feedback for Instructional Visual Editing", "abstract": "Incorporating human feedback has been shown to be crucial to align text\ngenerated by large language models to human preferences. We hypothesize that\nstate-of-the-art instructional image editing models, where outputs are\ngenerated based on an input image and an editing instruction, could similarly\nbenefit from human feedback, as their outputs may not adhere to the correct\ninstructions and preferences of users. In this paper, we present a novel\nframework to harness human feedback for instructional visual editing (HIVE).\nSpecifically, we collect human feedback on the edited images and learn a reward\nfunction to capture the underlying user preferences. We then introduce scalable\ndiffusion model fine-tuning methods that can incorporate human preferences\nbased on the estimated reward. Besides, to mitigate the bias brought by the\nlimitation of data, we contribute a new 1M training dataset, a 3.6K reward\ndataset for rewards learning, and a 1K evaluation dataset to boost the\nperformance of instructional image editing. We conduct extensive empirical\nexperiments quantitatively and qualitatively, showing that HIVE is favored over\nprevious state-of-the-art instructional image editing approaches by a large\nmargin.", "published": "2023-03-16 19:47:41", "link": "http://arxiv.org/abs/2303.09618v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.HC", "cs.LG"], "primary_category": "cs.CV"}
{"title": "TriAAN-VC: Triple Adaptive Attention Normalization for Any-to-Any Voice\n  Conversion", "abstract": "Voice Conversion (VC) must be achieved while maintaining the content of the\nsource speech and representing the characteristics of the target speaker. The\nexisting methods do not simultaneously satisfy the above two aspects of VC, and\ntheir conversion outputs suffer from a trade-off problem between maintaining\nsource contents and target characteristics. In this study, we propose Triple\nAdaptive Attention Normalization VC (TriAAN-VC), comprising an encoder-decoder\nand an attention-based adaptive normalization block, that can be applied to\nnon-parallel any-to-any VC. The proposed adaptive normalization block extracts\ntarget speaker representations and achieves conversion while minimizing the\nloss of the source content with siamese loss. We evaluated TriAAN-VC on the\nVCTK dataset in terms of the maintenance of the source content and target\nspeaker similarity. Experimental results for one-shot VC suggest that TriAAN-VC\nachieves state-of-the-art performance while mitigating the trade-off problem\nencountered in the existing VC methods.", "published": "2023-03-16 03:13:58", "link": "http://arxiv.org/abs/2303.09057v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DistillW2V2: A Small and Streaming Wav2vec 2.0 Based ASR Model", "abstract": "Wav2vec 2.0 (W2V2) has shown impressive performance in automatic speech\nrecognition (ASR). However, the large model size and the non-streaming\narchitecture make it hard to be used under low-resource or streaming scenarios.\nIn this work, we propose a two-stage knowledge distillation method to solve\nthese two problems: the first step is to make the big and non-streaming teacher\nmodel smaller, and the second step is to make it streaming. Specially, we adopt\nthe MSE loss for the distillation of hidden layers and the modified LF-MMI loss\nfor the distillation of the prediction layer. Experiments are conducted on\nGigaspeech, Librispeech, and an in-house dataset. The results show that the\ndistilled student model (DistillW2V2) we finally get is 8x faster and 12x\nsmaller than the original teacher model. For the 480ms latency setup, the\nDistillW2V2's relative word error rate (WER) degradation varies from 9% to\n23.4% on test sets, which reveals a promising way to extend the W2V2's\napplication scope.", "published": "2023-03-16 12:59:17", "link": "http://arxiv.org/abs/2303.09278v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Taming Diffusion Models for Audio-Driven Co-Speech Gesture Generation", "abstract": "Animating virtual avatars to make co-speech gestures facilitates various\napplications in human-machine interaction. The existing methods mainly rely on\ngenerative adversarial networks (GANs), which typically suffer from notorious\nmode collapse and unstable training, thus making it difficult to learn accurate\naudio-gesture joint distributions. In this work, we propose a novel\ndiffusion-based framework, named Diffusion Co-Speech Gesture (DiffGesture), to\neffectively capture the cross-modal audio-to-gesture associations and preserve\ntemporal coherence for high-fidelity audio-driven co-speech gesture generation.\nSpecifically, we first establish the diffusion-conditional generation process\non clips of skeleton sequences and audio to enable the whole framework. Then, a\nnovel Diffusion Audio-Gesture Transformer is devised to better attend to the\ninformation from multiple modalities and model the long-term temporal\ndependency. Moreover, to eliminate temporal inconsistency, we propose an\neffective Diffusion Gesture Stabilizer with an annealed noise sampling\nstrategy. Benefiting from the architectural advantages of diffusion models, we\nfurther incorporate implicit classifier-free guidance to trade off between\ndiversity and gesture quality. Extensive experiments demonstrate that\nDiffGesture achieves state-of-theart performance, which renders coherent\ngestures with better mode coverage and stronger audio correlations. Code is\navailable at https://github.com/Advocate99/DiffGesture.", "published": "2023-03-16 07:32:31", "link": "http://arxiv.org/abs/2303.09119v2", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Development of a Voice Controlled Robotic Arm", "abstract": "This paper describes a robotic arm with 5 degrees-of-freedom (DOF) which is\ncontrolled by human voice and has been developed in the Mechatronics\nLaboratory, CUET. This robotic arm is interfaced with a PC by serial\ncommunication (RS-232). Users' voice command is captured by a microphone, and\nthis voice is processed by software which is made by Microsoft visual studio.\nThen the specific signal (obtained by signal processing) is sent to control\nunit. The main control unit that is used in the robotic arm is a\nmicrocontroller whose model no. is PIC18f452. Then Control unit drives the\nactuators, (Hitec HS-422, HS-81) according to the signal or signals to give\nrequired motion of the robotic arm. At present the robotic arm can perform a\nset action like pick & pull, gripping, holding & releasing, and some other\nextra function like dance-like movement, and can turn according to the voice\ncommands.", "published": "2023-03-16 20:53:44", "link": "http://arxiv.org/abs/2303.09645v1", "categories": ["cs.RO", "cs.SD", "eess.AS"], "primary_category": "cs.RO"}
{"title": "Visual Information Matters for ASR Error Correction", "abstract": "Aiming to improve the Automatic Speech Recognition (ASR) outputs with a\npost-processing step, ASR error correction (EC) techniques have been widely\ndeveloped due to their efficiency in using parallel text data. Previous works\nmainly focus on using text or/ and speech data, which hinders the performance\ngain when not only text and speech information, but other modalities, such as\nvisual information are critical for EC. The challenges are mainly two folds:\none is that previous work fails to emphasize visual information, thus rare\nexploration has been studied. The other is that the community lacks a\nhigh-quality benchmark where visual information matters for the EC models.\nTherefore, this paper provides 1) simple yet effective methods, namely gated\nfusion and image captions as prompts to incorporate visual information to help\nEC; 2) large-scale benchmark datasets, namely Visual-ASR-EC, where each item in\nthe training data consists of visual, speech, and text information, and the\ntest data are carefully selected by human annotators to ensure that even humans\ncould make mistakes when visual information is missing. Experimental results\nshow that using captions as prompts could effectively use the visual\ninformation and surpass state-of-the-art methods by upto 1.2% in Word Error\nRate(WER), which also indicates that visual information is critical in our\nproposed Visual-ASR-EC dataset", "published": "2023-03-16 06:33:53", "link": "http://arxiv.org/abs/2303.10160v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improving Perceptual Quality, Intelligibility, and Acoustics on VoIP\n  Platforms", "abstract": "In this paper, we present a method for fine-tuning models trained on the Deep\nNoise Suppression (DNS) 2020 Challenge to improve their performance on Voice\nover Internet Protocol (VoIP) applications. Our approach involves adapting the\nDNS 2020 models to the specific acoustic characteristics of VoIP\ncommunications, which includes distortion and artifacts caused by compression,\ntransmission, and platform-specific processing. To this end, we propose a\nmulti-task learning framework for VoIP-DNS that jointly optimizes noise\nsuppression and VoIP-specific acoustics for speech enhancement. We evaluate our\napproach on a diverse VoIP scenarios and show that it outperforms both industry\nperformance and state-of-the-art methods for speech enhancement on VoIP\napplications. Our results demonstrate the potential of models trained on\nDNS-2020 to be improved and tailored to different VoIP platforms using\nVoIP-DNS, whose findings have important applications in areas such as speech\nrecognition, voice assistants, and telecommunication.", "published": "2023-03-16 02:36:02", "link": "http://arxiv.org/abs/2303.09048v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
