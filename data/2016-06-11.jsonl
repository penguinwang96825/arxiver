{"title": "Data Recombination for Neural Semantic Parsing", "abstract": "Modeling crisp logical regularities is crucial in semantic parsing, making it\ndifficult for neural models with no task-specific prior knowledge to achieve\ngood results. In this paper, we introduce data recombination, a novel framework\nfor injecting such prior knowledge into a model. From the training data, we\ninduce a high-precision synchronous context-free grammar, which captures\nimportant conditional independence properties commonly found in semantic\nparsing. We then train a sequence-to-sequence recurrent network (RNN) model\nwith a novel attention-based copying mechanism on datapoints sampled from this\ngrammar, thereby teaching the model about these structural properties. Data\nrecombination improves the accuracy of our RNN model on three semantic parsing\ndatasets, leading to new state-of-the-art performance on the standard GeoQuery\ndataset for models with comparable supervision.", "published": "2016-06-11 20:34:09", "link": "http://arxiv.org/abs/1606.03622v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Natural Language Generation in Dialogue using Lexicalized and\n  Delexicalized Data", "abstract": "Natural language generation plays a critical role in spoken dialogue systems.\nWe present a new approach to natural language generation for task-oriented\ndialogue using recurrent neural networks in an encoder-decoder framework. In\ncontrast to previous work, our model uses both lexicalized and delexicalized\ncomponents i.e. slot-value pairs for dialogue acts, with slots and\ncorresponding values aligned together. This allows our model to learn from all\navailable data including the slot-value pairing, rather than being restricted\nto delexicalized slots. We show that this helps our model generate more natural\nsentences with better grammar. We further improve our model's performance by\ntransferring weights learnt from a pretrained sentence auto-encoder. Human\nevaluation of our best-performing model indicates that it generates sentences\nwhich users find more appealing.", "published": "2016-06-11 21:24:43", "link": "http://arxiv.org/abs/1606.03632v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Human Attention in Visual Question Answering: Do Humans and Deep\n  Networks Look at the Same Regions?", "abstract": "We conduct large-scale studies on `human attention' in Visual Question\nAnswering (VQA) to understand where humans choose to look to answer questions\nabout images. We design and test multiple game-inspired novel\nattention-annotation interfaces that require the subject to sharpen regions of\na blurred image to answer a question. Thus, we introduce the VQA-HAT (Human\nATtention) dataset. We evaluate attention maps generated by state-of-the-art\nVQA models against human attention both qualitatively (via visualizations) and\nquantitatively (via rank-order correlation). Overall, our experiments show that\ncurrent attention models in VQA do not seem to be looking at the same regions\nas humans.", "published": "2016-06-11 05:41:10", "link": "http://arxiv.org/abs/1606.03556v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Word Sense Disambiguation using a Bidirectional LSTM", "abstract": "In this paper we present a clean, yet effective, model for word sense\ndisambiguation. Our approach leverage a bidirectional long short-term memory\nnetwork which is shared between all words. This enables the model to share\nstatistical strength and to scale well with vocabulary size. The model is\ntrained end-to-end, directly from the raw text to sense labels, and makes\neffective use of word order. We evaluate our approach on two standard datasets,\nusing identical hyperparameter settings, which are in turn tuned on a third set\nof held out data. We employ no external resources (e.g. knowledge graphs,\npart-of-speech tagging, etc), language specific features, or hand crafted\nrules, but still achieve statistically equivalent results to the best\nstate-of-the-art systems, that employ no such limitations.", "published": "2016-06-11 08:12:02", "link": "http://arxiv.org/abs/1606.03568v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
