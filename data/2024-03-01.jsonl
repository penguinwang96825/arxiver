{"title": "A Semantic Distance Metric Learning approach for Lexical Semantic Change\n  Detection", "abstract": "Detecting temporal semantic changes of words is an important task for various\nNLP applications that must make time-sensitive predictions. Lexical Semantic\nChange Detection (SCD) task involves predicting whether a given target word,\n$w$, changes its meaning between two different text corpora, $C_1$ and $C_2$.\nFor this purpose, we propose a supervised two-staged SCD method that uses\nexisting Word-in-Context (WiC) datasets. In the first stage, for a target word\n$w$, we learn two sense-aware encoders that represent the meaning of $w$ in a\ngiven sentence selected from a corpus. Next, in the second stage, we learn a\nsense-aware distance metric that compares the semantic representations of a\ntarget word across all of its occurrences in $C_1$ and $C_2$. Experimental\nresults on multiple benchmark datasets for SCD show that our proposed method\nachieves strong performance in multiple languages. Additionally, our method\nachieves significant improvements on WiC benchmarks compared to a sense-aware\nencoder with conventional distance functions. Source code is available at\nhttps://github.com/LivNLP/svp-sdml .", "published": "2024-03-01 02:09:25", "link": "http://arxiv.org/abs/2403.00226v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CASIMIR: A Corpus of Scientific Articles enhanced with Multiple\n  Author-Integrated Revisions", "abstract": "Writing a scientific article is a challenging task as it is a highly codified\nand specific genre, consequently proficiency in written communication is\nessential for effectively conveying research findings and ideas. In this\narticle, we propose an original textual resource on the revision step of the\nwriting process of scientific articles. This new dataset, called CASIMIR,\ncontains the multiple revised versions of 15,646 scientific articles from\nOpenReview, along with their peer reviews. Pairs of consecutive versions of an\narticle are aligned at sentence-level while keeping paragraph location\ninformation as metadata for supporting future revision studies at the discourse\nlevel. Each pair of revised sentences is enriched with automatically extracted\nedits and associated revision intention. To assess the initial quality on the\ndataset, we conducted a qualitative study of several state-of-the-art text\nrevision approaches and compared various evaluation metrics. Our experiments\nled us to question the relevance of the current evaluation methods for the text\nrevision task.", "published": "2024-03-01 03:07:32", "link": "http://arxiv.org/abs/2403.00241v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extracting Polymer Nanocomposite Samples from Full-Length Documents", "abstract": "This paper investigates the use of large language models (LLMs) for\nextracting sample lists of polymer nanocomposites (PNCs) from full-length\nmaterials science research papers. The challenge lies in the complex nature of\nPNC samples, which have numerous attributes scattered throughout the text. The\ncomplexity of annotating detailed information on PNCs limits the availability\nof data, making conventional document-level relation extraction techniques\nimpractical due to the challenge in creating comprehensive named entity span\nannotations. To address this, we introduce a new benchmark and an evaluation\ntechnique for this task and explore different prompting strategies in a\nzero-shot manner. We also incorporate self-consistency to improve the\nperformance. Our findings show that even advanced LLMs struggle to extract all\nof the samples from an article. Finally, we analyze the errors encountered in\nthis process, categorizing them into three main challenges, and discuss\npotential strategies for future research to overcome them.", "published": "2024-03-01 03:51:56", "link": "http://arxiv.org/abs/2403.00260v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Gender Bias in Large Language Models across Multiple Languages", "abstract": "With the growing deployment of large language models (LLMs) across various\napplications, assessing the influence of gender biases embedded in LLMs becomes\ncrucial. The topic of gender bias within the realm of natural language\nprocessing (NLP) has gained considerable focus, particularly in the context of\nEnglish. Nonetheless, the investigation of gender bias in languages other than\nEnglish is still relatively under-explored and insufficiently analyzed. In this\nwork, We examine gender bias in LLMs-generated outputs for different languages.\nWe use three measurements: 1) gender bias in selecting descriptive words given\nthe gender-related context. 2) gender bias in selecting gender-related pronouns\n(she/he) given the descriptive words. 3) gender bias in the topics of\nLLM-generated dialogues. We investigate the outputs of the GPT series of LLMs\nin various languages using our three measurement methods. Our findings revealed\nsignificant gender biases across all the languages we examined.", "published": "2024-03-01 04:47:16", "link": "http://arxiv.org/abs/2403.00277v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Jailbreak Attacks with Diversity Guidance", "abstract": "As large language models(LLMs) become commonplace in practical applications,\nthe security issues of LLMs have attracted societal concerns. Although\nextensive efforts have been made to safety alignment, LLMs remain vulnerable to\njailbreak attacks. We find that redundant computations limit the performance of\nexisting jailbreak attack methods. Therefore, we propose DPP-based Stochastic\nTrigger Searching (DSTS), a new optimization algorithm for jailbreak attacks.\nDSTS incorporates diversity guidance through techniques including stochastic\ngradient search and DPP selection during optimization. Detailed experiments and\nablation studies demonstrate the effectiveness of the algorithm. Moreover, we\nuse the proposed algorithm to compute the risk boundaries for different LLMs,\nproviding a new perspective on LLM safety evaluation.", "published": "2024-03-01 05:28:06", "link": "http://arxiv.org/abs/2403.00292v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semi-Instruct: Bridging Natural-Instruct and Self-Instruct for Code\n  Large Language Models", "abstract": "Instruction tuning plays a pivotal role in Code Large Language Models (Code\nLLMs) for the task of program synthesis. Presently, two dominant paradigms for\ncollecting tuning data are natural-instruct (human-written) and self-instruct\n(automatically generated). Natural-instruct includes diverse and correct codes\nbut lacks instruction-code pairs, and exists improper code formats like nested\nsingle-line codes. In contrast, self-instruct automatically generates proper\npaired data. However, it suffers from low diversity due to generating\nduplicates and cannot ensure the correctness of codes. To bridge the both\nparadigms, we propose \\textbf{Semi-Instruct}. It first converts diverse but\nimproper codes from natural-instruct into proper instruction-code pairs through\na method similar to self-instruct. To verify the correctness of generated\ncodes, we design a novel way to construct test cases by generating cases'\ninputs and executing correct codes from natural-instruct to get outputs.\nFinally, diverse and correct instruction-code pairs are retained for\ninstruction tuning. Experiments show that semi-instruct is significantly better\nthan natural-instruct and self-instruct. Furthermore, the performance steadily\nimproves as data scale increases.", "published": "2024-03-01 08:05:44", "link": "http://arxiv.org/abs/2403.00338v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Consistent Reasoning-based Aspect-Sentiment Quad Prediction with\n  Extract-Then-Assign Strategy", "abstract": "In the task of aspect sentiment quad prediction (ASQP), generative methods\nfor predicting sentiment quads have shown promising results. However, they\nstill suffer from imprecise predictions and limited interpretability, caused by\ndata scarcity and inadequate modeling of the quadruplet composition process. In\nthis paper, we propose Self-Consistent Reasoning-based Aspect-sentiment\nquadruple Prediction (SCRAP), optimizing its model to generate reasonings and\nthe corresponding sentiment quadruplets in sequence. SCRAP adopts the\nExtract-Then-Assign reasoning strategy, which closely mimics human cognition.\nIn the end, SCRAP significantly improves the model's ability to handle complex\nreasoning tasks and correctly predict quadruplets through consistency voting,\nresulting in enhanced interpretability and accuracy in ASQP.", "published": "2024-03-01 08:34:02", "link": "http://arxiv.org/abs/2403.00354v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-Lingual Learning vs. Low-Resource Fine-Tuning: A Case Study with\n  Fact-Checking in Turkish", "abstract": "The rapid spread of misinformation through social media platforms has raised\nconcerns regarding its impact on public opinion. While misinformation is\nprevalent in other languages, the majority of research in this field has\nconcentrated on the English language. Hence, there is a scarcity of datasets\nfor other languages, including Turkish. To address this concern, we have\nintroduced the FCTR dataset, consisting of 3238 real-world claims. This dataset\nspans multiple domains and incorporates evidence collected from three Turkish\nfact-checking organizations. Additionally, we aim to assess the effectiveness\nof cross-lingual transfer learning for low-resource languages, with a\nparticular focus on Turkish. We demonstrate in-context learning (zero-shot and\nfew-shot) performance of large language models in this context. The\nexperimental results indicate that the dataset has the potential to advance\nresearch in the Turkish language.", "published": "2024-03-01 09:57:46", "link": "http://arxiv.org/abs/2403.00411v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rethinking Tokenization: Crafting Better Tokenizers for Large Language\n  Models", "abstract": "Tokenization significantly influences language models(LMs)' performance. This\npaper traces the evolution of tokenizers from word-level to subword-level,\nanalyzing how they balance tokens and types to enhance model adaptability while\ncontrolling complexity. Despite subword tokenizers like Byte Pair Encoding\n(BPE) overcoming many word tokenizer limitations, they encounter difficulties\nin handling non-Latin languages and depend heavily on extensive training data\nand computational resources to grasp the nuances of multiword expressions\n(MWEs). This article argues that tokenizers, more than mere technical tools,\nshould drawing inspiration from the cognitive science about human language\nprocessing. This study then introduces the \"Principle of Least Effort\" from\ncognitive science, that humans naturally seek to reduce cognitive effort, and\ndiscusses the benefits of this principle for tokenizer development. Based on\nthis principle, the paper proposes that the Less-is-Better (LiB) model could be\na new approach for LLM tokenizer. The LiB model can autonomously learn an\nintegrated vocabulary consisting of subwords, words, and MWEs, which\neffectively reduces both the numbers of tokens and types. Comparative\nevaluations show that the LiB tokenizer outperforms existing word and BPE\ntokenizers, presenting an innovative method for tokenizer development, and\nhinting at the possibility of future cognitive science-based tokenizers being\nmore efficient.", "published": "2024-03-01 10:03:07", "link": "http://arxiv.org/abs/2403.00417v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLMs for Targeted Sentiment in News Headlines: Exploring the\n  Descriptive-Prescriptive Dilemma", "abstract": "News headlines often evoke sentiment by intentionally portraying entities in\nparticular ways, making targeted sentiment analysis (TSA) of headlines a\nworthwhile but difficult task. Due to its subjectivity, creating TSA datasets\ncan involve various annotation paradigms, from descriptive to prescriptive,\neither encouraging or limiting subjectivity. LLMs are a good fit for TSA due to\ntheir broad linguistic and world knowledge and in-context learning abilities,\nyet their performance depends on prompt design. In this paper, we compare the\naccuracy of state-of-the-art LLMs and fine-tuned encoder models for TSA of news\nheadlines using descriptive and prescriptive datasets across several languages.\nExploring the descriptive--prescriptive continuum, we analyze how performance\nis affected by prompt prescriptiveness, ranging from plain zero-shot to\nelaborate few-shot prompts. Finally, we evaluate the ability of LLMs to\nquantify uncertainty via calibration error and comparison to human label\nvariation. We find that LLMs outperform fine-tuned encoders on descriptive\ndatasets, while calibration and F1-score generally improve with increased\nprescriptiveness, yet the optimal level varies.", "published": "2024-03-01 10:10:34", "link": "http://arxiv.org/abs/2403.00418v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hierarchical Indexing for Retrieval-Augmented Opinion Summarization", "abstract": "We propose a method for unsupervised abstractive opinion summarization, that\ncombines the attributability and scalability of extractive approaches with the\ncoherence and fluency of Large Language Models (LLMs). Our method, HIRO, learns\nan index structure that maps sentences to a path through a semantically\norganized discrete hierarchy. At inference time, we populate the index and use\nit to identify and retrieve clusters of sentences containing popular opinions\nfrom input reviews. Then, we use a pretrained LLM to generate a readable\nsummary that is grounded in these extracted evidential clusters. The modularity\nof our approach allows us to evaluate its efficacy at each stage. We show that\nHIRO learns an encoding space that is more semantically structured than prior\nwork, and generates summaries that are more representative of the opinions in\nthe input reviews. Human evaluation confirms that HIRO generates significantly\nmore coherent, detailed and accurate summaries.", "published": "2024-03-01 10:38:07", "link": "http://arxiv.org/abs/2403.00435v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Your Model Is Not Predicting Depression Well And That Is Why: A Case\n  Study of PRIMATE Dataset", "abstract": "This paper addresses the quality of annotations in mental health datasets\nused for NLP-based depression level estimation from social media texts. While\nprevious research relies on social media-based datasets annotated with binary\ncategories, i.e. depressed or non-depressed, recent datasets such as D2S and\nPRIMATE aim for nuanced annotations using PHQ-9 symptoms. However, most of\nthese datasets rely on crowd workers without the domain knowledge for\nannotation. Focusing on the PRIMATE dataset, our study reveals concerns\nregarding annotation validity, particularly for the lack of interest or\npleasure symptom. Through reannotation by a mental health professional, we\nintroduce finer labels and textual spans as evidence, identifying a notable\nnumber of false positives. Our refined annotations, to be released under a Data\nUse Agreement, offer a higher-quality test set for anhedonia detection. This\nstudy underscores the necessity of addressing annotation quality issues in\nmental health datasets, advocating for improved methodologies to enhance NLP\nmodel reliability in mental health assessments.", "published": "2024-03-01 10:47:02", "link": "http://arxiv.org/abs/2403.00438v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LUCID: LLM-Generated Utterances for Complex and Interesting Dialogues", "abstract": "Spurred by recent advances in Large Language Models (LLMs), virtual\nassistants are poised to take a leap forward in terms of their dialogue\ncapabilities. Yet a major bottleneck to achieving genuinely transformative\ntask-oriented dialogue capabilities remains the scarcity of high quality data.\nExisting datasets, while impressive in scale, have limited domain coverage and\ncontain few genuinely challenging conversational phenomena; those which are\npresent are typically unlabelled, making it difficult to assess the strengths\nand weaknesses of models without time-consuming and costly human evaluation.\nMoreover, creating high quality dialogue data has until now required\nconsiderable human input, limiting both the scale of these datasets and the\nability to rapidly bootstrap data for a new target domain. We aim to overcome\nthese issues with LUCID, a modularised and highly automated LLM-driven data\ngeneration system that produces realistic, diverse and challenging dialogues.\nWe use LUCID to generate a seed dataset of 4,277 conversations across 100\nintents to demonstrate its capabilities, with a human review finding\nconsistently high quality labels in the generated data.", "published": "2024-03-01 11:33:53", "link": "http://arxiv.org/abs/2403.00462v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Do Zombies Understand? A Choose-Your-Own-Adventure Exploration of\n  Machine Cognition", "abstract": "Recent advances in LLMs have sparked a debate on whether they understand\ntext. In this position paper, we argue that opponents in this debate hold\ndifferent definitions for understanding, and particularly differ in their view\non the role of consciousness. To substantiate this claim, we propose a thought\nexperiment involving an open-source chatbot $Z$ which excels on every possible\nbenchmark, seemingly without subjective experience. We ask whether $Z$ is\ncapable of understanding, and show that different schools of thought within\nseminal AI research seem to answer this question differently, uncovering their\nterminological disagreement. Moving forward, we propose two distinct working\ndefinitions for understanding which explicitly acknowledge the question of\nconsciousness, and draw connections with a rich literature in philosophy,\npsychology and neuroscience.", "published": "2024-03-01 12:42:47", "link": "http://arxiv.org/abs/2403.00499v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PoTeC: A German Naturalistic Eye-tracking-while-reading Corpus", "abstract": "The Potsdam Textbook Corpus (PoTeC) is a naturalistic\neye-tracking-while-reading corpus containing data from 75 participants reading\n12 scientific texts. PoTeC is the first naturalistic eye-tracking-while-reading\ncorpus that contains eye-movements from domain-experts as well as novices in a\nwithin-participant manipulation: It is based on a 2x2x2 fully-crossed factorial\ndesign which includes the participants' level of study and the participants'\ndiscipline of study as between-subject factors and the text domain as a\nwithin-subject factor. The participants' reading comprehension was assessed by\na series of text comprehension questions and their domain knowledge was tested\nby text-independent background questions for each of the texts. The materials\nare annotated for a variety of linguistic features at different levels. We\nenvision PoTeC to be used for a wide range of studies including but not limited\nto analyses of expert and non-expert reading strategies. The corpus and all the\naccompanying data at all stages of the preprocessing pipeline and all code used\nto preprocess the data are made available via GitHub:\nhttps://github.com/DiLi-Lab/PoTeC.", "published": "2024-03-01 13:07:39", "link": "http://arxiv.org/abs/2403.00506v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Standardizing the Measurement of Text Diversity: A Tool and a\n  Comparative Analysis of Scores", "abstract": "The diversity across outputs generated by LLMs shapes perception of their\nquality and utility. High lexical diversity is often desirable, but there is no\nstandard method to measure this property. Templated answer structures and\n``canned'' responses across different documents are readily noticeable, but\ndifficult to visualize across large corpora. This work aims to standardize\nmeasurement of text diversity. Specifically, we empirically investigate the\nconvergent validity of existing scores across English texts, and we release\ndiversity, an open-source Python package for measuring and extracting\nrepetition in text. We also build a platform based on diversity for users to\ninteractively explore repetition in text. We find that fast compression\nalgorithms capture information similar to what is measured by slow-to-compute\n$n$-gram overlap homogeneity scores. Further, a combination of measures --\ncompression ratios, self-repetition of long $n$-grams, and Self-BLEU and\nBERTScore -- are sufficient to report, as they have low mutual correlation with\neach other.", "published": "2024-03-01 14:23:12", "link": "http://arxiv.org/abs/2403.00553v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling the Quality of Dialogical Explanations", "abstract": "Explanations are pervasive in our lives. Mostly, they occur in dialogical\nform where an {\\em explainer} discusses a concept or phenomenon of interest\nwith an {\\em explainee}. Leaving the explainee with a clear understanding is\nnot straightforward due to the knowledge gap between the two participants.\nPrevious research looked at the interaction of explanation moves, dialogue\nacts, and topics in successful dialogues with expert explainers. However,\ndaily-life explanations often fail, raising the question of what makes a\ndialogue successful. In this work, we study explanation dialogues in terms of\nthe interactions between the explainer and explainee and how they correlate\nwith the quality of explanations in terms of a successful understanding on the\nexplainee's side. In particular, we first construct a corpus of 399 dialogues\nfrom the Reddit forum {\\em Explain Like I am Five} and annotate it for\ninteraction flows and explanation quality. We then analyze the interaction\nflows, comparing them to those appearing in expert dialogues. Finally, we\nencode the interaction flows using two language models that can handle long\ninputs, and we provide empirical evidence for the effectiveness boost gained\nthrough the encoding in predicting the success of explanation dialogues.", "published": "2024-03-01 16:49:55", "link": "http://arxiv.org/abs/2403.00662v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Bit of a Problem: Measurement Disparities in Dataset Sizes Across\n  Languages", "abstract": "How should text dataset sizes be compared across languages? Even for\ncontent-matched (parallel) corpora, UTF-8 encoded text can require a\ndramatically different number of bytes for different languages. In our work, we\ndefine the byte premium between two languages as the ratio of bytes used to\nencode content-matched text in those languages. We compute byte premiums for\n1155 languages, and we use linear regressions to estimate byte premiums for\nother languages. We release a tool to obtain byte premiums for any two\nlanguages, enabling comparisons of dataset sizes across languages for more\nequitable multilingual model development and data practices.", "published": "2024-03-01 17:20:11", "link": "http://arxiv.org/abs/2403.00686v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Consistent Decoding for More Factual Open Responses", "abstract": "Self-consistency has emerged as a powerful method for improving the accuracy\nof short answers generated by large language models. As previously defined, it\nonly concerns the accuracy of a final answer parsed from generated text. In\nthis work, we extend the idea to open response generation, by integrating\nvoting into the decoding method. Each output sentence is selected from among\nmultiple samples, conditioning on the previous selections, based on a simple\ntoken overlap score. We compare this \"Sample & Select\" method to greedy\ndecoding, beam search, nucleus sampling, and the recently introduced\nhallucination avoiding decoders of DoLA, P-CRR, and S-CRR. We show that Sample\n& Select improves factuality by a 30% relative margin against these decoders in\nNLI-based evaluation on the subsets of CNN/DM and XSum used in the FRANK\nbenchmark, while maintaining comparable ROUGE-1 F1 scores against reference\nsummaries. We collect human verifications of the generated summaries,\nconfirming the factual superiority of our method.", "published": "2024-03-01 17:31:09", "link": "http://arxiv.org/abs/2403.00696v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LocalRQA: From Generating Data to Locally Training, Testing, and\n  Deploying Retrieval-Augmented QA Systems", "abstract": "Retrieval-augmented question-answering systems combine retrieval techniques\nwith large language models to provide answers that are more accurate and\ninformative. Many existing toolkits allow users to quickly build such systems\nusing off-the-shelf models, but they fall short in supporting researchers and\ndevelopers to customize the model training, testing, and deployment process. We\npropose LocalRQA, an open-source toolkit that features a wide selection of\nmodel training algorithms, evaluation methods, and deployment tools curated\nfrom the latest research. As a showcase, we build QA systems using online\ndocumentation obtained from Databricks and Faire's websites. We find 7B-models\ntrained and deployed using LocalRQA reach a similar performance compared to\nusing OpenAI's text-ada-002 and GPT-4-turbo.", "published": "2024-03-01 21:10:20", "link": "http://arxiv.org/abs/2403.00982v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Formulation Comparison for Timeline Construction using LLMs", "abstract": "Constructing a timeline requires identifying the chronological order of\nevents in an article. In prior timeline construction datasets, temporal orders\nare typically annotated by either event-to-time anchoring or event-to-event\npairwise ordering, both of which suffer from missing temporal information. To\nmitigate the issue, we develop a new evaluation dataset, TimeSET, consisting of\nsingle-document timelines with document-level order annotation. TimeSET\nfeatures saliency-based event selection and partial ordering, which enable a\npractical annotation workload. Aiming to build better automatic timeline\nconstruction systems, we propose a novel evaluation framework to compare\nmultiple task formulations with TimeSET by prompting open LLMs, i.e., Llama 2\nand Flan-T5. Considering that identifying temporal orders of events is a core\nsubtask in timeline construction, we further benchmark open LLMs on existing\nevent temporal ordering datasets to gain a robust understanding of their\ncapabilities. Our experiments show that (1) NLI formulation with Flan-T5\ndemonstrates a strong performance among others, while (2) timeline construction\nand event temporal ordering are still challenging tasks for few-shot LLMs. Our\ncode and data are available at https://github.com/kimihiroh/timeset.", "published": "2024-03-01 21:24:24", "link": "http://arxiv.org/abs/2403.00990v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Predictions from language models for multiple-choice tasks are not\n  robust under variation of scoring methods", "abstract": "This paper systematically compares different methods of deriving item-level\npredictions of language models for multiple-choice tasks. It compares scoring\nmethods for answer options based on free generation of responses, various\nprobability-based scores, a Likert-scale style rating method, and embedding\nsimilarity. In a case study on pragmatic language interpretation, we find that\nLLM predictions are not robust under variation of method choice, both within a\nsingle LLM and across different LLMs. As this variability entails pronounced\nresearcher degrees of freedom in reporting results, knowledge of the\nvariability is crucial to secure robustness of results and research integrity.", "published": "2024-03-01 21:48:08", "link": "http://arxiv.org/abs/2403.00998v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of\n  Large Vision-Language Models", "abstract": "Large vision-language models (LVLMs) excel across diverse tasks involving\nconcrete images from natural scenes. However, their ability to interpret\nabstract figures, such as geometry shapes and scientific plots, remains limited\ndue to a scarcity of training datasets in scientific domains. To fill this gap,\nwe introduce Multimodal ArXiv, consisting of ArXivCap and ArXivQA, for\nenhancing LVLMs scientific comprehension. ArXivCap is a figure-caption dataset\ncomprising 6.4M images and 3.9M captions, sourced from 572K ArXiv papers\nspanning various scientific domains. Drawing from ArXivCap, we introduce\nArXivQA, a question-answering dataset generated by prompting GPT-4V based on\nscientific figures. ArXivQA greatly enhances open-sourced LVLMs' mathematical\nreasoning capabilities, achieving a 10.4\\% absolute accuracy gain on a\nmultimodal mathematical reasoning benchmark. Furthermore, employing ArXivCap,\nwe devise four vision-to-text tasks for benchmarking LVLMs. Evaluation results\nwith state-of-the-art LVLMs underscore their struggle with the nuanced\nsemantics of academic figures, while domain-specific training yields\nsubstantial performance gains. Our error analysis uncovers misinterpretations\nof visual context, recognition errors, and the production of overly simplified\ncaptions by current LVLMs, shedding light on future improvements.", "published": "2024-03-01 02:21:30", "link": "http://arxiv.org/abs/2403.00231v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "TRUCE: Private Benchmarking to Prevent Contamination and Improve\n  Comparative Evaluation of LLMs", "abstract": "Benchmarking is the de-facto standard for evaluating LLMs, due to its speed,\nreplicability and low cost. However, recent work has pointed out that the\nmajority of the open source benchmarks available today have been contaminated\nor leaked into LLMs, meaning that LLMs have access to test data during\npretraining and/or fine-tuning. This raises serious concerns about the validity\nof benchmarking studies conducted so far and the future of evaluation using\nbenchmarks. To solve this problem, we propose Private Benchmarking, a solution\nwhere test datasets are kept private and models are evaluated without revealing\nthe test data to the model. We describe various scenarios (depending on the\ntrust placed on model owners or dataset owners), and present solutions to avoid\ndata contamination using private benchmarking. For scenarios where the model\nweights need to be kept private, we describe solutions from confidential\ncomputing and cryptography that can aid in private benchmarking. We build an\nend-to-end system, TRUCE, that enables such private benchmarking showing that\nthe overheads introduced to protect models and benchmark are negligible (in the\ncase of confidential computing) and tractable (when cryptographic security is\nrequired). Finally, we also discuss solutions to the problem of benchmark\ndataset auditing, to ensure that private benchmarks are of sufficiently high\nquality.", "published": "2024-03-01 09:28:38", "link": "http://arxiv.org/abs/2403.00393v2", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Provably Robust DPO: Aligning Language Models with Noisy Feedback", "abstract": "Learning from preference-based feedback has recently gained traction as a\npromising approach to align language models with human interests. While these\naligned generative models have demonstrated impressive capabilities across\nvarious tasks, their dependence on high-quality human preference data poses a\nbottleneck in practical applications. Specifically, noisy (incorrect and\nambiguous) preference pairs in the dataset might restrict the language models\nfrom capturing human intent accurately. While practitioners have recently\nproposed heuristics to mitigate the effect of noisy preferences, a complete\ntheoretical understanding of their workings remain elusive.\n  In this work, we aim to bridge this gap by by introducing a general framework\nfor policy optimization in the presence of random preference flips. We focus on\nthe direct preference optimization (DPO) algorithm in particular since it\nassumes that preferences adhere to the Bradley-Terry-Luce (BTL) model, raising\nconcerns about the impact of noisy data on the learned policy. We design a\nnovel loss function, which de-bias the effect of noise on average, making a\npolicy trained by minimizing that loss robust to the noise. Under log-linear\nparameterization of the policy class and assuming good feature coverage of the\nSFT policy, we prove that the sub-optimality gap of the proposed robust DPO\n(rDPO) policy compared to the optimal policy is of the order\n$O(\\frac{1}{1-2\\epsilon}\\sqrt{\\frac{d}{n}})$, where $\\epsilon < 1/2$ is flip\nrate of labels, $d$ is policy parameter dimension and $n$ is size of dataset.\nOur experiments on IMDb sentiment generation and Anthropic's helpful-harmless\ndataset show that rDPO is robust to noise in preference labels compared to\nvanilla DPO and other heuristics proposed by practitioners.", "published": "2024-03-01 09:55:18", "link": "http://arxiv.org/abs/2403.00409v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "ROME: Memorization Insights from Text, Logits and Representation", "abstract": "Previous works have evaluated memorization by comparing model outputs with\ntraining corpora, examining how factors such as data duplication, model size,\nand prompt length influence memorization. However, analyzing these extensive\ntraining corpora is highly time-consuming. To address this challenge, this\npaper proposes an innovative approach named ROME that bypasses direct\nprocessing of the training data. Specifically, we select datasets categorized\ninto three distinct types -- context-independent, conventional, and factual --\nand redefine memorization as the ability to produce correct answers under these\nconditions. Our analysis then focuses on disparities between memorized and\nnon-memorized samples by examining the logits and representations of generated\ntexts. Experimental findings reveal that longer words are less likely to be\nmemorized, higher confidence correlates with greater memorization, and\nrepresentations of the same concepts are more similar across different\ncontexts. Our code and data will be publicly available when the paper is\naccepted.", "published": "2024-03-01 13:15:30", "link": "http://arxiv.org/abs/2403.00510v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Language Models for Simultaneous Named Entity Extraction and\n  Spelling Correction", "abstract": "Language Models (LMs) such as BERT, have been shown to perform well on the\ntask of identifying Named Entities (NE) in text. A BERT LM is typically used as\na classifier to classify individual tokens in the input text, or to classify\nspans of tokens, as belonging to one of a set of possible NE categories.\n  In this paper, we hypothesise that decoder-only Large Language Models (LLMs)\ncan also be used generatively to extract both the NE, as well as potentially\nrecover the correct surface form of the NE, where any spelling errors that were\npresent in the input text get automatically corrected.\n  We fine-tune two BERT LMs as baselines, as well as eight open-source LLMs, on\nthe task of producing NEs from text that was obtained by applying Optical\nCharacter Recognition (OCR) to images of Japanese shop receipts; in this work,\nwe do not attempt to find or evaluate the location of NEs in the text.\n  We show that the best fine-tuned LLM performs as well as, or slightly better\nthan, the best fine-tuned BERT LM, although the differences are not\nsignificant. However, the best LLM is also shown to correct OCR errors in some\ncases, as initially hypothesised.", "published": "2024-03-01 13:36:04", "link": "http://arxiv.org/abs/2403.00528v1", "categories": ["cs.CL", "cs.CV", "H.3.3; H.3.4; I.2.7; I.7.1; I.7.5"], "primary_category": "cs.CL"}
{"title": "Few-Shot Relation Extraction with Hybrid Visual Evidence", "abstract": "The goal of few-shot relation extraction is to predict relations between name\nentities in a sentence when only a few labeled instances are available for\ntraining. Existing few-shot relation extraction methods focus on uni-modal\ninformation such as text only. This reduces performance when there are no clear\ncontexts between the name entities described in text. We propose a multi-modal\nfew-shot relation extraction model (MFS-HVE) that leverages both textual and\nvisual semantic information to learn a multi-modal representation jointly. The\nMFS-HVE includes semantic feature extractors and multi-modal fusion components.\nThe MFS-HVE semantic feature extractors are developed to extract both textual\nand visual features. The visual features include global image features and\nlocal object features within the image. The MFS-HVE multi-modal fusion unit\nintegrates information from various modalities using image-guided attention,\nobject-guided attention, and hybrid feature attention to fully capture the\nsemantic interaction between visual regions of images and relevant texts.\nExtensive experiments conducted on two public datasets demonstrate that\nsemantic visual information significantly improves the performance of few-shot\nrelation prediction.", "published": "2024-03-01 18:20:11", "link": "http://arxiv.org/abs/2403.00724v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "AtP*: An efficient and scalable method for localizing LLM behaviour to\n  components", "abstract": "Activation Patching is a method of directly computing causal attributions of\nbehavior to model components. However, applying it exhaustively requires a\nsweep with cost scaling linearly in the number of model components, which can\nbe prohibitively expensive for SoTA Large Language Models (LLMs). We\ninvestigate Attribution Patching (AtP), a fast gradient-based approximation to\nActivation Patching and find two classes of failure modes of AtP which lead to\nsignificant false negatives. We propose a variant of AtP called AtP*, with two\nchanges to address these failure modes while retaining scalability. We present\nthe first systematic study of AtP and alternative methods for faster activation\npatching and show that AtP significantly outperforms all other investigated\nmethods, with AtP* providing further significant improvement. Finally, we\nprovide a method to bound the probability of remaining false negatives of AtP*\nestimates.", "published": "2024-03-01 18:43:51", "link": "http://arxiv.org/abs/2403.00745v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "SoftTiger: A Clinical Foundation Model for Healthcare Workflows", "abstract": "We introduce SoftTiger, a clinical large language model (CLaM) designed as a\nfoundation model for healthcare workflows. The narrative and unstructured\nnature of clinical notes is a major obstacle for healthcare intelligentization.\nWe address a critical problem of structuring clinical notes into clinical data,\naccording to international interoperability standards. We collect and annotate\ndata for three subtasks, namely, international patient summary, clinical\nimpression and medical encounter. We then supervised fine-tuned a\nstate-of-the-art LLM using public and credentialed clinical data. The training\nis orchestrated in a way that the target model can first support basic clinical\ntasks such as abbreviation expansion and temporal information extraction, and\nthen learn to perform more complex downstream clinical tasks. Moreover, we\naddress several modeling challenges in the healthcare context, e.g., extra long\ncontext window. Our blind pairwise evaluation shows that SoftTiger outperforms\nother popular open-source models and GPT-3.5, comparable to Gemini-pro, with a\nmild gap from GPT-4. We believe that LLMs may become a step-stone towards\nhealthcare digitalization and democratization. Therefore, we publicly release\nSoftTiger models at scales of 13 billion and 70 billion parameters, as well as\ndatasets and code for our innovative scalable evaluation, hopefully, making a\nsignificant contribution to the healthcare industry.", "published": "2024-03-01 04:39:16", "link": "http://arxiv.org/abs/2403.00868v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Word Order and World Knowledge", "abstract": "Word order is an important concept in natural language, and in this work, we\nstudy how word order affects the induction of world knowledge from raw text\nusing language models. We use word analogies to probe for such knowledge.\nSpecifically, in addition to the natural word order, we first respectively\nextract texts of six fixed word orders from five languages and then pretrain\nthe language models on these texts. Finally, we analyze the experimental\nresults of the fixed word orders on word analogies and show that i) certain\nfixed word orders consistently outperform or underperform others, though the\nspecifics vary across languages, and ii) the Wov2Lex hypothesis is not hold in\npre-trained language models, and the natural word order typically yields\nmediocre results. The source code will be made publicly available at\nhttps://github.com/lshowway/probing_by_analogy.", "published": "2024-03-01 08:13:48", "link": "http://arxiv.org/abs/2403.00876v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Margin Discrepancy-based Adversarial Training for Multi-Domain Text\n  Classification", "abstract": "Multi-domain text classification (MDTC) endeavors to harness available\nresources from correlated domains to enhance the classification accuracy of the\ntarget domain. Presently, most MDTC approaches that embrace adversarial\ntraining and the shared-private paradigm exhibit cutting-edge performance.\nUnfortunately, these methods face a non-negligible challenge: the absence of\ntheoretical guarantees in the design of MDTC algorithms. The dearth of\ntheoretical underpinning poses a substantial impediment to the advancement of\nMDTC algorithms. To tackle this problem, we first provide a theoretical\nanalysis of MDTC by decomposing the MDTC task into multiple domain adaptation\ntasks. We incorporate the margin discrepancy as the measure of domain\ndivergence and establish a new generalization bound based on Rademacher\ncomplexity. Subsequently, we propose a margin discrepancy-based adversarial\ntraining (MDAT) approach for MDTC, in accordance with our theoretical analysis.\nTo validate the efficacy of the proposed MDAT method, we conduct empirical\nstudies on two MDTC benchmarks. The experimental results demonstrate that our\nMDAT approach surpasses state-of-the-art baselines on both datasets.", "published": "2024-03-01 11:54:14", "link": "http://arxiv.org/abs/2403.00888v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large\n  Language Models", "abstract": "Since large language models (LLMs) achieve significant success in recent\nyears, the hallucination issue remains a challenge, numerous benchmarks are\nproposed to detect the hallucination. Nevertheless, some of these benchmarks\nare not naturally generated by LLMs but are intentionally induced. Also, many\nmerely focus on the factuality hallucination while ignoring the faithfulness\nhallucination. Additionally, although dialogue pattern is more widely utilized\nin the era of LLMs, current benchmarks only concentrate on sentence-level and\npassage-level hallucination. In this study, we propose DiaHalu, the first\ndialogue-level hallucination evaluation benchmark to our knowledge. Initially,\nwe integrate the collected topics into system prompts and facilitate a dialogue\nbetween two ChatGPT3.5. Subsequently, we manually modify the contents that do\nnot adhere to human language conventions and then have LLMs re-generate,\nsimulating authentic human-machine interaction scenarios. Finally, professional\nscholars annotate all the samples in the dataset. DiaHalu covers four common\nmulti-turn dialogue domains and five hallucination subtypes, extended from\nfactuality and faithfulness hallucination. Experiments through some well-known\nLLMs and detection methods on the dataset show that DiaHalu is a challenging\nbenchmark, holding significant value for further research.", "published": "2024-03-01 15:38:55", "link": "http://arxiv.org/abs/2403.00896v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Interpretable Ensemble of Graph and Language Models for Improving\n  Search Relevance in E-Commerce", "abstract": "The problem of search relevance in the E-commerce domain is a challenging one\nsince it involves understanding the intent of a user's short nuanced query and\nmatching it with the appropriate products in the catalog. This problem has\ntraditionally been addressed using language models (LMs) and graph neural\nnetworks (GNNs) to capture semantic and inter-product behavior signals,\nrespectively. However, the rapid development of new architectures has created a\ngap between research and the practical adoption of these techniques. Evaluating\nthe generalizability of these models for deployment requires extensive\nexperimentation on complex, real-world datasets, which can be non-trivial and\nexpensive. Furthermore, such models often operate on latent space\nrepresentations that are incomprehensible to humans, making it difficult to\nevaluate and compare the effectiveness of different models. This lack of\ninterpretability hinders the development and adoption of new techniques in the\nfield. To bridge this gap, we propose Plug and Play Graph LAnguage Model\n(PP-GLAM), an explainable ensemble of plug and play models. Our approach uses a\nmodular framework with uniform data processing pipelines. It employs additive\nexplanation metrics to independently decide whether to include (i) language\nmodel candidates, (ii) GNN model candidates, and (iii) inter-product behavioral\nsignals. For the task of search relevance, we show that PP-GLAM outperforms\nseveral state-of-the-art baselines as well as a proprietary model on real-world\nmultilingual, multi-regional e-commerce datasets. To promote better model\ncomprehensibility and adoption, we also provide an analysis of the\nexplainability and computational complexity of our model. We also provide the\npublic codebase and provide a deployment strategy for practical implementation.", "published": "2024-03-01 19:08:25", "link": "http://arxiv.org/abs/2403.00923v1", "categories": ["cs.IR", "cs.CL", "H.3.3; I.2.7; J.7"], "primary_category": "cs.IR"}
{"title": "MediSwift: Efficient Sparse Pre-trained Biomedical Language Models", "abstract": "Large language models (LLMs) are typically trained on general source data for\nvarious domains, but a recent surge in domain-specific LLMs has shown their\npotential to outperform general-purpose models in domain-specific tasks (e.g.,\nbiomedicine). Although domain-specific pre-training enhances efficiency and\nleads to smaller models, the computational costs of training these LLMs remain\nhigh, posing budgeting challenges. We introduce MediSwift, a suite of\nbiomedical LMs that leverage sparse pre-training on domain-specific biomedical\ntext data. By inducing up to 75% weight sparsity during the pre-training phase,\nMediSwift achieves a 2-2.5x reduction in training FLOPs. Notably, all sparse\npre-training was performed on the Cerebras CS-2 system, which is specifically\ndesigned to realize the acceleration benefits from unstructured weight\nsparsity, thereby significantly enhancing the efficiency of the MediSwift\nmodels. Through subsequent dense fine-tuning and strategic soft prompting,\nMediSwift models outperform existing LLMs up to 7B parameters on biomedical\ntasks, setting new benchmarks w.r.t efficiency-accuracy on tasks such as\nPubMedQA. Our results show that sparse pre-training, along with dense\nfine-tuning and soft prompting, offers an effective method for creating\nhigh-performing, computationally efficient models in specialized domains.", "published": "2024-03-01 20:03:44", "link": "http://arxiv.org/abs/2403.00952v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AutoRD: An Automatic and End-to-End System for Rare Disease Knowledge\n  Graph Construction Based on Ontologies-enhanced Large Language Models", "abstract": "Rare diseases affect millions worldwide but often face limited research focus\ndue to their low prevalence. This results in prolonged diagnoses and a lack of\napproved therapies. Recent advancements in Large Language Models (LLMs) have\nshown promise in automating the extraction of medical information, offering\npotential to improve medical diagnosis and management. However, most LLMs lack\nprofessional medical knowledge, especially concerning rare diseases, and\nstruggle to handle the latest rare disease information. They also cannot\neffectively manage rare disease data and are not directly suitable for\ndiagnosis and management tasks. Our objective is to create an end-to-end system\ncalled AutoRD, which automates the extraction of information from medical texts\nabout rare diseases, focusing on entities and their relations. AutoRD\nintegrates up-to-date structured knowledge and demonstrates superior\nperformance in rare disease extraction tasks. We conduct various experiments to\nevaluate AutoRD's performance, aiming to surpass common LLMs and traditional\nmethods.", "published": "2024-03-01 20:06:39", "link": "http://arxiv.org/abs/2403.00953v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MALTO at SemEval-2024 Task 6: Leveraging Synthetic Data for LLM\n  Hallucination Detection", "abstract": "In Natural Language Generation (NLG), contemporary Large Language Models\n(LLMs) face several challenges, such as generating fluent yet inaccurate\noutputs and reliance on fluency-centric metrics. This often leads to neural\nnetworks exhibiting \"hallucinations\". The SHROOM challenge focuses on\nautomatically identifying these hallucinations in the generated text. To tackle\nthese issues, we introduce two key components, a data augmentation pipeline\nincorporating LLM-assisted pseudo-labelling and sentence rephrasing, and a\nvoting ensemble from three models pre-trained on Natural Language Inference\n(NLI) tasks and fine-tuned on diverse datasets.", "published": "2024-03-01 20:31:10", "link": "http://arxiv.org/abs/2403.00964v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Attribute Structuring Improves LLM-Based Evaluation of Clinical Text\n  Summaries", "abstract": "Summarizing clinical text is crucial in health decision-support and clinical\nresearch. Large language models (LLMs) have shown the potential to generate\naccurate clinical text summaries, but still struggle with issues regarding\ngrounding and evaluation, especially in safety-critical domains such as health.\nHolistically evaluating text summaries is challenging because they may contain\nunsubstantiated information. Here, we explore a general mitigation framework\nusing Attribute Structuring (AS), which structures the summary evaluation\nprocess. It decomposes the evaluation process into a grounded procedure that\nuses an LLM for relatively simple structuring and scoring tasks, rather than\nthe full task of holistic summary evaluation. Experiments show that AS\nconsistently improves the correspondence between human annotations and\nautomated metrics in clinical text summarization. Additionally, AS yields\ninterpretations in the form of a short text span corresponding to each output,\nwhich enables efficient human auditing, paving the way towards trustworthy\nevaluation of clinical information in resource-constrained scenarios. We\nrelease our code, prompts, and an open-source benchmark at\nhttps://github.com/microsoft/attribute-structuring.", "published": "2024-03-01 21:59:03", "link": "http://arxiv.org/abs/2403.01002v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Peacock: A Family of Arabic Multimodal Large Language Models and\n  Benchmarks", "abstract": "Multimodal large language models (MLLMs) have proven effective in a wide\nrange of tasks requiring complex reasoning and linguistic comprehension.\nHowever, due to a lack of high-quality multimodal resources in languages other\nthan English, success of MLLMs remains relatively limited to English-based\nsettings. This poses significant challenges in developing comparable models for\nother languages, including even those with large speaker populations such as\nArabic. To alleviate this challenge, we introduce a comprehensive family of\nArabic MLLMs, dubbed \\textit{Peacock}, with strong vision and language\ncapabilities. Through comprehensive qualitative and quantitative analysis, we\ndemonstrate the solid performance of our models on various visual reasoning\ntasks and further show their emerging dialectal potential. Additionally, we\nintroduce ~\\textit{Henna}, a new benchmark specifically designed for assessing\nMLLMs on aspects related to Arabic culture, setting the first stone for\nculturally-aware Arabic MLLMs.The GitHub repository for the \\textit{Peacock}\nproject is available at \\url{https://github.com/UBC-NLP/peacock}.", "published": "2024-03-01 23:38:02", "link": "http://arxiv.org/abs/2403.01031v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AXOLOTL: Fairness through Assisted Self-Debiasing of Large Language\n  Model Outputs", "abstract": "Pre-trained Large Language Models (LLMs) have significantly advanced natural\nlanguage processing capabilities but are susceptible to biases present in their\ntraining data, leading to unfair outcomes in various applications. While\nnumerous strategies have been proposed to mitigate bias, they often require\nextensive computational resources and may compromise model performance. In this\nwork, we introduce AXOLOTL, a novel post-processing framework, which operates\nagnostically across tasks and models, leveraging public APIs to interact with\nLLMs without direct access to internal parameters. Through a three-step process\nresembling zero-shot learning, AXOLOTL identifies biases, proposes resolutions,\nand guides the model to self-debias its outputs. This approach minimizes\ncomputational costs and preserves model performance, making AXOLOTL a promising\ntool for debiasing LLM outputs with broad applicability and ease of use.", "published": "2024-03-01 00:02:37", "link": "http://arxiv.org/abs/2403.00198v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Socratic Question Generation using Data Augmentation and\n  Preference Optimization", "abstract": "The Socratic method is a way of guiding students toward solving a problem\nindependently without directly revealing the solution to the problem. Although\nthis method has been shown to significantly improve student learning outcomes,\nit remains a complex labor-intensive task for instructors. Large language\nmodels (LLMs) can be used to augment human effort by automatically generating\nSocratic questions for students. However, existing methods that involve\nprompting these LLMs sometimes produce invalid outputs, e.g., those that\ndirectly reveal the solution to the problem or provide irrelevant or premature\nquestions. To alleviate this problem, inspired by reinforcement learning with\nAI feedback (RLAIF), we first propose a data augmentation method to enrich\nexisting Socratic questioning datasets with questions that are invalid in\nspecific ways. Next, we propose a method to optimize open-source LLMs such as\nLLama 2 to prefer ground-truth questions over generated invalid ones, using\ndirect preference optimization (DPO). Our experiments on a Socratic questions\ndataset for student code debugging show that a DPO-optimized 7B LLama 2 model\ncan effectively avoid generating invalid questions, and as a result,\noutperforms existing state-of-the-art prompting methods.", "published": "2024-03-01 00:08:20", "link": "http://arxiv.org/abs/2403.00199v3", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Benchmarking zero-shot stance detection with FlanT5-XXL: Insights from\n  training data, prompting, and decoding strategies into its near-SoTA\n  performance", "abstract": "We investigate the performance of LLM-based zero-shot stance detection on\ntweets. Using FlanT5-XXL, an instruction-tuned open-source LLM, with the\nSemEval 2016 Tasks 6A, 6B, and P-Stance datasets, we study the performance and\nits variations under different prompts and decoding strategies, as well as the\npotential biases of the model. We show that the zero-shot approach can match or\noutperform state-of-the-art benchmarks, including fine-tuned models. We provide\nvarious insights into its performance including the sensitivity to instructions\nand prompts, the decoding strategies, the perplexity of the prompts, and to\nnegations and oppositions present in prompts. Finally, we ensure that the LLM\nhas not been trained on test datasets, and identify a positivity bias which may\npartially explain the performance differences across decoding strategie", "published": "2024-03-01 02:33:26", "link": "http://arxiv.org/abs/2403.00236v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "EUROPA: A Legal Multilingual Keyphrase Generation Dataset", "abstract": "Keyphrase generation has primarily been explored within the context of\nacademic research articles, with a particular focus on scientific domains and\nthe English language. In this work, we present EUROPA, a dataset for\nmultilingual keyphrase generation in the legal domain. It is derived from legal\njudgments from the Court of Justice of the European Union (EU), and contains\ninstances in all 24 EU official languages. We run multilingual models on our\ncorpus and analyze the results, showing room for improvement on a\ndomain-specific multilingual corpus such as the one we present.", "published": "2024-03-01 03:30:38", "link": "http://arxiv.org/abs/2403.00252v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Post-decoder Biasing for End-to-End Speech Recognition of Multi-turn\n  Medical Interview", "abstract": "End-to-end (E2E) approach is gradually replacing hybrid models for automatic\nspeech recognition (ASR) tasks. However, the optimization of E2E models lacks\nan intuitive method for handling decoding shifts, especially in scenarios with\na large number of domain-specific rare words that hold specific important\nmeanings. Furthermore, the absence of knowledge-intensive speech datasets in\nacademia has been a significant limiting factor, and the commonly used speech\ncorpora exhibit significant disparities with realistic conversation. To address\nthese challenges, we present Medical Interview (MED-IT), a multi-turn\nconsultation speech dataset that contains a substantial number of\nknowledge-intensive named entities. We also explore methods to enhance the\nrecognition performance of rare words for E2E models. We propose a novel\napproach, post-decoder biasing, which constructs a transform probability matrix\nbased on the distribution of training transcriptions. This guides the model to\nprioritize recognizing words in the biasing list. In our experiments, for\nsubsets of rare words appearing in the training speech between 10 and 20 times,\nand between 1 and 5 times, the proposed method achieves a relative improvement\nof 9.3% and 5.1%, respectively.", "published": "2024-03-01 08:53:52", "link": "http://arxiv.org/abs/2403.00370v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Surveying the Dead Minds: Historical-Psychological Text Analysis with\n  Contextualized Construct Representation (CCR) for Classical Chinese", "abstract": "In this work, we develop a pipeline for historical-psychological text\nanalysis in classical Chinese. Humans have produced texts in various languages\nfor thousands of years; however, most of the computational literature is\nfocused on contemporary languages and corpora. The emerging field of historical\npsychology relies on computational techniques to extract aspects of psychology\nfrom historical corpora using new methods developed in natural language\nprocessing (NLP). The present pipeline, called Contextualized Construct\nRepresentations (CCR), combines expert knowledge in psychometrics (i.e.,\npsychological surveys) with text representations generated via\ntransformer-based language models to measure psychological constructs such as\ntraditionalism, norm strength, and collectivism in classical Chinese corpora.\nConsidering the scarcity of available data, we propose an indirect supervised\ncontrastive learning approach and build the first Chinese historical psychology\ncorpus (C-HI-PSY) to fine-tune pre-trained models. We evaluate the pipeline to\ndemonstrate its superior performance compared with other approaches. The CCR\nmethod outperforms word-embedding-based approaches across all of our tasks and\nexceeds prompting with GPT-4 in most tasks. Finally, we benchmark the pipeline\nagainst objective, external data to further verify its validity.", "published": "2024-03-01 13:14:45", "link": "http://arxiv.org/abs/2403.00509v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Metamorpheus: Interactive, Affective, and Creative Dream Narration\n  Through Metaphorical Visual Storytelling", "abstract": "Human emotions are essentially molded by lived experiences, from which we\nconstruct personalised meaning. The engagement in such meaning-making process\nhas been practiced as an intervention in various psychotherapies to promote\nwellness. Nevertheless, to support recollecting and recounting lived\nexperiences in everyday life remains under explored in HCI. It also remains\nunknown how technologies such as generative AI models can facilitate the\nmeaning making process, and ultimately support affective mindfulness. In this\npaper we present Metamorpheus, an affective interface that engages users in a\ncreative visual storytelling of emotional experiences during dreams.\nMetamorpheus arranges the storyline based on a dream's emotional arc, and\nprovokes self-reflection through the creation of metaphorical images and text\ndepictions. The system provides metaphor suggestions, and generates visual\nmetaphors and text depictions using generative AI models, while users can apply\ngenerations to recolour and re-arrange the interface to be visually affective.\nOur experience-centred evaluation manifests that, by interacting with\nMetamorpheus, users can recall their dreams in vivid detail, through which they\nrelive and reflect upon their experiences in a meaningful way.", "published": "2024-03-01 16:09:32", "link": "http://arxiv.org/abs/2403.00632v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.HC"}
{"title": "Dialect prejudice predicts AI decisions about people's character,\n  employability, and criminality", "abstract": "Hundreds of millions of people now interact with language models, with uses\nranging from serving as a writing aid to informing hiring decisions. Yet these\nlanguage models are known to perpetuate systematic racial prejudices, making\ntheir judgments biased in problematic ways about groups like African Americans.\nWhile prior research has focused on overt racism in language models, social\nscientists have argued that racism with a more subtle character has developed\nover time. It is unknown whether this covert racism manifests in language\nmodels. Here, we demonstrate that language models embody covert racism in the\nform of dialect prejudice: we extend research showing that Americans hold\nraciolinguistic stereotypes about speakers of African American English and find\nthat language models have the same prejudice, exhibiting covert stereotypes\nthat are more negative than any human stereotypes about African Americans ever\nexperimentally recorded, although closest to the ones from before the civil\nrights movement. By contrast, the language models' overt stereotypes about\nAfrican Americans are much more positive. We demonstrate that dialect prejudice\nhas the potential for harmful consequences by asking language models to make\nhypothetical decisions about people, based only on how they speak. Language\nmodels are more likely to suggest that speakers of African American English be\nassigned less prestigious jobs, be convicted of crimes, and be sentenced to\ndeath. Finally, we show that existing methods for alleviating racial bias in\nlanguage models such as human feedback training do not mitigate the dialect\nprejudice, but can exacerbate the discrepancy between covert and overt\nstereotypes, by teaching language models to superficially conceal the racism\nthat they maintain on a deeper level. Our findings have far-reaching\nimplications for the fair and safe employment of language technology.", "published": "2024-03-01 18:43:09", "link": "http://arxiv.org/abs/2403.00742v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Mitigating Reversal Curse in Large Language Models via Semantic-aware\n  Permutation Training", "abstract": "While large language models (LLMs) have achieved impressive performance\nacross diverse tasks, recent studies showcase that causal LLMs suffer from the\n\"reversal curse\". It is a typical example that the model knows \"A's father is\nB\", but is unable to reason \"B's child is A\". This limitation poses a challenge\nto the advancement of artificial general intelligence (AGI), as it suggests a\ngap in the models' ability to comprehend and apply bidirectional reasoning. In\nthis paper, we first conduct substantial evaluation and identify that the root\ncause of the reversal curse lies in the different word order between the\ntraining and inference stage, namely, the poor ability of causal language\nmodels to predict antecedent words within the training data. Accordingly,\npermutation on the training data is considered as a potential solution, since\nthis can make the model predict antecedent words or tokens. However, previous\npermutation methods may disrupt complete phrases or entities, thereby posing\nchallenges for the model to comprehend and learn from training data. To address\nthis issue, we propose Semantic-aware Permutation Training (SPT), which\naddresses this issue by segmenting the training sentences into semantic units\n(i.e., entities or phrases) with an assistant language model and permuting\nthese units before feeding into the model. Extensive experiments demonstrate\nthat SPT effectively mitigates the reversal curse since the performance on\nreversed questions approximates that on the forward ones, and significantly\nadvances the performance of existing works.", "published": "2024-03-01 18:55:20", "link": "http://arxiv.org/abs/2403.00758v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by\n  Exploring Refusal Loss Landscapes", "abstract": "Large Language Models (LLMs) are becoming a prominent generative AI tool,\nwhere the user enters a query and the LLM generates an answer. To reduce harm\nand misuse, efforts have been made to align these LLMs to human values using\nadvanced training techniques such as Reinforcement Learning from Human Feedback\n(RLHF). However, recent studies have highlighted the vulnerability of LLMs to\nadversarial jailbreak attempts aiming at subverting the embedded safety\nguardrails. To address this challenge, this paper defines and investigates the\nRefusal Loss of LLMs and then proposes a method called Gradient Cuff to detect\njailbreak attempts. Gradient Cuff exploits the unique properties observed in\nthe refusal loss landscape, including functional values and its smoothness, to\ndesign an effective two-step detection strategy. Experimental results on two\naligned LLMs (LLaMA-2-7B-Chat and Vicuna-7B-V1.5) and six types of jailbreak\nattacks (GCG, AutoDAN, PAIR, TAP, Base64, and LRL) show that Gradient Cuff can\nsignificantly improve the LLM's rejection capability for malicious jailbreak\nqueries, while maintaining the model's performance for benign user queries by\nadjusting the detection threshold.", "published": "2024-03-01 03:29:54", "link": "http://arxiv.org/abs/2403.00867v3", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Teach LLMs to Phish: Stealing Private Information from Language Models", "abstract": "When large language models are trained on private data, it can be a\nsignificant privacy risk for them to memorize and regurgitate sensitive\ninformation. In this work, we propose a new practical data extraction attack\nthat we call \"neural phishing\". This attack enables an adversary to target and\nextract sensitive or personally identifiable information (PII), e.g., credit\ncard numbers, from a model trained on user data with upwards of 10% attack\nsuccess rates, at times, as high as 50%. Our attack assumes only that an\nadversary can insert as few as 10s of benign-appearing sentences into the\ntraining dataset using only vague priors on the structure of the user data.", "published": "2024-03-01 06:15:07", "link": "http://arxiv.org/abs/2403.00871v1", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "A Regularization-based Transfer Learning Method for Information\n  Extraction via Instructed Graph Decoder", "abstract": "Information extraction (IE) aims to extract complex structured information\nfrom the text. Numerous datasets have been constructed for various IE tasks,\nleading to time-consuming and labor-intensive data annotations. Nevertheless,\nmost prevailing methods focus on training task-specific models, while the\ncommon knowledge among different IE tasks is not explicitly modeled. Moreover,\nthe same phrase may have inconsistent labels in different tasks, which poses a\nbig challenge for knowledge transfer using a unified model. In this study, we\npropose a regularization-based transfer learning method for IE (TIE) via an\ninstructed graph decoder. Specifically, we first construct an instruction pool\nfor datasets from all well-known IE tasks, and then present an instructed graph\ndecoder, which decodes various complex structures into a graph uniformly based\non corresponding instructions. In this way, the common knowledge shared with\nexisting datasets can be learned and transferred to a new dataset with new\nlabels. Furthermore, to alleviate the label inconsistency problem among various\nIE tasks, we introduce a task-specific regularization strategy, which does not\nupdate the gradients of two tasks with 'opposite direction'. We conduct\nextensive experiments on 12 datasets spanning four IE tasks, and the results\ndemonstrate the great advantages of our proposed method", "published": "2024-03-01 13:04:12", "link": "http://arxiv.org/abs/2403.00891v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Comparing large language models and human programmers for generating\n  programming code", "abstract": "We systematically evaluated the performance of seven large language models in\ngenerating programming code using various prompt strategies, programming\nlanguages, and task difficulties. GPT-4 substantially outperforms other large\nlanguage models, including Gemini Ultra and Claude 2. The coding performance of\nGPT-4 varies considerably with different prompt strategies. In most LeetCode\nand GeeksforGeeks coding contests evaluated in this study, GPT-4 employing the\noptimal prompt strategy outperforms 85 percent of human participants.\nAdditionally, GPT-4 demonstrates strong capabilities in translating code\nbetween different programming languages and in learning from past errors. The\ncomputational efficiency of the code generated by GPT-4 is comparable to that\nof human programmers. These results suggest that GPT-4 has the potential to\nserve as a reliable assistant in programming code generation and software\ndevelopment.", "published": "2024-03-01 14:43:06", "link": "http://arxiv.org/abs/2403.00894v2", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.PL"], "primary_category": "cs.SE"}
{"title": "Differentially Private Knowledge Distillation via Synthetic Text\n  Generation", "abstract": "Large Language models (LLMs) are achieving state-of-the-art performance in\nmany different downstream tasks. However, the increasing urgency of data\nprivacy puts pressure on practitioners to train LLMs with Differential Privacy\n(DP) on private data. Concurrently, the exponential growth in parameter size of\nLLMs necessitates model compression before deployment of LLMs on\nresource-constrained devices or latency-sensitive applications. Differential\nprivacy and model compression generally must trade off utility loss to achieve\ntheir objectives. Moreover, simultaneously applying both schemes can compound\nthe utility degradation. To this end, we propose DistilDP: a novel\ndifferentially private knowledge distillation algorithm that exploits synthetic\ndata generated by a differentially private teacher LLM. The knowledge of a\nteacher LLM is transferred onto the student in two ways: one way from the\nsynthetic data itself -- the hard labels, and the other way by the output\ndistribution of the teacher evaluated on the synthetic data -- the soft labels.\nFurthermore, if the teacher and student share a similar architectural\nstructure, we can further distill knowledge by aligning the hidden\nrepresentations between both. Our experimental results demonstrate that\nDistilDP can substantially improve the utility over existing baselines, at\nleast $9.0$ PPL on the Big Patent dataset, with strong privacy parameters,\n$\\epsilon=2$. These promising results progress privacy-preserving compression\nof autoregressive LLMs. Our code can be accessed here:\nhttps://github.com/james-flemings/dp_compress.", "published": "2024-03-01 19:22:24", "link": "http://arxiv.org/abs/2403.00932v2", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Merging Text Transformer Models from Different Initializations", "abstract": "Recent work on permutation-based model merging has shown impressive low- or\nzero-barrier mode connectivity between models from completely different\ninitializations. However, this line of work has not yet extended to the\nTransformer architecture, despite its dominant popularity in the language\ndomain. Therefore, in this work, we investigate the extent to which separate\nTransformer minima learn similar features, and propose a model merging\ntechnique to investigate the relationship between these minima in the loss\nlandscape. The specifics of the architecture, like its residual connections,\nmulti-headed attention, and discrete, sequential input, require specific\ninterventions in order to compute model permutations that remain within the\nsame functional equivalence class. In merging these models with our method, we\nconsistently find lower loss barriers between minima compared to model\naveraging, across models trained on a masked-language modeling task or\nfine-tuned on a language understanding benchmark. Our results show that the\nminima of these models are less sharp and isolated than previously understood,\nand provide a basis for future work on merging separately trained Transformer\nmodels.", "published": "2024-03-01 21:16:29", "link": "http://arxiv.org/abs/2403.00986v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Leveraging Prompt-Based Large Language Models: Predicting Pandemic\n  Health Decisions and Outcomes Through Social Media Language", "abstract": "We introduce a multi-step reasoning framework using prompt-based LLMs to\nexamine the relationship between social media language patterns and trends in\nnational health outcomes. Grounded in fuzzy-trace theory, which emphasizes the\nimportance of gists of causal coherence in effective health communication, we\nintroduce Role-Based Incremental Coaching (RBIC), a prompt-based LLM framework,\nto identify gists at-scale. Using RBIC, we systematically extract gists from\nsubreddit discussions opposing COVID-19 health measures (Study 1). We then\ntrack how these gists evolve across key events (Study 2) and assess their\ninfluence on online engagement (Study 3). Finally, we investigate how the\nvolume of gists is associated with national health trends like vaccine uptake\nand hospitalizations (Study 4). Our work is the first to empirically link\nsocial media linguistic patterns to real-world public health trends,\nhighlighting the potential of prompt-based LLMs in identifying critical online\ndiscussion patterns that can form the basis of public health communication\nstrategies.", "published": "2024-03-01 21:29:32", "link": "http://arxiv.org/abs/2403.00994v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.SI"], "primary_category": "cs.HC"}
{"title": "Transcription and translation of videos using fine-tuned XLSR Wav2Vec2\n  on custom dataset and mBART", "abstract": "This research addresses the challenge of training an ASR model for\npersonalized voices with minimal data. Utilizing just 14 minutes of custom\naudio from a YouTube video, we employ Retrieval-Based Voice Conversion (RVC) to\ncreate a custom Common Voice 16.0 corpus. Subsequently, a Cross-lingual\nSelf-supervised Representations (XLSR) Wav2Vec2 model is fine-tuned on this\ndataset. The developed web-based GUI efficiently transcribes and translates\ninput Hindi videos. By integrating XLSR Wav2Vec2 and mBART, the system aligns\nthe translated text with the video timeline, delivering an accessible solution\nfor multilingual video content transcription and translation for personalized\nvoice.", "published": "2024-03-01 01:15:45", "link": "http://arxiv.org/abs/2403.00212v1", "categories": ["cs.CL", "cs.CV", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "SEGAA: A Unified Approach to Predicting Age, Gender, and Emotion in\n  Speech", "abstract": "The interpretation of human voices holds importance across various\napplications. This study ventures into predicting age, gender, and emotion from\nvocal cues, a field with vast applications. Voice analysis tech advancements\nspan domains, from improving customer interactions to enhancing healthcare and\nretail experiences. Discerning emotions aids mental health, while age and\ngender detection are vital in various contexts. Exploring deep learning models\nfor these predictions involves comparing single, multi-output, and sequential\nmodels highlighted in this paper. Sourcing suitable data posed challenges,\nresulting in the amalgamation of the CREMA-D and EMO-DB datasets. Prior work\nshowed promise in individual predictions, but limited research considered all\nthree variables simultaneously. This paper identifies flaws in an individual\nmodel approach and advocates for our novel multi-output learning architecture\nSpeech-based Emotion Gender and Age Analysis (SEGAA) model. The experiments\nsuggest that Multi-output models perform comparably to individual models,\nefficiently capturing the intricate relationships between variables and speech\ninputs, all while achieving improved runtime.", "published": "2024-03-01 11:28:37", "link": "http://arxiv.org/abs/2403.00887v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The Impact of Frequency Bands on Acoustic Anomaly Detection of Machines\n  using Deep Learning Based Model", "abstract": "In this paper, we propose a deep learning based model for Acoustic Anomaly\nDetection of Machines, the task for detecting abnormal machines by analysing\nthe machine sound. By conducting extensive experiments, we indicate that\nmultiple techniques of pseudo audios, audio segment, data augmentation,\nMahalanobis distance, and narrow frequency bands, which mainly focus on feature\nengineering, are effective to enhance the system performance. Among the\nevaluating techniques, the narrow frequency bands presents a significant\nimpact. Indeed, our proposed model, which focuses on the narrow frequency\nbands, outperforms the DCASE baseline on the benchmark dataset of DCASE 2022\nTask 2 Development set. The important role of the narrow frequency bands\nindicated in this paper inspires the research community on the task of Acoustic\nAnomaly Detection of Machines to further investigate and propose novel network\narchitectures focusing on the frequency bands.", "published": "2024-03-01 09:05:55", "link": "http://arxiv.org/abs/2403.00379v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Scaling Up Adaptive Filter Optimizers", "abstract": "We introduce a new online adaptive filtering method called supervised\nmulti-step adaptive filters (SMS-AF). Our method uses neural networks to\ncontrol or optimize linear multi-delay or multi-channel frequency-domain\nfilters and can flexibly scale-up performance at the cost of increased compute\n-- a property rarely addressed in the AF literature, but critical for many\napplications. To do so, we extend recent work with a set of improvements\nincluding feature pruning, a supervised loss, and multiple optimization steps\nper time-frame. These improvements work in a cohesive manner to unlock scaling.\nFurthermore, we show how our method relates to Kalman filtering and\nmeta-adaptive filtering, making it seamlessly applicable to a diverse set of AF\ntasks. We evaluate our method on acoustic echo cancellation (AEC) and\nmulti-channel speech enhancement tasks and compare against several baselines on\nstandard synthetic and real-world datasets. Results show our method performance\nscales with inference cost and model capacity, yields multi-dB performance\ngains for both tasks, and is real-time capable on a single CPU core.", "published": "2024-03-01 20:58:43", "link": "http://arxiv.org/abs/2403.00977v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "CustomListener: Text-guided Responsive Interaction for User-friendly\n  Listening Head Generation", "abstract": "Listening head generation aims to synthesize a non-verbal responsive listener\nhead by modeling the correlation between the speaker and the listener in\ndynamic conversion.The applications of listener agent generation in virtual\ninteraction have promoted many works achieving the diverse and fine-grained\nmotion generation. However, they can only manipulate motions through simple\nemotional labels, but cannot freely control the listener's motions. Since\nlistener agents should have human-like attributes (e.g. identity, personality)\nwhich can be freely customized by users, this limits their realism. In this\npaper, we propose a user-friendly framework called CustomListener to realize\nthe free-form text prior guided listener generation. To achieve\nspeaker-listener coordination, we design a Static to Dynamic Portrait module\n(SDP), which interacts with speaker information to transform static text into\ndynamic portrait token with completion rhythm and amplitude information. To\nachieve coherence between segments, we design a Past Guided Generation Module\n(PGG) to maintain the consistency of customized listener attributes through the\nmotion prior, and utilize a diffusion-based structure conditioned on the\nportrait token and the motion prior to realize the controllable generation. To\ntrain and evaluate our model, we have constructed two text-annotated listening\nhead datasets based on ViCo and RealTalk, which provide text-video paired\nlabels. Extensive experiments have verified the effectiveness of our model.", "published": "2024-03-01 04:31:56", "link": "http://arxiv.org/abs/2403.00274v2", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Efficient Adapter Tuning of Pre-trained Speech Models for Automatic\n  Speaker Verification", "abstract": "With excellent generalization ability, self-supervised speech models have\nshown impressive performance on various downstream speech tasks in the\npre-training and fine-tuning paradigm. However, as the growing size of\npre-trained models, fine-tuning becomes practically unfeasible due to heavy\ncomputation and storage overhead, as well as the risk of overfitting. Adapters\nare lightweight modules inserted into pre-trained models to facilitate\nparameter-efficient adaptation. In this paper, we propose an effective adapter\nframework designed for adapting self-supervised speech models to the speaker\nverification task. With a parallel adapter design, our proposed framework\ninserts two types of adapters into the pre-trained model, allowing the\nadaptation of latent features within intermediate Transformer layers and output\nembeddings from all Transformer layers. We conduct comprehensive experiments to\nvalidate the efficiency and effectiveness of the proposed framework.\nExperimental results on the VoxCeleb1 dataset demonstrate that the proposed\nadapters surpass fine-tuning and other parameter-efficient transfer learning\nmethods, achieving superior performance while updating only 5% of the\nparameters.", "published": "2024-03-01 05:32:14", "link": "http://arxiv.org/abs/2403.00293v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "VoxGenesis: Unsupervised Discovery of Latent Speaker Manifold for Speech\n  Synthesis", "abstract": "Achieving nuanced and accurate emulation of human voice has been a\nlongstanding goal in artificial intelligence. Although significant progress has\nbeen made in recent years, the mainstream of speech synthesis models still\nrelies on supervised speaker modeling and explicit reference utterances.\nHowever, there are many aspects of human voice, such as emotion, intonation,\nand speaking style, for which it is hard to obtain accurate labels. In this\npaper, we propose VoxGenesis, a novel unsupervised speech synthesis framework\nthat can discover a latent speaker manifold and meaningful voice editing\ndirections without supervision. VoxGenesis is conceptually simple. Instead of\nmapping speech features to waveforms deterministically, VoxGenesis transforms a\nGaussian distribution into speech distributions conditioned and aligned by\nsemantic tokens. This forces the model to learn a speaker distribution\ndisentangled from the semantic content. During the inference, sampling from the\nGaussian distribution enables the creation of novel speakers with distinct\ncharacteristics. More importantly, the exploration of latent space uncovers\nhuman-interpretable directions associated with specific speaker characteristics\nsuch as gender attributes, pitch, tone, and emotion, allowing for voice editing\nby manipulating the latent codes along these identified directions. We conduct\nextensive experiments to evaluate the proposed VoxGenesis using both subjective\nand objective metrics, finding that it produces significantly more diverse and\nrealistic speakers with distinct characteristics than the previous approaches.\nWe also show that latent space manipulation produces consistent and\nhuman-identifiable effects that are not detrimental to the speech quality,\nwhich was not possible with previous approaches. Audio samples of VoxGenesis\ncan be found at: \\url{https://bit.ly/VoxGenesis}.", "published": "2024-03-01 13:39:56", "link": "http://arxiv.org/abs/2403.00529v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
