{"title": "Achieving Conversational Goals with Unsupervised Post-hoc Knowledge\n  Injection", "abstract": "A limitation of current neural dialog models is that they tend to suffer from\na lack of specificity and informativeness in generated responses, primarily due\nto dependence on training data that covers a limited variety of scenarios and\nconveys limited knowledge. One way to alleviate this issue is to extract\nrelevant knowledge from external sources at decoding time and incorporate it\ninto the dialog response. In this paper, we propose a post-hoc\nknowledge-injection technique where we first retrieve a diverse set of relevant\nknowledge snippets conditioned on both the dialog history and an initial\nresponse from an existing dialog model. We construct multiple candidate\nresponses, individually injecting each retrieved snippet into the initial\nresponse using a gradient-based decoding method, and then select the final\nresponse with an unsupervised ranking step. Our experiments in goal-oriented\nand knowledge-grounded dialog settings demonstrate that human annotators judge\nthe outputs from the proposed method to be more engaging and informative\ncompared to responses from prior dialog systems. We further show that\nknowledge-augmentation promotes success in achieving conversational goals in\nboth experimental settings.", "published": "2022-03-22 00:42:27", "link": "http://arxiv.org/abs/2203.11399v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "VLSP 2021 - ViMRC Challenge: Vietnamese Machine Reading Comprehension", "abstract": "One of the emerging research trends in natural language understanding is\nmachine reading comprehension (MRC) which is the task to find answers to human\nquestions based on textual data. Existing Vietnamese datasets for MRC research\nconcentrate solely on answerable questions. However, in reality, questions can\nbe unanswerable for which the correct answer is not stated in the given textual\ndata. To address the weakness, we provide the research community with a\nbenchmark dataset named UIT-ViQuAD 2.0 for evaluating the MRC task and question\nanswering systems for the Vietnamese language. We use UIT-ViQuAD 2.0 as a\nbenchmark dataset for the challenge on Vietnamese MRC at the Eighth Workshop on\nVietnamese Language and Speech Processing (VLSP 2021). This task attracted 77\nparticipant teams from 34 universities and other organizations. In this\narticle, we present details of the organization of the challenge, an overview\nof the methods employed by shared-task participants, and the results. The\nhighest performances are 77.24% in F1-score and 67.43% in Exact Match on the\nprivate test set. The Vietnamese MRC systems proposed by the top 3 teams use\nXLM-RoBERTa, a powerful pre-trained language model based on the transformer\narchitecture. The UIT-ViQuAD 2.0 dataset motivates researchers to further\nexplore the Vietnamese machine reading comprehension task and related tasks\nsuch as question answering, question generation, and natural language\ninference.", "published": "2022-03-22 00:44:41", "link": "http://arxiv.org/abs/2203.11400v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Task-guided Disentangled Tuning for Pretrained Language Models", "abstract": "Pretrained language models (PLMs) trained on large-scale unlabeled corpus are\ntypically fine-tuned on task-specific downstream datasets, which have produced\nstate-of-the-art results on various NLP tasks. However, the data discrepancy\nissue in domain and scale makes fine-tuning fail to efficiently capture\ntask-specific patterns, especially in the low data regime. To address this\nissue, we propose Task-guided Disentangled Tuning (TDT) for PLMs, which\nenhances the generalization of representations by disentangling task-relevant\nsignals from the entangled representations. For a given task, we introduce a\nlearnable confidence model to detect indicative guidance from context, and\nfurther propose a disentangled regularization to mitigate the over-reliance\nproblem. Experimental results on GLUE and CLUE benchmarks show that TDT gives\nconsistently better results than fine-tuning with different PLMs, and extensive\nanalysis demonstrates the effectiveness and robustness of our method. Code is\navailable at https://github.com/lemon0830/TDT.", "published": "2022-03-22 03:11:39", "link": "http://arxiv.org/abs/2203.11431v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Demo of the Linguistic Field Data Management and Analysis System -- LiFE", "abstract": "In the proposed demo, we will present a new software - Linguistic Field Data\nManagement and Analysis System - LiFE (https://github.com/kmi-linguistics/life)\n- an open-source, web-based linguistic data management and analysis application\nthat allows for systematic storage, management, sharing and usage of linguistic\ndata collected from the field. The application allows users to store lexical\nitems, sentences, paragraphs, audio-visual content with rich glossing /\nannotation; generate interactive and print dictionaries; and also train and use\nnatural language processing tools and models for various purposes using this\ndata. Since its a web-based application, it also allows for seamless\ncollaboration among multiple persons and sharing the data, models, etc with\neach other.\n  The system uses the Python-based Flask framework and MongoDB in the backend\nand HTML, CSS and Javascript at the frontend. The interface allows creation of\nmultiple projects that could be shared with the other users. At the backend,\nthe application stores the data in RDF format so as to allow its release as\nLinked Data over the web using semantic web technologies - as of now it makes\nuse of the OntoLex-Lemon for storing the lexical data and Ligt for storing the\ninterlinear glossed text and then internally linking it to the other linked\nlexicons and databases such as DBpedia and WordNet. Furthermore it provides\nsupport for training the NLP systems using scikit-learn and HuggingFace\nTransformers libraries as well as make use of any model trained using these\nlibraries - while the user interface itself provides limited options for tuning\nthe system, an externally-trained model could be easily incorporated within the\napplication; similarly the dataset itself could be easily exported into a\nstandard machine-readable format like JSON or CSV that could be consumed by\nother programs and pipelines.", "published": "2022-03-22 03:34:10", "link": "http://arxiv.org/abs/2203.11443v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SU-NLP at SemEval-2022 Task 11: Complex Named Entity Recognition with\n  Entity Linking", "abstract": "This paper describes the system proposed by Sabanc{\\i} University Natural\nLanguage Processing Group in the SemEval-2022 MultiCoNER task. We developed an\nunsupervised entity linking pipeline that detects potential entity mentions\nwith the help of Wikipedia and also uses the corresponding Wikipedia context to\nhelp the classifier in finding the named entity type of that mention. Our\nresults showed that our pipeline improved performance significantly, especially\nfor complex entities in low-context settings.", "published": "2022-03-22 16:09:34", "link": "http://arxiv.org/abs/2203.11841v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transformer based ensemble for emotion detection", "abstract": "Detecting emotions in languages is important to accomplish a complete\ninteraction between humans and machines. This paper describes our contribution\nto the WASSA 2022 shared task which handles this crucial task of emotion\ndetection. We have to identify the following emotions: sadness, surprise,\nneutral, anger, fear, disgust, joy based on a given essay text. We are using an\nensemble of ELECTRA and BERT models to tackle this problem achieving an F1\nscore of $62.76\\%$. Our codebase (https://bit.ly/WASSA_shared_task) and our\nWandB project (https://wandb.ai/acl_wassa_pictxmanipal/acl_wassa) is publicly\navailable.", "published": "2022-03-22 17:11:18", "link": "http://arxiv.org/abs/2203.11899v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Suum Cuique: Studying Bias in Taboo Detection with a Community\n  Perspective", "abstract": "Prior research has discussed and illustrated the need to consider linguistic\nnorms at the community level when studying taboo (hateful/offensive/toxic etc.)\nlanguage. However, a methodology for doing so, that is firmly founded on\ncommunity language norms is still largely absent. This can lead both to biases\nin taboo text classification and limitations in our understanding of the causes\nof bias. We propose a method to study bias in taboo classification and\nannotation where a community perspective is front and center. This is\naccomplished by using special classifiers tuned for each community's language.\nIn essence, these classifiers represent community level language norms. We use\nthese to study bias and find, for example, biases are largest against African\nAmericans (7/10 datasets and all 3 classifiers examined). In contrast to\nprevious papers we also study other communities and find, for example, strong\nbiases against South Asians. In a small scale user study we illustrate our key\nidea which is that common utterances, i.e., those with high alignment scores\nwith a community (community classifier confidence scores) are unlikely to be\nregarded taboo. Annotators who are community members contradict taboo\nclassification decisions and annotations in a majority of instances. This paper\nis a significant step toward reducing false positive taboo decisions that over\ntime harm minority communities.", "published": "2022-03-22 00:45:39", "link": "http://arxiv.org/abs/2203.11401v1", "categories": ["cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Learning Confidence for Transformer-based Neural Machine Translation", "abstract": "Confidence estimation aims to quantify the confidence of the model\nprediction, providing an expectation of success. A well-calibrated confidence\nestimate enables accurate failure prediction and proper risk measurement when\ngiven noisy samples and out-of-distribution data in real-world settings.\nHowever, this task remains a severe challenge for neural machine translation\n(NMT), where probabilities from softmax distribution fail to describe when the\nmodel is probably mistaken. To address this problem, we propose an unsupervised\nconfidence estimate learning jointly with the training of the NMT model. We\nexplain confidence as how many hints the NMT model needs to make a correct\nprediction, and more hints indicate low confidence. Specifically, the NMT model\nis given the option to ask for hints to improve translation accuracy at the\ncost of some slight penalty. Then, we approximate their level of confidence by\ncounting the number of hints the model uses. We demonstrate that our learned\nconfidence estimate achieves high accuracy on extensive sentence/word-level\nquality estimation tasks. Analytical results verify that our confidence\nestimate can correctly assess underlying risk in two real-world scenarios: (1)\ndiscovering noisy samples and (2) detecting out-of-domain data. We further\npropose a novel confidence-based instance-specific label smoothing approach\nbased on our learned confidence estimate, which outperforms standard label\nsmoothing.", "published": "2022-03-22 01:51:58", "link": "http://arxiv.org/abs/2203.11413v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Abstractive Grounded Summarization of Podcast Transcripts", "abstract": "Podcasts have recently shown a rapid rise in popularity. Summarization of\npodcast transcripts is of practical benefit to both content providers and\nconsumers. It helps consumers to quickly decide whether they will listen to the\npodcasts and reduces the cognitive load of content providers to write\nsummaries. Nevertheless, podcast summarization faces significant challenges\nincluding factual inconsistencies with respect to the inputs. The problem is\nexacerbated by speech disfluencies and recognition errors in transcripts of\nspoken language. In this paper, we explore a novel abstractive summarization\nmethod to alleviate these challenges. Specifically, our approach learns to\nproduce an abstractive summary while grounding summary segments in specific\nportions of the transcript to allow for full inspection of summary details. We\nconduct a series of analyses of the proposed approach on a large podcast\ndataset and show that the approach can achieve promising results. Grounded\nsummaries bring clear benefits in locating the summary and transcript segments\nthat contain inconsistent information, and hence significantly improve\nsummarization quality in both automatic and human evaluation metrics.", "published": "2022-03-22 02:44:39", "link": "http://arxiv.org/abs/2203.11425v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "WuDaoMM: A large-scale Multi-Modal Dataset for Pre-training models", "abstract": "Compared with the domain-specific model, the vision-language pre-training\nmodels (VLPMs) have shown superior performance on downstream tasks with fast\nfine-tuning process. For example, ERNIE-ViL, Oscar and UNIMO trained VLPMs with\na uniform transformers stack architecture and large amounts of image-text\npaired data, achieving remarkable results on downstream tasks such as\nimage-text reference(IR and TR), vision question answering (VQA) and image\ncaptioning (IC) etc. During the training phase, VLPMs are always fed with a\ncombination of multiple public datasets to meet the demand of large-scare\ntraining data. However, due to the unevenness of data distribution including\nsize, task type and quality, using the mixture of multiple datasets for model\ntraining can be problematic. In this work, we introduce a large-scale\nmulti-modal corpora named WuDaoMM, totally containing more than 650M image-text\npairs. Specifically, about 600 million pairs of data are collected from\nmultiple webpages in which image and caption present weak correlation, and the\nother 50 million strong-related image-text pairs are collected from some\nhigh-quality graphic websites. We also release a base version of WuDaoMM with 5\nmillion strong-correlated image-text pairs, which is sufficient to support the\ncommon cross-modal model pre-training. Besides, we trained both an\nunderstanding and a generation vision-language (VL) model to test the dataset\neffectiveness. The results show that WuDaoMM can be applied as an efficient\ndataset for VLPMs, especially for the model in text-to-image generation task.\nThe data is released at https://data.wudaoai.cn", "published": "2022-03-22 06:12:20", "link": "http://arxiv.org/abs/2203.11480v5", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Approaches for Improving the Performance of Fake News Detection in\n  Bangla: Imbalance Handling and Model Stacking", "abstract": "Imbalanced datasets can lead to biasedness into the detection of fake news.\nIn this work, we present several strategies for resolving the imbalance issue\nfor fake news detection in Bangla with a comparative assessment of proposed\nmethodologies. Additionally, we propose a technique for improving performance\neven when the dataset is imbalanced. We applied our proposed approaches to\nBanFakeNews, a dataset developed for the purpose of detecting fake news in\nBangla comprising of 50K instances but is significantly skewed, with 97% of\nmajority instances. We obtained a 93.1% F1-score using data manipulation\nmanipulation techniques such as SMOTE, and a 79.1% F1-score using without data\nmanipulation approaches such as Stacked Generalization. Without implementing\nthese techniques, the F1-score would have been 67.6% for baseline models. We\nsee this work as an important step towards paving the way of fake news\ndetection in Bangla. By implementing these strategies the obstacles of\nimbalanced dataset can be removed and improvement in the performance can be\nachieved.", "published": "2022-03-22 06:33:01", "link": "http://arxiv.org/abs/2203.11486v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Factual Consistency of Multilingual Pretrained Language Models", "abstract": "Pretrained language models can be queried for factual knowledge, with\npotential applications in knowledge base acquisition and tasks that require\ninference. However, for that, we need to know how reliable this knowledge is,\nand recent work has shown that monolingual English language models lack\nconsistency when predicting factual knowledge, that is, they fill-in-the-blank\ndifferently for paraphrases describing the same fact. In this paper, we extend\nthe analysis of consistency to a multilingual setting. We introduce a resource,\nmParaRel, and investigate (i) whether multilingual language models such as\nmBERT and XLM-R are more consistent than their monolingual counterparts; and\n(ii) if such models are equally consistent across languages. We find that mBERT\nis as inconsistent as English BERT in English paraphrases, but that both mBERT\nand XLM-R exhibit a high degree of inconsistency in English and even more so\nfor all the other 45 languages.", "published": "2022-03-22 09:15:53", "link": "http://arxiv.org/abs/2203.11552v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Utterance Rewriting with Contrastive Learning in Multi-turn Dialogue", "abstract": "Context modeling plays a significant role in building multi-turn dialogue\nsystems. In order to make full use of context information, systems can use\nIncomplete Utterance Rewriting(IUR) methods to simplify the multi-turn dialogue\ninto single-turn by merging current utterance and context information into a\nself-contained utterance. However, previous approaches ignore the intent\nconsistency between the original query and rewritten query. The detection of\nomitted or coreferred locations in the original query can be further improved.\nIn this paper, we introduce contrastive learning and multi-task learning to\njointly model the problem. Our method benefits from carefully designed\nself-supervised objectives, which act as auxiliary tasks to capture semantics\nat both sentence-level and token-level. The experiments show that our proposed\nmodel achieves state-of-the-art performance on several public datasets.", "published": "2022-03-22 10:13:27", "link": "http://arxiv.org/abs/2203.11587v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "HOP: History-and-Order Aware Pre-training for Vision-and-Language\n  Navigation", "abstract": "Pre-training has been adopted in a few of recent works for\nVision-and-Language Navigation (VLN). However, previous pre-training methods\nfor VLN either lack the ability to predict future actions or ignore the\ntrajectory contexts, which are essential for a greedy navigation process. In\nthis work, to promote the learning of spatio-temporal visual-textual\ncorrespondence as well as the agent's capability of decision making, we propose\na novel history-and-order aware pre-training paradigm (HOP) with VLN-specific\nobjectives that exploit the past observations and support future action\nprediction. Specifically, in addition to the commonly used Masked Language\nModeling (MLM) and Trajectory-Instruction Matching (TIM), we design two proxy\ntasks to model temporal order information: Trajectory Order Modeling (TOM) and\nGroup Order Modeling (GOM). Moreover, our navigation action prediction is also\nenhanced by introducing the task of Action Prediction with History (APH), which\ntakes into account the history visual perceptions. Extensive experimental\nresults on four downstream VLN tasks (R2R, REVERIE, NDH, RxR) demonstrate the\neffectiveness of our proposed method compared against several state-of-the-art\nagents.", "published": "2022-03-22 10:17:12", "link": "http://arxiv.org/abs/2203.11591v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Learning Relation-Specific Representations for Few-shot Knowledge Graph\n  Completion", "abstract": "Recent years have witnessed increasing interest in few-shot knowledge graph\ncompletion (FKGC), which aims to infer unseen query triples for a few-shot\nrelation using a few reference triples about the relation. The primary focus of\nexisting FKGC methods lies in learning relation representations that can\nreflect the common information shared by the query and reference triples. To\nthis end, these methods learn entity-pair representations from the direct\nneighbors of head and tail entities, and then aggregate the representations of\nreference entity pairs. However, the entity-pair representations learned only\nfrom direct neighbors may have low expressiveness when the involved entities\nhave sparse direct neighbors or share a common local neighborhood with other\nentities. Moreover, merely modeling the semantic information of head and tail\nentities is insufficient to accurately infer their relational information\nespecially when they have multiple relations. To address these issues, we\npropose a Relation-Specific Context Learning (RSCL) framework, which exploits\ngraph contexts of triples to learn global and local relation-specific\nrepresentations for few-shot relations. Specifically, we first extract graph\ncontexts for each triple, which can provide long-term entity-relation\ndependencies. To encode the extracted graph contexts, we then present a\nhierarchical attention network to capture contextualized information of triples\nand highlight valuable local neighborhood information of entities. Finally, we\ndesign a hybrid attention aggregator to evaluate the likelihood of the query\ntriples at the global and local levels. Experimental results on two public\ndatasets demonstrate that RSCL outperforms state-of-the-art FKGC methods.", "published": "2022-03-22 11:45:48", "link": "http://arxiv.org/abs/2203.11639v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Meta-learning for Low-resource Text Classification and\n  Generation via Memory Imitation", "abstract": "Building models of natural language processing (NLP) is challenging in\nlow-resource scenarios where only limited data are available.\nOptimization-based meta-learning algorithms achieve promising results in\nlow-resource scenarios by adapting a well-generalized model initialization to\nhandle new tasks. Nonetheless, these approaches suffer from the memorization\noverfitting issue, where the model tends to memorize the meta-training tasks\nwhile ignoring support sets when adapting to new tasks. To address this issue,\nwe propose a memory imitation meta-learning (MemIML) method that enhances the\nmodel's reliance on support sets for task adaptation. Specifically, we\nintroduce a task-specific memory module to store support set information and\nconstruct an imitation module to force query sets to imitate the behaviors of\nsome representative support-set samples stored in the memory. A theoretical\nanalysis is provided to prove the effectiveness of our method, and empirical\nresults also demonstrate that our method outperforms competitive baselines on\nboth text classification and generation tasks.", "published": "2022-03-22 12:41:55", "link": "http://arxiv.org/abs/2203.11670v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Listening to Affected Communities to Define Extreme Speech: Dataset and\n  Experiments", "abstract": "Building on current work on multilingual hate speech (e.g., Ousidhoum et al.\n(2019)) and hate speech reduction (e.g., Sap et al. (2020)), we present\nXTREMESPEECH, a new hate speech dataset containing 20,297 social media passages\nfrom Brazil, Germany, India and Kenya. The key novelty is that we directly\ninvolve the affected communities in collecting and annotating the data - as\nopposed to giving companies and governments control over defining and\ncombatting hate speech. This inclusive approach results in datasets more\nrepresentative of actually occurring online speech and is likely to facilitate\nthe removal of the social media content that marginalized communities view as\ncausing the most harm. Based on XTREMESPEECH, we establish novel tasks with\naccompanying baselines, provide evidence that cross-country training is\ngenerally not feasible due to cultural differences between countries and\nperform an interpretability analysis of BERT's predictions.", "published": "2022-03-22 14:24:56", "link": "http://arxiv.org/abs/2203.11764v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Computational Approach to Understand Mental Health from Reddit:\n  Knowledge-aware Multitask Learning Framework", "abstract": "Analyzing gender is critical to study mental health (MH) support in CVD\n(cardiovascular disease). The existing studies on using social media for\nextracting MH symptoms consider symptom detection and tend to ignore user\ncontext, disease, or gender. The current study aims to design and evaluate a\nsystem to capture how MH symptoms associated with CVD are expressed differently\nwith the gender on social media. We observe that the reliable detection of MH\nsymptoms expressed by persons with heart disease in user posts is challenging\nbecause of the co-existence of (dis)similar MH symptoms in one post and due to\nvariation in the description of symptoms based on gender. We collect a corpus\nof $150k$ items (posts and comments) annotated using the subreddit labels and\ntransfer learning approaches. We propose GeM, a novel task-adaptive multi-task\nlearning approach to identify the MH symptoms in CVD patients based on gender.\nSpecifically, we adapt a knowledge-assisted RoBERTa based bi-encoder model to\ncapture CVD-related MH symptoms. Moreover, it enhances the reliability for\ndifferentiating the gender language in MH symptoms when compared to the\nstate-of-art language models. Our model achieves high (statistically\nsignificant) performance and predicts four labels of MH issues and two gender\nlabels, which outperforms RoBERTa, improving the recall by 2.14% on the symptom\nidentification task and by 2.55% on the gender identification task.", "published": "2022-03-22 16:32:51", "link": "http://arxiv.org/abs/2203.11856v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Text Transformations in Contrastive Self-Supervised Learning: A Review", "abstract": "Contrastive self-supervised learning has become a prominent technique in\nrepresentation learning. The main step in these methods is to contrast\nsemantically similar and dissimilar pairs of samples. However, in the domain of\nNatural Language Processing (NLP), the augmentation methods used in creating\nsimilar pairs with regard to contrastive learning (CL) assumptions are\nchallenging. This is because, even simply modifying a word in the input might\nchange the semantic meaning of the sentence, and hence, would violate the\ndistributional hypothesis. In this review paper, we formalize the contrastive\nlearning framework, emphasize the considerations that need to be addressed in\nthe data transformation step, and review the state-of-the-art methods and\nevaluations for contrastive representation learning in NLP. Finally, we\ndescribe some challenges and potential directions for learning better text\nrepresentations using contrastive methods.", "published": "2022-03-22 19:02:43", "link": "http://arxiv.org/abs/2203.12000v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Pseudo Label Is Better Than Human Label", "abstract": "State-of-the-art automatic speech recognition (ASR) systems are trained with\ntens of thousands of hours of labeled speech data. Human transcription is\nexpensive and time consuming. Factors such as the quality and consistency of\nthe transcription can greatly affect the performance of the ASR models trained\nwith these data. In this paper, we show that we can train a strong teacher\nmodel to produce high quality pseudo labels by utilizing recent self-supervised\nand semi-supervised learning techniques. Specifically, we use JUST (Joint\nUnsupervised/Supervised Training) and iterative noisy student teacher training\nto train a 600 million parameter bi-directional teacher model. This model\nachieved 4.0% word error rate (WER) on a voice search task, 11.1% relatively\nbetter than a baseline. We further show that by using this strong teacher model\nto generate high-quality pseudo labels for training, we can achieve 13.6%\nrelative WER reduction (5.9% to 5.1%) for a streaming model compared to using\nhuman labels.", "published": "2022-03-22 00:03:13", "link": "http://arxiv.org/abs/2203.12668v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Towards Textual Out-of-Domain Detection without In-Domain Labels", "abstract": "In many real-world settings, machine learning models need to identify user\ninputs that are out-of-domain (OOD) so as to avoid performing wrong actions.\nThis work focuses on a challenging case of OOD detection, where no labels for\nin-domain data are accessible (e.g., no intent labels for the intent\nclassification task). To this end, we first evaluate different language model\nbased approaches that predict likelihood for a sequence of tokens. Furthermore,\nwe propose a novel representation learning based method by combining\nunsupervised clustering and contrastive learning so that better data\nrepresentations for OOD detection can be learned. Through extensive\nexperiments, we demonstrate that this method can significantly outperform\nlikelihood-based methods and can be even competitive to the state-of-the-art\nsupervised approaches with label information.", "published": "2022-03-22 00:11:46", "link": "http://arxiv.org/abs/2203.11396v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Modeling speech recognition and synthesis simultaneously: Encoding and\n  decoding lexical and sublexical semantic information into speech with no\n  direct access to speech data", "abstract": "Human speakers encode information into raw speech which is then decoded by\nthe listeners. This complex relationship between encoding (production) and\ndecoding (perception) is often modeled separately. Here, we test how encoding\nand decoding of lexical semantic information can emerge automatically from raw\nspeech in unsupervised generative deep convolutional networks that combine the\nproduction and perception principles of speech. We introduce, to our knowledge,\nthe most challenging objective in unsupervised lexical learning: a network that\nmust learn unique representations for lexical items with no direct access to\ntraining data. We train several models (ciwGAN and fiwGAN arXiv:2006.02951) and\ntest how the networks classify acoustic lexical items in unobserved test data.\nStrong evidence in favor of lexical learning and a causal relationship between\nlatent codes and meaningful sublexical units emerge. The architecture that\ncombines the production and perception principles is thus able to learn to\ndecode unique information from raw acoustic data without accessing real\ntraining data directly. We propose a technique to explore lexical (holistic)\nand sublexical (featural) learned representations in the classifier network.\nThe results bear implications for unsupervised speech technology, as well as\nfor unsupervised semantic modeling as language models increasingly bypass text\nand operate from raw acoustics.", "published": "2022-03-22 06:04:34", "link": "http://arxiv.org/abs/2203.11476v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A Text-to-Speech Pipeline, Evaluation Methodology, and Initial\n  Fine-Tuning Results for Child Speech Synthesis", "abstract": "Speech synthesis has come a long way as current text-to-speech (TTS) models\ncan now generate natural human-sounding speech. However, most of the TTS\nresearch focuses on using adult speech data and there has been very limited\nwork done on child speech synthesis. This study developed and validated a\ntraining pipeline for fine-tuning state-of-the-art (SOTA) neural TTS models\nusing child speech datasets. This approach adopts a multi-speaker TTS retuning\nworkflow to provide a transfer-learning pipeline. A publicly available child\nspeech dataset was cleaned to provide a smaller subset of approximately 19\nhours, which formed the basis of our fine-tuning experiments. Both subjective\nand objective evaluations were performed using a pretrained MOSNet for\nobjective evaluation and a novel subjective framework for mean opinion score\n(MOS) evaluations. Subjective evaluations achieved the MOS of 3.95 for speech\nintelligibility, 3.89 for voice naturalness, and 3.96 for voice consistency.\nObjective evaluation using a pretrained MOSNet showed a strong correlation\nbetween real and synthetic child voices. Speaker similarity was also verified\nby calculating the cosine similarity between the embeddings of utterances. An\nautomatic speech recognition (ASR) model is also used to provide a word error\nrate (WER) comparison between the real and synthetic child voices. The final\ntrained TTS model was able to synthesize child-like speech from reference audio\nsamples as short as 5 seconds.", "published": "2022-03-22 09:34:21", "link": "http://arxiv.org/abs/2203.11562v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Are You Misinformed? A Study of Covid-Related Fake News in Bengali on\n  Facebook", "abstract": "Our opinions and views of life can be shaped by how we perceive the opinions\nof others on social media like Facebook. This dependence has increased during\nCOVID-19 periods when we have fewer means to connect with others. However, fake\nnews related to COVID-19 has become a significant problem on Facebook. Bengali\nis the seventh most spoken language worldwide, yet we are aware of no previous\nresearch that studied the prevalence of COVID-19 related fake news in Bengali\non Facebook. In this paper, we develop machine learning models to detect fake\nnews in Bengali automatically. The best performing model is BERT, with an\nF1-score of 0.97. We apply BERT on all Facebook Bengali posts related to\nCOVID-19. We find 10 topics in the COVID-19 Bengali fake news grouped into\nthree categories: System (e.g., medical system), belief (e.g., religious\nrituals), and social (e.g., scientific awareness).", "published": "2022-03-22 12:41:42", "link": "http://arxiv.org/abs/2203.11669v1", "categories": ["cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "BERT-ASC: Auxiliary-Sentence Construction for Implicit Aspect Learning\n  in Sentiment Analysis", "abstract": "Aspect-based sentiment analysis (ABSA) aims to associate a text with a set of\naspects and infer their respective sentimental polarities. State-of-the-art\napproaches are built on fine-tuning pre-trained language models, focusing on\nlearning aspect-specific representations from the corpus. However, aspects are\noften expressed implicitly, making implicit mapping challenging without\nsufficient labeled examples, which may be scarce in real-world scenarios. This\npaper proposes a unified framework to address aspect categorization and\naspect-based sentiment subtasks. We introduce a mechanism to construct an\nauxiliary-sentence for the implicit aspect using the corpus's semantic\ninformation. We then encourage BERT to learn aspect-specific representation in\nresponse to this auxiliary-sentence, not the aspect itself. We evaluate our\napproach on real benchmark datasets for both ABSA and Targeted-ABSA tasks. Our\nexperiments show that it consistently achieves state-of-the-art performance in\naspect categorization and aspect-based sentiment across all datasets, with\nconsiderable improvement margins. The BERT-ASC code is available at\nhttps://github.com/amurtadha/BERT-ASC.", "published": "2022-03-22 13:12:27", "link": "http://arxiv.org/abs/2203.11702v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Girl Has A Name, And It's ... Adversarial Authorship Attribution for\n  Deobfuscation", "abstract": "Recent advances in natural language processing have enabled powerful\nprivacy-invasive authorship attribution. To counter authorship attribution,\nresearchers have proposed a variety of rule-based and learning-based text\nobfuscation approaches. However, existing authorship obfuscation approaches do\nnot consider the adversarial threat model. Specifically, they are not evaluated\nagainst adversarially trained authorship attributors that are aware of\npotential obfuscation. To fill this gap, we investigate the problem of\nadversarial authorship attribution for deobfuscation. We show that\nadversarially trained authorship attributors are able to degrade the\neffectiveness of existing obfuscators from 20-30% to 5-10%. We also evaluate\nthe effectiveness of adversarial training when the attributor makes incorrect\nassumptions about whether and which obfuscator was used. While there is a a\nclear degradation in attribution accuracy, it is noteworthy that this\ndegradation is still at or above the attribution accuracy of the attributor\nthat is not adversarially trained at all. Our results underline the need for\nstronger obfuscation approaches that are resistant to deobfuscation", "published": "2022-03-22 16:26:09", "link": "http://arxiv.org/abs/2203.11849v1", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Prompt Array Keeps the Bias Away: Debiasing Vision-Language Models\n  with Adversarial Learning", "abstract": "Vision-language models can encode societal biases and stereotypes, but there\nare challenges to measuring and mitigating these multimodal harms due to\nlacking measurement robustness and feature degradation. To address these\nchallenges, we investigate bias measures and apply ranking metrics for\nimage-text representations. We then investigate debiasing methods and show that\nprepending learned embeddings to text queries that are jointly trained with\nadversarial debiasing and a contrastive loss reduces various bias measures with\nminimal degradation to the image-text representation.", "published": "2022-03-22 17:59:04", "link": "http://arxiv.org/abs/2203.11933v4", "categories": ["cs.LG", "cs.CL", "cs.CV", "cs.CY"], "primary_category": "cs.LG"}
{"title": "Building Robust Spoken Language Understanding by Cross Attention between\n  Phoneme Sequence and ASR Hypothesis", "abstract": "Building Spoken Language Understanding (SLU) robust to Automatic Speech\nRecognition (ASR) errors is an essential issue for various voice-enabled\nvirtual assistants. Considering that most ASR errors are caused by phonetic\nconfusion between similar-sounding expressions, intuitively, leveraging the\nphoneme sequence of speech can complement ASR hypothesis and enhance the\nrobustness of SLU. This paper proposes a novel model with Cross Attention for\nSLU (denoted as CASLU). The cross attention block is devised to catch the\nfine-grained interactions between phoneme and word embeddings in order to make\nthe joint representations catch the phonetic and semantic features of input\nsimultaneously and for overcoming the ASR errors in downstream natural language\nunderstanding (NLU) tasks. Extensive experiments are conducted on three\ndatasets, showing the effectiveness and competitiveness of our approach.\nAdditionally, We also validate the universality of CASLU and prove its\ncomplementarity when combining with other robust SLU techniques.", "published": "2022-03-22 21:59:29", "link": "http://arxiv.org/abs/2203.12067v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future\n  Directions", "abstract": "A long-term goal of AI research is to build intelligent agents that can\ncommunicate with humans in natural language, perceive the environment, and\nperform real-world tasks. Vision-and-Language Navigation (VLN) is a fundamental\nand interdisciplinary research topic towards this goal, and receives increasing\nattention from natural language processing, computer vision, robotics, and\nmachine learning communities. In this paper, we review contemporary studies in\nthe emerging field of VLN, covering tasks, evaluation metrics, methods, etc.\nThrough structured analysis of current progress and challenges, we highlight\nthe limitations of current VLN and opportunities for future work. This paper\nserves as a thorough reference for the VLN research community.", "published": "2022-03-22 16:58:10", "link": "http://arxiv.org/abs/2203.12667v3", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Exploring Continuous Integrate-and-Fire for Adaptive Simultaneous Speech\n  Translation", "abstract": "Simultaneous speech translation (SimulST) is a challenging task aiming to\ntranslate streaming speech before the complete input is observed. A SimulST\nsystem generally includes two components: the pre-decision that aggregates the\nspeech information and the policy that decides to read or write. While recent\nworks had proposed various strategies to improve the pre-decision, they mainly\nadopt the fixed wait-k policy, leaving the adaptive policies rarely explored.\nThis paper proposes to model the adaptive policy by adapting the Continuous\nIntegrate-and-Fire (CIF). Compared with monotonic multihead attention (MMA),\nour method has the advantage of simpler computation, superior quality at low\nlatency, and better generalization to long utterances. We conduct experiments\non the MuST-C V2 dataset and show the effectiveness of our approach.", "published": "2022-03-22 23:33:18", "link": "http://arxiv.org/abs/2204.09595v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Joint Noise Reduction and Listening Enhancement for Full-End Speech\n  Enhancement", "abstract": "Speech enhancement (SE) methods mainly focus on recovering clean speech from\nnoisy input. In real-world speech communication, however, noises often exist in\nnot only speaker but also listener environments. Although SE methods can\nsuppress the noise contained in the speaker's voice, they cannot deal with the\nnoise that is physically present in the listener side. To address such a\ncomplicated but common scenario, we investigate a deep learning-based joint\nframework integrating noise reduction (NR) with listening enhancement (LE), in\nwhich the NR module first suppresses noise and the LE module then modifies the\ndenoised speech, i.e., the output of the NR module, to further improve speech\nintelligibility. The enhanced speech can thus be less noisy and more\nintelligible for listeners. Experimental results show that our proposed method\nachieves promising results and significantly outperforms the disjoint\nprocessing methods in terms of various speech evaluation metrics.", "published": "2022-03-22 07:21:44", "link": "http://arxiv.org/abs/2203.11500v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "CT-SAT: Contextual Transformer for Sequential Audio Tagging", "abstract": "Sequential audio event tagging can provide not only the type information of\naudio events, but also the order information between events and the number of\nevents that occur in an audio clip. Most previous works on audio event sequence\nanalysis rely on connectionist temporal classification (CTC). However, CTC's\nconditional independence assumption prevents it from effectively learning\ncorrelations between diverse audio events. This paper first attempts to\nintroduce Transformer into sequential audio tagging, since Transformers perform\nwell in sequence-related tasks. To better utilize contextual information of\naudio event sequences, we draw on the idea of bidirectional recurrent neural\nnetworks, and propose a contextual Transformer (cTransformer) with a\nbidirectional decoder that could exploit the forward and backward information\nof event sequences. Experiments on the real-life polyphonic audio dataset show\nthat, compared to CTC-based methods, the cTransformer can effectively combine\nthe fine-grained acoustic representations from the encoder and coarse-grained\naudio event cues to exploit contextual information to successfully recognize\nand predict audio event sequences.", "published": "2022-03-22 09:53:02", "link": "http://arxiv.org/abs/2203.11573v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Analysis of Disfluencies for automatic detection of Mild Cognitive\n  Impartment: a deep learning approach", "abstract": "The so-called Mild Cognitive Impairment (MCI) or cognitive loss appears in a\nprevious stage before Alzheimer's Disease (AD), but it does not seem\nsufficiently severe to interfere in independent abilities of daily life, so it\nusually does not receive an appropriate diagnosis. Its detection is a\nchallenging issue to be addressed by medical specialists. This work presents a\nnovel proposal based on automatic analysis of speech and disfluencies aimed at\nsupporting MCI diagnosis. The approach includes deep learning by means of\nConvolutional Neural Networks (CNN) and non-linear multifeature modelling.\nMoreover, to select the most relevant features non-parametric Mann-Whitney\nU-testt and Support Vector Machine Attribute (SVM) evaluation are used.", "published": "2022-03-22 10:42:12", "link": "http://arxiv.org/abs/2203.11606v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Nonlinear prediction with neural nets in ADPCM", "abstract": "In the last years there has been a growing interest for nonlinear speech\nmodels. Several works have been published revealing the better performance of\nnonlinear techniques, but little attention has been dedicated to the\nimplementation of the nonlinear model into real applications. This work is\nfocused on the study of the behaviour of a nonlinear predictive model based on\nneural nets, in a speech waveform coder. Our novel scheme obtains an\nimprovement in SEGSNR between 1 and 2 dB for an adaptive quantization ranging\nfrom 2 to 5 bits.", "published": "2022-03-22 10:59:42", "link": "http://arxiv.org/abs/2203.11612v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speaker recognition with a MLP classifier and LPCC codebook", "abstract": "This paper improves the speaker recognition rates of a MLP classifier and\nLPCC codebook alone, using a linear combination between both methods. In\nsimulations we have obtained an improvement of 4.7% over a LPCC codebook of 32\nvectors and 1.5% for a codebook of 128 vectors (error rate drops from 3.68% to\n2.1%). Also we propose an efficient algorithm that reduces the computational\ncomplexity of the LPCC-VQ system by a factor of 4.", "published": "2022-03-22 11:11:02", "link": "http://arxiv.org/abs/2203.11614v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Upmixing via style transfer: a variational autoencoder for disentangling\n  spatial images and musical content", "abstract": "In the stereo-to-multichannel upmixing problem for music, one of the main\ntasks is to set the directionality of the instrument sources in the\nmultichannel rendering results. In this paper, we propose a modified\nvariational autoencoder model that learns a latent space to describe the\nspatial images in multichannel music. We seek to disentangle the spatial images\nand music content, so the learned latent variables are invariant to the music.\nAt test time, we use the latent variables to control the panning of sources. We\npropose two upmixing use cases: transferring the spatial images from one song\nto another and blind panning based on the generative model. We report objective\nand subjective evaluation results to empirically show that our model captures\nspatial images separately from music content and achieves transfer-based\ninteractive panning.", "published": "2022-03-22 21:25:18", "link": "http://arxiv.org/abs/2203.12053v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Residual-Guided Non-Intrusive Speech Quality Assessment", "abstract": "This paper proposes an approach to improve Non-Intrusive speech quality\nassessment(NI-SQA) based on the residuals between impaired speech and enhanced\nspeech. The difficulty in our task is particularly lack of information, for\nwhich the corresponding reference speech is absent. We generate an enhanced\nspeech on the impaired speech to compensate for the absence of the reference\naudio, then pair the information of residuals with the impaired speech.\nCompared to feeding the impaired speech directly into the model, residuals\ncould bring some extra helpful information from the contrast in enhancement.\nThe human ear is sensitive to certain noises but different to deep learning\nmodel. Causing the Mean Opinion Score(MOS) the model predicted is not enough to\nfit our subjective sensitive well and causes deviation. These residuals have a\nclose relationship to reference speech and then improve the ability of the deep\nlearning models to predict MOS. During the training phase, experimental results\ndemonstrate that paired with residuals can quickly obtain better evaluation\nindicators under the same conditions. Furthermore, our final results improved\n31.3 percent and 14.1 percent, respectively, in PLCC and RMSE.", "published": "2022-03-22 07:19:58", "link": "http://arxiv.org/abs/2203.11499v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Conditional Generative Data Augmentation for Clinical Audio Datasets", "abstract": "In this work, we propose a novel data augmentation method for clinical audio\ndatasets based on a conditional Wasserstein Generative Adversarial Network with\nGradient Penalty (cWGAN-GP), operating on log-mel spectrograms. To validate our\nmethod, we created a clinical audio dataset which was recorded in a real-world\noperating room during Total Hip Arthroplasty (THA) procedures and contains\ntypical sounds which resemble the different phases of the intervention. We\ndemonstrate the capability of the proposed method to generate realistic\nclass-conditioned samples from the dataset distribution and show that training\nwith the generated augmented samples outperforms classical audio augmentation\nmethods in terms of classification performance. The performance was evaluated\nusing a ResNet-18 classifier which shows a mean Macro F1-score improvement of\n1.70% in a 5-fold cross validation experiment using the proposed augmentation\nmethod. Because clinical data is often expensive to acquire, the development of\nrealistic and high-quality data augmentation methods is crucial to improve the\nrobustness and generalization capabilities of learning-based algorithms which\nis especially important for safety-critical medical applications. Therefore,\nthe proposed data augmentation method is an important step towards improving\nthe data bottleneck for clinical audio-based machine learning systems.", "published": "2022-03-22 09:47:31", "link": "http://arxiv.org/abs/2203.11570v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Estimation of speaker age and height from speech signal using bi-encoder\n  transformer mixture model", "abstract": "The estimation of speaker characteristics such as age and height is a\nchallenging task, having numerous applications in voice forensic analysis. In\nthis work, we propose a bi-encoder transformer mixture model for speaker age\nand height estimation. Considering the wide differences in male and female\nvoice characteristics such as differences in formant and fundamental\nfrequencies, we propose the use of two separate transformer encoders for the\nextraction of specific voice features in the male and female gender, using\nwav2vec 2.0 as a common-level feature extractor. This architecture reduces the\ninterference effects during backpropagation and improves the generalizability\nof the model. We perform our experiments on the TIMIT dataset and significantly\noutperform the current state-of-the-art results on age estimation.\nSpecifically, we achieve root mean squared error (RMSE) of 5.54 years and 6.49\nyears for male and female age estimation, respectively. Further experiment to\nevaluate the relative importance of different phonetic types for our task\ndemonstrate that vowel sounds are the most distinguishing for age estimation.", "published": "2022-03-22 14:39:56", "link": "http://arxiv.org/abs/2203.11774v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Federated Self-Supervised Learning for Acoustic Event Classification", "abstract": "Standard acoustic event classification (AEC) solutions require large-scale\ncollection of data from client devices for model optimization. Federated\nlearning (FL) is a compelling framework that decouples data collection and\nmodel training to enhance customer privacy. In this work, we investigate the\nfeasibility of applying FL to improve AEC performance while no customer data\ncan be directly uploaded to the server. We assume no pseudo labels can be\ninferred from on-device user inputs, aligning with the typical use cases of\nAEC. We adapt self-supervised learning to the FL framework for on-device\ncontinual learning of representations, and it results in improved performance\nof the downstream AEC classifiers without labeled/pseudo-labeled data\navailable. Compared to the baseline w/o FL, the proposed method improves\nprecision up to 20.3\\% relatively while maintaining the recall. Our work\ndiffers from prior work in FL in that our approach does not require\nuser-generated learning targets, and the data we use is collected from our Beta\nprogram and is de-identified, to maximally simulate the production settings.", "published": "2022-03-22 18:49:52", "link": "http://arxiv.org/abs/2203.11997v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
