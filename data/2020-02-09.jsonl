{"title": "Short Text Classification via Knowledge powered Attention with\n  Similarity Matrix based CNN", "abstract": "Short text is becoming more and more popular on the web, such as Chat\nMessage, SMS and Product Reviews. Accurately classifying short text is an\nimportant and challenging task. A number of studies have difficulties in\naddressing this problem because of the word ambiguity and data sparsity. To\naddress this issue, we propose a knowledge powered attention with similarity\nmatrix based convolutional neural network (KASM) model, which can compute\ncomprehensive information by utilizing the knowledge and deep neural network.\nWe use knowledge graph (KG) to enrich the semantic representation of short\ntext, specially, the information of parent-entity is introduced in our model.\nMeanwhile, we consider the word interaction in the literal-level between short\ntext and the representation of label, and utilize similarity matrix based\nconvolutional neural network (CNN) to extract it. For the purpose of measuring\nthe importance of knowledge, we introduce the attention mechanisms to choose\nthe important information. Experimental results on five standard datasets show\nthat our model significantly outperforms state-of-the-art methods.", "published": "2020-02-09 12:08:43", "link": "http://arxiv.org/abs/2002.03350v2", "categories": ["cs.CL", "I.7.0"], "primary_category": "cs.CL"}
{"title": "Rough Set based Aggregate Rank Measure & its Application to Supervised\n  Multi Document Summarization", "abstract": "Most problems in Machine Learning cater to classification and the objects of\nuniverse are classified to a relevant class. Ranking of classified objects of\nuniverse per decision class is a challenging problem. We in this paper propose\na novel Rough Set based membership called Rank Measure to solve to this\nproblem. It shall be utilized for ranking the elements to a particular class.\nIt differs from Pawlak Rough Set based membership function which gives an\nequivalent characterization of the Rough Set based approximations. It becomes\nparamount to look beyond the traditional approach of computing memberships\nwhile handling inconsistent, erroneous and missing data that is typically\npresent in real world problems. This led us to propose the aggregate Rank\nMeasure. The contribution of the paper is three fold. Firstly, it proposes a\nRough Set based measure to be utilized for numerical characterization of within\nclass ranking of objects. Secondly, it proposes and establish the properties of\nRank Measure and aggregate Rank Measure based membership. Thirdly, we apply the\nconcept of membership and aggregate ranking to the problem of supervised Multi\nDocument Summarization wherein first the important class of sentences are\ndetermined using various supervised learning techniques and are post processed\nusing the proposed ranking measure. The results proved to have significant\nimprovement in accuracy.", "published": "2020-02-09 01:03:25", "link": "http://arxiv.org/abs/2002.03259v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Attend to the beginning: A study on using bidirectional attention for\n  extractive summarization", "abstract": "Forum discussion data differ in both structure and properties from generic\nform of textual data such as news. Henceforth, summarization techniques should,\nin turn, make use of such differences, and craft models that can benefit from\nthe structural nature of discussion data. In this work, we propose attending to\nthe beginning of a document, to improve the performance of extractive\nsummarization models when applied to forum discussion data. Evaluations\ndemonstrated that with the help of bidirectional attention mechanism, attending\nto the beginning of a document (initial comment/post) in a discussion thread,\ncan introduce a consistent boost in ROUGE scores, as well as introducing a new\nState Of The Art (SOTA) ROUGE scores on the forum discussions dataset.\nAdditionally, we explored whether this hypothesis is extendable to other\ngeneric forms of textual data. We make use of the tendency of introducing\nimportant information early in the text, by attending to the first few\nsentences in generic textual data. Evaluations demonstrated that attending to\nintroductory sentences using bidirectional attention, improves the performance\nof extractive summarization models when even applied to more generic form of\ntextual data.", "published": "2020-02-09 17:46:22", "link": "http://arxiv.org/abs/2002.03405v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Abstractive Summarization for Low Resource Data using Domain Transfer\n  and Data Synthesis", "abstract": "Training abstractive summarization models typically requires large amounts of\ndata, which can be a limitation for many domains. In this paper we explore\nusing domain transfer and data synthesis to improve the performance of recent\nabstractive summarization methods when applied to small corpora of student\nreflections. First, we explored whether tuning state of the art model trained\non newspaper data could boost performance on student reflection data.\nEvaluations demonstrated that summaries produced by the tuned model achieved\nhigher ROUGE scores compared to model trained on just student reflection data\nor just newspaper data. The tuned model also achieved higher scores compared to\nextractive summarization baselines, and additionally was judged to produce more\ncoherent and readable summaries in human evaluations. Second, we explored\nwhether synthesizing summaries of student data could additionally boost\nperformance. We proposed a template-based model to synthesize new data, which\nwhen incorporated into training further increased ROUGE scores. Finally, we\nshowed that combining data synthesis with domain transfer achieved higher ROUGE\nscores compared to only using one of the two approaches.", "published": "2020-02-09 17:49:08", "link": "http://arxiv.org/abs/2002.03407v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Limits of Detecting Text Generated by Large-Scale Language Models", "abstract": "Some consider large-scale language models that can generate long and coherent\npieces of text as dangerous, since they may be used in misinformation\ncampaigns. Here we formulate large-scale language model output detection as a\nhypothesis testing problem to classify text as genuine or generated. We show\nthat error exponents for particular language models are bounded in terms of\ntheir perplexity, a standard measure of language generation performance. Under\nthe assumption that human language is stationary and ergodic, the formulation\nis extended from considering specific language models to considering maximum\nlikelihood language models, among the class of k-order Markov approximations;\nerror probabilities are characterized. Some discussion of incorporating\nsemantic side information is also given.", "published": "2020-02-09 19:53:23", "link": "http://arxiv.org/abs/2002.03438v1", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Application of Pre-training Models in Named Entity Recognition", "abstract": "Named Entity Recognition (NER) is a fundamental Natural Language Processing\n(NLP) task to extract entities from unstructured data. The previous methods for\nNER were based on machine learning or deep learning. Recently, pre-training\nmodels have significantly improved performance on multiple NLP tasks. In this\npaper, firstly, we introduce the architecture and pre-training tasks of four\ncommon pre-training models: BERT, ERNIE, ERNIE2.0-tiny, and RoBERTa. Then, we\napply these pre-training models to a NER task by fine-tuning, and compare the\neffects of the different model architecture and pre-training tasks on the NER\ntask. The experiment results showed that RoBERTa achieved state-of-the-art\nresults on the MSRA-2006 dataset.", "published": "2020-02-09 08:18:20", "link": "http://arxiv.org/abs/2002.08902v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "FastWave: Accelerating Autoregressive Convolutional Neural Networks on\n  FPGA", "abstract": "Autoregressive convolutional neural networks (CNNs) have been widely\nexploited for sequence generation tasks such as audio synthesis, language\nmodeling and neural machine translation. WaveNet is a deep autoregressive CNN\ncomposed of several stacked layers of dilated convolution that is used for\nsequence generation. While WaveNet produces state-of-the art audio generation\nresults, the naive inference implementation is quite slow; it takes a few\nminutes to generate just one second of audio on a high-end GPU. In this work,\nwe develop the first accelerator platform~\\textit{FastWave} for autoregressive\nconvolutional neural networks, and address the associated design challenges. We\ndesign the Fast-Wavenet inference model in Vivado HLS and perform a wide range\nof optimizations including fixed-point implementation, array partitioning and\npipelining. Our model uses a fully parameterized parallel architecture for fast\nmatrix-vector multiplication that enables per-layer customized latency\nfine-tuning for further throughput improvement. Our experiments comparatively\nassess the trade-off between throughput and resource utilization for various\noptimizations. Our best WaveNet design on the Xilinx XCVU13P FPGA that uses\nonly on-chip memory, achieves 66 faster generation speed compared to CPU\nimplementation and 11 faster generation speed than GPU implementation.", "published": "2020-02-09 06:15:09", "link": "http://arxiv.org/abs/2002.04971v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
