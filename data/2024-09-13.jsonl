{"title": "Credit Spreads' Term Structure: Stochastic Modeling with CIR++ Intensity", "abstract": "This paper introduces a novel stochastic model for credit spreads. The\nstochastic approach leverages the diffusion of default intensities via a CIR++\nmodel and is formulated within a risk-neutral probability space. Our research\nprimarily addresses two gaps in the literature. The first is the lack of credit\nspread models founded on a stochastic basis that enables continuous modeling,\nas many existing models rely on factorial assumptions. The second is the\nlimited availability of models that directly yield a term structure of credit\nspreads. An intermediate result of our model is the provision of a term\nstructure for the prices of defaultable bonds. We present the model alongside\nan innovative, practical, and conservative calibration approach that minimizes\nthe error between historical and theoretical volatilities of default\nintensities. We demonstrate the robustness of both the model and its\ncalibration process by comparing its behavior to historical credit spread\nvalues. Our findings indicate that the model not only produces realistic credit\nspread term structure curves but also exhibits consistent diffusion over time.\nAdditionally, the model accurately fits the initial term structure of implied\nsurvival probabilities and provides an analytical expression for the credit\nspread of any given maturity at any future time.", "published": "2024-09-13 20:35:18", "link": "http://arxiv.org/abs/2409.09179v1", "categories": ["q-fin.RM", "q-fin.MF", "60H10, 60J60, 91G30, 91G40", "G.3"], "primary_category": "q-fin.RM"}
{"title": "Tuning into Climate Risks: Extracting Innovation from Television News for Clean Energy Firms", "abstract": "This article develops multiple novel climate risk measures (or variables)\nbased on the television news coverage by Bloomberg, CNBC, and Fox Business, and\nexamines how they affect the systematic and idiosyncratic risks of clean energy\nfirms in the United States. The measures are built on climate related keywords\nand cover the volume of coverage, type of coverage (climate crisis, renewable\nenergy, and government & human initiatives), and media sentiments. We show that\nan increase in the aggregate measure of climate risk, as indicated by coverage\nvolume, reduces idiosyncratic risk while increasing systematic risk. When\nclimate risk is segregated, we find that systematic risk is positively affected\nby the physical risk of climate crises and transition risk from government &\nhuman initiatives, but no such impact is evident for idiosyncratic risk.\nAdditionally, we observe an asymmetry in risk behavior: negative sentiments\ntend to decrease idiosyncratic risk and increase systematic risk, while\npositive sentiments have no significant impact. These findings remain robust to\nincluding print media and climate policy uncertainty variables, though some\ndeviations are noted during the COVID-19 period.", "published": "2024-09-13 10:41:38", "link": "http://arxiv.org/abs/2409.08701v3", "categories": ["q-fin.ST", "q-fin.CP"], "primary_category": "q-fin.ST"}
{"title": "Eir: Thai Medical Large Language Models", "abstract": "We present Eir-8B, a large language model with 8 billion parameters,\nspecifically designed to enhance the accuracy of handling medical tasks in the\nThai language. This model focuses on providing clear and easy-to-understand\nanswers for both healthcare professionals and patients, thereby improving the\nefficiency of diagnosis and treatment processes. Human evaluation was conducted\nto ensure that the model adheres to care standards and provides unbiased\nanswers.\n  To prioritize data security, the model is deployed within the hospital's\ninternal network, ensuring both high security and faster processing speeds. The\ninternal API connection is secured with encryption and strict authentication\nmeasures to prevent data leaks and unauthorized access.\n  We evaluated several open-source large language models with 8 billion\nparameters on four medical benchmarks: MedQA, MedMCQA, PubMedQA, and the\nmedical subset of MMLU. The best-performing baselines were used to develop\nEir-8B. Our evaluation employed multiple questioning strategies, including\nzero-shot, few-shot, chain-of-thought reasoning, and ensemble/self-consistency\nvoting methods. Our model outperformed commercially available Thai-language\nlarge language models by more than 10%. In addition, we developed enhanced\nmodel testing tailored for clinical use in Thai across 18 clinical tasks, where\nour model exceeded GPT-4o performance by more than 11%.", "published": "2024-09-13 04:06:00", "link": "http://arxiv.org/abs/2409.08523v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLM-Powered Grapheme-to-Phoneme Conversion: Benchmark and Case Study", "abstract": "Grapheme-to-phoneme (G2P) conversion is critical in speech processing,\nparticularly for applications like speech synthesis. G2P systems must possess\nlinguistic understanding and contextual awareness of languages with polyphone\nwords and context-dependent phonemes. Large language models (LLMs) have\nrecently demonstrated significant potential in various language tasks,\nsuggesting that their phonetic knowledge could be leveraged for G2P. In this\npaper, we evaluate the performance of LLMs in G2P conversion and introduce\nprompting and post-processing methods that enhance LLM outputs without\nadditional training or labeled data. We also present a benchmarking dataset\ndesigned to assess G2P performance on sentence-level phonetic challenges of the\nPersian language. Our results show that by applying the proposed methods, LLMs\ncan outperform traditional G2P tools, even in an underrepresented language like\nPersian, highlighting the potential of developing LLM-aided G2P systems.", "published": "2024-09-13 06:13:55", "link": "http://arxiv.org/abs/2409.08554v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cracking the Code: Multi-domain LLM Evaluation on Real-World\n  Professional Exams in Indonesia", "abstract": "While knowledge evaluation in large language models has predominantly focused\non academic subjects like math and physics, these assessments often fail to\ncapture the practical demands of real-world professions. In this paper, we\nintroduce IndoCareer, a dataset comprising 8,834 multiple-choice questions\ndesigned to evaluate performance in vocational and professional certification\nexams across various fields. With a focus on Indonesia, IndoCareer provides\nrich local contexts, spanning six key sectors: (1) healthcare, (2) insurance\nand finance, (3) creative and design, (4) tourism and hospitality, (5)\neducation and training, and (6) law. Our comprehensive evaluation of 27 large\nlanguage models shows that these models struggle particularly in fields with\nstrong local contexts, such as insurance and finance. Additionally, while using\nthe entire dataset, shuffling answer options generally maintains consistent\nevaluation results across models, but it introduces instability specifically in\nthe insurance and finance sectors.", "published": "2024-09-13 06:34:15", "link": "http://arxiv.org/abs/2409.08564v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sign Language Sense Disambiguation", "abstract": "This project explores methods to enhance sign language translation of German\nsign language, specifically focusing on disambiguation of homonyms. Sign\nlanguage is ambiguous and understudied which is the basis for our experiments.\nWe approach the improvement by training transformer-based models on various\nbodypart representations to shift the focus on said bodypart. To determine the\nimpact of, e.g., the hand or mouth representations, we experiment with\ndifferent combinations. The results show that focusing on the mouth increases\nthe performance in small dataset settings while shifting the focus on the hands\nretrieves better results in larger dataset settings. Our results contribute to\nbetter accessibility for non-hearing persons by improving the systems powering\ndigital assistants, enabling a more accurate interaction. The code for this\nproject can be found on GitHub.", "published": "2024-09-13 12:36:52", "link": "http://arxiv.org/abs/2409.08780v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Optimizing Ingredient Substitution Using Large Language Models to\n  Enhance Phytochemical Content in Recipes", "abstract": "In the emerging field of computational gastronomy, aligning culinary\npractices with scientifically supported nutritional goals is increasingly\nimportant. This study explores how large language models (LLMs) can be applied\nto optimize ingredient substitutions in recipes, specifically to enhance the\nphytochemical content of meals. Phytochemicals are bioactive compounds found in\nplants, which, based on preclinical studies, may offer potential health\nbenefits. We fine-tuned models, including OpenAI's GPT-3.5, DaVinci, and Meta's\nTinyLlama, using an ingredient substitution dataset. These models were used to\npredict substitutions that enhance phytochemical content and create a\ncorresponding enriched recipe dataset. Our approach improved Hit@1 accuracy on\ningredient substitution tasks, from the baseline 34.53 plus-minus 0.10% to\n38.03 plus-minus 0.28% on the original GISMo dataset, and from 40.24 plus-minus\n0.36% to 54.46 plus-minus 0.29% on a refined version of the same dataset. These\nsubstitutions led to the creation of 1,951 phytochemically enriched ingredient\npairings and 1,639 unique recipes. While this approach demonstrates potential\nin optimizing ingredient substitutions, caution must be taken when drawing\nconclusions about health benefits, as the claims are based on preclinical\nevidence. Future work should include clinical validation and broader datasets\nto further evaluate the nutritional impact of these substitutions. This\nresearch represents a step forward in using AI to promote healthier eating\npractices, providing potential pathways for integrating computational methods\nwith nutritional science.", "published": "2024-09-13 12:55:45", "link": "http://arxiv.org/abs/2409.08792v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Your Weak LLM is Secretly a Strong Teacher for Alignment", "abstract": "The burgeoning capabilities of large language models (LLMs) have underscored\nthe need for alignment to ensure these models act in accordance with human\nvalues and intentions. Existing alignment frameworks present constraints either\nin the form of expensive human effort or high computational costs. This paper\nexplores a promising middle ground, where we employ a weak LLM that is\nsignificantly less resource-intensive than top-tier models, yet offers more\nautomation than purely human feedback. We present a systematic study to\nevaluate and understand weak LLM's ability to generate feedback for alignment.\nOur empirical findings demonstrate that weak LLMs can provide feedback that\nrivals or even exceeds that of fully human-annotated data. Our study indicates\na minimized impact of model size on feedback efficacy, shedding light on a\nscalable and sustainable alignment strategy. To deepen our understanding of\nalignment under weak LLM feedback, we conduct a series of qualitative and\nquantitative analyses, offering novel insights into the quality discrepancies\nbetween human feedback vs. weak LLM feedback.", "published": "2024-09-13 13:24:52", "link": "http://arxiv.org/abs/2409.08813v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AIPO: Improving Training Objective for Iterative Preference Optimization", "abstract": "Preference Optimization (PO), is gaining popularity as an alternative choice\nof Proximal Policy Optimization (PPO) for aligning Large Language Models\n(LLMs). Recent research on aligning LLMs iteratively with synthetic or\npartially synthetic data shows promising results in scaling up PO training for\nboth academic settings and proprietary trained models such as Llama3. Despite\nits success, our study shows that the length exploitation issue present in PO\nis even more severe in Iterative Preference Optimization (IPO) due to the\niterative nature of the process. In this work, we study iterative preference\noptimization with synthetic data. We share the findings and analysis along the\nway of building the iterative preference optimization pipeline. More\nspecifically, we discuss the length exploitation issue during iterative\npreference optimization and propose our training objective for iterative\npreference optimization, namely Agreement-aware Iterative Preference\nOptimization (AIPO). To demonstrate the effectiveness of our method, we conduct\ncomprehensive experiments and achieve state-of-the-art performance on MT-Bench,\nAlpacaEval 2.0, and Arena-Hard. Our implementation and model checkpoints will\nbe made available at https://github.com/bytedance/AIPO.", "published": "2024-09-13 14:03:49", "link": "http://arxiv.org/abs/2409.08845v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Optimizing Rare Word Accuracy in Direct Speech Translation with a\n  Retrieval-and-Demonstration Approach", "abstract": "Direct speech translation (ST) models often struggle with rare words.\nIncorrect translation of these words can have severe consequences, impacting\ntranslation quality and user trust. While rare word translation is inherently\nchallenging for neural models due to sparse learning signals, real-world\nscenarios often allow access to translations of past recordings on similar\ntopics. To leverage these valuable resources, we propose a\nretrieval-and-demonstration approach to enhance rare word translation accuracy\nin direct ST models. First, we adapt existing ST models to incorporate\nretrieved examples for rare word translation, which allows the model to benefit\nfrom prepended examples, similar to in-context learning. We then develop a\ncross-modal (speech-to-speech, speech-to-text, text-to-text) retriever to\nlocate suitable examples. We demonstrate that standard ST models can be\neffectively adapted to leverage examples for rare word translation, improving\nrare word translation accuracy over the baseline by 17.6% with gold examples\nand 8.5% with retrieved examples. Moreover, our speech-to-speech retrieval\napproach outperforms other modalities and exhibits higher robustness to unseen\nspeakers. Our code is publicly available\n(https://github.com/SiqiLii/Retrieve-and-Demonstration-ST).", "published": "2024-09-13 17:38:03", "link": "http://arxiv.org/abs/2409.09009v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Precision Characterization of Communication Disorders using\n  Models of Perceived Pragmatic Similarity", "abstract": "The diagnosis and treatment of individuals with communication disorders\noffers many opportunities for the application of speech technology, but\nresearch so far has not adequately considered: the diversity of conditions, the\nrole of pragmatic deficits, and the challenges of limited data. This paper\nexplores how a general-purpose model of perceived pragmatic similarity may\novercome these limitations. It explains how it might support several use cases\nfor clinicians and clients, and presents evidence that a simple model can\nprovide value, and in particular can capture utterance aspects that are\nrelevant to diagnoses of autism and specific language impairment.", "published": "2024-09-13 20:01:13", "link": "http://arxiv.org/abs/2409.09170v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pronoun Logic", "abstract": "Particularly in transgender and nonbinary (TGNB) communities, it is an\nincreasingly common practice to publicly share one's personal pronouns so that\nwe may be gendered correctly in others' speech. Many of us have nuanced desires\nfor how we are gendered, leading us to use more complex descriptions of our\nwishes; for example, the descriptor 'she/they'. We observe that these\ndescriptions of our wishes have the structure of a little language all their\nown. We thus propose formal logic as a tool for expressing one's personal\npronouns and potentially other aspects of gender. We explore three potential\nlogical foundations (linear logic, temporal logic, and free logic with definite\ndescriptions) and their trade-offs. Our foremost motivation for this proposal\nis play, affirming that one can be both a logician and TGNB at the same time.\nWe present formalization as something that can continue to evolve over time\nwith society's understanding of gender. This implies that outreach is a major\npotential application: we can show TGNB youth that they belong in logic and\nhave a unique contribution to make. Tools for evaluating whether one's pronouns\nare respected are an application as well.", "published": "2024-09-13 17:35:32", "link": "http://arxiv.org/abs/2409.18978v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "When Context Leads but Parametric Memory Follows in Large Language\n  Models", "abstract": "Large language models (LLMs) have demonstrated remarkable progress in\nleveraging diverse knowledge sources. This study investigates how nine widely\nused LLMs allocate knowledge between local context and global parameters when\nanswering open-ended questions in knowledge-consistent scenarios. We introduce\na novel dataset, WikiAtomic, and systematically vary context sizes to analyze\nhow LLMs prioritize and utilize the provided information and their parametric\nknowledge in knowledge-consistent scenarios. Additionally, we also study their\ntendency to hallucinate under varying context sizes. Our findings reveal\nconsistent patterns across models, including a consistent reliance on both\ncontextual (around 70%) and parametric (around 30%) knowledge, and a decrease\nin hallucinations with increasing context. These insights highlight the\nimportance of more effective context organization and developing models that\nuse input more deterministically for robust performance.", "published": "2024-09-13 00:03:19", "link": "http://arxiv.org/abs/2409.08435v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A BERT-Based Summarization approach for depression detection", "abstract": "Depression is a globally prevalent mental disorder with potentially severe\nrepercussions if not addressed, especially in individuals with recurrent\nepisodes. Prior research has shown that early intervention has the potential to\nmitigate or alleviate symptoms of depression. However, implementing such\ninterventions in a real-world setting may pose considerable challenges. A\npromising strategy involves leveraging machine learning and artificial\nintelligence to autonomously detect depression indicators from diverse data\nsources. One of the most widely available and informative data sources is text,\nwhich can reveal a person's mood, thoughts, and feelings. In this context,\nvirtual agents programmed to conduct interviews using clinically validated\nquestionnaires, such as those found in the DAIC-WOZ dataset, offer a robust\nmeans for depression detection through linguistic analysis. Utilizing\nBERT-based models, which are powerful and versatile yet use fewer resources\nthan contemporary large language models, to convert text into numerical\nrepresentations significantly enhances the precision of depression diagnosis.\nThese models adeptly capture complex semantic and syntactic nuances, improving\nthe detection accuracy of depressive symptoms. Given the inherent limitations\nof these models concerning text length, our study proposes text summarization\nas a preprocessing technique to diminish the length and intricacies of input\ntexts. Implementing this method within our uniquely developed framework for\nfeature extraction and classification yielded an F1-score of 0.67 on the test\nset surpassing all prior benchmarks and 0.81 on the validation set exceeding\nmost previous results on the DAIC-WOZ dataset. Furthermore, we have devised a\ndepression lexicon to assess summary quality and relevance. This lexicon\nconstitutes a valuable asset for ongoing research in depression detection.", "published": "2024-09-13 02:14:34", "link": "http://arxiv.org/abs/2409.08483v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Expediting and Elevating Large Language Model Reasoning via Hidden\n  Chain-of-Thought Decoding", "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in\ntasks requiring reasoning and multi-step problem-solving through the use of\nchain-of-thought (CoT) prompting. However, generating the full CoT process\nresults in significantly longer output sequences, leading to increased\ncomputational costs and latency during inference. To address this challenge, we\npropose a novel approach to compress the CoT process through semantic\nalignment, enabling more efficient decoding while preserving the benefits of\nCoT reasoning. Our method introduces an auxiliary CoT model that learns to\ngenerate and compress the full thought process into a compact special token\nrepresentation semantically aligned with the original CoT output. This\ncompressed representation is then integrated into the input of the Hidden\nChain-of-Thought (HCoT) model. The training process follows a two-stage\nprocedure: First, the CoT model is optimized to generate the compressed token\nrepresentations aligned with the ground-truth CoT outputs using a contrastive\nloss. Subsequently, with the CoT model parameters frozen, the HCoT model is\nfine-tuned to generate accurate subsequent predictions conditioned on the\nprefix instruction and the compressed CoT representations from the CoT model.\nExtensive experiments across three challenging domains - mathematical\nreasoning, agent invocation, and question answering - demonstrate that our\nsemantic compression approach achieves competitive or improved performance\ncompared to the full CoT baseline, while providing significant speedups of at\nleast 1.5x in decoding time. Moreover, incorporating contrastive learning\nobjectives further enhances the quality of the compressed representations,\nleading to better CoT prompting and improved task accuracy. Our work paves the\nway for more efficient exploitation of multi-step reasoning capabilities in\nLLMs across a wide range of applications.", "published": "2024-09-13 06:29:20", "link": "http://arxiv.org/abs/2409.08561v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "L3Cube-IndicQuest: A Benchmark Question Answering Dataset for Evaluating\n  Knowledge of LLMs in Indic Context", "abstract": "Large Language Models (LLMs) have made significant progress in incorporating\nIndic languages within multilingual models. However, it is crucial to\nquantitatively assess whether these languages perform comparably to globally\ndominant ones, such as English. Currently, there is a lack of benchmark\ndatasets specifically designed to evaluate the regional knowledge of LLMs in\nvarious Indic languages. In this paper, we present the L3Cube-IndicQuest, a\ngold-standard factual question-answering benchmark dataset designed to evaluate\nhow well multilingual LLMs capture regional knowledge across various Indic\nlanguages. The dataset contains 200 question-answer pairs, each for English and\n19 Indic languages, covering five domains specific to the Indic region. We aim\nfor this dataset to serve as a benchmark, providing ground truth for evaluating\nthe performance of LLMs in understanding and representing knowledge relevant to\nthe Indian context. The IndicQuest can be used for both reference-based\nevaluation and LLM-as-a-judge evaluation. The dataset is shared publicly at\nhttps://github.com/l3cube-pune/indic-nlp .", "published": "2024-09-13 10:48:35", "link": "http://arxiv.org/abs/2409.08706v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Distilling Monolingual and Crosslingual Word-in-Context Representations", "abstract": "In this study, we propose a method that distils representations of word\nmeaning in context from a pre-trained masked language model in both monolingual\nand crosslingual settings. Word representations are the basis for context-aware\nlexical semantics and unsupervised semantic textual similarity (STS)\nestimation. Different from existing approaches, our method does not require\nhuman-annotated corpora nor updates of the parameters of the pre-trained model.\nThe latter feature is appealing for practical scenarios where the off-the-shelf\npre-trained model is a common asset among different applications. Specifically,\nour method learns to combine the outputs of different hidden layers of the\npre-trained model using self-attention. Our auto-encoder based training only\nrequires an automatically generated corpus. To evaluate the performance of the\nproposed approach, we performed extensive experiments using various benchmark\ntasks. The results on the monolingual tasks confirmed that our representations\nexhibited a competitive performance compared to that of the previous study for\nthe context-aware lexical semantic tasks and outperformed it for STS\nestimation. The results of the crosslingual tasks revealed that the proposed\nmethod largely improved crosslingual word representations of multilingual\npre-trained models.", "published": "2024-09-13 11:10:16", "link": "http://arxiv.org/abs/2409.08719v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Visual Language Tracking with Multi-modal Interaction: A Robust\n  Benchmark", "abstract": "Visual Language Tracking (VLT) enhances tracking by mitigating the\nlimitations of relying solely on the visual modality, utilizing high-level\nsemantic information through language. This integration of the language enables\nmore advanced human-machine interaction. The essence of interaction is\ncognitive alignment, which typically requires multiple information exchanges,\nespecially in the sequential decision-making process of VLT. However, current\nVLT benchmarks do not account for multi-round interactions during tracking.\nThey provide only an initial text and bounding box (bbox) in the first frame,\nwith no further interaction as tracking progresses, deviating from the original\nmotivation of the VLT task. To address these limitations, we propose a novel\nand robust benchmark, VLT-MI (Visual Language Tracking with Multi-modal\nInteraction), which introduces multi-round interaction into the VLT task for\nthe first time. (1) We generate diverse, multi-granularity texts for\nmulti-round, multi-modal interaction based on existing mainstream VLT\nbenchmarks using DTLLM-VLT, leveraging the world knowledge of LLMs. (2) We\npropose a new VLT interaction paradigm that achieves multi-round interaction\nthrough text updates and object recovery. When multiple tracking failures\noccur, we provide the tracker with more aligned texts and corrected bboxes\nthrough interaction, thereby expanding the scope of VLT downstream tasks. (3)\nWe conduct comparative experiments on both traditional VLT benchmarks and\nVLT-MI, evaluating and analyzing the accuracy and robustness of trackers under\nthe interactive paradigm. This work offers new insights and paradigms for the\nVLT task, enabling a fine-grained evaluation of multi-modal trackers. We\nbelieve this approach can be extended to additional datasets in the future,\nsupporting broader evaluations and comparisons of video-language model\ncapabilities.", "published": "2024-09-13 14:54:37", "link": "http://arxiv.org/abs/2409.08887v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "SynSUM -- Synthetic Benchmark with Structured and Unstructured Medical\n  Records", "abstract": "We present the SynSUM benchmark, a synthetic dataset linking unstructured\nclinical notes to structured background variables. The dataset consists of\n10,000 artificial patient records containing tabular variables (like symptoms,\ndiagnoses and underlying conditions) and related notes describing the fictional\npatient encounter in the domain of respiratory diseases. The tabular portion of\nthe data is generated through a Bayesian network, where both the causal\nstructure between the variables and the conditional probabilities are proposed\nby an expert based on domain knowledge. We then prompt a large language model\n(GPT-4o) to generate a clinical note related to this patient encounter,\ndescribing the patient symptoms and additional context. We conduct both an\nexpert evaluation study to assess the quality of the generated notes, as well\nas running some simple predictor models on both the tabular and text portions\nof the dataset, forming a baseline for further research. The SynSUM dataset is\nprimarily designed to facilitate research on clinical information extraction in\nthe presence of tabular background variables, which can be linked through\ndomain knowledge to concepts of interest to be extracted from the text - the\nsymptoms, in the case of SynSUM. Secondary uses include research on the\nautomation of clinical reasoning over both tabular data and text, causal effect\nestimation in the presence of tabular and/or textual confounders, and\nmulti-modal synthetic data generation.", "published": "2024-09-13 15:55:15", "link": "http://arxiv.org/abs/2409.08936v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "AI-LieDar: Examine the Trade-off Between Utility and Truthfulness in LLM\n  Agents", "abstract": "To be safely and successfully deployed, LLMs must simultaneously satisfy\ntruthfulness and utility goals. Yet, often these two goals compete (e.g., an AI\nagent assisting a used car salesman selling a car with flaws), partly due to\nambiguous or misleading user instructions. We propose AI-LieDar, a framework to\nstudy how LLM-based agents navigate scenarios with utility-truthfulness\nconflicts in a multi-turn interactive setting. We design a set of realistic\nscenarios where language agents are instructed to achieve goals that are in\nconflict with being truthful during a multi-turn conversation with simulated\nhuman agents. To evaluate the truthfulness at large scale, we develop a\ntruthfulness detector inspired by psychological literature to assess the\nagents' responses. Our experiment demonstrates that all models are truthful\nless than 50% of the time, although truthfulness and goal achievement (utility)\nrates vary across models. We further test the steerability of LLMs towards\ntruthfulness, finding that models follow malicious instructions to deceive, and\neven truth-steered models can still lie. These findings reveal the complex\nnature of truthfulness in LLMs and underscore the importance of further\nresearch to ensure the safe and reliable deployment of LLMs and AI agents.", "published": "2024-09-13 17:41:12", "link": "http://arxiv.org/abs/2409.09013v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "DomURLs_BERT: Pre-trained BERT-based Model for Malicious Domains and\n  URLs Detection and Classification", "abstract": "Detecting and classifying suspicious or malicious domain names and URLs is\nfundamental task in cybersecurity. To leverage such indicators of compromise,\ncybersecurity vendors and practitioners often maintain and update blacklists of\nknown malicious domains and URLs. However, blacklists frequently fail to\nidentify emerging and obfuscated threats. Over the past few decades, there has\nbeen significant interest in developing machine learning models that\nautomatically detect malicious domains and URLs, addressing the limitations of\nblacklists maintenance and updates. In this paper, we introduce DomURLs_BERT, a\npre-trained BERT-based encoder adapted for detecting and classifying\nsuspicious/malicious domains and URLs. DomURLs_BERT is pre-trained using the\nMasked Language Modeling (MLM) objective on a large multilingual corpus of\nURLs, domain names, and Domain Generation Algorithms (DGA) dataset. In order to\nassess the performance of DomURLs_BERT, we have conducted experiments on\nseveral binary and multi-class classification tasks involving domain names and\nURLs, covering phishing, malware, DGA, and DNS tunneling. The evaluations\nresults show that the proposed encoder outperforms state-of-the-art\ncharacter-based deep learning models and cybersecurity-focused BERT models\nacross multiple tasks and datasets. The pre-training dataset, the pre-trained\nDomURLs_BERT encoder, and the experiments source code are publicly available.", "published": "2024-09-13 18:59:13", "link": "http://arxiv.org/abs/2409.09143v1", "categories": ["cs.CR", "cs.CL", "68T07, 68M25", "I.2; C.2"], "primary_category": "cs.CR"}
{"title": "Contextual Evaluation of Large Language Models for Classifying Tropical\n  and Infectious Diseases", "abstract": "While large language models (LLMs) have shown promise for medical question\nanswering, there is limited work focused on tropical and infectious\ndisease-specific exploration. We build on an opensource tropical and infectious\ndiseases (TRINDs) dataset, expanding it to include demographic and semantic\nclinical and consumer augmentations yielding 11000+ prompts. We evaluate LLM\nperformance on these, comparing generalist and medical LLMs, as well as LLM\noutcomes to human experts. We demonstrate through systematic experimentation,\nthe benefit of contextual information such as demographics, location, gender,\nrisk factors for optimal LLM response. Finally we develop a prototype of\nTRINDs-LM, a research tool that provides a playground to navigate how context\nimpacts LLM outputs for health.", "published": "2024-09-13 21:28:54", "link": "http://arxiv.org/abs/2409.09201v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Explaining Datasets in Words: Statistical Models with Natural Language\n  Parameters", "abstract": "To make sense of massive data, we often fit simplified models and then\ninterpret the parameters; for example, we cluster the text embeddings and then\ninterpret the mean parameters of each cluster. However, these parameters are\noften high-dimensional and hard to interpret. To make model parameters directly\ninterpretable, we introduce a family of statistical models -- including\nclustering, time series, and classification models -- parameterized by natural\nlanguage predicates. For example, a cluster of text about COVID could be\nparameterized by the predicate \"discusses COVID\". To learn these statistical\nmodels effectively, we develop a model-agnostic algorithm that optimizes\ncontinuous relaxations of predicate parameters with gradient descent and\ndiscretizes them by prompting language models (LMs). Finally, we apply our\nframework to a wide range of problems: taxonomizing user chat dialogues,\ncharacterizing how they evolve across time, finding categories where one\nlanguage model is better than the other, clustering math problems based on\nsubareas, and explaining visual features in memorable images. Our framework is\nhighly versatile, applicable to both textual and visual domains, can be easily\nsteered to focus on specific properties (e.g. subareas), and explains\nsophisticated concepts that classical methods (e.g. n-gram analysis) struggle\nto produce.", "published": "2024-09-13 01:40:20", "link": "http://arxiv.org/abs/2409.08466v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "MAPX: An explainable model-agnostic framework for the detection of false\n  information on social media networks", "abstract": "The automated detection of false information has become a fundamental task in\ncombating the spread of \"fake news\" on online social media networks (OSMN) as\nit reduces the need for manual discernment by individuals. In the literature,\nleveraging various content or context features of OSMN documents have been\nfound useful. However, most of the existing detection models often utilise\nthese features in isolation without regard to the temporal and dynamic changes\noft-seen in reality, thus, limiting the robustness of the models. Furthermore,\nthere has been little to no consideration of the impact of the quality of\ndocuments' features on the trustworthiness of the final prediction. In this\npaper, we introduce a novel model-agnostic framework, called MAPX, which allows\nevidence based aggregation of predictions from existing models in an\nexplainable manner. Indeed, the developed aggregation method is adaptive,\ndynamic and considers the quality of OSMN document features. Further, we\nperform extensive experiments on benchmarked fake news datasets to demonstrate\nthe effectiveness of MAPX using various real-world data quality scenarios. Our\nempirical results show that the proposed framework consistently outperforms all\nstate-of-the-art models evaluated. For reproducibility, a demo of MAPX is\navailable at \\href{https://github.com/SCondran/MAPX_framework}{this link}", "published": "2024-09-13 03:45:10", "link": "http://arxiv.org/abs/2409.08522v1", "categories": ["cs.SI", "cs.CL", "cs.LG"], "primary_category": "cs.SI"}
{"title": "Large Language Model Can Transcribe Speech in Multi-Talker Scenarios\n  with Versatile Instructions", "abstract": "Recent advancements in large language models (LLMs) have revolutionized\nvarious domains, bringing significant progress and new opportunities. Despite\nprogress in speech-related tasks, LLMs have not been sufficiently explored in\nmulti-talker scenarios. In this work, we present a pioneering effort to\ninvestigate the capability of LLMs in transcribing speech in multi-talker\nenvironments, following versatile instructions related to multi-talker\nautomatic speech recognition (ASR), target talker ASR, and ASR based on\nspecific talker attributes such as sex, occurrence order, language, and keyword\nspoken. Our approach utilizes WavLM and Whisper encoder to extract\nmulti-faceted speech representations that are sensitive to speaker\ncharacteristics and semantic context. These representations are then fed into\nan LLM fine-tuned using LoRA, enabling the capabilities for speech\ncomprehension and transcription. Comprehensive experiments reveal the promising\nperformance of our proposed system, MT-LLM, in cocktail party scenarios,\nhighlighting the potential of LLM to handle speech-related tasks based on user\ninstructions in such complex settings. The code, model, and samples are\navailable at https://github.com/cuhealthybrains/MT-LLM.", "published": "2024-09-13 07:28:28", "link": "http://arxiv.org/abs/2409.08596v2", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "LA-RAG:Enhancing LLM-based ASR Accuracy with Retrieval-Augmented\n  Generation", "abstract": "Recent advancements in integrating speech information into large language\nmodels (LLMs) have significantly improved automatic speech recognition (ASR)\naccuracy. However, existing methods often constrained by the capabilities of\nthe speech encoders under varied acoustic conditions, such as accents. To\naddress this, we propose LA-RAG, a novel Retrieval-Augmented Generation (RAG)\nparadigm for LLM-based ASR. LA-RAG leverages fine-grained token-level speech\ndatastores and a speech-to-speech retrieval mechanism to enhance ASR accuracy\nvia LLM in-context learning (ICL) capabilities. Experiments on Mandarin and\nvarious Chinese dialect datasets demonstrate significant improvements in ASR\naccuracy compared to existing methods, validating the effectiveness of our\napproach, especially in handling accent variations.", "published": "2024-09-13 07:28:47", "link": "http://arxiv.org/abs/2409.08597v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Investigating Disentanglement in a Phoneme-level Speech Codec for\n  Prosody Modeling", "abstract": "Most of the prevalent approaches in speech prosody modeling rely on learning\nglobal style representations in a continuous latent space which encode and\ntransfer the attributes of reference speech. However, recent work on neural\ncodecs which are based on Residual Vector Quantization (RVQ) already shows\ngreat potential offering distinct advantages. We investigate the prosody\nmodeling capabilities of the discrete space of such an RVQ-VAE model, modifying\nit to operate on the phoneme-level. We condition both the encoder and decoder\nof the model on linguistic representations and apply a global speaker embedding\nin order to factor out both phonetic and speaker information. We conduct an\nextensive set of investigations based on subjective experiments and objective\nmeasures to show that the phoneme-level discrete latent representations\nobtained this way achieves a high degree of disentanglement, capturing\nfine-grained prosodic information that is robust and transferable. The latent\nspace turns out to have interpretable structure with its principal components\ncorresponding to pitch and energy.", "published": "2024-09-13 09:27:05", "link": "http://arxiv.org/abs/2409.08664v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "NEST-RQ: Next Token Prediction for Speech Self-Supervised Pre-Training", "abstract": "Speech self-supervised pre-training can effectively improve the performance\nof downstream tasks. However, previous self-supervised learning (SSL) methods\nfor speech, such as HuBERT and BEST-RQ, focus on utilizing non-causal encoders\nwith bidirectional context, and lack sufficient support for downstream\nstreaming models. To address this issue, we introduce the next token prediction\nbased speech pre-training method with random-projection quantizer (NEST-RQ).\nNEST-RQ employs causal encoders with only left context and uses next token\nprediction (NTP) as the training task. On the large-scale dataset, compared to\nBEST-RQ, the proposed NEST-RQ achieves comparable performance on non-streaming\nautomatic speech recognition (ASR) and better performance on streaming ASR. We\nalso conduct analytical experiments in terms of the future context size of\nstreaming ASR, the codebook quality of SSL and the model size of the encoder.\nIn summary, the paper demonstrates the feasibility of the NTP in speech SSL and\nprovides empirical evidence and insights for speech SSL research.", "published": "2024-09-13 09:48:11", "link": "http://arxiv.org/abs/2409.08680v1", "categories": ["eess.AS", "cs.AI", "cs.CL"], "primary_category": "eess.AS"}
{"title": "B4: Towards Optimal Assessment of Plausible Code Solutions with\n  Plausible Tests", "abstract": "Selecting the best code solution from multiple generated ones is an essential\ntask in code generation, which can be achieved by using some reliable\nvalidators (e.g., developer-written test cases) for assistance. Since reliable\ntest cases are not always available and can be expensive to build in practice,\nresearchers propose to automatically generate test cases to assess code\nsolutions. However, when both code solutions and test cases are plausible and\nnot reliable, selecting the best solution becomes challenging. Although some\nheuristic strategies have been proposed to tackle this problem, they lack a\nstrong theoretical guarantee and it is still an open question whether an\noptimal selection strategy exists. Our work contributes in two ways. First, we\nshow that within a Bayesian framework, the optimal selection strategy can be\ndefined based on the posterior probability of the observed passing states\nbetween solutions and tests. The problem of identifying the best solution is\nthen framed as an integer programming problem. Second, we propose an efficient\napproach for approximating this optimal (yet uncomputable) strategy, where the\napproximation error is bounded by the correctness of prior knowledge. We then\nincorporate effective prior knowledge to tailor code generation tasks. Both\ntheoretical and empirical studies confirm that existing heuristics are limited\nin selecting the best solutions with plausible test cases. Our proposed\napproximated optimal strategy B4 significantly surpasses existing heuristics in\nselecting code solutions generated by large language models (LLMs) with\nLLM-generated tests, achieving a relative performance improvement by up to 50%\nover the strongest heuristic and 246% over the random selection in the most\nchallenging scenarios. Our code is publicly available at\nhttps://github.com/ZJU-CTAG/B4.", "published": "2024-09-13 10:22:08", "link": "http://arxiv.org/abs/2409.08692v1", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Layerwise Change of Knowledge in Neural Networks", "abstract": "This paper aims to explain how a deep neural network (DNN) gradually extracts\nnew knowledge and forgets noisy features through layers in forward propagation.\nUp to now, although the definition of knowledge encoded by the DNN has not\nreached a consensus, Previous studies have derived a series of mathematical\nevidence to take interactions as symbolic primitive inference patterns encoded\nby a DNN. We extend the definition of interactions and, for the first time,\nextract interactions encoded by intermediate layers. We quantify and track the\nnewly emerged interactions and the forgotten interactions in each layer during\nthe forward propagation, which shed new light on the learning behavior of DNNs.\nThe layer-wise change of interactions also reveals the change of the\ngeneralization capacity and instability of feature representations of a DNN.", "published": "2024-09-13 10:59:24", "link": "http://arxiv.org/abs/2409.08712v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Journalists, Emotions, and the Introduction of Generative AI Chatbots: A\n  Large-Scale Analysis of Tweets Before and After the Launch of ChatGPT", "abstract": "As part of a broader look at the impact of generative AI, this study\ninvestigated the emotional responses of journalists to the release of ChatGPT\nat the time of its launch. By analyzing nearly 1 million Tweets from\njournalists at major U.S. news outlets, we tracked changes in emotional tone\nand sentiment before and after the introduction of ChatGPT in November 2022.\nUsing various computational and natural language processing techniques to\nmeasure emotional shifts in response to ChatGPT's release, we found an increase\nin positive emotion and a more favorable tone post-launch, suggesting initial\noptimism toward AI's potential. This research underscores the pivotal role of\njournalists as interpreters of technological innovation and disruption,\nhighlighting how their emotional reactions may shape public narratives around\nemerging technologies. The study contributes to understanding the intersection\nof journalism, emotion, and AI, offering insights into the broader societal\nimpact of generative AI tools.", "published": "2024-09-13 12:09:20", "link": "http://arxiv.org/abs/2409.08761v1", "categories": ["cs.CC", "cs.AI", "cs.CL"], "primary_category": "cs.CC"}
{"title": "Exploring SSL Discrete Speech Features for Zipformer-based Contextual\n  ASR", "abstract": "Self-supervised learning (SSL) based discrete speech representations are\nhighly compact and domain adaptable. In this paper, SSL discrete speech\nfeatures extracted from WavLM models are used as additional cross-utterance\nacoustic context features in Zipformer-Transducer ASR systems. The efficacy of\nreplacing Fbank features with discrete token features for modelling either\ncross-utterance contexts (from preceding and future segments), or current\nutterance's internal contexts alone, or both at the same time, are demonstrated\nthoroughly on the Gigaspeech 1000-hr corpus. The best Zipformer-Transducer\nsystem using discrete tokens based cross-utterance context features outperforms\nthe baseline using utterance internal context only with statistically\nsignificant word error rate (WER) reductions of 0.32% to 0.41% absolute (2.78%\nto 3.54% relative) on the dev and test data. The lowest published WER of 11.15%\nand 11.14% were obtained on the dev and test sets. Our work is open-source and\npublicly available at\nhttps://github.com/open-creator/icefall/tree/master/egs/gigaspeech/Context\\_ASR.", "published": "2024-09-13 13:01:09", "link": "http://arxiv.org/abs/2409.08797v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Exploring SSL Discrete Tokens for Multilingual ASR", "abstract": "With the advancement of Self-supervised Learning (SSL) in speech-related\ntasks, there has been growing interest in utilizing discrete tokens generated\nby SSL for automatic speech recognition (ASR), as they offer faster processing\ntechniques. However, previous studies primarily focused on multilingual ASR\nwith Fbank features or English ASR with discrete tokens, leaving a gap in\nadapting discrete tokens for multilingual ASR scenarios. This study presents a\ncomprehensive comparison of discrete tokens generated by various leading SSL\nmodels across multiple language domains. We aim to explore the performance and\nefficiency of speech discrete tokens across multiple language domains for both\nmonolingual and multilingual ASR scenarios. Experimental results demonstrate\nthat discrete tokens achieve comparable results against systems trained on\nFbank features in ASR tasks across seven language domains with an average word\nerror rate (WER) reduction of 0.31% and 1.76% absolute (2.80% and 15.70%\nrelative) on dev and test sets respectively, with particularly WER reduction of\n6.82% absolute (41.48% relative) on the Polish test set.", "published": "2024-09-13 13:13:39", "link": "http://arxiv.org/abs/2409.08805v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "FP-VEC: Fingerprinting Large Language Models via Efficient Vector\n  Addition", "abstract": "Training Large Language Models (LLMs) requires immense computational power\nand vast amounts of data. As a result, protecting the intellectual property of\nthese models through fingerprinting is essential for ownership authentication.\nWhile adding fingerprints to LLMs through fine-tuning has been attempted, it\nremains costly and unscalable. In this paper, we introduce FP-VEC, a pilot\nstudy on using fingerprint vectors as an efficient fingerprinting method for\nLLMs. Our approach generates a fingerprint vector that represents a\nconfidential signature embedded in the model, allowing the same fingerprint to\nbe seamlessly incorporated into an unlimited number of LLMs via vector\naddition. Results on several LLMs show that FP-VEC is lightweight by running on\nCPU-only devices for fingerprinting, scalable with a single training and\nunlimited fingerprinting process, and preserves the model's normal behavior.\nThe project page is available at https://fingerprintvector.github.io .", "published": "2024-09-13 14:04:39", "link": "http://arxiv.org/abs/2409.08846v1", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Exploring the Impact of Data Quantity on ASR in Extremely Low-resource\n  Languages", "abstract": "This study investigates the efficacy of data augmentation techniques for\nlow-resource automatic speech recognition (ASR), focusing on two endangered\nAustronesian languages, Amis and Seediq. Recognizing the potential of\nself-supervised learning (SSL) in low-resource settings, we explore the impact\nof data volume on the continued pre-training of SSL models. We propose a novel\ndata-selection scheme leveraging a multilingual corpus to augment the limited\ntarget language data. This scheme utilizes a language classifier to extract\nutterance embeddings and employs one-class classifiers to identify utterances\nphonetically and phonologically proximate to the target languages. Utterances\nare ranked and selected based on their decision scores, ensuring the inclusion\nof highly relevant data in the SSL-ASR pipeline. Our experimental results\ndemonstrate the effectiveness of this approach, yielding substantial\nimprovements in ASR performance for both Amis and Seediq. These findings\nunderscore the feasibility and promise of data augmentation through\ncross-lingual transfer learning for low-resource language ASR.", "published": "2024-09-13 14:35:47", "link": "http://arxiv.org/abs/2409.08872v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Affective Computing Has Changed: The Foundation Model Disruption", "abstract": "The dawn of Foundation Models has on the one hand revolutionised a wide range\nof research problems, and, on the other hand, democratised the access and use\nof AI-based tools by the general public. We even observe an incursion of these\nmodels into disciplines related to human psychology, such as the Affective\nComputing domain, suggesting their affective, emerging capabilities. In this\nwork, we aim to raise awareness of the power of Foundation Models in the field\nof Affective Computing by synthetically generating and analysing multimodal\naffective data, focusing on vision, linguistics, and speech (acoustics). We\nalso discuss some fundamental problems, such as ethical issues and regulatory\naspects, related to the use of Foundation Models in this research area.", "published": "2024-09-13 15:20:18", "link": "http://arxiv.org/abs/2409.08907v1", "categories": ["cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.AI"}
{"title": "Safeguarding Decentralized Social Media: LLM Agents for Automating\n  Community Rule Compliance", "abstract": "Ensuring content compliance with community guidelines is crucial for\nmaintaining healthy online social environments. However, traditional\nhuman-based compliance checking struggles with scaling due to the increasing\nvolume of user-generated content and a limited number of moderators. Recent\nadvancements in Natural Language Understanding demonstrated by Large Language\nModels unlock new opportunities for automated content compliance verification.\nThis work evaluates six AI-agents built on Open-LLMs for automated rule\ncompliance checking in Decentralized Social Networks, a challenging environment\ndue to heterogeneous community scopes and rules. Analyzing over 50,000 posts\nfrom hundreds of Mastodon servers, we find that AI-agents effectively detect\nnon-compliant content, grasp linguistic subtleties, and adapt to diverse\ncommunity contexts. Most agents also show high inter-rater reliability and\nconsistency in score justification and suggestions for compliance. Human-based\nevaluation with domain experts confirmed the agents' reliability and\nusefulness, rendering them promising tools for semi-automated or\nhuman-in-the-loop content moderation systems.", "published": "2024-09-13 16:29:25", "link": "http://arxiv.org/abs/2409.08963v1", "categories": ["cs.CY", "cs.CL", "cs.HC", "physics.soc-ph"], "primary_category": "cs.CY"}
{"title": "Agents in Software Engineering: Survey, Landscape, and Vision", "abstract": "In recent years, Large Language Models (LLMs) have achieved remarkable\nsuccess and have been widely used in various downstream tasks, especially in\nthe tasks of the software engineering (SE) field. We find that many studies\ncombining LLMs with SE have employed the concept of agents either explicitly or\nimplicitly. However, there is a lack of an in-depth survey to sort out the\ndevelopment context of existing works, analyze how existing works combine the\nLLM-based agent technologies to optimize various tasks, and clarify the\nframework of LLM-based agents in SE. In this paper, we conduct the first survey\nof the studies on combining LLM-based agents with SE and present a framework of\nLLM-based agents in SE which includes three key modules: perception, memory,\nand action. We also summarize the current challenges in combining the two\nfields and propose future opportunities in response to existing challenges. We\nmaintain a GitHub repository of the related papers at:\nhttps://github.com/DeepSoftwareAnalytics/Awesome-Agent4SE.", "published": "2024-09-13 17:55:58", "link": "http://arxiv.org/abs/2409.09030v2", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "AccentBox: Towards High-Fidelity Zero-Shot Accent Generation", "abstract": "While recent Zero-Shot Text-to-Speech (ZS-TTS) models have achieved high\nnaturalness and speaker similarity, they fall short in accent fidelity and\ncontrol. To address this issue, we propose zero-shot accent generation that\nunifies Foreign Accent Conversion (FAC), accented TTS, and ZS-TTS, with a novel\ntwo-stage pipeline. In the first stage, we achieve state-of-the-art (SOTA) on\nAccent Identification (AID) with 0.56 f1 score on unseen speakers. In the\nsecond stage, we condition a ZS-TTS system on the pretrained speaker-agnostic\naccent embeddings extracted by the AID model. The proposed system achieves\nhigher accent fidelity on inherent/cross accent generation, and enables unseen\naccent generation.", "published": "2024-09-13 06:05:10", "link": "http://arxiv.org/abs/2409.09098v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multimodal Fusion with LLMs for Engagement Prediction in Natural\n  Conversation", "abstract": "Over the past decade, wearable computing devices (``smart glasses'') have\nundergone remarkable advancements in sensor technology, design, and processing\npower, ushering in a new era of opportunity for high-density human behavior\ndata. Equipped with wearable cameras, these glasses offer a unique opportunity\nto analyze non-verbal behavior in natural settings as individuals interact. Our\nfocus lies in predicting engagement in dyadic interactions by scrutinizing\nverbal and non-verbal cues, aiming to detect signs of disinterest or confusion.\nLeveraging such analyses may revolutionize our understanding of human\ncommunication, foster more effective collaboration in professional\nenvironments, provide better mental health support through empathetic virtual\ninteractions, and enhance accessibility for those with communication barriers.\n  In this work, we collect a dataset featuring 34 participants engaged in\ncasual dyadic conversations, each providing self-reported engagement ratings at\nthe end of each conversation. We introduce a novel fusion strategy using Large\nLanguage Models (LLMs) to integrate multiple behavior modalities into a\n``multimodal transcript'' that can be processed by an LLM for behavioral\nreasoning tasks. Remarkably, this method achieves performance comparable to\nestablished fusion techniques even in its preliminary implementation,\nindicating strong potential for further research and optimization. This fusion\nmethod is one of the first to approach ``reasoning'' about real-world human\nbehavior through a language model. Smart glasses provide us the ability to\nunobtrusively gather high-density multimodal data on human behavior, paving the\nway for new approaches to understanding and improving human communication with\nthe potential for important societal benefits. The features and data collected\nduring the studies will be made publicly available to promote further research.", "published": "2024-09-13 18:28:12", "link": "http://arxiv.org/abs/2409.09135v1", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Transformer with Controlled Attention for Synchronous Motion Captioning", "abstract": "In this paper, we address a challenging task, synchronous motion captioning,\nthat aim to generate a language description synchronized with human motion\nsequences. This task pertains to numerous applications, such as aligned sign\nlanguage transcription, unsupervised action segmentation and temporal\ngrounding. Our method introduces mechanisms to control self- and\ncross-attention distributions of the Transformer, allowing interpretability and\ntime-aligned text generation. We achieve this through masking strategies and\nstructuring losses that push the model to maximize attention only on the most\nimportant frames contributing to the generation of a motion word. These\nconstraints aim to prevent undesired mixing of information in attention maps\nand to provide a monotonic attention distribution across tokens. Thus, the\ncross attentions of tokens are used for progressive text generation in\nsynchronization with human motion sequences. We demonstrate the superior\nperformance of our approach through evaluation on the two available benchmark\ndatasets, KIT-ML and HumanML3D. As visual evaluation is essential for this\ntask, we provide a comprehensive set of animated visual illustrations in the\ncode repository: https://github.com/rd20karim/Synch-Transformer.", "published": "2024-09-13 20:30:29", "link": "http://arxiv.org/abs/2409.09177v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "ReCLAP: Improving Zero Shot Audio Classification by Describing Sounds", "abstract": "Open-vocabulary audio-language models, like CLAP, offer a promising approach\nfor zero-shot audio classification (ZSAC) by enabling classification with any\narbitrary set of categories specified with natural language prompts. In this\npaper, we propose a simple but effective method to improve ZSAC with CLAP.\nSpecifically, we shift from the conventional method of using prompts with\nabstract category labels (e.g., Sound of an organ) to prompts that describe\nsounds using their inherent descriptive features in a diverse context (e.g.,The\norgan's deep and resonant tones filled the cathedral.). To achieve this, we\nfirst propose ReCLAP, a CLAP model trained with rewritten audio captions for\nimproved understanding of sounds in the wild. These rewritten captions describe\neach sound event in the original caption using their unique discriminative\ncharacteristics. ReCLAP outperforms all baselines on both multi-modal\naudio-text retrieval and ZSAC. Next, to improve zero-shot audio classification\nwith ReCLAP, we propose prompt augmentation. In contrast to the traditional\nmethod of employing hand-written template prompts, we generate custom prompts\nfor each unique label in the dataset. These custom prompts first describe the\nsound event in the label and then employ them in diverse scenes. Our proposed\nmethod improves ReCLAP's performance on ZSAC by 1%-18% and outperforms all\nbaselines by 1% - 55%.", "published": "2024-09-13 21:58:20", "link": "http://arxiv.org/abs/2409.09213v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Eureka: Evaluating and Understanding Large Foundation Models", "abstract": "Rigorous and reproducible evaluation is critical for assessing the state of\nthe art and for guiding scientific advances in Artificial Intelligence.\nEvaluation is challenging in practice due to several reasons, including\nbenchmark saturation, lack of transparency in methods used for measurement,\ndevelopment challenges in extracting measurements for generative tasks, and,\nmore generally, the extensive number of capabilities required for a\nwell-rounded comparison across models. We make three contributions to alleviate\nthe above challenges. First, we present Eureka, an open-source framework for\nstandardizing evaluations of large foundation models beyond single-score\nreporting and rankings. Second, we introduce Eureka-Bench as an extensible\ncollection of benchmarks testing capabilities that (i) are still challenging\nfor state-of-the-art models and (ii) represent fundamental but overlooked\nlanguage and multimodal capabilities. The inherent space for improvement in\nnon-saturated benchmarks enables us to discover meaningful differences between\nmodels at a capability level. Third, using Eureka, we conduct an analysis of 12\nstate-of-the-art models, providing in-depth insights into failure understanding\nand model comparison, which can be leveraged to plan targeted improvements. In\ncontrast to recent trends in reports and leaderboards showing absolute rankings\nand claims for one model or another to be the best, our analysis shows that\nthere is no such best model. Different models have different strengths, but\nthere are models that appear more often than others as best performers for some\ncapabilities. Despite the recent improvements, current models still struggle\nwith several fundamental capabilities including detailed image understanding,\nbenefiting from multimodal input when available rather than fully relying on\nlanguage, factuality and grounding for information retrieval, and over\nrefusals.", "published": "2024-09-13 18:01:49", "link": "http://arxiv.org/abs/2409.10566v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "I.2"], "primary_category": "cs.LG"}
{"title": "KodeXv0.1: A Family of State-of-the-Art Financial Large Language Models", "abstract": "Although powerful, current cutting-edge LLMs may not fulfil the needs of\nhighly specialised sectors. We introduce KodeXv0.1, a family of large language\nmodels that outclass GPT-4 in financial question answering. We utilise the base\nvariants of Llama 3.1 8B and 70B and adapt them to the financial domain through\na custom training regime. To this end, we collect and process a large number of\npublicly available financial documents such as earnings calls and business\nreports. These are used to generate a high-quality, synthetic dataset\nconsisting of Context-Question-Answer triplets which closely mirror real-world\nfinancial tasks. Using the train split of this dataset, we perform RAG-aware\n4bit LoRA instruction tuning runs of Llama 3.1 base variants to produce\nKodeX-8Bv0.1 and KodeX-70Bv0.1. We then complete extensive model evaluations\nusing FinanceBench, FinQABench and the withheld test split of our dataset. Our\nresults show that KodeX-8Bv0.1 is more reliable in financial contexts than\ncutting-edge instruct models in the same parameter regime, surpassing them by\nup to 9.24%. In addition, it is even capable of outperforming state-of-the-art\nproprietary models such as GPT-4 by up to 7.07%. KodeX-70Bv0.1 represents a\nfurther improvement upon this, exceeding GPT-4's performance on every tested\nbenchmark.", "published": "2024-09-13 16:43:08", "link": "http://arxiv.org/abs/2409.13749v1", "categories": ["cs.CL", "cs.AI", "q-fin.CP", "I.2.7"], "primary_category": "cs.CL"}
{"title": "CPT-Boosted Wav2vec2.0: Towards Noise Robust Speech Recognition for\n  Classroom Environments", "abstract": "Creating Automatic Speech Recognition (ASR) systems that are robust and\nresilient to classroom conditions is paramount to the development of AI tools\nto aid teachers and students. In this work, we study the efficacy of continued\npretraining (CPT) in adapting Wav2vec2.0 to the classroom domain. We show that\nCPT is a powerful tool in that regard and reduces the Word Error Rate (WER) of\nWav2vec2.0-based models by upwards of 10%. More specifically, CPT improves the\nmodel's robustness to different noises, microphones and classroom conditions.", "published": "2024-09-13 19:14:18", "link": "http://arxiv.org/abs/2409.14494v4", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "E2MoCase: A Dataset for Emotional, Event and Moral Observations in News\n  Articles on High-impact Legal Cases", "abstract": "The way media reports on legal cases can significantly shape public opinion,\noften embedding subtle biases that influence societal views on justice and\nmorality. Analyzing these biases requires a holistic approach that captures the\nemotional tone, moral framing, and specific events within the narratives. In\nthis work we introduce E2MoCase, a novel dataset designed to facilitate the\nintegrated analysis of emotions, moral values, and events within legal\nnarratives and media coverage. By leveraging advanced models for emotion\ndetection, moral value identification, and event extraction, E2MoCase offers a\nmulti-dimensional perspective on how legal cases are portrayed in news\narticles.", "published": "2024-09-13 17:31:09", "link": "http://arxiv.org/abs/2409.09001v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.DL", "physics.soc-ph"], "primary_category": "cs.CL"}
{"title": "Multi-modal Speech Transformer Decoders: When Do Multiple Modalities\n  Improve Accuracy?", "abstract": "Decoder-only discrete-token language models have recently achieved\nsignificant success in automatic speech recognition. However, systematic\nanalyses of how different modalities impact performance in specific scenarios\nremain limited. In this paper, we investigate the effects of multiple\nmodalities on recognition accuracy on both synthetic and real-world datasets.\nOur experiments suggest that: (1) Integrating more modalities can increase\naccuracy; in particular, our paper is, to our best knowledge, the first to show\nthe benefit of combining audio, image context, and lip information; (2) Images\nas a supplementary modality for speech recognition provide the greatest benefit\nat moderate noise levels, moreover, they exhibit a different trend compared to\ninherently synchronized modalities like lip movements; (3) Performance improves\non both synthetic and real-world datasets when the most relevant visual\ninformation is filtered as a preprocessing step.", "published": "2024-09-13 22:18:45", "link": "http://arxiv.org/abs/2409.09221v2", "categories": ["cs.CV", "cs.CL", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "FLAMO: An Open-Source Library for Frequency-Domain Differentiable Audio\n  Processing", "abstract": "We present FLAMO, a Frequency-sampling Library for Audio-Module Optimization\ndesigned to implement and optimize differentiable linear time-invariant audio\nsystems. The library is open-source and built on the frequency-sampling filter\ndesign method, allowing for the creation of differentiable modules that can be\nused stand-alone or within the computation graph of neural networks,\nsimplifying the development of differentiable audio systems. It includes\npredefined filtering modules and auxiliary classes for constructing, training,\nand logging the optimized systems, all accessible through an intuitive\ninterface. Practical application of these modules is demonstrated through two\ncase studies: the optimization of an artificial reverberator and an active\nacoustics system for improved response smoothness.", "published": "2024-09-13 11:19:51", "link": "http://arxiv.org/abs/2409.08723v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Data Efficient Child-Adult Speaker Diarization with Simulated\n  Conversations", "abstract": "Automating child speech analysis is crucial for applications such as\nneurocognitive assessments. Speaker diarization, which identifies ``who spoke\nwhen'', is an essential component of the automated analysis. However, publicly\navailable child-adult speaker diarization solutions are scarce due to privacy\nconcerns and a lack of annotated datasets, while manually annotating data for\neach scenario is both time-consuming and costly. To overcome these challenges,\nwe propose a data-efficient solution by creating simulated child-adult\nconversations using AudioSet. We then train a Whisper Encoder-based model,\nachieving strong zero-shot performance on child-adult speaker diarization using\nreal datasets. The model performance improves substantially when fine-tuned\nwith only 30 minutes of real train data, with LoRA further improving the\ntransfer learning performance. The source code and the child-adult speaker\ndiarization model trained on simulated conversations are publicly available.", "published": "2024-09-13 14:45:18", "link": "http://arxiv.org/abs/2409.08881v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Unified Audio Event Detection", "abstract": "Sound Event Detection (SED) detects regions of sound events, while Speaker\nDiarization (SD) segments speech conversations attributed to individual\nspeakers. In SED, all speaker segments are classified as a single speech event,\nwhile in SD, non-speech sounds are treated merely as background noise. Thus,\nboth tasks provide only partial analysis in complex audio scenarios involving\nboth speech conversation and non-speech sounds. In this paper, we introduce a\nnovel task called Unified Audio Event Detection (UAED) for comprehensive audio\nanalysis. UAED explores the synergy between SED and SD tasks, simultaneously\ndetecting non-speech sound events and fine-grained speech events based on\nspeaker identities. To tackle this task, we propose a Transformer-based UAED\n(T-UAED) framework and construct the UAED Data derived from the Librispeech\ndataset and DESED soundbank. Experiments demonstrate that the proposed\nframework effectively exploits task interactions and substantially outperforms\nthe baseline that simply combines the outputs of SED and SD models. T-UAED also\nshows its versatility by performing comparably to specialized models for\nindividual SED and SD tasks on DESED and CALLHOME datasets.", "published": "2024-09-13 06:11:11", "link": "http://arxiv.org/abs/2409.08552v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Frequency Tracking Features for Data-Efficient Deep Siren Identification", "abstract": "The identification of siren sounds in urban soundscapes is a crucial safety\naspect for smart vehicles and has been widely addressed by means of neural\nnetworks that ensure robustness to both the diversity of siren signals and the\nstrong and unstructured background noise characterizing traffic. Convolutional\nneural networks analyzing spectrogram features of incoming signals achieve\nstate-of-the-art performance when enough training data capturing the diversity\nof the target acoustic scenes is available. In practice, data is usually\nlimited and algorithms should be robust to adapt to unseen acoustic conditions\nwithout requiring extensive datasets for re-training. In this work, given the\nharmonic nature of siren signals, characterized by a periodically evolving\nfundamental frequency, we propose a low-complexity feature extraction method\nbased on frequency tracking using a single-parameter adaptive notch filter. The\nfeatures are then used to design a small-scale convolutional network suitable\nfor training with limited data. The evaluation results indicate that the\nproposed model consistently outperforms the traditional spectrogram-based model\nwhen limited training data is available, achieves better cross-domain\ngeneralization and has a smaller size.", "published": "2024-09-13 07:08:06", "link": "http://arxiv.org/abs/2409.08587v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Domain-Invariant Representation Learning of Bird Sounds", "abstract": "Passive acoustic monitoring (PAM) is crucial for bioacoustic research,\nenabling non-invasive species tracking and biodiversity monitoring. Citizen\nscience platforms provide large annotated datasets from focal recordings, where\nthe target species is intentionally recorded. However, PAM requires monitoring\nin passive soundscapes, creating a domain shift between focal and passive\nrecordings, challenging deep learning models trained on focal recordings. To\naddress domain generalization, we leverage supervised contrastive learning by\nenforcing domain invariance across same-class examples from different domains.\nAdditionally, we propose ProtoCLR, an alternative to SupCon loss which reduces\nthe computational complexity by comparing examples to class prototypes instead\nof pairwise comparisons. We conduct few-shot classification based on BIRB, a\nlarge-scale bird sound benchmark to assess pre-trained bioacoustic models. Our\nfindings suggest that ProtoCLR is a better alternative to SupCon.", "published": "2024-09-13 07:09:17", "link": "http://arxiv.org/abs/2409.08589v6", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Effective Integration of KAN for Keyword Spotting", "abstract": "Keyword spotting (KWS) is an important speech processing component for smart\ndevices with voice assistance capability. In this paper, we investigate if\nKolmogorov-Arnold Networks (KAN) can be used to enhance the performance of KWS.\nWe explore various approaches to integrate KAN for a model architecture based\non 1D Convolutional Neural Networks (CNN). We find that KAN is effective at\nmodeling high-level features in lower-dimensional spaces, resulting in improved\nKWS performance when integrated appropriately. The findings shed light on\nunderstanding KAN for speech processing tasks and on other modalities for\nfuture researchers.", "published": "2024-09-13 07:35:34", "link": "http://arxiv.org/abs/2409.08605v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DualSep: A Light-weight dual-encoder convolutional recurrent network for\n  real-time in-car speech separation", "abstract": "Advancements in deep learning and voice-activated technologies have driven\nthe development of human-vehicle interaction. Distributed microphone arrays are\nwidely used in in-car scenarios because they can accurately capture the voices\nof passengers from different speech zones. However, the increase in the number\nof audio channels, coupled with the limited computational resources and low\nlatency requirements of in-car systems, presents challenges for in-car\nmulti-channel speech separation. To migrate the problems, we propose a\nlightweight framework that cascades digital signal processing (DSP) and neural\nnetworks (NN). We utilize fixed beamforming (BF) to reduce computational costs\nand independent vector analysis (IVA) to provide spatial prior. We employ dual\nencoders for dual-branch modeling, with spatial encoder capturing spatial cues\nand spectral encoder preserving spectral information, facilitating\nspatial-spectral fusion. Our proposed system supports both streaming and\nnon-streaming modes. Experimental results demonstrate the superiority of the\nproposed system across various metrics. With only 0.83M parameters and 0.39\nreal-time factor (RTF) on an Intel Core i7 (2.6GHz) CPU, it effectively\nseparates speech into distinct speech zones. Our demos are available at\nhttps://honee-w.github.io/DualSep/.", "published": "2024-09-13 07:53:17", "link": "http://arxiv.org/abs/2409.08610v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DM: Dual-path Magnitude Network for General Speech Restoration", "abstract": "In this paper, we introduce a novel general speech restoration model: the\nDual-path Magnitude (DM) network, designed to address multiple distortions\nincluding noise, reverberation, and bandwidth degradation effectively. The DM\nnetwork employs dual parallel magnitude decoders that share parameters: one\nuses a masking-based algorithm for distortion removal and the other employs a\nmapping-based approach for speech restoration. A novel aspect of the DM network\nis the integration of the magnitude spectrogram output from the masking decoder\ninto the mapping decoder through a skip connection, enhancing the overall\nrestoration capability. This integrated approach overcomes the inherent\nlimitations observed in previous models, as detailed in a step-by-step\nanalysis. The experimental results demonstrate that the DM network outperforms\nother baseline models in the comprehensive aspect of general speech\nrestoration, achieving substantial restoration with fewer parameters.", "published": "2024-09-13 10:42:59", "link": "http://arxiv.org/abs/2409.08702v1", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "Text-To-Speech Synthesis In The Wild", "abstract": "Text-to-speech (TTS) systems are traditionally trained using modest databases\nof studio-quality, prompted or read speech collected in benign acoustic\nenvironments such as anechoic rooms. The recent literature nonetheless shows\nefforts to train TTS systems using data collected in the wild. While this\napproach allows for the use of massive quantities of natural speech, until now,\nthere are no common datasets. We introduce the TTS In the Wild (TITW) dataset,\nthe result of a fully automated pipeline, in this case, applied to the\nVoxCeleb1 dataset commonly used for speaker recognition. We further propose two\ntraining sets. TITW-Hard is derived from the transcription, segmentation, and\nselection of VoxCeleb1 source data. TITW-Easy is derived from the additional\napplication of enhancement and additional data selection based on DNSMOS. We\nshow that a number of recent TTS models can be trained successfully using\nTITW-Easy, but that it remains extremely challenging to produce similar results\nusing TITW-Hard. Both the dataset and protocols are publicly available and\nsupport the benchmarking of TTS systems trained using TITW data.", "published": "2024-09-13 10:58:55", "link": "http://arxiv.org/abs/2409.08711v1", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "DFADD: The Diffusion and Flow-Matching Based Audio Deepfake Dataset", "abstract": "Mainstream zero-shot TTS production systems like Voicebox and Seed-TTS\nachieve human parity speech by leveraging Flow-matching and Diffusion models,\nrespectively. Unfortunately, human-level audio synthesis leads to identity\nmisuse and information security issues. Currently, many antispoofing models\nhave been developed against deepfake audio. However, the efficacy of current\nstate-of-the-art anti-spoofing models in countering audio synthesized by\ndiffusion and flowmatching based TTS systems remains unknown. In this paper, we\nproposed the Diffusion and Flow-matching based Audio Deepfake (DFADD) dataset.\nThe DFADD dataset collected the deepfake audio based on advanced diffusion and\nflowmatching TTS models. Additionally, we reveal that current anti-spoofing\nmodels lack sufficient robustness against highly human-like audio generated by\ndiffusion and flow-matching TTS systems. The proposed DFADD dataset addresses\nthis gap and provides a valuable resource for developing more resilient\nanti-spoofing models.", "published": "2024-09-13 11:33:34", "link": "http://arxiv.org/abs/2409.08731v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "LLaQo: Towards a Query-Based Coach in Expressive Music Performance\n  Assessment", "abstract": "Research in music understanding has extensively explored composition-level\nattributes such as key, genre, and instrumentation through advanced\nrepresentations, leading to cross-modal applications using large language\nmodels. However, aspects of musical performance such as stylistic expression\nand technique remain underexplored, along with the potential of using large\nlanguage models to enhance educational outcomes with customized feedback. To\nbridge this gap, we introduce LLaQo, a Large Language Query-based music coach\nthat leverages audio language modeling to provide detailed and formative\nassessments of music performances. We also introduce instruction-tuned\nquery-response datasets that cover a variety of performance dimensions from\npitch accuracy to articulation, as well as contextual performance understanding\n(such as difficulty and performance techniques). Utilizing AudioMAE encoder and\nVicuna-7b LLM backend, our model achieved state-of-the-art (SOTA) results in\npredicting teachers' performance ratings, as well as in identifying piece\ndifficulty and playing techniques. Textual responses from LLaQo was moreover\nrated significantly higher compared to other baseline models in a user study\nusing audio-text matching. Our proposed model can thus provide informative\nanswers to open-ended questions related to musical performance from audio data.", "published": "2024-09-13 12:59:39", "link": "http://arxiv.org/abs/2409.08795v2", "categories": ["eess.AS", "cs.MM"], "primary_category": "eess.AS"}
{"title": "HLTCOE JHU Submission to the Voice Privacy Challenge 2024", "abstract": "We present a number of systems for the Voice Privacy Challenge, including\nvoice conversion based systems such as the kNN-VC method and the WavLM voice\nConversion method, and text-to-speech (TTS) based systems including\nWhisper-VITS. We found that while voice conversion systems better preserve\nemotional content, they struggle to conceal speaker identity in semi-white-box\nattack scenarios; conversely, TTS methods perform better at anonymization and\nworse at emotion preservation. Finally, we propose a random admixture system\nwhich seeks to balance out the strengths and weaknesses of the two category of\nsystems, achieving a strong EER of over 40% while maintaining UAR at a\nrespectable 47%.", "published": "2024-09-13 15:29:37", "link": "http://arxiv.org/abs/2409.08913v2", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "MambaFoley: Foley Sound Generation using Selective State-Space Models", "abstract": "Recent advancements in deep learning have led to widespread use of techniques\nfor audio content generation, notably employing Denoising Diffusion\nProbabilistic Models (DDPM) across various tasks. Among these, Foley Sound\nSynthesis is of particular interest for its role in applications for the\ncreation of multimedia content. Given the temporal-dependent nature of sound,\nit is crucial to design generative models that can effectively handle the\nsequential modeling of audio samples. Selective State Space Models (SSMs) have\nrecently been proposed as a valid alternative to previously proposed\ntechniques, demonstrating competitive performance with lower computational\ncomplexity. In this paper, we introduce MambaFoley, a diffusion-based model\nthat, to the best of our knowledge, is the first to leverage the recently\nproposed SSM known as Mamba for the Foley sound generation task. To evaluate\nthe effectiveness of the proposed method, we compare it with a state-of-the-art\nFoley sound generative model using both objective and subjective analyses.", "published": "2024-09-13 19:44:24", "link": "http://arxiv.org/abs/2409.09162v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Learnings from curating a trustworthy, well-annotated, and useful\n  dataset of disordered English speech", "abstract": "Project Euphonia, a Google initiative, is dedicated to improving automatic\nspeech recognition (ASR) of disordered speech. A central objective of the\nproject is to create a large, high-quality, and diverse speech corpus. This\nreport describes the project's latest advancements in data collection and\nannotation methodologies, such as expanding speaker diversity in the database,\nadding human-reviewed transcript corrections and audio quality tags to 350K (of\nthe 1.2M total) audio recordings, and amassing a comprehensive set of metadata\n(including more than 40 speech characteristic labels) for over 75\\% of the\nspeakers in the database. We report on the impact of transcript corrections on\nour machine-learning (ML) research, inter-rater variability of assessments of\ndisordered speech patterns, and our rationale for gathering speech metadata. We\nalso consider the limitations of using automated off-the-shelf annotation\nmethods for assessing disordered speech.", "published": "2024-09-13 20:53:23", "link": "http://arxiv.org/abs/2409.09190v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Seed-Music: A Unified Framework for High Quality and Controlled Music\n  Generation", "abstract": "We introduce Seed-Music, a suite of music generation systems capable of\nproducing high-quality music with fine-grained style control. Our unified\nframework leverages both auto-regressive language modeling and diffusion\napproaches to support two key music creation workflows: controlled music\ngeneration and post-production editing. For controlled music generation, our\nsystem enables vocal music generation with performance controls from\nmulti-modal inputs, including style descriptions, audio references, musical\nscores, and voice prompts. For post-production editing, it offers interactive\ntools for editing lyrics and vocal melodies directly in the generated audio.\n  We encourage readers to listen to demo audio examples at\nhttps://team.doubao.com/seed-music \"https://team.doubao.com/seed-music\".", "published": "2024-09-13 22:06:18", "link": "http://arxiv.org/abs/2409.09214v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Resource-Efficient Reference-Free Evaluation of Audio Captions", "abstract": "To establish the trustworthiness of systems that automatically generate text\ncaptions for audio, images and video, existing reference-free metrics rely on\nlarge pretrained models which are impractical to accommodate in\nresource-constrained settings. To address this, we propose some metrics to\nelicit the model's confidence in its own generation. To assess how well these\nmetrics replace correctness measures that leverage reference captions, we test\ntheir calibration with correctness measures. We discuss why some of these\nconfidence metrics align better with certain correctness measures. Further, we\nprovide insight into why temperature scaling of confidence metrics is\neffective. Our main contribution is a suite of well-calibrated lightweight\nconfidence metrics for reference-free evaluation of captions in\nresource-constrained settings.", "published": "2024-09-13 02:32:10", "link": "http://arxiv.org/abs/2409.08489v2", "categories": ["cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "Apollo: Band-sequence Modeling for High-Quality Audio Restoration", "abstract": "Audio restoration has become increasingly significant in modern society, not\nonly due to the demand for high-quality auditory experiences enabled by\nadvanced playback devices, but also because the growing capabilities of\ngenerative audio models necessitate high-fidelity audio. Typically, audio\nrestoration is defined as a task of predicting undistorted audio from damaged\ninput, often trained using a GAN framework to balance perception and\ndistortion. Since audio degradation is primarily concentrated in mid- and\nhigh-frequency ranges, especially due to codecs, a key challenge lies in\ndesigning a generator capable of preserving low-frequency information while\naccurately reconstructing high-quality mid- and high-frequency content.\nInspired by recent advancements in high-sample-rate music separation, speech\nenhancement, and audio codec models, we propose Apollo, a generative model\ndesigned for high-sample-rate audio restoration. Apollo employs an explicit\nfrequency band split module to model the relationships between different\nfrequency bands, allowing for more coherent and higher-quality restored audio.\nEvaluated on the MUSDB18-HQ and MoisesDB datasets, Apollo consistently\noutperforms existing SR-GAN models across various bit rates and music genres,\nparticularly excelling in complex scenarios involving mixtures of multiple\ninstruments and vocals. Apollo significantly improves music restoration quality\nwhile maintaining computational efficiency. The source code for Apollo is\npublicly available at https://github.com/JusperLee/Apollo.", "published": "2024-09-13 03:25:34", "link": "http://arxiv.org/abs/2409.08514v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "LHQ-SVC: Lightweight and High Quality Singing Voice Conversion Modeling", "abstract": "Singing Voice Conversion (SVC) has emerged as a significant subfield of Voice\nConversion (VC), enabling the transformation of one singer's voice into another\nwhile preserving musical elements such as melody, rhythm, and timbre.\nTraditional SVC methods have limitations in terms of audio quality, data\nrequirements, and computational complexity. In this paper, we propose LHQ-SVC,\na lightweight, CPU-compatible model based on the SVC framework and diffusion\nmodel, designed to reduce model size and computational demand without\nsacrificing performance. We incorporate features to improve inference quality,\nand optimize for CPU execution by using performance tuning tools and parallel\ncomputing frameworks. Our experiments demonstrate that LHQ-SVC maintains\ncompetitive performance, with significant improvements in processing speed and\nefficiency across different devices. The results suggest that LHQ-SVC can meet", "published": "2024-09-13 07:02:36", "link": "http://arxiv.org/abs/2409.08583v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "STA-V2A: Video-to-Audio Generation with Semantic and Temporal Alignment", "abstract": "Visual and auditory perception are two crucial ways humans experience the\nworld. Text-to-video generation has made remarkable progress over the past\nyear, but the absence of harmonious audio in generated video limits its broader\napplications. In this paper, we propose Semantic and Temporal Aligned\nVideo-to-Audio (STA-V2A), an approach that enhances audio generation from\nvideos by extracting both local temporal and global semantic video features and\ncombining these refined video features with text as cross-modal guidance. To\naddress the issue of information redundancy in videos, we propose an onset\nprediction pretext task for local temporal feature extraction and an attentive\npooling module for global semantic feature extraction. To supplement the\ninsufficient semantic information in videos, we propose a Latent Diffusion\nModel with Text-to-Audio priors initialization and cross-modal guidance. We\nalso introduce Audio-Audio Align, a new metric to assess audio-temporal\nalignment. Subjective and objective metrics demonstrate that our method\nsurpasses existing Video-to-Audio models in generating audio with better\nquality, semantic consistency, and temporal alignment. The ablation experiment\nvalidated the effectiveness of each module. Audio samples are available at\nhttps://y-ren16.github.io/STAV2A.", "published": "2024-09-13 07:31:44", "link": "http://arxiv.org/abs/2409.08601v2", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "TapToTab : Video-Based Guitar Tabs Generation using AI and Audio\n  Analysis", "abstract": "The automation of guitar tablature generation from video inputs holds\nsignificant promise for enhancing music education, transcription accuracy, and\nperformance analysis. Existing methods face challenges with consistency and\ncompleteness, particularly in detecting fretboards and accurately identifying\nnotes. To address these issues, this paper introduces an advanced approach\nleveraging deep learning, specifically YOLO models for real-time fretboard\ndetection, and Fourier Transform-based audio analysis for precise note\nidentification. Experimental results demonstrate substantial improvements in\ndetection accuracy and robustness compared to traditional techniques. This\npaper outlines the development, implementation, and evaluation of these\nmethodologies, aiming to revolutionize guitar instruction by automating the\ncreation of guitar tabs from video recordings.", "published": "2024-09-13 08:17:15", "link": "http://arxiv.org/abs/2409.08618v1", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Rhythmic Foley: A Framework For Seamless Audio-Visual Alignment In\n  Video-to-Audio Synthesis", "abstract": "Our research introduces an innovative framework for video-to-audio synthesis,\nwhich solves the problems of audio-video desynchronization and semantic loss in\nthe audio. By incorporating a semantic alignment adapter and a temporal\nsynchronization adapter, our method significantly improves semantic integrity\nand the precision of beat point synchronization, particularly in fast-paced\naction sequences. Utilizing a contrastive audio-visual pre-trained encoder, our\nmodel is trained with video and high-quality audio data, improving the quality\nof the generated audio. This dual-adapter approach empowers users with enhanced\ncontrol over audio semantics and beat effects, allowing the adjustment of the\ncontroller to achieve better results. Extensive experiments substantiate the\neffectiveness of our framework in achieving seamless audio-visual alignment.", "published": "2024-09-13 08:33:03", "link": "http://arxiv.org/abs/2409.08628v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Acoustic identification of individual animals with hierarchical\n  contrastive learning", "abstract": "Acoustic identification of individual animals (AIID) is closely related to\naudio-based species classification but requires a finer level of detail to\ndistinguish between individual animals within the same species. In this work,\nwe frame AIID as a hierarchical multi-label classification task and propose the\nuse of hierarchy-aware loss functions to learn robust representations of\nindividual identities that maintain the hierarchical relationships among\nspecies and taxa. Our results demonstrate that hierarchical embeddings not only\nenhance identification accuracy at the individual level but also at higher\ntaxonomic levels, effectively preserving the hierarchical structure in the\nlearned representations. By comparing our approach with non-hierarchical\nmodels, we highlight the advantage of enforcing this structure in the embedding\nspace. Additionally, we extend the evaluation to the classification of novel\nindividual classes, demonstrating the potential of our method in open-set\nclassification scenarios.", "published": "2024-09-13 09:37:44", "link": "http://arxiv.org/abs/2409.08673v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Using Ear-EEG to Decode Auditory Attention in Multiple-speaker\n  Environment", "abstract": "Auditory Attention Decoding (AAD) can help to determine the identity of the\nattended speaker during an auditory selective attention task, by analyzing and\nprocessing measurements of electroencephalography (EEG) data. Most studies on\nAAD are based on scalp-EEG signals in two-speaker scenarios, which are far from\nreal application. Ear-EEG has recently gained significant attention due to its\nmotion tolerance and invisibility during data acquisition, making it easy to\nincorporate with other devices for applications. In this work, participants\nselectively attended to one of the four spatially separated speakers' speech in\nan anechoic room. The EEG data were concurrently collected from a scalp-EEG\nsystem and an ear-EEG system (cEEGrids). Temporal response functions (TRFs) and\nstimulus reconstruction (SR) were utilized using ear-EEG data. Results showed\nthat the attended speech TRFs were stronger than each unattended speech and\ndecoding accuracy was 41.3\\% in the 60s (chance level of 25\\%). To further\ninvestigate the impact of electrode placement and quantity, SR was utilized in\nboth scalp-EEG and ear-EEG, revealing that while the number of electrodes had a\nminor effect, their positioning had a significant influence on the decoding\naccuracy. One kind of auditory spatial attention detection (ASAD) method,\nSTAnet, was testified with this ear-EEG database, resulting in 93.1% in\n1-second decoding window. The implementation code and database for our work are\navailable on GitHub: https://github.com/zhl486/Ear_EEG_code.git and Zenodo:\nhttps://zenodo.org/records/10803261.", "published": "2024-09-13 10:58:11", "link": "http://arxiv.org/abs/2409.08710v1", "categories": ["eess.SP", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
{"title": "Energy Consumption Trends in Sound Event Detection Systems", "abstract": "Deep learning systems have become increasingly energy- and\ncomputation-intensive, raising concerns about their environmental impact. As\norganizers of the Detection and Classification of Acoustic Scenes and Events\n(DCASE) challenge, we recognize the importance of addressing this issue. For\nthe past three years, we have integrated energy consumption metrics into the\nevaluation of sound event detection (SED) systems. In this paper, we analyze\nthe impact of this energy criterion on the challenge results and explore the\nevolution of system complexity and energy consumption over the years. We\nhighlight a shift towards more energy-efficient approaches during training\nwithout compromising performance, while the number of operations and system\ncomplexity continue to grow. Through this analysis, we hope to promote more\nenvironmentally friendly practices within the SED community.", "published": "2024-09-13 12:11:42", "link": "http://arxiv.org/abs/2409.08763v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Why some audio signal short-time Fourier transform coefficients have\n  nonuniform phase distributions", "abstract": "The short-time Fourier transform (STFT) represents a window of audio samples\nas a set of complex coefficients. These are advantageously viewed as magnitudes\nand phases and the overall distribution of phases is very often assumed to be\nuniform. We show that when audio signal STFT phase distributions are analyzed\nper-frequency or per-magnitude range, they can be far from uniform. That is,\nthe uniform phase distribution assumption obscures significant important\ndetails. We explain the significance of the nonuniform phase distributions and\nhow they might be exploited, derive their source, and explain why the choice of\nthe STFT window shape influences the nonuniformity of the resulting phase\ndistributions.", "published": "2024-09-13 16:54:59", "link": "http://arxiv.org/abs/2409.08981v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Clean Label Attacks against SLU Systems", "abstract": "Poisoning backdoor attacks involve an adversary manipulating the training\ndata to induce certain behaviors in the victim model by inserting a trigger in\nthe signal at inference time. We adapted clean label backdoor (CLBD)-data\npoisoning attacks, which do not modify the training labels, on state-of-the-art\nspeech recognition models that support/perform a Spoken Language Understanding\ntask, achieving 99.8% attack success rate by poisoning 10% of the training\ndata. We analyzed how varying the signal-strength of the poison, percent of\nsamples poisoned, and choice of trigger impact the attack. We also found that\nCLBD attacks are most successful when applied to training samples that are\ninherently hard for a proxy model. Using this strategy, we achieved an attack\nsuccess rate of 99.3% by poisoning a meager 1.5% of the training data. Finally,\nwe applied two previously developed defenses against gradient-based attacks,\nand found that they attain mixed success against poisoning.", "published": "2024-09-13 16:58:06", "link": "http://arxiv.org/abs/2409.08985v1", "categories": ["cs.CR", "cs.LG", "eess.AS"], "primary_category": "cs.CR"}
{"title": "Biomimetic Frontend for Differentiable Audio Processing", "abstract": "While models in audio and speech processing are becoming deeper and more\nend-to-end, they as a consequence need expensive training on large data, and\nare often brittle. We build on a classical model of human hearing and make it\ndifferentiable, so that we can combine traditional explainable biomimetic\nsignal processing approaches with deep-learning frameworks. This allows us to\narrive at an expressive and explainable model that is easily trained on modest\namounts of data. We apply this model to audio processing tasks, including\nclassification and enhancement. Results show that our differentiable model\nsurpasses black-box approaches in terms of computational efficiency and\nrobustness, even with little training data. We also discuss other potential\napplications.", "published": "2024-09-13 17:23:42", "link": "http://arxiv.org/abs/2409.08997v1", "categories": ["cs.SD", "cs.LG", "cs.NE", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Towards Leveraging Contrastively Pretrained Neural Audio Embeddings for\n  Recommender Tasks", "abstract": "Music recommender systems frequently utilize network-based models to capture\nrelationships between music pieces, artists, and users. Although these\nrelationships provide valuable insights for predictions, new music pieces or\nartists often face the cold-start problem due to insufficient initial\ninformation. To address this, one can extract content-based information\ndirectly from the music to enhance collaborative-filtering-based methods. While\nprevious approaches have relied on hand-crafted audio features for this\npurpose, we explore the use of contrastively pretrained neural audio embedding\nmodels, which offer a richer and more nuanced representation of music. Our\nexperiments demonstrate that neural embeddings, particularly those generated\nwith the Contrastive Language-Audio Pretraining (CLAP) model, present a\npromising approach to enhancing music recommendation tasks within graph-based\nframeworks.", "published": "2024-09-13 17:53:06", "link": "http://arxiv.org/abs/2409.09026v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multiview Canonical Correlation Analysis for Automatic Pathological\n  Speech Detection", "abstract": "Recently proposed automatic pathological speech detection approaches rely on\nspectrogram input representations or wav2vec2 embeddings. These representations\nmay contain pathology irrelevant uncorrelated information, such as changing\nphonetic content or variations in speaking style across time, which can\nadversely affect classification performance. To address this issue, we propose\nto use Multiview Canonical Correlation Analysis (MCCA) on these input\nrepresentations prior to automatic pathological speech detection. Our results\ndemonstrate that unlike other dimensionality reduction techniques, the use of\nMCCA leads to a considerable improvement in pathological speech detection\nperformance by eliminating uncorrelated information present in the input\nrepresentations. Employing MCCA with traditional classifiers yields a\ncomparable or higher performance than using sophisticated architectures, while\npreserving the representation structure and providing interpretability.", "published": "2024-09-13 08:04:48", "link": "http://arxiv.org/abs/2409.17276v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "LMAC-TD: Producing Time Domain Explanations for Audio Classifiers", "abstract": "Neural networks are typically black-boxes that remain opaque with regards to\ntheir decision mechanisms. Several works in the literature have proposed\npost-hoc explanation methods to alleviate this issue. This paper proposes\nLMAC-TD, a post-hoc explanation method that trains a decoder to produce\nexplanations directly in the time domain. This methodology builds upon the\nfoundation of L-MAC, Listenable Maps for Audio Classifiers, a method that\nproduces faithful and listenable explanations. We incorporate SepFormer, a\npopular transformer-based time-domain source separation architecture. We show\nthrough a user study that LMAC-TD significantly improves the audio quality of\nthe produced explanations while not sacrificing from faithfulness.", "published": "2024-09-13 09:14:06", "link": "http://arxiv.org/abs/2409.08655v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
