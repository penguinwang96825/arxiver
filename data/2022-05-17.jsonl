{"title": "When to Use Multi-Task Learning vs Intermediate Fine-Tuning for\n  Pre-Trained Encoder Transfer Learning", "abstract": "Transfer learning (TL) in natural language processing (NLP) has seen a surge\nof interest in recent years, as pre-trained models have shown an impressive\nability to transfer to novel tasks. Three main strategies have emerged for\nmaking use of multiple supervised datasets during fine-tuning: training on an\nintermediate task before training on the target task (STILTs), using multi-task\nlearning (MTL) to train jointly on a supplementary task and the target task\n(pairwise MTL), or simply using MTL to train jointly on all available datasets\n(MTL-ALL). In this work, we compare all three TL methods in a comprehensive\nanalysis on the GLUE dataset suite. We find that there is a simple heuristic\nfor when to use one of these techniques over the other: pairwise MTL is better\nthan STILTs when the target task has fewer instances than the supporting task\nand vice versa. We show that this holds true in more than 92% of applicable\ncases on the GLUE dataset and validate this hypothesis with experiments varying\ndataset size. The simplicity and effectiveness of this heuristic is surprising\nand warrants additional exploration by the TL community. Furthermore, we find\nthat MTL-ALL is worse than the pairwise methods in almost every case. We hope\nthis study will aid others as they choose between TL methods for NLP tasks.", "published": "2022-05-17 06:48:45", "link": "http://arxiv.org/abs/2205.08124v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Unsupervised Sentence Compression by Fine-tuning Transformers\n  with Reinforcement Learning", "abstract": "Sentence compression reduces the length of text by removing non-essential\ncontent while preserving important facts and grammaticality. Unsupervised\nobjective driven methods for sentence compression can be used to create\ncustomized models without the need for ground-truth training data, while\nallowing flexibility in the objective function(s) that are used for learning\nand inference. Recent unsupervised sentence compression approaches use custom\nobjectives to guide discrete search; however, guided search is expensive at\ninference time. In this work, we explore the use of reinforcement learning to\ntrain effective sentence compression models that are also fast when generating\npredictions. In particular, we cast the task as binary sequence labelling and\nfine-tune a pre-trained transformer using a simple policy gradient approach.\nOur approach outperforms other unsupervised models while also being more\nefficient at inference time.", "published": "2022-05-17 10:34:28", "link": "http://arxiv.org/abs/2205.08221v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Letters From the Past: Modeling Historical Sound Change Through\n  Diachronic Character Embeddings", "abstract": "While a great deal of work has been done on NLP approaches to lexical\nsemantic change detection, other aspects of language change have received less\nattention from the NLP community. In this paper, we address the detection of\nsound change through historical spelling. We propose that a sound change can be\ncaptured by comparing the relative distance through time between their\ndistributions using PPMI character embeddings. We verify this hypothesis in\nsynthetic data and then test the method's ability to trace the well-known\nhistorical change of lenition of plosives in Danish historical sources. We show\nthat the models are able to identify several of the changes under consideration\nand to uncover meaningful contexts in which they appeared. The methodology has\nthe potential to contribute to the study of open questions such as the relative\nchronology of sound shifts and their geographical distribution.", "published": "2022-05-17 11:57:17", "link": "http://arxiv.org/abs/2205.08256v1", "categories": ["cs.CL", "68T50 (Primary)", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Feature Aggregation in Zero-Shot Cross-Lingual Transfer Using\n  Multilingual BERT", "abstract": "Multilingual BERT (mBERT), a language model pre-trained on large multilingual\ncorpora, has impressive zero-shot cross-lingual transfer capabilities and\nperforms surprisingly well on zero-shot POS tagging and Named Entity\nRecognition (NER), as well as on cross-lingual model transfer. At present, the\nmainstream methods to solve the cross-lingual downstream tasks are always using\nthe last transformer layer's output of mBERT as the representation of\nlinguistic information. In this work, we explore the complementary property of\nlower layers to the last transformer layer of mBERT. A feature aggregation\nmodule based on an attention mechanism is proposed to fuse the information\ncontained in different layers of mBERT. The experiments are conducted on four\nzero-shot cross-lingual transfer datasets, and the proposed method obtains\nperformance improvements on key multilingual benchmark tasks XNLI (+1.5 %),\nPAWS-X (+2.4 %), NER (+1.2 F1), and POS (+1.5 F1). Through the analysis of the\nexperimental results, we prove that the layers before the last layer of mBERT\ncan provide extra useful information for cross-lingual downstream tasks and\nexplore the interpretability of mBERT empirically.", "published": "2022-05-17 17:12:19", "link": "http://arxiv.org/abs/2205.08497v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Consistent Human Evaluation of Machine Translation across Language Pairs", "abstract": "Obtaining meaningful quality scores for machine translation systems through\nhuman evaluation remains a challenge given the high variability between human\nevaluators, partly due to subjective expectations for translation quality for\ndifferent language pairs. We propose a new metric called XSTS that is more\nfocused on semantic equivalence and a cross-lingual calibration method that\nenables more consistent assessment. We demonstrate the effectiveness of these\nnovel contributions in large scale evaluation studies across up to 14 language\npairs, with translation both into and out of English.", "published": "2022-05-17 17:57:06", "link": "http://arxiv.org/abs/2205.08533v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unbiased Math Word Problems Benchmark for Mitigating Solving Bias", "abstract": "In this paper, we revisit the solving bias when evaluating models on current\nMath Word Problem (MWP) benchmarks. However, current solvers exist solving bias\nwhich consists of data bias and learning bias due to biased dataset and\nimproper training strategy. Our experiments verify MWP solvers are easy to be\nbiased by the biased training datasets which do not cover diverse questions for\neach problem narrative of all MWPs, thus a solver can only learn shallow\nheuristics rather than deep semantics for understanding problems. Besides, an\nMWP can be naturally solved by multiple equivalent equations while current\ndatasets take only one of the equivalent equations as ground truth, forcing the\nmodel to match the labeled ground truth and ignoring other equivalent\nequations. Here, we first introduce a novel MWP dataset named UnbiasedMWP which\nis constructed by varying the grounded expressions in our collected data and\nannotating them with corresponding multiple new questions manually. Then, to\nfurther mitigate learning bias, we propose a Dynamic Target Selection (DTS)\nStrategy to dynamically select more suitable target expressions according to\nthe longest prefix match between the current model output and candidate\nequivalent equations which are obtained by applying commutative law during\ntraining. The results show that our UnbiasedMWP has significantly fewer biases\nthan its original data and other datasets, posing a promising benchmark for\nfairly evaluating the solvers' reasoning skills rather than matching nearest\nneighbors. And the solvers trained with our DTS achieve higher accuracies on\nmultiple MWP benchmarks. The source code is available at\nhttps://github.com/yangzhch6/UnbiasedMWP.", "published": "2022-05-17 06:07:04", "link": "http://arxiv.org/abs/2205.08108v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "SEMI-FND: Stacked Ensemble Based Multimodal Inference For Faster Fake\n  News Detection", "abstract": "Fake News Detection (FND) is an essential field in natural language\nprocessing that aims to identify and check the truthfulness of major claims in\na news article to decide the news veracity. FND finds its uses in preventing\nsocial, political and national damage caused due to misrepresentation of facts\nwhich may harm a certain section of society. Further, with the explosive rise\nin fake news dissemination over social media, including images and text, it has\nbecome imperative to identify fake news faster and more accurately. To solve\nthis problem, this work investigates a novel multimodal stacked ensemble-based\napproach (SEMIFND) to fake news detection. Focus is also kept on ensuring\nfaster performance with fewer parameters. Moreover, to improve multimodal\nperformance, a deep unimodal analysis is done on the image modality to identify\nNasNet Mobile as the most appropriate model for the task. For text, an ensemble\nof BERT and ELECTRA is used. The approach was evaluated on two datasets:\nTwitter MediaEval and Weibo Corpus. The suggested framework offered accuracies\nof 85.80% and 86.83% on the Twitter and Weibo datasets respectively. These\nreported metrics are found to be superior when compared to similar recent\nworks. Further, we also report a reduction in the number of parameters used in\ntraining when compared to recent relevant works. SEMI-FND offers an overall\nparameter reduction of at least 20% with unimodal parametric reduction on text\nbeing 60%. Therefore, based on the investigations presented, it is concluded\nthat the application of a stacked ensembling significantly improves FND over\nother approaches while also improving speed.", "published": "2022-05-17 07:51:55", "link": "http://arxiv.org/abs/2205.08159v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LogicSolver: Towards Interpretable Math Word Problem Solving with\n  Logical Prompt-enhanced Learning", "abstract": "Recently, deep learning models have made great progress in MWP solving on\nanswer accuracy. However, they are uninterpretable since they mainly rely on\nshallow heuristics to achieve high performance without understanding and\nreasoning the grounded math logic. To address this issue and make a step\ntowards interpretable MWP solving, we first construct a high-quality MWP\ndataset named InterMWP which consists of 11,495 MWPs and annotates\ninterpretable logical formulas based on algebraic knowledge as the grounded\nlinguistic logic of each solution equation. Different from existing MWP\ndatasets, our InterMWP benchmark asks for a solver to not only output the\nsolution expressions but also predict the corresponding logical formulas. We\nfurther propose a novel approach with logical prompt and interpretation\ngeneration, called LogicSolver. For each MWP, our LogicSolver first retrieves\nsome highly-correlated algebraic knowledge and then passes them to the backbone\nmodel as prompts to improve the semantic representations of MWPs. With these\nimproved semantic representations, our LogicSolver generates corresponding\nsolution expressions and interpretable knowledge formulas in accord with the\ngenerated solution expressions, simultaneously. Experimental results show that\nour LogicSolver has stronger logical formula-based interpretability than\nbaselines while achieving higher answer accuracy with the help of logical\nprompts, simultaneously. The source code and dataset is available at\nhttps://github.com/yangzhch6/InterMWP.", "published": "2022-05-17 11:01:52", "link": "http://arxiv.org/abs/2205.08232v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Tackling Math Word Problems with Fine-to-Coarse Abstracting and\n  Reasoning", "abstract": "Math Word Problems (MWP) is an important task that requires the ability of\nunderstanding and reasoning over mathematical text. Existing approaches mostly\nformalize it as a generation task by adopting Seq2Seq or Seq2Tree models to\nencode an input math problem in natural language as a global representation and\ngenerate the output mathematical expression. Such approaches only learn shallow\nheuristics and fail to capture fine-grained variations in inputs. In this\npaper, we propose to model a math word problem in a fine-to-coarse manner to\ncapture both the local fine-grained information and the global logical\nstructure of it. Instead of generating a complete equation sequence or\nexpression tree from the global features, we iteratively combine low-level\noperands to predict a higher-level operator, abstracting the problem and\nreasoning about the solving operators from bottom to up. Our model is naturally\nmore sensitive to local variations and can better generalize to unseen problem\ntypes. Extensive evaluations on Math23k and SVAMP datasets demonstrate the\naccuracy and robustness of our method.", "published": "2022-05-17 12:14:44", "link": "http://arxiv.org/abs/2205.08274v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Geographical Distance Is The New Hyperparameter: A Case Study Of Finding\n  The Optimal Pre-trained Language For English-isiZulu Machine Translation", "abstract": "Stemming from the limited availability of datasets and textual resources for\nlow-resource languages such as isiZulu, there is a significant need to be able\nto harness knowledge from pre-trained models to improve low resource machine\ntranslation. Moreover, a lack of techniques to handle the complexities of\nmorphologically rich languages has compounded the unequal development of\ntranslation models, with many widely spoken African languages being left\nbehind. This study explores the potential benefits of transfer learning in an\nEnglish-isiZulu translation framework. The results indicate the value of\ntransfer learning from closely related languages to enhance the performance of\nlow-resource translation models, thus providing a key strategy for low-resource\ntranslation going forward. We gathered results from 8 different language\ncorpora, including one multi-lingual corpus, and saw that isiXhosa-isiZulu\noutperformed all languages, with a BLEU score of 8.56 on the test set which was\nbetter from the multi-lingual corpora pre-trained model by 2.73. We also\nderived a new coefficient, Nasir's Geographical Distance Coefficient (NGDC)\nwhich provides an easy selection of languages for the pre-trained models. NGDC\nalso indicated that isiXhosa should be selected as the language for the\npre-trained model.", "published": "2022-05-17 20:41:25", "link": "http://arxiv.org/abs/2205.08621v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ViralBERT: A User Focused BERT-Based Approach to Virality Prediction", "abstract": "Recently, Twitter has become the social network of choice for sharing and\nspreading information to a multitude of users through posts called 'tweets'.\nUsers can easily re-share these posts to other users through 'retweets', which\nallow information to cascade to many more users, increasing its outreach.\nClearly, being able to know the extent to which a post can be retweeted has\ngreat value in advertising, influencing and other such campaigns. In this paper\nwe propose ViralBERT, which can be used to predict the virality of tweets using\ncontent- and user-based features. We employ a method of concatenating numerical\nfeatures such as hashtags and follower numbers to tweet text, and utilise two\nBERT modules: one for semantic representation of the combined text and\nnumerical features, and another module purely for sentiment analysis of text,\nas both the information within text and it's ability to elicit an emotional\nresponse play a part in retweet proneness. We collect a dataset of 330k tweets\nto train ViralBERT and validate the efficacy of our model using baselines from\ncurrent studies in this field. Our experiments show that our approach\noutperforms these baselines, with a 13% increase in both F1 Score and Accuracy\ncompared to the best performing baseline method. We then undergo an ablation\nstudy to investigate the importance of chosen features, finding that text\nsentiment and follower counts, and to a lesser extent mentions and following\ncounts, are the strongest features for the model, and that hashtag counts are\ndetrimental to the model.", "published": "2022-05-17 21:40:24", "link": "http://arxiv.org/abs/2206.10298v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "\"What makes a question inquisitive?\" A Study on Type-Controlled\n  Inquisitive Question Generation", "abstract": "We propose a type-controlled framework for inquisitive question generation.\nWe annotate an inquisitive question dataset with question types, train question\ntype classifiers, and finetune models for type-controlled question generation.\nEmpirical results demonstrate that we can generate a variety of questions that\nadhere to specific types while drawing from the source texts. We also\ninvestigate strategies for selecting a single question from a generated set,\nconsidering both an informative vs.~inquisitive question classifier and a\npairwise ranker trained from a small set of expert annotations. Question\nselection using the pairwise ranker yields strong results in automatic and\nmanual evaluation. Our human evaluation assesses multiple aspects of the\ngenerated questions, finding that the ranker chooses questions with the best\nsyntax (4.59), semantics (4.37), and inquisitiveness (3.92) on a scale of 1-5,\neven rivaling the performance of human-written questions.", "published": "2022-05-17 02:05:50", "link": "http://arxiv.org/abs/2205.08056v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SAMU-XLSR: Semantically-Aligned Multimodal Utterance-level Cross-Lingual\n  Speech Representation", "abstract": "We propose the SAMU-XLSR: Semantically-Aligned Multimodal Utterance-level\nCross-Lingual Speech Representation learning framework. Unlike previous works\non speech representation learning, which learns multilingual contextual speech\nembedding at the resolution of an acoustic frame (10-20ms), this work focuses\non learning multimodal (speech-text) multilingual speech embedding at the\nresolution of a sentence (5-10s) such that the embedding vector space is\nsemantically aligned across different languages. We combine state-of-the-art\nmultilingual acoustic frame-level speech representation learning model XLS-R\nwith the Language Agnostic BERT Sentence Embedding (LaBSE) model to create an\nutterance-level multimodal multilingual speech encoder SAMU-XLSR. Although we\ntrain SAMU-XLSR with only multilingual transcribed speech data, cross-lingual\nspeech-text and speech-speech associations emerge in its learned representation\nspace. To substantiate our claims, we use SAMU-XLSR speech encoder in\ncombination with a pre-trained LaBSE text sentence encoder for cross-lingual\nspeech-to-text translation retrieval, and SAMU-XLSR alone for cross-lingual\nspeech-to-speech translation retrieval. We highlight these applications by\nperforming several cross-lingual text and speech translation retrieval tasks\nacross several datasets.", "published": "2022-05-17 08:58:48", "link": "http://arxiv.org/abs/2205.08180v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "SKILL: Structured Knowledge Infusion for Large Language Models", "abstract": "Large language models (LLMs) have demonstrated human-level performance on a\nvast spectrum of natural language tasks. However, it is largely unexplored\nwhether they can better internalize knowledge from a structured data, such as a\nknowledge graph, or from text. In this work, we propose a method to infuse\nstructured knowledge into LLMs, by directly training T5 models on factual\ntriples of knowledge graphs (KGs). We show that models pre-trained on Wikidata\nKG with our method outperform the T5 baselines on FreebaseQA and WikiHop, as\nwell as the Wikidata-answerable subset of TriviaQA and NaturalQuestions. The\nmodels pre-trained on factual triples compare competitively with the ones on\nnatural language sentences that contain the same knowledge. Trained on a\nsmaller size KG, WikiMovies, we saw 3x improvement of exact match score on\nMetaQA task compared to T5 baseline. The proposed method has an advantage that\nno alignment between the knowledge graph and text corpus is required in\ncurating training data. This makes our method particularly useful when working\nwith industry-scale knowledge graphs.", "published": "2022-05-17 09:12:22", "link": "http://arxiv.org/abs/2205.08184v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Measuring Alignment Bias in Neural Seq2Seq Semantic Parsers", "abstract": "Prior to deep learning the semantic parsing community has been interested in\nunderstanding and modeling the range of possible word alignments between\nnatural language sentences and their corresponding meaning representations.\nSequence-to-sequence models changed the research landscape suggesting that we\nno longer need to worry about alignments since they can be learned\nautomatically by means of an attention mechanism. More recently, researchers\nhave started to question such premise. In this work we investigate whether\nseq2seq models can handle both simple and complex alignments. To answer this\nquestion we augment the popular Geo semantic parsing dataset with alignment\nannotations and create Geo-Aligned. We then study the performance of standard\nseq2seq models on the examples that can be aligned monotonically versus\nexamples that require more complex alignments. Our empirical study shows that\nperformance is significantly better over monotonic alignments.", "published": "2022-05-17 12:35:52", "link": "http://arxiv.org/abs/2205.08288v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Evaluation Framework for Legal Document Summarization", "abstract": "A law practitioner has to go through numerous lengthy legal case proceedings\nfor their practices of various categories, such as land dispute, corruption,\netc. Hence, it is important to summarize these documents, and ensure that\nsummaries contain phrases with intent matching the category of the case. To the\nbest of our knowledge, there is no evaluation metric that evaluates a summary\nbased on its intent. We propose an automated intent-based summarization metric,\nwhich shows a better agreement with human evaluation as compared to other\nautomated metrics like BLEU, ROUGE-L etc. in terms of human satisfaction. We\nalso curate a dataset by annotating intent phrases in legal documents, and show\na proof of concept as to how this system can be automated. Additionally, all\nthe code and data to generate reproducible results is available on Github.", "published": "2022-05-17 16:42:03", "link": "http://arxiv.org/abs/2205.08478v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Recovering Private Text in Federated Learning of Language Models", "abstract": "Federated learning allows distributed users to collaboratively train a model\nwhile keeping each user's data private. Recently, a growing body of work has\ndemonstrated that an eavesdropping attacker can effectively recover image data\nfrom gradients transmitted during federated learning. However, little progress\nhas been made in recovering text data. In this paper, we present a novel attack\nmethod FILM for federated learning of language models (LMs). For the first\ntime, we show the feasibility of recovering text from large batch sizes of up\nto 128 sentences. Unlike image-recovery methods that are optimized to match\ngradients, we take a distinct approach that first identifies a set of words\nfrom gradients and then directly reconstructs sentences based on beam search\nand a prior-based reordering strategy. We conduct the FILM attack on several\nlarge-scale datasets and show that it can successfully reconstruct single\nsentences with high fidelity for large batch sizes and even multiple sentences\nif applied iteratively. We evaluate three defense methods: gradient pruning,\nDPSGD, and a simple approach to freeze word embeddings that we propose. We show\nthat both gradient pruning and DPSGD lead to a significant drop in utility.\nHowever, if we fine-tune a public pre-trained LM on private text without\nupdating word embeddings, it can effectively defend the attack with minimal\ndata utility loss. Together, we hope that our results can encourage the\ncommunity to rethink the privacy concerns of LM training and its standard\npractices in the future.", "published": "2022-05-17 17:38:37", "link": "http://arxiv.org/abs/2205.08514v2", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Deploying self-supervised learning in the wild for hybrid automatic\n  speech recognition", "abstract": "Self-supervised learning (SSL) methods have proven to be very successful in\nautomatic speech recognition (ASR). These great improvements have been reported\nmostly based on highly curated datasets such as LibriSpeech for non-streaming\nEnd-to-End ASR models. However, the pivotal characteristics of SSL is to be\nutilized for any untranscribed audio data. In this paper, we provide a full\nexploration on how to utilize uncurated audio data in SSL from data\npre-processing to deploying an streaming hybrid ASR model. More specifically,\nwe present (1) the effect of Audio Event Detection (AED) model in data\npre-processing pipeline (2) analysis on choosing optimizer and learning rate\nscheduling (3) comparison of recently developed contrastive losses, (4)\ncomparison of various pre-training strategies such as utilization of in-domain\nversus out-domain pre-training data, monolingual versus multilingual\npre-training data, multi-head multilingual SSL versus single-head multilingual\nSSL and supervised pre-training versus SSL. The experimental results show that\nSSL pre-training with in-domain uncurated data can achieve better performance\nin comparison to all the alternative out-domain pre-training strategies.", "published": "2022-05-17 19:37:40", "link": "http://arxiv.org/abs/2205.08598v1", "categories": ["cs.SD", "cs.CL", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "OneAligner: Zero-shot Cross-lingual Transfer with One Rich-Resource\n  Language Pair for Low-Resource Sentence Retrieval", "abstract": "Aligning parallel sentences in multilingual corpora is essential to curating\ndata for downstream applications such as Machine Translation. In this work, we\npresent OneAligner, an alignment model specially designed for sentence\nretrieval tasks. This model is able to train on only one language pair and\ntransfers, in a cross-lingual fashion, to low-resource language pairs with\nnegligible degradation in performance. When trained with all language pairs of\na large-scale parallel multilingual corpus (OPUS-100), this model achieves the\nstate-of-the-art result on the Tateoba dataset, outperforming an equally-sized\nprevious model by 8.0 points in accuracy while using less than 0.6% of their\nparallel data. When finetuned on a single rich-resource language pair, be it\nEnglish-centered or not, our model is able to match the performance of the ones\nfinetuned on all language pairs under the same data budget with less than 2.0\npoints decrease in accuracy. Furthermore, with the same setup, scaling up the\nnumber of rich-resource language pairs monotonically improves the performance,\nreaching a minimum of 0.4 points discrepancy in accuracy, making it less\nmandatory to collect any low-resource parallel data. Finally, we conclude\nthrough empirical results and analyses that the performance of the sentence\nalignment task depends mostly on the monolingual and parallel data size, up to\na certain size threshold, rather than on what language pairs are used for\ntraining or evaluation.", "published": "2022-05-17 19:52:42", "link": "http://arxiv.org/abs/2205.08605v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Generic and Trend-aware Curriculum Learning for Relation Extraction in\n  Graph Neural Networks", "abstract": "We present a generic and trend-aware curriculum learning approach for graph\nneural networks. It extends existing approaches by incorporating sample-level\nloss trends to better discriminate easier from harder samples and schedule them\nfor training. The model effectively integrates textual and structural\ninformation for relation extraction in text graphs. Experimental results show\nthat the model provides robust estimations of sample difficulty and shows\nsizable improvement over the state-of-the-art approaches across several\ndatasets.", "published": "2022-05-17 20:46:02", "link": "http://arxiv.org/abs/2205.08625v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Federated learning for violence incident prediction in a simulated\n  cross-institutional psychiatric setting", "abstract": "Inpatient violence is a common and severe problem within psychiatry. Knowing\nwho might become violent can influence staffing levels and mitigate severity.\nPredictive machine learning models can assess each patient's likelihood of\nbecoming violent based on clinical notes. Yet, while machine learning models\nbenefit from having more data, data availability is limited as hospitals\ntypically do not share their data for privacy preservation. Federated Learning\n(FL) can overcome the problem of data limitation by training models in a\ndecentralised manner, without disclosing data between collaborators. However,\nalthough several FL approaches exist, none of these train Natural Language\nProcessing models on clinical notes. In this work, we investigate the\napplication of Federated Learning to clinical Natural Language Processing,\napplied to the task of Violence Risk Assessment by simulating a\ncross-institutional psychiatric setting. We train and compare four models: two\nlocal models, a federated model and a data-centralised model. Our results\nindicate that the federated model outperforms the local models and has similar\nperformance as the data-centralised model. These findings suggest that\nFederated Learning can be used successfully in a cross-institutional setting\nand is a step towards new applications of Federated Learning based on clinical\nnotes", "published": "2022-05-17 07:37:12", "link": "http://arxiv.org/abs/2205.10234v1", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Use of NLP-Based Text Representation Techniques to Support\n  Requirement Engineering Tasks: A Systematic Mapping Review", "abstract": "Natural Language Processing (NLP) is widely used to support the automation of\ndifferent Requirements Engineering (RE) tasks. Most of the proposed approaches\nstart with various NLP steps that analyze requirements statements, extract\ntheir linguistic information, and convert them to easy-to-process\nrepresentations, such as lists of features or embedding-based vector\nrepresentations. These NLP-based representations are usually used at a later\nstage as inputs for machine learning techniques or rule-based methods. Thus,\nrequirements representations play a major role in determining the accuracy of\ndifferent approaches. In this paper, we conducted a survey in the form of a\nsystematic literature mapping (classification) to find out (1) what are the\nrepresentations used in RE tasks literature, (2) what is the main focus of\nthese works, (3) what are the main research directions in this domain, and (4)\nwhat are the gaps and potential future directions. After compiling an initial\npool of 2,227 papers, and applying a set of inclusion/exclusion criteria, we\nobtained a final pool containing 104 relevant papers. Our survey shows that the\nresearch direction has changed from the use of lexical and syntactic features\nto the use of advanced embedding techniques, especially in the last two years.\nUsing advanced embedding representations has proved its effectiveness in most\nRE tasks (such as requirement analysis, extracting requirements from reviews\nand forums, and semantic-level quality tasks). However, representations that\nare based on lexical and syntactic features are still more appropriate for\nother RE tasks (such as modeling and syntax-level quality tasks) since they\nprovide the required information for the rules and regular expressions used\nwhen handling these tasks. In addition, we identify four gaps in the existing\nliterature, why they matter, and how future research can begin to address them.", "published": "2022-05-17 02:47:26", "link": "http://arxiv.org/abs/2206.00421v1", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Global Contentious Politics Database (GLOCON) Annotation Manuals", "abstract": "The database creation utilized automated text processing tools that detect if\na news article contains a protest event, locate protest information within the\narticle, and extract pieces of information regarding the detected protest\nevents. The basis of training and testing the automated tools is the GLOCON\nGold Standard Corpus (GSC), which contains news articles from multiple sources\nfrom each focus country. The articles in the GSC were manually coded by skilled\nannotators in both classification and extraction tasks with the utmost accuracy\nand consistency that automated tool development demands. In order to assure\nthese, the annotation manuals in this document lay out the rules according to\nwhich annotators code the news articles. Annotators refer to the manuals at all\ntimes for all annotation tasks and apply the rules that they contain. The\ncontent of the annotation manual is built on the general principles and\nstandards of linguistic annotation laid out in other prominent annotation\nmanuals such as ACE, CAMEO, and TimeML. These principles, however, have been\nadapted or rather modified heavily to accommodate the social scientific\nconcepts and variables employed in the EMW project. The manual has been molded\nthroughout a long trial and error process that accompanied the annotation of\nthe GSC. It owes much of its current shape to the meticulous work and\ninvaluable feedback provided by highly specialized teams of annotators, whose\ndiligence and expertise greatly increased the quality of the corpus.", "published": "2022-05-17 13:16:50", "link": "http://arxiv.org/abs/2206.10299v1", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Composing General Audio Representation by Fusing Multilayer Features of\n  a Pre-trained Model", "abstract": "Many application studies rely on audio DNN models pre-trained on a\nlarge-scale dataset as essential feature extractors, and they extract features\nfrom the last layers. In this study, we focus on our finding that the middle\nlayer features of existing supervised pre-trained models are more effective\nthan the late layer features for some tasks. We propose a simple approach to\ncompose features effective for general-purpose applications, consisting of two\nsteps: (1) calculating feature vectors along the time frame from middle/late\nlayer outputs, and (2) fusing them. This approach improves the utility of\nfrequency and channel information in downstream processes, and combines the\neffectiveness of middle and late layer features for different tasks. As a\nresult, the feature vectors become effective for general purposes. In the\nexperiments using VGGish, PANNs' CNN14, and AST on nine downstream tasks, we\nfirst show that each layer output of these models serves different tasks. Then,\nwe demonstrate that the proposed approach significantly improves their\nperformance and brings it to a level comparable to that of the\nstate-of-the-art. In particular, the performance of the non-semantic speech\n(NOSS) tasks greatly improves, especially on Speech commands V2 with VGGish of\n+77.1 (14.3% to 91.4%).", "published": "2022-05-17 07:10:19", "link": "http://arxiv.org/abs/2205.08138v1", "categories": ["eess.AS", "cs.SD", "68T07"], "primary_category": "eess.AS"}
{"title": "Streaming Noise Context Aware Enhancement For Automatic Speech\n  Recognition in Multi-Talker Environments", "abstract": "One of the most challenging scenarios for smart speakers is multi-talker,\nwhen target speech from the desired speaker is mixed with interfering speech\nfrom one or more speakers. A smart assistant needs to determine which voice to\nrecognize and which to ignore and it needs to do so in a streaming, low-latency\nmanner. This work presents two multi-microphone speech enhancement algorithms\ntargeted at this scenario. Targeting on-device use-cases, we assume that the\nalgorithm has access to the signal before the hotword, which is referred to as\nthe noise context. First is the Context Aware Beamformer which uses the noise\ncontext and detected hotword to determine how to target the desired speaker.\nThe second is an adaptive noise cancellation algorithm called Speech Cleaner\nwhich trains a filter using the noise context. It is demonstrated that the two\nalgorithms are complementary in the signal-to-noise ratio conditions under\nwhich they work well. We also propose an algorithm to select which one to use\nbased on estimated SNR. When using 3 microphone channels, the final system\nachieves a relative word error rate reduction of 55% at -12dB, and 43\\% at\n12dB.", "published": "2022-05-17 18:00:41", "link": "http://arxiv.org/abs/2205.08555v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Utterance Weighted Multi-Dilation Temporal Convolutional Networks for\n  Monaural Speech Dereverberation", "abstract": "Speech dereverberation is an important stage in many speech technology\napplications. Recent work in this area has been dominated by deep neural\nnetwork models. Temporal convolutional networks (TCNs) are deep learning models\nthat have been proposed for sequence modelling in the task of dereverberating\nspeech. In this work a weighted multi-dilation depthwise-separable convolution\nis proposed to replace standard depthwise-separable convolutions in TCN models.\nThis proposed convolution enables the TCN to dynamically focus on more or less\nlocal information in its receptive field at each convolutional block in the\nnetwork. It is shown that this weighted multi-dilation temporal convolutional\nnetwork (WD-TCN) consistently outperforms the TCN across various model\nconfigurations and using the WD-TCN model is a more parameter efficient method\nto improve the performance of the model than increasing the number of\nconvolutional blocks. The best performance improvement over the baseline TCN is\n0.55 dB scale-invariant signal-to-distortion ratio (SISDR) and the best\nperforming WD-TCN model attains 12.26 dB SISDR on the WHAMR dataset.", "published": "2022-05-17 15:56:31", "link": "http://arxiv.org/abs/2205.08455v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Dynamic Recognition of Speakers for Consent Management by Contrastive\n  Embedding Replay", "abstract": "Voice assistants overhear conversations and a consent management mechanism is\nrequired. Consent management can be implemented using speaker recognition.\nUsers that do not give consent enrol their voice and all their further\nrecordings are discarded. Building speaker recognition-based consent management\nis challenging as dynamic registration, removal, and re-registration of\nspeakers must be efficiently handled. This work proposes a consent management\nsystem addressing the aforementioned challenges. A contrastive based training\nis applied to learn the underlying speaker equivariance inductive bias. The\ncontrastive features for buckets of speakers are trained a few steps into each\niteration and act as replay buffers. These features are progressively selected\nusing a multi-strided random sampler for classification. Moreover, new methods\nfor dynamic registration using a portion of old utterances, removal, and\nre-registration of speakers are proposed. The results verify memory efficiency\nand dynamic capabilities of the proposed methods and outperform the existing\napproach from the literature.", "published": "2022-05-17 16:00:17", "link": "http://arxiv.org/abs/2205.08459v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The Power of Fragmentation: A Hierarchical Transformer Model for\n  Structural Segmentation in Symbolic Music Generation", "abstract": "Symbolic Music Generation relies on the contextual representation\ncapabilities of the generative model, where the most prevalent approach is the\nTransformer-based model. The learning of musical context is also related to the\nstructural elements in music, i.e. intro, verse, and chorus, which are\ncurrently overlooked by the research community. In this paper, we propose a\nhierarchical Transformer model to learn multi-scale contexts in music. In the\nencoding phase, we first designed a Fragment Scope Localization layer to\nsyncopate the music into chords and sections. Then, we use a multi-scale\nattention mechanism to learn note-, chord-, and section-level contexts. In the\ndecoding phase, we proposed a hierarchical Transformer model that uses\nfine-decoders to generate sections in parallel and a coarse-decoder to decode\nthe combined music. We also designed a Music Style Normalization layer to\nachieve a consistent music style between the generated sections. Our model is\nevaluated on two open MIDI datasets, and experiments show that our model\noutperforms the best contemporary music generative models. More excitingly, the\nvisual evaluation shows that our model is superior in melody reuse, resulting\nin more realistic music.", "published": "2022-05-17 18:48:14", "link": "http://arxiv.org/abs/2205.08579v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
