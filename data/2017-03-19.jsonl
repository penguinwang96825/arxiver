{"title": "M\u00e9todos de Otimiza\u00e7\u00e3o Combinat\u00f3ria Aplicados ao Problema de\n  Compress\u00e3o MultiFrases", "abstract": "The Internet has led to a dramatic increase in the amount of available\ninformation. In this context, reading and understanding this flow of\ninformation have become costly tasks. In the last years, to assist people to\nunderstand textual data, various Natural Language Processing (NLP) applications\nbased on Combinatorial Optimization have been devised. However, for\nMulti-Sentences Compression (MSC), method which reduces the sentence length\nwithout removing core information, the insertion of optimization methods\nrequires further study to improve the performance of MSC. This article\ndescribes a method for MSC using Combinatorial Optimization and Graph Theory to\ngenerate more informative sentences while maintaining their grammaticality. An\nexperiment led on a corpus of 40 clusters of sentences shows that our system\nhas achieved a very good quality and is better than the state-of-the-art.", "published": "2017-03-19 19:56:25", "link": "http://arxiv.org/abs/1703.06501v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Native Language Identification using Stacked Generalization", "abstract": "Ensemble methods using multiple classifiers have proven to be the most\nsuccessful approach for the task of Native Language Identification (NLI),\nachieving the current state of the art. However, a systematic examination of\nensemble methods for NLI has yet to be conducted. Additionally, deeper ensemble\narchitectures such as classifier stacking have not been closely evaluated. We\npresent a set of experiments using three ensemble-based models, testing each\nwith multiple configurations and algorithms. This includes a rigorous\napplication of meta-classification models for NLI, achieving state-of-the-art\nresults on three datasets from different languages. We also present the first\nuse of statistical significance testing for comparing NLI systems, showing that\nour results are significantly better than the previous state of the art. We\nmake available a collection of test set predictions to facilitate future\nstatistical tests.", "published": "2017-03-19 23:42:28", "link": "http://arxiv.org/abs/1703.06541v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "VQABQ: Visual Question Answering by Basic Questions", "abstract": "Taking an image and question as the input of our method, it can output the\ntext-based answer of the query question about the given image, so called Visual\nQuestion Answering (VQA). There are two main modules in our algorithm. Given a\nnatural language question about an image, the first module takes the question\nas input and then outputs the basic questions of the main given question. The\nsecond module takes the main question, image and these basic questions as input\nand then outputs the text-based answer of the main question. We formulate the\nbasic questions generation problem as a LASSO optimization problem, and also\npropose a criterion about how to exploit these basic questions to help answer\nmain question. Our method is evaluated on the challenging VQA dataset and\nyields state-of-the-art accuracy, 60.34% in open-ended task.", "published": "2017-03-19 19:14:55", "link": "http://arxiv.org/abs/1703.06492v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
