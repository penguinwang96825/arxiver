{"title": "Controlling for Stereotypes in Multimodal Language Model Evaluation", "abstract": "We propose a methodology and design two benchmark sets for measuring to what\nextent language-and-vision language models use the visual signal in the\npresence or absence of stereotypes. The first benchmark is designed to test for\nstereotypical colors of common objects, while the second benchmark considers\ngender stereotypes. The key idea is to compare predictions when the image\nconforms to the stereotype to predictions when it does not.\n  Our results show that there is significant variation among multimodal models:\nthe recent Transformer-based FLAVA seems to be more sensitive to the choice of\nimage and less affected by stereotypes than older CNN-based models such as\nVisualBERT and LXMERT. This effect is more discernible in this type of\ncontrolled setting than in traditional evaluations where we do not know whether\nthe model relied on the stereotype or the visual signal.", "published": "2023-02-03 07:27:50", "link": "http://arxiv.org/abs/2302.01582v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bioformer: an efficient transformer language model for biomedical text\n  mining", "abstract": "Pretrained language models such as Bidirectional Encoder Representations from\nTransformers (BERT) have achieved state-of-the-art performance in natural\nlanguage processing (NLP) tasks. Recently, BERT has been adapted to the\nbiomedical domain. Despite the effectiveness, these models have hundreds of\nmillions of parameters and are computationally expensive when applied to\nlarge-scale NLP applications. We hypothesized that the number of parameters of\nthe original BERT can be dramatically reduced with minor impact on performance.\nIn this study, we present Bioformer, a compact BERT model for biomedical text\nmining. We pretrained two Bioformer models (named Bioformer8L and Bioformer16L)\nwhich reduced the model size by 60% compared to BERTBase. Bioformer uses a\nbiomedical vocabulary and was pre-trained from scratch on PubMed abstracts and\nPubMed Central full-text articles. We thoroughly evaluated the performance of\nBioformer as well as existing biomedical BERT models including BioBERT and\nPubMedBERT on 15 benchmark datasets of four different biomedical NLP tasks:\nnamed entity recognition, relation extraction, question answering and document\nclassification. The results show that with 60% fewer parameters, Bioformer16L\nis only 0.1% less accurate than PubMedBERT while Bioformer8L is 0.9% less\naccurate than PubMedBERT. Both Bioformer16L and Bioformer8L outperformed\nBioBERTBase-v1.1. In addition, Bioformer16L and Bioformer8L are 2-3 fold as\nfast as PubMedBERT/BioBERTBase-v1.1. Bioformer has been successfully deployed\nto PubTator Central providing gene annotations over 35 million PubMed abstracts\nand 5 million PubMed Central full-text articles. We make Bioformer publicly\navailable via https://github.com/WGLab/bioformer, including pre-trained models,\ndatasets, and instructions for downstream use.", "published": "2023-02-03 08:04:59", "link": "http://arxiv.org/abs/2302.01588v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Around the world in 60 words: A generative vocabulary test for online\n  research", "abstract": "Conducting experiments with diverse participants in their native languages\ncan uncover insights into culture, cognition, and language that may not be\nrevealed otherwise. However, conducting these experiments online makes it\ndifficult to validate self-reported language proficiency. Furthermore, existing\nproficiency tests are small and cover only a few languages. We present an\nautomated pipeline to generate vocabulary tests using text from Wikipedia. Our\npipeline samples rare nouns and creates pseudowords with the same low-level\nstatistics. Six behavioral experiments (N=236) in six countries and eight\nlanguages show that (a) our test can distinguish between native speakers of\nclosely related languages, (b) the test is reliable ($r=0.82$), and (c)\nperformance strongly correlates with existing tests (LexTale) and self-reports.\nWe further show that test accuracy is negatively correlated with the linguistic\ndistance between the tested and the native language. Our test, available in\neight languages, can easily be extended to other languages.", "published": "2023-02-03 09:27:12", "link": "http://arxiv.org/abs/2302.01614v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LIQUID: A Framework for List Question Answering Dataset Generation", "abstract": "Question answering (QA) models often rely on large-scale training datasets,\nwhich necessitates the development of a data generation framework to reduce the\ncost of manual annotations. Although several recent studies have aimed to\ngenerate synthetic questions with single-span answers, no study has been\nconducted on the creation of list questions with multiple, non-contiguous spans\nas answers. To address this gap, we propose LIQUID, an automated framework for\ngenerating list QA datasets from unlabeled corpora. We first convert a passage\nfrom Wikipedia or PubMed into a summary and extract named entities from the\nsummarized text as candidate answers. This allows us to select answers that are\nsemantically correlated in context and is, therefore, suitable for constructing\nlist questions. We then create questions using an off-the-shelf question\ngenerator with the extracted entities and original passage. Finally, iterative\nfiltering and answer expansion are performed to ensure the accuracy and\ncompleteness of the answers. Using our synthetic data, we significantly improve\nthe performance of the previous best list QA models by exact-match F1 scores of\n5.0 on MultiSpanQA, 1.9 on Quoref, and 2.8 averaged across three BioASQ\nbenchmarks.", "published": "2023-02-03 12:42:45", "link": "http://arxiv.org/abs/2302.01691v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lexical Simplification using multi level and modular approach", "abstract": "Text Simplification is an ongoing problem in Natural Language Processing,\nsolution to which has varied implications. In conjunction with the TSAR-2022\nWorkshop @EMNLP2022 Lexical Simplification is the process of reducing the\nlexical complexity of a text by replacing difficult words with easier to read\n(or understand) expressions while preserving the original information and\nmeaning. This paper explains the work done by our team \"teamPN\" for English sub\ntask. We created a modular pipeline which combines modern day transformers\nbased models with traditional NLP methods like paraphrasing and verb sense\ndisambiguation. We created a multi level and modular pipeline where the target\ntext is treated according to its semantics(Part of Speech Tag). Pipeline is\nmulti level as we utilize multiple source models to find potential candidates\nfor replacement, It is modular as we can switch the source models and their\nweight-age in the final re-ranking.", "published": "2023-02-03 15:57:54", "link": "http://arxiv.org/abs/2302.01823v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating Stylistic Profiles for the Task of Empathy Classification\n  in Medical Narrative Essays", "abstract": "One important aspect of language is how speakers generate utterances and\ntexts to convey their intended meanings. In this paper, we bring various\naspects of the Construction Grammar (CxG) and the Systemic Functional Grammar\n(SFG) theories in a deep learning computational framework to model empathic\nlanguage. Our corpus consists of 440 essays written by premed students as\nnarrated simulated patient-doctor interactions. We start with baseline\nclassifiers (state-of-the-art recurrent neural networks and transformer\nmodels). Then, we enrich these models with a set of linguistic constructions\nproving the importance of this novel approach to the task of empathy\nclassification for this dataset. Our results indicate the potential of such\nconstructions to contribute to the overall empathy profile of first-person\nnarrative essays.", "published": "2023-02-03 16:30:09", "link": "http://arxiv.org/abs/2302.01839v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Entity-Agnostic Representation Learning for Parameter-Efficient\n  Knowledge Graph Embedding", "abstract": "We propose an entity-agnostic representation learning method for handling the\nproblem of inefficient parameter storage costs brought by embedding knowledge\ngraphs. Conventional knowledge graph embedding methods map elements in a\nknowledge graph, including entities and relations, into continuous vector\nspaces by assigning them one or multiple specific embeddings (i.e., vector\nrepresentations). Thus the number of embedding parameters increases linearly as\nthe growth of knowledge graphs. In our proposed model, Entity-Agnostic\nRepresentation Learning (EARL), we only learn the embeddings for a small set of\nentities and refer to them as reserved entities. To obtain the embeddings for\nthe full set of entities, we encode their distinguishable information from\ntheir connected relations, k-nearest reserved entities, and multi-hop\nneighbors. We learn universal and entity-agnostic encoders for transforming\ndistinguishable information into entity embeddings. This approach allows our\nproposed EARL to have a static, efficient, and lower parameter count than\nconventional knowledge graph embedding methods. Experimental results show that\nEARL uses fewer parameters and performs better on link prediction tasks than\nbaselines, reflecting its parameter efficiency.", "published": "2023-02-03 16:49:46", "link": "http://arxiv.org/abs/2302.01849v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generalizing to Unseen Elements: A Survey on Knowledge Extrapolation for\n  Knowledge Graphs", "abstract": "Knowledge graphs (KGs) have become valuable knowledge resources in various\napplications, and knowledge graph embedding (KGE) methods have garnered\nincreasing attention in recent years. However, conventional KGE methods still\nface challenges when it comes to handling unseen entities or relations during\nmodel testing. To address this issue, much effort has been devoted to various\nfields of KGs. In this paper, we use a set of general terminologies to unify\nthese methods and refer to them collectively as Knowledge Extrapolation. We\ncomprehensively summarize these methods, classified by our proposed taxonomy,\nand describe their interrelationships. Additionally, we introduce benchmarks\nand provide comparisons of these methods based on aspects that are not captured\nby the taxonomy. Finally, we suggest potential directions for future research.", "published": "2023-02-03 17:05:59", "link": "http://arxiv.org/abs/2302.01859v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GLADIS: A General and Large Acronym Disambiguation Benchmark", "abstract": "Acronym Disambiguation (AD) is crucial for natural language understanding on\nvarious sources, including biomedical reports, scientific papers, and search\nengine queries. However, existing acronym disambiguation benchmarks and tools\nare limited to specific domains, and the size of prior benchmarks is rather\nsmall. To accelerate the research on acronym disambiguation, we construct a new\nbenchmark named GLADIS with three components: (1) a much larger acronym\ndictionary with 1.5M acronyms and 6.4M long forms; (2) a pre-training corpus\nwith 160 million sentences; (3) three datasets that cover the general,\nscientific, and biomedical domains. We then pre-train a language model,\n\\emph{AcroBERT}, on our constructed corpus for general acronym disambiguation,\nand show the challenges and values of our new benchmark.", "published": "2023-02-03 17:07:23", "link": "http://arxiv.org/abs/2302.01860v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Few-Shot Identification of Morality Frames using In-Context\n  Learning", "abstract": "Data scarcity is a common problem in NLP, especially when the annotation\npertains to nuanced socio-linguistic concepts that require specialized\nknowledge. As a result, few-shot identification of these concepts is desirable.\nFew-shot in-context learning using pre-trained Large Language Models (LLMs) has\nbeen recently applied successfully in many NLP tasks. In this paper, we study\nfew-shot identification of a psycho-linguistic concept, Morality Frames (Roy et\nal., 2021), using LLMs. Morality frames are a representation framework that\nprovides a holistic view of the moral sentiment expressed in text, identifying\nthe relevant moral foundation (Haidt and Graham, 2007) and at a finer level of\ngranularity, the moral sentiment expressed towards the entities mentioned in\nthe text. Previous studies relied on human annotation to identify morality\nframes in text which is expensive. In this paper, we propose prompting-based\napproaches using pretrained Large Language Models for identification of\nmorality frames, relying only on few-shot exemplars. We compare our models'\nperformance with few-shot RoBERTa and found promising results.", "published": "2023-02-03 23:26:59", "link": "http://arxiv.org/abs/2302.02029v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Witgenstein's influence on artificial intelligence", "abstract": "We examine how much of the contemporary progress in artificial intelligence\n(and, specifically, in natural language processing), can be, more or less\ndirectly, traced back to the seminal work and ideas of the Austrian-British\nphilosopher Ludwig Wittgenstein, with particular focus on his late views.\nDiscussing Wittgenstein's original theses will give us the chance to survey the\nstate of artificial intelligence, and comment on both its strengths and\nweaknesses. A similar text appeared first in Spanish as a chapter of CENTENARIO\nDEL SILENCIO (2021), a book celebrating 100 years since the publication of the\nTractatus.", "published": "2023-02-03 06:47:20", "link": "http://arxiv.org/abs/2302.01570v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Modeling Sequential Sentence Relation to Improve Cross-lingual Dense\n  Retrieval", "abstract": "Recently multi-lingual pre-trained language models (PLM) such as mBERT and\nXLM-R have achieved impressive strides in cross-lingual dense retrieval.\nDespite its successes, they are general-purpose PLM while the multilingual PLM\ntailored for cross-lingual retrieval is still unexplored. Motivated by an\nobservation that the sentences in parallel documents are approximately in the\nsame order, which is universal across languages, we propose to model this\nsequential sentence relation to facilitate cross-lingual representation\nlearning. Specifically, we propose a multilingual PLM called masked sentence\nmodel (MSM), which consists of a sentence encoder to generate the sentence\nrepresentations, and a document encoder applied to a sequence of sentence\nvectors from a document. The document encoder is shared for all languages to\nmodel the universal sequential sentence relation across languages. To train the\nmodel, we propose a masked sentence prediction task, which masks and predicts\nthe sentence vector via a hierarchical contrastive loss with sampled negatives.\nComprehensive experiments on four cross-lingual retrieval tasks show MSM\nsignificantly outperforms existing advanced pre-training models, demonstrating\nthe effectiveness and stronger cross-lingual retrieval capabilities of our\napproach. Code and model will be available.", "published": "2023-02-03 09:54:27", "link": "http://arxiv.org/abs/2302.01626v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Mitigating Data Scarcity for Large Language Models", "abstract": "In recent years, pretrained neural language models (PNLMs) have taken the\nfield of natural language processing by storm, achieving new benchmarks and\nstate-of-the-art performances. These models often rely heavily on annotated\ndata, which may not always be available. Data scarcity are commonly found in\nspecialized domains, such as medical, or in low-resource languages that are\nunderexplored by AI research. In this dissertation, we focus on mitigating data\nscarcity using data augmentation and neural ensemble learning techniques for\nneural language models. In both research directions, we implement neural\nnetwork algorithms and evaluate their impact on assisting neural language\nmodels in downstream NLP tasks. Specifically, for data augmentation, we explore\ntwo techniques: 1) creating positive training data by moving an answer span\naround its original context and 2) using text simplification techniques to\nintroduce a variety of writing styles to the original training data. Our\nresults indicate that these simple and effective solutions improve the\nperformance of neural language models considerably in low-resource NLP domains\nand tasks. For neural ensemble learning, we use a multilabel neural classifier\nto select the best prediction outcome from a variety of individual pretrained\nneural language models trained for a low-resource medical text simplification\ntask.", "published": "2023-02-03 15:17:53", "link": "http://arxiv.org/abs/2302.01806v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CAB: Empathetic Dialogue Generation with Cognition, Affection and\n  Behavior", "abstract": "Empathy is an important characteristic to be considered when building a more\nintelligent and humanized dialogue agent. However, existing methods did not\nfully comprehend empathy as a complex process involving three aspects:\ncognition, affection and behavior. In this paper, we propose CAB, a novel\nframework that takes a comprehensive perspective of cognition, affection and\nbehavior to generate empathetic responses. For cognition, we build paths\nbetween critical keywords in the dialogue by leveraging external knowledge.\nThis is because keywords in a dialogue are the core of sentences. Building the\nlogic relationship between keywords, which is overlooked by the majority of\nexisting works, can improve the understanding of keywords and contextual logic,\nthus enhance the cognitive ability. For affection, we capture the emotional\ndependencies with dual latent variables that contain both interlocutors'\nemotions. The reason is that considering both interlocutors' emotions\nsimultaneously helps to learn the emotional dependencies. For behavior, we use\nappropriate dialogue acts to guide the dialogue generation to enhance the\nempathy expression. Extensive experiments demonstrate that our\nmulti-perspective model outperforms the state-of-the-art models in both\nautomatic and manual evaluation.", "published": "2023-02-03 14:31:17", "link": "http://arxiv.org/abs/2302.01935v2", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Witscript: A System for Generating Improvised Jokes in a Conversation", "abstract": "A chatbot is perceived as more humanlike and likeable if it includes some\njokes in its output. But most existing joke generators were not designed to be\nintegrated into chatbots. This paper presents Witscript, a novel joke\ngeneration system that can improvise original, contextually relevant jokes,\nsuch as humorous responses during a conversation. The system is based on joke\nwriting algorithms created by an expert comedy writer. Witscript employs\nwell-known tools of natural language processing to extract keywords from a\ntopic sentence and, using wordplay, to link those keywords and related words to\ncreate a punch line. Then a pretrained neural network language model that has\nbeen fine-tuned on a dataset of TV show monologue jokes is used to complete the\njoke response by filling the gap between the topic sentence and the punch line.\nA method of internal scoring filters out jokes that don't meet a preset\nstandard of quality. Human evaluators judged Witscript's responses to input\nsentences to be jokes more than 40% of the time. This is evidence that\nWitscript represents an important next step toward giving a chatbot a humanlike\nsense of humor.", "published": "2023-02-03 21:30:34", "link": "http://arxiv.org/abs/2302.02008v1", "categories": ["cs.CL", "cs.AI", "I.2.7; J.5"], "primary_category": "cs.CL"}
{"title": "Improving Interpretability via Explicit Word Interaction Graph Layer", "abstract": "Recent NLP literature has seen growing interest in improving model\ninterpretability. Along this direction, we propose a trainable neural network\nlayer that learns a global interaction graph between words and then selects\nmore informative words using the learned word interactions. Our layer, we call\nWIGRAPH, can plug into any neural network-based NLP text classifiers right\nafter its word embedding layer. Across multiple SOTA NLP models and various NLP\ndatasets, we demonstrate that adding the WIGRAPH layer substantially improves\nNLP models' interpretability and enhances models' prediction performance at the\nsame time.", "published": "2023-02-03 21:56:32", "link": "http://arxiv.org/abs/2302.02016v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TextShield: Beyond Successfully Detecting Adversarial Sentences in Text\n  Classification", "abstract": "Adversarial attack serves as a major challenge for neural network models in\nNLP, which precludes the model's deployment in safety-critical applications. A\nrecent line of work, detection-based defense, aims to distinguish adversarial\nsentences from benign ones. However, {the core limitation of previous detection\nmethods is being incapable of giving correct predictions on adversarial\nsentences unlike defense methods from other paradigms.} To solve this issue,\nthis paper proposes TextShield: (1) we discover a link between text attack and\nsaliency information, and then we propose a saliency-based detector, which can\neffectively detect whether an input sentence is adversarial or not. (2) We\ndesign a saliency-based corrector, which converts the detected adversary\nsentences to benign ones. By combining the saliency-based detector and\ncorrector, TextShield extends the detection-only paradigm to a\ndetection-correction paradigm, thus filling the gap in the existing\ndetection-based defense. Comprehensive experiments show that (a) TextShield\nconsistently achieves higher or comparable performance than state-of-the-art\ndefense methods across various attacks on different benchmarks. (b) our\nsaliency-based detector outperforms existing detectors for detecting\nadversarial sentences.", "published": "2023-02-03 22:58:07", "link": "http://arxiv.org/abs/2302.02023v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LaMPP: Language Models as Probabilistic Priors for Perception and Action", "abstract": "Language models trained on large text corpora encode rich distributional\ninformation about real-world environments and action sequences. This\ninformation plays a crucial role in current approaches to language processing\ntasks like question answering and instruction generation. We describe how to\nleverage language models for *non-linguistic* perception and control tasks. Our\napproach casts labeling and decision-making as inference in probabilistic\ngraphical models in which language models parameterize prior distributions over\nlabels, decisions and parameters, making it possible to integrate uncertain\nobservations and incomplete background knowledge in a principled way. Applied\nto semantic segmentation, household navigation, and activity recognition tasks,\nthis approach improves predictions on rare, out-of-distribution, and\nstructurally novel inputs.", "published": "2023-02-03 15:14:04", "link": "http://arxiv.org/abs/2302.02801v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Witscript 2: A System for Generating Improvised Jokes Without Wordplay", "abstract": "A previous paper presented Witscript, a system for generating conversational\njokes that rely on wordplay. This paper extends that work by presenting\nWitscript 2, which uses a large language model to generate conversational jokes\nthat rely on common sense instead of wordplay. Like Witscript, Witscript 2 is\nbased on joke-writing algorithms created by an expert comedy writer. Human\nevaluators judged Witscript 2's responses to input sentences to be jokes 46% of\nthe time, compared to 70% of the time for human-written responses. This is\nevidence that Witscript 2 represents another step toward giving a chatbot a\nhumanlike sense of humor.", "published": "2023-02-03 21:51:55", "link": "http://arxiv.org/abs/2302.03036v1", "categories": ["cs.CL", "cs.AI", "I.2.7; J.5"], "primary_category": "cs.CL"}
{"title": "Efficient Domain Adaptation for Speech Foundation Models", "abstract": "Foundation models (FMs), that are trained on broad data at scale and are\nadaptable to a wide range of downstream tasks, have brought large interest in\nthe research community. Benefiting from the diverse data sources such as\ndifferent modalities, languages and application domains, foundation models have\ndemonstrated strong generalization and knowledge transfer capabilities. In this\npaper, we present a pioneering study towards building an efficient solution for\nFM-based speech recognition systems. We adopt the recently developed\nself-supervised BEST-RQ for pretraining, and propose the joint finetuning with\nboth source and unsupervised target domain data using JUST Hydra. The FM\nencoder adapter and decoder are then finetuned to the target domain with a\nsmall amount of supervised in-domain data. On a large-scale YouTube and Voice\nSearch task, our method is shown to be both data and model parameter efficient.\nIt achieves the same quality with only 21.6M supervised in-domain data and\n130.8M finetuned parameters, compared to the 731.1M model trained from scratch\non additional 300M supervised in-domain data.", "published": "2023-02-03 02:10:35", "link": "http://arxiv.org/abs/2302.01496v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Revisiting Intermediate Layer Distillation for Compressing Language\n  Models: An Overfitting Perspective", "abstract": "Knowledge distillation (KD) is a highly promising method for mitigating the\ncomputational problems of pre-trained language models (PLMs). Among various KD\napproaches, Intermediate Layer Distillation (ILD) has been a de facto standard\nKD method with its performance efficacy in the NLP field. In this paper, we\nfind that existing ILD methods are prone to overfitting to training datasets,\nalthough these methods transfer more information than the original KD. Next, we\npresent the simple observations to mitigate the overfitting of ILD: distilling\nonly the last Transformer layer and conducting ILD on supplementary tasks.\nBased on our two findings, we propose a simple yet effective\nconsistency-regularized ILD (CR-ILD), which prevents the student model from\noverfitting the training dataset. Substantial experiments on distilling BERT on\nthe GLUE benchmark and several synthetic datasets demonstrate that our proposed\nILD method outperforms other KD techniques. Our code is available at\nhttps://github.com/jongwooko/CR-ILD.", "published": "2023-02-03 04:09:22", "link": "http://arxiv.org/abs/2302.01530v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Using natural language processing and structured medical data to\n  phenotype patients hospitalized due to COVID-19", "abstract": "To identify patients who are hospitalized because of COVID-19 as opposed to\nthose who were admitted for other indications, we compared the performance of\ndifferent computable phenotype definitions for COVID-19 hospitalizations that\nuse different types of data from the electronic health records (EHR), including\nstructured EHR data elements, provider notes, or a combination of both data\ntypes. And conduct a retrospective data analysis utilizing chart review-based\nvalidation. Participants are 586 hospitalized individuals who tested positive\nfor SARS-CoV-2 during January 2022. We used natural language processing to\nincorporate data from provider notes and LASSO regression and Random Forests to\nfit classification algorithms that incorporated structured EHR data elements,\nprovider notes, or a combination of structured data and provider notes.\nResults: Based on a chart review, 38% of 586 patients were determined to be\nhospitalized for reasons other than COVID-19 despite having tested positive for\nSARS-CoV-2. A classification algorithm that used provider notes had\nsignificantly better discrimination than one that used structured EHR data\nelements (AUROC: 0.894 vs 0.841, p < 0.001), and performed similarly to a model\nthat combined provider notes with structured data elements (AUROC: 0.894 vs\n0.893). Assessments of hospital outcome metrics significantly differed based on\nwhether the population included all hospitalized patients who tested positive\nfor SARS-CoV-2 versus those who were determined to have been hospitalized due\nto COVID-19. This work demonstrates the utility of natural language processing\napproaches to derive information related to patient hospitalizations in cases\nwhere there may be multiple conditions that could serve as the primary\nindication for hospitalization.", "published": "2023-02-03 04:22:29", "link": "http://arxiv.org/abs/2302.01536v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "A Case Study for Compliance as Code with Graphs and Language Models:\n  Public release of the Regulatory Knowledge Graph", "abstract": "The paper presents a study on using language models to automate the\nconstruction of executable Knowledge Graph (KG) for compliance. The paper\nfocuses on Abu Dhabi Global Market regulations and taxonomy, involves manual\ntagging a portion of the regulations, training BERT-based models, which are\nthen applied to the rest of the corpus. Coreference resolution and syntax\nanalysis were used to parse the relationships between the tagged entities and\nto form KG stored in a Neo4j database. The paper states that the use of machine\nlearning models released by regulators to automate the interpretation of rules\nis a vital step towards compliance automation, demonstrates the concept\nquerying with Cypher, and states that the produced sub-graphs combined with\nGraph Neural Networks (GNN) will achieve expandability in judgment automation\nsystems. The graph is open sourced on GitHub to provide structured data for\nfuture advancements in the field.", "published": "2023-02-03 16:37:08", "link": "http://arxiv.org/abs/2302.01842v1", "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.LG", "I.2.1; I.2.4; J.1"], "primary_category": "cs.AI"}
{"title": "Measuring The Impact Of Programming Language Distribution", "abstract": "Current benchmarks for evaluating neural code models focus on only a small\nsubset of programming languages, excluding many popular languages such as Go or\nRust. To ameliorate this issue, we present the BabelCode framework for\nexecution-based evaluation of any benchmark in any language. BabelCode enables\nnew investigations into the qualitative performance of models' memory, runtime,\nand individual test case results. Additionally, we present a new code\ntranslation dataset called Translating Python Programming Puzzles (TP3) from\nthe Python Programming Puzzles (Schuster et al. 2021) benchmark that involves\ntranslating expert-level python functions to any language. With both BabelCode\nand the TP3 benchmark, we investigate if balancing the distributions of 14\nlanguages in a training dataset improves a large language model's performance\non low-resource languages. Training a model on a balanced corpus results in, on\naverage, 12.34% higher $pass@k$ across all tasks and languages compared to the\nbaseline. We find that this strategy achieves 66.48% better $pass@k$ on\nlow-resource languages at the cost of only a 12.94% decrease to high-resource\nlanguages. In our three translation tasks, this strategy yields, on average,\n30.77% better low-resource $pass@k$ while having 19.58% worse high-resource\n$pass@k$.", "published": "2023-02-03 19:47:22", "link": "http://arxiv.org/abs/2302.01973v3", "categories": ["cs.LG", "cs.CL", "cs.PL"], "primary_category": "cs.LG"}
{"title": "PSST! Prosodic Speech Segmentation with Transformers", "abstract": "Self-attention mechanisms have enabled transformers to achieve\nsuperhuman-level performance on many speech-to-text (STT) tasks, yet the\nchallenge of automatic prosodic segmentation has remained unsolved. In this\npaper we finetune Whisper, a pretrained STT model, to annotate intonation unit\n(IU) boundaries by repurposing low-frequency tokens. Our approach achieves an\naccuracy of 95.8%, outperforming previous methods without the need for\nlarge-scale labeled data or enterprise grade compute resources. We also\ndiminish input signals by applying a series of filters, finding that low pass\nfilters at a 3.2 kHz level improve segmentation performance in out of sample\nand out of distribution contexts. We release our model as both a transcription\ntool and a baseline for further improvements in prosodic segmentation.", "published": "2023-02-03 20:09:17", "link": "http://arxiv.org/abs/2302.01984v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Detecting Reddit Users with Depression Using a Hybrid Neural Network\n  SBERT-CNN", "abstract": "Depression is a widespread mental health issue, affecting an estimated 3.8%\nof the global population. It is also one of the main contributors to disability\nworldwide. Recently it is becoming popular for individuals to use social media\nplatforms (e.g., Reddit) to express their difficulties and health issues (e.g.,\ndepression) and seek support from other users in online communities. It opens\ngreat opportunities to automatically identify social media users with\ndepression by parsing millions of posts for potential interventions. Deep\nlearning methods have begun to dominate in the field of machine learning and\nnatural language processing (NLP) because of their ease of use, efficient\nprocessing, and state-of-the-art results on many NLP tasks. In this work, we\npropose a hybrid deep learning model which combines a pretrained sentence BERT\n(SBERT) and convolutional neural network (CNN) to detect individuals with\ndepression with their Reddit posts. The sentence BERT is used to learn the\nmeaningful representation of semantic information in each post. CNN enables the\nfurther transformation of those embeddings and the temporal identification of\nbehavioral patterns of users. We trained and evaluated the model performance to\nidentify Reddit users with depression by utilizing the Self-reported Mental\nHealth Diagnoses (SMHD) data. The hybrid deep learning model achieved an\naccuracy of 0.86 and an F1 score of 0.86 and outperformed the state-of-the-art\ndocumented result (F1 score of 0.79) by other machine learning models in the\nliterature. The results show the feasibility of the hybrid model to identify\nindividuals with depression. Although the hybrid model is validated to detect\ndepression with Reddit posts, it can be easily tuned and applied to other text\nclassification tasks and different clinical applications.", "published": "2023-02-03 06:22:18", "link": "http://arxiv.org/abs/2302.02759v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exploring the Cognitive Dynamics of Artificial Intelligence in the\n  Post-COVID-19 and Learning 3.0 Era: A Case Study of ChatGPT", "abstract": "The emergence of artificial intelligence has incited a paradigm shift across\nthe spectrum of human endeavors, with ChatGPT serving as a catalyst for the\ntransformation of various established domains, including but not limited to\neducation, journalism, security, and ethics. In the post-pandemic era, the\nwidespread adoption of remote work has prompted the educational sector to\nreassess conventional pedagogical methods. This paper is to scrutinize the\nunderlying psychological principles of ChatGPT, delve into the factors that\ncaptivate user attention, and implicate its ramifications on the future of\nlearning. The ultimate objective of this study is to instigate a scholarly\ndiscourse on the interplay between technological advancements in education and\nthe evolution of human learning patterns, raising the question of whether\ntechnology is driving human evolution or vice versa.", "published": "2023-02-03 19:25:13", "link": "http://arxiv.org/abs/2302.04818v1", "categories": ["cs.CY", "cs.CL", "cs.HC"], "primary_category": "cs.CY"}
{"title": "The Heritage Digital Twin: a bicycle made for two. The integration of\n  digital methodologies into cultural heritage research", "abstract": "The paper concerns the definition of a novel ontology for cultural heritage\nbased on the concept of digital twin. The ontology, called Heritage Digital\nTwin ontology, is a compatible extension of the well-known CIDOC CRM ISO\nstandard for cultural heritage documentation and incorporates all the different\ndocumentation systems presently in use for cultural heritage documentation. In\nthe authors' view, it supports documentation interoperability at a higher level\nthan the ones currently in use and enables effective cooperation among\ndifferent users.", "published": "2023-02-03 08:36:37", "link": "http://arxiv.org/abs/2302.07138v1", "categories": ["cs.CY", "cs.CL", "cs.IR", "E.1; H.2.3; H.3.1; J.5"], "primary_category": "cs.CY"}
{"title": "Show me your NFT and I tell you how it will perform: Multimodal\n  representation learning for NFT selling price prediction", "abstract": "Non-Fungible Tokens (NFTs) represent deeds of ownership, based on blockchain\ntechnologies and smart contracts, of unique crypto assets on digital art forms\n(e.g., artworks or collectibles). In the spotlight after skyrocketing in 2021,\nNFTs have attracted the attention of crypto enthusiasts and investors intent on\nplacing promising investments in this profitable market. However, the NFT\nfinancial performance prediction has not been widely explored to date.\n  In this work, we address the above problem based on the hypothesis that NFT\nimages and their textual descriptions are essential proxies to predict the NFT\nselling prices. To this purpose, we propose MERLIN, a novel multimodal deep\nlearning framework designed to train Transformer-based language and visual\nmodels, along with graph neural network models, on collections of NFTs' images\nand texts. A key aspect in MERLIN is its independence on financial features, as\nit exploits only the primary data a user interested in NFT trading would like\nto deal with, i.e., NFT images and textual descriptions. By learning dense\nrepresentations of such data, a price-category classification task is performed\nby MERLIN models, which can also be tuned according to user preferences in the\ninference phase to mimic different risk-return investment profiles.\nExperimental evaluation on a publicly available dataset has shown that MERLIN\nmodels achieve significant performances according to several financial\nassessment criteria, fostering profitable investments, and also beating\nbaseline machine-learning classifiers based on financial features.", "published": "2023-02-03 11:56:38", "link": "http://arxiv.org/abs/2302.01676v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.NE"], "primary_category": "cs.LG"}
{"title": "Relating EEG to continuous speech using deep neural networks: a review", "abstract": "Objective. When a person listens to continuous speech, a corresponding\nresponse is elicited in the brain and can be recorded using\nelectroencephalography (EEG). Linear models are presently used to relate the\nEEG recording to the corresponding speech signal. The ability of linear models\nto find a mapping between these two signals is used as a measure of neural\ntracking of speech. Such models are limited as they assume linearity in the\nEEG-speech relationship, which omits the nonlinear dynamics of the brain. As an\nalternative, deep learning models have recently been used to relate EEG to\ncontinuous speech. Approach. This paper reviews and comments on\ndeep-learning-based studies that relate EEG to continuous speech in single- or\nmultiple-speakers paradigms. We point out recurrent methodological pitfalls and\nthe need for a standard benchmark of model analysis. Main results. We gathered\n29 studies. The main methodological issues we found are biased\ncross-validations, data leakage leading to over-fitted models, or\ndisproportionate data size compared to the model's complexity. In addition, we\naddress requirements for a standard benchmark model analysis, such as public\ndatasets, common evaluation metrics, and good practices for the match-mismatch\ntask. Significance. We present a review paper summarizing the main\ndeep-learning-based studies that relate EEG to speech while addressing\nmethodological pitfalls and important considerations for this newly expanding\nfield. Our study is particularly relevant given the growing application of deep\nlearning in EEG-speech decoding.", "published": "2023-02-03 13:51:01", "link": "http://arxiv.org/abs/2302.01736v4", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SPADE: Self-supervised Pretraining for Acoustic DisEntanglement", "abstract": "Self-supervised representation learning approaches have grown in popularity\ndue to the ability to train models on large amounts of unlabeled data and have\ndemonstrated success in diverse fields such as natural language processing,\ncomputer vision, and speech. Previous self-supervised work in the speech domain\nhas disentangled multiple attributes of speech such as linguistic content,\nspeaker identity, and rhythm. In this work, we introduce a self-supervised\napproach to disentangle room acoustics from speech and use the acoustic\nrepresentation on the downstream task of device arbitration. Our results\ndemonstrate that our proposed approach significantly improves performance over\na baseline when labeled training data is scarce, indicating that our\npretraining scheme learns to encode room acoustic information while remaining\ninvariant to other attributes of the speech signal.", "published": "2023-02-03 01:36:38", "link": "http://arxiv.org/abs/2302.01483v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
