{"title": "Query Tracking for E-commerce Conversational Search: A Machine\n  Comprehension Perspective", "abstract": "With the development of dialog techniques, conversational search has\nattracted more and more attention as it enables users to interact with the\nsearch engine in a natural and efficient manner. However, comparing with the\nnatural language understanding in traditional task-oriented dialog which\nfocuses on slot filling and tracking, the query understanding in E-commerce\nconversational search is quite different and more challenging due to more\ndiverse user expressions and complex intentions. In this work, we define the\nreal-world problem of query tracking in E-commerce conversational search, in\nwhich the goal is to update the internal query after each round of interaction.\nWe also propose a self attention based neural network to handle the task in a\nmachine comprehension perspective. Further more we build a novel E-commerce\nquery tracking dataset from an operational E-commerce Search Engine, and\nexperimental results on this dataset suggest that our proposed model\noutperforms several baseline methods by a substantial gain for Exact Match\naccuracy and F1 score, showing the potential of machine comprehension like\nmodel for this task.", "published": "2018-10-08 05:27:44", "link": "http://arxiv.org/abs/1810.03274v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Task Learning for Domain-General Spoken Disfluency Detection in\n  Dialogue Systems", "abstract": "Spontaneous spoken dialogue is often disfluent, containing pauses,\nhesitations, self-corrections and false starts. Processing such phenomena is\nessential in understanding a speaker's intended meaning and controlling the\nflow of the conversation. Furthermore, this processing needs to be word-by-word\nincremental to allow further downstream processing to begin as early as\npossible in order to handle real spontaneous human conversational behaviour.\n  In addition, from a developer's point of view, it is highly desirable to be\nable to develop systems which can be trained from `clean' examples while also\nable to generalise to the very diverse disfluent variations on the same data --\nthereby enhancing both data-efficiency and robustness. In this paper, we\npresent a multi-task LSTM-based model for incremental detection of disfluency\nstructure, which can be hooked up to any component for incremental\ninterpretation (e.g. an incremental semantic parser), or else simply used to\n`clean up' the current utterance as it is being produced.\n  We train the system on the Switchboard Dialogue Acts (SWDA) corpus and\npresent its accuracy on this dataset. Our model outperforms prior neural\nnetwork-based incremental approaches by about 10 percentage points on SWDA\nwhile employing a simpler architecture. To test the model's generalisation\npotential, we evaluate the same model on the bAbI+ dataset, without any\nadditional training. bAbI+ is a dataset of synthesised goal-oriented dialogues\nwhere we control the distribution of disfluencies and their types. This shows\nthat our approach has good generalisation potential, and sheds more light on\nwhich types of disfluency might be amenable to domain-general processing.", "published": "2018-10-08 09:57:44", "link": "http://arxiv.org/abs/1810.03352v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "An AMR Aligner Tuned by Transition-based Parser", "abstract": "In this paper, we propose a new rich resource enhanced AMR aligner which\nproduces multiple alignments and a new transition system for AMR parsing along\nwith its oracle parser. Our aligner is further tuned by our oracle parser via\npicking the alignment that leads to the highest-scored achievable AMR graph.\nExperimental results show that our aligner outperforms the rule-based aligner\nin previous work by achieving higher alignment F1 score and consistently\nimproving two open-sourced AMR parsers. Based on our aligner and transition\nsystem, we develop a transition-based AMR parser that parses a sentence into\nits AMR graph directly. An ensemble of our parsers with only words and POS tags\nas input leads to 68.4 Smatch F1 score.", "published": "2018-10-08 16:00:50", "link": "http://arxiv.org/abs/1810.03541v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving the Transformer Translation Model with Document-Level Context", "abstract": "Although the Transformer translation model (Vaswani et al., 2017) has\nachieved state-of-the-art performance in a variety of translation tasks, how to\nuse document-level context to deal with discourse phenomena problematic for\nTransformer still remains a challenge. In this work, we extend the Transformer\nmodel with a new context encoder to represent document-level context, which is\nthen incorporated into the original encoder and decoder. As large-scale\ndocument-level parallel corpora are usually not available, we introduce a\ntwo-step training method to take full advantage of abundant sentence-level\nparallel corpora and limited document-level parallel corpora. Experiments on\nthe NIST Chinese-English datasets and the IWSLT French-English datasets show\nthat our approach improves over Transformer significantly.", "published": "2018-10-08 17:09:10", "link": "http://arxiv.org/abs/1810.03581v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "End-to-End Text Classification via Image-based Embedding using\n  Character-level Networks", "abstract": "For analysing and/or understanding languages having no word boundaries based\non morphological analysis such as Japanese, Chinese, and Thai, it is desirable\nto perform appropriate word segmentation before word embeddings. But it is\ninherently difficult in these languages. In recent years, various language\nmodels based on deep learning have made remarkable progress, and some of these\nmethodologies utilizing character-level features have successfully avoided such\na difficult problem. However, when a model is fed character-level features of\nthe above languages, it often causes overfitting due to a large number of\ncharacter types. In this paper, we propose a CE-CLCNN, character-level\nconvolutional neural networks using a character encoder to tackle these\nproblems. The proposed CE-CLCNN is an end-to-end learning model and has an\nimage-based character encoder, i.e. the CE-CLCNN handles each character in the\ntarget document as an image. Through various experiments, we found and\nconfirmed that our CE-CLCNN captured closely embedded features for visually and\nsemantically similar characters and achieves state-of-the-art results on\nseveral open document classification tasks. In this paper we report the\nperformance of our CE-CLCNN with the Wikipedia title estimation task and\nanalyse the internal behaviour.", "published": "2018-10-08 17:44:34", "link": "http://arxiv.org/abs/1810.03595v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparing Models of Associative Meaning: An Empirical Investigation of\n  Reference in Simple Language Games", "abstract": "Simple reference games are of central theoretical and empirical importance in\nthe study of situated language use. Although language provides rich,\ncompositional truth-conditional semantics to facilitate reference, speakers and\nlisteners may sometimes lack the overall lexical and cognitive resources to\nguarantee successful reference through these means alone. However, language\nalso has rich associational structures that can serve as a further resource for\nachieving successful reference. Here we investigate this use of associational\ninformation in a setting where only associational information is available: a\nsimplified version of the popular game Codenames. Using optimal experiment\ndesign techniques, we compare a range of models varying in the type of\nassociative information deployed and in level of pragmatic sophistication\nagainst human behavior. In this setting, we find that listeners' behavior\nreflects direct bigram collocational associations more strongly than\nword-embedding or semantic knowledge graph-based associations and that there is\nlittle evidence for pragmatically sophisticated behavior by either speakers or\nlisteners of the type that might be predicted by recursive-reasoning models\nsuch as the Rational Speech Acts theory. These results shed light on the nature\nof the lexical resources that speakers and listeners can bring to bear in\nachieving reference through associative meaning alone.", "published": "2018-10-08 21:51:44", "link": "http://arxiv.org/abs/1810.03717v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Word Embeddings from Large-Scale Greek Web Content", "abstract": "Word embeddings are undoubtedly very useful components in many NLP tasks. In\nthis paper, we present word embeddings and other linguistic resources trained\non the largest to date digital Greek language corpus. We also present a live\nweb tool for testing the Greek word embeddings, by offering \"analogy\",\n\"similarity score\" and \"most similar words\" functions. Through our explorer,\none could interact with the Greek word vectors.", "published": "2018-10-08 17:19:07", "link": "http://arxiv.org/abs/1810.06694v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Source Cross-Lingual Model Transfer: Learning What to Share", "abstract": "Modern NLP applications have enjoyed a great boost utilizing neural networks\nmodels. Such deep neural models, however, are not applicable to most human\nlanguages due to the lack of annotated training data for various NLP tasks.\nCross-lingual transfer learning (CLTL) is a viable method for building NLP\nmodels for a low-resource target language by leveraging labeled data from other\n(source) languages. In this work, we focus on the multilingual transfer setting\nwhere training data in multiple source languages is leveraged to further boost\ntarget language performance.\n  Unlike most existing methods that rely only on language-invariant features\nfor CLTL, our approach coherently utilizes both language-invariant and\nlanguage-specific features at instance level. Our model leverages adversarial\nnetworks to learn language-invariant features, and mixture-of-experts models to\ndynamically exploit the similarity between the target language and each\nindividual source language. This enables our model to learn effectively what to\nshare between various languages in the multilingual setup. Moreover, when\ncoupled with unsupervised multilingual embeddings, our model can operate in a\nzero-resource setting where neither target language training data nor\ncross-lingual resources are available. Our model achieves significant\nperformance gains over prior art, as shown in an extensive set of experiments\nover multiple text classification and sequence tagging tasks including a\nlarge-scale industry dataset.", "published": "2018-10-08 16:11:01", "link": "http://arxiv.org/abs/1810.03552v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DepecheMood++: a Bilingual Emotion Lexicon Built Through Simple Yet\n  Powerful Techniques", "abstract": "Several lexica for sentiment analysis have been developed and made available\nin the NLP community. While most of these come with word polarity annotations\n(e.g. positive/negative), attempts at building lexica for finer-grained emotion\nanalysis (e.g. happiness, sadness) have recently attracted significant\nattention. Such lexica are often exploited as a building block in the process\nof developing learning models for which emotion recognition is needed, and/or\nused as baselines to which compare the performance of the models. In this work,\nwe contribute two new resources to the community: a) an extension of an\nexisting and widely used emotion lexicon for English; and b) a novel version of\nthe lexicon targeting Italian. Furthermore, we show how simple techniques can\nbe used, both in supervised and unsupervised experimental settings, to boost\nperformances on datasets and tasks of varying degree of domain-specificity.", "published": "2018-10-08 19:05:23", "link": "http://arxiv.org/abs/1810.03660v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Cross Script Hindi English NER Corpus from Wikipedia", "abstract": "The text generated on social media platforms is essentially a mixed lingual\ntext. The mixing of language in any form produces considerable amount of\ndifficulty in language processing systems. Moreover, the advancements in\nlanguage processing research depends upon the availability of standard corpora.\nThe development of mixed lingual Indian Named Entity Recognition (NER) systems\nare facing obstacles due to unavailability of the standard evaluation corpora.\nSuch corpora may be of mixed lingual nature in which text is written using\nmultiple languages predominantly using a single script only. The motivation of\nour work is to emphasize the automatic generation such kind of corpora in order\nto encourage mixed lingual Indian NER. The paper presents the preparation of a\nCross Script Hindi-English Corpora from Wikipedia category pages. The corpora\nis successfully annotated using standard CoNLL-2003 categories of PER, LOC,\nORG, and MISC. Its evaluation is carried out on a variety of machine learning\nalgorithms and favorable results are achieved.", "published": "2018-10-08 13:25:05", "link": "http://arxiv.org/abs/1810.03430v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Recognizing Overlapped Speech in Meetings: A Multichannel Separation\n  Approach Using Neural Networks", "abstract": "The goal of this work is to develop a meeting transcription system that can\nrecognize speech even when utterances of different speakers are overlapped.\nWhile speech overlaps have been regarded as a major obstacle in accurately\ntranscribing meetings, a traditional beamformer with a single output has been\nexclusively used because previously proposed speech separation techniques have\ncritical constraints for application to real meetings. This paper proposes a\nnew signal processing module, called an unmixing transducer, and describes its\nimplementation using a windowed BLSTM. The unmixing transducer has a fixed\nnumber, say J, of output channels, where J may be different from the number of\nmeeting attendees, and transforms an input multi-channel acoustic signal into J\ntime-synchronous audio streams. Each utterance in the meeting is separated and\nemitted from one of the output channels. Then, each output signal can be simply\nfed to a speech recognition back-end for segmentation and transcription. Our\nmeeting transcription system using the unmixing transducer outperforms a system\nbased on a state-of-the-art neural mask-based beamformer by 10.8%. Significant\nimprovements are observed in overlapped segments. To the best of our knowledge,\nthis is the first report that applies overlapped speech recognition to\nunconstrained real meeting audio.", "published": "2018-10-08 18:50:54", "link": "http://arxiv.org/abs/1810.03655v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
