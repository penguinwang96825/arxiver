{"title": "Improving Twitter Sentiment Classification via Multi-Level\n  Sentiment-Enriched Word Embeddings", "abstract": "Most of existing work learn sentiment-specific word representation for\nimproving Twitter sentiment classification, which encoded both n-gram and\ndistant supervised tweet sentiment information in learning process. They assume\nall words within a tweet have the same sentiment polarity as the whole tweet,\nwhich ignores the word its own sentiment polarity. To address this problem, we\npropose to learn sentiment-specific word embedding by exploiting both lexicon\nresource and distant supervised information. We develop a multi-level\nsentiment-enriched word embedding learning method, which uses parallel\nasymmetric neural network to model n-gram, word level sentiment and tweet level\nsentiment in learning process. Experiments on standard benchmarks show our\napproach outperforms state-of-the-art methods.", "published": "2016-11-01 04:48:09", "link": "http://arxiv.org/abs/1611.00126v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dual Learning for Machine Translation", "abstract": "While neural machine translation (NMT) is making good progress in the past\ntwo years, tens of millions of bilingual sentence pairs are needed for its\ntraining. However, human labeling is very costly. To tackle this training data\nbottleneck, we develop a dual-learning mechanism, which can enable an NMT\nsystem to automatically learn from unlabeled data through a dual-learning game.\nThis mechanism is inspired by the following observation: any machine\ntranslation task has a dual task, e.g., English-to-French translation (primal)\nversus French-to-English translation (dual); the primal and dual tasks can form\na closed loop, and generate informative feedback signals to train the\ntranslation models, even if without the involvement of a human labeler. In the\ndual-learning mechanism, we use one agent to represent the model for the primal\ntask and the other agent to represent the model for the dual task, then ask\nthem to teach each other through a reinforcement learning process. Based on the\nfeedback signals generated during this process (e.g., the language-model\nlikelihood of the output of a model, and the reconstruction error of the\noriginal sentence after the primal and dual translations), we can iteratively\nupdate the two models until convergence (e.g., using the policy gradient\nmethods). We call the corresponding approach to neural machine translation\n\\emph{dual-NMT}. Experiments show that dual-NMT works very well on\nEnglish$\\leftrightarrow$French translation; especially, by learning from\nmonolingual data (with 10% bilingual data for warm start), it achieves a\ncomparable accuracy to NMT trained from the full bilingual data for the\nFrench-to-English translation task.", "published": "2016-11-01 10:38:29", "link": "http://arxiv.org/abs/1611.00179v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Recurrent Neural Network Language Model Adaptation Derived Document\n  Vector", "abstract": "In many natural language processing (NLP) tasks, a document is commonly\nmodeled as a bag of words using the term frequency-inverse document frequency\n(TF-IDF) vector. One major shortcoming of the frequency-based TF-IDF feature\nvector is that it ignores word orders that carry syntactic and semantic\nrelationships among the words in a document, and they can be important in some\nNLP tasks such as genre classification. This paper proposes a novel distributed\nvector representation of a document: a simple recurrent-neural-network language\nmodel (RNN-LM) or a long short-term memory RNN language model (LSTM-LM) is\nfirst created from all documents in a task; some of the LM parameters are then\nadapted by each document, and the adapted parameters are vectorized to\nrepresent the document. The new document vectors are labeled as DV-RNN and\nDV-LSTM respectively. We believe that our new document vectors can capture some\nhigh-level sequential information in the documents, which other current\ndocument representations fail to capture. The new document vectors were\nevaluated in the genre classification of documents in three corpora: the Brown\nCorpus, the BNC Baby Corpus and an artificially created Penn Treebank dataset.\nTheir classification performances are compared with the performance of TF-IDF\nvector and the state-of-the-art distributed memory model of paragraph vector\n(PV-DM). The results show that DV-LSTM significantly outperforms TF-IDF and\nPV-DM in most cases, and combinations of the proposed document vectors with\nTF-IDF or PV-DM may further improve performance.", "published": "2016-11-01 12:14:02", "link": "http://arxiv.org/abs/1611.00196v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Faster decoding for subword level Phrase-based SMT between related\n  languages", "abstract": "A common and effective way to train translation systems between related\nlanguages is to consider sub-word level basic units. However, this increases\nthe length of the sentences resulting in increased decoding time. The increase\nin length is also impacted by the specific choice of data format for\nrepresenting the sentences as subwords. In a phrase-based SMT framework, we\ninvestigate different choices of decoder parameters as well as data format and\ntheir impact on decoding time and translation accuracy. We suggest best options\nfor these settings that significantly improve decoding time with little impact\non the translation accuracy.", "published": "2016-11-01 19:56:36", "link": "http://arxiv.org/abs/1611.00354v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MusicMood: Predicting the mood of music from song lyrics using machine\n  learning", "abstract": "Sentiment prediction of contemporary music can have a wide-range of\napplications in modern society, for instance, selecting music for public\ninstitutions such as hospitals or restaurants to potentially improve the\nemotional well-being of personnel, patients, and customers, respectively. In\nthis project, music recommendation system built upon on a naive Bayes\nclassifier, trained to predict the sentiment of songs based on song lyrics\nalone. The experimental results show that music corresponding to a happy mood\ncan be detected with high precision based on text features obtained from song\nlyrics.", "published": "2016-11-01 06:05:49", "link": "http://arxiv.org/abs/1611.00138v1", "categories": ["cs.LG", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "Using Artificial Intelligence to Identify State Secrets", "abstract": "Whether officials can be trusted to protect national security information has\nbecome a matter of great public controversy, reigniting a long-standing debate\nabout the scope and nature of official secrecy. The declassification of\nmillions of electronic records has made it possible to analyze these issues\nwith greater rigor and precision. Using machine-learning methods, we examined\nnearly a million State Department cables from the 1970s to identify features of\nrecords that are more likely to be classified, such as international\nnegotiations, military operations, and high-level communications. Even with\nincomplete data, algorithms can use such features to identify 90% of classified\ncables with <11% false positives. But our results also show that there are\nlongstanding problems in the identification of sensitive information. Error\nanalysis reveals many examples of both overclassification and\nunderclassification. This indicates both the need for research on inter-coder\nreliability among officials as to what constitutes classified material and the\nopportunity to develop recommender systems to better manage both classification\nand declassification.", "published": "2016-11-01 19:59:48", "link": "http://arxiv.org/abs/1611.00356v1", "categories": ["cs.CY", "cs.CL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "CB2CF: A Neural Multiview Content-to-Collaborative Filtering Model for\n  Completely Cold Item Recommendations", "abstract": "In Recommender Systems research, algorithms are often characterized as either\nCollaborative Filtering (CF) or Content Based (CB). CF algorithms are trained\nusing a dataset of user preferences while CB algorithms are typically based on\nitem profiles. These approaches harness different data sources and therefore\nthe resulting recommended items are generally very different. This paper\npresents the CB2CF, a deep neural multiview model that serves as a bridge from\nitems content into their CF representations. CB2CF is a real-world algorithm\ndesigned for Microsoft Store services that handle around a billion users\nworldwide. CB2CF is demonstrated on movies and apps recommendations, where it\nis shown to outperform an alternative CB model on completely cold items.", "published": "2016-11-01 20:48:34", "link": "http://arxiv.org/abs/1611.00384v2", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
