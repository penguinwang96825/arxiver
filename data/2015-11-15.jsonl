{"title": "A System for Extracting Sentiment from Large-Scale Arabic Social Data", "abstract": "Social media data in Arabic language is becoming more and more abundant. It\nis a consensus that valuable information lies in social media data. Mining this\ndata and making the process easier are gaining momentum in the industries. This\npaper describes an enterprise system we developed for extracting sentiment from\nlarge volumes of social data in Arabic dialects. First, we give an overview of\nthe Big Data system for information extraction from multilingual social data\nfrom a variety of sources. Then, we focus on the Arabic sentiment analysis\ncapability that was built on top of the system including normalizing written\nArabic dialects, building sentiment lexicons, sentiment classification, and\nperformance evaluation. Lastly, we demonstrate the value of enriching sentiment\nresults with user profiles in understanding sentiments of a specific user\ngroup.", "published": "2015-11-15 05:53:13", "link": "http://arxiv.org/abs/1511.04661v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Word Embedding based Correlation Model for Question/Answer Matching", "abstract": "With the development of community based question answering (Q&A) services, a\nlarge scale of Q&A archives have been accumulated and are an important\ninformation and knowledge resource on the web. Question and answer matching has\nbeen attached much importance to for its ability to reuse knowledge stored in\nthese systems: it can be useful in enhancing user experience with recurrent\nquestions. In this paper, we try to improve the matching accuracy by overcoming\nthe lexical gap between question and answer pairs. A Word Embedding based\nCorrelation (WEC) model is proposed by integrating advantages of both the\ntranslation model and word embedding, given a random pair of words, WEC can\nscore their co-occurrence probability in Q&A pairs and it can also leverage the\ncontinuity and smoothness of continuous space word representation to deal with\nnew pairs of words that are rare in the training parallel text. An experimental\nstudy on Yahoo! Answers dataset and Baidu Zhidao dataset shows this new\nmethod's promising potential.", "published": "2015-11-15 02:59:22", "link": "http://arxiv.org/abs/1511.04646v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning Representations of Affect from Speech", "abstract": "There has been a lot of prior work on representation learning for speech\nrecognition applications, but not much emphasis has been given to an\ninvestigation of effective representations of affect from speech, where the\nparalinguistic elements of speech are separated out from the verbal content. In\nthis paper, we explore denoising autoencoders for learning paralinguistic\nattributes i.e. categorical and dimensional affective traits from speech. We\nshow that the representations learnt by the bottleneck layer of the autoencoder\nare highly discriminative of activation intensity and at separating out\nnegative valence (sadness and anger) from positive valence (happiness). We\nexperiment with different input speech features (such as FFT and log-mel\nspectrograms with temporal context windows), and different autoencoder\narchitectures (such as stacked and deep autoencoders). We also learn utterance\nspecific representations by a combination of denoising autoencoders and BLSTM\nbased recurrent autoencoders. Emotion classification is performed with the\nlearnt temporal/dynamic representations to evaluate the quality of the\nrepresentations. Experiments on a well-established real-life speech dataset\n(IEMOCAP) show that the learnt representations are comparable to state of the\nart feature extractors (such as voice quality features and MFCCs) and are\ncompetitive with state-of-the-art approaches at emotion and dimensional affect\nrecognition.", "published": "2015-11-15 18:16:20", "link": "http://arxiv.org/abs/1511.04747v6", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
