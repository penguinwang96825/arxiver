{"title": "PersonaBank: A Corpus of Personal Narratives and Their Story Intention\n  Graphs", "abstract": "We present a new corpus, PersonaBank, consisting of 108 personal stories from\nweblogs that have been annotated with their Story Intention Graphs, a deep\nrepresentation of the fabula of a story. We describe the topics of the stories\nand the basis of the Story Intention Graph representation, as well as the\nprocess of annotating the stories to produce the Story Intention Graphs and the\nchallenges of adapting the tool to this new personal narrative domain We also\ndiscuss how the corpus can be used in applications that retell the story using\ndifferent styles of tellings, co-tellings, or as a content planner.", "published": "2017-08-30 01:56:23", "link": "http://arxiv.org/abs/1708.09082v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Argument Strength is in the Eye of the Beholder: Audience Effects in\n  Persuasion", "abstract": "Americans spend about a third of their time online, with many participating\nin online conversations on social and political issues. We hypothesize that\nsocial media arguments on such issues may be more engaging and persuasive than\ntraditional media summaries, and that particular types of people may be more or\nless convinced by particular styles of argument, e.g. emotional arguments may\nresonate with some personalities while factual arguments resonate with others.\nWe report a set of experiments testing at large scale how audience variables\ninteract with argument style to affect the persuasiveness of an argument, an\nunder-researched topic within natural language processing. We show that belief\nchange is affected by personality factors, with conscientious, open and\nagreeable people being more convinced by emotional arguments.", "published": "2017-08-30 02:01:30", "link": "http://arxiv.org/abs/1708.09085v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automating Direct Speech Variations in Stories and Games", "abstract": "Dialogue authoring in large games requires not only content creation but the\nsubtlety of its delivery, which can vary from character to character. Manually\nauthoring this dialogue can be tedious, time-consuming, or even altogether\ninfeasible. This paper utilizes a rich narrative representation for modeling\ndialogue and an expressive natural language generation engine for realizing it,\nand expands upon a translation tool that bridges the two. We add functionality\nto the translator to allow direct speech to be modeled by the narrative\nrepresentation, whereas the original translator supports only narratives told\nby a third person narrator. We show that we can perform character substitution\nin dialogues. We implement and evaluate a potential application to dialogue\nimplementation: generating dialogue for games with big, dynamic, or\nprocedurally-generated open worlds. We present a pilot study on human\nperceptions of the personalities of characters using direct speech, assuming\nunknown personality types at the time of authoring.", "published": "2017-08-30 02:19:39", "link": "http://arxiv.org/abs/1708.09090v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Paradigm Completion for Derivational Morphology", "abstract": "The generation of complex derived word forms has been an overlooked problem\nin NLP; we fill this gap by applying neural sequence-to-sequence models to the\ntask. We overview the theoretical motivation for a paradigmatic treatment of\nderivational morphology, and introduce the task of derivational paradigm\ncompletion as a parallel to inflectional paradigm completion. State-of-the-art\nneural models, adapted from the inflection task, are able to learn a range of\nderivation patterns, and outperform a non-neural baseline by 16.4%. However,\ndue to semantic, historical, and lexical considerations involved in\nderivational morphology, future work will be needed to achieve performance\nparity with inflection-generating systems.", "published": "2017-08-30 07:55:57", "link": "http://arxiv.org/abs/1708.09151v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-lingual, Character-Level Neural Morphological Tagging", "abstract": "Even for common NLP tasks, sufficient supervision is not available in many\nlanguages -- morphological tagging is no exception. In the work presented here,\nwe explore a transfer learning scheme, whereby we train character-level\nrecurrent neural taggers to predict morphological taggings for high-resource\nlanguages and low-resource languages together. Learning joint character\nrepresentations among multiple related languages successfully enables knowledge\ntransfer from the high-resource languages to the low-resource ones, improving\naccuracy by up to 30% over a monolingual model.", "published": "2017-08-30 08:14:34", "link": "http://arxiv.org/abs/1708.09157v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Empirical Study of Discriminative Sequence Labeling Models for\n  Vietnamese Text Processing", "abstract": "This paper presents an empirical study of two widely-used sequence prediction\nmodels, Conditional Random Fields (CRFs) and Long Short-Term Memory Networks\n(LSTMs), on two fundamental tasks for Vietnamese text processing, including\npart-of-speech tagging and named entity recognition. We show that a strong\nlower bound for labeling accuracy can be obtained by relying only on simple\nword-based features with minimal hand-crafted feature engineering, of 90.65\\%\nand 86.03\\% performance scores on the standard test sets for the two tasks\nrespectively. In particular, we demonstrate empirically the surprising\nefficiency of word embeddings in both of the two tasks, with both of the two\nmodels. We point out that the state-of-the-art LSTMs model does not always\noutperform significantly the traditional CRFs model, especially on\nmoderate-sized data sets. Finally, we give some suggestions and discussions for\nefficient use of sequence labeling models in practical applications.", "published": "2017-08-30 08:32:32", "link": "http://arxiv.org/abs/1708.09163v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Look-ahead Attention for Generation in Neural Machine Translation", "abstract": "The attention model has become a standard component in neural machine\ntranslation (NMT) and it guides translation process by selectively focusing on\nparts of the source sentence when predicting each target word. However, we find\nthat the generation of a target word does not only depend on the source\nsentence, but also rely heavily on the previous generated target words,\nespecially the distant words which are difficult to model by using recurrent\nneural networks. To solve this problem, we propose in this paper a novel\nlook-ahead attention mechanism for generation in NMT, which aims at directly\ncapturing the dependency relationship between target words. We further design\nthree patterns to integrate our look-ahead attention into the conventional\nattention model. Experiments on NIST Chinese-to-English and WMT\nEnglish-to-German translation tasks show that our proposed look-ahead attention\nmechanism achieves substantial improvements over state-of-the-art baselines.", "published": "2017-08-30 11:27:02", "link": "http://arxiv.org/abs/1708.09217v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TANKER: Distributed Architecture for Named Entity Recognition and\n  Disambiguation", "abstract": "Named Entity Recognition and Disambiguation (NERD) systems have recently been\nwidely researched to deal with the significant growth of the Web. NERD systems\nare crucial for several Natural Language Processing (NLP) tasks such as\nsummarization, understanding, and machine translation. However, there is no\nstandard interface specification, i.e. these systems may vary significantly\neither for exporting their outputs or for processing the inputs. Thus, when a\ngiven company desires to implement more than one NERD system, the process is\nquite exhaustive and prone to failure. In addition, industrial solutions demand\ncritical requirements, e.g., large-scale processing, completeness, versatility,\nand licenses. Commonly, these requirements impose a limitation, making good\nNERD models to be ignored by companies. This paper presents TANKER, a\ndistributed architecture which aims to overcome scalability, reliability and\nfailure tolerance limitations related to industrial needs by combining NERD\nsystems. To this end, TANKER relies on a micro-services oriented architecture,\nwhich enables agile development and delivery of complex enterprise\napplications. In addition, TANKER provides a standardized API which makes\npossible to combine several NERD systems at once.", "published": "2017-08-30 12:10:20", "link": "http://arxiv.org/abs/1708.09230v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fighting with the Sparsity of Synonymy Dictionaries", "abstract": "Graph-based synset induction methods, such as MaxMax and Watset, induce\nsynsets by performing a global clustering of a synonymy graph. However, such\nmethods are sensitive to the structure of the input synonymy graph: sparseness\nof the input dictionary can substantially reduce the quality of the extracted\nsynsets. In this paper, we propose two different approaches designed to\nalleviate the incompleteness of the input dictionaries. The first one performs\na pre-processing of the graph by adding missing edges, while the second one\nperforms a post-processing by merging similar synset clusters. We evaluate\nthese approaches on two datasets for the Russian language and discuss their\nimpact on the performance of synset induction methods. Finally, we perform an\nextensive error analysis of each approach and discuss prominent alternative\nmethods for coping with the problem of the sparsity of the synonymy\ndictionaries.", "published": "2017-08-30 12:29:04", "link": "http://arxiv.org/abs/1708.09234v1", "categories": ["cs.CL", "I.2.6; I.5.3; I.2.4"], "primary_category": "cs.CL"}
{"title": "Fast(er) Exact Decoding and Global Training for Transition-Based\n  Dependency Parsing via a Minimal Feature Set", "abstract": "We first present a minimal feature set for transition-based dependency\nparsing, continuing a recent trend started by Kiperwasser and Goldberg (2016a)\nand Cross and Huang (2016a) of using bi-directional LSTM features. We plug our\nminimal feature set into the dynamic-programming framework of Huang and Sagae\n(2010) and Kuhlmann et al. (2011) to produce the first implementation of\nworst-case O(n^3) exact decoders for arc-hybrid and arc-eager transition\nsystems. With our minimal features, we also present O(n^3) global training\nmethods. Finally, using ensembles including our new parsers, we achieve the\nbest unlabeled attachment score reported (to our knowledge) on the Chinese\nTreebank and the \"second-best-in-class\" result on the English Penn Treebank.", "published": "2017-08-30 18:01:08", "link": "http://arxiv.org/abs/1708.09403v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "LangPro: Natural Language Theorem Prover", "abstract": "LangPro is an automated theorem prover for natural language\n(https://github.com/kovvalsky/LangPro). Given a set of premises and a\nhypothesis, it is able to prove semantic relations between them. The prover is\nbased on a version of analytic tableau method specially designed for natural\nlogic. The proof procedure operates on logical forms that preserve linguistic\nexpressions to a large extent. %This property makes the logical forms easily\nobtainable from syntactic trees. %, in particular, Combinatory Categorial\nGrammar derivation trees. The nature of proofs is deductive and transparent. On\nthe FraCaS and SICK textual entailment datasets, the prover achieves high\nresults comparable to state-of-the-art.", "published": "2017-08-30 18:22:28", "link": "http://arxiv.org/abs/1708.09417v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Inferring Narrative Causality between Event Pairs in Films", "abstract": "To understand narrative, humans draw inferences about the underlying\nrelations between narrative events. Cognitive theories of narrative\nunderstanding define these inferences as four different types of causality,\nthat include pairs of events A, B where A physically causes B (X drop, X\nbreak), to pairs of events where A causes emotional state B (Y saw X, Y felt\nfear). Previous work on learning narrative relations from text has either\nfocused on \"strict\" physical causality, or has been vague about what relation\nis being learned. This paper learns pairs of causal events from a corpus of\nfilm scene descriptions which are action rich and tend to be told in\nchronological order. We show that event pairs induced using our methods are of\nhigh quality and are judged to have a stronger causal relation than event pairs\nfrom Rel-grams.", "published": "2017-08-30 22:51:08", "link": "http://arxiv.org/abs/1708.09496v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Induction of Contingent Event Pairs from Film Scenes", "abstract": "Human engagement in narrative is partially driven by reasoning about\ndiscourse relations between narrative events, and the expectations about what\nis likely to happen next that results from such reasoning. Researchers in NLP\nhave tackled modeling such expectations from a range of perspectives, including\ntreating it as the inference of the contingent discourse relation, or as a type\nof common-sense causal reasoning. Our approach is to model likelihood between\nevents by drawing on several of these lines of previous work. We implement and\nevaluate different unsupervised methods for learning event pairs that are\nlikely to be contingent on one another. We refine event pairs that we learn\nfrom a corpus of film scene descriptions utilizing web search counts, and\nevaluate our results by collecting human judgments of contingency. Our results\nindicate that the use of web search counts increases the average accuracy of\nour best method to 85.64% over a baseline of 50%, as compared to an average\naccuracy of 75.15% without web search.", "published": "2017-08-30 23:02:06", "link": "http://arxiv.org/abs/1708.09497v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "End-to-end Learning for Short Text Expansion", "abstract": "Effectively making sense of short texts is a critical task for many real\nworld applications such as search engines, social media services, and\nrecommender systems. The task is particularly challenging as a short text\ncontains very sparse information, often too sparse for a machine learning\nalgorithm to pick up useful signals. A common practice for analyzing short text\nis to first expand it with external information, which is usually harvested\nfrom a large collection of longer texts. In literature, short text expansion\nhas been done with all kinds of heuristics. We propose an end-to-end solution\nthat automatically learns how to expand short text to optimize a given learning\ntask. A novel deep memory network is proposed to automatically find relevant\ninformation from a collection of longer documents and reformulate the short\ntext through a gating mechanism. Using short text classification as a\ndemonstrating task, we show that the deep memory network significantly\noutperforms classical text expansion methods with comprehensive experiments on\nreal world data sets.", "published": "2017-08-30 04:24:06", "link": "http://arxiv.org/abs/1709.00389v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Learning Fine-Grained Knowledge about Contingent Relations between\n  Everyday Events", "abstract": "Much of the user-generated content on social media is provided by ordinary\npeople telling stories about their daily lives. We develop and test a novel\nmethod for learning fine-grained common-sense knowledge from these stories\nabout contingent (causal and conditional) relationships between everyday\nevents. This type of knowledge is useful for text and story understanding,\ninformation extraction, question answering, and text summarization. We test and\ncompare different methods for learning contingency relation, and compare what\nis learned from topic-sorted story collections vs. general-domain stories. Our\nexperiments show that using topic-specific datasets enables learning\nfiner-grained knowledge about events and results in significant improvement\nover the baselines. An evaluation on Amazon Mechanical Turk shows 82% of the\nrelations between events that we learn from topic-sorted stories are judged as\ncontingent.", "published": "2017-08-30 20:01:34", "link": "http://arxiv.org/abs/1708.09450v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Inference of Fine-Grained Event Causality from Blogs and Films", "abstract": "Human understanding of narrative is mainly driven by reasoning about causal\nrelations between events and thus recognizing them is a key capability for\ncomputational models of language understanding. Computational work in this area\nhas approached this via two different routes: by focusing on acquiring a\nknowledge base of common causal relations between events, or by attempting to\nunderstand a particular story or macro-event, along with its storyline. In this\nposition paper, we focus on knowledge acquisition approach and claim that\nnewswire is a relatively poor source for learning fine-grained causal relations\nbetween everyday events. We describe experiments using an unsupervised method\nto learn causal relations between events in the narrative genres of\nfirst-person narratives and film scene descriptions. We show that our method\nlearns fine-grained causal relations, judged by humans as likely to be causal\nover 80% of the time. We also demonstrate that the learned event pairs do not\nexist in publicly available event-pair datasets extracted from newswire.", "published": "2017-08-30 20:12:01", "link": "http://arxiv.org/abs/1708.09453v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Automatically Generating Commit Messages from Diffs using Neural Machine\n  Translation", "abstract": "Commit messages are a valuable resource in comprehension of software\nevolution, since they provide a record of changes such as feature additions and\nbug repairs. Unfortunately, programmers often neglect to write good commit\nmessages. Different techniques have been proposed to help programmers by\nautomatically writing these messages. These techniques are effective at\ndescribing what changed, but are often verbose and lack context for\nunderstanding the rationale behind a change. In contrast, humans write messages\nthat are short and summarize the high level rationale. In this paper, we adapt\nNeural Machine Translation (NMT) to automatically \"translate\" diffs into commit\nmessages. We trained an NMT algorithm using a corpus of diffs and human-written\ncommit messages from the top 1k Github projects. We designed a filter to help\nensure that we only trained the algorithm on higher-quality commit messages.\nOur evaluation uncovered a pattern in which the messages we generate tend to be\neither very high or very low quality. Therefore, we created a quality-assurance\nfilter to detect cases in which we are unable to produce good messages, and\nreturn a warning instead.", "published": "2017-08-30 22:26:48", "link": "http://arxiv.org/abs/1708.09492v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
