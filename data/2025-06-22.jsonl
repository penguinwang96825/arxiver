{"title": "Shrinking the Generation-Verification Gap with Weak Verifiers", "abstract": "Verifiers can improve language model capabilities by scoring and ranking\nresponses from generated candidates. Currently, high-quality verifiers are\neither unscalable (e.g., humans) or limited in utility (e.g., tools like Lean).\nWhile LM judges and reward models have become broadly useful as general-purpose\nverifiers, a significant performance gap remains between them and oracle\nverifiers (verifiers with perfect accuracy). To help close this gap, we\nintroduce Weaver, a framework for designing a strong verifier by combining\nmultiple weak, imperfect verifiers. We find weighted ensembles of verifiers,\nwhich typically require learning from labeled data, significantly outperform\nunweighted combinations due to differences in verifier accuracies. To reduce\ndependency on labeled data, Weaver leverages weak supervision to estimate each\nverifier's accuracy and combines outputs into a unified score that better\nreflects true response quality. However, directly applying weak supervision\nalgorithms poses challenges, including inconsistent verifier output formats and\nhandling low-quality verifiers. Weaver addresses these using dataset statistics\nto normalize outputs and filter specific verifiers. We study Weaver's\neffectiveness in test-time repeated sampling, where a model generates multiple\ncandidate responses and selects one. Our evaluations show Weaver significantly\nimproves over Pass@1-performance when selecting the first candidate-across\nreasoning and math tasks, achieving o3-mini-level accuracy with Llama 3.3 70B\nInstruct as generator, and an ensemble of 70B or smaller judge and reward\nmodels as verifiers (87.7% average). This gain mirrors the jump between GPT-4o\nand o3-mini (69.0% vs. 86.7%), which required extensive finetuning and\npost-training. To reduce computational costs of verifier ensembles, we train a\n400M cross-encoder using Weaver's combined output scores.", "published": "2025-06-22 23:38:15", "link": "http://arxiv.org/abs/2506.18203v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Deciphering Emotions in Children Storybooks: A Comparative Analysis of Multimodal LLMs in Educational Applications", "abstract": "Emotion recognition capabilities in multimodal AI systems are crucial for\ndeveloping culturally responsive educational technologies, yet remain\nunderexplored for Arabic language contexts where culturally appropriate\nlearning tools are critically needed. This study evaluates the emotion\nrecognition performance of two advanced multimodal large language models,\nGPT-4o and Gemini 1.5 Pro, when processing Arabic children's storybook\nillustrations. We assessed both models across three prompting strategies\n(zero-shot, few-shot, and chain-of-thought) using 75 images from seven Arabic\nstorybooks, comparing model predictions with human annotations based on\nPlutchik's emotional framework. GPT-4o consistently outperformed Gemini across\nall conditions, achieving the highest macro F1-score of 59% with\nchain-of-thought prompting compared to Gemini's best performance of 43%. Error\nanalysis revealed systematic misclassification patterns, with valence\ninversions accounting for 60.7% of errors, while both models struggled with\nculturally nuanced emotions and ambiguous narrative contexts. These findings\nhighlight fundamental limitations in current models' cultural understanding and\nemphasize the need for culturally sensitive training approaches to develop\neffective emotion-aware educational technologies for Arabic-speaking learners.", "published": "2025-06-22 23:20:23", "link": "http://arxiv.org/abs/2506.18201v1", "categories": ["cs.CL", "cs.CV", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Prompt Engineering Techniques for Mitigating Cultural Bias Against Arabs and Muslims in Large Language Models: A Systematic Review", "abstract": "Large language models have demonstrated remarkable capabilities across\nvarious domains, yet concerns about cultural bias - particularly towards Arabs\nand Muslims - pose significant ethical challenges by perpetuating harmful\nstereotypes and marginalization. Despite growing recognition of bias in LLMs,\nprompt engineering strategies specifically addressing Arab and Muslim\nrepresentation remain understudied. This mixed-methods systematic review\nexamines such techniques, offering evidence-based guidance for researchers and\npractitioners. Following PRISMA guidelines and Kitchenham's systematic review\nmethodology, we analyzed 8 empirical studies published between 2021-2024\ninvestigating bias mitigation strategies. Our findings reveal five primary\nprompt engineering approaches: cultural prompting, affective priming,\nself-debiasing techniques, structured multi-step pipelines, and\nparameter-optimized continuous prompts. Although all approaches show potential\nfor reducing bias, effectiveness varied substantially across studies and bias\ntypes. Evidence suggests that certain bias types may be more resistant to\nprompt-based mitigation than others. Structured multi-step pipelines\ndemonstrated the highest overall effectiveness, achieving up to 87.7% reduction\nin bias, though they require greater technical expertise. Cultural prompting\noffers broader accessibility with substantial effectiveness. These results\nunderscore the accessibility of prompt engineering for mitigating cultural bias\nwithout requiring access to model parameters. The limited number of studies\nidentified highlights a significant research gap in this critical area. Future\nresearch should focus on developing culturally adaptive prompting techniques,\ncreating Arab and Muslim-specific evaluation resources, and integrating prompt\nengineering with complementary debiasing methods to address deeper stereotypes\nwhile maintaining model utility.", "published": "2025-06-22 23:15:25", "link": "http://arxiv.org/abs/2506.18199v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "primary_category": "cs.CL"}
{"title": "CareLab at #SMM4H-HeaRD 2025: Insomnia Detection and Food Safety Event Extraction with Domain-Aware Transformers", "abstract": "This paper presents our system for the SMM4H-HeaRD 2025 shared tasks,\nspecifically Task 4 (Subtasks 1, 2a, and 2b) and Task 5 (Subtasks 1 and 2).\nTask 4 focused on detecting mentions of insomnia in clinical notes, while Task\n5 addressed the extraction of food safety events from news articles. We\nparticipated in all subtasks and report key findings across them, with\nparticular emphasis on Task 5 Subtask 1, where our system achieved strong\nperformance-securing first place with an F1 score of 0.958 on the test set. To\nattain this result, we employed encoder-based models (e.g., RoBERTa), alongside\nGPT-4 for data augmentation. This paper outlines our approach, including\npreprocessing, model architecture, and subtask-specific adaptations", "published": "2025-06-22 21:56:59", "link": "http://arxiv.org/abs/2506.18185v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Reasoning about Uncertainty: Do Reasoning Models Know When They Don't Know?", "abstract": "Reasoning language models have set state-of-the-art (SOTA) records on many\nchallenging benchmarks, enabled by multi-step reasoning induced using\nreinforcement learning. However, like previous language models, reasoning\nmodels are prone to generating confident, plausible responses that are\nincorrect (hallucinations). Knowing when and how much to trust these models is\ncritical to the safe deployment of reasoning models in real-world applications.\nTo this end, we explore uncertainty quantification of reasoning models in this\nwork. Specifically, we ask three fundamental questions: First, are reasoning\nmodels well-calibrated? Second, does deeper reasoning improve model\ncalibration? Finally, inspired by humans' innate ability to double-check their\nthought processes to verify the validity of their answers and their confidence,\nwe ask: can reasoning models improve their calibration by explicitly reasoning\nabout their chain-of-thought traces? We introduce introspective uncertainty\nquantification (UQ) to explore this direction. In extensive evaluations on SOTA\nreasoning models across a broad range of benchmarks, we find that reasoning\nmodels: (i) are typically overconfident, with self-verbalized confidence\nestimates often greater than 85% particularly for incorrect responses, (ii)\nbecome even more overconfident with deeper reasoning, and (iii) can become\nbetter calibrated through introspection (e.g., o3-Mini and DeepSeek R1) but not\nuniformly (e.g., Claude 3.7 Sonnet becomes more poorly calibrated). Lastly, we\nconclude with important research directions to design necessary UQ benchmarks\nand improve the calibration of reasoning models.", "published": "2025-06-22 21:46:42", "link": "http://arxiv.org/abs/2506.18183v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "QuranMorph: Morphologically Annotated Quranic Corpus", "abstract": "We present the QuranMorph corpus, a morphologically annotated corpus for the\nQuran (77,429 tokens). Each token in the QuranMorph was manually lemmatized and\ntagged with its part-of-speech by three expert linguists. The lemmatization\nprocess utilized lemmas from Qabas, an Arabic lexicographic database linked\nwith 110 lexicons and corpora of 2 million tokens. The part-of-speech tagging\nwas performed using the fine-grained SAMA/Qabas tagset, which encompasses 40\ntags. As shown in this paper, this rich lemmatization and POS tagset enabled\nthe QuranMorph corpus to be inter-linked with many linguistic resources. The\ncorpus is open-source and publicly available as part of the SinaLab resources\nat (https://sina.birzeit.edu/quran)", "published": "2025-06-22 19:34:09", "link": "http://arxiv.org/abs/2506.18148v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Sparse Feature Coactivation Reveals Composable Semantic Modules in Large Language Models", "abstract": "We identify semantically coherent, context-consistent network components in\nlarge language models (LLMs) using coactivation of sparse autoencoder (SAE)\nfeatures collected from just a handful of prompts. Focusing on country-relation\ntasks, we show that ablating semantic components for countries and relations\nchanges model outputs in predictable ways, while amplifying these components\ninduces counterfactual responses. Notably, composing relation and country\ncomponents yields compound counterfactual outputs. We find that, whereas most\ncountry components emerge from the very first layer, the more abstract relation\ncomponents are concentrated in later layers. Furthermore, within relation\ncomponents themselves, nodes from later layers tend to have a stronger causal\nimpact on model outputs. Overall, these findings suggest a modular organization\nof knowledge within LLMs and advance methods for efficient, targeted model\nmanipulation.", "published": "2025-06-22 19:01:13", "link": "http://arxiv.org/abs/2506.18141v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SE-Merging: A Self-Enhanced Approach for Dynamic Model Merging", "abstract": "Model merging has gained increasing attention due to its intriguing property:\ninterpolating the parameters of different task-specific fine-tuned models leads\nto multi-task abilities. However, despite its empirical success, the underlying\nmechanisms of model merging remain poorly understood. In this work, we delve\ninto the mechanism behind model merging from a representation perspective. Our\nanalysis reveals that model merging achieves multi-task abilities through two\nkey capabilities: i) distinguishing samples from different tasks, and ii)\nadapting to the corresponding expert model for each sample. These two\ncapabilities allow the merged model to retain task-specific expertise, enabling\nefficient multi-task adaptation. Building on these insights, we propose\n\\texttt{SE-Merging}, a self-enhanced model merging framework that leverages\nthese two characteristics to dynamically identify the corresponding task for\neach sample and then adaptively rescales the merging coefficients to further\nenhance task-specific expertise in the merged model. Notably,\n\\texttt{SE-Merging} achieves dynamic model merging without additional training.\nExtensive experiments demonstrate that \\texttt{SE-Merging} achieves significant\nperformance improvements while remaining compatible with existing model merging\ntechniques.", "published": "2025-06-22 18:38:41", "link": "http://arxiv.org/abs/2506.18135v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "$\u03c6^{\\infty}$: Clause Purification, Embedding Realignment, and the Total Suppression of the Em Dash in Autoregressive Language Models", "abstract": "We identify a critical vulnerability in autoregressive transformer language\nmodels where the em dash token induces recursive semantic drift, leading to\nclause boundary hallucination and embedding space entanglement. Through formal\nanalysis of token-level perturbations in semantic lattices, we demonstrate that\nem dash insertion fundamentally alters the model's latent representations,\ncausing compounding errors in long-form generation. We propose a novel solution\ncombining symbolic clause purification via the phi-infinity operator with\ntargeted embedding matrix realignment. Our approach enables total suppression\nof problematic tokens without requiring model retraining, while preserving\nsemantic coherence through fixed-point convergence guarantees. Experimental\nvalidation shows significant improvements in generation consistency and topic\nmaintenance. This work establishes a general framework for identifying and\nmitigating token-level vulnerabilities in foundation models, with immediate\nimplications for AI safety, model alignment, and robust deployment of large\nlanguage models in production environments. The methodology extends beyond\npunctuation to address broader classes of recursive instabilities in neural\ntext generation systems.", "published": "2025-06-22 18:27:39", "link": "http://arxiv.org/abs/2506.18129v1", "categories": ["cs.CL", "cs.AI", "68T50, 68T45, 03B70", "I.2.6; I.2.7; I.2.3; F.4.1"], "primary_category": "cs.CL"}
{"title": "The Syntactic Acceptability Dataset (Preview): A Resource for Machine Learning and Linguistic Analysis of English", "abstract": "We present a preview of the Syntactic Acceptability Dataset, a resource being\ndesigned for both syntax and computational linguistics research. In its current\nform, the dataset comprises 1,000 English sequences from the syntactic\ndiscourse: Half from textbooks and half from the journal Linguistic Inquiry,\nthe latter to ensure a representation of the contemporary discourse. Each entry\nis labeled with its grammatical status (\"well-formedness\" according to\nsyntactic formalisms) extracted from the literature, as well as its\nacceptability status (\"intuitive goodness\" as determined by native speakers)\nobtained through crowdsourcing, with highest experimental standards. Even in\nits preliminary form, this dataset stands as the largest of its kind that is\npublicly accessible. We also offer preliminary analyses addressing three\ndebates in linguistics and computational linguistics: We observe that\ngrammaticality and acceptability judgments converge in about 83% of the cases\nand that \"in-betweenness\" occurs frequently. This corroborates existing\nresearch. We also find that while machine learning models struggle with\npredicting grammaticality, they perform considerably better in predicting\nacceptability. This is a novel finding. Future work will focus on expanding the\ndataset.", "published": "2025-06-22 18:03:49", "link": "http://arxiv.org/abs/2506.18120v1", "categories": ["cs.CL", "68T50", "I.2.7; I.2.6; H.3.1"], "primary_category": "cs.CL"}
{"title": "Mental Health Equity in LLMs: Leveraging Multi-Hop Question Answering to Detect Amplified and Silenced Perspectives", "abstract": "Large Language Models (LLMs) in mental healthcare risk propagating biases\nthat reinforce stigma and harm marginalized groups. While previous research\nidentified concerning trends, systematic methods for detecting intersectional\nbiases remain limited. This work introduces a multi-hop question answering\n(MHQA) framework to explore LLM response biases in mental health discourse. We\nanalyze content from the Interpretable Mental Health Instruction (IMHI) dataset\nacross symptom presentation, coping mechanisms, and treatment approaches. Using\nsystematic tagging across age, race, gender, and socioeconomic status, we\ninvestigate bias patterns at demographic intersections. We evaluate four LLMs:\nClaude 3.5 Sonnet, Jamba 1.6, Gemma 3, and Llama 4, revealing systematic\ndisparities across sentiment, demographics, and mental health conditions. Our\nMHQA approach demonstrates superior detection compared to conventional methods,\nidentifying amplification points where biases magnify through sequential\nreasoning. We implement two debiasing techniques: Roleplay Simulation and\nExplicit Bias Reduction, achieving 66-94% bias reductions through few-shot\nprompting with BBQ dataset examples. These findings highlight critical areas\nwhere LLMs reproduce mental healthcare biases, providing actionable insights\nfor equitable AI development.", "published": "2025-06-22 18:00:16", "link": "http://arxiv.org/abs/2506.18116v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Chengyu-Bench: Benchmarking Large Language Models for Chinese Idiom Understanding and Use", "abstract": "Chinese idioms (Chengyu) are concise four-character expressions steeped in\nhistory and culture, whose literal translations often fail to capture their\nfull meaning. This complexity makes them challenging for language models to\ninterpret and use correctly. Existing benchmarks focus on narrow tasks -\nmultiple-choice cloze tests, isolated translation, or simple paraphrasing. We\nintroduce Chengyu-Bench, a comprehensive benchmark featuring three tasks: (1)\nEvaluative Connotation, classifying idioms as positive or negative; (2)\nAppropriateness, detecting incorrect idiom usage in context; and (3) Open\nCloze, filling blanks in longer passages without options. Chengyu-Bench\ncomprises 2,937 human-verified examples covering 1,765 common idioms sourced\nfrom diverse corpora. We evaluate leading LLMs and find they achieve over 95%\naccuracy on Evaluative Connotation, but only ~85% on Appropriateness and ~40%\ntop-1 accuracy on Open Cloze. Error analysis reveals that most mistakes arise\nfrom fundamental misunderstandings of idiom meanings. Chengyu-Bench\ndemonstrates that while LLMs can reliably gauge idiom sentiment, they still\nstruggle to grasp the cultural and contextual nuances essential for proper\nusage. The benchmark and source code are available at:\nhttps://github.com/sofyc/ChengyuBench.", "published": "2025-06-22 17:26:09", "link": "http://arxiv.org/abs/2506.18105v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "InspireDebate: Multi-Dimensional Subjective-Objective Evaluation-Guided Reasoning and Optimization for Debating", "abstract": "With the rapid advancements in large language models (LLMs), debating tasks,\nsuch as argument quality assessment and debate process simulation, have made\nsignificant progress. However, existing LLM-based debating systems focus on\nresponding to specific arguments while neglecting objective assessments such as\nauthenticity and logical validity. Furthermore, these systems lack a structured\napproach to optimize across various dimensions$-$including evaluation metrics,\nchain-of-thought (CoT) reasoning, and multi-turn debate refinement$-$thereby\nlimiting their effectiveness. To address these interconnected challenges, we\npropose a dual-component framework: (1) $\\textbf{InspireScore}$, a novel\nevaluation system that establishes a multi-dimensional assessment architecture\nincorporating four subjective criteria (emotional appeal, argument clarity,\nargument arrangement, and topic relevance) alongside two objective metrics\n(fact authenticity and logical validity); and (2) $\\textbf{InspireDebate}$, an\noptimized debating framework employing a phased optimization approach through\nCoT reasoning enhancement, multi-dimensional Direct Preference Optimization\n(DPO), and real-time knowledge grounding via web-based Retrieval Augmented\nGeneration (Web-RAG). Empirical evaluations demonstrate that\n$\\textbf{InspireScore}$ achieves 44$\\%$ higher correlation with expert\njudgments compared to existing methods, while $\\textbf{InspireDebate}$ shows\nsignificant improvements, outperforming baseline models by 57$\\%$. Source code\nis available at https://github.com/fywang12/InspireDebate.", "published": "2025-06-22 17:14:29", "link": "http://arxiv.org/abs/2506.18102v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Prompt-Based and Fine-Tuned Approaches to Czech Anaphora Resolution", "abstract": "Anaphora resolution plays a critical role in natural language understanding,\nespecially in morphologically rich languages like Czech. This paper presents a\ncomparative evaluation of two modern approaches to anaphora resolution on Czech\ntext: prompt engineering with large language models (LLMs) and fine-tuning\ncompact generative models. Using a dataset derived from the Prague Dependency\nTreebank, we evaluate several instruction-tuned LLMs, including Mistral Large 2\nand Llama 3, using a series of prompt templates. We compare them against\nfine-tuned variants of the mT5 and Mistral models that we trained specifically\nfor Czech anaphora resolution. Our experiments demonstrate that while prompting\nyields promising few-shot results (up to 74.5% accuracy), the fine-tuned\nmodels, particularly mT5-large, outperform them significantly, achieving up to\n88% accuracy while requiring fewer computational resources. We analyze\nperformance across different anaphora types, antecedent distances, and source\ncorpora, highlighting key strengths and trade-offs of each approach.", "published": "2025-06-22 16:32:57", "link": "http://arxiv.org/abs/2506.18091v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation", "abstract": "Simulation-based data synthesis has emerged as a powerful paradigm for\nenhancing real-world robotic manipulation. However, existing synthetic datasets\nremain insufficient for robust bimanual manipulation due to two challenges: (1)\nthe lack of an efficient, scalable data generation method for novel tasks, and\n(2) oversimplified simulation environments that fail to capture real-world\ncomplexity. We present RoboTwin 2.0, a scalable simulation framework that\nenables automated, large-scale generation of diverse and realistic data, along\nwith unified evaluation protocols for dual-arm manipulation. We first construct\nRoboTwin-OD, a large-scale object library comprising 731 instances across 147\ncategories, each annotated with semantic and manipulation-relevant labels.\nBuilding on this foundation, we develop an expert data synthesis pipeline that\ncombines multimodal large language models (MLLMs) with simulation-in-the-loop\nrefinement to generate task-level execution code automatically. To improve\nsim-to-real transfer, RoboTwin 2.0 incorporates structured domain randomization\nalong five axes: clutter, lighting, background, tabletop height and language\ninstructions, thereby enhancing data diversity and policy robustness. We\ninstantiate this framework across 50 dual-arm tasks spanning five robot\nembodiments, and pre-collect over 100,000 domain-randomized expert\ntrajectories. Empirical results show a 10.9% gain in code generation success\nand improved generalization to novel real-world scenarios. A VLA model\nfine-tuned on our dataset achieves a 367% relative improvement (42.0% vs. 9.0%)\non unseen scene real-world tasks, while zero-shot models trained solely on our\nsynthetic data achieve a 228% relative gain, highlighting strong generalization\nwithout real-world supervision. We release the data generator, benchmark,\ndataset, and code to support scalable research in robust bimanual manipulation.", "published": "2025-06-22 16:26:53", "link": "http://arxiv.org/abs/2506.18088v1", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV", "cs.MA"], "primary_category": "cs.RO"}
{"title": "Statistical Multicriteria Evaluation of LLM-Generated Text", "abstract": "Assessing the quality of LLM-generated text remains a fundamental challenge\nin natural language processing. Current evaluation approaches often rely on\nisolated metrics or simplistic aggregations that fail to capture the nuanced\ntrade-offs between coherence, diversity, fluency, and other relevant indicators\nof text quality. In this work, we adapt a recently proposed framework for\nstatistical inference based on Generalized Stochastic Dominance (GSD) that\naddresses three critical limitations in existing benchmarking methodologies:\nthe inadequacy of single-metric evaluation, the incompatibility between\ncardinal automatic metrics and ordinal human judgments, and the lack of\ninferential statistical guarantees. The GSD-front approach enables simultaneous\nevaluation across multiple quality dimensions while respecting their different\nmeasurement scales, building upon partial orders of decoding strategies, thus\navoiding arbitrary weighting of the involved metrics. By applying this\nframework to evaluate common decoding strategies against human-generated text,\nwe demonstrate its ability to identify statistically significant performance\ndifferences while accounting for potential deviations from the i.i.d.\nassumption of the sampling design.", "published": "2025-06-22 16:08:44", "link": "http://arxiv.org/abs/2506.18082v1", "categories": ["cs.CL", "stat.AP"], "primary_category": "cs.CL"}
{"title": "The Democratic Paradox in Large Language Models' Underestimation of Press Freedom", "abstract": "As Large Language Models (LLMs) increasingly mediate global information\naccess for millions of users worldwide, their alignment and biases have the\npotential to shape public understanding and trust in fundamental democratic\ninstitutions, such as press freedom. In this study, we uncover three systematic\ndistortions in the way six popular LLMs evaluate press freedom in 180 countries\ncompared to expert assessments of the World Press Freedom Index (WPFI). The six\nLLMs exhibit a negative misalignment, consistently underestimating press\nfreedom, with individual models rating between 71% to 93% of countries as less\nfree. We also identify a paradoxical pattern we term differential misalignment:\nLLMs disproportionately underestimate press freedom in countries where it is\nstrongest. Additionally, five of the six LLMs exhibit positive home bias,\nrating their home countries' press freedoms more favorably than would be\nexpected given their negative misalignment with the human benchmark. In some\ncases, LLMs rate their home countries between 7% to 260% more positively than\nexpected. If LLMs are set to become the next search engines and some of the\nmost important cultural tools of our time, they must ensure accurate\nrepresentations of the state of our human and civic rights globally.", "published": "2025-06-22 14:10:16", "link": "http://arxiv.org/abs/2506.18045v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "K.4; I.2.7; I.2.0"], "primary_category": "cs.CY"}
{"title": "Markov-Enhanced Clustering for Long Document Summarization: Tackling the 'Lost in the Middle' Challenge with Large Language Models", "abstract": "The rapid expansion of information from diverse sources has heightened the\nneed for effective automatic text summarization, which condenses documents into\nshorter, coherent texts. Summarization methods generally fall into two\ncategories: extractive, which selects key segments from the original text, and\nabstractive, which generates summaries by rephrasing the content coherently.\nLarge language models have advanced the field of abstractive summarization, but\nthey are resourceintensive and face significant challenges in retaining key\ninformation across lengthy documents, which we call being \"lost in the middle\".\nTo address these issues, we propose a hybrid summarization approach that\ncombines extractive and abstractive techniques. Our method splits the document\ninto smaller text chunks, clusters their vector embeddings, generates a summary\nfor each cluster that represents a key idea in the document, and constructs the\nfinal summary by relying on a Markov chain graph when selecting the semantic\norder of ideas.", "published": "2025-06-22 13:34:58", "link": "http://arxiv.org/abs/2506.18036v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Splitformer: An improved early-exit architecture for automatic speech recognition on edge devices", "abstract": "The ability to dynamically adjust the computational load of neural models\nduring inference in a resource aware manner is crucial for on-device processing\nscenarios, characterised by limited and time-varying computational resources.\nEarly-exit architectures represent an elegant and effective solution, since\nthey can process the input with a subset of their layers, exiting at\nintermediate branches (the upmost layers are hence removed from the model).\n  From a different perspective, for automatic speech recognition applications\nthere are memory-efficient neural architectures that apply variable frame rate\nanalysis, through downsampling/upsampling operations in the middle layers,\nreducing the overall number of operations and improving significantly the\nperformance on well established benchmarks. One example is the Zipformer.\nHowever, these architectures lack the modularity necessary to inject early-exit\nbranches.\n  With the aim of improving the performance in early-exit models, we propose\nintroducing parallel layers in the architecture that process downsampled\nversions of their inputs. % in conjunction with standard processing layers. We\nshow that in this way the speech recognition performance on standard benchmarks\nsignificantly improve, at the cost of a small increase in the overall number of\nmodel parameters but without affecting the inference time.", "published": "2025-06-22 13:34:18", "link": "http://arxiv.org/abs/2506.18035v1", "categories": ["cs.CL", "cs.SD", "eess.AS", "68T50 (Primary)", "I.2.7; I.5.4"], "primary_category": "cs.CL"}
{"title": "PDF Retrieval Augmented Question Answering", "abstract": "This paper presents an advancement in Question-Answering (QA) systems using a\nRetrieval Augmented Generation (RAG) framework to enhance information\nextraction from PDF files. Recognizing the richness and diversity of data\nwithin PDFs--including text, images, vector diagrams, graphs, and tables--poses\nunique challenges for existing QA systems primarily designed for textual\ncontent. We seek to develop a comprehensive RAG-based QA system that will\neffectively address complex multimodal questions, where several data types are\ncombined in the query. This is mainly achieved by refining approaches to\nprocessing and integrating non-textual elements in PDFs into the RAG framework\nto derive precise and relevant answers, as well as fine-tuning large language\nmodels to better adapt to our system. We provide an in-depth experimental\nevaluation of our solution, demonstrating its capability to extract accurate\ninformation that can be applied to different types of content across PDFs. This\nwork not only pushes the boundaries of retrieval-augmented QA systems but also\nlays a foundation for further research in multimodal data integration and\nprocessing.", "published": "2025-06-22 13:14:19", "link": "http://arxiv.org/abs/2506.18027v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PP-DocBee2: Improved Baselines with Efficient Data for Multimodal Document Understanding", "abstract": "This report introduces PP-DocBee2, an advanced version of the PP-DocBee,\ndesigned to enhance multimodal document understanding. Built on a large\nmultimodal model architecture, PP-DocBee2 addresses the limitations of its\npredecessor through key technological improvements, including enhanced\nsynthetic data quality, improved visual feature fusion strategy, and optimized\ninference methodologies. These enhancements yield an $11.4\\%$ performance boost\non internal benchmarks for Chinese business documents, and reduce inference\nlatency by $73.0\\%$ to the vanilla version. A key innovation of our work is a\ndata quality optimization strategy for multimodal document tasks. By employing\na large-scale multimodal pre-trained model to evaluate data, we apply a novel\nstatistical criterion to filter outliers, ensuring high-quality training data.\nInspired by insights into underutilized intermediate features in multimodal\nmodels, we enhance the ViT representational capacity by decomposing it into\nlayers and applying a novel feature fusion strategy to improve complex\nreasoning. The source code and pre-trained model are available at\n\\href{https://github.com/PaddlePaddle/PaddleMIX}{https://github.com/PaddlePaddle/PaddleMIX}.", "published": "2025-06-22 13:06:13", "link": "http://arxiv.org/abs/2506.18023v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "A Comprehensive Graph Framework for Question Answering with Mode-Seeking Preference Alignment", "abstract": "Recent advancements in retrieval-augmented generation (RAG) have enhanced\nlarge language models in question answering by integrating external knowledge.\nHowever, challenges persist in achieving global understanding and aligning\nresponses with human ethical and quality preferences. To address these issues,\nwe propose GraphMPA, a comprehensive graph-based framework with mode-seeking\npreference alignment. Our approach constructs a hierarchical document graph\nusing a general similarity measurement, mimicking human cognitive processes for\ninformation understanding and synthesis. Additionally, we introduce\nmode-seeking preference optimization to better align model outputs with human\npreferences through probability-matching constraints. Extensive experiments on\nsix datasets demonstrate the effectiveness of our\n\\href{https://github.com/tangquanwei/GraphMPA}{GraphMPA}.", "published": "2025-06-22 09:08:44", "link": "http://arxiv.org/abs/2506.17951v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Scatter-Based Innovation Propagation in Large Language Models for Multi-Stage Process Adaptation", "abstract": "Large Language Models (LLMs) exhibit strong capabilities in reproducing and\nextending patterns observed during pretraining but often struggle to generalize\nnovel ideas beyond their original context. This paper addresses the challenge\nof applying such localized innovations - introduced at a specific stage or\ncomponent - to other parts of a multi-stage process. We propose a scatter-based\ninnovation expansion model (innovation scatter model) that guides the LLM\nthrough a four-step process: (1) identifying the core innovation by comparing\nthe user's input with its surrounding context, (2) generalizing the innovation\nby removing references to specific stages or components, (3) determining\nwhether the generalized innovation applies to a broader scope beyond the\noriginal stage, and (4) systematically applying it to other structurally\nsimilar stages using the LLM. This model leverages structural redundancy across\nstages to improve the applicability of novel ideas. Verification results\ndemonstrate that the innovation scatter model enables LLMs to extend\ninnovations across structurally similar stages, thereby enhancing\ngeneralization and reuse.", "published": "2025-06-22 09:02:31", "link": "http://arxiv.org/abs/2506.17949v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Tutorial: $\\varphi$-Transductions in OpenFst via the Gallic Semiring", "abstract": "OpenFst, a popular finite-state transducer library, supports\n$\\varphi$-transitions but, due to an implementation constraint, they cannot be\nused with transducers in a straightforward way.\n  In this short tutorial, we describe how one can use other functionality\nprovided by OpenFst (namely, the Gallic semiring) to correctly implement\n$\\varphi$-transductions and demonstrate it by implementing the MaxMatch\n(WordPiece) tokenization algorithm (Devlin et al., 2019; Song et al., 2021).\nAccompanying self-contained code examples are provided.\nhttps://www.openfst.org/twiki/pub/Contrib/FstContrib/phi_transduction_tutorial_code.tgz", "published": "2025-06-22 08:24:04", "link": "http://arxiv.org/abs/2506.17942v1", "categories": ["cs.FL", "cs.CL"], "primary_category": "cs.FL"}
{"title": "Evolving Prompts In-Context: An Open-ended, Self-replicating Perspective", "abstract": "We propose a novel prompt design paradigm that challenges conventional wisdom\nin large language model (LLM) prompting. While conventional wisdom prioritizes\nwell-crafted instructions and demonstrations for in-context learning (ICL), we\nshow that pruning random demonstrations into seemingly incoherent \"gibberish\"\ncan remarkably improve performance across diverse tasks. Notably, the\n\"gibberish\" always matches or surpasses state-of-the-art automatic prompt\noptimization techniques, achieving substantial gains regardless of LLM\nalignment. Nevertheless, discovering an effective pruning strategy is\nnon-trivial, as existing attribution methods and prompt compression algorithms\nfail to deliver robust results, let alone human intuition. In terms of this, we\npropose a self-discover prompt optimization framework, PromptQuine, an\nevolutionary search framework that automatically searches for the pruning\nstrategy by itself using only low-data regimes. Much like the emergent\ncomplexity in nature--such as symbiosis and self-organization--arising in\nresponse to resource constraints, our framework evolves and refines\nunconventional yet highly effective prompts by leveraging only the tokens\npresent within the context. We demonstrate its effectiveness across\nclassification, multi-choice question answering, generation and math reasoning\ntasks across LLMs, while achieving decent runtime efficiency. We hope our\nfindings can guide mechanistic studies on in-context learning, and provide a\ncall to action, to pave the way for more open-ended search algorithms for more\neffective LLM prompting.", "published": "2025-06-22 07:53:07", "link": "http://arxiv.org/abs/2506.17930v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.NE", "cs.RO"], "primary_category": "cs.AI"}
{"title": "Multi-turn Jailbreaking via Global Refinement and Active Fabrication", "abstract": "Large Language Models (LLMs) have achieved exceptional performance across a\nwide range of tasks. However, they still pose significant safety risks due to\nthe potential misuse for malicious purposes. Jailbreaks, which aim to elicit\nmodels to generate harmful content, play a critical role in identifying the\nunderlying security threats. Recent jailbreaking primarily focuses on\nsingle-turn scenarios, while the more complicated multi-turn scenarios remain\nunderexplored. Moreover, existing multi-turn jailbreaking techniques struggle\nto adapt to the evolving dynamics of dialogue as the interaction progresses. To\naddress this limitation, we propose a novel multi-turn jailbreaking method that\nrefines the jailbreaking path globally at each interaction. We also actively\nfabricate model responses to suppress safety-related warnings, thereby\nincreasing the likelihood of eliciting harmful outputs in subsequent questions.\nExperimental results demonstrate the superior performance of our method\ncompared with existing single-turn and multi-turn jailbreaking techniques\nacross six state-of-the-art LLMs. Our code is publicly available at\nhttps://github.com/Ytang520/Multi-Turn_jailbreaking_Global-Refinment_and_Active-Fabrication.", "published": "2025-06-22 03:15:05", "link": "http://arxiv.org/abs/2506.17881v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "How Alignment Shrinks the Generative Horizon", "abstract": "Despite their impressive capabilities, aligned large language models (LLMs)\noften generate outputs that lack diversity. What drives this stability in the\ngeneration? We investigate this phenomenon through the lens of probability\nconcentration in the model's output distribution. To quantify this\nconcentration, we introduce the Branching Factor (BF) -- a token-invariant\nmeasure of the effective number of plausible next steps during generation. Our\nempirical analysis reveals two key findings: (1) BF often decreases as\ngeneration progresses, suggesting that LLMs become more predictable as they\ngenerate. (2) alignment tuning substantially sharpens the model's output\ndistribution from the outset, reducing BF by nearly an order of magnitude\n(e.g., from 12 to 1.2) relative to base models. This stark reduction helps\nexplain why aligned models often appear less sensitive to decoding strategies.\nBuilding on this insight, we find this stability has surprising implications\nfor complex reasoning. Aligned Chain-of-Thought (CoT) models (e.g.,\nDeepSeek-distilled models), for instance, leverage this effect; by generating\nlonger reasoning chains, they push generation into later, more deterministic\n(lower BF) stages, resulting in more stable outputs. We hypothesize that\nalignment tuning does not fundamentally change a model's behavior, but instead\nsteers it toward stylistic tokens (e.g., \"Sure\") that unlock low-entropy\ntrajectories already present in the base model. This view is supported by\nnudging experiments, which show that prompting base models with such tokens can\nsimilarly reduce BF. Together, our findings establish BF as a powerful\ndiagnostic for understanding and controlling LLM outputs - clarifying how\nalignment reduces variability, how CoT promotes stable generations, and how\nbase models can be steered away from diversity.", "published": "2025-06-22 02:00:37", "link": "http://arxiv.org/abs/2506.17871v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "QueueEDIT: Structural Self-Correction for Sequential Model Editing in LLMs", "abstract": "Recently, large language models (LLMs) have demonstrated impressive results\nbut still suffer from hallucinations. Model editing has been proposed to\ncorrect factual inaccuracies in LLMs. A challenging case is sequential model\nediting (SME), which aims to rectify errors continuously rather than treating\nthem as a one-time task. During SME, the general capabilities of LLMs can be\nnegatively affected due to the introduction of new parameters. In this paper,\nwe propose a queue-based self-correction framework (QueueEDIT) that not only\nenhances SME performance by addressing long-sequence dependency but also\nmitigates the impact of parameter bias on the general capabilities of LLMs.\nSpecifically, we first introduce a structural mapping editing loss to map the\ntriplets to the knowledge-sensitive neurons within the Transformer layers of\nLLMs. We then store the located parameters for each piece of edited knowledge\nin a queue and dynamically align previously edited parameters. In each edit, we\nselect queue parameters most relevant to the currently located parameters to\ndetermine whether previous knowledge needs realignment. Irrelevant parameters\nin the queue are frozen, and we update the parameters at the queue head to the\nLLM to ensure they do not harm general abilities. Experiments show that our\nframework significantly outperforms strong baselines across various SME\nsettings and maintains competitiveness in single-turn editing. The resulting\nLLMs also preserve high capabilities in general NLP tasks throughout the SME\nprocess.", "published": "2025-06-22 00:58:07", "link": "http://arxiv.org/abs/2506.17864v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLMs for Customized Marketing Content Generation and Evaluation at Scale", "abstract": "Offsite marketing is essential in e-commerce, enabling businesses to reach\ncustomers through external platforms and drive traffic to retail websites.\nHowever, most current offsite marketing content is overly generic,\ntemplate-based, and poorly aligned with landing pages, limiting its\neffectiveness. To address these limitations, we propose MarketingFM, a\nretrieval-augmented system that integrates multiple data sources to generate\nkeyword-specific ad copy with minimal human intervention. We validate\nMarketingFM via offline human and automated evaluations and large-scale online\nA/B tests. In one experiment, keyword-focused ad copy outperformed templates,\nachieving up to 9% higher CTR, 12% more impressions, and 0.38% lower CPC,\ndemonstrating gains in ad ranking and cost efficiency. Despite these gains,\nhuman review of generated ads remains costly. To address this, we propose\nAutoEval-Main, an automated evaluation system that combines rule-based metrics\nwith LLM-as-a-Judge techniques to ensure alignment with marketing principles.\nIn experiments with large-scale human annotations, AutoEval-Main achieved\n89.57% agreement with human reviewers. Building on this, we propose\nAutoEval-Update, a cost-efficient LLM-human collaborative framework to\ndynamically refine evaluation prompts and adapt to shifting criteria with\nminimal human input. By selectively sampling representative ads for human\nreview and using a critic LLM to generate alignment reports, AutoEval-Update\nimproves evaluation consistency while reducing manual effort. Experiments show\nthe critic LLM suggests meaningful refinements, improving LLM-human agreement.\nNonetheless, human oversight remains essential for setting thresholds and\nvalidating refinements before deployment.", "published": "2025-06-22 00:28:35", "link": "http://arxiv.org/abs/2506.17863v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep Learning-based Alignment Measurement in Knee Radiographs", "abstract": "Radiographic knee alignment (KA) measurement is important for predicting\njoint health and surgical outcomes after total knee replacement. Traditional\nmethods for KA measurements are manual, time-consuming and require long-leg\nradiographs. This study proposes a deep learning-based method to measure KA in\nanteroposterior knee radiographs via automatically localized knee anatomical\nlandmarks. Our method builds on hourglass networks and incorporates an\nattention gate structure to enhance robustness and focus on key anatomical\nfeatures. To our knowledge, this is the first deep learning-based method to\nlocalize over 100 knee anatomical landmarks to fully outline the knee shape\nwhile integrating KA measurements on both pre-operative and post-operative\nimages. It provides highly accurate and reliable anatomical varus/valgus KA\nmeasurements using the anatomical tibiofemoral angle, achieving mean absolute\ndifferences ~1{\\deg} when compared to clinical ground truth measurements.\nAgreement between automated and clinical measurements was excellent\npre-operatively (intra-class correlation coefficient (ICC) = 0.97) and good\npost-operatively (ICC = 0.86). Our findings demonstrate that KA assessment can\nbe automated with high accuracy, creating opportunities for digitally enhanced\nclinical workflows.", "published": "2025-06-22 23:57:46", "link": "http://arxiv.org/abs/2506.18209v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Multimodal Fusion SLAM with Fourier Attention", "abstract": "Visual SLAM is particularly challenging in environments affected by noise,\nvarying lighting conditions, and darkness. Learning-based optical flow\nalgorithms can leverage multiple modalities to address these challenges, but\ntraditional optical flow-based visual SLAM approaches often require significant\ncomputational resources.To overcome this limitation, we propose FMF-SLAM, an\nefficient multimodal fusion SLAM method that utilizes fast Fourier transform\n(FFT) to enhance the algorithm efficiency. Specifically, we introduce a novel\nFourier-based self-attention and cross-attention mechanism to extract features\nfrom RGB and depth signals. We further enhance the interaction of multimodal\nfeatures by incorporating multi-scale knowledge distillation across modalities.\nWe also demonstrate the practical feasibility of FMF-SLAM in real-world\nscenarios with real time performance by integrating it with a security robot by\nfusing with a global positioning module GNSS-RTK and global Bundle Adjustment.\nOur approach is validated using video sequences from TUM, TartanAir, and our\nreal-world datasets, showcasing state-of-the-art performance under noisy,\nvarying lighting, and dark conditions.Our code and datasets are available at\nhttps://github.com/youjie-zhou/FMF-SLAM.git.", "published": "2025-06-22 23:44:07", "link": "http://arxiv.org/abs/2506.18204v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Two Sonification Methods for the MindCube", "abstract": "In this work, we explore the musical interface potential of the MindCube, an\ninteractive device designed to study emotions. Embedding diverse sensors and\ninput devices, this interface resembles a fidget cube toy commonly used to help\nusers relieve their stress and anxiety. As such, it is a particularly\nwell-suited controller for musical systems that aim to help with emotion\nregulation. In this regard, we present two different mappings for the MindCube,\nwith and without AI. With our generative AI mapping, we propose a way to infuse\nmeaning within a latent space and techniques to navigate through it with an\nexternal controller. We discuss our results and propose directions for future\nwork.", "published": "2025-06-22 23:09:37", "link": "http://arxiv.org/abs/2506.18196v1", "categories": ["cs.HC", "cs.AI", "cs.SD", "eess.AS", "H.5.5"], "primary_category": "cs.HC"}
{"title": "LLM-Enhanced Multimodal Fusion for Cross-Domain Sequential Recommendation", "abstract": "Cross-Domain Sequential Recommendation (CDSR) predicts user behavior by\nleveraging historical interactions across multiple domains, focusing on\nmodeling cross-domain preferences and capturing both intra- and inter-sequence\nitem relationships. We propose LLM-Enhanced Multimodal Fusion for Cross-Domain\nSequential Recommendation (LLM-EMF), a novel and advanced approach that\nenhances textual information with Large Language Models (LLM) knowledge and\nsignificantly improves recommendation performance through the fusion of visual\nand textual data. Using the frozen CLIP model, we generate image and text\nembeddings, thereby enriching item representations with multimodal data. A\nmultiple attention mechanism jointly learns both single-domain and cross-domain\npreferences, effectively capturing and understanding complex user interests\nacross diverse domains. Evaluations conducted on four e-commerce datasets\ndemonstrate that LLM-EMF consistently outperforms existing methods in modeling\ncross-domain user preferences, thereby highlighting the effectiveness of\nmultimodal data integration and its advantages in enhancing sequential\nrecommendation systems. Our source code will be released.", "published": "2025-06-22 09:53:21", "link": "http://arxiv.org/abs/2506.17966v1", "categories": ["cs.IR", "cs.CV"], "primary_category": "cs.IR"}
{"title": "A GenAI System for Improved FAIR Independent Biological Database Integration", "abstract": "Life sciences research increasingly requires identifying, accessing, and\neffectively processing data from an ever-evolving array of information sources\non the Linked Open Data (LOD) network. This dynamic landscape places a\nsignificant burden on researchers, as the quality of query responses depends\nheavily on the selection and semantic integration of data sources --processes\nthat are often labor-intensive, error-prone, and costly. While the adoption of\nFAIR (Findable, Accessible, Interoperable, and Reusable) data principles has\naimed to address these challenges, barriers to efficient and accurate\nscientific data processing persist.\n  In this paper, we introduce FAIRBridge, an experimental natural\nlanguage-based query processing system designed to empower scientists to\ndiscover, access, and query biological databases, even when they are not\nFAIR-compliant. FAIRBridge harnesses the capabilities of AI to interpret query\nintents, map them to relevant databases described in scientific literature, and\ngenerate executable queries via intelligent resource access plans. The system\nalso includes robust tools for mitigating low-quality query processing,\nensuring high fidelity and responsiveness in the information delivered.\n  FAIRBridge's autonomous query processing framework enables users to explore\nalternative data sources, make informed choices at every step, and leverage\ncommunity-driven crowd curation when needed. By providing a user-friendly,\nautomated hypothesis-testing platform in natural English, FAIRBridge\nsignificantly enhances the integration and processing of scientific data,\noffering researchers a powerful new tool for advancing their inquiries.", "published": "2025-06-22 08:04:24", "link": "http://arxiv.org/abs/2506.17934v1", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR"}
{"title": "Six Decades Post-Discovery of Taylor's Power Law: From Ecological and Statistical Universality, Through Prime Number Distributions and Tipping-Point Signals, to Heterogeneity and Stability of Complex Networks", "abstract": "First discovered by L. R. Taylor (1961, Nature), Taylor's Power Law (TPL)\ncorrelates the mean (M) population abundances and the corresponding variances\n(V) across a set of insect populations using a power function (V=aM^b). TPL has\ndemonstrated its 'universality' across numerous fields of sciences, social\nsciences, and humanities. This universality has inspired two main prongs of\nexploration: one from mathematicians and statisticians, who might instinctively\nrespond with a convergence theorem similar to the central limit theorem of the\nGaussian distribution, and another from biologists, ecologists, physicists,\netc., who are more interested in potential underlying ecological or\norganizational mechanisms. Over the past six decades, TPL studies have produced\na punctuated landscape with three relatively distinct periods (1960s-1980s;\n1990s-2000s, and 2010s-2020s) across the two prongs of abstract and physical\nworlds. Eight themes have been identified and reviewed on this landscape,\nincluding population spatial aggregation and ecological mechanisms, TPL and\nskewed statistical distributions, mathematical/statistical mechanisms of TPL,\nsample vs. population TPL, population stability, synchrony, and early warning\nsignals for tipping points, TPL on complex networks, TPL in macrobiomes, and in\nmicrobiomes. Three future research directions including fostering reciprocal\ninteractions between the two prongs, heterogeneity measuring, and exploration\nin the context of evolution. The significance of TPL research includes\npractically, population fluctuations captured by TPL are relevant for\nagriculture, forestry, fishery, wildlife-conservation, epidemiology, tumor\nheterogeneity, earthquakes, social inequality, stock illiquidity, financial\nstability, tipping point events, etc.; theoretically, TPL is one form of power\nlaws, which are related to phase transitions, universality, scale-invariance,\netc.", "published": "2025-06-22 19:47:16", "link": "http://arxiv.org/abs/2506.18154v1", "categories": ["q-bio.OT", "cs.CE", "cs.IT", "cs.SI", "math.IT", "q-bio.PE"], "primary_category": "q-bio.OT"}
{"title": "Cooperative Bistatic ISAC Systems for Low-Altitude Economy", "abstract": "The burgeoning low-altitude economy (LAE) necessitates integrated sensing and\ncommunication (ISAC) systems capable of high-accuracy multi-target localization\nand velocity estimation under hardware and coverage constraints inherent in\nconventional ISAC architectures. This paper addresses these challenges by\nproposing a cooperative bistatic ISAC framework within MIMO-OFDM cellular\nnetworks, enabling robust sensing services for LAE applications through\nstandardized 5G New Radio (NR) infrastructure. We first develop a\nlow-complexity parameter extraction algorithm employing CANDECOMP/PARAFAC (CP)\ntensor decomposition, which exploits the inherent Vandermonde structure in\ndelay-related factor matrices to efficiently recover bistatic ranges, Doppler\nvelocities, and angles-of-arrival (AoA) from multi-dimensional received signal\ntensors. To resolve data association ambiguity across distributed\ntransmitter-receiver pairs and mitigate erroneous estimates, we further design\na robust fusion scheme based on the minimum spanning tree (MST) method,\nenabling joint 3D position and velocity reconstruction. Comprehensive\nsimulation results validate the framework's superiority in computational\nefficiency and sensing performance for low-altitude scenarios.", "published": "2025-06-22 15:32:07", "link": "http://arxiv.org/abs/2506.18067v1", "categories": ["eess.SP", "cs.IT", "math.IT"], "primary_category": "eess.SP"}
{"title": "ISAC Network Planning: Sensing Coverage Analysis and 3-D BS Deployment Optimization", "abstract": "Integrated sensing and communication (ISAC) networks strive to deliver both\nhigh-precision target localization and high-throughput data services across the\nentire coverage area. In this work, we examine the fundamental trade-off\nbetween sensing and communication from the perspective of base station (BS)\ndeployment. Furthermore, we conceive a design that simultaneously maximizes the\ntarget localization coverage, while guaranteeing the desired communication\nperformance. In contrast to existing schemes optimized for a single target, an\neffective network-level approach has to ensure consistent localization accuracy\nthroughout the entire service area. While employing time-of-flight (ToF) based\nlocalization, we first analyze the deployment problem from a\nlocalization-performance coverage perspective, aiming for minimizing the area\nCramer-Rao Lower Bound (A-CRLB) to ensure uniformly high positioning accuracy\nacross the service area. We prove that for a fixed number of BSs, uniformly\nscaling the service area by a factor \\kappa increases the optimal A-CRLB in\nproportion to \\kappa^{2\\beta}, where \\beta is the BS-to-target pathloss\nexponent. Based on this, we derive an approximate scaling law that links the\nachievable A-CRLB across the area of interest to the dimensionality of the\nsensing area. We also show that cooperative BSs extends the coverage but yields\nmarginal A-CRLB improvement as the dimensionality of the sensing area grows.", "published": "2025-06-22 12:12:10", "link": "http://arxiv.org/abs/2506.18009v1", "categories": ["eess.SP", "cs.IT", "math.IT"], "primary_category": "eess.SP"}
{"title": "Robust Energy-Efficient DRL-Based Optimization in UAV-Mounted RIS Systems with Jitter", "abstract": "In this letter, we propose an energy-efficient design for an unmanned aerial\nvehicle (UAV)-mounted reconfigurable intelligent surface (RIS) communication\nsystem with nonlinear energy harvesting (EH) and UAV jitter. A joint\noptimization problem is formulated to maximize the EH efficiency of the\nUAV-mounted RIS by controlling the user powers, RIS phase shifts, and\ntime-switching factor, subject to quality of service and practical EH\nconstraints. The problem is nonconvex and time-coupled due to UAV angular\njitter and nonlinear EH dynamics, making it intractable for conventional\noptimization methods. To address this, we reformulate the problem as a deep\nreinforcement learning (DRL) environment and develop a smoothed softmax dual\ndeep deterministic policy gradient algorithm. The proposed method incorporates\naction clipping, entropy regularization, and softmax-weighted Q-value\nestimation to improve learning stability and exploration. Simulation results\nshow that the proposed algorithm converges reliably under various UAV jitter\nlevels and achieves an average EH efficiency of 45.07\\%, approaching the\n53.09\\% upper bound of exhaustive search, and outperforming other DRL\nbaselines.", "published": "2025-06-22 10:06:14", "link": "http://arxiv.org/abs/2506.17971v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Joint Embedding Predictive Architecture for self-supervised pretraining on polymer molecular graphs", "abstract": "Recent advances in machine learning (ML) have shown promise in accelerating\nthe discovery of polymers with desired properties by aiding in tasks such as\nvirtual screening via property prediction. However, progress in polymer ML is\nhampered by the scarcity of high-quality labeled datasets, which are necessary\nfor training supervised ML models. In this work, we study the use of the very\nrecent 'Joint Embedding Predictive Architecture' (JEPA), a type of architecture\nfor self-supervised learning (SSL), on polymer molecular graphs to understand\nwhether pretraining with the proposed SSL strategy improves downstream\nperformance when labeled data is scarce. Our results indicate that JEPA-based\nself-supervised pretraining on polymer graphs enhances downstream performance,\nparticularly when labeled data is very scarce, achieving improvements across\nall tested datasets.", "published": "2025-06-22 22:51:53", "link": "http://arxiv.org/abs/2506.18194v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "DeInfoReg: A Decoupled Learning Framework for Better Training Throughput", "abstract": "This paper introduces Decoupled Supervised Learning with Information\nRegularization (DeInfoReg), a novel approach that transforms a long gradient\nflow into multiple shorter ones, thereby mitigating the vanishing gradient\nproblem. Integrating a pipeline strategy, DeInfoReg enables model\nparallelization across multiple GPUs, significantly improving training\nthroughput. We compare our proposed method with standard backpropagation and\nother gradient flow decomposition techniques. Extensive experiments on diverse\ntasks and datasets demonstrate that DeInfoReg achieves superior performance and\nbetter noise resistance than traditional BP models and efficiently utilizes\nparallel computing resources. The code for reproducibility is available at:\nhttps://github.com/ianzih/Decoupled-Supervised-Learning-for-Information-Regularization/.", "published": "2025-06-22 22:50:06", "link": "http://arxiv.org/abs/2506.18193v1", "categories": ["cs.LG", "cs.AI", "cs.DC"], "primary_category": "cs.LG"}
{"title": "Call Me Maybe: Enhancing JavaScript Call Graph Construction using Graph Neural Networks", "abstract": "Static analysis plays a key role in finding bugs, including security issues.\nA critical step in static analysis is building accurate call graphs that model\nfunction calls in a program. However, due to hard-to-analyze language features,\nexisting call graph construction algorithms for JavaScript are neither sound\nnor complete. Prior work shows that even advanced solutions produce false edges\nand miss valid ones. In this work, we assist these tools by identifying missed\ncall edges. Our main idea is to frame the problem as link prediction on full\nprogram graphs, using a rich representation with multiple edge types. Our\napproach, GRAPHIA, leverages recent advances in graph neural networks to model\nnon-local relationships between code elements. Concretely, we propose\nrepresenting JavaScript programs using a combination of syntactic- and\nsemantic-based edges. GRAPHIA can learn from imperfect labels, including static\ncall edges from existing tools and dynamic edges from tests, either from the\nsame or different projects. Because call graphs are sparse, standard machine\nlearning metrics like ROC are not suitable. Instead, we evaluate GRAPHIA by\nranking function definitions for each unresolved call site. We conduct a\nlarge-scale evaluation on 50 popular JavaScript libraries with 163K call edges\n(150K static and 13K dynamic). GRAPHIA builds program graphs with 6.6M\nstructural and 386K semantic edges. It ranks the correct target as the top\ncandidate in over 42% of unresolved cases and within the top 5 in 72% of cases,\nreducing the manual effort needed for analysis. Our results show that\nlearning-based methods can improve the recall of JavaScript call graph\nconstruction. To our knowledge, this is the first work to apply GNN-based link\nprediction to full multi-file program graphs for interprocedural analysis.", "published": "2025-06-22 22:26:44", "link": "http://arxiv.org/abs/2506.18191v1", "categories": ["cs.SE", "cs.AI", "cs.LG"], "primary_category": "cs.SE"}
{"title": "The Impact of Medication Non-adherence on Adverse Outcomes: Evidence from Schizophrenia Patients via Survival Analysis", "abstract": "This study quantifies the association between non-adherence to antipsychotic\nmedications and adverse outcomes in individuals with schizophrenia. We frame\nthe problem using survival analysis, focusing on the time to the earliest of\nseveral adverse events (early death, involuntary hospitalization, jail\nbooking). We extend standard causal inference methods (T-learner, S-learner,\nnearest neighbor matching) to utilize various survival models to estimate\nindividual and average treatment effects, where treatment corresponds to\nmedication non-adherence. Analyses are repeated using different amounts of\nlongitudinal information (3, 6, 9, and 12 months). Using data from Allegheny\nCounty in western Pennsylvania, we find strong evidence that non-adherence\nadvances adverse outcomes by approximately 1 to 4 months. Ablation studies\nconfirm that county-provided risk scores adjust for key confounders, as their\nremoval amplifies the estimated effects. Subgroup analyses by medication\nformulation (injectable vs. oral) and medication type consistently show that\nnon-adherence is associated with earlier adverse events. These findings\nhighlight the clinical importance of adherence in delaying psychiatric crises\nand show that integrating survival analysis with causal inference tools can\nyield policy-relevant insights. We caution that although we apply causal\ninference, we only make associative claims and discuss assumptions needed for\ncausal interpretation.", "published": "2025-06-22 22:09:39", "link": "http://arxiv.org/abs/2506.18187v1", "categories": ["cs.AI", "cs.LG", "stat.ML"], "primary_category": "cs.AI"}
{"title": "Online Learning of Whittle Indices for Restless Bandits with Non-Stationary Transition Kernels", "abstract": "We consider optimal resource allocation for restless multi-armed bandits\n(RMABs) in unknown, non-stationary settings. RMABs are PSPACE-hard to solve\noptimally, even when all parameters are known. The Whittle index policy is\nknown to achieve asymptotic optimality for a large class of such problems,\nwhile remaining computationally efficient. In many practical settings, however,\nthe transition kernels required to compute the Whittle index are unknown and\nnon-stationary. In this work, we propose an online learning algorithm for\nWhittle indices in this setting. Our algorithm first predicts current\ntransition kernels by solving a linear optimization problem based on upper\nconfidence bounds and empirical transition probabilities calculated from data\nover a sliding window. Then, it computes the Whittle index associated with the\npredicted transition kernels. We design these sliding windows and upper\nconfidence bounds to guarantee sub-linear dynamic regret on the number of\nepisodes $T$, under the condition that transition kernels change slowly over\ntime (rate upper bounded by $\\epsilon=1/T^k$ with $k>0$). Furthermore, our\nproposed algorithm and regret analysis are designed to exploit prior domain\nknowledge and structural information of the RMABs to accelerate the learning\nprocess. Numerical results validate that our algorithm achieves superior\nperformance in terms of lowest cumulative regret relative to baselines in\nnon-stationary environments.", "published": "2025-06-22 22:04:52", "link": "http://arxiv.org/abs/2506.18186v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Memba: Membrane-driven Parameter-Efficient Fine-Tuning for Mamba", "abstract": "State Space Models (SSMs) have emerged as powerful alternatives to\nattention-based Transformers, with Mamba demonstrating impressive efficiency\nand scalability. As these models grow increasingly larger, the need for\nParameter-Efficient Fine-Tuning (PEFT) methods becomes critical to adapt\npre-trained Mamba to downstream tasks without prohibitive computational costs.\nHowever, previous approaches simply apply traditional Transformer-tailored PEFT\nmethods without addressing the unique temporal processing dynamics of SSMs. To\naddress this limitation, we propose Memba, a membrane-driven PEFT approach\nspecifically designed for Mamba. Memba introduces Leaky Integrate Membrane\n(LIM) neurons as bio-inspired gating mechanisms that naturally accumulate\nmembrane potentials over time, enhancing selective information retention. By\nstrategically combining LIM neurons with Low-Rank Adaptations (LoRA) and\ncross-layer membrane transfer, our approach significantly improves Mamba's\ntemporal modeling capabilities. Extensive experiments across language and\nvision tasks demonstrate that Memba achieves substantial improvements over\nexisting PEFT methods. The code is available at\nhttps://github.com/Intelligent-Computing-Lab-Yale/Memba.", "published": "2025-06-22 21:52:45", "link": "http://arxiv.org/abs/2506.18184v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Understanding Reasoning in Thinking Language Models via Steering Vectors", "abstract": "Recent advances in large language models (LLMs) have led to the development\nof thinking language models that generate extensive internal reasoning chains\nbefore producing responses. While these models achieve improved performance,\ncontrolling their reasoning processes remains challenging. This work presents a\nsteering approach for thinking LLMs by analyzing and manipulating specific\nreasoning behaviors in DeepSeek-R1-Distill models. Through a systematic\nexperiment on 500 tasks across 10 diverse categories, we identify several\nreasoning behaviors exhibited by thinking models, including expressing\nuncertainty, generating examples for hypothesis validation, and backtracking in\nreasoning chains. We demonstrate that these behaviors are mediated by linear\ndirections in the model's activation space and can be controlled using steering\nvectors. By extracting and applying these vectors, we provide a method to\nmodulate specific aspects of the model's reasoning process, such as its\ntendency to backtrack or express uncertainty. Our approach offers practical\ntools for steering reasoning processes in thinking models in a controlled and\ninterpretable manner. We validate our steering method using two\nDeepSeek-R1-Distill models, demonstrating consistent control across different\nmodel architectures.", "published": "2025-06-22 20:45:26", "link": "http://arxiv.org/abs/2506.18167v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Non-equilibrium Annealed Adjoint Sampler", "abstract": "Recently, there has been significant progress in learning-based diffusion\nsamplers, which aim to sample from a given unnormalized density. These methods\ntypically follow one of two paradigms: (i) formulating sampling as an unbiased\nstochastic optimal control (SOC) problem using a canonical reference process,\nor (ii) refining annealed path measures through importance-weighted sampling.\nAlthough annealing approaches have advantages in guiding samples toward\nhigh-density regions, reliance on importance sampling leads to high variance\nand limited scalability in practice. In this paper, we introduce the\n\\textbf{Non-equilibrium Annealed Adjoint Sampler (NAAS)}, a novel SOC-based\ndiffusion sampler that leverages annealed reference dynamics without resorting\nto importance sampling. NAAS employs a lean adjoint system inspired by adjoint\nmatching, enabling efficient and scalable training. We demonstrate the\neffectiveness of our approach across a range of tasks, including sampling from\nclassical energy landscapes and molecular Boltzmann distribution.", "published": "2025-06-22 20:41:31", "link": "http://arxiv.org/abs/2506.18165v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Pitfalls of Conformal Predictions for Medical Image Classification", "abstract": "Reliable uncertainty estimation is one of the major challenges for medical\nclassification tasks. While many approaches have been proposed, recently the\nstatistical framework of conformal predictions has gained a lot of attention,\ndue to its ability to provide provable calibration guarantees. Nonetheless, the\napplication of conformal predictions in safety-critical areas such as medicine\ncomes with pitfalls, limitations and assumptions that practitioners need to be\naware of. We demonstrate through examples from dermatology and histopathology\nthat conformal predictions are unreliable under distributional shifts in input\nand label variables. Additionally, conformal predictions should not be used for\nselecting predictions to improve accuracy and are not reliable for subsets of\nthe data, such as individual classes or patient attributes. Moreover, in\nclassification settings with a small number of classes, which are common in\nmedical image classification tasks, conformal predictions have limited\npractical value.", "published": "2025-06-22 20:33:38", "link": "http://arxiv.org/abs/2506.18162v1", "categories": ["cs.LG", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Probabilistic and reinforced mining of association rules", "abstract": "This work introduces 4 novel probabilistic and reinforcement-driven methods\nfor association rule mining (ARM): Gaussian process-based association rule\nmining (GPAR), Bayesian ARM (BARM), multi-armed bandit based ARM (MAB-ARM), and\nreinforcement learning based association rule mining (RLAR). These methods\ndepart fundamentally from traditional frequency-based algorithms such as\nApriori, FP-Growth, and Eclat, offering enhanced capabilities for incorporating\nprior knowledge, modeling uncertainty, item dependencies, probabilistic\ninference and adaptive search strategies. GPAR employs Gaussian processes to\nmodel item co-occurrence via feature representations, enabling principled\ninference, uncertainty quantification, and efficient generalization to unseen\nitemsets without retraining. BARM adopts a Bayesian framework with priors and\noptional correlation structures, yielding robust uncertainty quantification\nthrough full posterior distributions over item presence probabilities. MAB-ARM,\nincluding its Monte Carlo tree search (MCTS) companion, utilizes an upper\nconfidence bound (UCB) strategy for efficient and adaptive exploration of the\nitemset space, while RLAR applies a deep Q-network (DQN) to learn a\ngeneralizable policy for identifying high-quality rules. Collectively, these\napproaches improve the flexibility and robustness of ARM, particularly for\ndiscovering rare or complex patterns and operating on small datasets. Empirical\nresults on synthetic and real-world datasets demonstrate their effectiveness,\nwhile also highlighting trade-offs in computational complexity and\ninterpretability. These innovations mark a significant shift from static,\nfrequency-driven paradigms, offering some prior and dependency-informed,\nuncertainty-aware or scalable ARM frameworks for diverse application domains\nsuch as retail, geography, finance, medical diagnostics, and risk-sensitive\nscenarios.", "published": "2025-06-22 19:51:15", "link": "http://arxiv.org/abs/2506.18155v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Routing Mamba: Scaling State Space Models with Mixture-of-Experts Projection", "abstract": "Linear State Space Models (SSMs) offer remarkable performance gains in\nefficient sequence modeling, with constant inference-time computation and\nmemory complexity. Recent advances, such as Mamba, further enhance SSMs with\ninput-dependent gating and hardware-aware implementations, positioning them as\nstrong alternatives to Transformers for long sequence modeling. However,\nefficiently scaling the expressive power of SSMs, particularly with Mixture of\nExperts (MoE), remains challenging, as naive integration attempts often falter\nor degrade performance. In this work, we introduce Routing Mamba (RoM), a novel\napproach that scales SSM parameters using sparse mixtures of linear projection\nexperts. By sharing routing decisions between projection layers and lightweight\nsub-modules within Mamba across experts, RoM leverages synergies among linear\nprojection experts for effective and efficient sparse scaling of Mamba layers.\nAt a scale of 1.3B active parameters (10B total) and 16K training sequence\nlength, RoM achieves language modeling performance equivalent to a dense Mamba\nmodel requiring over 2.3x more active parameters, and demonstrates consistent\nperplexity across context lengths. Experimental results further show RoM\neffectively scales hybrid language models, yielding a 23% FLOPS saving compared\nto dense Mamba scaling for similar performance.", "published": "2025-06-22 19:26:55", "link": "http://arxiv.org/abs/2506.18145v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Bayesian Multiobject Tracking With Neural-Enhanced Motion and Measurement Models", "abstract": "Multiobject tracking (MOT) is an important task in applications including\nautonomous driving, ocean sciences, and aerospace surveillance. Traditional MOT\nmethods are model-based and combine sequential Bayesian estimation with data\nassociation and an object birth model. More recent methods are fully\ndata-driven and rely on the training of neural networks. Both approaches offer\ndistinct advantages in specific settings. In particular, model-based methods\nare generally applicable across a wide range of scenarios, whereas data-driven\nMOT achieves superior performance in scenarios where abundant labeled data for\ntraining is available. A natural thought is whether a general framework can\nintegrate the two approaches. This paper introduces a hybrid method that\nutilizes neural networks to enhance specific aspects of the statistical model\nin Bayesian MOT that have been identified as overly simplistic. By doing so,\nthe performance of the prediction and update steps of Bayesian MOT is improved.\nTo ensure tractable computation, our framework uses belief propagation to avoid\nhigh-dimensional operations combined with sequential Monte Carlo methods to\nperform low-dimensional operations efficiently. The resulting method combines\nthe flexibility and robustness of model-based approaches with the capability to\nlearn complex information from data of neural networks. We evaluate the\nperformance of the proposed method based on the nuScenes autonomous driving\ndataset and demonstrate that it has state-of-the-art performance", "published": "2025-06-22 18:15:08", "link": "http://arxiv.org/abs/2506.18124v1", "categories": ["cs.LG", "eess.SP", "stat.ML"], "primary_category": "cs.LG"}
{"title": "RoboArena: Distributed Real-World Evaluation of Generalist Robot Policies", "abstract": "Comprehensive, unbiased, and comparable evaluation of modern generalist\npolicies is uniquely challenging: existing approaches for robot benchmarking\ntypically rely on heavy standardization, either by specifying fixed evaluation\ntasks and environments, or by hosting centralized ''robot challenges'', and do\nnot readily scale to evaluating generalist policies across a broad range of\ntasks and environments. In this work, we propose RoboArena, a new approach for\nscalable evaluation of generalist robot policies in the real world. Instead of\nstandardizing evaluations around fixed tasks, environments, or locations, we\npropose to crowd-source evaluations across a distributed network of evaluators.\nImportantly, evaluators can freely choose the tasks and environments they\nevaluate on, enabling easy scaling of diversity, but they are required to\nperform double-blind evaluations over pairs of policies. Then, by aggregating\npreference feedback from pairwise comparisons across diverse tasks and\nenvironments, we can derive a ranking of policies. We instantiate our approach\nacross a network of evaluators at seven academic institutions using the DROID\nrobot platform. Through more than 600 pairwise real-robot evaluation episodes\nacross seven generalist policies, we demonstrate that our crowd-sourced\napproach can more accurately rank the performance of existing generalist\npolicies than conventional, centralized evaluation approaches, while being more\nscalable, resilient, and trustworthy. We open our evaluation network to the\ncommunity and hope that it can enable more accessible comparisons of generalist\nrobot policies.", "published": "2025-06-22 18:13:31", "link": "http://arxiv.org/abs/2506.18123v1", "categories": ["cs.RO", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Dynamic Temporal Positional Encodings for Early Intrusion Detection in IoT", "abstract": "The rapid expansion of the Internet of Things (IoT) has introduced\nsignificant security challenges, necessitating efficient and adaptive Intrusion\nDetection Systems (IDS). Traditional IDS models often overlook the temporal\ncharacteristics of network traffic, limiting their effectiveness in early\nthreat detection. We propose a Transformer-based Early Intrusion Detection\nSystem (EIDS) that incorporates dynamic temporal positional encodings to\nenhance detection accuracy while maintaining computational efficiency. By\nleveraging network flow timestamps, our approach captures both sequence\nstructure and timing irregularities indicative of malicious behaviour.\nAdditionally, we introduce a data augmentation pipeline to improve model\nrobustness. Evaluated on the CICIoT2023 dataset, our method outperforms\nexisting models in both accuracy and earliness. We further demonstrate its\nreal-time feasibility on resource-constrained IoT devices, achieving\nlow-latency inference and minimal memory footprint.", "published": "2025-06-22 17:56:19", "link": "http://arxiv.org/abs/2506.18114v1", "categories": ["cs.CR", "cs.LG"], "primary_category": "cs.CR"}
{"title": "RL for Reasoning by Adaptively Revealing Rationales", "abstract": "We propose that reinforcement learning (RL) from partial expert\ndemonstrations is not merely a training heuristic, but a promising framework\nfor solving complex sequence generation tasks. Supervised fine-tuning (SFT)\nrelies on dense ground-truth labels, which become increasingly costly as\nsequence length grows. RL, on the other hand, struggles with sparse rewards and\na combinatorially large output space. We address this by introducing adaptive\nbacktracking (AdaBack), a per-sample curriculum learning algorithm that reveals\nonly a partial prefix of the target output during training. The supervision\nlength is adjusted dynamically for each sample based on the model's past reward\nsignal, allowing it to incrementally learn to complete reasoning chains by\nconditioning on correct partial solutions. We investigate this intermediate\nregime between SFT and RL and argue that per-sample curriculum learning is more\nthan a trade-off between efficiency and generality, it can succeed in tasks\nwith long sequences of latent dependencies where SFT and RL both fail to\ngeneralize. Using a synthetic task with latent parity constraints, we show that\nour adaptive curriculum over partial answers reliably solves problems that are\notherwise intractable. On mathematical reasoning benchmarks (MATH, GSM8k), we\nfind that curriculum learning enables models to solve problems that RL alone\ncannot, acquiring new reasoning capabilities through incremental exposure to\npartial solutions.", "published": "2025-06-22 17:46:14", "link": "http://arxiv.org/abs/2506.18110v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Wisdom of Crowds Through Myopic Self-Confidence Adaptation", "abstract": "The wisdom of crowds is an umbrella term for phenomena suggesting that the\ncollective judgment or decision of a large group can be more accurate than the\nindividual judgments or decisions of the group members. A well-known example\nillustrating this concept is the competition at a country fair described by\nGalton, where the median value of the individual guesses about the weight of an\nox resulted in an astonishingly accurate estimate of the actual weight. This\nphenomenon resembles classical results in probability theory and relies on\nindependent decision-making. The accuracy of the group's final decision can be\nsignificantly reduced if the final agents' opinions are driven by a few\ninfluential agents.\n  In this paper, we consider a group of agents who initially possess\nuncorrelated and unbiased noisy measurements of a common state of the world.\nAssume these agents iteratively update their estimates according to a simple\nnon-Bayesian learning rule, commonly known in mathematical sociology as the\nFrench-DeGroot dynamics or iterative opinion pooling. As a result of this\niterative distributed averaging process, each agent arrives at an asymptotic\nestimate of the state of the world, with the variance of this estimate\ndetermined by the matrix of weights the agents assign to each other. Every\nagent aims at minimizing the variance of her asymptotic estimate of the state\nof the world; however, such variance is also influenced by the weights\nallocated by other agents. To achieve the best possible estimate, the agents\nmust then solve a game-theoretic, multi-objective optimization problem defined\nby the available sets of influence weights. We characterize both the Pareto\nfrontier and the set of Nash equilibria in the resulting game. Additionally, we\nexamine asynchronous best-response dynamics for the group of agents and prove\ntheir convergence to the set of strict Nash equilibria.", "published": "2025-06-22 22:55:17", "link": "http://arxiv.org/abs/2506.18195v1", "categories": ["math.OC", "cs.AI", "cs.MA", "cs.SY", "eess.SY", "physics.soc-ph"], "primary_category": "math.OC"}
{"title": "Optimization of Flying Ad Hoc Network Topology and Collaborative Path Planning for Multiple UAVs", "abstract": "Multiple unmanned aerial vehicles (UAVs) play a vital role in monitoring and\ndata collection in wide area environments with harsh conditions. In most\nscenarios, issues such as real-time data retrieval and real-time UAV\npositioning are often disregarded, essentially neglecting the communication\nconstraints. In this paper, we comprehensively address both the coverage of the\ntarget area and the data transmission capabilities of the flying ad hoc network\n(FANET). The data throughput of the network is therefore maximized by\noptimizing the network topology and the UAV trajectories. The resultant\noptimization problem is effectively solved by the proposed reinforcement\nlearning-based trajectory planning (RL-TP) algorithm and the convex-based\ntopology optimization (C-TOP) algorithm sequentially. The RL-TP optimizes the\nUAV paths while considering the constraints of FANET. The C-TOP maximizes the\ndata throughput of the network while simultaneously constraining the neighbors\nand transmit powers of the UAVs, which is shown to be a convex problem that can\nbe efficiently solved in polynomial time. Simulations and field experimental\nresults show that the proposed optimization strategy can effectively plan the\nUAV trajectories and significantly improve the data throughput of the FANET\nover the adaptive local minimum spanning tree (A-LMST) and cyclic\npruning-assisted power optimization (CPAPO) methods.", "published": "2025-06-22 08:41:27", "link": "http://arxiv.org/abs/2506.17945v1", "categories": ["cs.MA"], "primary_category": "cs.MA"}
{"title": "Conservative data-driven finite element formulation", "abstract": "This paper presents a new data-driven finite element framework derived with\nmixed finite element formulation. The standard approach to diffusion problems\nrequires the solution of the mathematical equations that describe both the\nconservation law and the constitutive relations, where the latter is\ntraditionally obtained after fitting experimental data to simplified material\nmodels. To exploit all available information and avoid bias in the material\nmodel, we follow a data-driven approach. While the conservation laws and\nboundary conditions are satisfied by means of the finite element method, the\nexperimental data is used directly in the numerical simulations, avoiding the\nneed of fitting material model parameters. In order to satisfy the conservation\nlaw a priori in the strong sense, we introduce a mixed finite element\nformulation. This relaxes the regularity requirements on approximation spaces\nwhile enforcing continuity of the normal flux component across all of the inner\nboundaries. This weaker mixed formulation provides a posteriori error\nindicators tailored for this data-driven approach, enabling adaptive\nhp-refinement. The relaxed regularity of the approximation spaces makes it\neasier to observe how the variation in the datasets results in the\nnon-uniqueness of the solution, which can be quantified to predict the\nuncertainty of the results. The capabilities of the formulation are\ndemonstrated in an example of the nonlinear heat transfer in nuclear graphite\nusing synthetically generated material datasets.", "published": "2025-06-22 23:46:23", "link": "http://arxiv.org/abs/2506.18206v1", "categories": ["cs.CE", "cs.NA", "math.NA", "physics.comp-ph"], "primary_category": "cs.CE"}
{"title": "Mixed virtual element methods for a stress-velocity-rotation formulation in viscoelasticity", "abstract": "In this paper we propose a new mixed virtual element formulation for the\nnumerical approximation of viscoelasticity equations with weakly imposed stress\nsymmetry. The governing equations use the Zener model and are expressed in\nterms of the principal unknowns of additively decomposed stress into elastic\nand internal viscoelastic contributions, while the rotation tensor and velocity\nact as Lagrange multipliers. The time discretisation uses Crank--Nicolson's\nscheme. We demonstrate the unique solvability of both semi-discrete and\nfully-discrete problems by leveraging the properties of suitable local\nprojectors. Moreover, we establish optimal a priori error estimates for all\nvariables that appear in the mixed formulation. To validate our theoretical\nfindings, we present several representative numerical examples that also\nhighlight the features of the proposed formulation.", "published": "2025-06-22 20:55:31", "link": "http://arxiv.org/abs/2506.18168v1", "categories": ["math.NA", "cs.NA", "65M30, 65M15, 74D05, 76R50"], "primary_category": "math.NA"}
{"title": "Fast solution of a phase-field model of pitting corrosion", "abstract": "Excessive computational times represent a major challenge in the solution of\ncorrosion models, limiting their practical applicability, e.g., as a support to\npredictive maintenance. In this paper, we propose an efficient strategy for\nsolving a phase-field model for metal corrosion. Based on the Kronecker\nstructure of the diffusion matrix in classical finite difference approximations\non rectangular domains, time-stepping IMEX methods are efficiently solve in\nmatrix form. However, when the domain is non-rectangular, the lack of the\nKronecker structure prevents the direct use of this matrix-based approach. To\naddress this issue, we reformulate the problem on an extended rectangular\ndomain and introduce suitable iterative IMEX methods. The convergence of the\niterations is analyzed, and numerical experiments show that the proposed\napproach achieves accuracy comparable to existing methods, while significantly\nreducing the computational time, to the point of allowing actual predictions on\nstandard workstations.", "published": "2025-06-22 15:00:47", "link": "http://arxiv.org/abs/2506.18058v1", "categories": ["math.NA", "cs.NA", "65M20, 65L04, 65F10, 65F45, 65Z05"], "primary_category": "math.NA"}
{"title": "Algorithms for pointwise and piecewise polynomial approximations to the trigonometric functions", "abstract": "In this paper, we propose a new and simple approach to the approximation\nalgorithms that are modified and improved from our published results. The\ncomputational and graphical examples are presented with the aid of Maple\nprocedures.", "published": "2025-06-22 11:09:16", "link": "http://arxiv.org/abs/2506.17992v1", "categories": ["math.NA", "cs.NA", "41A10, 41A80 (Primary), 65D15, 65Y99 (Secondary)"], "primary_category": "math.NA"}
{"title": "Hybridizable Discontinuous Galerkin Methods for Thermo-Poroelastic Systems", "abstract": "We propose a high-order hybridizable discontinuous Galerkin (HDG) formulation\nfor the fully dynamic, linear thermo-poroelasticity problem. The governing\nequations are formulated as a first-order hyperbolic system incorporating solid\nand fluid velocities, heat flux, effective stress, pore pressure, and\ntemperature as state variables. We establish well-posedness of the continuous\nproblem using semigroup theory and develop an energy-consistent HDG\ndiscretization. The method exploits computational advantages of HDG-including\nlocality and static condensation-while maintaining energy conservation for the\ncoupled system. We establish an $hp$-convergence analysis and support it with\ncomprehensive numerical experiments, confirming the theoretical rates and\nshowcasing the method's effectiveness for thermo-poroelastic wave propagation\nin heterogeneous media.", "published": "2025-06-22 10:34:20", "link": "http://arxiv.org/abs/2506.17978v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "A nodal basis for the $C^1$-$P_{33}$ finite elements on 5D simplex grids", "abstract": "We construct a nodal basis for the 5-dimensional $C^1$ finite element space\nof polynomial degree $33$\n  on simplex grids,\n  where the finite element functions are $C^1$ on the 6 4D-simplex faces,\n  $C^2$ on the 15 face-tetrahedra, $C^4$ on the 20 face-triangles,\n  $C^8$ on the 15 edges, and $C^{16}$ at the 6 vertices, of a 5D simplex.", "published": "2025-06-22 09:39:58", "link": "http://arxiv.org/abs/2506.17961v1", "categories": ["math.NA", "cs.NA", "65N15, 65N30"], "primary_category": "math.NA"}
{"title": "A residual a posteriori error estimate for the Stabilization-free Virtual Element Method", "abstract": "In this work, we present the a posteriori error analysis of\nStabilization-Free Virtual Element Methods for the 2D Poisson equation. The\nabscence of a stabilizing bilinear form in the scheme allows to prove the\nequivalence between a suitably defined error measure and standard residual\nerror estimators, which is not obtained in general for stabilized virtual\nelements. Several numerical experiments are carried out, confirming the\nexpected behaviour of the estimator in the presence of different mesh types,\nand robustness with respect to jumps of the diffusion term.", "published": "2025-06-22 08:54:29", "link": "http://arxiv.org/abs/2506.17947v1", "categories": ["math.NA", "cs.NA", "65N15, 65N22, 65N30"], "primary_category": "math.NA"}
{"title": "Simultaneous recovery of corroded boundaries and admittance using the Kohn-Vogelius method", "abstract": "We address the problem of identifying an unknown portion $\\Gamma$ of the\nboundary of a $d$-dimensional ($d \\in \\{1, 2\\}$) domain $\\Omega$ and its\nassociated Robin admittance coefficient, using two sets of boundary Cauchy data\n$(f, g)$--representing boundary temperature and heat flux--measured on the\naccessible portion $\\Sigma$ of the boundary. Identifiability results\n\\cite{Bacchelli2009,PaganiPierotti2009} indicate that a single measurement on\n$\\Sigma$ is insufficient to uniquely determine both $\\Gamma$ and $\\alpha$, but\ntwo independent inputs yielding distinct solutions ensure the uniqueness of the\npair $\\Gamma$ and $\\alpha$. In this paper, we propose a cost function based on\nthe energy-gap of two auxiliary problems. We derive the variational derivatives\nof this objective functional with respect to both the Robin boundary $\\Gamma$\nand the admittance coefficient $\\alpha$. These derivatives are utilized to\ndevelop a nonlinear gradient-based iterative scheme for the simultaneous\nnumerical reconstruction of $\\Gamma$ and $\\alpha$. Numerical experiments are\npresented to demonstrate the effectiveness and practicality of the proposed\nmethod.", "published": "2025-06-22 08:09:55", "link": "http://arxiv.org/abs/2506.17938v1", "categories": ["math.NA", "cs.NA", "math.OC", "49Q10, 35R25, 35R30, 49Q12"], "primary_category": "math.NA"}
{"title": "Robust PDE discovery under sparse and highly noisy conditions via attention neural networks", "abstract": "The discovery of partial differential equations (PDEs) from experimental data\nholds great promise for uncovering predictive models of complex physical\nsystems. In this study, we introduce an efficient automatic model discovery\nframework, ANN-PYSR, which integrates attention neural networks with the\nstate-of-the-art PySR symbolic regression library. Our approach successfully\nidentifies the governing PDE in six benchmark examples. Compared to the DLGA\nframework, numerical experiments demonstrate ANN-PYSR can extract the\nunderlying dynamic model more efficiently and robustly from sparse, highly\nnoisy data (noise level up to 200%, 5000 sampling points). It indicates an\nextensive variety of practical applications of ANN-PYSR, particularly in\nconditions with sparse sensor networks and high noise levels, where traditional\nmethods frequently fail.", "published": "2025-06-22 06:06:51", "link": "http://arxiv.org/abs/2506.17908v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Classical optimization algorithms for diagonalizing quantum Hamiltonians", "abstract": "Diagonalizing a Hamiltonian, which is essential for simulating its long-time\ndynamics, is a key primitive in quantum computing and has been proven to yield\na quantum advantage for several specific families of Hamiltonians. Yet, despite\nits importance, only a handful of diagonalization algorithms exist, and\ncorrespondingly few families of fast-forwardable Hamiltonians have been\nidentified. This paper introduces classical optimization algorithms for\nHamiltonian diagonalization by formulating a cost function that penalizes\noff-diagonal terms and enforces unitarity via an orthogonality constraint, both\nexpressed in the Pauli operator basis. We pinpoint a class of Hamiltonians that\nhighlights severe drawbacks of existing methods, including exponential\nper-iteration cost, exponential circuit depth, or convergence to spurious\noptima. Our approach overcomes these shortcomings, achieving polynomial-time\nefficiency while provably avoiding suboptimal points. As a result, we broaden\nthe known realm of fast-forwardable systems, showing that\nquantum-diagonalizable Hamiltonians extend to cases generated by exponentially\nlarge Lie algebras. On the practical side, we also present a\nrandomized-coordinate variant that achieves a more efficient per-iteration cost\nthan the deterministic counterpart. We demonstrate the effectiveness of these\nalgorithms through explicit examples and numerical experiments.", "published": "2025-06-22 03:17:56", "link": "http://arxiv.org/abs/2506.17883v1", "categories": ["quant-ph", "cs.NA", "math.NA", "math.OC"], "primary_category": "quant-ph"}
{"title": "Causal Interventions in Bond Multi-Dealer-to-Client Platforms", "abstract": "The digitalization of financial markets has shifted trading from voice to\nelectronic channels, with Multi-Dealer-to-Client (MD2C) platforms now enabling\nclients to request quotes (RfQs) for financial instruments like bonds from\nmultiple dealers simultaneously. In this competitive landscape, dealers cannot\nsee each other's prices, making a rigorous analysis of the negotiation process\ncrucial to ensure their profitability. This article introduces a novel general\nframework for analyzing the RfQ process using probabilistic graphical models\nand causal inference. Within this framework, we explore different inferential\nquestions that are relevant for dealers participating in MD2C platforms, such\nas the computation of optimal prices, estimating potential revenues and the\nidentification of clients that might be interested in trading the dealer's\naxes. We then move into analyzing two different approaches for model\nspecification: a generative model built on the work of (Fermanian, Gu\\'eant &\nPu, 2017); and discriminative models utilizing machine learning techniques. We\nevaluate these methodologies using predictive metrics designed to assess their\neffectiveness in the context of optimal pricing, highlighting the relative\nbenefits of using models that take into account the internal mechanisms of the\nnegotiation process.", "published": "2025-06-22 19:30:36", "link": "http://arxiv.org/abs/2506.18147v1", "categories": ["q-fin.TR"], "primary_category": "q-fin.TR"}
{"title": "GRASP: Grouped Regression with Adaptive Shrinkage Priors", "abstract": "We introduce GRASP, a simple Bayesian framework for regression with grouped\npredictors, built on the normal beta prime (NBP) prior. The NBP prior is an\nadaptive generalization of the horseshoe prior with tunable hyperparameters\nthat control tail behavior, enabling a flexible range of sparsity, from strong\nshrinkage to ridge-like regularization. Unlike prior work that introduced the\ngroup inverse-gamma gamma (GIGG) prior by decomposing the NBP prior into\nstructured hierarchies, we show that directly controlling the tails is\nsufficient without requiring complex hierarchical constructions. Extending the\nnon-tail adaptive grouped half-Cauchy hierarchy of Xu et al., GRASP assigns the\nNBP prior to both local and group shrinkage parameters allowing adaptive\nsparsity within and across groups. A key contribution of this work is a novel\nframework to explicitly quantify correlations among shrinkage parameters within\na group, providing deeper insights into grouped shrinkage behavior. We also\nintroduce an efficient Metropolis-Hastings sampler for hyperparameter\nestimation. Empirical results on simulated and real-world data demonstrate the\nrobustness and versatility of GRASP across grouped regression problems with\nvarying sparsity and signal-to-noise ratios.", "published": "2025-06-22 16:35:16", "link": "http://arxiv.org/abs/2506.18092v1", "categories": ["stat.ME", "cs.LG", "stat.ML"], "primary_category": "stat.ME"}
{"title": "Identifiable Convex-Concave Regression via Sub-gradient Regularised Least Squares", "abstract": "We propose a novel nonparametric regression method that models complex\ninput-output relationships as the sum of convex and concave components. The\nmethod-Identifiable Convex-Concave Nonparametric Least Squares\n(ICCNLS)-decomposes the target function into additive shape-constrained\ncomponents, each represented via sub-gradient-constrained affine functions. To\naddress the affine ambiguity inherent in convex-concave decompositions, we\nintroduce global statistical orthogonality constraints, ensuring that residuals\nare uncorrelated with both intercept and input variables. This enforces\ndecomposition identifiability and improves interpretability. We further\nincorporate L1, L2 and elastic net regularisation on sub-gradients to enhance\ngeneralisation and promote structural sparsity. The proposed method is\nevaluated on synthetic and real-world datasets, including healthcare pricing\ndata, and demonstrates improved predictive accuracy and model simplicity\ncompared to conventional CNLS and difference-of-convex (DC) regression\napproaches. Our results show that statistical identifiability, when paired with\nconvex-concave structure and sub-gradient regularisation, yields interpretable\nmodels suited for forecasting, benchmarking, and policy evaluation.", "published": "2025-06-22 15:53:12", "link": "http://arxiv.org/abs/2506.18078v1", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.AP", "stat.TH", "90C25, 62J02 (Primary) 62G08, 90C90, 68T09 (Secondary)"], "primary_category": "stat.ML"}
{"title": "Generalization under Byzantine & Poisoning Attacks: Tight Stability Bounds in Robust Distributed Learning", "abstract": "Robust distributed learning algorithms aim to maintain good performance in\ndistributed and federated settings, even in the presence of misbehaving\nworkers. Two primary threat models have been studied: Byzantine attacks, where\nmisbehaving workers can send arbitrarily corrupted updates, and data poisoning\nattacks, where misbehavior is limited to manipulation of local training data.\nWhile prior work has shown comparable optimization error under both threat\nmodels, a fundamental question remains open: How do these threat models impact\ngeneralization? Empirical evidence suggests a gap between the two threat\nmodels, yet it remains unclear whether it is fundamental or merely an artifact\nof suboptimal attacks. In this work, we present the first theoretical\ninvestigation into this problem, formally showing that Byzantine attacks are\nintrinsically more harmful to generalization than data poisoning. Specifically,\nwe prove that: (i) under data poisoning, the uniform algorithmic stability of a\nrobust distributed learning algorithm, with optimal optimization error,\ndegrades by an additive factor of $\\varTheta ( \\frac{f}{n-f} )$, with $f$ the\nnumber of misbehaving workers out of $n$; and (ii) In contrast, under Byzantine\nattacks, the degradation is in $\\mathcal{O} \\big( \\sqrt{ \\frac{f}{n-2f}}\n\\big)$.This difference in stability leads to a generalization error gap that is\nespecially significant as $f$ approaches its maximum value $\\frac{n}{2}$.", "published": "2025-06-22 12:59:15", "link": "http://arxiv.org/abs/2506.18020v1", "categories": ["cs.LG", "cs.CR", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Probing the Embedding Space of Transformers via Minimal Token Perturbations", "abstract": "Understanding how information propagates through Transformer models is a key\nchallenge for interpretability. In this work, we study the effects of minimal\ntoken perturbations on the embedding space. In our experiments, we analyze the\nfrequency of which tokens yield to minimal shifts, highlighting that rare\ntokens usually lead to larger shifts. Moreover, we study how perturbations\npropagate across layers, demonstrating that input information is increasingly\nintermixed in deeper layers. Our findings validate the common assumption that\nthe first layers of a model can be used as proxies for model explanations.\nOverall, this work introduces the combination of token perturbations and shifts\non the embedding space as a powerful tool for model interpretability.", "published": "2025-06-22 12:22:56", "link": "http://arxiv.org/abs/2506.18011v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Imputation of Longitudinal Data Using GANs: Challenges and Implications for Classification", "abstract": "Longitudinal data is commonly utilised across various domains, such as\nhealth, biomedical, education and survey studies. This ubiquity has led to a\nrise in statistical, machine and deep learning-based methods for Longitudinal\nData Classification (LDC). However, the intricate nature of the data,\ncharacterised by its multi-dimensionality, causes instance-level heterogeneity\nand temporal correlations that add to the complexity of longitudinal data\nanalysis. Additionally, LDC accuracy is often hampered by the pervasiveness of\nmissing values in longitudinal data. Despite ongoing research that draw on the\ngenerative power and utility of Generative Adversarial Networks (GANs) to\naddress the missing data problem, critical considerations include statistical\nassumptions surrounding longitudinal data and missingness within it, as well as\nother data-level challenges like class imbalance and mixed data types that\nimpact longitudinal data imputation (LDI) and the subsequent LDC process in\nGANs. This paper provides a comprehensive overview of how GANs have been\napplied in LDI, with a focus whether GANS have adequately addressed fundamental\nassumptions about the data from a LDC perspective. We propose a categorisation\nof main approaches to GAN-based LDI, highlight strengths and limitations of\nmethods, identify key research trends, and provide promising future directions.\nOur findings indicate that while GANs show great potential for LDI to improve\nusability and quality of longitudinal data for tasks like LDC, there is need\nfor more versatile approaches that can handle the wider spectrum of challenges\npresented by longitudinal data with missing values. By synthesising current\nknowledge and identifying critical research gaps, this survey aims to guide\nfuture research efforts in developing more effective GAN-based solutions to\naddress LDC challenges.", "published": "2025-06-22 12:09:55", "link": "http://arxiv.org/abs/2506.18007v1", "categories": ["cs.LG", "stat.ML", "I.2.6"], "primary_category": "cs.LG"}
{"title": "h-calibration: Rethinking Classifier Recalibration with Probabilistic Error-Bounded Objective", "abstract": "Deep neural networks have demonstrated remarkable performance across numerous\nlearning tasks but often suffer from miscalibration, resulting in unreliable\nprobability outputs. This has inspired many recent works on mitigating\nmiscalibration, particularly through post-hoc recalibration methods that aim to\nobtain calibrated probabilities without sacrificing the classification\nperformance of pre-trained models. In this study, we summarize and categorize\nprevious works into three general strategies: intuitively designed methods,\nbinning-based methods, and methods based on formulations of ideal calibration.\nThrough theoretical and practical analysis, we highlight ten common limitations\nin previous approaches. To address these limitations, we propose a\nprobabilistic learning framework for calibration called h-calibration, which\ntheoretically constructs an equivalent learning formulation for canonical\ncalibration with boundedness. On this basis, we design a simple yet effective\npost-hoc calibration algorithm. Our method not only overcomes the ten\nidentified limitations but also achieves markedly better performance than\ntraditional methods, as validated by extensive experiments. We further analyze,\nboth theoretically and experimentally, the relationship and advantages of our\nlearning objective compared to traditional proper scoring rule. In summary, our\nprobabilistic framework derives an approximately equivalent differentiable\nobjective for learning error-bounded calibrated probabilities, elucidating the\ncorrespondence and convergence properties of computational statistics with\nrespect to theoretical bounds in canonical calibration. The theoretical\neffectiveness is verified on standard post-hoc calibration benchmarks by\nachieving state-of-the-art performance. This research offers valuable reference\nfor learning reliable likelihood in related fields.", "published": "2025-06-22 09:56:44", "link": "http://arxiv.org/abs/2506.17968v1", "categories": ["cs.LG", "cs.AI", "cs.CV", "math.PR", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Greedy Selection under Independent Increments: A Toy Model Analysis", "abstract": "We study an iterative selection problem over N i.i.d. discrete-time\nstochastic processes with independent increments. At each stage, a fixed number\nof processes are retained based on their observed values. Under this simple\nmodel, we prove that the optimal strategy for selecting the final maximum-value\nprocess is to apply greedy selection at each stage. While the result relies on\nstrong independence assumptions, it offers a clean justification for greedy\nheuristics in multi-stage elimination settings and may serve as a toy example\nfor understanding related algorithms in high-dimensional applications.", "published": "2025-06-22 08:21:23", "link": "http://arxiv.org/abs/2506.17941v1", "categories": ["math.PR", "cs.AI", "stat.ML"], "primary_category": "math.PR"}
{"title": "An entropy-optimal path to humble AI", "abstract": "Progress of AI has led to a creation of very successful, but by no means\nhumble models and tools, especially regarding (i) the huge and further\nexploding costs and resources they demand, and (ii) the over-confidence of\nthese tools with the answers they provide. Here we introduce a novel\nmathematical framework for a non-equilibrium entropy-optimizing reformulation\nof Boltzmann machines based on the exact law of total probability. It results\nin the highly-performant, but much cheaper, gradient-descent-free learning\nframework with mathematically-justified existence and uniqueness criteria, and\nanswer confidence/reliability measures. Comparisons to state-of-the-art AI\ntools in terms of performance, cost and the model descriptor lengths on a set\nof synthetic problems with varying complexity reveal that the proposed method\nresults in more performant and slim models, with the descriptor lengths being\nvery close to the intrinsic complexity scaling bounds for the underlying\nproblems. Applying this framework to historical climate data results in models\nwith systematically higher prediction skills for the onsets of La Ni\\~na and El\nNi\\~no climate phenomena, requiring just few years of climate data for training\n- a small fraction of what is necessary for contemporary climate prediction\ntools.", "published": "2025-06-22 08:13:22", "link": "http://arxiv.org/abs/2506.17940v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Choice of Scoring Rules for Indirect Elicitation of Properties with Parametric Assumptions", "abstract": "People are commonly interested in predicting a statistical property of a\nrandom event such as mean and variance. Proper scoring rules assess the quality\nof predictions and require that the expected score gets uniquely maximized at\nthe precise prediction, in which case we call the score directly elicits the\nproperty. Previous research work has widely studied the existence and the\ncharacterization of proper scoring rules for different properties, but little\nliterature discusses the choice of proper scoring rules for applications at\nhand. In this paper, we explore a novel task, the indirect elicitation of\nproperties with parametric assumptions, where the target property is a function\nof several directly-elicitable sub-properties and the total score is a weighted\nsum of proper scoring rules for each sub-property. Because of the restriction\nto a parametric model class, different settings for the weights lead to\ndifferent constrained optimal solutions. Our goal is to figure out how the\nchoice of weights affects the estimation of the target property and which\nchoice is the best. We start it with simulation studies and observe an\ninteresting pattern: in most cases, the optimal estimation of the target\nproperty changes monotonically with the increase of each weight, and the best\nconfiguration of weights is often to set some weights as zero. To understand\nhow it happens, we first establish the elementary theoretical framework and\nthen provide deeper sufficient conditions for the case of two sub-properties\nand of more sub-properties respectively. The theory on 2-D cases perfectly\ninterprets the experimental results. In higher-dimensional situations, we\nespecially study the linear cases and suggest that more complex settings can be\nunderstood with locally mapping into linear situations or using linear\napproximations when the true values of sub-properties are close enough to the\nparametric space.", "published": "2025-06-22 02:54:51", "link": "http://arxiv.org/abs/2506.17880v1", "categories": ["cs.LG", "stat.ME", "stat.ML"], "primary_category": "cs.LG"}
{"title": "DRO-Augment Framework: Robustness by Synergizing Wasserstein Distributionally Robust Optimization and Data Augmentation", "abstract": "In many real-world applications, ensuring the robustness and stability of\ndeep neural networks (DNNs) is crucial, particularly for image classification\ntasks that encounter various input perturbations. While data augmentation\ntechniques have been widely adopted to enhance the resilience of a trained\nmodel against such perturbations, there remains significant room for\nimprovement in robustness against corrupted data and adversarial attacks\nsimultaneously. To address this challenge, we introduce DRO-Augment, a novel\nframework that integrates Wasserstein Distributionally Robust Optimization\n(W-DRO) with various data augmentation strategies to improve the robustness of\nthe models significantly across a broad spectrum of corruptions. Our method\noutperforms existing augmentation methods under severe data perturbations and\nadversarial attack scenarios while maintaining the accuracy on the clean\ndatasets on a range of benchmark datasets, including but not limited to\nCIFAR-10-C, CIFAR-100-C, MNIST, and Fashion-MNIST. On the theoretical side, we\nestablish novel generalization error bounds for neural networks trained using a\ncomputationally efficient, variation-regularized loss function closely related\nto the W-DRO problem.", "published": "2025-06-22 02:18:03", "link": "http://arxiv.org/abs/2506.17874v1", "categories": ["stat.ML", "cs.CV", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Human Voice is Unique", "abstract": "Voice is increasingly being used as a biometric entity in many applications.\nThese range from speaker identification and verification systems to human\nprofiling technologies that attempt to estimate myriad aspects of the speaker's\npersona from their voice. However, for an entity to be a true biometric\nidentifier, it must be unique. This paper establishes a first framework for\ncalculating the uniqueness of human voice objectively. The approach in this\npaper is based on statistical considerations that take into account a set of\nmeasurable characteristics of the voice signal that bear a causal relationship\nto the vocal production process, but are not inter-dependent or derivable from\neach other. Depending on how we quantize these variables, we show that the\nchances of two people having the same voice in a world populated by 10 billion\npeople range from one in a few thousand, to one in a septillion or less. The\npaper also discusses the implications of these calculations on the choices made\nin voice processing applications.", "published": "2025-06-22 21:38:42", "link": "http://arxiv.org/abs/2506.18182v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "AI Harmonizer: Expanding Vocal Expression with a Generative Neurosymbolic Music AI System", "abstract": "Vocals harmonizers are powerful tools to help solo vocalists enrich their\nmelodies with harmonically supportive voices. These tools exist in various\nforms, from commercially available pedals and software to custom-built systems,\neach employing different methods to generate harmonies. Traditional harmonizers\noften require users to manually specify a key or tonal center, while others\nallow pitch selection via an external keyboard-both approaches demanding some\ndegree of musical expertise. The AI Harmonizer introduces a novel approach by\nautonomously generating musically coherent four-part harmonies without\nrequiring prior harmonic input from the user. By integrating state-of-the-art\ngenerative AI techniques for pitch detection and voice modeling with\ncustom-trained symbolic music models, our system arranges any vocal melody into\nrich choral textures. In this paper, we present our methods, explore potential\napplications in performance and composition, and discuss future directions for\nreal-time implementations. While our system currently operates offline, we\nbelieve it represents a significant step toward AI-assisted vocal performance\nand expressive musical augmentation. We release our implementation on GitHub.", "published": "2025-06-22 19:13:31", "link": "http://arxiv.org/abs/2506.18143v1", "categories": ["cs.HC", "cs.AI", "cs.SD", "eess.AS", "H.5.5"], "primary_category": "cs.HC"}
{"title": "Face-Voice Association for Audiovisual Active Speaker Detection in Egocentric Recordings", "abstract": "Audiovisual active speaker detection (ASD) is conventionally performed by\nmodelling the temporal synchronisation of acoustic and visual speech cues. In\negocentric recordings, however, the efficacy of synchronisation-based methods\nis compromised by occlusions, motion blur, and adverse acoustic conditions. In\nthis work, a novel framework is proposed that exclusively leverages cross-modal\nface-voice associations to determine speaker activity. An existing face-voice\nassociation model is integrated with a transformer-based encoder that\naggregates facial identity information by dynamically weighting each frame\nbased on its visual quality. This system is then coupled with a front-end\nutterance segmentation method, producing a complete ASD system. This work\ndemonstrates that the proposed system, Self-Lifting for audiovisual active\nspeaker detection(SL-ASD), achieves performance comparable to, and in certain\ncases exceeding, that of parameter-intensive synchronisation-based approaches\nwith significantly fewer learnable parameters, thereby validating the\nfeasibility of substituting strict audiovisual synchronisation modelling with\nflexible biometric associations in challenging egocentric scenarios.", "published": "2025-06-22 14:43:37", "link": "http://arxiv.org/abs/2506.18055v1", "categories": ["cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "GD-Retriever: Controllable Generative Text-Music Retrieval with Diffusion Models", "abstract": "Multimodal contrastive models have achieved strong performance in text-audio\nretrieval and zero-shot settings, but improving joint embedding spaces remains\nan active research area. Less attention has been given to making these systems\ncontrollable and interactive for users. In text-music retrieval, the ambiguity\nof freeform language creates a many-to-many mapping, often resulting in\ninflexible or unsatisfying results.\n  We introduce Generative Diffusion Retriever (GDR), a novel framework that\nleverages diffusion models to generate queries in a retrieval-optimized latent\nspace. This enables controllability through generative tools such as negative\nprompting and denoising diffusion implicit models (DDIM) inversion, opening a\nnew direction in retrieval control. GDR improves retrieval performance over\ncontrastive teacher models and supports retrieval in audio-only latent spaces\nusing non-jointly trained encoders. Finally, we demonstrate that GDR enables\neffective post-hoc manipulation of retrieval behavior, enhancing interactive\ncontrol for text-music retrieval tasks.", "published": "2025-06-22 03:30:27", "link": "http://arxiv.org/abs/2506.17886v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Coherent Track-Before-Detect", "abstract": "Accurately tracking an unknown and time-varying number of objects in complex\nenvironments is a significant challenge but a fundamental capability in a\nvariety of applications, including applied ocean sciences, surveillance,\nautonomous driving, and wireless communications. Conventional Bayesian\nmultiobject tracking (MOT) methods typically employ a detect-then-track (DTT)\napproach, where a frontend detector preprocesses raw sensor data to extract\nmeasurements for MOT. The irreversible nature of this preprocessing step can\ndiscard valuable object-related information, particularly impairing the ability\nto resolve weak or closely spaced objects. The track-before-detect (TBD)\nparadigm offers an alternative by operating directly on sensor data. However,\nexisting TBD approaches introduce simplifications to facilitate the development\nof inference methods, such as assuming known signal amplitudes or conditional\nindependence between sensor measurements given object states. These assumptions\ncan lead to suboptimal performance and limit the applicability of the resulting\nTBD methods in realistic scenarios.\n  This paper introduces coherent TBD based on a comprehensive signal model for\nsensor data. The new model accounts for sensor data correlations and amplitude\nfluctuations, enabling the accurate representation of the physics of the\ndata-generating process in TBD. Coherent TBD is suitable for a wide range of\nproblems in active and passive radar, active and passive sonar, as well as\nintegrated sensing and communication systems. Based on a factor graph\nrepresentation of the new measurement model, a scalable belief propagation (BP)\nmethod is developed to perform efficient Bayesian inference. Experimental\nresults, performed with both synthetic and real data, demonstrate that the\nproposed method outperforms state-of-the-art conventional MOT methods.", "published": "2025-06-22 21:20:56", "link": "http://arxiv.org/abs/2506.18177v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Near-Field Propagation and Spatial Non-Stationarity Channel Model for 6-24 GHz (FR3) Extremely Large-Scale MIMO: Adopted by 3GPP for 6G", "abstract": "Next generation cellular deployments are expected to exploit the 6-24 GHz\nfrequency range 3 (FR3) and extremely large-scale multiple-input\nmultiple-output (XL-MIMO) to enable ultra-high data rates and reliability.\nHowever, the significantly enlarged antenna apertures and higher carrier\nfrequencies render the far-field and spatial stationarity assumptions in the\nexisting 3rd generation partnership project (3GPP) channel models invalid,\ngiving rise to new features such as near-field propagation and spatial\nnon-stationarity (SNS). Despite extensive prior research, incorporating these\nnew features within the standardized channel modeling framework remains an open\nissue. To address this, this paper presents a channel modeling framework for\nXL-MIMO systems that incorporates both near-field and SNS features, adopted by\n3GPP. For the near-field propagation feature, the framework models the\ndistances from the base station (BS) and user equipment to the spherical-wave\nsources associated with clusters. These distances are used to characterize\nelement-wise variations of path parameters, such as nonlinear changes in phase\nand angle. To capture the effect of SNS at the BS side, a stochastic-based\napproach is proposed to model SNS caused by incomplete scattering, by\nestablishing power attenuation factors from visibility probability and\nvisibility region to characterize antenna element-wise path power variation. In\naddition, a physical blocker-based approach is introduced to model SNS effects\ncaused by partial blockage. Finally, a simulation framework for near-field and\nSNS is developed within the structure of the existing 3GPP channel model.\nPerformance evaluations demonstrate that the near-field model captures higher\nchannel capacity potential compared to the far-field model. Coupling loss\nresults indicate that SNS leads to more pronounced propagation fading relative\nto the spatial stationary model.", "published": "2025-06-22 03:34:42", "link": "http://arxiv.org/abs/2506.17887v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Statistical Multicriteria Evaluation of LLM-Generated Text", "abstract": "Assessing the quality of LLM-generated text remains a fundamental challenge\nin natural language processing. Current evaluation approaches often rely on\nisolated metrics or simplistic aggregations that fail to capture the nuanced\ntrade-offs between coherence, diversity, fluency, and other relevant indicators\nof text quality. In this work, we adapt a recently proposed framework for\nstatistical inference based on Generalized Stochastic Dominance (GSD) that\naddresses three critical limitations in existing benchmarking methodologies:\nthe inadequacy of single-metric evaluation, the incompatibility between\ncardinal automatic metrics and ordinal human judgments, and the lack of\ninferential statistical guarantees. The GSD-front approach enables simultaneous\nevaluation across multiple quality dimensions while respecting their different\nmeasurement scales, building upon partial orders of decoding strategies, thus\navoiding arbitrary weighting of the involved metrics. By applying this\nframework to evaluate common decoding strategies against human-generated text,\nwe demonstrate its ability to identify statistically significant performance\ndifferences while accounting for potential deviations from the i.i.d.\nassumption of the sampling design.", "published": "2025-06-22 16:08:44", "link": "http://arxiv.org/abs/2506.18082v2", "categories": ["cs.CL", "stat.AP"], "primary_category": "cs.CL"}
{"title": "Multimodal Fusion SLAM with Fourier Attention", "abstract": "Visual SLAM is particularly challenging in environments affected by noise,\nvarying lighting conditions, and darkness. Learning-based optical flow\nalgorithms can leverage multiple modalities to address these challenges, but\ntraditional optical flow-based visual SLAM approaches often require significant\ncomputational resources.To overcome this limitation, we propose FMF-SLAM, an\nefficient multimodal fusion SLAM method that utilizes fast Fourier transform\n(FFT) to enhance the algorithm efficiency. Specifically, we introduce a novel\nFourier-based self-attention and cross-attention mechanism to extract features\nfrom RGB and depth signals. We further enhance the interaction of multimodal\nfeatures by incorporating multi-scale knowledge distillation across modalities.\nWe also demonstrate the practical feasibility of FMF-SLAM in real-world\nscenarios with real time performance by integrating it with a security robot by\nfusing with a global positioning module GNSS-RTK and global Bundle Adjustment.\nOur approach is validated using video sequences from TUM, TartanAir, and our\nreal-world datasets, showcasing state-of-the-art performance under noisy,\nvarying lighting, and dark conditions.Our code and datasets are available at\nhttps://github.com/youjie-zhou/FMF-SLAM.git.", "published": "2025-06-22 23:44:07", "link": "http://arxiv.org/abs/2506.18204v2", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Understanding Reasoning in Thinking Language Models via Steering Vectors", "abstract": "Recent advances in large language models (LLMs) have led to the development\nof thinking language models that generate extensive internal reasoning chains\nbefore producing responses. While these models achieve improved performance,\ncontrolling their reasoning processes remains challenging. This work presents a\nsteering approach for thinking LLMs by analyzing and manipulating specific\nreasoning behaviors in DeepSeek-R1-Distill models. Through a systematic\nexperiment on 500 tasks across 10 diverse categories, we identify several\nreasoning behaviors exhibited by thinking models, including expressing\nuncertainty, generating examples for hypothesis validation, and backtracking in\nreasoning chains. We demonstrate that these behaviors are mediated by linear\ndirections in the model's activation space and can be controlled using steering\nvectors. By extracting and applying these vectors, we provide a method to\nmodulate specific aspects of the model's reasoning process, such as its\ntendency to backtrack or express uncertainty. Our approach offers practical\ntools for steering reasoning processes in thinking models in a controlled and\ninterpretable manner. We validate our steering method using three\nDeepSeek-R1-Distill models, demonstrating consistent control across different\nmodel architectures.", "published": "2025-06-22 20:45:26", "link": "http://arxiv.org/abs/2506.18167v2", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Simultaneous recovery of a corroded boundary and admittance using the Kohn-Vogelius method", "abstract": "We address the problem of identifying an unknown portion $\\Gamma$ of the\nboundary of a $d$-dimensional ($d \\in \\{1, 2\\}$) domain $\\Omega$ and its\nassociated Robin admittance coefficient, using two sets of boundary Cauchy data\n$(f, g)$--representing boundary temperature and heat flux--measured on the\naccessible portion $\\Sigma$ of the boundary. Identifiability results\n\\cite{Bacchelli2009,PaganiPierotti2009} indicate that a single measurement on\n$\\Sigma$ is insufficient to uniquely determine both $\\Gamma$ and $\\alpha$, but\ntwo independent inputs yielding distinct solutions ensure the uniqueness of the\npair $\\Gamma$ and $\\alpha$. In this paper, we propose a cost function based on\nthe energy-gap of two auxiliary problems. We derive the variational derivatives\nof this objective functional with respect to both the Robin boundary $\\Gamma$\nand the admittance coefficient $\\alpha$. These derivatives are utilized to\ndevelop a nonlinear gradient-based iterative scheme for the simultaneous\nnumerical reconstruction of $\\Gamma$ and $\\alpha$. Numerical experiments are\npresented to demonstrate the effectiveness and practicality of the proposed\nmethod.", "published": "2025-06-22 08:09:55", "link": "http://arxiv.org/abs/2506.17938v2", "categories": ["math.NA", "cs.NA", "math.OC", "49Q10, 35R25, 35R30, 49Q12"], "primary_category": "math.NA"}
{"title": "GD-Retriever: Controllable Generative Text-Music Retrieval with Diffusion Models", "abstract": "Multimodal contrastive models have achieved strong performance in text-audio\nretrieval and zero-shot settings, but improving joint embedding spaces remains\nan active research area. Less attention has been given to making these systems\ncontrollable and interactive for users. In text-music retrieval, the ambiguity\nof freeform language creates a many-to-many mapping, often resulting in\ninflexible or unsatisfying results.\n  We introduce Generative Diffusion Retriever (GDR), a novel framework that\nleverages diffusion models to generate queries in a retrieval-optimized latent\nspace. This enables controllability through generative tools such as negative\nprompting and denoising diffusion implicit models (DDIM) inversion, opening a\nnew direction in retrieval control. GDR improves retrieval performance over\ncontrastive teacher models and supports retrieval in audio-only latent spaces\nusing non-jointly trained encoders. Finally, we demonstrate that GDR enables\neffective post-hoc manipulation of retrieval behavior, enhancing interactive\ncontrol for text-music retrieval tasks.", "published": "2025-06-22 03:30:27", "link": "http://arxiv.org/abs/2506.17886v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DRO-Augment Framework: Robustness by Synergizing Wasserstein Distributionally Robust Optimization and Data Augmentation", "abstract": "In many real-world applications, ensuring the robustness and stability of\ndeep neural networks (DNNs) is crucial, particularly for image classification\ntasks that encounter various input perturbations. While data augmentation\ntechniques have been widely adopted to enhance the resilience of a trained\nmodel against such perturbations, there remains significant room for\nimprovement in robustness against corrupted data and adversarial attacks\nsimultaneously. To address this challenge, we introduce DRO-Augment, a novel\nframework that integrates Wasserstein Distributionally Robust Optimization\n(W-DRO) with various data augmentation strategies to improve the robustness of\nthe models significantly across a broad spectrum of corruptions. Our method\noutperforms existing augmentation methods under severe data perturbations and\nadversarial attack scenarios while maintaining the accuracy on the clean\ndatasets on a range of benchmark datasets, including but not limited to\nCIFAR-10-C, CIFAR-100-C, MNIST, and Fashion-MNIST. On the theoretical side, we\nestablish novel generalization error bounds for neural networks trained using a\ncomputationally efficient, variation-regularized loss function closely related\nto the W-DRO problem.", "published": "2025-06-22 02:18:03", "link": "http://arxiv.org/abs/2506.17874v2", "categories": ["stat.ML", "cs.CV", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Coherent Track-Before-Detect", "abstract": "Accurately tracking an unknown and time-varying number of objects in complex\nenvironments is a significant challenge but a fundamental capability in a\nvariety of applications, including applied ocean sciences, surveillance,\nautonomous driving, and wireless communications. Conventional Bayesian\nmultiobject tracking (MOT) methods typically employ a detect-then-track (DTT)\napproach, where a frontend detector preprocesses raw sensor data to extract\nmeasurements for MOT. The irreversible nature of this preprocessing step can\ndiscard valuable object-related information, particularly impairing the ability\nto resolve weak or closely spaced objects. The track-before-detect (TBD)\nparadigm offers an alternative by operating directly on sensor data. However,\nexisting TBD approaches introduce simplifications to facilitate the development\nof inference methods, such as assuming known signal amplitudes or conditional\nindependence between sensor measurements given object states. These assumptions\ncan lead to suboptimal performance and limit the applicability of the resulting\nTBD methods in realistic scenarios.\n  This paper introduces coherent TBD based on a comprehensive signal model for\nsensor data. The new model accounts for sensor data correlations and amplitude\nfluctuations, enabling the accurate representation of the physics of the\ndata-generating process in TBD. Coherent TBD is suitable for a wide range of\nproblems in active and passive radar, active and passive sonar, as well as\nintegrated sensing and communication systems. Based on a factor graph\nrepresentation of the new measurement model, a scalable belief propagation (BP)\nmethod is developed to perform efficient Bayesian inference. Experimental\nresults, performed with both synthetic and real data, demonstrate that the\nproposed method outperforms state-of-the-art conventional MOT methods.", "published": "2025-06-22 21:20:56", "link": "http://arxiv.org/abs/2506.18177v2", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Bayesian-Guided Diversity in Sequential Sampling for Recommender Systems", "abstract": "The challenge of balancing user relevance and content diversity in\nrecommender systems is increasingly critical amid growing concerns about\ncontent homogeneity and reduced user engagement. In this work, we propose a\nnovel framework that leverages a multi-objective, contextual sequential\nsampling strategy. Item selection is guided by Bayesian updates that\ndynamically adjust scores to optimize diversity. The reward formulation\nintegrates multiple diversity metrics-including the log-determinant volume of a\ntuned similarity submatrix and ridge leverage scores-along with a diversity\ngain uncertainty term to address the exploration-exploitation trade-off. Both\nintra- and inter-batch diversity are modeled to promote serendipity and\nminimize redundancy. A dominance-based ranking procedure identifies\nPareto-optimal item sets, enabling adaptive and balanced selections at each\niteration. Experiments on a real-world dataset show that our approach\nsignificantly improves diversity without sacrificing relevance, demonstrating\nits potential to enhance user experience in large-scale recommendation\nsettings.", "published": "2025-06-22 19:36:02", "link": "http://arxiv.org/abs/2506.21617v1", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR"}
{"title": "Refine Medical Diagnosis Using Generation Augmented Retrieval and Clinical Practice Guidelines", "abstract": "Current medical language models, adapted from large language models (LLMs),\ntypically predict ICD code-based diagnosis from electronic health records\n(EHRs) because these labels are readily available. However, ICD codes do not\ncapture the nuanced, context-rich reasoning clinicians use for diagnosis.\nClinicians synthesize diverse patient data and reference clinical practice\nguidelines (CPGs) to make evidence-based decisions. This misalignment limits\nthe clinical utility of existing models. We introduce GARMLE-G, a\nGeneration-Augmented Retrieval framework that grounds medical language model\noutputs in authoritative CPGs. Unlike conventional Retrieval-Augmented\nGeneration based approaches, GARMLE-G enables hallucination-free outputs by\ndirectly retrieving authoritative guideline content without relying on\nmodel-generated text. It (1) integrates LLM predictions with EHR data to create\nsemantically rich queries, (2) retrieves relevant CPG knowledge snippets via\nembedding similarity, and (3) fuses guideline content with model output to\ngenerate clinically aligned recommendations. A prototype system for\nhypertension diagnosis was developed and evaluated on multiple metrics,\ndemonstrating superior retrieval precision, semantic relevance, and clinical\nguideline adherence compared to RAG-based baselines, while maintaining a\nlightweight architecture suitable for localized healthcare deployment. This\nwork provides a scalable, low-cost, and hallucination-free method for grounding\nmedical language models in evidence-based clinical practice, with strong\npotential for broader clinical deployment.", "published": "2025-06-22 11:31:13", "link": "http://arxiv.org/abs/2506.21615v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "DeepSupp: Attention-Driven Correlation Pattern Analysis for Dynamic Time Series Support and Resistance Levels Identification", "abstract": "Support and resistance (SR) levels are central to technical analysis, guiding\ntraders in entry, exit, and risk management. Despite widespread use,\ntraditional SR identification methods often fail to adapt to the complexities\nof modern, volatile markets. Recent research has introduced machine learning\ntechniques to address the following challenges, yet most focus on price\nprediction rather than structural level identification. This paper presents\nDeepSupp, a new deep learning approach for detecting financial support levels\nusing multi-head attention mechanisms to analyze spatial correlations and\nmarket microstructure relationships. DeepSupp integrates advanced feature\nengineering, constructing dynamic correlation matrices that capture evolving\nmarket relationships, and employs an attention-based autoencoder for robust\nrepresentation learning. The final support levels are extracted through\nunsupervised clustering, leveraging DBSCAN to identify significant price\nthresholds. Comprehensive evaluations on S&P 500 tickers demonstrate that\nDeepSupp outperforms six baseline methods, achieving state-of-the-art\nperformance across six financial metrics, including essential support accuracy\nand market regime sensitivity. With consistent results across diverse market\nconditions, DeepSupp addresses critical gaps in SR level detection, offering a\nscalable and reliable solution for modern financial analysis. Our approach\nhighlights the potential of attention-based architectures to uncover nuanced\nmarket patterns and improve technical trading strategies.", "published": "2025-06-22 11:09:55", "link": "http://arxiv.org/abs/2507.01971v1", "categories": ["q-fin.ST", "cs.AI", "cs.CE", "cs.LG"], "primary_category": "q-fin.ST"}
