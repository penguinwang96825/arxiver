{"title": "WikiLingua: A New Benchmark Dataset for Cross-Lingual Abstractive\n  Summarization", "abstract": "We introduce WikiLingua, a large-scale, multilingual dataset for the\nevaluation of crosslingual abstractive summarization systems. We extract\narticle and summary pairs in 18 languages from WikiHow, a high quality,\ncollaborative resource of how-to guides on a diverse set of topics written by\nhuman authors. We create gold-standard article-summary alignments across\nlanguages by aligning the images that are used to describe each how-to step in\nan article. As a set of baselines for further studies, we evaluate the\nperformance of existing cross-lingual abstractive summarization methods on our\ndataset. We further propose a method for direct crosslingual summarization\n(i.e., without requiring translation at inference time) by leveraging synthetic\ndata and Neural Machine Translation as a pre-training step. Our method\nsignificantly outperforms the baseline approaches, while being more cost\nefficient during inference.", "published": "2020-10-07 00:28:05", "link": "http://arxiv.org/abs/2010.03093v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DiPair: Fast and Accurate Distillation for Trillion-Scale Text Matching\n  and Pair Modeling", "abstract": "Pre-trained models like BERT (Devlin et al., 2018) have dominated NLP / IR\napplications such as single sentence classification, text pair classification,\nand question answering. However, deploying these models in real systems is\nhighly non-trivial due to their exorbitant computational costs. A common remedy\nto this is knowledge distillation (Hinton et al., 2015), leading to faster\ninference. However -- as we show here -- existing works are not optimized for\ndealing with pairs (or tuples) of texts. Consequently, they are either not\nscalable or demonstrate subpar performance. In this work, we propose DiPair --\na novel framework for distilling fast and accurate models on text pair tasks.\nCoupled with an end-to-end training strategy, DiPair is both highly scalable\nand offers improved quality-speed tradeoffs. Empirical studies conducted on\nboth academic and real-world e-commerce benchmarks demonstrate the efficacy of\nthe proposed approach with speedups of over 350x and minimal quality drop\nrelative to the cross-attention teacher BERT model.", "published": "2020-10-07 01:19:23", "link": "http://arxiv.org/abs/2010.03099v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pre-training Multilingual Neural Machine Translation by Leveraging\n  Alignment Information", "abstract": "We investigate the following question for machine translation (MT): can we\ndevelop a single universal MT model to serve as the common seed and obtain\nderivative and improved models on arbitrary language pairs? We propose mRASP,\nan approach to pre-train a universal multilingual neural machine translation\nmodel. Our key idea in mRASP is its novel technique of random aligned\nsubstitution, which brings words and phrases with similar meanings across\nmultiple languages closer in the representation space. We pre-train a mRASP\nmodel on 32 language pairs jointly with only public datasets. The model is then\nfine-tuned on downstream language pairs to obtain specialized MT models. We\ncarry out extensive experiments on 42 translation directions across a diverse\nsettings, including low, medium, rich resource, and as well as transferring to\nexotic language pairs. Experimental results demonstrate that mRASP achieves\nsignificant performance improvement compared to directly training on those\ntarget pairs. It is the first time to verify that multiple low-resource\nlanguage pairs can be utilized to improve rich resource MT. Surprisingly, mRASP\nis even able to improve the translation quality on exotic languages that never\noccur in the pre-training corpus. Code, data, and pre-trained models are\navailable at https://github.com/linzehui/mRASP.", "published": "2020-10-07 03:57:54", "link": "http://arxiv.org/abs/2010.03142v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OpenIE6: Iterative Grid Labeling and Coordination Analysis for Open\n  Information Extraction", "abstract": "A recent state-of-the-art neural open information extraction (OpenIE) system\ngenerates extractions iteratively, requiring repeated encoding of partial\noutputs. This comes at a significant computational cost. On the other hand,\nsequence labeling approaches for OpenIE are much faster, but worse in\nextraction quality. In this paper, we bridge this trade-off by presenting an\niterative labeling-based system that establishes a new state of the art for\nOpenIE, while extracting 10x faster. This is achieved through a novel Iterative\nGrid Labeling (IGL) architecture, which treats OpenIE as a 2-D grid labeling\ntask. We improve its performance further by applying coverage (soft)\nconstraints on the grid at training time.\n  Moreover, on observing that the best OpenIE systems falter at handling\ncoordination structures, our OpenIE system also incorporates a new coordination\nanalyzer built with the same IGL architecture. This IGL based coordination\nanalyzer helps our OpenIE system handle complicated coordination structures,\nwhile also establishing a new state of the art on the task of coordination\nanalysis, with a 12.3 pts improvement in F1 over previous analyzers. Our OpenIE\nsystem, OpenIE6, beats the previous systems by as much as 4 pts in F1, while\nbeing much faster.", "published": "2020-10-07 04:05:37", "link": "http://arxiv.org/abs/2010.03147v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fortifying Toxic Speech Detectors Against Veiled Toxicity", "abstract": "Modern toxic speech detectors are incompetent in recognizing disguised\noffensive language, such as adversarial attacks that deliberately avoid known\ntoxic lexicons, or manifestations of implicit bias. Building a large annotated\ndataset for such veiled toxicity can be very expensive. In this work, we\npropose a framework aimed at fortifying existing toxic speech detectors without\na large labeled corpus of veiled toxicity. Just a handful of probing examples\nare used to surface orders of magnitude more disguised offenses. We augment the\ntoxic speech detector's training data with these discovered offensive examples,\nthereby making it more robust to veiled toxicity while preserving its utility\nin detecting overt toxicity.", "published": "2020-10-07 04:43:48", "link": "http://arxiv.org/abs/2010.03154v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Self-Refinement Strategy for Noise Reduction in Grammatical Error\n  Correction", "abstract": "Existing approaches for grammatical error correction (GEC) largely rely on\nsupervised learning with manually created GEC datasets. However, there has been\nlittle focus on verifying and ensuring the quality of the datasets, and on how\nlower-quality data might affect GEC performance. We indeed found that there is\na non-negligible amount of \"noise\" where errors were inappropriately edited or\nleft uncorrected. To address this, we designed a self-refinement method where\nthe key idea is to denoise these datasets by leveraging the prediction\nconsistency of existing models, and outperformed strong denoising baseline\nmethods. We further applied task-specific techniques and achieved\nstate-of-the-art performance on the CoNLL-2014, JFLEG, and BEA-2019 benchmarks.\nWe then analyzed the effect of the proposed denoising method, and found that\nour approach leads to improved coverage of corrections and facilitated fluency\nedits which are reflected in higher recall and overall performance.", "published": "2020-10-07 04:45:09", "link": "http://arxiv.org/abs/2010.03155v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transformer-GCRF: Recovering Chinese Dropped Pronouns with General\n  Conditional Random Fields", "abstract": "Pronouns are often dropped in Chinese conversations and recovering the\ndropped pronouns is important for NLP applications such as Machine Translation.\nExisting approaches usually formulate this as a sequence labeling task of\npredicting whether there is a dropped pronoun before each token and its type.\nEach utterance is considered to be a sequence and labeled independently.\nAlthough these approaches have shown promise, labeling each utterance\nindependently ignores the dependencies between pronouns in neighboring\nutterances. Modeling these dependencies is critical to improving the\nperformance of dropped pronoun recovery. In this paper, we present a novel\nframework that combines the strength of Transformer network with General\nConditional Random Fields (GCRF) to model the dependencies between pronouns in\nneighboring utterances. Results on three Chinese conversation datasets show\nthat the Transformer-GCRF model outperforms the state-of-the-art dropped\npronoun recovery models. Exploratory analysis also demonstrates that the GCRF\ndid help to capture the dependencies between pronouns in neighboring\nutterances, thus contributes to the performance improvements.", "published": "2020-10-07 07:06:09", "link": "http://arxiv.org/abs/2010.03224v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring and Evaluating Attributes, Values, and Structures for Entity\n  Alignment", "abstract": "Entity alignment (EA) aims at building a unified Knowledge Graph (KG) of rich\ncontent by linking the equivalent entities from various KGs. GNN-based EA\nmethods present promising performances by modeling the KG structure defined by\nrelation triples. However, attribute triples can also provide crucial alignment\nsignal but have not been well explored yet. In this paper, we propose to\nutilize an attributed value encoder and partition the KG into subgraphs to\nmodel the various types of attribute triples efficiently. Besides, the\nperformances of current EA methods are overestimated because of the name-bias\nof existing EA datasets. To make an objective evaluation, we propose a hard\nexperimental setting where we select equivalent entity pairs with very\ndifferent names as the test set. Under both the regular and hard settings, our\nmethod achieves significant improvements ($5.10\\%$ on average Hits@$1$ in\nDBP$15$k) over $12$ baselines in cross-lingual and monolingual datasets.\nAblation studies on different subgraphs and a case study about attribute types\nfurther demonstrate the effectiveness of our method. Source code and data can\nbe found at https://github.com/thunlp/explore-and-evaluate.", "published": "2020-10-07 08:03:58", "link": "http://arxiv.org/abs/2010.03249v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving the Efficiency of Grammatical Error Correction with Erroneous\n  Span Detection and Correction", "abstract": "We propose a novel language-independent approach to improve the efficiency\nfor Grammatical Error Correction (GEC) by dividing the task into two subtasks:\nErroneous Span Detection (ESD) and Erroneous Span Correction (ESC). ESD\nidentifies grammatically incorrect text spans with an efficient sequence\ntagging model. Then, ESC leverages a seq2seq model to take the sentence with\nannotated erroneous spans as input and only outputs the corrected text for\nthese spans. Experiments show our approach performs comparably to conventional\nseq2seq approaches in both English and Chinese GEC benchmarks with less than\n50% time cost for inference.", "published": "2020-10-07 08:29:11", "link": "http://arxiv.org/abs/2010.03260v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ZEST: Zero-shot Learning from Text Descriptions using Textual Similarity\n  and Visual Summarization", "abstract": "We study the problem of recognizing visual entities from the textual\ndescriptions of their classes. Specifically, given birds' images with free-text\ndescriptions of their species, we learn to classify images of previously-unseen\nspecies based on specie descriptions. This setup has been studied in the vision\ncommunity under the name zero-shot learning from text, focusing on learning to\ntransfer knowledge about visual aspects of birds from seen classes to\npreviously-unseen ones. Here, we suggest focusing on the textual description\nand distilling from the description the most relevant information to\neffectively match visual features to the parts of the text that discuss them.\nSpecifically, (1) we propose to leverage the similarity between species,\nreflected in the similarity between text descriptions of the species. (2) we\nderive visual summaries of the texts, i.e., extractive summaries that focus on\nthe visual features that tend to be reflected in images. We propose a simple\nattention-based model augmented with the similarity and visual summaries\ncomponents. Our empirical results consistently and significantly outperform the\nstate-of-the-art on the largest benchmarks for text-based zero-shot learning,\nillustrating the critical importance of texts for zero-shot image-recognition.", "published": "2020-10-07 08:57:34", "link": "http://arxiv.org/abs/2010.03276v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "COMETA: A Corpus for Medical Entity Linking in the Social Media", "abstract": "Whilst there has been growing progress in Entity Linking (EL) for general\nlanguage, existing datasets fail to address the complex nature of health\nterminology in layman's language. Meanwhile, there is a growing need for\napplications that can understand the public's voice in the health domain. To\naddress this we introduce a new corpus called COMETA, consisting of 20k English\nbiomedical entity mentions from Reddit expert-annotated with links to SNOMED\nCT, a widely-used medical knowledge graph. Our corpus satisfies a combination\nof desirable properties, from scale and coverage to diversity and quality, that\nto the best of our knowledge has not been met by any of the existing resources\nin the field. Through benchmark experiments on 20 EL baselines from string- to\nneural-based models we shed light on the ability of these systems to perform\ncomplex inference on entities and concepts under 2 challenging evaluation\nscenarios. Our experimental results on COMETA illustrate that no golden bullet\nexists and even the best mainstream techniques still have a significant\nperformance gap to fill, while the best solution relies on combining different\nviews of data.", "published": "2020-10-07 09:16:45", "link": "http://arxiv.org/abs/2010.03295v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving QA Generalization by Concurrent Modeling of Multiple Biases", "abstract": "Existing NLP datasets contain various biases that models can easily exploit\nto achieve high performances on the corresponding evaluation sets. However,\nfocusing on dataset-specific biases limits their ability to learn more\ngeneralizable knowledge about the task from more general data patterns. In this\npaper, we investigate the impact of debiasing methods for improving\ngeneralization and propose a general framework for improving the performance on\nboth in-domain and out-of-domain datasets by concurrent modeling of multiple\nbiases in the training data. Our framework weights each example based on the\nbiases it contains and the strength of those biases in the training data. It\nthen uses these weights in the training objective so that the model relies less\non examples with high bias weights. We extensively evaluate our framework on\nextractive question answering with training data from various domains with\nmultiple biases of different strengths. We perform the evaluations in two\ndifferent settings, in which the model is trained on a single domain or\nmultiple domains simultaneously, and show its effectiveness in both settings\ncompared to state-of-the-art debiasing methods.", "published": "2020-10-07 11:18:49", "link": "http://arxiv.org/abs/2010.03338v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Why do you think that? Exploring Faithful Sentence-Level Rationales\n  Without Supervision", "abstract": "Evaluating the trustworthiness of a model's prediction is essential for\ndifferentiating between `right for the right reasons' and `right for the wrong\nreasons'. Identifying textual spans that determine the target label, known as\nfaithful rationales, usually relies on pipeline approaches or reinforcement\nlearning. However, such methods either require supervision and thus costly\nannotation of the rationales or employ non-differentiable models. We propose a\ndifferentiable training-framework to create models which output faithful\nrationales on a sentence level, by solely applying supervision on the target\ntask. To achieve this, our model solves the task based on each rationale\nindividually and learns to assign high scores to those which solved the task\nbest. Our evaluation on three different datasets shows competitive results\ncompared to a standard BERT blackbox while exceeding a pipeline counterpart's\nperformance in two cases. We further exploit the transparent decision-making\nprocess of these models to prefer selecting the correct rationales by applying\ndirect supervision, thereby boosting the performance on the rationale-level.", "published": "2020-10-07 12:54:28", "link": "http://arxiv.org/abs/2010.03384v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-lingual Extended Named Entity Classification of Wikipedia Articles", "abstract": "The FPT.AI team participated in the SHINRA2020-ML subtask of the NTCIR-15\nSHINRA task. This paper describes our method to solving the problem and\ndiscusses the official results. Our method focuses on learning cross-lingual\nrepresentations, both on the word level and document level for page\nclassification. We propose a three-stage approach including multilingual model\npre-training, monolingual model fine-tuning and cross-lingual voting. Our\nsystem is able to achieve the best scores for 25 out of 30 languages; and its\naccuracy gaps to the best performing systems of the other five languages are\nrelatively small.", "published": "2020-10-07 14:06:09", "link": "http://arxiv.org/abs/2010.03424v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"I'd rather just go to bed\": Understanding Indirect Answers", "abstract": "We revisit a pragmatic inference problem in dialog: understanding indirect\nresponses to questions. Humans can interpret 'I'm starving.' in response to\n'Hungry?', even without direct cue words such as 'yes' and 'no'. In dialog\nsystems, allowing natural responses rather than closed vocabularies would be\nsimilarly beneficial. However, today's systems are only as sensitive to these\npragmatic moves as their language model allows. We create and release the first\nlarge-scale English language corpus 'Circa' with 34,268 (polar question,\nindirect answer) pairs to enable progress on this task. The data was collected\nvia elaborate crowdsourcing, and contains utterances with yes/no meaning, as\nwell as uncertain, middle-ground, and conditional responses. We also present\nBERT-based neural models to predict such categories for a question-answer pair.\nWe find that while transfer learning from entailment works reasonably,\nperformance is not yet sufficient for robust dialog. Our models reach 82-88%\naccuracy for a 4-class distinction, and 74-85% for 6 classes.", "published": "2020-10-07 14:41:40", "link": "http://arxiv.org/abs/2010.03450v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning a Cost-Effective Annotation Policy for Question Answering", "abstract": "State-of-the-art question answering (QA) relies upon large amounts of\ntraining data for which labeling is time consuming and thus expensive. For this\nreason, customizing QA systems is challenging. As a remedy, we propose a novel\nframework for annotating QA datasets that entails learning a cost-effective\nannotation policy and a semi-supervised annotation scheme. The latter reduces\nthe human effort: it leverages the underlying QA system to suggest potential\ncandidate annotations. Human annotators then simply provide binary feedback on\nthese candidates. Our system is designed such that past annotations\ncontinuously improve the future performance and thus overall annotation cost.\nTo the best of our knowledge, this is the first paper to address the problem of\nannotating questions with minimal annotation cost. We compare our framework\nagainst traditional manual annotations in an extensive set of experiments. We\nfind that our approach can reduce up to 21.1% of the annotation cost.", "published": "2020-10-07 15:25:41", "link": "http://arxiv.org/abs/2010.03476v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ELMo and BERT in semantic change detection for Russian", "abstract": "We study the effectiveness of contextualized embeddings for the task of\ndiachronic semantic change detection for Russian language data. Evaluation test\nsets consist of Russian nouns and adjectives annotated based on their\noccurrences in texts created in pre-Soviet, Soviet and post-Soviet time\nperiods. ELMo and BERT architectures are compared on the task of ranking\nRussian words according to the degree of their semantic change over time. We\nuse several methods for aggregation of contextualized embeddings from these\narchitectures and evaluate their performance. Finally, we compare unsupervised\nand supervised techniques in this task.", "published": "2020-10-07 15:34:00", "link": "http://arxiv.org/abs/2010.03481v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Sentiment Analysis over non-English Tweets using Multilingual\n  Transformers and Automatic Translation for Data-Augmentation", "abstract": "Tweets are specific text data when compared to general text. Although\nsentiment analysis over tweets has become very popular in the last decade for\nEnglish, it is still difficult to find huge annotated corpora for non-English\nlanguages. The recent rise of the transformer models in Natural Language\nProcessing allows to achieve unparalleled performances in many tasks, but these\nmodels need a consequent quantity of text to adapt to the tweet domain. We\npropose the use of a multilingual transformer model, that we pre-train over\nEnglish tweets and apply data-augmentation using automatic translation to adapt\nthe model to non-English languages. Our experiments in French, Spanish, German\nand Italian suggest that the proposed technique is an efficient way to improve\nthe results of the transformers over small corpora of tweets in a non-English\nlanguage.", "published": "2020-10-07 15:44:55", "link": "http://arxiv.org/abs/2010.03486v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TeaForN: Teacher-Forcing with N-grams", "abstract": "Sequence generation models trained with teacher-forcing suffer from issues\nrelated to exposure bias and lack of differentiability across timesteps. Our\nproposed method, Teacher-Forcing with N-grams (TeaForN), addresses both these\nproblems directly, through the use of a stack of N decoders trained to decode\nalong a secondary time axis that allows model parameter updates based on N\nprediction steps. TeaForN can be used with a wide class of decoder\narchitectures and requires minimal modifications from a standard\nteacher-forcing setup. Empirically, we show that TeaForN boosts generation\nquality on one Machine Translation benchmark, WMT 2014 English-French, and two\nNews Summarization benchmarks, CNN/Dailymail and Gigaword.", "published": "2020-10-07 15:58:25", "link": "http://arxiv.org/abs/2010.03494v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring the Role of Argument Structure in Online Debate Persuasion", "abstract": "Online debate forums provide users a platform to express their opinions on\ncontroversial topics while being exposed to opinions from diverse set of\nviewpoints. Existing work in Natural Language Processing (NLP) has shown that\nlinguistic features extracted from the debate text and features encoding the\ncharacteristics of the audience are both critical in persuasion studies. In\nthis paper, we aim to further investigate the role of discourse structure of\nthe arguments from online debates in their persuasiveness. In particular, we\nuse the factor graph model to obtain features for the argument structure of\ndebates from an online debating platform and incorporate these features to an\nLSTM-based model to predict the debater that makes the most convincing\narguments. We find that incorporating argument structure features play an\nessential role in achieving the better predictive performance in assessing the\npersuasiveness of the arguments in online debates.", "published": "2020-10-07 17:34:50", "link": "http://arxiv.org/abs/2010.03538v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Galileo at SemEval-2020 Task 12: Multi-lingual Learning for Offensive\n  Language Identification using Pre-trained Language Models", "abstract": "This paper describes Galileo's performance in SemEval-2020 Task 12 on\ndetecting and categorizing offensive language in social media. For Offensive\nLanguage Identification, we proposed a multi-lingual method using Pre-trained\nLanguage Models, ERNIE and XLM-R. For offensive language categorization, we\nproposed a knowledge distillation method trained on soft labels generated by\nseveral supervised models. Our team participated in all three sub-tasks. In\nSub-task A - Offensive Language Identification, we ranked first in terms of\naverage F1 scores in all languages. We are also the only team which ranked\namong the top three across all languages. We also took the first place in\nSub-task B - Automatic Categorization of Offense Types and Sub-task C - Offence\nTarget Identification.", "published": "2020-10-07 17:40:19", "link": "http://arxiv.org/abs/2010.03542v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Low-Resource Domain Adaptation for Compositional Task-Oriented Semantic\n  Parsing", "abstract": "Task-oriented semantic parsing is a critical component of virtual assistants,\nwhich is responsible for understanding the user's intents (set reminder, play\nmusic, etc.). Recent advances in deep learning have enabled several approaches\nto successfully parse more complex queries (Gupta et al., 2018; Rongali et\nal.,2020), but these models require a large amount of annotated training data\nto parse queries on new domains (e.g. reminder, music).\n  In this paper, we focus on adapting task-oriented semantic parsers to\nlow-resource domains, and propose a novel method that outperforms a supervised\nneural model at a 10-fold data reduction. In particular, we identify two\nfundamental factors for low-resource domain adaptation: better representation\nlearning and better training techniques. Our representation learning uses BART\n(Lewis et al., 2019) to initialize our model which outperforms encoder-only\npre-trained representations used in previous work. Furthermore, we train with\noptimization-based meta-learning (Finn et al., 2017) to improve generalization\nto low-resource domains. This approach significantly outperforms all baseline\nmethods in the experiments on a newly collected multi-domain task-oriented\nsemantic parsing dataset (TOPv2), which we release to the public.", "published": "2020-10-07 17:47:53", "link": "http://arxiv.org/abs/2010.03546v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Probabilistic Case-based Reasoning for Open-World Knowledge Graph\n  Completion", "abstract": "A case-based reasoning (CBR) system solves a new problem by retrieving\n`cases' that are similar to the given problem. If such a system can achieve\nhigh accuracy, it is appealing owing to its simplicity, interpretability, and\nscalability. In this paper, we demonstrate that such a system is achievable for\nreasoning in knowledge-bases (KBs). Our approach predicts attributes for an\nentity by gathering reasoning paths from similar entities in the KB. Our\nprobabilistic model estimates the likelihood that a path is effective at\nanswering a query about the given entity. The parameters of our model can be\nefficiently computed using simple path statistics and require no iterative\noptimization. Our model is non-parametric, growing dynamically as new entities\nand relations are added to the KB. On several benchmark datasets our approach\nsignificantly outperforms other rule learning approaches and performs\ncomparably to state-of-the-art embedding-based approaches. Furthermore, we\ndemonstrate the effectiveness of our model in an \"open-world\" setting where new\nentities arrive in an online fashion, significantly outperforming\nstate-of-the-art approaches and nearly matching the best offline method. Code\navailable at https://github.com/ameyagodbole/Prob-CBR", "published": "2020-10-07 17:48:12", "link": "http://arxiv.org/abs/2010.03548v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding Clinical Trial Reports: Extracting Medical Entities and\n  Their Relations", "abstract": "The best evidence concerning comparative treatment effectiveness comes from\nclinical trials, the results of which are reported in unstructured articles.\nMedical experts must manually extract information from articles to inform\ndecision-making, which is time-consuming and expensive. Here we consider the\nend-to-end task of both (a) extracting treatments and outcomes from full-text\narticles describing clinical trials (entity identification) and, (b) inferring\nthe reported results for the former with respect to the latter (relation\nextraction). We introduce new data for this task, and evaluate models that have\nrecently achieved state-of-the-art results on similar tasks in Natural Language\nProcessing. We then propose a new method motivated by how trial results are\ntypically presented that outperforms these purely data-driven baselines.\nFinally, we run a fielded evaluation of the model with a non-profit seeking to\nidentify existing drugs that might be re-purposed for cancer, showing the\npotential utility of end-to-end evidence extraction systems.", "published": "2020-10-07 17:50:58", "link": "http://arxiv.org/abs/2010.03550v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SRLGRN: Semantic Role Labeling Graph Reasoning Network", "abstract": "This work deals with the challenge of learning and reasoning over multi-hop\nquestion answering (QA). We propose a graph reasoning network based on the\nsemantic structure of the sentences to learn cross paragraph reasoning paths\nand find the supporting facts and the answer jointly. The proposed graph is a\nheterogeneous document-level graph that contains nodes of type sentence\n(question, title, and other sentences), and semantic role labeling sub-graphs\nper sentence that contain arguments as nodes and predicates as edges.\nIncorporating the argument types, the argument phrases, and the semantics of\nthe edges originated from SRL predicates into the graph encoder helps in\nfinding and also the explainability of the reasoning paths. Our proposed\napproach shows competitive performance on the HotpotQA distractor setting\nbenchmark compared to the recent state-of-the-art models.", "published": "2020-10-07 18:51:17", "link": "http://arxiv.org/abs/2010.03604v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Combining Deep Learning and String Kernels for the Localization of Swiss\n  German Tweets", "abstract": "In this work, we introduce the methods proposed by the UnibucKernel team in\nsolving the Social Media Variety Geolocation task featured in the 2020 VarDial\nEvaluation Campaign. We address only the second subtask, which targets a data\nset composed of nearly 30 thousand Swiss German Jodels. The dialect\nidentification task is about accurately predicting the latitude and longitude\nof test samples. We frame the task as a double regression problem, employing a\nvariety of machine learning approaches to predict both latitude and longitude.\nFrom simple models for regression, such as Support Vector Regression, to deep\nneural networks, such as Long Short-Term Memory networks and character-level\nconvolutional neural networks, and, finally, to ensemble models based on\nmeta-learners, such as XGBoost, our interest is focused on approaching the\nproblem from a few different perspectives, in an attempt to minimize the\nprediction error. With the same goal in mind, we also considered many types of\nfeatures, from high-level features, such as BERT embeddings, to low-level\nfeatures, such as characters n-grams, which are known to provide good results\nin dialect identification. Our empirical results indicate that the handcrafted\nmodel based on string kernels outperforms the deep learning approaches.\nNevertheless, our best performance is given by the ensemble model that combines\nboth handcrafted and deep learning models.", "published": "2020-10-07 19:16:45", "link": "http://arxiv.org/abs/2010.03614v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MuSeM: Detecting Incongruent News Headlines using Mutual Attentive\n  Semantic Matching", "abstract": "Measuring the congruence between two texts has several useful applications,\nsuch as detecting the prevalent deceptive and misleading news headlines on the\nweb. Many works have proposed machine learning based solutions such as text\nsimilarity between the headline and body text to detect the incongruence. Text\nsimilarity based methods fail to perform well due to different inherent\nchallenges such as relative length mismatch between the news headline and its\nbody content and non-overlapping vocabulary. On the other hand, more recent\nworks that use headline guided attention to learn a headline derived contextual\nrepresentation of the news body also result in convoluting overall\nrepresentation due to the news body's lengthiness. This paper proposes a method\nthat uses inter-mutual attention-based semantic matching between the original\nand synthetically generated headlines, which utilizes the difference between\nall pairs of word embeddings of words involved. The paper also investigates two\nmore variations of our method, which use concatenation and dot-products of word\nembeddings of the words of original and synthetic headlines. We observe that\nthe proposed method outperforms prior arts significantly for two publicly\navailable datasets.", "published": "2020-10-07 19:19:42", "link": "http://arxiv.org/abs/2010.03617v1", "categories": ["cs.CL", "68T50", "H.1.1; H.3.1; H.3.3"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Stance Detection: A Dataset and Model using Generalized Topic\n  Representations", "abstract": "Stance detection is an important component of understanding hidden influences\nin everyday life. Since there are thousands of potential topics to take a\nstance on, most with little to no training data, we focus on zero-shot stance\ndetection: classifying stance from no training examples. In this paper, we\npresent a new dataset for zero-shot stance detection that captures a wider\nrange of topics and lexical variation than in previous datasets. Additionally,\nwe propose a new model for stance detection that implicitly captures\nrelationships between topics using generalized topic representations and show\nthat this model improves performance on a number of challenging linguistic\nphenomena.", "published": "2020-10-07 20:27:12", "link": "http://arxiv.org/abs/2010.03640v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-Thought for Sentence Encoder Pre-training", "abstract": "In this paper, we propose Cross-Thought, a novel approach to pre-training\nsequence encoder, which is instrumental in building reusable sequence\nembeddings for large-scale NLP tasks such as question answering. Instead of\nusing the original signals of full sentences, we train a Transformer-based\nsequence encoder over a large set of short sequences, which allows the model to\nautomatically select the most useful information for predicting masked words.\nExperiments on question answering and textual entailment tasks demonstrate that\nour pre-trained encoder can outperform state-of-the-art encoders trained with\ncontinuous sentence signals as well as traditional masked language modeling\nbaselines. Our proposed approach also achieves new state of the art on HotpotQA\n(full-wiki setting) by improving intermediate information retrieval\nperformance.", "published": "2020-10-07 21:02:41", "link": "http://arxiv.org/abs/2010.03652v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exposing Shallow Heuristics of Relation Extraction Models with Challenge\n  Data", "abstract": "The process of collecting and annotating training data may introduce\ndistribution artifacts which may limit the ability of models to learn correct\ngeneralization behavior. We identify failure modes of SOTA relation extraction\n(RE) models trained on TACRED, which we attribute to limitations in the data\nannotation process. We collect and annotate a challenge-set we call Challenging\nRE (CRE), based on naturally occurring corpus examples, to benchmark this\nbehavior. Our experiments with four state-of-the-art RE models show that they\nhave indeed adopted shallow heuristics that do not generalize to the\nchallenge-set data. Further, we find that alternative question answering\nmodeling performs significantly better than the SOTA models on the\nchallenge-set, despite worse overall TACRED performance. By adding some of the\nchallenge data as training examples, the performance of the model improves.\nFinally, we provide concrete suggestion on how to improve RE data collection to\nalleviate this behavior.", "published": "2020-10-07 21:17:25", "link": "http://arxiv.org/abs/2010.03656v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Fine-Grained Cross-Lingual Semantic Divergences without\n  Supervision by Learning to Rank", "abstract": "Detecting fine-grained differences in content conveyed in different languages\nmatters for cross-lingual NLP and multilingual corpora analysis, but it is a\nchallenging machine learning problem since annotation is expensive and hard to\nscale. This work improves the prediction and annotation of fine-grained\nsemantic divergences. We introduce a training strategy for multilingual BERT\nmodels by learning to rank synthetic divergent examples of varying granularity.\nWe evaluate our models on the Rationalized English-French Semantic Divergences,\na new dataset released with this work, consisting of English-French\nsentence-pairs annotated with semantic divergence classes and token-level\nrationales. Learning to rank helps detect fine-grained sentence-level\ndivergences more accurately than a strong sentence-level similarity model,\nwhile token-level predictions have the potential of further distinguishing\nbetween coarse and fine-grained divergences.", "published": "2020-10-07 21:26:20", "link": "http://arxiv.org/abs/2010.03662v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge-aware Method for Confusing Charge Prediction", "abstract": "Automatic charge prediction task aims to determine the final charges based on\nfact descriptions of criminal cases, which is a vital application of legal\nassistant systems. Conventional works usually depend on fact descriptions to\npredict charges while ignoring the legal schematic knowledge, which makes it\ndifficult to distinguish confusing charges. In this paper, we propose a\nknowledge-attentive neural network model, which introduces legal schematic\nknowledge about charges and exploit the knowledge hierarchical representation\nas the discriminative features to differentiate confusing charges. Our model\ntakes the textual fact description as the input and learns fact representation\nthrough a graph convolutional network. A legal schematic knowledge transformer\nis utilized to generate crucial knowledge representations oriented to the legal\nschematic knowledge at both the schema and charge levels. We apply a knowledge\nmatching network for effectively incorporating charge information into the fact\nto learn knowledge-aware fact representation. Finally, we use the\nknowledge-aware fact representation for charge prediction. We create two\nreal-world datasets and experimental results show that our proposed model can\noutperform other state-of-the-art baselines on accuracy and F1 score,\nespecially on dealing with confusing charges.", "published": "2020-10-07 00:58:10", "link": "http://arxiv.org/abs/2010.03096v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "VCDM: Leveraging Variational Bi-encoding and Deep Contextualized Word\n  Representations for Improved Definition Modeling", "abstract": "In this paper, we tackle the task of definition modeling, where the goal is\nto learn to generate definitions of words and phrases. Existing approaches for\nthis task are discriminative, combining distributional and lexical semantics in\nan implicit rather than direct way. To tackle this issue we propose a\ngenerative model for the task, introducing a continuous latent variable to\nexplicitly model the underlying relationship between a phrase used within a\ncontext and its definition. We rely on variational inference for estimation and\nleverage contextualized word embeddings for improved performance. Our approach\nis evaluated on four existing challenging benchmarks with the addition of two\nnew datasets, \"Cambridge\" and the first non-English corpus \"Robert\", which we\nrelease to complement our empirical study. Our Variational Contextual\nDefinition Modeler (VCDM) achieves state-of-the-art performance in terms of\nautomatic and human evaluation metrics, demonstrating the effectiveness of our\napproach.", "published": "2020-10-07 02:48:44", "link": "http://arxiv.org/abs/2010.03124v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Linguistic Analysis of Visually Grounded Dialogues Based on Spatial\n  Expressions", "abstract": "Recent models achieve promising results in visually grounded dialogues.\nHowever, existing datasets often contain undesirable biases and lack\nsophisticated linguistic analyses, which make it difficult to understand how\nwell current models recognize their precise linguistic structures. To address\nthis problem, we make two design choices: first, we focus on OneCommon Corpus\n\\citep{udagawa2019natural,udagawa2020annotated}, a simple yet challenging\ncommon grounding dataset which contains minimal bias by design. Second, we\nanalyze their linguistic structures based on \\textit{spatial expressions} and\nprovide comprehensive and reliable annotation for 600 dialogues. We show that\nour annotation captures important linguistic structures including\npredicate-argument structure, modification and ellipsis. In our experiments, we\nassess the model's understanding of these structures through reference\nresolution. We demonstrate that our annotation can reveal both the strengths\nand weaknesses of baseline models in essential levels of detail. Overall, we\npropose a novel framework and resource for investigating fine-grained language\nunderstanding in visually grounded dialogues.", "published": "2020-10-07 02:50:38", "link": "http://arxiv.org/abs/2010.03127v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Context Modeling in Neural Topic Segmentation", "abstract": "Topic segmentation is critical in key NLP tasks and recent works favor highly\neffective neural supervised approaches. However, current neural solutions are\narguably limited in how they model context. In this paper, we enhance a\nsegmenter based on a hierarchical attention BiLSTM network to better model\ncontext, by adding a coherence-related auxiliary task and restricted\nself-attention. Our optimized segmenter outperforms SOTA approaches when\ntrained and tested on three datasets. We also the robustness of our proposed\nmodel in domain transfer setting by training a model on a large-scale dataset\nand testing it on four challenging real-world benchmarks. Furthermore, we apply\nour proposed strategy to two other languages (German and Chinese), and show its\neffectiveness in multilingual scenarios.", "published": "2020-10-07 03:40:49", "link": "http://arxiv.org/abs/2010.03138v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unsupervised Parsing via Constituency Tests", "abstract": "We propose a method for unsupervised parsing based on the linguistic notion\nof a constituency test. One type of constituency test involves modifying the\nsentence via some transformation (e.g. replacing the span with a pronoun) and\nthen judging the result (e.g. checking if it is grammatical). Motivated by this\nidea, we design an unsupervised parser by specifying a set of transformations\nand using an unsupervised neural acceptability model to make grammaticality\ndecisions. To produce a tree given a sentence, we score each span by\naggregating its constituency test judgments, and we choose the binary tree with\nthe highest total score. While this approach already achieves performance in\nthe range of current methods, we further improve accuracy by fine-tuning the\ngrammaticality model through a refinement procedure, where we alternate between\nimproving the estimated trees and improving the grammaticality model. The\nrefined model achieves 62.8 F1 on the Penn Treebank test set, an absolute\nimprovement of 7.6 points over the previous best published result.", "published": "2020-10-07 04:05:01", "link": "http://arxiv.org/abs/2010.03146v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Knowledge-enriched, Type-constrained and Grammar-guided Question\n  Generation over Knowledge Bases", "abstract": "Question generation over knowledge bases (KBQG) aims at generating\nnatural-language questions about a subgraph, i.e. a set of (connected) triples.\nTwo main challenges still face the current crop of encoder-decoder-based\nmethods, especially on small subgraphs: (1) low diversity and poor fluency due\nto the limited information contained in the subgraphs, and (2) semantic drift\ndue to the decoder's oblivion of the semantics of the answer entity. We propose\nan innovative knowledge-enriched, type-constrained and grammar-guided KBQG\nmodel, named KTG, to addresses the above challenges. In our model, the encoder\nis equipped with auxiliary information from the KB, and the decoder is\nconstrained with word types during QG. Specifically, entity domain and\ndescription, as well as relation hierarchy information are considered to\nconstruct question contexts, while a conditional copy mechanism is incorporated\nto modulate question semantics according to current word types. Besides, a\nnovel reward function featuring grammatical similarity is designed to improve\nboth generative richness and syntactic correctness via reinforcement learning.\nExtensive experiments show that our proposed model outperforms existing methods\nby a significant margin on two widely-used benchmark datasets SimpleQuestion\nand PathQuestion.", "published": "2020-10-07 04:49:48", "link": "http://arxiv.org/abs/2010.03157v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Transfer Learning and Distant Supervision for Multilingual Transformer\n  Models: A Study on African Languages", "abstract": "Multilingual transformer models like mBERT and XLM-RoBERTa have obtained\ngreat improvements for many NLP tasks on a variety of languages. However,\nrecent works also showed that results from high-resource languages could not be\neasily transferred to realistic, low-resource scenarios. In this work, we study\ntrends in performance for different amounts of available resources for the\nthree African languages Hausa, isiXhosa and Yor\\`ub\\'a on both NER and topic\nclassification. We show that in combination with transfer learning or distant\nsupervision, these models can achieve with as little as 10 or 100 labeled\nsentences the same performance as baselines with much more supervised training\ndata. However, we also find settings where this does not hold. Our discussions\nand additional experiments on assumptions such as time and hardware\nrestrictions highlight challenges and opportunities in low-resource learning.", "published": "2020-10-07 05:23:27", "link": "http://arxiv.org/abs/2010.03179v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Theedhum Nandrum@Dravidian-CodeMix-FIRE2020: A Sentiment Polarity\n  Classifier for YouTube Comments with Code-switching between Tamil, Malayalam\n  and English", "abstract": "Theedhum Nandrum is a sentiment polarity detection system using two\napproaches--a Stochastic Gradient Descent (SGD) based classifier and a Long\nShort-term Memory (LSTM) based Classifier. Our approach utilises language\nfeatures like use of emoji, choice of scripts and code mixing which appeared\nquite marked in the datasets specified for the Dravidian Codemix - FIRE 2020\ntask. The hyperparameters for the SGD were tuned using GridSearchCV. Our system\nwas ranked 4th in Tamil-English with a weighted average F1 score of 0.62 and\n9th in Malayalam-English with a score of 0.65. We achieved a weighted average\nF1 score of 0.77 for Tamil-English using a Logistic Regression based model\nafter the task deadline. This performance betters the top ranked classifier on\nthis dataset by a wide margin. Our use of language-specific Soundex to\nharmonise the spelling variants in code-mixed data appears to be a novel\napplication of Soundex. Our complete code is published in github at\nhttps://github.com/oligoglot/theedhum-nandrum.", "published": "2020-10-07 05:40:25", "link": "http://arxiv.org/abs/2010.03189v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Like hiking? You probably enjoy nature: Persona-grounded Dialog with\n  Commonsense Expansions", "abstract": "Existing persona-grounded dialog models often fail to capture simple\nimplications of given persona descriptions, something which humans are able to\ndo seamlessly. For example, state-of-the-art models cannot infer that interest\nin hiking might imply love for nature or longing for a break. In this paper, we\npropose to expand available persona sentences using existing commonsense\nknowledge bases and paraphrasing resources to imbue dialog models with access\nto an expanded and richer set of persona descriptions. Additionally, we\nintroduce fine-grained grounding on personas by encouraging the model to make a\ndiscrete choice among persona sentences while synthesizing a dialog response.\nSince such a choice is not observed in the data, we model it using a discrete\nlatent random variable and use variational learning to sample from hundreds of\npersona expansions. Our model outperforms competitive baselines on the\nPersonaChat dataset in terms of dialog quality and diversity while achieving\npersona-consistent and controllable dialog generation.", "published": "2020-10-07 06:25:39", "link": "http://arxiv.org/abs/2010.03205v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unsupervised Evaluation for Question Answering with Transformers", "abstract": "It is challenging to automatically evaluate the answer of a QA model at\ninference time. Although many models provide confidence scores, and simple\nheuristics can go a long way towards indicating answer correctness, such\nmeasures are heavily dataset-dependent and are unlikely to generalize. In this\nwork, we begin by investigating the hidden representations of questions,\nanswers, and contexts in transformer-based QA architectures. We observe a\nconsistent pattern in the answer representations, which we show can be used to\nautomatically evaluate whether or not a predicted answer span is correct. Our\nmethod does not require any labeled data and outperforms strong heuristic\nbaselines, across 2 datasets and 7 domains. We are able to predict whether or\nnot a model's answer is correct with 91.37% accuracy on SQuAD, and 80.7%\naccuracy on SubjQA. We expect that this method will have broad applications,\ne.g., in the semi-automatic development of QA datasets", "published": "2020-10-07 07:03:30", "link": "http://arxiv.org/abs/2010.03222v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Narrative Text Generation with a Latent Discrete Plan", "abstract": "Past work on story generation has demonstrated the usefulness of conditioning\non a generation plan to generate coherent stories. However, these approaches\nhave used heuristics or off-the-shelf models to first tag training stories with\nthe desired type of plan, and then train generation models in a supervised\nfashion. In this paper, we propose a deep latent variable model that first\nsamples a sequence of anchor words, one per sentence in the story, as part of\nits generative process. During training, our model treats the sequence of\nanchor words as a latent variable and attempts to induce anchoring sequences\nthat help guide generation in an unsupervised fashion. We conduct experiments\nwith several types of sentence decoder distributions: left-to-right and\nnon-monotonic, with different degrees of restriction. Further, since we use\namortized variational inference to train our model, we introduce two\ncorresponding types of inference network for predicting the posterior on anchor\nwords. We conduct human evaluations which demonstrate that the stories produced\nby our model are rated better in comparison with baselines which do not\nconsider story plans, and are similar or better in quality relative to\nbaselines which use external supervision for plans. Additionally, the proposed\nmodel gets favorable scores when evaluated on perplexity, diversity, and\ncontrol of story via discrete plan.", "published": "2020-10-07 08:45:37", "link": "http://arxiv.org/abs/2010.03272v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning to Explain: Datasets and Models for Identifying Valid Reasoning\n  Chains in Multihop Question-Answering", "abstract": "Despite the rapid progress in multihop question-answering (QA), models still\nhave trouble explaining why an answer is correct, with limited explanation\ntraining data available to learn from. To address this, we introduce three\nexplanation datasets in which explanations formed from corpus facts are\nannotated. Our first dataset, eQASC, contains over 98K explanation annotations\nfor the multihop question answering dataset QASC, and is the first that\nannotates multiple candidate explanations for each answer. The second dataset\neQASC-perturbed is constructed by crowd-sourcing perturbations (while\npreserving their validity) of a subset of explanations in QASC, to test\nconsistency and generalization of explanation prediction models. The third\ndataset eOBQA is constructed by adding explanation annotations to the OBQA\ndataset to test generalization of models trained on eQASC. We show that this\ndata can be used to significantly improve explanation quality (+14% absolute F1\nover a strong retrieval baseline) using a BERT-based classifier, but still\nbehind the upper bound, offering a new challenge for future research. We also\nexplore a delexicalized chain representation in which repeated noun phrases are\nreplaced by variables, thus turning them into generalized reasoning chains (for\nexample: \"X is a Y\" AND \"Y has Z\" IMPLIES \"X has Z\"). We find that generalized\nchains maintain performance while also being more robust to certain\nperturbations.", "published": "2020-10-07 08:46:02", "link": "http://arxiv.org/abs/2010.03274v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Slice-Aware Neural Ranking", "abstract": "Understanding when and why neural ranking models fail for an IR task via\nerror analysis is an important part of the research cycle. Here we focus on the\nchallenges of (i) identifying categories of difficult instances (a pair of\nquestion and response candidates) for which a neural ranker is ineffective and\n(ii) improving neural ranking for such instances. To address both challenges we\nresort to slice-based learning for which the goal is to improve effectiveness\nof neural models for slices (subsets) of data. We address challenge (i) by\nproposing different slicing functions (SFs) that select slices of the\ndataset---based on prior work we heuristically capture different failures of\nneural rankers. Then, for challenge (ii) we adapt a neural ranking model to\nlearn slice-aware representations, i.e. the adapted model learns to represent\nthe question and responses differently based on the model's prediction of which\nslices they belong to. Our experimental results (the source code and data are\navailable at https://github.com/Guzpenha/slice_based_learning) across three\ndifferent ranking tasks and four corpora show that slice-based learning\nimproves the effectiveness by an average of 2% over a neural ranker that is not\nslice-aware.", "published": "2020-10-07 11:40:49", "link": "http://arxiv.org/abs/2010.03343v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Toward Stance-based Personas for Opinionated Dialogues", "abstract": "In the context of chit-chat dialogues it has been shown that endowing systems\nwith a persona profile is important to produce more coherent and meaningful\nconversations. Still, the representation of such personas has thus far been\nlimited to a fact-based representation (e.g. \"I have two cats.\"). We argue that\nthese representations remain superficial w.r.t. the complexity of human\npersonality. In this work, we propose to make a step forward and investigate\nstance-based persona, trying to grasp more profound characteristics, such as\nopinions, values, and beliefs to drive language generation. To this end, we\nintroduce a novel dataset allowing to explore different stance-based persona\nrepresentations and their impact on claim generation, showing that they are\nable to grasp abstract and profound aspects of the author persona.", "published": "2020-10-07 12:30:30", "link": "http://arxiv.org/abs/2010.03369v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dual Reconstruction: a Unifying Objective for Semi-Supervised Neural\n  Machine Translation", "abstract": "While Iterative Back-Translation and Dual Learning effectively incorporate\nmonolingual training data in neural machine translation, they use different\nobjectives and heuristic gradient approximation strategies, and have not been\nextensively compared. We introduce a novel dual reconstruction objective that\nprovides a unified view of Iterative Back-Translation and Dual Learning. It\nmotivates a theoretical analysis and controlled empirical study on\nGerman-English and Turkish-English tasks, which both suggest that Iterative\nBack-Translation is more effective than Dual Learning despite its relative\nsimplicity.", "published": "2020-10-07 13:40:32", "link": "http://arxiv.org/abs/2010.03412v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Analogies minus analogy test: measuring regularities in word embeddings", "abstract": "Vector space models of words have long been claimed to capture linguistic\nregularities as simple vector translations, but problems have been raised with\nthis claim. We decompose and empirically analyze the classic arithmetic word\nanalogy test, to motivate two new metrics that address the issues with the\nstandard test, and which distinguish between class-wise offset concentration\n(similar directions between pairs of words drawn from different broad classes,\nsuch as France--London, China--Ottawa, ...) and pairing consistency (the\nexistence of a regular transformation between correctly-matched pairs such as\nFrance:Paris::China:Beijing). We show that, while the standard analogy test is\nflawed, several popular word embeddings do nevertheless encode linguistic\nregularities.", "published": "2020-10-07 14:38:35", "link": "http://arxiv.org/abs/2010.03446v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MOCHA: A Dataset for Training and Evaluating Generative Reading\n  Comprehension Metrics", "abstract": "Posing reading comprehension as a generation problem provides a great deal of\nflexibility, allowing for open-ended questions with few restrictions on\npossible answers. However, progress is impeded by existing generation metrics,\nwhich rely on token overlap and are agnostic to the nuances of reading\ncomprehension. To address this, we introduce a benchmark for training and\nevaluating generative reading comprehension metrics: MOdeling Correctness with\nHuman Annotations. MOCHA contains 40K human judgement scores on model outputs\nfrom 6 diverse question answering datasets and an additional set of minimal\npairs for evaluation. Using MOCHA, we train a Learned Evaluation metric for\nReading Comprehension, LERC, to mimic human judgement scores. LERC outperforms\nbaseline metrics by 10 to 36 absolute Pearson points on held-out annotations.\nWhen we evaluate robustness on minimal pairs, LERC achieves 80% accuracy,\noutperforming baselines by 14 to 26 absolute percentage points while leaving\nsignificant room for improvement. MOCHA presents a challenging problem for\ndeveloping accurate and robust generative reading comprehension metrics.", "published": "2020-10-07 20:22:54", "link": "http://arxiv.org/abs/2010.03636v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multilingual Knowledge Graph Completion via Ensemble Knowledge Transfer", "abstract": "Predicting missing facts in a knowledge graph (KG) is a crucial task in\nknowledge base construction and reasoning, and it has been the subject of much\nresearch in recent works using KG embeddings. While existing KG embedding\napproaches mainly learn and predict facts within a single KG, a more plausible\nsolution would benefit from the knowledge in multiple language-specific KGs,\nconsidering that different KGs have their own strengths and limitations on data\nquality and coverage. This is quite challenging, since the transfer of\nknowledge among multiple independently maintained KGs is often hindered by the\ninsufficiency of alignment information and the inconsistency of described\nfacts. In this paper, we propose KEnS, a novel framework for embedding learning\nand ensemble knowledge transfer across a number of language-specific KGs. KEnS\nembeds all KGs in a shared embedding space, where the association of entities\nis captured based on self-learning. Then, KEnS performs ensemble inference to\ncombine prediction results from embeddings of multiple language-specific KGs,\nfor which multiple ensemble techniques are investigated. Experiments on five\nreal-world language-specific KGs show that KEnS consistently improves\nstate-of-the-art methods on KG completion, via effectively identifying and\nleveraging complementary knowledge.", "published": "2020-10-07 04:54:03", "link": "http://arxiv.org/abs/2010.03158v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "WER we are and WER we think we are", "abstract": "Natural language processing of conversational speech requires the\navailability of high-quality transcripts. In this paper, we express our\nskepticism towards the recent reports of very low Word Error Rates (WERs)\nachieved by modern Automatic Speech Recognition (ASR) systems on benchmark\ndatasets. We outline several problems with popular benchmarks and compare three\nstate-of-the-art commercial ASR systems on an internal dataset of real-life\nspontaneous human conversations and HUB'05 public benchmark. We show that WERs\nare significantly higher than the best reported results. We formulate a set of\nguidelines which may aid in the creation of real-life, multi-domain datasets\nwith high quality annotations for training and testing of robust ASR systems.", "published": "2020-10-07 14:20:31", "link": "http://arxiv.org/abs/2010.03432v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Inductive Entity Representations from Text via Link Prediction", "abstract": "Knowledge Graphs (KG) are of vital importance for multiple applications on\nthe web, including information retrieval, recommender systems, and metadata\nannotation. Regardless of whether they are built manually by domain experts or\nwith automatic pipelines, KGs are often incomplete. Recent work has begun to\nexplore the use of textual descriptions available in knowledge graphs to learn\nvector representations of entities in order to preform link prediction.\nHowever, the extent to which these representations learned for link prediction\ngeneralize to other tasks is unclear. This is important given the cost of\nlearning such representations. Ideally, we would prefer representations that do\nnot need to be trained again when transferring to a different task, while\nretaining reasonable performance.\n  In this work, we propose a holistic evaluation protocol for entity\nrepresentations learned via a link prediction objective. We consider the\ninductive link prediction and entity classification tasks, which involve\nentities not seen during training. We also consider an information retrieval\ntask for entity-oriented search. We evaluate an architecture based on a\npretrained language model, that exhibits strong generalization to entities not\nobserved during training, and outperforms related state-of-the-art methods (22%\nMRR improvement in link prediction on average). We further provide evidence\nthat the learned representations transfer well to other tasks without\nfine-tuning. In the entity classification task we obtain an average improvement\nof 16% in accuracy compared with baselines that also employ pre-trained models.\nIn the information retrieval task, we obtain significant improvements of up to\n8.8% in NDCG@10 for natural language queries. We thus show that the learned\nrepresentations are not limited KG-specific tasks, and have greater\ngeneralization properties than evaluated in previous work.", "published": "2020-10-07 16:04:06", "link": "http://arxiv.org/abs/2010.03496v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TeMP: Temporal Message Passing for Temporal Knowledge Graph Completion", "abstract": "Inferring missing facts in temporal knowledge graphs (TKGs) is a fundamental\nand challenging task. Previous works have approached this problem by augmenting\nmethods for static knowledge graphs to leverage time-dependent representations.\nHowever, these methods do not explicitly leverage multi-hop structural\ninformation and temporal facts from recent time steps to enhance their\npredictions. Additionally, prior work does not explicitly address the temporal\nsparsity and variability of entity distributions in TKGs. We propose the\nTemporal Message Passing (TeMP) framework to address these challenges by\ncombining graph neural networks, temporal dynamics models, data imputation and\nfrequency-based gating techniques. Experiments on standard TKG tasks show that\nour approach provides substantial gains compared to the previous state of the\nart, achieving a 10.7% average relative improvement in Hits@10 across three\nstandard benchmarks. Our analysis also reveals important sources of variability\nboth within and across TKG datasets, and we introduce several simple but strong\nbaselines that outperform the prior state of the art in certain settings.", "published": "2020-10-07 17:11:53", "link": "http://arxiv.org/abs/2010.03526v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "What Can We Learn from Collective Human Opinions on Natural Language\n  Inference Data?", "abstract": "Despite the subjective nature of many NLP tasks, most NLU evaluations have\nfocused on using the majority label with presumably high agreement as the\nground truth. Less attention has been paid to the distribution of human\nopinions. We collect ChaosNLI, a dataset with a total of 464,500 annotations to\nstudy Collective HumAn OpinionS in oft-used NLI evaluation sets. This dataset\nis created by collecting 100 annotations per example for 3,113 examples in SNLI\nand MNLI and 1,532 examples in Abductive-NLI. Analysis reveals that: (1) high\nhuman disagreement exists in a noticeable amount of examples in these datasets;\n(2) the state-of-the-art models lack the ability to recover the distribution\nover human labels; (3) models achieve near-perfect accuracy on the subset of\ndata with a high level of human agreement, whereas they can barely beat a\nrandom guess on the data with low levels of human agreement, which compose most\nof the common errors made by state-of-the-art models on the evaluation sets.\nThis questions the validity of improving model performance on old metrics for\nthe low-agreement part of evaluation datasets. Hence, we argue for a detailed\nexamination of human agreement in future data collection efforts, and\nevaluating model outputs against the distribution over collective human\nopinions. The ChaosNLI dataset and experimental scripts are available at\nhttps://github.com/easonnie/ChaosNLI", "published": "2020-10-07 17:26:06", "link": "http://arxiv.org/abs/2010.03532v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Self-supervised Approach for Semantic Indexing in the Context of\n  COVID-19 Pandemic", "abstract": "The pandemic has accelerated the pace at which COVID-19 scientific papers are\npublished. In addition, the process of manually assigning semantic indexes to\nthese papers by experts is even more time-consuming and overwhelming in the\ncurrent health crisis. Therefore, there is an urgent need for automatic\nsemantic indexing models which can effectively scale-up to newly introduced\nconcepts and rapidly evolving distributions of the hyperfocused related\nliterature. In this research, we present a novel semantic indexing approach\nbased on the state-of-the-art self-supervised representation learning and\ntransformer encoding exclusively suitable for pandemic crises. We present a\ncase study on a novel dataset that is based on COVID-19 papers published and\nmanually indexed in PubMed. Our study shows that our self-supervised model\noutperforms the best performing models of BioASQ Task 8a by micro-F1 score of\n0.1 and LCA-F score of 0.08 on average. Our model also shows superior\nperformance on detecting the supplementary concepts which is quite important\nwhen the focus of the literature has drastically shifted towards specific\nconcepts related to the pandemic. Our study sheds light on the main challenges\nconfronting semantic indexing models during a pandemic, namely new domains and\ndrastic changes of their distributions, and as a superior alternative for such\nsituations, propose a model founded on approaches which have shown auspicious\nperformance in improving generalization and data efficiency in various NLP\ntasks. We also show the joint indexing of major Medical Subject Headings (MeSH)\nand supplementary concepts improves the overall performance.", "published": "2020-10-07 17:43:55", "link": "http://arxiv.org/abs/2010.03544v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Characterizing the Value of Information in Medical Notes", "abstract": "Machine learning models depend on the quality of input data. As electronic\nhealth records are widely adopted, the amount of data in health care is\ngrowing, along with complaints about the quality of medical notes. We use two\nprediction tasks, readmission prediction and in-hospital mortality prediction,\nto characterize the value of information in medical notes. We show that as a\nwhole, medical notes only provide additional predictive power over structured\ninformation in readmission prediction. We further propose a probing framework\nto select parts of notes that enable more accurate predictions than using all\nnotes, despite that the selected information leads to a distribution shift from\nthe training data (\"all notes\"). Finally, we demonstrate that models trained on\nthe selected valuable information achieve even better predictive performance,\nwith only 6.8% of all the tokens for readmission prediction.", "published": "2020-10-07 18:00:03", "link": "http://arxiv.org/abs/2010.03574v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Domain Adversarial Neural Networks for Dysarthric Speech Recognition", "abstract": "Speech recognition systems have improved dramatically over the last few\nyears, however, their performance is significantly degraded for the cases of\naccented or impaired speech. This work explores domain adversarial neural\nnetworks (DANN) for speaker-independent speech recognition on the UAS dataset\nof dysarthric speech. The classification task on 10 spoken digits is performed\nusing an end-to-end CNN taking raw audio as input. The results are compared to\na speaker-adaptive (SA) model as well as speaker-dependent (SD) and multi-task\nlearning models (MTL). The experiments conducted in this paper show that DANN\nachieves an absolute recognition rate of 74.91% and outperforms the baseline by\n12.18%. Additionally, the DANN model achieves comparable results to the SA\nmodel's recognition rate of 77.65%. We also observe that when labelled\ndysarthric speech data is available DANN and MTL perform similarly, but when\nthey are not DANN performs better than MTL.", "published": "2020-10-07 19:51:41", "link": "http://arxiv.org/abs/2010.03623v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Towards Understanding Sample Variance in Visually Grounded Language\n  Generation: Evaluations and Observations", "abstract": "A major challenge in visually grounded language generation is to build robust\nbenchmark datasets and models that can generalize well in real-world settings.\nTo do this, it is critical to ensure that our evaluation protocols are correct,\nand benchmarks are reliable. In this work, we set forth to design a set of\nexperiments to understand an important but often ignored problem in visually\ngrounded language generation: given that humans have different utilities and\nvisual attention, how will the sample variance in multi-reference datasets\naffect the models' performance? Empirically, we study several multi-reference\ndatasets and corresponding vision-and-language tasks. We show that it is of\nparamount importance to report variance in experiments; that human-generated\nreferences could vary drastically in different datasets/tasks, revealing the\nnature of each task; that metric-wise, CIDEr has shown systematically larger\nvariances than others. Our evaluations on reference-per-instance shed light on\nthe design of reliable datasets in the future.", "published": "2020-10-07 20:45:14", "link": "http://arxiv.org/abs/2010.03644v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "A Mathematical Exploration of Why Language Models Help Solve Downstream\n  Tasks", "abstract": "Autoregressive language models, pretrained using large text corpora to do\nwell on next word prediction, have been successful at solving many downstream\ntasks, even with zero-shot usage. However, there is little theoretical\nunderstanding of this success. This paper initiates a mathematical study of\nthis phenomenon for the downstream task of text classification by considering\nthe following questions: (1) What is the intuitive connection between the\npretraining task of next word prediction and text classification? (2) How can\nwe mathematically formalize this connection and quantify the benefit of\nlanguage modeling? For (1), we hypothesize, and verify empirically, that\nclassification tasks of interest can be reformulated as sentence completion\ntasks, thus making language modeling a meaningful pretraining task. With a\nmathematical formalization of this hypothesis, we make progress towards (2) and\nshow that language models that are $\\epsilon$-optimal in cross-entropy\n(log-perplexity) learn features that can linearly solve such classification\ntasks with $\\mathcal{O}(\\sqrt{\\epsilon})$ error, thus demonstrating that doing\nwell on language modeling can be beneficial for downstream tasks. We\nexperimentally verify various assumptions and theoretical findings, and also\nuse insights from the analysis to design a new objective function that performs\nwell on some classification tasks.", "published": "2020-10-07 20:56:40", "link": "http://arxiv.org/abs/2010.03648v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Adaptive Self-training for Few-shot Neural Sequence Labeling", "abstract": "Sequence labeling is an important technique employed for many Natural\nLanguage Processing (NLP) tasks, such as Named Entity Recognition (NER), slot\ntagging for dialog systems and semantic parsing. Large-scale pre-trained\nlanguage models obtain very good performance on these tasks when fine-tuned on\nlarge amounts of task-specific labeled data. However, such large-scale labeled\ndatasets are difficult to obtain for several tasks and domains due to the high\ncost of human annotation as well as privacy and data access constraints for\nsensitive user applications. This is exacerbated for sequence labeling tasks\nrequiring such annotations at token-level. In this work, we develop techniques\nto address the label scarcity challenge for neural sequence labeling models.\nSpecifically, we develop self-training and meta-learning techniques for\ntraining neural sequence taggers with few labels. While self-training serves as\nan effective mechanism to learn from large amounts of unlabeled data --\nmeta-learning helps in adaptive sample re-weighting to mitigate error\npropagation from noisy pseudo-labels. Extensive experiments on six benchmark\ndatasets including two for massive multilingual NER and four slot tagging\ndatasets for task-oriented dialog systems demonstrate the effectiveness of our\nmethod. With only 10 labeled examples for each class for each task, our method\nobtains 10% improvement over state-of-the-art systems demonstrating its\neffectiveness for the low-resource setting.", "published": "2020-10-07 22:29:05", "link": "http://arxiv.org/abs/2010.03680v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AxFormer: Accuracy-driven Approximation of Transformers for Faster,\n  Smaller and more Accurate NLP Models", "abstract": "Transformers have greatly advanced the state-of-the-art in Natural Language\nProcessing (NLP) in recent years, but present very large computation and\nstorage requirements. We observe that the design process of Transformers\n(pre-train a foundation model on a large dataset in a self-supervised manner,\nand subsequently fine-tune it for different downstream tasks) leads to\ntask-specific models that are highly over-parameterized, adversely impacting\nboth accuracy and inference efficiency. We propose AxFormer, a systematic\nframework that applies accuracy-driven approximations to create optimized\ntransformer models for a given downstream task. AxFormer combines two key\noptimizations -- accuracy-driven pruning and selective hard attention.\nAccuracy-driven pruning identifies and removes parts of the fine-tuned\ntransformer that hinder performance on the given downstream task. Sparse\nhard-attention optimizes attention blocks in selected layers by eliminating\nirrelevant word aggregations, thereby helping the model focus only on the\nrelevant parts of the input. In effect, AxFormer leads to models that are more\naccurate, while also being faster and smaller. Our experiments on GLUE and\nSQUAD tasks show that AxFormer models are up to 4.5% more accurate, while also\nbeing up to 2.5X faster and up to 3.2X smaller than conventional fine-tuned\nmodels. In addition, we demonstrate that AxFormer can be combined with previous\nefforts such as distillation or quantization to achieve further efficiency\ngains.", "published": "2020-10-07 23:29:34", "link": "http://arxiv.org/abs/2010.03688v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Deep Learning for Information Systems Research", "abstract": "Artificial Intelligence (AI) has rapidly emerged as a key disruptive\ntechnology in the 21st century. At the heart of modern AI lies Deep Learning\n(DL), an emerging class of algorithms that has enabled today's platforms and\norganizations to operate at unprecedented efficiency, effectiveness, and scale.\nDespite significant interest, IS contributions in DL have been limited, which\nwe argue is in part due to issues with defining, positioning, and conducting DL\nresearch. Recognizing the tremendous opportunity here for the IS community,\nthis work clarifies, streamlines, and presents approaches for IS scholars to\nmake timely and high-impact contributions. Related to this broader goal, this\npaper makes five timely contributions. First, we systematically summarize the\nmajor components of DL in a novel Deep Learning for Information Systems\nResearch (DL-ISR) schematic that illustrates how technical DL processes are\ndriven by key factors from an application environment. Second, we present a\nnovel Knowledge Contribution Framework (KCF) to help IS scholars position their\nDL contributions for maximum impact. Third, we provide ten guidelines to help\nIS scholars generate rigorous and relevant DL-ISR in a systematic, high-quality\nfashion. Fourth, we present a review of prevailing journal and conference\nvenues to examine how IS scholars have leveraged DL for various research\ninquiries. Finally, we provide a unique perspective on how IS scholars can\nformulate DL-ISR inquiries by carefully considering the interplay of business\nfunction(s), application areas(s), and the KCF. This perspective intentionally\nemphasizes inter-disciplinary, intra-disciplinary, and cross-IS tradition\nperspectives. Taken together, these contributions provide IS scholars a timely\nframework to advance the scale, scope, and impact of deep learning research.", "published": "2020-10-07 15:23:05", "link": "http://arxiv.org/abs/2010.05774v1", "categories": ["cs.LG", "cs.CL", "stat.ME", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Pkwrap: a PyTorch Package for LF-MMI Training of Acoustic Models", "abstract": "We present a simple wrapper that is useful to train acoustic models in\nPyTorch using Kaldi's LF-MMI training framework. The wrapper, called pkwrap\n(short form of PyTorch kaldi wrapper), enables the user to utilize the\nflexibility provided by PyTorch in designing model architectures. It exposes\nthe LF-MMI cost function as an autograd function. Other capabilities of Kaldi\nhave also been ported to PyTorch. This includes the parallel training ability\nwhen multi-GPU environments are unavailable and decode with graphs created in\nKaldi. The package is available on Github at https://github.com/idiap/pkwrap.", "published": "2020-10-07 15:02:11", "link": "http://arxiv.org/abs/2010.03466v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improving the efficiency of spectral features extraction by structuring\n  the audio files", "abstract": "The extraction of spectral features from a music clip is a computationally\nexpensive task. As in order to extract accurate features, we need to process\nthe clip for its whole length. This preprocessing task creates a large overhead\nand also makes the extraction process slower. We show how formatting a dataset\nin a certain way, can help make the process more efficient by eliminating the\nneed for processing the clip for its whole duration, and still extract the\nfeatures accurately. In addition, we discuss the possibility of defining set\ngeneric durations for analyzing a certain type of music clip while training.\nAnd in doing so we cut down the need of processing the clip duration to just\n10% of the global average.", "published": "2020-10-07 03:35:39", "link": "http://arxiv.org/abs/2010.03136v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Adversarial attacks on audio source separation", "abstract": "Despite the excellent performance of neural-network-based audio source\nseparation methods and their wide range of applications, their robustness\nagainst intentional attacks has been largely neglected. In this work, we\nreformulate various adversarial attack methods for the audio source separation\nproblem and intensively investigate them under different attack conditions and\ntarget models. We further propose a simple yet effective regularization method\nto obtain imperceptible adversarial noise while maximizing the impact on\nseparation quality with low computational complexity. Experimental results show\nthat it is possible to largely degrade the separation quality by adding\nimperceptibly small noise when the noise is crafted for the target model. We\nalso show the robustness of source separation models against a black-box\nattack. This study provides potentially useful insights for developing content\nprotection methods against the abuse of separated signals and improving the\nseparation performance and robustness.", "published": "2020-10-07 05:02:21", "link": "http://arxiv.org/abs/2010.03164v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Generative Melody Composition with Human-in-the-Loop Bayesian\n  Optimization", "abstract": "Deep generative models allow even novice composers to generate various\nmelodies by sampling latent vectors. However, finding the desired melody is\nchallenging since the latent space is unintuitive and high-dimensional. In this\nwork, we present an interactive system that supports generative melody\ncomposition with human-in-the-loop Bayesian optimization (BO). This system\ntakes a mixed-initiative approach; the system generates candidate melodies to\nevaluate, and the user evaluates them and provides preferential feedback (i.e.,\npicking the best melody among the candidates) to the system. This process is\niteratively performed based on BO techniques until the user finds the desired\nmelody. We conducted a pilot study using our prototype system, suggesting the\npotential of this approach.", "published": "2020-10-07 05:54:20", "link": "http://arxiv.org/abs/2010.03190v1", "categories": ["cs.SD", "cs.HC", "eess.AS", "J.5; H.5.5; H.5.2"], "primary_category": "cs.SD"}
{"title": "Transformer Transducer: One Model Unifying Streaming and Non-streaming\n  Speech Recognition", "abstract": "In this paper we present a Transformer-Transducer model architecture and a\ntraining technique to unify streaming and non-streaming speech recognition\nmodels into one model. The model is composed of a stack of transformer layers\nfor audio encoding with no lookahead or right context and an additional stack\nof transformer layers on top trained with variable right context. In inference\ntime, the context length for the variable context layers can be changed to\ntrade off the latency and the accuracy of the model. We also show that we can\nrun this model in a Y-model architecture with the top layers running in\nparallel in low latency and high latency modes. This allows us to have\nstreaming speech recognition results with limited latency and delayed speech\nrecognition results with large improvements in accuracy (20% relative\nimprovement for voice-search task). We show that with limited right context\n(1-2 seconds of audio) and small additional latency (50-100 milliseconds) at\nthe end of decoding, we can achieve similar accuracy with models using\nunlimited audio right context. We also present optimizations for audio and\nlabel encoders to speed up the inference in streaming and non-streaming speech\ndecoding.", "published": "2020-10-07 05:58:28", "link": "http://arxiv.org/abs/2010.03192v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Designing, Playing, and Performing with a Vision-based Mouth Interface", "abstract": "The role of the face and mouth in speech production as well asnon-verbal\ncommunication suggests the use of facial action tocontrol musical sound. Here\nwe document work on theMouthesizer, a system which uses a headworn\nminiaturecamera and computer vision algorithm to extract shapeparameters from\nthe mouth opening and output these as MIDIcontrol changes. We report our\nexperience with variousgesture-to-sound mappings and musical applications,\nanddescribe a live performance which used the Mouthesizerinterface.", "published": "2020-10-07 06:47:42", "link": "http://arxiv.org/abs/2010.03213v1", "categories": ["cs.HC", "cs.CV", "cs.SD", "eess.AS", "H.5.5"], "primary_category": "cs.HC"}
{"title": "Sonification of Facial Actions for Musical Expression", "abstract": "The central role of the face in social interaction and non-verbal\ncommunication suggests we explore facial action as a means of musical\nexpression. This paper presents the design, implementation, and preliminary\nstudies of a novel system utilizing face detection and optic flow algorithms to\nassociate facial movements with sound synthesis in a topographically specific\nfashion. We report on our experience with various gesture-to-sound mappings and\napplications, and describe our preliminary experiments at musical performance\nusing the system.", "published": "2020-10-07 07:04:07", "link": "http://arxiv.org/abs/2010.03223v1", "categories": ["cs.HC", "cs.CV", "cs.SD", "eess.AS", "H.5.5"], "primary_category": "cs.HC"}
{"title": "A Novel Face-tracking Mouth Controller and its Application to\n  Interacting with Bioacoustic Models", "abstract": "We describe a simple, computationally light, real-time system for tracking\nthe lower face and extracting information about the shape of the open mouth\nfrom a video sequence. The system allows unencumbered control of audio\nsynthesis modules by the action of the mouth. We report work in progress to use\nthe mouth controller to interact with a physical model of sound production by\nthe avian syrinx.", "published": "2020-10-07 08:36:43", "link": "http://arxiv.org/abs/2010.03265v1", "categories": ["cs.HC", "cs.CV", "cs.SD", "eess.AS", "H.5.5"], "primary_category": "cs.HC"}
{"title": "Less is more: Faster and better music version identification with\n  embedding distillation", "abstract": "Version identification systems aim to detect different renditions of the same\nunderlying musical composition (loosely called cover songs). By learning to\nencode entire recordings into plain vector embeddings, recent systems have made\nsignificant progress in bridging the gap between accuracy and scalability,\nwhich has been a key challenge for nearly two decades. In this work, we propose\nto further narrow this gap by employing a set of data distillation techniques\nthat reduce the embedding dimensionality of a pre-trained state-of-the-art\nmodel. We compare a wide range of techniques and propose new ones, from\nclassical dimensionality reduction to more sophisticated distillation schemes.\nWith those, we obtain 99% smaller embeddings that, moreover, yield up to a 3%\naccuracy increase. Such small embeddings can have an important impact in\nretrieval time, up to the point of making a real-world system practical on a\nstandalone laptop.", "published": "2020-10-07 09:02:12", "link": "http://arxiv.org/abs/2010.03284v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "An Audio-Video Deep and Transfer Learning Framework for Multimodal\n  Emotion Recognition in the wild", "abstract": "In this paper, we present our contribution to ABAW facial expression\nchallenge. We report the proposed system and the official challenge results\nadhering to the challenge protocol. Using end-to-end deep learning and\nbenefiting from transfer learning approaches, we reached a test set challenge\nperformance measure of 42.10%.", "published": "2020-10-07 23:45:24", "link": "http://arxiv.org/abs/2010.03692v3", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Interpreting Imagined Speech Waves with Machine Learning techniques", "abstract": "This work explores the possibility of decoding Imagined Speech (IS) signals\nwhich can be used to create a new design of Human-Computer Interface (HCI).\nSince the underlying process generating EEG signals is unknown, various feature\nextraction methods, along with different neural network (NN) models, are used\nto approximate data distribution and classify IS signals. Based on the\nexperimental results, feed-forward NN model with ensemble and covariance matrix\ntransformed features showed the highest performance in comparison to other\nexisting methods. For comparison, three publicly available datasets were used.\nWe report a mean classification accuracy of 80% between rest and imagined\nstate, 96% and 80% for decoding long and short words on two datasets. These\nresults show that it is possible to differentiate brain signals (generated\nduring rest state) from the IS brain signals. Based on the experimental\nresults, we suggest that the word length and complexity can be used to decode\nIS signals with high accuracy, and a BCI system can be designed with IS signals\nfor computer interaction. These ideas, and results give direction for the\ndevelopment of a commercial level IS based BCI system, which can be used for\nhuman-computer interaction in daily life.", "published": "2020-10-07 12:13:39", "link": "http://arxiv.org/abs/2010.03360v2", "categories": ["eess.SP", "cs.HC", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
