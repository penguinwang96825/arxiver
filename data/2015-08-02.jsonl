{"title": "Class Vectors: Embedding representation of Document Classes", "abstract": "Distributed representations of words and paragraphs as semantic embeddings in\nhigh dimensional data are used across a number of Natural Language\nUnderstanding tasks such as retrieval, translation, and classification. In this\nwork, we propose \"Class Vectors\" - a framework for learning a vector per class\nin the same embedding space as the word and paragraph embeddings. Similarity\nbetween these class vectors and word vectors are used as features to classify a\ndocument to a class. In experiment on several sentiment analysis tasks such as\nYelp reviews and Amazon electronic product reviews, class vectors have shown\nbetter or comparable results in classification while learning very meaningful\nclass embeddings.", "published": "2015-08-02 04:17:40", "link": "http://arxiv.org/abs/1508.00189v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "PTE: Predictive Text Embedding through Large-scale Heterogeneous Text\n  Networks", "abstract": "Unsupervised text embedding methods, such as Skip-gram and Paragraph Vector,\nhave been attracting increasing attention due to their simplicity, scalability,\nand effectiveness. However, comparing to sophisticated deep learning\narchitectures such as convolutional neural networks, these methods usually\nyield inferior results when applied to particular machine learning tasks. One\npossible reason is that these text embedding methods learn the representation\nof text in a fully unsupervised way, without leveraging the labeled information\navailable for the task. Although the low dimensional representations learned\nare applicable to many different tasks, they are not particularly tuned for any\ntask. In this paper, we fill this gap by proposing a semi-supervised\nrepresentation learning method for text data, which we call the\n\\textit{predictive text embedding} (PTE). Predictive text embedding utilizes\nboth labeled and unlabeled data to learn the embedding of text. The labeled\ninformation and different levels of word co-occurrence information are first\nrepresented as a large-scale heterogeneous text network, which is then embedded\ninto a low dimensional space through a principled and efficient algorithm. This\nlow dimensional embedding not only preserves the semantic closeness of words\nand documents, but also has a strong predictive power for the particular task.\nCompared to recent supervised approaches based on convolutional neural\nnetworks, predictive text embedding is comparable or more effective, much more\nefficient, and has fewer parameters to tune.", "published": "2015-08-02 06:18:10", "link": "http://arxiv.org/abs/1508.00200v1", "categories": ["cs.CL", "cs.LG", "cs.NE", "I.2.6"], "primary_category": "cs.CL"}
