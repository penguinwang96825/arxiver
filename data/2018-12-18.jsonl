{"title": "Multiple topic identification in human/human conversations", "abstract": "The paper deals with the automatic analysis of real-life telephone\nconversations between an agent and a customer of a customer care service (ccs).\nThe application domain is the public transportation system in Paris and the\npurpose is to collect statistics about customer problems in order to monitor\nthe service and decide priorities on the intervention for improving user\nsatisfaction. Of primary importance for the analysis is the detection of themes\nthat are the object of customer problems. Themes are defined in the application\nrequirements and are part of the application ontology that is implicit in the\nccs documentation. Due to variety of customer population, the structure of\nconversations with an agent is unpredictable. A conversation may be about one\nor more themes. Theme mentions can be interleaved with mentions of facts that\nare irrelevant for the application purpose. Furthermore, in certain\nconversations theme mentions are localized in specific conversation segments\nwhile in other conversations mentions cannot be localized. As a consequence,\napproaches to feature extraction with and without mention localization are\nconsidered. Application domain relevant themes identified by an automatic\nprocedure are expressed by specific sentences whose words are hypothesized by\nan automatic speech recognition (asr) system. The asr system is error prone.\nThe word error rates can be very high for many reasons. Among them it is worth\nmentioning unpredictable background noise, speaker accent, and various types of\nspeech disfluencies. As the application task requires the composition of\nproportions of theme mentions, a sequential decision strategy is introduced in\nthis paper for performing a survey of the large amount of conversations made\navailable in a given time period. The strategy has to sample the conversations\nto form a survey containing enough data analyzed with high accuracy so that\nproportions can be estimated with sufficient accuracy. Due to the unpredictable\ntype of theme mentions, it is appropriate to consider methods for theme\nhypothesization based on global as well as local feature extraction. Two\nsystems based on each type of feature extraction will be considered by the\nstrategy. One of the four methods is novel. It is based on a new definition of\ndensity of theme mentions and on the localization of high density zones whose\nboundaries do not need to be precisely detected. The sequential decision\nstrategy starts by grouping theme hypotheses into sets of different expected\naccuracy and coverage levels. For those sets for which accuracy can be improved\nwith a consequent increase of coverage a new system with new features is\nintroduced. Its execution is triggered only when specific preconditions are met\non the hypotheses generated by the basic four systems. Experimental results are\nprovided on a corpus collected in the call center of the Paris transportation\nsystem known as ratp. The results show that surveys with high accuracy and\ncoverage can be composed with the proposed strategy and systems. This makes it\npossible to apply a previously published proportion estimation approach that\ntakes into account hypothesization errors .", "published": "2018-12-18 07:24:17", "link": "http://arxiv.org/abs/1812.07207v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Predicting user intent from search queries using both CNNs and RNNs", "abstract": "Predicting user behaviour on a website is a difficult task, which requires\nthe integration of multiple sources of information, such as geo-location, user\nprofile or web surfing history. In this paper we tackle the problem of\npredicting the user intent, based on the queries that were used to access a\ncertain webpage. We make no additional assumptions, such as domain detection,\ndevice used or location, and only use the word information embedded in the\ngiven query. In order to build competitive classifiers, we label a small\nfraction of the EDI query intent prediction dataset\n\\cite{edi-challenge-dataset}, which is used as ground truth. Then, using\nvarious rule-based approaches, we automatically label the rest of the dataset,\ntrain the classifiers and evaluate the quality of the automatic labeling on the\nground truth dataset. We used both recurrent and convolutional networks as the\nmodels, while representing the words in the query with multiple embedding\nmethods.", "published": "2018-12-18 12:15:33", "link": "http://arxiv.org/abs/1812.07324v1", "categories": ["cs.CL", "68T50"], "primary_category": "cs.CL"}
{"title": "Supervised Domain Enablement Attention for Personalized Domain\n  Classification", "abstract": "In large-scale domain classification for natural language understanding,\nleveraging each user's domain enablement information, which refers to the\npreferred or authenticated domains by the user, with attention mechanism has\nbeen shown to improve the overall domain classification performance. In this\npaper, we propose a supervised enablement attention mechanism, which utilizes\nsigmoid activation for the attention weighting so that the attention can be\ncomputed with more expressive power without the weight sum constraint of\nsoftmax attention. The attention weights are explicitly encouraged to be\nsimilar to the corresponding elements of the ground-truth's one-hot vector by\nsupervised attention, and the attention information of the other enabled\ndomains is leveraged through self-distillation. By evaluating on the actual\nutterances from a large-scale IPDA, we show that our approach significantly\nimproves domain classification performance.", "published": "2018-12-18 18:21:24", "link": "http://arxiv.org/abs/1812.07546v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "wav2letter++: The Fastest Open-source Speech Recognition System", "abstract": "This paper introduces wav2letter++, the fastest open-source deep learning\nspeech recognition framework. wav2letter++ is written entirely in C++, and uses\nthe ArrayFire tensor library for maximum efficiency. Here we explain the\narchitecture and design of the wav2letter++ system and compare it to other\nmajor open-source speech recognition systems. In some cases wav2letter++ is\nmore than 2x faster than other optimized frameworks for training end-to-end\nneural networks for speech recognition. We also show that wav2letter++'s\ntraining times scale linearly to 64 GPUs, the highest we tested, for models\nwith 100 million parameters. High-performance frameworks enable fast iteration,\nwhich is often a crucial factor in successful research and model tuning on new\ndatasets and tasks.", "published": "2018-12-18 19:57:24", "link": "http://arxiv.org/abs/1812.07625v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "D{\u00e9}tection de locuteurs dans les s{\u00e9}ries TV", "abstract": "Speaker diarization of audio streams turns out to be particularly challenging\nwhen applied to fictional films, where many characters talk in various acoustic\nconditions (background music, sound effects, variations in intonation...).\nDespite this acoustic variability, such movies exhibit specific visual\npatterns, particularly within dialogue scenes. In this paper, we introduce a\ntwo-step method to achieve speaker diarization in TV series: speaker\ndiarization is first performed locally within scenes visually identified as\ndialogues; then, the hypothesized local speakers are compared to each other\nduring a second clustering process in order to detect recurring speakers: this\nsecond stage of clustering is subject to the constraint that the different\nspeakers involved in the same dialogue have to be assigned to different\nclusters. The performances of our approach are compared to those obtained by\nstandard speaker diarization tools applied to the same data.", "published": "2018-12-18 07:11:19", "link": "http://arxiv.org/abs/1812.07200v1", "categories": ["cs.MM", "cs.CL"], "primary_category": "cs.MM"}
{"title": "Audiovisual speaker diarization of TV series", "abstract": "Speaker diarization may be difficult to achieve when applied to narrative\nfilms, where speakers usually talk in adverse acoustic conditions: background\nmusic, sound effects, wide variations in intonation may hide the inter-speaker\nvariability and make audio-based speaker diarization approaches error prone. On\nthe other hand, such fictional movies exhibit strong regularities at the image\nlevel, particularly within dialogue scenes. In this paper, we propose to\nperform speaker diarization within dialogue scenes of TV series by combining\nthe audio and video modalities: speaker diarization is first performed by using\neach modality, the two resulting partitions of the instance set are then\noptimally matched, before the remaining instances, corresponding to cases of\ndisagreement between both modalities, are finally processed. The results\nobtained by applying such a multi-modal approach to fictional films turn out to\noutperform those obtained by relying on a single modality.", "published": "2018-12-18 07:21:36", "link": "http://arxiv.org/abs/1812.07205v2", "categories": ["cs.MM", "cs.CL"], "primary_category": "cs.MM"}
{"title": "Constrained speaker diarization of TV series based on visual patterns", "abstract": "Speaker diarization, usually denoted as the ''who spoke when'' task, turns\nout to be particularly challenging when applied to fictional films, where many\ncharacters talk in various acoustic conditions (background music, sound\neffects...). Despite this acoustic variability , such movies exhibit specific\nvisual patterns in the dialogue scenes. In this paper, we introduce a two-step\nmethod to achieve speaker diarization in TV series: a speaker diarization is\nfirst performed locally in the scenes detected as dialogues; then, the\nhypothesized local speakers are merged in a second agglomerative clustering\nprocess, with the constraint that speakers locally hypothesized to be distinct\nmust not be assigned to the same cluster. The performances of our approach are\ncompared to those obtained by standard speaker diarization tools applied to the\nsame data.", "published": "2018-12-18 07:29:27", "link": "http://arxiv.org/abs/1812.07209v2", "categories": ["cs.MM", "cs.CL"], "primary_category": "cs.MM"}
{"title": "Attend, Copy, Parse -- End-to-end information extraction from documents", "abstract": "Document information extraction tasks performed by humans create data\nconsisting of a PDF or document image input, and extracted string outputs. This\nend-to-end data is naturally consumed and produced when performing the task\nbecause it is valuable in and of itself. It is naturally available, at no\nadditional cost. Unfortunately, state-of-the-art word classification methods\nfor information extraction cannot use this data, instead requiring word-level\nlabels which are expensive to create and consequently not available for many\nreal life tasks. In this paper we propose the Attend, Copy, Parse architecture,\na deep neural network model that can be trained directly on end-to-end data,\nbypassing the need for word-level labels. We evaluate the proposed architecture\non a large diverse set of invoices, and outperform a state-of-the-art\nproduction system based on word classification. We believe our proposed\narchitecture can be used on many real life information extraction tasks where\nword classification cannot be used due to a lack of the required word-level\nlabels.", "published": "2018-12-18 09:05:53", "link": "http://arxiv.org/abs/1812.07248v3", "categories": ["cs.CL", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Towards Deep Conversational Recommendations", "abstract": "There has been growing interest in using neural networks and deep learning\ntechniques to create dialogue systems. Conversational recommendation is an\ninteresting setting for the scientific exploration of dialogue with natural\nlanguage as the associated discourse involves goal-driven dialogue that often\ntransforms naturally into more free-form chat. This paper provides two\ncontributions. First, until now there has been no publicly available\nlarge-scale dataset consisting of real-world dialogues centered around\nrecommendations. To address this issue and to facilitate our exploration here,\nwe have collected ReDial, a dataset consisting of over 10,000 conversations\ncentered around the theme of providing movie recommendations. We make this data\navailable to the community for further research. Second, we use this dataset to\nexplore multiple facets of conversational recommendations. In particular we\nexplore new neural architectures, mechanisms, and methods suitable for\ncomposing conversational recommendation systems. Our dataset allows us to\nsystematically probe model sub-components addressing different parts of the\noverall problem domain ranging from: sentiment analysis and cold-start\nrecommendation generation to detailed aspects of how natural language is used\nin this setting in the real world. We combine such sub-components into a\nfull-blown dialogue system and examine its behavior.", "published": "2018-12-18 19:34:32", "link": "http://arxiv.org/abs/1812.07617v2", "categories": ["cs.LG", "cs.CL", "cs.IR", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Automatic Summarization of Natural Language", "abstract": "Automatic summarization of natural language is a current topic in computer\nscience research and industry, studied for decades because of its usefulness\nacross multiple domains. For example, summarization is necessary to create\nreviews such as this one. Research and applications have achieved some success\nin extractive summarization (where key sentences are curated), however,\nabstractive summarization (synthesis and re-stating) is a hard problem and\ngenerally unsolved in computer science. This literature review contrasts\nhistorical progress up through current state of the art, comparing dimensions\nsuch as: extractive vs. abstractive, supervised vs. unsupervised, NLP (Natural\nLanguage Processing) vs Knowledge-based, deep learning vs algorithms,\nstructured vs. unstructured sources, and measurement metrics such as Rouge and\nBLEU. Multiple dimensions are contrasted since current research uses\ncombinations of approaches as seen in the review matrix. Throughout this\nsummary, synthesis and critique is provided. This review concludes with\ninsights for improved abstractive summarization measurement, with surprising\nimplications for detecting understanding and comprehension in general.", "published": "2018-12-18 14:17:56", "link": "http://arxiv.org/abs/1812.10549v1", "categories": ["cs.CL", "cs.LG", "cs.NE", "stat.ML"], "primary_category": "cs.CL"}
{"title": "BandNet: A Neural Network-based, Multi-Instrument Beatles-Style MIDI\n  Music Composition Machine", "abstract": "In this paper, we propose a recurrent neural network (RNN)-based MIDI music\ncomposition machine that is able to learn musical knowledge from existing\nBeatles' songs and generate music in the style of the Beatles with little human\nintervention. In the learning stage, a sequence of stylistically uniform,\nmultiple-channel music samples was modeled by a RNN. In the composition stage,\na short clip of randomly-generated music was used as a seed for the RNN to\nstart music score prediction. To form structured music, segments of generated\nmusic from different seeds were concatenated together. To improve the quality\nand structure of the generated music, we integrated music theory knowledge into\nthe model, such as controlling the spacing of gaps in the vocal melody,\nnormalizing the timing of chord changes, and requiring notes to be related to\nthe song's key (C major, for example). This integration improved the quality of\nthe generated music as verified by a professional composer. We also conducted a\nsubjective listening test that showed our generated music was close to original\nmusic by the Beatles in terms of style similarity, professional quality, and\ninterestingness. Generated music samples are at https://goo.gl/uaLXoB.", "published": "2018-12-18 01:26:13", "link": "http://arxiv.org/abs/1812.07126v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Autoencoder Based Architecture For Fast & Real Time Audio Style Transfer", "abstract": "Recently, there has been great interest in the field of audio style transfer,\nwhere a stylized audio is generated by imposing the style of a reference audio\non the content of a target audio. We improve on the current approaches which\nuse neural networks to extract the content and the style of the audio signal\nand propose a new autoencoder based architecture for the task. This network\ngenerates a stylized audio for a content audio in a single forward pass. The\nproposed network architecture proves to be advantageous over the quality of\naudio produced and the time taken to train the network. The network is\nexperimented on speech signals to confirm the validity of our proposal.", "published": "2018-12-18 04:04:38", "link": "http://arxiv.org/abs/1812.07159v2", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Uniform Convergence Bounds for Codec Selection", "abstract": "We frame the problem of selecting an optimal audio encoding scheme as a\nsupervised learning task. Through uniform convergence theory, we guarantee\napproximately optimal codec selection while controlling for selection bias. We\npresent rigorous statistical guarantees for the codec selection problem that\nhold for arbitrary distributions over audio sequences and for arbitrary quality\nmetrics. Our techniques can thus balance sound quality and compression ratio,\nand use audio samples from the distribution to select a codec that performs\nwell on that particular type of data. The applications of our technique are\nimmense, as it can be used to optimize for quality and bandwidth usage of\nstreaming and other digital media, while significantly outperforming approaches\nthat apply a fixed codec to all data sources.", "published": "2018-12-18 04:42:34", "link": "http://arxiv.org/abs/1812.07568v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
