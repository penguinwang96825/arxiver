{"title": "User Guide for KOTE: Korean Online Comments Emotions Dataset", "abstract": "Sentiment analysis that classifies data into positive or negative has been\ndominantly used to recognize emotional aspects of texts, despite the deficit of\nthorough examination of emotional meanings. Recently, corpora labeled with more\nthan just valence are built to exceed this limit. However, most Korean emotion\ncorpora are small in the number of instances and cover a limited range of\nemotions. We introduce KOTE dataset. KOTE contains 50k (250k cases) Korean\nonline comments, each of which is manually labeled for 43 emotion labels or one\nspecial label (NO EMOTION) by crowdsourcing (Ps = 3,048). The emotion taxonomy\nof the 43 emotions is systematically established by cluster analysis of Korean\nemotion concepts expressed on word embedding space. After explaining how KOTE\nis developed, we also discuss the results of finetuning and analysis for social\ndiscrimination in the corpus.", "published": "2022-05-11 06:54:10", "link": "http://arxiv.org/abs/2205.05300v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Query-Based Keyphrase Extraction from Long Documents", "abstract": "Transformer-based architectures in natural language processing force input\nsize limits that can be problematic when long documents need to be processed.\nThis paper overcomes this issue for keyphrase extraction by chunking the long\ndocuments while keeping a global context as a query defining the topic for\nwhich relevant keyphrases should be extracted. The developed system employs a\npre-trained BERT model and adapts it to estimate the probability that a given\ntext span forms a keyphrase. We experimented using various context sizes on two\npopular datasets, Inspec and SemEval, and a large novel dataset. The presented\nresults show that a shorter context with a query overcomes a longer one without\nthe query on long documents.", "published": "2022-05-11 10:29:30", "link": "http://arxiv.org/abs/2205.05391v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ALIGNMEET: A Comprehensive Tool for Meeting Annotation, Alignment, and\n  Evaluation", "abstract": "Summarization is a challenging problem, and even more challenging is to\nmanually create, correct, and evaluate the summaries. The severity of the\nproblem grows when the inputs are multi-party dialogues in a meeting setup. To\nfacilitate the research in this area, we present ALIGNMEET, a comprehensive\ntool for meeting annotation, alignment, and evaluation. The tool aims to\nprovide an efficient and clear interface for fast annotation while mitigating\nthe risk of introducing errors. Moreover, we add an evaluation mode that\nenables a comprehensive quality evaluation of meeting minutes. To the best of\nour knowledge, there is no such tool available. We release the tool as open\nsource. It is also directly installable from PyPI.", "published": "2022-05-11 12:16:56", "link": "http://arxiv.org/abs/2205.05433v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Clinical Prompt Learning with Frozen Language Models", "abstract": "Prompt learning is a new paradigm in the Natural Language Processing (NLP)\nfield which has shown impressive performance on a number of natural language\ntasks with common benchmarking text datasets in full, few-shot, and zero-shot\ntrain-evaluation setups. Recently, it has even been observed that large but\nfrozen pre-trained language models (PLMs) with prompt learning outperform\nsmaller but fine-tuned models. However, as with many recent NLP trends, the\nperformance of even the largest PLMs such as GPT-3 do not perform well on\nspecialized domains (e.g. medical text), and the common practice to achieve\nState of the Art (SoTA) results still consists of pre-training and fine-tuning\nthe PLMs on downstream tasks. The reliance on fine-tuning large PLMs is\nproblematic in clinical settings where data is often held in non-GPU\nenvironments, and more resource efficient methods of training specialized\ndomain models is crucial. We investigated the viability of prompt learning on\nclinically meaningful decision tasks and directly compared with more\ntraditional fine-tuning methods. Results are partially in line with the prompt\nlearning literature, with prompt learning able to match or improve on\ntraditional fine-tuning with substantially fewer trainable parameters and\nrequiring less training data. We argue that prompt learning therefore provides\nlower computational resource costs applicable to clinical settings, that can\nserve as an alternative to fine-tuning ever increasing in size PLMs.\nComplementary code to reproduce experiments presented in this work can be found\nat: https://github.com/NtaylorOX/Public_Clinical_Prompt.", "published": "2022-05-11 14:25:13", "link": "http://arxiv.org/abs/2205.05535v1", "categories": ["cs.CL", "ACM-class: J.2"], "primary_category": "cs.CL"}
{"title": "KETOD: Knowledge-Enriched Task-Oriented Dialogue", "abstract": "Existing studies in dialogue system research mostly treat task-oriented\ndialogue and chit-chat as separate domains. Towards building a human-like\nassistant that can converse naturally and seamlessly with users, it is\nimportant to build a dialogue system that conducts both types of conversations\neffectively. In this work, we investigate how task-oriented dialogue and\nknowledge-grounded chit-chat can be effectively integrated into a single model.\nTo this end, we create a new dataset, KETOD (Knowledge-Enriched Task-Oriented\nDialogue), where we naturally enrich task-oriented dialogues with chit-chat\nbased on relevant entity knowledge. We also propose two new models,\nSimpleToDPlus and Combiner, for the proposed task. Experimental results on both\nautomatic and human evaluations show that the proposed methods can\nsignificantly improve the performance in knowledge-enriched response generation\nwhile maintaining a competitive task-oriented dialog performance. We believe\nour new dataset will be a valuable resource for future studies. Our dataset and\ncode are publicly available at \\url{https://github.com/facebookresearch/ketod}.", "published": "2022-05-11 16:01:03", "link": "http://arxiv.org/abs/2205.05589v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Identifying Moments of Change from Longitudinal User Text", "abstract": "Identifying changes in individuals' behaviour and mood, as observed via\ncontent shared on online platforms, is increasingly gaining importance. Most\nresearch to-date on this topic focuses on either: (a) identifying individuals\nat risk or with a certain mental health condition given a batch of posts or (b)\nproviding equivalent labels at the post level. A disadvantage of such work is\nthe lack of a strong temporal component and the inability to make longitudinal\nassessments following an individual's trajectory and allowing timely\ninterventions. Here we define a new task, that of identifying moments of change\nin individuals on the basis of their shared content online. The changes we\nconsider are sudden shifts in mood (switches) or gradual mood progression\n(escalations). We have created detailed guidelines for capturing moments of\nchange and a corpus of 500 manually annotated user timelines (18.7K posts). We\nhave developed a variety of baseline models drawing inspiration from related\ntasks and show that the best performance is obtained through context aware\nsequential modelling. We also introduce new metrics for capturing rare events\nin temporal windows.", "published": "2022-05-11 16:03:47", "link": "http://arxiv.org/abs/2205.05593v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ontology-Driven and Weakly Supervised Rare Disease Identification from\n  Clinical Notes", "abstract": "Computational text phenotyping is the practice of identifying patients with\ncertain disorders and traits from clinical notes. Rare diseases are challenging\nto be identified due to few cases available for machine learning and the need\nfor data annotation from domain experts. We propose a method using ontologies\nand weak supervision, with recent pre-trained contextual representations from\nBi-directional Transformers (e.g. BERT). The ontology-based framework includes\ntwo steps: (i) Text-to-UMLS, extracting phenotypes by contextually linking\nmentions to concepts in Unified Medical Language System (UMLS), with a Named\nEntity Recognition and Linking (NER+L) tool, SemEHR, and weak supervision with\ncustomised rules and contextual mention representation; (ii) UMLS-to-ORDO,\nmatching UMLS concepts to rare diseases in Orphanet Rare Disease Ontology\n(ORDO). The weakly supervised approach is proposed to learn a phenotype\nconfirmation model to improve Text-to-UMLS linking, without annotated data from\ndomain experts. We evaluated the approach on three clinical datasets, MIMIC-III\ndischarge summaries, MIMIC-III radiology reports, and NHS Tayside brain imaging\nreports from two institutions in the US and the UK, with annotations. The\nimprovements in the precision were pronounced (by over 30% to 50% absolute\nscore for Text-to-UMLS linking), with almost no loss of recall compared to the\nexisting NER+L tool, SemEHR. Results on radiology reports from MIMIC-III and\nNHS Tayside were consistent with the discharge summaries. The overall pipeline\nprocessing clinical notes can extract rare disease cases, mostly uncaptured in\nstructured data (manually assigned ICD codes). We discuss the usefulness of the\nweak supervision approach and propose directions for future studies.", "published": "2022-05-11 17:38:24", "link": "http://arxiv.org/abs/2205.05656v5", "categories": ["cs.CL", "68T50 (Primary), 68T30 (Secondary)", "I.2.7; J.3"], "primary_category": "cs.CL"}
{"title": "SubER: A Metric for Automatic Evaluation of Subtitle Quality", "abstract": "This paper addresses the problem of evaluating the quality of automatically\ngenerated subtitles, which includes not only the quality of the\nmachine-transcribed or translated speech, but also the quality of line\nsegmentation and subtitle timing. We propose SubER - a single novel metric\nbased on edit distance with shifts that takes all of these subtitle properties\ninto account. We compare it to existing metrics for evaluating transcription,\ntranslation, and subtitle quality. A careful human evaluation in a post-editing\nscenario shows that the new metric has a high correlation with the post-editing\neffort and direct human assessment scores, outperforming baseline metrics\nconsidering only the subtitle text, such as WER and BLEU, and existing methods\nto integrate segmentation and timing features.", "published": "2022-05-11 23:52:09", "link": "http://arxiv.org/abs/2205.05805v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Relational Triple Extraction: One Step is Enough", "abstract": "Extracting relational triples from unstructured text is an essential task in\nnatural language processing and knowledge graph construction. Existing\napproaches usually contain two fundamental steps: (1) finding the boundary\npositions of head and tail entities; (2) concatenating specific tokens to form\ntriples. However, nearly all previous methods suffer from the problem of error\naccumulation, i.e., the boundary recognition error of each entity in step (1)\nwill be accumulated into the final combined triples. To solve the problem, in\nthis paper, we introduce a fresh perspective to revisit the triple extraction\ntask, and propose a simple but effective model, named DirectRel. Specifically,\nthe proposed model first generates candidate entities through enumerating token\nsequences in a sentence, and then transforms the triple extraction task into a\nlinking problem on a \"head $\\rightarrow$ tail\" bipartite graph. By doing so,\nall triples can be directly extracted in only one step. Extensive experimental\nresults on two widely used datasets demonstrate that the proposed model\nperforms better than the state-of-the-art baselines.", "published": "2022-05-11 05:09:14", "link": "http://arxiv.org/abs/2205.05270v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Unified Prompt Tuning for Few-shot Text Classification", "abstract": "Prompt-based fine-tuning has boosted the performance of Pre-trained Language\nModels (PLMs) on few-shot text classification by employing task-specific\nprompts. Yet, PLMs are unfamiliar with prompt-style expressions during\npre-training, which limits the few-shot learning performance on downstream\ntasks. It would be desirable if the models can acquire some prompting knowledge\nbefore adaptation to specific NLP tasks. We present the Unified Prompt Tuning\n(UPT) framework, leading to better few-shot text classification for BERT-style\nmodels by explicitly capturing prompting semantics from non-target NLP\ndatasets. In UPT, a novel paradigm Prompt-Options-Verbalizer is proposed for\njoint prompt learning across different NLP tasks, forcing PLMs to capture\ntask-invariant prompting knowledge. We further design a self-supervised task\nnamed Knowledge-enhanced Selective Masked Language Modeling to improve the\nPLM's generalization abilities for accurate adaptation to previously unseen\ntasks. After multi-task learning across multiple tasks, the PLM can be better\nprompt-tuned towards any dissimilar target tasks in low-resourced settings.\nExperiments over a variety of NLP tasks show that UPT consistently outperforms\nstate-of-the-arts for prompt-based fine-tuning.", "published": "2022-05-11 07:40:45", "link": "http://arxiv.org/abs/2205.05313v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Efficient Summation Algorithm for the Accuracy, Convergence and\n  Reproducibility of Parallel Numerical Methods", "abstract": "Nowadays, parallel computing is ubiquitous in several application fields,\nboth in engineering and science. The computations rely on the floating-point\narithmetic specified by the IEEE754 Standard. In this context, an elementary\nbrick of computation, used everywhere, is the sum of a sequence of numbers.\nThis sum is subject to many numerical errors in floating-point arithmetic. To\nalleviate this issue, we have introduced a new parallel algorithm for summing a\nsequence of floating-point numbers. This algorithm which scales up easily with\nthe number of processors, adds numbers of the same exponents first. In this\narticle, our main contribution is an extensive analysis of its efficiency with\nrespect to several properties: accuracy, convergence and reproducibility. In\norder to show the usefulness of our algorithm, we have chosen a set of\nrepresentative numerical methods which are Simpson, Jacobi, LU factorization\nand the Iterated power method.", "published": "2022-05-11 08:31:48", "link": "http://arxiv.org/abs/2205.05339v1", "categories": ["cs.CL", "cs.FL"], "primary_category": "cs.CL"}
{"title": "Building for Tomorrow: Assessing the Temporal Persistence of Text\n  Classifiers", "abstract": "Performance of text classification models tends to drop over time due to\nchanges in data, which limits the lifetime of a pretrained model. Therefore an\nability to predict a model's ability to persist over time can help design\nmodels that can be effectively used over a longer period of time. In this\npaper, we provide a thorough discussion into the problem, establish an\nevaluation setup for the task. We look at this problem from a practical\nperspective by assessing the ability of a wide range of language models and\nclassification algorithms to persist over time, as well as how dataset\ncharacteristics can help predict the temporal stability of different models. We\nperform longitudinal classification experiments on three datasets spanning\nbetween 6 and 19 years, and involving diverse tasks and types of data. By\nsplitting the longitudinal datasets into years, we perform a comprehensive set\nof experiments by training and testing across data that are different numbers\nof years apart from each other, both in the past and in the future. This\nenables a gradual investigation into the impact of the temporal gap between\ntraining and test sets on the classification performance, as well as measuring\nthe extent of the persistence over time.", "published": "2022-05-11 12:21:14", "link": "http://arxiv.org/abs/2205.05435v6", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Making Pretrained Language Models Good Long-tailed Learners", "abstract": "Prompt-tuning has shown appealing performance in few-shot classification by\nvirtue of its capability in effectively exploiting pre-trained knowledge. This\nmotivates us to check the hypothesis that prompt-tuning is also a promising\nchoice for long-tailed classification, since the tail classes are intuitively\nfew-shot ones. To achieve this aim, we conduct empirical studies to examine the\nhypothesis. The results demonstrate that prompt-tuning makes pretrained\nlanguage models at least good long-tailed learners. For intuitions on why\nprompt-tuning can achieve good performance in long-tailed classification, we\ncarry out in-depth analyses by progressively bridging the gap between\nprompt-tuning and commonly used finetuning. The summary is that the classifier\nstructure and parameterization form the key to making good long-tailed\nlearners, in comparison with the less important input structure. Finally, we\nverify the applicability of our finding to few-shot classification. Good\nlong-tailed learners can be abbreviated as Glee.", "published": "2022-05-11 13:03:55", "link": "http://arxiv.org/abs/2205.05461v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Utilizing coarse-grained data in low-data settings for event extraction", "abstract": "Annotating text data for event information extraction systems is hard,\nexpensive, and error-prone. We investigate the feasibility of integrating\ncoarse-grained data (document or sentence labels), which is far more feasible\nto obtain, instead of annotating more documents. We utilize a multi-task model\nwith two auxiliary tasks, document and sentence binary classification, in\naddition to the main task of token classification. We perform a series of\nexperiments with varying data regimes for the aforementioned integration.\nResults show that while introducing extra coarse-grained data offers greater\nimprovement and robustness, a gain is still possible with only the addition of\nnegative documents that have no information on any event.", "published": "2022-05-11 13:07:42", "link": "http://arxiv.org/abs/2205.05468v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Identifying concept libraries from language about object structure", "abstract": "Our understanding of the visual world goes beyond naming objects,\nencompassing our ability to parse objects into meaningful parts, attributes,\nand relations. In this work, we leverage natural language descriptions for a\ndiverse set of 2K procedurally generated objects to identify the parts people\nuse and the principles leading these parts to be favored over others. We\nformalize our problem as search over a space of program libraries that contain\ndifferent part concepts, using tools from machine translation to evaluate how\nwell programs expressed in each library align to human language. By combining\nnaturalistic language at scale with structured program representations, we\ndiscover a fundamental information-theoretic tradeoff governing the part\nconcepts people name: people favor a lexicon that allows concise descriptions\nof each object, while also minimizing the size of the lexicon itself.", "published": "2022-05-11 17:49:25", "link": "http://arxiv.org/abs/2205.05666v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards the Generation of Musical Explanations with GPT-3", "abstract": "Open AI's language model, GPT-3, has shown great potential for many NLP\ntasks, with applications in many different domains. In this work we carry out a\nfirst study on GPT-3's capability to communicate musical decisions through\ntextual explanations when prompted with a textual representation of a piece of\nmusic. Enabling a dialogue in human-AI music partnerships is an important step\ntowards more engaging and creative human-AI interactions. Our results show that\nGPT-3 lacks the necessary intelligence to really understand musical decisions.\nA major barrier to reach a better performance is the lack of data that includes\nexplanations of the creative process carried out by artists for musical pieces.\nWe believe such a resource would aid the understanding and collaboration with\nAI music systems.", "published": "2022-05-11 13:04:54", "link": "http://arxiv.org/abs/2206.08264v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Improved Zero-shot Voice Conversion with Conditional DSVAE", "abstract": "Disentangling content and speaking style information is essential for\nzero-shot non-parallel voice conversion (VC). Our previous study investigated a\nnovel framework with disentangled sequential variational autoencoder (DSVAE) as\nthe backbone for information decomposition. We have demonstrated that\nsimultaneous disentangling content embedding and speaker embedding from one\nutterance is feasible for zero-shot VC. In this study, we continue the\ndirection by raising one concern about the prior distribution of content branch\nin the DSVAE baseline. We find the random initialized prior distribution will\nforce the content embedding to reduce the phonetic-structure information during\nthe learning process, which is not a desired property. Here, we seek to achieve\na better content embedding with more phonetic information preserved. We propose\nconditional DSVAE, a new model that enables content bias as a condition to the\nprior modeling and reshapes the content embedding sampled from the posterior\ndistribution. In our experiment on the VCTK dataset, we demonstrate that\ncontent embeddings derived from the conditional DSVAE overcome the randomness\nand achieve a much better phoneme classification accuracy, a stabilized\nvocalization and a better zero-shot VC performance compared with the\ncompetitive DSVAE baseline.", "published": "2022-05-11 01:19:42", "link": "http://arxiv.org/abs/2205.05227v2", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Pre-trained Language Models as Re-Annotators", "abstract": "Annotation noise is widespread in datasets, but manually revising a flawed\ncorpus is time-consuming and error-prone. Hence, given the prior knowledge in\nPre-trained Language Models and the expected uniformity across all annotations,\nwe attempt to reduce annotation noise in the corpus through two tasks\nautomatically: (1) Annotation Inconsistency Detection that indicates the\ncredibility of annotations, and (2) Annotation Error Correction that rectifies\nthe abnormal annotations.\n  We investigate how to acquire semantic sensitive annotation representations\nfrom Pre-trained Language Models, expecting to embed the examples with\nidentical annotations to the mutually adjacent positions even without\nfine-tuning. We proposed a novel credibility score to reveal the likelihood of\nannotation inconsistencies based on the neighbouring consistency. Then, we\nfine-tune the Pre-trained Language Models based classifier with\ncross-validation for annotation correction. The annotation corrector is further\nelaborated with two approaches: (1) soft labelling by Kernel Density Estimation\nand (2) a novel distant-peer contrastive loss.\n  We study the re-annotation in relation extraction and create a new manually\nrevised dataset, Re-DocRED, for evaluating document-level re-annotation. The\nproposed credibility scores show promising agreement with human revisions,\nachieving a Binary F1 of 93.4 and 72.5 in detecting inconsistencies on TACRED\nand DocRED respectively. Moreover, the neighbour-aware classifiers based on\ndistant-peer contrastive learning and uncertain labels achieve Macro F1 up to\n66.2 and 57.8 in correcting annotations on TACRED and DocRED respectively.\nThese improvements are not merely theoretical: Rather, automatically denoised\ntraining sets demonstrate up to 3.6% performance improvement for\nstate-of-the-art relation extraction models.", "published": "2022-05-11 09:28:23", "link": "http://arxiv.org/abs/2205.05368v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AutoKE: An automatic knowledge embedding framework for scientific\n  machine learning", "abstract": "Imposing physical constraints on neural networks as a method of knowledge\nembedding has achieved great progress in solving physical problems described by\ngoverning equations. However, for many engineering problems, governing\nequations often have complex forms, including complex partial derivatives or\nstochastic physical fields, which results in significant inconveniences from\nthe perspective of implementation. In this paper, a scientific machine learning\nframework, called AutoKE, is proposed, and a reservoir flow problem is taken as\nan instance to demonstrate that this framework can effectively automate the\nprocess of embedding physical knowledge. In AutoKE, an emulator comprised of\ndeep neural networks (DNNs) is built for predicting the physical variables of\ninterest. An arbitrarily complex equation can be parsed and automatically\nconverted into a computational graph through the equation parser module, and\nthe fitness of the emulator to the governing equation is evaluated via\nautomatic differentiation. Furthermore, the fixed weights in the loss function\nare substituted with adaptive weights by incorporating the Lagrangian dual\nmethod. Neural architecture search (NAS) is also introduced into the AutoKE to\nselect an optimal network architecture of the emulator according to the\nspecific problem. Finally, we apply transfer learning to enhance the\nscalability of the emulator. In experiments, the framework is verified by a\nseries of physical problems in which it can automatically embed physical\nknowledge into an emulator without heavy hand-coding. The results demonstrate\nthat the emulator can not only make accurate predictions, but also be applied\nto similar problems with high efficiency via transfer learning.", "published": "2022-05-11 10:26:02", "link": "http://arxiv.org/abs/2205.05390v1", "categories": ["cs.LG", "cs.CL", "physics.data-an", "stat.ML"], "primary_category": "cs.LG"}
{"title": "A neural prosody encoder for end-ro-end dialogue act classification", "abstract": "Dialogue act classification (DAC) is a critical task for spoken language\nunderstanding in dialogue systems. Prosodic features such as energy and pitch\nhave been shown to be useful for DAC. Despite their importance, little research\nhas explored neural approaches to integrate prosodic features into end-to-end\n(E2E) DAC models which infer dialogue acts directly from audio signals. In this\nwork, we propose an E2E neural architecture that takes into account the need\nfor characterizing prosodic phenomena co-occurring at different levels inside\nan utterance. A novel part of this architecture is a learnable gating mechanism\nthat assesses the importance of prosodic features and selectively retains core\ninformation necessary for E2E DAC. Our proposed model improves DAC accuracy by\n1.07% absolute across three publicly available benchmark datasets.", "published": "2022-05-11 16:01:06", "link": "http://arxiv.org/abs/2205.05590v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than\n  In-Context Learning", "abstract": "Few-shot in-context learning (ICL) enables pre-trained language models to\nperform a previously-unseen task without any gradient-based training by feeding\na small number of training examples as part of the input. ICL incurs\nsubstantial computational, memory, and storage costs because it involves\nprocessing all of the training examples every time a prediction is made.\nParameter-efficient fine-tuning (PEFT) (e.g. adapter modules, prompt tuning,\nsparse update methods, etc.) offers an alternative paradigm where a small set\nof parameters are trained to enable a model to perform the new task. In this\npaper, we rigorously compare few-shot ICL and PEFT and demonstrate that the\nlatter offers better accuracy as well as dramatically lower computational\ncosts. Along the way, we introduce a new PEFT method called (IA)$^3$ that\nscales activations by learned vectors, attaining stronger performance while\nonly introducing a relatively tiny amount of new parameters. We also propose a\nsimple recipe based on the T0 model called T-Few that can be applied to new\ntasks without task-specific tuning or modifications. We validate the\neffectiveness of T-Few on completely unseen tasks by applying it to the RAFT\nbenchmark, attaining super-human performance for the first time and\noutperforming the state-of-the-art by 6% absolute. All of the code used in our\nexperiments is publicly available.", "published": "2022-05-11 17:10:41", "link": "http://arxiv.org/abs/2205.05638v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Aggregating Pairwise Semantic Differences for Few-Shot Claim Veracity\n  Classification", "abstract": "As part of an automated fact-checking pipeline, the claim veracity\nclassification task consists in determining if a claim is supported by an\nassociated piece of evidence. The complexity of gathering labelled\nclaim-evidence pairs leads to a scarcity of datasets, particularly when dealing\nwith new domains. In this paper, we introduce SEED, a novel vector-based method\nto few-shot claim veracity classification that aggregates pairwise semantic\ndifferences for claim-evidence pairs. We build on the hypothesis that we can\nsimulate class representative vectors that capture average semantic differences\nfor claim-evidence pairs in a class, which can then be used for classification\nof new instances. We compare the performance of our method with competitive\nbaselines including fine-tuned BERT/RoBERTa models, as well as the\nstate-of-the-art few-shot veracity classification method that leverages\nlanguage model perplexity. Experiments conducted on the FEVER and SCIFACT\ndatasets show consistent improvements over competitive baselines in few-shot\nsettings. Our code is available.", "published": "2022-05-11 17:23:37", "link": "http://arxiv.org/abs/2205.05646v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Structured, flexible, and robust: benchmarking and improving large\n  language models towards more human-like behavior in out-of-distribution\n  reasoning tasks", "abstract": "Human language offers a powerful window into our thoughts -- we tell stories,\ngive explanations, and express our beliefs and goals through words. Abundant\nevidence also suggests that language plays a developmental role in structuring\nour learning. Here, we ask: how much of human-like thinking can be captured by\nlearning statistical patterns in language alone? We first contribute a new\nchallenge benchmark for comparing humans and distributional large language\nmodels (LLMs). Our benchmark contains two problem-solving domains (planning and\nexplanation generation) and is designed to require generalization to new,\nout-of-distribution problems expressed in language. We find that humans are far\nmore robust than LLMs on this benchmark. Next, we propose a hybrid\nParse-and-Solve model, which augments distributional LLMs with a structured\nsymbolic reasoning module. We find that this model shows more robust adaptation\nto out-of-distribution planning problems, demonstrating the promise of hybrid\nAI models for more human-like reasoning.", "published": "2022-05-11 18:14:33", "link": "http://arxiv.org/abs/2205.05718v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SC"], "primary_category": "cs.CL"}
{"title": "Some Grammatical Errors are Frequent, Others are Important", "abstract": "In Grammatical Error Correction, systems are evaluated by the number of\nerrors they correct. However, no one has assessed whether all error types are\nequally important. We provide and apply a method to quantify the importance of\ndifferent grammatical error types to humans. We show that some rare errors are\nconsidered disturbing while other common ones are not. This affects possible\ndirections to improve both systems and their evaluation.", "published": "2022-05-11 18:59:20", "link": "http://arxiv.org/abs/2205.05730v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Improved Meta Learning for Low Resource Speech Recognition", "abstract": "We propose a new meta learning based framework for low resource speech\nrecognition that improves the previous model agnostic meta learning (MAML)\napproach. The MAML is a simple yet powerful meta learning approach. However,\nthe MAML presents some core deficiencies such as training instabilities and\nslower convergence speed. To address these issues, we adopt multi-step loss\n(MSL). The MSL aims to calculate losses at every step of the inner loop of MAML\nand then combines them with a weighted importance vector. The importance vector\nensures that the loss at the last step has more importance than the previous\nsteps. Our empirical evaluation shows that MSL significantly improves the\nstability of the training procedure and it thus also improves the accuracy of\nthe overall system. Our proposed system outperforms MAML based low resource ASR\nsystem on various languages in terms of character error rates and stable\ntraining behavior.", "published": "2022-05-11 15:50:47", "link": "http://arxiv.org/abs/2205.06182v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Bias and Fairness on Multimodal Emotion Detection Algorithms", "abstract": "Numerous studies have shown that machine learning algorithms can latch onto\nprotected attributes such as race and gender and generate predictions that\nsystematically discriminate against one or more groups. To date the majority of\nbias and fairness research has been on unimodal models. In this work, we\nexplore the biases that exist in emotion recognition systems in relationship to\nthe modalities utilized, and study how multimodal approaches affect system bias\nand fairness. We consider audio, text, and video modalities, as well as all\npossible multimodal combinations of those, and find that text alone has the\nleast bias, and accounts for the majority of the models' performances, raising\ndoubts about the worthiness of multimodal emotion recognition systems when bias\nand fairness are desired alongside model performance.", "published": "2022-05-11 20:03:25", "link": "http://arxiv.org/abs/2205.08383v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "DISARM: Detecting the Victims Targeted by Harmful Memes", "abstract": "Internet memes have emerged as an increasingly popular means of communication\non the Web. Although typically intended to elicit humour, they have been\nincreasingly used to spread hatred, trolling, and cyberbullying, as well as to\ntarget specific individuals, communities, or society on political,\nsocio-cultural, and psychological grounds. While previous work has focused on\ndetecting harmful, hateful, and offensive memes, identifying whom they attack\nremains a challenging and underexplored area. Here we aim to bridge this gap.\nIn particular, we create a dataset where we annotate each meme with its\nvictim(s) such as the name of the targeted person(s), organization(s), and\ncommunity(ies). We then propose DISARM (Detecting vIctimS targeted by hARmful\nMemes), a framework that uses named entity recognition and person\nidentification to detect all entities a meme is referring to, and then,\nincorporates a novel contextualized multimodal deep neural network to classify\nwhether the meme intends to harm these entities. We perform several systematic\nexperiments on three test setups, corresponding to entities that are (a) all\nseen while training, (b) not seen as a harmful target on training, and (c) not\nseen at all on training. The evaluation results show that DISARM significantly\noutperforms ten unimodal and multimodal systems. Finally, we show that DISARM\nis interpretable and comparatively more generalizable and that it can reduce\nthe relative error rate for harmful target identification by up to 9 points\nabsolute over several strong multimodal rivals.", "published": "2022-05-11 19:14:26", "link": "http://arxiv.org/abs/2205.05738v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.CY", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Learning to Retrieve Videos by Asking Questions", "abstract": "The majority of traditional text-to-video retrieval systems operate in static\nenvironments, i.e., there is no interaction between the user and the agent\nbeyond the initial textual query provided by the user. This can be sub-optimal\nif the initial query has ambiguities, which would lead to many falsely\nretrieved videos. To overcome this limitation, we propose a novel framework for\nVideo Retrieval using Dialog (ViReD), which enables the user to interact with\nan AI agent via multiple rounds of dialog, where the user refines retrieved\nresults by answering questions generated by an AI agent. Our novel multimodal\nquestion generator learns to ask questions that maximize the subsequent video\nretrieval performance using (i) the video candidates retrieved during the last\nround of interaction with the user and (ii) the text-based dialog history\ndocumenting all previous interactions, to generate questions that incorporate\nboth visual and linguistic cues relevant to video retrieval. Furthermore, to\ngenerate maximally informative questions, we propose an Information-Guided\nSupervision (IGS), which guides the question generator to ask questions that\nwould boost subsequent video retrieval accuracy. We validate the effectiveness\nof our interactive ViReD framework on the AVSD dataset, showing that our\ninteractive method performs significantly better than traditional\nnon-interactive video retrieval systems. We also demonstrate that our proposed\napproach generalizes to the real-world settings that involve interactions with\nreal humans, thus, demonstrating the robustness and generality of our framework", "published": "2022-05-11 19:14:39", "link": "http://arxiv.org/abs/2205.05739v3", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.HC", "cs.MA"], "primary_category": "cs.CV"}
{"title": "Cryptocurrency Bubble Detection: A New Stock Market Dataset, Financial\n  Task & Hyperbolic Models", "abstract": "The rapid spread of information over social media influences quantitative\ntrading and investments. The growing popularity of speculative trading of\nhighly volatile assets such as cryptocurrencies and meme stocks presents a\nfresh challenge in the financial realm. Investigating such \"bubbles\" - periods\nof sudden anomalous behavior of markets are critical in better understanding\ninvestor behavior and market dynamics. However, high volatility coupled with\nmassive volumes of chaotic social media texts, especially for underexplored\nassets like cryptocoins pose a challenge to existing methods. Taking the first\nstep towards NLP for cryptocoins, we present and publicly release\nCryptoBubbles, a novel multi-span identification task for bubble detection, and\na dataset of more than 400 cryptocoins from 9 exchanges over five years\nspanning over two million tweets. Further, we develop a set of\nsequence-to-sequence hyperbolic models suited to this multi-span identification\ntask based on the power-law dynamics of cryptocurrencies and user behavior on\nsocial media. We further test the effectiveness of our models under zero-shot\nsettings on a test set of Reddit posts pertaining to 29 \"meme stocks\", which\nsee an increase in trade volume due to social media hype. Through quantitative,\nqualitative, and zero-shot analyses on Reddit and Twitter spanning cryptocoins\nand meme-stocks, we show the practical applicability of CryptoBubbles and\nhyperbolic models.", "published": "2022-05-11 08:10:02", "link": "http://arxiv.org/abs/2206.06320v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SI", "q-fin.ST"], "primary_category": "cs.CL"}
{"title": "Beyond the Status Quo: A Contemporary Survey of Advances and Challenges\n  in Audio Captioning", "abstract": "Automated audio captioning (AAC), a task that mimics human perception as well\nas innovatively links audio processing and natural language processing, has\noverseen much progress over the last few years. AAC requires recognizing\ncontents such as the environment, sound events and the temporal relationships\nbetween sound events and describing these elements with a fluent sentence.\nCurrently, an encoder-decoder-based deep learning framework is the standard\napproach to tackle this problem. Plenty of works have proposed novel network\narchitectures and training schemes, including extra guidance, reinforcement\nlearning, audio-text self-supervised learning and diverse or controllable\ncaptioning. Effective data augmentation techniques, especially based on large\nlanguage models are explored. Benchmark datasets and AAC-oriented evaluation\nmetrics also accelerate the improvement of this field. This paper situates\nitself as a comprehensive survey covering the comparison between AAC and its\nrelated tasks, the existing deep learning techniques, datasets, and the\nevaluation metrics in AAC, with insights provided to guide potential future\nresearch directions.", "published": "2022-05-11 09:09:15", "link": "http://arxiv.org/abs/2205.05357v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A deep representation learning speech enhancement method using\n  $\u03b2$-VAE", "abstract": "In previous work, we proposed a variational autoencoder-based (VAE) Bayesian\npermutation training speech enhancement (SE) method (PVAE) which indicated that\nthe SE performance of the traditional deep neural network-based (DNN) method\ncould be improved by deep representation learning (DRL). Based on our previous\nwork, we in this paper propose to use $\\beta$-VAE to further improve PVAE's\nability of representation learning. More specifically, our $\\beta$-VAE can\nimprove PVAE's capacity of disentangling different latent variables from the\nobserved signal without the trade-off problem between disentanglement and\nsignal reconstruction. This trade-off problem widely exists in previous\n$\\beta$-VAE algorithms. Unlike the previous $\\beta$-VAE algorithms, the\nproposed $\\beta$-VAE strategy can also be used to optimize the DNN's structure.\nThis means that the proposed method can not only improve PVAE's SE performance\nbut also reduce the number of PVAE training parameters. The experimental\nresults show that the proposed method can acquire better speech and noise\nlatent representation than PVAE. Meanwhile, it also obtains a higher\nscale-invariant signal-to-distortion ratio, speech quality, and speech\nintelligibility.", "published": "2022-05-11 15:49:16", "link": "http://arxiv.org/abs/2205.05581v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Real-Time Packet Loss Concealment With Mixed Generative and Predictive\n  Model", "abstract": "As deep speech enhancement algorithms have recently demonstrated capabilities\ngreatly surpassing their traditional counterparts for suppressing noise,\nreverberation and echo, attention is turning to the problem of packet loss\nconcealment (PLC). PLC is a challenging task because it not only involves\nreal-time speech synthesis, but also frequent transitions between the received\naudio and the synthesized concealment. We propose a hybrid neural PLC\narchitecture where the missing speech is synthesized using a generative model\nconditioned using a predictive model. The resulting algorithm achieves natural\nconcealment that surpasses the quality of existing conventional PLC algorithms\nand ranked second in the Interspeech 2022 PLC Challenge. We show that our\nsolution not only works for uncompressed audio, but is also applicable to a\nmodern speech codec.", "published": "2022-05-11 21:55:06", "link": "http://arxiv.org/abs/2205.05785v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Generalized Fast Multichannel Nonnegative Matrix Factorization Based on\n  Gaussian Scale Mixtures for Blind Source Separation", "abstract": "This paper describes heavy-tailed extensions of a state-of-the-art versatile\nblind source separation method called fast multichannel nonnegative matrix\nfactorization (FastMNMF) from a unified point of view. The common way of\nderiving such an extension is to replace the multivariate complex Gaussian\ndistribution in the likelihood function with its heavy-tailed generalization,\ne.g., the multivariate complex Student's t and leptokurtic generalized Gaussian\ndistributions, and tailor-make the corresponding parameter optimization\nalgorithm. Using a wider class of heavy-tailed distributions called a Gaussian\nscale mixture (GSM), i.e., a mixture of Gaussian distributions whose variances\nare perturbed by positive random scalars called impulse variables, we propose\nGSM-FastMNMF and develop an expectationmaximization algorithm that works even\nwhen the probability density function of the impulse variables have no\nanalytical expressions. We show that existing heavy-tailed FastMNMF extensions\nare instances of GSM-FastMNMF and derive a new instance based on the\ngeneralized hyperbolic distribution that include the normal-inverse Gaussian,\nStudent's t, and Gaussian distributions as the special cases. Our experiments\nshow that the normalinverse Gaussian FastMNMF outperforms the state-of-the-art\nFastMNMF extensions and ILRMA model in speech enhancement and separation in\nterms of the signal-to-distortion ratio.", "published": "2022-05-11 08:09:39", "link": "http://arxiv.org/abs/2205.05330v1", "categories": ["cs.SD", "eess.AS", "eess.SP", "stat.ML"], "primary_category": "cs.SD"}
{"title": "DeepFilterNet2: Towards Real-Time Speech Enhancement on Embedded Devices\n  for Full-Band Audio", "abstract": "Deep learning-based speech enhancement has seen huge improvements and\nrecently also expanded to full band audio (48 kHz). However, many approaches\nhave a rather high computational complexity and require big temporal buffers\nfor real time usage e.g. due to temporal convolutions or attention. Both make\nthose approaches not feasible on embedded devices. This work further extends\nDeepFilterNet, which exploits harmonic structure of speech allowing for\nefficient speech enhancement (SE). Several optimizations in the training\nprocedure, data augmentation, and network structure result in state-of-the-art\nSE performance while reducing the real-time factor to 0.04 on a notebook\nCore-i5 CPU. This makes the algorithm applicable to run on embedded devices in\nreal-time. The DeepFilterNet framework can be obtained under an open source\nlicense.", "published": "2022-05-11 13:19:41", "link": "http://arxiv.org/abs/2205.05474v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Automatic Tuberculosis and COVID-19 cough classification using deep\n  learning", "abstract": "We present a deep learning based automatic cough classifier which can\ndiscriminate tuberculosis (TB) coughs from COVID-19 coughs and healthy coughs.\nBoth TB and COVID-19 are respiratory diseases, contagious, have cough as a\npredominant symptom and claim thousands of lives each year. The cough audio\nrecordings were collected at both indoor and outdoor settings and also uploaded\nusing smartphones from subjects around the globe, thus containing various\nlevels of noise. This cough data include 1.68 hours of TB coughs, 18.54 minutes\nof COVID-19 coughs and 1.69 hours of healthy coughs from 47 TB patients, 229\nCOVID-19 patients and 1498 healthy patients and were used to train and evaluate\na CNN, LSTM and Resnet50. These three deep architectures were also pre-trained\non 2.14 hours of sneeze, 2.91 hours of speech and 2.79 hours of noise for\nimproved performance. The class-imbalance in our dataset was addressed by using\nSMOTE data balancing technique and using performance metrics such as F1-score\nand AUC. Our study shows that the highest F1-scores of 0.9259 and 0.8631 have\nbeen achieved from a pre-trained Resnet50 for two-class (TB vs COVID-19) and\nthree-class (TB vs COVID-19 vs healthy) cough classification tasks,\nrespectively. The application of deep transfer learning has improved the\nclassifiers' performance and makes them more robust as they generalise better\nover the cross-validation folds. Their performances exceed the TB triage test\nrequirements set by the world health organisation (WHO). The features producing\nthe best performance contain higher order of MFCCs suggesting that the\ndifferences between TB and COVID-19 coughs are not perceivable by the human\near. This type of cough audio classification is non-contact, cost-effective and\ncan easily be deployed on a smartphone, thus it can be an excellent tool for\nboth TB and COVID-19 screening.", "published": "2022-05-11 13:22:50", "link": "http://arxiv.org/abs/2205.05480v2", "categories": ["cs.LG", "cs.SD", "eess.AS", "q-bio.QM"], "primary_category": "cs.LG"}
{"title": "Beyond Griffin-Lim: Improved Iterative Phase Retrieval for Speech", "abstract": "Phase retrieval is a problem encountered not only in speech and audio\nprocessing, but in many other fields such as optics. Iterative algorithms based\non non-convex set projections are effective and frequently used for retrieving\nthe phase when only STFT magnitudes are available. While the basic Griffin-Lim\nalgorithm and its variants have been the prevalent method for decades, more\nrecent advances, e.g. in optics, raise the question: Can we do better than\nGriffin-Lim for speech signals, using the same principle of iterative\nprojection?\n  In this paper we compare the classical algorithms in the speech domain with\ntwo modern methods from optics with respect to reconstruction quality and\nconvergence rate. Based on this study, we propose to combine Griffin-Lim with\nthe Difference Map algorithm in a hybrid approach which shows superior results,\nin terms of both convergence and quality of the final reconstruction.", "published": "2022-05-11 13:38:54", "link": "http://arxiv.org/abs/2205.05496v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Scream Detection in Heavy Metal Music", "abstract": "Harsh vocal effects such as screams or growls are far more common in heavy\nmetal vocals than the traditionally sung vocal. This paper explores the problem\nof detection and classification of extreme vocal techniques in heavy metal\nmusic, specifically the identification of different scream techniques. We\ninvestigate the suitability of various feature representations, including\ncepstral, spectral, and temporal features as input representations for\nclassification. The main contributions of this work are (i) a manually\nannotated dataset comprised of over 280 minutes of heavy metal songs of various\ngenres with a statistical analysis of occurrences of different extreme vocal\ntechniques in heavy metal music, and (ii) a systematic study of different input\nfeature representations for the classification of heavy metal vocals", "published": "2022-05-11 15:48:56", "link": "http://arxiv.org/abs/2205.05580v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "End-to-End Multi-Person Audio/Visual Automatic Speech Recognition", "abstract": "Traditionally, audio-visual automatic speech recognition has been studied\nunder the assumption that the speaking face on the visual signal is the face\nmatching the audio. However, in a more realistic setting, when multiple faces\nare potentially on screen one needs to decide which face to feed to the A/V ASR\nsystem. The present work takes the recent progress of A/V ASR one step further\nand considers the scenario where multiple people are simultaneously on screen\n(multi-person A/V ASR). We propose a fully differentiable A/V ASR model that is\nable to handle multiple face tracks in a video. Instead of relying on two\nseparate models for speaker face selection and audio-visual ASR on a single\nface track, we introduce an attention layer to the ASR encoder that is able to\nsoft-select the appropriate face video track. Experiments carried out on an A/V\nsystem trained on over 30k hours of YouTube videos illustrate that the proposed\napproach can automatically select the proper face tracks with minor WER\ndegradation compared to an oracle selection of the speaking face while still\nshowing benefits of employing the visual signal instead of the audio alone.", "published": "2022-05-11 15:57:47", "link": "http://arxiv.org/abs/2205.05586v1", "categories": ["eess.AS", "cs.CV", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Closer Look at Audio-Visual Multi-Person Speech Recognition and Active\n  Speaker Selection", "abstract": "Audio-visual automatic speech recognition is a promising approach to robust\nASR under noisy conditions. However, up until recently it had been\ntraditionally studied in isolation assuming the video of a single speaking face\nmatches the audio, and selecting the active speaker at inference time when\nmultiple people are on screen was put aside as a separate problem. As an\nalternative, recent work has proposed to address the two problems\nsimultaneously with an attention mechanism, baking the speaker selection\nproblem directly into a fully differentiable model. One interesting finding was\nthat the attention indirectly learns the association between the audio and the\nspeaking face even though this correspondence is never explicitly provided at\ntraining time. In the present work we further investigate this connection and\nexamine the interplay between the two problems. With experiments involving over\n50 thousand hours of public YouTube videos as training data, we first evaluate\nthe accuracy of the attention layer on an active speaker selection task.\nSecondly, we show under closer scrutiny that an end-to-end model performs at\nleast as well as a considerably larger two-step system that utilizes a hard\ndecision boundary under various noise conditions and number of parallel face\ntracks.", "published": "2022-05-11 15:55:31", "link": "http://arxiv.org/abs/2205.05684v1", "categories": ["eess.AS", "cs.CV", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Deep Learning and Synthetic Media", "abstract": "Deep learning algorithms are rapidly changing the way in which audiovisual\nmedia can be produced. Synthetic audiovisual media generated with deep learning\n- often subsumed colloquially under the label \"deepfakes\" - have a number of\nimpressive characteristics; they are increasingly trivial to produce, and can\nbe indistinguishable from real sounds and images recorded with a sensor. Much\nattention has been dedicated to ethical concerns raised by this technological\ndevelopment. Here, I focus instead on a set of issues related to the notion of\nsynthetic audiovisual media, its place within a broader taxonomy of audiovisual\nmedia, and how deep learning techniques differ from more traditional approaches\nto media synthesis. After reviewing important etiological features of deep\nlearning pipelines for media manipulation and generation, I argue that\n\"deepfakes\" and related synthetic media produced with such pipelines do not\nmerely offer incremental improvements over previous methods, but challenge\ntraditional taxonomical distinctions, and pave the way for genuinely novel\nkinds of audiovisual media.", "published": "2022-05-11 20:28:09", "link": "http://arxiv.org/abs/2205.05764v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
