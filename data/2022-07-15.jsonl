{"title": "Reasoning about Actions over Visual and Linguistic Modalities: A Survey", "abstract": "'Actions' play a vital role in how humans interact with the world and enable\nthem to achieve desired goals. As a result, most common sense (CS) knowledge\nfor humans revolves around actions. While 'Reasoning about Actions & Change'\n(RAC) has been widely studied in the Knowledge Representation community, it has\nrecently piqued the interest of NLP and computer vision researchers. This paper\nsurveys existing tasks, benchmark datasets, various techniques and models, and\ntheir respective performance concerning advancements in RAC in the vision and\nlanguage domain. Towards the end, we summarize our key takeaways, discuss the\npresent challenges facing this research area, and outline potential directions\nfor future research.", "published": "2022-07-15 16:15:46", "link": "http://arxiv.org/abs/2207.07568v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling Non-Cooperative Dialogue: Theoretical and Empirical Insights", "abstract": "Investigating cooperativity of interlocutors is central in studying\npragmatics of dialogue. Models of conversation that only assume cooperative\nagents fail to explain the dynamics of strategic conversations. Thus, we\ninvestigate the ability of agents to identify non-cooperative interlocutors\nwhile completing a concurrent visual-dialogue task. Within this novel setting,\nwe study the optimality of communication strategies for achieving this\nmulti-task objective. We use the tools of learning theory to develop a\ntheoretical model for identifying non-cooperative interlocutors and apply this\ntheory to analyze different communication strategies. We also introduce a\ncorpus of non-cooperative conversations about images in the GuessWhat?! dataset\nproposed by De Vries et al. (2017). We use reinforcement learning to implement\nmultiple communication strategies in this context and find empirical results\nvalidate our theory.", "published": "2022-07-15 02:08:41", "link": "http://arxiv.org/abs/2207.07255v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Z-Index at CheckThat! Lab 2022: Check-Worthiness Identification on Tweet\n  Text", "abstract": "The wide use of social media and digital technologies facilitates sharing\nvarious news and information about events and activities. Despite sharing\npositive information misleading and false information is also spreading on\nsocial media. There have been efforts in identifying such misleading\ninformation both manually by human experts and automatic tools. Manual effort\ndoes not scale well due to the high volume of information, containing factual\nclaims, are appearing online. Therefore, automatically identifying check-worthy\nclaims can be very useful for human experts. In this study, we describe our\nparticipation in Subtask-1A: Check-worthiness of tweets (English, Dutch and\nSpanish) of CheckThat! lab at CLEF 2022. We performed standard preprocessing\nsteps and applied different models to identify whether a given text is worthy\nof fact checking or not. We use the oversampling technique to balance the\ndataset and applied SVM and Random Forest (RF) with TF-IDF representations. We\nalso used BERT multilingual (BERT-m) and XLM-RoBERTa-base pre-trained models\nfor the experiments. We used BERT-m for the official submissions and our\nsystems ranked as 3rd, 5th, and 12th in Spanish, Dutch, and English,\nrespectively. In further experiments, our evaluation shows that transformer\nmodels (BERT-m and XLM-RoBERTa-base) outperform the SVM and RF in Dutch and\nEnglish languages where a different scenario is observed for Spanish.", "published": "2022-07-15 06:21:35", "link": "http://arxiv.org/abs/2207.07308v1", "categories": ["cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "A No-Code Low-Code Paradigm for Authoring Business Automations Using\n  Natural Language", "abstract": "Most business process automation is still developed using traditional\nautomation technologies such as workflow engines. These systems provide domain\nspecific languages that require both business knowledge and programming skills\nto effectively use. As such, business users often lack adequate programming\nskills to fully leverage these code oriented environments. We propose a\nparadigm for the construction of business automations using natural language.\nThe approach applies a large language model to translate business rules and\nautomations described in natural language, into a domain specific language\ninterpretable by a business rule engine. We compare the performance of various\nlanguage model configurations, across various target domains, and explore the\nuse of constrained decoding to ensure syntactically correct generation of\noutput.", "published": "2022-07-15 19:17:55", "link": "http://arxiv.org/abs/2207.10648v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LineCap: Line Charts for Data Visualization Captioning Models", "abstract": "Data visualization captions help readers understand the purpose of a\nvisualization and are crucial for individuals with visual impairments. The\nprevalence of poor figure captions and the successful application of deep\nlearning approaches to image captioning motivate the use of similar techniques\nfor automated figure captioning. However, research in this field has been\nstunted by the lack of suitable datasets. We introduce LineCap, a novel figure\ncaptioning dataset of 3,528 figures, and we provide insights from curating this\ndataset and using end-to-end deep learning models for automated figure\ncaptioning.", "published": "2022-07-15 00:35:59", "link": "http://arxiv.org/abs/2207.07243v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "A Flexible Schema-Guided Dialogue Management Framework: From Friendly\n  Peer to Virtual Standardized Cancer Patient", "abstract": "A schema-guided approach to dialogue management has been shown in recent work\nto be effective in creating robust customizable virtual agents capable of\nacting as friendly peers or task assistants. However, successful applications\nof these methods in open-ended, mixed-initiative domains remain elusive --\nparticularly within medical domains such as virtual standardized patients,\nwhere such complex interactions are commonplace -- and require more extensive\nand flexible dialogue management capabilities than previous systems provide. In\nthis paper, we describe a general-purpose schema-guided dialogue management\nframework used to develop SOPHIE, a virtual standardized cancer patient that\nallows a doctor to conveniently practice for interactions with patients. We\nconduct a crowdsourced evaluation of conversations between medical students and\nSOPHIE. Our agent is judged to produce responses that are natural, emotionally\nappropriate, and consistent with her role as a cancer patient. Furthermore, it\nsignificantly outperforms an end-to-end neural model fine-tuned on a human\nstandardized patient corpus, attesting to the advantages of a schema-guided\napproach.", "published": "2022-07-15 03:52:00", "link": "http://arxiv.org/abs/2207.07276v1", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI"}
{"title": "Boosting Multi-Modal E-commerce Attribute Value Extraction via Unified\n  Learning Scheme and Dynamic Range Minimization", "abstract": "With the prosperity of e-commerce industry, various modalities, e.g., vision\nand language, are utilized to describe product items. It is an enormous\nchallenge to understand such diversified data, especially via extracting the\nattribute-value pairs in text sequences with the aid of helpful image regions.\nAlthough a series of previous works have been dedicated to this task, there\nremain seldomly investigated obstacles that hinder further improvements: 1)\nParameters from up-stream single-modal pretraining are inadequately applied,\nwithout proper jointly fine-tuning in a down-stream multi-modal task. 2) To\nselect descriptive parts of images, a simple late fusion is widely applied,\nregardless of priori knowledge that language-related information should be\nencoded into a common linguistic embedding space by stronger encoders. 3) Due\nto diversity across products, their attribute sets tend to vary greatly, but\ncurrent approaches predict with an unnecessary maximal range and lead to more\npotential false positives. To address these issues, we propose in this paper a\nnovel approach to boost multi-modal e-commerce attribute value extraction via\nunified learning scheme and dynamic range minimization: 1) Firstly, a unified\nscheme is designed to jointly train a multi-modal task with pretrained\nsingle-modal parameters. 2) Secondly, a text-guided information range\nminimization method is proposed to adaptively encode descriptive parts of each\nmodality into an identical space with a powerful pretrained linguistic model.\n3) Moreover, a prototype-guided attribute range minimization method is proposed\nto first determine the proper attribute set of the current product, and then\nselect prototypes to guide the prediction of the chosen attributes. Experiments\non the popular multi-modal e-commerce benchmarks show that our approach\nachieves superior performance over the other state-of-the-art techniques.", "published": "2022-07-15 03:58:04", "link": "http://arxiv.org/abs/2207.07278v2", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Probing Semantic Grounding in Language Models of Code with\n  Representational Similarity Analysis", "abstract": "Representational Similarity Analysis is a method from cognitive neuroscience,\nwhich helps in comparing representations from two different sources of data. In\nthis paper, we propose using Representational Similarity Analysis to probe the\nsemantic grounding in language models of code. We probe representations from\nthe CodeBERT model for semantic grounding by using the data from the IBM\nCodeNet dataset. Through our experiments, we show that current pre-training\nmethods do not induce semantic grounding in language models of code, and\ninstead focus on optimizing form-based patterns. We also show that even a\nlittle amount of fine-tuning on semantically relevant tasks increases the\nsemantic grounding in CodeBERT significantly. Our ablations with the input\nmodality to the CodeBERT model show that using bimodal inputs (code and natural\nlanguage) over unimodal inputs (only code) gives better semantic grounding and\nsample efficiency during semantic fine-tuning. Finally, our experiments with\nsemantic perturbations in code reveal that CodeBERT is able to robustly\ndistinguish between semantically correct and incorrect code.", "published": "2022-07-15 19:04:43", "link": "http://arxiv.org/abs/2207.07706v1", "categories": ["cs.CL", "cs.IR", "cs.PL"], "primary_category": "cs.CL"}
{"title": "The DKU-OPPO System for the 2022 Spoofing-Aware Speaker Verification\n  Challenge", "abstract": "This paper describes our DKU-OPPO system for the 2022 Spoofing-Aware Speaker\nVerification (SASV) Challenge. First, we split the joint task into speaker\nverification (SV) and spoofing countermeasure (CM), these two tasks which are\noptimized separately. For ASV systems, four state-of-the-art methods are\nemployed. For CM systems, we propose two methods on top of the challenge\nbaseline to further improve the performance, namely Embedding Random Sampling\nAugmentation (ERSA) and One-Class Confusion Loss(OCCL). Second, we also explore\nwhether SV embedding could help improve CM system performance. We observe a\ndramatic performance degradation of existing CM systems on the\ndomain-mismatched Voxceleb2 dataset. Third, we compare different fusion\nstrategies, including parallel score fusion and sequential cascaded systems.\nCompared to the 1.71% SASV-EER baseline, our submitted cascaded system obtains\na 0.21% SASV-EER on the challenge official evaluation set.", "published": "2022-07-15 14:56:22", "link": "http://arxiv.org/abs/2207.07510v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Adversarial Reweighting for Speaker Verification Fairness", "abstract": "We address performance fairness for speaker verification using the\nadversarial reweighting (ARW) method. ARW is reformulated for speaker\nverification with metric learning, and shown to improve results across\ndifferent subgroups of gender and nationality, without requiring annotation of\nsubgroups in the training data. An adversarial network learns a weight for each\ntraining sample in the batch so that the main learner is forced to focus on\npoorly performing instances. Using a min-max optimization algorithm, this\nmethod improves overall speaker verification fairness. We present three\ndifferent ARWformulations: accumulated pairwise similarity, pseudo-labeling,\nand pairwise weighting, and measure their performance in terms of equal error\nrate (EER) on the VoxCeleb corpus. Results show that the pairwise weighting\nmethod can achieve 1.08% overall EER, 1.25% for male and 0.67% for female\nspeakers, with relative EER reductions of 7.7%, 10.1% and 3.0%, respectively.\nFor nationality subgroups, the proposed algorithm showed 1.04% EER for US\nspeakers, 0.76% for UK speakers, and 1.22% for all others. The absolute EER gap\nbetween gender groups was reduced from 0.70% to 0.58%, while the standard\ndeviation over nationality groups decreased from 0.21 to 0.19.", "published": "2022-07-15 22:50:31", "link": "http://arxiv.org/abs/2207.07776v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Knowledge Transfer and Distillation from Autoregressive to\n  Non-Autoregressive Speech Recognition", "abstract": "Modern non-autoregressive~(NAR) speech recognition systems aim to accelerate\nthe inference speed; however, they suffer from performance degradation compared\nwith autoregressive~(AR) models as well as the huge model size issue. We\npropose a novel knowledge transfer and distillation architecture that leverages\nknowledge from AR models to improve the NAR performance while reducing the\nmodel's size. Frame- and sequence-level objectives are well-designed for\ntransfer learning. To further boost the performance of NAR, a beam search\nmethod on Mask-CTC is developed to enlarge the search space during the\ninference stage. Experiments show that the proposed NAR beam search relatively\nreduces CER by over 5% on AISHELL-1 benchmark with a tolerable\nreal-time-factor~(RTF) increment. By knowledge transfer, the NAR student who\nhas the same size as the AR teacher obtains relative CER reductions of 8/16% on\nAISHELL-1 dev/test sets, and over 25% relative WER reductions on LibriSpeech\ntest-clean/other sets. Moreover, the ~9x smaller NAR models achieve ~25%\nrelative CER/WER reductions on both AISHELL-1 and LibriSpeech benchmarks with\nthe proposed knowledge transfer and distillation.", "published": "2022-07-15 13:38:45", "link": "http://arxiv.org/abs/2207.10600v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Direction-Aware Joint Adaptation of Neural Speech Enhancement and\n  Recognition in Real Multiparty Conversational Environments", "abstract": "This paper describes noisy speech recognition for an augmented reality\nheadset that helps verbal communication within real multiparty conversational\nenvironments. A major approach that has actively been studied in simulated\nenvironments is to sequentially perform speech enhancement and automatic speech\nrecognition (ASR) based on deep neural networks (DNNs) trained in a supervised\nmanner. In our task, however, such a pretrained system fails to work due to the\nmismatch between the training and test conditions and the head movements of the\nuser. To enhance only the utterances of a target speaker, we use beamforming\nbased on a DNN-based speech mask estimator that can adaptively extract the\nspeech components corresponding to a head-relative particular direction. We\npropose a semi-supervised adaptation method that jointly updates the mask\nestimator and the ASR model at run-time using clean speech signals with\nground-truth transcriptions and noisy speech signals with highly-confident\nestimated transcriptions. Comparative experiments using the state-of-the-art\ndistant speech recognition system show that the proposed method significantly\nimproves the ASR performance.", "published": "2022-07-15 03:43:35", "link": "http://arxiv.org/abs/2207.07273v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Direction-Aware Adaptive Online Neural Speech Enhancement with an\n  Augmented Reality Headset in Real Noisy Conversational Environments", "abstract": "This paper describes the practical response- and performance-aware\ndevelopment of online speech enhancement for an augmented reality (AR) headset\nthat helps a user understand conversations made in real noisy echoic\nenvironments (e.g., cocktail party). One may use a state-of-the-art blind\nsource separation method called fast multichannel nonnegative matrix\nfactorization (FastMNMF) that works well in various environments thanks to its\nunsupervised nature. Its heavy computational cost, however, prevents its\napplication to real-time processing. In contrast, a supervised beamforming\nmethod that uses a deep neural network (DNN) for estimating spatial information\nof speech and noise readily fits real-time processing, but suffers from drastic\nperformance degradation in mismatched conditions. Given such complementary\ncharacteristics, we propose a dual-process robust online speech enhancement\nmethod based on DNN-based beamforming with FastMNMF-guided adaptation. FastMNMF\n(back end) is performed in a mini-batch style and the noisy and enhanced speech\npairs are used together with the original parallel training data for updating\nthe direction-aware DNN (front end) with backpropagation at a\ncomputationally-allowable interval. This method is used with a blind\ndereverberation method called weighted prediction error (WPE) for transcribing\nthe noisy reverberant speech of a speaker, which can be detected from video or\nselected by a user's hand gesture or eye gaze, in a streaming manner and\nspatially showing the transcriptions with an AR technique. Our experiment\nshowed that the word error rate was improved by more than 10 points with the\nrun-time adaptation using only twelve minutes of observation.", "published": "2022-07-15 05:14:27", "link": "http://arxiv.org/abs/2207.07296v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MIMO-DoAnet: Multi-channel Input and Multiple Outputs DoA Network with\n  Unknown Number of Sound Sources", "abstract": "Recent neural network based Direction of Arrival (DoA) estimation algorithms\nhave performed well on unknown number of sound sources scenarios. These\nalgorithms are usually achieved by mapping the multi-channel audio input to the\nsingle output (i.e. overall spatial pseudo-spectrum (SPS) of all sources), that\nis called MISO. However, such MISO algorithms strongly depend on empirical\nthreshold setting and the angle assumption that the angles between the sound\nsources are greater than a fixed angle. To address these limitations, we\npropose a novel multi-channel input and multiple outputs DoA network called\nMIMO-DoAnet. Unlike the general MISO algorithms, MIMO-DoAnet predicts the SPS\ncoding of each sound source with the help of the informative spatial covariance\nmatrix. By doing so, the threshold task of detecting the number of sound\nsources becomes an easier task of detecting whether there is a sound source in\neach output, and the serious interaction between sound sources disappears\nduring inference stage. Experimental results show that MIMO-DoAnet achieves\nrelative 18.6% and absolute 13.3%, relative 34.4% and absolute 20.2% F1 score\nimprovement compared with the MISO baseline system in 3, 4 sources scenes. The\nresults also demonstrate MIMO-DoAnet alleviates the threshold setting problem\nand solves the angle assumption problem effectively.", "published": "2022-07-15 06:18:00", "link": "http://arxiv.org/abs/2207.07307v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "PoLyScriber: Integrated Fine-tuning of Extractor and Lyrics Transcriber\n  for Polyphonic Music", "abstract": "Lyrics transcription of polyphonic music is challenging as the background\nmusic affects lyrics intelligibility. Typically, lyrics transcription can be\nperformed by a two-step pipeline, i.e. a singing vocal extraction front end,\nfollowed by a lyrics transcriber back end, where the front end and back end are\ntrained separately. Such a two-step pipeline suffers from both imperfect vocal\nextraction and mismatch between front end and back end. In this work, we\npropose a novel end-to-end integrated fine-tuning framework, that we call\nPoLyScriber, to globally optimize the vocal extractor front end and lyrics\ntranscriber back end for lyrics transcription in polyphonic music. The\nexperimental results show that our proposed PoLyScriber achieves substantial\nimprovements over the existing approaches on publicly available test datasets.", "published": "2022-07-15 08:24:23", "link": "http://arxiv.org/abs/2207.07336v4", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "PodcastMix: A dataset for separating music and speech in podcasts", "abstract": "We introduce PodcastMix, a dataset formalizing the task of separating\nbackground music and foreground speech in podcasts. We aim at defining a\nbenchmark suitable for training and evaluating (deep learning) source\nseparation models. To that end, we release a large and diverse training dataset\nbased on programatically generated podcasts. However, current (deep learning)\nmodels can incur into generalization issues, specially when trained on\nsynthetic data. To target potential generalization issues, we release an\nevaluation set based on real podcasts for which we design objective and\nsubjective tests. Out of our experiments with real podcasts, we find that\ncurrent (deep learning) models may have generalization issues. Yet, these can\nperform competently, e.g., our best baseline separates speech with a mean\nopinion score of 3.84 (rating \"overall separation quality\" from 1 to 5). The\ndataset and baselines are accessible online.", "published": "2022-07-15 11:12:21", "link": "http://arxiv.org/abs/2207.07403v1", "categories": ["cs.SD", "cs.DB", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Continual Learning For On-Device Environmental Sound Classification", "abstract": "Continuously learning new classes without catastrophic forgetting is a\nchallenging problem for on-device environmental sound classification given the\nrestrictions on computation resources (e.g., model size, running memory). To\naddress this issue, we propose a simple and efficient continual learning\nmethod. Our method selects the historical data for the training by measuring\nthe per-sample classification uncertainty. Specifically, we measure the\nuncertainty by observing how the classification probability of data fluctuates\nagainst the parallel perturbations added to the classifier embedding. In this\nway, the computation cost can be significantly reduced compared with adding\nperturbation to the raw data. Experimental results on the DCASE 2019 Task 1 and\nESC-50 dataset show that our proposed method outperforms baseline continual\nlearning methods on classification accuracy and computational efficiency,\nindicating our method can efficiently and incrementally learn new classes\nwithout the catastrophic forgetting problem for on-device environmental sound\nclassification.", "published": "2022-07-15 12:13:04", "link": "http://arxiv.org/abs/2207.07429v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Low-bit Shift Network for End-to-End Spoken Language Understanding", "abstract": "Deep neural networks (DNN) have achieved impressive success in multiple\ndomains. Over the years, the accuracy of these models has increased with the\nproliferation of deeper and more complex architectures. Thus, state-of-the-art\nsolutions are often computationally expensive, which makes them unfit to be\ndeployed on edge computing platforms. In order to mitigate the high\ncomputation, memory, and power requirements of inferring convolutional neural\nnetworks (CNNs), we propose the use of power-of-two quantization, which\nquantizes continuous parameters into low-bit power-of-two values. This reduces\ncomputational complexity by removing expensive multiplication operations and\nwith the use of low-bit weights. ResNet is adopted as the building block of our\nsolution and the proposed model is evaluated on a spoken language understanding\n(SLU) task. Experimental results show improved performance for shift neural\nnetwork architectures, with our low-bit quantization achieving 98.76 \\% on the\ntest set which is comparable performance to its full-precision counterpart and\nstate-of-the-art solutions.", "published": "2022-07-15 14:34:22", "link": "http://arxiv.org/abs/2207.07497v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Position Prediction as an Effective Pretraining Strategy", "abstract": "Transformers have gained increasing popularity in a wide range of\napplications, including Natural Language Processing (NLP), Computer Vision and\nSpeech Recognition, because of their powerful representational capacity.\nHowever, harnessing this representational capacity effectively requires a large\namount of data, strong regularization, or both, to mitigate overfitting.\nRecently, the power of the Transformer has been unlocked by self-supervised\npretraining strategies based on masked autoencoders which rely on\nreconstructing masked inputs, directly, or contrastively from unmasked content.\nThis pretraining strategy which has been used in BERT models in NLP, Wav2Vec\nmodels in Speech and, recently, in MAE models in Vision, forces the model to\nlearn about relationships between the content in different parts of the input\nusing autoencoding related objectives. In this paper, we propose a novel, but\nsurprisingly simple alternative to content reconstruction~-- that of predicting\nlocations from content, without providing positional information for it. Doing\nso requires the Transformer to understand the positional relationships between\ndifferent parts of the input, from their content alone. This amounts to an\nefficient implementation where the pretext task is a classification problem\namong all possible positions for each input token. We experiment on both Vision\nand Speech benchmarks, where our approach brings improvements over strong\nsupervised training baselines and is comparable to modern\nunsupervised/self-supervised pretraining methods. Our method also enables\nTransformers trained without position embeddings to outperform ones trained\nwith full position information.", "published": "2022-07-15 17:10:48", "link": "http://arxiv.org/abs/2207.07611v1", "categories": ["cs.LG", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Segment-level Metric Learning for Few-shot Bioacoustic Event Detection", "abstract": "Few-shot bioacoustic event detection is a task that detects the occurrence\ntime of a novel sound given a few examples. Previous methods employ metric\nlearning to build a latent space with the labeled part of different sound\nclasses, also known as positive events. In this study, we propose a\nsegment-level few-shot learning framework that utilizes both the positive and\nnegative events during model optimization. Training with negative events, which\nare larger in volume than positive events, can increase the generalization\nability of the model. In addition, we use transductive inference on the\nvalidation set during training for better adaptation to novel classes. We\nconduct ablation studies on our proposed method with different setups on input\nfeatures, training data, and hyper-parameters. Our final system achieves an\nF-measure of 62.73 on the DCASE 2022 challenge task 5 (DCASE2022-T5) validation\nset, outperforming the performance of the baseline prototypical network 34.02\nby a large margin. Using the proposed method, our submitted system ranks 2nd in\nDCASE2022-T5. The code of this paper is fully open-sourced at\nhttps://github.com/haoheliu/DCASE_2022_Task_5.", "published": "2022-07-15 22:41:30", "link": "http://arxiv.org/abs/2207.07773v1", "categories": ["eess.AS", "cs.AI", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
