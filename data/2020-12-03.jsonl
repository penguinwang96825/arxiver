{"title": "Federated Learning for Personalized Humor Recognition", "abstract": "Computational understanding of humor is an important topic under creative\nlanguage understanding and modeling. It can play a key role in complex human-AI\ninteractions. The challenge here is that human perception of humorous content\nis highly subjective. The same joke may receive different funniness ratings\nfrom different readers. This makes it highly challenging for humor recognition\nmodels to achieve personalization in practical scenarios. Existing approaches\nare generally designed based on the assumption that users have a consensus on\nwhether a given text is humorous or not. Thus, they cannot handle diverse humor\npreferences well. In this paper, we propose the FedHumor approach for the\nrecognition of humorous content in a personalized manner through Federated\nLearning (FL). Extending a pre-trained language model, FedHumor guides the\nfine-tuning process by considering diverse distributions of humor preferences\nfrom individuals. It incorporates a diversity adaptation strategy into the FL\nparadigm to train a personalized humor recognition model. To the best of our\nknowledge, FedHumor is the first text-based personalized humor recognition\nmodel through federated learning. Extensive experiments demonstrate the\nadvantage of FedHumor in recognizing humorous texts compared to nine\nstate-of-the-art humor recognition approaches with superior capability for\nhandling the diversity in humor labels produced by users with diverse\npreferences.", "published": "2020-12-03 03:24:24", "link": "http://arxiv.org/abs/2012.01675v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Neural RST Discourse Parsing", "abstract": "Text discourse parsing plays an important role in understanding information\nflow and argumentative structure in natural language. Previous research under\nthe Rhetorical Structure Theory (RST) has mostly focused on inducing and\nevaluating models from the English treebank. However, the parsing tasks for\nother languages such as German, Dutch, and Portuguese are still challenging due\nto the shortage of annotated data. In this work, we investigate two approaches\nto establish a neural, cross-lingual discourse parser via: (1) utilizing\nmultilingual vector representations; and (2) adopting segment-level translation\nof the source content. Experiment results show that both methods are effective\neven with limited training data, and achieve state-of-the-art performance on\ncross-lingual, document-level discourse parsing on all sub-tasks.", "published": "2020-12-03 05:03:38", "link": "http://arxiv.org/abs/2012.01704v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Explaining Structures Improve NLP Models", "abstract": "Existing approaches to explaining deep learning models in NLP usually suffer\nfrom two major drawbacks: (1) the main model and the explaining model are\ndecoupled: an additional probing or surrogate model is used to interpret an\nexisting model, and thus existing explaining tools are not self-explainable;\n(2) the probing model is only able to explain a model's predictions by\noperating on low-level features by computing saliency scores for individual\nwords but are clumsy at high-level text units such as phrases, sentences, or\nparagraphs. To deal with these two issues, in this paper, we propose a simple\nyet general and effective self-explaining framework for deep learning models in\nNLP. The key point of the proposed framework is to put an additional layer, as\nis called by the interpretation layer, on top of any existing NLP model. This\nlayer aggregates the information for each text span, which is then associated\nwith a specific weight, and their weighted combination is fed to the softmax\nfunction for the final prediction. The proposed model comes with the following\nmerits: (1) span weights make the model self-explainable and do not require an\nadditional probing model for interpretation; (2) the proposed model is general\nand can be adapted to any existing deep learning structures in NLP; (3) the\nweight associated with each text span provides direct importance scores for\nhigher-level text units such as phrases and sentences. We for the first time\nshow that interpretability does not come at the cost of performance: a neural\nmodel of self-explaining features obtains better performances than its\ncounterpart without the self-explaining nature, achieving a new SOTA\nperformance of 59.1 on SST-5 and a new SOTA performance of 92.3 on SNLI.", "published": "2020-12-03 09:32:05", "link": "http://arxiv.org/abs/2012.01786v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Label Enhanced Event Detection with Heterogeneous Graph Attention\n  Networks", "abstract": "Event Detection (ED) aims to recognize instances of specified types of event\ntriggers in text. Different from English ED, Chinese ED suffers from the\nproblem of word-trigger mismatch due to the uncertain word boundaries. Existing\napproaches injecting word information into character-level models have achieved\npromising progress to alleviate this problem, but they are limited by two\nissues. First, the interaction between characters and lexicon words is not\nfully exploited. Second, they ignore the semantic information provided by event\nlabels. We thus propose a novel architecture named Label enhanced Heterogeneous\nGraph Attention Networks (L-HGAT). Specifically, we transform each sentence\ninto a graph, where character nodes and word nodes are connected with different\ntypes of edges, so that the interaction between words and characters is fully\nreserved. A heterogeneous graph attention networks is then introduced to\npropagate relational message and enrich information interaction. Furthermore,\nwe convert each label into a trigger-prototype-based embedding, and design a\nmargin loss to guide the model distinguish confusing event labels. Experiments\non two benchmark datasets show that our model achieves significant improvement\nover a range of competitive baseline methods.", "published": "2020-12-03 12:49:22", "link": "http://arxiv.org/abs/2012.01878v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Context in Informational Bias Detection", "abstract": "Informational bias is bias conveyed through sentences or clauses that provide\ntangential, speculative or background information that can sway readers'\nopinions towards entities. By nature, informational bias is context-dependent,\nbut previous work on informational bias detection has not explored the role of\ncontext beyond the sentence. In this paper, we explore four kinds of context\nfor informational bias in English news articles: neighboring sentences, the\nfull article, articles on the same event from other news publishers, and\narticles from the same domain (but potentially different events). We find that\nintegrating event context improves classification performance over a very\nstrong baseline. In addition, we perform the first error analysis of models on\nthis task. We find that the best-performing context-inclusive model outperforms\nthe baseline on longer sentences, and sentences from politically centrist\narticles.", "published": "2020-12-03 15:50:20", "link": "http://arxiv.org/abs/2012.02015v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do We Really Need That Many Parameters In Transformer For Extractive\n  Summarization? Discourse Can Help !", "abstract": "The multi-head self-attention of popular transformer models is widely used\nwithin Natural Language Processing (NLP), including for the task of extractive\nsummarization. With the goal of analyzing and pruning the parameter-heavy\nself-attention mechanism, there are multiple approaches proposing more\nparameter-light self-attention alternatives. In this paper, we present a novel\nparameter-lean self-attention mechanism using discourse priors. Our new tree\nself-attention is based on document-level discourse information, extending the\nrecently proposed \"Synthesizer\" framework with another lightweight alternative.\nWe show empirical results that our tree self-attention approach achieves\ncompetitive ROUGE-scores on the task of extractive summarization. When compared\nto the original single-head transformer model, the tree attention approach\nreaches similar performance on both, EDU and sentence level, despite the\nsignificant reduction of parameters in the attention component. We further\nsignificantly outperform the 8-head transformer model on sentence level when\napplying a more balanced hyper-parameter setting, requiring an order of\nmagnitude less parameters.", "published": "2020-12-03 18:23:21", "link": "http://arxiv.org/abs/2012.02144v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentiment analysis in Bengali via transfer learning using multi-lingual\n  BERT", "abstract": "Sentiment analysis (SA) in Bengali is challenging due to this Indo-Aryan\nlanguage's highly inflected properties with more than 160 different inflected\nforms for verbs and 36 different forms for noun and 24 different forms for\npronouns. The lack of standard labeled datasets in the Bengali domain makes the\ntask of SA even harder. In this paper, we present manually tagged 2-class and\n3-class SA datasets in Bengali. We also demonstrate that the multi-lingual BERT\nmodel with relevant extensions can be trained via the approach of transfer\nlearning over those novel datasets to improve the state-of-the-art performance\nin sentiment classification tasks. This deep learning model achieves an\naccuracy of 71\\% for 2-class sentiment classification compared to the current\nstate-of-the-art accuracy of 68\\%. We also present the very first Bengali SA\nclassifier for the 3-class manually tagged dataset, and our proposed model\nachieves an accuracy of 60\\%. We further use this model to analyze the\nsentiment of public comments in the online daily newspaper. Our analysis shows\nthat people post negative comments for political or sports news more often,\nwhile the religious article comments represent positive sentiment. The dataset\nand code is publicly available at\nhttps://github.com/KhondokerIslam/Bengali\\_Sentiment.", "published": "2020-12-03 10:21:11", "link": "http://arxiv.org/abs/2012.07538v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Circles are like Ellipses, or Ellipses are like Circles? Measuring the\n  Degree of Asymmetry of Static and Contextual Embeddings and the Implications\n  to Representation Learning", "abstract": "Human judgments of word similarity have been a popular method of evaluating\nthe quality of word embedding. But it fails to measure the geometry properties\nsuch as asymmetry. For example, it is more natural to say \"Ellipses are like\nCircles\" than \"Circles are like Ellipses\". Such asymmetry has been observed\nfrom a psychoanalysis test called word evocation experiment, where one word is\nused to recall another. Although useful, such experimental data have been\nsignificantly understudied for measuring embedding quality. In this paper, we\nuse three well-known evocation datasets to gain insights into asymmetry\nencoding of embedding. We study both static embedding as well as contextual\nembedding, such as BERT. Evaluating asymmetry for BERT is generally hard due to\nthe dynamic nature of embedding. Thus, we probe BERT's conditional\nprobabilities (as a language model) using a large number of Wikipedia contexts\nto derive a theoretically justifiable Bayesian asymmetry score. The result\nshows that contextual embedding shows randomness than static embedding on\nsimilarity judgments while performing well on asymmetry judgment, which aligns\nwith its strong performance on \"extrinsic evaluations\" such as text\nclassification. The asymmetry judgment and the Bayesian approach provides a new\nperspective to evaluate contextual embedding on intrinsic evaluation, and its\ncomparison to similarity evaluation concludes our work with a discussion on the\ncurrent state and the future of representation learning.", "published": "2020-12-03 01:48:37", "link": "http://arxiv.org/abs/2012.01631v1", "categories": ["cs.CL", "cs.AI", "91F20", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Leveraging Abstract Meaning Representation for Knowledge Base Question\n  Answering", "abstract": "Knowledge base question answering (KBQA)is an important task in Natural\nLanguage Processing. Existing approaches face significant challenges including\ncomplex question understanding, necessity for reasoning, and lack of large\nend-to-end training datasets. In this work, we propose Neuro-Symbolic Question\nAnswering (NSQA), a modular KBQA system, that leverages (1) Abstract Meaning\nRepresentation (AMR) parses for task-independent question understanding; (2) a\nsimple yet effective graph transformation approach to convert AMR parses into\ncandidate logical queries that are aligned to the KB; (3) a pipeline-based\napproach which integrates multiple, reusable modules that are trained\nspecifically for their individual tasks (semantic parser, entity\nandrelationship linkers, and neuro-symbolic reasoner) and do not require\nend-to-end training data. NSQA achieves state-of-the-art performance on two\nprominent KBQA datasets based on DBpedia (QALD-9 and LC-QuAD1.0). Furthermore,\nour analysis emphasizes that AMR is a powerful tool for KBQA systems.", "published": "2020-12-03 05:17:55", "link": "http://arxiv.org/abs/2012.01707v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning Class-Transductive Intent Representations for Zero-shot Intent\n  Detection", "abstract": "Zero-shot intent detection (ZSID) aims to deal with the continuously emerging\nintents without annotated training data. However, existing ZSID systems suffer\nfrom two limitations: 1) They are not good at modeling the relationship between\nseen and unseen intents. 2) They cannot effectively recognize unseen intents\nunder the generalized intent detection (GZSID) setting. A critical problem\nbehind these limitations is that the representations of unseen intents cannot\nbe learned in the training stage. To address this problem, we propose a novel\nframework that utilizes unseen class labels to learn Class-Transductive Intent\nRepresentations (CTIR). Specifically, we allow the model to predict unseen\nintents during training, with the corresponding label names serving as input\nutterances. On this basis, we introduce a multi-task learning objective, which\nencourages the model to learn the distinctions among intents, and a similarity\nscorer, which estimates the connections among intents more accurately. CTIR is\neasy to implement and can be integrated with existing methods. Experiments on\ntwo real-world datasets show that CTIR brings considerable improvement to the\nbaseline systems.", "published": "2020-12-03 06:41:09", "link": "http://arxiv.org/abs/2012.01721v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Bengali Abstractive News Summarization(BANS): A Neural Attention\n  Approach", "abstract": "Abstractive summarization is the process of generating novel sentences based\non the information extracted from the original text document while retaining\nthe context. Due to abstractive summarization's underlying complexities, most\nof the past research work has been done on the extractive summarization\napproach. Nevertheless, with the triumph of the sequence-to-sequence (seq2seq)\nmodel, abstractive summarization becomes more viable. Although a significant\nnumber of notable research has been done in the English language based on\nabstractive summarization, only a couple of works have been done on Bengali\nabstractive news summarization (BANS). In this article, we presented a seq2seq\nbased Long Short-Term Memory (LSTM) network model with attention at\nencoder-decoder. Our proposed system deploys a local attention-based model that\nproduces a long sequence of words with lucid and human-like generated sentences\nwith noteworthy information of the original document. We also prepared a\ndataset of more than 19k articles and corresponding human-written summaries\ncollected from bangla.bdnews24.com1 which is till now the most extensive\ndataset for Bengali news document summarization and publicly published in\nKaggle2. We evaluated our model qualitatively and quantitatively and compared\nit with other published results. It showed significant improvement in terms of\nhuman evaluation scores with state-of-the-art approaches for BANS.", "published": "2020-12-03 08:17:31", "link": "http://arxiv.org/abs/2012.01747v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SemMT: A Semantic-based Testing Approach for Machine Translation Systems", "abstract": "Machine translation has wide applications in daily life. In mission-critical\napplications such as translating official documents, incorrect translation can\nhave unpleasant or sometimes catastrophic consequences. This motivates recent\nresearch on testing methodologies for machine translation systems. Existing\nmethodologies mostly rely on metamorphic relations designed at the textual\nlevel (e.g., Levenshtein distance) or syntactic level (e.g., the distance\nbetween grammar structures) to determine the correctness of translation\nresults. However, these metamorphic relations do not consider whether the\noriginal and translated sentences have the same meaning (i.e., Semantic\nsimilarity). Therefore, in this paper, we propose SemMT, an automatic testing\napproach for machine translation systems based on semantic similarity checking.\nSemMT applies round-trip translation and measures the semantic similarity\nbetween the original and translated sentences. Our insight is that the\nsemantics expressed by the logic and numeric constraint in sentences can be\ncaptured using regular expressions (or deterministic finite automata) where\nefficient equivalence/similarity checking algorithms are available. Leveraging\nthe insight, we propose three semantic similarity metrics and implement them in\nSemMT. The experiment result reveals SemMT can achieve higher effectiveness\ncompared with state-of-the-art works, achieving an increase of 21% and 23% on\naccuracy and F-Score, respectively. We also explore potential improvements that\ncan be achieved when proper combinations of metrics are adopted. Finally, we\ndiscuss a solution to locate the suspicious trip in round-trip translation,\nwhich may shed lights on further exploration.", "published": "2020-12-03 10:42:56", "link": "http://arxiv.org/abs/2012.01815v2", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "CUT: Controllable Unsupervised Text Simplification", "abstract": "In this paper, we focus on the challenge of learning controllable text\nsimplifications in unsupervised settings. While this problem has been\npreviously discussed for supervised learning algorithms, the literature on the\nanalogies in unsupervised methods is scarse. We propose two unsupervised\nmechanisms for controlling the output complexity of the generated texts,\nnamely, back translation with control tokens (a learning-based approach) and\nsimplicity-aware beam search (decoding-based approach). We show that by nudging\na back-translation algorithm to understand the relative simplicity of a text in\ncomparison to its noisy translation, the algorithm self-supervises itself to\nproduce the output of the desired complexity. This approach achieves\ncompetitive performance on well-established benchmarks: SARI score of 46.88%\nand FKGL of 3.65% on the Newsela dataset.", "published": "2020-12-03 14:14:30", "link": "http://arxiv.org/abs/2012.01936v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "GottBERT: a pure German Language Model", "abstract": "Lately, pre-trained language models advanced the field of natural language\nprocessing (NLP). The introduction of Bidirectional Encoders for Transformers\n(BERT) and its optimized version RoBERTa have had significant impact and\nincreased the relevance of pre-trained models. First, research in this field\nmainly started on English data followed by models trained with multilingual\ntext corpora. However, current research shows that multilingual models are\ninferior to monolingual models. Currently, no German single language RoBERTa\nmodel is yet published, which we introduce in this work (GottBERT). The German\nportion of the OSCAR data set was used as text corpus. In an evaluation we\ncompare its performance on the two Named Entity Recognition (NER) tasks Conll\n2003 and GermEval 2014 as well as on the text classification tasks GermEval\n2018 (fine and coarse) and GNAD with existing German single language BERT\nmodels and two multilingual ones. GottBERT was pre-trained related to the\noriginal RoBERTa model using fairseq. All downstream tasks were trained using\nhyperparameter presets taken from the benchmark of German BERT. The experiments\nwere setup utilizing FARM. Performance was measured by the $F_{1}$ score.\nGottBERT was successfully pre-trained on a 256 core TPU pod using the RoBERTa\nBASE architecture. Even without extensive hyper-parameter optimization, in all\nNER and one text classification task, GottBERT already outperformed all other\ntested German and multilingual models. In order to support the German NLP\nfield, we publish GottBERT under the AGPLv3 license.", "published": "2020-12-03 17:45:03", "link": "http://arxiv.org/abs/2012.02110v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BERT-hLSTMs: BERT and Hierarchical LSTMs for Visual Storytelling", "abstract": "Visual storytelling is a creative and challenging task, aiming to\nautomatically generate a story-like description for a sequence of images. The\ndescriptions generated by previous visual storytelling approaches lack\ncoherence because they use word-level sequence generation methods and do not\nadequately consider sentence-level dependencies. To tackle this problem, we\npropose a novel hierarchical visual storytelling framework which separately\nmodels sentence-level and word-level semantics. We use the transformer-based\nBERT to obtain embeddings for sentences and words. We then employ a\nhierarchical LSTM network: the bottom LSTM receives as input the sentence\nvector representation from BERT, to learn the dependencies between the\nsentences corresponding to images, and the top LSTM is responsible for\ngenerating the corresponding word vector representations, taking input from the\nbottom LSTM. Experimental results demonstrate that our model outperforms most\nclosely related baselines under automatic evaluation metrics BLEU and CIDEr,\nand also show the effectiveness of our method with human evaluation.", "published": "2020-12-03 18:07:28", "link": "http://arxiv.org/abs/2012.02128v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Addressing machine learning concept drift reveals declining vaccine\n  sentiment during the COVID-19 pandemic", "abstract": "Social media analysis has become a common approach to assess public opinion\non various topics, including those about health, in near real-time. The growing\nvolume of social media posts has led to an increased usage of modern machine\nlearning methods in natural language processing. While the rapid dynamics of\nsocial media can capture underlying trends quickly, it also poses a technical\nproblem: algorithms trained on annotated data in the past may underperform when\napplied to contemporary data. This phenomenon, known as concept drift, can be\nparticularly problematic when rapid shifts occur either in the topic of\ninterest itself, or in the way the topic is discussed. Here, we explore the\neffect of machine learning concept drift by focussing on vaccine sentiments\nexpressed on Twitter, a topic of central importance especially during the\nCOVID-19 pandemic. We show that while vaccine sentiment has declined\nconsiderably during the COVID-19 pandemic in 2020, algorithms trained on\npre-pandemic data would have largely missed this decline due to concept drift.\nOur results suggest that social media analysis systems must address concept\ndrift in a continuous fashion in order to avoid the risk of systematic\nmisclassification of data, which is particularly likely during a crisis when\nthe underlying data can change suddenly and rapidly.", "published": "2020-12-03 18:53:57", "link": "http://arxiv.org/abs/2012.02197v2", "categories": ["cs.SI", "cs.CL", "I.2.7; J.3"], "primary_category": "cs.SI"}
{"title": "Adapt-and-Adjust: Overcoming the Long-Tail Problem of Multilingual\n  Speech Recognition", "abstract": "One crucial challenge of real-world multilingual speech recognition is the\nlong-tailed distribution problem, where some resource-rich languages like\nEnglish have abundant training data, but a long tail of low-resource languages\nhave varying amounts of limited training data. To overcome the long-tail\nproblem, in this paper, we propose Adapt-and-Adjust (A2), a transformer-based\nmulti-task learning framework for end-to-end multilingual speech recognition.\nThe A2 framework overcomes the long-tail problem via three techniques: (1)\nexploiting a pretrained multilingual language model (mBERT) to improve the\nperformance of low-resource languages; (2) proposing dual adapters consisting\nof both language-specific and language-agnostic adaptation with minimal\nadditional parameters; and (3) overcoming the class imbalance, either by\nimposing class priors in the loss during training or adjusting the logits of\nthe softmax output during inference. Extensive experiments on the CommonVoice\ncorpus show that A2 significantly outperforms conventional approaches.", "published": "2020-12-03 03:46:16", "link": "http://arxiv.org/abs/2012.01687v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DialogBERT: Discourse-Aware Response Generation via Learning to Recover\n  and Rank Utterances", "abstract": "Recent advances in pre-trained language models have significantly improved\nneural response generation. However, existing methods usually view the dialogue\ncontext as a linear sequence of tokens and learn to generate the next word\nthrough token-level self-attention. Such token-level encoding hinders the\nexploration of discourse-level coherence among utterances. This paper presents\nDialogBERT, a novel conversational response generation model that enhances\nprevious PLM-based dialogue models. DialogBERT employs a hierarchical\nTransformer architecture. To efficiently capture the discourse-level coherence\namong utterances, we propose two training objectives, including masked\nutterance regression and distributed utterance order ranking in analogy to the\noriginal BERT training. Experiments on three multi-turn conversation datasets\nshow that our approach remarkably outperforms the baselines, such as BART and\nDialoGPT, in terms of quantitative evaluation. The human evaluation suggests\nthat DialogBERT generates more coherent, informative, and human-like responses\nthan the baselines with significant margins.", "published": "2020-12-03 09:06:23", "link": "http://arxiv.org/abs/2012.01775v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Saying No is An Art: Contextualized Fallback Responses for Unanswerable\n  Dialogue Queries", "abstract": "Despite end-to-end neural systems making significant progress in the last\ndecade for task-oriented as well as chit-chat based dialogue systems, most\ndialogue systems rely on hybrid approaches which use a combination of\nrule-based, retrieval and generative approaches for generating a set of ranked\nresponses. Such dialogue systems need to rely on a fallback mechanism to\nrespond to out-of-domain or novel user queries which are not answerable within\nthe scope of the dialog system. While, dialog systems today rely on static and\nunnatural responses like \"I don't know the answer to that question\" or \"I'm not\nsure about that\", we design a neural approach which generates responses which\nare contextually aware with the user query as well as say no to the user. Such\ncustomized responses provide paraphrasing ability and contextualization as well\nas improve the interaction with the user and reduce dialogue monotonicity. Our\nsimple approach makes use of rules over dependency parses and a text-to-text\ntransformer fine-tuned on synthetic data of question-response pairs generating\nhighly relevant, grammatical as well as diverse questions. We perform automatic\nand manual evaluations to demonstrate the efficacy of the system.", "published": "2020-12-03 12:34:22", "link": "http://arxiv.org/abs/2012.01873v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Drugs4Covid: Drug-driven Knowledge Exploitation based on Scientific\n  Publications", "abstract": "In the absence of sufficient medication for COVID patients due to the\nincreased demand, disused drugs have been employed or the doses of those\navailable were modified by hospital pharmacists. Some evidences for the use of\nalternative drugs can be found in the existing scientific literature that could\nassist in such decisions. However, exploiting large corpus of documents in an\nefficient manner is not easy, since drugs may not appear explicitly related in\nthe texts and could be mentioned under different brand names. Drugs4Covid\ncombines word embedding techniques and semantic web technologies to enable a\ndrug-oriented exploration of large medical literature. Drugs and diseases are\nidentified according to the ATC classification and MeSH categories\nrespectively. More than 60K articles and 2M paragraphs have been processed from\nthe CORD-19 corpus with information of COVID-19, SARS, and other related\ncoronaviruses. An open catalogue of drugs has been created and results are\npublicly available through a drug browser, a keyword-guided text explorer, and\na knowledge graph.", "published": "2020-12-03 14:26:54", "link": "http://arxiv.org/abs/2012.01953v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "End to End ASR System with Automatic Punctuation Insertion", "abstract": "Recent Automatic Speech Recognition systems have been moving towards\nend-to-end systems that can be trained together. Numerous techniques that have\nbeen proposed recently enabled this trend, including feature extraction with\nCNNs, context capturing and acoustic feature modeling with RNNs, automatic\nalignment of input and output sequences using Connectionist Temporal\nClassifications, as well as replacing traditional n-gram language models with\nRNN Language Models. Historically, there has been a lot of interest in\nautomatic punctuation in textual or speech to text context. However, there\nseems to be little interest in incorporating automatic punctuation into the\nemerging neural network based end-to-end speech recognition systems, partially\ndue to the lack of English speech corpus with punctuated transcripts. In this\nstudy, we propose a method to generate punctuated transcript for the TEDLIUM\ndataset using transcripts available from ted.com. We also propose an end-to-end\nASR system that outputs words and punctuations concurrently from speech\nsignals. Combining Damerau Levenshtein Distance and slot error rate into\nDLev-SER, we enable measurement of punctuation error rate when the hypothesis\ntext is not perfectly aligned with the reference. Compared with previous\nmethods, our model reduces slot error rate from 0.497 to 0.341.", "published": "2020-12-03 15:46:43", "link": "http://arxiv.org/abs/2012.02012v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A Correspondence Variational Autoencoder for Unsupervised Acoustic Word\n  Embeddings", "abstract": "We propose a new unsupervised model for mapping a variable-duration speech\nsegment to a fixed-dimensional representation. The resulting acoustic word\nembeddings can form the basis of search, discovery, and indexing systems for\nlow- and zero-resource languages. Our model, which we refer to as a maximal\nsampling correspondence variational autoencoder (MCVAE), is a recurrent neural\nnetwork (RNN) trained with a novel self-supervised correspondence loss that\nencourages consistency between embeddings of different instances of the same\nword. Our training scheme improves on previous correspondence training\napproaches through the use and comparison of multiple samples from the\napproximate posterior distribution. In the zero-resource setting, the MCVAE can\nbe trained in an unsupervised way, without any ground-truth word pairs, by\nusing the word-like segments discovered via an unsupervised term discovery\nsystem. In both this setting and a semi-supervised low-resource setting (with a\nlimited set of ground-truth word pairs), the MCVAE outperforms previous\nstate-of-the-art models, such as Siamese-, CAE- and VAE-based RNNs.", "published": "2020-12-03 19:24:42", "link": "http://arxiv.org/abs/2012.02221v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Evolving Character-level Convolutional Neural Networks for Text\n  Classification", "abstract": "Character-level convolutional neural networks (char-CNN) require no knowledge\nof the semantic or syntactic structure of the language they classify. This\nproperty simplifies its implementation but reduces its classification accuracy.\nIncreasing the depth of char-CNN architectures does not result in breakthrough\naccuracy improvements. Research has not established which char-CNN\narchitectures are optimal for text classification tasks. Manually designing and\ntraining char-CNNs is an iterative and time-consuming process that requires\nexpert domain knowledge. Evolutionary deep learning (EDL) techniques, including\nsurrogate-based versions, have demonstrated success in automatically searching\nfor performant CNN architectures for image analysis tasks. Researchers have not\napplied EDL techniques to search the architecture space of char-CNNs for text\nclassification tasks. This article demonstrates the first work in evolving\nchar-CNN architectures using a novel EDL algorithm based on genetic\nprogramming, an indirect encoding and surrogate models, to search for\nperformant char-CNN architectures automatically. The algorithm is evaluated on\neight text classification datasets and benchmarked against five manually\ndesigned CNN architecture and one long short-term memory (LSTM) architecture.\nExperiment results indicate that the algorithm can evolve architectures that\noutperform the LSTM in terms of classification accuracy and five of the\nmanually designed CNN architectures in terms of classification accuracy and\nparameter count.", "published": "2020-12-03 19:27:29", "link": "http://arxiv.org/abs/2012.02223v1", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Evolving Character-Level DenseNet Architectures using Genetic\n  Programming", "abstract": "DenseNet architectures have demonstrated impressive performance in image\nclassification tasks, but limited research has been conducted on using\ncharacter-level DenseNet (char-DenseNet) architectures for text classification\ntasks. It is not clear what DenseNet architectures are optimal for text\nclassification tasks. The iterative task of designing, training and testing of\nchar-DenseNets is an NP-Hard problem that requires expert domain knowledge.\nEvolutionary deep learning (EDL) has been used to automatically design CNN\narchitectures for the image classification domain, thereby mitigating the need\nfor expert domain knowledge. This study demonstrates the first work on using\nEDL to evolve char-DenseNet architectures for text classification tasks. A\nnovel genetic programming-based algorithm (GP-Dense) coupled with an\nindirect-encoding scheme, facilitates the evolution of performant char DenseNet\narchitectures. The algorithm is evaluated on two popular text datasets, and the\nbest-evolved models are benchmarked against four current state-of-the-art\ncharacter-level CNN and DenseNet models. Results indicate that the algorithm\nevolves performant models for both datasets that outperform two of the\nstate-of-the-art models in terms of model accuracy and three of the\nstate-of-the-art models in terms of parameter size.", "published": "2020-12-03 23:28:56", "link": "http://arxiv.org/abs/2012.02327v1", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "GraphPB: Graphical Representations of Prosody Boundary in Speech\n  Synthesis", "abstract": "This paper introduces a graphical representation approach of prosody boundary\n(GraphPB) in the task of Chinese speech synthesis, intending to parse the\nsemantic and syntactic relationship of input sequences in a graphical domain\nfor improving the prosody performance. The nodes of the graph embedding are\nformed by prosodic words, and the edges are formed by the other prosodic\nboundaries, namely prosodic phrase boundary (PPH) and intonation phrase\nboundary (IPH). Different Graph Neural Networks (GNN) like Gated Graph Neural\nNetwork (GGNN) and Graph Long Short-term Memory (G-LSTM) are utilised as graph\nencoders to exploit the graphical prosody boundary information.\nGraph-to-sequence model is proposed and formed by a graph encoder and an\nattentional decoder. Two techniques are proposed to embed sequential\ninformation into the graph-to-sequence text-to-speech model. The experimental\nresults show that this proposed approach can encode the phonetic and prosody\nrhythm of an utterance. The mean opinion score (MOS) of these GNN models shows\ncomparative results with the state-of-the-art sequence-to-sequence models with\nbetter performance in the aspect of prosody. This provides an alternative\napproach for prosody modelling in end-to-end speech synthesis.", "published": "2020-12-03 03:34:05", "link": "http://arxiv.org/abs/2012.02626v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MelGlow: Efficient Waveform Generative Network Based on\n  Location-Variable Convolution", "abstract": "Recent neural vocoders usually use a WaveNet-like network to capture the\nlong-term dependencies of the waveform, but a large number of parameters are\nrequired to obtain good modeling capabilities. In this paper, an efficient\nnetwork, named location-variable convolution, is proposed to model the\ndependencies of waveforms. Different from the use of unified convolution\nkernels in WaveNet to capture the dependencies of arbitrary waveforms,\nlocation-variable convolutions utilizes a kernel predictor to generate multiple\nsets of convolution kernels based on the mel-spectrum, where each set of\nconvolution kernels is used to perform convolution operations on the associated\nwaveform intervals. Combining WaveGlow and location-variable convolutions, an\nefficient vocoder, named MelGlow, is designed. Experiments on the LJSpeech\ndataset show that MelGlow achieves better performance than WaveGlow at small\nmodel sizes, which verifies the effectiveness and potential optimization space\nof location-variable convolutions.", "published": "2020-12-03 03:43:22", "link": "http://arxiv.org/abs/2012.01684v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Phonetic Posteriorgrams based Many-to-Many Singing Voice Conversion via\n  Adversarial Training", "abstract": "This paper describes an end-to-end adversarial singing voice conversion\n(EA-SVC) approach. It can directly generate arbitrary singing waveform by given\nphonetic posteriorgram (PPG) representing content, F0 representing pitch, and\nspeaker embedding representing timbre, respectively. Proposed system is\ncomposed of three modules: generator $G$, the audio generation discriminator\n$D_{A}$, and the feature disentanglement discriminator $D_F$. The generator $G$\nencodes the features in parallel and inversely transforms them into the target\nwaveform. In order to make timbre conversion more stable and controllable,\nspeaker embedding is further decomposed to the weighted sum of a group of\ntrainable vectors representing different timbre clusters. Further, to realize\nmore robust and accurate singing conversion, disentanglement discriminator\n$D_F$ is proposed to remove pitch and timbre related information that remains\nin the encoded PPG. Finally, a two-stage training is conducted to keep a stable\nand effective adversarial training process. Subjective evaluation results\ndemonstrate the effectiveness of our proposed methods. Proposed system\noutperforms conventional cascade approach and the WaveNet based end-to-end\napproach in terms of both singing quality and singer similarity. Further\nobjective analysis reveals that the model trained with the proposed two-stage\ntraining strategy can produce a smoother and sharper formant which leads to\nhigher audio quality.", "published": "2020-12-03 11:13:27", "link": "http://arxiv.org/abs/2012.01837v1", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Text-to-speech for the hearing impaired", "abstract": "Text-to-speech (TTS) systems offer the opportunity to compensate for a\nhearing loss at the source rather than correcting for it at the receiving end.\nThis removes limitations such as time constraints for algorithms that amplify a\nsound in a hearing aid and can lead to higher speech quality. We propose an\nalgorithm that restores loudness to normal perception at a high resolution in\ntime, frequency and level, and embed it in a TTS system that uses Tacotron2 and\nWaveGlow to produce individually amplified speech. Subjective evaluations of\nspeech quality showed that the proposed algorithm led to high-quality audio\nwith sound quality similar to original or linearly amplified speech but\nconsiderably higher speech intelligibility in noise. Transfer learning led to a\nquick adaptation of the produced spectra from original speech to individually\namplified speech, resulted in high speech quality and intelligibility, and thus\ngives us a way to train an individual TTS system efficiently.", "published": "2020-12-03 18:52:03", "link": "http://arxiv.org/abs/2012.02174v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Sonic Sculpture: Activating Engagement with Head-Mounted Augmented\n  Reality", "abstract": "This work examines how head-mounted AR can be used to build an interactive\nsonic landscape to engage with a public sculpture. We describe a sonic artwork,\n\"Listening To Listening\", that has been designed to accompany a real-world\nsculpture with two prototype interaction schemes. Our artwork is created for\nthe HoloLens platform so that users can have an individual experience in a\nmixed reality context. Personal head-mounted AR systems have recently become\navailable and practical for integration into public art projects, however\nresearch into sonic sculpture works has yet to account for the affordances of\ncurrent portable and mainstream AR systems. In this work, we take advantage of\nthe HoloLens' spatial awareness to build sonic spaces that have a precise\nspatial relationship to a given sculpture and where the sculpture itself is\nmodelled in the augmented scene as an \"invisible hologram\". We describe the\nartistic rationale for our artwork, the design of the two interaction schemes,\nand the technical and usability feedback that we have obtained from\ndemonstrations during iterative development.", "published": "2020-12-03 22:35:49", "link": "http://arxiv.org/abs/2012.02311v1", "categories": ["cs.HC", "cs.SD", "eess.AS", "H.5.5; H.5.1"], "primary_category": "cs.HC"}
{"title": "A Laptop Ensemble Performance System using Recurrent Neural Networks", "abstract": "The popularity of applying machine learning techniques in musical domains has\ncreated an inherent availability of freely accessible pre-trained neural\nnetwork (NN) models ready for use in creative applications. This work outlines\nthe implementation of one such application in the form of an assistance tool\ndesigned for live improvisational performances by laptop ensembles. The primary\nintention was to leverage off-the-shelf pre-trained NN models as a basis for\nassisting individual performers either as musical novices looking to engage\nwith more experienced performers or as a tool to expand musical possibilities\nthrough new forms of creative expression. The system expands upon a variety of\nideas found in different research areas including new interfaces for musical\nexpression, generative music and group performance to produce a networked\nperformance solution served via a web-browser interface. The final\nimplementation of the system offers performers a mixture of high and low-level\ncontrols to influence the shape of sequences of notes output by locally run NN\nmodels in real time, also allowing performers to define their level of\nengagement with the assisting generative models. Two test performances were\nplayed, with the system shown to feasibly support four performers over a four\nminute piece while producing musically cohesive and engaging music. Iterations\non the design of the system exposed technical constraints on the use of a\nJavaScript environment for generative models in a live music context, largely\nderived from inescapable processing overheads.", "published": "2020-12-03 23:11:16", "link": "http://arxiv.org/abs/2012.02322v1", "categories": ["cs.HC", "cs.SD", "eess.AS", "H.5.5; H.5.3"], "primary_category": "cs.HC"}
{"title": "Triplet Entropy Loss: Improving The Generalisation of Short Speech\n  Language Identification Systems", "abstract": "We present several methods to improve the generalisation of language\nidentification (LID) systems to new speakers and to new domains. These methods\ninvolve Spectral augmentation, where spectrograms are masked in the frequency\nor time bands during training and CNN architectures that are pre-trained on the\nImagenet dataset. The paper also introduces the novel Triplet Entropy Loss\ntraining method, which involves training a network simultaneously using Cross\nEntropy and Triplet loss. It was found that all three methods improved the\ngeneralisation of the models, though not significantly. Even though the models\ntrained using Triplet Entropy Loss showed a better understanding of the\nlanguages and higher accuracies, it appears as though the models still memorise\nword patterns present in the spectrograms rather than learning the finer\nnuances of a language. The research shows that Triplet Entropy Loss has great\npotential and should be investigated further, not only in language\nidentification tasks but any classification task.", "published": "2020-12-03 08:20:03", "link": "http://arxiv.org/abs/2012.03775v1", "categories": ["cs.SD", "cs.CV", "cs.LG", "eess.AS", "I.2.7"], "primary_category": "cs.SD"}
