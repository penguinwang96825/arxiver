{"title": "Structured Prediction of Sequences and Trees using Infinite Contexts", "abstract": "Linguistic structures exhibit a rich array of global phenomena, however\ncommonly used Markov models are unable to adequately describe these phenomena\ndue to their strong locality assumptions. We propose a novel hierarchical model\nfor structured prediction over sequences and trees which exploits global\ncontext by conditioning each generation decision on an unbounded context of\nprior decisions. This builds on the success of Markov models but without\nimposing a fixed bound in order to better represent global phenomena. To\nfacilitate learning of this large and unbounded model, we use a hierarchical\nPitman-Yor process prior which provides a recursive form of smoothing. We\npropose prediction algorithms based on A* and Markov Chain Monte Carlo\nsampling. Empirical results demonstrate the potential of our model compared to\nbaseline finite-context Markov models on part-of-speech tagging and syntactic\nparsing.", "published": "2015-03-09 10:35:10", "link": "http://arxiv.org/abs/1503.02417v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Context-Dependent Translation Selection Using Convolutional Neural\n  Network", "abstract": "We propose a novel method for translation selection in statistical machine\ntranslation, in which a convolutional neural network is employed to judge the\nsimilarity between a phrase pair in two languages. The specifically designed\nconvolutional architecture encodes not only the semantic similarity of the\ntranslation pair, but also the context containing the phrase in the source\nlanguage. Therefore, our approach is able to capture context-dependent semantic\nsimilarities of translation pairs. We adopt a curriculum learning strategy to\ntrain the model: we classify the training examples into easy, medium, and\ndifficult categories, and gradually build the ability of representing phrase\nand sentence level context by using training examples from easy to difficult.\nExperimental results show that our approach significantly outperforms the\nbaseline system by up to 1.4 BLEU points.", "published": "2015-03-09 02:16:19", "link": "http://arxiv.org/abs/1503.02357v2", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Neural Responding Machine for Short-Text Conversation", "abstract": "We propose Neural Responding Machine (NRM), a neural network-based response\ngenerator for Short-Text Conversation. NRM takes the general encoder-decoder\nframework: it formalizes the generation of response as a decoding process based\non the latent representation of the input text, while both encoding and\ndecoding are realized with recurrent neural networks (RNN). The NRM is trained\nwith a large amount of one-round conversation data collected from a\nmicroblogging service. Empirical study shows that NRM can generate\ngrammatically correct and content-wise appropriate responses to over 75% of the\ninput text, outperforming state-of-the-arts in the same setting, including\nretrieval-based and SMT-based models.", "published": "2015-03-09 02:54:29", "link": "http://arxiv.org/abs/1503.02364v2", "categories": ["cs.CL", "cs.AI", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Syntax-based Deep Matching of Short Texts", "abstract": "Many tasks in natural language processing, ranging from machine translation\nto question answering, can be reduced to the problem of matching two sentences\nor more generally two short texts. We propose a new approach to the problem,\ncalled Deep Match Tree (DeepMatch$_{tree}$), under a general setting. The\napproach consists of two components, 1) a mining algorithm to discover patterns\nfor matching two short-texts, defined in the product space of dependency trees,\nand 2) a deep neural network for matching short texts using the mined patterns,\nas well as a learning algorithm to build the network having a sparse structure.\nWe test our algorithm on the problem of matching a tweet and a response in\nsocial media, a hard matching problem proposed in [Wang et al., 2013], and show\nthat DeepMatch$_{tree}$ can outperform a number of competitor models including\none without using dependency trees and one based on word-embedding, all with\nlarge margins", "published": "2015-03-09 11:11:15", "link": "http://arxiv.org/abs/1503.02427v6", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Compositional Distributional Semantics with Long Short Term Memory", "abstract": "We are proposing an extension of the recursive neural network that makes use\nof a variant of the long short-term memory architecture. The extension allows\ninformation low in parse trees to be stored in a memory register (the `memory\ncell') and used much later higher up in the parse tree. This provides a\nsolution to the vanishing gradient problem and allows the network to capture\nlong range dependencies. Experimental results show that our composition\noutperformed the traditional neural-network composition on the Stanford\nSentiment Treebank.", "published": "2015-03-09 15:13:38", "link": "http://arxiv.org/abs/1503.02510v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
