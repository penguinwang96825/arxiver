{"title": "HEVAL: Yet Another Human Evaluation Metric", "abstract": "Machine translation evaluation is a very important activity in machine\ntranslation development. Automatic evaluation metrics proposed in literature\nare inadequate as they require one or more human reference translations to\ncompare them with output produced by machine translation. This does not always\ngive accurate results as a text can have several different translations. Human\nevaluation metrics, on the other hand, lacks inter-annotator agreement and\nrepeatability. In this paper we have proposed a new human evaluation metric\nwhich addresses these issues. Moreover this metric also provides solid grounds\nfor making sound assumptions on the quality of the text produced by a machine\ntranslation.", "published": "2013-11-15 19:45:25", "link": "http://arxiv.org/abs/1311.3961v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
