{"title": "Multilingual Lexical Simplification via Paraphrase Generation", "abstract": "Lexical simplification (LS) methods based on pretrained language models have\nmade remarkable progress, generating potential substitutes for a complex word\nthrough analysis of its contextual surroundings. However, these methods require\nseparate pretrained models for different languages and disregard the\npreservation of sentence meaning. In this paper, we propose a novel\nmultilingual LS method via paraphrase generation, as paraphrases provide\ndiversity in word selection while preserving the sentence's meaning. We regard\nparaphrasing as a zero-shot translation task within multilingual neural machine\ntranslation that supports hundreds of languages. After feeding the input\nsentence into the encoder of paraphrase modeling, we generate the substitutes\nbased on a novel decoding strategy that concentrates solely on the lexical\nvariations of the complex word. Experimental results demonstrate that our\napproach surpasses BERT-based methods and zero-shot GPT3-based method\nsignificantly on English, Spanish, and Portuguese.", "published": "2023-07-28 03:47:44", "link": "http://arxiv.org/abs/2307.15286v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ChatHome: Development and Evaluation of a Domain-Specific Language Model\n  for Home Renovation", "abstract": "This paper presents the development and evaluation of ChatHome, a\ndomain-specific language model (DSLM) designed for the intricate field of home\nrenovation. Considering the proven competencies of large language models (LLMs)\nlike GPT-4 and the escalating fascination with home renovation, this study\nendeavors to reconcile these aspects by generating a dedicated model that can\nyield high-fidelity, precise outputs relevant to the home renovation arena.\nChatHome's novelty rests on its methodology, fusing domain-adaptive pretraining\nand instruction-tuning over an extensive dataset. This dataset includes\nprofessional articles, standard documents, and web content pertinent to home\nrenovation. This dual-pronged strategy is designed to ensure that our model can\nassimilate comprehensive domain knowledge and effectively address user\ninquiries. Via thorough experimentation on diverse datasets, both universal and\ndomain-specific, including the freshly introduced \"EvalHome\" domain dataset, we\nsubstantiate that ChatHome not only amplifies domain-specific functionalities\nbut also preserves its versatility.", "published": "2023-07-28 04:04:43", "link": "http://arxiv.org/abs/2307.15290v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TrafficSafetyGPT: Tuning a Pre-trained Large Language Model to a\n  Domain-Specific Expert in Transportation Safety", "abstract": "Large Language Models (LLMs) have shown remarkable effectiveness in various\ngeneral-domain natural language processing (NLP) tasks. However, their\nperformance in transportation safety domain tasks has been suboptimal,\nprimarily attributed to the requirement for specialized transportation safety\nexpertise in generating accurate responses [1]. To address this challenge, we\nintroduce TrafficSafetyGPT, a novel LLAMA-based model, which has undergone\nsupervised fine-tuning using TrafficSafety-2K dataset which has human labels\nfrom government produced guiding books and ChatGPT-generated instruction-output\npairs. Our proposed TrafficSafetyGPT model and TrafficSafety-2K train dataset\nare accessible at https://github.com/ozheng1993/TrafficSafetyGPT.", "published": "2023-07-28 05:17:11", "link": "http://arxiv.org/abs/2307.15311v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Teach Me How to Improve My Argumentation Skills: A Survey on Feedback in\n  Argumentation", "abstract": "The use of argumentation in education has been shown to improve critical\nthinking skills for end-users such as students, and computational models for\nargumentation have been developed to assist in this process. Although these\nmodels are useful for evaluating the quality of an argument, they oftentimes\ncannot explain why a particular argument is considered poor or not, which makes\nit difficult to provide constructive feedback to users to strengthen their\ncritical thinking skills. In this survey, we aim to explore the different\ndimensions of feedback (Richness, Visualization, Interactivity, and\nPersonalization) provided by the current computational models for\nargumentation, and the possibility of enhancing the power of explanations of\nsuch models, ultimately helping learners improve their critical thinking\nskills.", "published": "2023-07-28 06:33:09", "link": "http://arxiv.org/abs/2307.15341v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Tourist Assistance using ChatGPT: Comparing Capabilities in\n  Hindi, Telugu, and Kannada", "abstract": "This research investigates the effectiveness of ChatGPT, an AI language model\nby OpenAI, in translating English into Hindi, Telugu, and Kannada languages,\naimed at assisting tourists in India's linguistically diverse environment. To\nmeasure the translation quality, a test set of 50 questions from diverse fields\nsuch as general knowledge, food, and travel was used. These were assessed by\nfive volunteers for accuracy and fluency, and the scores were subsequently\nconverted into a BLEU score. The BLEU score evaluates the closeness of a\nmachine-generated translation to a human translation, with a higher score\nindicating better translation quality. The Hindi translations outperformed\nothers, showcasing superior accuracy and fluency, whereas Telugu translations\nlagged behind. Human evaluators rated both the accuracy and fluency of\ntranslations, offering a comprehensive perspective on the language model's\nperformance.", "published": "2023-07-28 07:52:26", "link": "http://arxiv.org/abs/2307.15376v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards a Fully Unsupervised Framework for Intent Induction in Customer\n  Support Dialogues", "abstract": "State of the art models in intent induction require annotated datasets.\nHowever, annotating dialogues is time-consuming, laborious and expensive. In\nthis work, we propose a completely unsupervised framework for intent induction\nwithin a dialogue. In addition, we show how pre-processing the dialogue corpora\ncan improve results. Finally, we show how to extract the dialogue flows of\nintentions by investigating the most common sequences. Although we test our\nwork in the MultiWOZ dataset, the fact that this framework requires no prior\nknowledge make it applicable to any possible use case, making it very relevant\nto real world customer support applications across industry.", "published": "2023-07-28 09:03:14", "link": "http://arxiv.org/abs/2307.15410v1", "categories": ["cs.CL", "I.2; I.7"], "primary_category": "cs.CL"}
{"title": "Investigating the Learning Behaviour of In-context Learning: A\n  Comparison with Supervised Learning", "abstract": "Large language models (LLMs) have shown remarkable capacity for in-context\nlearning (ICL), where learning a new task from just a few training examples is\ndone without being explicitly pre-trained. However, despite the success of\nLLMs, there has been little understanding of how ICL learns the knowledge from\nthe given prompts. In this paper, to make progress toward understanding the\nlearning behaviour of ICL, we train the same LLMs with the same demonstration\nexamples via ICL and supervised learning (SL), respectively, and investigate\ntheir performance under label perturbations (i.e., noisy labels and label\nimbalance) on a range of classification tasks. First, via extensive\nexperiments, we find that gold labels have significant impacts on the\ndownstream in-context performance, especially for large language models;\nhowever, imbalanced labels matter little to ICL across all model sizes. Second,\nwhen comparing with SL, we show empirically that ICL is less sensitive to label\nperturbations than SL, and ICL gradually attains comparable performance to SL\nas the model size increases.", "published": "2023-07-28 09:03:19", "link": "http://arxiv.org/abs/2307.15411v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CFN-ESA: A Cross-Modal Fusion Network with Emotion-Shift Awareness for\n  Dialogue Emotion Recognition", "abstract": "Multimodal emotion recognition in conversation (ERC) has garnered growing\nattention from research communities in various fields. In this paper, we\npropose a Cross-modal Fusion Network with Emotion-Shift Awareness (CFN-ESA) for\nERC. Extant approaches employ each modality equally without distinguishing the\namount of emotional information in these modalities, rendering it hard to\nadequately extract complementary information from multimodal data. To cope with\nthis problem, in CFN-ESA, we treat textual modality as the primary source of\nemotional information, while visual and acoustic modalities are taken as the\nsecondary sources. Besides, most multimodal ERC models ignore emotion-shift\ninformation and overfocus on contextual information, leading to the failure of\nemotion recognition under emotion-shift scenario. We elaborate an emotion-shift\nmodule to address this challenge. CFN-ESA mainly consists of unimodal encoder\n(RUME), cross-modal encoder (ACME), and emotion-shift module (LESM). RUME is\napplied to extract conversation-level contextual emotional cues while pulling\ntogether data distributions between modalities; ACME is utilized to perform\nmultimodal interaction centered on textual modality; LESM is used to model\nemotion shift and capture emotion-shift information, thereby guiding the\nlearning of the main task. Experimental results demonstrate that CFN-ESA can\neffectively promote performance for ERC and remarkably outperform\nstate-of-the-art models.", "published": "2023-07-28 09:29:42", "link": "http://arxiv.org/abs/2307.15432v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Trie-NLG: Trie Context Augmentation to Improve Personalized Query\n  Auto-Completion for Short and Unseen Prefixes", "abstract": "Query auto-completion (QAC) aims to suggest plausible completions for a given\nquery prefix. Traditionally, QAC systems have leveraged tries curated from\nhistorical query logs to suggest most popular completions. In this context,\nthere are two specific scenarios that are difficult to handle for any QAC\nsystem: short prefixes (which are inherently ambiguous) and unseen prefixes.\nRecently, personalized Natural Language Generation (NLG) models have been\nproposed to leverage previous session queries as context for addressing these\ntwo challenges. However, such NLG models suffer from two drawbacks: (1) some of\nthe previous session queries could be noisy and irrelevant to the user intent\nfor the current prefix, and (2) NLG models cannot directly incorporate\nhistorical query popularity. This motivates us to propose a novel NLG model for\nQAC, Trie-NLG, which jointly leverages popularity signals from trie and\npersonalization signals from previous session queries. We train the Trie-NLG\nmodel by augmenting the prefix with rich context comprising of recent session\nqueries and top trie completions. This simple modeling approach overcomes the\nlimitations of trie-based and NLG-based approaches and leads to\nstate-of-the-art performance. We evaluate the Trie-NLG model using two large\nQAC datasets. On average, our model achieves huge ~57% and ~14% boost in MRR\nover the popular trie-based lookup and the strong BART-based baseline methods,\nrespectively. We make our code publicly available.", "published": "2023-07-28 10:17:30", "link": "http://arxiv.org/abs/2307.15455v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The timing bottleneck: Why timing and overlap are mission-critical for\n  conversational user interfaces, speech recognition and dialogue systems", "abstract": "Speech recognition systems are a key intermediary in voice-driven\nhuman-computer interaction. Although speech recognition works well for pristine\nmonologic audio, real-life use cases in open-ended interactive settings still\npresent many challenges. We argue that timing is mission-critical for dialogue\nsystems, and evaluate 5 major commercial ASR systems for their conversational\nand multilingual support. We find that word error rates for natural\nconversational data in 6 languages remain abysmal, and that overlap remains a\nkey challenge (study 1). This impacts especially the recognition of\nconversational words (study 2), and in turn has dire consequences for\ndownstream intent recognition (study 3). Our findings help to evaluate the\ncurrent state of conversational ASR, contribute towards multidimensional error\nanalysis and evaluation, and identify phenomena that need most attention on the\nway to build robust interactive speech technologies.", "published": "2023-07-28 11:38:05", "link": "http://arxiv.org/abs/2307.15493v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Road to Quality is Paved with Good Revisions: A Detailed Evaluation\n  Methodology for Revision Policies in Incremental Sequence Labelling", "abstract": "Incremental dialogue model components produce a sequence of output prefixes\nbased on incoming input. Mistakes can occur due to local ambiguities or to\nwrong hypotheses, making the ability to revise past outputs a desirable\nproperty that can be governed by a policy. In this work, we formalise and\ncharacterise edits and revisions in incremental sequence labelling and propose\nmetrics to evaluate revision policies. We then apply our methodology to profile\nthe incremental behaviour of three Transformer-based encoders in various tasks,\npaving the road for better revision policies.", "published": "2023-07-28 12:08:15", "link": "http://arxiv.org/abs/2307.15508v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "'What are you referring to?' Evaluating the Ability of Multi-Modal\n  Dialogue Models to Process Clarificational Exchanges", "abstract": "Referential ambiguities arise in dialogue when a referring expression does\nnot uniquely identify the intended referent for the addressee. Addressees\nusually detect such ambiguities immediately and work with the speaker to repair\nit using meta-communicative, Clarificational Exchanges (CE): a Clarification\nRequest (CR) and a response. Here, we argue that the ability to generate and\nrespond to CRs imposes specific constraints on the architecture and objective\nfunctions of multi-modal, visually grounded dialogue models. We use the SIMMC\n2.0 dataset to evaluate the ability of different state-of-the-art model\narchitectures to process CEs, with a metric that probes the contextual updates\nthat arise from them in the model. We find that language-based models are able\nto encode simple multi-modal semantic information and process some CEs,\nexcelling with those related to the dialogue history, whilst multi-modal models\ncan use additional learning objectives to obtain disentangled object\nrepresentations, which become crucial to handle complex referential ambiguities\nacross modalities overall.", "published": "2023-07-28 13:44:33", "link": "http://arxiv.org/abs/2307.15554v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "When to generate hedges in peer-tutoring interactions", "abstract": "This paper explores the application of machine learning techniques to predict\nwhere hedging occurs in peer-tutoring interactions. The study uses a\nnaturalistic face-to-face dataset annotated for natural language turns,\nconversational strategies, tutoring strategies, and nonverbal behaviours. These\nelements are processed into a vector representation of the previous turns,\nwhich serves as input to several machine learning models. Results show that\nembedding layers, that capture the semantic information of the previous turns,\nsignificantly improves the model's performance. Additionally, the study\nprovides insights into the importance of various features, such as\ninterpersonal rapport and nonverbal behaviours, in predicting hedges by using\nShapley values for feature explanation. We discover that the eye gaze of both\nthe tutor and the tutee has a significant impact on hedge prediction. We\nfurther validate this observation through a follow-up ablation study.", "published": "2023-07-28 14:29:19", "link": "http://arxiv.org/abs/2307.15582v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dialogue Shaping: Empowering Agents through NPC Interaction", "abstract": "One major challenge in reinforcement learning (RL) is the large amount of\nsteps for the RL agent needs to converge in the training process and learn the\noptimal policy, especially in text-based game environments where the action\nspace is extensive. However, non-player characters (NPCs) sometimes hold some\nkey information about the game, which can potentially help to train RL agents\nfaster. Thus, this paper explores how to interact and converse with NPC agents\nto get the key information using large language models (LLMs), as well as\nincorporate this information to speed up RL agent's training using knowledge\ngraphs (KGs) and Story Shaping.", "published": "2023-07-28 22:44:54", "link": "http://arxiv.org/abs/2307.15833v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WC-SBERT: Zero-Shot Text Classification via SBERT with Self-Training for\n  Wikipedia Categories", "abstract": "Our research focuses on solving the zero-shot text classification problem in\nNLP, with a particular emphasis on innovative self-training strategies. To\nachieve this objective, we propose a novel self-training strategy that uses\nlabels rather than text for training, significantly reducing the model's\ntraining time. Specifically, we use categories from Wikipedia as our training\nset and leverage the SBERT pre-trained model to establish positive correlations\nbetween pairs of categories within the same text, facilitating associative\ntraining. For new test datasets, we have improved the original self-training\napproach, eliminating the need for prior training and testing data from each\ntarget dataset. Instead, we adopt Wikipedia as a unified training dataset to\nbetter approximate the zero-shot scenario. This modification allows for rapid\nfine-tuning and inference across different datasets, greatly reducing the time\nrequired for self-training. Our experimental results demonstrate that this\nmethod can adapt the model to the target dataset within minutes. Compared to\nother BERT-based transformer models, our approach significantly reduces the\namount of training data by training only on labels, not the actual text, and\ngreatly improves training efficiency by utilizing a unified training set.\nAdditionally, our method achieves state-of-the-art results on both the Yahoo\nTopic and AG News datasets.", "published": "2023-07-28 04:17:41", "link": "http://arxiv.org/abs/2307.15293v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Tutorials on Stance Detection using Pre-trained Language Models:\n  Fine-tuning BERT and Prompting Large Language Models", "abstract": "This paper presents two self-contained tutorials on stance detection in\nTwitter data using BERT fine-tuning and prompting large language models (LLMs).\nThe first tutorial explains BERT architecture and tokenization, guiding users\nthrough training, tuning, and evaluating standard and domain-specific BERT\nmodels with HuggingFace transformers. The second focuses on constructing\nprompts and few-shot examples to elicit stances from ChatGPT and open-source\nFLAN-T5 without fine-tuning. Various prompting strategies are implemented and\nevaluated using confusion matrices and macro F1 scores. The tutorials provide\ncode, visualizations, and insights revealing the strengths of few-shot ChatGPT\nand FLAN-T5 which outperform fine-tuned BERTs. By covering both model\nfine-tuning and prompting-based techniques in an accessible, hands-on manner,\nthese tutorials enable learners to gain applied experience with cutting-edge\nmethods for stance detection.", "published": "2023-07-28 06:15:27", "link": "http://arxiv.org/abs/2307.15331v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "BARTPhoBEiT: Pre-trained Sequence-to-Sequence and Image Transformers\n  Models for Vietnamese Visual Question Answering", "abstract": "Visual Question Answering (VQA) is an intricate and demanding task that\nintegrates natural language processing (NLP) and computer vision (CV),\ncapturing the interest of researchers. The English language, renowned for its\nwealth of resources, has witnessed notable advancements in both datasets and\nmodels designed for VQA. However, there is a lack of models that target\nspecific countries such as Vietnam. To address this limitation, we introduce a\ntransformer-based Vietnamese model named BARTPhoBEiT. This model includes\npre-trained Sequence-to-Sequence and bidirectional encoder representation from\nImage Transformers in Vietnamese and evaluates Vietnamese VQA datasets.\nExperimental results demonstrate that our proposed model outperforms the strong\nbaseline and improves the state-of-the-art in six metrics: Accuracy, Precision,\nRecall, F1-score, WUPS 0.0, and WUPS 0.9.", "published": "2023-07-28 06:23:32", "link": "http://arxiv.org/abs/2307.15335v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Skeleton-of-Thought: Prompting LLMs for Efficient Parallel Generation", "abstract": "This work aims at decreasing the end-to-end generation latency of large\nlanguage models (LLMs). One of the major causes of the high generation latency\nis the sequential decoding approach adopted by almost all state-of-the-art\nLLMs. In this work, motivated by the thinking and writing process of humans, we\npropose Skeleton-of-Thought (SoT), which first guides LLMs to generate the\nskeleton of the answer, and then conducts parallel API calls or batched\ndecoding to complete the contents of each skeleton point in parallel. Not only\ndoes SoT provide considerable speed-ups across 12 LLMs, but it can also\npotentially improve the answer quality on several question categories. SoT is\nan initial attempt at data-centric optimization for inference efficiency, and\nshowcases the potential of eliciting high-quality answers by explicitly\nplanning the answer structure in language.", "published": "2023-07-28 06:31:34", "link": "http://arxiv.org/abs/2307.15337v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Critical Review of Large Language Models: Sensitivity, Bias, and the\n  Path Toward Specialized AI", "abstract": "This paper examines the comparative effectiveness of a specialized compiled\nlanguage model and a general-purpose model like OpenAI's GPT-3.5 in detecting\nSDGs within text data. It presents a critical review of Large Language Models\n(LLMs), addressing challenges related to bias and sensitivity. The necessity of\nspecialized training for precise, unbiased analysis is underlined. A case study\nusing a company descriptions dataset offers insight into the differences\nbetween the GPT-3.5 and the specialized SDG detection model. While GPT-3.5\nboasts broader coverage, it may identify SDGs with limited relevance to the\ncompanies' activities. In contrast, the specialized model zeroes in on highly\npertinent SDGs. The importance of thoughtful model selection is emphasized,\ntaking into account task requirements, cost, complexity, and transparency.\nDespite the versatility of LLMs, the use of specialized models is suggested for\ntasks demanding precision and accuracy. The study concludes by encouraging\nfurther research to find a balance between the capabilities of LLMs and the\nneed for domain-specific expertise and interpretability.", "published": "2023-07-28 09:20:22", "link": "http://arxiv.org/abs/2307.15425v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "From Probabilistic Programming to Complexity-based Programming", "abstract": "The paper presents the main characteristics and a preliminary implementation\nof a novel computational framework named CompLog. Inspired by probabilistic\nprogramming systems like ProbLog, CompLog builds upon the inferential\nmechanisms proposed by Simplicity Theory, relying on the computation of two\nKolmogorov complexities (here implemented as min-path searches via ASP\nprograms) rather than probabilistic inference. The proposed system enables\nusers to compute ex-post and ex-ante measures of unexpectedness of a certain\nsituation, mapping respectively to posterior and prior subjective\nprobabilities. The computation is based on the specification of world and\nmental models by means of causal and descriptive relations between predicates\nweighted by complexity. The paper illustrates a few examples of application:\ngenerating relevant descriptions, and providing alternative approaches to\ndisjunction and to negation.", "published": "2023-07-28 10:11:01", "link": "http://arxiv.org/abs/2307.15453v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Cross-Modal Concept Learning and Inference for Vision-Language Models", "abstract": "Large-scale pre-trained Vision-Language Models (VLMs), such as CLIP,\nestablish the correlation between texts and images, achieving remarkable\nsuccess on various downstream tasks with fine-tuning. In existing fine-tuning\nmethods, the class-specific text description is matched against the whole\nimage. We recognize that this whole image matching is not effective since\nimages from the same class often contain a set of different semantic objects,\nand an object further consists of a set of semantic parts or concepts.\nIndividual semantic parts or concepts may appear in image samples from\ndifferent classes. To address this issue, in this paper, we develop a new\nmethod called cross-model concept learning and inference (CCLI). Using the\npowerful text-image correlation capability of CLIP, our method automatically\nlearns a large set of distinctive visual concepts from images using a set of\nsemantic text concepts. Based on these visual concepts, we construct a\ndiscriminative representation of images and learn a concept inference network\nto perform downstream image classification tasks, such as few-shot learning and\ndomain generalization. Extensive experimental results demonstrate that our CCLI\nmethod is able to improve the performance upon the current state-of-the-art\nmethods by large margins, for example, by up to 8.0% improvement on few-shot\nlearning and by up to 1.3% for domain generalization.", "published": "2023-07-28 10:26:28", "link": "http://arxiv.org/abs/2307.15460v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Exploring Format Consistency for Instruction Tuning", "abstract": "Instruction tuning has emerged as a promising approach to enhancing large\nlanguage models in following human instructions. It is shown that increasing\nthe diversity and number of instructions in the training data can consistently\nenhance generalization performance, which facilitates a recent endeavor to\ncollect various instructions and integrate existing instruction tuning datasets\ninto larger collections. However, different users have their unique ways of\nexpressing instructions, and there often exist variations across different\ndatasets in the instruction styles and formats, i.e., format inconsistency. In\nthis work, we propose a framework named Unified Instruction Tuning (UIT), which\ncalls OpenAI APIs for automatic format transfer among different instruction\ntuning datasets such as PromptSource, FLAN and CrossFit. With the framework, we\n(1) demonstrate the necessity of maintaining format consistency in instruction\ntuning; (2) improve the generalization performance on unseen instructions on\nT5-LM-xl; (3) provide a novel perplexity-based denoising method to reduce the\nnoise of automatic format transfer to make the UIT framework more practical and\na smaller offline model based on GPT-J that achieves comparable format transfer\ncapability to OpenAI APIs to reduce costs in practice. Further analysis\nregarding variations of targeted formats and other effects is intended.", "published": "2023-07-28 12:00:13", "link": "http://arxiv.org/abs/2307.15504v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Context-VQA: Towards Context-Aware and Purposeful Visual Question\n  Answering", "abstract": "Visual question answering (VQA) has the potential to make the Internet more\naccessible in an interactive way, allowing people who cannot see images to ask\nquestions about them. However, multiple studies have shown that people who are\nblind or have low-vision prefer image explanations that incorporate the context\nin which an image appears, yet current VQA datasets focus on images in\nisolation. We argue that VQA models will not fully succeed at meeting people's\nneeds unless they take context into account. To further motivate and analyze\nthe distinction between different contexts, we introduce Context-VQA, a VQA\ndataset that pairs images with contexts, specifically types of websites (e.g.,\na shopping website). We find that the types of questions vary systematically\nacross contexts. For example, images presented in a travel context garner 2\ntimes more \"Where?\" questions, and images on social media and news garner 2.8\nand 1.8 times more \"Who?\" questions than the average. We also find that context\neffects are especially important when participants can't see the image. These\nresults demonstrate that context affects the types of questions asked and that\nVQA models should be context-sensitive to better meet people's needs,\nespecially in accessibility settings.", "published": "2023-07-28 18:01:08", "link": "http://arxiv.org/abs/2307.15745v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Resume Evaluation through Latent Dirichlet Allocation and Natural\n  Language Processing for Effective Candidate Selection", "abstract": "In this paper, we propose a method for resume rating using Latent Dirichlet\nAllocation (LDA) and entity detection with SpaCy. The proposed method first\nextracts relevant entities such as education, experience, and skills from the\nresume using SpaCy's Named Entity Recognition (NER). The LDA model then uses\nthese entities to rate the resume by assigning topic probabilities to each\nentity. Furthermore, we conduct a detailed analysis of the entity detection\nusing SpaCy's NER and report its evaluation metrics. Using LDA, our proposed\nsystem breaks down resumes into latent topics and extracts meaningful semantic\nrepresentations. With a vision to define our resume score to be more\ncontent-driven rather than a structure and keyword match driven, our model has\nachieved 77% accuracy with respect to only skills in consideration and an\noverall 82% accuracy with all attributes in consideration. (like college name,\nwork experience, degree and skills)", "published": "2023-07-28 18:11:17", "link": "http://arxiv.org/abs/2307.15752v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CHATREPORT: Democratizing Sustainability Disclosure Analysis through\n  LLM-based Tools", "abstract": "In the face of climate change, are companies really taking substantial steps\ntoward more sustainable operations? A comprehensive answer lies in the dense,\ninformation-rich landscape of corporate sustainability reports. However, the\nsheer volume and complexity of these reports make human analysis very costly.\nTherefore, only a few entities worldwide have the resources to analyze these\nreports at scale, which leads to a lack of transparency in sustainability\nreporting. Empowering stakeholders with LLM-based automatic analysis tools can\nbe a promising way to democratize sustainability report analysis. However,\ndeveloping such tools is challenging due to (1) the hallucination of LLMs and\n(2) the inefficiency of bringing domain experts into the AI development loop.\nIn this paper, we ChatReport, a novel LLM-based system to automate the analysis\nof corporate sustainability reports, addressing existing challenges by (1)\nmaking the answers traceable to reduce the harm of hallucination and (2)\nactively involving domain experts in the development loop. We make our\nmethodology, annotated datasets, and generated analyses of 1015 reports\npublicly available.", "published": "2023-07-28 18:58:16", "link": "http://arxiv.org/abs/2307.15770v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Select and Augment: Enhanced Dense Retrieval Knowledge Graph\n  Augmentation", "abstract": "Injecting textual information into knowledge graph (KG) entity\nrepresentations has been a worthwhile expedition in terms of improving\nperformance in KG oriented tasks within the NLP community. External knowledge\noften adopted to enhance KG embeddings ranges from semantically rich lexical\ndependency parsed features to a set of relevant key words to entire text\ndescriptions supplied from an external corpus such as wikipedia and many more.\nDespite the gains this innovation (Text-enhanced KG embeddings) has made, the\nproposal in this work suggests that it can be improved even further. Instead of\nusing a single text description (which would not sufficiently represent an\nentity because of the inherent lexical ambiguity of text), we propose a\nmulti-task framework that jointly selects a set of text descriptions relevant\nto KG entities as well as align or augment KG embeddings with text\ndescriptions. Different from prior work that plugs formal entity descriptions\ndeclared in knowledge bases, this framework leverages a retriever model to\nselectively identify richer or highly relevant text descriptions to use in\naugmenting entities. Furthermore, the framework treats the number of\ndescriptions to use in augmentation process as a parameter, which allows the\nflexibility of enumerating across several numbers before identifying an\nappropriate number. Experiment results for Link Prediction demonstrate a 5.5%\nand 3.5% percentage increase in the Mean Reciprocal Rank (MRR) and Hits@10\nscores respectively, in comparison to text-enhanced knowledge graph\naugmentation methods using traditional CNNs.", "published": "2023-07-28 19:33:18", "link": "http://arxiv.org/abs/2307.15776v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Reasoning before Responding: Integrating Commonsense-based Causality\n  Explanation for Empathetic Response Generation", "abstract": "Recent approaches to empathetic response generation try to incorporate\ncommonsense knowledge or reasoning about the causes of emotions to better\nunderstand the user's experiences and feelings. However, these approaches\nmainly focus on understanding the causalities of context from the user's\nperspective, ignoring the system's perspective. In this paper, we propose a\ncommonsense-based causality explanation approach for diverse empathetic\nresponse generation that considers both the user's perspective (user's desires\nand reactions) and the system's perspective (system's intentions and\nreactions). We enhance ChatGPT's ability to reason for the system's perspective\nby integrating in-context learning with commonsense knowledge. Then, we\nintegrate the commonsense-based causality explanation with both ChatGPT and a\nT5-based model. Experimental evaluations demonstrate that our method\noutperforms other comparable methods on both automatic and human evaluations.", "published": "2023-07-28 01:52:16", "link": "http://arxiv.org/abs/2308.00085v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SAP-sLDA: An Interpretable Interface for Exploring Unstructured Text", "abstract": "A common way to explore text corpora is through low-dimensional projections\nof the documents, where one hopes that thematically similar documents will be\nclustered together in the projected space. However, popular algorithms for\ndimensionality reduction of text corpora, like Latent Dirichlet Allocation\n(LDA), often produce projections that do not capture human notions of document\nsimilarity. We propose a semi-supervised human-in-the-loop LDA-based method for\nlearning topics that preserve semantically meaningful relationships between\ndocuments in low-dimensional projections. On synthetic corpora, our method\nyields more interpretable projections than baseline methods with only a\nfraction of labels provided. On a real corpus, we obtain qualitatively similar\nresults.", "published": "2023-07-28 05:43:39", "link": "http://arxiv.org/abs/2308.01420v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Med-HALT: Medical Domain Hallucination Test for Large Language Models", "abstract": "This research paper focuses on the challenges posed by hallucinations in\nlarge language models (LLMs), particularly in the context of the medical\ndomain. Hallucination, wherein these models generate plausible yet unverified\nor incorrect information, can have serious consequences in healthcare\napplications. We propose a new benchmark and dataset, Med-HALT (Medical Domain\nHallucination Test), designed specifically to evaluate and reduce\nhallucinations. Med-HALT provides a diverse multinational dataset derived from\nmedical examinations across various countries and includes multiple innovative\ntesting modalities. Med-HALT includes two categories of tests reasoning and\nmemory-based hallucination tests, designed to assess LLMs's problem-solving and\ninformation retrieval abilities.\n  Our study evaluated leading LLMs, including Text Davinci, GPT-3.5, LlaMa-2,\nMPT, and Falcon, revealing significant differences in their performance. The\npaper provides detailed insights into the dataset, promoting transparency and\nreproducibility. Through this work, we aim to contribute to the development of\nsafer and more reliable language models in healthcare. Our benchmark can be\nfound at medhalt.github.io", "published": "2023-07-28 06:43:04", "link": "http://arxiv.org/abs/2307.15343v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Improving Social Media Popularity Prediction with Multiple Post\n  Dependencies", "abstract": "Social Media Popularity Prediction has drawn a lot of attention because of\nits profound impact on many different applications, such as recommendation\nsystems and multimedia advertising. Despite recent efforts to leverage the\ncontent of social media posts to improve prediction accuracy, many existing\nmodels fail to fully exploit the multiple dependencies between posts, which are\nimportant to comprehensively extract content information from posts. To tackle\nthis problem, we propose a novel prediction framework named Dependency-aware\nSequence Network (DSN) that exploits both intra- and inter-post dependencies.\nFor intra-post dependency, DSN adopts a multimodal feature extractor with an\nefficient fine-tuning strategy to obtain task-specific representations from\nimages and textual information of posts. For inter-post dependency, DSN uses a\nhierarchical information propagation method to learn category representations\nthat could better describe the difference between posts. DSN also exploits\nrecurrent networks with a series of gating layers for more flexible local\ntemporal processing abilities and multi-head attention for long-term\ndependencies. The experimental results on the Social Media Popularity Dataset\ndemonstrate the superiority of our method compared to existing state-of-the-art\nmodels.", "published": "2023-07-28 09:06:50", "link": "http://arxiv.org/abs/2307.15413v1", "categories": ["cs.MM", "cs.AI", "cs.CL"], "primary_category": "cs.MM"}
{"title": "Minimally-Supervised Speech Synthesis with Conditional Diffusion Model\n  and Language Model: A Comparative Study of Semantic Coding", "abstract": "Recently, there has been a growing interest in text-to-speech (TTS) methods\nthat can be trained with minimal supervision by combining two types of discrete\nspeech representations and using two sequence-to-sequence tasks to decouple\nTTS. However, existing methods suffer from three problems: the high\ndimensionality and waveform distortion of discrete speech representations, the\nprosodic averaging problem caused by the duration prediction model in\nnon-autoregressive frameworks, and the information redundancy and dimension\nexplosion problems of existing semantic encoding methods. To address these\nproblems, three progressive methods are proposed. First, we propose\nDiff-LM-Speech, an autoregressive structure consisting of a language model and\ndiffusion models, which models the semantic embedding into the mel-spectrogram\nbased on a diffusion model to achieve higher audio quality. We also introduce a\nprompt encoder structure based on a variational autoencoder and a prosody\nbottleneck to improve prompt representation ability. Second, we propose\nTetra-Diff-Speech, a non-autoregressive structure consisting of four diffusion\nmodel-based modules that design a duration diffusion model to achieve diverse\nprosodic expressions. Finally, we propose Tri-Diff-Speech, a non-autoregressive\nstructure consisting of three diffusion model-based modules that verify the\nnon-necessity of existing semantic encoding models and achieve the best\nresults. Experimental results show that our proposed methods outperform\nbaseline methods. We provide a website with audio samples.", "published": "2023-07-28 11:20:23", "link": "http://arxiv.org/abs/2307.15484v3", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ETHER: Aligning Emergent Communication for Hindsight Experience Replay", "abstract": "Natural language instruction following is paramount to enable collaboration\nbetween artificial agents and human beings. Natural language-conditioned\nreinforcement learning (RL) agents have shown how natural languages'\nproperties, such as compositionality, can provide a strong inductive bias to\nlearn complex policies. Previous architectures like HIGhER combine the benefit\nof language-conditioning with Hindsight Experience Replay (HER) to deal with\nsparse rewards environments. Yet, like HER, HIGhER relies on an oracle\npredicate function to provide a feedback signal highlighting which linguistic\ndescription is valid for which state. This reliance on an oracle limits its\napplication. Additionally, HIGhER only leverages the linguistic information\ncontained in successful RL trajectories, thus hurting its final performance and\ndata-efficiency. Without early successful trajectories, HIGhER is no better\nthan DQN upon which it is built. In this paper, we propose the Emergent Textual\nHindsight Experience Replay (ETHER) agent, which builds on HIGhER and addresses\nboth of its limitations by means of (i) a discriminative visual referential\ngame, commonly studied in the subfield of Emergent Communication (EC), used\nhere as an unsupervised auxiliary task and (ii) a semantic grounding scheme to\nalign the emergent language with the natural language of the\ninstruction-following benchmark. We show that the referential game's agents\nmake an artificial language emerge that is aligned with the natural-like\nlanguage used to describe goals in the BabyAI benchmark and that it is\nexpressive enough so as to also describe unsuccessful RL trajectories and thus\nprovide feedback to the RL agent to leverage the linguistic, structured\ninformation contained in all trajectories. Our work shows that EC is a viable\nunsupervised auxiliary task for RL and provides missing pieces to make HER more\nwidely applicable.", "published": "2023-07-28 11:42:31", "link": "http://arxiv.org/abs/2307.15494v2", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Oracle Computability and Turing Reducibility in the Calculus of\n  Inductive Constructions", "abstract": "We develop synthetic notions of oracle computability and Turing reducibility\nin the Calculus of Inductive Constructions (CIC), the constructive type theory\nunderlying the Coq proof assistant. As usual in synthetic approaches, we employ\na definition of oracle computations based on meta-level functions rather than\nobject-level models of computation, relying on the fact that in constructive\nsystems such as CIC all definable functions are computable by construction.\nSuch an approach lends itself well to machine-checked proofs, which we carry\nout in Coq.\n  There is a tension in finding a good synthetic rendering of the higher-order\nnotion of oracle computability. On the one hand, it has to be informative\nenough to prove central results, ensuring that all notions are faithfully\ncaptured. On the other hand, it has to be restricted enough to benefit from\naxioms for synthetic computability, which usually concern first-order objects.\nDrawing inspiration from a definition by Andrej Bauer based on continuous\nfunctions in the effective topos, we use a notion of sequential continuity to\ncharacterise valid oracle computations.\n  As main technical results, we show that Turing reducibility forms an upper\nsemilattice, transports decidability, and is strictly more expressive than\ntruth-table reducibility, and prove that whenever both a predicate $p$ and its\ncomplement are semi-decidable relative to an oracle $q$, then $p$\nTuring-reduces to $q$.", "published": "2023-07-28 13:16:46", "link": "http://arxiv.org/abs/2307.15543v1", "categories": ["cs.LO", "cs.CL", "math.LO"], "primary_category": "cs.LO"}
{"title": "All-for-One and One-For-All: Deep learning-based feature fusion for\n  Synthetic Speech Detection", "abstract": "Recent advances in deep learning and computer vision have made the synthesis\nand counterfeiting of multimedia content more accessible than ever, leading to\npossible threats and dangers from malicious users. In the audio field, we are\nwitnessing the growth of speech deepfake generation techniques, which solicit\nthe development of synthetic speech detection algorithms to counter possible\nmischievous uses such as frauds or identity thefts. In this paper, we consider\nthree different feature sets proposed in the literature for the synthetic\nspeech detection task and present a model that fuses them, achieving overall\nbetter performances with respect to the state-of-the-art solutions. The system\nwas tested on different scenarios and datasets to prove its robustness to\nanti-forensic attacks and its generalization capabilities.", "published": "2023-07-28 13:50:25", "link": "http://arxiv.org/abs/2307.15555v1", "categories": ["cs.SD", "cs.CL", "cs.CR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Robust Distortion-free Watermarks for Language Models", "abstract": "We propose a methodology for planting watermarks in text from an\nautoregressive language model that are robust to perturbations without changing\nthe distribution over text up to a certain maximum generation budget. We\ngenerate watermarked text by mapping a sequence of random numbers -- which we\ncompute using a randomized watermark key -- to a sample from the language\nmodel. To detect watermarked text, any party who knows the key can align the\ntext to the random number sequence. We instantiate our watermark methodology\nwith two sampling schemes: inverse transform sampling and exponential minimum\nsampling. We apply these watermarks to three language models -- OPT-1.3B,\nLLaMA-7B and Alpaca-7B -- to experimentally validate their statistical power\nand robustness to various paraphrasing attacks. Notably, for both the OPT-1.3B\nand LLaMA-7B models, we find we can reliably detect watermarked text ($p \\leq\n0.01$) from $35$ tokens even after corrupting between $40$-$50\\%$ of the tokens\nvia random edits (i.e., substitutions, insertions or deletions). For the\nAlpaca-7B model, we conduct a case study on the feasibility of watermarking\nresponses to typical user instructions. Due to the lower entropy of the\nresponses, detection is more difficult: around $25\\%$ of the responses -- whose\nmedian length is around $100$ tokens -- are detectable with $p \\leq 0.01$, and\nthe watermark is also less robust to certain automated paraphrasing attacks we\nimplement.", "published": "2023-07-28 14:52:08", "link": "http://arxiv.org/abs/2307.15593v3", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Scaling Data Generation in Vision-and-Language Navigation", "abstract": "Recent research in language-guided visual navigation has demonstrated a\nsignificant demand for the diversity of traversable environments and the\nquantity of supervision for training generalizable agents. To tackle the common\ndata scarcity issue in existing vision-and-language navigation datasets, we\npropose an effective paradigm for generating large-scale data for learning,\nwhich applies 1200+ photo-realistic environments from HM3D and Gibson datasets\nand synthesizes 4.9 million instruction trajectory pairs using fully-accessible\nresources on the web. Importantly, we investigate the influence of each\ncomponent in this paradigm on the agent's performance and study how to\nadequately apply the augmented data to pre-train and fine-tune an agent. Thanks\nto our large-scale dataset, the performance of an existing agent can be pushed\nup (+11% absolute with regard to previous SoTA) to a significantly new best of\n80% single-run success rate on the R2R test split by simple imitation learning.\nThe long-lasting generalization gap between navigating in seen and unseen\nenvironments is also reduced to less than 1% (versus 8% in the previous best\nmethod). Moreover, our paradigm also facilitates different models to achieve\nnew state-of-the-art navigation results on CVDN, REVERIE, and R2R in continuous\nenvironments.", "published": "2023-07-28 16:03:28", "link": "http://arxiv.org/abs/2307.15644v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Uncertainty in Natural Language Generation: From Theory to Applications", "abstract": "Recent advances of powerful Language Models have allowed Natural Language\nGeneration (NLG) to emerge as an important technology that can not only perform\ntraditional tasks like summarisation or translation, but also serve as a\nnatural language interface to a variety of applications. As such, it is crucial\nthat NLG systems are trustworthy and reliable, for example by indicating when\nthey are likely to be wrong; and supporting multiple views, backgrounds and\nwriting styles -- reflecting diverse human sub-populations. In this paper, we\nargue that a principled treatment of uncertainty can assist in creating systems\nand evaluation protocols better aligned with these goals. We first present the\nfundamental theory, frameworks and vocabulary required to represent\nuncertainty. We then characterise the main sources of uncertainty in NLG from a\nlinguistic perspective, and propose a two-dimensional taxonomy that is more\ninformative and faithful than the popular aleatoric/epistemic dichotomy.\nFinally, we move from theory to applications and highlight exciting research\ndirections that exploit uncertainty to power decoding, controllable generation,\nself-assessment, selective answering, active learning and more.", "published": "2023-07-28 17:51:21", "link": "http://arxiv.org/abs/2307.15703v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Lessons in Reproducibility: Insights from NLP Studies in Materials\n  Science", "abstract": "Natural Language Processing (NLP), a cornerstone field within artificial\nintelligence, has been increasingly utilized in the field of materials science\nliterature. Our study conducts a reproducibility analysis of two pioneering\nworks within this domain: \"Machine-learned and codified synthesis parameters of\noxide materials\" by Kim et al., and \"Unsupervised word embeddings capture\nlatent knowledge from materials science literature\" by Tshitoyan et al. We aim\nto comprehend these studies from a reproducibility perspective, acknowledging\ntheir significant influence on the field of materials informatics, rather than\ncritiquing them. Our study indicates that both papers offered thorough\nworkflows, tidy and well-documented codebases, and clear guidance for model\nevaluation. This makes it easier to replicate their results successfully and\npartially reproduce their findings. In doing so, they set commendable standards\nfor future materials science publications to aspire to. However, our analysis\nalso highlights areas for improvement such as to provide access to training\ndata where copyright restrictions permit, more transparency on model\narchitecture and the training process, and specifications of software\ndependency versions. We also cross-compare the word embedding models between\npapers, and find that some key differences in reproducibility and\ncross-compatibility are attributable to design choices outside the bounds of\nthe models themselves. In summary, our study appreciates the benchmark set by\nthese seminal papers while advocating for further enhancements in research\nreproducibility practices in the field of NLP for materials science. This\nbalance of understanding and continuous improvement will ultimately propel the\nintersecting domains of NLP and materials science literature into a future of\nexciting discoveries.", "published": "2023-07-28 18:36:42", "link": "http://arxiv.org/abs/2307.15759v1", "categories": ["physics.chem-ph", "cs.AI", "cs.CL"], "primary_category": "physics.chem-ph"}
{"title": "The Hydra Effect: Emergent Self-repair in Language Model Computations", "abstract": "We investigate the internal structure of language model computations using\ncausal analysis and demonstrate two motifs: (1) a form of adaptive computation\nwhere ablations of one attention layer of a language model cause another layer\nto compensate (which we term the Hydra effect) and (2) a counterbalancing\nfunction of late MLP layers that act to downregulate the maximum-likelihood\ntoken. Our ablation studies demonstrate that language model layers are\ntypically relatively loosely coupled (ablations to one layer only affect a\nsmall number of downstream layers). Surprisingly, these effects occur even in\nlanguage models trained without any form of dropout. We analyse these effects\nin the context of factual recall and consider their implications for\ncircuit-level attribution in language models.", "published": "2023-07-28 19:13:26", "link": "http://arxiv.org/abs/2307.15771v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic\n  Control", "abstract": "We study how vision-language models trained on Internet-scale data can be\nincorporated directly into end-to-end robotic control to boost generalization\nand enable emergent semantic reasoning. Our goal is to enable a single\nend-to-end trained model to both learn to map robot observations to actions and\nenjoy the benefits of large-scale pretraining on language and vision-language\ndata from the web. To this end, we propose to co-fine-tune state-of-the-art\nvision-language models on both robotic trajectory data and Internet-scale\nvision-language tasks, such as visual question answering. In contrast to other\napproaches, we propose a simple, general recipe to achieve this goal: in order\nto fit both natural language responses and robotic actions into the same\nformat, we express the actions as text tokens and incorporate them directly\ninto the training set of the model in the same way as natural language tokens.\nWe refer to such category of models as vision-language-action models (VLA) and\ninstantiate an example of such a model, which we call RT-2. Our extensive\nevaluation (6k evaluation trials) shows that our approach leads to performant\nrobotic policies and enables RT-2 to obtain a range of emergent capabilities\nfrom Internet-scale training. This includes significantly improved\ngeneralization to novel objects, the ability to interpret commands not present\nin the robot training data (such as placing an object onto a particular number\nor icon), and the ability to perform rudimentary reasoning in response to user\ncommands (such as picking up the smallest or largest object, or the one closest\nto another object). We further show that incorporating chain of thought\nreasoning allows RT-2 to perform multi-stage semantic reasoning, for example\nfiguring out which object to pick up for use as an improvised hammer (a rock),\nor which type of drink is best suited for someone who is tired (an energy\ndrink).", "published": "2023-07-28 21:18:02", "link": "http://arxiv.org/abs/2307.15818v1", "categories": ["cs.RO", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Testing the Depth of ChatGPT's Comprehension via Cross-Modal Tasks Based\n  on ASCII-Art: GPT3.5's Abilities in Regard to Recognizing and Generating\n  ASCII-Art Are Not Totally Lacking", "abstract": "Over the eight months since its release, ChatGPT and its underlying model,\nGPT3.5, have garnered massive attention, due to their potent mix of capability\nand accessibility. While a niche-industry of papers have emerged examining the\nscope of capabilities these models possess, the information fed to and\nextracted from these networks has been either natural language text or\nstylized, code-like language. Drawing inspiration from the prowess we expect a\ntruly human-level intelligent agent to have across multiple signal modalities,\nin this work we examine GPT3.5's aptitude for visual tasks, where the inputs\nfeature content provided as ASCII-art without overt distillation into a lingual\nsummary. We conduct experiments analyzing the model's performance on image\nrecognition tasks after various transforms typical in visual settings, trials\ninvestigating knowledge of image parts, and tasks covering image generation.", "published": "2023-07-28 10:45:14", "link": "http://arxiv.org/abs/2307.16806v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Overview Of Temporal Commonsense Reasoning and Acquisition", "abstract": "Temporal commonsense reasoning refers to the ability to understand the\ntypical temporal context of phrases, actions, and events, and use it to reason\nover problems requiring such knowledge. This trait is essential in temporal\nnatural language processing tasks, with possible applications such as timeline\nsummarization, temporal question answering, and temporal natural language\ninference. Recent research on the performance of large language models suggests\nthat, although they are adept at generating syntactically correct sentences and\nsolving classification tasks, they often take shortcuts in their reasoning and\nfall prey to simple linguistic traps. This article provides an overview of\nresearch in the domain of temporal commonsense reasoning, particularly focusing\non enhancing language model performance through a variety of augmentations and\ntheir evaluation across a growing number of datasets. However, these augmented\nmodels still struggle to approach human performance on reasoning tasks over\ntemporal common sense properties, such as the typical occurrence times,\norderings, or durations of events. We further emphasize the need for careful\ninterpretation of research to guard against overpromising evaluation results in\nlight of the shallow reasoning present in transformers. This can be achieved by\nappropriately preparing datasets and suitable evaluation metrics.", "published": "2023-07-28 01:30:15", "link": "http://arxiv.org/abs/2308.00002v3", "categories": ["cs.AI", "cs.CL", "cs.LG", "68T50", "I.2.7"], "primary_category": "cs.AI"}
{"title": "A Time-Frequency Generative Adversarial based method for Audio Packet\n  Loss Concealment", "abstract": "Packet loss is a major cause of voice quality degradation in VoIP\ntransmissions with serious impact on intelligibility and user experience. This\npaper describes a system based on a generative adversarial approach, which aims\nto repair the lost fragments during the transmission of audio streams. Inspired\nby the powerful image-to-image translation capability of Generative Adversarial\nNetworks (GANs), we propose bin2bin, an improved pix2pix framework to achieve\nthe translation task from magnitude spectrograms of audio frames with lost\npackets, to noncorrupted speech spectrograms. In order to better maintain the\nstructural information after spectrogram translation, this paper introduces the\ncombination of two STFT-based loss functions, mixed with the traditional GAN\nobjective. Furthermore, we employ a modified PatchGAN structure as\ndiscriminator and we lower the concealment time by a proper initialization of\nthe phase reconstruction algorithm. Experimental results show that the proposed\nmethod has obvious advantages when compared with the current state-of-the-art\nmethods, as it can better handle both high packet loss rates and large gaps.", "published": "2023-07-28 15:13:59", "link": "http://arxiv.org/abs/2307.15611v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Efficient Acoustic Echo Suppression with Condition-Aware Training", "abstract": "The topic of deep acoustic echo control (DAEC) has seen many approaches with\nvarious model topologies in recent years. Convolutional recurrent networks\n(CRNs), consisting of a convolutional encoder and decoder encompassing a\nrecurrent bottleneck, are repeatedly employed due to their ability to preserve\nnearend speech even in double-talk (DT) condition. However, past architectures\nare either computationally complex or trade off smaller model sizes with a\ndecrease in performance. We propose an improved CRN topology which, compared to\nother realizations of this class of architectures, not only saves parameters\nand computational complexity, but also shows improved performance in DT,\noutperforming both baseline architectures FCRN and CRUSE. Striving for a\ncondition-aware training, we also demonstrate the importance of a high\nproportion of double-talk and the missing value of nearend-only speech in DAEC\ntraining data. Finally, we show how to control the trade-off between aggressive\necho suppression and near-end speech preservation by fine-tuning with\ncondition-aware component loss functions.", "published": "2023-07-28 15:40:28", "link": "http://arxiv.org/abs/2307.15630v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "PCNN: A Lightweight Parallel Conformer Neural Network for Efficient\n  Monaural Speech Enhancement", "abstract": "Convolutional neural networks (CNN) and Transformer have wildly succeeded in\nmultimedia applications. However, more effort needs to be made to harmonize\nthese two architectures effectively to satisfy speech enhancement. This paper\naims to unify these two architectures and presents a Parallel Conformer for\nspeech enhancement. In particular, the CNN and the self-attention (SA) in the\nTransformer are fully exploited for local format patterns and global structure\nrepresentations. Based on the small receptive field size of CNN and the high\ncomputational complexity of SA, we specially designed a multi-branch dilated\nconvolution (MBDC) and a self-channel-time-frequency attention (Self-CTFA)\nmodule. MBDC contains three convolutional layers with different dilation rates\nfor the feature from local to non-local processing. Experimental results show\nthat our method performs better than state-of-the-art methods in most\nevaluation criteria while maintaining the lowest model parameters.", "published": "2023-07-28 01:34:20", "link": "http://arxiv.org/abs/2307.15251v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improving Audio-Text Retrieval via Hierarchical Cross-Modal Interaction\n  and Auxiliary Captions", "abstract": "Most existing audio-text retrieval (ATR) methods focus on constructing\ncontrastive pairs between whole audio clips and complete caption sentences,\nwhile ignoring fine-grained cross-modal relationships, e.g., short segments and\nphrases or frames and words. In this paper, we introduce a hierarchical\ncross-modal interaction (HCI) method for ATR by simultaneously exploring\nclip-sentence, segment-phrase, and frame-word relationships, achieving a\ncomprehensive multi-modal semantic comparison. Besides, we also present a novel\nATR framework that leverages auxiliary captions (AC) generated by a pretrained\ncaptioner to perform feature interaction between audio and generated captions,\nwhich yields enhanced audio representations and is complementary to the\noriginal ATR matching branch. The audio and generated captions can also form\nnew audio-text pairs as data augmentation for training. Experiments show that\nour HCI significantly improves the ATR performance. Moreover, our AC framework\nalso shows stable performance gains on multiple datasets.", "published": "2023-07-28 06:46:30", "link": "http://arxiv.org/abs/2307.15344v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The FlySpeech Audio-Visual Speaker Diarization System for MISP Challenge\n  2022", "abstract": "This paper describes the FlySpeech speaker diarization system submitted to\nthe second \\textbf{M}ultimodal \\textbf{I}nformation Based \\textbf{S}peech\n\\textbf{P}rocessing~(\\textbf{MISP}) Challenge held in ICASSP 2022. We develop\nan end-to-end audio-visual speaker diarization~(AVSD) system, which consists of\na lip encoder, a speaker encoder, and an audio-visual decoder. Specifically, to\nmitigate the degradation of diarization performance caused by separate\ntraining, we jointly train the speaker encoder and the audio-visual decoder. In\naddition, we leverage the large-data pretrained speaker extractor to initialize\nthe speaker encoder.", "published": "2023-07-28 08:50:29", "link": "http://arxiv.org/abs/2307.15400v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Automated approach for source location in shallow waters", "abstract": "This paper proposes a fully automated method for recovering the location of a\nsource and medium parameters in shallow waters. The scenario involves an\nunknown source emitting low-frequency sound waves in a shallow water\nenvironment, and a single hydrophone recording the signal. Firstly, theoretical\ntools are introduced to understand the robustness of the warping method and to\npropose and analyze an automated way to separate the modal components of the\nrecorded signal. Secondly, using the spectrogram of each modal component, the\npaper investigates the best way to recover the modal travel times and provides\nstability estimates. Finally, a penalized minimization algorithm is presented\nto recover estimates of the source location and medium parameters. The proposed\nmethod is tested on experimental data of right whale gunshot and combustive\nsound sources, demonstrating its effectiveness in real-world scenarios.", "published": "2023-07-28 11:37:33", "link": "http://arxiv.org/abs/2307.15491v1", "categories": ["cs.SD", "eess.AS", "physics.class-ph"], "primary_category": "cs.SD"}
