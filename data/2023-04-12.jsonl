{"title": "Global Prompt Cell: A Portable Control Module for Effective Prompt\n  Tuning", "abstract": "As a novel approach to tuning pre-trained models, prompt tuning involves\nfreezing the parameters in downstream tasks while inserting trainable\nembeddings into inputs in the first layer. However, previous methods have\nmainly focused on the initialization of prompt embeddings. The strategy of\ntraining and utilizing prompt embeddings in a reasonable way has become a\nlimiting factor in the effectiveness of prompt tuning. To address this issue,\nwe introduce the Global Prompt Cell (GPC), a portable control module for prompt\ntuning that selectively preserves prompt information across all encoder layers.\nOur experimental results demonstrate a 5.8% improvement on SuperGLUE datasets\ncompared to vanilla prompt tuning.", "published": "2023-04-12 06:46:33", "link": "http://arxiv.org/abs/2304.05642v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Measuring Normative and Descriptive Biases in Language Models Using\n  Census Data", "abstract": "We investigate in this paper how distributions of occupations with respect to\ngender is reflected in pre-trained language models. Such distributions are not\nalways aligned to normative ideals, nor do they necessarily reflect a\ndescriptive assessment of reality. In this paper, we introduce an approach for\nmeasuring to what degree pre-trained language models are aligned to normative\nand descriptive occupational distributions. To this end, we use official\ndemographic information about gender--occupation distributions provided by the\nnational statistics agencies of France, Norway, United Kingdom, and the United\nStates. We manually generate template-based sentences combining gendered\npronouns and nouns with occupations, and subsequently probe a selection of ten\nlanguage models covering the English, French, and Norwegian languages. The\nscoring system we introduce in this work is language independent, and can be\nused on any combination of template-based sentences, occupations, and\nlanguages. The approach could also be extended to other dimensions of national\ncensus data and other demographic variables.", "published": "2023-04-12 11:06:14", "link": "http://arxiv.org/abs/2304.05764v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Measuring Gender Bias in West Slavic Language Models", "abstract": "Pre-trained language models have been known to perpetuate biases from the\nunderlying datasets to downstream tasks. However, these findings are\npredominantly based on monolingual language models for English, whereas there\nare few investigative studies of biases encoded in language models for\nlanguages beyond English. In this paper, we fill this gap by analysing gender\nbias in West Slavic language models. We introduce the first template-based\ndataset in Czech, Polish, and Slovak for measuring gender bias towards male,\nfemale and non-binary subjects. We complete the sentences using both mono- and\nmultilingual language models and assess their suitability for the masked\nlanguage modelling objective. Next, we measure gender bias encoded in West\nSlavic language models by quantifying the toxicity and genderness of the\ngenerated words. We find that these language models produce hurtful completions\nthat depend on the subject's gender. Perhaps surprisingly, Czech, Slovak, and\nPolish language models produce more hurtful completions with men as subjects,\nwhich, upon inspection, we find is due to completions being related to\nviolence, death, and sickness.", "published": "2023-04-12 11:49:43", "link": "http://arxiv.org/abs/2304.05783v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rethinking Dense Retrieval's Few-Shot Ability", "abstract": "Few-shot dense retrieval (DR) aims to effectively generalize to novel search\nscenarios by learning a few samples. Despite its importance, there is little\nstudy on specialized datasets and standardized evaluation protocols. As a\nresult, current methods often resort to random sampling from supervised\ndatasets to create \"few-data\" setups and employ inconsistent training\nstrategies during evaluations, which poses a challenge in accurately comparing\nrecent progress. In this paper, we propose a customized FewDR dataset and a\nunified evaluation benchmark. Specifically, FewDR employs class-wise sampling\nto establish a standardized \"few-shot\" setting with finely-defined classes,\nreducing variability in multiple sampling rounds. Moreover, the dataset is\ndisjointed into base and novel classes, allowing DR models to be continuously\ntrained on ample data from base classes and a few samples in novel classes.\nThis benchmark eliminates the risk of novel class leakage, providing a reliable\nestimation of the DR model's few-shot ability. Our extensive empirical results\nreveal that current state-of-the-art DR models still face challenges in the\nstandard few-shot scene. Our code and data will be open-sourced at\nhttps://github.com/OpenMatch/ANCE-Tele.", "published": "2023-04-12 13:20:16", "link": "http://arxiv.org/abs/2304.05845v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detection of Fake Generated Scientific Abstracts", "abstract": "The widespread adoption of Large Language Models and publicly available\nChatGPT has marked a significant turning point in the integration of Artificial\nIntelligence into people's everyday lives. The academic community has taken\nnotice of these technological advancements and has expressed concerns regarding\nthe difficulty of discriminating between what is real and what is artificially\ngenerated. Thus, researchers have been working on developing effective systems\nto identify machine-generated text. In this study, we utilize the GPT-3 model\nto generate scientific paper abstracts through Artificial Intelligence and\nexplore various text representation methods when combined with Machine Learning\nmodels with the aim of identifying machine-written text. We analyze the models'\nperformance and address several research questions that rise during the\nanalysis of the results. By conducting this research, we shed light on the\ncapabilities and limitations of Artificial Intelligence generated text.", "published": "2023-04-12 20:20:22", "link": "http://arxiv.org/abs/2304.06148v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large\n  Language Models in Multilingual Learning", "abstract": "Over the last few years, large language models (LLMs) have emerged as the\nmost important breakthroughs in natural language processing (NLP) that\nfundamentally transform research and developments in the field. ChatGPT\nrepresents one of the most exciting LLM systems developed recently to showcase\nimpressive skills for language generation and highly attract public attention.\nAmong various exciting applications discovered for ChatGPT in English, the\nmodel can process and generate texts for multiple languages due to its\nmultilingual training data. Given the broad adoption of ChatGPT for English in\ndifferent problems and areas, a natural question is whether ChatGPT can also be\napplied effectively for other languages or it is necessary to develop more\nlanguage-specific technologies. The answer to this question requires a thorough\nevaluation of ChatGPT over multiple tasks with diverse languages and large\ndatasets (i.e., beyond reported anecdotes), which is still missing or limited\nin current research. Our work aims to fill this gap for the evaluation of\nChatGPT and similar LLMs to provide more comprehensive information for\nmultilingual NLP applications. While this work will be an ongoing effort to\ninclude additional experiments in the future, our current paper evaluates\nChatGPT on 7 different tasks, covering 37 diverse languages with high, medium,\nlow, and extremely low resources. We also focus on the zero-shot learning\nsetting for ChatGPT to improve reproducibility and better simulate the\ninteractions of general users. Compared to the performance of previous models,\nour extensive experimental results demonstrate a worse performance of ChatGPT\nfor different NLP tasks and languages, calling for further research to develop\nbetter models and understanding for multilingual learning.", "published": "2023-04-12 05:08:52", "link": "http://arxiv.org/abs/2304.05613v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning Homographic Disambiguation Representation for Neural Machine\n  Translation", "abstract": "Homographs, words with the same spelling but different meanings, remain\nchallenging in Neural Machine Translation (NMT). While recent works leverage\nvarious word embedding approaches to differentiate word sense in NMT, they do\nnot focus on the pivotal components in resolving ambiguities of homographs in\nNMT: the hidden states of an encoder. In this paper, we propose a novel\napproach to tackle homographic issues of NMT in the latent space. We first\ntrain an encoder (aka \"HDR-encoder\") to learn universal sentence\nrepresentations in a natural language inference (NLI) task. We further\nfine-tune the encoder using homograph-based synset sentences from WordNet,\nenabling it to learn word-level homographic disambiguation representations\n(HDR). The pre-trained HDR-encoder is subsequently integrated with a\ntransformer-based NMT in various schemes to improve translation accuracy.\nExperiments on four translation directions demonstrate the effectiveness of the\nproposed method in enhancing the performance of NMT systems in the BLEU scores\n(up to +2.3 compared to a solid baseline). The effects can be verified by other\nmetrics (F1, precision, and recall) of translation accuracy in an additional\ndisambiguation task. Visualization methods like heatmaps, T-SNE and translation\nexamples are also utilized to demonstrate the effects of the proposed method.", "published": "2023-04-12 13:42:59", "link": "http://arxiv.org/abs/2304.05860v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ReDWINE: A Clinical Datamart with Text Analytical Capabilities to\n  Facilitate Rehabilitation Research", "abstract": "Rehabilitation research focuses on determining the components of a treatment\nintervention, the mechanism of how these components lead to recovery and\nrehabilitation, and ultimately the optimal intervention strategies to maximize\npatients' physical, psychologic, and social functioning. Traditional randomized\nclinical trials that study and establish new interventions face several\nchallenges, such as high cost and time commitment. Observational studies that\nuse existing clinical data to observe the effect of an intervention have shown\nseveral advantages over RCTs. Electronic Health Records (EHRs) have become an\nincreasingly important resource for conducting observational studies. To\nsupport these studies, we developed a clinical research datamart, called\nReDWINE (Rehabilitation Datamart With Informatics iNfrastructure for rEsearch),\nthat transforms the rehabilitation-related EHR data collected from the UPMC\nhealth care system to the Observational Health Data Sciences and Informatics\n(OHDSI) Observational Medical Outcomes Partnership (OMOP) Common Data Model\n(CDM) to facilitate rehabilitation research. The standardized EHR data stored\nin ReDWINE will further reduce the time and effort required by investigators to\npool, harmonize, clean, and analyze data from multiple sources, leading to more\nrobust and comprehensive research findings. ReDWINE also includes deployment of\ndata visualization and data analytics tools to facilitate cohort definition and\nclinical data analysis. These include among others the Open Health Natural\nLanguage Processing (OHNLP) toolkit, a high-throughput NLP pipeline, to provide\ntext analytical capabilities at scale in ReDWINE. Using this comprehensive\nrepresentation of patient data in ReDWINE for rehabilitation research will\nfacilitate real-world evidence for health interventions and outcomes.", "published": "2023-04-12 15:49:41", "link": "http://arxiv.org/abs/2304.05929v1", "categories": ["cs.CL", "cs.DB"], "primary_category": "cs.CL"}
{"title": "ASL Citizen: A Community-Sourced Dataset for Advancing Isolated Sign\n  Language Recognition", "abstract": "Sign languages are used as a primary language by approximately 70 million\nD/deaf people world-wide. However, most communication technologies operate in\nspoken and written languages, creating inequities in access. To help tackle\nthis problem, we release ASL Citizen, the first crowdsourced Isolated Sign\nLanguage Recognition (ISLR) dataset, collected with consent and containing\n83,399 videos for 2,731 distinct signs filmed by 52 signers in a variety of\nenvironments. We propose that this dataset be used for sign language dictionary\nretrieval for American Sign Language (ASL), where a user demonstrates a sign to\ntheir webcam to retrieve matching signs from a dictionary. We show that\ntraining supervised machine learning classifiers with our dataset advances the\nstate-of-the-art on metrics relevant for dictionary retrieval, achieving 63%\naccuracy and a recall-at-10 of 91%, evaluated entirely on videos of users who\nare not present in the training or validation sets. An accessible PDF of this\narticle is available at the following link:\nhttps://aashakadesai.github.io/research/ASLCitizen_arxiv_updated.pdf", "published": "2023-04-12 15:52:53", "link": "http://arxiv.org/abs/2304.05934v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Boosted Prompt Ensembles for Large Language Models", "abstract": "Methods such as chain-of-thought prompting and self-consistency have pushed\nthe frontier of language model reasoning performance with no additional\ntraining. To further improve performance, we propose a prompt ensembling method\nfor large language models, which uses a small dataset to construct a set of few\nshot prompts that together comprise a ``boosted prompt ensemble''. The few shot\nexamples for each prompt are chosen in a stepwise fashion to be ``hard''\nexamples on which the previous step's ensemble is uncertain. We show that this\noutperforms single-prompt output-space ensembles and bagged prompt-space\nensembles on the GSM8k and AQuA datasets, among others. We propose both\ntrain-time and test-time versions of boosted prompting that use different\nlevels of available annotation and conduct a detailed empirical study of our\nalgorithm.", "published": "2023-04-12 16:47:15", "link": "http://arxiv.org/abs/2304.05970v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "HiPrompt: Few-Shot Biomedical Knowledge Fusion via Hierarchy-Oriented\n  Prompting", "abstract": "Medical decision-making processes can be enhanced by comprehensive biomedical\nknowledge bases, which require fusing knowledge graphs constructed from\ndifferent sources via a uniform index system. The index system often organizes\nbiomedical terms in a hierarchy to provide the aligned entities with\nfine-grained granularity. To address the challenge of scarce supervision in the\nbiomedical knowledge fusion (BKF) task, researchers have proposed various\nunsupervised methods. However, these methods heavily rely on ad-hoc lexical and\nstructural matching algorithms, which fail to capture the rich semantics\nconveyed by biomedical entities and terms. Recently, neural embedding models\nhave proved effective in semantic-rich tasks, but they rely on sufficient\nlabeled data to be adequately trained. To bridge the gap between the\nscarce-labeled BKF and neural embedding models, we propose HiPrompt, a\nsupervision-efficient knowledge fusion framework that elicits the few-shot\nreasoning ability of large language models through hierarchy-oriented prompts.\nEmpirical results on the collected KG-Hi-BKF benchmark datasets demonstrate the\neffectiveness of HiPrompt.", "published": "2023-04-12 16:54:26", "link": "http://arxiv.org/abs/2304.05973v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "LINGO : Visually Debiasing Natural Language Instructions to Support Task\n  Diversity", "abstract": "Cross-task generalization is a significant outcome that defines mastery in\nnatural language understanding. Humans show a remarkable aptitude for this, and\ncan solve many different types of tasks, given definitions in the form of\ntextual instructions and a small set of examples. Recent work with pre-trained\nlanguage models mimics this learning style: users can define and exemplify a\ntask for the model to attempt as a series of natural language prompts or\ninstructions. While prompting approaches have led to higher cross-task\ngeneralization compared to traditional supervised learning, analyzing 'bias' in\nthe task instructions given to the model is a difficult problem, and has thus\nbeen relatively unexplored. For instance, are we truly modeling a task, or are\nwe modeling a user's instructions? To help investigate this, we develop LINGO,\na novel visual analytics interface that supports an effective, task-driven\nworkflow to (1) help identify bias in natural language task instructions, (2)\nalter (or create) task instructions to reduce bias, and (3) evaluate\npre-trained model performance on debiased task instructions. To robustly\nevaluate LINGO, we conduct a user study with both novice and expert instruction\ncreators, over a dataset of 1,616 linguistic tasks and their natural language\ninstructions, spanning 55 different languages. For both user groups, LINGO\npromotes the creation of more difficult tasks for pre-trained models, that\ncontain higher linguistic diversity and lower instruction bias. We additionally\ndiscuss how the insights learned in developing and evaluating LINGO can aid in\nthe design of future dashboards that aim to minimize the effort involved in\nprompt creation across multiple domains.", "published": "2023-04-12 22:55:52", "link": "http://arxiv.org/abs/2304.06184v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Using Large Language Models for (De-)Formalization and Natural\n  Argumentation Exercises for Beginner's Students", "abstract": "We describe two systems currently being developed that use large language\nmodels for the automatized correction of (i) exercises in translating back and\nforth between natural language and the languages of propositional logic and\nfirst-order predicate logic and (ii) exercises in writing simple arguments in\nnatural language in non-mathematical scenarios.", "published": "2023-04-12 23:05:02", "link": "http://arxiv.org/abs/2304.06186v3", "categories": ["cs.CL", "math.LO"], "primary_category": "cs.CL"}
{"title": "Can Large Language Models Transform Computational Social Science?", "abstract": "Large Language Models (LLMs) are capable of successfully performing many\nlanguage processing tasks zero-shot (without training data). If zero-shot LLMs\ncan also reliably classify and explain social phenomena like persuasiveness and\npolitical ideology, then LLMs could augment the Computational Social Science\n(CSS) pipeline in important ways. This work provides a road map for using LLMs\nas CSS tools. Towards this end, we contribute a set of prompting best practices\nand an extensive evaluation pipeline to measure the zero-shot performance of 13\nlanguage models on 25 representative English CSS benchmarks. On taxonomic\nlabeling tasks (classification), LLMs fail to outperform the best fine-tuned\nmodels but still achieve fair levels of agreement with humans. On free-form\ncoding tasks (generation), LLMs produce explanations that often exceed the\nquality of crowdworkers' gold references. We conclude that the performance of\ntoday's LLMs can augment the CSS research pipeline in two ways: (1) serving as\nzero-shot data annotators on human annotation teams, and (2) bootstrapping\nchallenging creative generation tasks (e.g., explaining the underlying\nattributes of a text). In summary, LLMs are posed to meaningfully participate\nin social science analysis in partnership with humans.", "published": "2023-04-12 17:33:28", "link": "http://arxiv.org/abs/2305.03514v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Galactic ChitChat: Using Large Language Models to Converse with\n  Astronomy Literature", "abstract": "We demonstrate the potential of the state-of-the-art OpenAI GPT-4 large\nlanguage model to engage in meaningful interactions with Astronomy papers using\nin-context prompting. To optimize for efficiency, we employ a distillation\ntechnique that effectively reduces the size of the original input paper by\n50\\%, while maintaining the paragraph structure and overall semantic integrity.\nWe then explore the model's responses using a multi-document context (ten\ndistilled documents). Our findings indicate that GPT-4 excels in the\nmulti-document domain, providing detailed answers contextualized within the\nframework of related research findings. Our results showcase the potential of\nlarge language models for the astronomical community, offering a promising\navenue for further exploration, particularly the possibility of utilizing the\nmodels for hypothesis generation.", "published": "2023-04-12 03:02:20", "link": "http://arxiv.org/abs/2304.05406v2", "categories": ["cs.CL", "astro-ph.GA", "astro-ph.IM"], "primary_category": "cs.CL"}
{"title": "Does Informativeness Matter? Active Learning for Educational Dialogue\n  Act Classification", "abstract": "Dialogue Acts (DAs) can be used to explain what expert tutors do and what\nstudents know during the tutoring process. Most empirical studies adopt the\nrandom sampling method to obtain sentence samples for manual annotation of DAs,\nwhich are then used to train DA classifiers. However, these studies have paid\nlittle attention to sample informativeness, which can reflect the information\nquantity of the selected samples and inform the extent to which a classifier\ncan learn patterns. Notably, the informativeness level may vary among the\nsamples and the classifier might only need a small amount of low informative\nsamples to learn the patterns. Random sampling may overlook sample\ninformativeness, which consumes human labelling costs and contributes less to\ntraining the classifiers. As an alternative, researchers suggest employing\nstatistical sampling methods of Active Learning (AL) to identify the\ninformative samples for training the classifiers. However, the use of AL\nmethods in educational DA classification tasks is under-explored. In this\npaper, we examine the informativeness of annotated sentence samples. Then, the\nstudy investigates how the AL methods can select informative samples to support\nDA classifiers in the AL sampling process. The results reveal that most\nannotated sentences present low informativeness in the training dataset and the\npatterns of these sentences can be easily captured by the DA classifier. We\nalso demonstrate how AL methods can reduce the cost of manual annotation in the\nAL sampling process.", "published": "2023-04-12 02:42:20", "link": "http://arxiv.org/abs/2304.05578v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Semantic Feature Verification in FLAN-T5", "abstract": "This study evaluates the potential of a large language model for aiding in\ngeneration of semantic feature norms - a critical tool for evaluating\nconceptual structure in cognitive science. Building from an existing\nhuman-generated dataset, we show that machine-verified norms capture aspects of\nconceptual structure beyond what is expressed in human norms alone, and better\nexplain human judgments of semantic similarity amongst items that are distally\nrelated. The results suggest that LLMs can greatly enhance traditional methods\nof semantic feature norm verification, with implications for our understanding\nof conceptual representation in humans and machines.", "published": "2023-04-12 03:37:57", "link": "http://arxiv.org/abs/2304.05591v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Angler: Helping Machine Translation Practitioners Prioritize Model\n  Improvements", "abstract": "Machine learning (ML) models can fail in unexpected ways in the real world,\nbut not all model failures are equal. With finite time and resources, ML\npractitioners are forced to prioritize their model debugging and improvement\nefforts. Through interviews with 13 ML practitioners at Apple, we found that\npractitioners construct small targeted test sets to estimate an error's nature,\nscope, and impact on users. We built on this insight in a case study with\nmachine translation models, and developed Angler, an interactive visual\nanalytics tool to help practitioners prioritize model improvements. In a user\nstudy with 7 machine translation experts, we used Angler to understand\nprioritization practices when the input space is infinite, and obtaining\nreliable signals of model quality is expensive. Our study revealed that\nparticipants could form more interesting and user-focused hypotheses for\nprioritization by analyzing quantitative summary statistics and qualitatively\nassessing data by reading sentences.", "published": "2023-04-12 16:43:39", "link": "http://arxiv.org/abs/2304.05967v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.HC"}
{"title": "Textual Explanations for Automated Commentary Driving", "abstract": "The provision of natural language explanations for the predictions of\ndeep-learning-based vehicle controllers is critical as it enhances transparency\nand easy audit. In this work, a state-of-the-art (SOTA) prediction and\nexplanation model is thoroughly evaluated and validated (as a benchmark) on the\nnew Sense--Assess--eXplain (SAX). Additionally, we developed a new explainer\nmodel that improved over the baseline architecture in two ways: (i) an\nintegration of part of speech prediction and (ii) an introduction of special\ntoken penalties. On the BLEU metric, our explanation generation technique\noutperformed SOTA by a factor of 7.7 when applied on the BDD-X dataset. The\ndescription generation technique is also improved by a factor of 1.3. Hence,\nour work contributes to the realisation of future explainable autonomous\nvehicles.", "published": "2023-04-12 21:50:20", "link": "http://arxiv.org/abs/2304.08178v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG", "cs.RO"], "primary_category": "cs.CL"}
{"title": "Speech Reconstruction from Silent Tongue and Lip Articulation By Pseudo\n  Target Generation and Domain Adversarial Training", "abstract": "This paper studies the task of speech reconstruction from ultrasound tongue\nimages and optical lip videos recorded in a silent speaking mode, where people\nonly activate their intra-oral and extra-oral articulators without producing\nsound. This task falls under the umbrella of articulatory-to-acoustic\nconversion, and may also be refered to as a silent speech interface. We propose\nto employ a method built on pseudo target generation and domain adversarial\ntraining with an iterative training strategy to improve the intelligibility and\nnaturalness of the speech recovered from silent tongue and lip articulation.\nExperiments show that our proposed method significantly improves the\nintelligibility and naturalness of the reconstructed speech in silent speaking\nmode compared to the baseline TaLNet model. When using an automatic speech\nrecognition (ASR) model to measure intelligibility, the word error rate (WER)\nof our proposed method decreases by over 15% compared to the baseline. In\naddition, our proposed method also outperforms the baseline on the\nintelligibility of the speech reconstructed in vocalized articulating mode,\nreducing the WER by approximately 10%.", "published": "2023-04-12 02:24:36", "link": "http://arxiv.org/abs/2304.05574v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Regularizing Contrastive Predictive Coding for Speech Applications", "abstract": "Self-supervised methods such as Contrastive predictive Coding (CPC) have\ngreatly improved the quality of the unsupervised representations. These\nrepresentations significantly reduce the amount of labeled data needed for\ndownstream task performance, such as automatic speech recognition. CPC learns\nrepresentations by learning to predict future frames given current frames.\nBased on the observation that the acoustic information, e.g., phones, changes\nslower than the feature extraction rate in CPC, we propose regularization\ntechniques that impose slowness constraints on the features. Here we propose\ntwo regularization techniques: Self-expressing constraint and Left-or-Right\nregularization. We evaluate the proposed model on ABX and linear phone\nclassification tasks, acoustic unit discovery, and automatic speech\nrecognition. The regularized CPC trained on 100 hours of unlabeled data matches\nthe performance of the baseline CPC trained on 360 hours of unlabeled data. We\nalso show that our regularization techniques are complementary to data\naugmentation and can further boost the system's performance. In monolingual,\ncross-lingual, or multilingual settings, with/without data augmentation,\nregardless of the amount of data used for training, our regularized models\noutperformed the baseline CPC models on the ABX task.", "published": "2023-04-12 16:57:19", "link": "http://arxiv.org/abs/2304.05974v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Self-Supervised Learning with Cluster-Aware-DINO for High-Performance\n  Robust Speaker Verification", "abstract": "Automatic speaker verification task has made great achievements using deep\nlearning approaches with the large-scale manually annotated dataset. However,\nit's very difficult and expensive to collect a large amount of well-labeled\ndata for system building. In this paper, we propose a novel and advanced\nself-supervised learning framework which can construct a high performance\nspeaker verification system without using any labeled data. To avoid the impact\nof false negative pairs, we adopt the self-distillation with no labels (DINO)\nframework as the initial model, which can be trained without exploiting\nnegative pairs. Then, we introduce a cluster-aware training strategy for DINO\nto improve the diversity of data. In the iteration learning stage, due to a\nmass of unreliable labels from clustering, the quality of pseudo labels is\nimportant for the system training. This motivates us to propose dynamic\nloss-gate and label correction (DLG-LC) methods to alleviate the performance\ndegradation caused by unreliable labels. More specifically, we model the loss\ndistribution with GMM and obtain the loss-gate threshold dynamically to\ndistinguish the reliable and unreliable labels. Besides, we adopt the model\npredictions to correct the unreliable label, for better utilizing the\nunreliable data rather than dropping them directly. Moreover, we extend the\nDLG-LC to multi-modality to further improve the performance. The experiments\nare performed on the commonly used Voxceleb dataset. Compared to the best-known\nself-supervised speaker verification system, our proposed method obtain 22.17%,\n27.94% and 25.56% relative EER improvement on Vox-O, Vox-E and Vox-H test sets,\neven with fewer iterations, smaller models, and simpler clustering methods.\nMore importantly, the newly proposed system even achieves comparable results\nwith the fully supervised system, but without using any human labeled data.", "published": "2023-04-12 10:32:29", "link": "http://arxiv.org/abs/2304.05754v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Filler Word Detection with Hard Category Mining and Inter-Category Focal\n  Loss", "abstract": "Filler words like ``um\" or ``uh\" are common in spontaneous speech. It is\ndesirable to automatically detect and remove them in recordings, as they affect\nthe fluency, confidence, and professionalism of speech. Previous studies and\nour preliminary experiments reveal that the biggest challenge in filler word\ndetection is that fillers can be easily confused with other hard categories\nlike ``a\" or ``I\". In this paper, we propose a novel filler word detection\nmethod that effectively addresses this challenge by adding auxiliary categories\ndynamically and applying an additional inter-category focal loss. The auxiliary\ncategories force the model to explicitly model the confusing words by mining\nhard categories. In addition, inter-category focal loss adaptively adjusts the\npenalty weight between ``filler\" and ``non-filler\" categories to deal with\nother confusing words left in the ``non-filler\" category. Our system achieves\nthe best results, with a huge improvement compared to other methods on the\nPodcastFillers dataset.", "published": "2023-04-12 15:40:07", "link": "http://arxiv.org/abs/2304.05922v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Acoustic absement in detail: Quantifying acoustic differences across\n  time-series representations of speech data", "abstract": "The speech signal is a consummate example of time-series data. The acoustics\nof the signal change over time, sometimes dramatically. Yet, the most common\ntype of comparison we perform in phonetics is between instantaneous acoustic\nmeasurements, such as formant values. In the present paper, I discuss the\nconcept of absement as a quantification of differences between two time-series.\nI then provide an experimental example of absement applied to phonetic analysis\nfor human and/or computer speech recognition. The experiment is a\ntemplate-based speech recognition task, using dynamic time warping to compare\nthe acoustics between recordings of isolated words. A recognition accuracy of\n57.9% was achieved. The results of the experiment are discussed in terms of\nusing absement as a tool, as well as the implications of using acoustics-only\nmodels of spoken word recognition with the word as the smallest discrete\nlinguistic unit.", "published": "2023-04-12 22:55:35", "link": "http://arxiv.org/abs/2304.06183v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Phoneme-Informed Neural Network Model for Note-Level Singing\n  Transcription", "abstract": "Note-level automatic music transcription is one of the most representative\nmusic information retrieval (MIR) tasks and has been studied for various\ninstruments to understand music. However, due to the lack of high-quality\nlabeled data, transcription of many instruments is still a challenging task. In\nparticular, in the case of singing, it is difficult to find accurate notes due\nto its expressiveness in pitch, timbre, and dynamics. In this paper, we propose\na method of finding note onsets of singing voice more accurately by leveraging\nthe linguistic characteristics of singing, which are not seen in other\ninstruments. The proposed model uses mel-scaled spectrogram and phonetic\nposteriorgram (PPG), a frame-wise likelihood of phoneme, as an input of the\nonset detection network while PPG is generated by the pre-trained network with\nsinging and speech data. To verify how linguistic features affect onset\ndetection, we compare the evaluation results through the dataset with different\nlanguages and divide onset types for detailed analysis. Our approach\nsubstantially improves the performance of singing transcription and therefore\nemphasizes the importance of linguistic features in singing analysis.", "published": "2023-04-12 15:36:01", "link": "http://arxiv.org/abs/2304.05917v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Scalable Framework for Automatic Playlist Continuation on Music\n  Streaming Services", "abstract": "Music streaming services often aim to recommend songs for users to extend the\nplaylists they have created on these services. However, extending playlists\nwhile preserving their musical characteristics and matching user preferences\nremains a challenging task, commonly referred to as Automatic Playlist\nContinuation (APC). Besides, while these services often need to select the best\nsongs to recommend in real-time and among large catalogs with millions of\ncandidates, recent research on APC mainly focused on models with few\nscalability guarantees and evaluated on relatively small datasets. In this\npaper, we introduce a general framework to build scalable yet effective APC\nmodels for large-scale applications. Based on a represent-then-aggregate\nstrategy, it ensures scalability by design while remaining flexible enough to\nincorporate a wide range of representation learning and sequence modeling\ntechniques, e.g., based on Transformers. We demonstrate the relevance of this\nframework through in-depth experimental validation on Spotify's Million\nPlaylist Dataset (MPD), the largest public dataset for APC. We also describe\nhow, in 2022, we successfully leveraged this framework to improve APC in\nproduction on Deezer. We report results from a large-scale online A/B test on\nthis service, emphasizing the practical impact of our approach in such a\nreal-world application.", "published": "2023-04-12 08:46:04", "link": "http://arxiv.org/abs/2304.09061v1", "categories": ["cs.IR", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.IR"}
{"title": "Looking Similar, Sounding Different: Leveraging Counterfactual\n  Cross-Modal Pairs for Audiovisual Representation Learning", "abstract": "Audiovisual representation learning typically relies on the correspondence\nbetween sight and sound. However, there are often multiple audio tracks that\ncan correspond with a visual scene. Consider, for example, different\nconversations on the same crowded street. The effect of such counterfactual\npairs on audiovisual representation learning has not been previously explored.\nTo investigate this, we use dubbed versions of movies and television shows to\naugment cross-modal contrastive learning. Our approach learns to represent\nalternate audio tracks, differing only in speech, similarly to the same video.\nOur results, from a comprehensive set of experiments investigating different\ntraining strategies, show this general approach improves performance on a range\nof downstream auditory and audiovisual tasks, without majorly affecting\nlinguistic task performance overall. These findings highlight the importance of\nconsidering speech variation when learning scene-level audiovisual\ncorrespondences and suggest that dubbed audio can be a useful augmentation\ntechnique for training audiovisual models toward more robust performance on\ndiverse downstream tasks.", "published": "2023-04-12 04:17:45", "link": "http://arxiv.org/abs/2304.05600v2", "categories": ["cs.SD", "cs.CV", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
