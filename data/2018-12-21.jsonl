{"title": "Multiple topic identification in telephone conversations", "abstract": "This paper deals with the automatic analysis of conversations between a\ncustomer and an agent in a call centre of a customer care service. The purpose\nof the analysis is to hypothesize themes about problems and complaints\ndiscussed in the conversation. Themes are defined by the application\ndocumentation topics. A conversation may contain mentions that are irrelevant\nfor the application purpose and multiple themes whose mentions may be\ninterleaved portions of a conversation that cannot be well defined. Two methods\nare proposed for multiple theme hypothesization. One of them is based on a\ncosine similarity measure using a bag of features extracted from the entire\nconversation. The other method introduces the concept of thematic density\ndistributed around specific word positions in a conversation. In addition to\nautomatically selected words, word bi-grams with possible gaps between\nsuccessive words are also considered and selected. Experimental results show\nthat the results obtained with the proposed methods outperform the results\nobtained with support vector machines on the same data. Furthermore, using the\ntheme skeleton of a conversation from which thematic densities are derived, it\nwill be possible to extract components of an automatic conversation report to\nbe used for improving the service performance. Index Terms: multi-topic audio\ndocument classification, hu-man/human conversation analysis, speech analytics,\ndistance bigrams", "published": "2018-12-21 15:31:17", "link": "http://arxiv.org/abs/1812.09321v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What Is One Grain of Sand in the Desert? Analyzing Individual Neurons in\n  Deep NLP Models", "abstract": "Despite the remarkable evolution of deep neural networks in natural language\nprocessing (NLP), their interpretability remains a challenge. Previous work\nlargely focused on what these models learn at the representation level. We\nbreak this analysis down further and study individual dimensions (neurons) in\nthe vector representation learned by end-to-end neural models in NLP tasks. We\npropose two methods: Linguistic Correlation Analysis, based on a supervised\nmethod to extract the most relevant neurons with respect to an extrinsic task,\nand Cross-model Correlation Analysis, an unsupervised method to extract salient\nneurons w.r.t. the model itself. We evaluate the effectiveness of our\ntechniques by ablating the identified neurons and reevaluating the network's\nperformance for two tasks: neural machine translation (NMT) and neural language\nmodeling (NLM). We further present a comprehensive analysis of neurons with the\naim to address the following questions: i) how localized or distributed are\ndifferent linguistic properties in the models? ii) are certain neurons\nexclusive to some properties and not others? iii) is the information more or\nless distributed in NMT vs. NLM? and iv) how important are the neurons\nidentified through the linguistic correlation method to the overall task? Our\ncode is publicly available as part of the NeuroX toolkit (Dalvi et al. 2019).", "published": "2018-12-21 19:51:47", "link": "http://arxiv.org/abs/1812.09355v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NeuroX: A Toolkit for Analyzing Individual Neurons in Neural Networks", "abstract": "We present a toolkit to facilitate the interpretation and understanding of\nneural network models. The toolkit provides several methods to identify salient\nneurons with respect to the model itself or an external task. A user can\nvisualize selected neurons, ablate them to measure their effect on the model\naccuracy, and manipulate them to control the behavior of the model at the test\ntime. Such an analysis has a potential to serve as a springboard in various\nresearch directions, such as understanding the model, better architectural\nchoices, model distillation and controlling data biases.", "published": "2018-12-21 20:20:26", "link": "http://arxiv.org/abs/1812.09359v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sources of Complexity in Semantic Frame Parsing for Information\n  Extraction", "abstract": "This paper describes a Semantic Frame parsing System based on sequence\nlabeling methods, precisely BiLSTM models with highway connections, for\nperforming information extraction on a corpus of French encyclopedic history\ntexts annotated according to the Berkeley FrameNet formalism. The approach\nproposed in this study relies on an integrated sequence labeling model which\njointly optimizes frame identification and semantic role segmentation and\nidentification. The purpose of this study is to analyze the task complexity, to\nhighlight the factors that make Semantic Frame parsing a difficult task and to\nprovide detailed evaluations of the performance on different types of frames\nand sentences.", "published": "2018-12-21 15:30:17", "link": "http://arxiv.org/abs/1812.09193v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Empirical Analysis of Deep Audio-Visual Models for Speech Recognition", "abstract": "In this project, we worked on speech recognition, specifically predicting\nindividual words based on both the video frames and audio. Empowered by\nconvolutional neural networks, the recent speech recognition and lip reading\nmodels are comparable to human level performance. We re-implemented and made\nderivations of the state-of-the-art model. Then, we conducted rich experiments\nincluding the effectiveness of attention mechanism, more accurate residual\nnetwork as the backbone with pre-trained weights and the sensitivity of our\nmodel with respect to audio input with/without noise.", "published": "2018-12-21 19:02:52", "link": "http://arxiv.org/abs/1812.09336v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Enhancing Person-Job Fit for Talent Recruitment: An Ability-aware Neural\n  Network Approach", "abstract": "The wide spread use of online recruitment services has led to information\nexplosion in the job market. As a result, the recruiters have to seek the\nintelligent ways for Person Job Fit, which is the bridge for adapting the right\njob seekers to the right positions. Existing studies on Person Job Fit have a\nfocus on measuring the matching degree between the talent qualification and the\njob requirements mainly based on the manual inspection of human resource\nexperts despite of the subjective, incomplete, and inefficient nature of the\nhuman judgement. To this end, in this paper, we propose a novel end to end\nAbility aware Person Job Fit Neural Network model, which has a goal of reducing\nthe dependence on manual labour and can provide better interpretation about the\nfitting results. The key idea is to exploit the rich information available at\nabundant historical job application data. Specifically, we propose a word level\nsemantic representation for both job requirements and job seekers' experiences\nbased on Recurrent Neural Network. Along this line, four hierarchical ability\naware attention strategies are designed to measure the different importance of\njob requirements for semantic representation, as well as measuring the\ndifferent contribution of each job experience to a specific ability\nrequirement. Finally, extensive experiments on a large scale real world data\nset clearly validate the effectiveness and interpretability of the APJFNN\nframework compared with several baselines.", "published": "2018-12-21 05:02:02", "link": "http://arxiv.org/abs/1812.08947v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Analysis Methods in Neural Language Processing: A Survey", "abstract": "The field of natural language processing has seen impressive progress in\nrecent years, with neural network models replacing many of the traditional\nsystems. A plethora of new models have been proposed, many of which are thought\nto be opaque compared to their feature-rich counterparts. This has led\nresearchers to analyze, interpret, and evaluate neural networks in novel and\nmore fine-grained ways. In this survey paper, we review analysis methods in\nneural language processing, categorize them according to prominent research\ntrends, highlight existing limitations, and point to potential directions for\nfuture work.", "published": "2018-12-21 05:13:03", "link": "http://arxiv.org/abs/1812.08951v2", "categories": ["cs.CL", "cs.LG", "cs.NE", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "The Design and Implementation of XiaoIce, an Empathetic Social Chatbot", "abstract": "This paper describes the development of Microsoft XiaoIce, the most popular\nsocial chatbot in the world. XiaoIce is uniquely designed as an AI companion\nwith an emotional connection to satisfy the human need for communication,\naffection, and social belonging. We take into account both intelligent quotient\n(IQ) and emotional quotient (EQ) in system design, cast human-machine social\nchat as decision-making over Markov Decision Processes (MDPs), and optimize\nXiaoIce for long-term user engagement, measured in expected Conversation-turns\nPer Session (CPS). We detail the system architecture and key components\nincluding dialogue manager, core chat, skills, and an empathetic computing\nmodule. We show how XiaoIce dynamically recognizes human feelings and states,\nunderstands user intent, and responds to user needs throughout long\nconversations. Since her launch in 2014, XiaoIce has communicated with over 660\nmillion active users and succeeded in establishing long-term relationships with\nmany of them. Analysis of large scale online logs shows that XiaoIce has\nachieved an average CPS of 23, which is significantly higher than that of other\nchatbots and even human conversations.", "published": "2018-12-21 08:01:31", "link": "http://arxiv.org/abs/1812.08989v2", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Learning to Navigate the Web", "abstract": "Learning in environments with large state and action spaces, and sparse\nrewards, can hinder a Reinforcement Learning (RL) agent's learning through\ntrial-and-error. For instance, following natural language instructions on the\nWeb (such as booking a flight ticket) leads to RL settings where input\nvocabulary and number of actionable elements on a page can grow very large.\nEven though recent approaches improve the success rate on relatively simple\nenvironments with the help of human demonstrations to guide the exploration,\nthey still fail in environments where the set of possible instructions can\nreach millions. We approach the aforementioned problems from a different\nperspective and propose guided RL approaches that can generate unbounded amount\nof experience for an agent to learn from. Instead of learning from a\ncomplicated instruction with a large vocabulary, we decompose it into multiple\nsub-instructions and schedule a curriculum in which an agent is tasked with a\ngradually increasing subset of these relatively easier sub-instructions. In\naddition, when the expert demonstrations are not available, we propose a novel\nmeta-learning framework that generates new instruction following tasks and\ntrains the agent more effectively. We train DQN, deep reinforcement learning\nagent, with Q-value function approximated with a novel QWeb neural network\narchitecture on these smaller, synthetic instructions. We evaluate the ability\nof our agent to generalize to new instructions on World of Bits benchmark, on\nforms with up to 100 elements, supporting 14 million possible instructions. The\nQWeb agent outperforms the baseline without using any human demonstration\nachieving 100% success rate on several difficult environments.", "published": "2018-12-21 15:32:59", "link": "http://arxiv.org/abs/1812.09195v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Symbolic inductive bias for visually grounded learning of spoken\n  language", "abstract": "A widespread approach to processing spoken language is to first automatically\ntranscribe it into text. An alternative is to use an end-to-end approach:\nrecent works have proposed to learn semantic embeddings of spoken language from\nimages with spoken captions, without an intermediate transcription step. We\npropose to use multitask learning to exploit existing transcribed speech within\nthe end-to-end setting. We describe a three-task architecture which combines\nthe objectives of matching spoken captions with corresponding images, speech\nwith text, and text with images. We show that the addition of the speech/text\ntask leads to substantial performance improvements on image retrieval when\ncompared to training the speech/image task in isolation. We conjecture that\nthis is due to a strong inductive bias transcribed speech provides to the\nmodel, and offer supporting evidence for this.", "published": "2018-12-21 16:37:29", "link": "http://arxiv.org/abs/1812.09244v3", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Multi-Domain Processing via Hybrid Denoising Networks for Speech\n  Enhancement", "abstract": "We present a hybrid framework that leverages the trade-off between temporal\nand frequency precision in audio representations to improve the performance of\nspeech enhancement task. We first show that conventional approaches using\nspecific representations such as raw-audio and spectrograms are each effective\nat targeting different types of noise. By integrating both approaches, our\nmodel can learn multi-scale and multi-domain features, effectively removing\nnoise existing on different regions on the time-frequency space in a\ncomplementary way. Experimental results show that the proposed hybrid model\nyields better performance and robustness than using each model individually.", "published": "2018-12-21 02:12:22", "link": "http://arxiv.org/abs/1812.08914v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "End-to-End Classification of Reverberant Rooms using DNNs", "abstract": "Reverberation is present in our workplaces, our homes, concert halls and\ntheatres. This paper investigates how deep learning can use the effect of\nreverberation on speech to classify a recording in terms of the room in which\nit was recorded. Existing approaches in the literature rely on domain expertise\nto manually select acoustic parameters as inputs to classifiers. Estimation of\nthese parameters from reverberant speech is adversely affected by estimation\nerrors, impacting the classification accuracy. In order to overcome the\nlimitations of previously proposed methods, this paper shows how DNNs can\nperform the classification by operating directly on reverberant speech spectra\nand a CRNN with an attention-mechanism is proposed for the task. The\nrelationship is investigated between the reverberant speech representations\nlearned by the DNNs and acoustic parameters. For evaluation, AIRs are used from\nthe ACE-challenge dataset that were measured in 7 real rooms. The\nclassification accuracy of the CRNN classifier in the experiments is 78% when\nusing 5 hours of training data and 90% when using 10 hours.", "published": "2018-12-21 18:39:12", "link": "http://arxiv.org/abs/1812.09324v6", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
