{"title": "Learning to Embed Words in Context for Syntactic Tasks", "abstract": "We present models for embedding words in the context of surrounding words.\nSuch models, which we refer to as token embeddings, represent the\ncharacteristics of a word that are specific to a given context, such as word\nsense, syntactic category, and semantic role. We explore simple, efficient\ntoken embedding models based on standard neural network architectures. We learn\ntoken embeddings on a large amount of unannotated text and evaluate them as\nfeatures for part-of-speech taggers and dependency parsers trained on much\nsmaller amounts of annotated data. We find that predictors endowed with token\nembeddings consistently outperform baseline predictors across a range of\ncontext window and training set sizes.", "published": "2017-06-09 01:39:12", "link": "http://arxiv.org/abs/1706.02807v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assigning personality/identity to a chatting machine for coherent\n  conversation generation", "abstract": "Endowing a chatbot with personality or an identity is quite challenging but\ncritical to deliver more realistic and natural conversations. In this paper, we\naddress the issue of generating responses that are coherent to a pre-specified\nagent profile. We design a model consisting of three modules: a profile\ndetector to decide whether a post should be responded using the profile and\nwhich key should be addressed, a bidirectional decoder to generate responses\nforward and backward starting from a selected profile value, and a position\ndetector that predicts a word position from which decoding should start given a\nselected profile value. We show that general conversation data from social\nmedia can be used to generate profile-coherent responses. Manual and automatic\nevaluation shows that our model can deliver more coherent, natural, and\ndiversified responses.", "published": "2017-06-09 08:13:21", "link": "http://arxiv.org/abs/1706.02861v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Overview of the NLPCC 2017 Shared Task: Chinese News Headline\n  Categorization", "abstract": "In this paper, we give an overview for the shared task at the CCF Conference\non Natural Language Processing \\& Chinese Computing (NLPCC 2017): Chinese News\nHeadline Categorization. The dataset of this shared task consists 18 classes,\n12,000 short texts along with corresponded labels for each class. The dataset\nand example code can be accessed at\nhttps://github.com/FudanNLP/nlpcc2017_news_headline_categorization.", "published": "2017-06-09 10:17:24", "link": "http://arxiv.org/abs/1706.02883v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Trimming and Improving Skip-thought Vectors", "abstract": "The skip-thought model has been proven to be effective at learning sentence\nrepresentations and capturing sentence semantics. In this paper, we propose a\nsuite of techniques to trim and improve it. First, we validate a hypothesis\nthat, given a current sentence, inferring the previous and inferring the next\nsentence provide similar supervision power, therefore only one decoder for\npredicting the next sentence is preserved in our trimmed skip-thought model.\nSecond, we present a connection layer between encoder and decoder to help the\nmodel to generalize better on semantic relatedness tasks. Third, we found that\na good word embedding initialization is also essential for learning better\nsentence representations. We train our model unsupervised on a large corpus\nwith contiguous sentences, and then evaluate the trained model on 7 supervised\ntasks, which includes semantic relatedness, paraphrase detection, and text\nclassification benchmarks. We empirically show that, our proposed model is a\nfaster, lighter-weight and equally powerful alternative to the original\nskip-thought model.", "published": "2017-06-09 22:44:31", "link": "http://arxiv.org/abs/1706.03148v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Depthwise Separable Convolutions for Neural Machine Translation", "abstract": "Depthwise separable convolutions reduce the number of parameters and\ncomputation used in convolutional operations while increasing representational\nefficiency. They have been shown to be successful in image classification\nmodels, both in obtaining better models than previously possible for a given\nparameter count (the Xception architecture) and considerably reducing the\nnumber of parameters required to perform at a given level (the MobileNets\nfamily of architectures). Recently, convolutional sequence-to-sequence networks\nhave been applied to machine translation tasks with good results. In this work,\nwe study how depthwise separable convolutions can be applied to neural machine\ntranslation. We introduce a new architecture inspired by Xception and ByteNet,\ncalled SliceNet, which enables a significant reduction of the parameter count\nand amount of computation needed to obtain results like ByteNet, and, with a\nsimilar parameter count, achieves new state-of-the-art results. In addition to\nshowing that depthwise separable convolutions perform well for machine\ntranslation, we investigate the architectural changes that they enable: we\nobserve that thanks to depthwise separability, we can increase the length of\nconvolution windows, removing the need for filter dilation. We also introduce a\nnew \"super-separable\" convolution operation that further reduces the number of\nparameters and computational cost for obtaining state-of-the-art results.", "published": "2017-06-09 17:59:16", "link": "http://arxiv.org/abs/1706.03059v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Rethinking Skip-thought: A Neighborhood based Approach", "abstract": "We study the skip-thought model with neighborhood information as weak\nsupervision. More specifically, we propose a skip-thought neighbor model to\nconsider the adjacent sentences as a neighborhood. We train our skip-thought\nneighbor model on a large corpus with continuous sentences, and then evaluate\nthe trained model on 7 tasks, which include semantic relatedness, paraphrase\ndetection, and classification benchmarks. Both quantitative comparison and\nqualitative investigation are conducted. We empirically show that, our\nskip-thought neighbor model performs as well as the skip-thought model on\nevaluation tasks. In addition, we found that, incorporating an autoencoder path\nin our model didn't aid our model to perform better, while it hurts the\nperformance of the skip-thought model.", "published": "2017-06-09 22:39:31", "link": "http://arxiv.org/abs/1706.03146v1", "categories": ["cs.CL", "cs.AI", "cs.NE"], "primary_category": "cs.CL"}
{"title": "A modulation property of time-frequency derivatives of filtered phase\n  and its application to aperiodicity and fo estimation", "abstract": "We introduce a simple and linear SNR (strictly speaking, periodic to random\npower ratio) estimator (0dB to 80dB without additional\ncalibration/linearization) for providing reliable descriptions of aperiodicity\nin speech corpus. The main idea of this method is to estimate the background\nrandom noise level without directly extracting the background noise. The\nproposed method is applicable to a wide variety of time windowing functions\nwith very low sidelobe levels. The estimate combines the frequency derivative\nand the time-frequency derivative of the mapping from filter center frequency\nto the output instantaneous frequency. This procedure can replace the\nperiodicity detection and aperiodicity estimation subsystems of recently\nintroduced open source vocoder, YANG vocoder. Source code of MATLAB\nimplementation of this method will also be open sourced.", "published": "2017-06-09 14:11:22", "link": "http://arxiv.org/abs/1706.02964v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
