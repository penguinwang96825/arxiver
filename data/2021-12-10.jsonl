{"title": "Findings on Conversation Disentanglement", "abstract": "Conversation disentanglement, the task to identify separate threads in\nconversations, is an important pre-processing step in multi-party\nconversational NLP applications such as conversational question answering and\nconversation summarization. Framing it as a utterance-to-utterance\nclassification problem -- i.e. given an utterance of interest (UOI), find which\npast utterance it replies to -- we explore a number of transformer-based models\nand found that BERT in combination with handcrafted features remains a strong\nbaseline. We then build a multi-task learning model that jointly learns\nutterance-to-utterance and utterance-to-thread classification. Observing that\nthe ground truth label (past utterance) is in the top candidates when our model\nmakes an error, we experiment with using bipartite graphs as a post-processing\nstep to learn how to best match a set of UOIs to past utterances. Experiments\non the Ubuntu IRC dataset show that this approach has the potential to\noutperform the conventional greedy approach of simply selecting the highest\nprobability candidate for each UOI independently, indicating a promising future\nresearch direction.", "published": "2021-12-10 05:54:48", "link": "http://arxiv.org/abs/2112.05346v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentiment Analysis on Brazilian Portuguese User Reviews", "abstract": "Sentiment Analysis is one of the most classical and primarily studied natural\nlanguage processing tasks. This problem had a notable advance with the\nproposition of more complex and scalable machine learning models. Despite this\nprogress, the Brazilian Portuguese language still disposes only of limited\nlinguistic resources, such as datasets dedicated to sentiment classification,\nespecially when considering the existence of predefined partitions in training,\ntesting, and validation sets that would allow a more fair comparison of\ndifferent algorithm alternatives. Motivated by these issues, this work analyzes\nthe predictive performance of a range of document embedding strategies,\nassuming the polarity as the system outcome. This analysis includes five\nsentiment analysis datasets in Brazilian Portuguese, unified in a single\ndataset, and a reference partitioning in training, testing, and validation\nsets, both made publicly available through a digital repository. A\ncross-evaluation of dataset-specific models over different contexts is\nconducted to evaluate their generalization capabilities and the feasibility of\nadopting a unique model for addressing all scenarios.", "published": "2021-12-10 11:18:26", "link": "http://arxiv.org/abs/2112.05459v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analysis and Prediction of NLP Models Via Task Embeddings", "abstract": "Task embeddings are low-dimensional representations that are trained to\ncapture task properties. In this paper, we propose MetaEval, a collection of\n$101$ NLP tasks. We fit a single transformer to all MetaEval tasks jointly\nwhile conditioning it on learned embeddings. The resulting task embeddings\nenable a novel analysis of the space of tasks. We then show that task aspects\ncan be mapped to task embeddings for new tasks without using any annotated\nexamples.\n  Predicted embeddings can modulate the encoder for zero-shot inference and\noutperform a zero-shot baseline on GLUE tasks. The provided multitask setup can\nfunction as a benchmark for future transfer learning research.", "published": "2021-12-10 16:23:24", "link": "http://arxiv.org/abs/2112.05647v1", "categories": ["cs.CL", "I.2.7; I.2.6"], "primary_category": "cs.CL"}
{"title": "Am I Me or You? State-of-the-Art Dialogue Models Cannot Maintain an\n  Identity", "abstract": "State-of-the-art dialogue models still often stumble with regards to factual\naccuracy and self-contradiction. Anecdotally, they have been observed to fail\nto maintain character identity throughout discourse; and more specifically, may\ntake on the role of their interlocutor. In this work we formalize and quantify\nthis deficiency, and show experimentally through human evaluations that this is\nindeed a problem. In contrast, we show that discriminative models trained\nspecifically to recognize who is speaking can perform well; and further, these\ncan be used as automated metrics. Finally, we evaluate a wide variety of\nmitigation methods, including changes to model architecture, training protocol,\nand decoding strategy. Our best models reduce mistaken identity issues by\nnearly 65% according to human annotators, while simultaneously improving\nengagingness. Despite these results, we find that maintaining character\nidentity still remains a challenging problem.", "published": "2021-12-10 21:58:16", "link": "http://arxiv.org/abs/2112.05843v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic Construction Grammar: Bridging the NL / Logic Divide", "abstract": "In this paper, we discuss Semantic Construction Grammar (SCG), a system\ndeveloped over the past several years to facilitate translation between natural\nlanguage and logical representations. Crucially, SCG is designed to support a\nvariety of different methods of representation, ranging from those that are\nfairly close to the NL structure (e.g. so-called 'logical forms'), to those\nthat are quite different from the NL structure, with higher-order and\nhigh-arity relations. Semantic constraints and checks on representations are\nintegral to the process of NL understanding with SCG, and are easily carried\nout due to the SCG's integration with Cyc's Knowledge Base and inference\nengine.", "published": "2021-12-10 00:02:40", "link": "http://arxiv.org/abs/2112.05256v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multimodal Interactions Using Pretrained Unimodal Models for SIMMC 2.0", "abstract": "This paper presents our work on the Situated Interactive MultiModal\nConversations 2.0 challenge held at Dialog State Tracking Challenge 10. SIMMC\n2.0 includes 4 subtasks, and we introduce our multimodal approaches for the\nsubtask \\#1, \\#2 and the generation of subtask \\#4. SIMMC 2.0 dataset is a\nmultimodal dataset containing image and text information, which is more\nchallenging than the problem of only text-based conversations because it must\nbe solved by understanding the relationship between image and text. Therefore,\nsince there is a limit to solving only text models such as BERT or GPT2, we\npropose a multimodal model combining image and text. We first pretrain the\nmultimodal model to understand the relationship between image and text, then\nfinetune our model for each task. We achieve the 3rd best performance in\nsubtask \\#1, \\#2 and a runner-up in the generation of subtask \\#4. The source\ncode is available at https://github.com/rungjoo/simmc2.0.", "published": "2021-12-10 04:20:08", "link": "http://arxiv.org/abs/2112.05328v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Human Guided Exploitation of Interpretable Attention Patterns in\n  Summarization and Topic Segmentation", "abstract": "The multi-head self-attention mechanism of the transformer model has been\nthoroughly investigated recently. In one vein of study, researchers are\ninterested in understanding why and how transformers work. In another vein,\nresearchers propose new attention augmentation methods to make transformers\nmore accurate, efficient and interpretable. In this paper, we combine these two\nlines of research in a human-in-the-loop pipeline to first discover important\ntask-specific attention patterns. Then those patterns are injected, not only to\nsmaller models, but also to the original model. The benefits of our pipeline\nand discovered patterns are demonstrated in two case studies with extractive\nsummarization and topic segmentation. After discovering interpretable patterns\nin BERT-based models fine-tuned for the two downstream tasks, experiments\nindicate that when we inject the patterns into attention heads, the models show\nconsiderable improvements in accuracy and efficiency.", "published": "2021-12-10 07:15:09", "link": "http://arxiv.org/abs/2112.05364v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unsupervised Editing for Counterfactual Stories", "abstract": "Creating what-if stories requires reasoning about prior statements and\npossible outcomes of the changed conditions. One can easily generate coherent\nendings under new conditions, but it would be challenging for current systems\nto do it with minimal changes to the original story. Therefore, one major\nchallenge is the trade-off between generating a logical story and rewriting\nwith minimal-edits. In this paper, we propose EDUCAT, an editing-based\nunsupervised approach for counterfactual story rewriting. EDUCAT includes a\ntarget position detection strategy based on estimating causal effects of the\nwhat-if conditions, which keeps the causal invariant parts of the story. EDUCAT\nthen generates the stories under fluency, coherence and minimal-edits\nconstraints. We also propose a new metric to alleviate the shortcomings of\ncurrent automatic metrics and better evaluate the trade-off. We evaluate EDUCAT\non a public counterfactual story rewriting benchmark. Experiments show that\nEDUCAT achieves the best trade-off over unsupervised SOTA methods according to\nboth automatic and human evaluation. The resources of EDUCAT are available at:\nhttps://github.com/jiangjiechen/EDUCAT.", "published": "2021-12-10 09:49:23", "link": "http://arxiv.org/abs/2112.05417v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving the Question Answering Quality using Answer Candidate\n  Filtering based on Natural-Language Features", "abstract": "Software with natural-language user interfaces has an ever-increasing\nimportance. However, the quality of the included Question Answering (QA)\nfunctionality is still not sufficient regarding the number of questions that\nare answered correctly. In our work, we address the research problem of how the\nQA quality of a given system can be improved just by evaluating the\nnatural-language input (i.e., the user's question) and output (i.e., the\nsystem's answer). Our main contribution is an approach capable of identifying\nwrong answers provided by a QA system. Hence, filtering incorrect answers from\na list of answer candidates is leading to a highly improved QA quality. In\nparticular, our approach has shown its potential while removing in many cases\nthe majority of incorrect answers, which increases the QA quality significantly\nin comparison to the non-filtered output of a system.", "published": "2021-12-10 11:09:44", "link": "http://arxiv.org/abs/2112.05452v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Automated tabulation of clinical trial results: A joint entity and\n  relation extraction approach with transformer-based language representations", "abstract": "Evidence-based medicine, the practice in which healthcare professionals refer\nto the best available evidence when making decisions, forms the foundation of\nmodern healthcare. However, it relies on labour-intensive systematic reviews,\nwhere domain specialists must aggregate and extract information from thousands\nof publications, primarily of randomised controlled trial (RCT) results, into\nevidence tables. This paper investigates automating evidence table generation\nby decomposing the problem across two language processing tasks: \\textit{named\nentity recognition}, which identifies key entities within text, such as drug\nnames, and \\textit{relation extraction}, which maps their relationships for\nseparating them into ordered tuples. We focus on the automatic tabulation of\nsentences from published RCT abstracts that report the results of the study\noutcomes. Two deep neural net models were developed as part of a joint\nextraction pipeline, using the principles of transfer learning and\ntransformer-based language representations. To train and test these models, a\nnew gold-standard corpus was developed, comprising almost 600 result sentences\nfrom six disease areas. This approach demonstrated significant advantages, with\nour system performing well across multiple natural language processing tasks\nand disease areas, as well as in generalising to disease domains unseen during\ntraining. Furthermore, we show these results were achievable through training\nour models on as few as 200 example sentences. The final system is a proof of\nconcept that the generation of evidence tables can be semi-automated,\nrepresenting a step towards fully automating systematic reviews.", "published": "2021-12-10 15:26:43", "link": "http://arxiv.org/abs/2112.05596v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Match Your Words! A Study of Lexical Matching in Neural Information\n  Retrieval", "abstract": "Neural Information Retrieval models hold the promise to replace lexical\nmatching models, e.g. BM25, in modern search engines. While their capabilities\nhave fully shone on in-domain datasets like MS MARCO, they have recently been\nchallenged on out-of-domain zero-shot settings (BEIR benchmark), questioning\ntheir actual generalization capabilities compared to bag-of-words approaches.\nParticularly, we wonder if these shortcomings could (partly) be the consequence\nof the inability of neural IR models to perform lexical matching off-the-shelf.\nIn this work, we propose a measure of discrepancy between the lexical matching\nperformed by any (neural) model and an 'ideal' one. Based on this, we study the\nbehavior of different state-of-the-art neural IR models, focusing on whether\nthey are able to perform lexical matching when it's actually useful, i.e. for\nimportant terms. Overall, we show that neural IR models fail to properly\ngeneralize term importance on out-of-domain collections or terms almost unseen\nduring training", "published": "2021-12-10 16:49:49", "link": "http://arxiv.org/abs/2112.05662v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Pruning Pretrained Encoders with a Multitask Objective", "abstract": "The sizes of pretrained language models make them challenging and expensive\nto use when there are multiple desired downstream tasks. In this work, we adopt\nrecent strategies for model pruning during finetuning to explore the question\nof whether it is possible to prune a single encoder so that it can be used for\nmultiple tasks. We allocate a fixed parameter budget and compare pruning a\nsingle model with a multitask objective against the best ensemble of\nsingle-task models. We find that under two pruning strategies (element-wise and\nrank pruning), the approach with the multitask objective outperforms training\nmodels separately when averaged across all tasks, and it is competitive on each\nindividual one. Additional analysis finds that using a multitask objective\nduring pruning can also be an effective method for reducing model sizes for\nlow-resource tasks.", "published": "2021-12-10 17:57:33", "link": "http://arxiv.org/abs/2112.05705v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AtteSTNet -- An attention and subword tokenization based approach for\n  code-switched text hate speech detection", "abstract": "Recent advancements in technology have led to a boost in social media usage\nwhich has ultimately led to large amounts of user-generated data which also\nincludes hateful and offensive speech. The language used in social media is\noften a combination of English and the native language in the region. In India,\nHindi is used predominantly and is often code-switched with English, giving\nrise to the Hinglish (Hindi+English) language. Various approaches have been\nmade in the past to classify the code-mixed Hinglish hate speech using\ndifferent machine learning and deep learning-based techniques. However, these\ntechniques make use of recurrence on convolution mechanisms which are\ncomputationally expensive and have high memory requirements. Past techniques\nalso make use of complex data processing making the existing techniques very\ncomplex and non-sustainable to change in data. Proposed work gives a much\nsimpler approach which is not only at par with these complex networks but also\nexceeds performance with the use of subword tokenization algorithms like BPE\nand Unigram, along with multi-head attention-based techniques, giving an\naccuracy of 87.41% and an F1 score of 0.851 on standard datasets. Efficient use\nof BPE and Unigram algorithms help handle the nonconventional Hinglish\nvocabulary making the proposed technique simple, efficient and sustainable to\nuse in the real world.", "published": "2021-12-10 20:01:44", "link": "http://arxiv.org/abs/2112.11479v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sketching as a Tool for Understanding and Accelerating Self-attention\n  for Long Sequences", "abstract": "Transformer-based models are not efficient in processing long sequences due\nto the quadratic space and time complexity of the self-attention modules. To\naddress this limitation, Linformer and Informer are proposed to reduce the\nquadratic complexity to linear (modulo logarithmic factors) via low-dimensional\nprojection and row selection respectively. These two models are intrinsically\nconnected, and to understand their connection, we introduce a theoretical\nframework of matrix sketching. Based on the theoretical analysis, we propose\nSkeinformer to accelerate self-attention and further improve the accuracy of\nmatrix approximation to self-attention with three carefully designed\ncomponents: column sampling, adaptive row normalization and pilot sampling\nreutilization. Experiments on the Long Range Arena (LRA) benchmark demonstrate\nthat our methods outperform alternatives with a consistently smaller time/space\nfootprint.", "published": "2021-12-10 06:58:05", "link": "http://arxiv.org/abs/2112.05359v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "DEBACER: a method for slicing moderated debates", "abstract": "Subjects change frequently in moderated debates with several participants,\nsuch as in parliamentary sessions, electoral debates, and trials. Partitioning\na debate into blocks with the same subject is essential for understanding.\nOften a moderator is responsible for defining when a new block begins so that\nthe task of automatically partitioning a moderated debate can focus solely on\nthe moderator's behavior. In this paper, we (i) propose a new algorithm,\nDEBACER, which partitions moderated debates; (ii) carry out a comparative study\nbetween conventional and BERTimbau pipelines; and (iii) validate DEBACER\napplying it to the minutes of the Assembly of the Republic of Portugal. Our\nresults show the effectiveness of DEBACER. Keywords: Natural Language\nProcessing, Political Documents, Spoken Text Processing, Speech Split, Dialogue\nPartitioning.", "published": "2021-12-10 10:39:07", "link": "http://arxiv.org/abs/2112.05438v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Shennong: a Python toolbox for audio speech features extraction", "abstract": "We introduce Shennong, a Python toolbox and command-line utility for speech\nfeatures extraction. It implements a wide range of well-established state of\nart algorithms including spectro-temporal filters such as Mel-Frequency\nCepstral Filterbanks or Predictive Linear Filters, pre-trained neural networks,\npitch estimators as well as speaker normalization methods and post-processing\nalgorithms. Shennong is an open source, easy-to-use, reliable and extensible\nframework. The use of Python makes the integration to others speech modeling\nand machine learning tools easy. It aims to replace or complement several\nheterogeneous software, such as Kaldi or Praat. After describing the Shennong\nsoftware architecture, its core components and implemented algorithms, this\npaper illustrates its use on three applications: a comparison of speech\nfeatures performances on a phones discrimination task, an analysis of a Vocal\nTract Length Normalization model as a function of the speech duration used for\ntraining and a comparison of pitch estimation algorithms under various noise\nconditions.", "published": "2021-12-10 14:08:52", "link": "http://arxiv.org/abs/2112.05555v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Unified Multimodal Pre-training and Prompt-based Tuning for\n  Vision-Language Understanding and Generation", "abstract": "Most existing vision-language pre-training methods focus on understanding\ntasks and use BERT-like objectives (masked language modeling and image-text\nmatching) during pretraining. Although they perform well in many understanding\ndownstream tasks, e.g., visual question answering, image-text retrieval and\nvisual entailment, they do not possess the ability to generate. To tackle this\nproblem, we propose Unified multimodal pre-training for both Vision-Language\nunderstanding and generation (UniVL). The proposed UniVL is capable of handling\nboth understanding tasks and generative tasks. We augment existing pretraining\nparadigms that only use random masks with causal masks, i.e., triangular masks\nthat mask out future tokens, such that the pre-trained models can have\nautoregressive generation abilities by design. We formulate several previous\nunderstanding tasks as a text generation task and propose to use prompt-based\nmethod for fine-tuning on different downstream tasks. Our experiments show that\nthere is a trade-off between understanding tasks and generation tasks while\nusing the same model, and a feasible way to improve both tasks is to use more\ndata. Our UniVL framework attains comparable performance to recent\nvision-language pre-training methods on both understanding tasks and generation\ntasks. Moreover, we demostrate that prompt-based finetuning is more\ndata-efficient - it outperforms discriminative methods in few-shot scenarios.", "published": "2021-12-10 14:59:06", "link": "http://arxiv.org/abs/2112.05587v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Sampling from Discrete Energy-Based Models with Quality/Efficiency\n  Trade-offs", "abstract": "Energy-Based Models (EBMs) allow for extremely flexible specifications of\nprobability distributions. However, they do not provide a mechanism for\nobtaining exact samples from these distributions. Monte Carlo techniques can\naid us in obtaining samples if some proposal distribution that we can easily\nsample from is available. For instance, rejection sampling can provide exact\nsamples but is often difficult or impossible to apply due to the need to find a\nproposal distribution that upper-bounds the target distribution everywhere.\nApproximate Markov chain Monte Carlo sampling techniques like\nMetropolis-Hastings are usually easier to design, exploiting a local proposal\ndistribution that performs local edits on an evolving sample. However, these\ntechniques can be inefficient due to the local nature of the proposal\ndistribution and do not provide an estimate of the quality of their samples. In\nthis work, we propose a new approximate sampling technique, Quasi Rejection\nSampling (QRS), that allows for a trade-off between sampling efficiency and\nsampling quality, while providing explicit convergence bounds and diagnostics.\nQRS capitalizes on the availability of high-quality global proposal\ndistributions obtained from deep learning models. We demonstrate the\neffectiveness of QRS sampling for discrete EBMs over text for the tasks of\ncontrolled text generation with distributional constraints and paraphrase\ngeneration. We show that we can sample from such EBMs with arbitrary precision\nat the cost of sampling efficiency.", "published": "2021-12-10 17:51:37", "link": "http://arxiv.org/abs/2112.05702v1", "categories": ["cs.LG", "cs.CL", "cs.NE"], "primary_category": "cs.LG"}
{"title": "Discourse-Aware Soft Prompting for Text Generation", "abstract": "Current efficient fine-tuning methods (e.g., adapters, prefix-tuning, etc.)\nhave optimized conditional text generation via training a small set of extra\nparameters of the neural language model, while freezing the rest for\nefficiency. While showing strong performance on some generation tasks, they\ndon't generalize across all generation tasks. We show that soft-prompt based\nconditional text generation can be improved with simple and efficient methods\nthat simulate modeling the discourse structure of human written text. We\ninvestigate two design choices: First, we apply \\textit{hierarchical blocking}\non the prefix parameters to simulate a higher-level discourse structure of\nhuman written text. Second, we apply \\textit{attention sparsity} on the prefix\nparameters at different layers of the network and learn sparse transformations\non the softmax-function. We show that structured design of prefix parameters\nyields more coherent, faithful and relevant generations than the baseline\nprefix-tuning on all generation tasks.", "published": "2021-12-10 18:15:44", "link": "http://arxiv.org/abs/2112.05717v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "TempoQR: Temporal Question Reasoning over Knowledge Graphs", "abstract": "Knowledge Graph Question Answering (KGQA) involves retrieving facts from a\nKnowledge Graph (KG) using natural language queries. A KG is a curated set of\nfacts consisting of entities linked by relations. Certain facts include also\ntemporal information forming a Temporal KG (TKG). Although many natural\nquestions involve explicit or implicit time constraints, question answering\n(QA) over TKGs has been a relatively unexplored area. Existing solutions are\nmainly designed for simple temporal questions that can be answered directly by\na single TKG fact. This paper puts forth a comprehensive embedding-based\nframework for answering complex questions over TKGs. Our method termed temporal\nquestion reasoning (TempoQR) exploits TKG embeddings to ground the question to\nthe specific entities and time scope it refers to. It does so by augmenting the\nquestion embeddings with context, entity and time-aware information by\nemploying three specialized modules. The first computes a textual\nrepresentation of a given question, the second combines it with the entity\nembeddings for entities involved in the question, and the third generates\nquestion-specific time embeddings. Finally, a transformer-based encoder learns\nto fuse the generated temporal information with the question representation,\nwhich is used for answer predictions. Extensive experiments show that TempoQR\nimproves accuracy by 25--45 percentage points on complex temporal questions\nover state-of-the-art approaches and it generalizes better to unseen question\ntypes.", "published": "2021-12-10 23:59:14", "link": "http://arxiv.org/abs/2112.05785v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Computer-Assisted Creation of Boolean Search Rules for Text\n  Classification in the Legal Domain", "abstract": "In this paper, we present a method of building strong, explainable\nclassifiers in the form of Boolean search rules. We developed an interactive\nenvironment called CASE (Computer Assisted Semantic Exploration) which exploits\nword co-occurrence to guide human annotators in selection of relevant search\nterms. The system seamlessly facilitates iterative evaluation and improvement\nof the classification rules. The process enables the human annotators to\nleverage the benefits of statistical information while incorporating their\nexpert intuition into the creation of such rules. We evaluate classifiers\ncreated with our CASE system on 4 datasets, and compare the results to machine\nlearning methods, including SKOPE rules, Random forest, Support Vector Machine,\nand fastText classifiers. The results drive the discussion on trade-offs\nbetween superior compactness, simplicity, and intuitiveness of the Boolean\nsearch rules versus the better performance of state-of-the-art machine learning\nmodels for text classification.", "published": "2021-12-10 19:53:41", "link": "http://arxiv.org/abs/2112.05807v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "Building a great multi-lingual teacher with sparsely-gated mixture of\n  experts for speech recognition", "abstract": "The sparsely-gated Mixture of Experts (MoE) can magnify a network capacity\nwith a little computational complexity. In this work, we investigate how\nmulti-lingual Automatic Speech Recognition (ASR) networks can be scaled up with\na simple routing algorithm in order to achieve better accuracy. More\nspecifically, we apply the sparsely-gated MoE technique to two types of\nnetworks: Sequence-to-Sequence Transformer (S2S-T) and Transformer Transducer\n(T-T). We demonstrate through a set of ASR experiments on multiple language\ndata that the MoE networks can reduce the relative word error rates by 16.3%\nand 4.6% with the S2S-T and T-T, respectively. Moreover, we thoroughly\ninvestigate the effect of the MoE on the T-T architecture in various\nconditions: streaming mode, non-streaming mode, the use of language ID and the\nlabel decoder with the MoE.", "published": "2021-12-10 20:37:03", "link": "http://arxiv.org/abs/2112.05820v3", "categories": ["cs.CL", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Sequence-level self-learning with multiple hypotheses", "abstract": "In this work, we develop new self-learning techniques with an attention-based\nsequence-to-sequence (seq2seq) model for automatic speech recognition (ASR).\nFor untranscribed speech data, the hypothesis from an ASR system must be used\nas a label. However, the imperfect ASR result makes unsupervised learning\ndifficult to consistently improve recognition performance especially in the\ncase that multiple powerful teacher models are unavailable. In contrast to\nconventional unsupervised learning approaches, we adopt the \\emph{multi-task\nlearning} (MTL) framework where the $n$-th best ASR hypothesis is used as the\nlabel of each task. The seq2seq network is updated through the MTL framework so\nas to find the common representation that can cover multiple hypotheses. By\ndoing so, the effect of the \\emph{hard-decision} errors can be alleviated.\n  We first demonstrate the effectiveness of our self-learning methods through\nASR experiments in an accent adaptation task between the US and British English\nspeech. Our experiment results show that our method can reduce the WER on the\nBritish speech data from 14.55\\% to 10.36\\% compared to the baseline model\ntrained with the US English data only. Moreover, we investigate the effect of\nour proposed methods in a federated learning scenario.", "published": "2021-12-10 20:47:58", "link": "http://arxiv.org/abs/2112.05826v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Revisiting the Boundary between ASR and NLU in the Age of Conversational\n  Dialog Systems", "abstract": "As more users across the world are interacting with dialog agents in their\ndaily life, there is a need for better speech understanding that calls for\nrenewed attention to the dynamics between research in automatic speech\nrecognition (ASR) and natural language understanding (NLU). We briefly review\nthese research areas and lay out the current relationship between them. In\nlight of the observations we make in this paper, we argue that (1) NLU should\nbe cognizant of the presence of ASR models being used upstream in a dialog\nsystem's pipeline, (2) ASR should be able to learn from errors found in NLU,\n(3) there is a need for end-to-end datasets that provide semantic annotations\non spoken input, (4) there should be stronger collaboration between ASR and NLU\nresearch communities.", "published": "2021-12-10 21:54:20", "link": "http://arxiv.org/abs/2112.05842v1", "categories": ["cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "LSH methods for data deduplication in a Wikipedia artificial dataset", "abstract": "This paper illustrates locality sensitive hasing (LSH) models for the\nidentification and removal of nearly redundant data in a text dataset. To\nevaluate the different models, we create an artificial dataset for data\ndeduplication using English Wikipedia articles. Area-Under-Curve (AUC) over 0.9\nwere observed for most models, with the best model reaching 0.96. Deduplication\nenables more effective model training by preventing the model from learning a\ndistribution that differs from the real one as a result of the repeated data.", "published": "2021-12-10 20:01:26", "link": "http://arxiv.org/abs/2112.11478v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Predicting Physical World Destinations for Commands Given to\n  Self-Driving Cars", "abstract": "In recent years, we have seen significant steps taken in the development of\nself-driving cars. Multiple companies are starting to roll out impressive\nsystems that work in a variety of settings. These systems can sometimes give\nthe impression that full self-driving is just around the corner and that we\nwould soon build cars without even a steering wheel. The increase in the level\nof autonomy and control given to an AI provides an opportunity for new modes of\nhuman-vehicle interaction. However, surveys have shown that giving more control\nto an AI in self-driving cars is accompanied by a degree of uneasiness by\npassengers. In an attempt to alleviate this issue, recent works have taken a\nnatural language-oriented approach by allowing the passenger to give commands\nthat refer to specific objects in the visual scene. Nevertheless, this is only\nhalf the task as the car should also understand the physical destination of the\ncommand, which is what we focus on in this paper. We propose an extension in\nwhich we annotate the 3D destination that the car needs to reach after\nexecuting the given command and evaluate multiple different baselines on\npredicting this destination location. Additionally, we introduce a model that\noutperforms the prior works adapted for this particular setting.", "published": "2021-12-10 09:51:16", "link": "http://arxiv.org/abs/2112.05419v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Directed Speech Separation for Automatic Speech Recognition of Long Form\n  Conversational Speech", "abstract": "Many of the recent advances in speech separation are primarily aimed at\nsynthetic mixtures of short audio utterances with high degrees of overlap. Most\nof these approaches need an additional stitching step to stitch the separated\nspeech chunks for long form audio. Since most of the approaches involve\nPermutation Invariant training (PIT), the order of separated speech chunks is\nnondeterministic and leads to difficulty in accurately stitching homogenous\nspeaker chunks for downstream tasks like Automatic Speech Recognition (ASR).\nAlso, most of these models are trained with synthetic mixtures and do not\ngeneralize to real conversational data. In this paper, we propose a speaker\nconditioned separator trained on speaker embeddings extracted directly from the\nmixed signal using an over-clustering based approach. This model naturally\nregulates the order of the separated chunks without the need for an additional\nstitching step. We also introduce a data sampling strategy with real and\nsynthetic mixtures which generalizes well to real conversation speech. With\nthis model and data sampling technique, we show significant improvements in\nspeaker-attributed word error rate (SA-WER) on Hub5 data.", "published": "2021-12-10 23:07:48", "link": "http://arxiv.org/abs/2112.05863v3", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "An Ensemble 1D-CNN-LSTM-GRU Model with Data Augmentation for Speech\n  Emotion Recognition", "abstract": "In this paper, we propose an ensemble of deep neural networks along with data\naugmentation (DA) learned using effective speech-based features to recognize\nemotions from speech. Our ensemble model is built on three deep neural\nnetwork-based models. These neural networks are built using the basic local\nfeature acquiring blocks (LFAB) which are consecutive layers of dilated 1D\nConvolutional Neural networks followed by the max pooling and batch\nnormalization layers. To acquire the long-term dependencies in speech signals\nfurther two variants are proposed by adding Gated Recurrent Unit (GRU) and Long\nShort Term Memory (LSTM) layers respectively. All three network models have\nconsecutive fully connected layers before the final softmax layer for\nclassification. The ensemble model uses a weighted average to provide the final\nclassification. We have utilized five standard benchmark datasets: TESS,\nEMO-DB, RAVDESS, SAVEE, and CREMA-D for evaluation. We have performed DA by\ninjecting Additive White Gaussian Noise, pitch shifting, and stretching the\nsignal level to generalize the models, and thus increasing the accuracy of the\nmodels and reducing the overfitting as well. We handcrafted five categories of\nfeatures: Mel-frequency cepstral coefficients, Log Mel-Scaled Spectrogram,\nZero-Crossing Rate, Chromagram, and statistical Root Mean Square Energy value\nfrom each audio sample. These features are used as the input to the LFAB blocks\nthat further extract the hidden local features which are then fed to either\nfully connected layers or to LSTM or GRU based on the model type to acquire the\nadditional long-term contextual representations. LFAB followed by GRU or LSTM\nresults in better performance compared to the baseline model. The ensemble\nmodel achieves the state-of-the-art weighted average accuracy in all the\ndatasets.", "published": "2021-12-10 16:57:53", "link": "http://arxiv.org/abs/2112.05666v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Learning-based personal speech enhancement for teleconferencing by\n  exploiting spatial-spectral features", "abstract": "Teleconferencing is becoming essential during the COVID-19 pandemic. However,\nin real-world applications, speech quality can deteriorate due to, for example,\nbackground interference, noise, or reverberation. To solve this problem, target\nspeech extraction from the mixture signals can be performed with the aid of the\nuser's vocal features. Various features are accounted for in this study's\nproposed system, including speaker embeddings derived from user enrollment and\na novel long-short-term spatial coherence feature pertaining to the target\nspeaker activity. As a learning-based approach, a target speech sifting network\nwas employed to extract the relevant features. The network trained with LSTSC\nin the proposed approach is robust to microphone array geometries and the\nnumber of microphones. Furthermore, the proposed enhancement system was\ncompared with a baseline system with speaker embeddings and interchannel phase\ndifference. The results demonstrated the superior performance of the proposed\nsystem over the baseline in enhancement performance and robustness.", "published": "2021-12-10 17:31:10", "link": "http://arxiv.org/abs/2112.05686v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Mean-square-error-based secondary source placement in sound field\n  synthesis with prior information on desired field", "abstract": "A method of optimizing secondary source placement in sound field synthesis is\nproposed. Such an optimization method will be useful when the allowable\nplacement region and available number of loudspeakers are limited. We formulate\na mean-square-error-based cost function, incorporating the statistical\nproperties of possible desired sound fields, for general\nlinear-least-squares-based sound field synthesis methods, including pressure\nmatching and (weighted) mode matching, whereas most of the current methods are\napplicable only to the pressure-matching method. An efficient greedy algorithm\nfor minimizing the proposed cost function is also derived. Numerical\nexperiments indicated that a high reproduction accuracy can be achieved by the\nplacement optimized by the proposed method compared with the empirically used\nregular placement.", "published": "2021-12-10 02:06:36", "link": "http://arxiv.org/abs/2112.06774v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
