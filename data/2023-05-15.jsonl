{"title": "From Pretraining Data to Language Models to Downstream Tasks: Tracking\n  the Trails of Political Biases Leading to Unfair NLP Models", "abstract": "Language models (LMs) are pretrained on diverse data sources, including news,\ndiscussion forums, books, and online encyclopedias. A significant portion of\nthis data includes opinions and perspectives which, on one hand, celebrate\ndemocracy and diversity of ideas, and on the other hand are inherently socially\nbiased. Our work develops new methods to (1) measure political biases in LMs\ntrained on such corpora, along social and economic axes, and (2) measure the\nfairness of downstream NLP models trained on top of politically biased LMs. We\nfocus on hate speech and misinformation detection, aiming to empirically\nquantify the effects of political (social, economic) biases in pretraining data\non the fairness of high-stakes social-oriented tasks. Our findings reveal that\npretrained LMs do have political leanings that reinforce the polarization\npresent in pretraining corpora, propagating social biases into hate speech\npredictions and misinformation detectors. We discuss the implications of our\nfindings for NLP research and propose future directions to mitigate unfairness.", "published": "2023-05-15 00:06:30", "link": "http://arxiv.org/abs/2305.08283v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Symbol tuning improves in-context learning in language models", "abstract": "We present symbol tuning - finetuning language models on in-context\ninput-label pairs where natural language labels (e.g., \"positive/negative\nsentiment\") are replaced with arbitrary symbols (e.g., \"foo/bar\"). Symbol\ntuning leverages the intuition that when a model cannot use instructions or\nnatural language labels to figure out a task, it must instead do so by learning\nthe input-label mappings.\n  We experiment with symbol tuning across Flan-PaLM models up to 540B\nparameters and observe benefits across various settings. First, symbol tuning\nboosts performance on unseen in-context learning tasks and is much more robust\nto underspecified prompts, such as those without instructions or without\nnatural language labels. Second, symbol-tuned models are much stronger at\nalgorithmic reasoning tasks, with up to 18.2% better performance on the List\nFunctions benchmark and up to 15.3% better performance on the Simple Turing\nConcepts benchmark. Finally, symbol-tuned models show large improvements in\nfollowing flipped-labels presented in-context, meaning that they are more\ncapable of using in-context information to override prior semantic knowledge.", "published": "2023-05-15 01:59:58", "link": "http://arxiv.org/abs/2305.08298v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"Nothing Abnormal\": Disambiguating Medical Reports via Contrastive\n  Knowledge Infusion", "abstract": "Sharing medical reports is essential for patient-centered care. A recent line\nof work has focused on automatically generating reports with NLP methods.\nHowever, different audiences have different purposes when writing/reading\nmedical reports -- for example, healthcare professionals care more about\npathology, whereas patients are more concerned with the diagnosis (\"Is there\nany abnormality?\"). The expectation gap results in a common situation where\npatients find their medical reports to be ambiguous and therefore unsure about\nthe next steps. In this work, we explore the audience expectation gap in\nhealthcare and summarize common ambiguities that lead patients to be confused\nabout their diagnosis into three categories: medical jargon, contradictory\nfindings, and misleading grammatical errors. Based on our analysis, we define a\ndisambiguation rewriting task to regenerate an input to be unambiguous while\npreserving information about the original content. We further propose a\nrewriting algorithm based on contrastive pretraining and perturbation-based\nrewriting. In addition, we create two datasets, OpenI-Annotated based on chest\nreports and VA-Annotated based on general medical reports, with available\nbinary labels for ambiguity and abnormality presence annotated by radiology\nspecialists. Experimental results on these datasets show that our proposed\nalgorithm effectively rewrites input sentences in a less ambiguous way with\nhigh content fidelity. Our code and annotated data are released to facilitate\nfuture research.", "published": "2023-05-15 02:01:20", "link": "http://arxiv.org/abs/2305.08300v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for\n  Foundation Models", "abstract": "New NLP benchmarks are urgently needed to align with the rapid development of\nlarge language models (LLMs). We present C-Eval, the first comprehensive\nChinese evaluation suite designed to assess advanced knowledge and reasoning\nabilities of foundation models in a Chinese context. C-Eval comprises\nmultiple-choice questions across four difficulty levels: middle school, high\nschool, college, and professional. The questions span 52 diverse disciplines,\nranging from humanities to science and engineering. C-Eval is accompanied by\nC-Eval Hard, a subset of very challenging subjects in C-Eval that requires\nadvanced reasoning abilities to solve. We conduct a comprehensive evaluation of\nthe most advanced LLMs on C-Eval, including both English- and Chinese-oriented\nmodels. Results indicate that only GPT-4 could achieve an average accuracy of\nover 60%, suggesting that there is still significant room for improvement for\ncurrent LLMs. We anticipate C-Eval will help analyze important strengths and\nshortcomings of foundation models, and foster their development and growth for\nChinese users.", "published": "2023-05-15 03:20:19", "link": "http://arxiv.org/abs/2305.08322v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Coreference-aware Double-channel Attention Network for Multi-party\n  Dialogue Reading Comprehension", "abstract": "We tackle Multi-party Dialogue Reading Comprehension (abbr., MDRC). MDRC\nstands for an extractive reading comprehension task grounded on a batch of\ndialogues among multiple interlocutors. It is challenging due to the\nrequirement of understanding cross-utterance contexts and relationships in a\nmulti-turn multi-party conversation. Previous studies have made great efforts\non the utterance profiling of a single interlocutor and graph-based interaction\nmodeling. The corresponding solutions contribute to the answer-oriented\nreasoning on a series of well-organized and thread-aware conversational\ncontexts. However, the current MDRC models still suffer from two bottlenecks.\nOn the one hand, a pronoun like \"it\" most probably produces multi-skip\nreasoning throughout the utterances of different interlocutors. On the other\nhand, an MDRC encoder is potentially puzzled by fuzzy features, i.e., the\nmixture of inner linguistic features in utterances and external interactive\nfeatures among utterances. To overcome the bottlenecks, we propose a\ncoreference-aware attention modeling method to strengthen the reasoning\nability. In addition, we construct a two-channel encoding network. It\nseparately encodes utterance profiles and interactive relationships, so as to\nrelieve the confusion among heterogeneous features. We experiment on the\nbenchmark corpora Molweni and FriendsQA. Experimental results demonstrate that\nour approach yields substantial improvements on both corpora, compared to the\nfine-tuned BERT and ELECTRA baselines. The maximum performance gain is about\n2.5\\% F1-score. Besides, our MDRC models outperform the state-of-the-art in\nmost cases.", "published": "2023-05-15 05:01:29", "link": "http://arxiv.org/abs/2305.08348v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SuperDialseg: A Large-scale Dataset for Supervised Dialogue Segmentation", "abstract": "Dialogue segmentation is a crucial task for dialogue systems allowing a\nbetter understanding of conversational texts. Despite recent progress in\nunsupervised dialogue segmentation methods, their performances are limited by\nthe lack of explicit supervised signals for training. Furthermore, the precise\ndefinition of segmentation points in conversations still remains as a\nchallenging problem, increasing the difficulty of collecting manual\nannotations. In this paper, we provide a feasible definition of dialogue\nsegmentation points with the help of document-grounded dialogues and release a\nlarge-scale supervised dataset called SuperDialseg, containing 9,478 dialogues\nbased on two prevalent document-grounded dialogue corpora, and also inherit\ntheir useful dialogue-related annotations. Moreover, we provide a benchmark\nincluding 18 models across five categories for the dialogue segmentation task\nwith several proper evaluation metrics. Empirical studies show that supervised\nlearning is extremely effective in in-domain datasets and models trained on\nSuperDialseg can achieve good generalization ability on out-of-domain data.\nAdditionally, we also conducted human verification on the test set and the\nKappa score confirmed the quality of our automatically constructed dataset. We\nbelieve our work is an important step forward in the field of dialogue\nsegmentation. Our codes and data can be found from:\nhttps://github.com/Coldog2333/SuperDialseg.", "published": "2023-05-15 06:08:01", "link": "http://arxiv.org/abs/2305.08371v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text Classification via Large Language Models", "abstract": "Despite the remarkable success of large-scale Language Models (LLMs) such as\nGPT-3, their performances still significantly underperform fine-tuned models in\nthe task of text classification. This is due to (1) the lack of reasoning\nability in addressing complex linguistic phenomena (e.g., intensification,\ncontrast, irony etc); (2) limited number of tokens allowed in in-context\nlearning.\n  In this paper, we introduce Clue And Reasoning Prompting (CARP). CARP adopts\na progressive reasoning strategy tailored to addressing the complex linguistic\nphenomena involved in text classification: CARP first prompts LLMs to find\nsuperficial clues (e.g., keywords, tones, semantic relations, references, etc),\nbased on which a diagnostic reasoning process is induced for final decisions.\nTo further address the limited-token issue, CARP uses a fine-tuned model on the\nsupervised dataset for $k$NN demonstration search in the in-context learning,\nallowing the model to take the advantage of both LLM's generalization ability\nand the task-specific evidence provided by the full labeled dataset.\nRemarkably, CARP yields new SOTA performances on 4 out of 5 widely-used\ntext-classification benchmarks, 97.39 (+1.24) on SST-2, 96.40 (+0.72) on\nAGNews, 98.78 (+0.25) on R8 and 96.95 (+0.6) on R52, and a performance\ncomparable to SOTA on MR (92.39 v.s. 93.3). More importantly, we find that CARP\ndelivers impressive abilities on low-resource and domain-adaptation setups.\nSpecifically, using 16 examples per class, CARP achieves comparable\nperformances to supervised models with 1,024 examples per class.", "published": "2023-05-15 06:24:45", "link": "http://arxiv.org/abs/2305.08377v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Impact of Incumbent/Opposition Status and Ideological Similitude on\n  Emotions in Political Manifestos", "abstract": "The study involved the analysis of emotion-associated language in the UK\nConservative and Labour party general election manifestos between 2000 to 2019.\nWhile previous research have shown a general correlation between ideological\npositioning and overlap of public policies, there are still conflicting results\nin matters of sentiments in such manifestos. Using new data, we present how\nvalence level can be swayed by party status within government with incumbent\nparties presenting a higher frequency in positive emotion-associated words\nwhile negative emotion-associated words are more prevalent in opposition\nparties. We also demonstrate that parties with ideological similitude use\npositive language prominently further adding to the literature on the\nrelationship between sentiments and party status.", "published": "2023-05-15 06:43:44", "link": "http://arxiv.org/abs/2305.08383v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Uncovering the Potential of ChatGPT for Discourse Analysis in Dialogue:\n  An Empirical Study", "abstract": "Large language models, like ChatGPT, have shown remarkable capability in many\ndownstream tasks, yet their ability to understand discourse structures of\ndialogues remains less explored, where it requires higher level capabilities of\nunderstanding and reasoning. In this paper, we aim to systematically inspect\nChatGPT's performance in two discourse analysis tasks: topic segmentation and\ndiscourse parsing, focusing on its deep semantic understanding of linear and\nhierarchical discourse structures underlying dialogue. To instruct ChatGPT to\ncomplete these tasks, we initially craft a prompt template consisting of the\ntask description, output format, and structured input. Then, we conduct\nexperiments on four popular topic segmentation datasets and two discourse\nparsing datasets. The experimental results showcase that ChatGPT demonstrates\nproficiency in identifying topic structures in general-domain conversations yet\nstruggles considerably in specific-domain conversations. We also found that\nChatGPT hardly understands rhetorical structures that are more complex than\ntopic structures. Our deeper investigation indicates that ChatGPT can give more\nreasonable topic structures than human annotations but only linearly parses the\nhierarchical rhetorical structures. In addition, we delve into the impact of\nin-context learning (e.g., chain-of-thought) on ChatGPT and conduct the\nablation study on various prompt components, which can provide a research\nfoundation for future work. The code is available at\n\\url{https://github.com/yxfanSuda/GPTforDDA}.", "published": "2023-05-15 07:14:41", "link": "http://arxiv.org/abs/2305.08391v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Legal Extractive Summarization of U.S. Court Opinions", "abstract": "This paper tackles the task of legal extractive summarization using a dataset\nof 430K U.S. court opinions with key passages annotated. According to automated\nsummary quality metrics, the reinforcement-learning-based MemSum model is best\nand even out-performs transformer-based models. In turn, expert human\nevaluation shows that MemSum summaries effectively capture the key points of\nlengthy court opinions. Motivated by these results, we open-source our models\nto the general public. This represents progress towards democratizing law and\nmaking U.S. court opinions more accessible to the general public.", "published": "2023-05-15 08:13:00", "link": "http://arxiv.org/abs/2305.08428v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EMBRACE: Evaluation and Modifications for Boosting RACE", "abstract": "When training and evaluating machine reading comprehension models, it is very\nimportant to work with high-quality datasets that are also representative of\nreal-world reading comprehension tasks. This requirement includes, for\ninstance, having questions that are based on texts of different genres and\nrequire generating inferences or reflecting on the reading material.\n  In this article we turn our attention to RACE, a dataset of English texts and\ncorresponding multiple-choice questions (MCQs). Each MCQ consists of a question\nand four alternatives (of which one is the correct answer). RACE was\nconstructed by Chinese teachers of English for human reading comprehension and\nis widely used as training material for machine reading comprehension models.\nBy construction, RACE should satisfy the aforementioned quality requirements\nand the purpose of this article is to check whether they are indeed satisfied.\n  We provide a detailed analysis of the test set of RACE for high-school\nstudents (1045 texts and 3498 corresponding MCQs) including (1) an evaluation\nof the difficulty of each MCQ and (2) annotations for the relevant pieces of\nthe texts (called \"bases\") that are used to justify the plausibility of each\nalternative. A considerable number of MCQs appear not to fulfill basic\nrequirements for this type of reading comprehension tasks, so we additionally\nidentify the high-quality subset of the evaluated RACE corpus. We also\ndemonstrate that the distribution of the positions of the bases for the\nalternatives is biased towards certain parts of texts, which is not necessarily\ndesirable when evaluating MCQ answering and generation models.", "published": "2023-05-15 08:21:32", "link": "http://arxiv.org/abs/2305.08433v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Crosslingual Investigation of Conceptualization in 1335 Languages", "abstract": "Languages differ in how they divide up the world into concepts and words;\ne.g., in contrast to English, Swahili has a single concept for `belly' and\n`womb'. We investigate these differences in conceptualization across 1,335\nlanguages by aligning concepts in a parallel corpus. To this end, we propose\nConceptualizer, a method that creates a bipartite directed alignment graph\nbetween source language concepts and sets of target language strings. In a\ndetailed linguistic analysis across all languages for one concept (`bird') and\nan evaluation on gold standard data for 32 Swadesh concepts, we show that\nConceptualizer has good alignment accuracy. We demonstrate the potential of\nresearch on conceptualization in NLP with two experiments. (1) We define\ncrosslingual stability of a concept as the degree to which it has 1-1\ncorrespondences across languages, and show that concreteness predicts\nstability. (2) We represent each language by its conceptualization pattern for\n83 concepts, and define a similarity measure on these representations. The\nresulting measure for the conceptual similarity of two languages is\ncomplementary to standard genealogical, typological, and surface similarity\nmeasures. For four out of six language families, we can assign languages to\ntheir correct family based on conceptual similarity with accuracy between 54%\nand 87%.", "published": "2023-05-15 09:27:34", "link": "http://arxiv.org/abs/2305.08475v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Taxi1500: A Multilingual Dataset for Text Classification in 1500\n  Languages", "abstract": "While natural language processing tools have been developed extensively for\nsome of the world's languages, a significant portion of the world's over 7000\nlanguages are still neglected. One reason for this is that evaluation datasets\ndo not yet cover a wide range of languages, including low-resource and\nendangered ones. We aim to address this issue by creating a text classification\ndataset encompassing a large number of languages, many of which currently have\nlittle to no annotated data available. We leverage parallel translations of the\nBible to construct such a dataset by first developing applicable topics and\nemploying a crowdsourcing tool to collect annotated data. By annotating the\nEnglish side of the data and projecting the labels onto other languages through\naligned verses, we generate text classification datasets for more than 1500\nlanguages. We extensively benchmark several existing multilingual language\nmodels using our dataset. To facilitate the advancement of research in this\narea, we will release our dataset and code.", "published": "2023-05-15 09:43:32", "link": "http://arxiv.org/abs/2305.08487v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Creative Data Generation: A Review Focusing on Text and Poetry", "abstract": "The rapid advancement in machine learning has led to a surge in automatic\ndata generation, making it increasingly challenging to differentiate between\nnaturally or human-generated data and machine-generated data. Despite these\nadvancements, the generation of creative data remains a challenge. This paper\naims to investigate and comprehend the essence of creativity, both in general\nand within the context of natural language generation. We review various\napproaches to creative writing devices and tasks, with a specific focus on the\ngeneration of poetry. We aim to shed light on the challenges and opportunities\nin the field of creative data generation.", "published": "2023-05-15 09:50:15", "link": "http://arxiv.org/abs/2305.08493v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Hierarchical Encoding-Decoding Scheme for Abstractive Multi-document\n  Summarization", "abstract": "Pre-trained language models (PLMs) have achieved outstanding achievements in\nabstractive single-document summarization (SDS). However, such benefits may not\nfully extend to multi-document summarization (MDS), where the handling of\ncross-document information is more complex. Previous works either design new\nMDS architectures or apply PLMs bluntly with concatenated source documents as a\nreformulated SDS task. While the former does not utilize previous pre-training\nefforts and may not generalize well across different domains, the latter may\nnot sufficiently attend to the intricate cross-document relationships unique to\nMDS tasks. Instead, we enforce hierarchy on both the encoder and decoder to\nbetter utilize a PLM to facilitate multi-document interactions for the MDS\ntask. Across 10 MDS benchmarks from various domains, our method outperforms or\nis competitive with the previous best models, including those with additional\nMDS pre-training or with more parameters. It outperforms its corresponding PLM\nbackbone by up to 3 Rouge-L and is favored by humans.", "published": "2023-05-15 10:03:31", "link": "http://arxiv.org/abs/2305.08503v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NLG Evaluation Metrics Beyond Correlation Analysis: An Empirical Metric\n  Preference Checklist", "abstract": "In this study, we analyze automatic evaluation metrics for Natural Language\nGeneration (NLG), specifically task-agnostic metrics and human-aligned metrics.\nTask-agnostic metrics, such as Perplexity, BLEU, BERTScore, are cost-effective\nand highly adaptable to diverse NLG tasks, yet they have a weak correlation\nwith human. Human-aligned metrics (CTC, CtrlEval, UniEval) improves correlation\nlevel by incorporating desirable human-like qualities as training objective.\nHowever, their effectiveness at discerning system-level performance and quality\nof system outputs remain unclear.\n  We present metric preference checklist as a framework to assess the\neffectiveness of automatic metrics in three NLG tasks: Text Summarization,\nDialogue Response Generation, and Controlled Generation. Our proposed framework\nprovides access: (i) for verifying whether automatic metrics are faithful to\nhuman preference, regardless of their correlation level to human; and (ii) for\ninspecting the strengths and limitations of NLG systems via pairwise\nevaluation. We show that automatic metrics provide a better guidance than human\non discriminating system-level performance in Text Summarization and Controlled\nGeneration tasks. We also show that multi-aspect human-aligned metric (UniEval)\nis not necessarily dominant over single-aspect human-aligned metrics (CTC,\nCtrlEval) and task-agnostic metrics (BLEU, BERTScore), particularly in\nControlled Generation tasks.", "published": "2023-05-15 11:51:55", "link": "http://arxiv.org/abs/2305.08566v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Estimating the Causal Effects of Natural Logic Features in Neural NLI\n  Models", "abstract": "Rigorous evaluation of the causal effects of semantic features on language\nmodel predictions can be hard to achieve for natural language reasoning\nproblems. However, this is such a desirable form of analysis from both an\ninterpretability and model evaluation perspective, that it is valuable to zone\nin on specific patterns of reasoning with enough structure and regularity to be\nable to identify and quantify systematic reasoning failures in widely-used\nmodels. In this vein, we pick a portion of the NLI task for which an explicit\ncausal diagram can be systematically constructed: in particular, the case where\nacross two sentences (the premise and hypothesis), two related words/terms\noccur in a shared context. In this work, we apply causal effect estimation\nstrategies to measure the effect of context interventions (whose effect on the\nentailment label is mediated by the semantic monotonicity characteristic) and\ninterventions on the inserted word-pair (whose effect on the entailment label\nis mediated by the relation between these words.). Following related work on\ncausal analysis of NLP models in different settings, we adapt the methodology\nfor the NLI task to construct comparative model profiles in terms of robustness\nto irrelevant changes and sensitivity to impactful changes.", "published": "2023-05-15 12:01:09", "link": "http://arxiv.org/abs/2305.08572v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DarkBERT: A Language Model for the Dark Side of the Internet", "abstract": "Recent research has suggested that there are clear differences in the\nlanguage used in the Dark Web compared to that of the Surface Web. As studies\non the Dark Web commonly require textual analysis of the domain, language\nmodels specific to the Dark Web may provide valuable insights to researchers.\nIn this work, we introduce DarkBERT, a language model pretrained on Dark Web\ndata. We describe the steps taken to filter and compile the text data used to\ntrain DarkBERT to combat the extreme lexical and structural diversity of the\nDark Web that may be detrimental to building a proper representation of the\ndomain. We evaluate DarkBERT and its vanilla counterpart along with other\nwidely used language models to validate the benefits that a Dark Web domain\nspecific model offers in various use cases. Our evaluations show that DarkBERT\noutperforms current language models and may serve as a valuable resource for\nfuture research on the Dark Web.", "published": "2023-05-15 12:23:10", "link": "http://arxiv.org/abs/2305.08596v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Unsupervised Semantic Variation Prediction using the Distribution of\n  Sibling Embeddings", "abstract": "Languages are dynamic entities, where the meanings associated with words\nconstantly change with time. Detecting the semantic variation of words is an\nimportant task for various NLP applications that must make time-sensitive\npredictions. Existing work on semantic variation prediction have predominantly\nfocused on comparing some form of an averaged contextualised representation of\na target word computed from a given corpus. However, some of the previously\nassociated meanings of a target word can become obsolete over time (e.g.\nmeaning of gay as happy), while novel usages of existing words are observed\n(e.g. meaning of cell as a mobile phone). We argue that mean representations\nalone cannot accurately capture such semantic variations and propose a method\nthat uses the entire cohort of the contextualised embeddings of the target\nword, which we refer to as the sibling distribution. Experimental results on\nSemEval-2020 Task 1 benchmark dataset for semantic variation prediction show\nthat our method outperforms prior work that consider only the mean embeddings,\nand is comparable to the current state-of-the-art. Moreover, a qualitative\nanalysis shows that our method detects important semantic changes in words that\nare not captured by the existing methods. Source code is available at\nhttps://github.com/a1da4/svp-gauss .", "published": "2023-05-15 13:58:21", "link": "http://arxiv.org/abs/2305.08654v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Sentence Representation Learning with Frequency-induced\n  Adversarial Tuning and Incomplete Sentence Filtering", "abstract": "Pre-trained Language Model (PLM) is nowadays the mainstay of Unsupervised\nSentence Representation Learning (USRL). However, PLMs are sensitive to the\nfrequency information of words from their pre-training corpora, resulting in\nanisotropic embedding space, where the embeddings of high-frequency words are\nclustered but those of low-frequency words disperse sparsely. This anisotropic\nphenomenon results in two problems of similarity bias and information bias,\nlowering the quality of sentence embeddings. To solve the problems, we\nfine-tune PLMs by leveraging the frequency information of words and propose a\nnovel USRL framework, namely Sentence Representation Learning with\nFrequency-induced Adversarial tuning and Incomplete sentence filtering\n(SLT-FAI). We calculate the word frequencies over the pre-training corpora of\nPLMs and assign words thresholding frequency labels. With them, (1) we\nincorporate a similarity discriminator used to distinguish the embeddings of\nhigh-frequency and low-frequency words, and adversarially tune the PLM with it,\nenabling to achieve uniformly frequency-invariant embedding space; and (2) we\npropose a novel incomplete sentence detection task, where we incorporate an\ninformation discriminator to distinguish the embeddings of original sentences\nand incomplete sentences by randomly masking several low-frequency words,\nenabling to emphasize the more informative low-frequency words. Our SLT-FAI is\na flexible and plug-and-play framework, and it can be integrated with existing\nUSRL techniques. We evaluate SLT-FAI with various backbones on benchmark\ndatasets. Empirical results indicate that SLT-FAI can be superior to the\nexisting USRL baselines. Our code is released in\n\\url{https://github.com/wangbing1416/SLT-FAI}.", "published": "2023-05-15 13:59:23", "link": "http://arxiv.org/abs/2305.08655v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Natural Language Decomposition and Interpretation of Complex Utterances", "abstract": "Designing natural language interfaces has historically required collecting\nsupervised data to translate user requests into carefully designed intent\nrepresentations. This requires enumerating and labeling a long tail of user\nrequests, which is challenging. At the same time, large language models (LLMs)\nencode knowledge about goals and plans that can help conversational assistants\ninterpret user requests requiring numerous steps to complete. We introduce an\napproach to handle complex-intent-bearing utterances from a user via a process\nof hierarchical natural language decomposition and interpretation. Our approach\nuses a pre-trained language model to decompose a complex utterance into a\nsequence of simpler natural language steps and interprets each step using the\nlanguage-to-program model designed for the interface. To test our approach, we\ncollect and release DeCU -- a new NL-to-program benchmark to evaluate\nDecomposition of Complex Utterances. Experiments show that the proposed\napproach enables the interpretation of complex utterances with almost no\ncomplex training data, while outperforming standard few-shot prompting\napproaches.", "published": "2023-05-15 14:35:00", "link": "http://arxiv.org/abs/2305.08677v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparing Variation in Tokenizer Outputs Using a Series of Problematic\n  and Challenging Biomedical Sentences", "abstract": "Background & Objective: Biomedical text data are increasingly available for\nresearch. Tokenization is an initial step in many biomedical text mining\npipelines. Tokenization is the process of parsing an input biomedical sentence\n(represented as a digital character sequence) into a discrete set of word/token\nsymbols, which convey focused semantic/syntactic meaning. The objective of this\nstudy is to explore variation in tokenizer outputs when applied across a series\nof challenging biomedical sentences.\n  Method: Diaz [2015] introduce 24 challenging example biomedical sentences for\ncomparing tokenizer performance. In this study, we descriptively explore\nvariation in outputs of eight tokenizers applied to each example biomedical\nsentence. The tokenizers compared in this study are the NLTK white space\ntokenizer, the NLTK Penn Tree Bank tokenizer, Spacy and SciSpacy tokenizers,\nStanza/Stanza-Craft tokenizers, the UDPipe tokenizer, and R-tokenizers.\n  Results: For many examples, tokenizers performed similarly effectively;\nhowever, for certain examples, there were meaningful variation in returned\noutputs. The white space tokenizer often performed differently than other\ntokenizers. We observed performance similarities for tokenizers implementing\nrule-based systems (e.g. pattern matching and regular expressions) and\ntokenizers implementing neural architectures for token classification.\nOftentimes, the challenging tokens resulting in the greatest variation in\noutputs, are those words which convey substantive and focused\nbiomedical/clinical meaning (e.g. x-ray, IL-10, TCR/CD3, CD4+ CD8+, and\n(Ca2+)-regulated).\n  Conclusion: When state-of-the-art, open-source tokenizers from Python and R\nwere applied to a series of challenging biomedical example sentences, we\nobserved subtle variation in the returned outputs.", "published": "2023-05-15 16:46:47", "link": "http://arxiv.org/abs/2305.08787v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Measuring Cross-Lingual Transferability of Multilingual Transformers on\n  Sentence Classification", "abstract": "Recent studies have exhibited remarkable capabilities of pre-trained\nmultilingual Transformers, especially cross-lingual transferability. However,\ncurrent methods do not measure cross-lingual transferability well, hindering\nthe understanding of multilingual Transformers. In this paper, we propose IGap,\na cross-lingual transferability metric for multilingual Transformers on\nsentence classification tasks. IGap takes training error into consideration,\nand can also estimate transferability without end-task data. Experimental\nresults show that IGap outperforms baseline metrics for transferability\nmeasuring and transfer direction ranking. Besides, we conduct extensive\nsystematic experiments where we compare transferability among various\nmultilingual Transformers, fine-tuning algorithms, and transfer directions.\nMore importantly, our results reveal three findings about cross-lingual\ntransfer, which helps us to better understand multilingual Transformers.", "published": "2023-05-15 17:05:45", "link": "http://arxiv.org/abs/2305.08800v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring In-Context Learning Capabilities of Foundation Models for\n  Generating Knowledge Graphs from Text", "abstract": "Knowledge graphs can represent information about the real-world using\nentities and their relations in a structured and semantically rich manner and\nthey enable a variety of downstream applications such as question-answering,\nrecommendation systems, semantic search, and advanced analytics. However, at\nthe moment, building a knowledge graph involves a lot of manual effort and thus\nhinders their application in some situations and the automation of this process\nmight benefit especially for small organizations. Automatically generating\nstructured knowledge graphs from a large volume of natural language is still a\nchallenging task and the research on sub-tasks such as named entity extraction,\nrelation extraction, entity and relation linking, and knowledge graph\nconstruction aims to improve the state of the art of automatic construction and\ncompletion of knowledge graphs from text. The recent advancement of foundation\nmodels with billions of parameters trained in a self-supervised manner with\nlarge volumes of training data that can be adapted to a variety of downstream\ntasks has helped to demonstrate high performance on a large range of Natural\nLanguage Processing (NLP) tasks. In this context, one emerging paradigm is\nin-context learning where a language model is used as it is with a prompt that\nprovides instructions and some examples to perform a task without changing the\nparameters of the model using traditional approaches such as fine-tuning. This\nway, no computing resources are needed for re-training/fine-tuning the models\nand the engineering effort is minimal. Thus, it would be beneficial to utilize\nsuch capabilities for generating knowledge graphs from text.", "published": "2023-05-15 17:10:19", "link": "http://arxiv.org/abs/2305.08804v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Interpretability at Scale: Identifying Causal Mechanisms in Alpaca", "abstract": "Obtaining human-interpretable explanations of large, general-purpose language\nmodels is an urgent goal for AI safety. However, it is just as important that\nour interpretability methods are faithful to the causal dynamics underlying\nmodel behavior and able to robustly generalize to unseen inputs. Distributed\nAlignment Search (DAS) is a powerful gradient descent method grounded in a\ntheory of causal abstraction that has uncovered perfect alignments between\ninterpretable symbolic algorithms and small deep learning models fine-tuned for\nspecific tasks. In the present paper, we scale DAS significantly by replacing\nthe remaining brute-force search steps with learned parameters -- an approach\nwe call Boundless DAS. This enables us to efficiently search for interpretable\ncausal structure in large language models while they follow instructions. We\napply Boundless DAS to the Alpaca model (7B parameters), which, off the shelf,\nsolves a simple numerical reasoning problem. With Boundless DAS, we discover\nthat Alpaca does this by implementing a causal model with two interpretable\nboolean variables. Furthermore, we find that the alignment of neural\nrepresentations with these variables is robust to changes in inputs and\ninstructions. These findings mark a first step toward faithfully understanding\nthe inner-workings of our ever-growing and most widely deployed language\nmodels. Our tool is extensible to larger LLMs and is released publicly at\n`https://github.com/stanfordnlp/pyvene`.", "published": "2023-05-15 17:15:40", "link": "http://arxiv.org/abs/2305.08809v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PMIndiaSum: Multilingual and Cross-lingual Headline Summarization for\n  Languages in India", "abstract": "This paper introduces PMIndiaSum, a multilingual and massively parallel\nsummarization corpus focused on languages in India. Our corpus provides a\ntraining and testing ground for four language families, 14 languages, and the\nlargest to date with 196 language pairs. We detail our construction workflow\nincluding data acquisition, processing, and quality assurance. Furthermore, we\npublish benchmarks for monolingual, cross-lingual, and multilingual\nsummarization by fine-tuning, prompting, as well as translate-and-summarize.\nExperimental results confirm the crucial role of our data in aiding\nsummarization between Indian languages. Our dataset is publicly available and\ncan be freely modified and re-distributed.", "published": "2023-05-15 17:41:15", "link": "http://arxiv.org/abs/2305.08828v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "RL4F: Generating Natural Language Feedback with Reinforcement Learning\n  for Repairing Model Outputs", "abstract": "Despite their unprecedented success, even the largest language models make\nmistakes. Similar to how humans learn and improve using feedback, previous work\nproposed providing language models with natural language feedback to guide them\nin repairing their outputs. Because human-generated critiques are expensive to\nobtain, researchers have devised learned critique generators in lieu of human\ncritics while assuming one can train downstream models to utilize generated\nfeedback. However, this approach does not apply to black-box or limited access\nmodels such as ChatGPT, as they cannot be fine-tuned. Moreover, in the era of\nlarge general-purpose language agents, fine-tuning is neither computationally\nnor spatially efficient as it results in multiple copies of the network. In\nthis work, we introduce RL4F (Reinforcement Learning for Feedback), a\nmulti-agent collaborative framework where the critique generator is trained to\nmaximize end-task performance of GPT-3, a fixed model more than 200 times its\nsize. RL4F produces critiques that help GPT-3 revise its outputs. We study\nthree datasets for action planning, summarization and alphabetization and show\nrelative improvements up to 10% in multiple text similarity metrics over other\nlearned, retrieval-augmented or prompting-based critique generators.", "published": "2023-05-15 17:57:16", "link": "http://arxiv.org/abs/2305.08844v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CQE: A Comprehensive Quantity Extractor", "abstract": "Quantities are essential in documents to describe factual information. They\nare ubiquitous in application domains such as finance, business, medicine, and\nscience in general. Compared to other information extraction approaches,\ninterestingly only a few works exist that describe methods for a proper\nextraction and representation of quantities in text. In this paper, we present\nsuch a comprehensive quantity extraction framework from text data. It\nefficiently detects combinations of values and units, the behavior of a\nquantity (e.g., rising or falling), and the concept a quantity is associated\nwith. Our framework makes use of dependency parsing and a dictionary of units,\nand it provides for a proper normalization and standardization of detected\nquantities. Using a novel dataset for evaluation, we show that our open source\nframework outperforms other systems and -- to the best of our knowledge -- is\nthe first to detect concepts associated with identified quantities. The code\nand data underlying our framework are available at\nhttps://github.com/vivkaz/CQE.", "published": "2023-05-15 17:59:41", "link": "http://arxiv.org/abs/2305.08853v1", "categories": ["cs.CL", "I.7; I.7.1; I.7.5"], "primary_category": "cs.CL"}
{"title": "It Takes Two to Tango: Navigating Conceptualizations of NLP Tasks and\n  Measurements of Performance", "abstract": "Progress in NLP is increasingly measured through benchmarks; hence,\ncontextualizing progress requires understanding when and why practitioners may\ndisagree about the validity of benchmarks. We develop a taxonomy of\ndisagreement, drawing on tools from measurement modeling, and distinguish\nbetween two types of disagreement: 1) how tasks are conceptualized and 2) how\nmeasurements of model performance are operationalized. To provide evidence for\nour taxonomy, we conduct a meta-analysis of relevant literature to understand\nhow NLP tasks are conceptualized, as well as a survey of practitioners about\ntheir impressions of different factors that affect benchmark validity. Our\nmeta-analysis and survey across eight tasks, ranging from coreference\nresolution to question answering, uncover that tasks are generally not clearly\nand consistently conceptualized and benchmarks suffer from operationalization\ndisagreements. These findings support our proposed taxonomy of disagreement.\nFinally, based on our taxonomy, we present a framework for constructing\nbenchmarks and documenting their limitations.", "published": "2023-05-15 21:12:07", "link": "http://arxiv.org/abs/2305.09022v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SGP-TOD: Building Task Bots Effortlessly via Schema-Guided LLM Prompting", "abstract": "Building end-to-end task bots and maintaining their integration with new\nfunctionalities using minimal human efforts is a long-standing challenge in\ndialog research. Recently large language models (LLMs) have demonstrated\nexceptional proficiency in conversational engagement and adherence to\ninstructions across various downstream tasks. In this work, we introduce\nSGP-TOD, Schema-Guided Prompting for building Task-Oriented Dialog systems\neffortlessly based on LLMs. Utilizing the symbolic knowledge -- task schema, we\ninstruct fixed LLMs to generate appropriate responses on novel tasks,\ncircumventing the need for training data. Specifically, SGP-TOD comprises three\ncomponents: a LLM for engaging with users, a DST Prompter to aid the LLM with\ndialog state tracking, which is then used to retrieve database items, and a\nPolicy Prompter to elicit proper responses adhering to the provided dialog\npolicy. Experimental results on Multiwoz, RADDLE and STAR datasets show that\nour training-free strategy SGP-TOD, without any task-specific data, yields\nstate-of-the-art (SOTA) zero-shot performance, greatly surpasses the few-shot\napproaches. In a domain-extension setting, SGP-TOD aptly adapts to new\nfunctionalities by merely adding supplementary schema rules. We make our code\nand data publicly available.", "published": "2023-05-15 23:29:56", "link": "http://arxiv.org/abs/2305.09067v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Parameter-Efficient Fine-Tuning with Layer Pruning on Free-Text\n  Sequence-to-Sequence Modeling", "abstract": "The increasing size of language models raises great research interests in\nparameter-efficient fine-tuning such as LoRA that freezes the pre-trained\nmodel, and injects small-scale trainable parameters for multiple downstream\ntasks (e.g., summarization, question answering and translation). To further\nenhance the efficiency of fine-tuning, we propose a framework that integrates\nLoRA and structured layer pruning. The integrated framework is validated on two\ncreated deidentified medical report summarization datasets based on\nMIMIC-IV-Note and two public medical dialogue datasets. By tuning 0.6%\nparameters of the original model and pruning over 30% Transformer-layers, our\nframework can reduce 50% of GPU memory usage and speed up 100% of the training\nphase, while preserving over 92% generation qualities on free-text\nsequence-to-sequence tasks.", "published": "2023-05-15 00:21:08", "link": "http://arxiv.org/abs/2305.08285v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Assessing the potential of LLM-assisted annotation for corpus-based\n  pragmatics and discourse analysis: The case of apology", "abstract": "Certain forms of linguistic annotation, like part of speech and semantic\ntagging, can be automated with high accuracy. However, manual annotation is\nstill necessary for complex pragmatic and discursive features that lack a\ndirect mapping to lexical forms. This manual process is time-consuming and\nerror-prone, limiting the scalability of function-to-form approaches in corpus\nlinguistics. To address this, our study explores the possibility of using large\nlanguage models (LLMs) to automate pragma-discursive corpus annotation. We\ncompare GPT-3.5 (the model behind the free-to-use version of ChatGPT), GPT-4\n(the model underpinning the precise mode of Bing chatbot), and a human coder in\nannotating apology components in English based on the local grammar framework.\nWe find that GPT-4 outperformed GPT-3.5, with accuracy approaching that of a\nhuman coder. These results suggest that LLMs can be successfully deployed to\naid pragma-discursive corpus annotation, making the process more efficient,\nscalable and accessible.", "published": "2023-05-15 04:10:13", "link": "http://arxiv.org/abs/2305.08339v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "KEPR: Knowledge Enhancement and Plausibility Ranking for Generative\n  Commonsense Question Answering", "abstract": "Generative commonsense question answering (GenCQA) is a task of automatically\ngenerating a list of answers given a question. The answer list is required to\ncover all reasonable answers. This presents the considerable challenges of\nproducing diverse answers and ranking them properly. Incorporating a variety of\nclosely-related background knowledge into the encoding of questions enables the\ngeneration of different answers. Meanwhile, learning to distinguish positive\nanswers from negative ones potentially enhances the probabilistic estimation of\nplausibility, and accordingly, the plausibility-based ranking. Therefore, we\npropose a Knowledge Enhancement and Plausibility Ranking (KEPR) approach\ngrounded on the Generate-Then-Rank pipeline architecture. Specifically, we\nexpand questions in terms of Wiktionary commonsense knowledge of keywords, and\nreformulate them with normalized patterns. Dense passage retrieval is utilized\nfor capturing relevant knowledge, and different PLM-based (BART, GPT2 and T5)\nnetworks are used for generating answers. On the other hand, we develop an\nELECTRA-based answer ranking model, where logistic regression is conducted\nduring training, with the aim of approximating different levels of plausibility\nin a polar classification scenario. Extensive experiments on the benchmark\nProtoQA show that KEPR obtains substantial improvements, compared to the strong\nbaselines. Within the experimental models, the T5-based GenCQA with KEPR\nobtains the best performance, which is up to 60.91% at the primary canonical\nmetric Inc@3. It outperforms the existing GenCQA models on the current\nleaderboard of ProtoQA.", "published": "2023-05-15 04:58:37", "link": "http://arxiv.org/abs/2305.08347v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Hierarchical Aligned Multimodal Learning for NER on Tweet Posts", "abstract": "Mining structured knowledge from tweets using named entity recognition (NER)\ncan be beneficial for many down stream applications such as recommendation and\nintention understanding. With tweet posts tending to be multimodal, multimodal\nnamed entity recognition (MNER) has attracted more attention. In this paper, we\npropose a novel approach, which can dynamically align the image and text\nsequence and achieve the multi-level cross-modal learning to augment textual\nword representation for MNER improvement. To be specific, our framework can be\nsplit into three main stages: the first stage focuses on intra-modality\nrepresentation learning to derive the implicit global and local knowledge of\neach modality, the second evaluates the relevance between the text and its\naccompanying image and integrates different grained visual information based on\nthe relevance, the third enforces semantic refinement via iterative cross-modal\ninteractions and co-attention. We conduct experiments on two open datasets, and\nthe results and detailed analysis demonstrate the advantage of our model.", "published": "2023-05-15 06:14:36", "link": "http://arxiv.org/abs/2305.08372v2", "categories": ["cs.CL", "cs.MM"], "primary_category": "cs.CL"}
{"title": "TESS: Text-to-Text Self-Conditioned Simplex Diffusion", "abstract": "Diffusion models have emerged as a powerful paradigm for generation,\nobtaining strong performance in various continuous domains. However, applying\ncontinuous diffusion models to natural language remains challenging due to its\ndiscrete nature and the need for a large number of diffusion steps to generate\ntext, making diffusion-based generation expensive. In this work, we propose\nText-to-text Self-conditioned Simplex Diffusion (TESS), a text diffusion model\nthat is fully non-autoregressive, employs a new form of self-conditioning, and\napplies the diffusion process on the logit simplex space rather than the\nlearned embedding space. Through extensive experiments on natural language\nunderstanding and generation tasks including summarization, text\nsimplification, paraphrase generation, and question generation, we demonstrate\nthat TESS outperforms state-of-the-art non-autoregressive models, requires\nfewer diffusion steps with minimal drop in performance, and is competitive with\npretrained autoregressive sequence-to-sequence models. We publicly release our\ncodebase at https://github.com/allenai/tess-diffusion.", "published": "2023-05-15 06:33:45", "link": "http://arxiv.org/abs/2305.08379v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "What's the Meaning of Superhuman Performance in Today's NLU?", "abstract": "In the last five years, there has been a significant focus in Natural\nLanguage Processing (NLP) on developing larger Pretrained Language Models\n(PLMs) and introducing benchmarks such as SuperGLUE and SQuAD to measure their\nabilities in language understanding, reasoning, and reading comprehension.\nThese PLMs have achieved impressive results on these benchmarks, even\nsurpassing human performance in some cases. This has led to claims of\nsuperhuman capabilities and the provocative idea that certain tasks have been\nsolved. In this position paper, we take a critical look at these claims and ask\nwhether PLMs truly have superhuman abilities and what the current benchmarks\nare really evaluating. We show that these benchmarks have serious limitations\naffecting the comparison between humans and PLMs and provide recommendations\nfor fairer and more transparent benchmarks.", "published": "2023-05-15 07:48:31", "link": "http://arxiv.org/abs/2305.08414v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Shared and Private Information Learning in Multimodal Sentiment Analysis\n  with Deep Modal Alignment and Self-supervised Multi-Task Learning", "abstract": "Designing an effective representation learning method for multimodal\nsentiment analysis tasks is a crucial research direction. The challenge lies in\nlearning both shared and private information in a complete modal\nrepresentation, which is difficult with uniform multimodal labels and a raw\nfeature fusion approach. In this work, we propose a deep modal shared\ninformation learning module based on the covariance matrix to capture the\nshared information between modalities. Additionally, we use a label generation\nmodule based on a self-supervised learning strategy to capture the private\ninformation of the modalities. Our module is plug-and-play in multimodal tasks,\nand by changing the parameterization, it can adjust the information exchange\nrelationship between the modes and learn the private or shared information\nbetween the specified modes. We also employ a multi-task learning strategy to\nhelp the model focus its attention on the modal differentiation training data.\nWe provide a detailed formulation derivation and feasibility proof for the\ndesign of the deep modal shared information learning module. We conduct\nextensive experiments on three common multimodal sentiment analysis baseline\ndatasets, and the experimental results validate the reliability of our model.\nFurthermore, we explore more combinatorial techniques for the use of the\nmodule. Our approach outperforms current state-of-the-art methods on most of\nthe metrics of the three public datasets.", "published": "2023-05-15 09:24:48", "link": "http://arxiv.org/abs/2305.08473v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Similarity-weighted Construction of Contextualized Commonsense Knowledge\n  Graphs for Knowledge-intense Argumentation Tasks", "abstract": "Arguments often do not make explicit how a conclusion follows from its\npremises. To compensate for this lack, we enrich arguments with structured\nbackground knowledge to support knowledge-intense argumentation tasks. We\npresent a new unsupervised method for constructing Contextualized Commonsense\nKnowledge Graphs (CCKGs) that selects contextually relevant knowledge from\nlarge knowledge graphs (KGs) efficiently and at high quality. Our work goes\nbeyond context-insensitive knowledge extraction heuristics by computing\nsemantic similarity between KG triplets and textual arguments. Using these\ntriplet similarities as weights, we extract contextualized knowledge paths that\nconnect a conclusion to its premise, while maximizing similarity to the\nargument. We combine multiple paths into a CCKG that we optionally prune to\nreduce noise and raise precision. Intrinsic evaluation of the quality of our\ngraphs shows that our method is effective for (re)constructing human\nexplanation graphs. Manual evaluations in a large-scale knowledge selection\nsetup confirm high recall and precision of implicit CSK in the CCKGs. Finally,\nwe demonstrate the effectiveness of CCKGs in a knowledge-insensitive argument\nquality rating task, outperforming strong baselines and rivaling a GPT-3 based\nsystem.", "published": "2023-05-15 09:52:36", "link": "http://arxiv.org/abs/2305.08495v1", "categories": ["cs.CL", "cs.DB"], "primary_category": "cs.CL"}
{"title": "MeeQA: Natural Questions in Meeting Transcripts", "abstract": "We present MeeQA, a dataset for natural-language question answering over\nmeeting transcripts. It includes real questions asked during meetings by its\nparticipants. The dataset contains 48K question-answer pairs, extracted from\n422 meeting transcripts, spanning multiple domains. Questions in transcripts\npose a special challenge as they are not always clear, and considerable context\nmay be required in order to provide an answer. Further, many questions asked\nduring meetings are left unanswered. To improve baseline model performance on\nthis type of questions, we also propose a novel loss function, \\emph{Flat\nHierarchical Loss}, designed to enhance performance over questions with no\nanswer in the text. Our experiments demonstrate the advantage of using our\napproach over standard QA models.", "published": "2023-05-15 10:02:47", "link": "http://arxiv.org/abs/2305.08502v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Beqi: Revitalize the Senegalese Wolof Language with a Robust Spelling\n  Corrector", "abstract": "The progress of Natural Language Processing (NLP), although fast in recent\nyears, is not at the same pace for all languages. African languages in\nparticular are still behind and lack automatic processing tools. Some of these\ntools are very important for the development of these languages but also have\nan important role in many NLP applications. This is particularly the case for\nautomatic spell checkers. Several approaches have been studied to address this\ntask and the one modeling spelling correction as a translation task from\nmisspelled (noisy) text to well-spelled (correct) text shows promising results.\nHowever, this approach requires a parallel corpus of noisy data on the one hand\nand correct data on the other hand, whereas Wolof is a low-resource language\nand does not have such a corpus. In this paper, we present a way to address the\nconstraint related to the lack of data by generating synthetic data and we\npresent sequence-to-sequence models using Deep Learning for spelling correction\nin Wolof. We evaluated these models in three different scenarios depending on\nthe subwording method applied to the data and showed that the latter had a\nsignificant impact on the performance of the models, which opens the way for\nfuture research in Wolof spelling correction.", "published": "2023-05-15 10:28:36", "link": "http://arxiv.org/abs/2305.08518v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Adam-Smith at SemEval-2023 Task 4: Discovering Human Values in Arguments\n  with Ensembles of Transformer-based Models", "abstract": "This paper presents the best-performing approach alias \"Adam Smith\" for the\nSemEval-2023 Task 4: \"Identification of Human Values behind Arguments\". The\ngoal of the task was to create systems that automatically identify the values\nwithin textual arguments. We train transformer-based models until they reach\ntheir loss minimum or f1-score maximum. Ensembling the models by selecting one\nglobal decision threshold that maximizes the f1-score leads to the\nbest-performing system in the competition. Ensembling based on stacking with\nlogistic regressions shows the best performance on an additional dataset\nprovided to evaluate the robustness (\"Nahj al-Balagha\"). Apart from outlining\nthe submitted system, we demonstrate that the use of the large ensemble model\nis not necessary and that the system size can be significantly reduced.", "published": "2023-05-15 13:20:14", "link": "http://arxiv.org/abs/2305.08625v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Text2Gender: A Deep Learning Architecture for Analysis of Blogger's Age\n  and Gender", "abstract": "Deep learning techniques have gained a lot of traction in the field of NLP\nresearch. The aim of this paper is to predict the age and gender of an\nindividual by inspecting their written text. We propose a supervised BERT-based\nclassification technique in order to predict the age and gender of bloggers.\nThe dataset used contains 681284 rows of data, with the information of the\nblogger's age, gender, and text of the blog written by them. We compare our\nalgorithm to previous works in the same domain and achieve a better accuracy\nand F1 score. The accuracy reported for the prediction of age group was 84.2%,\nwhile the accuracy for the prediction of gender was 86.32%. This study relies\non the raw capabilities of BERT to predict the classes of textual data\nefficiently. This paper shows promising capability in predicting the\ndemographics of the author with high accuracy and can have wide applicability\nacross multiple domains.", "published": "2023-05-15 13:26:50", "link": "http://arxiv.org/abs/2305.08633v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AdamR at SemEval-2023 Task 10: Solving the Class Imbalance Problem in\n  Sexism Detection with Ensemble Learning", "abstract": "The Explainable Detection of Online Sexism task presents the problem of\nexplainable sexism detection through fine-grained categorisation of sexist\ncases with three subtasks. Our team experimented with different ways to combat\nclass imbalance throughout the tasks using data augmentation and loss\nalteration techniques. We tackled the challenge by utilising ensembles of\nTransformer models trained on different datasets, which are tested to find the\nbalance between performance and interpretability. This solution ranked us in\nthe top 40\\% of teams for each of the tracks.", "published": "2023-05-15 13:28:59", "link": "http://arxiv.org/abs/2305.08636v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Recyclable Tuning for Continual Pre-training", "abstract": "Continual pre-training is the paradigm where pre-trained language models\n(PLMs) continually acquire fresh knowledge from growing data and gradually get\nupgraded. Before an upgraded PLM is released, we may have tuned the original\nPLM for various tasks and stored the adapted weights. However, when tuning the\nupgraded PLM, these outdated adapted weights will typically be ignored and\ndiscarded, causing a potential waste of resources. We bring this issue to the\nforefront and contend that proper algorithms for recycling outdated adapted\nweights should be developed. To this end, we formulate the task of recyclable\ntuning for continual pre-training. In pilot studies, we find that after\ncontinual pre-training, the upgraded PLM remains compatible with the outdated\nadapted weights to some extent. Motivated by this finding, we analyze the\nconnection between continually pre-trained PLMs from two novel aspects, i.e.,\nmode connectivity, and functional similarity. Based on the corresponding\nfindings, we propose both an initialization-based method and a\ndistillation-based method for our task. We demonstrate their feasibility in\nimproving the convergence and performance for tuning the upgraded PLM. We also\nshow that both methods can be combined to achieve better performance. The\nsource codes are publicly available at\nhttps://github.com/thunlp/RecyclableTuning.", "published": "2023-05-15 15:05:44", "link": "http://arxiv.org/abs/2305.08702v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Sensitivity and Robustness of Large Language Models to Prompt Template\n  in Japanese Text Classification Tasks", "abstract": "Prompt engineering relevance research has seen a notable surge in recent\nyears, primarily driven by advancements in pre-trained language models and\nlarge language models. However, a critical issue has been identified within\nthis domain: the inadequate of sensitivity and robustness of these models\ntowards Prompt Templates, particularly in lesser-studied languages such as\nJapanese. This paper explores this issue through a comprehensive evaluation of\nseveral representative Large Language Models (LLMs) and a widely-utilized\npre-trained model(PLM). These models are scrutinized using a benchmark dataset\nin Japanese, with the aim to assess and analyze the performance of the current\nmultilingual models in this context. Our experimental results reveal startling\ndiscrepancies. A simple modification in the sentence structure of the Prompt\nTemplate led to a drastic drop in the accuracy of GPT-4 from 49.21 to 25.44.\nThis observation underscores the fact that even the highly performance GPT-4\nmodel encounters significant stability issues when dealing with diverse\nJapanese prompt templates, rendering the consistency of the model's output\nresults questionable. In light of these findings, we conclude by proposing\npotential research trajectories to further enhance the development and\nperformance of Large Language Models in their current stage.", "published": "2023-05-15 15:19:08", "link": "http://arxiv.org/abs/2305.08714v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Sentence Level Curriculum Learning for Improved Neural Conversational\n  Models", "abstract": "Designing machine intelligence to converse with a human user necessarily\nrequires an understanding of how humans participate in conversation, and thus\nconversation modeling is an important task in natural language processing. New\nbreakthroughs in architecture and data gathering continue to push the\nperformance of such conversational AI models. However, designs neglect the\ngradual buildup in sentence structure and complexity experienced by humans as\nwe learn to communicate. During training, our model accepts one or more\nsentences as input and attempts to predict the next sentence in the\nconversation one word at a time, so our goal is to separate training into\nsegments, with each segment's corpus comprised of longer sentence pairs than\nthe previous one. This will mimic the desired \"buildup\" component of human\nlearning. We begin with only \"short\" length sentence pairs, then only \"medium\"\nlength pairs, and so on. A majority of our experiments were toward optimizing\nthis technique, ensuring a proper representation of the technique's potential,\nsince many of the details were new questions. Our segment-trained models were\nthen able to achieve lower validation loss at the end of training than models\ntrained with standard text preparation. This segmented training is\nstraightforward to implement and our results provide a general direction for\nfuture research to implement and improve it.", "published": "2023-05-15 17:28:59", "link": "http://arxiv.org/abs/2305.08818v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Large Language Models are Zero-Shot Rankers for Recommender Systems", "abstract": "Recently, large language models (LLMs) (e.g., GPT-4) have demonstrated\nimpressive general-purpose task-solving abilities, including the potential to\napproach recommendation tasks. Along this line of research, this work aims to\ninvestigate the capacity of LLMs that act as the ranking model for recommender\nsystems. We first formalize the recommendation problem as a conditional ranking\ntask, considering sequential interaction histories as conditions and the items\nretrieved by other candidate generation models as candidates. To solve the\nranking task by LLMs, we carefully design the prompting template and conduct\nextensive experiments on two widely-used datasets. We show that LLMs have\npromising zero-shot ranking abilities but (1) struggle to perceive the order of\nhistorical interactions, and (2) can be biased by popularity or item positions\nin the prompts. We demonstrate that these issues can be alleviated using\nspecially designed prompting and bootstrapping strategies. Equipped with these\ninsights, zero-shot LLMs can even challenge conventional recommendation models\nwhen ranking candidates are retrieved by multiple candidate generators. The\ncode and processed datasets are available at\nhttps://github.com/RUCAIBox/LLMRank.", "published": "2023-05-15 17:57:39", "link": "http://arxiv.org/abs/2305.08845v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Helping the Helper: Supporting Peer Counselors via AI-Empowered Practice\n  and Feedback", "abstract": "Millions of users come to online peer counseling platforms to seek support.\nHowever, studies show that online peer support groups are not always as\neffective as expected, largely due to users' negative experiences with\nunhelpful counselors. Peer counselors are key to the success of online peer\ncounseling platforms, but most often do not receive appropriate training.Hence,\nwe introduce CARE: an AI-based tool to empower and train peer counselors\nthrough practice and feedback. Concretely, CARE helps diagnose which counseling\nstrategies are needed in a given situation and suggests example responses to\ncounselors during their practice sessions. Building upon the Motivational\nInterviewing framework, CARE utilizes large-scale counseling conversation data\nwith text generation techniques to enable these functionalities. We demonstrate\nthe efficacy of CARE by performing quantitative evaluations and qualitative\nuser studies through simulated chats and semi-structured interviews, finding\nthat CARE especially helps novice counselors in challenging situations. The\ncode is available at https://github.com/SALT-NLP/CARE", "published": "2023-05-15 19:48:59", "link": "http://arxiv.org/abs/2305.08982v2", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Soft Prompt Decoding for Multilingual Dense Retrieval", "abstract": "In this work, we explore a Multilingual Information Retrieval (MLIR) task,\nwhere the collection includes documents in multiple languages. We demonstrate\nthat applying state-of-the-art approaches developed for cross-lingual\ninformation retrieval to MLIR tasks leads to sub-optimal performance. This is\ndue to the heterogeneous and imbalanced nature of multilingual collections --\nsome languages are better represented in the collection and some benefit from\nlarge-scale training data. To address this issue, we present KD-SPD, a novel\nsoft prompt decoding approach for MLIR that implicitly \"translates\" the\nrepresentation of documents in different languages into the same embedding\nspace. To address the challenges of data scarcity and imbalance, we introduce a\nknowledge distillation strategy. The teacher model is trained on rich English\nretrieval data, and by leveraging bi-text data, our distillation framework\ntransfers its retrieval knowledge to the multilingual document encoder.\nTherefore, our approach does not require any multilingual retrieval training\ndata. Extensive experiments on three MLIR datasets with a total of 15 languages\ndemonstrate that KD-SPD significantly outperforms competitive baselines in all\ncases. We conduct extensive analyses to show that our method has less language\nbias and better zero-shot transfer ability towards new languages.", "published": "2023-05-15 21:17:17", "link": "http://arxiv.org/abs/2305.09025v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Memorization for Good: Encryption with Autoregressive Language Models", "abstract": "Over-parameterized neural language models (LMs) can memorize and recite long\nsequences of training data. While such memorization is normally associated with\nundesired properties such as overfitting and information leaking, our work\ncasts memorization as an unexplored capability of LMs. We propose the first\nsymmetric encryption algorithm with autoregressive language models (SELM). We\nshow that autoregressive LMs can encode arbitrary data into a compact\nreal-valued vector (i.e., encryption) and then losslessly decode the vector to\nthe original message (i.e., decryption) via random subspace optimization and\ngreedy decoding. While SELM is not amenable to conventional cryptanalysis, we\ninvestigate its security through a novel empirical variant of the classic\nIND-CPA (indistinguishability under chosen-plaintext attack) game and show\npromising results on security. Our code and datasets are available at\nhttps://github.com/OSU-NLP-Group/SELM.", "published": "2023-05-15 05:42:34", "link": "http://arxiv.org/abs/2305.10445v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Emotion Recognition based on Psychological Components in Guided\n  Narratives for Emotion Regulation", "abstract": "Emotion regulation is a crucial element in dealing with emotional events and\nhas positive effects on mental health. This paper aims to provide a more\ncomprehensive understanding of emotional events by introducing a new French\ncorpus of emotional narratives collected using a questionnaire for emotion\nregulation. We follow the theoretical framework of the Component Process Model\nwhich considers emotions as dynamic processes composed of four interrelated\ncomponents (behavior, feeling, thinking and territory). Each narrative is\nrelated to a discrete emotion and is structured based on all emotion components\nby the writers. We study the interaction of components and their impact on\nemotion classification with machine learning methods and pre-trained language\nmodels. Our results show that each component improves prediction performance,\nand that the best results are achieved by jointly considering all components.\nOur results also show the effectiveness of pre-trained language models in\npredicting discrete emotion from certain components, which reveal differences\nin how emotion components are expressed.", "published": "2023-05-15 12:06:31", "link": "http://arxiv.org/abs/2305.10446v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Effectiveness of a Dynamic Loss Function in Neural Network Based\n  Automated Essay Scoring", "abstract": "Neural networks and in particular the attention mechanism have brought\nsignificant advances to the field of Automated Essay Scoring. Many of these\nsystems use a regression-based model which may be prone to underfitting when\nthe model only predicts the mean of the training data. In this paper, we\npresent a dynamic loss function that creates an incentive for the model to\npredict with the correct distribution, as well as predicting the correct\nvalues. Our loss function achieves this goal without sacrificing any\nperformance achieving a Quadratic Weighted Kappa score of 0.752 on the\nAutomated Student Assessment Prize Automated Essay Scoring dataset.", "published": "2023-05-15 16:39:35", "link": "http://arxiv.org/abs/2305.10447v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Semantic Composition in Visually Grounded Language Models", "abstract": "What is sentence meaning and its ideal representation? Much of the expressive\npower of human language derives from semantic composition, the mind's ability\nto represent meaning hierarchically & relationally over constituents. At the\nsame time, much sentential meaning is outside the text and requires grounding\nin sensory, motor, and experiential modalities to be adequately learned.\nAlthough large language models display considerable compositional ability,\nrecent work shows that visually-grounded language models drastically fail to\nrepresent compositional structure. In this thesis, we explore whether & how\nmodels compose visually grounded semantics, and how we might improve their\nability to do so.\n  Specifically, we introduce 1) WinogroundVQA, a new compositional visual\nquestion answering benchmark, 2) Syntactic Neural Module Distillation, a\nmeasure of compositional ability in sentence embedding models, 3) Causal\nTracing for Image Captioning Models to locate neural representations vital for\nvision-language composition, 4) Syntactic MeanPool to inject a compositional\ninductive bias into sentence embeddings, and 5) Cross-modal Attention\nCongruence Regularization, a self-supervised objective function for\nvision-language relation alignment. We close by discussing connections of our\nwork to neuroscience, psycholinguistics, formal semantics, and philosophy.", "published": "2023-05-15 03:19:42", "link": "http://arxiv.org/abs/2305.16328v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Document Understanding Dataset and Evaluation (DUDE)", "abstract": "We call on the Document AI (DocAI) community to reevaluate current\nmethodologies and embrace the challenge of creating more practically-oriented\nbenchmarks. Document Understanding Dataset and Evaluation (DUDE) seeks to\nremediate the halted research progress in understanding visually-rich documents\n(VRDs). We present a new dataset with novelties related to types of questions,\nanswers, and document layouts based on multi-industry, multi-domain, and\nmulti-page VRDs of various origins, and dates. Moreover, we are pushing the\nboundaries of current methods by creating multi-task and multi-domain\nevaluation setups that more accurately simulate real-world situations where\npowerful generalization and adaptation under low-resource settings are desired.\nDUDE aims to set a new standard as a more practical, long-standing benchmark\nfor the community, and we hope that it will lead to future extensions and\ncontributions that address real-world challenges. Finally, our work illustrates\nthe importance of finding more efficient ways to model language, images, and\nlayout in DocAI.", "published": "2023-05-15 08:54:32", "link": "http://arxiv.org/abs/2305.08455v3", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Measuring Consistency in Text-based Financial Forecasting Models", "abstract": "Financial forecasting has been an important and active area of machine\nlearning research, as even the most modest advantage in predictive accuracy can\nbe parlayed into significant financial gains. Recent advances in natural\nlanguage processing (NLP) bring the opportunity to leverage textual data, such\nas earnings reports of publicly traded companies, to predict the return rate\nfor an asset. However, when dealing with such a sensitive task, the consistency\nof models -- their invariance under meaning-preserving alternations in input --\nis a crucial property for building user trust. Despite this, current financial\nforecasting methods do not consider consistency. To address this problem, we\npropose FinTrust, an evaluation tool that assesses logical consistency in\nfinancial text. Using FinTrust, we show that the consistency of\nstate-of-the-art NLP models for financial forecasting is poor. Our analysis of\nthe performance degradation caused by meaning-preserving alternations suggests\nthat current text-based methods are not suitable for robustly predicting market\ninformation. All resources are available at\nhttps://github.com/yingpengma/fintrust.", "published": "2023-05-15 10:32:26", "link": "http://arxiv.org/abs/2305.08524v2", "categories": ["cs.CL", "cs.AI", "econ.GN", "q-fin.EC"], "primary_category": "cs.CL"}
{"title": "Understanding and Bridging the Modality Gap for Speech Translation", "abstract": "How to achieve better end-to-end speech translation (ST) by leveraging (text)\nmachine translation (MT) data? Among various existing techniques, multi-task\nlearning is one of the effective ways to share knowledge between ST and MT in\nwhich additional MT data can help to learn source-to-target mapping. However,\ndue to the differences between speech and text, there is always a gap between\nST and MT. In this paper, we first aim to understand this modality gap from the\ntarget-side representation differences, and link the modality gap to another\nwell-known problem in neural machine translation: exposure bias. We find that\nthe modality gap is relatively small during training except for some difficult\ncases, but keeps increasing during inference due to the cascading effect. To\naddress these problems, we propose the Cross-modal Regularization with\nScheduled Sampling (Cress) method. Specifically, we regularize the output\npredictions of ST and MT, whose target-side contexts are derived by sampling\nbetween ground truth words and self-generated words with a varying probability.\nFurthermore, we introduce token-level adaptive training which assigns different\ntraining weights to target tokens to handle difficult cases with large modality\ngaps. Experiments and analysis show that our approach effectively bridges the\nmodality gap, and achieves promising results in all eight directions of the\nMuST-C dataset.", "published": "2023-05-15 15:09:18", "link": "http://arxiv.org/abs/2305.08706v1", "categories": ["cs.CL", "cs.SD", "eess.AS", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Back Translation for Speech-to-text Translation Without Transcripts", "abstract": "The success of end-to-end speech-to-text translation (ST) is often achieved\nby utilizing source transcripts, e.g., by pre-training with automatic speech\nrecognition (ASR) and machine translation (MT) tasks, or by introducing\nadditional ASR and MT data. Unfortunately, transcripts are only sometimes\navailable since numerous unwritten languages exist worldwide. In this paper, we\naim to utilize large amounts of target-side monolingual data to enhance ST\nwithout transcripts. Motivated by the remarkable success of back translation in\nMT, we develop a back translation algorithm for ST (BT4ST) to synthesize pseudo\nST data from monolingual target data. To ease the challenges posed by\nshort-to-long generation and one-to-many mapping, we introduce self-supervised\ndiscrete units and achieve back translation by cascading a target-to-unit model\nand a unit-to-speech model. With our synthetic ST data, we achieve an average\nboost of 2.3 BLEU on MuST-C En-De, En-Fr, and En-Es datasets. More experiments\nshow that our method is especially effective in low-resource scenarios.", "published": "2023-05-15 15:12:40", "link": "http://arxiv.org/abs/2305.08709v1", "categories": ["cs.CL", "cs.SD", "eess.AS", "I.2.7"], "primary_category": "cs.CL"}
{"title": "sustain.AI: a Recommender System to analyze Sustainability Reports", "abstract": "We present sustainAI, an intelligent, context-aware recommender system that\nassists auditors and financial investors as well as the general public to\nefficiently analyze companies' sustainability reports. The tool leverages an\nend-to-end trainable architecture that couples a BERT-based encoding module\nwith a multi-label classification head to match relevant text passages from\nsustainability reports to their respective law regulations from the Global\nReporting Initiative (GRI) standards. We evaluate our model on two novel German\nsustainability reporting data sets and consistently achieve a significantly\nhigher recommendation performance compared to multiple strong baselines.\nFurthermore, sustainAI is publicly available for everyone at\nhttps://sustain.ki.nrw/.", "published": "2023-05-15 15:16:19", "link": "http://arxiv.org/abs/2305.08711v3", "categories": ["cs.CL", "cs.AI", "cs.LG", "H.3.3"], "primary_category": "cs.CL"}
{"title": "Knowledge Rumination for Pre-trained Language Models", "abstract": "Previous studies have revealed that vanilla pre-trained language models\n(PLMs) lack the capacity to handle knowledge-intensive NLP tasks alone; thus,\nseveral works have attempted to integrate external knowledge into PLMs.\nHowever, despite the promising outcome, we empirically observe that PLMs may\nhave already encoded rich knowledge in their pre-trained parameters but fail to\nfully utilize them when applying them to knowledge-intensive tasks. In this\npaper, we propose a new paradigm dubbed Knowledge Rumination to help the\npre-trained language model utilize that related latent knowledge without\nretrieving it from the external corpus. By simply adding a prompt like \"As far\nas I know\" to the PLMs, we try to review related latent knowledge and inject\nthem back into the model for knowledge consolidation. We apply the proposed\nknowledge rumination to various language models, including RoBERTa, DeBERTa,\nand GPT-3. Experimental results on six commonsense reasoning tasks and GLUE\nbenchmarks demonstrate the effectiveness of our proposed approach, which proves\nthat the knowledge stored in PLMs can be better exploited to enhance\nperformance. Code is available in\nhttps://github.com/zjunlp/knowledge-rumination.", "published": "2023-05-15 15:47:09", "link": "http://arxiv.org/abs/2305.08732v3", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Question-Answering System Extracts Information on Injection Drug Use\n  from Clinical Notes", "abstract": "Background: Injection drug use (IDU) is a dangerous health behavior that\nincreases mortality and morbidity. Identifying IDU early and initiating harm\nreduction interventions can benefit individuals at risk. However, extracting\nIDU behaviors from patients' electronic health records (EHR) is difficult\nbecause there is no International Classification of Disease (ICD) code and the\nonly place IDU information can be indicated is unstructured free-text clinical\nnotes. Although natural language processing can efficiently extract this\ninformation from unstructured data, there are no validated tools. Methods: To\naddress this gap in clinical information, we design and demonstrate a\nquestion-answering (QA) framework to extract information on IDU from clinical\nnotes. Our framework involves two main steps: (1) generating a gold-standard QA\ndataset and (2) developing and testing the QA model. We utilize 2323 clinical\nnotes of 1145 patients sourced from the VA Corporate Data Warehouse to\nconstruct the gold-standard dataset for developing and evaluating the QA model.\nWe also demonstrate the QA model's ability to extract IDU-related information\non temporally out-of-distribution data. Results: Here we show that for a strict\nmatch between gold-standard and predicted answers, the QA model achieves 51.65%\nF1 score. For a relaxed match between the gold-standard and predicted answers,\nthe QA model obtains 78.03% F1 score, along with 85.38% Precision and 79.02%\nRecall scores. Moreover, the QA model demonstrates consistent performance when\nsubjected to temporally out-of-distribution data. Conclusions: Our study\nintroduces a QA framework designed to extract IDU information from clinical\nnotes, aiming to enhance the accurate and efficient detection of people who\ninject drugs, extract relevant information, and ultimately facilitate informed\npatient care.", "published": "2023-05-15 16:37:00", "link": "http://arxiv.org/abs/2305.08777v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Small Models are Valuable Plug-ins for Large Language Models", "abstract": "Large language models (LLMs) such as GPT-3 and GPT-4 are powerful but their\nweights are often publicly unavailable and their immense sizes make the models\ndifficult to be tuned with common hardware. As a result, effectively tuning\nthese models with large-scale supervised data can be challenging. As an\nalternative, In-Context Learning (ICL) can only use a small number of\nsupervised examples due to context length limits. In this paper, we propose\nSuper In-Context Learning (SuperICL) which allows black-box LLMs to work with\nlocally fine-tuned smaller models, resulting in superior performance on\nsupervised tasks. Our experiments demonstrate that SuperICL can improve\nperformance beyond state-of-the-art fine-tuned models while addressing the\ninstability problem of in-context learning. Furthermore, SuperICL can enhance\nthe capabilities of smaller models, such as multilinguality and\ninterpretability.", "published": "2023-05-15 17:59:01", "link": "http://arxiv.org/abs/2305.08848v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An assessment of measuring local levels of homelessness through proxy\n  social media signals", "abstract": "Recent studies suggest social media activity can function as a proxy for\nmeasures of state-level public health, detectable through natural language\nprocessing. We present results of our efforts to apply this approach to\nestimate homelessness at the state level throughout the US during the period\n2010-2019 and 2022 using a dataset of roughly 1 million geotagged tweets\ncontaining the substring ``homeless.'' Correlations between\nhomelessness-related tweet counts and ranked per capita homelessness volume,\nbut not general-population densities, suggest a relationship between the\nlikelihood of Twitter users to personally encounter or observe homelessness in\ntheir everyday lives and their likelihood to communicate about it online. An\nincrease to the log-odds of ``homeless'' appearing in an English-language\ntweet, as well as an acceleration in the increase in average tweet sentiment,\nsuggest that tweets about homelessness are also affected by trends at the\nnation-scale. Additionally, changes to the lexical content of tweets over time\nsuggest that reversals to the polarity of national or state-level trends may be\ndetectable through an increase in political or service-sector language over the\nsemantics of charity or direct appeals. An analysis of user account type also\nrevealed changes to Twitter-use patterns by accounts authored by individuals\nversus entities that may provide an additional signal to confirm changes to\nhomelessness density in a given jurisdiction. While a computational approach to\nsocial media analysis may provide a low-cost, real-time dataset rich with\ninformation about nationwide and localized impacts of homelessness and\nhomelessness policy, we find that practical issues abound, limiting the\npotential of social media as a proxy to complement other measures of\nhomelessness.", "published": "2023-05-15 19:40:28", "link": "http://arxiv.org/abs/2305.08978v1", "categories": ["cs.SI", "cs.CL", "cs.CY"], "primary_category": "cs.SI"}
{"title": "OOD-Speech: A Large Bengali Speech Recognition Dataset for\n  Out-of-Distribution Benchmarking", "abstract": "We present OOD-Speech, the first out-of-distribution (OOD) benchmarking\ndataset for Bengali automatic speech recognition (ASR). Being one of the most\nspoken languages globally, Bengali portrays large diversity in dialects and\nprosodic features, which demands ASR frameworks to be robust towards\ndistribution shifts. For example, islamic religious sermons in Bengali are\ndelivered with a tonality that is significantly different from regular speech.\nOur training dataset is collected via massively online crowdsourcing campaigns\nwhich resulted in 1177.94 hours collected and curated from $22,645$ native\nBengali speakers from South Asia. Our test dataset comprises 23.03 hours of\nspeech collected and manually annotated from 17 different sources, e.g.,\nBengali TV drama, Audiobook, Talk show, Online class, and Islamic sermons to\nname a few. OOD-Speech is jointly the largest publicly available speech\ndataset, as well as the first out-of-distribution ASR benchmarking dataset for\nBengali.", "published": "2023-05-15 18:00:39", "link": "http://arxiv.org/abs/2305.09688v1", "categories": ["eess.AS", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Large Language Model Guided Tree-of-Thought", "abstract": "In this paper, we introduce the Tree-of-Thought (ToT) framework, a novel\napproach aimed at improving the problem-solving capabilities of auto-regressive\nlarge language models (LLMs). The ToT technique is inspired by the human mind's\napproach for solving complex reasoning tasks through trial and error. In this\nprocess, the human mind explores the solution space through a tree-like thought\nprocess, allowing for backtracking when necessary. To implement ToT as a\nsoftware system, we augment an LLM with additional modules including a prompter\nagent, a checker module, a memory module, and a ToT controller. In order to\nsolve a given problem, these modules engage in a multi-round conversation with\nthe LLM. The memory module records the conversation and state history of the\nproblem solving process, which allows the system to backtrack to the previous\nsteps of the thought-process and explore other directions from there. To verify\nthe effectiveness of the proposed technique, we implemented a ToT-based solver\nfor the Sudoku Puzzle. Experimental results show that the ToT framework can\nsignificantly increase the success rate of Sudoku puzzle solving. Our\nimplementation of the ToT-based Sudoku solver is available on GitHub:\n\\url{https://github.com/jieyilong/tree-of-thought-puzzle-solver}.", "published": "2023-05-15 01:18:23", "link": "http://arxiv.org/abs/2305.08291v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG", "cs.NE"], "primary_category": "cs.AI"}
{"title": "Continual Multimodal Knowledge Graph Construction", "abstract": "Current Multimodal Knowledge Graph Construction (MKGC) models struggle with\nthe real-world dynamism of continuously emerging entities and relations, often\nsuccumbing to catastrophic forgetting-loss of previously acquired knowledge.\nThis study introduces benchmarks aimed at fostering the development of the\ncontinual MKGC domain. We further introduce MSPT framework, designed to\nsurmount the shortcomings of existing MKGC approaches during multimedia data\nprocessing. MSPT harmonizes the retention of learned knowledge (stability) and\nthe integration of new data (plasticity), outperforming current continual\nlearning and multimodal methods. Our results confirm MSPT's superior\nperformance in evolving knowledge environments, showcasing its capacity to\nnavigate balance between stability and plasticity.", "published": "2023-05-15 14:58:28", "link": "http://arxiv.org/abs/2305.08698v3", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.LG", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Schema-adaptable Knowledge Graph Construction", "abstract": "Conventional Knowledge Graph Construction (KGC) approaches typically follow\nthe static information extraction paradigm with a closed set of pre-defined\nschema. As a result, such approaches fall short when applied to dynamic\nscenarios or domains, whereas a new type of knowledge emerges. This\nnecessitates a system that can handle evolving schema automatically to extract\ninformation for KGC. To address this need, we propose a new task called\nschema-adaptable KGC, which aims to continually extract entity, relation, and\nevent based on a dynamically changing schema graph without re-training. We\nfirst split and convert existing datasets based on three principles to build a\nbenchmark, i.e., horizontal schema expansion, vertical schema expansion, and\nhybrid schema expansion; then investigate the schema-adaptable performance of\nseveral well-known approaches such as Text2Event, TANL, UIE and GPT-3.5. We\nfurther propose a simple yet effective baseline dubbed \\textsc{AdaKGC}, which\ncontains schema-enriched prefix instructor and schema-conditioned dynamic\ndecoding to better handle evolving schema. Comprehensive experimental results\nillustrate that AdaKGC can outperform baselines but still have room for\nimprovement. We hope the proposed work can deliver benefits to the community.\nCode and datasets available at https://github.com/zjunlp/AdaKGC.", "published": "2023-05-15 15:06:20", "link": "http://arxiv.org/abs/2305.08703v4", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ForkNet: Simultaneous Time and Time-Frequency Domain Modeling for Speech\n  Enhancement", "abstract": "Previous research in speech enhancement has mostly focused on modeling time\nor time-frequency domain information alone, with little consideration given to\nthe potential benefits of simultaneously modeling both domains. Since these\ndomains contain complementary information, combining them may improve the\nperformance of the model. In this letter, we propose a new approach to\nsimultaneously model time and time-frequency domain information in a single\nmodel. We begin with the DPT-FSNet (causal version) model as a baseline and\nmodify the encoder structure by replacing the original encoder with three\nseparate encoders, each dedicated to modeling time-domain, real-imaginary, and\nmagnitude information, respectively. Additionally, we introduce a feature\nfusion module both before and after the dual-path processing blocks to better\nleverage information from the different domains. The outcomes of our\nexperiments reveal that the proposed approach achieves superior performance\ncompared to existing state-of-the-art causal models, while preserving a\nrelatively compact model size and low computational complexity.", "published": "2023-05-15 01:24:49", "link": "http://arxiv.org/abs/2305.08292v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Ripple sparse self-attention for monaural speech enhancement", "abstract": "The use of Transformer represents a recent success in speech enhancement.\nHowever, as its core component, self-attention suffers from quadratic\ncomplexity, which is computationally prohibited for long speech recordings.\nMoreover, it allows each time frame to attend to all time frames, neglecting\nthe strong local correlations of speech signals. This study presents a simple\nyet effective sparse self-attention for speech enhancement, called ripple\nattention, which simultaneously performs fine- and coarse-grained modeling for\nlocal and global dependencies, respectively. Specifically, we employ local band\nattention to enable each frame to attend to its closest neighbor frames in a\nwindow at fine granularity, while employing dilated attention outside the\nwindow to model the global dependencies at a coarse granularity. We evaluate\nthe efficacy of our ripple attention for speech enhancement on two commonly\nused training objectives. Extensive experimental results consistently confirm\nthe superior performance of the ripple attention design over standard full\nself-attention, blockwise attention, and dual-path attention (Sep-Former) in\nterms of speech quality and intelligibility.", "published": "2023-05-15 11:12:20", "link": "http://arxiv.org/abs/2305.08541v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Integrating Uncertainty into Neural Network-based Speech Enhancement", "abstract": "Supervised masking approaches in the time-frequency domain aim to employ deep\nneural networks to estimate a multiplicative mask to extract clean speech. This\nleads to a single estimate for each input without any guarantees or measures of\nreliability. In this paper, we study the benefits of modeling uncertainty in\nclean speech estimation. Prediction uncertainty is typically categorized into\naleatoric uncertainty and epistemic uncertainty. The former refers to inherent\nrandomness in data, while the latter describes uncertainty in the model\nparameters. In this work, we propose a framework to jointly model aleatoric and\nepistemic uncertainties in neural network-based speech enhancement. The\nproposed approach captures aleatoric uncertainty by estimating the statistical\nmoments of the speech posterior distribution and explicitly incorporates the\nuncertainty estimate to further improve clean speech estimation. For epistemic\nuncertainty, we investigate two Bayesian deep learning approaches: Monte Carlo\ndropout and Deep ensembles to quantify the uncertainty of the neural network\nparameters. Our analyses show that the proposed framework promotes capturing\npractical and reliable uncertainty, while combining different sources of\nuncertainties yields more reliable predictive uncertainty estimates.\nFurthermore, we demonstrate the benefits of modeling uncertainty on speech\nenhancement performance by evaluating the framework on different datasets,\nexhibiting notable improvement over comparable models that fail to account for\nuncertainty.", "published": "2023-05-15 15:55:12", "link": "http://arxiv.org/abs/2305.08744v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Whisper transformer for audio captioning trained with synthetic\n  captions and transfer learning", "abstract": "The field of audio captioning has seen significant advancements in recent\nyears, driven by the availability of large-scale audio datasets and\nadvancements in deep learning techniques. In this technical report, we present\nour approach to audio captioning, focusing on the use of a pretrained\nspeech-to-text Whisper model and pretraining on synthetic captions. We discuss\nour training procedures and present our experiments' results, which include\nmodel size variations, dataset mixtures, and other hyperparameters. Our\nfindings demonstrate the impact of different training strategies on the\nperformance of the audio captioning model. Our code and trained models are\npublicly available on GitHub and Hugging Face Hub.", "published": "2023-05-15 22:20:07", "link": "http://arxiv.org/abs/2305.09690v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
