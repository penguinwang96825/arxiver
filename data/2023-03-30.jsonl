{"title": "TLAG: An Informative Trigger and Label-Aware Knowledge Guided Model for\n  Dialogue-based Relation Extraction", "abstract": "Dialogue-based Relation Extraction (DRE) aims to predict the relation type of\nargument pairs that are mentioned in dialogue. The latest trigger-enhanced\nmethods propose trigger prediction tasks to promote DRE. However, these methods\nare not able to fully leverage the trigger information and even bring noise to\nrelation extraction. To solve these problems, we propose TLAG, which fully\nleverages the trigger and label-aware knowledge to guide the relation\nextraction. First, we design an adaptive trigger fusion module to fully\nleverage the trigger information. Then, we introduce label-aware knowledge to\nfurther promote our model's performance. Experimental results on the DialogRE\ndataset show that our TLAG outperforms the baseline models, and detailed\nanalyses demonstrate the effectiveness of our approach.", "published": "2023-03-30 03:10:28", "link": "http://arxiv.org/abs/2303.17119v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Topics in the Haystack: Extracting and Evaluating Topics beyond\n  Coherence", "abstract": "Extracting and identifying latent topics in large text corpora has gained\nincreasing importance in Natural Language Processing (NLP). Most models,\nwhether probabilistic models similar to Latent Dirichlet Allocation (LDA) or\nneural topic models, follow the same underlying approach of topic\ninterpretability and topic extraction. We propose a method that incorporates a\ndeeper understanding of both sentence and document themes, and goes beyond\nsimply analyzing word frequencies in the data. This allows our model to detect\nlatent topics that may include uncommon words or neologisms, as well as words\nnot present in the documents themselves. Additionally, we propose several new\nevaluation metrics based on intruder words and similarity measures in the\nsemantic space. We present correlation coefficients with human identification\nof intruder words and achieve near-human level results at the word-intrusion\ntask. We demonstrate the competitive performance of our method with a large\nbenchmark study, and achieve superior results compared to state-of-the-art\ntopic modeling and document clustering models.", "published": "2023-03-30 12:24:25", "link": "http://arxiv.org/abs/2303.17324v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A BERT-based Unsupervised Grammatical Error Correction Framework", "abstract": "Grammatical error correction (GEC) is a challenging task of natural language\nprocessing techniques. While more attempts are being made in this approach for\nuniversal languages like English or Chinese, relatively little work has been\ndone for low-resource languages for the lack of large annotated corpora. In\nlow-resource languages, the current unsupervised GEC based on language model\nscoring performs well. However, the pre-trained language model is still to be\nexplored in this context. This study proposes a BERT-based unsupervised GEC\nframework, where GEC is viewed as multi-class classification task. The\nframework contains three modules: data flow construction module, sentence\nperplexity scoring module, and error detecting and correcting module. We\npropose a novel scoring method for pseudo-perplexity to evaluate a sentence's\nprobable correctness and construct a Tagalog corpus for Tagalog GEC research.\nIt obtains competitive performance on the Tagalog corpus we construct and\nopen-source Indonesian corpus and it demonstrates that our framework is\ncomplementary to baseline method for low-resource GEC task.", "published": "2023-03-30 13:29:49", "link": "http://arxiv.org/abs/2303.17367v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "P-Transformer: A Prompt-based Multimodal Transformer Architecture For\n  Medical Tabular Data", "abstract": "Medical tabular data, abundant in Electronic Health Records (EHRs), is a\nvaluable resource for diverse medical tasks such as risk prediction. While deep\nlearning approaches, particularly transformer-based models, have shown\nremarkable performance in tabular data prediction, there are still problems\nremaining for existing work to be effectively adapted into medical domain, such\nas ignoring unstructured free-texts and underutilizing the textual information\nin structured data. To address these issues, we propose PTransformer, a\n\\underline{P}rompt-based multimodal \\underline{Transformer} architecture\ndesigned specifically for medical tabular data. This framework consists of two\ncritical components: a tabular cell embedding generator and a tabular\ntransformer. The former efficiently encodes diverse modalities from both\nstructured and unstructured tabular data into a harmonized language semantic\nspace with the help of pre-trained sentence encoder and medical prompts. The\nlatter integrates cell representations to generate patient embeddings for\nvarious medical tasks. In comprehensive experiments on two real-world datasets\nfor three medical tasks, PTransformer demonstrated the improvements with\n10.9%/11.0% on RMSE/MAE, 0.5%/2.2% on RMSE/MAE, and 1.6%/0.8% on BACC/AUROC\ncompared to state-of-the-art (SOTA) baselines in predictability.", "published": "2023-03-30 14:25:44", "link": "http://arxiv.org/abs/2303.17408v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assessing Cross-Cultural Alignment between ChatGPT and Human Societies:\n  An Empirical Study", "abstract": "The recent release of ChatGPT has garnered widespread recognition for its\nexceptional ability to generate human-like responses in dialogue. Given its\nusage by users from various nations and its training on a vast multilingual\ncorpus that incorporates diverse cultural and societal norms, it is crucial to\nevaluate its effectiveness in cultural adaptation. In this paper, we\ninvestigate the underlying cultural background of ChatGPT by analyzing its\nresponses to questions designed to quantify human cultural differences. Our\nfindings suggest that, when prompted with American context, ChatGPT exhibits a\nstrong alignment with American culture, but it adapts less effectively to other\ncultural contexts. Furthermore, by using different prompts to probe the model,\nwe show that English prompts reduce the variance in model responses, flattening\nout cultural differences and biasing them towards American culture. This study\nprovides valuable insights into the cultural implications of ChatGPT and\nhighlights the necessity of greater diversity and cultural awareness in\nlanguage technologies.", "published": "2023-03-30 15:43:39", "link": "http://arxiv.org/abs/2303.17466v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparing Abstractive Summaries Generated by ChatGPT to Real Summaries\n  Through Blinded Reviewers and Text Classification Algorithms", "abstract": "Large Language Models (LLMs) have gathered significant attention due to their\nimpressive performance on a variety of tasks. ChatGPT, developed by OpenAI, is\na recent addition to the family of language models and is being called a\ndisruptive technology by a few, owing to its human-like text-generation\ncapabilities. Although, many anecdotal examples across the internet have\nevaluated ChatGPT's strength and weakness, only a few systematic research\nstudies exist. To contribute to the body of literature of systematic research\non ChatGPT, we evaluate the performance of ChatGPT on Abstractive Summarization\nby the means of automated metrics and blinded human reviewers. We also build\nautomatic text classifiers to detect ChatGPT generated summaries. We found that\nwhile text classification algorithms can distinguish between real and generated\nsummaries, humans are unable to distinguish between real summaries and those\nproduced by ChatGPT.", "published": "2023-03-30 18:28:33", "link": "http://arxiv.org/abs/2303.17650v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Federated Learning Based Multilingual Emoji Prediction In Clean and\n  Attack Scenarios", "abstract": "Federated learning is a growing field in the machine learning community due\nto its decentralized and private design. Model training in federated learning\nis distributed over multiple clients giving access to lots of client data while\nmaintaining privacy. Then, a server aggregates the training done on these\nmultiple clients without access to their data, which could be emojis widely\nused in any social media service and instant messaging platforms to express\nusers' sentiments. This paper proposes federated learning-based multilingual\nemoji prediction in both clean and attack scenarios. Emoji prediction data have\nbeen crawled from both Twitter and SemEval emoji datasets. This data is used to\ntrain and evaluate different transformer model sizes including a sparsely\nactivated transformer with either the assumption of clean data in all clients\nor poisoned data via label flipping attack in some clients. Experimental\nresults on these models show that federated learning in either clean or\nattacked scenarios performs similarly to centralized training in multilingual\nemoji prediction on seen and unseen languages under different data sources and\ndistributions. Our trained transformers perform better than other techniques on\nthe SemEval emoji dataset in addition to the privacy as well as distributed\nbenefits of federated learning.", "published": "2023-03-30 15:31:49", "link": "http://arxiv.org/abs/2304.01005v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can ChatGPT be used to generate scientific hypotheses?", "abstract": "We investigate whether large language models can perform the creative\nhypothesis generation that human researchers regularly do. While the error rate\nis high, generative AI seems to be able to effectively structure vast amounts\nof scientific knowledge and provide interesting and testable hypotheses. The\nfuture scientific enterprise may include synergistic efforts with a swarm of\n\"hypothesis machines\", challenged by automated experimentation and adversarial\npeer reviews.", "published": "2023-03-30 20:40:52", "link": "http://arxiv.org/abs/2304.12208v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TreePiece: Faster Semantic Parsing via Tree Tokenization", "abstract": "Autoregressive (AR) encoder-decoder neural networks have proved successful in\nmany NLP problems, including Semantic Parsing -- a task that translates natural\nlanguage to machine-readable parse trees. However, the sequential prediction\nprocess of AR models can be slow. To accelerate AR for semantic parsing, we\nintroduce a new technique called TreePiece that tokenizes a parse tree into\nsubtrees and generates one subtree per decoding step. On TopV2 benchmark,\nTreePiece shows 4.6 times faster decoding speed than standard AR, and\ncomparable speed but significantly higher accuracy compared to\nNon-Autoregressive (NAR).", "published": "2023-03-30 05:44:44", "link": "http://arxiv.org/abs/2303.17161v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "The Nordic Pile: A 1.2TB Nordic Dataset for Language Modeling", "abstract": "Pre-training Large Language Models (LLMs) require massive amounts of text\ndata, and the performance of the LLMs typically correlates with the scale and\nquality of the datasets. This means that it may be challenging to build LLMs\nfor smaller languages such as Nordic ones, where the availability of text\ncorpora is limited. In order to facilitate the development of the LLMS in the\nNordic languages, we curate a high-quality dataset consisting of 1.2TB of text,\nin all of the major North Germanic languages (Danish, Icelandic, Norwegian, and\nSwedish), as well as some high-quality English data. This paper details our\nconsiderations and processes for collecting, cleaning, and filtering the\ndataset.", "published": "2023-03-30 06:42:22", "link": "http://arxiv.org/abs/2303.17183v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Going Beyond Nouns With Vision & Language Models Using Synthetic Data", "abstract": "Large-scale pre-trained Vision & Language (VL) models have shown remarkable\nperformance in many applications, enabling replacing a fixed set of supported\nclasses with zero-shot open vocabulary reasoning over (almost arbitrary)\nnatural language prompts. However, recent works have uncovered a fundamental\nweakness of these models. For example, their difficulty to understand Visual\nLanguage Concepts (VLC) that go 'beyond nouns' such as the meaning of\nnon-object words (e.g., attributes, actions, relations, states, etc.), or\ndifficulty in performing compositional reasoning such as understanding the\nsignificance of the order of the words in a sentence. In this work, we\ninvestigate to which extent purely synthetic data could be leveraged to teach\nthese models to overcome such shortcomings without compromising their zero-shot\ncapabilities. We contribute Synthetic Visual Concepts (SyViC) - a million-scale\nsynthetic dataset and data generation codebase allowing to generate additional\nsuitable data to improve VLC understanding and compositional reasoning of VL\nmodels. Additionally, we propose a general VL finetuning strategy for\neffectively leveraging SyViC towards achieving these improvements. Our\nextensive experiments and ablations on VL-Checklist, Winoground, and ARO\nbenchmarks demonstrate that it is possible to adapt strong pre-trained VL\nmodels with synthetic data significantly enhancing their VLC understanding\n(e.g. by 9.9% on ARO and 4.3% on VL-Checklist) with under 1% drop in their\nzero-shot accuracy.", "published": "2023-03-30 17:57:43", "link": "http://arxiv.org/abs/2303.17590v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Detecting and Grounding Important Characters in Visual Stories", "abstract": "Characters are essential to the plot of any story. Establishing the\ncharacters before writing a story can improve the clarity of the plot and the\noverall flow of the narrative. However, previous work on visual storytelling\ntends to focus on detecting objects in images and discovering relationships\nbetween them. In this approach, characters are not distinguished from other\nobjects when they are fed into the generation pipeline. The result is a\ncoherent sequence of events rather than a character-centric story. In order to\naddress this limitation, we introduce the VIST-Character dataset, which\nprovides rich character-centric annotations, including visual and textual\nco-reference chains and importance ratings for characters. Based on this\ndataset, we propose two new tasks: important character detection and character\ngrounding in visual stories. For both tasks, we develop simple, unsupervised\nmodels based on distributional similarity and pre-trained vision-and-language\nmodels. Our new dataset, together with these models, can serve as the\nfoundation for subsequent work on analysing and generating stories from a\ncharacter-centric perspective.", "published": "2023-03-30 18:24:06", "link": "http://arxiv.org/abs/2303.17647v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Aligning a medium-size GPT model in English to a small closed domain in\n  Spanish", "abstract": "In this paper, we propose a methodology to align a medium-sized GPT model,\noriginally trained in English for an open domain, to a small closed domain in\nSpanish. The application for which the model is finely tuned is the question\nanswering task. To achieve this we also needed to train and implement another\nneural network (which we called the reward model) that could score and\ndetermine whether an answer is appropriate for a given question. This component\nserved to improve the decoding and generation of the answers of the system.\nNumerical metrics such as BLEU and perplexity were used to evaluate the model,\nand human judgment was also used to compare the decoding technique with others.\nFinally, the results favored the proposed method, and it was determined that it\nis feasible to use a reward model to align the generation of responses.", "published": "2023-03-30 18:27:15", "link": "http://arxiv.org/abs/2303.17649v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Fine-Tuning BERT with Character-Level Noise for Zero-Shot Transfer to\n  Dialects and Closely-Related Languages", "abstract": "In this work, we induce character-level noise in various forms when\nfine-tuning BERT to enable zero-shot cross-lingual transfer to unseen dialects\nand languages. We fine-tune BERT on three sentence-level classification tasks\nand evaluate our approach on an assortment of unseen dialects and languages. We\nfind that character-level noise can be an extremely effective agent of\ncross-lingual transfer under certain conditions, while it is not as helpful in\nothers. Specifically, we explore these differences in terms of the nature of\nthe task and the relationships between source and target languages, finding\nthat introduction of character-level noise during fine-tuning is particularly\nhelpful when a task draws on surface level cues and the source-target\ncross-lingual pair has a relatively high lexical overlap with shorter (i.e.,\nless meaningful) unseen tokens on average.", "published": "2023-03-30 19:51:18", "link": "http://arxiv.org/abs/2303.17683v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "What Types of Questions Require Conversation to Answer? A Case Study of\n  AskReddit Questions", "abstract": "The proliferation of automated conversational systems such as chatbots,\nspoken-dialogue systems, and smart speakers, has significantly impacted modern\ndigital life. However, these systems are primarily designed to provide answers\nto well-defined questions rather than to support users in exploring complex,\nill-defined questions. In this paper, we aim to push the boundaries of\nconversational systems by examining the types of nebulous, open-ended questions\nthat can best be answered through conversation. We first sampled 500 questions\nfrom one million open-ended requests posted on AskReddit, and then recruited\nonline crowd workers to answer eight inquiries about these questions. We also\nperformed open coding to categorize the questions into 27 different domains. We\nfound that the issues people believe require conversation to resolve\nsatisfactorily are highly social and personal. Our work provides insights into\nhow future research could be geared to align with users' needs.", "published": "2023-03-30 21:05:22", "link": "http://arxiv.org/abs/2303.17710v2", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Evaluation of GPT and BERT-based models on identifying protein-protein\n  interactions in biomedical text", "abstract": "Detecting protein-protein interactions (PPIs) is crucial for understanding\ngenetic mechanisms, disease pathogenesis, and drug design. However, with the\nfast-paced growth of biomedical literature, there is a growing need for\nautomated and accurate extraction of PPIs to facilitate scientific knowledge\ndiscovery. Pre-trained language models, such as generative pre-trained\ntransformers (GPT) and bidirectional encoder representations from transformers\n(BERT), have shown promising results in natural language processing (NLP)\ntasks. We evaluated the performance of PPI identification of multiple GPT and\nBERT models using three manually curated gold-standard corpora: Learning\nLanguage in Logic (LLL) with 164 PPIs in 77 sentences, Human Protein Reference\nDatabase with 163 PPIs in 145 sentences, and Interaction Extraction Performance\nAssessment with 335 PPIs in 486 sentences. BERT-based models achieved the best\noverall performance, with BioBERT achieving the highest recall (91.95%) and\nF1-score (86.84%) and PubMedBERT achieving the highest precision (85.25%).\nInterestingly, despite not being explicitly trained for biomedical texts, GPT-4\nachieved commendable performance, comparable to the top-performing BERT models.\nIt achieved a precision of 88.37%, a recall of 85.14%, and an F1-score of\n86.49% on the LLL dataset. These results suggest that GPT models can\neffectively detect PPIs from text data, offering promising avenues for\napplication in biomedical literature mining. Further research could explore how\nthese models might be fine-tuned for even more specialized tasks within the\nbiomedical domain.", "published": "2023-03-30 22:06:10", "link": "http://arxiv.org/abs/2303.17728v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "QUADRo: Dataset and Models for QUestion-Answer Database Retrieval", "abstract": "An effective paradigm for building Automated Question Answering systems is\nthe re-use of previously answered questions, e.g., for FAQs or forum\napplications. Given a database (DB) of question/answer (q/a) pairs, it is\npossible to answer a target question by scanning the DB for similar questions.\nIn this paper, we scale this approach to open domain, making it competitive\nwith other standard methods, e.g., unstructured document or graph based. For\nthis purpose, we (i) build a large scale DB of 6.3M q/a pairs, using public\nquestions, (ii) design a new system based on neural IR and a q/a pair reranker,\nand (iii) construct training and test data to perform comparative experiments\nwith our models. We demonstrate that Transformer-based models using (q,a) pairs\noutperform models only based on question representation, for both neural search\nand reranking. Additionally, we show that our DB-based approach is competitive\nwith Web-based methods, i.e., a QA system built on top the BING search engine,\ndemonstrating the challenge of finding relevant information. Finally, we make\nour data and models available for future research.", "published": "2023-03-30 00:42:07", "link": "http://arxiv.org/abs/2304.01003v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DERA: Enhancing Large Language Model Completions with Dialog-Enabled\n  Resolving Agents", "abstract": "Large language models (LLMs) have emerged as valuable tools for many natural\nlanguage understanding tasks. In safety-critical applications such as\nhealthcare, the utility of these models is governed by their ability to\ngenerate outputs that are factually accurate and complete. In this work, we\npresent dialog-enabled resolving agents (DERA). DERA is a paradigm made\npossible by the increased conversational abilities of LLMs, namely GPT-4. It\nprovides a simple, interpretable forum for models to communicate feedback and\niteratively improve output. We frame our dialog as a discussion between two\nagent types - a Researcher, who processes information and identifies crucial\nproblem components, and a Decider, who has the autonomy to integrate the\nResearcher's information and makes judgments on the final output.\n  We test DERA against three clinically-focused tasks. For medical conversation\nsummarization and care plan generation, DERA shows significant improvement over\nthe base GPT-4 performance in both human expert preference evaluations and\nquantitative metrics. In a new finding, we also show that GPT-4's performance\n(70%) on an open-ended version of the MedQA question-answering (QA) dataset\n(Jin et al. 2021, USMLE) is well above the passing level (60%), with DERA\nshowing similar performance. We release the open-ended MEDQA dataset at\nhttps://github.com/curai/curai-research/tree/main/DERA.", "published": "2023-03-30 00:30:19", "link": "http://arxiv.org/abs/2303.17071v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Humans in Humans Out: On GPT Converging Toward Common Sense in both\n  Success and Failure", "abstract": "Increase in computational scale and fine-tuning has seen a dramatic\nimprovement in the quality of outputs of large language models (LLMs) like GPT.\nGiven that both GPT-3 and GPT-4 were trained on large quantities of\nhuman-generated text, we might ask to what extent their outputs reflect\npatterns of human thinking, both for correct and incorrect cases. The Erotetic\nTheory of Reason (ETR) provides a symbolic generative model of both human\nsuccess and failure in thinking, across propositional, quantified, and\nprobabilistic reasoning, as well as decision-making. We presented GPT-3,\nGPT-3.5, and GPT-4 with 61 central inference and judgment problems from a\nrecent book-length presentation of ETR, consisting of experimentally verified\ndata-points on human judgment and extrapolated data-points predicted by ETR,\nwith correct inference patterns as well as fallacies and framing effects (the\nETR61 benchmark). ETR61 includes classics like Wason's card task, illusory\ninferences, the decoy effect, and opportunity-cost neglect, among others. GPT-3\nshowed evidence of ETR-predicted outputs for 59% of these examples, rising to\n77% in GPT-3.5 and 75% in GPT-4. Remarkably, the production of human-like\nfallacious judgments increased from 18% in GPT-3 to 33% in GPT-3.5 and 34% in\nGPT-4. This suggests that larger and more advanced LLMs may develop a tendency\ntoward more human-like mistakes, as relevant thought patterns are inherent in\nhuman-produced training data. According to ETR, the same fundamental patterns\nare involved both in successful and unsuccessful ordinary reasoning, so that\nthe \"bad\" cases could paradoxically be learned from the \"good\" cases. We\nfurther present preliminary evidence that ETR-inspired prompt engineering could\nreduce instances of these mistakes.", "published": "2023-03-30 10:32:18", "link": "http://arxiv.org/abs/2303.17276v1", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG", "00, 68", "I.2.0; I.2.6"], "primary_category": "cs.AI"}
{"title": "Yes but.. Can ChatGPT Identify Entities in Historical Documents?", "abstract": "Large language models (LLMs) have been leveraged for several years now,\nobtaining state-of-the-art performance in recognizing entities from modern\ndocuments. For the last few months, the conversational agent ChatGPT has\n\"prompted\" a lot of interest in the scientific community and public due to its\ncapacity of generating plausible-sounding answers. In this paper, we explore\nthis ability by probing it in the named entity recognition and classification\n(NERC) task in primary sources (e.g., historical newspapers and classical\ncommentaries) in a zero-shot manner and by comparing it with state-of-the-art\nLM-based systems. Our findings indicate several shortcomings in identifying\nentities in historical text that range from the consistency of entity\nannotation guidelines, entity complexity, and code-switching, to the\nspecificity of prompting. Moreover, as expected, the inaccessibility of\nhistorical archives to the public (and thus on the Internet) also impacts its\nperformance.", "published": "2023-03-30 12:23:39", "link": "http://arxiv.org/abs/2303.17322v1", "categories": ["cs.DL", "cs.CL", "cs.IR"], "primary_category": "cs.DL"}
{"title": "WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for\n  Audio-Language Multimodal Research", "abstract": "The advancement of audio-language (AL) multimodal learning tasks has been\nsignificant in recent years. However, researchers face challenges due to the\ncostly and time-consuming collection process of existing audio-language\ndatasets, which are limited in size. To address this data scarcity issue, we\nintroduce WavCaps, the first large-scale weakly-labelled audio captioning\ndataset, comprising approximately 400k audio clips with paired captions. We\nsourced audio clips and their raw descriptions from web sources and a sound\nevent detection dataset. However, the online-harvested raw descriptions are\nhighly noisy and unsuitable for direct use in tasks such as automated audio\ncaptioning. To overcome this issue, we propose a three-stage processing\npipeline for filtering noisy data and generating high-quality captions, where\nChatGPT, a large language model, is leveraged to filter and transform raw\ndescriptions automatically. We conduct a comprehensive analysis of the\ncharacteristics of WavCaps dataset and evaluate it on multiple downstream\naudio-language multimodal learning tasks. The systems trained on WavCaps\noutperform previous state-of-the-art (SOTA) models by a significant margin. Our\naspiration is for the WavCaps dataset we have proposed to facilitate research\nin audio-language multimodal learning and demonstrate the potential of\nutilizing ChatGPT to enhance academic research. Our dataset and codes are\navailable at https://github.com/XinhaoMei/WavCaps.", "published": "2023-03-30 14:07:47", "link": "http://arxiv.org/abs/2303.17395v2", "categories": ["eess.AS", "cs.CL", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Efficient distributed representations with linear-time attention scores\n  normalization", "abstract": "The attention score matrix ${\\rm SoftMax}(XY^T)$ encodes relational\nsimilarity patterns between objects and is extremely popular in machine\nlearning. However, the complexity required to calculate it runs quadratically\nwith the problem size, making it a computationally heavy solution. In this\narticle, we propose a linear-time approximation of the attention score\nnormalization constants for embedding vectors with bounded norms. We show on\nseveral pre-trained embeddings that the accuracy of our estimation formula\nsurpasses competing kernel methods by even orders of magnitude. From this\nresult, we design a linear-time and task-agnostic embedding algorithm based on\nthe optimization of the attention scores. The proposed algorithm is highly\ninterpretable and easily adapted to an arbitrary embedding problem. We consider\na few use-cases and observe similar or higher performances and a lower\ncomputational time with respect to comparable embedding algorithms.", "published": "2023-03-30 15:48:26", "link": "http://arxiv.org/abs/2303.17475v3", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Language Models can Solve Computer Tasks", "abstract": "Agents capable of carrying out general tasks on a computer can improve\nefficiency and productivity by automating repetitive tasks and assisting in\ncomplex problem-solving. Ideally, such agents should be able to solve new\ncomputer tasks presented to them through natural language commands. However,\nprevious approaches to this problem require large amounts of expert\ndemonstrations and task-specific reward functions, both of which are\nimpractical for new tasks. In this work, we show that a pre-trained large\nlanguage model (LLM) agent can execute computer tasks guided by natural\nlanguage using a simple prompting scheme where the agent Recursively Criticizes\nand Improves its output (RCI). The RCI approach significantly outperforms\nexisting LLM methods for automating computer tasks and surpasses supervised\nlearning (SL) and reinforcement learning (RL) approaches on the MiniWoB++\nbenchmark. We compare multiple LLMs and find that RCI with the\nInstructGPT-3+RLHF LLM is state-of-the-art on MiniWoB++, using only a handful\nof demonstrations per task rather than tens of thousands, and without a\ntask-specific reward function. Furthermore, we demonstrate RCI prompting's\neffectiveness in enhancing LLMs' reasoning abilities on a suite of natural\nlanguage reasoning tasks, outperforming chain of thought (CoT) prompting with\nexternal feedback. We find that RCI combined with CoT performs better than\neither separately. Our code can be found here:\nhttps://github.com/posgnu/rci-agent.", "published": "2023-03-30 16:01:52", "link": "http://arxiv.org/abs/2303.17491v3", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Hindi as a Second Language: Improving Visually Grounded Speech with\n  Semantically Similar Samples", "abstract": "The objective of this work is to explore the learning of visually grounded\nspeech models (VGS) from multilingual perspective. Bilingual VGS models are\ngenerally trained with an equal number of spoken captions from both languages.\nHowever, in reality, there can be an imbalance among the languages for the\navailable spoken captions. Our key contribution in this work is to leverage the\npower of a high-resource language in a bilingual visually grounded speech model\nto improve the performance of a low-resource language. We introduce two methods\nto distill the knowledge of high-resource language into low-resource languages:\n(1) incorporating a strong pre-trained high-resource language encoder and (2)\nusing semantically similar spoken captions. Our experiments show that combining\nthese two approaches effectively enables the low-resource language to surpass\nthe performances of monolingual and bilingual counterparts for cross-modal\nretrieval tasks.", "published": "2023-03-30 16:34:10", "link": "http://arxiv.org/abs/2303.17517v1", "categories": ["cs.CL", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Whose Opinions Do Language Models Reflect?", "abstract": "Language models (LMs) are increasingly being used in open-ended contexts,\nwhere the opinions reflected by LMs in response to subjective queries can have\na profound impact, both on user satisfaction, as well as shaping the views of\nsociety at large. In this work, we put forth a quantitative framework to\ninvestigate the opinions reflected by LMs -- by leveraging high-quality public\nopinion polls and their associated human responses. Using this framework, we\ncreate OpinionsQA, a new dataset for evaluating the alignment of LM opinions\nwith those of 60 US demographic groups over topics ranging from abortion to\nautomation. Across topics, we find substantial misalignment between the views\nreflected by current LMs and those of US demographic groups: on par with the\nDemocrat-Republican divide on climate change. Notably, this misalignment\npersists even after explicitly steering the LMs towards particular demographic\ngroups. Our analysis not only confirms prior observations about the\nleft-leaning tendencies of some human feedback-tuned LMs, but also surfaces\ngroups whose opinions are poorly reflected by current LMs (e.g., 65+ and\nwidowed individuals). Our code and data are available at\nhttps://github.com/tatsu-lab/opinions_qa.", "published": "2023-03-30 17:17:08", "link": "http://arxiv.org/abs/2303.17548v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Recognition, recall, and retention of few-shot memories in large\n  language models", "abstract": "The training of modern large language models (LLMs) takes place in a regime\nwhere most training examples are seen only a few times by the model during the\ncourse of training. What does a model remember about such examples seen only a\nfew times during training and how long does that memory persist in the face of\ncontinuous training with new examples? Here, we investigate these questions\nthrough simple recognition, recall, and retention experiments with LLMs. In\nrecognition experiments, we ask if the model can distinguish the seen example\nfrom a novel example; in recall experiments, we ask if the model can correctly\nrecall the seen example when cued by a part of it; and in retention\nexperiments, we periodically probe the model's memory for the original examples\nas the model is trained continuously with new examples. We find that a single\nexposure is generally sufficient for a model to achieve near perfect accuracy\neven in very challenging recognition experiments. We estimate that the\nrecognition performance of even small language models easily exceeds human\nrecognition performance reported in similar experiments with humans (Shepard,\n1967). Achieving near perfect recall takes more exposures, but most models can\ndo it in just 3 exposures. The flip side of this remarkable capacity for fast\nlearning is that precise memories are quickly overwritten: recall performance\nfor the original examples drops steeply over the first 10 training updates with\nnew examples, followed by a more gradual decline. Even after 100K updates,\nhowever, some of the original examples are still recalled near perfectly. A\nqualitatively similar retention pattern has been observed in human long-term\nmemory retention studies before (Bahrick, 1984). Finally, recognition is much\nmore robust to interference than recall and memory for natural language\nsentences is generally superior to memory for stimuli without structure.", "published": "2023-03-30 17:26:16", "link": "http://arxiv.org/abs/2303.17557v1", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "BloombergGPT: A Large Language Model for Finance", "abstract": "The use of NLP in the realm of financial technology is broad and complex,\nwith applications ranging from sentiment analysis and named entity recognition\nto question answering. Large Language Models (LLMs) have been shown to be\neffective on a variety of tasks; however, no LLM specialized for the financial\ndomain has been reported in literature. In this work, we present BloombergGPT,\na 50 billion parameter language model that is trained on a wide range of\nfinancial data. We construct a 363 billion token dataset based on Bloomberg's\nextensive data sources, perhaps the largest domain-specific dataset yet,\naugmented with 345 billion tokens from general purpose datasets. We validate\nBloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite\nof internal benchmarks that most accurately reflect our intended usage. Our\nmixed dataset training leads to a model that outperforms existing models on\nfinancial tasks by significant margins without sacrificing performance on\ngeneral LLM benchmarks. Additionally, we explain our modeling choices, training\nprocess, and evaluation methodology. We release Training Chronicles (Appendix\nC) detailing our experience in training BloombergGPT.", "published": "2023-03-30 17:30:36", "link": "http://arxiv.org/abs/2303.17564v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "q-fin.GN"], "primary_category": "cs.LG"}
{"title": "Elastic Weight Removal for Faithful and Abstractive Dialogue Generation", "abstract": "Ideally, dialogue systems should generate responses that are faithful to the\nknowledge contained in relevant documents. However, many models generate\nhallucinated responses instead that contradict it or contain unverifiable\ninformation. To mitigate such undesirable behaviour, it has been proposed to\nfine-tune a `negative expert' on negative examples and subtract its parameters\nfrom those of a pre-trained model. However, intuitively, this does not take\ninto account that some parameters are more responsible than others in causing\nhallucinations. Thus, we propose to weigh their individual importance via (an\napproximation of) the Fisher Information matrix, which measures the uncertainty\nof their estimate. We call this method Elastic Weight Removal (EWR). We\nevaluate our method -- using different variants of Flan-T5 as a backbone\nlanguage model -- on multiple datasets for information-seeking dialogue\ngeneration and compare our method with state-of-the-art techniques for\nfaithfulness, such as CTRL, Quark, DExperts, and Noisy Channel reranking.\nExtensive automatic and human evaluation shows that EWR systematically\nincreases faithfulness at minor costs in terms of other metrics. However, we\nnotice that only discouraging hallucinations may increase extractiveness, i.e.\nshallow copy-pasting of document spans, which can be undesirable. Hence, as a\nsecond main contribution, we show that our method can be extended to\nsimultaneously discourage hallucinations and extractive responses. We publicly\nrelease the code for reproducing EWR and all baselines.", "published": "2023-03-30 17:40:30", "link": "http://arxiv.org/abs/2303.17574v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging\n  Face", "abstract": "Solving complicated AI tasks with different domains and modalities is a key\nstep toward artificial general intelligence. While there are numerous AI models\navailable for various domains and modalities, they cannot handle complicated AI\ntasks autonomously. Considering large language models (LLMs) have exhibited\nexceptional abilities in language understanding, generation, interaction, and\nreasoning, we advocate that LLMs could act as a controller to manage existing\nAI models to solve complicated AI tasks, with language serving as a generic\ninterface to empower this. Based on this philosophy, we present HuggingGPT, an\nLLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI\nmodels in machine learning communities (e.g., Hugging Face) to solve AI tasks.\nSpecifically, we use ChatGPT to conduct task planning when receiving a user\nrequest, select models according to their function descriptions available in\nHugging Face, execute each subtask with the selected AI model, and summarize\nthe response according to the execution results. By leveraging the strong\nlanguage capability of ChatGPT and abundant AI models in Hugging Face,\nHuggingGPT can tackle a wide range of sophisticated AI tasks spanning different\nmodalities and domains and achieve impressive results in language, vision,\nspeech, and other challenging tasks, which paves a new way towards the\nrealization of artificial general intelligence.", "published": "2023-03-30 17:48:28", "link": "http://arxiv.org/abs/2303.17580v4", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "oBERTa: Improving Sparse Transfer Learning via improved initialization,\n  distillation, and pruning regimes", "abstract": "In this paper, we introduce the range of oBERTa language models, an\neasy-to-use set of language models which allows Natural Language Processing\n(NLP) practitioners to obtain between 3.8 and 24.3 times faster models without\nexpertise in model compression. Specifically, oBERTa extends existing work on\npruning, knowledge distillation, and quantization and leverages frozen\nembeddings improves distillation and model initialization to deliver higher\naccuracy on a broad range of transfer tasks. In generating oBERTa, we explore\nhow the highly optimized RoBERTa differs from the BERT for pruning during\npre-training and finetuning. We find it less amenable to compression during\nfine-tuning. We explore the use of oBERTa on seven representative NLP tasks and\nfind that the improved compression techniques allow a pruned oBERTa model to\nmatch the performance of BERTbase and exceed the performance of Prune OFA Large\non the SQUAD V1.1 Question Answering dataset, despite being 8x and 2x,\nrespectively faster in inference. We release our code, training regimes, and\nassociated model for broad usage to encourage usage and experimentation", "published": "2023-03-30 01:37:19", "link": "http://arxiv.org/abs/2303.17612v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Self-Refine: Iterative Refinement with Self-Feedback", "abstract": "Like humans, large language models (LLMs) do not always generate the best\noutput on their first try. Motivated by how humans refine their written text,\nwe introduce Self-Refine, an approach for improving initial outputs from LLMs\nthrough iterative feedback and refinement. The main idea is to generate an\ninitial output using an LLMs; then, the same LLMs provides feedback for its\noutput and uses it to refine itself, iteratively. Self-Refine does not require\nany supervised training data, additional training, or reinforcement learning,\nand instead uses a single LLM as the generator, refiner, and feedback provider.\nWe evaluate Self-Refine across 7 diverse tasks, ranging from dialog response\ngeneration to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT,\nand GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine\nare preferred by humans and automatic metrics over those generated with the\nsame LLM using conventional one-step generation, improving by ~20% absolute on\naverage in task performance. Our work demonstrates that even state-of-the-art\nLLMs like GPT-4 can be further improved at test time using our simple,\nstandalone approach.", "published": "2023-03-30 18:30:01", "link": "http://arxiv.org/abs/2303.17651v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Task Oriented Conversational Modelling With Subjective Knowledge", "abstract": "Existing conversational models are handled by a database(DB) and API based\nsystems. However, very often users' questions require information that cannot\nbe handled by such systems. Nonetheless, answers to these questions are\navailable in the form of customer reviews and FAQs. DSTC-11 proposes a three\nstage pipeline consisting of knowledge seeking turn detection, knowledge\nselection and response generation to create a conversational model grounded on\nthis subjective knowledge. In this paper, we focus on improving the knowledge\nselection module to enhance the overall system performance. In particular, we\npropose entity retrieval methods which result in an accurate and faster\nknowledge search. Our proposed Named Entity Recognition (NER) based entity\nretrieval method results in 7X faster search compared to the baseline model.\nAdditionally, we also explore a potential keyword extraction method which can\nimprove the accuracy of knowledge selection. Preliminary results show a 4 \\%\nimprovement in exact match score on knowledge selection task. The code is\navailable https://github.com/raja-kumar/knowledge-grounded-TODS", "published": "2023-03-30 20:23:49", "link": "http://arxiv.org/abs/2303.17695v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Solving morphological analogies: from retrieval to generation", "abstract": "Analogical inference is a remarkable capability of human reasoning, and has\nbeen used to solve hard reasoning tasks. Analogy based reasoning (AR) has\ngained increasing interest from the artificial intelligence community and has\nshown its potential in multiple machine learning tasks such as classification,\ndecision making and recommendation with competitive results. We propose a deep\nlearning (DL) framework to address and tackle two key tasks in AR: analogy\ndetection and solving. The framework is thoroughly tested on the Siganalogies\ndataset of morphological analogical proportions (APs) between words, and shown\nto outperform symbolic approaches in many languages. Previous work have\nexplored the behavior of the Analogy Neural Network for classification (ANNc)\non analogy detection and of the Analogy Neural Network for retrieval (ANNr) on\nanalogy solving by retrieval, as well as the potential of an autoencoder (AE)\nfor analogy solving by generating the solution word. In this article we\nsummarize these findings and we extend them by combining ANNr and the AE\nembedding model, and checking the performance of ANNc as an retrieval method.\nThe combination of ANNr and AE outperforms the other approaches in almost all\ncases, and ANNc as a retrieval method achieves competitive or better\nperformance than 3CosMul. We conclude with general guidelines on using our\nframework to tackle APs with DL.", "published": "2023-03-30 12:36:46", "link": "http://arxiv.org/abs/2303.18062v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unsupervised Word Segmentation Using Temporal Gradient Pseudo-Labels", "abstract": "Unsupervised word segmentation in audio utterances is challenging as, in\nspeech, there is typically no gap between words. In a preliminary experiment,\nwe show that recent deep self-supervised features are very effective for word\nsegmentation but require supervision for training the classification head. To\nextend their effectiveness to unsupervised word segmentation, we propose a\npseudo-labeling strategy. Our approach relies on the observation that the\ntemporal gradient magnitude of the embeddings (i.e. the distance between the\nembeddings of subsequent frames) is typically minimal far from the boundaries\nand higher nearer the boundaries. We use a thresholding function on the\ntemporal gradient magnitude to define a psuedo-label for wordness. We train a\nlinear classifier, mapping the embedding of a single frame to the pseudo-label.\nFinally, we use the classifier score to predict whether a frame is a word or a\nboundary. In an empirical investigation, our method, despite its simplicity and\nfast run time, is shown to significantly outperform all previous methods on two\ndatasets.", "published": "2023-03-30 17:59:46", "link": "http://arxiv.org/abs/2304.00993v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Synthesis of Mathematical programs from Natural Language Specifications", "abstract": "Several decision problems that are encountered in various business domains\ncan be modeled as mathematical programs, i.e. optimization problems. The\nprocess of conducting such modeling often requires the involvement of experts\ntrained in operations research and advanced algorithms. Surprisingly, despite\nthe significant advances in the methods for program and code synthesis, AutoML,\nlearning to optimize etc., there has been little or no attention paid to\nautomating the task of synthesizing mathematical programs. We imagine a\nscenario where the specifications for modeling, i.e. the objective and\nconstraints are expressed in an unstructured form in natural language (NL) and\nthe mathematical program has to be synthesized from such an NL specification.\nIn this work we evaluate the efficacy of employing CodeT5 with data\naugmentation and post-processing of beams. We utilize GPT-3 with back\ntranslation for generation of synthetic examples. Further we apply rules of\nlinear programming to score beams and correct beams based on common error\npatterns. We observe that with these enhancements CodeT5 base gives an\nexecution accuracy of 0.73 which is significantly better than zero-shot\nexecution accuracy of 0.41 by ChatGPT and 0.36 by Codex.", "published": "2023-03-30 06:10:00", "link": "http://arxiv.org/abs/2304.03287v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Social Biases through the Text-to-Image Generation Lens", "abstract": "Text-to-Image (T2I) generation is enabling new applications that support\ncreators, designers, and general end users of productivity software by\ngenerating illustrative content with high photorealism starting from a given\ndescriptive text as a prompt. Such models are however trained on massive\namounts of web data, which surfaces the peril of potential harmful biases that\nmay leak in the generation process itself. In this paper, we take a\nmulti-dimensional approach to studying and quantifying common social biases as\nreflected in the generated images, by focusing on how occupations, personality\ntraits, and everyday situations are depicted across representations of\n(perceived) gender, age, race, and geographical location. Through an extensive\nset of both automated and human evaluation experiments we present findings for\ntwo popular T2I models: DALLE-v2 and Stable Diffusion. Our results reveal that\nthere exist severe occupational biases of neutral prompts majorly excluding\ngroups of people from results for both models. Such biases can get mitigated by\nincreasing the amount of specification in the prompt itself, although the\nprompting mitigation will not address discrepancies in image quality or other\nusages of the model or its representations in other scenarios. Further, we\nobserve personality traits being associated with only a limited set of people\nat the intersection of race, gender, and age. Finally, an analysis of\ngeographical location representations on everyday situations (e.g., park, food,\nweddings) shows that for most situations, images generated through default\nlocation-neutral prompts are closer and more similar to images generated for\nlocations of United States and Germany.", "published": "2023-03-30 05:29:13", "link": "http://arxiv.org/abs/2304.06034v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.CY"}
{"title": "PROCTER: PROnunciation-aware ConTextual adaptER for personalized speech\n  recognition in neural transducers", "abstract": "End-to-End (E2E) automatic speech recognition (ASR) systems used in voice\nassistants often have difficulties recognizing infrequent words personalized to\nthe user, such as names and places. Rare words often have non-trivial\npronunciations, and in such cases, human knowledge in the form of a\npronunciation lexicon can be useful. We propose a PROnunCiation-aware\nconTextual adaptER (PROCTER) that dynamically injects lexicon knowledge into an\nRNN-T model by adding a phonemic embedding along with a textual embedding. The\nexperimental results show that the proposed PROCTER architecture outperforms\nthe baseline RNN-T model by improving the word error rate (WER) by 44% and 57%\nwhen measured on personalized entities and personalized rare entities,\nrespectively, while increasing the model size (number of trainable parameters)\nby only 1%. Furthermore, when evaluated in a zero-shot setting to recognize\npersonalized device names, we observe 7% WER improvement with PROCTER, as\ncompared to only 1% WER improvement with text-only contextual attention", "published": "2023-03-30 03:36:32", "link": "http://arxiv.org/abs/2303.17131v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SynthVSR: Scaling Up Visual Speech Recognition With Synthetic\n  Supervision", "abstract": "Recently reported state-of-the-art results in visual speech recognition (VSR)\noften rely on increasingly large amounts of video data, while the publicly\navailable transcribed video datasets are limited in size. In this paper, for\nthe first time, we study the potential of leveraging synthetic visual data for\nVSR. Our method, termed SynthVSR, substantially improves the performance of VSR\nsystems with synthetic lip movements. The key idea behind SynthVSR is to\nleverage a speech-driven lip animation model that generates lip movements\nconditioned on the input speech. The speech-driven lip animation model is\ntrained on an unlabeled audio-visual dataset and could be further optimized\ntowards a pre-trained VSR model when labeled videos are available. As plenty of\ntranscribed acoustic data and face images are available, we are able to\ngenerate large-scale synthetic data using the proposed lip animation model for\nsemi-supervised VSR training. We evaluate the performance of our approach on\nthe largest public VSR benchmark - Lip Reading Sentences 3 (LRS3). SynthVSR\nachieves a WER of 43.3% with only 30 hours of real labeled data, outperforming\noff-the-shelf approaches using thousands of hours of video. The WER is further\nreduced to 27.9% when using all 438 hours of labeled data from LRS3, which is\non par with the state-of-the-art self-supervised AV-HuBERT method. Furthermore,\nwhen combined with large-scale pseudo-labeled audio-visual data SynthVSR yields\na new state-of-the-art VSR WER of 16.9% using publicly available data only,\nsurpassing the recent state-of-the-art approaches trained with 29 times more\nnon-public machine-transcribed video data (90,000 hours). Finally, we perform\nextensive ablation studies to understand the effect of each component in our\nproposed method.", "published": "2023-03-30 07:43:27", "link": "http://arxiv.org/abs/2303.17200v2", "categories": ["cs.CV", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Prefix tuning for automated audio captioning", "abstract": "Audio captioning aims to generate text descriptions from environmental\nsounds. One challenge of audio captioning is the difficulty of the\ngeneralization due to the lack of audio-text paired training data. In this\nwork, we propose a simple yet effective method of dealing with small-scaled\ndatasets by leveraging a pre-trained language model. We keep the language model\nfrozen to maintain the expressivity for text generation, and we only learn to\nextract global and temporal features from the input audio. To bridge a modality\ngap between the audio features and the language model, we employ mapping\nnetworks that translate audio features to the continuous vectors the language\nmodel can understand, called prefixes. We evaluate our proposed method on the\nClotho and AudioCaps dataset and show our method outperforms prior arts in\ndiverse experimental settings.", "published": "2023-03-30 16:01:28", "link": "http://arxiv.org/abs/2303.17489v2", "categories": ["eess.AS", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The Music Note Ontology", "abstract": "In this paper we propose the Music Note Ontology, an ontology for modelling\nmusic notes and their realisation. The ontology addresses the relation between\na note represented in a symbolic representation system, and its realisation,\ni.e. a musical performance. This work therefore aims to solve the modelling and\nrepresentation issues that arise when analysing the relationships between\nabstract symbolic features and the corresponding physical features of an audio\nsignal. The ontology is composed of three different Ontology Design Patterns\n(ODP), which model the structure of the score (Score Part Pattern), the note in\nthe symbolic notation (Music Note Pattern) and its realisation (Musical Object\nPattern).", "published": "2023-03-30 10:51:10", "link": "http://arxiv.org/abs/2304.00986v1", "categories": ["cs.AI", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.AI"}
{"title": "The Music Annotation Pattern", "abstract": "The annotation of music content is a complex process to represent due to its\ninherent multifaceted, subjectivity, and interdisciplinary nature. Numerous\nsystems and conventions for annotating music have been developed as independent\nstandards over the past decades. Little has been done to make them\ninteroperable, which jeopardises cross-corpora studies as it requires users to\nfamiliarise with a multitude of conventions. Most of these systems lack the\nsemantic expressiveness needed to represent the complexity of the musical\nlanguage and cannot model multi-modal annotations originating from audio and\nsymbolic sources. In this article, we introduce the Music Annotation Pattern,\nan Ontology Design Pattern (ODP) to homogenise different annotation systems and\nto represent several types of musical objects (e.g. chords, patterns,\nstructures). This ODP preserves the semantics of the object's content at\ndifferent levels and temporal granularity. Moreover, our ODP accounts for\nmulti-modality upfront, to describe annotations derived from different sources,\nand it is the first to enable the integration of music datasets at a large\nscale.", "published": "2023-03-30 11:13:59", "link": "http://arxiv.org/abs/2304.00988v1", "categories": ["cs.AI", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.AI"}
{"title": "Sound to Visual Scene Generation by Audio-to-Visual Latent Alignment", "abstract": "How does audio describe the world around us? In this paper, we propose a\nmethod for generating an image of a scene from sound. Our method addresses the\nchallenges of dealing with the large gaps that often exist between sight and\nsound. We design a model that works by scheduling the learning procedure of\neach model component to associate audio-visual modalities despite their\ninformation gaps. The key idea is to enrich the audio features with visual\ninformation by learning to align audio to visual latent space. We translate the\ninput audio to visual features, then use a pre-trained generator to produce an\nimage. To further improve the quality of our generated images, we use sound\nsource localization to select the audio-visual pairs that have strong\ncross-modal correlations. We obtain substantially better results on the VEGAS\nand VGGSound datasets than prior approaches. We also show that we can control\nour model's predictions by applying simple manipulations to the input waveform,\nor to the latent space.", "published": "2023-03-30 16:01:50", "link": "http://arxiv.org/abs/2303.17490v1", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.CV"}
