{"title": "Optimal payoff under Bregman-Wasserstein divergence constraints", "abstract": "We study optimal payoff choice for an expected utility maximizer under the\nconstraint that their payoff is not allowed to deviate ``too much'' from a\ngiven benchmark. We solve this problem when the deviation is assessed via a\nBregman-Wasserstein (BW) divergence, generated by a convex function $\\phi$.\nUnlike the Wasserstein distance (i.e., when $\\phi(x)=x^2$). The inherent\nasymmetry of the BW divergence makes it possible to penalize positive\ndeviations different than negative ones. As a main contribution, we provide the\noptimal payoff in this setting. Numerical examples illustrate that the choice\nof $\\phi$ allow to better align the payoff choice with the objectives of\ninvestors.", "published": "2024-11-27 14:40:20", "link": "http://arxiv.org/abs/2411.18397v1", "categories": ["q-fin.PM", "q-fin.MF", "q-fin.RM"], "primary_category": "q-fin.PM"}
{"title": "Semiclassical CEV Option Pricing Model: an Analytical Approach", "abstract": "This paper is devoted to obtain closed form solutions for the semiclassical\n(or WKB) approximation of the heat kernel propagator of the diffusion equation\ndefined by the constant elasticity variance (CEV) option pricing model. One of\nthe key points is that our calculations are based on the Van Vleck-Morette\ndeterminant instead of the Van Vleck determinant used by other authors. In\nfact, we compute this determinant in two different ways: by means of the\nsolution of the classical Hamiltonian equations, and by solving the variational\nequations. Furthermore, the calculation reveals an exponential factor in the\nprefactor of the kernel not considered in previous works.", "published": "2024-11-27 09:02:25", "link": "http://arxiv.org/abs/2411.18154v3", "categories": ["q-fin.MF"], "primary_category": "q-fin.MF"}
{"title": "Limit Order Book Event Stream Prediction with Diffusion Model", "abstract": "Limit order book (LOB) is a dynamic, event-driven system that records\nreal-time market demand and supply for a financial asset in a stream flow.\nEvent stream prediction in LOB refers to forecasting both the timing and the\ntype of events. The challenge lies in modeling the time-event distribution to\ncapture the interdependence between time and event type, which has\ntraditionally relied on stochastic point processes. However, modeling complex\nmarket dynamics using stochastic processes, e.g., Hawke stochastic process, can\nbe simplistic and struggle to capture the evolution of market dynamics. In this\nstudy, we present LOBDIF (LOB event stream prediction with diffusion model),\nwhich offers a new paradigm for event stream prediction within the LOB system.\nLOBDIF learns the complex time-event distribution by leveraging a diffusion\nmodel, which decomposes the time-event distribution into sequential steps, with\neach step represented by a Gaussian distribution. Additionally, we propose a\ndenoising network and a skip-step sampling strategy. The former facilitates\neffective learning of time-event interdependence, while the latter accelerates\nthe sampling process during inference. By introducing a diffusion model, our\napproach breaks away from traditional modeling paradigms, offering novel\ninsights and providing an effective and efficient solution for learning the\ntime-event distribution in order streams within the LOB system. Extensive\nexperiments using real-world data from the limit order books of three widely\ntraded assets confirm that LOBDIF significantly outperforms current\nstate-of-the-art methods.", "published": "2024-11-27 19:52:43", "link": "http://arxiv.org/abs/2412.09631v1", "categories": ["q-fin.ST"], "primary_category": "q-fin.ST"}
{"title": "QuaLLM-Health: An Adaptation of an LLM-Based Framework for Quantitative\n  Data Extraction from Online Health Discussions", "abstract": "Health-related discussions on social media like Reddit offer valuable\ninsights, but extracting quantitative data from unstructured text is\nchallenging. In this work, we present an adapted framework from QuaLLM into\nQuaLLM-Health for extracting clinically relevant quantitative data from Reddit\ndiscussions about glucagon-like peptide-1 (GLP-1) receptor agonists using large\nlanguage models (LLMs). We collected 410k posts and comments from five\nGLP-1-related communities using the Reddit API in July 2024. After filtering\nfor cancer-related discussions, 2,059 unique entries remained. We developed\nannotation guidelines to manually extract variables such as cancer\nsurvivorship, family cancer history, cancer types mentioned, risk perceptions,\nand discussions with physicians. Two domain-experts independently annotated a\nrandom sample of 100 entries to create a gold-standard dataset. We then\nemployed iterative prompt engineering with OpenAI's \"GPT-4o-mini\" on the\ngold-standard dataset to build an optimized pipeline that allowed us to extract\nvariables from the large dataset. The optimized LLM achieved accuracies above\n0.85 for all variables, with precision, recall and F1 score macro averaged >\n0.90, indicating balanced performance. Stability testing showed a 95% match\nrate across runs, confirming consistency. Applying the framework to the full\ndataset enabled efficient extraction of variables necessary for downstream\nanalysis, costing under $3 and completing in approximately one hour.\nQuaLLM-Health demonstrates that LLMs can effectively and efficiently extract\nclinically relevant quantitative data from unstructured social media content.\nIncorporating human expertise and iterative prompt refinement ensures accuracy\nand reliability. This methodology can be adapted for large-scale analysis of\npatient-generated data across various health domains, facilitating valuable\ninsights for healthcare research.", "published": "2024-11-27 00:52:21", "link": "http://arxiv.org/abs/2411.17967v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DRS: Deep Question Reformulation With Structured Output", "abstract": "Question answering represents a core capability of large language models\n(LLMs). However, when individuals encounter unfamiliar knowledge in texts, they\noften formulate questions that the text itself cannot answer due to\ninsufficient understanding of the underlying information. Recent studies reveal\nthat while LLMs can detect unanswerable questions, they struggle to assist\nusers in reformulating these questions. Even advanced models like GPT-3.5\ndemonstrate limited effectiveness in this regard. To address this limitation,\nwe propose DRS: Deep Question Reformulation with Structured Output, a novel\nzero-shot method aimed at enhancing LLMs ability to assist users in\nreformulating questions to extract relevant information from new documents. DRS\ncombines the strengths of LLMs with a DFS-based algorithm to iteratively\nexplore potential entity combinations and constrain outputs using predefined\nentities. This structured approach significantly enhances the reformulation\ncapabilities of LLMs. Comprehensive experimental evaluations demonstrate that\nDRS improves the reformulation accuracy of GPT-3.5 from 23.03% to 70.42%, while\nalso enhancing the performance of open-source models, such as Gemma2-9B, from\n26.35% to 56.75%.", "published": "2024-11-27 02:20:44", "link": "http://arxiv.org/abs/2411.17993v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can bidirectional encoder become the ultimate winner for downstream\n  applications of foundation models?", "abstract": "Over the past few decades, Artificial Intelligence(AI) has progressed from\nthe initial machine learning stage to the deep learning stage, and now to the\nstage of foundational models. Foundational models have the characteristics of\npre-training, transfer learning, and self-supervised learning, and pre-trained\nmodels can be fine-tuned and applied to various downstream tasks. Under the\nframework of foundational models, models such as Bidirectional Encoder\nRepresentations from Transformers(BERT) and Generative Pre-trained\nTransformer(GPT) have greatly advanced the development of natural language\nprocessing(NLP), especially the emergence of many models based on BERT. BERT\nbroke through the limitation of only using one-way methods for language\nmodeling in pre-training by using a masked language model. It can capture\nbidirectional context information to predict the masked words in the sequence,\nthis can improve the feature extraction ability of the model. This makes the\nmodel very useful for downstream tasks, especially for specialized\napplications. The model using the bidirectional encoder can better understand\nthe domain knowledge and be better applied to these downstream tasks. So we\nhope to help understand how this technology has evolved and improved model\nperformance in various natural language processing tasks under the background\nof foundational models and reveal its importance in capturing context\ninformation and improving the model's performance on downstream tasks. This\narticle analyzes one-way and bidirectional models based on GPT and BERT and\ncompares their differences based on the purpose of the model. It also briefly\nanalyzes BERT and the improvements of some models based on BERT. The model's\nperformance on the Stanford Question Answering Dataset(SQuAD) and General\nLanguage Understanding Evaluation(GLUE) was compared.", "published": "2024-11-27 03:31:14", "link": "http://arxiv.org/abs/2411.18021v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-Tuning Small Embeddings for Elevated Performance", "abstract": "Contextual Embeddings have yielded state-of-the-art results in various\nnatural language processing tasks. However, these embeddings are constrained by\nmodels requiring large amounts of data and huge computing power. This is an\nissue for low-resource languages like Nepali as the amount of data available\nover the internet is not always sufficient for the models. This work has taken\nan incomplete BERT model with six attention heads pretrained on Nepali language\nand finetuned it on previously unseen data. The obtained results from intrinsic\nand extrinsic evaluations have been compared to the results drawn from the\noriginal model baseline and a complete BERT model pretrained on Nepali language\nas the oracle. The results demonstrate that even though the oracle is better on\naverage, finetuning the small embeddings drastically improves results compared\nto the original baseline.", "published": "2024-11-27 07:25:07", "link": "http://arxiv.org/abs/2411.18099v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Curriculum Demonstration Selection for In-Context Learning", "abstract": "Large Language Models (LLMs) have shown strong in-context learning (ICL)\nabilities with a few demonstrations. However, one critical challenge is how to\nselect demonstrations to elicit the full potential of LLMs. In this paper, we\npropose Curriculum Demonstration Selection (CDS), a novel demonstration\nselection method for ICL. Instead of merely using similarity, CDS additionally\npartitions samples by their complexity measurements. Following curriculum\nlearning, CDS then selects demonstrations from easy to difficult. Thus the\nselected demonstrations cover a wide range of difficulty levels, enabling LLMs\nto learn from varied complexities within the training set. Experiments\ndemonstrate that our CDS consistently outperforms baseline methods, achieving\nnotable improvements across nine LLMs on three benchmarks. Moreover, CDS proves\nespecially effective in enhancing LLM performance in solving challenging\nproblems.", "published": "2024-11-27 08:16:41", "link": "http://arxiv.org/abs/2411.18126v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SentiXRL: An advanced large language Model Framework for Multilingual\n  Fine-Grained Emotion Classification in Complex Text Environment", "abstract": "With strong expressive capabilities in Large Language Models(LLMs),\ngenerative models effectively capture sentiment structures and deep semantics,\nhowever, challenges remain in fine-grained sentiment classification across\nmulti-lingual and complex contexts. To address this, we propose the Sentiment\nCross-Lingual Recognition and Logic Framework (SentiXRL), which incorporates\ntwo modules,an emotion retrieval enhancement module to improve sentiment\nclassification accuracy in complex contexts through historical dialogue and\nlogical reasoning,and a self-circulating analysis negotiation mechanism\n(SANM)to facilitates autonomous decision-making within a single model for\nclassification tasks.We have validated SentiXRL's superiority on multiple\nstandard datasets, outperforming existing models on CPED and CH-SIMS,and\nachieving overall better performance on MELD,Emorynlp and IEMOCAP. Notably, we\nunified labels across several fine-grained sentiment annotation datasets and\nconducted category confusion experiments, revealing challenges and impacts of\nclass imbalance in standard datasets.", "published": "2024-11-27 09:18:26", "link": "http://arxiv.org/abs/2411.18162v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MetaphorShare: A Dynamic Collaborative Repository of Open Metaphor\n  Datasets", "abstract": "The metaphor studies community has developed numerous valuable labelled\ncorpora in various languages over the years. Many of these resources are not\nonly unknown to the NLP community, but are also often not easily shared among\nthe researchers. Both in human sciences and in NLP, researchers could benefit\nfrom a centralised database of labelled resources, easily accessible and\nunified under an identical format. To facilitate this, we present\nMetaphorShare, a website to integrate metaphor datasets making them open and\naccessible. With this effort, our aim is to encourage researchers to share and\nupload more datasets in any language in order to facilitate metaphor studies\nand the development of future metaphor processing NLP systems. The website has\nfour main functionalities: upload, download, search and label metaphor\ndatasets. It is accessible at www.metaphorshare.com.", "published": "2024-11-27 11:58:34", "link": "http://arxiv.org/abs/2411.18260v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neutralizing Backdoors through Information Conflicts for Large Language\n  Models", "abstract": "Large language models (LLMs) have seen significant advancements, achieving\nsuperior performance in various Natural Language Processing (NLP) tasks, from\nunderstanding to reasoning. However, they remain vulnerable to backdoor\nattacks, where models behave normally for standard queries but generate harmful\nresponses or unintended output when specific triggers are activated. Existing\nbackdoor defenses often suffer from drawbacks that they either focus on\ndetection without removal, rely on rigid assumptions about trigger properties,\nor prove to be ineffective against advanced attacks like multi-trigger\nbackdoors. In this paper, we present a novel method to eliminate backdoor\nbehaviors from LLMs through the construction of information conflicts using\nboth internal and external mechanisms. Internally, we leverage a lightweight\ndataset to train a conflict model, which is then merged with the backdoored\nmodel to neutralize malicious behaviors by embedding contradictory information\nwithin the model's parametric memory. Externally, we incorporate convincing\ncontradictory evidence into the prompt to challenge the model's internal\nbackdoor knowledge. Experimental results on classification and conversational\ntasks across 4 widely used LLMs demonstrate that our method outperforms 8\nstate-of-the-art backdoor defense baselines. We can reduce the attack success\nrate of advanced backdoor attacks by up to 98% while maintaining over 90% clean\ndata accuracy. Furthermore, our method has proven to be robust against adaptive\nbackdoor attacks. The code will be open-sourced upon publication.", "published": "2024-11-27 12:15:22", "link": "http://arxiv.org/abs/2411.18280v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can LLMs assist with Ambiguity? A Quantitative Evaluation of various\n  Large Language Models on Word Sense Disambiguation", "abstract": "Ambiguous words are often found in modern digital communications. Lexical\nambiguity challenges traditional Word Sense Disambiguation (WSD) methods, due\nto limited data. Consequently, the efficiency of translation, information\nretrieval, and question-answering systems is hindered by these limitations.\nThis study investigates the use of Large Language Models (LLMs) to improve WSD\nusing a novel approach combining a systematic prompt augmentation mechanism\nwith a knowledge base (KB) consisting of different sense interpretations. The\nproposed method incorporates a human-in-loop approach for prompt augmentation\nwhere prompt is supported by Part-of-Speech (POS) tagging, synonyms of\nambiguous words, aspect-based sense filtering and few-shot prompting to guide\nthe LLM. By utilizing a few-shot Chain of Thought (COT) prompting-based\napproach, this work demonstrates a substantial improvement in performance. The\nevaluation was conducted using FEWS test data and sense tags. This research\nadvances accurate word interpretation in social media and digital\ncommunication.", "published": "2024-11-27 13:35:32", "link": "http://arxiv.org/abs/2411.18337v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Parole de pr\u00e9sidents (1958-2022)", "abstract": "En plus de soixante ans, huit pr\\'esidents se sont succ\\'ed\\'e \\`a la t\\^ete\nde la Ve R\\'epublique fran\\c{c}aise (de Gaulle, Pompidou, Giscard d'Estaing,\nMitterrand, Chirac, Sarkozy, Hollande, Macron). Apr\\`es avoir pr\\'esent\\'e le\ncorpus de leurs discours -- soit 9202 textes et plus de 20 millions de mots\n\\'etiquet\\'es -- le style de chacun des pr\\'esidents sera caract\\'eris\\'e \\`a\nl'aide de leurs vocabulaire (vocables et cat\\'egories grammaticales). Une\nanalyse plus approfondie r\\'ev\\`ele les s\\'equences typiques de chaque\nlocataire de l'\\'Elys\\'ee. Bas\\'ee sur les distances entre l'ensemble des\nallocutions, une figure illustre les similitudes et diff\\'erences entre les\ndiff\\'erents pr\\'esidents.\n  Over the past sixty-six years, eight presidents successively headed the Fifth\nFrench Republic (de Gaulle, Pompidou, Giscard d'Estaing, Mitterrand, Chirac,\nSarkozy, Holland, Macron). After presenting the corpus of their speeches --\n9,202 texts and more than 20 million labelled words -- the style of each of\nthem will be characterized by their vocabulary (lemmas and part-of-speech). A\ndeeper analysis reveals the typical sequences of each tenant of the Elys\\'ee.\nBased on an intertextual distance between all presidential speeches, a\nsynthesis can be drawn reflecting the similarities and differences between\npresidents.", "published": "2024-11-27 16:01:51", "link": "http://arxiv.org/abs/2411.18468v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Examples: High-level Automated Reasoning Paradigm in In-Context\n  Learning via MCTS", "abstract": "In-context Learning (ICL) enables large language models (LLMs) to tackle\ndownstream tasks through sophisticated prompting and high-quality\ndemonstrations. However, this traditional ICL paradigm shows limitations when\nfacing complex mathematical reasoning tasks, primarily due to its heavy\ndependence on example quality and the necessity for human intervention in\nchallenging scenarios. To address these limitations, this paper presents\nHiAR-ICL, a \\textbf{Hi}gh-level \\textbf{A}utomated \\textbf{R}easoning paradigm\nin \\textbf{ICL} that shifts focus from specific examples to abstract thinking\npatterns, extending the conventional concept of context in ICL. HiAR-ICL\nintroduces five atomic reasoning actions as fundamental components for\nconstructing chain-structured patterns. Using Monte Carlo Tree Search, we\nexplore reasoning paths and construct thought cards to guide subsequent\ninference. We then develop a cognitive complexity framework that dynamically\nmatches problems with appropriate thought cards. Experimental results\ndemonstrate HiAR-ICL's effectiveness, achieving state-of-the-art accuracy\n(79.6$\\%$) on the MATH benchmark with Qwen2.5-7B-Instruct, surpassing GPT-4o\n(76.6$\\%$) and Claude 3.5 (71.1$\\%$).", "published": "2024-11-27 16:19:00", "link": "http://arxiv.org/abs/2411.18478v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Retrofitting Large Language Models with Dynamic Tokenization", "abstract": "Current language models (LMs) use a fixed, static subword tokenizer. This\ndefault choice typically results in degraded efficiency and language\ncapabilities, especially in languages other than English. To address this\nissue, we challenge the static design and propose retrofitting LMs with dynamic\ntokenization: a way to dynamically decide on token boundaries based on the\ninput text via a subword-merging algorithm inspired by byte-pair encoding. We\nmerge frequent subword sequences in a batch, then apply a pre-trained\nembedding-prediction hypernetwork to compute the token embeddings on-the-fly.\nFor encoder-style models (e.g., XLM-R), this on average reduces token sequence\nlengths by >20% across 14 languages while degrading performance by less than\n2%. The same method applied to pre-filling and scoring in decoder-style models\n(e.g., Mistral-7B; evaluated on English) results in minimal performance\ndegradation at up to 6% reduction in sequence length. Overall, we find that\ndynamic tokenization can mitigate the limitations of static tokenization by\nsubstantially improving inference speed and promoting fairness across\nlanguages, enabling more equitable and adaptable LMs.", "published": "2024-11-27 17:51:58", "link": "http://arxiv.org/abs/2411.18553v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-Tuning Large Language Models for Scientific Text Classification: A\n  Comparative Study", "abstract": "The exponential growth of online textual content across diverse domains has\nnecessitated advanced methods for automated text classification. Large Language\nModels (LLMs) based on transformer architectures have shown significant success\nin this area, particularly in natural language processing (NLP) tasks. However,\ngeneral-purpose LLMs often struggle with domain-specific content, such as\nscientific texts, due to unique challenges like specialized vocabulary and\nimbalanced data. In this study, we fine-tune four state-of-the-art LLMs BERT,\nSciBERT, BioBERT, and BlueBERT on three datasets derived from the WoS-46985\ndataset to evaluate their performance in scientific text classification. Our\nexperiments reveal that domain-specific models, particularly SciBERT,\nconsistently outperform general-purpose models in both abstract-based and\nkeyword-based classification tasks. Additionally, we compare our achieved\nresults with those reported in the literature for deep learning models, further\nhighlighting the advantages of LLMs, especially when utilized in specific\ndomains. The findings emphasize the importance of domain-specific adaptations\nfor LLMs to enhance their effectiveness in specialized text classification\ntasks.", "published": "2024-11-27 18:58:53", "link": "http://arxiv.org/abs/2412.00098v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "VideoLLM Knows When to Speak: Enhancing Time-Sensitive Video\n  Comprehension with Video-Text Duet Interaction Format", "abstract": "Recent researches on video large language models (VideoLLM) predominantly\nfocus on model architectures and training datasets, leaving the interaction\nformat between the user and the model under-explored. In existing works, users\noften interact with VideoLLMs by using the entire video and a query as input,\nafter which the model generates a response. This interaction format constrains\nthe application of VideoLLMs in scenarios such as live-streaming comprehension\nwhere videos do not end and responses are required in a real-time manner, and\nalso results in unsatisfactory performance on time-sensitive tasks that\nrequires localizing video segments. In this paper, we focus on a video-text\nduet interaction format. This interaction format is characterized by the\ncontinuous playback of the video, and both the user and the model can insert\ntheir text messages at any position during the video playback. When a text\nmessage ends, the video continues to play, akin to the alternative of two\nperformers in a duet. We construct MMDuetIT, a video-text training dataset\ndesigned to adapt VideoLLMs to video-text duet interaction format. We also\nintroduce the Multi-Answer Grounded Video Question Answering (MAGQA) task to\nbenchmark the real-time response ability of VideoLLMs. Trained on MMDuetIT,\nMMDuet demonstrates that adopting the video-text duet interaction format\nenables the model to achieve significant improvements in various time-sensitive\ntasks (76% CIDEr on YouCook2 dense video captioning, 90\\% mAP on QVHighlights\nhighlight detection and 25% R@0.5 on Charades-STA temporal video grounding)\nwith minimal training efforts, and also enable VideoLLMs to reply in a\nreal-time manner as the video plays. Code, data and demo are available at:\nhttps://github.com/yellow-binary-tree/MMDuet.", "published": "2024-11-27 02:15:34", "link": "http://arxiv.org/abs/2411.17991v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "New Faithfulness-Centric Interpretability Paradigms for Natural Language\n  Processing", "abstract": "As machine learning becomes more widespread and is used in more critical\napplications, it's important to provide explanations for these models, to\nprevent unintended behavior. Unfortunately, many current interpretability\nmethods struggle with faithfulness. Therefore, this Ph.D. thesis investigates\nthe question \"How to provide and ensure faithful explanations for complex\ngeneral-purpose neural NLP models?\" The main thesis is that we should develop\nnew paradigms in interpretability. This is achieved by first developing solid\nfaithfulness metrics and then applying the lessons learned from this\ninvestigation to develop new paradigms. The two new paradigms explored are\nfaithfulness measurable models (FMMs) and self-explanations. The idea in\nself-explanations is to have large language models explain themselves, we\nidentify that current models are not capable of doing this consistently.\nHowever, we suggest how this could be achieved. The idea of FMMs is to create\nmodels that are designed such that measuring faithfulness is cheap and precise.\nThis makes it possible to optimize an explanation towards maximum faithfulness,\nwhich makes FMMs designed to be explained. We find that FMMs yield explanations\nthat are near theoretical optimal in terms of faithfulness. Overall, from all\ninvestigations of faithfulness, results show that post-hoc and intrinsic\nexplanations are by default model and task-dependent. However, this was not the\ncase when using FMMs, even with the same post-hoc explanation methods. This\nshows, that even simple modifications to the model, such as randomly masking\nthe training dataset, as was done in FMMs, can drastically change the situation\nand result in consistently faithful explanations. This answers the question of\nhow to provide and ensure faithful explanations.", "published": "2024-11-27 02:17:34", "link": "http://arxiv.org/abs/2411.17992v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MiniKV: Pushing the Limits of LLM Inference via 2-Bit\n  Layer-Discriminative KV Cache", "abstract": "How to efficiently serve LLMs in practice has become exceptionally\nchallenging due to their prohibitive memory and computation requirements. In\nthis study, we investigate optimizing the KV cache, whose memory footprint\nposes a critical bottleneck in LLM inference, especially when dealing with long\ncontext tasks. To tackle the challenge, we introduce MiniKV, a KV cache\noptimization method that simultaneously preserves long context task accuracy\nwhile significantly reducing KV cache size via a novel 2-bit\nlayer-discriminative KV cache. More importantly, we develop specialized CUDA\nkernels to make MiniKV compatible with FlashAttention. Experiments on a wide\nrange of long context tasks show that MiniKV effectively achieves 86% KV cache\ncompression ratio while recovering over 98.5% of accuracy, outperforming\nstate-of-the-art methods while achieving excellent measured system performance\nimprovements.", "published": "2024-11-27 06:10:49", "link": "http://arxiv.org/abs/2411.18077v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A survey on cutting-edge relation extraction techniques based on\n  language models", "abstract": "This comprehensive survey delves into the latest advancements in Relation\nExtraction (RE), a pivotal task in natural language processing essential for\napplications across biomedical, financial, and legal sectors. This study\nhighlights the evolution and current state of RE techniques by analyzing 137\npapers presented at the Association for Computational Linguistics (ACL)\nconferences over the past four years, focusing on models that leverage language\nmodels. Our findings underscore the dominance of BERT-based methods in\nachieving state-of-the-art results for RE while also noting the promising\ncapabilities of emerging large language models (LLMs) like T5, especially in\nfew-shot relation extraction scenarios where they excel in identifying\npreviously unseen relations.", "published": "2024-11-27 09:04:47", "link": "http://arxiv.org/abs/2411.18157v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning", "abstract": "Vision-language models (VLMs) have shown remarkable advancements in\nmultimodal reasoning tasks. However, they still often generate inaccurate or\nirrelevant responses due to issues like hallucinated image understandings or\nunrefined reasoning paths. To address these challenges, we introduce Critic-V,\na novel framework inspired by the Actor-Critic paradigm to boost the reasoning\ncapability of VLMs. This framework decouples the reasoning process and critic\nprocess by integrating two independent components: the Reasoner, which\ngenerates reasoning paths based on visual and textual inputs, and the Critic,\nwhich provides constructive critique to refine these paths. In this approach,\nthe Reasoner generates reasoning responses according to text prompts, which can\nevolve iteratively as a policy based on feedback from the Critic. This\ninteraction process was theoretically driven by a reinforcement learning\nframework where the Critic offers natural language critiques instead of scalar\nrewards, enabling more nuanced feedback to boost the Reasoner's capability on\ncomplex reasoning tasks. The Critic model is trained using Direct Preference\nOptimization (DPO), leveraging a preference dataset of critiques ranked by\nRule-based Reward~(RBR) to enhance its critic capabilities. Evaluation results\nshow that the Critic-V framework significantly outperforms existing methods,\nincluding GPT-4V, on 5 out of 8 benchmarks, especially regarding reasoning\naccuracy and efficiency. Combining a dynamic text-based policy for the Reasoner\nand constructive feedback from the preference-optimized Critic enables a more\nreliable and context-sensitive multimodal reasoning process. Our approach\nprovides a promising solution to enhance the reliability of VLMs, improving\ntheir performance in real-world reasoning-heavy multimodal applications such as\nautonomous driving and embodied intelligence.", "published": "2024-11-27 10:28:57", "link": "http://arxiv.org/abs/2411.18203v4", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Thai Financial Domain Adaptation of THaLLE -- Technical Report", "abstract": "Large Language Models (LLMs) excel in general tasks but struggle with\ndomain-specific challenges, such as specialized terminology and localized\nregulations. Existing financial LLMs, like FinGPT and BloombergGPT, lack\nsupport for the Thai financial domain. We developed a Thai Financial LLM using\nthe Investment Consultant (IC) exam dataset from the Stock Exchange of\nThailand. To address dataset limitations, we applied data augmentation, ReLoRA\nfor efficient training, Continued Pretraining (CPT) for domain knowledge, and\nRank-Stabilized LoRA (rsLoRA) for fine-tuning. Supervised Fine-Tuning (SFT)\nsimulated exam scenarios, while Direct Preference Optimization (DPO) refined\nthe model using feedback. The model achieved scores of 72%, 72%, and 84% on IC\nexam levels P1, P2, and P3, respectively, demonstrating its effectiveness in\nThai financial advisory tasks and its potential for specialized applications.", "published": "2024-11-27 11:30:00", "link": "http://arxiv.org/abs/2411.18242v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A gentle push funziona benissimo: making instructed models in Italian\n  via contrastive activation steering", "abstract": "Adapting models to a language that was only partially present in the\npre-training data requires fine-tuning, which is expensive in terms of both\ndata and computational resources. As an alternative to fine-tuning, we explore\nthe potential of activation steering-based techniques to enhance model\nperformance on Italian tasks. Through our experiments we show that Italian\nsteering (i) can be successfully applied to different models, (ii) achieves\nperformances comparable to, or even better than, fine-tuned models for Italian,\nand (iii) yields higher quality and consistency in Italian generations. We also\ndiscuss the utility of steering and fine-tuning in the contemporary LLM\nlandscape where models are anyway getting high Italian performances even if not\nexplicitly trained in this language.", "published": "2024-11-27 11:38:09", "link": "http://arxiv.org/abs/2411.18247v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Hidden Data Privacy Breaches in Federated Learning", "abstract": "Federated Learning (FL) emerged as a paradigm for conducting machine learning\nacross broad and decentralized datasets, promising enhanced privacy by\nobviating the need for direct data sharing. However, recent studies show that\nattackers can steal private data through model manipulation or gradient\nanalysis. Existing attacks are constrained by low theft quantity or\nlow-resolution data, and they are often detected through anomaly monitoring in\ngradients or weights. In this paper, we propose a novel data-reconstruction\nattack leveraging malicious code injection, supported by two key techniques,\ni.e., distinctive and sparse encoding design and block partitioning. Unlike\nconventional methods that require detectable changes to the model, our method\nstealthily embeds a hidden model using parameter sharing to systematically\nextract sensitive data. The Fibonacci-based index design ensures efficient,\nstructured retrieval of memorized data, while the block partitioning method\nenhances our method's capability to handle high-resolution images by dividing\nthem into smaller, manageable units. Extensive experiments on 4 datasets\nconfirmed that our method is superior to the five state-of-the-art\ndata-reconstruction attacks under the five respective detection methods. Our\nmethod can handle large-scale and high-resolution data without being detected\nor mitigated by state-of-the-art data reconstruction defense methods. In\ncontrast to baselines, our method can be directly applied to both FedAVG and\nFedSGD scenarios, underscoring the need for developers to devise new defenses\nagainst such vulnerabilities. We will open-source our code upon acceptance.", "published": "2024-11-27 12:04:37", "link": "http://arxiv.org/abs/2411.18269v1", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Topic Modeling and Sentiment Analysis on Japanese Online Media's\n  Coverage of Nuclear Energy", "abstract": "Thirteen years after the Fukushima Daiichi nuclear power plant accident,\nJapan's nuclear energy accounts for only approximately 6% of electricity\nproduction, as most nuclear plants remain shut down. To revitalize the nuclear\nindustry and achieve sustainable development goals, effective communication\nwith Japanese citizens, grounded in an accurate understanding of public\nsentiment, is of paramount importance. While nationwide surveys have\ntraditionally been used to gauge public views, the rise of social media in\nrecent years has provided a promising new avenue for understanding public\nsentiment. To explore domestic sentiment on nuclear energy-related issues\nexpressed online, we analyzed the content and comments of over 3,000 YouTube\nvideos covering topics related to nuclear energy. Topic modeling was used to\nextract the main topics from the videos, and sentiment analysis with large\nlanguage models classified user sentiments towards each topic. Additionally,\nword co-occurrence network analysis was performed to examine the shift in\nonline discussions during August and September 2023 regarding the release of\ntreated water. Overall, our results provide valuable insights into the online\ndiscourse on nuclear energy and contribute to a more comprehensive\nunderstanding of public sentiment in Japan.", "published": "2024-11-27 14:29:18", "link": "http://arxiv.org/abs/2411.18383v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Politicians vs ChatGPT. A study of presuppositions in French and Italian\n  political communication", "abstract": "This paper aims to provide a comparison between texts produced by French and\nItalian politicians on polarizing issues, such as immigration and the European\nUnion, and their chatbot counterparts created with ChatGPT 3.5. In this study,\nwe focus on implicit communication, in particular on presuppositions and their\nfunctions in discourse, which have been considered in the literature as a\npotential linguistic feature of manipulation. This study also aims to\ncontribute to the emerging literature on the pragmatic competences of Large\nLanguage Models.", "published": "2024-11-27 14:46:41", "link": "http://arxiv.org/abs/2411.18403v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Is my Meeting Summary Good? Estimating Quality with a Multi-LLM\n  Evaluator", "abstract": "The quality of meeting summaries generated by natural language generation\n(NLG) systems is hard to measure automatically. Established metrics such as\nROUGE and BERTScore have a relatively low correlation with human judgments and\nfail to capture nuanced errors. Recent studies suggest using large language\nmodels (LLMs), which have the benefit of better context understanding and\nadaption of error definitions without training on a large number of human\npreference judgments. However, current LLM-based evaluators risk masking errors\nand can only serve as a weak proxy, leaving human evaluation the gold standard\ndespite being costly and hard to compare across studies. In this work, we\npresent MESA, an LLM-based framework employing a three-step assessment of\nindividual error types, multi-agent discussion for decision refinement, and\nfeedback-based self-training to refine error definition understanding and\nalignment with human judgment. We show that MESA's components enable thorough\nerror detection, consistent rating, and adaptability to custom error\nguidelines. Using GPT-4o as its backbone, MESA achieves mid to high\nPoint-Biserial correlation with human judgment in error detection and mid\nSpearman and Kendall correlation in reflecting error impact on summary quality,\non average 0.25 higher than previous methods. The framework's flexibility in\nadapting to custom error guidelines makes it suitable for various tasks with\nlimited human-labeled data.", "published": "2024-11-27 15:35:32", "link": "http://arxiv.org/abs/2411.18444v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Draft Model Knows When to Stop: A Self-Verification Length Policy for\n  Speculative Decoding", "abstract": "Speculative Decoding (SD) has become an important technique in accelerating\nthe inference speed of large language models. Conventional SD methods employ a\nfixed draft length, which ignores the token generation difficulty across tasks.\nConsequently, in this paper, we address such an issue and introduce SVIP - a\ndifficulty-aware dynamic draft length policy for speculative decoding systems.\nBased on a theoretical lower bound of draft token acceptance rate and its\ninference-time approximation, SVIP adaptively determines the lengths of draft\nsequences based on the entropy of each draft token distribution. Experimental\nresults on mainstream SD benchmarks and frameworks demonstrate the superior\nperformance of SVIP, achieving up to 20\\% walltime speedup on SpecBench over\nbaseline SD methods and 60\\% speedup on MT-Bench for long-form generation of up\nto 8K tokens. Moreover, SVIP is totally training-free and compatible with any\nexisting SD methods that generate draft tokens autoregressively. Experimental\nresults also show that SVIP yields consistent walltime improvement on top of\nGliDe & CaPE and EAGLE-2.", "published": "2024-11-27 15:53:17", "link": "http://arxiv.org/abs/2411.18462v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Isolating authorship from content with semantic embeddings and\n  contrastive learning", "abstract": "Authorship has entangled style and content inside. Authors frequently write\nabout the same topics in the same style, so when different authors write about\nthe exact same topic the easiest way out to distinguish them is by\nunderstanding the nuances of their style. Modern neural models for authorship\ncan pick up these features using contrastive learning, however, some amount of\ncontent leakage is always present. Our aim is to reduce the inevitable impact\nand correlation between content and authorship. We present a technique to use\ncontrastive learning (InfoNCE) with additional hard negatives synthetically\ncreated using a semantic similarity model. This disentanglement technique aims\nto distance the content embedding space from the style embedding space, leading\nto embeddings more informed by style. We demonstrate the performance with\nablations on two different datasets and compare them on out-of-domain\nchallenges. Improvements are clearly shown on challenging evaluations on\nprolific authors with up to a 10% increase in accuracy when the settings are\nparticularly hard. Trials on challenges also demonstrate the preservation of\nzero-shot capabilities of this method as fine tuning.", "published": "2024-11-27 16:08:46", "link": "http://arxiv.org/abs/2411.18472v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Emergence of Self-Identity in AI: A Mathematical Framework and Empirical\n  Study with Generative Large Language Models", "abstract": "This paper introduces a mathematical framework for defining and quantifying\nself-identity in artificial intelligence (AI) systems, addressing a critical\ngap in the theoretical foundations of artificial consciousness. While existing\napproaches to artificial self-awareness often rely on heuristic implementations\nor philosophical abstractions, we present a formal framework grounded in metric\nspace theory, measure theory, and functional analysis. Our framework posits\nthat self-identity emerges from two mathematically quantifiable conditions: the\nexistence of a connected continuum of memories $C \\subseteq \\mathcal{M}$ in a\nmetric space $(\\mathcal{M}, d_{\\mathcal{M}})$, and a continuous mapping $I:\n\\mathcal{M} \\to \\mathcal{S}$ that maintains consistent self-recognition across\nthis continuum, where $(\\mathcal{S}, d_{\\mathcal{S}})$ represents the metric\nspace of possible self-identities. To validate this theoretical framework, we\nconducted empirical experiments using the Llama 3.2 1B model, employing\nLow-Rank Adaptation (LoRA) for efficient fine-tuning. The model was trained on\na synthetic dataset containing temporally structured memories, designed to\ncapture the complexity of coherent self-identity formation. Our evaluation\nmetrics included quantitative measures of self-awareness, response consistency,\nand linguistic precision. The experimental results demonstrate substantial\nimprovements in measurable self-awareness metrics, with the primary\nself-awareness score increasing from 0.276 to 0.801. This enables the\nstructured creation of AI systems with validated self-identity features. The\nimplications of our study are immediately relevant to the fields of humanoid\nrobotics and autonomous systems.", "published": "2024-11-27 17:23:47", "link": "http://arxiv.org/abs/2411.18530v1", "categories": ["cs.CL", "math.MG"], "primary_category": "cs.CL"}
{"title": "Dspy-based Neural-Symbolic Pipeline to Enhance Spatial Reasoning in LLMs", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, yet they often struggle with spatial reasoning. This paper\npresents a novel neural-symbolic framework that enhances LLMs' spatial\nreasoning abilities through iterative feedback between LLMs and Answer Set\nProgramming (ASP). We evaluate our approach on two benchmark datasets: StepGame\nand SparQA, implementing three distinct strategies: (1) direct prompting\nbaseline, (2) Facts+Rules prompting, and (3) DSPy-based LLM+ASP pipeline with\niterative refinement. Our experimental results demonstrate that the LLM+ASP\npipeline significantly outperforms baseline methods, achieving an average 82%\naccuracy on StepGame and 69% on SparQA, marking improvements of 40-50% and\n8-15% respectively over direct prompting. The success stems from three key\ninnovations: (1) effective separation of semantic parsing and logical reasoning\nthrough a modular pipeline, (2) iterative feedback mechanism between LLMs and\nASP solvers that improves program rate, and (3) robust error handling that\naddresses parsing, grounding, and solving failures. Additionally, we propose\nFacts+Rules as a lightweight alternative that achieves comparable performance\non complex SparQA dataset, while reducing computational overhead.Our analysis\nacross different LLM architectures (Deepseek, Llama3-70B, GPT-4.0 mini)\ndemonstrates the framework's generalizability and provides insights into the\ntrade-offs between implementation complexity and reasoning capability,\ncontributing to the development of more interpretable and reliable AI systems.", "published": "2024-11-27 18:04:05", "link": "http://arxiv.org/abs/2411.18564v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Challenges in Adapting Multilingual LLMs to Low-Resource Languages using\n  LoRA PEFT Tuning", "abstract": "Large Language Models (LLMs) have demonstrated remarkable multilingual\ncapabilities, yet challenges persist in adapting these models for low-resource\nlanguages. In this study, we investigate the effects of Low-Rank Adaptation\n(LoRA) Parameter-Efficient Fine-Tuning (PEFT) on multilingual Gemma models for\nMarathi, a language with limited resources. Using a translated Alpaca dataset\nwith 52,000 instruction-response pairs, our findings reveal that while\nevaluation metrics often show a performance decline post-fine-tuning, manual\nassessments frequently suggest that the fine-tuned models outperform their\noriginal counterparts. The observations indicate improvements in target\nlanguage generation capabilities but a reduction in reasoning abilities\nfollowing language adaptation. These results underscore the need for improved\nevaluation methodologies and the creation of high-quality native datasets to\naccurately assess language-specific model performance in low-resource settings.", "published": "2024-11-27 18:14:38", "link": "http://arxiv.org/abs/2411.18571v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On Importance of Code-Mixed Embeddings for Hate Speech Identification", "abstract": "Code-mixing is the practice of using two or more languages in a single\nsentence, which often occurs in multilingual communities such as India where\npeople commonly speak multiple languages. Classic NLP tools, trained on\nmonolingual data, face challenges when dealing with code-mixed data. Extracting\nmeaningful information from sentences containing multiple languages becomes\ndifficult, particularly in tasks like hate speech detection, due to linguistic\nvariation, cultural nuances, and data sparsity. To address this, we aim to\nanalyze the significance of code-mixed embeddings and evaluate the performance\nof BERT and HingBERT models (trained on a Hindi-English corpus) in hate speech\ndetection. Our study demonstrates that HingBERT models, benefiting from\ntraining on the extensive Hindi-English dataset L3Cube-HingCorpus, outperform\nBERT models when tested on hate speech text datasets. We also found that\ncode-mixed Hing-FastText performs better than standard English FastText and\nvanilla BERT models.", "published": "2024-11-27 18:23:57", "link": "http://arxiv.org/abs/2411.18577v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An indicator for effectiveness of text-to-image guardrails utilizing the\n  Single-Turn Crescendo Attack (STCA)", "abstract": "The Single-Turn Crescendo Attack (STCA), first introduced in Aqrawi and\nAbbasi [2024], is an innovative method designed to bypass the ethical\nsafeguards of text-to-text AI models, compelling them to generate harmful\ncontent. This technique leverages a strategic escalation of context within a\nsingle prompt, combined with trust-building mechanisms, to subtly deceive the\nmodel into producing unintended outputs. Extending the application of STCA to\ntext-to-image models, we demonstrate its efficacy by compromising the\nguardrails of a widely-used model, DALL-E 3, achieving outputs comparable to\noutputs from the uncensored model Flux Schnell, which served as a baseline\ncontrol. This study provides a framework for researchers to rigorously evaluate\nthe robustness of guardrails in text-to-image models and benchmark their\nresilience against adversarial attacks.", "published": "2024-11-27 19:09:16", "link": "http://arxiv.org/abs/2411.18699v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "On the Effectiveness of Incremental Training of Large Language Models", "abstract": "Training large language models is a computationally intensive process that\noften requires substantial resources to achieve state-of-the-art results.\nIncremental layer-wise training has been proposed as a potential strategy to\noptimize the training process by progressively introducing layers, with the\nexpectation that this approach would lead to faster convergence and more\nefficient use of computational resources. In this paper, we investigate the\neffectiveness of incremental training for LLMs, dividing the training process\ninto multiple stages where layers are added progressively. Our experimental\nresults indicate that while the incremental approach initially demonstrates\nsome computational efficiency, it ultimately requires greater overall\ncomputational costs to reach comparable performance to traditional full-scale\ntraining. Although the incremental training process can eventually close the\nperformance gap with the baseline, it does so only after significantly extended\ncontinual training. These findings suggest that incremental layer-wise training\nmay not be a viable alternative for training large language models,\nhighlighting its limitations and providing valuable insights into the\ninefficiencies of this approach.", "published": "2024-11-27 19:11:49", "link": "http://arxiv.org/abs/2411.18700v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evaluating Vision-Language Models as Evaluators in Path Planning", "abstract": "Despite their promise to perform complex reasoning, large language models\n(LLMs) have been shown to have limited effectiveness in end-to-end planning.\nThis has inspired an intriguing question: if these models cannot plan well, can\nthey still contribute to the planning framework as a helpful plan evaluator? In\nthis work, we generalize this question to consider LLMs augmented with visual\nunderstanding, i.e., Vision-Language Models (VLMs). We introduce PathEval, a\nnovel benchmark evaluating VLMs as plan evaluators in complex path-planning\nscenarios. Succeeding in the benchmark requires a VLM to be able to abstract\ntraits of optimal paths from the scenario description, demonstrate precise\nlow-level perception on each path, and integrate this information to decide the\nbetter path. Our analysis of state-of-the-art VLMs reveals that these models\nface significant challenges on the benchmark. We observe that the VLMs can\nprecisely abstract given scenarios to identify the desired traits and exhibit\nmixed performance in integrating the provided information. Yet, their vision\ncomponent presents a critical bottleneck, with models struggling to perceive\nlow-level details about a path. Our experimental results show that this issue\ncannot be trivially addressed via end-to-end fine-tuning; rather, task-specific\ndiscriminative adaptation of these vision encoders is needed for these VLMs to\nbecome effective path evaluators.", "published": "2024-11-27 19:32:03", "link": "http://arxiv.org/abs/2411.18711v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Reconstructing Animals and the Wild", "abstract": "The idea of 3D reconstruction as scene understanding is foundational in\ncomputer vision. Reconstructing 3D scenes from 2D visual observations requires\nstrong priors to disambiguate structure. Much work has been focused on the\nanthropocentric, which, characterized by smooth surfaces, coherent normals, and\nregular edges, allows for the integration of strong geometric inductive biases.\nHere, we consider a more challenging problem where such assumptions do not\nhold: the reconstruction of natural scenes containing trees, bushes, boulders,\nand animals. While numerous works have attempted to tackle the problem of\nreconstructing animals in the wild, they have focused solely on the animal,\nneglecting environmental context. This limits their usefulness for analysis\ntasks, as animals exist inherently within the 3D world, and information is lost\nwhen environmental factors are disregarded. We propose a method to reconstruct\nnatural scenes from single images. We base our approach on recent advances\nleveraging the strong world priors ingrained in Large Language Models and train\nan autoregressive model to decode a CLIP embedding into a structured\ncompositional scene representation, encompassing both animals and the wild\n(RAW). To enable this, we propose a synthetic dataset comprising one million\nimages and thousands of assets. Our approach, having been trained solely on\nsynthetic data, generalizes to the task of reconstructing animals and their\nenvironments in real-world images. We will release our dataset and code to\nencourage future research at https://raw.is.tue.mpg.de/", "published": "2024-11-27 23:24:27", "link": "http://arxiv.org/abs/2411.18807v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Enhancing Document AI Data Generation Through Graph-Based Synthetic\n  Layouts", "abstract": "The development of robust Document AI models has been constrained by limited\naccess to high-quality, labeled datasets, primarily due to data privacy\nconcerns, scarcity, and the high cost of manual annotation. Traditional methods\nof synthetic data generation, such as text and image augmentation, have proven\neffective for increasing data diversity but often fail to capture the complex\nlayout structures present in real world documents. This paper proposes a novel\napproach to synthetic document layout generation using Graph Neural Networks\n(GNNs). By representing document elements (e.g., text blocks, images, tables)\nas nodes in a graph and their spatial relationships as edges, GNNs are trained\nto generate realistic and diverse document layouts. This method leverages\ngraph-based learning to ensure structural coherence and semantic consistency,\naddressing the limitations of traditional augmentation techniques. The proposed\nframework is evaluated on tasks such as document classification, named entity\nrecognition (NER), and information extraction, demonstrating significant\nperformance improvements. Furthermore, we address the computational challenges\nof GNN based synthetic data generation and propose solutions to mitigate domain\nadaptation issues between synthetic and real-world datasets. Our experimental\nresults show that graph-augmented document layouts outperform existing\naugmentation techniques, offering a scalable and flexible solution for training\nDocument AI models.", "published": "2024-11-27 21:15:02", "link": "http://arxiv.org/abs/2412.03590v1", "categories": ["cs.CL", "cs.AI", "I.2.6; I.2.7; I.5.4; H.3.3; H.2.8; G.2.2"], "primary_category": "cs.CL"}
{"title": "JPPO: Joint Power and Prompt Optimization for Accelerated Large Language\n  Model Services", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious tasks, leading to their increasing deployment in wireless networks for\na wide variety of user services. However, the growing longer prompt setting\nhighlights the crucial issue of computational resource demands and huge\ncommunication load. To address this challenge, we propose Joint Power and\nPrompt Optimization (JPPO), a framework that combines Small Language Model\n(SLM)-based prompt compression with wireless power allocation optimization. By\ndeploying SLM at user devices for prompt compression and employing Deep\nReinforcement Learning for joint optimization of compression ratio and\ntransmission power, JPPO effectively balances service quality with resource\nefficiency. Experimental results demonstrate that our framework achieves high\nservice fidelity and low bit error rates while optimizing power usage in\nwireless LLM services. The system reduces response time by about 17%, with the\nimprovement varying based on the length of the original prompt.", "published": "2024-11-27 03:05:32", "link": "http://arxiv.org/abs/2411.18010v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Training and Evaluating Language Models with Template-based Data\n  Generation", "abstract": "The rapid advancement of large language models (LLMs) such as GPT-3, PaLM,\nand Llama has significantly transformed natural language processing, showcasing\nremarkable capabilities in understanding and generating language. However,\nthese models often struggle with tasks requiring complex reasoning,\nparticularly in mathematical problem-solving, due in part to the scarcity of\nlarge-scale, high-quality, domain-specific datasets necessary for training\nsophisticated reasoning abilities. To address this limitation, we introduce\nTemplate-based Data Generation (TDG), a novel approach that leverages LLMs\n(GPT-4) to automatically generate parameterized meta-templates, which are then\nused to synthesize a vast array of high-quality problems and solutions.\nLeveraging TDG, we create TemplateMath Part I: TemplateGSM, a dataset\ncomprising over 7 million synthetically generated grade school math\nproblems--each accompanied by code-based and natural language solutions--with\nthe potential to generate an effectively unlimited number more. This dataset\nalleviates the scarcity of large-scale mathematical datasets and serves as a\nvaluable resource for pre-training, fine-tuning, and evaluating LLMs in\nmathematical reasoning. Our method not only enables the generation of virtually\ninfinite data but also elevates data augmentation to a new level by using GPT-4\nfor meta-template generation, ensuring diverse and high-quality problem\nstructures. The TemplateMath Part I: TemplateGSM dataset is publicly available\nat https://huggingface.co/datasets/math-ai/TemplateGSM. The code is available\nat https://github.com/iiis-ai/TemplateMath.", "published": "2024-11-27 07:32:56", "link": "http://arxiv.org/abs/2411.18104v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SALMONN-omni: A Codec-free LLM for Full-duplex Speech Understanding and\n  Generation", "abstract": "Full-duplex multimodal large language models (LLMs) provide a unified\nframework for addressing diverse speech understanding and generation tasks,\nenabling more natural and seamless human-machine conversations. Unlike\ntraditional modularised conversational AI systems, which separate speech\nrecognition, understanding, and text-to-speech generation into distinct\ncomponents, multimodal LLMs operate as single end-to-end models. This\nstreamlined design eliminates error propagation across components and fully\nleverages the rich non-verbal information embedded in input speech signals. We\nintroduce SALMONN-omni, a codec-free, full-duplex speech understanding and\ngeneration model capable of simultaneously listening to its own generated\nspeech and background sounds while speaking. To support this capability, we\npropose a novel duplex spoken dialogue framework incorporating a ``thinking''\nmechanism that facilitates asynchronous text and speech generation relying on\nembeddings instead of codecs (quantized speech and audio tokens). Experimental\nresults demonstrate SALMONN-omni's versatility across a broad range of\nstreaming speech tasks, including speech recognition, speech enhancement, and\nspoken question answering. Additionally, SALMONN-omni excels at managing\nturn-taking, barge-in, and echo cancellation scenarios, establishing its\npotential as a robust prototype for full-duplex conversational AI systems. To\nthe best of our knowledge, SALMONN-omni is the first codec-free model of its\nkind. A full technical report along with model checkpoints will be released\nsoon.", "published": "2024-11-27 08:38:57", "link": "http://arxiv.org/abs/2411.18138v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MSA-ASR: Efficient Multilingual Speaker Attribution with frozen ASR\n  Models", "abstract": "Speaker-attributed automatic speech recognition (SA-ASR) aims to transcribe\nspeech while assigning transcripts to the corresponding speakers accurately.\nExisting methods often rely on complex modular systems or require extensive\nfine-tuning of joint modules, limiting their adaptability and general\nefficiency. This paper introduces a novel approach, leveraging a frozen\nmultilingual ASR model to incorporate speaker attribution into the\ntranscriptions, using only standard monolingual ASR datasets. Our method\ninvolves training a speaker module to predict speaker embeddings based on weak\nlabels without requiring additional ASR model modifications. Despite being\ntrained exclusively with non-overlapping monolingual data, our approach\neffectively extracts speaker attributes across diverse multilingual datasets,\nincluding those with overlapping speech. Experimental results demonstrate\ncompetitive performance compared to strong baselines, highlighting the model's\nrobustness and potential for practical applications.", "published": "2024-11-27 09:01:08", "link": "http://arxiv.org/abs/2411.18152v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "How to Learn a New Language? An Efficient Solution for Self-Supervised\n  Learning Models Unseen Languages Adaption in Low-Resource Scenario", "abstract": "The utilization of speech Self-Supervised Learning (SSL) models achieves\nimpressive performance on Automatic Speech Recognition (ASR). However, in\nlow-resource language ASR, they encounter the domain mismatch problem between\npre-trained and low-resource languages. Typical solutions like fine-tuning the\nSSL model suffer from high computation costs while using frozen SSL models as\nfeature extractors comes with poor performance. To handle these issues, we\nextend a conventional efficient fine-tuning scheme based on the adapter. We add\nan extra intermediate adaptation to warm up the adapter and downstream model\ninitialization. Remarkably, we update only 1-5% of the total model parameters\nto achieve the adaptation. Experimental results on the ML-SUPERB dataset show\nthat our solution outperforms conventional efficient fine-tuning. It achieves\nup to a 28% relative improvement in the Character/Phoneme error rate when\nadapting to unseen languages.", "published": "2024-11-27 10:51:00", "link": "http://arxiv.org/abs/2411.18217v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Large Language Model-Brained GUI Agents: A Survey", "abstract": "GUIs have long been central to human-computer interaction, providing an\nintuitive and visually-driven way to access and interact with digital systems.\nThe advent of LLMs, particularly multimodal models, has ushered in a new era of\nGUI automation. They have demonstrated exceptional capabilities in natural\nlanguage understanding, code generation, and visual processing. This has paved\nthe way for a new generation of LLM-brained GUI agents capable of interpreting\ncomplex GUI elements and autonomously executing actions based on natural\nlanguage instructions. These agents represent a paradigm shift, enabling users\nto perform intricate, multi-step tasks through simple conversational commands.\nTheir applications span across web navigation, mobile app interactions, and\ndesktop automation, offering a transformative user experience that\nrevolutionizes how individuals interact with software. This emerging field is\nrapidly advancing, with significant progress in both research and industry.\n  To provide a structured understanding of this trend, this paper presents a\ncomprehensive survey of LLM-brained GUI agents, exploring their historical\nevolution, core components, and advanced techniques. We address research\nquestions such as existing GUI agent frameworks, the collection and utilization\nof data for training specialized GUI agents, the development of large action\nmodels tailored for GUI tasks, and the evaluation metrics and benchmarks\nnecessary to assess their effectiveness. Additionally, we examine emerging\napplications powered by these agents. Through a detailed analysis, this survey\nidentifies key research gaps and outlines a roadmap for future advancements in\nthe field. By consolidating foundational knowledge and state-of-the-art\ndevelopments, this work aims to guide both researchers and practitioners in\novercoming challenges and unlocking the full potential of LLM-brained GUI\nagents.", "published": "2024-11-27 12:13:39", "link": "http://arxiv.org/abs/2411.18279v10", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI"}
{"title": "Aligning Pre-trained Models for Spoken Language Translation", "abstract": "This paper investigates a novel approach to end-to-end speech translation\n(ST) based on aligning frozen pre-trained automatic speech recognition (ASR)\nand machine translation (MT) models via a small connector module (Q-Former, our\nSubsampler-Transformer Encoder). This connector bridges the gap between the\nspeech and text modalities, transforming ASR encoder embeddings into the latent\nrepresentation space of the MT encoder while being the only part of the system\noptimized during training. Experiments are conducted on the How2\nEnglish-Portuguese dataset as we investigate the alignment approach in a\nsmall-scale scenario focusing on ST. While keeping the size of the connector\nmodule constant and small in comparison ( < 5% of the size of the larger\naligned models), increasing the size and capability of the foundation ASR and\nMT models universally improves translation results. We also find that the\nconnectors can serve as domain adapters for the foundation MT models,\nsignificantly improving translation performance in the aligned ST setting. We\nconclude that this approach represents a viable and scalable approach to\ntraining end-to-end ST systems.", "published": "2024-11-27 12:32:41", "link": "http://arxiv.org/abs/2411.18294v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Continual Learning in Machine Speech Chain Using Gradient Episodic\n  Memory", "abstract": "Continual learning for automatic speech recognition (ASR) systems poses a\nchallenge, especially with the need to avoid catastrophic forgetting while\nmaintaining performance on previously learned tasks. This paper introduces a\nnovel approach leveraging the machine speech chain framework to enable\ncontinual learning in ASR using gradient episodic memory (GEM). By\nincorporating a text-to-speech (TTS) component within the machine speech chain,\nwe support the replay mechanism essential for GEM, allowing the ASR model to\nlearn new tasks sequentially without significant performance degradation on\nearlier tasks. Our experiments, conducted on the LJ Speech dataset, demonstrate\nthat our method outperforms traditional fine-tuning and multitask learning\napproaches, achieving a substantial error rate reduction while maintaining high\nperformance across varying noise conditions. We showed the potential of our\nsemi-supervised machine speech chain approach for effective and efficient\ncontinual learning in speech recognition.", "published": "2024-11-27 13:19:20", "link": "http://arxiv.org/abs/2411.18320v1", "categories": ["cs.CL", "cs.AI", "eess.AS"], "primary_category": "cs.CL"}
{"title": "GPT as ghostwriter at the White House", "abstract": "Recently several large language models (LLMs) have demonstrated their\ncapability to generate a message in response to a user request. Such scientific\nbreakthroughs promote new perspectives but also some fears. The main focus of\nthis study is to analyze the written style of one LLM called ChatGPT 3.5 by\ncomparing its generated messages with those of the recent US presidents. To\nachieve this objective, we compare the State of the Union addresses written by\nReagan to Obama with those automatically produced by ChatGPT. We found that\nChatGPT tends to overuse the lemma \"we\" as well as nouns and commas. On the\nother hand, the generated speeches employ less verbs and include, in mean,\nlonger sentences. Even when imposing a given style to ChatGPT, the resulting\nspeech remains distinct from messages written by the target author. Moreover,\nChatGPT opts for a neutral tone with mainly positive emotional expressions and\nsymbolic terms (e.g., freedom, nation). Finally, we show that the GPT's style\nexposes distinct features compared to real presidential addresses.", "published": "2024-11-27 14:12:36", "link": "http://arxiv.org/abs/2411.18365v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "AMPS: ASR with Multimodal Paraphrase Supervision", "abstract": "Spontaneous or conversational multilingual speech presents many challenges\nfor state-of-the-art automatic speech recognition (ASR) systems. In this work,\nwe present a new technique AMPS that augments a multilingual multimodal ASR\nsystem with paraphrase-based supervision for improved conversational ASR in\nmultiple languages, including Hindi, Marathi, Malayalam, Kannada, and Nyanja.\nWe use paraphrases of the reference transcriptions as additional supervision\nwhile training the multimodal ASR model and selectively invoke this paraphrase\nobjective for utterances with poor ASR performance. Using AMPS with a\nstate-of-the-art multimodal model SeamlessM4T, we obtain significant relative\nreductions in word error rates (WERs) of up to 5%. We present detailed analyses\nof our system using both objective and human evaluation metrics.", "published": "2024-11-27 14:16:51", "link": "http://arxiv.org/abs/2411.18368v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "ChatGPT as speechwriter for the French presidents", "abstract": "Generative AI proposes several large language models (LLMs) to automatically\ngenerate a message in response to users' requests. Such scientific\nbreakthroughs promote new writing assistants but with some fears. The main\nfocus of this study is to analyze the written style of one LLM called ChatGPT\nby comparing its generated messages with those of the recent French presidents.\nTo achieve this, we compare end-of-the-year addresses written by Chirac,\nSarkozy, Hollande, and Macron with those automatically produced by ChatGPT. We\nfound that ChatGPT tends to overuse nouns, possessive determiners, and numbers.\nOn the other hand, the generated speeches employ less verbs, pronouns, and\nadverbs and include, in mean, too standardized sentences. Considering some\nwords, one can observe that ChatGPT tends to overuse \"to must\" (devoir), \"to\ncontinue\" or the lemma \"we\" (nous). Moreover, GPT underuses the auxiliary verb\n\"to be\" (^etre), or the modal verbs \"to will\" (vouloir) or \"to have to\"\n(falloir). In addition, when a short text is provided as example to ChatGPT,\nthe machine can generate a short message with a style closed to the original\nwording. Finally, we reveal that ChatGPT style exposes distinct features\ncompared to real presidential speeches.", "published": "2024-11-27 14:29:10", "link": "http://arxiv.org/abs/2411.18382v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Automated Literature Review Using NLP Techniques and LLM-Based\n  Retrieval-Augmented Generation", "abstract": "This research presents and compares multiple approaches to automate the\ngeneration of literature reviews using several Natural Language Processing\n(NLP) techniques and retrieval-augmented generation (RAG) with a Large Language\nModel (LLM). The ever-increasing number of research articles provides a huge\nchallenge for manual literature review. It has resulted in an increased demand\nfor automation. Developing a system capable of automatically generating the\nliterature reviews from only the PDF files as input is the primary objective of\nthis research work. The effectiveness of several Natural Language Processing\n(NLP) strategies, such as the frequency-based method (spaCy), the transformer\nmodel (Simple T5), and retrieval-augmented generation (RAG) with Large Language\nModel (GPT-3.5-turbo), is evaluated to meet the primary objective. The SciTLDR\ndataset is chosen for this research experiment and three distinct techniques\nare utilized to implement three different systems for auto-generating the\nliterature reviews. The ROUGE scores are used for the evaluation of all three\nsystems. Based on the evaluation, the Large Language Model GPT-3.5-turbo\nachieved the highest ROUGE-1 score, 0.364. The transformer model comes in\nsecond place and spaCy is at the last position. Finally, a graphical user\ninterface is created for the best system based on the large language model.", "published": "2024-11-27 18:27:07", "link": "http://arxiv.org/abs/2411.18583v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cross-modal Information Flow in Multimodal Large Language Models", "abstract": "The recent advancements in auto-regressive multimodal large language models\n(MLLMs) have demonstrated promising progress for vision-language tasks. While\nthere exists a variety of studies investigating the processing of linguistic\ninformation within large language models, little is currently known about the\ninner working mechanism of MLLMs and how linguistic and visual information\ninteract within these models. In this study, we aim to fill this gap by\nexamining the information flow between different modalities -- language and\nvision -- in MLLMs, focusing on visual question answering. Specifically, given\nan image-question pair as input, we investigate where in the model and how the\nvisual and linguistic information are combined to generate the final\nprediction. Conducting experiments with a series of models from the LLaVA\nseries, we find that there are two distinct stages in the process of\nintegration of the two modalities. In the lower layers, the model first\ntransfers the more general visual features of the whole image into the\nrepresentations of (linguistic) question tokens. In the middle layers, it once\nagain transfers visual information about specific objects relevant to the\nquestion to the respective token positions of the question. Finally, in the\nhigher layers, the resulting multimodal representation is propagated to the\nlast position of the input sequence for the final prediction. Overall, our\nfindings provide a new and comprehensive perspective on the spatial and\nfunctional aspects of image and language processing in the MLLMs, thereby\nfacilitating future research into multimodal information localization and\nediting. Our code and collected dataset are released here:\nhttps://github.com/FightingFighting/cross-modal-information-flow-in-MLLM.git.", "published": "2024-11-27 18:59:26", "link": "http://arxiv.org/abs/2411.18620v2", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "Verbalized Representation Learning for Interpretable Few-Shot\n  Generalization", "abstract": "Humans recognize objects after observing only a few examples, a remarkable\ncapability enabled by their inherent language understanding of the real-world\nenvironment. Developing verbalized and interpretable representation can\nsignificantly improve model generalization in low-data settings. In this work,\nwe propose Verbalized Representation Learning (VRL), a novel approach for\nautomatically extracting human-interpretable features for object recognition\nusing few-shot data. Our method uniquely captures inter-class differences and\nintra-class commonalities in the form of natural language by employing a\nVision-Language Model (VLM) to identify key discriminative features between\ndifferent classes and shared characteristics within the same class. These\nverbalized features are then mapped to numeric vectors through the VLM. The\nresulting feature vectors can be further utilized to train and infer with\ndownstream classifiers. Experimental results show that, at the same model\nscale, VRL achieves a 24% absolute improvement over prior state-of-the-art\nmethods while using 95% less data and a smaller mode. Furthermore, compared to\nhuman-labeled attributes, the features learned by VRL exhibit a 20% absolute\ngain when used for downstream classification tasks. Code is available at:\nhttps://github.com/joeyy5588/VRL/tree/main.", "published": "2024-11-27 01:55:08", "link": "http://arxiv.org/abs/2411.18651v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Multi-Task Model Merging via Adaptive Weight Disentanglement", "abstract": "Model merging has recently gained attention as an economical and scalable\napproach to incorporate task-specific weights from various tasks into a unified\nmulti-task model. For example, in Task Arithmetic (TA), adding the fine-tuned\nweights of different tasks can enhance the model's performance on those tasks,\nwhile subtracting them leads to task forgetting. Although TA is highly\neffective, interference among task still hampers the performance of the merged\nmodel. Existing methods for handling conflicts between task generally rely on\nempirical selection, resulting in suboptimal performance. In this paper, we\nintroduce an Adaptive Weight Disentanglement method. We begin by theoretically\nproving that task vectors employed in model merging should be orthogonal to\nminimize interference among tasks. Guided by this insight, we initialize\nredundant vectors such that, when subtracted from the original task vectors,\nthe resulting vectors exhibit increased orthogonality. Additionally, we impose\nan norm constraint on the redundant vectors to preserve the performance of the\ntask-specific models. Experimental results demonstrate the effectiveness of our\nproposed technique: it successfully extracts redundant vectors, and after their\nsubtraction, the task vectors not only retain robust performance but also\nachieve superior fusion outcomes. Our code is available at\n\\href{https://github.com/FarisXiong/AWD.git}{https://github.com/FarisXiong/AWD.git}.", "published": "2024-11-27 20:08:55", "link": "http://arxiv.org/abs/2411.18729v2", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Cyber-Attack Technique Classification Using Two-Stage Trained Large\n  Language Models", "abstract": "Understanding the attack patterns associated with a cyberattack is crucial\nfor comprehending the attacker's behaviors and implementing the right\nmitigation measures. However, majority of the information regarding new attacks\nis typically presented in unstructured text, posing significant challenges for\nsecurity analysts in collecting necessary information. In this paper, we\npresent a sentence classification system that can identify the attack\ntechniques described in natural language sentences from cyber threat\nintelligence (CTI) reports. We propose a new method for utilizing auxiliary\ndata with the same labels to improve classification for the low-resource\ncyberattack classification task. The system first trains the model using the\naugmented training data and then trains more using only the primary data. We\nvalidate our model using the TRAM data1 and the MITRE ATT&CK framework.\nExperiments show that our method enhances Macro-F1 by 5 to 9 percentage points\nand keeps Micro-F1 scores competitive when compared to the baseline performance\non the TRAM dataset.", "published": "2024-11-27 21:09:02", "link": "http://arxiv.org/abs/2411.18755v1", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "UOE: Unlearning One Expert Is Enough For Mixture-of-experts LLMS", "abstract": "Recent advancements in large language model (LLM) unlearning have shown\nremarkable success in removing unwanted data-model influences while preserving\nthe model's utility for legitimate knowledge. However, despite these strides,\nsparse Mixture-of-Experts (MoE) LLMs--a key subset of the LLM family--have\nreceived little attention and remain largely unexplored in the context of\nunlearning. As MoE LLMs are celebrated for their exceptional performance and\nhighly efficient inference processes, we ask: How can unlearning be performed\neffectively and efficiently on MoE LLMs? And will traditional unlearning\nmethods be applicable to MoE architectures? Our pilot study shows that the\ndynamic routing nature of MoE LLMs introduces unique challenges, leading to\nsubstantial utility drops when existing unlearning methods are applied.\nSpecifically, unlearning disrupts the router's expert selection, causing\nsignificant selection shift from the most unlearning target-related experts to\nirrelevant ones. As a result, more experts than necessary are affected, leading\nto excessive forgetting and loss of control over which knowledge is erased. To\naddress this, we propose a novel single-expert unlearning framework, referred\nto as UOE, for MoE LLMs. Through expert attribution, unlearning is concentrated\non the most actively engaged expert for the specified knowledge. Concurrently,\nan anchor loss is applied to the router to stabilize the active state of this\ntargeted expert, ensuring focused and controlled unlearning that preserves\nmodel utility. The proposed UOE framework is also compatible with various\nunlearning algorithms. Extensive experiments demonstrate that UOE enhances both\nforget quality up to 5% and model utility by 35% on MoE LLMs across various\nbenchmarks, LLM architectures, while only unlearning 0.06% of the model\nparameters.", "published": "2024-11-27 22:46:08", "link": "http://arxiv.org/abs/2411.18797v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "NewsEdits 2.0: Learning the Intentions Behind Updating News", "abstract": "As events progress, news articles often update with new information: if we\nare not cautious, we risk propagating outdated facts. In this work, we\nhypothesize that linguistic features indicate factual fluidity, and that we can\npredict which facts in a news article will update using solely the text of a\nnews article (i.e. not external resources like search engines). We test this\nhypothesis, first, by isolating fact-updates in large news revisions corpora.\nNews articles may update for many reasons (e.g. factual, stylistic, narrative).\nWe introduce the NewsEdits 2.0 taxonomy, an edit-intentions schema that\nseparates fact updates from stylistic and narrative updates in news writing. We\nannotate over 9,200 pairs of sentence revisions and train high-scoring ensemble\nmodels to apply this schema. Then, taking a large dataset of silver-labeled\npairs, we show that we can predict when facts will update in older article\ndrafts with high precision. Finally, to demonstrate the usefulness of these\nfindings, we construct a language model question asking (LLM-QA) abstention\ntask. We wish the LLM to abstain from answering questions when information is\nlikely to become outdated. Using our predictions, we show, LLM absention\nreaches near oracle levels of accuracy.", "published": "2024-11-27 23:35:23", "link": "http://arxiv.org/abs/2411.18811v1", "categories": ["cs.CL", "cs.AI", "cs.DL"], "primary_category": "cs.CL"}
{"title": "Energy-Efficient Split Learning for Fine-Tuning Large Language Models in\n  Edge Networks", "abstract": "In this letter, we propose an energy-efficient split learning (SL) framework\nfor fine-tuning large language models (LLMs) using geo-distributed personal\ndata at the network edge, where LLMs are split and alternately across massive\nmobile devices and an edge server. Considering the device heterogeneity and\nchannel dynamics in edge networks, a \\underline{C}ut l\\underline{A}yer and\ncomputing \\underline{R}esource \\underline{D}ecision (CARD) algorithm is\ndeveloped to minimize training delay and energy consumption. Simulation results\ndemonstrate that the proposed approach reduces the average training delay and\nserver's energy consumption by 70.8% and 53.1%, compared to the benchmarks,\nrespectively.", "published": "2024-11-27 12:34:45", "link": "http://arxiv.org/abs/2412.00090v2", "categories": ["cs.LG", "cs.CL", "cs.DC"], "primary_category": "cs.LG"}
{"title": "ElectroVizQA: How well do Multi-modal LLMs perform in Electronics Visual\n  Question Answering?", "abstract": "Multi-modal Large Language Models (MLLMs) are gaining significant attention\nfor their ability to process multi-modal data, providing enhanced contextual\nunderstanding of complex problems. MLLMs have demonstrated exceptional\ncapabilities in tasks such as Visual Question Answering (VQA); however, they\noften struggle with fundamental engineering problems, and there is a scarcity\nof specialized datasets for training on topics like digital electronics. To\naddress this gap, we propose a benchmark dataset called ElectroVizQA\nspecifically designed to evaluate MLLMs' performance on digital electronic\ncircuit problems commonly found in undergraduate curricula. This dataset, the\nfirst of its kind tailored for the VQA task in digital electronics, comprises\napproximately 626 visual questions, offering a comprehensive overview of\ndigital electronics topics. This paper rigorously assesses the extent to which\nMLLMs can understand and solve digital electronic circuit questions, providing\ninsights into their capabilities and limitations within this specialized\ndomain. By introducing this benchmark dataset, we aim to motivate further\nresearch and development in the application of MLLMs to engineering education,\nultimately bridging the performance gap and enhancing the efficacy of these\nmodels in technical fields.", "published": "2024-11-27 20:25:07", "link": "http://arxiv.org/abs/2412.00102v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Human Evaluation of Procedural Knowledge Graph Extraction from Text with\n  Large Language Models", "abstract": "Procedural Knowledge is the know-how expressed in the form of sequences of\nsteps needed to perform some tasks. Procedures are usually described by means\nof natural language texts, such as recipes or maintenance manuals, possibly\nspread across different documents and systems, and their interpretation and\nsubsequent execution is often left to the reader. Representing such procedures\nin a Knowledge Graph (KG) can be the basis to build digital tools to support\nthose users who need to apply or execute them. In this paper, we leverage Large\nLanguage Model (LLM) capabilities and propose a prompt engineering approach to\nextract steps, actions, objects, equipment and temporal information from a\ntextual procedure, in order to populate a Procedural KG according to a\npre-defined ontology. We evaluate the KG extraction results by means of a user\nstudy, in order to qualitatively and quantitatively assess the perceived\nquality and usefulness of the LLM-extracted procedural knowledge. We show that\nLLMs can produce outputs of acceptable quality and we assess the subjective\nperception of AI by human evaluators.", "published": "2024-11-27 10:36:28", "link": "http://arxiv.org/abs/2412.03589v1", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI"}
{"title": "Speech Separation using Neural Audio Codecs with Embedding Loss", "abstract": "Neural audio codecs have revolutionized audio processing by enabling speech\ntasks to be performed on highly compressed representations. Recent work has\nshown that speech separation can be achieved within these compressed domains,\noffering faster training and reduced inference costs. However, current\napproaches still rely on waveform-based loss functions, necessitating\nunnecessary decoding steps during training. We propose a novel embedding loss\nfor neural audio codec-based speech separation that operates directly on\ncompressed audio representations, eliminating the need for decoding during\ntraining. To validate our approach, we conduct comprehensive evaluations using\nboth objective metrics and perceptual assessment techniques, including\nintrusive and non-intrusive methods. Our results demonstrate that embedding\nloss can be used to train codec-based speech separation models with a 2x\nimprovement in training speed and computational cost while achieving better\nDNSMOS and STOI performance on the WSJ0-2mix dataset across 3 different\npre-trained codecs.", "published": "2024-11-27 02:31:52", "link": "http://arxiv.org/abs/2411.17998v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Novel Class Discovery for Open Set Raga Classification", "abstract": "The task of Raga classification in Indian Art Music (IAM) is constrained by\nthe limited availability of labeled datasets, resulting in many Ragas being\nunrepresented during the training of machine learning models. Traditional Raga\nclassification methods rely on supervised learning, and assume that for a test\naudio to be classified by a Raga classification model, it must have been\nrepresented in the training data, which limits their effectiveness in\nreal-world scenarios where novel, unseen Ragas may appear. To address this\nlimitation, we propose a method based on Novel Class Discovery (NCD) to detect\nand classify previously unseen Ragas. Our approach utilizes a feature extractor\ntrained in a supervised manner to generate embeddings, which are then employed\nwithin a contrastive learning framework for self-supervised training, enabling\nthe identification of previously unseen Raga classes. The results demonstrate\nthat the proposed method can accurately detect audio samples corresponding to\nthese novel Ragas, offering a robust solution for utilizing the vast amount of\nunlabeled music data available online. This approach reduces the need for\nmanual labeling while expanding the repertoire of recognized Ragas, and other\nmusic data in Music Information Retrieval (MIR).", "published": "2024-11-27 18:56:49", "link": "http://arxiv.org/abs/2411.18611v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "TS3-Codec: Transformer-Based Simple Streaming Single Codec", "abstract": "Neural audio codecs (NACs) have garnered significant attention as key\ntechnologies for audio compression as well as audio representation for speech\nlanguage models. While mainstream NAC models are predominantly\nconvolution-based, the performance of NACs with a purely transformer-based, and\nconvolution-free architecture remains unexplored. This paper introduces\nTS3-Codec, a Transformer-Based Simple Streaming Single Codec. TS3-Codec\nconsists of only a stack of transformer layers with a few linear layers,\noffering greater simplicity and expressiveness by fully eliminating convolution\nlayers that require careful hyperparameter tuning and large computations. Under\nthe streaming setup, the proposed TS3-Codec achieves comparable or superior\nperformance compared to the codec with state-of-the-art convolution-based\narchitecture while requiring only 12% of the computation and 77% of bitrate.\nFurthermore, it significantly outperforms the convolution-based codec when\nusing similar computational resources.", "published": "2024-11-27 23:07:52", "link": "http://arxiv.org/abs/2411.18803v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Music2Fail: Transfer Music to Failed Recorder Style", "abstract": "The goal of music style transfer is to convert a music performance by one\ninstrument into another while keeping the musical contents unchanged. In this\npaper, we investigate another style transfer scenario called ``failed-music\nstyle transfer''. Unlike the usual music style transfer where the content\nremains the same and only the instrumental characteristics are changed, this\nscenario seeks to transfer the music from the source instrument to the target\ninstrument which is deliberately performed off-pitch. Our work attempts to\ntransfer normally played music into off-pitch recorder music, which we call\n``failed-style recorder'', and study the results of the conversion. To carry\nout this work, we have also proposed a dataset of failed-style recorders for\nthis task, called ``FR109 Dataset''. Such an experiment explores the music\nstyle transfer task in a more expressive setting, as the generated audio should\nsound like an ``off-pitch recorder'' while maintaining a certain degree of\nnaturalness.", "published": "2024-11-27 06:05:29", "link": "http://arxiv.org/abs/2411.18075v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Fusion of Discrete Representations and Self-Augmented Representations\n  for Multilingual Automatic Speech Recognition", "abstract": "Self-supervised learning (SSL) models have shown exceptional capabilities\nacross various speech-processing tasks. Continuous SSL representations are\neffective but suffer from high computational and storage demands. On the other\nhand, discrete SSL representations, although with degraded performance, reduce\ntransmission and storage costs, and improve input sequence efficiency through\nde-duplication and subword-modeling. To boost the performance of discrete\nrepresentations for ASR, we introduce a novel fusion mechanism that integrates\ntwo discrete representations. The fusion mechanism preserves all the benefits\nof discrete representation while enhancing the model's performance by\nintegrating complementary information. Additionally, we explore\n\"self-augmented'' discrete representations, which apply transformations to a\nsingle continuous SSL representation, eliminating the fusion mechanism's\ndependency on multiple SSL models and further decreasing its inference costs.\nExperimental results on benchmarks, including LibriSpeech and ML-SUPERB,\nindicate up to 19% and 24% relative character error rate improvement compared\nwith the non-fusion baseline, validating the effectiveness of our proposed\nmethods.", "published": "2024-11-27 07:36:52", "link": "http://arxiv.org/abs/2411.18107v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Towards Improved Objective Perceptual Audio Quality Assessment -- Part\n  1: A Novel Data-Driven Cognitive Model", "abstract": "Efficient audio quality assessment is vital for streamlining audio codec\ndevelopment. Objective assessment tools have been developed over time to\nalgorithmically predict quality ratings from subjective assessments, the gold\nstandard for quality judgment. Many of these tools use perceptual auditory\nmodels to extract audio features that are mapped to a basic audio quality score\nprediction using machine learning algorithms and subjective scores as training\ndata. However, existing tools struggle with generalization in quality\nprediction, especially when faced with unknown signal and distortion types.\nThis is particularly evident in the presence of signals coded using\nnon-waveform-preserving parametric techniques. Addressing these challenges,\nthis two-part work proposes extensions to the Perceptual Evaluation of Audio\nQuality (PEAQ - ITU-R BS.1387-1) recommendation. Part 1 focuses on increasing\ngeneralization, while Part 2 targets accurate spatial audio quality measurement\nin audio coding.\n  To enhance prediction generalization, this paper (Part 1) introduces a novel\nmachine learning approach that uses subjective data to model cognitive aspects\nof audio quality perception. The proposed method models the perceived severity\nof audible distortions by adaptively weighting different distortion metrics.\nThe weights are determined using an interaction cost function that captures\nrelationships between distortion salience and cognitive effects. Compared to\nother machine learning methods and established tools, the proposed architecture\nachieves higher prediction accuracy on large databases of previously unseen\nsubjective quality scores. The perceptually-motivated model offers a more\nmanageable alternative to general-purpose machine learning algorithms, allowing\npotential extensions and improvements to multi-dimensional quality measurement\nwithout complete retraining.", "published": "2024-11-27 10:58:13", "link": "http://arxiv.org/abs/2411.18222v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Continuous Autoregressive Models with Noise Augmentation Avoid Error\n  Accumulation", "abstract": "Autoregressive models are typically applied to sequences of discrete tokens,\nbut recent research indicates that generating sequences of continuous\nembeddings in an autoregressive manner is also feasible. However, such\nContinuous Autoregressive Models (CAMs) can suffer from a decline in generation\nquality over extended sequences due to error accumulation during inference. We\nintroduce a novel method to address this issue by injecting random noise into\nthe input embeddings during training. This procedure makes the model robust\nagainst varying error levels at inference. We further reduce error accumulation\nthrough an inference procedure that introduces low-level noise. Experiments on\nmusical audio generation show that CAM substantially outperforms existing\nautoregressive and non-autoregressive approaches while preserving audio quality\nover extended sequences. This work paves the way for generating continuous\nembeddings in a purely autoregressive setting, opening new possibilities for\nreal-time and interactive generative applications.", "published": "2024-11-27 15:38:20", "link": "http://arxiv.org/abs/2411.18447v1", "categories": ["cs.LG", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Multiple Choice Learning for Efficient Speech Separation with Many\n  Speakers", "abstract": "Training speech separation models in the supervised setting raises a\npermutation problem: finding the best assignation between the model predictions\nand the ground truth separated signals. This inherently ambiguous task is\ncustomarily solved using Permutation Invariant Training (PIT). In this article,\nwe instead consider using the Multiple Choice Learning (MCL) framework, which\nwas originally introduced to tackle ambiguous tasks. We demonstrate\nexperimentally on the popular WSJ0-mix and LibriMix benchmarks that MCL matches\nthe performances of PIT, while being computationally advantageous. This opens\nthe door to a promising research direction, as MCL can be naturally extended to\nhandle a variable number of speakers, or to tackle speech separation in the\nunsupervised setting.", "published": "2024-11-27 16:38:34", "link": "http://arxiv.org/abs/2411.18497v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Wearable intelligent throat enables natural speech in stroke patients\n  with dysarthria", "abstract": "Wearable silent speech systems hold significant potential for restoring\ncommunication in patients with speech impairments. However, seamless, coherent\nspeech remains elusive, and clinical efficacy is still unproven. Here, we\npresent an AI-driven intelligent throat (IT) system that integrates throat\nmuscle vibrations and carotid pulse signal sensors with large language model\n(LLM) processing to enable fluent, emotionally expressive communication. The\nsystem utilizes ultrasensitive textile strain sensors to capture high-quality\nsignals from the neck area and supports token-level processing for real-time,\ncontinuous speech decoding, enabling seamless, delay-free communication. In\ntests with five stroke patients with dysarthria, IT's LLM agents intelligently\ncorrected token errors and enriched sentence-level emotional and logical\ncoherence, achieving low error rates (4.2% word error rate, 2.9% sentence error\nrate) and a 55% increase in user satisfaction. This work establishes a\nportable, intuitive communication platform for patients with dysarthria with\nthe potential to be applied broadly across different neurological conditions\nand in multi-language support systems.", "published": "2024-11-27 12:03:52", "link": "http://arxiv.org/abs/2411.18266v3", "categories": ["eess.AS", "cs.AI", "cs.SD", "cs.SY", "eess.SY"], "primary_category": "eess.AS"}
{"title": "GaussianSpeech: Audio-Driven Gaussian Avatars", "abstract": "We introduce GaussianSpeech, a novel approach that synthesizes high-fidelity\nanimation sequences of photo-realistic, personalized 3D human head avatars from\nspoken audio. To capture the expressive, detailed nature of human heads,\nincluding skin furrowing and finer-scale facial movements, we propose to couple\nspeech signal with 3D Gaussian splatting to create realistic, temporally\ncoherent motion sequences. We propose a compact and efficient 3DGS-based avatar\nrepresentation that generates expression-dependent color and leverages wrinkle-\nand perceptually-based losses to synthesize facial details, including wrinkles\nthat occur with different expressions. To enable sequence modeling of 3D\nGaussian splats with audio, we devise an audio-conditioned transformer model\ncapable of extracting lip and expression features directly from audio input.\nDue to the absence of high-quality datasets of talking humans in correspondence\nwith audio, we captured a new large-scale multi-view dataset of audio-visual\nsequences of talking humans with native English accents and diverse facial\ngeometry. GaussianSpeech consistently achieves state-of-the-art performance\nwith visually natural motion at real time rendering rates, while encompassing\ndiverse facial expressions and styles.", "published": "2024-11-27 18:54:08", "link": "http://arxiv.org/abs/2411.18675v1", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
