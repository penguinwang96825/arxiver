{"title": "AugESC: Dialogue Augmentation with Large Language Models for Emotional\n  Support Conversation", "abstract": "Crowdsourced dialogue corpora are usually limited in scale and topic coverage\ndue to the expensive cost of data curation. This would hinder the\ngeneralization of downstream dialogue models to open-domain topics. In this\nwork, we leverage large language models for dialogue augmentation in the task\nof emotional support conversation (ESC). By treating dialogue augmentation as a\ndialogue completion task, we prompt a fine-tuned language model to complete\nfull dialogues from available dialogue posts of various topics, which are then\npostprocessed based on heuristics. Applying this approach, we construct AugESC,\nan augmented dataset for the ESC task, which largely extends the scale and\ntopic coverage of the crowdsourced ESConv corpus. Through comprehensive human\nevaluation, we demonstrate that our approach is superior to strong baselines of\ndialogue augmentation and that AugESC has comparable dialogue quality to the\ncrowdsourced corpus. We also conduct human interactive evaluation and prove\nthat post-training on AugESC improves downstream dialogue models'\ngeneralization ability to open-domain topics. These results suggest the utility\nof AugESC and highlight the potential of large language models in improving\ndata-scarce dialogue generation tasks.", "published": "2022-02-26 03:17:08", "link": "http://arxiv.org/abs/2202.13047v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bi-directional Joint Neural Networks for Intent Classification and Slot\n  Filling", "abstract": "Intent classification and slot filling are two critical tasks for natural\nlanguage understanding. Traditionally the two tasks proceeded independently.\nHowever, more recently joint models for intent classification and slot filling\nhave achieved state-of-the-art performance, and have proved that there exists a\nstrong relationship between the two tasks. In this paper, we propose a\nbi-directional joint model for intent classification and slot filling, which\nincludes a multi-stage hierarchical process via BERT and bi-directional joint\nnatural language understanding mechanisms, including intent2slot and\nslot2intent, to obtain mutual performance enhancement between intent\nclassification and slot filling. The evaluations show that our model achieves\nstate-of-the-art results on intent classification accuracy, slot filling F1,\nand significantly improves sentence-level semantic frame accuracy when applied\nto publicly available benchmark datasets, ATIS (88.6%) and SNIPS (92.8%).", "published": "2022-02-26 06:35:21", "link": "http://arxiv.org/abs/2202.13079v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Level Contrastive Learning for Cross-Lingual Alignment", "abstract": "Cross-language pre-trained models such as multilingual BERT (mBERT) have\nachieved significant performance in various cross-lingual downstream NLP tasks.\nThis paper proposes a multi-level contrastive learning (ML-CTL) framework to\nfurther improve the cross-lingual ability of pre-trained models. The proposed\nmethod uses translated parallel data to encourage the model to generate similar\nsemantic embeddings for different languages. However, unlike the sentence-level\nalignment used in most previous studies, in this paper, we explicitly integrate\nthe word-level information of each pair of parallel sentences into contrastive\nlearning. Moreover, cross-zero noise contrastive estimation (CZ-NCE) loss is\nproposed to alleviate the impact of the floating-point error in the training\nprocess with a small batch size. The proposed method significantly improves the\ncross-lingual transfer ability of our basic model (mBERT) and outperforms on\nmultiple zero-shot cross-lingual downstream tasks compared to the same-size\nmodels in the Xtreme benchmark.", "published": "2022-02-26 07:14:20", "link": "http://arxiv.org/abs/2202.13083v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring the Impact of Negative Samples of Contrastive Learning: A Case\n  Study of Sentence Embedding", "abstract": "Contrastive learning is emerging as a powerful technique for extracting\nknowledge from unlabeled data. This technique requires a balanced mixture of\ntwo ingredients: positive (similar) and negative (dissimilar) samples. This is\ntypically achieved by maintaining a queue of negative samples during training.\nPrior works in the area typically uses a fixed-length negative sample queue,\nbut how the negative sample size affects the model performance remains unclear.\nThe opaque impact of the number of negative samples on performance when\nemploying contrastive learning aroused our in-depth exploration. This paper\npresents a momentum contrastive learning model with negative sample queue for\nsentence embedding, namely MoCoSE. We add the prediction layer to the online\nbranch to make the model asymmetric and together with EMA update mechanism of\nthe target branch to prevent the model from collapsing. We define a maximum\ntraceable distance metric, through which we learn to what extent the text\ncontrastive learning benefits from the historical information of negative\nsamples. Our experiments find that the best results are obtained when the\nmaximum traceable distance is at a certain range, demonstrating that there is\nan optimal range of historical information for a negative sample queue. We\nevaluate the proposed unsupervised MoCoSE on the semantic text similarity (STS)\ntask and obtain an average Spearman's correlation of $77.27\\%$. Source code is\navailable at https://github.com/xbdxwyh/mocose.", "published": "2022-02-26 08:29:25", "link": "http://arxiv.org/abs/2202.13093v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "COMPASS: a Creative Support System that Alerts Novelists to the\n  Unnoticed Missing Contents", "abstract": "When humans write, they may unintentionally omit some information.\nComplementing the omitted information using a computer is helpful in providing\nwriting support. Recently, in the field of story understanding and generation,\nstory completion (SC) was proposed to generate the missing parts of an\nincomplete story. Although its applicability is limited because it requires\nthat the user have prior knowledge of the missing part of a story, missing\nposition prediction (MPP) can be used to compensate for this problem. MPP aims\nto predict the position of the missing part, but the prerequisite knowledge\nthat \"one sentence is missing\" is still required. In this study, we propose\nVariable Number MPP (VN-MPP), a new MPP task that removes this restriction;\nthat is, the task to predict multiple missing sentences or to judge whether\nthere are no missing sentences in the first place. We also propose two methods\nfor this new MPP task. Furthermore, based on the novel task and methods, we\ndeveloped a creative writing support system, COMPASS. The results of a user\nexperiment involving professional creators who write texts in Japanese confirm\nthe efficacy and utility of the developed system.", "published": "2022-02-26 14:40:03", "link": "http://arxiv.org/abs/2202.13151v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ASSIST: Towards Label Noise-Robust Dialogue State Tracking", "abstract": "The MultiWOZ 2.0 dataset has greatly boosted the research on dialogue state\ntracking (DST). However, substantial noise has been discovered in its state\nannotations. Such noise brings about huge challenges for training DST models\nrobustly. Although several refined versions, including MultiWOZ 2.1-2.4, have\nbeen published recently, there are still lots of noisy labels, especially in\nthe training set. Besides, it is costly to rectify all the problematic\nannotations. In this paper, instead of improving the annotation quality\nfurther, we propose a general framework, named ASSIST (lAbel noiSe-robuSt\ndIalogue State Tracking), to train DST models robustly from noisy labels.\nASSIST first generates pseudo labels for each sample in the training set by\nusing an auxiliary model trained on a small clean dataset, then puts the\ngenerated pseudo labels and vanilla noisy labels together to train the primary\nmodel. We show the validity of ASSIST theoretically. Experimental results also\ndemonstrate that ASSIST improves the joint goal accuracy of DST by up to\n$28.16\\%$ on MultiWOZ 2.0 and $8.41\\%$ on MultiWOZ 2.4, compared to using only\nthe vanilla noisy labels.", "published": "2022-02-26 00:33:32", "link": "http://arxiv.org/abs/2202.13024v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Systematic Evaluation of Large Language Models of Code", "abstract": "Large language models (LMs) of code have recently shown tremendous promise in\ncompleting code and synthesizing code from natural language descriptions.\nHowever, the current state-of-the-art code LMs (e.g., Codex (Chen et al.,\n2021)) are not publicly available, leaving many questions about their model and\ndata design decisions. We aim to fill in some of these blanks through a\nsystematic evaluation of the largest existing models: Codex, GPT-J, GPT-Neo,\nGPT-NeoX-20B, and CodeParrot, across various programming languages. Although\nCodex itself is not open-source, we find that existing open-source models do\nachieve close results in some programming languages, although targeted mainly\nfor natural language modeling. We further identify an important missing piece\nin the form of a large open-source model trained exclusively on a multi-lingual\ncorpus of code. We release a new model, PolyCoder, with 2.7B parameters based\non the GPT-2 architecture, which was trained on 249GB of code across 12\nprogramming languages on a single machine. In the C programming language,\nPolyCoder outperforms all models including Codex. Our trained models are\nopen-source and publicly available at https://github.com/VHellendoorn/Code-LMs,\nwhich enables future research and application in this area.", "published": "2022-02-26 15:53:55", "link": "http://arxiv.org/abs/2202.13169v3", "categories": ["cs.PL", "cs.CL"], "primary_category": "cs.PL"}
{"title": "A Generative Model for Relation Extraction and Classification", "abstract": "Relation extraction (RE) is an important information extraction task which\nprovides essential information to many NLP applications such as knowledge base\npopulation and question answering. In this paper, we present a novel generative\nmodel for relation extraction and classification (which we call GREC), where RE\nis modeled as a sequence-to-sequence generation task. We explore various\nencoding representations for the source and target sequences, and design\neffective schemes that enable GREC to achieve state-of-the-art performance on\nthree benchmark RE datasets. In addition, we introduce negative sampling and\ndecoding scaling techniques which provide a flexible tool to tune the precision\nand recall performance of the model. Our approach can be extended to extract\nall relation triples from a sentence in one pass. Although the one-pass\napproach incurs certain performance loss, it is much more computationally\nefficient.", "published": "2022-02-26 21:17:18", "link": "http://arxiv.org/abs/2202.13229v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Automated Identification of Toxic Code Reviews Using ToxiCR", "abstract": "Toxic conversations during software development interactions may have serious\nrepercussions on a Free and Open Source Software (FOSS) development project.\nFor example, victims of toxic conversations may become afraid to express\nthemselves, therefore get demotivated, and may eventually leave the project.\nAutomated filtering of toxic conversations may help a FOSS community to\nmaintain healthy interactions among its members. However, off-the-shelf\ntoxicity detectors perform poorly on Software Engineering (SE) datasets, such\nas one curated from code review comments. To encounter this challenge, we\npresent ToxiCR, a supervised learning-based toxicity identification tool for\ncode review interactions. ToxiCR includes a choice to select one of the ten\nsupervised learning algorithms, an option to select text vectorization\ntechniques, eight preprocessing steps, and a large-scale labeled dataset of\n19,571 code review comments. Two out of those eight preprocessing steps are SE\ndomain specific. With our rigorous evaluation of the models with various\ncombinations of preprocessing steps and vectorization techniques, we have\nidentified the best combination for our dataset that boosts 95.8% accuracy and\n88.9% F1 score. ToxiCR significantly outperforms existing toxicity detectors on\nour dataset. We have released our dataset, pre-trained models, evaluation\nresults, and source code publicly available at:\nhttps://github.com/WSU-SEAL/ToxiCR", "published": "2022-02-26 04:27:39", "link": "http://arxiv.org/abs/2202.13056v3", "categories": ["cs.SE", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "SemSup: Semantic Supervision for Simple and Scalable Zero-shot\n  Generalization", "abstract": "Zero-shot learning is the problem of predicting instances over classes not\nseen during training. One approach to zero-shot learning is providing auxiliary\nclass information to the model. Prior work along this vein have largely used\nexpensive per-instance annotation or singular class-level descriptions, but\nper-instance descriptions are hard to scale and single class descriptions may\nnot be rich enough. Furthermore, these works have used natural-language\ndescriptions exclusively, simple bi-encoders models, and modality or\ntask-specific methods. These approaches have several limitations: text\nsupervision may not always be available or optimal and bi-encoders may only\nlearn coarse relations between inputs and class descriptions. In this work, we\npresent SemSup, a novel approach that uses (1) a scalable multiple description\nsampling method which improves performance over single descriptions, (2)\nalternative description formats such as JSON that are easy to generate and\noutperform text on certain settings, and (3) hybrid lexical-semantic similarity\nto leverage fine-grained information in class descriptions. We demonstrate the\neffectiveness of SemSup across four datasets, two modalities, and three\ngeneralization settings. For example, across text and image datasets, SemSup\nincreases unseen class generalization accuracy by 15 points on average compared\nto the closest baseline.", "published": "2022-02-26 09:55:54", "link": "http://arxiv.org/abs/2202.13100v4", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "QuoteR: A Benchmark of Quote Recommendation for Writing", "abstract": "It is very common to use quotations (quotes) to make our writings more\nelegant or convincing. To help people find appropriate quotes efficiently, the\ntask of quote recommendation is presented, aiming to recommend quotes that fit\nthe current context of writing. There have been various quote recommendation\napproaches, but they are evaluated on different unpublished datasets. To\nfacilitate the research on this task, we build a large and fully open quote\nrecommendation dataset called QuoteR, which comprises three parts including\nEnglish, standard Chinese and classical Chinese. Any part of it is larger than\nprevious unpublished counterparts. We conduct an extensive evaluation of\nexisting quote recommendation methods on QuoteR. Furthermore, we propose a new\nquote recommendation model that significantly outperforms previous methods on\nall three parts of QuoteR. All the code and data of this paper are available at\nhttps://github.com/thunlp/QuoteR.", "published": "2022-02-26 14:01:44", "link": "http://arxiv.org/abs/2202.13145v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Integrating Text Inputs For Training and Adapting RNN Transducer ASR\n  Models", "abstract": "Compared to hybrid automatic speech recognition (ASR) systems that use a\nmodular architecture in which each component can be independently adapted to a\nnew domain, recent end-to-end (E2E) ASR system are harder to customize due to\ntheir all-neural monolithic construction. In this paper, we propose a novel\ntext representation and training framework for E2E ASR models. With this\napproach, we show that a trained RNN Transducer (RNN-T) model's internal LM\ncomponent can be effectively adapted with text-only data. An RNN-T model\ntrained using both speech and text inputs improves over a baseline model\ntrained on just speech with close to 13% word error rate (WER) reduction on the\nSwitchboard and CallHome test sets of the NIST Hub5 2000 evaluation. The\nusefulness of the proposed approach is further demonstrated by customizing this\ngeneral purpose RNN-T model to three separate datasets. We observe 20-45%\nrelative word error rate (WER) reduction in these settings with this novel LM\nstyle customization technique using only unpaired text data from the new\ndomains.", "published": "2022-02-26 15:03:09", "link": "http://arxiv.org/abs/2202.13155v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "BioADAPT-MRC: Adversarial Learning-based Domain Adaptation Improves\n  Biomedical Machine Reading Comprehension Task", "abstract": "Biomedical machine reading comprehension (biomedical-MRC) aims to comprehend\ncomplex biomedical narratives and assist healthcare professionals in retrieving\ninformation from them. The high performance of modern neural network-based MRC\nsystems depends on high-quality, large-scale, human-annotated training\ndatasets. In the biomedical domain, a crucial challenge in creating such\ndatasets is the requirement for domain knowledge, inducing the scarcity of\nlabeled data and the need for transfer learning from the labeled\ngeneral-purpose (source) domain to the biomedical (target) domain. However,\nthere is a discrepancy in marginal distributions between the general-purpose\nand biomedical domains due to the variances in topics. Therefore,\ndirect-transferring of learned representations from a model trained on a\ngeneral-purpose domain to the biomedical domain can hurt the model's\nperformance. We present an adversarial learning-based domain adaptation\nframework for the biomedical machine reading comprehension task (BioADAPT-MRC),\na neural network-based method to address the discrepancies in the marginal\ndistributions between the general and biomedical domain datasets. BioADAPT-MRC\nrelaxes the need for generating pseudo labels for training a well-performing\nbiomedical-MRC model. We extensively evaluate the performance of BioADAPT-MRC\nby comparing it with the best existing methods on three widely used benchmark\nbiomedical-MRC datasets -- BioASQ-7b, BioASQ-8b, and BioASQ-9b. Our results\nsuggest that without using any synthetic or human-annotated data from the\nbiomedical domain, BioADAPT-MRC can achieve state-of-the-art performance on\nthese datasets. Availability: BioADAPT-MRC is freely available as an\nopen-source project at \\url{https://github.com/mmahbub/BioADAPT-MRC}.", "published": "2022-02-26 16:14:27", "link": "http://arxiv.org/abs/2202.13174v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Reducing the Need for Speech Training Data To Build Spoken\n  Language Understanding Systems", "abstract": "The lack of speech data annotated with labels required for spoken language\nunderstanding (SLU) is often a major hurdle in building end-to-end (E2E)\nsystems that can directly process speech inputs. In contrast, large amounts of\ntext data with suitable labels are usually available. In this paper, we propose\na novel text representation and training methodology that allows E2E SLU\nsystems to be effectively constructed using these text resources. With very\nlimited amounts of additional speech, we show that these models can be further\nimproved to perform at levels close to similar systems built on the full speech\ndatasets. The efficacy of our proposed approach is demonstrated on both intent\nand entity tasks using three different SLU datasets. With text-only training,\nthe proposed system achieves up to 90% of the performance possible with full\nspeech training. With just an additional 10% of speech data, these models\nsignificantly improve further to 97% of full performance.", "published": "2022-02-26 15:21:13", "link": "http://arxiv.org/abs/2203.00006v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "AI agents for facilitating social interactions and wellbeing", "abstract": "Wellbeing AI has been becoming a new trend in individuals' mental health,\norganizational health, and flourishing our societies. Various applications of\nwellbeing AI have been introduced to our daily lives. While social\nrelationships within groups are a critical factor for wellbeing, the\ndevelopment of wellbeing AI for social interactions remains relatively scarce.\nIn this paper, we provide an overview of the mediative role of AI-augmented\nagents for social interactions. First, we discuss the two-dimensional framework\nfor classifying wellbeing AI: individual/group and analysis/intervention.\nFurthermore, wellbeing AI touches on intervening social relationships between\nhuman-human interactions since positive social relationships are key to human\nwellbeing. This intervention may raise technical and ethical challenges. We\ndiscuss opportunities and challenges of the relational approach with wellbeing\nAI to promote wellbeing in our societies.", "published": "2022-02-26 04:05:23", "link": "http://arxiv.org/abs/2203.06244v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC", "cs.LG"], "primary_category": "cs.CY"}
{"title": "Revisiting Over-Smoothness in Text to Speech", "abstract": "Non-autoregressive text to speech (NAR-TTS) models have attracted much\nattention from both academia and industry due to their fast generation speed.\nOne limitation of NAR-TTS models is that they ignore the correlation in time\nand frequency domains while generating speech mel-spectrograms, and thus cause\nblurry and over-smoothed results. In this work, we revisit this over-smoothing\nproblem from a novel perspective: the degree of over-smoothness is determined\nby the gap between the complexity of data distributions and the capability of\nmodeling methods. Both simplifying data distributions and improving modeling\nmethods can alleviate the problem. Accordingly, we first study methods reducing\nthe complexity of data distributions. Then we conduct a comprehensive study on\nNAR-TTS models that use some advanced modeling methods. Based on these studies,\nwe find that 1) methods that provide additional condition inputs reduce the\ncomplexity of data distributions to model, thus alleviating the over-smoothing\nproblem and achieving better voice quality. 2) Among advanced modeling methods,\nLaplacian mixture loss performs well at modeling multimodal distributions and\nenjoys its simplicity, while GAN and Glow achieve the best voice quality while\nsuffering from increased training or model complexity. 3) The two categories of\nmethods can be combined to further alleviate the over-smoothness and improve\nthe voice quality. 4) Our experiments on the multi-speaker dataset lead to\nsimilar conclusions as above and providing more variance information can reduce\nthe difficulty of modeling the target data distribution and alleviate the\nrequirements for model capacity.", "published": "2022-02-26 05:22:32", "link": "http://arxiv.org/abs/2202.13066v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Language-Independent Speaker Anonymization Approach using\n  Self-Supervised Pre-Trained Models", "abstract": "Speaker anonymization aims to protect the privacy of speakers while\npreserving spoken linguistic information from speech. Current mainstream neural\nnetwork speaker anonymization systems are complicated, containing an F0\nextractor, speaker encoder, automatic speech recognition acoustic model (ASR\nAM), speech synthesis acoustic model and speech waveform generation model.\nMoreover, as an ASR AM is language-dependent, trained on English data, it is\nhard to adapt it into another language. In this paper, we propose a simpler\nself-supervised learning (SSL)-based method for language-independent speaker\nanonymization without any explicit language-dependent model, which can be\neasily used for other languages. Extensive experiments were conducted on the\nVoicePrivacy Challenge 2020 datasets in English and AISHELL-3 datasets in\nMandarin to demonstrate the effectiveness of our proposed SSL-based\nlanguage-independent speaker anonymization method.", "published": "2022-02-26 09:35:11", "link": "http://arxiv.org/abs/2202.13097v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "An acoustic signal cavitation detection framework based on XGBoost with\n  adaptive selection feature engineering", "abstract": "Valves are widely used in industrial and domestic pipeline systems. However,\nduring their operation, they may suffer from the occurrence of the cavitation,\nwhich can cause loud noise, vibration and damage to the internal components of\nthe valve. Therefore, monitoring the flow status inside valves is significantly\nbeneficial to prevent the additional cost induced by cavitation. In this paper,\na novel acoustic signal cavitation detection framework--based on XGBoost with\nadaptive selection feature engineering--is proposed. Firstly, a data\naugmentation method with non-overlapping sliding window (NOSW) is developed to\nsolve small-sample problem involved in this study. Then, the each segmented\npiece of time-domain acoustic signal is transformed by fast Fourier transform\n(FFT) and its statistical features are extracted to be the input to the\nadaptive selection feature engineering (ASFE) procedure, where the adaptive\nfeature aggregation and feature crosses are performed. Finally, with the\nselected features the XGBoost algorithm is trained for cavitation detection and\ntested on valve acoustic signal data provided by Samson AG (Frankfurt). Our\nmethod has achieved state-of-the-art results. The prediction performance on the\nbinary classification (cavitation and no-cavitation) and the four-class\nclassification (cavitation choked flow, constant cavitation, incipient\ncavitation and no-cavitation) are satisfactory and outperform the traditional\nXGBoost by 4.67% and 11.11% increase of the accuracy.", "published": "2022-02-26 20:48:42", "link": "http://arxiv.org/abs/2202.13226v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Regional-Local Adversarially Learned One-Class Classifier Anomalous\n  Sound Detection in Global Long-Term Space", "abstract": "Anomalous sound detection (ASD) is one of the most significant tasks of\nmechanical equipment monitoring and maintaining in complex industrial systems.\nIn practice, it is vital to precisely identify abnormal status of the working\nmechanical system, which can further facilitate the failure troubleshooting. In\nthis paper, we propose a multi-pattern adversarial learning one-class\nclassification framework, which allows us to use both the generator and the\ndiscriminator of an adversarial model for efficient ASD. The core idea is\nlearning to reconstruct the normal patterns of acoustic data through two\ndifferent patterns of auto-encoding generators, which succeeds in extending the\nfundamental role of a discriminator from identifying real and fake data to\ndistinguishing between regional and local pattern reconstructions. Furthermore,\nwe present a global filter layer for long-term interactions in the frequency\ndomain space, which directly learns from the original data without introducing\nany human priors. Extensive experiments performed on four real-world datasets\nfrom different industrial domains (three cavitation datasets provided by SAMSON\nAG, and one existing publicly) for anomaly detection show superior results, and\noutperform recent state-of-the-art ASD methods.", "published": "2022-02-26 22:54:17", "link": "http://arxiv.org/abs/2202.13245v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Visual Speech Recognition for Multiple Languages in the Wild", "abstract": "Visual speech recognition (VSR) aims to recognize the content of speech based\non lip movements, without relying on the audio stream. Advances in deep\nlearning and the availability of large audio-visual datasets have led to the\ndevelopment of much more accurate and robust VSR models than ever before.\nHowever, these advances are usually due to the larger training sets rather than\nthe model design. Here we demonstrate that designing better models is equally\nas important as using larger training sets. We propose the addition of\nprediction-based auxiliary tasks to a VSR model, and highlight the importance\nof hyperparameter optimization and appropriate data augmentations. We show that\nsuch a model works for different languages and outperforms all previous methods\ntrained on publicly available datasets by a large margin. It even outperforms\nmodels that were trained on non-publicly available datasets containing up to to\n21 times more data. We show, furthermore, that using additional training data,\neven in other languages or with automatically generated transcriptions, results\nin further improvement.", "published": "2022-02-26 07:21:00", "link": "http://arxiv.org/abs/2202.13084v2", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
