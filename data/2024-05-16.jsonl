{"title": "An Analysis of Sentential Neighbors in Implicit Discourse Relation\n  Prediction", "abstract": "Discourse relation classification is an especially difficult task without\nexplicit context markers (Prasad et al., 2008). Current approaches to implicit\nrelation prediction solely rely on two neighboring sentences being targeted,\nignoring the broader context of their surrounding environments (Atwell et al.,\n2021). In this research, we propose three new methods in which to incorporate\ncontext in the task of sentence relation prediction: (1) Direct Neighbors\n(DNs), (2) Expanded Window Neighbors (EWNs), and (3) Part-Smart Random\nNeighbors (PSRNs). Our findings indicate that the inclusion of context beyond\none discourse unit is harmful in the task of discourse relation classification.", "published": "2024-05-16 00:06:36", "link": "http://arxiv.org/abs/2405.09735v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Chameleon: Mixed-Modal Early-Fusion Foundation Models", "abstract": "We present Chameleon, a family of early-fusion token-based mixed-modal models\ncapable of understanding and generating images and text in any arbitrary\nsequence. We outline a stable training approach from inception, an alignment\nrecipe, and an architectural parameterization tailored for the early-fusion,\ntoken-based, mixed-modal setting. The models are evaluated on a comprehensive\nrange of tasks, including visual question answering, image captioning, text\ngeneration, image generation, and long-form mixed modal generation. Chameleon\ndemonstrates broad and general capabilities, including state-of-the-art\nperformance in image captioning tasks, outperforms Llama-2 in text-only tasks\nwhile being competitive with models such as Mixtral 8x7B and Gemini-Pro, and\nperforms non-trivial image generation, all in a single model. It also matches\nor exceeds the performance of much larger models, including Gemini Pro and\nGPT-4V, according to human judgments on a new long-form mixed-modal generation\nevaluation, where either the prompt or outputs contain mixed sequences of both\nimages and text. Chameleon marks a significant step forward in a unified\nmodeling of full multimodal documents.", "published": "2024-05-16 05:23:41", "link": "http://arxiv.org/abs/2405.09818v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Striking a Balance between Classical and Deep Learning Approaches in\n  Natural Language Processing Pedagogy", "abstract": "While deep learning approaches represent the state-of-the-art of natural\nlanguage processing (NLP) today, classical algorithms and approaches still find\na place in NLP textbooks and courses of recent years. This paper discusses the\nperspectives of conveners of two introductory NLP courses taught in Australia\nand India, and examines how classical and deep learning approaches can be\nbalanced within the lecture plan and assessments of the courses. We also draw\nparallels with the objects-first and objects-later debate in CS1 education. We\nobserve that teaching classical approaches adds value to student learning by\nbuilding an intuitive understanding of NLP problems, potential solutions, and\neven deep learning models themselves. Despite classical approaches not being\nstate-of-the-art, the paper makes a case for their inclusion in NLP courses\ntoday.", "published": "2024-05-16 07:14:13", "link": "http://arxiv.org/abs/2405.09854v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TransMI: A Framework to Create Strong Baselines from Multilingual\n  Pretrained Language Models for Transliterated Data", "abstract": "Transliterating related languages that use different scripts into a common\nscript is effective for improving crosslingual transfer in downstream tasks.\nHowever, this methodology often makes pretraining a model from scratch\nunavoidable, as transliteration brings about new subwords not covered in\nexisting multilingual pretrained language models (mPLMs). This is undesirable\nbecause it requires a large computation budget. A more promising way is to make\nfull use of available mPLMs. To this end, this paper proposes a simple but\neffective framework: Transliterate-Merge-Initialize (TransMI). TransMI can\ncreate strong baselines for data that is transliterated into a common script by\nexploiting an existing mPLM and its tokenizer without any training. TransMI has\nthree stages: (a) transliterate the vocabulary of an mPLM into a common script;\n(b) merge the new vocabulary with the original vocabulary; and (c) initialize\nthe embeddings of the new subwords. We apply TransMI to three strong recent\nmPLMs. Our experiments demonstrate that TransMI not only preserves the mPLM's\nability to handle non-transliterated data, but also enables it to effectively\nprocess transliterated data, thereby facilitating crosslingual transfer across\nscripts. The results show consistent improvements of 3% to 34% for different\nmPLMs and tasks. We make our code and models publicly available at\n\\url{https://github.com/cisnlp/TransMI}.", "published": "2024-05-16 09:08:09", "link": "http://arxiv.org/abs/2405.09913v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mitigating Text Toxicity with Counterfactual Generation", "abstract": "Toxicity mitigation consists in rephrasing text in order to remove offensive\nor harmful meaning. Neural natural language processing (NLP) models have been\nwidely used to target and mitigate textual toxicity. However, existing methods\nfail to detoxify text while preserving the initial non-toxic meaning at the\nsame time. In this work, we propose to apply counterfactual generation methods\nfrom the eXplainable AI (XAI) field to target and mitigate textual toxicity. In\nparticular, we perform text detoxification by applying local feature importance\nand counterfactual generation methods to a toxicity classifier distinguishing\nbetween toxic and non-toxic texts. We carry out text detoxification through\ncounterfactual generation on three datasets and compare our approach to three\ncompetitors. Automatic and human evaluations show that recently developed NLP\ncounterfactual generators can mitigate toxicity accurately while better\npreserving the meaning of the initial text as compared to classical\ndetoxification methods. Finally, we take a step back from using automated\ndetoxification tools, and discuss how to manage the polysemous nature of\ntoxicity and the risk of malicious use of detoxification tools. This work is\nthe first to bridge the gap between counterfactual generation and text\ndetoxification and paves the way towards more practical application of XAI\nmethods.", "published": "2024-05-16 09:52:21", "link": "http://arxiv.org/abs/2405.09948v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Turkronicles: Diachronic Resources for the Fast Evolving Turkish\n  Language", "abstract": "Over the past century, the Turkish language has undergone substantial\nchanges, primarily driven by governmental interventions. In this work, our goal\nis to investigate the evolution of the Turkish language since the establishment\nof T\\\"urkiye in 1923. Thus, we first introduce Turkronicles which is a\ndiachronic corpus for Turkish derived from the Official Gazette of T\\\"urkiye.\nTurkronicles contains 45,375 documents, detailing governmental actions, making\nit a pivotal resource for analyzing the linguistic evolution influenced by the\nstate policies. In addition, we expand an existing diachronic Turkish corpus\nwhich consists of the records of the Grand National Assembly of T\\\"urkiye by\ncovering additional years. Next, combining these two diachronic corpora, we\nseek answers for two main research questions: How have the Turkish vocabulary\nand the writing conventions changed since the 1920s? Our analysis reveals that\nthe vocabularies of two different time periods diverge more as the time between\nthem increases, and newly coined Turkish words take the place of their old\ncounterparts. We also observe changes in writing conventions. In particular,\nthe use of circumflex noticeably decreases and words ending with the letters\n\"-b\" and \"-d\" are successively replaced with \"-p\" and \"-t\" letters,\nrespectively. Overall, this study quantitatively highlights the dramatic\nchanges in Turkish from various aspects of the language in a diachronic\nperspective.", "published": "2024-05-16 14:31:07", "link": "http://arxiv.org/abs/2405.10133v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PL-MTEB: Polish Massive Text Embedding Benchmark", "abstract": "In this paper, we introduce the Polish Massive Text Embedding Benchmark\n(PL-MTEB), a comprehensive benchmark for text embeddings in Polish. The PL-MTEB\nconsists of 28 diverse NLP tasks from 5 task types. We adapted the tasks based\non previously used datasets by the Polish NLP community. In addition, we\ncreated a new PLSC (Polish Library of Science Corpus) dataset consisting of\ntitles and abstracts of scientific publications in Polish, which was used as\nthe basis for two novel clustering tasks. We evaluated 15 publicly available\nmodels for text embedding, including Polish and multilingual ones, and\ncollected detailed results for individual tasks and aggregated results for each\ntask type and the entire benchmark. PL-MTEB comes with open-source code at\nhttps://github.com/rafalposwiata/pl-mteb.", "published": "2024-05-16 14:33:39", "link": "http://arxiv.org/abs/2405.10138v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Speaker Verification in Agent-Generated Conversations", "abstract": "The recent success of large language models (LLMs) has attracted widespread\ninterest to develop role-playing conversational agents personalized to the\ncharacteristics and styles of different speakers to enhance their abilities to\nperform both general and special purpose dialogue tasks. However, the ability\nto personalize the generated utterances to speakers, whether conducted by human\nor LLM, has not been well studied. To bridge this gap, our study introduces a\nnovel evaluation challenge: speaker verification in agent-generated\nconversations, which aimed to verify whether two sets of utterances originate\nfrom the same speaker. To this end, we assemble a large dataset collection\nencompassing thousands of speakers and their utterances. We also develop and\nevaluate speaker verification models under experiment setups. We further\nutilize the speaker verification models to evaluate the personalization\nabilities of LLM-based role-playing models. Comprehensive experiments suggest\nthat the current role-playing models fail in accurately mimicking speakers,\nprimarily due to their inherent linguistic characteristics.", "published": "2024-05-16 14:46:18", "link": "http://arxiv.org/abs/2405.10150v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hierarchical Attention Graph for Scientific Document Summarization in\n  Global and Local Level", "abstract": "Scientific document summarization has been a challenging task due to the long\nstructure of the input text. The long input hinders the simultaneous effective\nmodeling of both global high-order relations between sentences and local\nintra-sentence relations which is the most critical step in extractive\nsummarization. However, existing methods mostly focus on one type of relation,\nneglecting the simultaneous effective modeling of both relations, which can\nlead to insufficient learning of semantic representations. In this paper, we\npropose HAESum, a novel approach utilizing graph neural networks to locally and\nglobally model documents based on their hierarchical discourse structure.\nFirst, intra-sentence relations are learned using a local heterogeneous graph.\nSubsequently, a novel hypergraph self-attention layer is introduced to further\nenhance the characterization of high-order inter-sentence relations. We\nvalidate our approach on two benchmark datasets, and the experimental results\ndemonstrate the effectiveness of HAESum and the importance of considering\nhierarchical structures in modeling long scientific documents. Our code will be\navailable at \\url{https://github.com/MoLICHENXI/HAESum}", "published": "2024-05-16 15:46:30", "link": "http://arxiv.org/abs/2405.10202v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CPsyExam: A Chinese Benchmark for Evaluating Psychology using\n  Examinations", "abstract": "In this paper, we introduce a novel psychological benchmark, CPsyExam,\nconstructed from questions sourced from Chinese language examinations. CPsyExam\nis designed to prioritize psychological knowledge and case analysis separately,\nrecognizing the significance of applying psychological knowledge to real-world\nscenarios. From the pool of 22k questions, we utilize 4k to create the\nbenchmark that offers balanced coverage of subjects and incorporates a diverse\nrange of case analysis techniques.Furthermore, we evaluate a range of existing\nlarge language models~(LLMs), spanning from open-sourced to API-based models.\nOur experiments and analysis demonstrate that CPsyExam serves as an effective\nbenchmark for enhancing the understanding of psychology within LLMs and enables\nthe comparison of LLMs across various granularities.", "published": "2024-05-16 16:02:18", "link": "http://arxiv.org/abs/2405.10212v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Systematic Evaluation of Large Language Models for Natural Language\n  Generation Tasks", "abstract": "Recent efforts have evaluated large language models (LLMs) in areas such as\ncommonsense reasoning, mathematical reasoning, and code generation. However, to\nthe best of our knowledge, no work has specifically investigated the\nperformance of LLMs in natural language generation (NLG) tasks, a pivotal\ncriterion for determining model excellence. Thus, this paper conducts a\ncomprehensive evaluation of well-known and high-performing LLMs, namely\nChatGPT, ChatGLM, T5-based models, LLaMA-based models, and Pythia-based models,\nin the context of NLG tasks. We select English and Chinese datasets\nencompassing Dialogue Generation and Text Summarization. Moreover, we propose a\ncommon evaluation setting that incorporates input templates and post-processing\nstrategies. Our study reports both automatic results, accompanied by a detailed\nanalysis.", "published": "2024-05-16 16:56:54", "link": "http://arxiv.org/abs/2405.10251v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Thinking Fair and Slow: On the Efficacy of Structured Prompts for\n  Debiasing Language Models", "abstract": "Existing debiasing techniques are typically training-based or require access\nto the model's internals and output distributions, so they are inaccessible to\nend-users looking to adapt LLM outputs for their particular needs. In this\nstudy, we examine whether structured prompting techniques can offer\nopportunities for fair text generation. We evaluate a comprehensive\nend-user-focused iterative framework of debiasing that applies System 2\nthinking processes for prompts to induce logical, reflective, and critical text\ngeneration, with single, multi-step, instruction, and role-based variants. By\nsystematically evaluating many LLMs across many datasets and different\nprompting strategies, we show that the more complex System 2-based Implicative\nPrompts significantly improve over other techniques demonstrating lower mean\nbias in the outputs with competitive performance on the downstream tasks. Our\nwork offers research directions for the design and the potential of\nend-user-focused evaluative frameworks for LLM use.", "published": "2024-05-16 20:27:58", "link": "http://arxiv.org/abs/2405.10431v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Hybrid Framework with Large Language Models for Rare Disease\n  Phenotyping", "abstract": "Rare diseases pose significant challenges in diagnosis and treatment due to\ntheir low prevalence and heterogeneous clinical presentations. Unstructured\nclinical notes contain valuable information for identifying rare diseases, but\nmanual curation is time-consuming and prone to subjectivity. This study aims to\ndevelop a hybrid approach combining dictionary-based natural language\nprocessing (NLP) tools with large language models (LLMs) to improve rare\ndisease identification from unstructured clinical reports. We propose a novel\nhybrid framework that integrates the Orphanet Rare Disease Ontology (ORDO) and\nthe Unified Medical Language System (UMLS) to create a comprehensive rare\ndisease vocabulary. The proposed hybrid approach demonstrates superior\nperformance compared to traditional NLP systems and standalone LLMs. Notably,\nthe approach uncovers a significant number of potential rare disease cases not\ndocumented in structured diagnostic records, highlighting its ability to\nidentify previously unrecognized patients.", "published": "2024-05-16 20:59:28", "link": "http://arxiv.org/abs/2405.10440v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Participle-Prepended Nominals Have Lower Entropy Than Nominals Appended\n  After the Participle", "abstract": "English allows for both compounds (e.g., London-made) and phrasal paraphrases\n(e.g., made in London). While these constructions have roughly the same\ntruth-conditional meaning, we hypothesize that the compound allows less freedom\nto express the nature of the semantic relationship between the participle and\nthe pre-participle nominal. We thus predict that the pre-participle slot is\nmore constrained than the equivalent position in the phrasal construction. We\ntest this prediction in a large corpus by measuring the entropy of\ncorresponding nominal slots, conditional on the participle used. That is, we\ncompare the entropy of $\\alpha$ in compound construction slots like\n$\\alpha$-[V]ed to the entropy of $\\alpha$ in phrasal constructions like [V]ed\nby $\\alpha$ for a given verb V. As predicted, there is significantly lower\nentropy in the compound construction than in the phrasal construction. We\nconsider how these predictions follow from more general grammatical properties\nand processing factors.", "published": "2024-05-16 22:05:45", "link": "http://arxiv.org/abs/2405.10457v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Many Hands Make Light Work: Task-Oriented Dialogue System with\n  Module-Based Mixture-of-Experts", "abstract": "Task-oriented dialogue systems are broadly used in virtual assistants and\nother automated services, providing interfaces between users and machines to\nfacilitate specific tasks. Nowadays, task-oriented dialogue systems have\ngreatly benefited from pre-trained language models (PLMs). However, their\ntask-solving performance is constrained by the inherent capacities of PLMs, and\nscaling these models is expensive and complex as the model size becomes larger.\nTo address these challenges, we propose Soft Mixture-of-Expert Task-Oriented\nDialogue system (SMETOD) which leverages an ensemble of Mixture-of-Experts\n(MoEs) to excel at subproblems and generate specialized outputs for\ntask-oriented dialogues. SMETOD also scales up a task-oriented dialogue system\nwith simplicity and flexibility while maintaining inference efficiency. We\nextensively evaluate our model on three benchmark functionalities: intent\nprediction, dialogue state tracking, and dialogue response generation.\nExperimental results demonstrate that SMETOD achieves state-of-the-art\nperformance on most evaluated metrics. Moreover, comparisons against existing\nstrong baselines show that SMETOD has a great advantage in the cost of\ninference and correctness in problem-solving.", "published": "2024-05-16 01:02:09", "link": "http://arxiv.org/abs/2405.09744v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unsupervised Extractive Dialogue Summarization in Hyperdimensional Space", "abstract": "We present HyperSum, an extractive summarization framework that captures both\nthe efficiency of traditional lexical summarization and the accuracy of\ncontemporary neural approaches. HyperSum exploits the pseudo-orthogonality that\nemerges when randomly initializing vectors at extremely high dimensions\n(\"blessing of dimensionality\") to construct representative and efficient\nsentence embeddings. Simply clustering the obtained embeddings and extracting\ntheir medoids yields competitive summaries. HyperSum often outperforms\nstate-of-the-art summarizers -- in terms of both summary accuracy and\nfaithfulness -- while being 10 to 100 times faster. We open-source HyperSum as\na strong baseline for unsupervised extractive summarization.", "published": "2024-05-16 02:11:03", "link": "http://arxiv.org/abs/2405.09765v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Optimization Techniques for Sentiment Analysis Based on LLM (GPT-3)", "abstract": "With the rapid development of natural language processing (NLP) technology,\nlarge-scale pre-trained language models such as GPT-3 have become a popular\nresearch object in NLP field. This paper aims to explore sentiment analysis\noptimization techniques based on large pre-trained language models such as\nGPT-3 to improve model performance and effect and further promote the\ndevelopment of natural language processing (NLP). By introducing the importance\nof sentiment analysis and the limitations of traditional methods, GPT-3 and\nFine-tuning techniques are introduced in this paper, and their applications in\nsentiment analysis are explained in detail. The experimental results show that\nthe Fine-tuning technique can optimize GPT-3 model and obtain good performance\nin sentiment analysis task. This study provides an important reference for\nfuture sentiment analysis using large-scale language models.", "published": "2024-05-16 02:21:13", "link": "http://arxiv.org/abs/2405.09770v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SecureLLM: Using Compositionality to Build Provably Secure Language\n  Models for Private, Sensitive, and Secret Data", "abstract": "Traditional security mechanisms isolate resources from users who should not\naccess them. We reflect the compositional nature of such security mechanisms\nback into the structure of LLMs to build a provably secure LLM; that we term\nSecureLLM. Other approaches to LLM safety attempt to protect against bad actors\nor bad outcomes, but can only do so to an extent making them inappropriate for\nsensitive data. SecureLLM blends access security with fine-tuning methods. Each\ndata silo has associated with it a separate fine-tuning and a user has access\nonly to the collection of fine-tunings that they have permission for. The model\nmust then perform on compositional tasks at the intersection of those data\nsilos with the combination of those individual fine-tunings. While applicable\nto any task like document QA or making API calls, in this work we concern\nourselves with models that learn the layouts of new SQL databases to provide\nnatural-language-to-SQL translation capabilities. Existing fine-tuning\ncomposition methods fail in this challenging environment, as they are not\nwell-equipped for handling compositional tasks. Compositionality remains a\nchallenge for LLMs. We contribute both a difficult new compositional\nnatural-language-to-SQL translation task and a new perspective on LLM security\nthat allows models to be deployed to secure environments today.", "published": "2024-05-16 04:25:53", "link": "http://arxiv.org/abs/2405.09805v2", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Enhancing Semantics in Multimodal Chain of Thought via Soft Negative\n  Sampling", "abstract": "Chain of thought (CoT) has proven useful for problems requiring complex\nreasoning. Many of these problems are both textual and multimodal. Given the\ninputs in different modalities, a model generates a rationale and then uses it\nto answer a question. Because of the hallucination issue, the generated soft\nnegative rationales with high textual quality but illogical semantics do not\nalways help improve answer accuracy. This study proposes a rationale generation\nmethod using soft negative sampling (SNSE-CoT) to mitigate hallucinations in\nmultimodal CoT. Five methods were applied to generate soft negative samples\nthat shared highly similar text but had different semantics from the original.\nBidirectional margin loss (BML) was applied to introduce them into the\ntraditional contrastive learning framework that involves only positive and\nnegative samples. Extensive experiments on the ScienceQA dataset demonstrated\nthe effectiveness of the proposed method. Code and data are released at\nhttps://github.com/zgMin/SNSE-CoT.", "published": "2024-05-16 06:55:11", "link": "http://arxiv.org/abs/2405.09848v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "IGOT: Information Gain Optimized Tokenizer on Domain Adaptive\n  Pretraining", "abstract": "Pretrained Large Language Models (LLM) such as ChatGPT, Claude, etc. have\ndemonstrated strong capabilities in various fields of natural language\ngeneration. However, there are still many problems when using LLM in\nspecialized domain-specific fields. When using generative AI to process\ndownstream tasks, a common approach is to add new knowledge (e.g., private\ndomain knowledge, cutting-edge information) to a pretrained model through\ncontinued training or fine-tuning. However, whether there is a universal\nparadigm for domain adaptation training is still an open question. In this\narticle, we proposed Information Gain Optimized Tokenizer (IGOT), which\nanalyzes the special token set of downstream tasks, constructs a new subset\nusing heuristic function $\\phi$ with the special token and its information\ngain, to build new domain-specific tokenizer, and continues pretraining on the\ndownstream task data. We explored the many positive effects of this method's\ncustomized tokenizer on domain-adaptive pretraining and verified this method\ncan perform better than the ordinary method of just collecting data and\nfine-tuning. Based on our experiment, the continued pretraining process of IGOT\nwith LLaMA-7B achieved 11.9\\% token saving, 12.2\\% training time saving, and\n5.8\\% maximum GPU VRAM usage saving, combined with the T5 model, we can even\nreach a 31.5\\% of training time saving, making porting general generative AI to\nspecific domains more effective than before. In domain-specific tasks,\nsupervised $IGOT_\\tau$ shows great performance on reducing both the convergence\nradius and convergence point during keep pretraining.", "published": "2024-05-16 07:25:10", "link": "http://arxiv.org/abs/2405.09857v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "\"Hunt Takes Hare\": Theming Games Through Game-Word Vector Translation", "abstract": "A game's theme is an important part of its design -- it conveys narrative\ninformation, rhetorical messages, helps the player intuit strategies, aids in\ntutorialisation and more. Thematic elements of games are notoriously difficult\nfor AI systems to understand and manipulate, however, and often rely on large\namounts of hand-written interpretations and knowledge. In this paper we present\na technique which connects game embeddings, a recent method for modelling game\ndynamics from log data, and word embeddings, which models semantic information\nabout language. We explain two different approaches for using game embeddings\nin this way, and show evidence that game embeddings enhance the linguistic\ntranslations of game concepts from one theme to another, opening up exciting\nnew possibilities for reasoning about the thematic elements of games in the\nfuture.", "published": "2024-05-16 08:19:11", "link": "http://arxiv.org/abs/2405.09893v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "DEBATE: Devil's Advocate-Based Assessment and Text Evaluation", "abstract": "As natural language generation (NLG) models have become prevalent,\nsystematically assessing the quality of machine-generated texts has become\nincreasingly important. Recent studies introduce LLM-based evaluators that\noperate as reference-free metrics, demonstrating their capability to adeptly\nhandle novel tasks. However, these models generally rely on a single-agent\napproach, which, we argue, introduces an inherent limit to their performance.\nThis is because there exist biases in LLM agent's responses, including\npreferences for certain text structure or content. In this work, we propose\nDEBATE, an NLG evaluation framework based on multi-agent scoring system\naugmented with a concept of Devil's Advocate. Within the framework, one agent\nis instructed to criticize other agents' arguments, potentially resolving the\nbias in LLM agent's answers. DEBATE substantially outperforms the previous\nstate-of-the-art methods in two meta-evaluation benchmarks in NLG evaluation,\nSummEval and TopicalChat. We also show that the extensiveness of debates among\nagents and the persona of an agent can influence the performance of evaluators.", "published": "2024-05-16 09:41:12", "link": "http://arxiv.org/abs/2405.09935v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SciQAG: A Framework for Auto-Generated Science Question Answering\n  Dataset with Fine-grained Evaluation", "abstract": "We introduce SciQAG, a novel framework for automatically generating\nhigh-quality science question-answer pairs from a large corpus of scientific\nliterature based on large language models (LLMs). SciQAG consists of a QA\ngenerator and a QA evaluator, which work together to extract diverse and\nresearch-level questions and answers from scientific papers. Utilizing this\nframework, we construct a large-scale, high-quality, open-ended science QA\ndataset containing 188,042 QA pairs extracted from 22,743 scientific papers\nacross 24 scientific domains. We also introduce SciQAG-24D, a new benchmark\ntask designed to evaluate the science question-answering ability of LLMs.\nExtensive experiments demonstrate that fine-tuning LLMs on the SciQAG dataset\nsignificantly improves their performance on both open-ended question answering\nand scientific tasks. To foster research and collaboration, we make the\ndatasets, models, and evaluation codes publicly available, contributing to the\nadvancement of science question answering and developing more interpretable and\nreasoning-capable AI systems.", "published": "2024-05-16 09:42:37", "link": "http://arxiv.org/abs/2405.09939v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FinTextQA: A Dataset for Long-form Financial Question Answering", "abstract": "Accurate evaluation of financial question answering (QA) systems necessitates\na comprehensive dataset encompassing diverse question types and contexts.\nHowever, current financial QA datasets lack scope diversity and question\ncomplexity. This work introduces FinTextQA, a novel dataset for long-form\nquestion answering (LFQA) in finance. FinTextQA comprises 1,262 high-quality,\nsource-attributed QA pairs extracted and selected from finance textbooks and\ngovernment agency websites.Moreover, we developed a Retrieval-Augmented\nGeneration (RAG)-based LFQA system, comprising an embedder, retriever,\nreranker, and generator. A multi-faceted evaluation approach, including human\nranking, automatic metrics, and GPT-4 scoring, was employed to benchmark the\nperformance of different LFQA system configurations under heightened noisy\nconditions. The results indicate that: (1) Among all compared generators,\nBaichuan2-7B competes closely with GPT-3.5-turbo in accuracy score; (2) The\nmost effective system configuration on our dataset involved setting the\nembedder, retriever, reranker, and generator as Ada2, Automated Merged\nRetrieval, Bge-Reranker-Base, and Baichuan2-7B, respectively; (3) models are\nless susceptible to noise after the length of contexts reaching a specific\nthreshold.", "published": "2024-05-16 10:53:31", "link": "http://arxiv.org/abs/2405.09980v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Hierarchical Classification on the Common Procurement\n  Vocabulary Taxonomy", "abstract": "Classifying public tenders is a useful task for both companies that are\ninvited to participate and for inspecting fraudulent activities. To facilitate\nthe task for both participants and public administrations, the European Union\npresented a common taxonomy (Common Procurement Vocabulary, CPV) which is\nmandatory for tenders of certain importance; however, the contracts in which a\nCPV label is mandatory are the minority compared to all the Public\nAdministrations activities. Classifying over a real-world taxonomy introduces\nsome difficulties that can not be ignored. First of all, some fine-grained\nclasses have an insufficient (if any) number of observations in the training\nset, while other classes are far more frequent (even thousands of times) than\nthe average. To overcome those difficulties, we present a zero-shot approach,\nbased on a pre-trained language model that relies only on label description and\nrespects the label taxonomy. To train our proposed model, we used industrial\ndata, which comes from contrattipubblici.org, a service by SpazioDati s.r.l.\nthat collects public contracts stipulated in Italy in the last 25 years.\nResults show that the proposed model achieves better performance in classifying\nlow-frequent classes compared to three different baselines, and is also able to\npredict never-seen classes.", "published": "2024-05-16 11:01:09", "link": "http://arxiv.org/abs/2405.09983v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "MarkLLM: An Open-Source Toolkit for LLM Watermarking", "abstract": "LLM watermarking, which embeds imperceptible yet algorithmically detectable\nsignals in model outputs to identify LLM-generated text, has become crucial in\nmitigating the potential misuse of large language models. However, the\nabundance of LLM watermarking algorithms, their intricate mechanisms, and the\ncomplex evaluation procedures and perspectives pose challenges for researchers\nand the community to easily experiment with, understand, and assess the latest\nadvancements. To address these issues, we introduce MarkLLM, an open-source\ntoolkit for LLM watermarking. MarkLLM offers a unified and extensible framework\nfor implementing LLM watermarking algorithms, while providing user-friendly\ninterfaces to ensure ease of access. Furthermore, it enhances understanding by\nsupporting automatic visualization of the underlying mechanisms of these\nalgorithms. For evaluation, MarkLLM offers a comprehensive suite of 12 tools\nspanning three perspectives, along with two types of automated evaluation\npipelines. Through MarkLLM, we aim to support researchers while improving the\ncomprehension and involvement of the general public in LLM watermarking\ntechnology, fostering consensus and driving further advancements in research\nand application. Our code is available at https://github.com/THU-BPM/MarkLLM.", "published": "2024-05-16 12:40:01", "link": "http://arxiv.org/abs/2405.10051v6", "categories": ["cs.CR", "cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CR"}
{"title": "Distilling Implicit Multimodal Knowledge into Large Language Models for\n  Zero-Resource Dialogue Generation", "abstract": "Integrating multimodal knowledge into large language models (LLMs) represents\na significant advancement in dialogue generation capabilities. However, the\neffective incorporation of such knowledge in zero-resource scenarios remains a\nsubstantial challenge due to the scarcity of diverse, high-quality dialogue\ndatasets. To address this, we propose the Visual Implicit Knowledge\nDistillation Framework (VIKDF), an innovative approach aimed at enhancing LLMs\nfor enriched dialogue generation in zero-resource contexts by leveraging\nimplicit multimodal knowledge. VIKDF comprises two main stages: knowledge\ndistillation, using an Implicit Query Transformer to extract and encode visual\nimplicit knowledge from image-text pairs into knowledge vectors; and knowledge\nintegration, employing a novel Bidirectional Variational Information Fusion\ntechnique to seamlessly integrate these distilled vectors into LLMs. This\nenables the LLMs to generate dialogues that are not only coherent and engaging\nbut also exhibit a deep understanding of the context through implicit\nmultimodal cues, effectively overcoming the limitations of zero-resource\nscenarios. Our extensive experimentation across two dialogue datasets shows\nthat VIKDF outperforms existing state-of-the-art models in generating\nhigh-quality dialogues. The code is available at\nhttps://github.com/zhangbo-nlp/VIKDF.", "published": "2024-05-16 14:21:33", "link": "http://arxiv.org/abs/2405.10121v2", "categories": ["cs.CL", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Red Teaming Language Models for Processing Contradictory Dialogues", "abstract": "Most language models currently available are prone to self-contradiction\nduring dialogues. To mitigate this issue, this study explores a novel\ncontradictory dialogue processing task that aims to detect and modify\ncontradictory statements in a conversation. This task is inspired by research\non context faithfulness and dialogue comprehension, which have demonstrated\nthat the detection and understanding of contradictions often necessitate\ndetailed explanations. We develop a dataset comprising contradictory dialogues,\nin which one side of the conversation contradicts itself. Each dialogue is\naccompanied by an explanatory label that highlights the location and details of\nthe contradiction. With this dataset, we present a Red Teaming framework for\ncontradictory dialogue processing. The framework detects and attempts to\nexplain the dialogue, then modifies the existing contradictory content using\nthe explanation. Our experiments demonstrate that the framework improves the\nability to detect contradictory dialogues and provides valid explanations.\nAdditionally, it showcases distinct capabilities for modifying such dialogues.\nOur study highlights the importance of the logical inconsistency problem in\nconversational AI.", "published": "2024-05-16 14:27:32", "link": "http://arxiv.org/abs/2405.10128v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "StyloAI: Distinguishing AI-Generated Content with Stylometric Analysis", "abstract": "The emergence of large language models (LLMs) capable of generating realistic\ntexts and images has sparked ethical concerns across various sectors. In\nresponse, researchers in academia and industry are actively exploring methods\nto distinguish AI-generated content from human-authored material. However, a\ncrucial question remains: What are the unique characteristics of AI-generated\ntext? Addressing this gap, this study proposes StyloAI, a data-driven model\nthat uses 31 stylometric features to identify AI-generated texts by applying a\nRandom Forest classifier on two multi-domain datasets. StyloAI achieves\naccuracy rates of 81% and 98% on the test set of the AuTextification dataset\nand the Education dataset, respectively. This approach surpasses the\nperformance of existing state-of-the-art models and provides valuable insights\ninto the differences between AI-generated and human-authored texts.", "published": "2024-05-16 14:28:01", "link": "http://arxiv.org/abs/2405.10129v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "LFED: A Literary Fiction Evaluation Dataset for Large Language Models", "abstract": "The rapid evolution of large language models (LLMs) has ushered in the need\nfor comprehensive assessments of their performance across various dimensions.\nIn this paper, we propose LFED, a Literary Fiction Evaluation Dataset, which\naims to evaluate the capability of LLMs on the long fiction comprehension and\nreasoning. We collect 95 literary fictions that are either originally written\nin Chinese or translated into Chinese, covering a wide range of topics across\nseveral centuries. We define a question taxonomy with 8 question categories to\nguide the creation of 1,304 questions. Additionally, we conduct an in-depth\nanalysis to ascertain how specific attributes of literary fictions (e.g., novel\ntypes, character numbers, the year of publication) impact LLM performance in\nevaluations. Through a series of experiments with various state-of-the-art\nLLMs, we demonstrate that these models face considerable challenges in\neffectively addressing questions related to literary fictions, with ChatGPT\nreaching only 57.08% under the zero-shot setting. The dataset will be publicly\navailable at https://github.com/tjunlp-lab/LFED.git", "published": "2024-05-16 15:02:24", "link": "http://arxiv.org/abs/2405.10166v1", "categories": ["cs.CL", "cs.PF"], "primary_category": "cs.CL"}
{"title": "Keep It Private: Unsupervised Privatization of Online Text", "abstract": "Authorship obfuscation techniques hold the promise of helping people protect\ntheir privacy in online communications by automatically rewriting text to hide\nthe identity of the original author. However, obfuscation has been evaluated in\nnarrow settings in the NLP literature and has primarily been addressed with\nsuperficial edit operations that can lead to unnatural outputs. In this work,\nwe introduce an automatic text privatization framework that fine-tunes a large\nlanguage model via reinforcement learning to produce rewrites that balance\nsoundness, sense, and privacy. We evaluate it extensively on a large-scale test\nset of English Reddit posts by 68k authors composed of short-medium length\ntexts. We study how the performance changes among evaluative conditions\nincluding authorial profile length and authorship detection strategy. Our\nmethod maintains high text quality according to both automated metrics and\nhuman evaluation, and successfully evades several automated authorship attacks.", "published": "2024-05-16 17:12:18", "link": "http://arxiv.org/abs/2405.10260v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Tale of Two Languages: Large-Vocabulary Continuous Sign Language\n  Recognition from Spoken Language Supervision", "abstract": "In this work, our goals are two fold: large-vocabulary continuous sign\nlanguage recognition (CSLR), and sign language retrieval. To this end, we\nintroduce a multi-task Transformer model, CSLR2, that is able to ingest a\nsigning sequence and output in a joint embedding space between signed language\nand spoken language text. To enable CSLR evaluation in the large-vocabulary\nsetting, we introduce new dataset annotations that have been manually\ncollected. These provide continuous sign-level annotations for six hours of\ntest videos, and will be made publicly available. We demonstrate that by a\ncareful choice of loss functions, training the model for both the CSLR and\nretrieval tasks is mutually beneficial in terms of performance -- retrieval\nimproves CSLR performance by providing context, while CSLR improves retrieval\nwith more fine-grained supervision. We further show the benefits of leveraging\nweak and noisy supervision from large-vocabulary datasets such as BOBSL, namely\nsign-level pseudo-labels, and English subtitles. Our model significantly\noutperforms the previous state of the art on both tasks.", "published": "2024-05-16 17:19:06", "link": "http://arxiv.org/abs/2405.10266v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Revisiting OPRO: The Limitations of Small-Scale LLMs as Optimizers", "abstract": "Numerous recent works aim to enhance the efficacy of Large Language Models\n(LLMs) through strategic prompting. In particular, the Optimization by\nPROmpting (OPRO) approach provides state-of-the-art performance by leveraging\nLLMs as optimizers where the optimization task is to find instructions that\nmaximize the task accuracy. In this paper, we revisit OPRO for automated\nprompting with relatively small-scale LLMs, such as LLaMa-2 family and Mistral\n7B. Our investigation reveals that OPRO shows limited effectiveness in\nsmall-scale LLMs, with limited inference capabilities constraining optimization\nability. We suggest future automatic prompting engineering to consider both\nmodel capabilities and computational costs. Additionally, for small-scale LLMs,\nwe recommend direct instructions that clearly outline objectives and\nmethodologies as robust prompt baselines, ensuring efficient and effective\nprompt engineering in ongoing research.", "published": "2024-05-16 17:33:50", "link": "http://arxiv.org/abs/2405.10276v2", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Timeline-based Sentence Decomposition with In-Context Learning for\n  Temporal Fact Extraction", "abstract": "Facts extraction is pivotal for constructing knowledge graphs. Recently, the\nincreasing demand for temporal facts in downstream tasks has led to the\nemergence of the task of temporal fact extraction. In this paper, we\nspecifically address the extraction of temporal facts from natural language\ntext. Previous studies fail to handle the challenge of establishing\ntime-to-fact correspondences in complex sentences. To overcome this hurdle, we\npropose a timeline-based sentence decomposition strategy using large language\nmodels (LLMs) with in-context learning, ensuring a fine-grained understanding\nof the timeline associated with various facts. In addition, we evaluate the\nperformance of LLMs for direct temporal fact extraction and get unsatisfactory\nresults. To this end, we introduce TSDRE, a method that incorporates the\ndecomposition capabilities of LLMs into the traditional fine-tuning of smaller\npre-trained language models (PLMs). To support the evaluation, we construct\nComplexTRED, a complex temporal fact extraction dataset. Our experiments show\nthat TSDRE achieves state-of-the-art results on both HyperRED-Temporal and\nComplexTRED datasets.", "published": "2024-05-16 17:48:21", "link": "http://arxiv.org/abs/2405.10288v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in\n  Fine-tuning LLMs for Simultaneous Translation", "abstract": "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost.", "published": "2024-05-16 21:07:42", "link": "http://arxiv.org/abs/2405.10443v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exploring Public Attention in the Circular Economy through Topic\n  Modelling with Twin Hyperparameter Optimisation", "abstract": "To advance the circular economy (CE), it is crucial to gain insights into the\nevolution of public attention, cognitive pathways of the masses concerning\ncircular products, and to identify primary concerns. To achieve this, we\ncollected data from diverse platforms, including Twitter, Reddit, and The\nGuardian, and utilised three topic models to analyse the data. Given the\nperformance of topic modelling may vary depending on hyperparameter settings,\nthis research proposed a novel framework that integrates twin (single and\nmulti-objective) hyperparameter optimisation for the CE. We conducted\nsystematic experiments to ensure that topic models are set with appropriate\nhyperparameters under different constraints, providing valuable insights into\nthe correlations between CE and public attention. In summary, our optimised\nmodel reveals that public remains concerned about the economic impacts of\nsustainability and circular practices, particularly regarding recyclable\nmaterials and environmentally sustainable technologies. The analysis shows that\nthe CE has attracted significant attention on The Guardian, especially in\ntopics related to sustainable development and environmental protection\ntechnologies, while discussions are comparatively less active on Twitter. These\ninsights highlight the need for policymakers to implement targeted education\nprograms, create incentives for businesses to adopt CE principles, and enforce\nmore stringent waste management policies alongside improved recycling\nprocesses.", "published": "2024-05-16 21:38:21", "link": "http://arxiv.org/abs/2405.10452v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Leveraging Human Revisions for Improving Text-to-Layout Models", "abstract": "Learning from human feedback has shown success in aligning large, pretrained\nmodels with human values. Prior works have mostly focused on learning from\nhigh-level labels, such as preferences between pairs of model outputs. On the\nother hand, many domains could benefit from more involved, detailed feedback,\nsuch as revisions, explanations, and reasoning of human users. Our work\nproposes using nuanced feedback through the form of human revisions for\nstronger alignment. In this paper, we ask expert designers to fix layouts\ngenerated from a generative layout model that is pretrained on a large-scale\ndataset of mobile screens. Then, we train a reward model based on how human\ndesigners revise these generated layouts. With the learned reward model, we\noptimize our model with reinforcement learning from human feedback (RLHF). Our\nmethod, Revision-Aware Reward Models ($\\method$), allows a generative\ntext-to-layout model to produce more modern, designer-aligned layouts, showing\nthe potential for utilizing human revisions and stronger forms of feedback in\nimproving generative models.", "published": "2024-05-16 01:33:09", "link": "http://arxiv.org/abs/2405.13026v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DuetSim: Building User Simulator with Dual Large Language Models for\n  Task-Oriented Dialogues", "abstract": "User Simulators play a pivotal role in training and evaluating task-oriented\ndialogue systems. Traditional user simulators typically rely on\nhuman-engineered agendas, resulting in generated responses that often lack\ndiversity and spontaneity. Although large language models (LLMs) exhibit a\nremarkable capacity for generating coherent and contextually appropriate\nutterances, they may fall short when tasked with generating responses that\neffectively guide users towards their goals, particularly in dialogues with\nintricate constraints and requirements. This paper introduces DuetSim, a novel\nframework designed to address the intricate demands of task-oriented dialogues\nby leveraging LLMs. DuetSim stands apart from conventional approaches by\nemploying two LLMs in tandem: one dedicated to response generation and the\nother focused on verification. This dual LLM approach empowers DuetSim to\nproduce responses that not only exhibit diversity but also demonstrate accuracy\nand are preferred by human users. We validate the efficacy of our method\nthrough extensive experiments conducted on the MultiWOZ dataset, highlighting\nimprovements in response quality and correctness, largely attributed to the\nincorporation of the second LLM. Our code is accessible at:\nhttps://github.com/suntea233/DuetSim.", "published": "2024-05-16 06:24:31", "link": "http://arxiv.org/abs/2405.13028v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Crowdsourcing with Enhanced Data Quality Assurance: An Efficient\n  Approach to Mitigate Resource Scarcity Challenges in Training Large Language\n  Models for Healthcare", "abstract": "Large Language Models (LLMs) have demonstrated immense potential in\nartificial intelligence across various domains, including healthcare. However,\ntheir efficacy is hindered by the need for high-quality labeled data, which is\noften expensive and time-consuming to create, particularly in low-resource\ndomains like healthcare. To address these challenges, we propose a\ncrowdsourcing (CS) framework enriched with quality control measures at the\npre-, real-time-, and post-data gathering stages. Our study evaluated the\neffectiveness of enhancing data quality through its impact on LLMs (Bio-BERT)\nfor predicting autism-related symptoms. The results show that real-time quality\ncontrol improves data quality by 19 percent compared to pre-quality control.\nFine-tuning Bio-BERT using crowdsourced data generally increased recall\ncompared to the Bio-BERT baseline but lowered precision. Our findings\nhighlighted the potential of crowdsourcing and quality control in\nresource-constrained environments and offered insights into optimizing\nhealthcare LLMs for informed decision-making and improved patient care.", "published": "2024-05-16 08:29:00", "link": "http://arxiv.org/abs/2405.13030v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Robust Autoencoder Ensemble-Based Approach for Anomaly Detection in\n  Text", "abstract": "Anomaly detection (AD) is a fast growing and popular domain among established\napplications like vision and time series. We observe a rich literature for\nthese applications, but anomaly detection in text is only starting to blossom.\nRecently, self-supervised methods with self-attention mechanism have been the\nmost popular choice. While recent works have proposed a working ground for\nbuilding and benchmarking state of the art approaches, we propose two principal\ncontributions in this paper: contextual anomaly contamination and a novel\nensemble-based approach. Our method, Textual Anomaly Contamination (TAC),\nallows to contaminate inlier classes with either independent or contextual\nanomalies. In the literature, it appears that this distinction is not\nperformed. For finding contextual anomalies, we propose RoSAE, a Robust\nSubspace Local Recovery Autoencoder Ensemble. All autoencoders of the ensemble\npresent a different latent representation through local manifold learning.\nBenchmark shows that our approach outperforms recent works on both independent\nand contextual anomalies, while being more robust. We also provide 8 dataset\ncomparison instead of only relying to Reuters and 20 Newsgroups corpora.", "published": "2024-05-16 10:45:43", "link": "http://arxiv.org/abs/2405.13031v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Can formal argumentative reasoning enhance LLMs performances?", "abstract": "Recent years witnessed significant performance advancements in\ndeep-learning-driven natural language models, with a strong focus on the\ndevelopment and release of Large Language Models (LLMs). These improvements\nresulted in better quality AI-generated output but rely on resource-expensive\ntraining and upgrading of models. Although different studies have proposed a\nrange of techniques to enhance LLMs without retraining, none have considered\ncomputational argumentation as an option. This is a missed opportunity since\ncomputational argumentation is an intuitive mechanism that formally captures\nagents' interactions and the information conflict that may arise during such\ninterplays, and so it seems well-suited for boosting the reasoning and\nconversational abilities of LLMs in a seamless manner. In this paper, we\npresent a pipeline (MQArgEng) and preliminary study to evaluate the effect of\nintroducing computational argumentation semantics on the performance of LLMs.\nOur experiment's goal was to provide a proof-of-concept and a feasibility\nanalysis in order to foster (or deter) future research towards a fully-fledged\nargumentation engine plugin for LLMs. Exploratory results using the MT-Bench\nindicate that MQArgEng provides a moderate performance gain in most of the\nexamined topical categories and, as such, show promise and warrant further\nresearch.", "published": "2024-05-16 22:09:31", "link": "http://arxiv.org/abs/2405.13036v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PyTorch-IE: Fast and Reproducible Prototyping for Information Extraction", "abstract": "The objective of Information Extraction (IE) is to derive structured\nrepresentations from unstructured or semi-structured documents. However,\ndeveloping IE models is complex due to the need of integrating several\nsubtasks. Additionally, representation of data among varied tasks and\ntransforming datasets into task-specific model inputs presents further\nchallenges. To streamline this undertaking for researchers, we introduce\nPyTorch-IE, a deep-learning-based framework uniquely designed to enable swift,\nreproducible, and reusable implementations of IE models. PyTorch-IE offers a\nflexible data model capable of creating complex data structures by integrating\ninterdependent layers of annotations derived from various data types, like\nplain text or semi-structured text, and even images. We propose task modules to\ndecouple the concerns of data representation and model-specific\nrepresentations, thereby fostering greater flexibility and reusability of code.\nPyTorch-IE also extends support for widely used libraries such as\nPyTorch-Lightning for training, HuggingFace datasets for dataset reading, and\nHydra for experiment configuration. Supplementary libraries and GitHub\ntemplates for the easy setup of new projects are also provided. By ensuring\nfunctionality and versatility, PyTorch-IE provides vital support to the\nresearch community engaged in Information Extraction.", "published": "2024-05-16 12:23:37", "link": "http://arxiv.org/abs/2406.00007v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Many-Shot In-Context Learning in Multimodal Foundation Models", "abstract": "Large language models are effective at few-shot in-context learning (ICL).\nRecent advancements in multimodal foundation models have enabled\nunprecedentedly long context windows, presenting an opportunity to explore\ntheir capability to perform ICL with many more demonstrating examples. In this\nwork, we evaluate the performance of multimodal foundation models scaling from\nfew-shot to many-shot ICL. We benchmark GPT-4o and Gemini 1.5 Pro across 14\ndatasets spanning multiple domains (natural imagery, medical imagery, remote\nsensing, and molecular imagery) and tasks (image classification, visual QA, and\nobject localization). We observe that many-shot ICL, including up to almost\n2,000 demonstrating examples, leads to substantial improvements compared to\nfew-shot (<100 examples) ICL across all of the datasets. Further, Gemini 1.5\nPro performance continues to improve log-linearly up to the maximum number of\ntested examples on many datasets. We also find open-weights multimodal\nfoundation models like Llama 3.2-Vision do not benefit from the demonstrating\nexamples, highlighting an important gap between open and closed multimodal\nfoundation models. Given the high inference costs required for many-shot ICL,\nwe also explore the impact of batching multiple queries in a single API call.\nWe show that batching up to 50 queries can lead to performance improvements\nunder zero-shot and many-shot ICL, with substantial gains in the zero-shot\nsetting on multiple datasets, while drastically reducing per-query cost and\nlatency. Finally, while GPT-4o and Gemini 1.5 Pro achieve similar zero-shot\nperformance across the datasets, Gemini 1.5 Pro learns more quickly than GPT-4o\non most datasets. Our results suggest that many-shot ICL could enable users to\nefficiently adapt multimodal foundation models to new applications and domains.\nOur codebase is publicly available at\nhttps://github.com/stanfordmlgroup/ManyICL .", "published": "2024-05-16 04:02:43", "link": "http://arxiv.org/abs/2405.09798v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "MediSyn: A Generalist Text-Guided Latent Diffusion Model For Diverse\n  Medical Image Synthesis", "abstract": "Deep learning algorithms require extensive data to achieve robust\nperformance. However, data availability is often restricted in the medical\ndomain due to patient privacy concerns. Synthetic data presents a possible\nsolution to these challenges. Recently, image generative models have found\nincreasing use for medical applications but are often designed for singular\nmedical specialties and imaging modalities, thus limiting their broader\nutility. To address this, we introduce MediSyn: a text-guided, latent diffusion\nmodel capable of generating synthetic images from 6 medical specialties and 10\nimage types. The synthetic images are validated by expert clinicians for\nalignment with their corresponding text prompts. Furthermore, a direct\ncomparison of the synthetic images against the real images confirms that our\nmodel synthesizes novel images and, crucially, may preserve patient privacy.\nFinally, classifiers trained on a mixture of synthetic and real data achieve\nsimilar performance to those trained on twice the amount of real data. Our\nfindings highlight the immense potential for generalist image generative models\nto accelerate algorithmic research and development in medicine.", "published": "2024-05-16 04:28:44", "link": "http://arxiv.org/abs/2405.09806v4", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Natural Language Can Help Bridge the Sim2Real Gap", "abstract": "The main challenge in learning image-conditioned robotic policies is\nacquiring a visual representation conducive to low-level control. Due to the\nhigh dimensionality of the image space, learning a good visual representation\nrequires a considerable amount of visual data. However, when learning in the\nreal world, data is expensive. Sim2Real is a promising paradigm for overcoming\ndata scarcity in the real-world target domain by using a simulator to collect\nlarge amounts of cheap data closely related to the target task. However, it is\ndifficult to transfer an image-conditioned policy from sim to real when the\ndomains are very visually dissimilar. To bridge the sim2real visual gap, we\npropose using natural language descriptions of images as a unifying signal\nacross domains that captures the underlying task-relevant semantics. Our key\ninsight is that if two image observations from different domains are labeled\nwith similar language, the policy should predict similar action distributions\nfor both images. We demonstrate that training the image encoder to predict the\nlanguage description or the distance between descriptions of a sim or real\nimage serves as a useful, data-efficient pretraining step that helps learn a\ndomain-invariant image representation. We can then use this image encoder as\nthe backbone of an IL policy trained simultaneously on a large amount of\nsimulated and a handful of real demonstrations. Our approach outperforms widely\nused prior sim2real methods and strong vision-language pretraining baselines\nlike CLIP and R3M by 25 to 40%. See additional videos and materials at\nhttps://robin-lab.cs.utexas.edu/lang4sim2real/.", "published": "2024-05-16 12:02:02", "link": "http://arxiv.org/abs/2405.10020v2", "categories": ["cs.RO", "cs.CL", "cs.CV", "cs.LG", "I.2.9; I.2.7; I.2.6"], "primary_category": "cs.RO"}
{"title": "SynthesizRR: Generating Diverse Datasets with Retrieval Augmentation", "abstract": "It is often desirable to distill the capabilities of large language models\n(LLMs) into smaller student models due to compute and memory constraints. One\nway to do this for classification tasks is via dataset synthesis, which can be\naccomplished by generating examples of each label from the LLM. Prior\napproaches to synthesis use few-shot prompting, which relies on the LLM's\nparametric knowledge to generate usable examples. However, this leads to issues\nof repetition, bias towards popular entities, and stylistic differences from\nhuman text. In this work, we propose Synthesize by Retrieval and Refinement\n(SynthesizRR), which uses retrieval augmentation to introduce variety into the\ndataset synthesis process: as retrieved passages vary, the LLM is seeded with\ndifferent content to generate its examples. We empirically study the synthesis\nof six datasets, covering topic classification, sentiment analysis, tone\ndetection, and humor, requiring complex synthesis strategies. We find that\nSynthesizRR greatly improves lexical and semantic diversity, similarity to\nhuman-written text, and distillation performance, when compared to 32-shot\nprompting and four prior approaches. We release our code to perform all steps\nat https://github.com/amazon-science/synthesizrr", "published": "2024-05-16 12:22:41", "link": "http://arxiv.org/abs/2405.10040v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Building a Luganda Text-to-Speech Model From Crowdsourced Data", "abstract": "Text-to-speech (TTS) development for African languages such as Luganda is\nstill limited, primarily due to the scarcity of high-quality, single-speaker\nrecordings essential for training TTS models. Prior work has focused on\nutilizing the Luganda Common Voice recordings of multiple speakers aged between\n20-49. Although the generated speech is intelligible, it is still of lower\nquality than the model trained on studio-grade recordings. This is due to the\ninsufficient data preprocessing methods applied to improve the quality of the\nCommon Voice recordings. Furthermore, speech convergence is more difficult to\nachieve due to varying intonations, as well as background noise. In this paper,\nwe show that the quality of Luganda TTS from Common Voice can improve by\ntraining on multiple speakers of close intonation in addition to further\npreprocessing of the training data. Specifically, we selected six female\nspeakers with close intonation determined by subjectively listening and\ncomparing their voice recordings. In addition to trimming out silent portions\nfrom the beginning and end of the recordings, we applied a pre-trained speech\nenhancement model to reduce background noise and enhance audio quality. We also\nutilized a pre-trained, non-intrusive, self-supervised Mean Opinion Score (MOS)\nestimation model to filter recordings with an estimated MOS over 3.5,\nindicating high perceived quality. Subjective MOS evaluations from nine native\nLuganda speakers demonstrate that our TTS model achieves a significantly better\nMOS of 3.55 compared to the reported 2.5 MOS of the existing model. Moreover,\nfor a fair comparison, our model trained on six speakers outperforms models\ntrained on a single-speaker (3.13 MOS) or two speakers (3.22 MOS). This\nshowcases the effectiveness of compensating for the lack of data from one\nspeaker with data from multiple speakers of close intonation to improve TTS\nquality.", "published": "2024-05-16 16:00:47", "link": "http://arxiv.org/abs/2405.10211v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Words as Trigger Points in Social Media Discussions", "abstract": "Trigger points, introduced by Mau et al . [30], are rooted in theories of\naffective political identity and relate to deeply lying beliefs about moral\nexpectations and social dispositions. Examining trigger points in online\ndiscussions helps understand why and when social media users engage in\ndisagreements or affective political deliberations. This opens the door to\nmodelling social media user engagement more effectively and studying the\nconditions and causal mechanisms that lead to adverse reactions, hate speech,\nand abusive language in online debates.", "published": "2024-05-16 16:02:42", "link": "http://arxiv.org/abs/2405.10213v2", "categories": ["cs.SI", "cs.CL", "cs.CY"], "primary_category": "cs.SI"}
{"title": "Fine-Tuning Large Vision-Language Models as Decision-Making Agents via\n  Reinforcement Learning", "abstract": "Large vision-language models (VLMs) fine-tuned on specialized visual\ninstruction-following data have exhibited impressive language reasoning\ncapabilities across various scenarios. However, this fine-tuning paradigm may\nnot be able to efficiently learn optimal decision-making agents in multi-step\ngoal-directed tasks from interactive environments. To address this challenge,\nwe propose an algorithmic framework that fine-tunes VLMs with reinforcement\nlearning (RL). Specifically, our framework provides a task description and then\nprompts the VLM to generate chain-of-thought (CoT) reasoning, enabling the VLM\nto efficiently explore intermediate reasoning steps that lead to the final\ntext-based action. Next, the open-ended text output is parsed into an\nexecutable action to interact with the environment to obtain goal-directed task\nrewards. Finally, our framework uses these task rewards to fine-tune the entire\nVLM with RL. Empirically, we demonstrate that our proposed framework enhances\nthe decision-making capabilities of VLM agents across various tasks, enabling\n7b models to outperform commercial models such as GPT4-V or Gemini.\nFurthermore, we find that CoT reasoning is a crucial component for performance\nimprovement, as removing the CoT reasoning results in a significant decrease in\nthe overall performance of our method.", "published": "2024-05-16 17:50:19", "link": "http://arxiv.org/abs/2405.10292v3", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.AI"}
{"title": "How Far Are We From AGI: Are LLMs All We Need?", "abstract": "The evolution of artificial intelligence (AI) has profoundly impacted human\nsociety, driving significant advancements in multiple sectors. AGI,\ndistinguished by its ability to execute diverse real-world tasks with\nefficiency and effectiveness comparable to human intelligence, reflects a\nparamount milestone in AI evolution. While existing studies have reviewed\nspecific advancements in AI and proposed potential paths to AGI, such as large\nlanguage models (LLMs), they fall short of providing a thorough exploration of\nAGI's definitions, objectives, and developmental trajectories. Unlike previous\nsurvey papers, this work goes beyond summarizing LLMs by addressing key\nquestions about our progress toward AGI and outlining the strategies essential\nfor its realization through comprehensive analysis, in-depth discussions, and\nnovel insights. We start by articulating the requisite capability frameworks\nfor AGI, integrating the internal, interface, and system dimensions. As the\nrealization of AGI requires more advanced capabilities and adherence to\nstringent constraints, we further discuss necessary AGI alignment technologies\nto harmonize these factors. Notably, we emphasize the importance of approaching\nAGI responsibly by first defining the key levels of AGI progression, followed\nby the evaluation framework that situates the status quo, and finally giving\nour roadmap of how to reach the pinnacle of AGI. Moreover, to give tangible\ninsights into the ubiquitous impact of the integration of AI, we outline\nexisting challenges and potential pathways toward AGI in multiple domains. In\nsum, serving as a pioneering exploration into the current state and future\ntrajectory of AGI, this paper aims to foster a collective comprehension and\ncatalyze broader public discussions among researchers and practitioners on AGI.", "published": "2024-05-16 17:59:02", "link": "http://arxiv.org/abs/2405.10313v2", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.AI"}
{"title": "AmazUtah_NLP at SemEval-2024 Task 9: A MultiChoice Question Answering\n  System for Commonsense Defying Reasoning", "abstract": "The SemEval 2024 BRAINTEASER task represents a pioneering venture in Natural\nLanguage Processing (NLP) by focusing on lateral thinking, a dimension of\ncognitive reasoning that is often overlooked in traditional linguistic\nanalyses. This challenge comprises of Sentence Puzzle and Word Puzzle subtasks\nand aims to test language models' capacity for divergent thinking.\n  In this paper, we present our approach to the BRAINTEASER task. We employ a\nholistic strategy by leveraging cutting-edge pre-trained models in multiple\nchoice architecture, and diversify the training data with Sentence and Word\nPuzzle datasets. To gain further improvement, we fine-tuned the model with\nsynthetic humor or jokes dataset and the RiddleSense dataset which helped\naugmenting the model's lateral thinking abilities. Empirical results show that\nour approach achieve 92.5% accuracy in Sentence Puzzle subtask and 80.2%\naccuracy in Word Puzzle subtask.", "published": "2024-05-16 18:26:38", "link": "http://arxiv.org/abs/2405.10385v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Memory-efficient Energy-adaptive Inference of Pre-Trained Models on\n  Batteryless Embedded Systems", "abstract": "Batteryless systems frequently face power failures, requiring extra runtime\nbuffers to maintain inference progress and leaving only a memory space for\nstoring ultra-tiny deep neural networks (DNNs). Besides, making these models\nresponsive to stochastic energy harvesting dynamics during inference requires a\nbalance between inference accuracy, latency, and energy overhead. Recent works\non compression mostly focus on time and memory, but often ignore energy\ndynamics or significantly reduce the accuracy of pre-trained DNNs. Existing\nenergy-adaptive inference works modify the architecture of pre-trained models\nand have significant memory overhead. Thus, energy-adaptive and accurate\ninference of pre-trained DNNs on batteryless devices with extreme memory\nconstraints is more challenging than traditional microcontrollers. We combat\nthese issues by proposing FreeML, a framework to optimize pre-trained DNN\nmodels for memory-efficient and energy-adaptive inference on batteryless\nsystems. FreeML comprises (1) a novel compression technique to reduce the model\nfootprint and runtime memory requirements simultaneously, making them\nexecutable on extremely memory-constrained batteryless platforms; and (2) the\nfirst early exit mechanism that uses a single exit branch for all exit points\nto terminate inference at any time, making models energy-adaptive with minimal\nmemory overhead. Our experiments showed that FreeML reduces the model sizes by\nup to $95 \\times$, supports adaptive inference with a $2.03-19.65 \\times$ less\nmemory overhead, and provides significant time and energy benefits with only a\nnegligible accuracy drop compared to the state-of-the-art.", "published": "2024-05-16 20:16:45", "link": "http://arxiv.org/abs/2405.10426v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Learnable Privacy Neurons Localization in Language Models", "abstract": "Concerns regarding Large Language Models (LLMs) to memorize and disclose\nprivate information, particularly Personally Identifiable Information (PII),\nbecome prominent within the community. Many efforts have been made to mitigate\nthe privacy risks. However, the mechanism through which LLMs memorize PII\nremains poorly understood. To bridge this gap, we introduce a pioneering method\nfor pinpointing PII-sensitive neurons (privacy neurons) within LLMs. Our method\nemploys learnable binary weight masks to localize specific neurons that account\nfor the memorization of PII in LLMs through adversarial training. Our\ninvestigations discover that PII is memorized by a small subset of neurons\nacross all layers, which shows the property of PII specificity. Furthermore, we\npropose to validate the potential in PII risk mitigation by deactivating the\nlocalized privacy neurons. Both quantitative and qualitative experiments\ndemonstrate the effectiveness of our neuron localization algorithm.", "published": "2024-05-16 08:11:08", "link": "http://arxiv.org/abs/2405.10989v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Large Language Models for Tuning Evolution Strategies", "abstract": "Large Language Models (LLMs) exhibit world knowledge and inference\ncapabilities, making them powerful tools for various applications. This paper\nproposes a feedback loop mechanism that leverages these capabilities to tune\nEvolution Strategies (ES) parameters effectively. The mechanism involves a\nstructured process of providing programming instructions, executing the\ncorresponding code, and conducting thorough analysis. This process is\nspecifically designed for the optimization of ES parameters. The method\noperates through an iterative cycle, ensuring continuous refinement of the ES\nparameters. First, LLMs process the instructions to generate or modify the\ncode. The code is then executed, and the results are meticulously logged.\nSubsequent analysis of these results provides insights that drive further\nimprovements. An experiment on tuning the learning rates of ES using the LLaMA3\nmodel demonstrate the feasibility of this approach. This research illustrates\nhow LLMs can be harnessed to improve ES algorithms' performance and suggests\nbroader applications for similar feedback loop mechanisms in various domains.", "published": "2024-05-16 21:14:32", "link": "http://arxiv.org/abs/2405.10999v1", "categories": ["cs.LG", "cs.CL", "cs.NE"], "primary_category": "cs.LG"}
{"title": "Faithful Attention Explainer: Verbalizing Decisions Based on\n  Discriminative Features", "abstract": "In recent years, model explanation methods have been designed to interpret\nmodel decisions faithfully and intuitively so that users can easily understand\nthem. In this paper, we propose a framework, Faithful Attention Explainer\n(FAE), capable of generating faithful textual explanations regarding the\nattended-to features. Towards this goal, we deploy an attention module that\ntakes the visual feature maps from the classifier for sentence generation.\nFurthermore, our method successfully learns the association between features\nand words, which allows a novel attention enforcement module for attention\nexplanation. Our model achieves promising performance in caption quality\nmetrics and a faithful decision-relevance metric on two datasets (CUB and\nACT-X). In addition, we show that FAE can interpret gaze-based human attention,\nas human gaze indicates the discriminative features that humans use for\ndecision-making, demonstrating the potential of deploying human gaze for\nadvanced human-AI interaction.", "published": "2024-05-16 12:13:24", "link": "http://arxiv.org/abs/2405.13032v2", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Autonomous Workflow for Multimodal Fine-Grained Training Assistants\n  Towards Mixed Reality", "abstract": "Autonomous artificial intelligence (AI) agents have emerged as promising\nprotocols for automatically understanding the language-based environment,\nparticularly with the exponential development of large language models (LLMs).\nHowever, a fine-grained, comprehensive understanding of multimodal environments\nremains under-explored. This work designs an autonomous workflow tailored for\nintegrating AI agents seamlessly into extended reality (XR) applications for\nfine-grained training. We present a demonstration of a multimodal fine-grained\ntraining assistant for LEGO brick assembly in a pilot XR environment.\nSpecifically, we design a cerebral language agent that integrates LLM with\nmemory, planning, and interaction with XR tools and a vision-language agent,\nenabling agents to decide their actions based on past experiences. Furthermore,\nwe introduce LEGO-MRTA, a multimodal fine-grained assembly dialogue dataset\nsynthesized automatically in the workflow served by a commercial LLM. This\ndataset comprises multimodal instruction manuals, conversations, XR responses,\nand vision question answering. Last, we present several prevailing\nopen-resource LLMs as benchmarks, assessing their performance with and without\nfine-tuning on the proposed dataset. We anticipate that the broader impact of\nthis workflow will advance the development of smarter assistants for seamless\nuser interaction in XR environments, fostering research in both AI and HCI\ncommunities.", "published": "2024-05-16 14:20:30", "link": "http://arxiv.org/abs/2405.13034v2", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Luganda Speech Intent Recognition for IoT Applications", "abstract": "The advent of Internet of Things (IoT) technology has generated massive\ninterest in voice-controlled smart homes. While many voice-controlled smart\nhome systems are designed to understand and support widely spoken languages\nlike English, speakers of low-resource languages like Luganda may need more\nsupport. This research project aimed to develop a Luganda speech intent\nclassification system for IoT applications to integrate local languages into\nsmart home environments. The project uses hardware components such as Raspberry\nPi, Wio Terminal, and ESP32 nodes as microcontrollers. The Raspberry Pi\nprocesses Luganda voice commands, the Wio Terminal is a display device, and the\nESP32 nodes control the IoT devices. The ultimate objective of this work was to\nenable voice control using Luganda, which was accomplished through a natural\nlanguage processing (NLP) model deployed on the Raspberry Pi. The NLP model\nutilized Mel Frequency Cepstral Coefficients (MFCCs) as acoustic features and a\nConvolutional Neural Network (Conv2D) architecture for speech intent\nclassification. A dataset of Luganda voice commands was curated for this\npurpose and this has been made open-source. This work addresses the\nlocalization challenges and linguistic diversity in IoT applications by\nincorporating Luganda voice commands, enabling users to interact with smart\nhome devices without English proficiency, especially in regions where local\nlanguages are predominant.", "published": "2024-05-16 10:14:00", "link": "http://arxiv.org/abs/2405.19343v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "KnowledgeHub: An end-to-end Tool for Assisted Scientific Discovery", "abstract": "This paper describes the KnowledgeHub tool, a scientific literature\nInformation Extraction (IE) and Question Answering (QA) pipeline. This is\nachieved by supporting the ingestion of PDF documents that are converted to\ntext and structured representations. An ontology can then be constructed where\na user defines the types of entities and relationships they want to capture. A\nbrowser-based annotation tool enables annotating the contents of the PDF\ndocuments according to the ontology. Named Entity Recognition (NER) and\nRelation Classification (RC) models can be trained on the resulting annotations\nand can be used to annotate the unannotated portion of the documents. A\nknowledge graph is constructed from these entity and relation triples which can\nbe queried to obtain insights from the data. Furthermore, we integrate a suite\nof Large Language Models (LLMs) that can be used for QA and summarisation that\nis grounded in the included documents via a retrieval component. KnowledgeHub\nis a unique tool that supports annotation, IE and QA, which gives the user full\ninsight into the knowledge discovery pipeline.", "published": "2024-05-16 13:17:14", "link": "http://arxiv.org/abs/2406.00008v2", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.DL"], "primary_category": "cs.IR"}
{"title": "Listen Again and Choose the Right Answer: A New Paradigm for Automatic\n  Speech Recognition with Large Language Models", "abstract": "Recent advances in large language models (LLMs) have promoted generative\nerror correction (GER) for automatic speech recognition (ASR), which aims to\npredict the ground-truth transcription from the decoded N-best hypotheses.\nThanks to the strong language generation ability of LLMs and rich information\nin the N-best list, GER shows great effectiveness in enhancing ASR results.\nHowever, it still suffers from two limitations: 1) LLMs are unaware of the\nsource speech during GER, which may lead to results that are grammatically\ncorrect but violate the source speech content, 2) N-best hypotheses usually\nonly vary in a few tokens, making it redundant to send all of them for GER,\nwhich could confuse LLM about which tokens to focus on and thus lead to\nincreased miscorrection. In this paper, we propose ClozeGER, a new paradigm for\nASR generative error correction. First, we introduce a multimodal LLM (i.e.,\nSpeechGPT) to receive source speech as extra input to improve the fidelity of\ncorrection output. Then, we reformat GER as a cloze test with logits\ncalibration to remove the input information redundancy and simplify GER with\nclear instructions. Experiments show that ClozeGER achieves a new breakthrough\nover vanilla GER on 9 popular ASR datasets.", "published": "2024-05-16 12:05:45", "link": "http://arxiv.org/abs/2405.10025v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Evaluating Text-to-Speech Synthesis from a Large Discrete Token-based\n  Speech Language Model", "abstract": "Recent advances in generative language modeling applied to discrete speech\ntokens presented a new avenue for text-to-speech (TTS) synthesis. These speech\nlanguage models (SLMs), similarly to their textual counterparts, are scalable,\nprobabilistic, and context-aware. While they can produce diverse and natural\noutputs, they sometimes face issues such as unintelligibility and the inclusion\nof non-speech noises or hallucination. As the adoption of this innovative\nparadigm in speech synthesis increases, there is a clear need for an in-depth\nevaluation of its capabilities and limitations. In this paper, we evaluate TTS\nfrom a discrete token-based SLM, through both automatic metrics and listening\ntests. We examine five key dimensions: speaking style, intelligibility, speaker\nconsistency, prosodic variation, spontaneous behaviour. Our results highlight\nthe model's strength in generating varied prosody and spontaneous outputs. It\nis also rated higher in naturalness and context appropriateness in listening\ntests compared to a conventional TTS. However, the model's performance in\nintelligibility and speaker consistency lags behind traditional TTS.\nAdditionally, we show that increasing the scale of SLMs offers a modest boost\nin robustness. Our findings aim to serve as a benchmark for future advancements\nin generative SLMs for speech synthesis.", "published": "2024-05-16 02:18:41", "link": "http://arxiv.org/abs/2405.09768v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Robust Singing Voice Transcription Serves Synthesis", "abstract": "Note-level Automatic Singing Voice Transcription (AST) converts singing\nrecordings into note sequences, facilitating the automatic annotation of\nsinging datasets for Singing Voice Synthesis (SVS) applications. Current AST\nmethods, however, struggle with accuracy and robustness when used for practical\nannotation. This paper presents ROSVOT, the first robust AST model that serves\nSVS, incorporating a multi-scale framework that effectively captures\ncoarse-grained note information and ensures fine-grained frame-level\nsegmentation, coupled with an attention-based pitch decoder for reliable pitch\nprediction. We also established a comprehensive annotation-and-training\npipeline for SVS to test the model in real-world settings. Experimental\nfindings reveal that ROSVOT achieves state-of-the-art transcription accuracy\nwith either clean or noisy inputs. Moreover, when trained on enlarged,\nautomatically annotated datasets, the SVS model outperforms its baseline,\naffirming the capability for practical application. Audio samples are available\nat https://rosvot.github.io.", "published": "2024-05-16 09:43:40", "link": "http://arxiv.org/abs/2405.09940v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Data-Efficient Low-Complexity Acoustic Scene Classification in the DCASE\n  2024 Challenge", "abstract": "This article describes the Data-Efficient Low-Complexity Acoustic Scene\nClassification Task in the DCASE 2024 Challenge and the corresponding baseline\nsystem. The task setup is a continuation of previous editions (2022 and 2023),\nwhich focused on recording device mismatches and low-complexity constraints.\nThis year's edition introduces an additional real-world problem: participants\nmust develop data-efficient systems for five scenarios, which progressively\nlimit the available training data. The provided baseline system is based on an\nefficient, factorized CNN architecture constructed from inverted residual\nblocks and uses Freq-MixStyle to tackle the device mismatch problem. The task\nreceived 37 submissions from 17 teams, with the large majority of systems\noutperforming the baseline. The top-ranked system's accuracy ranges from 54.3%\non the smallest to 61.8% on the largest subset, corresponding to relative\nimprovements of approximately 23% and 9% over the baseline system on the\nevaluation set.", "published": "2024-05-16 12:00:39", "link": "http://arxiv.org/abs/2405.10018v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Monaural speech enhancement on drone via Adapter based transfer learning", "abstract": "Monaural Speech enhancement on drones is challenging because the ego-noise\nfrom the rotating motors and propellers leads to extremely low signal-to-noise\nratios at onboard microphones. Although recent masking-based deep neural\nnetwork methods excel in monaural speech enhancement, they struggle in the\nchallenging drone noise scenario. Furthermore, existing drone noise datasets\nare limited, causing models to overfit. Considering the harmonic nature of\ndrone noise, this paper proposes a frequency domain bottleneck adapter to\nenable transfer learning. Specifically, the adapter's parameters are trained on\ndrone noise while retaining the parameters of the pre-trained Frequency\nRecurrent Convolutional Recurrent Network (FRCRN) fixed. Evaluation results\ndemonstrate the proposed method can effectively enhance speech quality.\nMoreover, it is a more efficient alternative to fine-tuning models for various\ndrone types, which typically requires substantial computational resources.", "published": "2024-05-16 12:03:58", "link": "http://arxiv.org/abs/2405.10022v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis", "abstract": "In this work, we present Semantic Gesticulator, a novel framework designed to\nsynthesize realistic gestures accompanying speech with strong semantic\ncorrespondence. Semantically meaningful gestures are crucial for effective\nnon-verbal communication, but such gestures often fall within the long tail of\nthe distribution of natural human motion. The sparsity of these movements makes\nit challenging for deep learning-based systems, trained on moderately sized\ndatasets, to capture the relationship between the movements and the\ncorresponding speech semantics. To address this challenge, we develop a\ngenerative retrieval framework based on a large language model. This framework\nefficiently retrieves suitable semantic gesture candidates from a motion\nlibrary in response to the input speech. To construct this motion library, we\nsummarize a comprehensive list of commonly used semantic gestures based on\nfindings in linguistics, and we collect a high-quality motion dataset\nencompassing both body and hand movements. We also design a novel GPT-based\nmodel with strong generalization capabilities to audio, capable of generating\nhigh-quality gestures that match the rhythm of speech. Furthermore, we propose\na semantic alignment mechanism to efficiently align the retrieved semantic\ngestures with the GPT's output, ensuring the naturalness of the final\nanimation. Our system demonstrates robustness in generating gestures that are\nrhythmically coherent and semantically explicit, as evidenced by a\ncomprehensive collection of examples. User studies confirm the quality and\nhuman-likeness of our results, and show that our system outperforms\nstate-of-the-art systems in terms of semantic appropriateness by a clear\nmargin.", "published": "2024-05-16 05:09:01", "link": "http://arxiv.org/abs/2405.09814v2", "categories": ["cs.GR", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.GR"}
{"title": "Whole-Song Hierarchical Generation of Symbolic Music Using Cascaded\n  Diffusion Models", "abstract": "Recent deep music generation studies have put much emphasis on long-term\ngeneration with structures. However, we are yet to see high-quality,\nwell-structured whole-song generation. In this paper, we make the first attempt\nto model a full music piece under the realization of compositional hierarchy.\nWith a focus on symbolic representations of pop songs, we define a hierarchical\nlanguage, in which each level of hierarchy focuses on the semantics and context\ndependency at a certain music scope. The high-level languages reveal whole-song\nform, phrase, and cadence, whereas the low-level languages focus on notes,\nchords, and their local patterns. A cascaded diffusion model is trained to\nmodel the hierarchical language, where each level is conditioned on its upper\nlevels. Experiments and analysis show that our model is capable of generating\nfull-piece music with recognizable global verse-chorus structure and cadences,\nand the music quality is higher than the baselines. Additionally, we show that\nthe proposed model is controllable in a flexible way. By sampling from the\ninterpretable hierarchical languages or adjusting pre-trained external\nrepresentations, users can control the music flow via various features such as\nphrase harmonic structures, rhythmic patterns, and accompaniment texture.", "published": "2024-05-16 08:48:23", "link": "http://arxiv.org/abs/2405.09901v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS", "68Txx"], "primary_category": "cs.SD"}
{"title": "Revisiting Deep Audio-Text Retrieval Through the Lens of Transportation", "abstract": "The Learning-to-match (LTM) framework proves to be an effective inverse\noptimal transport approach for learning the underlying ground metric between\ntwo sources of data, facilitating subsequent matching. However, the\nconventional LTM framework faces scalability challenges, necessitating the use\nof the entire dataset each time the parameters of the ground metric are\nupdated. In adapting LTM to the deep learning context, we introduce the\nmini-batch Learning-to-match (m-LTM) framework for audio-text retrieval\nproblems. This framework leverages mini-batch subsampling and\nMahalanobis-enhanced family of ground metrics. Moreover, to cope with\nmisaligned training data in practice, we propose a variant using partial\noptimal transport to mitigate the harm of misaligned data pairs in training\ndata. We conduct extensive experiments on audio-text matching problems using\nthree datasets: AudioCaps, Clotho, and ESC-50. Results demonstrate that our\nproposed method is capable of learning rich and expressive joint embedding\nspace, which achieves SOTA performance. Beyond this, the proposed m-LTM\nframework is able to close the modality gap across audio and text embedding,\nwhich surpasses both triplet and contrastive loss in the zero-shot sound event\ndetection task on the ESC-50 dataset. Notably, our strategy of employing\npartial optimal transport with m-LTM demonstrates greater noise tolerance than\ncontrastive loss, especially under varying noise ratios in training data on the\nAudioCaps dataset. Our code is available at\nhttps://github.com/v-manhlt3/m-LTM-Audio-Text-Retrieval", "published": "2024-05-16 13:28:10", "link": "http://arxiv.org/abs/2405.10084v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A novel Reservoir Architecture for Periodic Time Series Prediction", "abstract": "This paper introduces a novel approach to predicting periodic time series\nusing reservoir computing. The model is tailored to deliver precise forecasts\nof rhythms, a crucial aspect for tasks such as generating musical rhythm.\nLeveraging reservoir computing, our proposed method is ultimately oriented\ntowards predicting human perception of rhythm. Our network accurately predicts\nrhythmic signals within the human frequency perception range. The model\narchitecture incorporates primary and intermediate neurons tasked with\ncapturing and transmitting rhythmic information. Two parameter matrices,\ndenoted as c and k, regulate the reservoir's overall dynamics. We propose a\nloss function to adapt c post-training and introduce a dynamic selection (DS)\nmechanism that adjusts $k$ to focus on areas with outstanding contributions.\nExperimental results on a diverse test set showcase accurate predictions,\nfurther improved through real-time tuning of the reservoir via c and k.\nComparative assessments highlight its superior performance compared to\nconventional models.", "published": "2024-05-16 13:55:53", "link": "http://arxiv.org/abs/2405.10102v1", "categories": ["cs.NE", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.NE"}
{"title": "Selfsupervised learning for pathological speech detection", "abstract": "Speech production is a complex phenomenon, wherein the brain orchestrates a\nsequence of processes involving thought processing, motor planning, and the\nexecution of articulatory movements. However, this intricate execution of\nvarious processes is susceptible to influence and disruption by various\nneurodegenerative pathological speech disorders, such as Parkinsons' disease,\nresulting in dysarthria, apraxia, and other conditions. These disorders lead to\npathological speech characterized by abnormal speech patterns and imprecise\narticulation. Diagnosing these speech disorders in clinical settings typically\ninvolves auditory perceptual tests, which are time-consuming, and the diagnosis\ncan vary among clinicians based on their experiences, biases, and cognitive\nload during the diagnosis. Additionally, unlike neurotypical speakers, patients\nwith speech pathologies or impairments are unable to access various virtual\nassistants such as Alexa, Siri, etc. To address these challenges, several\nautomatic pathological speech detection (PSD) approaches have been proposed.\nThese approaches aim to provide efficient and accurate detection of speech\ndisorders, thereby facilitating timely intervention and support for individuals\naffected by these conditions. These approaches mainly vary in two aspects: the\ninput representations utilized and the classifiers employed. Due to the limited\navailability of data, the performance of detection remains subpar.\nSelf-supervised learning (SSL) embeddings, such as wav2vec2, and their\nmultilingual versions, are being explored as a promising avenue to improve\nperformance. These embeddings leverage self-supervised learning techniques to\nextract rich representations from audio data, thereby offering a potential\nsolution to address the limitations posed by the scarcity of labeled data.", "published": "2024-05-16 07:12:47", "link": "http://arxiv.org/abs/2406.02572v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Faces that Speak: Jointly Synthesising Talking Face and Speech from Text", "abstract": "The goal of this work is to simultaneously generate natural talking faces and\nspeech outputs from text. We achieve this by integrating Talking Face\nGeneration (TFG) and Text-to-Speech (TTS) systems into a unified framework. We\naddress the main challenges of each task: (1) generating a range of head poses\nrepresentative of real-world scenarios, and (2) ensuring voice consistency\ndespite variations in facial motion for the same identity. To tackle these\nissues, we introduce a motion sampler based on conditional flow matching, which\nis capable of high-quality motion code generation in an efficient way.\nMoreover, we introduce a novel conditioning method for the TTS system, which\nutilises motion-removed features from the TFG model to yield uniform speech\noutputs. Our extensive experiments demonstrate that our method effectively\ncreates natural-looking talking faces and speech that accurately match the\ninput text. To our knowledge, this is the first effort to build a multimodal\nsynthesis system that can generalise to unseen identities.", "published": "2024-05-16 17:29:37", "link": "http://arxiv.org/abs/2405.10272v1", "categories": ["cs.CV", "cs.AI", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.CV"}
