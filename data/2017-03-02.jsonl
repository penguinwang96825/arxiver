{"title": "Structural Embedding of Syntactic Trees for Machine Comprehension", "abstract": "Deep neural networks for machine comprehension typically utilizes only word\nor character embeddings without explicitly taking advantage of structured\nlinguistic information such as constituency trees and dependency trees. In this\npaper, we propose structural embedding of syntactic trees (SEST), an algorithm\nframework to utilize structured information and encode them into vector\nrepresentations that can boost the performance of algorithms for the machine\ncomprehension. We evaluate our approach using a state-of-the-art neural\nattention model on the SQuAD dataset. Experimental results demonstrate that our\nmodel can accurately identify the syntactic boundaries of the sentences and\nextract answers that are syntactically coherent over the baseline methods.", "published": "2017-03-02 01:08:10", "link": "http://arxiv.org/abs/1703.00572v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lock-Free Parallel Perceptron for Graph-based Dependency Parsing", "abstract": "Dependency parsing is an important NLP task. A popular approach for\ndependency parsing is structured perceptron. Still, graph-based dependency\nparsing has the time complexity of $O(n^3)$, and it suffers from slow training.\nTo deal with this problem, we propose a parallel algorithm called parallel\nperceptron. The parallel algorithm can make full use of a multi-core computer\nwhich saves a lot of training time. Based on experiments we observe that\ndependency parsing with parallel perceptron can achieve 8-fold faster training\nspeed than traditional structured perceptron methods when using 10 threads, and\nwith no loss at all in accuracy.", "published": "2017-03-02 13:49:23", "link": "http://arxiv.org/abs/1703.00782v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Comparative Study of Word Embeddings for Reading Comprehension", "abstract": "The focus of past machine learning research for Reading Comprehension tasks\nhas been primarily on the design of novel deep learning architectures. Here we\nshow that seemingly minor choices made on (1) the use of pre-trained word\nembeddings, and (2) the representation of out-of-vocabulary tokens at test\ntime, can turn out to have a larger impact than architectural choices on the\nfinal performance. We systematically explore several options for these choices,\nand provide recommendations to researchers working in this area.", "published": "2017-03-02 23:58:54", "link": "http://arxiv.org/abs/1703.00993v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Scattertext: a Browser-Based Tool for Visualizing how Corpora Differ", "abstract": "Scattertext is an open source tool for visualizing linguistic variation\nbetween document categories in a language-independent way. The tool presents a\nscatterplot, where each axis corresponds to the rank-frequency a term occurs in\na category of documents. Through a tie-breaking strategy, the tool is able to\ndisplay thousands of visible term-representing points and find space to legibly\nlabel hundreds of them. Scattertext also lends itself to a query-based\nvisualization of how the use of terms with similar embeddings differs between\ndocument categories, as well as a visualization for comparing the importance\nscores of bag-of-words features to univariate metrics.", "published": "2017-03-02 00:48:15", "link": "http://arxiv.org/abs/1703.00565v3", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Dynamic Word Embeddings for Evolving Semantic Discovery", "abstract": "Word evolution refers to the changing meanings and associations of words\nthroughout time, as a byproduct of human language evolution. By studying word\nevolution, we can infer social trends and language constructs over different\nperiods of human history. However, traditional techniques such as word\nrepresentation learning do not adequately capture the evolving language\nstructure and vocabulary. In this paper, we develop a dynamic statistical model\nto learn time-aware word vector representation. We propose a model that\nsimultaneously learns time-aware embeddings and solves the resulting \"alignment\nproblem\". This model is trained on a crawled NYTimes dataset. Additionally, we\ndevelop multiple intuitive evaluation strategies of temporal word embeddings.\nOur qualitative and quantitative tests indicate that our method not only\nreliably captures this evolution over time, but also consistently outperforms\nstate-of-the-art temporal embedding approaches on both semantic accuracy and\nalignment quality.", "published": "2017-03-02 03:59:18", "link": "http://arxiv.org/abs/1703.00607v2", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "A Generic Online Parallel Learning Framework for Large Margin Models", "abstract": "To speed up the training process, many existing systems use parallel\ntechnology for online learning algorithms. However, most research mainly focus\non stochastic gradient descent (SGD) instead of other algorithms. We propose a\ngeneric online parallel learning framework for large margin models, and also\nanalyze our framework on popular large margin algorithms, including MIRA and\nStructured Perceptron. Our framework is lock-free and easy to implement on\nexisting systems. Experiments show that systems with our framework can gain\nnear linear speed up by increasing running threads, and with no loss in\naccuracy.", "published": "2017-03-02 13:52:47", "link": "http://arxiv.org/abs/1703.00786v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DAWT: Densely Annotated Wikipedia Texts across multiple languages", "abstract": "In this work, we open up the DAWT dataset - Densely Annotated Wikipedia Texts\nacross multiple languages. The annotations include labeled text mentions\nmapping to entities (represented by their Freebase machine ids) as well as the\ntype of the entity. The data set contains total of 13.6M articles, 5.0B tokens,\n13.8M mention entity co-occurrences. DAWT contains 4.8 times more anchor text\nto entity links than originally present in the Wikipedia markup. Moreover, it\nspans several languages including English, Spanish, Italian, German, French and\nArabic. We also present the methodology used to generate the dataset which\nenriches Wikipedia markup in order to increase number of links. In addition to\nthe main dataset, we open up several derived datasets including mention entity\nco-occurrence counts and entity embeddings, as well as mappings between\nFreebase ids and Wikidata item ids. We also discuss two applications of these\ndatasets and hope that opening them up would prove useful for the Natural\nLanguage Processing and Information Retrieval communities, as well as\nfacilitate multi-lingual research.", "published": "2017-03-02 20:55:20", "link": "http://arxiv.org/abs/1703.00948v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.SI"], "primary_category": "cs.IR"}
{"title": "Toward Controlled Generation of Text", "abstract": "Generic generation and manipulation of text is challenging and has limited\nsuccess compared to recent deep generative modeling in visual domain. This\npaper aims at generating plausible natural language sentences, whose attributes\nare dynamically controlled by learning disentangled latent representations with\ndesignated semantics. We propose a new neural generative model which combines\nvariational auto-encoders and holistic attribute discriminators for effective\nimposition of semantic structures. With differentiable approximation to\ndiscrete text samples, explicit constraints on independent attribute controls,\nand efficient collaborative learning of generator and discriminators, our model\nlearns highly interpretable representations from even only word annotations,\nand produces realistic sentences with desired attributes. Quantitative\nevaluation validates the accuracy of sentence and attribute generation.", "published": "2017-03-02 21:23:47", "link": "http://arxiv.org/abs/1703.00955v4", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
