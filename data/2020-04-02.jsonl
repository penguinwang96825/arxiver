{"title": "4chan & 8chan embeddings", "abstract": "We have collected over 30M messages from the publicly available /pol/ message\nboards on 4chan and 8chan, and compiled them into a model of toxic language\nuse. The trained word embeddings (0.4GB) are released for free and may be\nuseful for further study on toxic discourse or to boost hate speech detection\nsystems: https://textgain.com/8chan.", "published": "2020-04-02 10:17:55", "link": "http://arxiv.org/abs/2005.06946v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mapping Languages: The Corpus of Global Language Use", "abstract": "This paper describes a web-based corpus of global language use with a focus\non how this corpus can be used for data-driven language mapping. First, the\ncorpus provides a representation of where national varieties of major languages\nare used (e.g., English, Arabic, Russian) together with consistently collected\ndata for each variety. Second, the paper evaluates a language identification\nmodel that supports more local languages with smaller sample sizes than\nalternative off-the-shelf models. Improved language identification is essential\nfor moving beyond majority languages. Given the focus on language mapping, the\npaper analyzes how well this digital language data represents actual\npopulations by (i) systematically comparing the corpus with demographic\nground-truth data and (ii) triangulating the corpus with an alternate\nTwitter-based dataset. In total, the corpus contains 423 billion words\nrepresenting 148 languages (with over 1 million words from each language) and\n158 countries (again with over 1 million words from each country), all\ndistilled from Common Crawl web data. The main contribution of this paper, in\naddition to describing this publicly-available corpus, is to provide a\ncomprehensive analysis of the relationship between two sources of digital data\n(the web and Twitter) as well as their connection to underlying populations.", "published": "2020-04-02 03:42:14", "link": "http://arxiv.org/abs/2004.00798v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mapping Languages and Demographics with Georeferenced Corpora", "abstract": "This paper evaluates large georeferenced corpora, taken from both web-crawled\nand social media sources, against ground-truth population and language-census\ndatasets. The goal is to determine (i) which dataset best represents population\ndemographics; (ii) in what parts of the world the datasets are most\nrepresentative of actual populations; and (iii) how to weight the datasets to\nprovide more accurate representations of underlying populations. The paper\nfinds that the two datasets represent very different populations and that they\ncorrelate with actual populations with values of r=0.60 (social media) and\nr=0.49 (web-crawled). Further, Twitter data makes better predictions about the\ninventory of languages used in each country.", "published": "2020-04-02 04:34:11", "link": "http://arxiv.org/abs/2004.00809v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Furiously Can Colourless Green Ideas Sleep? Sentence Acceptability\n  in Context", "abstract": "We study the influence of context on sentence acceptability. First we compare\nthe acceptability ratings of sentences judged in isolation, with a relevant\ncontext, and with an irrelevant context. Our results show that context induces\na cognitive load for humans, which compresses the distribution of ratings.\nMoreover, in relevant contexts we observe a discourse coherence effect which\nuniformly raises acceptability. Next, we test unidirectional and bidirectional\nlanguage models in their ability to predict acceptability ratings. The\nbidirectional models show very promising results, with the best model achieving\na new state-of-the-art for unsupervised acceptability prediction. The two sets\nof experiments provide insights into the cognitive aspects of sentence\nprocessing and central issues in the computational modelling of text and\ndiscourse.", "published": "2020-04-02 08:58:44", "link": "http://arxiv.org/abs/2004.00881v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Understanding Linearity of Cross-Lingual Word Embedding Mappings", "abstract": "The technique of Cross-Lingual Word Embedding (CLWE) plays a fundamental role\nin tackling Natural Language Processing challenges for low-resource languages.\nIts dominant approaches assumed that the relationship between embeddings could\nbe represented by a linear mapping, but there has been no exploration of the\nconditions under which this assumption holds. Such a research gap becomes very\ncritical recently, as it has been evidenced that relaxing mappings to be\nnon-linear can lead to better performance in some cases. We, for the first\ntime, present a theoretical analysis that identifies the preservation of\nanalogies encoded in monolingual word embeddings as a necessary and sufficient\ncondition for the ground-truth CLWE mapping between those embeddings to be\nlinear. On a novel cross-lingual analogy dataset that covers five\nrepresentative analogy categories for twelve distinct languages, we carry out\nexperiments which provide direct empirical support for our theoretical claim.\nThese results offer additional insight into the observations of other\nresearchers and contribute inspiration for the development of more effective\ncross-lingual representation learning strategies.", "published": "2020-04-02 15:40:59", "link": "http://arxiv.org/abs/2004.01079v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NUBES: A Corpus of Negation and Uncertainty in Spanish Clinical Texts", "abstract": "This paper introduces the first version of the NUBes corpus (Negation and\nUncertainty annotations in Biomedical texts in Spanish). The corpus is part of\nan on-going research and currently consists of 29,682 sentences obtained from\nanonymised health records annotated with negation and uncertainty. The article\nincludes an exhaustive comparison with similar corpora in Spanish, and presents\nthe main annotation and design decisions. Additionally, we perform preliminary\nexperiments using deep learning algorithms to validate the annotated dataset.\nAs far as we know, NUBes is the largest publicly available corpus for negation\nin Spanish and the first that also incorporates the annotation of speculation\ncues, scopes, and events.", "published": "2020-04-02 15:51:31", "link": "http://arxiv.org/abs/2004.01092v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Causal Inference of Script Knowledge", "abstract": "When does a sequence of events define an everyday scenario and how can this\nknowledge be induced from text? Prior works in inducing such scripts have\nrelied on, in one form or another, measures of correlation between instances of\nevents in a corpus. We argue from both a conceptual and practical sense that a\npurely correlation-based approach is insufficient, and instead propose an\napproach to script induction based on the causal effect between events,\nformally defined via interventions. Through both human and automatic\nevaluations, we show that the output of our method based on causal effects\nbetter matches the intuition of what a script represents", "published": "2020-04-02 17:54:24", "link": "http://arxiv.org/abs/2004.01174v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MZET: Memory Augmented Zero-Shot Fine-grained Named Entity Typing", "abstract": "Named entity typing (NET) is a classification task of assigning an entity\nmention in the context with given semantic types. However, with the growing\nsize and granularity of the entity types, rare researches in previous concern\nwith newly emerged entity types. In this paper, we propose MZET, a novel memory\naugmented FNET (Fine-grained NET) model, to tackle the unseen types in a\nzero-shot manner. MZET incorporates character-level, word-level, and\ncontextural-level information to learn the entity mention representation.\nBesides, MZET considers the semantic meaning and the hierarchical structure\ninto the entity type representation. Finally, through the memory component\nwhich models the relationship between the entity mention and the entity type,\nMZET transfer the knowledge from seen entity types to the zero-shot ones.\nExtensive experiments on three public datasets show prominent performance\nobtained by MZET, which surpasses the state-of-the-art FNET neural network\nmodels with up to 7% gain in Micro-F1 and Macro-F1 score.", "published": "2020-04-02 21:17:33", "link": "http://arxiv.org/abs/2004.01267v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MCEN: Bridging Cross-Modal Gap between Cooking Recipes and Dish Images\n  with Latent Variable Model", "abstract": "Nowadays, driven by the increasing concern on diet and health, food computing\nhas attracted enormous attention from both industry and research community. One\nof the most popular research topics in this domain is Food Retrieval, due to\nits profound influence on health-oriented applications. In this paper, we focus\non the task of cross-modal retrieval between food images and cooking recipes.\nWe present Modality-Consistent Embedding Network (MCEN) that learns\nmodality-invariant representations by projecting images and texts to the same\nembedding space. To capture the latent alignments between modalities, we\nincorporate stochastic latent variables to explicitly exploit the interactions\nbetween textual and visual features. Importantly, our method learns the\ncross-modal alignments during training but computes embeddings of different\nmodalities independently at inference time for the sake of efficiency.\nExtensive experimental results clearly demonstrate that the proposed MCEN\noutperforms all existing approaches on the benchmark Recipe1M dataset and\nrequires less computational cost.", "published": "2020-04-02 16:00:10", "link": "http://arxiv.org/abs/2004.01095v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "R3: A Reading Comprehension Benchmark Requiring Reasoning Processes", "abstract": "Existing question answering systems can only predict answers without explicit\nreasoning processes, which hinder their explainability and make us overestimate\ntheir ability of understanding and reasoning over natural language. In this\nwork, we propose a novel task of reading comprehension, in which a model is\nrequired to provide final answers and reasoning processes. To this end, we\nintroduce a formalism for reasoning over unstructured text, namely Text\nReasoning Meaning Representation (TRMR). TRMR consists of three phrases, which\nis expressive enough to characterize the reasoning process to answer reading\ncomprehension questions. We develop an annotation platform to facilitate TRMR's\nannotation, and release the R3 dataset, a \\textbf{R}eading comprehension\nbenchmark \\textbf{R}equiring \\textbf{R}easoning processes. R3 contains over 60K\npairs of question-answer pairs and their TRMRs. Our dataset is available at:\n\\url{http://anonymous}.", "published": "2020-04-02 20:39:12", "link": "http://arxiv.org/abs/2004.01251v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal\n  Transformers", "abstract": "We propose Pixel-BERT to align image pixels with text by deep multi-modal\ntransformers that jointly learn visual and language embedding in a unified\nend-to-end framework. We aim to build a more accurate and thorough connection\nbetween image pixels and language semantics directly from image and sentence\npairs instead of using region-based image features as the most recent vision\nand language tasks. Our Pixel-BERT which aligns semantic connection in pixel\nand text level solves the limitation of task-specific visual representation for\nvision and language tasks. It also relieves the cost of bounding box\nannotations and overcomes the unbalance between semantic labels in visual task\nand language semantic. To provide a better representation for down-stream\ntasks, we pre-train a universal end-to-end model with image and sentence pairs\nfrom Visual Genome dataset and MS-COCO dataset. We propose to use a random\npixel sampling mechanism to enhance the robustness of visual representation and\nto apply the Masked Language Model and Image-Text Matching as pre-training\ntasks. Extensive experiments on downstream tasks with our pre-trained model\nshow that our approach makes the most state-of-the-arts in downstream tasks,\nincluding Visual Question Answering (VQA), image-text retrieval, Natural\nLanguage for Visual Reasoning for Real (NLVR). Particularly, we boost the\nperformance of a single model in VQA task by 2.17 points compared with SOTA\nunder fair comparison.", "published": "2020-04-02 07:39:28", "link": "http://arxiv.org/abs/2004.00849v2", "categories": ["cs.CV", "cs.CL", "cs.LG", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Learning to cooperate: Emergent communication in multi-agent navigation", "abstract": "Emergent communication in artificial agents has been studied to understand\nlanguage evolution, as well as to develop artificial systems that learn to\ncommunicate with humans. We show that agents performing a cooperative\nnavigation task in various gridworld environments learn an interpretable\ncommunication protocol that enables them to efficiently, and in many cases,\noptimally, solve the task. An analysis of the agents' policies reveals that\nemergent signals spatially cluster the state space, with signals referring to\nspecific locations and spatial directions such as \"left\", \"up\", or \"upper left\nroom\". Using populations of agents, we show that the emergent protocol has\nbasic compositional structure, thus exhibiting a core property of natural\nlanguage.", "published": "2020-04-02 16:03:17", "link": "http://arxiv.org/abs/2004.01097v2", "categories": ["cs.LG", "cs.CL", "cs.MA", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Evaluating the Calibration of Knowledge Graph Embeddings for Trustworthy\n  Link Prediction", "abstract": "Little is known about the trustworthiness of predictions made by knowledge\ngraph embedding (KGE) models. In this paper we take initial steps toward this\ndirection by investigating the calibration of KGE models, or the extent to\nwhich they output confidence scores that reflect the expected correctness of\npredicted knowledge graph triples. We first conduct an evaluation under the\nstandard closed-world assumption (CWA), in which predicted triples not already\nin the knowledge graph are considered false, and show that existing calibration\ntechniques are effective for KGE under this common but narrow assumption. Next,\nwe introduce the more realistic but challenging open-world assumption (OWA), in\nwhich unobserved predictions are not considered true or false until\nground-truth labels are obtained. Here, we show that existing calibration\ntechniques are much less effective under the OWA than the CWA, and provide\nexplanations for this discrepancy. Finally, to motivate the utility of\ncalibration for KGE from a practitioner's perspective, we conduct a unique case\nstudy of human-AI collaboration, showing that calibrated predictions can\nimprove human performance in a knowledge graph completion task.", "published": "2020-04-02 17:46:47", "link": "http://arxiv.org/abs/2004.01168v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Gestalt: a Stacking Ensemble for SQuAD2.0", "abstract": "We propose a deep-learning system -- for the SQuAD2.0 task -- that finds, or\nindicates the lack of, a correct answer to a question in a context paragraph.\nOur goal is to learn an ensemble of heterogeneous SQuAD2.0 models that, when\nblended properly, outperforms the best model in the ensemble per se. We created\na stacking ensemble that combines top-N predictions from two models, based on\nALBERT and RoBERTa, into a multiclass classification task to pick the best\nanswer out of their predictions. We explored various ensemble configurations,\ninput representations, and model architectures. For evaluation, we examined\ntest-set EM and F1 scores; our best-performing ensemble incorporated a\nCNN-based meta-model and scored 87.117 and 90.306, respectively -- a relative\nimprovement of 0.55% for EM and 0.61% for F1 scores, compared to the baseline\nperformance of the best model in the ensemble, an ALBERT-based model, at 86.644\nfor EM and 89.760 for F1.", "published": "2020-04-02 08:09:22", "link": "http://arxiv.org/abs/2004.07067v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Towards Relevance and Sequence Modeling in Language Recognition", "abstract": "The task of automatic language identification (LID) involving multiple\ndialects of the same language family in the presence of noise is a challenging\nproblem. In these scenarios, the identity of the language/dialect may be\nreliably present only in parts of the temporal sequence of the speech signal.\nThe conventional approaches to LID (and for speaker recognition) ignore the\nsequence information by extracting long-term statistical summary of the\nrecording assuming an independence of the feature frames. In this paper, we\npropose a neural network framework utilizing short-sequence information in\nlanguage recognition. In particular, a new model is proposed for incorporating\nrelevance in language recognition, where parts of speech data are weighted more\nbased on their relevance for the language recognition task. This relevance\nweighting is achieved using the bidirectional long short-term memory (BLSTM)\nnetwork with attention modeling. We explore two approaches, the first approach\nuses segment level i-vector/x-vector representations that are aggregated in the\nneural model and the second approach where the acoustic features are directly\nmodeled in an end-to-end neural model. Experiments are performed using the\nlanguage recognition task in NIST LRE 2017 Challenge using clean, noisy and\nmulti-speaker speech data as well as in the RATS language recognition corpus.\nIn these experiments on noisy LRE tasks as well as the RATS dataset, the\nproposed approach yields significant improvements over the conventional\ni-vector/x-vector based language recognition approaches as well as with other\nprevious models incorporating sequence information.", "published": "2020-04-02 18:31:18", "link": "http://arxiv.org/abs/2004.01221v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "iMetricGAN: Intelligibility Enhancement for Speech-in-Noise using\n  Generative Adversarial Network-based Metric Learning", "abstract": "The intelligibility of natural speech is seriously degraded when exposed to\nadverse noisy environments. In this work, we propose a deep learning-based\nspeech modification method to compensate for the intelligibility loss, with the\nconstraint that the root mean square (RMS) level and duration of the speech\nsignal are maintained before and after modifications. Specifically, we utilize\nan iMetricGAN approach to optimize the speech intelligibility metrics with\ngenerative adversarial networks (GANs). Experimental results show that the\nproposed iMetricGAN outperforms conventional state-of-the-art algorithms in\nterms of objective measures, i.e., speech intelligibility in bits (SIIB) and\nextended short-time objective intelligibility (ESTOI), under a Cafeteria noise\ncondition. In addition, formal listening tests reveal significant\nintelligibility gains when both noise and reverberation exist.", "published": "2020-04-02 11:01:16", "link": "http://arxiv.org/abs/2004.00932v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The RWTH ASR System for TED-LIUM Release 2: Improving Hybrid HMM with\n  SpecAugment", "abstract": "We present a complete training pipeline to build a state-of-the-art hybrid\nHMM-based ASR system on the 2nd release of the TED-LIUM corpus. Data\naugmentation using SpecAugment is successfully applied to improve performance\non top of our best SAT model using i-vectors. By investigating the effect of\ndifferent maskings, we achieve improvements from SpecAugment on hybrid HMM\nmodels without increasing model size and training time. A subsequent sMBR\ntraining is applied to fine-tune the final acoustic model, and both LSTM and\nTransformer language models are trained and evaluated. Our best system achieves\na 5.6% WER on the test set, which outperforms the previous state-of-the-art by\n27% relative.", "published": "2020-04-02 12:41:15", "link": "http://arxiv.org/abs/2004.00960v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Full-Sum Decoding for Hybrid HMM based Speech Recognition using LSTM\n  Language Model", "abstract": "In hybrid HMM based speech recognition, LSTM language models have been widely\napplied and achieved large improvements. The theoretical capability of modeling\nany unlimited context suggests that no recombination should be applied in\ndecoding. This motivates to reconsider full summation over the HMM-state\nsequences instead of Viterbi approximation in decoding. We explore the\npotential gain from more accurate probabilities in terms of decision making and\napply the full-sum decoding with a modified prefix-tree search framework. The\nproposed full-sum decoding is evaluated on both Switchboard and Librispeech\ncorpora. Different models using CE and sMBR training criteria are used.\nAdditionally, both MAP and confusion network decoding as approximated variants\nof general Bayes decision rule are evaluated. Consistent improvements over\nstrong baselines are achieved in almost all cases without extra cost. We also\ndiscuss tuning effort, efficiency and some limitations of full-sum decoding.", "published": "2020-04-02 13:07:05", "link": "http://arxiv.org/abs/2004.00967v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improving auditory attention decoding performance of linear and\n  non-linear methods using state-space model", "abstract": "Identifying the target speaker in hearing aid applications is crucial to\nimprove speech understanding. Recent advances in electroencephalography (EEG)\nhave shown that it is possible to identify the target speaker from single-trial\nEEG recordings using auditory attention decoding (AAD) methods. AAD methods\nreconstruct the attended speech envelope from EEG recordings, based on a linear\nleast-squares cost function or non-linear neural networks, and then directly\ncompare the reconstructed envelope with the speech envelopes of speakers to\nidentify the attended speaker using Pearson correlation coefficients. Since\nthese correlation coefficients are highly fluctuating, for a reliable decoding\na large correlation window is used, which causes a large processing delay. In\nthis paper, we investigate a state-space model using correlation coefficients\nobtained with a small correlation window to improve the decoding performance of\nthe linear and the non-linear AAD methods. The experimental results show that\nthe state-space model significantly improves the decoding performance.", "published": "2020-04-02 09:56:06", "link": "http://arxiv.org/abs/2004.00910v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Temporarily-Aware Context Modelling using Generative Adversarial\n  Networks for Speech Activity Detection", "abstract": "This paper presents a novel framework for Speech Activity Detection (SAD).\nInspired by the recent success of multi-task learning approaches in the speech\nprocessing domain, we propose a novel joint learning framework for SAD. We\nutilise generative adversarial networks to automatically learn a loss function\nfor joint prediction of the frame-wise speech/ non-speech classifications\ntogether with the next audio segment. In order to exploit the temporal\nrelationships within the input signal, we propose a temporal discriminator\nwhich aims to ensure that the predicted signal is temporally consistent. We\nevaluate the proposed framework on multiple public benchmarks, including NIST\nOpenSAT' 17, AMI Meeting and HAVIC, where we demonstrate its capability to\noutperform state-of-the-art SAD approaches. Furthermore, our cross-database\nevaluations demonstrate the robustness of the proposed approach across\ndifferent languages, accents, and acoustic environments.", "published": "2020-04-02 02:33:13", "link": "http://arxiv.org/abs/2004.01546v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Multi-Modal Video Forensic Platform for Investigating Post-Terrorist\n  Attack Scenarios", "abstract": "The forensic investigation of a terrorist attack poses a significant\nchallenge to the investigative authorities, as often several thousand hours of\nvideo footage must be viewed. Large scale Video Analytic Platforms (VAP) assist\nlaw enforcement agencies (LEA) in identifying suspects and securing evidence.\nCurrent platforms focus primarily on the integration of different computer\nvision methods and thus are restricted to a single modality. We present a video\nanalytic platform that integrates visual and audio analytic modules and fuses\ninformation from surveillance cameras and video uploads from eyewitnesses.\nVideos are analyzed according their acoustic and visual content. Specifically,\nAudio Event Detection is applied to index the content according to\nattack-specific acoustic concepts. Audio similarity search is utilized to\nidentify similar video sequences recorded from different perspectives. Visual\nobject detection and tracking are used to index the content according to\nrelevant concepts. Innovative user-interface concepts are introduced to harness\nthe full potential of the heterogeneous results of the analytical modules,\nallowing investigators to more quickly follow-up on leads and eyewitness\nreports.", "published": "2020-04-02 14:29:27", "link": "http://arxiv.org/abs/2004.01023v1", "categories": ["cs.MM", "cs.CV", "cs.CY", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "AI4COVID-19: AI Enabled Preliminary Diagnosis for COVID-19 from Cough\n  Samples via an App", "abstract": "Background: The inability to test at scale has become humanity's Achille's\nheel in the ongoing war against the COVID-19 pandemic. A scalable screening\ntool would be a game changer. Building on the prior work on cough-based\ndiagnosis of respiratory diseases, we propose, develop and test an Artificial\nIntelligence (AI)-powered screening solution for COVID-19 infection that is\ndeployable via a smartphone app. The app, named AI4COVID-19 records and sends\nthree 3-second cough sounds to an AI engine running in the cloud, and returns a\nresult within two minutes. Methods: Cough is a symptom of over thirty\nnon-COVID-19 related medical conditions. This makes the diagnosis of a COVID-19\ninfection by cough alone an extremely challenging multidisciplinary problem. We\naddress this problem by investigating the distinctness of pathomorphological\nalterations in the respiratory system induced by COVID-19 infection when\ncompared to other respiratory infections. To overcome the COVID-19 cough\ntraining data shortage we exploit transfer learning. To reduce the misdiagnosis\nrisk stemming from the complex dimensionality of the problem, we leverage a\nmulti-pronged mediator centered risk-averse AI architecture. Results: Results\nshow AI4COVID-19 can distinguish among COVID-19 coughs and several types of\nnon-COVID-19 coughs. The accuracy is promising enough to encourage a\nlarge-scale collection of labeled cough data to gauge the generalization\ncapability of AI4COVID-19. AI4COVID-19 is not a clinical grade testing tool.\nInstead, it offers a screening tool deployable anytime, anywhere, by anyone. It\ncan also be a clinical decision assistance tool used to channel\nclinical-testing and treatment to those who need it the most, thereby saving\nmore lives.", "published": "2020-04-02 21:39:34", "link": "http://arxiv.org/abs/2004.01275v6", "categories": ["eess.AS", "cs.LG", "cs.SD", "q-bio.QM", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Heart Sound Segmentation using Bidirectional LSTMs with Attention", "abstract": "This paper proposes a novel framework for the segmentation of phonocardiogram\n(PCG) signals into heart states, exploiting the temporal evolution of the PCG\nas well as considering the salient information that it provides for the\ndetection of the heart state. We propose the use of recurrent neural networks\nand exploit recent advancements in attention based learning to segment the PCG\nsignal. This allows the network to identify the most salient aspects of the\nsignal and disregard uninformative information. The proposed method attains\nstate-of-the-art performance on multiple benchmarks including both human and\nanimal heart recordings. Furthermore, we empirically analyse different feature\ncombinations including envelop features, wavelet and Mel Frequency Cepstral\nCoefficients (MFCC), and provide quantitative measurements that explore the\nimportance of different features in the proposed approach. We demonstrate that\na recurrent neural network coupled with attention mechanisms can effectively\nlearn from irregular and noisy PCG recordings. Our analysis of different\nfeature combinations shows that MFCC features and their derivatives offer the\nbest performance compared to classical wavelet and envelop features. Heart\nsound segmentation is a crucial pre-processing step for many diagnostic\napplications. The proposed method provides a cost effective alternative to\nlabour extensive manual segmentation, and provides a more accurate segmentation\nthan existing methods. As such, it can improve the performance of further\nanalysis including the detection of murmurs and ejection clicks. The proposed\nmethod is also applicable for detection and segmentation of other one\ndimensional biomedical signals.", "published": "2020-04-02 02:09:11", "link": "http://arxiv.org/abs/2004.03712v1", "categories": ["eess.SP", "cs.LG", "cs.SD", "eess.AS", "q-bio.QM", "stat.ML"], "primary_category": "eess.SP"}
