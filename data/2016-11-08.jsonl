{"title": "Cruciform: Solving Crosswords with Natural Language Processing", "abstract": "Crossword puzzles are popular word games that require not only a large\nvocabulary, but also a broad knowledge of topics. Answering each clue is a\nnatural language task on its own as many clues contain nuances, puns, or\ncounter-intuitive word definitions. Additionally, it can be extremely difficult\nto ascertain definitive answers without the constraints of the crossword grid\nitself. This task is challenging for both humans and computers. We describe\nhere a new crossword solving system, Cruciform. We employ a group of natural\nlanguage components, each of which returns a list of candidate words with\nscores when given a clue. These lists are used in conjunction with the fill\nintersections in the puzzle grid to formulate a constraint satisfaction\nproblem, in a manner similar to the one used in the Dr. Fill system. We\ndescribe the results of several of our experiments with the system.", "published": "2016-11-08 01:47:41", "link": "http://arxiv.org/abs/1611.02360v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dependency Sensitive Convolutional Neural Networks for Modeling\n  Sentences and Documents", "abstract": "The goal of sentence and document modeling is to accurately represent the\nmeaning of sentences and documents for various Natural Language Processing\ntasks. In this work, we present Dependency Sensitive Convolutional Neural\nNetworks (DSCNN) as a general-purpose classification system for both sentences\nand documents. DSCNN hierarchically builds textual representations by\nprocessing pretrained word embeddings via Long Short-Term Memory networks and\nsubsequently extracting features with convolution operators. Compared with\nexisting recursive neural models with tree structures, DSCNN does not rely on\nparsers and expensive phrase labeling, and thus is not restricted to\nsentence-level tasks. Moreover, unlike other CNN-based models that analyze\nsentences locally by sliding windows, our system captures both the dependency\ninformation within each sentence and relationships across sentences in the same\ndocument. Experiment results demonstrate that our approach is achieving\nstate-of-the-art performance on several tasks, including sentiment analysis,\nquestion type classification, and subjectivity classification.", "published": "2016-11-08 01:48:15", "link": "http://arxiv.org/abs/1611.02361v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Surrogate-based Generic Classifier for Chinese TV Series Reviews", "abstract": "With the emerging of various online video platforms like Youtube, Youku and\nLeTV, online TV series' reviews become more and more important both for viewers\nand producers. Customers rely heavily on these reviews before selecting TV\nseries, while producers use them to improve the quality. As a result,\nautomatically classifying reviews according to different requirements evolves\nas a popular research topic and is essential in our daily life. In this paper,\nwe focused on reviews of hot TV series in China and successfully trained\ngeneric classifiers based on eight predefined categories. The experimental\nresults showed promising performance and effectiveness of its generalization to\ndifferent TV series.", "published": "2016-11-08 03:33:46", "link": "http://arxiv.org/abs/1611.02378v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Discriminative Acoustic Word Embeddings: Recurrent Neural Network-Based\n  Approaches", "abstract": "Acoustic word embeddings --- fixed-dimensional vector representations of\nvariable-length spoken word segments --- have begun to be considered for tasks\nsuch as speech recognition and query-by-example search. Such embeddings can be\nlearned discriminatively so that they are similar for speech segments\ncorresponding to the same word, while being dissimilar for segments\ncorresponding to different words. Recent work has found that acoustic word\nembeddings can outperform dynamic time warping on query-by-example search and\nrelated word discrimination tasks. However, the space of embedding models and\ntraining approaches is still relatively unexplored. In this paper we present\nnew discriminative embedding models based on recurrent neural networks (RNNs).\nWe consider training losses that have been successful in prior work, in\nparticular a cross entropy loss for word classification and a contrastive loss\nthat explicitly aims to separate same-word and different-word pairs in a\n\"Siamese network\" training setting. We find that both classifier-based and\nSiamese RNN embeddings improve over previously reported results on a word\ndiscrimination task, with Siamese RNNs outperforming classification models. In\naddition, we present analyses of the learned embeddings and the effects of\nvariables such as dimensionality and network structure.", "published": "2016-11-08 15:13:19", "link": "http://arxiv.org/abs/1611.02550v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contradiction Detection for Rumorous Claims", "abstract": "The utilization of social media material in journalistic workflows is\nincreasing, demanding automated methods for the identification of mis- and\ndisinformation. Since textual contradiction across social media posts can be a\nsignal of rumorousness, we seek to model how claims in Twitter posts are being\ntextually contradicted. We identify two different contexts in which\ncontradiction emerges: its broader form can be observed across independently\nposted tweets and its more specific form in threaded conversations. We define\nhow the two scenarios differ in terms of central elements of argumentation:\nclaims and conversation structure. We design and evaluate models for the two\nscenarios uniformly as 3-way Recognizing Textual Entailment tasks in order to\nrepresent claims and conversation structure implicitly in a generic inference\nmodel, while previous studies used explicit or no representation of these\nproperties. To address noisy text, our classifiers use simple similarity\nfeatures derived from the string and part-of-speech level. Corpus statistics\nreveal distribution differences for these features in contradictory as opposed\nto non-contradictory tweet relations, and the classifiers yield state of the\nart performance.", "published": "2016-11-08 16:19:17", "link": "http://arxiv.org/abs/1611.02588v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Veracity Computing from Lexical Cues and Perceived Certainty Trends", "abstract": "We present a data-driven method for determining the veracity of a set of\nrumorous claims on social media data. Tweets from different sources pertaining\nto a rumor are processed on three levels: first, factuality values are assigned\nto each tweet based on four textual cue categories relevant for our journalism\nuse case; these amalgamate speaker support in terms of polarity and commitment\nin terms of certainty and speculation. Next, the proportions of these lexical\ncues are utilized as predictors for tweet certainty in a generalized linear\nregression model. Subsequently, lexical cue proportions, predicted certainty,\nas well as their time course characteristics are used to compute veracity for\neach rumor in terms of the identity of the rumor-resolving tweet and its binary\nresolution value judgment. The system operates without access to\nextralinguistic resources. Evaluated on the data portion for which hand-labeled\nexamples were available, it achieves .74 F1-score on identifying rumor\nresolving tweets and .76 F1-score on predicting if a rumor is resolved as true\nor false.", "published": "2016-11-08 16:21:16", "link": "http://arxiv.org/abs/1611.02590v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic recognition of child speech for robotic applications in noisy\n  environments", "abstract": "Automatic speech recognition (ASR) allows a natural and intuitive interface\nfor robotic educational applications for children. However there are a number\nof challenges to overcome to allow such an interface to operate robustly in\nrealistic settings, including the intrinsic difficulties of recognising child\nspeech and high levels of background noise often present in classrooms. As part\nof the EU EASEL project we have provided several contributions to address these\nchallenges, implementing our own ASR module for use in robotics applications.\nWe used the latest deep neural network algorithms which provide a leap in\nperformance over the traditional GMM approach, and apply data augmentation\nmethods to improve robustness to noise and speaker variation. We provide a\nclose integration between the ASR module and the rest of the dialogue system,\nallowing the ASR to receive in real-time the language models relevant to the\ncurrent section of the dialogue, greatly improving the accuracy. We integrated\nour ASR module into an interactive, multimodal system using a small humanoid\nrobot to help children learn about exercise and energy. The system was\ninstalled at a public museum event as part of a research study where 320\nchildren (aged 3 to 14) interacted with the robot, with our ASR achieving 90%\naccuracy for fluent and near-fluent speech.", "published": "2016-11-08 09:50:30", "link": "http://arxiv.org/abs/1611.02695v1", "categories": ["cs.CL", "cs.SD"], "primary_category": "cs.CL"}
{"title": "An Automated System for Essay Scoring of Online Exams in Arabic based on\n  Stemming Techniques and Levenshtein Edit Operations", "abstract": "In this article, an automated system is proposed for essay scoring in Arabic\nlanguage for online exams based on stemming techniques and Levenshtein edit\noperations. An online exam has been developed on the proposed mechanisms,\nexploiting the capabilities of light and heavy stemming. The implemented online\ngrading system has shown to be an efficient tool for automated scoring of essay\nquestions.", "published": "2016-11-08 15:25:02", "link": "http://arxiv.org/abs/1611.02815v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Getting Started with Neural Models for Semantic Matching in Web Search", "abstract": "The vocabulary mismatch problem is a long-standing problem in information\nretrieval. Semantic matching holds the promise of solving the problem. Recent\nadvances in language technology have given rise to unsupervised neural models\nfor learning representations of words as well as bigger textual units. Such\nrepresentations enable powerful semantic matching methods. This survey is meant\nas an introduction to the use of neural models for semantic matching. To remain\nfocused we limit ourselves to web search. We detail the required background and\nterminology, a taxonomy grouping the rapidly growing body of work in the area,\nand then survey work on neural models for semantic matching in the context of\nthree tasks: query suggestion, ad retrieval, and document retrieval. We include\na section on resources and best practices that we believe will help readers who\nare new to the area. We conclude with an assessment of the state-of-the-art and\nsuggestions for future work.", "published": "2016-11-08 14:28:40", "link": "http://arxiv.org/abs/1611.03305v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "The Neural Noisy Channel", "abstract": "We formulate sequence to sequence transduction as a noisy channel decoding\nproblem and use recurrent neural networks to parameterise the source and\nchannel models. Unlike direct models which can suffer from explaining-away\neffects during training, noisy channel models must produce outputs that explain\ntheir inputs, and their component models can be trained with not only paired\ntraining samples but also unpaired samples from the marginal output\ndistribution. Using a latent variable to control how much of the conditioning\nsequence the channel model needs to read in order to generate a subsequent\nsymbol, we obtain a tractable and effective beam search decoder. Experimental\nresults on abstractive sentence summarisation, morphological inflection, and\nmachine translation show that noisy channel models outperform direct models,\nand that they significantly benefit from increased amounts of unpaired output\ndata that direct models cannot easily use.", "published": "2016-11-08 15:18:44", "link": "http://arxiv.org/abs/1611.02554v2", "categories": ["cs.CL", "cs.AI", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Sentence Ordering and Coherence Modeling using Recurrent Neural Networks", "abstract": "Modeling the structure of coherent texts is a key NLP problem. The task of\ncoherently organizing a given set of sentences has been commonly used to build\nand evaluate models that understand such structure. We propose an end-to-end\nunsupervised deep learning approach based on the set-to-sequence framework to\naddress this problem. Our model strongly outperforms prior methods in the order\ndiscrimination task and a novel task of ordering abstracts from scientific\narticles. Furthermore, our work shows that useful text representations can be\nobtained by learning to order sentences. Visualizing the learned sentence\nrepresentations shows that the model captures high-level logical structure in\nparagraphs. Our representations perform comparably to state-of-the-art\npre-training methods on sentence similarity and paraphrase detection tasks.", "published": "2016-11-08 19:04:09", "link": "http://arxiv.org/abs/1611.02654v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unsupervised Pretraining for Sequence to Sequence Learning", "abstract": "This work presents a general unsupervised learning method to improve the\naccuracy of sequence to sequence (seq2seq) models. In our method, the weights\nof the encoder and decoder of a seq2seq model are initialized with the\npretrained weights of two language models and then fine-tuned with labeled\ndata. We apply this method to challenging benchmarks in machine translation and\nabstractive summarization and find that it significantly improves the\nsubsequent supervised models. Our main result is that pretraining improves the\ngeneralization of seq2seq models. We achieve state-of-the art results on the\nWMT English$\\rightarrow$German task, surpassing a range of methods using both\nphrase-based machine translation and neural machine translation. Our method\nachieves a significant improvement of 1.3 BLEU from the previous best models on\nboth WMT'14 and WMT'15 English$\\rightarrow$German. We also conduct human\nevaluations on abstractive summarization and find that our method outperforms a\npurely supervised learning baseline in a statistically significant manner.", "published": "2016-11-08 20:42:26", "link": "http://arxiv.org/abs/1611.02683v2", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
