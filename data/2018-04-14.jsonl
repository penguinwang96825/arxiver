{"title": "Developing Far-Field Speaker System Via Teacher-Student Learning", "abstract": "In this study, we develop the keyword spotting (KWS) and acoustic model (AM)\ncomponents in a far-field speaker system. Specifically, we use teacher-student\n(T/S) learning to adapt a close-talk well-trained production AM to far-field by\nusing parallel close-talk and simulated far-field data. We also use T/S\nlearning to compress a large-size KWS model into a small-size one to fit the\ndevice computational cost. Without the need of transcription, T/S learning well\nutilizes untranscribed data to boost the model performance in both the AM\nadaptation and KWS model compression. We further optimize the models with\nsequence discriminative training and live data to reach the best performance of\nsystems. The adapted AM improved from the baseline by 72.60% and 57.16%\nrelative word error rate reduction on play-back and live test data,\nrespectively. The final KWS model size was reduced by 27 times from a\nlarge-size KWS model without losing accuracy.", "published": "2018-04-14 04:52:23", "link": "http://arxiv.org/abs/1804.05166v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"With 1 follower I must be AWESOME :P\". Exploring the role of irony\n  markers in irony recognition", "abstract": "Conversations in social media often contain the use of irony or sarcasm, when\nthe users say the opposite of what they really mean. Irony markers are the\nmeta-communicative clues that inform the reader that an utterance is ironic. We\npropose a thorough analysis of theoretically grounded irony markers in two\nsocial media platforms: $Twitter$ and $Reddit$. Classification and frequency\nanalysis show that for $Twitter$, typographic markers such as emoticons and\nemojis are the most discriminative markers to recognize ironic utterances,\nwhile for $Reddit$ the morphological markers (e.g., interjections, tag\nquestions) are the most discriminative.", "published": "2018-04-14 17:39:45", "link": "http://arxiv.org/abs/1804.05253v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Frustratingly Easy Meta-Embedding -- Computing Meta-Embeddings by\n  Averaging Source Word Embeddings", "abstract": "Creating accurate meta-embeddings from pre-trained source embeddings has\nreceived attention lately. Methods based on global and locally-linear\ntransformation and concatenation have shown to produce accurate\nmeta-embeddings. In this paper, we show that the arithmetic mean of two\ndistinct word embedding sets yields a performant meta-embedding that is\ncomparable or better than more complex meta-embedding learning methods. The\nresult seems counter-intuitive given that vector spaces in different source\nembeddings are not comparable and cannot be simply averaged. We give insight\ninto why averaging can still produce accurate meta-embedding despite the\nincomparability of the source vector spaces.", "published": "2018-04-14 18:30:01", "link": "http://arxiv.org/abs/1804.05262v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Predicting Cyber Events by Leveraging Hacker Sentiment", "abstract": "Recent high-profile cyber attacks exemplify why organizations need better\ncyber defenses. Cyber threats are hard to accurately predict because attackers\nusually try to mask their traces. However, they often discuss exploits and\ntechniques on hacking forums. The community behavior of the hackers may provide\ninsights into groups' collective malicious activity. We propose a novel\napproach to predict cyber events using sentiment analysis. We test our approach\nusing cyber attack data from 2 major business organizations. We consider 3\ntypes of events: malicious software installation, malicious destination visits,\nand malicious emails that surpassed the target organizations' defenses. We\nconstruct predictive signals by applying sentiment analysis on hacker forum\nposts to better understand hacker behavior. We analyze over 400K posts\ngenerated between January 2016 and January 2018 on over 100 hacking forums both\non surface and Dark Web. We find that some forums have significantly more\npredictive power than others. Sentiment-based models that leverage specific\nforums can outperform state-of-the-art deep learning and time-series models on\nforecasting cyber attacks weeks ahead of the events.", "published": "2018-04-14 20:56:58", "link": "http://arxiv.org/abs/1804.05276v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Distribution-based Prediction of the Degree of Grammaticalization for\n  German Prepositions", "abstract": "We test the hypothesis that the degree of grammaticalization of German\nprepositions correlates with their corpus-based contextual dispersion measured\nby word entropy. We find that there is indeed a moderate correlation for\nentropy, but a stronger correlation for frequency and number of context types.", "published": "2018-04-14 18:19:38", "link": "http://arxiv.org/abs/1804.06719v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ClassiNet -- Predicting Missing Features for Short-Text Classification", "abstract": "The fundamental problem in short-text classification is \\emph{feature\nsparseness} -- the lack of feature overlap between a trained model and a test\ninstance to be classified. We propose \\emph{ClassiNet} -- a network of\nclassifiers trained for predicting missing features in a given instance, to\novercome the feature sparseness problem. Using a set of unlabeled training\ninstances, we first learn binary classifiers as feature predictors for\npredicting whether a particular feature occurs in a given instance. Next, each\nfeature predictor is represented as a vertex $v_i$ in the ClassiNet where a\none-to-one correspondence exists between feature predictors and vertices. The\nweight of the directed edge $e_{ij}$ connecting a vertex $v_i$ to a vertex\n$v_j$ represents the conditional probability that given $v_i$ exists in an\ninstance, $v_j$ also exists in the same instance. We show that ClassiNets\ngeneralize word co-occurrence graphs by considering implicit co-occurrences\nbetween features. We extract numerous features from the trained ClassiNet to\novercome feature sparseness. In particular, for a given instance $\\vec{x}$, we\nfind similar features from ClassiNet that did not appear in $\\vec{x}$, and\nappend those features in the representation of $\\vec{x}$. Moreover, we propose\na method based on graph propagation to find features that are indirectly\nrelated to a given short-text. We evaluate ClassiNets on several benchmark\ndatasets for short-text classification. Our experimental results show that by\nusing ClassiNet, we can statistically significantly improve the accuracy in\nshort-text classification tasks, without having to use any external resources\nsuch as thesauri for finding related features.", "published": "2018-04-14 18:24:06", "link": "http://arxiv.org/abs/1804.05260v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exploring the Encoding Layer and Loss Function in End-to-End Speaker and\n  Language Recognition System", "abstract": "In this paper, we explore the encoding/pooling layer and loss function in the\nend-to-end speaker and language recognition system. First, a unified and\ninterpretable end-to-end system for both speaker and language recognition is\ndeveloped. It accepts variable-length input and produces an utterance level\nresult. In the end-to-end system, the encoding layer plays a role in\naggregating the variable-length input sequence into an utterance level\nrepresentation. Besides the basic temporal average pooling, we introduce a\nself-attentive pooling layer and a learnable dictionary encoding layer to get\nthe utterance level representation. In terms of loss function for open-set\nspeaker verification, to get more discriminative speaker embedding, center loss\nand angular softmax loss is introduced in the end-to-end system. Experimental\nresults on Voxceleb and NIST LRE 07 datasets show that the performance of\nend-to-end learning system could be significantly improved by the proposed\nencoding layer and loss function.", "published": "2018-04-14 03:52:46", "link": "http://arxiv.org/abs/1804.05160v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
