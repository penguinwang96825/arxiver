{"title": "Variational Smoothing in Recurrent Neural Network Language Models", "abstract": "We present a new theoretical perspective of data noising in recurrent neural\nnetwork language models (Xie et al., 2017). We show that each variant of data\nnoising is an instance of Bayesian recurrent neural networks with a particular\nvariational distribution (i.e., a mixture of Gaussians whose weights depend on\nstatistics derived from the corpus such as the unigram distribution). We use\nthis insight to propose a more principled method to apply at prediction time\nand propose natural extensions to data noising under the variational framework.\nIn particular, we propose variational smoothing with tied input and output\nembedding matrices and an element-wise variational smoothing method. We\nempirically verify our analysis on two benchmark language modeling datasets and\ndemonstrate performance improvements over existing data noising methods.", "published": "2019-01-27 00:46:27", "link": "http://arxiv.org/abs/1901.09296v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dual Co-Matching Network for Multi-choice Reading Comprehension", "abstract": "Multi-choice reading comprehension is a challenging task that requires\ncomplex reasoning procedure. Given passage and question, a correct answer need\nto be selected from a set of candidate answers. In this paper, we propose\n\\textbf{D}ual \\textbf{C}o-\\textbf{M}atching \\textbf{N}etwork (\\textbf{DCMN})\nwhich model the relationship among passage, question and answer\nbidirectionally. Different from existing approaches which only calculate\nquestion-aware or option-aware passage representation, we calculate\npassage-aware question representation and passage-aware answer representation\nat the same time. To demonstrate the effectiveness of our model, we evaluate\nour model on a large-scale multiple choice machine reading comprehension\ndataset (i.e. RACE). Experimental result show that our proposed model achieves\nnew state-of-the-art results.", "published": "2019-01-27 14:15:37", "link": "http://arxiv.org/abs/1901.09381v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Promoting Diversity for End-to-End Conversation Response Generation", "abstract": "We present our work on Track 2 in the Dialog System Technology Challenges 7\n(DSTC7). The DSTC7-Track 2 aims to evaluate the response generation of fully\ndata-driven conversation models in knowledge-grounded settings, which provides\nthe contextual-relevant factual texts. The Sequenceto-Sequence models have been\nwidely used for end-to-end generative conversation modelling and achieved\nimpressive results. However, they tend to output dull and repeated responses in\nprevious studies. Our work aims to promote the diversity for end-to-end\nconversation response generation, which follows a two-stage pipeline: 1)\nGenerate multiple responses. At this stage, two different models are proposed,\ni.e., a variational generative (VariGen) model and a retrieval based\n(Retrieval) model. 2) Rank and return the most related response by training a\ntopic coherence discrimination (TCD) model for the ranking process. According\nto the official evaluation results, our proposed Retrieval and VariGen systems\nranked first and second respectively on objective diversity metrics, i.e.,\nEntropy, among all participant systems. And the VariGen system ranked second on\nNIST and METEOR metrics.", "published": "2019-01-27 21:40:10", "link": "http://arxiv.org/abs/1901.09444v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Convolutional Neural Network model based on Neutrosophy for Noisy\n  Speech Recognition", "abstract": "Convolutional neural networks are sensitive to unknown noisy condition in the\ntest phase and so their performance degrades for the noisy data classification\ntask including noisy speech recognition. In this research, a new convolutional\nneural network (CNN) model with data uncertainty handling; referred as NCNN\n(Neutrosophic Convolutional Neural Network); is proposed for classification\ntask. Here, speech signals are used as input data and their noise is modeled as\nuncertainty. In this task, using speech spectrogram, a definition of\nuncertainty is proposed in neutrosophic (NS) domain. Uncertainty is computed\nfor each Time-frequency point of speech spectrogram as like a pixel. Therefore,\nuncertainty matrix with the same size of spectrogram is created in NS domain.\nIn the next step, a two parallel paths CNN classification model is proposed.\nSpeech spectrogram is used as input of the first path and uncertainty matrix\nfor the second path. The outputs of two paths are combined to compute the final\noutput of the classifier. To show the effectiveness of the proposed method, it\nhas been compared with conventional CNN on the isolated words of Aurora2\ndataset. The proposed method achieves the average accuracy of 85.96 in noisy\ntrain data. It is more robust against Car, Airport and Subway noises with\naccuracies 90, 88 and 81 in test sets A, B and C, respectively. Results show\nthat the proposed method outperforms conventional CNN with the improvement of\n6, 5 and 2 percentage in test set A, test set B and test sets C, respectively.\nIt means that the proposed method is more robust against noisy data and handle\nthese data effectively.", "published": "2019-01-27 10:55:50", "link": "http://arxiv.org/abs/1901.10629v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
