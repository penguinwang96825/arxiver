{"title": "Zero-Shot Strategies for Length-Controllable Summarization", "abstract": "Large language models (LLMs) struggle with precise length control,\nparticularly in zero-shot settings. We conduct a comprehensive study evaluating\nLLMs' length control capabilities across multiple measures and propose\npractical methods to improve controllability. Our experiments with LLaMA 3\nreveal stark differences in length adherence across measures and highlight\ninherent biases of the model. To address these challenges, we introduce a set\nof methods: length approximation, target adjustment, sample filtering, and\nautomated revisions. By combining these methods, we demonstrate substantial\nimprovements in length compliance while maintaining or enhancing summary\nquality, providing highly effective zero-shot strategies for precise length\ncontrol without the need for model fine-tuning or architectural changes. With\nour work, we not only advance our understanding of LLM behavior in controlled\ntext generation but also pave the way for more reliable and adaptable\nsummarization systems in real-world applications.", "published": "2024-12-31 02:53:27", "link": "http://arxiv.org/abs/2501.00233v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Have We Designed Generalizable Structural Knowledge Promptings?\n  Systematic Evaluation and Rethinking", "abstract": "Large language models (LLMs) have demonstrated exceptional performance in\ntext generation within current NLP research. However, the lack of factual\naccuracy is still a dark cloud hanging over the LLM skyscraper. Structural\nknowledge prompting (SKP) is a prominent paradigm to integrate external\nknowledge into LLMs by incorporating structural representations, achieving\nstate-of-the-art results in many knowledge-intensive tasks. However, existing\nmethods often focus on specific problems, lacking a comprehensive exploration\nof the generalization and capability boundaries of SKP. This paper aims to\nevaluate and rethink the generalization capability of the SKP paradigm from\nfour perspectives including Granularity, Transferability, Scalability, and\nUniversality. To provide a thorough evaluation, we introduce a novel\nmulti-granular, multi-level benchmark called SUBARU, consisting of 9 different\ntasks with varying levels of granularity and difficulty.", "published": "2024-12-31 03:20:22", "link": "http://arxiv.org/abs/2501.00244v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EQUATOR: A Deterministic Framework for Evaluating LLM Reasoning with\n  Open-Ended Questions. # v1.0.0-beta", "abstract": "Despite the remarkable coherence of Large Language Models (LLMs), existing\nevaluation methods often suffer from fluency bias and rely heavily on\nmultiple-choice formats, making it difficult to assess factual accuracy and\ncomplex reasoning effectively. LLMs thus frequently generate factually\ninaccurate responses, especially in complex reasoning tasks, highlighting two\nprominent challenges: (1) the inadequacy of existing methods to evaluate\nreasoning and factual accuracy effectively, and (2) the reliance on human\nevaluators for nuanced judgment, as illustrated by Williams and Huckle\n(2024)[1], who found manual grading indispensable despite automated grading\nadvancements.\n  To address evaluation gaps in open-ended reasoning tasks, we introduce the\nEQUATOR Evaluator (Evaluation of Question Answering Thoroughness in Open-ended\nReasoning). This framework combines deterministic scoring with a focus on\nfactual accuracy and robust reasoning assessment. Using a vector database,\nEQUATOR pairs open-ended questions with human-evaluated answers, enabling more\nprecise and scalable evaluations. In practice, EQUATOR significantly reduces\nreliance on human evaluators for scoring and improves scalability compared to\nWilliams and Huckle's (2004)[1] methods.\n  Our results demonstrate that this framework significantly outperforms\ntraditional multiple-choice evaluations while maintaining high accuracy\nstandards. Additionally, we introduce an automated evaluation process\nleveraging smaller, locally hosted LLMs. We used LLaMA 3.2B, running on the\nOllama binaries to streamline our assessments. This work establishes a new\nparadigm for evaluating LLM performance, emphasizing factual accuracy and\nreasoning ability, and provides a robust methodological foundation for future\nresearch.", "published": "2024-12-31 03:56:17", "link": "http://arxiv.org/abs/2501.00257v1", "categories": ["cs.CL", "68T20", "I.2.7; I.2.6; H.3.3"], "primary_category": "cs.CL"}
{"title": "A review of faithfulness metrics for hallucination assessment in Large\n  Language Models", "abstract": "This review examines the means with which faithfulness has been evaluated\nacross open-ended summarization, question-answering and machine translation\ntasks. We find that the use of LLMs as a faithfulness evaluator is commonly the\nmetric that is most highly correlated with human judgement. The means with\nwhich other studies have mitigated hallucinations is discussed, with both\nretrieval augmented generation (RAG) and prompting framework approaches having\nbeen linked with superior faithfulness, whilst other recommendations for\nmitigation are provided. Research into faithfulness is integral to the\ncontinued widespread use of LLMs, as unfaithful responses can pose major risks\nto many areas whereby LLMs would otherwise be suitable. Furthermore, evaluating\nopen-ended generation provides a more comprehensive measure of LLM performance\nthan commonly used multiple-choice benchmarking, which can help in advancing\nthe trust that can be placed within LLMs.", "published": "2024-12-31 04:41:13", "link": "http://arxiv.org/abs/2501.00269v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Echoes in AI: Quantifying Lack of Plot Diversity in LLM Outputs", "abstract": "With rapid advances in large language models (LLMs), there has been an\nincreasing application of LLMs in creative content ideation and generation. A\ncritical question emerges: can current LLMs provide ideas that are diverse\nenough to truly bolster the collective creativity? We examine two\nstate-of-the-art LLMs, GPT-4 and LLaMA-3, on story generation and discover that\nLLM-generated stories often consist of plot elements that are echoed across a\nnumber of generations. To quantify this phenomenon, we introduce the Sui\nGeneris score, which estimates how unlikely a plot element is to appear in\nalternative storylines generated by the same LLM. Evaluating on 100 short\nstories, we find that LLM-generated stories often contain combinations of\nidiosyncratic plot elements echoed frequently across generations, while the\noriginal human-written stories are rarely recreated or even echoed in pieces.\nMoreover, our human evaluation shows that the ranking of Sui Generis scores\namong story segments correlates moderately with human judgment of surprise\nlevel, even though score computation is completely automatic without relying on\nhuman judgment.", "published": "2024-12-31 04:54:48", "link": "http://arxiv.org/abs/2501.00273v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLM-Rubric: A Multidimensional, Calibrated Approach to Automated\n  Evaluation of Natural Language Texts", "abstract": "This paper introduces a framework for the automated evaluation of natural\nlanguage texts. A manually constructed rubric describes how to assess multiple\ndimensions of interest. To evaluate a text, a large language model (LLM) is\nprompted with each rubric question and produces a distribution over potential\nresponses. The LLM predictions often fail to agree well with human judges --\nindeed, the humans do not fully agree with one another. However, the multiple\nLLM distributions can be $\\textit{combined}$ to $\\textit{predict}$ each human\njudge's annotations on all questions, including a summary question that\nassesses overall quality or relevance. LLM-Rubric accomplishes this by training\na small feed-forward neural network that includes both judge-specific and\njudge-independent parameters. When evaluating dialogue systems in a human-AI\ninformation-seeking task, we find that LLM-Rubric with 9 questions (assessing\ndimensions such as naturalness, conciseness, and citation quality) predicts\nhuman judges' assessment of overall user satisfaction, on a scale of 1--4, with\nRMS error $< 0.5$, a $2\\times$ improvement over the uncalibrated baseline.", "published": "2024-12-31 04:57:01", "link": "http://arxiv.org/abs/2501.00274v1", "categories": ["cs.CL", "I.2.1; I.2.6; I.2.7"], "primary_category": "cs.CL"}
{"title": "MapEval: A Map-Based Evaluation of Geo-Spatial Reasoning in Foundation\n  Models", "abstract": "Recent advancements in foundation models have enhanced AI systems'\ncapabilities in autonomous tool usage and reasoning. However, their ability in\nlocation or map-based reasoning - which improves daily life by optimizing\nnavigation, facilitating resource discovery, and streamlining logistics - has\nnot been systematically studied. To bridge this gap, we introduce MapEval, a\nbenchmark designed to assess diverse and complex map-based user queries with\ngeo-spatial reasoning. MapEval features three task types (textual, API-based,\nand visual) that require collecting world information via map tools, processing\nheterogeneous geo-spatial contexts (e.g., named entities, travel distances,\nuser reviews or ratings, images), and compositional reasoning, which all\nstate-of-the-art foundation models find challenging. Comprising 700 unique\nmultiple-choice questions about locations across 180 cities and 54 countries,\nMapEval evaluates foundation models' ability to handle spatial relationships,\nmap infographics, travel planning, and navigation challenges. Using MapEval, we\nconducted a comprehensive evaluation of 28 prominent foundation models. While\nno single model excelled across all tasks, Claude-3.5-Sonnet, GPT-4o, and\nGemini-1.5-Pro achieved competitive performance overall. However, substantial\nperformance gaps emerged, particularly in MapEval, where agents with\nClaude-3.5-Sonnet outperformed GPT-4o and Gemini-1.5-Pro by 16% and 21%,\nrespectively, and the gaps became even more amplified when compared to\nopen-source LLMs. Our detailed analyses provide insights into the strengths and\nweaknesses of current models, though all models still fall short of human\nperformance by more than 20% on average, struggling with complex map images and\nrigorous geo-spatial reasoning. This gap highlights MapEval's critical role in\nadvancing general-purpose foundation models with stronger geo-spatial\nunderstanding.", "published": "2024-12-31 07:20:32", "link": "http://arxiv.org/abs/2501.00316v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing LLM Reasoning with Multi-Path Collaborative Reactive and\n  Reflection agents", "abstract": "Agents have demonstrated their potential in scientific reasoning tasks\nthrough large language models. However, they often face challenges such as\ninsufficient accuracy and degeneration of thought when handling complex\nreasoning tasks, which impede their performance. To overcome these issues, we\npropose the Reactive and Reflection agents with Multi-Path Reasoning (RR-MP)\nFramework, aimed at enhancing the reasoning capabilities of LLMs. Our approach\nimproves scientific reasoning accuracy by employing a multi-path reasoning\nmechanism where each path consists of a reactive agent and a reflection agent\nthat collaborate to prevent degeneration of thought inherent in single-agent\nreliance. Additionally, the RR-MP framework does not require additional\ntraining; it utilizes multiple dialogue instances for each reasoning path and a\nseparate summarizer to consolidate insights from all paths. This design\nintegrates diverse perspectives and strengthens reasoning across each path. We\nconducted zero-shot and few-shot evaluations on tasks involving moral\nscenarios, college-level physics, and mathematics. Experimental results\ndemonstrate that our method outperforms baseline approaches, highlighting the\neffectiveness and advantages of the RR-MP framework in managing complex\nscientific reasoning tasks.", "published": "2024-12-31 13:11:20", "link": "http://arxiv.org/abs/2501.00430v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sinhala Transliteration: A Comparative Analysis Between Rule-based and\n  Seq2Seq Approaches", "abstract": "Due to reasons of convenience and lack of tech literacy, transliteration\n(i.e., Romanizing native scripts instead of using localization tools) is\neminently prevalent in the context of low-resource languages such as Sinhala,\nwhich have their own writing script. In this study, our focus is on Romanized\nSinhala transliteration. We propose two methods to address this problem: Our\nbaseline is a rule-based method, which is then compared against our second\nmethod where we approach the transliteration problem as a sequence-to-sequence\ntask akin to the established Neural Machine Translation (NMT) task. For the\nlatter, we propose a Transformer-based Encode-Decoder solution. We witnessed\nthat the Transformer-based method could grab many ad-hoc patterns within the\nRomanized scripts compared to the rule-based method. The code base associated\nwith this paper is available on GitHub -\nhttps://github.com/kasunw22/Sinhala-Transliterator/", "published": "2024-12-31 16:27:47", "link": "http://arxiv.org/abs/2501.00529v1", "categories": ["cs.CL", "F.2.2, I.2.7 68T50"], "primary_category": "cs.CL"}
{"title": "KnowRA: Knowledge Retrieval Augmented Method for Document-level Relation\n  Extraction with Comprehensive Reasoning Abilities", "abstract": "Document-level relation extraction (Doc-RE) aims to extract relations between\nentities across multiple sentences. Therefore, Doc-RE requires more\ncomprehensive reasoning abilities like humans, involving complex cross-sentence\ninteractions between entities, contexts, and external general knowledge,\ncompared to the sentence-level RE. However, most existing Doc-RE methods focus\non optimizing single reasoning ability, but lack the ability to utilize\nexternal knowledge for comprehensive reasoning on long documents. To solve\nthese problems, a knowledge retrieval augmented method, named KnowRA, was\nproposed with comprehensive reasoning to autonomously determine whether to\naccept external knowledge to assist DocRE. Firstly, we constructed a document\ngraph for semantic encoding and integrated the co-reference resolution model to\naugment the co-reference reasoning ability. Then, we expanded the document\ngraph into a document knowledge graph by retrieving the external knowledge base\nfor common-sense reasoning and a novel knowledge filtration method was\npresented to filter out irrelevant knowledge. Finally, we proposed the axis\nattention mechanism to build direct and indirect associations with intermediary\nentities for achieving cross-sentence logical reasoning. Extensive experiments\nconducted on two datasets verified the effectiveness of our method compared to\nthe state-of-the-art baselines. Our code is available at\nhttps://anonymous.4open.science/r/KnowRA.", "published": "2024-12-31 17:58:36", "link": "http://arxiv.org/abs/2501.00571v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Setting Standards in Turkish NLP: TR-MMLU for Large Language Model\n  Evaluation", "abstract": "Language models have made remarkable advancements in understanding and\ngenerating human language, achieving notable success across a wide array of\napplications. However, evaluating these models remains a significant challenge,\nparticularly for resource-limited languages such as Turkish. To address this\ngap, we introduce the Turkish MMLU (TR-MMLU) benchmark, a comprehensive\nevaluation framework designed to assess the linguistic and conceptual\ncapabilities of large language models (LLMs) in Turkish. TR-MMLU is constructed\nfrom a carefully curated dataset comprising 6200 multiple-choice questions\nacross 62 sections, selected from a pool of 280000 questions spanning 67\ndisciplines and over 800 topics within the Turkish education system. This\nbenchmark provides a transparent, reproducible, and culturally relevant tool\nfor evaluating model performance. It serves as a standard framework for Turkish\nNLP research, enabling detailed analyses of LLMs' capabilities in processing\nTurkish text and fostering the development of more robust and accurate language\nmodels. In this study, we evaluate state-of-the-art LLMs on TR-MMLU, providing\ninsights into their strengths and limitations for Turkish-specific tasks. Our\nfindings reveal critical challenges, such as the impact of tokenization and\nfine-tuning strategies, and highlight areas for improvement in model design. By\nsetting a new standard for evaluating Turkish language models, TR-MMLU aims to\ninspire future innovations and support the advancement of Turkish NLP research.", "published": "2024-12-31 18:43:49", "link": "http://arxiv.org/abs/2501.00593v2", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "\"Dialogue\" vs \"Dialog\" in NLP and AI research: Statistics from a\n  Confused Discourse", "abstract": "Within computing research, there are two spellings for an increasingly\nimportant term - dialogue and dialog. We analyze thousands of research papers\nto understand this \"dialog(ue) debacle\". Among publications in top venues that\nuse \"dialog(ue)\" in the title or abstract, 72% use \"dialogue\", 24% use\n\"dialog\", and 5% use both in the same title and abstract. This split\ndistribution is more common in Computing than any other academic discipline. We\ninvestigate trends over ~20 years of NLP/AI research, not finding clear\nevidence of a shift over time. Author nationality is weakly correlated with\nspelling choice, but far from explains the mixed use. Many prolific authors\npublish papers with both spellings. We use several methods (such as syntactic\nparses and LM embeddings) to study how dialog(ue) context influences spelling,\nfinding limited influence. Combining these results together, we discuss\ndifferent theories that might explain the dialog(ue) divergence.", "published": "2024-12-31 18:56:33", "link": "http://arxiv.org/abs/2501.00598v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GPT-4 on Clinic Depression Assessment: An LLM-Based Pilot Study", "abstract": "Depression has impacted millions of people worldwide and has become one of\nthe most prevalent mental disorders. Early mental disorder detection can lead\nto cost savings for public health agencies and avoid the onset of other major\ncomorbidities. Additionally, the shortage of specialized personnel is a\ncritical issue because clinical depression diagnosis is highly dependent on\nexpert professionals and is time consuming.\n  In this study, we explore the use of GPT-4 for clinical depression assessment\nbased on transcript analysis. We examine the model's ability to classify\npatient interviews into binary categories: depressed and not depressed. A\ncomparative analysis is conducted considering prompt complexity (e.g., using\nboth simple and complex prompts) as well as varied temperature settings to\nassess the impact of prompt complexity and randomness on the model's\nperformance.\n  Results indicate that GPT-4 exhibits considerable variability in accuracy and\nF1-Score across configurations, with optimal performance observed at lower\ntemperature values (0.0-0.2) for complex prompts. However, beyond a certain\nthreshold (temperature >= 0.3), the relationship between randomness and\nperformance becomes unpredictable, diminishing the gains from prompt\ncomplexity.\n  These findings suggest that, while GPT-4 shows promise for clinical\nassessment, the configuration of the prompts and model parameters requires\ncareful calibration to ensure consistent results. This preliminary study\ncontributes to understanding the dynamics between prompt engineering and large\nlanguage models, offering insights for future development of AI-powered tools\nin clinical settings.", "published": "2024-12-31 00:32:43", "link": "http://arxiv.org/abs/2501.00199v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Empirical Evaluation of Large Language Models on Consumer Health\n  Questions", "abstract": "This study evaluates the performance of several Large Language Models (LLMs)\non MedRedQA, a dataset of consumer-based medical questions and answers by\nverified experts extracted from the AskDocs subreddit. While LLMs have shown\nproficiency in clinical question answering (QA) benchmarks, their effectiveness\non real-world, consumer-based, medical questions remains less understood.\nMedRedQA presents unique challenges, such as informal language and the need for\nprecise responses suited to non-specialist queries. To assess model\nperformance, responses were generated using five LLMs: GPT-4o mini, Llama 3.1:\n70B, Mistral-123B, Mistral-7B, and Gemini-Flash. A cross-evaluation method was\nused, where each model evaluated its responses as well as those of others to\nminimize bias. The results indicated that GPT-4o mini achieved the highest\nalignment with expert responses according to four out of the five models'\njudges, while Mistral-7B scored lowest according to three out of five models'\njudges. This study highlights the potential and limitations of current LLMs for\nconsumer health medical question answering, indicating avenues for further\ndevelopment.", "published": "2024-12-31 01:08:15", "link": "http://arxiv.org/abs/2501.00208v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Generative Emergent Communication: Large Language Model is a Collective\n  World Model", "abstract": "This study proposes a unifying theoretical framework called generative\nemergent communication (generative EmCom) that bridges emergent communication,\nworld models, and large language models (LLMs) through the lens of collective\npredictive coding (CPC). The proposed framework formalizes the emergence of\nlanguage and symbol systems through decentralized Bayesian inference across\nmultiple agents, extending beyond conventional discriminative model-based\napproaches to emergent communication. This study makes the following two key\ncontributions: First, we propose generative EmCom as a novel framework for\nunderstanding emergent communication, demonstrating how communication emergence\nin multi-agent reinforcement learning (MARL) can be derived from control as\ninference while clarifying its relationship to conventional discriminative\napproaches. Second, we propose a mathematical formulation showing the\ninterpretation of LLMs as collective world models that integrate multiple\nagents' experiences through CPC. The framework provides a unified theoretical\nfoundation for understanding how shared symbol systems emerge through\ncollective predictive coding processes, bridging individual cognitive\ndevelopment and societal language evolution. Through mathematical formulations\nand discussion on prior works, we demonstrate how this framework explains\nfundamental aspects of language emergence and offers practical insights for\nunderstanding LLMs and developing sophisticated AI systems for improving\nhuman-AI interaction and multi-agent systems.", "published": "2024-12-31 02:23:10", "link": "http://arxiv.org/abs/2501.00226v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Exploring Variability in Fine-Tuned Models for Text Classification with\n  DistilBERT", "abstract": "This study evaluates fine-tuning strategies for text classification using the\nDistilBERT model, specifically the\ndistilbert-base-uncased-finetuned-sst-2-english variant. Through structured\nexperiments, we examine the influence of hyperparameters such as learning rate,\nbatch size, and epochs on accuracy, F1-score, and loss. Polynomial regression\nanalyses capture foundational and incremental impacts of these hyperparameters,\nfocusing on fine-tuning adjustments relative to a baseline model.\n  Results reveal variability in metrics due to hyperparameter configurations,\nshowing trade-offs among performance metrics. For example, a higher learning\nrate reduces loss in relative analysis (p=0.027) but challenges accuracy\nimprovements. Meanwhile, batch size significantly impacts accuracy and F1-score\nin absolute regression (p=0.028 and p=0.005) but has limited influence on loss\noptimization (p=0.170). The interaction between epochs and batch size maximizes\nF1-score (p=0.001), underscoring the importance of hyperparameter interplay.\n  These findings highlight the need for fine-tuning strategies addressing\nnon-linear hyperparameter interactions to balance performance across metrics.\nSuch variability and metric trade-offs are relevant for tasks beyond text\nclassification, including NLP and computer vision. This analysis informs\nfine-tuning strategies for large language models and promotes adaptive designs\nfor broader model applicability.", "published": "2024-12-31 03:16:15", "link": "http://arxiv.org/abs/2501.00241v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Automatically Planning Optimal Parallel Strategy for Large Language\n  Models", "abstract": "The number of parameters in large-scale language models based on transformers\nis gradually increasing, and the scale of computing clusters is also growing.\nThe technology of quickly mobilizing large amounts of computing resources for\nparallel computing is becoming increasingly important. In this paper, we\npropose an automatic parallel algorithm that automatically plans the parallel\nstrategy with maximum throughput based on model and hardware information. By\ndecoupling the training time into computation, communication, and overlap, we\nestablished a training duration simulation model. Based on this simulation\nmodel, we prune the parallel solution space to shorten the search time\nrequired. The multi-node experiment results show that the algorithm can\nestimate the parallel training duration in real time with an average accuracy\nof 96%. In our test, the recommendation strategy provided by the algorithm is\nalways globally optimal.", "published": "2024-12-31 03:51:14", "link": "http://arxiv.org/abs/2501.00254v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "MAIN-RAG: Multi-Agent Filtering Retrieval-Augmented Generation", "abstract": "Large Language Models (LLMs) are becoming essential tools for various natural\nlanguage processing tasks but often suffer from generating outdated or\nincorrect information. Retrieval-Augmented Generation (RAG) addresses this\nissue by incorporating external, real-time information retrieval to ground LLM\nresponses. However, the existing RAG systems frequently struggle with the\nquality of retrieval documents, as irrelevant or noisy documents degrade\nperformance, increase computational overhead, and undermine response\nreliability. To tackle this problem, we propose Multi-Agent Filtering\nRetrieval-Augmented Generation (MAIN-RAG), a training-free RAG framework that\nleverages multiple LLM agents to collaboratively filter and score retrieved\ndocuments. Specifically, MAIN-RAG introduces an adaptive filtering mechanism\nthat dynamically adjusts the relevance filtering threshold based on score\ndistributions, effectively minimizing noise while maintaining high recall of\nrelevant documents. The proposed approach leverages inter-agent consensus to\nensure robust document selection without requiring additional training data or\nfine-tuning. Experimental results across four QA benchmarks demonstrate that\nMAIN-RAG consistently outperforms traditional RAG approaches, achieving a 2-11%\nimprovement in answer accuracy while reducing the number of irrelevant\nretrieved documents. Quantitative analysis further reveals that our approach\nachieves superior response consistency and answer accuracy over baseline\nmethods, offering a competitive and practical alternative to training-based\nsolutions.", "published": "2024-12-31 08:07:26", "link": "http://arxiv.org/abs/2501.00332v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Loss-Aware Curriculum Learning for Chinese Grammatical Error Correction", "abstract": "Chinese grammatical error correction (CGEC) aims to detect and correct errors\nin the input Chinese sentences. Recently, Pre-trained Language Models (PLMS)\nhave been employed to improve the performance. However, current approaches\nignore that correction difficulty varies across different instances and treat\nthese samples equally, enhancing the challenge of model learning. To address\nthis problem, we propose a multi-granularity Curriculum Learning (CL)\nframework. Specifically, we first calculate the correction difficulty of these\nsamples and feed them into the model from easy to hard batch by batch. Then\nInstance-Level CL is employed to help the model optimize in the appropriate\ndirection automatically by regulating the loss function. Extensive experimental\nresults and comprehensive analyses of various datasets prove the effectiveness\nof our method.", "published": "2024-12-31 08:11:49", "link": "http://arxiv.org/abs/2501.00334v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Rethinking Layer Removal: A Hybrid Pruning Framework Combining Layer\n  Removal and Singular Value Selection for Efficient LLM Compression", "abstract": "Layer removal is an effective technique for compressing large language models\n(LLMs) by reducing redundancy and improving inference efficiency. However,\nindiscriminate pruning disrupts representation stability, leading to\nperformance degradation. We propose GRASP (Gradient-based Retention of Adaptive\nSingular Parameters), which preserves representation-critical singular values\nto mitigate these effects. Unlike direct layer removal, GRASP leverages\ngradient-based attribution on a syntax- and semantics-rich dataset to guide the\nselection of representation-critical singular values. By selectively applying\nsingular value decomposition (SVD) to affected layers, GRASP achieves efficient\ncompression while maintaining representation stability with minimal overhead.\nExperiments across multiple LLMs show that GRASP consistently outperforms\nexisting compression methods in perplexity and downstream task performance.", "published": "2024-12-31 08:22:21", "link": "http://arxiv.org/abs/2501.00339v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Chunk-Distilled Language Modeling", "abstract": "We introduce Chunk-Distilled Language Modeling (CD-LM), an approach to text\ngeneration that addresses two challenges in current large language models\n(LLMs): the inefficiency of token-level generation, and the difficulty of\nadapting to new data and knowledge. Our method combines deep network-based LLMs\nwith a straightforward retrieval module, which allows the generation of\nmulti-token text chunks at a single decoding step. Our retrieval framework\nenables flexible construction of model- or domain-specific datastores, either\nleveraging the internal knowledge of existing models, or incorporating expert\ninsights from human-annotated corpora. This adaptability allows for enhanced\ncontrol over the language model's distribution without necessitating additional\ntraining. We present the CD-LM formulation along with performance metrics\ndemonstrating its ability to improve language model performance and efficiency\nacross a diverse set of downstream tasks. Code and data will be made publicly\navailable.", "published": "2024-12-31 08:32:15", "link": "http://arxiv.org/abs/2501.00343v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Trajectories of Change: Approaches for Tracking Knowledge Evolution", "abstract": "We explore local vs. global evolution of knowledge systems through the\nframework of socio-epistemic networks (SEN), applying two complementary methods\nto a corpus of scientific texts. The framework comprises three interconnected\nlayers-social, semiotic (material), and semantic-proposing a multilayered\napproach to understanding structural developments of knowledge. To analyse\ndiachronic changes on the semantic layer, we first use information-theoretic\nmeasures based on relative entropy to detect semantic shifts, assess their\nsignificance, and identify key driving features. Second, variations in document\nembedding densities reveal changes in semantic neighbourhoods, tracking how\nconcentration of similar documents increase, remain stable, or disperse. This\nenables us to trace document trajectories based on content (topics) or metadata\n(authorship, institution). Case studies of Joseph Silk and Hans-J\\\"urgen Treder\nillustrate how individual scholar's work aligns with broader disciplinary\nshifts in general relativity and gravitation research, demonstrating the\napplications, limitations, and further potential of this approach.", "published": "2024-12-31 11:09:37", "link": "http://arxiv.org/abs/2501.00391v1", "categories": ["cs.CL", "physics.hist-ph"], "primary_category": "cs.CL"}
{"title": "Two Cases of Deduction with Non-referring Descriptions", "abstract": "Formal reasoning with non-denoting terms, esp. non-referring descriptions\nsuch as \"the King of France\", is still an under-investigated area. The recent\nexception being a series of papers e.g. by Indrzejczak, Zawidzki and K\\\"rbis.\nThe present paper offers an alternative to their approach since instead of free\nlogic and sequent calculus, it's framed in partial type theory with natural\ndeduction in sequent style. Using a Montague- and Tich\\'y-style formalization\nof natural language, the paper successfully handles deduction with intensional\ntransitives whose complements are non-referring descriptions, and derives\nStrawsonian rules for existential presuppositions of sentences with such\ndescriptions.", "published": "2024-12-31 15:11:47", "link": "http://arxiv.org/abs/2501.00485v1", "categories": ["cs.LO", "cs.CL"], "primary_category": "cs.LO"}
{"title": "TinyHelen's First Curriculum: Training and Evaluating Tiny Language\n  Models in a Simpler Language Environment", "abstract": "Training language models (LMs) and their application agents is increasingly\ncostly due to large datasets and models, making test failures difficult to\nbear. Simplified language environments serve as primordial training and testing\ngrounds, retaining essential commonsense and communication skills but in a more\ndigestible form, potentially enhancing the learning efficiency of LMs, and thus\nreducing the required model size and data volume for effective training and\nevaluation. In these simplified language environments, workable strategies for\nsmall models, datasets, and agents may be adaptable to larger models, datasets,\nand agents in complex language environments.\n  To create such environments, we focus on two aspects: i) minimizing language\ndataset noise and complexity, and ii) preserving the essential text\ndistribution characteristics. Unlike previous methods, we propose a pipeline to\nrefine text data by eliminating noise, minimizing vocabulary, and maintaining\ngenre-specific patterns (e.g., for books, conversation, code, etc.).\nImplementing this pipeline with large LMs, we have created a leaner suite of LM\ntraining and evaluation datasets: 71M Leaner-Pretrain, 7M Leaner-Instruct,\nLeaner-Glue for assessing linguistic proficiency, and Leaner-Eval for testing\ninstruction-following ability.\n  Our experiments show that leaner pre-training boosts LM learning efficiency.\nTiny LMs trained on these datasets outperform those trained on original\ndatasets in instruction-following across different language granularity levels.\nMoreover, the Leaner-Pretrain dataset's alignment with conventional large LM\ntraining sets enables resource-optimized analysis of how learning objectives,\nmodel architectures, and training techniques impact performance on language\nmodeling and downstream tasks. Our code and datasets are available at\nhttps://github.com/EmpathYang/TinyHelen.git.", "published": "2024-12-31 16:08:15", "link": "http://arxiv.org/abs/2501.00522v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Superposition in Transformers: A Novel Way of Building Mixture of\n  Experts", "abstract": "Catastrophic forgetting remains a major challenge when adapting large\nlanguage models (LLMs) to new tasks or domains. Conventional fine-tuning often\noverwrites existing knowledge, causing performance degradation on original\ntasks. We introduce Superposition in Transformers, a novel architecture that\nleverages autoencoders to superimpose the hidden representations of a base\nmodel and a fine-tuned model within a shared parameter space. By using\nB-spline-based blending coefficients and autoencoders that adaptively\nreconstruct hidden states based on the input data distribution, our method\neffectively mitigates catastrophic forgetting and enables a new paradigm of\n\"in-model\" superposition. This approach preserves original model capabilities\nwhile allowing compact domain-specific expertise to be added, and it supports\ndynamic switching between model states during inference.", "published": "2024-12-31 16:28:23", "link": "http://arxiv.org/abs/2501.00530v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AraSTEM: A Native Arabic Multiple Choice Question Benchmark for\n  Evaluating LLMs Knowledge In STEM Subjects", "abstract": "Large Language Models (LLMs) have shown remarkable capabilities, not only in\ngenerating human-like text, but also in acquiring knowledge. This highlights\nthe need to go beyond the typical Natural Language Processing downstream\nbenchmarks and asses the various aspects of LLMs including knowledge and\nreasoning. Numerous benchmarks have been developed to evaluate LLMs knowledge,\nbut they predominantly focus on the English language. Given that many LLMs are\nmultilingual, relying solely on benchmarking English knowledge is insufficient.\nTo address this issue, we introduce AraSTEM, a new Arabic multiple-choice\nquestion dataset aimed at evaluating LLMs knowledge in STEM subjects. The\ndataset spans a range of topics at different levels which requires models to\ndemonstrate a deep understanding of scientific Arabic in order to achieve high\naccuracy. Our findings show that publicly available models of varying sizes\nstruggle with this dataset, and underscores the need for more localized\nlanguage models. The dataset is freely accessible on Hugging Face.", "published": "2024-12-31 17:45:12", "link": "http://arxiv.org/abs/2501.00559v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Overview and Discussion on Using Large Language Models for\n  Implementation Generation of Solutions to Open-Ended Problems", "abstract": "Large Language Models offer new opportunities to devise automated\nimplementation generation methods that can tackle problem solving activities\nbeyond traditional methods, which require algorithmic specifications and can\nuse only static domain knowledge, like performance metrics and libraries of\nbasic building blocks. Large Language Models could support creating new methods\nto support problem solving activities for open-ended problems, like problem\nframing, exploring possible solving approaches, feature elaboration and\ncombination, more advanced implementation assessment, and handling unexpected\nsituations. This report summarized the current work on Large Language Models,\nincluding model prompting, Reinforcement Learning, and Retrieval-Augmented\nGeneration. Future research requirements were also discussed.", "published": "2024-12-31 17:48:33", "link": "http://arxiv.org/abs/2501.00562v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Optimizing Speech-Input Length for Speaker-Independent Depression\n  Classification", "abstract": "Machine learning models for speech-based depression classification offer\npromise for health care applications. Despite growing work on depression\nclassification, little is understood about how the length of speech-input\nimpacts model performance. We analyze results for speaker-independent\ndepression classification using a corpus of over 1400 hours of speech from a\nhuman-machine health screening application. We examine performance as a\nfunction of response input length for two NLP systems that differ in overall\nperformance.\n  Results for both systems show that performance depends on natural length,\nelapsed length, and ordering of the response within a session. Systems share a\nminimum length threshold, but differ in a response saturation threshold, with\nthe latter higher for the better system. At saturation it is better to pose a\nnew question to the speaker, than to continue the current response. These and\nadditional reported results suggest how applications can be better designed to\nboth elicit and process optimal input lengths for depression classification.", "published": "2024-12-31 19:12:15", "link": "http://arxiv.org/abs/2501.00608v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Efficient Standardization of Clinical Notes using Large Language Models", "abstract": "Clinician notes are a rich source of patient information but often contain\ninconsistencies due to varied writing styles, colloquialisms, abbreviations,\nmedical jargon, grammatical errors, and non-standard formatting. These\ninconsistencies hinder the extraction of meaningful data from electronic health\nrecords (EHRs), posing challenges for quality improvement, population health,\nprecision medicine, decision support, and research.\n  We present a large language model approach to standardizing a corpus of 1,618\nclinical notes. Standardization corrected an average of $4.9 +/- 1.8$\ngrammatical errors, $3.3 +/- 5.2$ spelling errors, converted $3.1 +/- 3.0$\nnon-standard terms to standard terminology, and expanded $15.8 +/- 9.1$\nabbreviations and acronyms per note. Additionally, notes were re-organized into\ncanonical sections with standardized headings. This process prepared notes for\nkey concept extraction, mapping to medical ontologies, and conversion to\ninteroperable data formats such as FHIR.\n  Expert review of randomly sampled notes found no significant data loss after\nstandardization. This proof-of-concept study demonstrates that standardization\nof clinical notes can improve their readability, consistency, and usability,\nwhile also facilitating their conversion into interoperable data formats.", "published": "2024-12-31 20:52:40", "link": "http://arxiv.org/abs/2501.00644v1", "categories": ["cs.CL", "cs.AI", "92", "J.3; I.2"], "primary_category": "cs.CL"}
{"title": "2 OLMo 2 Furious", "abstract": "We present OLMo 2, the next generation of our fully open language models.\nOLMo 2 includes dense autoregressive models with improved architecture and\ntraining recipe, pretraining data mixtures, and instruction tuning recipes. Our\nmodified model architecture and training recipe achieve both better training\nstability and improved per-token efficiency. Our updated pretraining data\nmixture introduces a new, specialized data mix called Dolmino Mix 1124, which\nsignificantly improves model capabilities across many downstream task\nbenchmarks when introduced via late-stage curriculum training (i.e. specialized\ndata during the annealing phase of pretraining). Finally, we incorporate best\npractices from T\\\"ulu 3 to develop OLMo 2-Instruct, focusing on permissive data\nand extending our final-stage reinforcement learning with verifiable rewards\n(RLVR). Our OLMo 2 base models sit at the Pareto frontier of performance to\ncompute, often matching or outperforming open-weight only models like Llama 3.1\nand Qwen 2.5 while using fewer FLOPs and with fully transparent training data,\ncode, and recipe. Our fully open OLMo 2-Instruct models are competitive with or\nsurpassing open-weight only models of comparable size, including Qwen 2.5,\nLlama 3.1 and Gemma 2. We release all OLMo 2 artifacts openly -- models at 7B\nand 13B scales, both pretrained and post-trained, including their full training\ndata, training code and recipes, training logs and thousands of intermediate\ncheckpoints. The final instruction model is available on the Ai2 Playground as\na free research demo.", "published": "2024-12-31 21:55:10", "link": "http://arxiv.org/abs/2501.00656v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Why Are Positional Encodings Nonessential for Deep Autoregressive\n  Transformers? Revisiting a Petroglyph", "abstract": "Do autoregressive Transformer language models require explicit positional\nencodings (PEs)? The answer is \"no\" as long as they have more than one layer --\nthey can distinguish sequences with permuted tokens without requiring explicit\nPEs. This property has been known since early efforts (those contemporary with\nGPT-2) adopting the Transformer for language modeling. However, this result\ndoes not appear to have been well disseminated and was even rediscovered\nrecently. This may be partially due to a sudden growth of the language modeling\ncommunity after the advent of GPT-2, but perhaps also due to the lack of a\nclear explanation in prior publications, despite being commonly understood by\npractitioners in the past. Here we review this long-forgotten explanation why\nexplicit PEs are nonessential for multi-layer autoregressive Transformers (in\ncontrast, one-layer models require PEs to discern order information of their\ninput tokens). We also review the origin of this result, and hope to\nre-establish it as a common knowledge.", "published": "2024-12-31 22:12:45", "link": "http://arxiv.org/abs/2501.00659v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "MLLM-as-a-Judge for Image Safety without Human Labeling", "abstract": "Image content safety has become a significant challenge with the rise of\nvisual media on online platforms. Meanwhile, in the age of AI-generated content\n(AIGC), many image generation models are capable of producing harmful content,\nsuch as images containing sexual or violent material. Thus, it becomes crucial\nto identify such unsafe images based on established safety rules. Pre-trained\nMultimodal Large Language Models (MLLMs) offer potential in this regard, given\ntheir strong pattern recognition abilities. Existing approaches typically\nfine-tune MLLMs with human-labeled datasets, which however brings a series of\ndrawbacks. First, relying on human annotators to label data following intricate\nand detailed guidelines is both expensive and labor-intensive. Furthermore,\nusers of safety judgment systems may need to frequently update safety rules,\nmaking fine-tuning on human-based annotation more challenging. This raises the\nresearch question: Can we detect unsafe images by querying MLLMs in a zero-shot\nsetting using a predefined safety constitution (a set of safety rules)? Our\nresearch showed that simply querying pre-trained MLLMs does not yield\nsatisfactory results. This lack of effectiveness stems from factors such as the\nsubjectivity of safety rules, the complexity of lengthy constitutions, and the\ninherent biases in the models. To address these challenges, we propose a\nMLLM-based method includes objectifying safety rules, assessing the relevance\nbetween rules and images, making quick judgments based on debiased token\nprobabilities with logically complete yet simplified precondition chains for\nsafety rules, and conducting more in-depth reasoning with cascaded\nchain-of-thought processes if necessary. Experiment results demonstrate that\nour method is highly effective for zero-shot image safety judgment tasks.", "published": "2024-12-31 00:06:04", "link": "http://arxiv.org/abs/2501.00192v2", "categories": ["cs.CV", "cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Extracting effective solutions hidden in large language models via\n  generated comprehensive specialists: case studies in developing electronic\n  devices", "abstract": "Recently, many studies have increasingly explored the use of large language\nmodels (LLMs) to generate research ideas and scientific hypotheses. However,\nreal-world research and development often require solving complex,\ninterdisciplinary challenges where solutions may not be readily found through\nexisting knowledge related to the problem. Therefore, it is desirable to\nleverage the vast, comprehensive knowledge of LLMs to generate effective,\nbreakthrough solutions by integrating various perspectives from other\ndisciplines. Here, we propose SELLM (Solution Enumeration via comprehensive\nList and LLM), a framework leveraging LLMs and structured guidance using MECE\n(Mutually Exclusive, Collectively Exhaustive) principles, such as International\nPatent Classification (IPC) and the periodic table of elements. SELLM\nsystematically constructs comprehensive expert agents from the list to generate\ncross-disciplinary and effective solutions. To evaluate SELLM's practicality,\nwe applied it to two challenges: improving light extraction in organic\nlight-emitting diode (OLED) lighting and developing electrodes for\nnext-generation memory materials. The results demonstrate that SELLM\nsignificantly facilitates the generation of effective solutions compared to\ncases without specific customization or effort, showcasing the potential of\nSELLM to enable LLMs to generate effective solutions even for challenging\nproblems.", "published": "2024-12-31 02:20:56", "link": "http://arxiv.org/abs/2501.00224v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Retrieval-Augmented Generation with Graphs (GraphRAG)", "abstract": "Retrieval-augmented generation (RAG) is a powerful technique that enhances\ndownstream task execution by retrieving additional information, such as\nknowledge, skills, and tools from external sources. Graph, by its intrinsic\n\"nodes connected by edges\" nature, encodes massive heterogeneous and relational\ninformation, making it a golden resource for RAG in tremendous real-world\napplications. As a result, we have recently witnessed increasing attention on\nequipping RAG with Graph, i.e., GraphRAG. However, unlike conventional RAG,\nwhere the retriever, generator, and external data sources can be uniformly\ndesigned in the neural-embedding space, the uniqueness of graph-structured\ndata, such as diverse-formatted and domain-specific relational knowledge, poses\nunique and significant challenges when designing GraphRAG for different\ndomains. Given the broad applicability, the associated design challenges, and\nthe recent surge in GraphRAG, a systematic and up-to-date survey of its key\nconcepts and techniques is urgently desired. Following this motivation, we\npresent a comprehensive and up-to-date survey on GraphRAG. Our survey first\nproposes a holistic GraphRAG framework by defining its key components,\nincluding query processor, retriever, organizer, generator, and data source.\nFurthermore, recognizing that graphs in different domains exhibit distinct\nrelational patterns and require dedicated designs, we review GraphRAG\ntechniques uniquely tailored to each domain. Finally, we discuss research\nchallenges and brainstorm directions to inspire cross-disciplinary\nopportunities. Our survey repository is publicly maintained at\nhttps://github.com/Graph-RAG/GraphRAG/.", "published": "2024-12-31 06:59:35", "link": "http://arxiv.org/abs/2501.00309v2", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "VoxVietnam: a Large-Scale Multi-Genre Dataset for Vietnamese Speaker\n  Recognition", "abstract": "Recent research in speaker recognition aims to address vulnerabilities due to\nvariations between enrolment and test utterances, particularly in the\nmulti-genre phenomenon where the utterances are in different speech genres.\nPrevious resources for Vietnamese speaker recognition are either limited in\nsize or do not focus on genre diversity, leaving studies in multi-genre effects\nunexplored. This paper introduces VoxVietnam, the first multi-genre dataset for\nVietnamese speaker recognition with over 187,000 utterances from 1,406 speakers\nand an automated pipeline to construct a dataset on a large scale from public\nsources. Our experiments show the challenges posed by the multi-genre\nphenomenon to models trained on a single-genre dataset, and demonstrate a\nsignificant increase in performance upon incorporating the VoxVietnam into the\ntraining process. Our experiments are conducted to study the challenges of the\nmulti-genre phenomenon in speaker recognition and the performance gain when the\nproposed dataset is used for multi-genre training.", "published": "2024-12-31 07:57:29", "link": "http://arxiv.org/abs/2501.00328v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Exploring the Implicit Semantic Ability of Multimodal Large Language\n  Models: A Pilot Study on Entity Set Expansion", "abstract": "The rapid development of multimodal large language models (MLLMs) has brought\nsignificant improvements to a wide range of tasks in real-world applications.\nHowever, LLMs still exhibit certain limitations in extracting implicit semantic\ninformation. In this paper, we apply MLLMs to the Multi-modal Entity Set\nExpansion (MESE) task, which aims to expand a handful of seed entities with new\nentities belonging to the same semantic class, and multi-modal information is\nprovided with each entity. We explore the capabilities of MLLMs to understand\nimplicit semantic information at the entity-level granularity through the MESE\ntask, introducing a listwise ranking method LUSAR that maps local scores to\nglobal rankings. Our LUSAR demonstrates significant improvements in MLLM's\nperformance on the MESE task, marking the first use of generative MLLM for ESE\ntasks and extending the applicability of listwise ranking.", "published": "2024-12-31 08:03:48", "link": "http://arxiv.org/abs/2501.00330v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "RAG-Instruct: Boosting LLMs with Diverse Retrieval-Augmented\n  Instructions", "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a key paradigm for\nenhancing large language models (LLMs) by incorporating external knowledge.\nHowever, current RAG methods face two limitations: (1) they only cover limited\nRAG scenarios. (2) They suffer from limited task diversity due to the lack of a\ngeneral RAG dataset. To address these limitations, we propose RAG-Instruct, a\ngeneral method for synthesizing diverse and high-quality RAG instruction data\nbased on any source corpus. Our approach leverages (1) five RAG paradigms,\nwhich encompass diverse query-document relationships, and (2) instruction\nsimulation, which enhances instruction diversity and quality by utilizing the\nstrengths of existing instruction datasets. Using this method, we construct a\n40K instruction dataset from Wikipedia, comprehensively covering diverse RAG\nscenarios and tasks. Experiments demonstrate that RAG-Instruct effectively\nenhances LLMs' RAG capabilities, achieving strong zero-shot performance and\nsignificantly outperforming various RAG baselines across a diverse set of\ntasks. RAG-Instruct is publicly available at\nhttps://github.com/FreedomIntelligence/RAG-Instruct.", "published": "2024-12-31 09:00:51", "link": "http://arxiv.org/abs/2501.00353v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Efficient Relational Context Perception for Knowledge Graph Completion", "abstract": "Knowledge Graphs (KGs) provide a structured representation of knowledge but\noften suffer from challenges of incompleteness. To address this, link\nprediction or knowledge graph completion (KGC) aims to infer missing new facts\nbased on existing facts in KGs. Previous knowledge graph embedding models are\nlimited in their ability to capture expressive features, especially when\ncompared to deeper, multi-layer models. These approaches also assign a single\nstatic embedding to each entity and relation, disregarding the fact that\nentities and relations can exhibit different behaviors in varying graph\ncontexts. Due to complex context over a fact triple of a KG, existing methods\nhave to leverage complex non-linear context encoder, like transformer, to\nproject entity and relation into low dimensional representations, resulting in\nhigh computation cost. To overcome these limitations, we propose Triple\nReceptance Perception (TRP) architecture to model sequential information,\nenabling the learning of dynamic context of entities and relations. Then we use\ntensor decomposition to calculate triple scores, providing robust relational\ndecoding capabilities. This integration allows for more expressive\nrepresentations. Experiments on benchmark datasets such as YAGO3-10, UMLS,\nFB15k, and FB13 in link prediction and triple classification tasks demonstrate\nthat our method performs better than several state-of-the-art models, proving\nthe effectiveness of the integration.", "published": "2024-12-31 11:25:58", "link": "http://arxiv.org/abs/2501.00397v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Whisper Turns Stronger: Augmenting Wav2Vec 2.0 for Superior ASR in\n  Low-Resource Languages", "abstract": "Approaching Speech-to-Text and Automatic Speech Recognition problems in\nlow-resource languages is notoriously challenging due to the scarcity of\nvalidated datasets and the diversity of dialects. Arabic, Russian, and\nPortuguese exemplify these difficulties, being low-resource languages due to\nthe many dialects of these languages across different continents worldwide.\nMoreover, the variety of accents and pronunciations of such languages\ncomplicate ASR models' success. With the increasing popularity of Deep Learning\nand Transformers, acoustic models like the renowned Wav2Vec2 have achieved\nsuperior performance in the Speech Recognition field compared to\nstate-of-the-art approaches. However, despite Wav2Vec2's improved efficiency\nover traditional methods, its performance significantly declines for\nunder-represented languages, even though it requires significantly less labeled\ndata. This paper introduces an end-to-end framework that enhances ASR systems\nfine-tuned on Wav2Vec2 through data augmentation techniques. To validate our\nframework's effectiveness, we conducted a detailed experimental evaluation\nusing three datasets from Mozilla's Common Voice project in Arabic, Russian,\nand Portuguese. Additionally, the framework presented in this paper\ndemonstrates robustness to different diacritics. Ultimately, our approach\noutperforms two previous baseline models, which are the pre-trained Wav2Vec2\nand the well-known Whisper ASR model, resulting in an average relative\nimprovement of 33.9\\% in Word Error Rate and a 53.2\\% relative improvement in\nCharacter Error Rate.", "published": "2024-12-31 13:03:20", "link": "http://arxiv.org/abs/2501.00425v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Unleashing Text-to-Image Diffusion Prior for Zero-Shot Image Captioning", "abstract": "Recently, zero-shot image captioning has gained increasing attention, where\nonly text data is available for training. The remarkable progress in\ntext-to-image diffusion model presents the potential to resolve this task by\nemploying synthetic image-caption pairs generated by this pre-trained prior.\nNonetheless, the defective details in the salient regions of the synthetic\nimages introduce semantic misalignment between the synthetic image and text,\nleading to compromised results. To address this challenge, we propose a novel\nPatch-wise Cross-modal feature Mix-up (PCM) mechanism to adaptively mitigate\nthe unfaithful contents in a fine-grained manner during training, which can be\nintegrated into most of encoder-decoder frameworks, introducing our PCM-Net.\nSpecifically, for each input image, salient visual concepts in the image are\nfirst detected considering the image-text similarity in CLIP space. Next, the\npatch-wise visual features of the input image are selectively fused with the\ntextual features of the salient visual concepts, leading to a mixed-up feature\nmap with less defective content. Finally, a visual-semantic encoder is\nexploited to refine the derived feature map, which is further incorporated into\nthe sentence decoder for caption generation. Additionally, to facilitate the\nmodel training with synthetic data, a novel CLIP-weighted cross-entropy loss is\ndevised to prioritize the high-quality image-text pairs over the low-quality\ncounterparts. Extensive experiments on MSCOCO and Flickr30k datasets\ndemonstrate the superiority of our PCM-Net compared with state-of-the-art\nVLMs-based approaches. It is noteworthy that our PCM-Net ranks first in both\nin-domain and cross-domain zero-shot image captioning. The synthetic dataset\nSynthImgCap and code are available at https://jianjieluo.github.io/SynthImgCap.", "published": "2024-12-31 13:39:08", "link": "http://arxiv.org/abs/2501.00437v1", "categories": ["cs.CV", "cs.CL", "cs.LG", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Differentiable Prompt Learning for Vision Language Models", "abstract": "Prompt learning is an effective way to exploit the potential of large-scale\npre-trained foundational models. Continuous prompts parameterize context tokens\nin prompts by turning them into differentiable vectors. Deep continuous prompts\ninsert prompts not only in the input but also in the intermediate hidden\nrepresentations. Manually designed deep continuous prompts exhibit a remarkable\nimprovement compared to the zero-shot pre-trained model on downstream tasks.\nHow to automate the continuous prompt design is an underexplored area, and a\nfundamental question arises, is manually designed deep prompt strategy optimal?\nTo answer this question, we propose a method dubbed differentiable prompt\nlearning (DPL). The DPL method is formulated as an optimization problem to\nautomatically determine the optimal context length of the prompt to be added to\neach layer, where the objective is to maximize the performance. We test the DPL\nmethod on the pre-trained CLIP. We empirically find that by using only limited\ndata, our DPL method can find deep continuous prompt configuration with high\nconfidence. The performance on the downstream tasks exhibits the superiority of\nthe automatic design: our method boosts the average test accuracy by 2.60% on\n11 datasets compared to baseline methods. Besides, our method focuses only on\nthe prompt configuration (i.e. context length for each layer), which means that\nour method is compatible with the baseline methods that have sophisticated\ndesigns to boost the performance. The DPL method can be deployed to large\nlanguage models or computer vision models at no cost.", "published": "2024-12-31 14:13:28", "link": "http://arxiv.org/abs/2501.00457v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Fotheidil: an Automatic Transcription System for the Irish Language", "abstract": "This paper sets out the first web-based transcription system for the Irish\nlanguage - Fotheidil, a system that utilises speech-related AI technologies as\npart of the ABAIR initiative. The system includes both off-the-shelf\npre-trained voice activity detection and speaker diarisation models and models\ntrained specifically for Irish automatic speech recognition and capitalisation\nand punctuation restoration. Semi-supervised learning is explored to improve\nthe acoustic model of a modular TDNN-HMM ASR system, yielding substantial\nimprovements for out-of-domain test sets and dialects that are underrepresented\nin the supervised training set. A novel approach to capitalisation and\npunctuation restoration involving sequence-to-sequence models is compared with\nthe conventional approach using a classification model. Experimental results\nshow here also substantial improvements in performance. The system will be made\nfreely available for public use, and represents an important resource to\nresearchers and others who transcribe Irish language materials. Human-corrected\ntranscriptions will be collected and included in the training dataset as the\nsystem is used, which should lead to incremental improvements to the ASR model\nin a cyclical, community-driven fashion.", "published": "2024-12-31 15:44:30", "link": "http://arxiv.org/abs/2501.00509v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "MCP-Solver: Integrating Language Models with Constraint Programming\n  Systems", "abstract": "The MCP Solver bridges Large Language Models (LLMs) with symbolic solvers\nthrough the Model Context Protocol (MCP), an open-source standard for AI system\nintegration. Providing LLMs access to formal solving and reasoning capabilities\naddresses their key deficiency while leveraging their strengths. Our\nimplementation offers interfaces for constraint programming (Minizinc),\npropositional satisfiability (PySAT), and SAT modulo Theories (Python Z3). The\nsystem employs an editing approach with iterated validation to ensure model\nconsistency during modifications and enable structured refinement.", "published": "2024-12-31 16:49:27", "link": "http://arxiv.org/abs/2501.00539v2", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.SE"], "primary_category": "cs.AI"}
{"title": "Re-evaluating Automatic LLM System Ranking for Alignment with Human\n  Preference", "abstract": "Evaluating and ranking the capabilities of different LLMs is crucial for\nunderstanding their performance and alignment with human preferences. Due to\nthe high cost and time-consuming nature of human evaluations, an automatic LLM\nbencher (i.e., an automatic evaluation framework that aims to rank LLMs based\non their alignment with human preferences) is indispensable. An automatic LLM\nbencher consists of four components: the input set (e.g., a user instruction),\nthe evaluation model (e.g., an LLM), the evaluation type (e.g., pairwise\ncomparison), and the aggregation method (e.g., the ELO rating system). However,\nprevious work has not thoroughly explored how to select these components or how\ntheir different combinations influence the results. In this work, through\ncontrolled experiments, we provide a series of recommendations on how to choose\neach component to better automate the evaluation of LLMs. Furthermore, we\ndiscovered that when evaluating LLMs with similar performance, the performance\nof the automatic LLM bencher declines sharply, underscoring the limitations of\ncurrent benchers and calling for future work. Lastly, we found that the\nevaluation models' performance at the instance level (e.g., the accuracy of\nselecting the best output) does not always align with their effectiveness when\nused as a component of a bencher, highlighting the importance of dedicated\nsystem-level evaluation of benchers.", "published": "2024-12-31 17:46:51", "link": "http://arxiv.org/abs/2501.00560v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Are the Values of LLMs Structurally Aligned with Humans? A Causal\n  Perspective", "abstract": "As large language models (LLMs) become increasingly integrated into critical\napplications, aligning their behavior with human values presents significant\nchallenges. Current methods, such as Reinforcement Learning from Human Feedback\n(RLHF), typically focus on a limited set of coarse-grained values and are\nresource-intensive. Moreover, the correlations between these values remain\nimplicit, leading to unclear explanations for value-steering outcomes. Our work\nargues that a latent causal value graph underlies the value dimensions of LLMs\nand that, despite alignment training, this structure remains significantly\ndifferent from human value systems. We leverage these causal value graphs to\nguide two lightweight value-steering methods: role-based prompting and sparse\nautoencoder (SAE) steering, effectively mitigating unexpected side effects.\nFurthermore, SAE provides a more fine-grained approach to value steering.\nExperiments on Gemma-2B-IT and Llama3-8B-IT demonstrate the effectiveness and\ncontrollability of our methods.", "published": "2024-12-31 18:12:05", "link": "http://arxiv.org/abs/2501.00581v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Toward Corpus Size Requirements for Training and Evaluating Depression\n  Risk Models Using Spoken Language", "abstract": "Mental health risk prediction is a growing field in the speech community, but\nmany studies are based on small corpora. This study illustrates how variations\nin test and train set sizes impact performance in a controlled study. Using a\ncorpus of over 65K labeled data points, results from a fully crossed design of\ndifferent train/test size combinations are provided. Two model types are\nincluded: one based on language and the other on speech acoustics. Both use\nmethods current in this domain. An age-mismatched test set was also included.\nResults show that (1) test sizes below 1K samples gave noisy results, even for\nlarger training set sizes; (2) training set sizes of at least 2K were needed\nfor stable results; (3) NLP and acoustic models behaved similarly with\ntrain/test size variations, and (4) the mismatched test set showed the same\npatterns as the matched test set. Additional factors are discussed, including\nlabel priors, model strength and pre-training, unique speakers, and data\nlengths. While no single study can specify exact size requirements, results\ndemonstrate the need for appropriately sized train and test sets for future\nstudies of mental health risk prediction from speech and language.", "published": "2024-12-31 19:32:25", "link": "http://arxiv.org/abs/2501.00617v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "ICONS: Influence Consensus for Vision-Language Data Selection", "abstract": "Visual Instruction Tuning typically requires a large amount of\nvision-language training data. This data often containing redundant information\nthat increases computational costs without proportional performance gains. In\nthis work, we introduce ICONS, a gradient-driven Influence CONsensus approach\nfor vision-language data Selection that selects a compact training dataset for\nefficient multi-task training. The key element of our approach is cross-task\ninfluence consensus, which uses majority voting across task-specific influence\nmatrices to identify samples that are consistently valuable across multiple\ntasks, allowing us to effectively prioritize data that optimizes for overall\nperformance. Experiments show that models trained on our selected data (20% of\nLLaVA-665K) achieve 98.6% of the relative performance obtained using the full\ndataset. Additionally, we release this subset, LLaVA-ICONS-133K, a compact yet\nhighly informative subset of LLaVA-665K visual instruction tuning data,\npreserving high impact training data for efficient vision-language model\ndevelopment.", "published": "2024-12-31 21:33:38", "link": "http://arxiv.org/abs/2501.00654v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Titans: Learning to Memorize at Test Time", "abstract": "Over more than a decade there has been an extensive research effort on how to\neffectively utilize recurrent models and attention. While recurrent models aim\nto compress the data into a fixed-size memory (called hidden state), attention\nallows attending to the entire context window, capturing the direct\ndependencies of all tokens. This more accurate modeling of dependencies,\nhowever, comes with a quadratic cost, limiting the model to a fixed-length\ncontext. We present a new neural long-term memory module that learns to\nmemorize historical context and helps attention to attend to the current\ncontext while utilizing long past information. We show that this neural memory\nhas the advantage of fast parallelizable training while maintaining a fast\ninference. From a memory perspective, we argue that attention due to its\nlimited context but accurate dependency modeling performs as a short-term\nmemory, while neural memory due to its ability to memorize the data, acts as a\nlong-term, more persistent, memory. Based on these two modules, we introduce a\nnew family of architectures, called Titans, and present three variants to\naddress how one can effectively incorporate memory into this architecture. Our\nexperimental results on language modeling, common-sense reasoning, genomics,\nand time series tasks show that Titans are more effective than Transformers and\nrecent modern linear recurrent models. They further can effectively scale to\nlarger than 2M context window size with higher accuracy in needle-in-haystack\ntasks compared to baselines.", "published": "2024-12-31 22:32:03", "link": "http://arxiv.org/abs/2501.00663v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Reinforcing Thinking through Reasoning-Enhanced Reward Models", "abstract": "Large Language Models (LLMs) exhibit great potential in complex multi-step\nreasoning through inference-time thinking but still struggle with deciding when\nto stop thinking due to limited self-awareness about their knowledge\nboundaries. While human preference alignment has shown extraordinary\nopportunities, expensive labeling challenges adherence to scaling law. Language\nmodel self-critique, as an alternative to using human-labeled reasoning data,\nis questioned with its inherited biases. This work addresses these challenges\nby distilling the LLM's own reasoning processes into synthetic behavioral data,\neliminating the need for manual labeling of intermediate steps. Building on\nthis concept, we propose Distillation-Reinforcement-Reasoning (DRR), a\nthree-step framework that leverages the LLM's inherent behaviors as external\nfeedback by first generating behavioral data using the Reasoner (LLM) to\nreflect its reasoning capabilities, then training a lightweight discriminative\nreward model (DM) on behavioral data, and finally deploying the DM at inference\ntime to assist the Reasoner's decision-making. Experiments on multiple\nbenchmarks show that the DRR framework outperforms self-critique approaches\nwithout relying on additional complex data annotation. Benefiting from\nlightweight design, ease of replication, and adaptability, DRR is applicable to\na wide range of LLM-centric tasks.", "published": "2024-12-31 04:50:15", "link": "http://arxiv.org/abs/2501.01457v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "LLM-MedQA: Enhancing Medical Question Answering through Case Studies in\n  Large Language Models", "abstract": "Accurate and efficient question-answering systems are essential for\ndelivering high-quality patient care in the medical field. While Large Language\nModels (LLMs) have made remarkable strides across various domains, they\ncontinue to face significant challenges in medical question answering,\nparticularly in understanding domain-specific terminologies and performing\ncomplex reasoning. These limitations undermine their effectiveness in critical\nmedical applications. To address these issues, we propose a novel approach\nincorporating similar case generation within a multi-agent medical\nquestion-answering (MedQA) system. Specifically, we leverage the Llama3.1:70B\nmodel, a state-of-the-art LLM, in a multi-agent architecture to enhance\nperformance on the MedQA dataset using zero-shot learning. Our method\ncapitalizes on the model's inherent medical knowledge and reasoning\ncapabilities, eliminating the need for additional training data. Experimental\nresults show substantial performance gains over existing benchmark models, with\nimprovements of 7% in both accuracy and F1-score across various medical QA\ntasks. Furthermore, we examine the model's interpretability and reliability in\naddressing complex medical queries. This research not only offers a robust\nsolution for medical question answering but also establishes a foundation for\nbroader applications of LLMs in the medical domain.", "published": "2024-12-31 19:55:45", "link": "http://arxiv.org/abs/2501.05464v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "TSPE: Task-Specific Prompt Ensemble for Improved Zero-Shot Audio\n  Classification", "abstract": "Audio-language models (ALMs) excel in zero-shot audio classification, a task\nwhere models classify previously unseen audio clips at test time by leveraging\ndescriptive natural language prompts. We introduce TSPE (Task-Specific Prompt\nEnsemble), a simple, training-free hard prompting method that boosts ALEs'\nzero-shot performance by customizing prompts for diverse audio classification\ntasks. Rather than using generic template-based prompts like \"Sound of a car\"\nwe generate context-rich prompts, such as \"Sound of a car coming from a\ntunnel\". Specifically, we leverage label information to identify suitable sound\nattributes, such as \"loud\" and \"feeble\", and appropriate sound sources, such as\n\"tunnel\" and \"street\" and incorporate this information into the prompts used by\nAudio-Language Models (ALMs) for audio classification. Further, to enhance\naudio-text alignment, we perform prompt ensemble across TSPE-generated\ntask-specific prompts. When evaluated on 12 diverse audio classification\ndatasets, TSPE improves performance across ALMs by showing an absolute\nimprovement of 1.23-16.36% over vanilla zero-shot evaluation.", "published": "2024-12-31 11:27:17", "link": "http://arxiv.org/abs/2501.00398v2", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Temporal Information Reconstruction and Non-Aligned Residual in Spiking\n  Neural Networks for Speech Classification", "abstract": "Recently, it can be noticed that most models based on spiking neural networks\n(SNNs) only use a same level temporal resolution to deal with speech\nclassification problems, which makes these models cannot learn the information\nof input data at different temporal scales. Additionally, owing to the\ndifferent time lengths of the data before and after the sub-modules of many\nmodels, the effective residual connections cannot be applied to optimize the\ntraining processes of these models.To solve these problems, on the one hand, we\nreconstruct the temporal dimension of the audio spectrum to propose a novel\nmethod named as Temporal Reconstruction (TR) by referring the hierarchical\nprocessing process of the human brain for understanding speech. Then, the\nreconstructed SNN model with TR can learn the information of input data at\ndifferent temporal scales and model more comprehensive semantic information\nfrom audio data because it enables the networks to learn the information of\ninput data at different temporal resolutions. On the other hand, we propose the\nNon-Aligned Residual (NAR) method by analyzing the audio data, which allows the\nresidual connection can be used in two audio data with different time lengths.\nWe have conducted plentiful experiments on the Spiking Speech Commands (SSC),\nthe Spiking Heidelberg Digits (SHD), and the Google Speech Commands v0.02 (GSC)\ndatasets. According to the experiment results, we have achieved the\nstate-of-the-art (SOTA) result 81.02\\% on SSC for the test classification\naccuracy of all SNN models, and we have obtained the SOTA result 96.04\\% on SHD\nfor the classification accuracy of all models.", "published": "2024-12-31 08:52:40", "link": "http://arxiv.org/abs/2501.00348v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Unrolled Creative Adversarial Network For Generating Novel Musical\n  Pieces", "abstract": "Music generation has been established as a prominent topic in artificial\nintelligence and machine learning over recent years. In most recent works on\nRNN-based neural network methods have been applied for sequence generation. In\ncontrast, generative adversarial networks (GANs) and their counterparts have\nbeen explored by very few researchersfor music generation.\n  In this paper, a classical system was employed alongside a new system to\ngenerate creative music. Both systems were designed based on adversarial\nnetworks to generate music by learning from examples. The classical system was\ntrained to learn a set of music pieces without differentiating between classes,\nwhereas the new system was trained to learn the different composers and their\nstyles to generate a creative music piece by deviating from the learned\ncomposers' styles.\n  The base structure utilized was generative adversarial networks (GANs), which\nare capable of generating novel outputs given a set of inputs to learn from and\nmimic their distribution. It has been shown in previous work that GANs are\nlimited in their original design with respect to creative outputs. Building on\nthe Creative Adversarial Networks (CAN) , this work applied them in the music\ndomain rather than the visual art domain. Additionally, unrolled CAN was\nintroduced to prevent mode collapse. Experiments were conducted on both GAN and\nCAN for generating music, and their capabilities were measured in terms of\ndeviation from the input set.", "published": "2024-12-31 14:07:59", "link": "http://arxiv.org/abs/2501.00452v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SoundBrush: Sound as a Brush for Visual Scene Editing", "abstract": "We propose SoundBrush, a model that uses sound as a brush to edit and\nmanipulate visual scenes. We extend the generative capabilities of the Latent\nDiffusion Model (LDM) to incorporate audio information for editing visual\nscenes. Inspired by existing image-editing works, we frame this task as a\nsupervised learning problem and leverage various off-the-shelf models to\nconstruct a sound-paired visual scene dataset for training. This richly\ngenerated dataset enables SoundBrush to learn to map audio features into the\ntextual space of the LDM, allowing for visual scene editing guided by diverse\nin-the-wild sound. Unlike existing methods, SoundBrush can accurately\nmanipulate the overall scenery or even insert sounding objects to best match\nthe audio inputs while preserving the original content. Furthermore, by\nintegrating with novel view synthesis techniques, our framework can be extended\nto edit 3D scenes, facilitating sound-driven 3D scene manipulation. Demos are\navailable at https://soundbrush.github.io/.", "published": "2024-12-31 20:53:45", "link": "http://arxiv.org/abs/2501.00645v1", "categories": ["cs.CV", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
