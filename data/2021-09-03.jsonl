{"title": "A Context-Aware Hierarchical BERT Fusion Network for Multi-turn Dialog\n  Act Detection", "abstract": "The success of interactive dialog systems is usually associated with the\nquality of the spoken language understanding (SLU) task, which mainly\nidentifies the corresponding dialog acts and slot values in each turn. By\ntreating utterances in isolation, most SLU systems often overlook the semantic\ncontext in which a dialog act is expected. The act dependency between turns is\nnon-trivial and yet critical to the identification of the correct semantic\nrepresentations. Previous works with limited context awareness have exposed the\ninadequacy of dealing with complexity in multiproned user intents, which are\nsubject to spontaneous change during turn transitions. In this work, we propose\nto enhance SLU in multi-turn dialogs, employing a context-aware hierarchical\nBERT fusion Network (CaBERT-SLU) to not only discern context information within\na dialog but also jointly identify multiple dialog acts and slots in each\nutterance. Experimental results show that our approach reaches new\nstate-of-the-art (SOTA) performances in two complicated multi-turn dialogue\ndatasets with considerable improvements compared with previous methods, which\nonly consider single utterances for multiple intents and slot filling.", "published": "2021-09-03 02:00:03", "link": "http://arxiv.org/abs/2109.01267v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Open-Source Dataset and A Multi-Task Model for Malay Named Entity\n  Recognition", "abstract": "Named entity recognition (NER) is a fundamental task of natural language\nprocessing (NLP). However, most state-of-the-art research is mainly oriented to\nhigh-resource languages such as English and has not been widely applied to\nlow-resource languages. In Malay language, relevant NER resources are limited.\nIn this work, we propose a dataset construction framework, which is based on\nlabeled datasets of homologous languages and iterative optimization, to build a\nMalay NER dataset (MYNER) comprising 28,991 sentences (over 384 thousand\ntokens). Additionally, to better integrate boundary information for NER, we\npropose a multi-task (MT) model with a bidirectional revision (Bi-revision)\nmechanism for Malay NER task. Specifically, an auxiliary task, boundary\ndetection, is introduced to improve NER training in both explicit and implicit\nways. Furthermore, a gated ignoring mechanism is proposed to conduct\nconditional label transfer and alleviate error propagation by the auxiliary\ntask. Experimental results demonstrate that our model achieves comparable\nresults over baselines on MYNER. The dataset and the model in this paper would\nbe publicly released as a benchmark dataset.", "published": "2021-09-03 03:29:25", "link": "http://arxiv.org/abs/2109.01293v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Speaker Personas from Conversational Texts", "abstract": "Personas are useful for dialogue response prediction. However, the personas\nused in current studies are pre-defined and hard to obtain before a\nconversation. To tackle this issue, we study a new task, named Speaker Persona\nDetection (SPD), which aims to detect speaker personas based on the plain\nconversational text. In this task, a best-matched persona is searched out from\ncandidates given the conversational text. This is a many-to-many semantic\nmatching task because both contexts and personas in SPD are composed of\nmultiple sentences. The long-term dependency and the dynamic redundancy among\nthese sentences increase the difficulty of this task. We build a dataset for\nSPD, dubbed as Persona Match on Persona-Chat (PMPC). Furthermore, we evaluate\nseveral baseline models and propose utterance-to-profile (U2P) matching\nnetworks for this task. The U2P models operate at a fine granularity which\ntreat both contexts and personas as sets of multiple sequences. Then, each\nsequence pair is scored and an interpretable overall score is obtained for a\ncontext-persona pair through aggregation. Evaluation results show that the U2P\nmodels outperform their baseline counterparts significantly.", "published": "2021-09-03 06:14:38", "link": "http://arxiv.org/abs/2109.01330v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Modeling, Lexical Translation, Reordering: The Training Process\n  of NMT through the Lens of Classical SMT", "abstract": "Differently from the traditional statistical MT that decomposes the\ntranslation task into distinct separately learned components, neural machine\ntranslation uses a single neural network to model the entire translation\nprocess. Despite neural machine translation being de-facto standard, it is\nstill not clear how NMT models acquire different competences over the course of\ntraining, and how this mirrors the different models in traditional SMT. In this\nwork, we look at the competences related to three core SMT components and find\nthat during training, NMT first focuses on learning target-side language\nmodeling, then improves translation quality approaching word-by-word\ntranslation, and finally learns more complicated reordering patterns. We show\nthat this behavior holds for several models and language pairs. Additionally,\nwe explain how such an understanding of the training process can be useful in\npractice and, as an example, show how it can be used to improve vanilla\nnon-autoregressive neural machine translation by guiding teacher model\nselection.", "published": "2021-09-03 09:38:50", "link": "http://arxiv.org/abs/2109.01396v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Neural Models for Natural Language Processing in the Face of\n  Distributional Shift", "abstract": "The dominating NLP paradigm of training a strong neural predictor to perform\none task on a specific dataset has led to state-of-the-art performance in a\nvariety of applications (eg. sentiment classification, span-prediction based\nquestion answering or machine translation). However, it builds upon the\nassumption that the data distribution is stationary, ie. that the data is\nsampled from a fixed distribution both at training and test time. This way of\ntraining is inconsistent with how we as humans are able to learn from and\noperate within a constantly changing stream of information. Moreover, it is\nill-adapted to real-world use cases where the data distribution is expected to\nshift over the course of a model's lifetime.\n  The first goal of this thesis is to characterize the different forms this\nshift can take in the context of natural language processing, and propose\nbenchmarks and evaluation metrics to measure its effect on current deep\nlearning architectures. We then proceed to take steps to mitigate the effect of\ndistributional shift on NLP models. To this end, we develop methods based on\nparametric reformulations of the distributionally robust optimization\nframework. Empirically, we demonstrate that these approaches yield more robust\nmodels as demonstrated on a selection of realistic problems. In the third and\nfinal part of this thesis, we explore ways of efficiently adapting existing\nmodels to new domains or tasks. Our contribution to this topic takes\ninspiration from information geometry to derive a new gradient update rule\nwhich alleviate catastrophic forgetting issues during adaptation.", "published": "2021-09-03 14:29:20", "link": "http://arxiv.org/abs/2109.01558v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contextualized Embeddings based Convolutional Neural Networks for\n  Duplicate Question Identification", "abstract": "Question Paraphrase Identification (QPI) is a critical task for large-scale\nQuestion-Answering forums. The purpose of QPI is to determine whether a given\npair of questions are semantically identical or not. Previous approaches for\nthis task have yielded promising results, but have often relied on complex\nrecurrence mechanisms that are expensive and time-consuming in nature. In this\npaper, we propose a novel architecture combining a Bidirectional Transformer\nEncoder with Convolutional Neural Networks for the QPI task. We produce the\npredictions from the proposed architecture using two different inference\nsetups: Siamese and Matched Aggregation. Experimental results demonstrate that\nour model achieves state-of-the-art performance on the Quora Question Pairs\ndataset. We empirically prove that the addition of convolution layers to the\nmodel architecture improves the results in both inference setups. We also\ninvestigate the impact of partial and complete fine-tuning and analyze the\ntrade-off between computational power and accuracy in the process. Based on the\nobtained results, we conclude that the Matched-Aggregation setup consistently\noutperforms the Siamese setup. Our work provides insights into what\narchitecture combinations and setups are likely to produce better results for\nthe QPI task.", "published": "2021-09-03 14:30:09", "link": "http://arxiv.org/abs/2109.01560v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Finetuned Language Models Are Zero-Shot Learners", "abstract": "This paper explores a simple method for improving the zero-shot learning\nabilities of language models. We show that instruction tuning -- finetuning\nlanguage models on a collection of tasks described via instructions --\nsubstantially improves zero-shot performance on unseen tasks.\n  We take a 137B parameter pretrained language model and instruction-tune it on\nover 60 NLP tasks verbalized via natural language instruction templates. We\nevaluate this instruction-tuned model, which we call FLAN, on unseen task\ntypes. FLAN substantially improves the performance of its unmodified\ncounterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we\nevaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE,\nBoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number\nof finetuning datasets, model scale, and natural language instructions are key\nto the success of instruction tuning.", "published": "2021-09-03 17:55:52", "link": "http://arxiv.org/abs/2109.01652v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Indexing Context-Sensitive Reachability", "abstract": "Many context-sensitive data flow analyses can be formulated as a variant of\nthe all-pairs Dyck-CFL reachability problem, which, in general, is of sub-cubic\ntime complexity and quadratic space complexity. Such high complexity\nsignificantly limits the scalability of context-sensitive data flow analysis\nand is not affordable for analyzing large-scale software. This paper presents\n\\textsc{Flare}, a reduction from the CFL reachability problem to the\nconventional graph reachability problem for context-sensitive data flow\nanalysis. This reduction allows us to benefit from recent advances in\nreachability indexing schemes, which often consume almost linear space for\nanswering reachability queries in almost constant time. We have applied our\nreduction to a context-sensitive alias analysis and a context-sensitive\ninformation-flow analysis for C/C++ programs. Experimental results on standard\nbenchmarks and open-source software demonstrate that we can achieve orders of\nmagnitude speedup at the cost of only moderate space to store the indexes. The\nimplementation of our approach is publicly available.", "published": "2021-09-03 05:41:30", "link": "http://arxiv.org/abs/2109.01321v1", "categories": ["cs.CL", "cs.PL"], "primary_category": "cs.CL"}
{"title": "An Exploratory Study on Utilising the Web of Linked Data for Product\n  Data Mining", "abstract": "The Linked Open Data practice has led to a significant growth of structured\ndata on the Web in the last decade. Such structured data describe real-world\nentities in a machine-readable way, and have created an unprecedented\nopportunity for research in the field of Natural Language Processing. However,\nthere is a lack of studies on how such data can be used, for what kind of\ntasks, and to what extent they can be useful for these tasks. This work focuses\non the e-commerce domain to explore methods of utilising such structured data\nto create language resources that may be used for product classification and\nlinking. We process billions of structured data points in the form of RDF\nn-quads, to create multi-million words of product-related corpora that are\nlater used in three different ways for creating of language resources: training\nword embedding models, continued pre-training of BERT-like language models, and\ntraining Machine Translation models that are used as a proxy to generate\nproduct-related keywords. Our evaluation on an extensive set of benchmarks\nshows word embeddings to be the most reliable and consistent method to improve\nthe accuracy on both tasks (with up to 6.9 percentage points in macro-average\nF1 on some datasets). The other two methods however, are not as useful. Our\nanalysis shows that this could be due to a number of reasons, including the\nbiased domain representation in the structured data and lack of vocabulary\ncoverage. We share our datasets and discuss how our lessons learned could be\ntaken forward to inform future research in this direction.", "published": "2021-09-03 09:58:36", "link": "http://arxiv.org/abs/2109.01411v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Contrastive Representation Learning for Exemplar-Guided Paraphrase\n  Generation", "abstract": "Exemplar-Guided Paraphrase Generation (EGPG) aims to generate a target\nsentence which conforms to the style of the given exemplar while encapsulating\nthe content information of the source sentence. In this paper, we propose a new\nmethod with the goal of learning a better representation of the style andthe\ncontent. This method is mainly motivated by the recent success of contrastive\nlearning which has demonstrated its power in unsupervised feature extraction\ntasks. The idea is to design two contrastive losses with respect to the content\nand the style by considering two problem characteristics during training. One\ncharacteristic is that the target sentence shares the same content with the\nsource sentence, and the second characteristic is that the target sentence\nshares the same style with the exemplar. These two contrastive losses are\nincorporated into the general encoder-decoder paradigm. Experiments on two\ndatasets, namely QQP-Pos and ParaNMT, demonstrate the effectiveness of our\nproposed constrastive losses.", "published": "2021-09-03 12:57:19", "link": "http://arxiv.org/abs/2109.01484v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Biomedical Data-to-Text Generation via Fine-Tuning Transformers", "abstract": "Data-to-text (D2T) generation in the biomedical domain is a promising - yet\nmostly unexplored - field of research. Here, we apply neural models for D2T\ngeneration to a real-world dataset consisting of package leaflets of European\nmedicines. We show that fine-tuned transformers are able to generate realistic,\nmultisentence text from data in the biomedical domain, yet have important\nlimitations. We also release a new dataset (BioLeaflets) for benchmarking D2T\ngeneration models in the biomedical domain.", "published": "2021-09-03 13:42:30", "link": "http://arxiv.org/abs/2109.01518v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Learning from Multiple Noisy Augmented Data Sets for Better\n  Cross-Lingual Spoken Language Understanding", "abstract": "Lack of training data presents a grand challenge to scaling out spoken\nlanguage understanding (SLU) to low-resource languages. Although various data\naugmentation approaches have been proposed to synthesize training data in\nlow-resource target languages, the augmented data sets are often noisy, and\nthus impede the performance of SLU models. In this paper we focus on mitigating\nnoise in augmented data. We develop a denoising training approach. Multiple\nmodels are trained with data produced by various augmented methods. Those\nmodels provide supervision signals to each other. The experimental results show\nthat our method outperforms the existing state of the art by 3.05 and 4.24\npercentage points on two benchmark datasets, respectively. The code will be\nmade open sourced on github.", "published": "2021-09-03 15:44:15", "link": "http://arxiv.org/abs/2109.01583v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Cross-Lingual Training with Dense Retrieval for Document Retrieval", "abstract": "Dense retrieval has shown great success in passage ranking in English.\nHowever, its effectiveness in document retrieval for non-English languages\nremains unexplored due to the limitation in training resources. In this work,\nwe explore different transfer techniques for document ranking from English\nannotations to multiple non-English languages. Our experiments on the test\ncollections in six languages (Chinese, Arabic, French, Hindi, Bengali, Spanish)\nfrom diverse language families reveal that zero-shot model-based transfer using\nmBERT improves the search quality in non-English mono-lingual retrieval. Also,\nwe find that weakly-supervised target language transfer yields competitive\nperformances against the generation-based target language transfer that\nrequires external translators and query generators.", "published": "2021-09-03 17:15:38", "link": "http://arxiv.org/abs/2109.01628v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Empirical Study of Named Entity Recognition Performance Using\n  Distribution-aware Word Embedding", "abstract": "With the fast development of Deep Learning techniques, Named Entity\nRecognition (NER) is becoming more and more important in the information\nextraction task. The greatest difficulty that the NER task faces is to keep the\ndetectability even when types of NE and documents are unfamiliar. Realizing\nthat the specificity information may contain potential meanings of a word and\ngenerate semantic-related features for word embedding, we develop a\ndistribution-aware word embedding and implement three different methods to make\nuse of the distribution information in a NER framework. And the result shows\nthat the performance of NER will be improved if the word specificity is\nincorporated into existing NER methods.", "published": "2021-09-03 17:28:04", "link": "http://arxiv.org/abs/2109.01636v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CREAK: A Dataset for Commonsense Reasoning over Entity Knowledge", "abstract": "Most benchmark datasets targeting commonsense reasoning focus on everyday\nscenarios: physical knowledge like knowing that you could fill a cup under a\nwaterfall [Talmor et al., 2019], social knowledge like bumping into someone is\nawkward [Sap et al., 2019], and other generic situations. However, there is a\nrich space of commonsense inferences anchored to knowledge about specific\nentities: for example, deciding the truthfulness of a claim \"Harry Potter can\nteach classes on how to fly on a broomstick.\" Can models learn to combine\nentity knowledge with commonsense reasoning in this fashion? We introduce\nCREAK, a testbed for commonsense reasoning about entity knowledge, bridging\nfact-checking about entities (Harry Potter is a wizard and is skilled at riding\na broomstick) with commonsense inferences (if you're good at a skill you can\nteach others how to do it). Our dataset consists of 13k human-authored English\nclaims about entities that are either true or false, in addition to a small\ncontrast set. Crowdworkers can easily come up with these statements and human\nperformance on the dataset is high (high 90s); we argue that models should be\nable to blend entity knowledge and commonsense reasoning to do well here. In\nour experiments, we focus on the closed-book setting and observe that a\nbaseline model finetuned on existing fact verification benchmark struggles on\nCREAK. Training a model on CREAK improves accuracy by a substantial margin, but\nstill falls short of human performance. Our benchmark provides a unique probe\ninto natural language understanding models, testing both its ability to\nretrieve facts (e.g., who teaches at the University of Chicago?) and unstated\ncommonsense knowledge (e.g., butlers do not yell at guests).", "published": "2021-09-03 17:56:40", "link": "http://arxiv.org/abs/2109.01653v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ALLWAS: Active Learning on Language models in WASserstein space", "abstract": "Active learning has emerged as a standard paradigm in areas with scarcity of\nlabeled training data, such as in the medical domain. Language models have\nemerged as the prevalent choice of several natural language tasks due to the\nperformance boost offered by these models. However, in several domains, such as\nmedicine, the scarcity of labeled training data is a common issue. Also, these\nmodels may not work well in cases where class imbalance is prevalent. Active\nlearning may prove helpful in these cases to boost the performance with a\nlimited label budget. To this end, we propose a novel method using sampling\ntechniques based on submodular optimization and optimal transport for active\nlearning in language models, dubbed ALLWAS. We construct a sampling strategy\nbased on submodular optimization of the designed objective in the gradient\ndomain. Furthermore, to enable learning from few samples, we propose a novel\nstrategy for sampling from the Wasserstein barycenters. Our empirical\nevaluations on standard benchmark datasets for text classification show that\nour methods perform significantly better (>20% relative increase in some cases)\nthan existing approaches for active learning on language models.", "published": "2021-09-03 18:11:07", "link": "http://arxiv.org/abs/2109.01691v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Information Symmetry Matters: A Modal-Alternating Propagation Network\n  for Few-Shot Learning", "abstract": "Semantic information provides intra-class consistency and inter-class\ndiscriminability beyond visual concepts, which has been employed in Few-Shot\nLearning (FSL) to achieve further gains. However, semantic information is only\navailable for labeled samples but absent for unlabeled samples, in which the\nembeddings are rectified unilaterally by guiding the few labeled samples with\nsemantics. Therefore, it is inevitable to bring a cross-modal bias between\nsemantic-guided samples and nonsemantic-guided samples, which results in an\ninformation asymmetry problem. To address this problem, we propose a\nModal-Alternating Propagation Network (MAP-Net) to supplement the absent\nsemantic information of unlabeled samples, which builds information symmetry\namong all samples in both visual and semantic modalities. Specifically, the\nMAP-Net transfers the neighbor information by the graph propagation to generate\nthe pseudo-semantics for unlabeled samples guided by the completed visual\nrelationships and rectify the feature embeddings. In addition, due to the large\ndiscrepancy between visual and semantic modalities, we design a Relation\nGuidance (RG) strategy to guide the visual relation vectors via semantics so\nthat the propagated information is more beneficial. Extensive experimental\nresults on three semantic-labeled datasets, i.e., Caltech-UCSD-Birds 200-2011,\nSUN Attribute Database, and Oxford 102 Flower, have demonstrated that our\nproposed method achieves promising performance and outperforms the\nstate-of-the-art approaches, which indicates the necessity of information\nsymmetry.", "published": "2021-09-03 03:43:53", "link": "http://arxiv.org/abs/2109.01295v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "LG4AV: Combining Language Models and Graph Neural Networks for Author\n  Verification", "abstract": "The automatic verification of document authorships is important in various\nsettings. Researchers are for example judged and compared by the amount and\nimpact of their publications and public figures are confronted by their posts\non social media platforms. Therefore, it is important that authorship\ninformation in frequently used web services and platforms is correct. The\nquestion whether a given document is written by a given author is commonly\nreferred to as authorship verification (AV). While AV is a widely investigated\nproblem in general, only few works consider settings where the documents are\nshort and written in a rather uniform style. This makes most approaches\nunpractical for online databases and knowledge graphs in the scholarly domain.\nHere, authorships of scientific publications have to be verified, often with\njust abstracts and titles available. To this point, we present our novel\napproach LG4AV which combines language models and graph neural networks for\nauthorship verification. By directly feeding the available texts in a\npre-trained transformer architecture, our model does not need any hand-crafted\nstylometric features that are not meaningful in scenarios where the writing\nstyle is, at least to some extent, standardized. By the incorporation of a\ngraph neural network structure, our model can benefit from relations between\nauthors that are meaningful with respect to the verification process. For\nexample, scientific authors are more likely to write about topics that are\naddressed by their co-authors and twitter users tend to post about the same\nsubjects as people they follow. We experimentally evaluate our model and study\nto which extent the inclusion of co-authorships enhances verification decisions\nin bibliometric environments.", "published": "2021-09-03 12:45:28", "link": "http://arxiv.org/abs/2109.01479v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.7; I.2.6"], "primary_category": "cs.LG"}
{"title": "A Longitudinal Multi-modal Dataset for Dementia Monitoring and Diagnosis", "abstract": "Dementia affects cognitive functions of adults, including memory, language,\nand behaviour. Standard diagnostic biomarkers such as MRI are costly, whilst\nneuropsychological tests suffer from sensitivity issues in detecting dementia\nonset. The analysis of speech and language has emerged as a promising and\nnon-intrusive technology to diagnose and monitor dementia. Currently, most work\nin this direction ignores the multi-modal nature of human communication and\ninteractive aspects of everyday conversational interaction. Moreover, most\nstudies ignore changes in cognitive status over time due to the lack of\nconsistent longitudinal data. Here we introduce a novel fine-grained\nlongitudinal multi-modal corpus collected in a natural setting from healthy\ncontrols and people with dementia over two phases, each spanning 28 sessions.\nThe corpus consists of spoken conversations, a subset of which are transcribed,\nas well as typed and written thoughts and associated extra-linguistic\ninformation such as pen strokes and keystrokes. We present the data collection\nprocess and describe the corpus in detail. Furthermore, we establish baselines\nfor capturing longitudinal changes in language across different modalities for\ntwo cohorts, healthy controls and people with dementia, outlining future\nresearch directions enabled by the corpus.", "published": "2021-09-03 14:02:12", "link": "http://arxiv.org/abs/2109.01537v2", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Phone Duration Modeling for Speaker Age Estimation in Children", "abstract": "Automatic inference of important paralinguistic information such as age from\nspeech is an important area of research with numerous spoken language\ntechnology based applications. Speaker age estimation has applications in\nenabling personalization and age-appropriate curation of information and\ncontent. However, research in speaker age estimation in children is especially\nchallenging due to paucity of relevant speech data representing the\ndevelopmental spectrum, and the high signal variability especially intra age\nvariability that complicates modeling. Most approaches in children speaker age\nestimation adopt methods directly from research on adult speech processing. In\nthis paper, we propose features specific to children and focus on speaker's\nphone duration as an important biomarker of children's age. We propose phone\nduration modeling for predicting age from child's speech. To enable that,\nchildren speech is first forced aligned with the corresponding transcription to\nderive phone duration distributions. Statistical functionals are computed from\nphone duration distributions for each phoneme which are in turn used to train\nregression models to predict speaker age. Two children speech datasets are\nemployed to demonstrate the robustness of phone duration features. We perform\nage regression experiments on age categories ranging from children studying in\nkindergarten to grade 10. Experimental results suggest phone durations contain\nimportant development-related information of children. Phonemes contributing\nmost to estimation of children speaker age are analyzed and presented.", "published": "2021-09-03 14:43:39", "link": "http://arxiv.org/abs/2109.01568v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Musical Tempo Estimation Using a Multi-scale Network", "abstract": "Recently, some single-step systems without onset detection have shown their\neffectiveness in automatic musical tempo estimation. Following the success of\nthese systems, in this paper we propose a Multi-scale Grouped Attention Network\nto further explore the potential of such methods. A multi-scale structure is\nintroduced as the overall network architecture where information from different\nscales is aggregated to strengthen contextual feature learning. Furthermore, we\npropose a Grouped Attention Module as the key component of the network. The\nproposed module separates the input feature into several groups along the\nfrequency axis, which makes it capable of capturing long-range dependencies\nfrom different frequency positions on the spectrogram. In comparison\nexperiments, the results on public datasets show that the proposed model\noutperforms existing state-of-the-art methods on Accuracy1.", "published": "2021-09-03 16:32:30", "link": "http://arxiv.org/abs/2109.01607v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
