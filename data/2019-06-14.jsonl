{"title": "Conceptor Debiasing of Word Representations Evaluated on WEAT", "abstract": "Bias in word embeddings such as Word2Vec has been widely investigated, and\nmany efforts made to remove such bias. We show how to use conceptors debiasing\nto post-process both traditional and contextualized word embeddings. Our\nconceptor debiasing can simultaneously remove racial and gender biases and,\nunlike standard debiasing methods, can make effect use of heterogeneous lists\nof biased words. We show that conceptor debiasing diminishes racial and gender\nbias of word representations as measured using the Word Embedding Association\nTest (WEAT) of Caliskan et al. (2017).", "published": "2019-06-14 02:53:26", "link": "http://arxiv.org/abs/1906.05993v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cost-sensitive Regularization for Label Confusion-aware Event Detection", "abstract": "In supervised event detection, most of the mislabeling occurs between a small\nnumber of confusing type pairs, including trigger-NIL pairs and sibling\nsub-types of the same coarse type. To address this label confusion problem,\nthis paper proposes cost-sensitive regularization, which can force the training\nprocedure to concentrate more on optimizing confusing type pairs. Specifically,\nwe introduce a cost-weighted term into the training loss, which penalizes more\non mislabeling between confusing label pairs. Furthermore, we also propose two\nestimators which can effectively measure such label confusion based on\ninstance-level or population-level statistics. Experiments on TAC-KBP 2017\ndatasets demonstrate that the proposed method can significantly improve the\nperformances of different models in both English and Chinese event detection.", "published": "2019-06-14 03:27:11", "link": "http://arxiv.org/abs/1906.06003v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Ask Unanswerable Questions for Machine Reading Comprehension", "abstract": "Machine reading comprehension with unanswerable questions is a challenging\ntask. In this work, we propose a data augmentation technique by automatically\ngenerating relevant unanswerable questions according to an answerable question\npaired with its corresponding paragraph that contains the answer. We introduce\na pair-to-sequence model for unanswerable question generation, which\neffectively captures the interactions between the question and the paragraph.\nWe also present a way to construct training data for our question generation\nmodels by leveraging the existing reading comprehension dataset. Experimental\nresults show that the pair-to-sequence model performs consistently better\ncompared with the sequence-to-sequence baseline. We further use the\nautomatically generated unanswerable questions as a means of data augmentation\non the SQuAD 2.0 dataset, yielding 1.9 absolute F1 improvement with BERT-base\nmodel and 1.7 absolute F1 improvement with BERT-large model.", "published": "2019-06-14 06:35:10", "link": "http://arxiv.org/abs/1906.06045v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Response Generation with Meta-Words", "abstract": "We present open domain response generation with meta-words. A meta-word is a\nstructured record that describes various attributes of a response, and thus\nallows us to explicitly model the one-to-many relationship within open domain\ndialogues and perform response generation in an explainable and controllable\nmanner. To incorporate meta-words into generation, we enhance the\nsequence-to-sequence architecture with a goal tracking memory network that\nformalizes meta-word expression as a goal and manages the generation process to\nachieve the goal with a state memory panel and a state controller. Experimental\nresults on two large-scale datasets indicate that our model can significantly\noutperform several state-of-the-art generation models in terms of response\nrelevance, response diversity, accuracy of one-to-many modeling, accuracy of\nmeta-word expression, and human evaluation.", "published": "2019-06-14 06:53:57", "link": "http://arxiv.org/abs/1906.06050v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Microsoft AI Challenge India 2018: Learning to Rank Passages for Web\n  Question Answering with Deep Attention Networks", "abstract": "This paper describes our system for The Microsoft AI Challenge India 2018:\nRanking Passages for Web Question Answering. The system uses the biLSTM network\nwith co-attention mechanism between query and passage representations.\nAdditionally, we use self attention on embeddings to increase the lexical\ncoverage by allowing the system to take union over different embeddings. We\nalso incorporate hand-crafted features to improve the system performance. Our\nsystem achieved a Mean Reciprocal Rank (MRR) of 0.67 on eval-1 dataset.", "published": "2019-06-14 07:29:29", "link": "http://arxiv.org/abs/1906.06056v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DocRED: A Large-Scale Document-Level Relation Extraction Dataset", "abstract": "Multiple entities in a document generally exhibit complex inter-sentence\nrelations, and cannot be well handled by existing relation extraction (RE)\nmethods that typically focus on extracting intra-sentence relations for single\nentity pairs. In order to accelerate the research on document-level RE, we\nintroduce DocRED, a new dataset constructed from Wikipedia and Wikidata with\nthree features: (1) DocRED annotates both named entities and relations, and is\nthe largest human-annotated dataset for document-level RE from plain text; (2)\nDocRED requires reading multiple sentences in a document to extract entities\nand infer their relations by synthesizing all information of the document; (3)\nalong with the human-annotated data, we also offer large-scale distantly\nsupervised data, which enables DocRED to be adopted for both supervised and\nweakly supervised scenarios. In order to verify the challenges of\ndocument-level RE, we implement recent state-of-the-art methods for RE and\nconduct a thorough evaluation of these methods on DocRED. Empirical results\nshow that DocRED is challenging for existing RE methods, which indicates that\ndocument-level RE remains an open problem and requires further efforts. Based\non the detailed analysis on the experiments, we discuss multiple promising\ndirections for future research.", "published": "2019-06-14 11:12:20", "link": "http://arxiv.org/abs/1906.06127v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Visual Question Answering by Referring to Generated Paragraph\n  Captions", "abstract": "Paragraph-style image captions describe diverse aspects of an image as\nopposed to the more common single-sentence captions that only provide an\nabstract description of the image. These paragraph captions can hence contain\nsubstantial information of the image for tasks such as visual question\nanswering. Moreover, this textual information is complementary with visual\ninformation present in the image because it can discuss both more abstract\nconcepts and more explicit, intermediate symbolic information about objects,\nevents, and scenes that can directly be matched with the textual question and\ncopied into the textual answer (i.e., via easier modality match). Hence, we\npropose a combined Visual and Textual Question Answering (VTQA) model which\ntakes as input a paragraph caption as well as the corresponding image, and\nanswers the given question based on both inputs. In our model, the inputs are\nfused to extract related information by cross-attention (early fusion), then\nfused again in the form of consensus (late fusion), and finally expected\nanswers are given an extra score to enhance the chance of selection (later\nfusion). Empirical results show that paragraph captions, even when\nautomatically generated (via an RL-based encoder-decoder model), help correctly\nanswer more visual questions. Overall, our joint model, when trained on the\nVisual Genome dataset, significantly improves the VQA performance over a strong\nbaseline model.", "published": "2019-06-14 14:14:42", "link": "http://arxiv.org/abs/1906.06216v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparison of Diverse Decoding Methods from Conditional Language Models", "abstract": "While conditional language models have greatly improved in their ability to\noutput high-quality natural language, many NLP applications benefit from being\nable to generate a diverse set of candidate sequences. Diverse decoding\nstrategies aim to, within a given-sized candidate list, cover as much of the\nspace of high-quality outputs as possible, leading to improvements for tasks\nthat re-rank and combine candidate outputs. Standard decoding methods, such as\nbeam search, optimize for generating high likelihood sequences rather than\ndiverse ones, though recent work has focused on increasing diversity in these\nmethods. In this work, we perform an extensive survey of decoding-time\nstrategies for generating diverse outputs from conditional language models. We\nalso show how diversity can be improved without sacrificing quality by\nover-sampling additional candidates, then filtering to the desired number.", "published": "2019-06-14 18:39:34", "link": "http://arxiv.org/abs/1906.06362v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"My Way of Telling a Story\": Persona based Grounded Story Generation", "abstract": "Visual storytelling is the task of generating stories based on a sequence of\nimages. Inspired by the recent works in neural generation focusing on\ncontrolling the form of text, this paper explores the idea of generating these\nstories in different personas. However, one of the main challenges of\nperforming this task is the lack of a dataset of visual stories in different\npersonas. Having said that, there are independent datasets for both visual\nstorytelling and annotated sentences for various persona. In this paper we\ndescribe an approach to overcome this by getting labelled persona data from a\ndifferent task and leveraging those annotations to perform persona based story\ngeneration. We inspect various ways of incorporating personality in both the\nencoder and the decoder representations to steer the generation in the target\ndirection. To this end, we propose five models which are incremental extensions\nto the baseline model to perform the task at hand. In our experiments we use\nfive different personas to guide the generation process. We find that the\nmodels based on our hypotheses perform better at capturing words while\ngenerating stories in the target persona.", "published": "2019-06-14 21:00:20", "link": "http://arxiv.org/abs/1906.06401v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Attention-based Modeling for Emotion Detection and Classification in\n  Textual Conversations", "abstract": "This paper addresses the problem of modeling textual conversations and\ndetecting emotions. Our proposed model makes use of 1) deep transfer learning\nrather than the classical shallow methods of word embedding; 2) self-attention\nmechanisms to focus on the most important parts of the texts and 3) turn-based\nconversational modeling for classifying the emotions. The approach does not\nrely on any hand-crafted features or lexicons. Our model was evaluated on the\ndata provided by the SemEval-2019 shared task on contextual emotion detection\nin text. The model shows very competitive results.", "published": "2019-06-14 14:56:55", "link": "http://arxiv.org/abs/1906.07020v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Image Captioning: Transforming Objects into Words", "abstract": "Image captioning models typically follow an encoder-decoder architecture\nwhich uses abstract image feature vectors as input to the encoder. One of the\nmost successful algorithms uses feature vectors extracted from the region\nproposals obtained from an object detector. In this work we introduce the\nObject Relation Transformer, that builds upon this approach by explicitly\nincorporating information about the spatial relationship between input detected\nobjects through geometric attention. Quantitative and qualitative results\ndemonstrate the importance of such geometric attention for image captioning,\nleading to improvements on all common captioning metrics on the MS-COCO\ndataset.", "published": "2019-06-14 00:00:29", "link": "http://arxiv.org/abs/1906.05963v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "NLProlog: Reasoning with Weak Unification for Question Answering in\n  Natural Language", "abstract": "Rule-based models are attractive for various tasks because they inherently\nlead to interpretable and explainable decisions and can easily incorporate\nprior knowledge. However, such systems are difficult to apply to problems\ninvolving natural language, due to its linguistic variability. In contrast,\nneural models can cope very well with ambiguity by learning distributed\nrepresentations of words and their composition from data, but lead to models\nthat are difficult to interpret. In this paper, we describe a model combining\nneural networks with logic programming in a novel manner for solving multi-hop\nreasoning tasks over natural language. Specifically, we propose to use a Prolog\nprover which we extend to utilize a similarity function over pretrained\nsentence encoders. We fine-tune the representations for the similarity function\nvia backpropagation. This leads to a system that can apply rule-based reasoning\nto natural language, and induce domain-specific rules from training data. We\nevaluate the proposed system on two different question answering tasks, showing\nthat it outperforms two baselines -- BIDAF (Seo et al., 2016a) and FAST QA\n(Weissenborn et al., 2017b) on a subset of the WikiHop corpus and achieves\ncompetitive results on the MedHop data set (Welbl et al., 2017).", "published": "2019-06-14 13:05:08", "link": "http://arxiv.org/abs/1906.06187v1", "categories": ["cs.CL", "cs.LO"], "primary_category": "cs.CL"}
{"title": "Cumulative Adaptation for BLSTM Acoustic Models", "abstract": "This paper addresses the robust speech recognition problem as an adaptation\ntask. Specifically, we investigate the cumulative application of adaptation\nmethods. A bidirectional Long Short-Term Memory (BLSTM) based neural network,\ncapable of learning temporal relationships and translation invariant\nrepresentations, is used for robust acoustic modelling. Further, i-vectors were\nused as an input to the neural network to perform instantaneous speaker and\nenvironment adaptation, providing 8\\% relative improvement in word error rate\non the NIST Hub5 2000 evaluation test set. By enhancing the first-pass i-vector\nbased adaptation with a second-pass adaptation using speaker and environment\ndependent transformations within the network, a further relative improvement of\n5\\% in word error rate was achieved. We have reevaluated the features used to\nestimate i-vectors and their normalization to achieve the best performance in a\nmodern large scale automatic speech recognition system.", "published": "2019-06-14 13:55:12", "link": "http://arxiv.org/abs/1906.06207v1", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "A Simple and Effective Approach to Automatic Post-Editing with Transfer\n  Learning", "abstract": "Automatic post-editing (APE) seeks to automatically refine the output of a\nblack-box machine translation (MT) system through human post-edits. APE systems\nare usually trained by complementing human post-edited data with large,\nartificial data generated through back-translations, a time-consuming process\noften no easier than training an MT system from scratch. In this paper, we\npropose an alternative where we fine-tune pre-trained BERT models on both the\nencoder and decoder of an APE system, exploring several parameter sharing\nstrategies. By only training on a dataset of 23K sentences for 3 hours on a\nsingle GPU, we obtain results that are competitive with systems that were\ntrained on 5M artificial sentences. When we add this artificial data, our\nmethod obtains state-of-the-art results.", "published": "2019-06-14 15:36:22", "link": "http://arxiv.org/abs/1906.06253v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "IITP at MEDIQA 2019: Systems Report for Natural Language Inference,\n  Question Entailment and Question Answering", "abstract": "This paper presents the experiments accomplished as a part of our\nparticipation in the MEDIQA challenge, an (Abacha et al., 2019) shared task. We\nparticipated in all the three tasks defined in this particular shared task. The\ntasks are viz. i. Natural Language Inference (NLI) ii. Recognizing Question\nEntailment(RQE) and their application in medical Question Answering (QA). We\nsubmitted runs using multiple deep learning based systems (runs) for each of\nthese three tasks. We submitted five system results in each of the NLI and RQE\ntasks, and four system results for the QA task. The systems yield encouraging\nresults in all three tasks. The highest performance obtained in NLI, RQE and QA\ntasks are 81.8%, 53.2%, and 71.7%, respectively.", "published": "2019-06-14 06:38:33", "link": "http://arxiv.org/abs/1906.06332v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On the Computational Power of RNNs", "abstract": "Recent neural network architectures such as the basic recurrent neural\nnetwork (RNN) and Gated Recurrent Unit (GRU) have gained prominence as\nend-to-end learning architectures for natural language processing tasks. But\nwhat is the computational power of such systems? We prove that finite precision\nRNNs with one hidden layer and ReLU activation and finite precision GRUs are\nexactly as computationally powerful as deterministic finite automata. Allowing\narbitrary precision, we prove that RNNs with one hidden layer and ReLU\nactivation are at least as computationally powerful as pushdown automata. If we\nalso allow infinite precision, infinite edge weights, and nonlinear output\nactivation functions, we prove that GRUs are at least as computationally\npowerful as pushdown automata. All results are shown constructively.", "published": "2019-06-14 18:04:39", "link": "http://arxiv.org/abs/1906.06349v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Principled Frameworks for Evaluating Ethics in NLP Systems", "abstract": "We critique recent work on ethics in natural language processing. Those\ndiscussions have focused on data collection, experimental design, and\ninterventions in modeling. But we argue that we ought to first understand the\nframeworks of ethics that are being used to evaluate the fairness and justice\nof algorithmic systems. Here, we begin that discussion by outlining\ndeontological ethics, and envision a research agenda prioritized by it.", "published": "2019-06-14 22:52:21", "link": "http://arxiv.org/abs/1906.06425v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Scalable Syntax-Aware Language Models Using Knowledge Distillation", "abstract": "Prior work has shown that, on small amounts of training data, syntactic\nneural language models learn structurally sensitive generalisations more\nsuccessfully than sequential language models. However, their computational\ncomplexity renders scaling difficult, and it remains an open question whether\nstructural biases are still necessary when sequential models have access to\never larger amounts of training data. To answer this question, we introduce an\nefficient knowledge distillation (KD) technique that transfers knowledge from a\nsyntactic language model trained on a small corpus to an LSTM language model,\nhence enabling the LSTM to develop a more structurally sensitive representation\nof the larger training data it learns from. On targeted syntactic evaluations,\nwe find that, while sequential LSTMs perform much better than previously\nreported, our proposed technique substantially improves on this baseline,\nyielding a new state of the art. Our findings and analysis affirm the\nimportance of structural biases, even in models that learn from large amounts\nof data.", "published": "2019-06-14 23:42:08", "link": "http://arxiv.org/abs/1906.06438v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Multi-turn Dialogue Modelling with Utterance ReWriter", "abstract": "Recent research has made impressive progress in single-turn dialogue\nmodelling. In the multi-turn setting, however, current models are still far\nfrom satisfactory. One major challenge is the frequently occurred coreference\nand information omission in our daily conversation, making it hard for machines\nto understand the real intention. In this paper, we propose rewriting the human\nutterance as a pre-process to help multi-turn dialgoue modelling. Each\nutterance is first rewritten to recover all coreferred and omitted information.\nThe next processing steps are then performed based on the rewritten utterance.\nTo properly train the utterance rewriter, we collect a new dataset with human\nannotations and introduce a Transformer-based utterance rewriting architecture\nusing the pointer network. We show the proposed architecture achieves\nremarkably good performance on the utterance rewriting task. The trained\nutterance rewriter can be easily integrated into online chatbots and brings\ngeneral improvement over different domains.", "published": "2019-06-14 06:45:08", "link": "http://arxiv.org/abs/1906.07004v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Augmenting Neural Networks with First-order Logic", "abstract": "Today, the dominant paradigm for training neural networks involves minimizing\ntask loss on a large dataset. Using world knowledge to inform a model, and yet\nretain the ability to perform end-to-end training remains an open question. In\nthis paper, we present a novel framework for introducing declarative knowledge\nto neural network architectures in order to guide training and prediction. Our\nframework systematically compiles logical statements into computation graphs\nthat augment a neural network without extra learnable parameters or manual\nredesign. We evaluate our modeling strategy on three tasks: machine\ncomprehension, natural language inference, and text chunking. Our experiments\nshow that knowledge-augmented networks can strongly improve over baselines,\nespecially in low-data regimes.", "published": "2019-06-14 17:10:42", "link": "http://arxiv.org/abs/1906.06298v3", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Perceptual Based Adversarial Audio Attacks", "abstract": "Recent work has shown the possibility of adversarial attacks on automatic\nspeechrecognition (ASR) systems. However, in the vast majority of work in this\narea, theattacks have been executed only in the digital space, or have involved\nshort phrasesand static room settings. In this paper, we demonstrate a\nphysically realizableaudio adversarial attack. We base our approach\nspecifically on a psychoacoustic-property-based loss function, and automated\ngeneration of room impulse responses, to create adversarial attacks that are\nrobust when played over a speaker in multiple environments. We show that such\nattacks are possible even while being virtually imperceptible to listeners.", "published": "2019-06-14 18:14:22", "link": "http://arxiv.org/abs/1906.06355v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Video-Driven Speech Reconstruction using Generative Adversarial Networks", "abstract": "Speech is a means of communication which relies on both audio and visual\ninformation. The absence of one modality can often lead to confusion or\nmisinterpretation of information. In this paper we present an end-to-end\ntemporal model capable of directly synthesising audio from silent video,\nwithout needing to transform to-and-from intermediate features. Our proposed\napproach, based on GANs is capable of producing natural sounding, intelligible\nspeech which is synchronised with the video. The performance of our model is\nevaluated on the GRID dataset for both speaker dependent and speaker\nindependent scenarios. To the best of our knowledge this is the first method\nthat maps video directly to raw audio and the first to produce intelligible\nspeech when tested on previously unseen speakers. We evaluate the synthesised\naudio not only based on the sound quality but also on the accuracy of the\nspoken words.", "published": "2019-06-14 17:15:27", "link": "http://arxiv.org/abs/1906.06301v1", "categories": ["eess.AS", "cs.CV", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Realistic Speech-Driven Facial Animation with GANs", "abstract": "Speech-driven facial animation is the process that automatically synthesizes\ntalking characters based on speech signals. The majority of work in this domain\ncreates a mapping from audio features to visual features. This approach often\nrequires post-processing using computer graphics techniques to produce\nrealistic albeit subject dependent results. We present an end-to-end system\nthat generates videos of a talking head, using only a still image of a person\nand an audio clip containing speech, without relying on handcrafted\nintermediate features. Our method generates videos which have (a) lip movements\nthat are in sync with the audio and (b) natural facial expressions such as\nblinks and eyebrow movements. Our temporal GAN uses 3 discriminators focused on\nachieving detailed frames, audio-visual synchronization, and realistic\nexpressions. We quantify the contribution of each component in our model using\nan ablation study and we provide insights into the latent representation of the\nmodel. The generated videos are evaluated based on sharpness, reconstruction\nquality, lip-reading accuracy, synchronization as well as their ability to\ngenerate natural blinks.", "published": "2019-06-14 16:52:27", "link": "http://arxiv.org/abs/1906.06337v1", "categories": ["cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.CV"}
{"title": "User Curated Shaping of Expressive Performances", "abstract": "Musicians produce individualized, expressive performances by manipulating\nparameters such as dynamics, tempo and articulation. This manipulation of\nexpressive parameters is informed by elements of score information such as\npitch, meter, and tempo and dynamics markings (among others). In this paper we\npresent an interactive interface that gives users the opportunity to explore\nthe relationship between structural elements of a score and expressive\nparameters. This interface draws on the basis function models, a data-driven\nframework for expressive performance. In this framework, expressive parameters\nare modeled as a function of score features, i.e., numerical encodings of\nspecific aspects of a musical score, using neural networks. With the proposed\ninterface, users are able to weight the contribution of individual score\nfeatures and understand how an expressive performance is constructed.", "published": "2019-06-14 22:59:23", "link": "http://arxiv.org/abs/1906.06428v1", "categories": ["cs.SD", "cs.HC", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Single-Channel Signal Separation and Deconvolution with Generative\n  Adversarial Networks", "abstract": "Single-channel signal separation and deconvolution aims to separate and\ndeconvolve individual sources from a single-channel mixture and is a\nchallenging problem in which no prior knowledge of the mixing filters is\navailable. Both individual sources and mixing filters need to be estimated. In\naddition, a mixture may contain non-stationary noise which is unseen in the\ntraining set. We propose a synthesizing-decomposition (S-D) approach to solve\nthe single-channel separation and deconvolution problem. In synthesizing, a\ngenerative model for sources is built using a generative adversarial network\n(GAN). In decomposition, both mixing filters and sources are optimized to\nminimize the reconstruction error of the mixture. The proposed S-D approach\nachieves a peak-to-noise-ratio (PSNR) of 18.9 dB and 15.4 dB in image\ninpainting and completion, outperforming a baseline convolutional neural\nnetwork PSNR of 15.3 dB and 12.2 dB, respectively and achieves a PSNR of 13.2\ndB in source separation together with deconvolution, outperforming a\nconvolutive non-negative matrix factorization (NMF) baseline of 10.1 dB.", "published": "2019-06-14 22:00:26", "link": "http://arxiv.org/abs/1906.07552v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
