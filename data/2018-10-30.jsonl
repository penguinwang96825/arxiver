{"title": "Simplifying Neural Machine Translation with Addition-Subtraction\n  Twin-Gated Recurrent Networks", "abstract": "In this paper, we propose an additionsubtraction twin-gated recurrent network\n(ATR) to simplify neural machine translation. The recurrent units of ATR are\nheavily simplified to have the smallest number of weight matrices among units\nof all existing gated RNNs. With the simple addition and subtraction operation,\nwe introduce a twin-gated mechanism to build input and forget gates which are\nhighly correlated. Despite this simplification, the essential non-linearities\nand capability of modeling long-distance dependencies are preserved.\nAdditionally, the proposed ATR is more transparent than LSTM/GRU due to the\nsimplification. Forward self-attention can be easily established in ATR, which\nmakes the proposed network interpretable. Experiments on WMT14 translation\ntasks demonstrate that ATR-based neural machine translation can yield\ncompetitive performance on English- German and English-French language pairs in\nterms of both translation quality and speed. Further experiments on NIST\nChinese-English translation, natural language inference and Chinese word\nsegmentation verify the generality and applicability of ATR on different\nnatural language processing tasks.", "published": "2018-10-30 06:55:23", "link": "http://arxiv.org/abs/1810.12546v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Machine Translation between Vietnamese and English: an Empirical Study", "abstract": "Machine translation is shifting to an end-to-end approach based on deep\nneural networks. The state of the art achieves impressive results for popular\nlanguage pairs such as English - French or English - Chinese. However for\nEnglish - Vietnamese the shortage of parallel corpora and expensive\nhyper-parameter search present practical challenges to neural-based approaches.\nThis paper highlights our efforts on improving English-Vietnamese translations\nin two directions: (1) Building the largest open Vietnamese - English corpus to\ndate, and (2) Extensive experiments with the latest neural models to achieve\nthe highest BLEU scores. Our experiments provide practical examples of\neffectively employing different neural machine translation models with\nlow-resource language pairs.", "published": "2018-10-30 07:33:52", "link": "http://arxiv.org/abs/1810.12557v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Neural Methods for Parsing Discourse Representation Structures", "abstract": "Neural methods have had several recent successes in semantic parsing, though\nthey have yet to face the challenge of producing meaning representations based\non formal semantics. We present a sequence-to-sequence neural semantic parser\nthat is able to produce Discourse Representation Structures (DRSs) for English\nsentences with high accuracy, outperforming traditional DRS parsers. To\nfacilitate the learning of the output, we represent DRSs as a sequence of flat\nclauses and introduce a method to verify that produced DRSs are well-formed and\ninterpretable. We compare models using characters and words as input and see\n(somewhat surprisingly) that the former performs better than the latter. We\nshow that eliminating variable names from the output using De Bruijn-indices\nincreases parser performance. Adding silver training data boosts performance\neven further.", "published": "2018-10-30 08:42:52", "link": "http://arxiv.org/abs/1810.12579v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Subword Encoding in Lattice LSTM for Chinese Word Segmentation", "abstract": "We investigate a lattice LSTM network for Chinese word segmentation (CWS) to\nutilize words or subwords. It integrates the character sequence features with\nall subsequences information matched from a lexicon. The matched subsequences\nserve as information shortcut tunnels which link their start and end characters\ndirectly. Gated units are used to control the contribution of multiple input\nlinks. Through formula derivation and comparison, we show that the lattice LSTM\nis an extension of the standard LSTM with the ability to take multiple inputs.\nPrevious lattice LSTM model takes word embeddings as the lexicon input, we\nprove that subword encoding can give the comparable performance and has the\nbenefit of not relying on any external segmentor. The contribution of lattice\nLSTM comes from both lexicon and pretrained embeddings information, we find\nthat the lexicon information contributes more than the pretrained embeddings\ninformation through controlled experiments. Our experiments show that the\nlattice structure with subword encoding gives competitive or better results\nwith previous state-of-the-art methods on four segmentation benchmarks.\nDetailed analyses are conducted to compare the performance of word encoding and\nsubword encoding in lattice LSTM. We also investigate the performance of\nlattice LSTM structure under different circumstances and when this model works\nor fails.", "published": "2018-10-30 09:14:37", "link": "http://arxiv.org/abs/1810.12594v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards End-to-end Automatic Code-Switching Speech Recognition", "abstract": "Speech recognition in mixed language has difficulties to adapt end-to-end\nframework due to the lack of data and overlapping phone sets, for example in\nwords such as \"one\" in English and \"w\\`an\" in Chinese. We propose a CTC-based\nend-to-end automatic speech recognition model for intra-sentential\nEnglish-Mandarin code-switching. The model is trained by joint training on\nmonolingual datasets, and fine-tuning with the mixed-language corpus. During\nthe decoding process, we apply a beam search and combine CTC predictions and\nlanguage model score. The proposed method is effective in leveraging\nmonolingual corpus and detecting language transitions and it improves the CER\nby 5%.", "published": "2018-10-30 10:11:35", "link": "http://arxiv.org/abs/1810.12620v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prosodic entrainment in dialog acts", "abstract": "We examined prosodic entrainment in spoken dialogs separately for several\ndialog acts in cooperative and competitive games. Entrainment was measured for\nintonation features derived from a superpositional intonation stylization as\nwell as for rhythm features. The found differences can be related to the\ncooperative or competitive nature of the game, as well as to dialog act\nproperties as its intrinsic authority, supportiveness and distributional\ncharacteristics. In cooperative games dialog acts with a high authority given\nby knowledge and with a high frequency showed the most entrainment. The results\nare discussed amongst others with respect to the degree of active entrainment\ncontrol in cooperative behavior.", "published": "2018-10-30 10:53:42", "link": "http://arxiv.org/abs/1810.12646v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Text GANs as Language Models", "abstract": "Generative Adversarial Networks (GANs) are a promising approach for text\ngeneration that, unlike traditional language models (LM), does not suffer from\nthe problem of ``exposure bias''. However, A major hurdle for understanding the\npotential of GANs for text generation is the lack of a clear evaluation metric.\nIn this work, we propose to approximate the distribution of text generated by a\nGAN, which permits evaluating them with traditional probability-based LM\nmetrics. We apply our approximation procedure on several GAN-based models and\nshow that they currently perform substantially worse than state-of-the-art LMs.\nOur evaluation procedure promotes better understanding of the relation between\nGANs and LMs, and can accelerate progress in GAN-based text generation.", "published": "2018-10-30 11:58:16", "link": "http://arxiv.org/abs/1810.12686v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Neural Machine Translation Initialized by Unsupervised\n  Statistical Machine Translation", "abstract": "Recent work achieved remarkable results in training neural machine\ntranslation (NMT) systems in a fully unsupervised way, with new and dedicated\narchitectures that rely on monolingual corpora only. In this work, we propose\nto define unsupervised NMT (UNMT) as NMT trained with the supervision of\nsynthetic bilingual data. Our approach straightforwardly enables the use of\nstate-of-the-art architectures proposed for supervised NMT by replacing\nhuman-made bilingual data with synthetic bilingual data for training. We\npropose to initialize the training of UNMT with synthetic bilingual data\ngenerated by unsupervised statistical machine translation (USMT). The UNMT\nsystem is then incrementally improved using back-translation. Our preliminary\nexperiments show that our approach achieves a new state-of-the-art for\nunsupervised machine translation on the WMT16 German--English news translation\ntask, for both translation directions.", "published": "2018-10-30 12:33:03", "link": "http://arxiv.org/abs/1810.12703v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Cross-Lingual Sentence Representations via a Multi-task\n  Dual-Encoder Model", "abstract": "A significant roadblock in multilingual neural language modeling is the lack\nof labeled non-English data. One potential method for overcoming this issue is\nlearning cross-lingual text representations that can be used to transfer the\nperformance from training on English tasks to non-English tasks, despite little\nto no task-specific non-English data. In this paper, we explore a natural setup\nfor learning cross-lingual sentence representations: the dual-encoder. We\nprovide a comprehensive evaluation of our cross-lingual representations on a\nnumber of monolingual, cross-lingual, and zero-shot/few-shot learning tasks,\nand also give an analysis of different learned cross-lingual embedding spaces.", "published": "2018-10-30 16:18:05", "link": "http://arxiv.org/abs/1810.12836v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ReCoRD: Bridging the Gap between Human and Machine Commonsense Reading\n  Comprehension", "abstract": "We present a large-scale dataset, ReCoRD, for machine reading comprehension\nrequiring commonsense reasoning. Experiments on this dataset demonstrate that\nthe performance of state-of-the-art MRC systems fall far behind human\nperformance. ReCoRD represents a challenge for future research to bridge the\ngap between human and machine commonsense reading comprehension. ReCoRD is\navailable at http://nlp.jhu.edu/record.", "published": "2018-10-30 17:32:16", "link": "http://arxiv.org/abs/1810.12885v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Combining Distant and Direct Supervision for Neural Relation Extraction", "abstract": "In relation extraction with distant supervision, noisy labels make it\ndifficult to train quality models. Previous neural models addressed this\nproblem using an attention mechanism that attends to sentences that are likely\nto express the relations. We improve such models by combining the distant\nsupervision data with an additional directly-supervised data, which we use as\nsupervision for the attention weights. We find that joint training on both\ntypes of supervision leads to a better model because it improves the model's\nability to identify noisy sentences. In addition, we find that sigmoidal\nattention weights with max pooling achieves better performance over the\ncommonly used weighted average attention in this setup. Our proposed method\nachieves a new state-of-the-art result on the widely used FB-NYT dataset.", "published": "2018-10-30 18:31:16", "link": "http://arxiv.org/abs/1810.12956v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Stress-Testing Neural Models of Natural Language Inference with\n  Multiply-Quantified Sentences", "abstract": "Standard evaluations of deep learning models for semantics using naturalistic\ncorpora are limited in what they can tell us about the fidelity of the learned\nrepresentations, because the corpora rarely come with good measures of semantic\ncomplexity. To overcome this limitation, we present a method for generating\ndata sets of multiply-quantified natural language inference (NLI) examples in\nwhich semantic complexity can be precisely characterized, and we use this\nmethod to show that a variety of common architectures for NLI inevitably fail\nto encode crucial information; only a model with forced lexical alignments\navoids this damaging information loss.", "published": "2018-10-30 23:31:50", "link": "http://arxiv.org/abs/1810.13033v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Almost-unsupervised Speech Recognition with Close-to-zero Resource Based\n  on Phonetic Structures Learned from Very Small Unpaired Speech and Text Data", "abstract": "Producing a large amount of annotated speech data for training ASR systems\nremains difficult for more than 95% of languages all over the world which are\nlow-resourced. However, we note human babies start to learn the language by the\nsounds of a small number of exemplar words without hearing a large amount of\ndata. We initiate some preliminary work in this direction in this paper. Audio\nWord2Vec is used to obtain embeddings of spoken words which carry phonetic\ninformation extracted from the signals. An autoencoder is used to generate\nembeddings of text words based on the articulatory features for the phoneme\nsequences. Both sets of embeddings for spoken and text words describe similar\nphonetic structures among words in their respective latent spaces. A mapping\nrelation from the audio embeddings to text embeddings actually gives the\nword-level ASR. This can be learned by aligning a small number of spoken words\nand the corresponding text words in the embedding spaces. In the initial\nexperiments only 200 annotated spoken words and one hour of speech data without\nannotation gave a word accuracy of 27.5%, which is low but a good starting\npoint.", "published": "2018-10-30 08:11:45", "link": "http://arxiv.org/abs/1810.12566v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Compositional Attention Networks for Interpretability in Natural\n  Language Question Answering", "abstract": "MAC Net is a compositional attention network designed for Visual Question\nAnswering. We propose a modified MAC net architecture for Natural Language\nQuestion Answering. Question Answering typically requires Language\nUnderstanding and multi-step Reasoning. MAC net's unique architecture - the\nseparation between memory and control, facilitates data-driven iterative\nreasoning. This makes it an ideal candidate for solving tasks that involve\nlogical reasoning. Our experiments with 20 bAbI tasks demonstrate the value of\nMAC net as a data-efficient and interpretable architecture for Natural Language\nQuestion Answering. The transparent nature of MAC net provides a highly\ngranular view of the reasoning steps taken by the network in answering a query.", "published": "2018-10-30 12:23:35", "link": "http://arxiv.org/abs/1810.12698v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Spoken Language Understanding on the Edge", "abstract": "We consider the problem of performing Spoken Language Understanding (SLU) on\nsmall devices typical of IoT applications. Our contributions are twofold.\nFirst, we outline the design of an embedded, private-by-design SLU system and\nshow that it has performance on par with cloud-based commercial solutions.\nSecond, we release the datasets used in our experiments in the interest of\nreproducibility and in the hope that they can prove useful to the SLU\ncommunity.", "published": "2018-10-30 13:49:37", "link": "http://arxiv.org/abs/1810.12735v2", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Long Short-Term Attention", "abstract": "Attention is an important cognition process of humans, which helps humans\nconcentrate on critical information during their perception and learning.\nHowever, although many machine learning models can remember information of\ndata, they have no the attention mechanism. For example, the long short-term\nmemory (LSTM) network is able to remember sequential information, but it cannot\npay special attention to part of the sequences. In this paper, we present a\nnovel model called long short-term attention (LSTA), which seamlessly\nintegrates the attention mechanism into the inner cell of LSTM. More than\nprocessing long short term dependencies, LSTA can focus on important\ninformation of the sequences with the attention mechanism. Extensive\nexperiments demonstrate that LSTA outperforms LSTM and related models on the\nsequence learning tasks.", "published": "2018-10-30 14:08:30", "link": "http://arxiv.org/abs/1810.12752v2", "categories": ["cs.LG", "cs.CL", "cs.NE"], "primary_category": "cs.LG"}
{"title": "Recurrent Attention Unit", "abstract": "Recurrent Neural Network (RNN) has been successfully applied in many sequence\nlearning problems. Such as handwriting recognition, image description, natural\nlanguage processing and video motion analysis. After years of development,\nresearchers have improved the internal structure of the RNN and introduced many\nvariants. Among others, Gated Recurrent Unit (GRU) is one of the most widely\nused RNN model. However, GRU lacks the capability of adaptively paying\nattention to certain regions or locations, so that it may cause information\nredundancy or loss during leaning. In this paper, we propose a RNN model,\ncalled Recurrent Attention Unit (RAU), which seamlessly integrates the\nattention mechanism into the interior of GRU by adding an attention gate. The\nattention gate can enhance GRU's ability to remember long-term memory and help\nmemory cells quickly discard unimportant content. RAU is capable of extracting\ninformation from the sequential data by adaptively selecting a sequence of\nregions or locations and pay more attention to the selected regions during\nlearning. Extensive experiments on image classification, sentiment\nclassification and language modeling show that RAU consistently outperforms GRU\nand other baseline methods.", "published": "2018-10-30 14:09:19", "link": "http://arxiv.org/abs/1810.12754v1", "categories": ["cs.LG", "cs.CL", "cs.NE", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Advancing PICO Element Detection in Biomedical Text via Deep Neural\n  Networks", "abstract": "In evidence-based medicine (EBM), defining a clinical question in terms of\nthe specific patient problem aids the physicians to efficiently identify\nappropriate resources and search for the best available evidence for medical\ntreatment. In order to formulate a well-defined, focused clinical question, a\nframework called PICO is widely used, which identifies the sentences in a given\nmedical text that belong to the four components typically reported in clinical\ntrials: Participants/Problem (P), Intervention (I), Comparison (C) and Outcome\n(O). In this work, we propose a novel deep learning model for recognizing PICO\nelements in biomedical abstracts. Based on the previous state-of-the-art\nbidirectional long-short term memory (biLSTM) plus conditional random field\n(CRF) architecture, we add another layer of biLSTM upon the sentence\nrepresentation vectors so that the contextual information from surrounding\nsentences can be gathered to help infer the interpretation of the current one.\nIn addition, we propose two methods to further generalize and improve the\nmodel: adversarial training and unsupervised pre-training over large corpora.\nWe tested our proposed approach over two benchmark datasets. One is the\nPubMed-PICO dataset, where our best results outperform the previous best by\n5.5%, 7.9%, and 5.8% for P, I, and O elements in terms of F1 score,\nrespectively. And for the other dataset named NICTA-PIBOSO, the improvements\nfor P/I/O elements are 2.4%, 13.6%, and 1.0% in F1 score, respectively.\nOverall, our proposed deep learning model can obtain unprecedented PICO element\ndetection accuracy while avoiding the need for any manual feature selection.", "published": "2018-10-30 14:44:24", "link": "http://arxiv.org/abs/1810.12780v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "JavaScript Convolutional Neural Networks for Keyword Spotting in the\n  Browser: An Experimental Analysis", "abstract": "Used for simple commands recognition on devices from smart routers to mobile\nphones, keyword spotting systems are everywhere. Ubiquitous as well are web\napplications, which have grown in popularity and complexity over the last\ndecade with significant improvements in usability under cross-platform\nconditions. However, despite their obvious advantage in natural language\ninteraction, voice-enabled web applications are still far and few between. In\nthis work, we attempt to bridge this gap by bringing keyword spotting\ncapabilities directly into the browser. To our knowledge, we are the first to\ndemonstrate a fully-functional implementation of convolutional neural networks\nin pure JavaScript that runs in any standards-compliant browser. We also apply\nnetwork slimming, a model compression technique, to explore the\naccuracy-efficiency tradeoffs, reporting latency measurements on a range of\ndevices and software. Overall, our robust, cross-device implementation for\nkeyword spotting realizes a new paradigm for serving neural network\napplications, and one of our slim models reduces latency by 66% with a minimal\ndecrease in accuracy of 4% from 94% to 90%.", "published": "2018-10-30 16:58:36", "link": "http://arxiv.org/abs/1810.12859v1", "categories": ["cs.CY", "cs.CL", "cs.NI"], "primary_category": "cs.CY"}
{"title": "Topic-Specific Sentiment Analysis Can Help Identify Political Ideology", "abstract": "Ideological leanings of an individual can often be gauged by the sentiment\none expresses about different issues. We propose a simple framework that\nrepresents a political ideology as a distribution of sentiment polarities\ntowards a set of topics. This representation can then be used to detect\nideological leanings of documents (speeches, news articles, etc.) based on the\nsentiments expressed towards different topics. Experiments performed using a\nwidely used dataset show the promise of our proposed approach that achieves\ncomparable performance to other methods despite being much simpler and more\ninterpretable.", "published": "2018-10-30 17:49:46", "link": "http://arxiv.org/abs/1810.12897v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "NPRF: A Neural Pseudo Relevance Feedback Framework for Ad-hoc\n  Information Retrieval", "abstract": "Pseudo-relevance feedback (PRF) is commonly used to boost the performance of\ntraditional information retrieval (IR) models by using top-ranked documents to\nidentify and weight new query terms, thereby reducing the effect of\nquery-document vocabulary mismatches. While neural retrieval models have\nrecently demonstrated strong results for ad-hoc retrieval, combining them with\nPRF is not straightforward due to incompatibilities between existing PRF\napproaches and neural architectures. To bridge this gap, we propose an\nend-to-end neural PRF framework that can be used with existing neural IR models\nby embedding different neural models as building blocks. Extensive experiments\non two standard test collections confirm the effectiveness of the proposed NPRF\nframework in improving the performance of two state-of-the-art neural IR\nmodels.", "published": "2018-10-30 18:03:12", "link": "http://arxiv.org/abs/1810.12936v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Bi-Directional Lattice Recurrent Neural Networks for Confidence\n  Estimation", "abstract": "The standard approach to mitigate errors made by an automatic speech\nrecognition system is to use confidence scores associated with each predicted\nword. In the simplest case, these scores are word posterior probabilities\nwhilst more complex schemes utilise bi-directional recurrent neural network\n(BiRNN) models. A number of upstream and downstream applications, however, rely\non confidence scores assigned not only to 1-best hypotheses but to all words\nfound in confusion networks or lattices. These include but are not limited to\nspeaker adaptation, semi-supervised training and information retrieval.\nAlthough word posteriors could be used in those applications as confidence\nscores, they are known to have reliability issues. To make improved confidence\nscores more generally available, this paper shows how BiRNNs can be extended\nfrom 1-best sequences to confusion network and lattice structures. Experiments\nare conducted using one of the Cambridge University submissions to the IARPA\nOpenKWS 2016 competition. The results show that confusion network and\nlattice-based BiRNNs can provide a significant improvement in confidence\nestimation.", "published": "2018-10-30 22:39:02", "link": "http://arxiv.org/abs/1810.13024v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Confidence Estimation and Deletion Prediction Using Bidirectional\n  Recurrent Neural Networks", "abstract": "The standard approach to assess reliability of automatic speech\ntranscriptions is through the use of confidence scores. If accurate, these\nscores provide a flexible mechanism to flag transcription errors for upstream\nand downstream applications. One challenging type of errors that recognisers\nmake are deletions. These errors are not accounted for by the standard\nconfidence estimation schemes and are hard to rectify in the upstream and\ndownstream processing. High deletion rates are prominent in limited resource\nand highly mismatched training/testing conditions studied under IARPA Babel and\nMaterial programs. This paper looks at the use of bidirectional recurrent\nneural networks to yield confidence estimates in predicted as well as deleted\nwords. Several simple schemes are examined for combination. To assess\nusefulness of this approach, the combined confidence score is examined for\nuntranscribed data selection that favours transcriptions with lower deletion\nerrors. Experiments are conducted using IARPA Babel/Material program languages.", "published": "2018-10-30 22:39:54", "link": "http://arxiv.org/abs/1810.13025v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Word Mover's Embedding: From Word2Vec to Document Embedding", "abstract": "While the celebrated Word2Vec technique yields semantically rich\nrepresentations for individual words, there has been relatively less success in\nextending to generate unsupervised sentences or documents embeddings. Recent\nwork has demonstrated that a distance measure between documents called\n\\emph{Word Mover's Distance} (WMD) that aligns semantically similar words,\nyields unprecedented KNN classification accuracy. However, WMD is expensive to\ncompute, and it is hard to extend its use beyond a KNN classifier. In this\npaper, we propose the \\emph{Word Mover's Embedding } (WME), a novel approach to\nbuilding an unsupervised document (sentence) embedding from pre-trained word\nembeddings. In our experiments on 9 benchmark text classification datasets and\n22 textual similarity tasks, the proposed technique consistently matches or\noutperforms state-of-the-art techniques, with significantly higher accuracy on\nproblems of short length.", "published": "2018-10-30 19:43:17", "link": "http://arxiv.org/abs/1811.01713v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "The Airbus Air Traffic Control speech recognition 2018 challenge:\n  towards ATC automatic transcription and call sign detection", "abstract": "In this paper, we describe the outcomes of the challenge organized and run by\nAirbus and partners in 2018. The challenge consisted of two tasks applied to\nAir Traffic Control (ATC) speech in English: 1) automatic speech-to-text\ntranscription, 2) call sign detection (CSD). The registered participants were\nprovided with 40 hours of speech along with manual transcriptions. Twenty-two\nteams submitted predictions on a five hour evaluation set. ATC speech\nprocessing is challenging for several reasons: high speech rate,\nforeign-accented speech with a great diversity of accents, noisy communication\nchannels. The best ranked team achieved a 7.62% Word Error Rate and a 82.41%\nCSD F1-score. Transcribing pilots' speech was found to be twice as harder as\ncontrollers' speech. Remaining issues towards solving ATC ASR are also\ndiscussed.", "published": "2018-10-30 09:54:44", "link": "http://arxiv.org/abs/1810.12614v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SubSpectralNet - Using Sub-Spectrogram based Convolutional Neural\n  Networks for Acoustic Scene Classification", "abstract": "Acoustic Scene Classification (ASC) is one of the core research problems in\nthe field of Computational Sound Scene Analysis. In this work, we present\nSubSpectralNet, a novel model which captures discriminative features by\nincorporating frequency band-level differences to model soundscapes. Using\nmel-spectrograms, we propose the idea of using band-wise crops of the input\ntime-frequency representations and train a convolutional neural network (CNN)\non the same. We also propose a modification in the training method for more\nefficient learning of the CNN models. We first give a motivation for using\nsub-spectrograms by giving intuitive and statistical analyses and finally we\ndevelop a sub-spectrogram based CNN architecture for ASC. The system is\nevaluated on the public ASC development dataset provided for the \"Detection and\nClassification of Acoustic Scenes and Events\" (DCASE) 2018 Challenge. Our best\nmodel achieves an improvement of +14% in terms of classification accuracy with\nrespect to the DCASE 2018 baseline system. Code and figures are available at\nhttps://github.com/ssrp/SubSpectralNet", "published": "2018-10-30 10:37:47", "link": "http://arxiv.org/abs/1810.12642v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Streamlined Encoder/Decoder Architecture for Melody Extraction", "abstract": "Melody extraction in polyphonic musical audio is important for music signal\nprocessing. In this paper, we propose a novel streamlined encoder/decoder\nnetwork that is designed for the task. We make two technical contributions.\nFirst, drawing inspiration from a state-of-the-art model for semantic\npixel-wise segmentation, we pass through the pooling indices between pooling\nand un-pooling layers to localize the melody in frequency. We can achieve\nresult close to the state-of-the-art with much fewer convolutional layers and\nsimpler convolution modules. Second, we propose a way to use the bottleneck\nlayer of the network to estimate the existence of a melody line for each time\nframe, and make it possible to use a simple argmax function instead of ad-hoc\nthresholding to get the final estimation of the melody line. Our experiments on\nboth vocal melody extraction and general melody extraction validate the\neffectiveness of the proposed model.", "published": "2018-10-30 18:15:03", "link": "http://arxiv.org/abs/1810.12947v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Waveform generation for text-to-speech synthesis using pitch-synchronous\n  multi-scale generative adversarial networks", "abstract": "The state-of-the-art in text-to-speech synthesis has recently improved\nconsiderably due to novel neural waveform generation methods, such as WaveNet.\nHowever, these methods suffer from their slow sequential inference process,\nwhile their parallel versions are difficult to train and even more expensive\ncomputationally. Meanwhile, generative adversarial networks (GANs) have\nachieved impressive results in image generation and are making their way into\naudio applications; parallel inference is among their lucrative properties. By\nadopting recent advances in GAN training techniques, this investigation studies\nwaveform generation for TTS in two domains (speech signal and glottal\nexcitation). Listening test results show that while direct waveform generation\nwith GAN is still far behind WaveNet, a GAN-based glottal excitation model can\nachieve quality and voice similarity on par with a WaveNet vocoder.", "published": "2018-10-30 09:23:56", "link": "http://arxiv.org/abs/1810.12598v1", "categories": ["eess.AS", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Generative Adversarial Networks for Unpaired Voice Transformation on\n  Impaired Speech", "abstract": "This paper focuses on using voice conversion (VC) to improve the speech\nintelligibility of surgical patients who have had parts of their articulators\nremoved. Due to the difficulty of data collection, VC without parallel data is\nhighly desired. Although techniques for unparallel VC, for example, CycleGAN,\nhave been developed, they usually focus on transforming the speaker identity,\nand directly transforming the speech of one speaker to that of another speaker\nand as such do not address the task here. In this paper, we propose a new\napproach for unparallel VC. The proposed approach transforms impaired speech to\nnormal speech while preserving the linguistic content and speaker\ncharacteristics. To our knowledge, this is the first end-to-end GAN-based\nunsupervised VC model applied to impaired speech. The experimental results show\nthat the proposed approach outperforms CycleGAN.", "published": "2018-10-30 11:11:14", "link": "http://arxiv.org/abs/1810.12656v3", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Feature Trajectory Dynamic Time Warping for Clustering of Speech\n  Segments", "abstract": "Dynamic time warping (DTW) can be used to compute the similarity between two\nsequences of generally differing length. We propose a modification to DTW that\nperforms individual and independent pairwise alignment of feature trajectories.\nThe modified technique, termed feature trajectory dynamic time warping (FTDTW),\nis applied as a similarity measure in the agglomerative hierarchical clustering\nof speech segments. Experiments using MFCC and PLP parametrisations extracted\nfrom TIMIT and from the Spoken Arabic Digit Dataset (SADD) show consistent and\nstatistically significant improvements in the quality of the resulting clusters\nin terms of F-measure and normalised mutual information (NMI).", "published": "2018-10-30 13:30:19", "link": "http://arxiv.org/abs/1810.12722v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Sparse Gaussian Process Audio Source Separation Using Spectrum Priors in\n  the Time-Domain", "abstract": "Gaussian process (GP) audio source separation is a time-domain approach that\ncircumvents the inherent phase approximation issue of spectrogram based\nmethods. Furthermore, through its kernel, GPs elegantly incorporate prior\nknowledge about the sources into the separation model. Despite these compelling\nadvantages, the computational complexity of GP inference scales cubically with\nthe number of audio samples. As a result, source separation GP models have been\nrestricted to the analysis of short audio frames. We introduce an efficient\napplication of GPs to time-domain audio source separation, without compromising\nperformance. For this purpose, we used GP regression, together with spectral\nmixture kernels, and variational sparse GPs. We compared our method with\nLD-PSDTF (positive semi-definite tensor factorization), KL-NMF\n(Kullback-Leibler non-negative matrix factorization), and IS-NMF (Itakura-Saito\nNMF). Results show that the proposed method outperforms these techniques.", "published": "2018-10-30 11:46:35", "link": "http://arxiv.org/abs/1810.12679v3", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP", "stat.ML"], "primary_category": "eess.AS"}
