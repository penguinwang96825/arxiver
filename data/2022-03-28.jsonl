{"title": "Interpretable Research Replication Prediction via Variational Contextual\n  Consistency Sentence Masking", "abstract": "Research Replication Prediction (RRP) is the task of predicting whether a\npublished research result can be replicated or not. Building an interpretable\nneural text classifier for RRP promotes the understanding of why a research\npaper is predicted as replicable or non-replicable and therefore makes its\nreal-world application more reliable and trustworthy. However, the prior works\non model interpretation mainly focused on improving the model interpretability\nat the word/phrase level, which are insufficient especially for long research\npapers in RRP. Furthermore, the existing methods cannot utilize a large size of\nunlabeled dataset to further improve the model interpretability. To address\nthese limitations, we aim to build an interpretable neural model which can\nprovide sentence-level explanations and apply weakly supervised approach to\nfurther leverage the large corpus of unlabeled datasets to boost the\ninterpretability in addition to improving prediction performance as existing\nworks have done. In this work, we propose the Variational Contextual\nConsistency Sentence Masking (VCCSM) method to automatically extract key\nsentences based on the context in the classifier, using both labeled and\nunlabeled datasets. Results of our experiments on RRP along with European\nConvention of Human Rights (ECHR) datasets demonstrate that VCCSM is able to\nimprove the model interpretability for the long document classification tasks\nusing the area over the perturbation curve and post-hoc accuracy as evaluation\nmetrics.", "published": "2022-03-28 03:27:13", "link": "http://arxiv.org/abs/2203.14474v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ANNA: Enhanced Language Representation for Question Answering", "abstract": "Pre-trained language models have brought significant improvements in\nperformance in a variety of natural language processing tasks. Most existing\nmodels performing state-of-the-art results have shown their approaches in the\nseparate perspectives of data processing, pre-training tasks, neural network\nmodeling, or fine-tuning. In this paper, we demonstrate how the approaches\naffect performance individually, and that the language model performs the best\nresults on a specific question answering task when those approaches are jointly\nconsidered in pre-training models. In particular, we propose an extended\npre-training task, and a new neighbor-aware mechanism that attends neighboring\ntokens more to capture the richness of context for pre-training language\nmodeling. Our best model achieves new state-of-the-art results of 95.7\\% F1 and\n90.6\\% EM on SQuAD 1.1 and also outperforms existing pre-trained language\nmodels such as RoBERTa, ALBERT, ELECTRA, and XLNet on the SQuAD 2.0 benchmark.", "published": "2022-03-28 05:26:52", "link": "http://arxiv.org/abs/2203.14507v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Moral Debater: A Study on the Computational Generation of Morally\n  Framed Arguments", "abstract": "An audience's prior beliefs and morals are strong indicators of how likely\nthey will be affected by a given argument. Utilizing such knowledge can help\nfocus on shared values to bring disagreeing parties towards agreement. In\nargumentation technology, however, this is barely exploited so far. This paper\nstudies the feasibility of automatically generating morally framed arguments as\nwell as their effect on different audiences. Following the moral foundation\ntheory, we propose a system that effectively generates arguments focusing on\ndifferent morals. In an in-depth user study, we ask liberals and conservatives\nto evaluate the impact of these arguments. Our results suggest that,\nparticularly when prior beliefs are challenged, an audience becomes more\naffected by morally framed arguments.", "published": "2022-03-28 08:07:13", "link": "http://arxiv.org/abs/2203.14563v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The SAME score: Improved cosine based bias score for word embeddings", "abstract": "With the enourmous popularity of large language models, many researchers have\nraised ethical concerns regarding social biases incorporated in such models.\nSeveral methods to measure social bias have been introduced, but apparently\nthese methods do not necessarily agree regarding the presence or severity of\nbias. Furthermore, some works have shown theoretical issues or severe\nlimitations with certain bias measures. For that reason, we introduce SAME, a\nnovel bias score for semantic bias in embeddings. We conduct a thorough\ntheoretical analysis as well as experiments to show its benefits compared to\nsimilar bias scores from the literature. We further highlight a substantial\nrelation of semantic bias measured by SAME with downstream bias, a connection\nthat has recently been argued to be negligible. Instead, we show that SAME is\ncapable of measuring semantic bias and identify potential causes for social\nbias in downstream tasks.", "published": "2022-03-28 09:28:13", "link": "http://arxiv.org/abs/2203.14603v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Isomorphic Cross-lingual Embeddings for Low-Resource Languages", "abstract": "Cross-Lingual Word Embeddings (CLWEs) are a key component to transfer\nlinguistic information learnt from higher-resource settings into lower-resource\nones. Recent research in cross-lingual representation learning has focused on\noffline mapping approaches due to their simplicity, computational efficacy, and\nability to work with minimal parallel resources. However, they crucially depend\non the assumption of embedding spaces being approximately isomorphic i.e.\nsharing similar geometric structure, which does not hold in practice, leading\nto poorer performance on low-resource and distant language pairs. In this\npaper, we introduce a framework to learn CLWEs, without assuming isometry, for\nlow-resource pairs via joint exploitation of a related higher-resource\nlanguage. In our work, we first pre-align the low-resource and related language\nembedding spaces using offline methods to mitigate the assumption of isometry.\nFollowing this, we use joint training methods to develops CLWEs for the related\nlanguage and the target embed-ding space. Finally, we remap the pre-aligned\nlow-resource space and the target space to generate the final CLWEs. We show\nconsistent gains over current methods in both quality and degree of\nisomorphism, as measured by bilingual lexicon induction (BLI) and eigenvalue\nsimilarity respectively, across several language pairs: {Nepali, Finnish,\nRomanian, Gujarati, Hungarian}-English. Lastly, our analysis also points to the\nrelatedness as well as the amount of related language data available as being\nkey factors in determining the quality of embeddings achieved.", "published": "2022-03-28 10:39:07", "link": "http://arxiv.org/abs/2203.14632v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Debate Evaluation with Argumentation Semantics and Natural\n  Language Argument Graph Networks", "abstract": "The lack of annotated data on professional argumentation and complete\nargumentative debates has led to the oversimplification and the inability of\napproaching more complex natural language processing tasks. Such is the case of\nthe automatic debate evaluation. In this paper, we propose an original hybrid\nmethod to automatically evaluate argumentative debates. For that purpose, we\ncombine concepts from argumentation theory such as argumentation frameworks and\nsemantics, with Transformer-based architectures and neural graph networks.\nFurthermore, we obtain promising results that lay the basis on an unexplored\nnew instance of the automatic analysis of natural language arguments.", "published": "2022-03-28 11:09:07", "link": "http://arxiv.org/abs/2203.14647v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transformer Feed-Forward Layers Build Predictions by Promoting Concepts\n  in the Vocabulary Space", "abstract": "Transformer-based language models (LMs) are at the core of modern NLP, but\ntheir internal prediction construction process is opaque and largely not\nunderstood. In this work, we make a substantial step towards unveiling this\nunderlying prediction process, by reverse-engineering the operation of the\nfeed-forward network (FFN) layers, one of the building blocks of transformer\nmodels. We view the token representation as a changing distribution over the\nvocabulary, and the output from each FFN layer as an additive update to that\ndistribution. Then, we analyze the FFN updates in the vocabulary space, showing\nthat each update can be decomposed to sub-updates corresponding to single FFN\nparameter vectors, each promoting concepts that are often human-interpretable.\nWe then leverage these findings for controlling LM predictions, where we reduce\nthe toxicity of GPT2 by almost 50%, and for improving computation efficiency\nwith a simple early exit rule, saving 20% of computation on average.", "published": "2022-03-28 12:26:00", "link": "http://arxiv.org/abs/2203.14680v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using Domain Knowledge for Low Resource Named Entity Recognition", "abstract": "In recent years, named entity recognition has always been a popular research\nin the field of natural language processing, while traditional deep learning\nmethods require a large amount of labeled data for model training, which makes\nthem not suitable for areas where labeling resources are scarce. In addition,\nthe existing cross-domain knowledge transfer methods need to adjust the entity\nlabels for different fields, so as to increase the training cost. To solve\nthese problems, enlightened by a processing method of Chinese named entity\nrecognition, we propose to use domain knowledge to improve the performance of\nnamed entity recognition in areas with low resources. The domain knowledge\nmainly applied by us is domain dictionary and domain labeled data. We use\ndictionary information for each word to strengthen its word embedding and\ndomain labeled data to reinforce the recognition effect. The proposed model\navoids large-scale data adjustments in different domains while handling named\nentities recognition with low resources. Experiments demonstrate the\neffectiveness of our method, which has achieved impressive results on the data\nset in the field of scientific and technological equipment, and the F1 score\nhas been significantly improved compared with many other baseline methods.", "published": "2022-03-28 13:26:47", "link": "http://arxiv.org/abs/2203.14738v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UTSA NLP at SemEval-2022 Task 4: An Exploration of Simple Ensembles of\n  Transformers, Convolutional, and Recurrent Neural Networks", "abstract": "The act of appearing kind or helpful via the use of but having a feeling of\nsuperiority condescending and patronizing language can have have serious mental\nhealth implications to those that experience it. Thus, detecting this\ncondescending and patronizing language online can be useful for online\nmoderation systems. Thus, in this manuscript, we describe the system developed\nby Team UTSA SemEval-2022 Task 4, Detecting Patronizing and Condescending\nLanguage. Our approach explores the use of several deep learning architectures\nincluding RoBERTa, convolutions neural networks, and Bidirectional Long\nShort-Term Memory Networks. Furthermore, we explore simple and effective\nmethods to create ensembles of neural network models. Overall, we experimented\nwith several ensemble models and found that the a simple combination of five\nRoBERTa models achieved an F-score of .6441 on the development dataset and\n.5745 on the final test dataset. Finally, we also performed a comprehensive\nerror analysis to better understand the limitations of the model and provide\nideas for further research.", "published": "2022-03-28 17:17:12", "link": "http://arxiv.org/abs/2203.14920v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Well-Composed Text is Half Done! Composition Sampling for Diverse\n  Conditional Generation", "abstract": "We propose Composition Sampling, a simple but effective method to generate\ndiverse outputs for conditional generation of higher quality compared to\nprevious stochastic decoding strategies. It builds on recently proposed\nplan-based neural generation models (Narayan et al, 2021) that are trained to\nfirst create a composition of the output and then generate by conditioning on\nit and the input. Our approach avoids text degeneration by first sampling a\ncomposition in the form of an entity chain and then using beam search to\ngenerate the best possible text grounded to this entity chain. Experiments on\nsummarization (CNN/DailyMail and XSum) and question generation (SQuAD), using\nexisting and newly proposed automatic metrics together with human-based\nevaluation, demonstrate that Composition Sampling is currently the best\navailable decoding strategy for generating diverse meaningful outputs.", "published": "2022-03-28 21:24:03", "link": "http://arxiv.org/abs/2203.15108v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large-scale Bilingual Language-Image Contrastive Learning", "abstract": "This paper is a technical report to share our experience and findings\nbuilding a Korean and English bilingual multimodal model. While many of the\nmultimodal datasets focus on English and multilingual multimodal research uses\nmachine-translated texts, employing such machine-translated texts is limited to\ndescribing unique expressions, cultural information, and proper noun in\nlanguages other than English. In this work, we collect 1.1 billion image-text\npairs (708 million Korean and 476 million English) and train a bilingual\nmultimodal model named KELIP. We introduce simple yet effective training\nschemes, including MAE pre-training and multi-crop augmentation. Extensive\nexperiments demonstrate that a model trained with such training schemes shows\ncompetitive performance in both languages. Moreover, we discuss\nmultimodal-related research questions: 1) strong augmentation-based methods can\ndistract the model from learning proper multimodal relations; 2) training\nmultimodal model without cross-lingual relation can learn the relation via\nvisual semantics; 3) our bilingual KELIP can capture cultural differences of\nvisual semantics for the same meaning of words; 4) a large-scale multimodal\nmodel can be used for multimodal feature analogy. We hope that this work will\nprovide helpful experience and findings for future research. We provide an\nopen-source pre-trained KELIP.", "published": "2022-03-28 03:02:03", "link": "http://arxiv.org/abs/2203.14463v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Specialized Document Embeddings for Aspect-based Similarity of Research\n  Papers", "abstract": "Document embeddings and similarity measures underpin content-based\nrecommender systems, whereby a document is commonly represented as a single\ngeneric embedding. However, similarity computed on single vector\nrepresentations provides only one perspective on document similarity that\nignores which aspects make two documents alike. To address this limitation,\naspect-based similarity measures have been developed using document\nsegmentation or pairwise multi-class document classification. While\nsegmentation harms the document coherence, the pairwise classification approach\nscales poorly to large scale corpora. In this paper, we treat aspect-based\nsimilarity as a classical vector similarity problem in aspect-specific\nembedding spaces. We represent a document not as a single generic embedding but\nas multiple specialized embeddings. Our approach avoids document segmentation\nand scales linearly w.r.t.the corpus size. In an empirical study, we use the\nPapers with Code corpus containing 157,606 research papers and consider the\ntask, method, and dataset of the respective research papers as their aspects.\nWe compare and analyze three generic document embeddings, six specialized\ndocument embeddings and a pairwise classification baseline in the context of\nresearch paper recommendations. As generic document embeddings, we consider\nFastText, SciBERT, and SPECTER. To compute the specialized document embeddings,\nwe compare three alternative methods inspired by retrofitting, fine-tuning, and\nSiamese networks. In our experiments, Siamese SciBERT achieved the highest\nscores. Additional analyses indicate an implicit bias of the generic document\nembeddings towards the dataset aspect and against the method aspect of each\nresearch paper. Our approach of aspect-based document embeddings mitigates\npotential risks arising from implicit biases by making them explicit.", "published": "2022-03-28 07:35:26", "link": "http://arxiv.org/abs/2203.14541v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Few-Shot Learning with Siamese Networks and Label Tuning", "abstract": "We study the problem of building text classifiers with little or no training\ndata, commonly known as zero and few-shot text classification. In recent years,\nan approach based on neural textual entailment models has been found to give\nstrong results on a diverse range of tasks. In this work, we show that with\nproper pre-training, Siamese Networks that embed texts and labels offer a\ncompetitive alternative. These models allow for a large reduction in inference\ncost: constant in the number of labels rather than linear. Furthermore, we\nintroduce label tuning, a simple and computationally efficient approach that\nallows to adapt the models in a few-shot setup by only changing the label\nembeddings. While giving lower performance than model fine-tuning, this\napproach has the architectural advantage that a single encoder can be shared by\nmany different tasks.", "published": "2022-03-28 11:16:46", "link": "http://arxiv.org/abs/2203.14655v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Hierarchical Transformer Model for Scientific Named Entity Recognition", "abstract": "The task of Named Entity Recognition (NER) is an important component of many\nnatural language processing systems, such as relation extraction and knowledge\ngraph construction. In this work, we present a simple and effective approach\nfor Named Entity Recognition. The main idea of our approach is to encode the\ninput subword sequence with a pre-trained transformer such as BERT, and then,\ninstead of directly classifying the word labels, another layer of transformer\nis added to the subword representation to better encode the word-level\ninteraction. We evaluate our approach on three benchmark datasets for\nscientific NER, particularly in the computer science and biomedical domains.\nExperimental results show that our model outperforms the current\nstate-of-the-art on SciERC and TDM datasets without requiring external\nresources or specific data augmentation. Code is available at\n\\url{https://github.com/urchade/HNER}.", "published": "2022-03-28 12:59:06", "link": "http://arxiv.org/abs/2203.14710v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multilingual Knowledge Graph Completion with Self-Supervised Adaptive\n  Graph Alignment", "abstract": "Predicting missing facts in a knowledge graph (KG) is crucial as modern KGs\nare far from complete. Due to labor-intensive human labeling, this phenomenon\ndeteriorates when handling knowledge represented in various languages. In this\npaper, we explore multilingual KG completion, which leverages limited seed\nalignment as a bridge, to embrace the collective knowledge from multiple\nlanguages. However, language alignment used in prior works is still not fully\nexploited: (1) alignment pairs are treated equally to maximally push parallel\nentities to be close, which ignores KG capacity inconsistency; (2) seed\nalignment is scarce and new alignment identification is usually in a noisily\nunsupervised manner. To tackle these issues, we propose a novel self-supervised\nadaptive graph alignment (SS-AGA) method. Specifically, SS-AGA fuses all KGs as\na whole graph by regarding alignment as a new edge type. As such, information\npropagation and noise influence across KGs can be adaptively controlled via\nrelation-aware attention weights. Meanwhile, SS-AGA features a new pair\ngenerator that dynamically captures potential alignment pairs in a\nself-supervised paradigm. Extensive experiments on both the public multilingual\nDBPedia KG and newly-created industrial multilingual E-commerce KG empirically\ndemonstrate the effectiveness of SS-AG", "published": "2022-03-28 18:00:51", "link": "http://arxiv.org/abs/2203.14987v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Comparing in context: Improving cosine similarity measures with a metric\n  tensor", "abstract": "Cosine similarity is a widely used measure of the relatedness of pre-trained\nword embeddings, trained on a language modeling goal. Datasets such as\nWordSim-353 and SimLex-999 rate how similar words are according to human\nannotators, and as such are often used to evaluate the performance of language\nmodels. Thus, any improvement on the word similarity task requires an improved\nword representation. In this paper, we propose instead the use of an extended\ncosine similarity measure to improve performance on that task, with gains in\ninterpretability. We explore the hypothesis that this approach is particularly\nuseful if the word-similarity pairs share the same context, for which distinct\ncontextualized similarity measures can be learned. We first use the dataset of\nRichie et al. (2020) to learn contextualized metrics and compare the results\nwith the baseline values obtained using the standard cosine similarity measure,\nwhich consistently shows improvement. We also train a contextualized similarity\nmeasure for both SimLex-999 and WordSim-353, comparing the results with the\ncorresponding baselines, and using these datasets as independent test sets for\nthe all-context similarity measure learned on the contextualized dataset,\nobtaining positive results for a number of tests.", "published": "2022-03-28 18:04:26", "link": "http://arxiv.org/abs/2203.14996v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Federated Named Entity Recognition", "abstract": "We present an analysis of the performance of Federated Learning in a\nparadigmatic natural-language processing task: Named-Entity Recognition (NER).\nFor our evaluation, we use the language-independent CoNLL-2003 dataset as our\nbenchmark dataset and a Bi-LSTM-CRF model as our benchmark NER model. We show\nthat federated training reaches almost the same performance as the centralized\nmodel, though with some performance degradation as the learning environments\nbecome more heterogeneous. We also show the convergence rate of federated\nmodels for NER. Finally, we discuss existing challenges of Federated Learning\nfor NLP applications that can foster future research directions.", "published": "2022-03-28 21:14:49", "link": "http://arxiv.org/abs/2203.15101v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Generative Design Ideation: A Natural Language Generation Approach", "abstract": "This paper aims to explore a generative approach for knowledge-based design\nideation by applying the latest pre-trained language models in artificial\nintelligence (AI). Specifically, a method of fine-tuning the generative\npre-trained transformer using the USPTO patent database is proposed. The\nAI-generated ideas are not only in concise and understandable language but also\nable to synthesize the target design with external knowledge sources with\ncontrollable knowledge distance. The method is tested in a case study of\nrolling toy design and the results show good performance in generating ideas of\nvaried novelty with near-field and far-field source knowledge.", "published": "2022-03-28 08:11:29", "link": "http://arxiv.org/abs/2204.09658v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "STaR: Bootstrapping Reasoning With Reasoning", "abstract": "Generating step-by-step \"chain-of-thought\" rationales improves language model\nperformance on complex reasoning tasks like mathematics or commonsense\nquestion-answering. However, inducing language model rationale generation\ncurrently requires either constructing massive rationale datasets or\nsacrificing accuracy by using only few-shot inference. We propose a technique\nto iteratively leverage a small number of rationale examples and a large\ndataset without rationales, to bootstrap the ability to perform successively\nmore complex reasoning. This technique, the \"Self-Taught Reasoner\" (STaR),\nrelies on a simple loop: generate rationales to answer many questions, prompted\nwith a few rationale examples; if the generated answers are wrong, try again to\ngenerate a rationale given the correct answer; fine-tune on all the rationales\nthat ultimately yielded correct answers; repeat. We show that STaR\nsignificantly improves performance on multiple datasets compared to a model\nfine-tuned to directly predict final answers, and performs comparably to\nfine-tuning a 30$\\times$ larger state-of-the-art language model on\nCommensenseQA. Thus, STaR lets a model improve itself by learning from its own\ngenerated reasoning.", "published": "2022-03-28 03:12:15", "link": "http://arxiv.org/abs/2203.14465v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "EnCBP: A New Benchmark Dataset for Finer-Grained Cultural Background\n  Prediction in English", "abstract": "While cultural backgrounds have been shown to affect linguistic expressions,\nexisting natural language processing (NLP) research on culture modeling is\noverly coarse-grained and does not examine cultural differences among speakers\nof the same language. To address this problem and augment NLP models with\ncultural background features, we collect, annotate, manually validate, and\nbenchmark EnCBP, a finer-grained news-based cultural background prediction\ndataset in English. Through language modeling (LM) evaluations and manual\nanalyses, we confirm that there are noticeable differences in linguistic\nexpressions among five English-speaking countries and across four states in the\nUS. Additionally, our evaluations on nine syntactic (CoNLL-2003), semantic\n(PAWS-Wiki, QNLI, STS-B, and RTE), and psycholinguistic tasks (SST-5, SST-2,\nEmotion, and Go-Emotions) show that, while introducing cultural background\ninformation does not benefit the Go-Emotions task due to text domain conflicts,\nit noticeably improves deep learning (DL) model performance on other tasks. Our\nfindings strongly support the importance of cultural background modeling to a\nwide variety of NLP tasks and demonstrate the applicability of EnCBP in\nculture-related research.", "published": "2022-03-28 04:57:17", "link": "http://arxiv.org/abs/2203.14498v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Computer Science Named Entity Recognition in the Open Research Knowledge\n  Graph", "abstract": "Domain-specific named entity recognition (NER) on Computer Science (CS)\nscholarly articles is an information extraction task that is arguably more\nchallenging for the various annotation aims that can beset the task and has\nbeen less studied than NER in the general domain. Given that significant\nprogress has been made on NER, we believe that scholarly domain-specific NER\nwill receive increasing attention in the years to come. Currently, progress on\nCS NER -- the focus of this work -- is hampered in part by its recency and the\nlack of a standardized annotation aim for scientific entities/terms. This work\nproposes a standardized task by defining a set of seven contribution-centric\nscholarly entities for CS NER viz., research problem, solution, resource,\nlanguage, tool, method, and dataset. Following which, its main contributions\nare: combines existing CS NER resources that maintain their annotation focus on\nthe set or subset of contribution-centric scholarly entities we consider;\nfurther, noting the need for big data to train neural NER models, this work\nadditionally supplies thousands of contribution-centric entity annotations from\narticle titles and abstracts, thus releasing a cumulative large novel resource\nfor CS NER; and, finally, trains a sequence labeling CS NER model inspired\nafter state-of-the-art neural architectures from the general domain NER task.\nThroughout the work, several practical considerations are made which can be\nuseful to information technology designers of the digital libraries.", "published": "2022-03-28 08:44:43", "link": "http://arxiv.org/abs/2203.14579v2", "categories": ["cs.CL", "cs.DL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multilingual Simultaneous Speech Translation", "abstract": "Applications designed for simultaneous speech translation during events such\nas conferences or meetings need to balance quality and lag while displaying\ntranslated text to deliver a good user experience. One common approach to\nbuilding online spoken language translation systems is by leveraging models\nbuilt for offline speech translation. Based on a technique to adapt end-to-end\nmonolingual models, we investigate multilingual models and different\narchitectures (end-to-end and cascade) on the ability to perform online speech\ntranslation. On the multilingual TEDx corpus, we show that the approach\ngeneralizes to different architectures. We see similar gains in latency\nreduction (40% relative) across languages and architectures. However, the\nend-to-end architecture leads to smaller translation quality losses after\nadapting to the online model. Furthermore, the approach even scales to\nzero-shot directions.", "published": "2022-03-28 15:17:11", "link": "http://arxiv.org/abs/2203.14835v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Finnish Parliament ASR corpus - Analysis, benchmarks and statistics", "abstract": "Public sources like parliament meeting recordings and transcripts provide\never-growing material for the training and evaluation of automatic speech\nrecognition (ASR) systems. In this paper, we publish and analyse the Finnish\nparliament ASR corpus, the largest publicly available collection of manually\ntranscribed speech data for Finnish with over 3000 hours of speech and 449\nspeakers for which it provides rich demographic metadata. This corpus builds on\nearlier initial work, and as a result the corpus has a natural split into two\ntraining subsets from two periods of time. Similarly, there are two official,\ncorrected test sets covering different times, setting an ASR task with\nlongitudinal distribution-shift characteristics. An official development set is\nalso provided. We develop a complete Kaldi-based data preparation pipeline, and\nhidden Markov model (HMM), hybrid deep neural network (HMM-DNN) and\nattention-based encoder-decoder (AED) ASR recipes. We set benchmarks on the\nofficial test sets, as well as multiple other recently used test sets. Both\ntemporal corpus subsets are already large, and we observe that beyond their\nscale, ASR performance on the official test sets plateaus, whereas other\ndomains benefit from added data. The HMM-DNN and AED approaches are compared in\na carefully matched equal data setting, with the HMM-DNN system consistently\nperforming better. Finally, the variation of the ASR accuracy is compared\nbetween the speaker categories available in the parliament metadata to detect\npotential biases based on factors such as gender, age, and education.", "published": "2022-03-28 16:29:49", "link": "http://arxiv.org/abs/2203.14876v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "FedVLN: Privacy-preserving Federated Vision-and-Language Navigation", "abstract": "Data privacy is a central problem for embodied agents that can perceive the\nenvironment, communicate with humans, and act in the real world. While helping\nhumans complete tasks, the agent may observe and process sensitive information\nof users, such as house environments, human activities, etc. In this work, we\nintroduce privacy-preserving embodied agent learning for the task of\nVision-and-Language Navigation (VLN), where an embodied agent navigates house\nenvironments by following natural language instructions. We view each house\nenvironment as a local client, which shares nothing other than local updates\nwith the cloud server and other clients, and propose a novel federated\nvision-and-language navigation (FedVLN) framework to protect data privacy\nduring both training and pre-exploration. Particularly, we propose a\ndecentralized training strategy to limit the data of each client to its local\nmodel training and a federated pre-exploration method to do partial model\naggregation to improve model generalizability to unseen environments. Extensive\nresults on R2R and RxR datasets show that under our FedVLN framework,\ndecentralized VLN models achieve comparable results with centralized training\nwhile protecting seen environment privacy, and federated pre-exploration\nsignificantly outperforms centralized pre-exploration while preserving unseen\nenvironment privacy.", "published": "2022-03-28 17:43:35", "link": "http://arxiv.org/abs/2203.14936v3", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Word Discovery in Visually Grounded, Self-Supervised Speech Models", "abstract": "We present a method for visually-grounded spoken term discovery. After\ntraining either a HuBERT or wav2vec2.0 model to associate spoken captions with\nnatural images, we show that powerful word segmentation and clustering\ncapability emerges within the model's self-attention heads. Our experiments\nreveal that this ability is not present to nearly the same extent in the base\nHuBERT and wav2vec2.0 models, suggesting that the visual grounding task is a\ncrucial component of the word discovery capability we observe. We also evaluate\nour method on the Buckeye word segmentation and ZeroSpeech spoken term\ndiscovery tasks, where we perform on par with or better than currently\npublished methods on several metrics. Code and model weights are available at\nhttps://github.com/jasonppy/word-discovery.", "published": "2022-03-28 20:41:17", "link": "http://arxiv.org/abs/2203.15081v5", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Text2Pos: Text-to-Point-Cloud Cross-Modal Localization", "abstract": "Natural language-based communication with mobile devices and home appliances\nis becoming increasingly popular and has the potential to become natural for\ncommunicating with mobile robots in the future. Towards this goal, we\ninvestigate cross-modal text-to-point-cloud localization that will allow us to\nspecify, for example, a vehicle pick-up or goods delivery location. In\nparticular, we propose Text2Pos, a cross-modal localization module that learns\nto align textual descriptions with localization cues in a coarse- to-fine\nmanner. Given a point cloud of the environment, Text2Pos locates a position\nthat is specified via a natural language-based description of the immediate\nsurroundings. To train Text2Pos and study its performance, we construct\nKITTI360Pose, the first dataset for this task based on the recently introduced\nKITTI360 dataset. Our experiments show that we can localize 65% of textual\nqueries within 15m distance to query locations for top-10 retrieved locations.\nThis is a starting point that we hope will spark future developments towards\nlanguage-based navigation.", "published": "2022-03-28 22:06:00", "link": "http://arxiv.org/abs/2203.15125v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Filler Word Detection and Classification: A Dataset and Benchmark", "abstract": "Filler words such as `uh' or `um' are sounds or words people use to signal\nthey are pausing to think. Finding and removing filler words from recordings is\na common and tedious task in media editing. Automatically detecting and\nclassifying filler words could greatly aid in this task, but few studies have\nbeen published on this problem to date. A key reason is the absence of a\ndataset with annotated filler words for model training and evaluation. In this\nwork, we present a novel speech dataset, PodcastFillers, with 35K annotated\nfiller words and 50K annotations of other sounds that commonly occur in\npodcasts such as breaths, laughter, and word repetitions. We propose a pipeline\nthat leverages VAD and ASR to detect filler candidates and a classifier to\ndistinguish between filler word types. We evaluate our proposed pipeline on\nPodcastFillers, compare to several baselines, and present a detailed ablation\nstudy. In particular, we evaluate the importance of using ASR and how it\ncompares to a transcription-free approach resembling keyword spotting. We show\nthat our pipeline obtains state-of-the-art results, and that leveraging ASR\nstrongly outperforms a keyword spotting approach. We make PodcastFillers\npublicly available, in the hope that our work serves as a benchmark for future\nresearch.", "published": "2022-03-28 22:53:54", "link": "http://arxiv.org/abs/2203.15135v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Human-AI Collaboration Enables More Empathic Conversations in Text-based\n  Peer-to-Peer Mental Health Support", "abstract": "Advances in artificial intelligence (AI) are enabling systems that augment\nand collaborate with humans to perform simple, mechanistic tasks like\nscheduling meetings and grammar-checking text. However, such Human-AI\ncollaboration poses challenges for more complex, creative tasks, such as\ncarrying out empathic conversations, due to difficulties of AI systems in\nunderstanding complex human emotions and the open-ended nature of these tasks.\nHere, we focus on peer-to-peer mental health support, a setting in which\nempathy is critical for success, and examine how AI can collaborate with humans\nto facilitate peer empathy during textual, online supportive conversations. We\ndevelop Hailey, an AI-in-the-loop agent that provides just-in-time feedback to\nhelp participants who provide support (peer supporters) respond more\nempathically to those seeking help (support seekers). We evaluate Hailey in a\nnon-clinical randomized controlled trial with real-world peer supporters on\nTalkLife (N=300), a large online peer-to-peer support platform. We show that\nour Human-AI collaboration approach leads to a 19.60% increase in\nconversational empathy between peers overall. Furthermore, we find a larger\n38.88% increase in empathy within the subsample of peer supporters who\nself-identify as experiencing difficulty providing support. We systematically\nanalyze the Human-AI collaboration patterns and find that peer supporters are\nable to use the AI feedback both directly and indirectly without becoming\noverly reliant on AI while reporting improved self-efficacy post-feedback. Our\nfindings demonstrate the potential of feedback-driven, AI-in-the-loop writing\nsystems to empower humans in open-ended, social, creative tasks such as\nempathic conversations.", "published": "2022-03-28 23:37:08", "link": "http://arxiv.org/abs/2203.15144v1", "categories": ["cs.CL", "cs.HC", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Subspace-based Representation and Learning for Phonotactic Spoken\n  Language Recognition", "abstract": "Phonotactic constraints can be employed to distinguish languages by\nrepresenting a speech utterance as a multinomial distribution or phone events.\nIn the present study, we propose a new learning mechanism based on\nsubspace-based representation, which can extract concealed phonotactic\nstructures from utterances, for language verification and dialect/accent\nidentification. The framework mainly involves two successive parts. The first\npart involves subspace construction. Specifically, it decodes each utterance\ninto a sequence of vectors filled with phone-posteriors and transforms the\nvector sequence into a linear orthogonal subspace based on low-rank matrix\nfactorization or dynamic linear modeling. The second part involves subspace\nlearning based on kernel machines, such as support vector machines and the\nnewly developed subspace-based neural networks (SNNs). The input layer of SNNs\nis specifically designed for the sample represented by subspaces. The topology\nensures that the same output can be derived from identical subspaces by\nmodifying the conventional feed-forward pass to fit the mathematical definition\nof subspace similarity. Evaluated on the \"General LR\" test of NIST LRE 2007,\nthe proposed method achieved up to 52%, 46%, 56%, and 27% relative reductions\nin equal error rates over the sequence-based PPR-LM, PPR-VSM, and PPR-IVEC\nmethods and the lattice-based PPR-LM method, respectively. Furthermore, on the\ndialect/accent identification task of NIST LRE 2009, the SNN-based system\nperformed better than the aforementioned four baseline methods.", "published": "2022-03-28 07:01:45", "link": "http://arxiv.org/abs/2203.15576v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Discovering material information using hierarchical Reformer model on\n  financial regulatory filings", "abstract": "Most applications of machine learning for finance are related to forecasting\ntasks for investment decisions. Instead, we aim to promote a better\nunderstanding of financial markets with machine learning techniques. Leveraging\nthe tremendous progress in deep learning models for natural language\nprocessing, we construct a hierarchical Reformer ([15]) model capable of\nprocessing a large document level dataset, SEDAR, from canadian financial\nregulatory filings. Using this model, we show that it is possible to predict\ntrade volume changes using regulatory filings. We adapt the pretraining task of\nHiBERT ([36]) to obtain good sentence level representations using a large\nunlabelled document dataset. Finetuning the model to successfully predict trade\nvolume changes indicates that the model captures a view from financial markets\nand processing regulatory filings is beneficial. Analyzing the attention\npatterns of our model reveals that it is able to detect some indications of\nmaterial information without explicit training, which is highly relevant for\ninvestors and also for the market surveillance mandate of financial regulators.", "published": "2022-03-28 19:47:34", "link": "http://arxiv.org/abs/2204.05979v1", "categories": ["q-fin.ST", "cs.CL", "cs.LG"], "primary_category": "q-fin.ST"}
{"title": "The MIT Voice Name System", "abstract": "This RFC white Paper summarizes our progress on the MIT Voice Name System\n(VNS) and Huey. The VNS, similar in name and function to the DNS, is a system\nto reserve and use \"wake words\" to activate Artificial Intelligence (AI)\ndevices. Just like you can say \"Hey Siri\" to activate Apple's personal\nassistant, we propose using the VNS in smart speakers and other devices to\nroute wake requests based on commands such as \"turn off\", \"open grocery\nshopping list\" or \"271, start flash card review of my computer vision class\".\nWe also introduce Huey, an unambiguous Natural Language to interact with AI\ndevices. We aim to standardize voice interactions to a universal reach similar\nto that of other systems such as phone numbering, with an agreed world-wide\napproach to assign and use numbers, or the Internet's DNS, with a standard\nnaming system, that has helped flourish popular services including the\nWorld-Wide-Web, FTP, and email. Just like these standards are \"neutral\", we\nalso aim to endow the VNS with \"wake neutrality\" so that each participant can\ndevelop its own digital voice. We focus on voice as a starting point to talk to\nany IoT object and explain briefly how the VNS may be expanded to other AI\ntechnologies enabling person-to-machine conversations (really\nmachine-to-machine), including computer vision or neural interfaces. We also\ndescribe briefly considerations for a broader set of standards, MIT Open AI\n(MOA), including a reference architecture to serve as a starting point for the\ndevelopment of a general conversational commerce infrastructure that has\nstandard \"Wake Words\", NLP commands such as \"Shopping Lists\" or \"Flash Card\nReviews\", and personalities such as Pi or 271. Privacy and security are key\nelements considered because of speech-to-text errors and the amount of personal\ninformation contained in a voice sample.", "published": "2022-03-28 19:09:26", "link": "http://arxiv.org/abs/2204.09657v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "eess.AS", "68T35", "I.2.0"], "primary_category": "cs.CL"}
{"title": "STUDIES: Corpus of Japanese Empathetic Dialogue Speech Towards Friendly\n  Voice Agent", "abstract": "We present STUDIES, a new speech corpus for developing a voice agent that can\nspeak in a friendly manner. Humans naturally control their speech prosody to\nempathize with each other. By incorporating this \"empathetic dialogue\" behavior\ninto a spoken dialogue system, we can develop a voice agent that can respond to\na user more naturally. We designed the STUDIES corpus to include a speaker who\nspeaks with empathy for the interlocutor's emotion explicitly. We describe our\nmethodology to construct an empathetic dialogue speech corpus and report the\nanalysis results of the STUDIES corpus. We conducted a text-to-speech\nexperiment to initially investigate how we can develop more natural voice agent\nthat can tune its speaking style corresponding to the interlocutor's emotion.\nThe results show that the use of interlocutor's emotion label and\nconversational context embedding can produce speech with the same degree of\nnaturalness as that synthesized by using the agent's emotion label. Our project\npage of the STUDIES corpus is http://sython.org/Corpus/STUDIES.", "published": "2022-03-28 13:49:59", "link": "http://arxiv.org/abs/2203.14757v2", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.HC", "cs.LG"], "primary_category": "cs.SD"}
{"title": "Separate What You Describe: Language-Queried Audio Source Separation", "abstract": "In this paper, we introduce the task of language-queried audio source\nseparation (LASS), which aims to separate a target source from an audio mixture\nbased on a natural language query of the target source (e.g., \"a man tells a\njoke followed by people laughing\"). A unique challenge in LASS is associated\nwith the complexity of natural language description and its relation with the\naudio sources. To address this issue, we proposed LASS-Net, an end-to-end\nneural network that is learned to jointly process acoustic and linguistic\ninformation, and separate the target source that is consistent with the\nlanguage query from an audio mixture. We evaluate the performance of our\nproposed system with a dataset created from the AudioCaps dataset. Experimental\nresults show that LASS-Net achieves considerable improvements over baseline\nmethods. Furthermore, we observe that LASS-Net achieves promising\ngeneralization results when using diverse human-annotated descriptions as\nqueries, indicating its potential use in real-world scenarios. The separated\naudio samples and source code are available at\nhttps://liuxubo717.github.io/LASS-demopage.", "published": "2022-03-28 23:47:57", "link": "http://arxiv.org/abs/2203.15147v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Curriculum learning for self-supervised speaker verification", "abstract": "The goal of this paper is to train effective self-supervised speaker\nrepresentations without identity labels. We propose two curriculum learning\nstrategies within a self-supervised learning framework. The first strategy aims\nto gradually increase the number of speakers in the training phase by enlarging\nthe used portion of the train dataset. The second strategy applies various data\naugmentations to more utterances within a mini-batch as the training proceeds.\nA range of experiments conducted using the DINO self-supervised framework on\nthe VoxCeleb1 evaluation protocol demonstrates the effectiveness of our\nproposed curriculum learning strategies. We report a competitive equal error\nrate of 4.47% with a single-phase training, and we also demonstrate that the\nperformance further improves to 1.84% by fine-tuning on a small labelled\ndataset.", "published": "2022-03-28 06:42:24", "link": "http://arxiv.org/abs/2203.14525v4", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Investigating Active-learning-based Training Data Selection for Speech\n  Spoofing Countermeasure", "abstract": "Training a spoofing countermeasure (CM) that generalizes to various unseen\ndata is desired but challenging. While methods such as data augmentation and\nself-supervised learning are applicable, the imperfect CM performance on\ndiverse test sets still calls for additional strategies. This study took the\ninitiative and investigated CM training using active learning (AL), a framework\nthat iteratively selects useful data from a large pool set and fine-tunes the\nCM. This study compared a few methods to measure the data usefulness and the\nimpact of using different pool sets collected from various sources. The results\nshowed that the AL-based CMs achieved better generalization than our strong\nbaseline on multiple test tests. Furthermore, compared with a top-line CM that\nsimply used the whole data pool set for training, the AL-based CMs achieved\nsimilar performance using less training data. Although no single best\nconfiguration was found for AL, the rule of thumb is to include diverse spoof\nand bona fide data in the pool set and to avoid any AL data selection method\nthat selects the data that the CM feels confident in.", "published": "2022-03-28 07:53:03", "link": "http://arxiv.org/abs/2203.14553v3", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "An Effective Dereverberation Algorithm by Fusing MVDR and MCLP", "abstract": "In the scenario with reverberation, the experience of human-machine\ninteraction will become worse. In order to solve this problem, many methods for\nthe dereverberation have emerged. At present, how to update the parameters of\nthe Kalman filter in the existing dereverberation methods based on multichannel\nlinear prediction (MCLP) is a challenging task, especially, accurate power\nspectral density (PSD) estimation of target speech. In this paper, minimum\nvariance distortionless response (MVDR) beamformer and MCLP are effectively\nfused in the dereverberation, where the PSD of target speech used for Kalman\nfilter is modified in the MCLP. In order to construct a MVDR beamformer, the\nPSD of late reverberation and the PSD of the noise are estimated simultaneously\nby the blocking-based PSD estimator. Thus, the PSD of target speech used for\nKalman filter can be obtained by subtracting the PSD of late reverberation and\nthe PSD of the noise from the PSD of observation signal. Compared to the\nreference methods, the proposed method shows an outstanding performance.", "published": "2022-03-28 08:05:35", "link": "http://arxiv.org/abs/2203.14561v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Analysis of Voice Conversion and Code-Switching Synthesis Using VQ-VAE", "abstract": "This paper presents an analysis of speech synthesis quality achieved by\nsimultaneously performing voice conversion and language code-switching using\nmultilingual VQ-VAE speech synthesis in German, French, English and Italian. In\nthis paper, we utilize VQ code indices representing phone information from\nVQ-VAE to perform code-switching and a VQ speaker code to perform voice\nconversion in a single system with a neural vocoder. Our analysis examines\nseveral aspects of code-switching including the number of language switches and\nthe number of words involved in each switch. We found that speech synthesis\nquality degrades after increasing the number of language switches within an\nutterance and decreasing the number of words. We also found some evidence of\naccent transfer when performing voice conversion across languages as observed\nwhen a speaker's original language differs from the language of a synthetic\ntarget utterance. We present results from our listening tests and discuss the\ninherent difficulties of assessing accent transfer in speech synthesis. Our\nwork highlights some of the limitations and strengths of using a\nsemi-supervised end-to-end system like VQ-VAE for handling multilingual\nsynthesis. Our work provides insight into why multilingual speech synthesis is\nchallenging and we suggest some directions for expanding work in this area.", "published": "2022-03-28 10:53:31", "link": "http://arxiv.org/abs/2203.14640v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "SASV 2022: The First Spoofing-Aware Speaker Verification Challenge", "abstract": "The first spoofing-aware speaker verification (SASV) challenge aims to\nintegrate research efforts in speaker verification and anti-spoofing. We extend\nthe speaker verification scenario by introducing spoofed trials to the usual\nset of target and impostor trials. In contrast to the established ASVspoof\nchallenge where the focus is upon separate, independently optimised spoofing\ndetection and speaker verification sub-systems, SASV targets the development of\nintegrated and jointly optimised solutions. Pre-trained spoofing detection and\nspeaker verification models are provided as open source and are used in two\nbaseline SASV solutions. Both models and baselines are freely available to\nparticipants and can be used to develop back-end fusion approaches or\nend-to-end solutions. Using the provided common evaluation protocol, 23 teams\nsubmitted SASV solutions. When assessed with target, bona fide non-target and\nspoofed non-target trials, the top-performing system reduces the equal error\nrate of a conventional speaker verification system from 23.83% to 0.13%. SASV\nchallenge results are a testament to the reliability of today's\nstate-of-the-art approaches to spoofing detection and speaker verification.", "published": "2022-03-28 13:19:14", "link": "http://arxiv.org/abs/2203.14732v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Multi-source wideband doa estimation method by frequency focusing and\n  error weighting", "abstract": "In this paper, a new multi-source wideband direction of arrival (MSW-DOA)\nestimation method is proposed for the signal with non-uniform distribution\nusing the sub-array of uniform linear array. Different from conventional\nmethods, based on the free far-field model, the proposed method mainly makes\ntwo contributions. One is that the sub-array decomposition is adopted to\nimprove the accuracy of MSW-DOA estimation by minimizing the weighted error,\nand the other one is that the frequency focusing procedure is optimized\naccording to the presence probability of sound sources for reducing the\ninfluence of the sub-bands with low signal to noise ratio (SNR). Simulation\nresults show that the proposed method can effectively improve the performance\nof wideband DOA estimation in the case of multiple sound sources.", "published": "2022-03-28 04:44:45", "link": "http://arxiv.org/abs/2203.14494v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SyncNet: Using Causal Convolutions and Correlating Objective for Time\n  Delay Estimation in Audio Signals", "abstract": "This paper addresses the task of performing robust and reliable time-delay\nestimation in audio-signals in noisy and reverberating environments. In\ncontrast to the popular signal processing based methods, this paper proposes\nmachine learning based method, i.e., a semi-causal convolutional neural network\nconsisting of a set of causal and anti-causal layers with a novel\ncorrelation-based objective function. The causality in the network ensures\nnon-leakage of representations from future time-intervals and the proposed loss\nfunction makes the network generate sequences with high correlation at the\nactual time delay. The proposed approach is also intrinsically interpretable as\nit does not lose time information. Even a shallow convolution network is able\nto capture local patterns in sequences, while also correlating them globally.\nSyncNet outperforms other classical approaches in estimating mutual time delays\nfor different types of audio signals including pulse, speech and musical beats.", "published": "2022-03-28 10:53:20", "link": "http://arxiv.org/abs/2203.14639v2", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Towards Transferable Speech Emotion Representation: On loss functions\n  for cross-lingual latent representations", "abstract": "In recent years, speech emotion recognition (SER) has been used in wide\nranging applications, from healthcare to the commercial sector. In addition to\nsignal processing approaches, methods for SER now also use deep learning\ntechniques which provide transfer learning possibilities. However, generalizing\nover languages, corpora and recording conditions is still an open challenge. In\nthis work we address this gap by exploring loss functions that aid in\ntransferability, specifically to non-tonal languages. We propose a variational\nautoencoder (VAE) with KL annealing and a semi-supervised VAE to obtain more\nconsistent latent embedding distributions across data sets. To ensure\ntransferability, the distribution of the latent embedding should be similar\nacross non-tonal languages (data sets). We start by presenting a low-complexity\nSER based on a denoising-autoencoder, which achieves an unweighted\nclassification accuracy of over 52.09% for four-class emotion classification.\nThis performance is comparable to that of similar baseline methods. Following\nthis, we employ a VAE, the semi-supervised VAE and the VAE with KL annealing to\nobtain a more regularized latent space. We show that while the DAE has the\nhighest classification accuracy among the methods, the semi-supervised VAE has\na comparable classification accuracy and a more consistent latent embedding\ndistribution over data sets.", "published": "2022-03-28 16:14:08", "link": "http://arxiv.org/abs/2203.14865v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Continuous Metric Learning For Transferable Speech Emotion Recognition\n  and Embedding Across Low-resource Languages", "abstract": "Speech emotion recognition~(SER) refers to the technique of inferring the\nemotional state of an individual from speech signals. SERs continue to garner\ninterest due to their wide applicability. Although the domain is mainly founded\non signal processing, machine learning, and deep learning, generalizing over\nlanguages continues to remain a challenge. However, developing generalizable\nand transferable models are critical due to a lack of sufficient resources in\nterms of data and labels for languages beyond the most commonly spoken ones. To\nimprove performance over languages, we propose a denoising autoencoder with\nsemi-supervision using a continuous metric loss based on either activation or\nvalence. The novelty of this work lies in our proposal of continuous metric\nlearning, which is among the first proposals on the topic to the best of our\nknowledge. Furthermore, to address the lack of activation and valence labels in\nthe transfer datasets, we annotate the signal samples with activation and\nvalence levels corresponding to a dimensional model of emotions, which were\nthen used to evaluate the quality of the embedding over the transfer datasets.\nWe show that the proposed semi-supervised model consistently outperforms the\nbaseline unsupervised method, which is a conventional denoising autoencoder, in\nterms of emotion classification accuracy as well as correlation with respect to\nthe dimensional variables. Further evaluation of classification accuracy with\nrespect to the reference, a BERT based speech representation model, shows that\nthe proposed method is comparable to the reference method in classifying\nspecific emotion classes at a much lower complexity.", "published": "2022-03-28 16:14:41", "link": "http://arxiv.org/abs/2203.14867v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improving Source Separation by Explicitly Modeling Dependencies Between\n  Sources", "abstract": "We propose a new method for training a supervised source separation system\nthat aims to learn the interdependent relationships between all combinations of\nsources in a mixture. Rather than independently estimating each source from a\nmix, we reframe the source separation problem as an Orderless Neural\nAutoregressive Density Estimator (NADE), and estimate each source from both the\nmix and a random subset of the other sources. We adapt a standard source\nseparation architecture, Demucs, with additional inputs for each individual\nsource, in addition to the input mixture. We randomly mask these input sources\nduring training so that the network learns the conditional dependencies between\nthe sources. By pairing this training method with a block Gibbs sampling\nprocedure at inference time, we demonstrate that the network can iteratively\nimprove its separation performance by conditioning a source estimate on its\nearlier source estimates. Experiments on two source separation datasets show\nthat training a Demucs model with an Orderless NADE approach and using Gibbs\nsampling (up to 512 steps) at inference time strongly outperforms a Demucs\nbaseline that uses a standard regression loss and direct (one step) estimation\nof sources.", "published": "2022-03-28 23:21:40", "link": "http://arxiv.org/abs/2203.15140v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "On-the-Fly Feature Based Rapid Speaker Adaptation for Dysarthric and\n  Elderly Speech Recognition", "abstract": "Accurate recognition of dysarthric and elderly speech remain challenging\ntasks to date. Speaker-level heterogeneity attributed to accent or gender, when\naggregated with age and speech impairment, create large diversity among these\nspeakers. Scarcity of speaker-level data limits the practical use of\ndata-intensive model based speaker adaptation methods. To this end, this paper\nproposes two novel forms of data-efficient, feature-based on-the-fly speaker\nadaptation methods: variance-regularized spectral basis embedding (SVR) and\nspectral feature driven f-LHUC transforms. Experiments conducted on UASpeech\ndysarthric and DementiaBank Pitt elderly speech corpora suggest the proposed\non-the-fly speaker adaptation approaches consistently outperform baseline\niVector adapted hybrid DNN/TDNN and E2E Conformer systems by statistically\nsignificant WER reduction of 2.48%-2.85% absolute (7.92%-8.06% relative), and\noffline model based LHUC adaptation by 1.82% absolute (5.63% relative)\nrespectively.", "published": "2022-03-28 09:12:24", "link": "http://arxiv.org/abs/2203.14593v3", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Training speaker recognition systems with limited data", "abstract": "This work considers training neural networks for speaker recognition with a\nmuch smaller dataset size compared to contemporary work. We artificially\nrestrict the amount of data by proposing three subsets of the popular VoxCeleb2\ndataset. These subsets are restricted to 50\\,k audio files (versus over 1\\,M\nfiles available), and vary on the axis of number of speakers and session\nvariability. We train three speaker recognition systems on these subsets; the\nX-vector, ECAPA-TDNN, and wav2vec2 network architectures. We show that the\nself-supervised, pre-trained weights of wav2vec2 substantially improve\nperformance when training data is limited. Code and data subsets are available\nat https://github.com/nikvaessen/w2v2-speaker-few-samples.", "published": "2022-03-28 12:41:41", "link": "http://arxiv.org/abs/2203.14688v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Joint Cross-Attention Model for Audio-Visual Fusion in Dimensional\n  Emotion Recognition", "abstract": "Multimodal emotion recognition has recently gained much attention since it\ncan leverage diverse and complementary relationships over multiple modalities\n(e.g., audio, visual, biosignals, etc.), and can provide some robustness to\nnoisy modalities. Most state-of-the-art methods for audio-visual (A-V) fusion\nrely on recurrent networks or conventional attention mechanisms that do not\neffectively leverage the complementary nature of A-V modalities. In this paper,\nwe focus on dimensional emotion recognition based on the fusion of facial and\nvocal modalities extracted from videos. Specifically, we propose a joint\ncross-attention model that relies on the complementary relationships to extract\nthe salient features across A-V modalities, allowing for accurate prediction of\ncontinuous values of valence and arousal. The proposed fusion model efficiently\nleverages the inter-modal relationships, while reducing the heterogeneity\nbetween the features. In particular, it computes the cross-attention weights\nbased on correlation between the combined feature representation and individual\nmodalities. By deploying the combined A-V feature representation into the\ncross-attention module, the performance of our fusion module improves\nsignificantly over the vanilla cross-attention module. Experimental results on\nvalidation-set videos from the AffWild2 dataset indicate that our proposed A-V\nfusion model provides a cost-effective solution that can outperform\nstate-of-the-art approaches. The code is available on GitHub:\nhttps://github.com/praveena2j/JointCrossAttentional-AV-Fusion.", "published": "2022-03-28 14:09:43", "link": "http://arxiv.org/abs/2203.14779v4", "categories": ["cs.CV", "cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Dual-Path Style Learning for End-to-End Noise-Robust Speech Recognition", "abstract": "Automatic speech recognition (ASR) systems degrade significantly under noisy\nconditions. Recently, speech enhancement (SE) is introduced as front-end to\nreduce noise for ASR, but it also suppresses some important speech information,\ni.e., over-suppression. To alleviate this, we propose a dual-path style\nlearning approach for end-to-end noise-robust speech recognition (DPSL-ASR).\nSpecifically, we first introduce clean speech feature along with the fused\nfeature from IFF-Net as dual-path inputs to recover the suppressed information.\nThen, we propose style learning to map the fused feature close to clean\nfeature, in order to learn latent speech information from the latter, i.e.,\nclean \"speech style\". Furthermore, we also minimize the distance of final ASR\noutputs in two paths to improve noise-robustness. Experiments show that the\nproposed approach achieves relative word error rate (WER) reductions of 10.6%\nand 8.6% over the best IFF-Net baseline, on RATS and CHiME-4 datasets\nrespectively.", "published": "2022-03-28 15:21:57", "link": "http://arxiv.org/abs/2203.14838v3", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improved singing voice separation with chromagram-based pitch-aware\n  remixing", "abstract": "Singing voice separation aims to separate music into vocals and accompaniment\ncomponents. One of the major constraints for the task is the limited amount of\ntraining data with separated vocals. Data augmentation techniques such as\nrandom source mixing have been shown to make better use of existing data and\nmildly improve model performance. We propose a novel data augmentation\ntechnique, chromagram-based pitch-aware remixing, where music segments with\nhigh pitch alignment are mixed. By performing controlled experiments in both\nsupervised and semi-supervised settings, we demonstrate that training models\nwith pitch-aware remixing significantly improves the test signal-to-distortion\nratio (SDR)", "published": "2022-03-28 20:55:54", "link": "http://arxiv.org/abs/2203.15092v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Robust Speaker Recognition with Transformers Using wav2vec 2.0", "abstract": "Recent advances in unsupervised speech representation learning discover new\napproaches and provide new state-of-the-art for diverse types of speech\nprocessing tasks. This paper presents an investigation of using wav2vec 2.0\ndeep speech representations for the speaker recognition task. The proposed\nfine-tuning procedure of wav2vec 2.0 with simple TDNN and statistic pooling\nback-end using additive angular margin loss allows to obtain deep speaker\nembedding extractor that is well-generalized across different domains. It is\nconcluded that Contrastive Predictive Coding pretraining scheme efficiently\nutilizes the power of unlabeled data, and thus opens the door to powerful\ntransformer-based speaker recognition systems. The experimental results\nobtained in this study demonstrate that fine-tuning can be done on relatively\nsmall sets and a clean version of data. Using data augmentation during\nfine-tuning provides additional performance gains in speaker verification. In\nthis study speaker recognition systems were analyzed on a wide range of\nwell-known verification protocols: VoxCeleb1 cleaned test set, NIST SRE 18\ndevelopment set, NIST SRE 2016 and NIST SRE 2019 evaluation set, VOiCES\nevaluation set, NIST 2021 SRE, and CTS challenges sets.", "published": "2022-03-28 20:59:58", "link": "http://arxiv.org/abs/2203.15095v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Investigation of Different Calibration Methods for Deep Speaker\n  Embedding based Verification Systems", "abstract": "Deep speaker embedding extractors have already become new state-of-the-art\nsystems in the speaker verification field. However, the problem of verification\nscore calibration for such systems often remains out of focus. An irrelevant\nscore calibration leads to serious issues, especially in the case of unknown\nacoustic conditions, even if we use a strong speaker verification system in\nterms of threshold-free metrics. This paper presents an investigation over\nseveral methods of score calibration: a classical approach based on the\nlogistic regression model; the recently presented magnitude estimation network\nMagnetO that uses activations from the pooling layer of the trained deep\nspeaker extractor and generalization of such approach based on separate scale\nand offset prediction neural networks. An additional focus of this research is\nto estimate the impact of score normalization on the calibration performance of\nthe system. The obtained results demonstrate that there are no serious problems\nif in-domain development data are used for calibration tuning. Otherwise, a\ntrade-off between good calibration performance and threshold-free system\nquality arises. In most cases using adaptive s-norm helps to stabilize score\ndistributions and to improve system performance. Meanwhile, some experiments\ndemonstrate that novel approaches have their limits in score stabilization on\nseveral datasets.", "published": "2022-03-28 21:22:22", "link": "http://arxiv.org/abs/2203.15106v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "CMGAN: Conformer-based Metric GAN for Speech Enhancement", "abstract": "Recently, convolution-augmented transformer (Conformer) has achieved\npromising performance in automatic speech recognition (ASR) and time-domain\nspeech enhancement (SE), as it can capture both local and global dependencies\nin the speech signal. In this paper, we propose a conformer-based metric\ngenerative adversarial network (CMGAN) for SE in the time-frequency (TF)\ndomain. In the generator, we utilize two-stage conformer blocks to aggregate\nall magnitude and complex spectrogram information by modeling both time and\nfrequency dependencies. The estimation of magnitude and complex spectrogram is\ndecoupled in the decoder stage and then jointly incorporated to reconstruct the\nenhanced speech. In addition, a metric discriminator is employed to further\nimprove the quality of the enhanced estimated speech by optimizing the\ngenerator with respect to a corresponding evaluation score. Quantitative\nanalysis on Voice Bank+DEMAND dataset indicates the capability of CMGAN in\noutperforming various previous models with a margin, i.e., PESQ of 3.41 and\nSSNR of 11.10 dB.", "published": "2022-03-28 23:53:34", "link": "http://arxiv.org/abs/2203.15149v4", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Neural Vocoder is All You Need for Speech Super-resolution", "abstract": "Speech super-resolution (SR) is a task to increase speech sampling rate by\ngenerating high-frequency components. Existing speech SR methods are trained in\nconstrained experimental settings, such as a fixed upsampling ratio. These\nstrong constraints can potentially lead to poor generalization ability in\nmismatched real-world cases. In this paper, we propose a neural vocoder based\nspeech super-resolution method (NVSR) that can handle a variety of input\nresolution and upsampling ratios. NVSR consists of a mel-bandwidth extension\nmodule, a neural vocoder module, and a post-processing module. Our proposed\nsystem achieves state-of-the-art results on the VCTK multi-speaker benchmark.\nOn 44.1 kHz target resolution, NVSR outperforms WSRGlow and Nu-wave by 8% and\n37% respectively on log spectral distance and achieves a significantly better\nperceptual quality. We also demonstrate that prior knowledge in the pre-trained\nvocoder is crucial for speech SR by performing mel-bandwidth extension with a\nsimple replication-padding method. Samples can be found in\nhttps://haoheliu.github.io/nvsr.", "published": "2022-03-28 17:51:00", "link": "http://arxiv.org/abs/2203.14941v1", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
