{"title": "Don't Go To Extremes: Revealing the Excessive Sensitivity and\n  Calibration Limitations of LLMs in Implicit Hate Speech Detection", "abstract": "The fairness and trustworthiness of Large Language Models (LLMs) are\nreceiving increasing attention. Implicit hate speech, which employs indirect\nlanguage to convey hateful intentions, occupies a significant portion of\npractice. However, the extent to which LLMs effectively address this issue\nremains insufficiently examined. This paper delves into the capability of LLMs\nto detect implicit hate speech (Classification Task) and express confidence in\ntheir responses (Calibration Task). Our evaluation meticulously considers\nvarious prompt patterns and mainstream uncertainty estimation methods. Our\nfindings highlight that LLMs exhibit two extremes: (1) LLMs display excessive\nsensitivity towards groups or topics that may cause fairness issues, resulting\nin misclassifying benign statements as hate speech. (2) LLMs' confidence scores\nfor each method excessively concentrate on a fixed range, remaining unchanged\nregardless of the dataset's complexity. Consequently, the calibration\nperformance is heavily reliant on primary classification accuracy. These\ndiscoveries unveil new limitations of LLMs, underscoring the need for caution\nwhen optimizing models to ensure they do not veer towards extremes. This serves\nas a reminder to carefully consider sensitivity and confidence in the pursuit\nof model fairness.", "published": "2024-02-18 00:04:40", "link": "http://arxiv.org/abs/2402.11406v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-dimensional Evaluation of Empathetic Dialog Responses", "abstract": "Empathy is critical for effective and satisfactory conversational\ncommunication. Prior efforts to measure conversational empathy mostly focus on\nexpressed communicative intents -- that is, the way empathy is expressed. Yet,\nthese works ignore the fact that conversation is also a collaboration involving\nboth speakers and listeners. In contrast, we propose a multi-dimensional\nempathy evaluation framework to measure both \\emph{expressed intents from the\nspeaker's perspective} and \\emph{perceived empathy from the listener's\nperspective}. We apply our analytical framework to examine internal\ncustomer-service dialogues. We find the two dimensions (expressed intent types\nand perceived empathy) are inter-connected, while perceived empathy has high\ncorrelations with dialogue satisfaction levels.\n  To reduce the annotation cost, we explore different options to automatically\nmeasure conversational empathy: prompting LLMs and training language\nmodel-based classifiers. Our experiments show that prompting methods with even\npopular models like GPT-4 and Flan family models perform relatively poorly on\nboth public and our internal datasets. In contrast, instruction-finetuned\nclassifiers based on Flan-T5 family models outperform prior works and\ncompetitive baselines. We conduct a detailed ablation study to give more\ninsights into instruction finetuning method's strong performance.", "published": "2024-02-18 00:32:33", "link": "http://arxiv.org/abs/2402.11409v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-grained and Explainable Factuality Evaluation for Multimodal\n  Summarization", "abstract": "Multimodal summarization aims to generate a concise summary based on the\ninput text and image. However, the existing methods potentially suffer from\nunfactual output. To evaluate the factuality of multimodal summarization\nmodels, we propose two fine-grained and explainable evaluation frameworks\n(FALLACIOUS) for different application scenarios, i.e. reference-based\nfactuality evaluation framework and reference-free factuality evaluation\nframework. Notably, the reference-free factuality evaluation framework doesn't\nneed ground truth and hence it has a wider application scenario. To evaluate\nthe effectiveness of the proposed frameworks, we compute the correlation\nbetween our frameworks and the other metrics. The experimental results show the\neffectiveness of our proposed method. We will release our code and dataset via\ngithub.", "published": "2024-02-18 01:03:25", "link": "http://arxiv.org/abs/2402.11414v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rethinking the Roles of Large Language Models in Chinese Grammatical\n  Error Correction", "abstract": "Recently, Large Language Models (LLMs) have been widely studied by\nresearchers for their roles in various downstream NLP tasks. As a fundamental\ntask in the NLP field, Chinese Grammatical Error Correction (CGEC) aims to\ncorrect all potential grammatical errors in the input sentences. Previous\nstudies have shown that LLMs' performance as correctors on CGEC remains\nunsatisfactory due to its challenging task focus. To promote the CGEC field to\nbetter adapt to the era of LLMs, we rethink the roles of LLMs in the CGEC task\nso that they can be better utilized and explored in CGEC. Considering the rich\ngrammatical knowledge stored in LLMs and their powerful semantic understanding\ncapabilities, we utilize LLMs as explainers to provide explanation information\nfor the CGEC small models during error correction to enhance performance. We\nalso use LLMs as evaluators to bring more reasonable CGEC evaluations, thus\nalleviating the troubles caused by the subjectivity of the CGEC task. In\nparticular, our work is also an active exploration of how LLMs and small models\nbetter collaborate in downstream tasks. Extensive experiments and detailed\nanalyses on widely used datasets verify the effectiveness of our thinking\nintuition and the proposed methods.", "published": "2024-02-18 01:40:34", "link": "http://arxiv.org/abs/2402.11420v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mitigating Catastrophic Forgetting in Multi-domain Chinese Spelling\n  Correction by Multi-stage Knowledge Transfer Framework", "abstract": "Chinese Spelling Correction (CSC) aims to detect and correct spelling errors\nin given sentences. Recently, multi-domain CSC has gradually attracted the\nattention of researchers because it is more practicable. In this paper, we\nfocus on the key flaw of the CSC model when adapting to multi-domain scenarios:\nthe tendency to forget previously acquired knowledge upon learning new\ndomain-specific knowledge (i.e., catastrophic forgetting). To address this, we\npropose a novel model-agnostic Multi-stage Knowledge Transfer (MKT) framework,\nwhich utilizes a continuously evolving teacher model for knowledge transfer in\neach domain, rather than focusing solely on new domain knowledge. It deserves\nto be mentioned that we are the first to apply continual learning methods to\nthe multi-domain CSC task. Experiments prove the effectiveness of our proposed\nmethod, and further analyses demonstrate the importance of overcoming\ncatastrophic forgetting for improving the model performance.", "published": "2024-02-18 01:46:46", "link": "http://arxiv.org/abs/2402.11422v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EventRL: Enhancing Event Extraction with Outcome Supervision for Large\n  Language Models", "abstract": "In this study, we present EventRL, a reinforcement learning approach\ndeveloped to enhance event extraction for large language models (LLMs). EventRL\nutilizes outcome supervision with specific reward functions to tackle prevalent\nchallenges in LLMs, such as instruction following and hallucination, manifested\nas the mismatch of event structure and the generation of undefined event types.\nWe evaluate EventRL against existing methods like Few-Shot Prompting (FSP)\n(based on GPT4) and Supervised Fine-Tuning (SFT) across various LLMs, including\nGPT-4, LLaMa, and CodeLLaMa models. Our findings show that EventRL\nsignificantly outperforms these conventional approaches by improving the\nperformance in identifying and structuring events, particularly in handling\nnovel event types. The study emphasizes the critical role of reward function\nselection and demonstrates the benefits of incorporating code data for better\nevent extraction. While increasing model size leads to higher accuracy,\nmaintaining the ability to generalize is essential to avoid overfitting.", "published": "2024-02-18 02:41:06", "link": "http://arxiv.org/abs/2402.11430v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Deception Detection Go Deeper? Dataset, Evaluation, and Benchmark\n  for Deception Reasoning", "abstract": "Deception detection has attracted increasing attention due to its importance\nin real-world scenarios. Its main goal is to detect deceptive behaviors from\nmultimodal clues such as gestures, facial expressions, prosody, etc. However,\nthese bases are usually subjective and related to personal habits. Therefore,\nwe extend deception detection to deception reasoning, further providing\nobjective evidence to support subjective judgment. Specifically, we provide\npotential lies and basic facts and then analyze why this sentence may be a lie\nby combining factual inconsistencies and intent behind them. Compared with\ndeception detection, this task is more applicable to real-world scenarios. For\nexample, in interrogation, the police should judge whether a person is lying\nbased on solid evidence. This paper presents our initial attempts at this task,\nincluding constructing a dataset and defining evaluation metrics. Meanwhile,\nthis task can serve as a benchmark for evaluating the complex reasoning\ncapability of large language models. Our code and data are provided in the\nsupplementary material.", "published": "2024-02-18 02:52:54", "link": "http://arxiv.org/abs/2402.11432v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can LLMs Reason with Rules? Logic Scaffolding for Stress-Testing and\n  Improving LLMs", "abstract": "Large language models (LLMs) have achieved impressive human-like performance\nacross various reasoning tasks. However, their mastery of underlying\ninferential rules still falls short of human capabilities. To investigate this,\nwe propose a logic scaffolding inferential rule generation framework, to\nconstruct an inferential rule base, ULogic, comprising both primitive and\ncompositional rules across five domains. Our analysis of GPT-series models over\na rule subset reveals significant gaps in LLMs' logic understanding compared to\nhuman performance, especially in compositional and structural complex rules\nwith certain bias patterns. We further distill these rules into a smaller-scale\ninference engine for flexible rule generation and enhancing downstream\nreasoning. Through a multi-judger evaluation, our inference engine proves\neffective in generating accurate, complex and abstract conclusions and\npremises, and improve various commonsense reasoning tasks. Overall, our work\nsheds light on LLMs' limitations in grasping inferential rule and suggests ways\nto enhance their logical reasoning abilities~\\footnote{Code and data are\navailable at \\url{https://github.com/SiyuanWangw/ULogic}.}.", "published": "2024-02-18 03:38:51", "link": "http://arxiv.org/abs/2402.11442v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Benchmark Self-Evolving: A Multi-Agent Framework for Dynamic LLM\n  Evaluation", "abstract": "This paper presents a benchmark self-evolving framework to dynamically\nevaluate rapidly advancing Large Language Models (LLMs), aiming for a more\naccurate assessment of their capabilities and limitations. We utilize a\nmulti-agent system to manipulate the context or question of original instances,\nreframing new evolving instances with high confidence that dynamically extend\nexisting benchmarks. Towards a more scalable, robust and fine-grained\nevaluation, we implement six reframing operations to construct evolving\ninstances testing LLMs against diverse queries, data noise and probing their\nproblem-solving sub-abilities. With this framework, we extend benchmark\ndatasets of four tasks. Experimental results show a general performance decline\nin most LLMs against their original results. This decline under our scalable\nand robust evaluations, alongside our fine-grained evaluation, more accurately\nreflect models' capabilities. Besides, our framework widens performance\ndiscrepancies both between different models and within the same model across\nvarious tasks, facilitating more informed model selection for specific tasks\n(Code and data are available at\nhttps://github.com/NanshineLoong/Self-Evolving-Benchmark).", "published": "2024-02-18 03:40:06", "link": "http://arxiv.org/abs/2402.11443v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "In-Context Example Ordering Guided by Label Distributions", "abstract": "By allowing models to predict without task-specific training, in-context\nlearning (ICL) with pretrained LLMs has enormous potential in NLP. However, a\nnumber of problems persist in ICL. In particular, its performance is sensitive\nto the choice and order of in-context examples. Given the same set of\nin-context examples with different orderings, model performance may vary\nbetween near random to near state-of-the-art. In this work, we formulate\nin-context example ordering as an optimization problem. We examine three\nproblem settings that differ in the assumptions they make about what is known\nabout the task. Inspired by the idea of learning from label proportions, we\npropose two principles for in-context example ordering guided by model's\nprobability predictions. We apply our proposed principles to thirteen text\nclassification datasets and nine different autoregressive LLMs with 700M to 13B\nparameters. We demonstrate that our approach outperforms the baselines by\nimproving the classification accuracy, reducing model miscalibration, and also\nby selecting better in-context examples.", "published": "2024-02-18 04:08:10", "link": "http://arxiv.org/abs/2402.11447v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AutoPRM: Automating Procedural Supervision for Multi-Step Reasoning via\n  Controllable Question Decomposition", "abstract": "Recent advancements in large language models (LLMs) have shown promise in\nmulti-step reasoning tasks, yet their reliance on extensive manual labeling to\nprovide procedural feedback remains a significant impediment. To address this\nchallenge, in this paper, we propose a novel self-supervised framework AutoPRM\nthat efficiently enhances the fine-tuning of LLMs for intricate reasoning\nchallenges. Specifically, AutoPRM first decomposes complex problems into more\nmanageable subquestions with a controllable granularity switch, then\nsequentially apply reinforcement learning to iteratively improve the\nsubquestion solver. Additionally, we propose context-guided-decoding to avoid\nreward tampering and guide the subquestion solver towards the solution of the\nholistic problem. Extensive experiments show that AutoPRM significantly\nimproves performance on mathematical and commonsense reasoning tasks over SOTA.\nMore encouragingly, AutoPRM can be easily integrated with other orthogonal\nreasoning pipelines.", "published": "2024-02-18 04:28:16", "link": "http://arxiv.org/abs/2402.11452v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific\n  Data Visualization", "abstract": "Scientific data visualization plays a crucial role in research by enabling\nthe direct display of complex information and assisting researchers in\nidentifying implicit patterns. Despite its importance, the use of Large\nLanguage Models (LLMs) for scientific data visualization remains rather\nunexplored. In this study, we introduce MatPlotAgent, an efficient\nmodel-agnostic LLM agent framework designed to automate scientific data\nvisualization tasks. Leveraging the capabilities of both code LLMs and\nmulti-modal LLMs, MatPlotAgent consists of three core modules: query\nunderstanding, code generation with iterative debugging, and a visual feedback\nmechanism for error correction. To address the lack of benchmarks in this\nfield, we present MatPlotBench, a high-quality benchmark consisting of 100\nhuman-verified test cases. Additionally, we introduce a scoring approach that\nutilizes GPT-4V for automatic evaluation. Experimental results demonstrate that\nMatPlotAgent can improve the performance of various LLMs, including both\ncommercial and open-source models. Furthermore, the proposed evaluation method\nshows a strong correlation with human-annotated scores.", "published": "2024-02-18 04:28:28", "link": "http://arxiv.org/abs/2402.11453v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LoRA-Flow: Dynamic LoRA Fusion for Large Language Models in Generative\n  Tasks", "abstract": "LoRA employs lightweight modules to customize large language models (LLMs)\nfor each downstream task or domain, where different learned additional modules\nrepresent diverse skills. Combining existing LoRAs to address new tasks can\nenhance the reusability of learned LoRAs, particularly beneficial for tasks\nwith limited annotated data. Most prior works on LoRA combination primarily\nrely on task-level weights for each involved LoRA, making different examples\nand tokens share the same LoRA weights. However, in generative tasks, different\ntokens may necessitate diverse skills to manage. Taking the Chinese math task\nas an example, understanding the problem description may depend more on the\nChinese LoRA, while the calculation part may rely more on the math LoRA. To\nthis end, we propose LoRA-Flow, which utilizes dynamic weights to adjust the\nimpact of different LoRAs. The weights at each step are determined by a fusion\ngate with extremely few parameters, which can be learned with only 200 training\nexamples. Experiments across six generative tasks demonstrate that our method\nconsistently outperforms baselines with task-level fusion weights. This\nunderscores the necessity of introducing dynamic fusion weights for LoRA\ncombination.", "published": "2024-02-18 04:41:25", "link": "http://arxiv.org/abs/2402.11455v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FactPICO: Factuality Evaluation for Plain Language Summarization of\n  Medical Evidence", "abstract": "Plain language summarization with LLMs can be useful for improving textual\naccessibility of technical content. But how factual are these summaries in a\nhigh-stakes domain like medicine? This paper presents FactPICO, a factuality\nbenchmark for plain language summarization of medical texts describing\nrandomized controlled trials (RCTs), which are the basis of evidence-based\nmedicine and can directly inform patient treatment. FactPICO consists of 345\nplain language summaries of RCT abstracts generated from three LLMs (i.e.,\nGPT-4, Llama-2, and Alpaca), with fine-grained evaluation and natural language\nrationales from experts. We assess the factuality of critical elements of RCTs\nin those summaries: Populations, Interventions, Comparators, Outcomes (PICO),\nas well as the reported findings concerning these. We also evaluate the\ncorrectness of the extra information (e.g., explanations) added by LLMs. Using\nFactPICO, we benchmark a range of existing factuality metrics, including the\nnewly devised ones based on LLMs. We find that plain language summarization of\nmedical evidence is still challenging, especially when balancing between\nsimplicity and factuality, and that existing metrics correlate poorly with\nexpert judgments on the instance level.", "published": "2024-02-18 04:45:01", "link": "http://arxiv.org/abs/2402.11456v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "When Do LLMs Need Retrieval Augmentation? Mitigating LLMs'\n  Overconfidence Helps Retrieval Augmentation", "abstract": "Large Language Models (LLMs) have been found to have difficulty knowing they\ndo not possess certain knowledge and tend to provide specious answers in such\ncases. Retrieval Augmentation (RA) has been extensively studied to mitigate\nLLMs' hallucinations. However, due to the extra overhead and unassured quality\nof retrieval, it may not be optimal to conduct RA all the time. A\nstraightforward idea is to only conduct retrieval when LLMs are uncertain about\na question. This motivates us to enhance the LLMs' ability to perceive their\nknowledge boundaries to help RA. In this paper, we first quantitatively measure\nLLMs' such ability and confirm their overconfidence. Then, we study how LLMs'\ncertainty about a question correlates with their dependence on external\nretrieved information. We propose several methods to enhance LLMs' perception\nof knowledge boundaries and show that they are effective in reducing\noverconfidence. Additionally, equipped with these methods, LLMs can achieve\ncomparable or even better performance of RA with much fewer retrieval calls.", "published": "2024-02-18 04:57:19", "link": "http://arxiv.org/abs/2402.11457v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DictLLM: Harnessing Key-Value Data Structures with Large Language Models\n  for Enhanced Medical Diagnostics", "abstract": "Structured data offers a sophisticated mechanism for the organization of\ninformation. Existing methodologies for the text-serialization of structured\ndata in the context of large language models fail to adequately address the\nheterogeneity inherent in key-value structured data. These methods are not\nideal and frequently result in larger input sizes and poor adaptability to\ninput changes. In this paper, we introduce DictLLM, an innovative framework\ndesigned to improve the modeling of key-value structured data, like medical\nlaboratory reports, for generating medical diagnoses. DictLLM integrates three\nkey components: (1) group positional encoding to maintain permutation\ninvariance, (2) hierarchical attention bias to capture the inherent bias in\nstructured data, and (3) an optimal transport alignment layer that aligns the\nembedding generated by the dictionary encoder with the LLM, thereby producing a\nsequence of fixed-length virtual tokens. We carry out experiments using various\nLLM models on a comprehensive real-world medical laboratory report dataset for\nautomatic diagnosis generation, our findings illustrate that DictLLM\nsignificantly outperforms established baseline methods and few-shot GPT-4\nimplementations in terms of both Rouge-L and Knowledge F1 scores. Furthermore,\nour evaluation of the framework's scalability and robustness, through a series\nof experiments, underscores its exceptional capability in accurately modeling\nthe complex key-value data structure of medical dictionary data.", "published": "2024-02-18 07:10:02", "link": "http://arxiv.org/abs/2402.11481v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What's the Plan? Evaluating and Developing Planning-Aware Techniques for\n  Language Models", "abstract": "Planning is a fundamental task in artificial intelligence that involves\nfinding a sequence of actions that achieve a specified goal in a given\nenvironment. Large language models (LLMs) are increasingly used for\napplications that require planning capabilities, such as web or embodied\nagents. In line with recent studies, we demonstrate through experimentation\nthat LLMs lack necessary skills required for planning. Based on these\nobservations, we advocate for the potential of a hybrid approach that combines\nLLMs with classical planning methodology. Then, we introduce SimPlan, a novel\nhybrid-method, and evaluate its performance in a new challenging setup. Our\nextensive experiments across various planning domains demonstrate that SimPlan\nsignificantly outperforms existing LLM-based planners.", "published": "2024-02-18 07:42:49", "link": "http://arxiv.org/abs/2402.11489v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Benchmarking Knowledge Boundary for Large Language Models: A Different\n  Perspective on Model Evaluation", "abstract": "In recent years, substantial advancements have been made in the development\nof large language models, achieving remarkable performance across diverse\ntasks. To evaluate the knowledge ability of language models, previous studies\nhave proposed lots of benchmarks based on question-answering pairs. We argue\nthat it is not reliable and comprehensive to evaluate language models with a\nfixed question or limited paraphrases as the query, since language models are\nsensitive to prompt. Therefore, we introduce a novel concept named knowledge\nboundary to encompass both prompt-agnostic and prompt-sensitive knowledge\nwithin language models. Knowledge boundary avoids prompt sensitivity in\nlanguage model evaluations, rendering them more dependable and robust. To\nexplore the knowledge boundary for a given model, we propose projected gradient\ndescent method with semantic constraints, a new algorithm designed to identify\nthe optimal prompt for each piece of knowledge. Experiments demonstrate a\nsuperior performance of our algorithm in computing the knowledge boundary\ncompared to existing methods. Furthermore, we evaluate the ability of multiple\nlanguage models in several domains with knowledge boundary.", "published": "2024-02-18 07:48:15", "link": "http://arxiv.org/abs/2402.11493v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge-to-SQL: Enhancing SQL Generation with Data Expert LLM", "abstract": "Generating accurate SQL queries for user questions (text-to-SQL) has been a\nlong-standing challenge since it requires a deep understanding of both the\nuser's question and the corresponding database schema in order to retrieve the\ndesired content accurately. Existing methods rely on the comprehensive\ncapability of large language models (LLMs) to generate the SQL. However, some\nnecessary knowledge is not explicitly included in the database schema and user\nquestion or has been learned by LLMs. Thus, the generated SQL of the\nknowledge-insufficient questions may be inaccurate, negatively influencing the\ntext-to-SQL models' performance and robustness. To address this challenge, we\npropose the Knowledge-to-SQL framework, which employs tailored Data Expert LLM\n(DELLM) to provide helpful knowledge for all text-to-SQL models. Specifically,\nwe introduce the detailed implementation of DELLM regarding table reading and\nthe basic fine-tuning process. We further propose a Preference Learning via\nDatabase Feedback (PLDBF) strategy, refining the DELLM to generate more helpful\nknowledge for LLMs. Extensive experiments verify that DELLM can enhance the\nstate-of-the-art approaches for text-to-SQL tasks. The corresponding code of\nDELLM is released for further research.", "published": "2024-02-18 09:10:04", "link": "http://arxiv.org/abs/2402.11517v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unveiling the Secrets of Engaging Conversations: Factors that Keep Users\n  Hooked on Role-Playing Dialog Agents", "abstract": "With the growing humanlike nature of dialog agents, people are now engaging\nin extended conversations that can stretch from brief moments to substantial\nperiods of time. Understanding the factors that contribute to sustaining these\ninteractions is crucial, yet existing studies primarily focusing on short-term\nsimulations that rarely explore such prolonged and real conversations.\n  In this paper, we investigate the factors influencing retention rates in real\ninteractions with roleplaying models. By analyzing a large dataset of\ninteractions between real users and thousands of characters, we systematically\nexamine multiple factors and assess their impact on user retention rate.\nSurprisingly, we find that the degree to which the bot embodies the roles it\nplays has limited influence on retention rates, while the length of each turn\nit speaks significantly affects retention rates. This study sheds light on the\ncritical aspects of user engagement with role-playing models and provides\nvaluable insights for future improvements in the development of large language\nmodels for role-playing purposes.", "published": "2024-02-18 09:42:41", "link": "http://arxiv.org/abs/2402.11522v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Chain-of-Instructions: Compositional Instruction Tuning on Large\n  Language Models", "abstract": "Fine-tuning large language models (LLMs) with a collection of large and\ndiverse instructions has improved the model's generalization to different\ntasks, even for unseen tasks. However, most existing instruction datasets\ninclude only single instructions, and they struggle to follow complex\ninstructions composed of multiple subtasks. In this work, we propose a novel\nconcept of compositional instructions called chain-of-instructions (CoI), where\nthe output of one instruction becomes an input for the next like a chain.\nUnlike the conventional practice of solving single instruction tasks, our\nproposed method encourages a model to solve each subtask step by step until the\nfinal answer is reached. CoI-tuning (i.e., fine-tuning with CoI instructions)\nimproves the model's ability to handle instructions composed of multiple\nsubtasks as well as unseen composite tasks such as multilingual summarization.\nOverall, our study find that simple CoI tuning of existing instruction data can\nprovide consistent generalization to solve more complex, unseen, and longer\nchains of instructions.", "published": "2024-02-18 10:10:40", "link": "http://arxiv.org/abs/2402.11532v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KMMLU: Measuring Massive Multitask Language Understanding in Korean", "abstract": "We propose KMMLU, a new Korean benchmark with 35,030 expert-level\nmultiple-choice questions across 45 subjects ranging from humanities to STEM.\nWhile prior Korean benchmarks are translated from existing English benchmarks,\nKMMLU is collected from original Korean exams, capturing linguistic and\ncultural aspects of the Korean language. We test 27 public and proprietary LLMs\nand observe the best public model to score 50.5%, leaving significant room for\nimprovement. This model was primarily trained for English and Chinese, not\nKorean. Current LLMs tailored to Korean, such as Polyglot-Ko, perform far\nworse. Surprisingly, even the most capable proprietary LLMs, e.g., GPT-4 and\nHyperCLOVA X do not exceed 60%. This suggests that further work is needed to\nimprove LLMs for Korean, and we believe KMMLU offers the appropriate tool to\ntrack this progress. We make our dataset publicly available on the Hugging Face\nHub and integrate the benchmark into EleutherAI's Language Model Evaluation\nHarness.", "published": "2024-02-18 11:41:07", "link": "http://arxiv.org/abs/2402.11548v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cobra Effect in Reference-Free Image Captioning Metrics", "abstract": "Evaluating the compatibility between textual descriptions and corresponding\nimages represents a core endeavor within multi-modal research. In recent years,\na proliferation of reference-free methods, leveraging visual-language\npre-trained models (VLMs), has emerged. Empirical evidence has substantiated\nthat these innovative approaches exhibit a higher correlation with human\njudgment, marking a significant advancement in the field. However, does a\nhigher correlation with human evaluations alone sufficiently denote the\ncomplete of a metric? In response to this question, in this paper, we study if\nthere are any deficiencies in reference-free metrics. Specifically, inspired by\nthe Cobra Effect, we utilize metric scores as rewards to direct the captioning\nmodel toward generating descriptions that closely align with the metric's\ncriteria. If a certain metric has flaws, it will be exploited by the model and\nreflected in the generated sentences. Our findings reveal that descriptions\nguided by these metrics contain significant flaws, e.g. incoherent statements\nand excessive repetition. Subsequently, we propose a novel method termed\nSelf-Improving to rectify the identified shortcomings within these metrics. We\nemploy GPT-4V as an evaluative tool to assess generated sentences and the\nresult reveals that our approach achieves state-of-the-art (SOTA) performance.\nIn addition, we also introduce a challenging evaluation benchmark called Flaws\nCaption to evaluate reference-free image captioning metrics comprehensively.\nOur code is available at\nhttps://github.com/aaronma2020/robust_captioning_metric", "published": "2024-02-18 12:36:23", "link": "http://arxiv.org/abs/2402.11572v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BGE Landmark Embedding: A Chunking-Free Embedding Method For Retrieval\n  Augmented Long-Context Large Language Models", "abstract": "Large language models (LLMs) call for extension of context to handle many\ncritical applications. However, the existing approaches are prone to expensive\ncosts and inferior quality of context extension. In this work, we\nproposeExtensible Embedding, which realizes high-quality extension of LLM's\ncontext with strong flexibility and cost-effectiveness. Extensible embedding\nstand as an enhancement of typical token embedding, which represents the\ninformation for an extensible scope of context instead of a single token. By\nleveraging such compact input units of higher information density, the LLM can\naccess to a vast scope of context even with a small context window. Extensible\nembedding is systematically optimized in architecture and training method,\nwhich leads to multiple advantages. 1) High flexibility of context extension,\nwhich flexibly supports ad-hoc extension of diverse context lengths. 2) Strong\nsample efficiency of training, which enables the embedding model to be learned\nin a cost-effective way. 3) Superior compatibility with the existing LLMs,\nwhere the extensible embedding can be seamlessly introduced as a plug-in\ncomponent. Comprehensive evaluations on long-context language modeling and\nunderstanding tasks verify extensible embedding as an effective, efficient,\nflexible, and compatible method to extend the LLM's context.", "published": "2024-02-18 12:41:01", "link": "http://arxiv.org/abs/2402.11573v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extensible Embedding: A Flexible Multipler For LLM's Context Length", "abstract": "Large language models (LLMs) call for extension of context to handle many\ncritical applications. However, the existing approaches are prone to expensive\ncosts and inferior quality of context extension. In this work, we propose\nExtensible Embedding, which realizes high-quality extension of LLM's context\nwith strong flexibility and cost-effectiveness. Extensible embedding stand as\nan enhancement of typical token embedding, which represents the information for\nan extensible scope of context instead of a single token. By leveraging such\ncompact input units of higher information density, the LLM can access to a vast\nscope of context even with a small context window. Extensible embedding is\nsystematically optimized in architecture and training method, which leads to\nmultiple advantages. 1) High flexibility of context extension, which flexibly\nsupports ad-hoc extension of diverse context lengths. 2) Strong sample\nefficiency of training, which enables the embedding model to be learned in a\ncost-effective way. 3) Superior compatibility with the existing LLMs, where the\nextensible embedding can be seamlessly introduced as a plug-in component.\nComprehensive evaluations on long-context language modeling and understanding\ntasks verify extensible embedding as an effective, efficient, flexible, and\ncompatible method to extend the LLM's context.", "published": "2024-02-18 12:50:19", "link": "http://arxiv.org/abs/2402.11577v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Task Inference: Can Large Language Models Follow Multiple\n  Instructions at Once?", "abstract": "Large language models (LLMs) are typically prompted to follow a single\ninstruction per inference call. In this work, we analyze whether LLMs also hold\nthe capability to handle multiple instructions simultaneously, denoted as\nMulti-Task Inference. For this purpose, we introduce the MTI Bench(Multi-Task\nInference Benchmark), a comprehensive evaluation benchmark encompassing 5,000\ninstances across 25 tasks. Each task in the MTI Bench involves 2 to 3\nsub-tasks. As expected, we first demonstrate that Multi-Task Inference reduces\nthe total inference time by 1.46 times in average since it does not require\nmultiple inference calls. Interestingly, contrary to the expectation that LLMs\nwould perform better when tasks are divided, we find that state-of-the-art\nLLMs, such as Llama-2-Chat-70B and GPT-4, show up to 7.3% and 12.4% improved\nperformance with Multi-Task Inference compared to Single-Task Inference on the\nMTI Bench. We release the MTI Bench dataset and our code at this link\nhttps://github.com/guijinSON/MTI-Bench.", "published": "2024-02-18 14:25:19", "link": "http://arxiv.org/abs/2402.11597v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Metric-Learning Encoding Models Identify Processing Profiles of\n  Linguistic Features in BERT's Representations", "abstract": "We introduce Metric-Learning Encoding Models (MLEMs) as a new approach to\nunderstand how neural systems represent the theoretical features of the objects\nthey process. As a proof-of-concept, we apply MLEMs to neural representations\nextracted from BERT, and track a wide variety of linguistic features (e.g.,\ntense, subject person, clause type, clause embedding). We find that: (1)\nlinguistic features are ordered: they separate representations of sentences to\ndifferent degrees in different layers; (2) neural representations are organized\nhierarchically: in some layers, we find clusters of representations nested\nwithin larger clusters, following successively important linguistic features;\n(3) linguistic features are disentangled in middle layers: distinct, selective\nunits are activated by distinct linguistic features. Methodologically, MLEMs\nare superior (4) to multivariate decoding methods, being more robust to type-I\nerrors, and (5) to univariate encoding methods, in being able to predict both\nlocal and distributed representations. Together, this demonstrates the utility\nof Metric-Learning Encoding Methods for studying how linguistic features are\nneurally encoded in language models and the advantage of MLEMs over traditional\nmethods. MLEMs can be extended to other domains (e.g. vision) and to other\nneural systems, such as the human brain.", "published": "2024-02-18 14:57:53", "link": "http://arxiv.org/abs/2402.11608v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Decoding News Narratives: A Critical Analysis of Large Language Models\n  in Framing Detection", "abstract": "Previous studies on framing have relied on manual analysis or fine-tuning\nmodels with limited annotated datasets. However, pre-trained models, with their\ndiverse training backgrounds, offer a promising alternative. This paper\npresents a comprehensive analysis of GPT-4, GPT-3.5 Turbo, and FLAN-T5 models\nin detecting framing in news headlines. We evaluated these models in various\nscenarios: zero-shot, few-shot with in-domain examples, cross-domain examples,\nand settings where models explain their predictions. Our results show that\nexplainable predictions lead to more reliable outcomes. GPT-4 performed\nexceptionally well in few-shot settings but often misinterpreted emotional\nlanguage as framing, highlighting a significant challenge. Additionally, the\nresults suggest that consistent predictions across multiple models could help\nidentify potential annotation inaccuracies in datasets. Finally, we propose a\nnew small dataset for real-world evaluation on headlines from a diverse set of\ntopics.", "published": "2024-02-18 15:27:48", "link": "http://arxiv.org/abs/2402.11621v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SpeCrawler: Generating OpenAPI Specifications from API Documentation\n  Using Large Language Models", "abstract": "In the digital era, the widespread use of APIs is evident. However, scalable\nutilization of APIs poses a challenge due to structure divergence observed in\nonline API documentation. This underscores the need for automatic tools to\nfacilitate API consumption. A viable approach involves the conversion of\ndocumentation into an API Specification format. While previous attempts have\nbeen made using rule-based methods, these approaches encountered difficulties\nin generalizing across diverse documentation. In this paper we introduce\nSpeCrawler, a comprehensive system that utilizes large language models (LLMs)\nto generate OpenAPI Specifications from diverse API documentation through a\ncarefully crafted pipeline. By creating a standardized format for numerous\nAPIs, SpeCrawler aids in streamlining integration processes within API\norchestrating systems and facilitating the incorporation of tools into LLMs.\nThe paper explores SpeCrawler's methodology, supported by empirical evidence\nand case studies, demonstrating its efficacy through LLM capabilities.", "published": "2024-02-18 15:33:24", "link": "http://arxiv.org/abs/2402.11625v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-seeding and Multi-intent Self-instructing LLMs for Generating\n  Intent-aware Information-Seeking dialogs", "abstract": "Identifying user intents in information-seeking dialogs is crucial for a\nsystem to meet user's information needs. Intent prediction (IP) is challenging\nand demands sufficient dialogs with human-labeled intents for training.\nHowever, manually annotating intents is resource-intensive. While large\nlanguage models (LLMs) have been shown to be effective in generating synthetic\ndata, there is no study on using LLMs to generate intent-aware\ninformation-seeking dialogs. In this paper, we focus on leveraging LLMs for\nzero-shot generation of large-scale, open-domain, and intent-aware\ninformation-seeking dialogs. We propose SOLID, which has novel self-seeding and\nmulti-intent self-instructing schemes. The former improves the generation\nquality by using the LLM's own knowledge scope to initiate dialog generation;\nthe latter prompts the LLM to generate utterances sequentially, and mitigates\nthe need for manual prompt design by asking the LLM to autonomously adapt its\nprompt instruction when generating complex multi-intent utterances.\nFurthermore, we propose SOLID-RL, which is further trained to generate a dialog\nin one step on the data generated by SOLID. We propose a length-based quality\nestimation mechanism to assign varying weights to SOLID-generated dialogs based\non their quality during the training process of SOLID-RL. We use SOLID and\nSOLID-RL to generate more than 300k intent-aware dialogs, surpassing the size\nof existing datasets. Experiments show that IP methods trained on dialogs\ngenerated by SOLID and SOLID-RL achieve better IP quality than ones trained on\nhuman-generated dialogs.", "published": "2024-02-18 16:20:43", "link": "http://arxiv.org/abs/2402.11633v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Stumbling Blocks: Stress Testing the Robustness of Machine-Generated\n  Text Detectors Under Attacks", "abstract": "The widespread use of large language models (LLMs) is increasing the demand\nfor methods that detect machine-generated text to prevent misuse. The goal of\nour study is to stress test the detectors' robustness to malicious attacks\nunder realistic scenarios. We comprehensively study the robustness of popular\nmachine-generated text detectors under attacks from diverse categories:\nediting, paraphrasing, prompting, and co-generating. Our attacks assume limited\naccess to the generator LLMs, and we compare the performance of detectors on\ndifferent attacks under different budget levels. Our experiments reveal that\nalmost none of the existing detectors remain robust under all the attacks, and\nall detectors exhibit different loopholes. Averaging all detectors, the\nperformance drops by 35% across all attacks. Further, we investigate the\nreasons behind these defects and propose initial out-of-the-box patches to\nimprove robustness.", "published": "2024-02-18 16:36:00", "link": "http://arxiv.org/abs/2402.11638v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning From Failure: Integrating Negative Examples when Fine-tuning\n  Large Language Models as Agents", "abstract": "Large language models (LLMs) have achieved success in acting as agents, which\ninteract with environments through tools such as search engines. However, LLMs\nare optimized for language generation instead of tool use during training or\nalignment, limiting their effectiveness as agents. To resolve this problem,\nprevious work has first collected interaction trajectories between LLMs and\nenvironments, using only trajectories that successfully finished the task to\nfine-tune smaller models, making fine-tuning data scarce and acquiring it both\ndifficult and costly. Discarding failed trajectories also leads to significant\nwastage of data and resources and limits the possible optimization paths during\nfine-tuning. In this paper, we argue that unsuccessful trajectories offer\nvaluable insights, and LLMs can learn from these trajectories through\nappropriate quality control and fine-tuning strategies. By simply adding a\nprefix or suffix that tells the model whether to generate a successful\ntrajectory during training, we improve model performance by a large margin on\nmathematical reasoning, multi-hop question answering, and strategic question\nanswering tasks. We further analyze the inference results and find that our\nmethod provides a better trade-off between valuable information and errors in\nunsuccessful trajectories. To our knowledge, we are the first to demonstrate\nthe value of negative trajectories and their application in agent-tunning\nscenarios. Our findings offer guidance for developing better agent-tuning\nmethods and low-resource data usage techniques.", "published": "2024-02-18 17:10:07", "link": "http://arxiv.org/abs/2402.11651v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Competition of Mechanisms: Tracing How Language Models Handle Facts and\n  Counterfactuals", "abstract": "Interpretability research aims to bridge the gap between empirical success\nand our scientific understanding of the inner workings of large language models\n(LLMs). However, most existing research focuses on analyzing a single\nmechanism, such as how models copy or recall factual knowledge. In this work,\nwe propose a formulation of competition of mechanisms, which focuses on the\ninterplay of multiple mechanisms instead of individual mechanisms and traces\nhow one of them becomes dominant in the final prediction. We uncover how and\nwhere mechanisms compete within LLMs using two interpretability methods: logit\ninspection and attention modification. Our findings show traces of the\nmechanisms and their competition across various model components and reveal\nattention positions that effectively control the strength of certain\nmechanisms. Code: https://github.com/francescortu/comp-mech. Data:\nhttps://huggingface.co/datasets/francescortu/comp-mech.", "published": "2024-02-18 17:26:51", "link": "http://arxiv.org/abs/2402.11655v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "One Prompt To Rule Them All: LLMs for Opinion Summary Evaluation", "abstract": "Evaluation of opinion summaries using conventional reference-based metrics\nrarely provides a holistic evaluation and has been shown to have a relatively\nlow correlation with human judgments. Recent studies suggest using Large\nLanguage Models (LLMs) as reference-free metrics for NLG evaluation, however,\nthey remain unexplored for opinion summary evaluation. Moreover, limited\nopinion summary evaluation datasets inhibit progress. To address this, we\nrelease the SUMMEVAL-OP dataset covering 7 dimensions related to the evaluation\nof opinion summaries: fluency, coherence, relevance, faithfulness, aspect\ncoverage, sentiment consistency, and specificity. We investigate Op-I-Prompt a\ndimension-independent prompt, and Op-Prompts, a dimension-dependent set of\nprompts for opinion summary evaluation. Experiments indicate that Op-I-Prompt\nemerges as a good alternative for evaluating opinion summaries achieving an\naverage Spearman correlation of 0.70 with humans, outperforming all previous\napproaches. To the best of our knowledge, we are the first to investigate LLMs\nas evaluators on both closed-source and open-source models in the opinion\nsummarization domain.", "published": "2024-02-18 19:13:52", "link": "http://arxiv.org/abs/2402.11683v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Why Lift so Heavy? Slimming Large Language Models by Cutting Off the\n  Layers", "abstract": "Large Language Models (LLMs) possess outstanding capabilities in addressing\nvarious natural language processing (NLP) tasks. However, the sheer size of\nthese models poses challenges in terms of storage, training and inference due\nto the inclusion of billions of parameters through layer stacking. While\ntraditional approaches such as model pruning or distillation offer ways for\nreducing model size, they often come at the expense of performance retention.\nIn our investigation, we systematically explore the approach of reducing the\nnumber of layers in LLMs. Surprisingly, we observe that even with fewer layers,\nLLMs maintain similar or better performance levels, particularly in\nprompt-based fine-tuning for text classification tasks. Remarkably, in certain\ncases, models with a single layer outperform their fully layered counterparts.\nThese findings offer valuable insights for future work aimed at mitigating the\nsize constraints of LLMs while preserving their performance, thereby opening\navenues for significantly more efficient use of LLMs.", "published": "2024-02-18 20:47:10", "link": "http://arxiv.org/abs/2402.11700v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Note on Bias to Complete", "abstract": "Minimizing social bias strengthens societal bonds, promoting shared\nunderstanding and better decision-making. We revisit the definition of bias by\ndiscovering new bias types (e.g., societal status) in dynamic environments and\ndescribe them relative to context, such as culture, region, time, and personal\nbackground. Our framework includes eight hypotheses about bias and a minimizing\nbias strategy for each assumption as well as five methods as proposed solutions\nin LLM. The realization of the framework is yet to be completed.", "published": "2024-02-18 21:20:33", "link": "http://arxiv.org/abs/2402.11710v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MORL-Prompt: An Empirical Analysis of Multi-Objective Reinforcement\n  Learning for Discrete Prompt Optimization", "abstract": "RL-based techniques can be employed to search for prompts that, when fed into\na target language model, maximize a set of user-specified reward functions.\nHowever, in many target applications, the natural reward functions are in\ntension with one another -- for example, content preservation vs. style\nmatching in style transfer tasks. Current techniques focus on maximizing the\naverage of reward functions, which does not necessarily lead to prompts that\nachieve balance across rewards -- an issue that has been well-studied in the\nmulti-objective and robust optimization literature. In this paper, we conduct\nan empirical comparison of several existing multi-objective optimization\ntechniques adapted to this new setting: RL-based discrete prompt optimization.\nWe compare two methods optimizing the volume of the Pareto reward surface and\none method that chooses an update direction that benefits all rewards\nsimultaneously. We evaluate performance on two NLP tasks: style transfer and\nmachine translation, each using three competing reward functions. Our\nexperiments demonstrate that multi-objective methods that directly optimize the\nvolume of the Pareto reward surface perform better and achieve a better balance\nof all rewards than those that attempt to find monotonic update directions.", "published": "2024-02-18 21:25:09", "link": "http://arxiv.org/abs/2402.11711v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modelling Political Coalition Negotiations Using LLM-based Agents", "abstract": "Coalition negotiations are a cornerstone of parliamentary democracies,\ncharacterised by complex interactions and strategic communications among\npolitical parties. Despite its significance, the modelling of these\nnegotiations has remained unexplored with the domain of Natural Language\nProcessing (NLP), mostly due to lack of proper data. In this paper, we\nintroduce coalition negotiations as a novel NLP task, and model it as a\nnegotiation between large language model-based agents. We introduce a\nmultilingual dataset, POLCA, comprising manifestos of European political\nparties and coalition agreements over a number of elections in these countries.\nThis dataset addresses the challenge of the current scope limitations in\npolitical negotiation modelling by providing a diverse, real-world basis for\nsimulation. Additionally, we propose a hierarchical Markov decision process\ndesigned to simulate the process of coalition negotiation between political\nparties and predict the outcomes. We evaluate the performance of\nstate-of-the-art large language models (LLMs) as agents in handling coalition\nnegotiations, offering insights into their capabilities and paving the way for\nfuture advancements in political modelling.", "published": "2024-02-18 21:28:06", "link": "http://arxiv.org/abs/2402.11712v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement", "abstract": "Recent studies show that large language models (LLMs) improve their\nperformance through self-feedback on certain tasks while degrade on others. We\ndiscovered that such a contrary is due to LLM's bias in evaluating their own\noutput. In this paper, we formally define LLM's self-bias - the tendency to\nfavor its own generation - using two statistics. We analyze six LLMs (GPT-4,\nGPT-3.5, Gemini, LLaMA2, Mixtral and DeepSeek) on translation, constrained text\ngeneration, and mathematical reasoning tasks. We find that self-bias is\nprevalent in all examined LLMs across multiple languages and tasks. Our\nanalysis reveals that while the self-refine pipeline improves the fluency and\nunderstandability of model outputs, it further amplifies self-bias. To mitigate\nsuch biases, we discover that larger model size and external feedback with\naccurate assessment can significantly reduce bias in the self-refine pipeline,\nleading to actual performance improvement in downstream tasks. The code and\ndata are released at https://github.com/xu1998hz/llm_self_bias.", "published": "2024-02-18 03:10:39", "link": "http://arxiv.org/abs/2402.11436v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SciAgent: Tool-augmented Language Models for Scientific Reasoning", "abstract": "Scientific reasoning poses an excessive challenge for even the most advanced\nLarge Language Models (LLMs). To make this task more practical and solvable for\nLLMs, we introduce a new task setting named tool-augmented scientific\nreasoning. This setting supplements LLMs with scalable toolsets, and shifts the\nfocus from pursuing an omniscient problem solver to a proficient tool-user. To\nfacilitate the research of such setting, we construct a tool-augmented training\ncorpus named MathFunc which encompasses over 30,000 samples and roughly 6,000\ntools. Building on MathFunc, we develop SciAgent to retrieve, understand and,\nif necessary, use tools for scientific problem solving. Additionally, we craft\na benchmark, SciToolBench, spanning five scientific domains to evaluate LLMs'\nabilities with tool assistance. Extensive experiments on SciToolBench confirm\nthe effectiveness of SciAgent. Notably, SciAgent-Mistral-7B surpasses other\nLLMs with the same size by more than 13% in absolute accuracy. Furthermore,\nSciAgent-DeepMath-7B shows much superior performance than ChatGPT.", "published": "2024-02-18 04:19:44", "link": "http://arxiv.org/abs/2402.11451v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Federated Fine-tuning of Large Language Models under Heterogeneous Tasks\n  and Client Resources", "abstract": "Federated Learning (FL) has recently been applied to the parameter-efficient\nfine-tuning of Large Language Models (LLMs). While promising, it raises\nsignificant challenges due to the heterogeneous resources and data\ndistributions of clients. This study introduces FlexLoRA, a simple yet\neffective aggregation scheme for LLM fine-tuning, which mitigates the ``bucket\neffect'' in traditional FL that restricts the potential of clients with ample\nresources by tying them to the capabilities of the least-resourced\nparticipants. FlexLoRA allows for dynamic adjustment of local LoRA ranks,\nfostering the development of a global model imbued with broader, less\ntask-specific knowledge. By synthesizing a full-size LoRA weight from\nindividual client contributions and employing Singular Value Decomposition\n(SVD) for weight redistribution, FlexLoRA fully leverages heterogeneous client\nresources. Involving thousands of clients performing heterogeneous NLP tasks\nand client resources, our experiments validate the efficacy of FlexLoRA, with\nthe federated global model achieving consistently better improvement over SOTA\nFL methods in downstream NLP task performance across various heterogeneous\ndistributions. FlexLoRA's practicality is further underscored by our\ntheoretical analysis and its seamless integration with existing LoRA-based FL\nmethods, offering a path toward cross-device, privacy-preserving federated\ntuning for LLMs.", "published": "2024-02-18 08:32:59", "link": "http://arxiv.org/abs/2402.11505v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "From Prejudice to Parity: A New Approach to Debiasing Large Language\n  Model Word Embeddings", "abstract": "Embeddings play a pivotal role in the efficacy of Large Language Models. They\nare the bedrock on which these models grasp contextual relationships and foster\na more nuanced understanding of language and consequently perform remarkably on\na plethora of complex tasks that require a fundamental understanding of human\nlanguage. Given that these embeddings themselves often reflect or exhibit bias,\nit stands to reason that these models may also inadvertently learn this bias.\nIn this work, we build on the seminal previous work and propose DeepSoftDebias,\nan algorithm that uses a neural network to perform 'soft debiasing'. We\nexhaustively evaluate this algorithm across a variety of SOTA datasets,\naccuracy metrics, and challenging NLP tasks. We find that DeepSoftDebias\noutperforms the current state-of-the-art methods at reducing bias across\ngender, race, and religion.", "published": "2024-02-18 08:53:41", "link": "http://arxiv.org/abs/2402.11512v6", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Large Language Model-driven Meta-structure Discovery in Heterogeneous\n  Information Network", "abstract": "Heterogeneous information networks (HIN) have gained increasing popularity in\nrecent years for capturing complex relations between diverse types of nodes.\nMeta-structures are proposed as a useful tool to identify the important\npatterns in HINs, but hand-crafted meta-structures pose significant challenges\nfor scaling up, drawing wide research attention towards developing automatic\nsearch algorithms. Previous efforts primarily focused on searching for\nmeta-structures with good empirical performance, overlooking the importance of\nhuman comprehensibility and generalizability. To address this challenge, we\ndraw inspiration from the emergent reasoning abilities of large language models\n(LLMs). We propose ReStruct, a meta-structure search framework that integrates\nLLM reasoning into the evolutionary procedure. ReStruct uses a grammar\ntranslator to encode the meta-structures into natural language sentences, and\nleverages the reasoning power of LLMs to evaluate their semantic feasibility.\nBesides, ReStruct also employs performance-oriented evolutionary operations.\nThese two competing forces allow ReStruct to jointly optimize the semantic\nexplainability and empirical performance of meta-structures. Furthermore,\nReStruct contains a differential LLM explainer to generate and refine natural\nlanguage explanations for the discovered meta-structures by reasoning through\nthe search history. Experiments on eight representative HIN datasets\ndemonstrate that ReStruct achieves state-of-the-art performance in both\nrecommendation and node classification tasks. Moreover, a survey study\ninvolving 73 graduate students shows that the discovered meta-structures and\ngenerated explanations by ReStruct are substantially more comprehensible. Our\ncode and questionnaire are available at https://github.com/LinChen-65/ReStruct.", "published": "2024-02-18 09:21:12", "link": "http://arxiv.org/abs/2402.11518v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Advancing Translation Preference Modeling with RLHF: A Step Towards\n  Cost-Effective Solution", "abstract": "Faithfulness, expressiveness, and elegance is the constant pursuit in machine\ntranslation. However, traditional metrics like \\textit{BLEU} do not strictly\nalign with human preference of translation quality. In this paper, we explore\nleveraging reinforcement learning with human feedback (\\textit{RLHF}) to\nimprove translation quality. It is non-trivial to collect a large high-quality\ndataset of human comparisons between translations, especially for low-resource\nlanguages. To address this issue, we propose a cost-effective preference\nlearning strategy, optimizing reward models by distinguishing between human and\nmachine translations. In this manner, the reward model learns the deficiencies\nof machine translation compared to human and guides subsequent improvements in\nmachine translation. Experimental results demonstrate that \\textit{RLHF} can\neffectively enhance translation quality and this improvement benefits other\ntranslation directions not trained with \\textit{RLHF}. Further analysis\nindicates that the model's language capabilities play a crucial role in\npreference learning. A reward model with strong language capabilities can more\nsensitively learn the subtle differences in translation quality and align\nbetter with real human translation preferences.", "published": "2024-02-18 09:51:49", "link": "http://arxiv.org/abs/2402.11525v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PreAct: Prediction Enhances Agent's Planning Ability", "abstract": "Addressing the disparity between forecasts and actual results can enable\nindividuals to expand their thought processes and stimulate self-reflection,\nthus promoting accurate planning. In this research, we present **PreAct**, an\nagent framework that integrates **pre**diction, **rea**soning, and **act**ion.\nBy utilizing the information derived from predictions, the large language model\n(LLM) agent can provide a wider range and more strategically focused reasoning.\nThis leads to more efficient actions that aid the agent in accomplishing\nintricate tasks. Our experimental results show that PreAct surpasses the ReAct\nmethod in completing complex tasks and that PreAct's performance can be further\nimproved when paired with other memory or selection strategy techniques. We\npresented the model with varying quantities of historical predictions and\ndiscovered that these predictions consistently enhance LLM planning.The\nvariances in single-step reasoning between PreAct and ReAct indicate that\nPreAct indeed has benefits in terms of diversity and strategic orientation over\nReAct.", "published": "2024-02-18 10:15:38", "link": "http://arxiv.org/abs/2402.11534v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Deciphering the Impact of Pretraining Data on Large Language Models\n  through Machine Unlearning", "abstract": "Through pretraining on a corpus with various sources, Large Language Models\n(LLMs) have gained impressive performance. However, the impact of each\ncomponent of the pretraining corpus remains opaque. As a result, the\norganization of the pretraining corpus is still empirical and may deviate from\nthe optimal. To address this issue, we systematically analyze the impact of 48\ndatasets from 5 major categories of pretraining data of LLMs and measure their\nimpacts on LLMs using benchmarks about nine major categories of model\ncapabilities. Our analyses provide empirical results about the contribution of\nmultiple corpora on the performances of LLMs, along with their joint impact\npatterns, including complementary, orthogonal, and correlational relationships.\nWe also identify a set of ``high-impact data'' such as Books that is\nsignificantly related to a set of model capabilities. These findings provide\ninsights into the organization of data to support more efficient pretraining of\nLLMs.", "published": "2024-02-18 10:36:05", "link": "http://arxiv.org/abs/2402.11537v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Language Models Can Better Understand Knowledge Graphs Than We\n  Thought", "abstract": "When we integrate factual knowledge from knowledge graphs (KGs) into large\nlanguage models (LLMs) to enhance their performance, the cost of injection\nthrough training increases with the scale of the models. Consequently, there is\nsignificant interest in developing prompt strategies that effectively\nincorporate KG information into LLMs. However, the community has not yet\ncomprehensively understood how LLMs process and interpret KG information in\ndifferent input formats and organizations within prompts, and researchers often\nrely on trial and error. To address this gap, we design extensive experiments\nto empirically study LLMs' comprehension of different KG prompts. At the\nliteral level, we reveal LLMs' preferences for various input formats (from\nlinearized triples to fluent natural language text). At the attention\ndistribution level, we discuss the underlying mechanisms driving these\npreferences. We then investigate how the organization of structured knowledge\nimpacts LLMs and evaluate LLMs' robustness in processing and utilizing KG\ninformation in practical scenarios. Our experiments show that (1) linearized\ntriples are more effective than fluent NL text in helping LLMs understand KG\ninformation and answer fact-intensive questions; (2) Different LLMs exhibit\nvarying preferences for different organizational formats of triples; (3) LLMs\nwith larger scales are more susceptible to noisy, incomplete subgraphs.", "published": "2024-02-18 10:44:03", "link": "http://arxiv.org/abs/2402.11541v4", "categories": ["cs.CL", "cs.AI", "I.2.4; I.2.7"], "primary_category": "cs.CL"}
{"title": "Question Answering Over Spatio-Temporal Knowledge Graph", "abstract": "Spatio-temporal knowledge graphs (STKGs) extend the concept of knowledge\ngraphs (KGs) by incorporating time and location information. While the research\ncommunity's focus on Knowledge Graph Question Answering (KGQA), the field of\nanswering questions incorporating both spatio-temporal information based on\nSTKGs remains largely unexplored. Furthermore, a lack of comprehensive datasets\nalso has hindered progress in this area. To address this issue, we present\nSTQAD, a dataset comprising 10,000 natural language questions for\nspatio-temporal knowledge graph question answering (STKGQA). Unfortunately,\nvarious state-of-the-art KGQA approaches fall far short of achieving\nsatisfactory performance on our dataset. In response, we propose STCQA, a new\nspatio-temporal KGQA approach that utilizes a novel STKG embedding method named\nSTComplEx. By extracting temporal and spatial information from a question, our\nQA model can better comprehend the question and retrieve accurate answers from\nthe STKG. Through extensive experiments, we demonstrate the quality of our\ndataset and the effectiveness of our STKGQA method.", "published": "2024-02-18 10:44:48", "link": "http://arxiv.org/abs/2402.11542v1", "categories": ["cs.CL", "cs.AI", "I.2.4; I.2.7"], "primary_category": "cs.CL"}
{"title": "Syntactic Language Change in English and German: Metrics, Parsers, and\n  Convergences", "abstract": "Many studies have shown that human languages tend to optimize for lower\ncomplexity and increased communication efficiency. Syntactic dependency\ndistance, which measures the linear distance between dependent words, is often\nconsidered a key indicator of language processing difficulty and working memory\nload. The current paper looks at diachronic trends in syntactic language change\nin both English and German, using corpora of parliamentary debates from the\nlast c. 160 years. We base our observations on five dependency parsers,\nincluding the widely used Stanford CoreNLP as well as 4 newer alternatives. Our\nanalysis of syntactic language change goes beyond linear dependency distance\nand explores 15 metrics relevant to dependency distance minimization (DDM)\nand/or based on tree graph properties, such as the tree height and degree\nvariance. Even though we have evidence that recent parsers trained on modern\ntreebanks are not heavily affected by data 'noise' such as spelling changes and\nOCR errors in our historic data, we find that results of syntactic language\nchange are sensitive to the parsers involved, which is a caution against using\na single parser for evaluating syntactic language change as done in previous\nwork. We also show that syntactic language change over the time period\ninvestigated is largely similar between English and German for the different\nmetrics explored: only 4% of cases we examine yield opposite conclusions\nregarding upwards and downtrends of syntactic metrics across German and\nEnglish. We also show that changes in syntactic measures seem to be more\nfrequent at the tails of sentence length distributions. To our best knowledge,\nours is the most comprehensive analysis of syntactic language change using\nmodern NLP technology in recent corpora of English and German.", "published": "2024-02-18 11:46:16", "link": "http://arxiv.org/abs/2402.11549v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LongAgent: Scaling Language Models to 128k Context through Multi-Agent\n  Collaboration", "abstract": "Large language models (LLMs) have demonstrated impressive performance in\nunderstanding language and executing complex reasoning tasks. However, LLMs\nwith long context windows have been notorious for their expensive training\ncosts and high inference latency. Even the most advanced models such as GPT-4\nand Claude2 often make mistakes when processing inputs of over $100k$ tokens, a\nphenomenon also known as \\textit{lost in the middle}. In this paper, we propose\n\\textsc{LongAgent}, a method based on multi-agent collaboration, which scales\nLLMs (e.g., LLaMA) to a context of 128K and demonstrates potential superiority\nin long-text processing compared to GPT-4. In \\textsc{LongAgent}, a leader is\nresponsible for understanding user intent and directing team members to acquire\ninformation from documents. Due to members' hallucinations, it is non-trivial\nfor a leader to obtain accurate information from the responses of dozens to\nhundreds of members. To address this, we develop an \\textit{inter-member\ncommunication} mechanism to resolve response conflicts caused by hallucinations\nthrough information sharing. Our experimental results indicate that\n\\textsc{LongAgent} offers a promising alternative for long-text processing. The\nagent team instantiated with LLaMA-7B achieves significant improvements in\ntasks such as 128k-long text retrieval, multi-hop question answering, compared\nto GPT-4.", "published": "2024-02-18 11:46:52", "link": "http://arxiv.org/abs/2402.11550v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Visual In-Context Learning for Large Vision-Language Models", "abstract": "In Large Visual Language Models (LVLMs), the efficacy of In-Context Learning\n(ICL) remains limited by challenges in cross-modal interactions and\nrepresentation disparities. To overcome these challenges, we introduce a novel\nVisual In-Context Learning (VICL) method comprising Visual Demonstration\nRetrieval, Intent-Oriented Image Summarization, and Intent-Oriented\nDemonstration Composition. Our approach retrieves images via ''Retrieval &\nRerank'' paradigm, summarises images with task intent and task-specific visual\nparsing, and composes language-based demonstrations that reduce token count and\nalleviate cross-modal interaction problem. Experimental evaluations on five\nvisual reasoning datasets demonstrate the effectiveness of our method.\nMoreover, our extensive experiments leverage information flow analysis to\nelucidate the effectiveness of our method, and investigate the impact of length\nand position of demonstrations for LVLM. The use of in-context unlearning\nfurther shows promise in resetting specific model knowledge without retraining.", "published": "2024-02-18 12:43:38", "link": "http://arxiv.org/abs/2402.11574v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Revisiting Zeroth-Order Optimization for Memory-Efficient LLM\n  Fine-Tuning: A Benchmark", "abstract": "In the evolving landscape of natural language processing (NLP), fine-tuning\npre-trained Large Language Models (LLMs) with first-order (FO) optimizers like\nSGD and Adam has become standard. Yet, as LLMs grow {in size}, the substantial\nmemory overhead from back-propagation (BP) for FO gradient computation presents\na significant challenge. Addressing this issue is crucial, especially for\napplications like on-device training where memory efficiency is paramount. This\npaper proposes a shift towards BP-free, zeroth-order (ZO) optimization as a\nsolution for reducing memory costs during LLM fine-tuning, building on the\ninitial concept introduced by MeZO. Unlike traditional ZO-SGD methods, our work\nexpands the exploration to a wider array of ZO optimization techniques, through\na comprehensive, first-of-its-kind benchmarking study across five LLM families\n(Roberta, OPT, LLaMA, Vicuna, Mistral), three task complexities, and five\nfine-tuning schemes. Our study unveils previously overlooked optimization\nprinciples, highlighting the importance of task alignment, the role of the\nforward gradient method, and the balance between algorithm complexity and\nfine-tuning performance. We further introduce novel enhancements to ZO\noptimization, including block-wise descent, hybrid training, and gradient\nsparsity. Our study offers a promising direction for achieving further\nmemory-efficient LLM fine-tuning. Codes to reproduce all our experiments are at\nhttps://github.com/ZO-Bench/ZO-LLM .", "published": "2024-02-18 14:08:48", "link": "http://arxiv.org/abs/2402.11592v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Metacognitive Retrieval-Augmented Large Language Models", "abstract": "Retrieval-augmented generation have become central in natural language\nprocessing due to their efficacy in generating factual content. While\ntraditional methods employ single-time retrieval, more recent approaches have\nshifted towards multi-time retrieval for multi-hop reasoning tasks. However,\nthese strategies are bound by predefined reasoning steps, potentially leading\nto inaccuracies in response generation. This paper introduces MetaRAG, an\napproach that combines the retrieval-augmented generation process with\nmetacognition. Drawing from cognitive psychology, metacognition allows an\nentity to self-reflect and critically evaluate its cognitive processes. By\nintegrating this, MetaRAG enables the model to monitor, evaluate, and plan its\nresponse strategies, enhancing its introspective reasoning abilities. Through a\nthree-step metacognitive regulation pipeline, the model can identify\ninadequacies in initial cognitive responses and fixes them. Empirical\nevaluations show that MetaRAG significantly outperforms existing methods.", "published": "2024-02-18 15:41:31", "link": "http://arxiv.org/abs/2402.11626v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Discrete Neural Algorithmic Reasoning", "abstract": "Neural algorithmic reasoning aims to capture computations with neural\nnetworks via learning the models to imitate the execution of classic\nalgorithms. While common architectures are expressive enough to contain the\ncorrect model in the weights space, current neural reasoners are struggling to\ngeneralize well on out-of-distribution data. On the other hand, classic\ncomputations are not affected by distributional shifts as they can be described\nas transitions between discrete computational states. In this work, we propose\nto force neural reasoners to maintain the execution trajectory as a combination\nof finite predefined states. To achieve that, we separate discrete and\ncontinuous data flows and describe the interaction between them. Trained with\nsupervision on the algorithm's state transitions, such models are able to\nperfectly align with the original algorithm. To show this, we evaluate our\napproach on multiple algorithmic problems and get perfect test scores both in\nsingle-task and multitask setups. Moreover, the proposed architectural choice\nallows us to prove the correctness of the learned algorithms for any test~data.", "published": "2024-02-18 16:03:04", "link": "http://arxiv.org/abs/2402.11628v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Autocorrect for Estonian texts: final report from project EKTB25", "abstract": "The project was funded in 2021-2023 by the National Programme of Estonian\nLanguage Technology. Its main aim was to develop spelling and grammar\ncorrection tools for the Estonian language. The main challenge was the very\nsmall amount of available error correction data needed for such development. To\nmitigate this, (1) we annotated more correction data for model training and\ntesting, (2) we tested transfer-learning, i.e. retraining machine learning\nmodels created for other tasks, so as not to depend solely on correction data,\n(3) we compared the developed method and model with alternatives, including\nlarge language models. We also developed automatic evaluation, which can\ncalculate the accuracy and yield of corrections by error category, so that the\neffectiveness of different methods can be compared in detail.\n  There has been a breakthrough in large language models during the project:\nGPT4, a commercial language model with Estonian-language support, has been\ncreated. We took into account the existence of the model when adjusting plans\nand in the report we present a comparison with the ability of GPT4 to improve\nthe Estonian language text.\n  The final results show that the approach we have developed provides better\nscores than GPT4 and the result is usable but not entirely reliable yet. The\nreport also contains ideas on how GPT4 and other major language models can be\nimplemented in the future, focusing on open-source solutions.\n  All results of this project are open-data/open-source, with licenses that\nallow them to be used for purposes including commercial ones.", "published": "2024-02-18 18:20:57", "link": "http://arxiv.org/abs/2402.11671v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Multi-Aspect Framework for Counter Narrative Evaluation using Large\n  Language Models", "abstract": "Counter narratives - informed responses to hate speech contexts designed to\nrefute hateful claims and de-escalate encounters - have emerged as an effective\nhate speech intervention strategy. While previous work has proposed automatic\ncounter narrative generation methods to aid manual interventions, the\nevaluation of these approaches remains underdeveloped. Previous automatic\nmetrics for counter narrative evaluation lack alignment with human judgment as\nthey rely on superficial reference comparisons instead of incorporating key\naspects of counter narrative quality as evaluation criteria. To address prior\nevaluation limitations, we propose a novel evaluation framework prompting LLMs\nto provide scores and feedback for generated counter narrative candidates using\n5 defined aspects derived from guidelines from counter narrative specialized\nNGOs. We found that LLM evaluators achieve strong alignment to human-annotated\nscores and feedback and outperform alternative metrics, indicating their\npotential as multi-aspect, reference-free and interpretable evaluators for\ncounter narrative evaluation.", "published": "2024-02-18 18:56:07", "link": "http://arxiv.org/abs/2402.11676v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ALLaVA: Harnessing GPT4V-Synthesized Data for Lite Vision-Language\n  Models", "abstract": "Large vision-language models (LVLMs) have shown premise in a broad range of\nvision-language tasks with their strong reasoning and generalization\ncapabilities. However, they require considerable computational resources for\ntraining and deployment. This study aims to bridge the performance gap between\ntraditional-scale LVLMs and resource-friendly lite versions by adopting\nhigh-quality training data. To this end, we propose a comprehensive pipeline\nfor generating a synthetic dataset. The key idea is to leverage strong\nproprietary models to generate (i) fine-grained image annotations for\nvision-language alignment and (ii) complex reasoning visual question-answering\npairs for visual instruction fine-tuning, yielding 1.3M samples in total. We\ntrain a series of lite VLMs on the synthetic dataset and experimental results\ndemonstrate the effectiveness of the proposed scheme, where they achieve\ncompetitive performance on 17 benchmarks among 4B LVLMs, and even perform on\npar with 7B/13B-scale models on various benchmarks. This work highlights the\nfeasibility of adopting high-quality data in crafting more efficient LVLMs. We\nname our dataset \\textit{ALLaVA}, and open-source it to research community for\ndeveloping better resource-efficient LVLMs for wider usage.", "published": "2024-02-18 19:26:49", "link": "http://arxiv.org/abs/2402.11684v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Vision-Flan: Scaling Human-Labeled Tasks in Visual Instruction Tuning", "abstract": "Despite vision-language models' (VLMs) remarkable capabilities as versatile\nvisual assistants, two substantial challenges persist within the existing VLM\nframeworks: (1) lacking task diversity in pretraining and visual instruction\ntuning, and (2) annotation error and bias in GPT-4 synthesized instruction\ntuning data. Both challenges lead to issues such as poor generalizability,\nhallucination, and catastrophic forgetting. To address these challenges, we\nconstruct Vision-Flan, the most diverse publicly available visual instruction\ntuning dataset to date, comprising 187 diverse tasks and 1,664,261 instances\nsourced from academic datasets, and each task is accompanied by an\nexpert-written instruction. In addition, we propose a two-stage instruction\ntuning framework, in which VLMs are firstly finetuned on Vision-Flan and\nfurther tuned on GPT-4 synthesized data. We find this two-stage tuning\nframework significantly outperforms the traditional single-stage visual\ninstruction tuning framework and achieves the state-of-the-art performance\nacross a wide range of multi-modal evaluation benchmarks. Finally, we conduct\nin-depth analyses to understand visual instruction tuning and our findings\nreveal that: (1) GPT-4 synthesized data does not substantially enhance VLMs'\ncapabilities but rather modulates the model's responses to human-preferred\nformats; (2) A minimal quantity (e.g., 1,000) of GPT-4 synthesized data can\neffectively align VLM responses with human-preference; (3) Visual instruction\ntuning mainly helps large-language models (LLMs) to understand visual features.", "published": "2024-02-18 19:38:44", "link": "http://arxiv.org/abs/2402.11690v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "GNNavi: Navigating the Information Flow in Large Language Models by\n  Graph Neural Network", "abstract": "Large Language Models (LLMs) exhibit strong In-Context Learning (ICL)\ncapabilities when prompts with demonstrations are used. However, fine-tuning\nstill remains crucial to further enhance their adaptability. Prompt-based\nfine-tuning proves to be an effective fine-tuning method in low-data scenarios,\nbut high demands on computing resources limit its practicality. We address this\nissue by introducing a prompt-based parameter-efficient fine-tuning (PEFT)\napproach. GNNavi leverages insights into ICL's information flow dynamics, which\nindicates that label words act in prompts as anchors for information\npropagation. GNNavi employs a Graph Neural Network (GNN) layer to precisely\nguide the aggregation and distribution of information flow during the\nprocessing of prompts by hardwiring the desired information flow into the GNN.\nOur experiments on text classification tasks with GPT-2 and Llama2 show GNNavi\nsurpasses standard prompt-based fine-tuning methods in few-shot settings by\nupdating just 0.2% to 0.5% of parameters. We compare GNNavi with prevalent PEFT\napproaches, such as prefix tuning, LoRA and Adapter in terms of performance and\nefficiency. Our analysis reveals that GNNavi enhances information flow and\nensures a clear aggregation process.", "published": "2024-02-18 21:13:05", "link": "http://arxiv.org/abs/2402.11709v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Shaping Human-AI Collaboration: Varied Scaffolding Levels in Co-writing\n  with Language Models", "abstract": "Advances in language modeling have paved the way for novel human-AI\nco-writing experiences. This paper explores how varying levels of scaffolding\nfrom large language models (LLMs) shape the co-writing process. Employing a\nwithin-subjects field experiment with a Latin square design, we asked\nparticipants (N=131) to respond to argumentative writing prompts under three\nrandomly sequenced conditions: no AI assistance (control), next-sentence\nsuggestions (low scaffolding), and next-paragraph suggestions (high\nscaffolding). Our findings reveal a U-shaped impact of scaffolding on writing\nquality and productivity (words/time). While low scaffolding did not\nsignificantly improve writing quality or productivity, high scaffolding led to\nsignificant improvements, especially benefiting non-regular writers and less\ntech-savvy users. No significant cognitive burden was observed while using the\nscaffolded writing tools, but a moderate decrease in text ownership and\nsatisfaction was noted. Our results have broad implications for the design of\nAI-powered writing tools, including the need for personalized scaffolding\nmechanisms.", "published": "2024-02-18 22:27:42", "link": "http://arxiv.org/abs/2402.11723v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Aligning Modalities in Vision Large Language Models via Preference\n  Fine-tuning", "abstract": "Instruction-following Vision Large Language Models (VLLMs) have achieved\nsignificant progress recently on a variety of tasks. These approaches merge\nstrong pre-trained vision models and large language models (LLMs). Since these\ncomponents are trained separately, the learned representations need to be\naligned with joint training on additional image-language pairs. This procedure\nis not perfect and can cause the model to hallucinate - provide answers that do\nnot accurately reflect the image, even when the core LLM is highly factual and\nthe vision backbone has sufficiently complete representations. In this work, we\nframe the hallucination problem as an alignment issue, tackle it with\npreference tuning. Specifically, we propose POVID to generate feedback data\nwith AI models. We use ground-truth instructions as the preferred response and\na two-stage approach to generate dispreferred data. First, we prompt GPT-4V to\ninject plausible hallucinations into the correct answer. Second, we distort the\nimage to trigger the inherent hallucination behavior of the VLLM. This is an\nautomated approach, which does not rely on human data generation or require a\nperfect expert, which makes it easily scalable. Finally, both of these\ngeneration strategies are integrated into an RLHF pipeline via Direct\nPreference Optimization. In experiments across broad benchmarks, we show that\nwe can not only reduce hallucinations, but improve model performance across\nstandard benchmarks, outperforming prior approaches. Our data and code are\navailable at https://github.com/YiyangZhou/POVID.", "published": "2024-02-18 00:56:16", "link": "http://arxiv.org/abs/2402.11411v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "LoRETTA: Low-Rank Economic Tensor-Train Adaptation for\n  Ultra-Low-Parameter Fine-Tuning of Large Language Models", "abstract": "Various parameter-efficient fine-tuning (PEFT) techniques have been proposed\nto enable computationally efficient fine-tuning while maintaining model\nperformance. However, existing PEFT methods are still limited by the growing\nnumber of trainable parameters with the rapid deployment of Large Language\nModels (LLMs). To address this challenge, we present LoRETTA, an\nultra-parameter-efficient framework that significantly reduces trainable\nparameters through tensor-train decomposition. Specifically, we propose two\nmethods, named {LoRETTA}$_{adp}$ and {LoRETTA}$_{rep}$. The former employs\ntensorized adapters, offering a high-performance yet lightweight approach for\nthe fine-tuning of LLMs. The latter emphasizes fine-tuning via weight\nparameterization with a set of small tensor factors. LoRETTA achieves\ncomparable or better performance than most widely used PEFT methods with up to\n$100\\times$ fewer parameters on the LLaMA-2-7B models. Furthermore, empirical\nresults demonstrate that the proposed method effectively improves training\nefficiency, enjoys better multi-task learning performance, and enhances the\nanti-overfitting capability. Plug-and-play codes built upon the Huggingface\nframework and PEFT library will be released.", "published": "2024-02-18 01:20:00", "link": "http://arxiv.org/abs/2402.11417v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "InfuserKI: Enhancing Large Language Models with Knowledge Graphs via\n  Infuser-Guided Knowledge Integration", "abstract": "Large Language Models (LLMs) have achieved exceptional capabilities in open\ngeneration across various domains, yet they encounter difficulties with tasks\nthat require intensive knowledge. To address these challenges, methods for\nintegrating knowledge have been developed, which augment LLMs with\ndomain-specific knowledge graphs through external modules. These approaches,\nhowever, face data inefficiency issues as they necessitate the processing of\nboth known and unknown knowledge for fine-tuning. Thus, our research focuses on\na novel problem: efficiently integrating unknown knowledge into LLMs without\nunnecessary overlap of known knowledge. A risk of introducing new knowledge is\nthe potential forgetting of existing knowledge. To mitigate this risk, we\npropose the innovative {\\method} framework. This framework employs transformer\ninternal states to determine when to enrich LLM outputs with additional\ninformation, effectively preventing knowledge forgetting. Performance\nevaluations using the UMLS-2.5k and MetaQA domain knowledge graphs reveal that\n{\\method} not only successfully integrates new knowledge but also outperforms\nstate-of-the-art baselines, reducing knowledge forgetting by 9\\% and 6\\%,\nrespectively.", "published": "2024-02-18 03:36:26", "link": "http://arxiv.org/abs/2402.11441v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Curious Case of Searching for the Correlation between Training Data\n  and Adversarial Robustness of Transformer Textual Models", "abstract": "Existing works have shown that fine-tuned textual transformer models achieve\nstate-of-the-art prediction performances but are also vulnerable to adversarial\ntext perturbations. Traditional adversarial evaluation is often done\n\\textit{only after} fine-tuning the models and ignoring the training data. In\nthis paper, we want to prove that there is also a strong correlation between\ntraining data and model robustness. To this end, we extract 13 different\nfeatures representing a wide range of input fine-tuning corpora properties and\nuse them to predict the adversarial robustness of the fine-tuned models.\nFocusing mostly on encoder-only transformer models BERT and RoBERTa with\nadditional results for BART, ELECTRA, and GPT2, we provide diverse evidence to\nsupport our argument. First, empirical analyses show that (a) extracted\nfeatures can be used with a lightweight classifier such as Random Forest to\npredict the attack success rate effectively, and (b) features with the most\ninfluence on the model robustness have a clear correlation with the robustness.\nSecond, our framework can be used as a fast and effective additional tool for\nrobustness evaluation since it (a) saves 30x-193x runtime compared to the\ntraditional technique, (b) is transferable across models, (c) can be used under\nadversarial training, and (d) robust to statistical randomness. Our code is\npublicly available at \\url{https://github.com/CaptainCuong/RobustText_ACL2024}.", "published": "2024-02-18 05:58:25", "link": "http://arxiv.org/abs/2402.11469v2", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "LEIA: Facilitating Cross-lingual Knowledge Transfer in Language Models\n  with Entity-based Data Augmentation", "abstract": "Adapting English-based large language models (LLMs) to other languages has\nbecome increasingly popular due to the efficiency and potential of\ncross-lingual transfer. However, existing language adaptation methods often\noverlook the benefits of cross-lingual supervision. In this study, we introduce\nLEIA, a language adaptation tuning method that utilizes Wikipedia entity names\naligned across languages. This method involves augmenting the target language\ncorpus with English entity names and training the model using left-to-right\nlanguage modeling. We assess LEIA on diverse question answering datasets using\n7B-parameter LLMs, demonstrating significant performance gains across various\nnon-English languages. The source code is available at\nhttps://github.com/studio-ousia/leia.", "published": "2024-02-18 07:24:34", "link": "http://arxiv.org/abs/2402.11485v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Developing Autonomous Robot-Mediated Behavior Coaching Sessions with\n  Haru", "abstract": "This study presents an empirical investigation into the design and impact of\nautonomous dialogues in human-robot interaction for behavior change coaching.\nWe focus on the use of Haru, a tabletop social robot, and explore the\nimplementation of the Tiny Habits method for fostering positive behavior\nchange. The core of our study lies in developing a fully autonomous dialogue\nsystem that maximizes Haru's emotional expressiveness and unique personality.\nOur methodology involved iterative design and extensive testing of the dialogue\nsystem, ensuring it effectively embodied the principles of the Tiny Habits\nmethod while also incorporating strategies for trust-raising and\ntrust-dampening. The effectiveness of the final version of the dialogue was\nevaluated in an experimental study with human participants (N=12). The results\nindicated a significant improvement in perceptions of Haru's liveliness,\ninteractivity, and neutrality. Additionally, our study contributes to the\nbroader understanding of dialogue design in social robotics, offering practical\ninsights for future developments in the field.", "published": "2024-02-18 12:33:54", "link": "http://arxiv.org/abs/2402.11569v1", "categories": ["cs.RO", "cs.AI", "cs.CL"], "primary_category": "cs.RO"}
{"title": "Ain't Misbehavin' -- Using LLMs to Generate Expressive Robot Behavior in\n  Conversations with the Tabletop Robot Haru", "abstract": "Social robots aim to establish long-term bonds with humans through engaging\nconversation. However, traditional conversational approaches, reliant on\nscripted interactions, often fall short in maintaining engaging conversations.\nThis paper addresses this limitation by integrating large language models\n(LLMs) into social robots to achieve more dynamic and expressive conversations.\nWe introduce a fully-automated conversation system that leverages LLMs to\ngenerate robot responses with expressive behaviors, congruent with the robot's\npersonality. We incorporate robot behavior with two modalities: 1) a\ntext-to-speech (TTS) engine capable of various delivery styles, and 2) a\nlibrary of physical actions for the robot. We develop a custom,\nstate-of-the-art emotion recognition model to dynamically select the robot's\ntone of voice and utilize emojis from LLM output as cues for generating robot\nactions. A demo of our system is available here. To illuminate design and\nimplementation issues, we conduct a pilot study where volunteers chat with a\nsocial robot using our proposed system, and we analyze their feedback,\nconducting a rigorous error analysis of chat transcripts. Feedback was\noverwhelmingly positive, with participants commenting on the robot's empathy,\nhelpfulness, naturalness, and entertainment. Most negative feedback was due to\nautomatic speech recognition (ASR) errors which had limited impact on\nconversations. However, we observed a small class of errors, such as the LLM\nrepeating itself or hallucinating fictitious information and human responses,\nthat have the potential to derail conversations, raising important issues for\nLLM application.", "published": "2024-02-18 12:35:52", "link": "http://arxiv.org/abs/2402.11571v1", "categories": ["cs.RO", "cs.AI", "cs.CL"], "primary_category": "cs.RO"}
{"title": "Logical Closed Loop: Uncovering Object Hallucinations in Large\n  Vision-Language Models", "abstract": "Object hallucination has been an Achilles' heel which hinders the broader\napplications of large vision-language models (LVLMs). Object hallucination\nrefers to the phenomenon that the LVLMs claim non-existent objects in the\nimage. To mitigate the object hallucinations, instruction tuning and external\nmodel-based detection methods have been proposed, which either require\nlarge-scare computational resources or depend on the detection result of\nexternal models. However, there remains an under-explored field to utilize the\nLVLM itself to alleviate object hallucinations. In this work, we adopt the\nintuition that the LVLM tends to respond logically consistently for existent\nobjects but inconsistently for hallucinated objects. Therefore, we propose a\nLogical Closed Loop-based framework for Object Hallucination Detection and\nMitigation, namely LogicCheckGPT. In specific, we devise logical consistency\nprobing to raise questions with logical correlations, inquiring about\nattributes from objects and vice versa. Whether their responses can form a\nlogical closed loop serves as an indicator of object hallucination. As a\nplug-and-play method, it can be seamlessly applied to all existing LVLMs.\nComprehensive experiments conducted on three benchmarks across four LVLMs have\ndemonstrated significant improvements brought by our method, indicating its\neffectiveness and generality.", "published": "2024-02-18 15:28:39", "link": "http://arxiv.org/abs/2402.11622v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "In-Context Learning with Transformers: Softmax Attention Adapts to\n  Function Lipschitzness", "abstract": "A striking property of transformers is their ability to perform in-context\nlearning (ICL), a machine learning framework in which the learner is presented\nwith a novel context during inference implicitly through some data, and tasked\nwith making a prediction in that context. As such, that learner must adapt to\nthe context without additional training. We explore the role of softmax\nattention in an ICL setting where each context encodes a regression task. We\nshow that an attention unit learns a window that it uses to implement a\nnearest-neighbors predictor adapted to the landscape of the pretraining tasks.\nSpecifically, we show that this window widens with decreasing Lipschitzness and\nincreasing label noise in the pretraining tasks. We also show that on low-rank,\nlinear problems, the attention unit learns to project onto the appropriate\nsubspace before inference. Further, we show that this adaptivity relies\ncrucially on the softmax activation and thus cannot be replicated by the linear\nactivation often studied in prior theoretical analyses.", "published": "2024-02-18 16:37:32", "link": "http://arxiv.org/abs/2402.11639v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Opening the black box of language acquisition", "abstract": "Recent advances in large language models using deep learning techniques have\nrenewed interest on how languages can be learned from data. However, it is\nunclear whether or how these models represent grammatical information from the\nlearned languages. In addition, the models must be pre-trained on large corpora\nbefore they can be used. In this work, we propose an alternative, more\ntransparent and cognitively plausible architecture for learning language.\nInstead of using deep learning, our approach uses a minimal cognitive\narchitecture based on sequence memory and chunking. The learning mechanism is\nbased on the principles of reinforcement learning. We test our architecture on\na number of natural-like toy languages. Results show that the model can learn\nthese artificial languages from scratch and extract grammatical information\nthat supports learning. Our study demonstrates the power of this simple\narchitecture and stresses the importance of sequence memory as a key component\nof the language learning process. Since other animals do not seem to have a\nfaithful sequence memory, this may explain why only humans have developed\ncomplex languages.", "published": "2024-02-18 19:11:58", "link": "http://arxiv.org/abs/2402.11681v1", "categories": ["cs.CL", "cs.NA", "math.NA"], "primary_category": "cs.CL"}
{"title": "How Susceptible are Large Language Models to Ideological Manipulation?", "abstract": "Large Language Models (LLMs) possess the potential to exert substantial\ninfluence on public perceptions and interactions with information. This raises\nconcerns about the societal impact that could arise if the ideologies within\nthese models can be easily manipulated. In this work, we investigate how\neffectively LLMs can learn and generalize ideological biases from their\ninstruction-tuning data. Our findings reveal a concerning vulnerability:\nexposure to only a small amount of ideologically driven samples significantly\nalters the ideology of LLMs. Notably, LLMs demonstrate a startling ability to\nabsorb ideology from one topic and generalize it to even unrelated ones. The\nease with which LLMs' ideologies can be skewed underscores the risks associated\nwith intentionally poisoned training data by malicious actors or inadvertently\nintroduced biases by data annotators. It also emphasizes the imperative for\nrobust safeguards to mitigate the influence of ideological manipulations on\nLLMs.", "published": "2024-02-18 22:36:19", "link": "http://arxiv.org/abs/2402.11725v3", "categories": ["cs.CL", "cs.CR", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Numerical Claim Detection in Finance: A New Financial Dataset,\n  Weak-Supervision Model, and Market Analysis", "abstract": "In this paper, we investigate the influence of claims in analyst reports and\nearnings calls on financial market returns, considering them as significant\nquarterly events for publicly traded companies. To facilitate a comprehensive\nanalysis, we construct a new financial dataset for the claim detection task in\nthe financial domain. We benchmark various language models on this dataset and\npropose a novel weak-supervision model that incorporates the knowledge of\nsubject matter experts (SMEs) in the aggregation function, outperforming\nexisting approaches. We also demonstrate the practical utility of our proposed\nmodel by constructing a novel measure of optimism. Here, we observe the\ndependence of earnings surprise and return on our optimism measure. Our\ndataset, models, and code are publicly (under CC BY 4.0 license) available on\nGitHub.", "published": "2024-02-18 22:55:26", "link": "http://arxiv.org/abs/2402.11728v2", "categories": ["cs.CL", "cs.LG", "q-fin.CP"], "primary_category": "cs.CL"}
{"title": "ModelGPT: Unleashing LLM's Capabilities for Tailored Model Generation", "abstract": "The rapid advancement of Large Language Models (LLMs) has revolutionized\nvarious sectors by automating routine tasks, marking a step toward the\nrealization of Artificial General Intelligence (AGI). However, they still\nstruggle to accommodate the diverse and specific needs of users and simplify\nthe utilization of AI models for the average user. In response, we propose\nModelGPT, a novel framework designed to determine and generate AI models\nspecifically tailored to the data or task descriptions provided by the user,\nleveraging the capabilities of LLMs. Given user requirements, ModelGPT is able\nto provide tailored models at most 270x faster than the previous paradigms\n(e.g. all-parameter or LoRA finetuning). Comprehensive experiments on NLP, CV,\nand Tabular datasets attest to the effectiveness of our framework in making AI\nmodels more accessible and user-friendly. Our code is available at\nhttps://github.com/IshiKura-a/ModelGPT.", "published": "2024-02-18 11:24:34", "link": "http://arxiv.org/abs/2402.12408v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "MSynFD: Multi-hop Syntax aware Fake News Detection", "abstract": "The proliferation of social media platforms has fueled the rapid\ndissemination of fake news, posing threats to our real-life society. Existing\nmethods use multimodal data or contextual information to enhance the detection\nof fake news by analyzing news content and/or its social context. However,\nthese methods often overlook essential textual news content (articles) and\nheavily rely on sequential modeling and global attention to extract semantic\ninformation. These existing methods fail to handle the complex, subtle twists\nin news articles, such as syntax-semantics mismatches and prior biases, leading\nto lower performance and potential failure when modalities or social context\nare missing. To bridge these significant gaps, we propose a novel multi-hop\nsyntax aware fake news detection (MSynFD) method, which incorporates\ncomplementary syntax information to deal with subtle twists in fake news.\nSpecifically, we introduce a syntactical dependency graph and design a\nmulti-hop subgraph aggregation mechanism to capture multi-hop syntax. It\nextends the effect of word perception, leading to effective noise filtering and\nadjacent relation enhancement. Subsequently, a sequential relative\nposition-aware Transformer is designed to capture the sequential information,\ntogether with an elaborate keyword debiasing module to mitigate the prior bias.\nExtensive experimental results on two public benchmark datasets verify the\neffectiveness and superior performance of our proposed MSynFD over\nstate-of-the-art detection models.", "published": "2024-02-18 05:40:33", "link": "http://arxiv.org/abs/2402.14834v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "MIKE: A New Benchmark for Fine-grained Multimodal Entity Knowledge\n  Editing", "abstract": "Multimodal knowledge editing represents a critical advancement in enhancing\nthe capabilities of Multimodal Large Language Models (MLLMs). Despite its\npotential, current benchmarks predominantly focus on coarse-grained knowledge,\nleaving the intricacies of fine-grained (FG) multimodal entity knowledge\nlargely unexplored. This gap presents a notable challenge, as FG entity\nrecognition is pivotal for the practical deployment and effectiveness of MLLMs\nin diverse real-world scenarios. To bridge this gap, we introduce MIKE, a\ncomprehensive benchmark and dataset specifically designed for the FG multimodal\nentity knowledge editing. MIKE encompasses a suite of tasks tailored to assess\ndifferent perspectives, including Vanilla Name Answering, Entity-Level Caption,\nand Complex-Scenario Recognition. In addition, a new form of knowledge editing,\nMulti-step Editing, is introduced to evaluate the editing efficiency. Through\nour extensive evaluations, we demonstrate that the current state-of-the-art\nmethods face significant challenges in tackling our proposed benchmark,\nunderscoring the complexity of FG knowledge editing in MLLMs. Our findings\nspotlight the urgent need for novel approaches in this domain, setting a clear\nagenda for future research and development efforts within the community.", "published": "2024-02-18 07:15:03", "link": "http://arxiv.org/abs/2402.14835v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Stealthy Attack on Large Language Model based Recommendation", "abstract": "Recently, the powerful large language models (LLMs) have been instrumental in\npropelling the progress of recommender systems (RS). However, while these\nsystems have flourished, their susceptibility to security threats has been\nlargely overlooked. In this work, we reveal that the introduction of LLMs into\nrecommendation models presents new security vulnerabilities due to their\nemphasis on the textual content of items. We demonstrate that attackers can\nsignificantly boost an item's exposure by merely altering its textual content\nduring the testing phase, without requiring direct interference with the\nmodel's training process. Additionally, the attack is notably stealthy, as it\ndoes not affect the overall recommendation performance and the modifications to\nthe text are subtle, making it difficult for users and platforms to detect. Our\ncomprehensive experiments across four mainstream LLM-based recommendation\nmodels demonstrate the superior efficacy and stealthiness of our approach. Our\nwork unveils a significant security gap in LLM-based recommendation systems and\npaves the way for future research on protecting these systems.", "published": "2024-02-18 16:51:02", "link": "http://arxiv.org/abs/2402.14836v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "An Empirical Categorization of Prompting Techniques for Large Language\n  Models: A Practitioner's Guide", "abstract": "Due to rapid advancements in the development of Large Language Models (LLMs),\nprogramming these models with prompts has recently gained significant\nattention. However, the sheer number of available prompt engineering techniques\ncreates an overwhelming landscape for practitioners looking to utilize these\ntools. For the most efficient and effective use of LLMs, it is important to\ncompile a comprehensive list of prompting techniques and establish a\nstandardized, interdisciplinary categorization framework. In this survey, we\nexamine some of the most well-known prompting techniques from both academic and\npractical viewpoints and classify them into seven distinct categories. We\npresent an overview of each category, aiming to clarify their unique\ncontributions and showcase their practical applications in real-world examples\nin order to equip fellow practitioners with a structured framework for\nunderstanding and categorizing prompting techniques tailored to their specific\ndomains. We believe that this approach will help simplify the complex landscape\nof prompt engineering and enable more effective utilization of LLMs in various\napplications. By providing practitioners with a systematic approach to prompt\ncategorization, we aim to assist in navigating the intricacies of effective\nprompt design for conversational pre-trained LLMs and inspire new possibilities\nin their respective fields.", "published": "2024-02-18 23:03:56", "link": "http://arxiv.org/abs/2402.14837v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BESA: Pruning Large Language Models with Blockwise Parameter-Efficient\n  Sparsity Allocation", "abstract": "Large language models (LLMs) have demonstrated outstanding performance in\nvarious tasks, such as text summarization, text question-answering, and etc.\nWhile their performance is impressive, the computational footprint due to their\nvast number of parameters can be prohibitive. Existing solutions such as\nSparseGPT and Wanda attempt to alleviate this issue through weight pruning.\nHowever, their layer-wise approach results in significant perturbation to the\nmodel's output and requires meticulous hyperparameter tuning, such as the\npruning rate, which can adversely affect overall model performance. To address\nthis, this paper introduces a novel LLM pruning technique dubbed blockwise\nparameter-efficient sparsity allocation (BESA) by applying a blockwise\nreconstruction loss. In contrast to the typical layer-wise pruning techniques,\nBESA is characterized by two distinctive attributes: i) it targets the overall\npruning error with respect to individual transformer blocks, and ii) it\nallocates layer-specific sparsity in a differentiable manner, both of which\nensure reduced performance degradation after pruning. Our experiments show that\nBESA achieves state-of-the-art performance, efficiently pruning LLMs like\nLLaMA1, and LLaMA2 with 7B to 70B parameters on a single A100 GPU in just five\nhours. Code is available at https://github.com/OpenGVLab/LLMPrune-BESA.", "published": "2024-02-18 12:44:15", "link": "http://arxiv.org/abs/2402.16880v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Ploutos: Towards interpretable stock movement prediction with financial\n  large language model", "abstract": "Recent advancements in large language models (LLMs) have opened new pathways\nfor many domains. However, the full potential of LLMs in financial investments\nremains largely untapped. There are two main challenges for typical deep\nlearning-based methods for quantitative finance. First, they struggle to fuse\ntextual and numerical information flexibly for stock movement prediction.\nSecond, traditional methods lack clarity and interpretability, which impedes\ntheir application in scenarios where the justification for predictions is\nessential. To solve the above challenges, we propose Ploutos, a novel financial\nLLM framework that consists of PloutosGen and PloutosGPT. The PloutosGen\ncontains multiple primary experts that can analyze different modal data, such\nas text and numbers, and provide quantitative strategies from different\nperspectives. Then PloutosGPT combines their insights and predictions and\ngenerates interpretable rationales. To generate accurate and faithful\nrationales, the training strategy of PloutosGPT leverage rearview-mirror\nprompting mechanism to guide GPT-4 to generate rationales, and a dynamic token\nweighting mechanism to finetune LLM by increasing key tokens weight. Extensive\nexperiments show our framework outperforms the state-of-the-art methods on both\nprediction accuracy and interpretability.", "published": "2024-02-18 10:28:18", "link": "http://arxiv.org/abs/2403.00782v1", "categories": ["q-fin.ST", "cs.AI", "cs.CL"], "primary_category": "q-fin.ST"}
{"title": "Utilizing BERT for Information Retrieval: Survey, Applications,\n  Resources, and Challenges", "abstract": "Recent years have witnessed a substantial increase in the use of deep\nlearning to solve various natural language processing (NLP) problems. Early\ndeep learning models were constrained by their sequential or unidirectional\nnature, such that they struggled to capture the contextual relationships across\ntext inputs. The introduction of bidirectional encoder representations from\ntransformers (BERT) leads to a robust encoder for the transformer model that\ncan understand the broader context and deliver state-of-the-art performance\nacross various NLP tasks. This has inspired researchers and practitioners to\napply BERT to practical problems, such as information retrieval (IR). A survey\nthat focuses on a comprehensive analysis of prevalent approaches that apply\npretrained transformer encoders like BERT to IR can thus be useful for academia\nand the industry. In light of this, we revisit a variety of BERT-based methods\nin this survey, cover a wide range of techniques of IR, and group them into six\nhigh-level categories: (i) handling long documents, (ii) integrating semantic\ninformation, (iii) balancing effectiveness and efficiency, (iv) predicting the\nweights of terms, (v) query expansion, and (vi) document expansion. We also\nprovide links to resources, including datasets and toolkits, for BERT-based IR\nsystems. A key highlight of our survey is the comparison between BERT's\nencoder-based models and the latest generative Large Language Models (LLMs),\nsuch as ChatGPT, which rely on decoders. Despite the popularity of LLMs, we\nfind that for specific tasks, finely tuned BERT encoders still outperform, and\nat a lower deployment cost. Finally, we summarize the comprehensive outcomes of\nthe survey and suggest directions for future research in the area.", "published": "2024-02-18 23:22:40", "link": "http://arxiv.org/abs/2403.00784v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "ChatGPT in Linear Algebra: Strides Forward, Steps to Go", "abstract": "As soon as a new technology emerges, the education community explores its\naffordances and the possibilities to apply it in education. In this paper, we\nanalyze sessions with ChatGPT around topics in basic Linear Algebra. We reflect\nthe process undertaken by the ChatGPT along the recent year in our area of\ninterest, emphasising the vast improvement that has been done in grappling with\nLinear Algebra problems. In particular, the question whether this software can\nbe a teaching assistant or even somehow replace the human teacher, is\naddressed. As of the time this paper is written, the answer is generally\nnegative. For the small part where the answer can be positive, some reflections\nabout an original instrumental genesis are given.\n  Communication with the software gives the impression to talk to a human, and\nsometimes the question is whether the software understands the question or not.\nTherefore, the reader's attention is drawn to the fact that ChatGPT works on a\nstatistical basis and not according to reflection and understanding.", "published": "2024-02-18 07:35:01", "link": "http://arxiv.org/abs/2403.15399v1", "categories": ["cs.CY", "cs.CL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "Integrating Pre-Trained Language Model with Physical Layer\n  Communications", "abstract": "The burgeoning field of on-device AI communication, where devices exchange\ninformation directly through embedded foundation models, such as language\nmodels (LMs), requires robust, efficient, and generalizable communication\nframeworks. However, integrating these frameworks with existing wireless\nsystems and effectively managing noise and bit errors pose significant\nchallenges. In this work, we introduce a practical ondevice AI communication\nframework, integrated with physical layer (PHY) communication functions,\ndemonstrated through its performance on a link-level simulator. Our framework\nincorporates end-to-end training with channel noise to enhance resilience,\nincorporates vector quantized variational autoencoders (VQ-VAE) for efficient\nand robust communication, and utilizes pre-trained encoder-decoder transformers\nfor improved generalization capabilities. Simulations, across various\ncommunication scenarios, reveal that our framework achieves a 50% reduction in\ntransmission size while demonstrating substantial generalization ability and\nnoise robustness under standardized 3GPP channel models.", "published": "2024-02-18 17:27:51", "link": "http://arxiv.org/abs/2402.11656v2", "categories": ["cs.IT", "cs.CL", "cs.LG", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
