{"title": "Let the Pretrained Language Models \"Imagine\" for Short Texts Topic\n  Modeling", "abstract": "Topic models are one of the compelling methods for discovering latent\nsemantics in a document collection. However, it assumes that a document has\nsufficient co-occurrence information to be effective. However, in short texts,\nco-occurrence information is minimal, which results in feature sparsity in\ndocument representation. Therefore, existing topic models (probabilistic or\nneural) mostly fail to mine patterns from them to generate coherent topics. In\nthis paper, we take a new approach to short-text topic modeling to address the\ndata-sparsity issue by extending short text into longer sequences using\nexisting pre-trained language models (PLMs). Besides, we provide a simple\nsolution extending a neural topic model to reduce the effect of noisy\nout-of-topics text generation from PLMs. We observe that our model can\nsubstantially improve the performance of short-text topic modeling. Extensive\nexperiments on multiple real-world datasets under extreme data sparsity\nscenarios show that our models can generate high-quality topics outperforming\nstate-of-the-art models.", "published": "2023-10-24 00:23:30", "link": "http://arxiv.org/abs/2310.15420v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What Makes it Ok to Set a Fire? Iterative Self-distillation of Contexts\n  and Rationales for Disambiguating Defeasible Social and Moral Situations", "abstract": "Moral or ethical judgments rely heavily on the specific contexts in which\nthey occur. Understanding varying shades of defeasible contextualizations\n(i.e., additional information that strengthens or attenuates the moral\nacceptability of an action) is critical to accurately represent the subtlety\nand intricacy of grounded human moral judgment in real-life scenarios.\n  We introduce defeasible moral reasoning: a task to provide grounded contexts\nthat make an action more or less morally acceptable, along with commonsense\nrationales that justify the reasoning. To elicit high-quality task data, we\ntake an iterative self-distillation approach that starts from a small amount of\nunstructured seed knowledge from GPT-3 and then alternates between (1)\nself-distillation from student models; (2) targeted filtering with a critic\nmodel trained by human judgment (to boost validity) and NLI (to boost\ndiversity); (3) self-imitation learning (to amplify the desired data quality).\nThis process yields a student model that produces defeasible contexts with\nimproved validity, diversity, and defeasibility. From this model we distill a\nhigh-quality dataset, \\delta-Rules-of-Thumb, of 1.2M entries of\ncontextualizations and rationales for 115K defeasible moral actions rated\nhighly by human annotators 85.9% to 99.8% of the time. Using \\delta-RoT we\nobtain a final student model that wins over all intermediate student models by\na notable margin.", "published": "2023-10-24 00:51:29", "link": "http://arxiv.org/abs/2310.15431v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Interpreting Answers to Yes-No Questions in User-Generated Content", "abstract": "Interpreting answers to yes-no questions in social media is difficult. Yes\nand no keywords are uncommon, and the few answers that include them are rarely\nto be interpreted what the keywords suggest. In this paper, we present a new\ncorpus of 4,442 yes-no question-answer pairs from Twitter. We discuss\nlinguistic characteristics of answers whose interpretation is yes or no, as\nwell as answers whose interpretation is unknown. We show that large language\nmodels are far from solving this problem, even after fine-tuning and blending\nother corpora for the same problem but outside social media.", "published": "2023-10-24 02:27:06", "link": "http://arxiv.org/abs/2310.15464v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CRaSh: Clustering, Removing, and Sharing Enhance Fine-tuning without\n  Full Large Language Model", "abstract": "Instruction tuning has recently been recognized as an effective way of\naligning Large Language Models (LLMs) to enhance their generalization ability\nacross various tasks. However, when tuning publicly accessible, centralized\nLLMs with private instruction data, privacy concerns are inevitable. While\ndirect transfer of parameterized modules between models is a plausible approach\nto address this, its implications and effectiveness need further exploration.\nThis paper focuses on Offsite-Tuning (OFT), a representative technique that\ntransfers transformer blocks between centralized LLMs and downstream emulators.\nGiven the limited understanding of the underlying mechanism of OFT, we perform\nan empirical analysis on LLMs from the perspectives of representation and\nfunctional similarity. Interestingly, our findings reveal a unique modular\nstructure within the layers of LLMs that appears to emerge as the model size\nexpands. Simultaneously, we note subtle but potentially significant changes in\nrepresentation and intermediate predictions across the layers. Inspired by\nthese observations, we propose CRaSh, involving Clustering, Removing, and\nSharing, a training-free strategy to derive improved emulators from LLMs. CRaSh\nsignificantly boosts performance of OFT with billions of parameters.\nFurthermore, we investigate the optimal solutions yielded by fine-tuning with\nand without full model through the lens of loss landscape. Our findings\ndemonstrate a linear connectivity among these optima falling over the same\nbasin, thereby highlighting the effectiveness of CRaSh and OFT. The source code\nis publicly available at https://github.com/TsinghuaC3I/CRaSh.", "published": "2023-10-24 03:08:58", "link": "http://arxiv.org/abs/2310.15477v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TRAMS: Training-free Memory Selection for Long-range Language Modeling", "abstract": "The Transformer architecture is crucial for numerous AI models, but it still\nfaces challenges in long-range language modeling. Though several specific\ntransformer architectures have been designed to tackle issues of long-range\ndependencies, existing methods like Transformer-XL are plagued by a high\npercentage of ineffective memories. In this study, we present a plug-and-play\nstrategy, known as TRAining-free Memory Selection (TRAMS), that selects tokens\nparticipating in attention calculation based on one simple metric. This\nstrategy allows us to keep tokens that are likely to have a high attention\nscore with the current queries and ignore the other ones. We have tested our\napproach on the word-level benchmark (WikiText-103) and the character-level\nbenchmark (enwik8), and the results indicate an improvement without having\nadditional training or adding additional parameters.", "published": "2023-10-24 03:42:49", "link": "http://arxiv.org/abs/2310.15494v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Joint Matrix Factorization Analysis of Multilingual Representations", "abstract": "We present an analysis tool based on joint matrix factorization for comparing\nlatent representations of multilingual and monolingual models. An alternative\nto probing, this tool allows us to analyze multiple sets of representations in\na joint manner. Using this tool, we study to what extent and how\nmorphosyntactic features are reflected in the representations learned by\nmultilingual pre-trained models. We conduct a large-scale empirical study of\nover 33 languages and 17 morphosyntactic categories. Our findings demonstrate\nvariations in the encoding of morphosyntactic information across upper and\nlower layers, with category-specific differences influenced by language\nproperties. Hierarchical clustering of the factorization outputs yields a tree\nstructure that is related to phylogenetic trees manually crafted by linguists.\nMoreover, we find the factorization outputs exhibit strong associations with\nperformance observed across different cross-lingual tasks. We release our code\nto facilitate future research.", "published": "2023-10-24 04:43:45", "link": "http://arxiv.org/abs/2310.15513v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fighting Fire with Fire: The Dual Role of LLMs in Crafting and Detecting\n  Elusive Disinformation", "abstract": "Recent ubiquity and disruptive impacts of large language models (LLMs) have\nraised concerns about their potential to be misused (.i.e, generating\nlarge-scale harmful and misleading content). To combat this emerging risk of\nLLMs, we propose a novel \"Fighting Fire with Fire\" (F3) strategy that harnesses\nmodern LLMs' generative and emergent reasoning capabilities to counter\nhuman-written and LLM-generated disinformation. First, we leverage\nGPT-3.5-turbo to synthesize authentic and deceptive LLM-generated content\nthrough paraphrase-based and perturbation-based prefix-style prompts,\nrespectively. Second, we apply zero-shot in-context semantic reasoning\ntechniques with cloze-style prompts to discern genuine from deceptive posts and\nnews articles. In our extensive experiments, we observe GPT-3.5-turbo's\nzero-shot superiority for both in-distribution and out-of-distribution\ndatasets, where GPT-3.5-turbo consistently achieved accuracy at 68-72%, unlike\nthe decline observed in previous customized and fine-tuned disinformation\ndetectors. Our codebase and dataset are available at\nhttps://github.com/mickeymst/F3.", "published": "2023-10-24 04:50:29", "link": "http://arxiv.org/abs/2310.15515v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MarkQA: A large scale KBQA dataset with numerical reasoning", "abstract": "While question answering over knowledge bases (KBQA) has shown progress in\naddressing factoid questions, KBQA with numerical reasoning remains relatively\nunexplored. In this paper, we focus on the complex numerical reasoning in KBQA\nand propose a new task, NR-KBQA, which necessitates the ability to perform both\nmulti-hop reasoning and numerical reasoning. We design a logic form in Python\nformat called PyQL to represent the reasoning process of numerical reasoning\nquestions. To facilitate the development of NR-KBQA, we present a large dataset\ncalled MarkQA, which is automatically constructed from a small set of seeds.\nEach question in MarkQA is equipped with its corresponding SPARQL query,\nalongside the step-by-step reasoning process in the QDMR format and PyQL\nprogram. Experimental results of some state-of-the-art QA methods on the MarkQA\nshow that complex numerical reasoning in KBQA faces great challenges.", "published": "2023-10-24 04:50:59", "link": "http://arxiv.org/abs/2310.15517v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Language Models Meaning Understanding and Consistency by\n  Learning Conceptual Roles from Dictionary", "abstract": "The non-humanlike behaviour of contemporary pre-trained language models\n(PLMs) is a leading cause undermining their trustworthiness. A striking\nphenomenon of such faulty behaviours is the generation of inconsistent\npredictions, which produces logically contradictory results, such as generating\ndifferent predictions for texts delivering the same meaning or violating\nlogical properties. Previous studies exploited data augmentation or implemented\nspecialised loss functions to alleviate the issue. However, their usage is\nlimited, because they consume expensive training resources for large-sized PLMs\nand can only handle a certain consistency type. To this end, we propose a\npractical approach that alleviates the inconsistent behaviour issue by\nfundamentally improving PLMs' meaning awareness. Based on the conceptual role\ntheory, our method allows PLMs to capture accurate meaning by learning precise\ninterrelationships between concepts from word-definition pairs in a dictionary.\nNext, we propose an efficient parameter integration technique that updates only\na few additional parameters to combine the learned interrelationship with PLMs'\npre-trained knowledge. Our experimental results reveal that the approach can\nconcurrently improve multiple types of consistency, enables efficient knowledge\nintegration, and easily applies to other languages.", "published": "2023-10-24 06:15:15", "link": "http://arxiv.org/abs/2310.15541v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unveiling Multilinguality in Transformer Models: Exploring Language\n  Specificity in Feed-Forward Networks", "abstract": "Recent research suggests that the feed-forward module within Transformers can\nbe viewed as a collection of key-value memories, where the keys learn to\ncapture specific patterns from the input based on the training examples. The\nvalues then combine the output from the 'memories' of the keys to generate\npredictions about the next token. This leads to an incremental process of\nprediction that gradually converges towards the final token choice near the\noutput layers. This interesting perspective raises questions about how\nmultilingual models might leverage this mechanism. Specifically, for\nautoregressive models trained on two or more languages, do all neurons (across\nlayers) respond equally to all languages? No! Our hypothesis centers around the\nnotion that during pretraining, certain model parameters learn strong\nlanguage-specific features, while others learn more language-agnostic (shared\nacross languages) features. To validate this, we conduct experiments utilizing\nparallel corpora of two languages that the model was initially pretrained on.\nOur findings reveal that the layers closest to the network's input or output\ntend to exhibit more language-specific behaviour compared to the layers in the\nmiddle.", "published": "2023-10-24 06:45:00", "link": "http://arxiv.org/abs/2310.15552v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MuLMS: A Multi-Layer Annotated Text Corpus for Information Extraction in\n  the Materials Science Domain", "abstract": "Keeping track of all relevant recent publications and experimental results\nfor a research area is a challenging task. Prior work has demonstrated the\nefficacy of information extraction models in various scientific areas.\nRecently, several datasets have been released for the yet understudied\nmaterials science domain. However, these datasets focus on sub-problems such as\nparsing synthesis procedures or on sub-domains, e.g., solid oxide fuel cells.\nIn this resource paper, we present MuLMS, a new dataset of 50 open-access\narticles, spanning seven sub-domains of materials science. The corpus has been\nannotated by domain experts with several layers ranging from named entities\nover relations to frame structures. We present competitive neural models for\nall tasks and demonstrate that multi-task training with existing related\nresources leads to benefits.", "published": "2023-10-24 07:23:46", "link": "http://arxiv.org/abs/2310.15569v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Visually Grounded Continual Language Learning with Selective\n  Specialization", "abstract": "A desirable trait of an artificial agent acting in the visual world is to\ncontinually learn a sequence of language-informed tasks while striking a\nbalance between sufficiently specializing in each task and building a\ngeneralized knowledge for transfer. Selective specialization, i.e., a careful\nselection of model components to specialize in each task, is a strategy to\nprovide control over this trade-off. However, the design of selection\nstrategies requires insights on the role of each model component in learning\nrather specialized or generalizable representations, which poses a gap in\ncurrent research. Thus, our aim with this work is to provide an extensive\nanalysis of selection strategies for visually grounded continual language\nlearning. Due to the lack of suitable benchmarks for this purpose, we introduce\ntwo novel diagnostic datasets that provide enough control and flexibility for a\nthorough model analysis. We assess various heuristics for module specialization\nstrategies as well as quantifiable measures for two different types of model\narchitectures. Finally, we design conceptually simple approaches based on our\nanalysis that outperform common continual learning baselines. Our results\ndemonstrate the need for further efforts towards better aligning continual\nlearning algorithms with the learning behaviors of individual model parts.", "published": "2023-10-24 07:35:23", "link": "http://arxiv.org/abs/2310.15571v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Natural Language Processing for Drug Discovery Knowledge Graphs:\n  promises and pitfalls", "abstract": "Building and analysing knowledge graphs (KGs) to aid drug discovery is a\ntopical area of research. A salient feature of KGs is their ability to combine\nmany heterogeneous data sources in a format that facilitates discovering\nconnections. The utility of KGs has been exemplified in areas such as drug\nrepurposing, with insights made through manual exploration and modelling of the\ndata. In this article, we discuss promises and pitfalls of using natural\nlanguage processing (NLP) to mine unstructured text typically from scientific\nliterature as a data source for KGs. This draws on our experience of initially\nparsing structured data sources such as ChEMBL as the basis for data within a\nKG, and then enriching or expanding upon them using NLP. The fundamental\npromise of NLP for KGs is the automated extraction of data from millions of\ndocuments a task practically impossible to do via human curation alone.\nHowever, there are many potential pitfalls in NLP-KG pipelines such as\nincorrect named entity recognition and ontology linking all of which could\nultimately lead to erroneous inferences and conclusions.", "published": "2023-10-24 07:35:24", "link": "http://arxiv.org/abs/2310.15572v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "POE: Process of Elimination for Multiple Choice Reasoning", "abstract": "Language models (LMs) are capable of conducting in-context learning for\nmultiple choice reasoning tasks, but the options in these tasks are treated\nequally. As humans often first eliminate wrong options before picking the final\ncorrect answer, we argue a similar two-step strategy can make LMs better at\nthese tasks. To this end, we present the Process of Elimination (POE), a\ntwo-step scoring method. In the first step, POE scores each option, and\neliminates seemingly wrong options. In the second step, POE masks these wrong\noptions, and makes the final prediction from the remaining options. Zero-shot\nexperiments on 8 reasoning tasks illustrate the effectiveness of POE, and a\nfollowing analysis finds our method to be especially performant on logical\nreasoning tasks. We further analyze the effect of masks, and show that POE\napplies to few-shot settings and large language models (LLMs) like ChatGPT.", "published": "2023-10-24 07:38:43", "link": "http://arxiv.org/abs/2310.15575v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ScanDL: A Diffusion Model for Generating Synthetic Scanpaths on Texts", "abstract": "Eye movements in reading play a crucial role in psycholinguistic research\nstudying the cognitive mechanisms underlying human language processing. More\nrecently, the tight coupling between eye movements and cognition has also been\nleveraged for language-related machine learning tasks such as the\ninterpretability, enhancement, and pre-training of language models, as well as\nthe inference of reader- and text-specific properties. However, scarcity of eye\nmovement data and its unavailability at application time poses a major\nchallenge for this line of research. Initially, this problem was tackled by\nresorting to cognitive models for synthesizing eye movement data. However, for\nthe sole purpose of generating human-like scanpaths, purely data-driven\nmachine-learning-based methods have proven to be more suitable. Following\nrecent advances in adapting diffusion processes to discrete data, we propose\nScanDL, a novel discrete sequence-to-sequence diffusion model that generates\nsynthetic scanpaths on texts. By leveraging pre-trained word representations\nand jointly embedding both the stimulus text and the fixation sequence, our\nmodel captures multi-modal interactions between the two inputs. We evaluate\nScanDL within- and across-dataset and demonstrate that it significantly\noutperforms state-of-the-art scanpath generation methods. Finally, we provide\nan extensive psycholinguistic analysis that underlines the model's ability to\nexhibit human-like reading behavior. Our implementation is made available at\nhttps://github.com/DiLi-Lab/ScanDL.", "published": "2023-10-24 07:52:19", "link": "http://arxiv.org/abs/2310.15587v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MUSER: A Multi-View Similar Case Retrieval Dataset", "abstract": "Similar case retrieval (SCR) is a representative legal AI application that\nplays a pivotal role in promoting judicial fairness. However, existing SCR\ndatasets only focus on the fact description section when judging the similarity\nbetween cases, ignoring other valuable sections (e.g., the court's opinion)\nthat can provide insightful reasoning process behind. Furthermore, the case\nsimilarities are typically measured solely by the textual semantics of the fact\ndescriptions, which may fail to capture the full complexity of legal cases from\nthe perspective of legal knowledge. In this work, we present MUSER, a similar\ncase retrieval dataset based on multi-view similarity measurement and\ncomprehensive legal element with sentence-level legal element annotations.\nSpecifically, we select three perspectives (legal fact, dispute focus, and law\nstatutory) and build a comprehensive and structured label schema of legal\nelements for each of them, to enable accurate and knowledgeable evaluation of\ncase similarities. The constructed dataset originates from Chinese civil cases\nand contains 100 query cases and 4,024 candidate cases. We implement several\ntext classification algorithms for legal element prediction and various\nretrieval methods for retrieving similar cases on MUSER. The experimental\nresults indicate that incorporating legal elements can benefit the performance\nof SCR models, but further efforts are still required to address the remaining\nchallenges posed by MUSER. The source code and dataset are released at\nhttps://github.com/THUlawtech/MUSER.", "published": "2023-10-24 08:17:11", "link": "http://arxiv.org/abs/2310.15602v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tips for making the most of 64-bit architectures in langage design,\n  libraries or garbage collection", "abstract": "The 64-bit architectures that have become standard today offer unprecedented\nlow-level programming possibilities. For the first time in the history of\ncomputing, the size of address registers far exceeded the physical capacity of\ntheir bus.After a brief reminder of the possibilities offered by the small size\nof addresses compared to the available 64 bits,we develop three concrete\nexamples of how the vacant bits of these registers can be used.Among these\nexamples, two of them concern the implementation of a library for a new\nstatically typed programming language.Firstly, the implementation of\nmulti-precision integers, with the aim of improving performance in terms of\nboth calculation speed and RAM savings.The second example focuses on the\nlibrary's handling of UTF-8 character strings.Here, the idea is to make\nindexing easier by ignoring the physical size of each UTF-8 characters.Finally,\nthe third example is a possible enhancement of garbage collectors, in\nparticular the mark \\& sweep for the object marking phase.", "published": "2023-10-24 08:54:23", "link": "http://arxiv.org/abs/2310.15632v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CoAnnotating: Uncertainty-Guided Work Allocation between Human and Large\n  Language Models for Data Annotation", "abstract": "Annotated data plays a critical role in Natural Language Processing (NLP) in\ntraining models and evaluating their performance. Given recent developments in\nLarge Language Models (LLMs), models such as ChatGPT demonstrate zero-shot\ncapability on many text-annotation tasks, comparable with or even exceeding\nhuman annotators. Such LLMs can serve as alternatives for manual annotation,\ndue to lower costs and higher scalability. However, limited work has leveraged\nLLMs as complementary annotators, nor explored how annotation work is best\nallocated among humans and LLMs to achieve both quality and cost objectives. We\npropose CoAnnotating, a novel paradigm for Human-LLM co-annotation of\nunstructured texts at scale. Under this framework, we utilize uncertainty to\nestimate LLMs' annotation capability. Our empirical study shows CoAnnotating to\nbe an effective means to allocate work from results on different datasets, with\nup to 21% performance improvement over random baseline. For code\nimplementation, see https://github.com/SALT-NLP/CoAnnotating.", "published": "2023-10-24 08:56:49", "link": "http://arxiv.org/abs/2310.15638v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Expression Syntax Information Bottleneck for Math Word Problems", "abstract": "Math Word Problems (MWP) aims to automatically solve mathematical questions\ngiven in texts. Previous studies tend to design complex models to capture\nadditional information in the original text so as to enable the model to gain\nmore comprehensive features. In this paper, we turn our attention in the\nopposite direction, and work on how to discard redundant features containing\nspurious correlations for MWP. To this end, we design an Expression Syntax\nInformation Bottleneck method for MWP (called ESIB) based on variational\ninformation bottleneck, which extracts essential features of expression syntax\ntree while filtering latent-specific redundancy containing syntax-irrelevant\nfeatures. The key idea of ESIB is to encourage multiple models to predict the\nsame expression syntax tree for different problem representations of the same\nproblem by mutual learning so as to capture consistent information of\nexpression syntax tree and discard latent-specific redundancy. To improve the\ngeneralization ability of the model and generate more diverse expressions, we\ndesign a self-distillation loss to encourage the model to rely more on the\nexpression syntax information in the latent space. Experimental results on two\nlarge-scale benchmarks show that our model not only achieves state-of-the-art\nresults but also generates more diverse solutions. The code is available.", "published": "2023-10-24 09:23:57", "link": "http://arxiv.org/abs/2310.15664v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prevalence and prevention of large language model use in crowd work", "abstract": "We show that the use of large language models (LLMs) is prevalent among crowd\nworkers, and that targeted mitigation strategies can significantly reduce, but\nnot eliminate, LLM use. On a text summarization task where workers were not\ndirected in any way regarding their LLM use, the estimated prevalence of LLM\nuse was around 30%, but was reduced by about half by asking workers to not use\nLLMs and by raising the cost of using them, e.g., by disabling copy-pasting.\nSecondary analyses give further insight into LLM use and its prevention: LLM\nuse yields high-quality but homogeneous responses, which may harm research\nconcerned with human (rather than model) behavior and degrade future models\ntrained with crowdsourced data. At the same time, preventing LLM use may be at\nodds with obtaining high-quality responses; e.g., when requesting workers not\nto use LLMs, summaries contained fewer keywords carrying essential information.\nOur estimates will likely change as LLMs increase in popularity or\ncapabilities, and as norms around their usage change. Yet, understanding the\nco-evolution of LLM-based tools and users is key to maintaining the validity of\nresearch done using crowdsourcing, and we provide a critical baseline before\nwidespread adoption ensues.", "published": "2023-10-24 09:52:09", "link": "http://arxiv.org/abs/2310.15683v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Creating a silver standard for patent simplification", "abstract": "Patents are legal documents that aim at protecting inventions on the one hand\nand at making technical knowledge circulate on the other. Their complex style\n-- a mix of legal, technical, and extremely vague language -- makes their\ncontent hard to access for humans and machines and poses substantial challenges\nto the information retrieval community. This paper proposes an approach to\nautomatically simplify patent text through rephrasing. Since no in-domain\nparallel simplification data exist, we propose a method to automatically\ngenerate a large-scale silver standard for patent sentences. To obtain\ncandidates, we use a general-domain paraphrasing system; however, the process\nis error-prone and difficult to control. Thus, we pair it with proper filters\nand construct a cleaner corpus that can successfully be used to train a\nsimplification system. Human evaluation of the synthetic silver corpus shows\nthat it is considered grammatical, adequate, and contains simple sentences.", "published": "2023-10-24 10:00:56", "link": "http://arxiv.org/abs/2310.15689v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Biomedical Lay Summarisation with External Knowledge Graphs", "abstract": "Previous approaches for automatic lay summarisation are exclusively reliant\non the source article that, given it is written for a technical audience (e.g.,\nresearchers), is unlikely to explicitly define all technical concepts or state\nall of the background information that is relevant for a lay audience. We\naddress this issue by augmenting eLife, an existing biomedical lay\nsummarisation dataset, with article-specific knowledge graphs, each containing\ndetailed information on relevant biomedical concepts. Using both automatic and\nhuman evaluations, we systematically investigate the effectiveness of three\ndifferent approaches for incorporating knowledge graphs within lay\nsummarisation models, with each method targeting a distinct area of the\nencoder-decoder model architecture. Our results confirm that integrating\ngraph-based domain knowledge can significantly benefit lay summarisation by\nsubstantially increasing the readability of generated text and improving the\nexplanation of technical concepts.", "published": "2023-10-24 10:25:21", "link": "http://arxiv.org/abs/2310.15702v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Re-Temp: Relation-Aware Temporal Representation Learning for Temporal\n  Knowledge Graph Completion", "abstract": "Temporal Knowledge Graph Completion (TKGC) under the extrapolation setting\naims to predict the missing entity from a fact in the future, posing a\nchallenge that aligns more closely with real-world prediction problems.\nExisting research mostly encodes entities and relations using sequential graph\nneural networks applied to recent snapshots. However, these approaches tend to\noverlook the ability to skip irrelevant snapshots according to entity-related\nrelations in the query and disregard the importance of explicit temporal\ninformation. To address this, we propose our model, Re-Temp (Relation-Aware\nTemporal Representation Learning), which leverages explicit temporal embedding\nas input and incorporates skip information flow after each timestamp to skip\nunnecessary information for prediction. Additionally, we introduce a two-phase\nforward propagation method to prevent information leakage. Through the\nevaluation on six TKGC (extrapolation) datasets, we demonstrate that our model\noutperforms all eight recent state-of-the-art models by a significant margin.", "published": "2023-10-24 10:58:33", "link": "http://arxiv.org/abs/2310.15722v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Variator: Accelerating Pre-trained Models with Plug-and-Play Compression\n  Modules", "abstract": "Pre-trained language models (PLMs) have achieved remarkable results on NLP\ntasks but at the expense of huge parameter sizes and the consequent\ncomputational costs. In this paper, we propose Variator, a parameter-efficient\nacceleration method that enhances computational efficiency through\nplug-and-play compression plugins. Compression plugins are designed to reduce\nthe sequence length via compressing multiple hidden vectors into one and\ntrained with original PLMs frozen. Different from traditional model\nacceleration methods, which compress PLMs to smaller sizes, Variator offers two\ndistinct advantages: (1) In real-world applications, the plug-and-play nature\nof our compression plugins enables dynamic selection of different compression\nplugins with varying acceleration ratios based on the current workload. (2) The\ncompression plugin comprises a few compact neural network layers with minimal\nparameters, significantly saving storage and memory overhead, particularly in\nscenarios with a growing number of tasks. We validate the effectiveness of\nVariator on seven datasets. Experimental results show that Variator can save\n53% computational costs using only 0.9% additional parameters with a\nperformance drop of less than 2%. Moreover, when the model scales to billions\nof parameters, Variator matches the strong performance of uncompressed PLMs.", "published": "2023-10-24 11:00:07", "link": "http://arxiv.org/abs/2310.15724v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RAPL: A Relation-Aware Prototype Learning Approach for Few-Shot\n  Document-Level Relation Extraction", "abstract": "How to identify semantic relations among entities in a document when only a\nfew labeled documents are available? Few-shot document-level relation\nextraction (FSDLRE) is crucial for addressing the pervasive data scarcity\nproblem in real-world scenarios. Metric-based meta-learning is an effective\nframework widely adopted for FSDLRE, which constructs class prototypes for\nclassification. However, existing works often struggle to obtain class\nprototypes with accurate relational semantics: 1) To build prototype for a\ntarget relation type, they aggregate the representations of all entity pairs\nholding that relation, while these entity pairs may also hold other relations,\nthus disturbing the prototype. 2) They use a set of generic NOTA\n(none-of-the-above) prototypes across all tasks, neglecting that the NOTA\nsemantics differs in tasks with different target relation types. In this paper,\nwe propose a relation-aware prototype learning method for FSDLRE to strengthen\nthe relational semantics of prototype representations. By judiciously\nleveraging the relation descriptions and realistic NOTA instances as guidance,\nour method effectively refines the relation prototypes and generates\ntask-specific NOTA prototypes. Extensive experiments demonstrate that our\nmethod outperforms state-of-the-art approaches by average 2.61% $F_1$ across\nvarious settings of two FSDLRE benchmarks.", "published": "2023-10-24 11:35:23", "link": "http://arxiv.org/abs/2310.15743v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Failures Pave the Way: Enhancing Large Language Models through\n  Tuning-free Rule Accumulation", "abstract": "Large Language Models (LLMs) have showcased impressive performance. However,\ndue to their inability to capture relationships among samples, these frozen\nLLMs inevitably keep repeating similar mistakes. In this work, we propose our\nTuning-free Rule Accumulation (TRAN) framework, which guides LLMs in improving\ntheir performance by learning from previous mistakes. Considering data arrives\nsequentially, LLMs gradually accumulate rules from incorrect cases, forming a\nrule collection. These rules are then utilized by the LLMs to avoid making\nsimilar mistakes when processing subsequent inputs. Moreover, the rules remain\nindependent of the primary prompts, seamlessly complementing prompt design\nstrategies. Experimentally, we show that TRAN improves over recent baselines by\na large margin.", "published": "2023-10-24 11:40:34", "link": "http://arxiv.org/abs/2310.15746v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do Differences in Values Influence Disagreements in Online Discussions?", "abstract": "Disagreements are common in online discussions. Disagreement may foster\ncollaboration and improve the quality of a discussion under some conditions.\nAlthough there exist methods for recognizing disagreement, a deeper\nunderstanding of factors that influence disagreement is lacking in the\nliterature. We investigate a hypothesis that differences in personal values are\nindicative of disagreement in online discussions. We show how state-of-the-art\nmodels can be used for estimating values in online discussions and how the\nestimated values can be aggregated into value profiles. We evaluate the\nestimated value profiles based on human-annotated agreement labels. We find\nthat the dissimilarity of value profiles correlates with disagreement in\nspecific cases. We also find that including value information in agreement\nprediction improves performance.", "published": "2023-10-24 12:00:59", "link": "http://arxiv.org/abs/2310.15757v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning From Free-Text Human Feedback -- Collect New Datasets Or Extend\n  Existing Ones?", "abstract": "Learning from free-text human feedback is essential for dialog systems, but\nannotated data is scarce and usually covers only a small fraction of error\ntypes known in conversational AI. Instead of collecting and annotating new\ndatasets from scratch, recent advances in synthetic dialog generation could be\nused to augment existing dialog datasets with the necessary annotations.\nHowever, to assess the feasibility of such an effort, it is important to know\nthe types and frequency of free-text human feedback included in these datasets.\nIn this work, we investigate this question for a variety of commonly used\ndialog datasets, including MultiWoZ, SGD, BABI, PersonaChat,\nWizards-of-Wikipedia, and the human-bot split of the Self-Feeding Chatbot.\nUsing our observations, we derive new taxonomies for the annotation of\nfree-text human feedback in dialogs and investigate the impact of including\nsuch data in response generation for three SOTA language generation models,\nincluding GPT-2, LLAMA, and Flan-T5. Our findings provide new insights into the\ncomposition of the datasets examined, including error types, user response\ntypes, and the relations between them.", "published": "2023-10-24 12:01:11", "link": "http://arxiv.org/abs/2310.15758v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BLESS: Benchmarking Large Language Models on Sentence Simplification", "abstract": "We present BLESS, a comprehensive performance benchmark of the most recent\nstate-of-the-art large language models (LLMs) on the task of text\nsimplification (TS). We examine how well off-the-shelf LLMs can solve this\nchallenging task, assessing a total of 44 models, differing in size,\narchitecture, pre-training methods, and accessibility, on three test sets from\ndifferent domains (Wikipedia, news, and medical) under a few-shot setting. Our\nanalysis considers a suite of automatic metrics as well as a large-scale\nquantitative investigation into the types of common edit operations performed\nby the different models. Furthermore, we perform a manual qualitative analysis\non a subset of model outputs to better gauge the quality of the generated\nsimplifications. Our evaluation indicates that the best LLMs, despite not being\ntrained on TS, perform comparably with state-of-the-art TS baselines.\nAdditionally, we find that certain LLMs demonstrate a greater range and\ndiversity of edit operations. Our performance benchmark will be available as a\nresource for the development of future TS methods and evaluation metrics.", "published": "2023-10-24 12:18:17", "link": "http://arxiv.org/abs/2310.15773v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unnatural language processing: How do language models handle\n  machine-generated prompts?", "abstract": "Language model prompt optimization research has shown that semantically and\ngrammatically well-formed manually crafted prompts are routinely outperformed\nby automatically generated token sequences with no apparent meaning or\nsyntactic structure, including sequences of vectors from a model's embedding\nspace. We use machine-generated prompts to probe how models respond to input\nthat is not composed of natural language expressions. We study the behavior of\nmodels of different sizes in multiple semantic tasks in response to both\ncontinuous and discrete machine-generated prompts, and compare it to the\nbehavior in response to human-generated natural-language prompts. Even when\nproducing a similar output, machine-generated and human prompts trigger\ndifferent response patterns through the network processing pathways, including\ndifferent perplexities, different attention and output entropy distributions,\nand different unit activation profiles. We provide preliminary insight into the\nnature of the units activated by different prompt types, suggesting that only\nnatural language prompts recruit a genuinely linguistic circuit.", "published": "2023-10-24 13:32:20", "link": "http://arxiv.org/abs/2310.15829v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Guard: Empower the LLM to Safeguard Itself", "abstract": "The jailbreak attack can bypass the safety measures of a Large Language Model\n(LLM), generating harmful content. This misuse of LLM has led to negative\nsocietal consequences. Currently, there are two main approaches to address\njailbreak attacks: safety training and safeguards. Safety training focuses on\nfurther training LLM to enhance its safety. On the other hand, safeguards\ninvolve implementing external models or filters to prevent harmful outputs.\nHowever, safety training has constraints in its ability to adapt to new attack\ntypes and often leads to a drop in model performance. Safeguards have proven to\nbe of limited help. To tackle these issues, we propose a novel approach called\nSelf-Guard, which combines the strengths of both safety methods. Self-Guard\nincludes two stages. In the first stage, we enhance the model's ability to\nassess harmful content, and in the second stage, we instruct the model to\nconsistently perform harmful content detection on its own responses. The\nexperiment has demonstrated that Self-Guard is robust against jailbreak\nattacks. In the bad case analysis, we find that LLM occasionally provides\nharmless responses to harmful queries. Additionally, we evaluated the general\ncapabilities of the LLM before and after safety training, providing evidence\nthat Self-Guard does not result in the LLM's performance degradation. In\nsensitivity tests, Self-Guard not only avoids inducing over-sensitivity in LLM\nbut also can even mitigate this issue.", "published": "2023-10-24 14:08:26", "link": "http://arxiv.org/abs/2310.15851v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "In-Context Learning Creates Task Vectors", "abstract": "In-context learning (ICL) in Large Language Models (LLMs) has emerged as a\npowerful new learning paradigm. However, its underlying mechanism is still not\nwell understood. In particular, it is challenging to map it to the \"standard\"\nmachine learning framework, where one uses a training set $S$ to find a\nbest-fitting function $f(x)$ in some hypothesis class. Here we make progress on\nthis problem by showing that the functions learned by ICL often have a very\nsimple structure: they correspond to the transformer LLM whose only inputs are\nthe query $x$ and a single \"task vector\" calculated from the training set.\nThus, ICL can be seen as compressing $S$ into a single task vector\n$\\boldsymbol{\\theta}(S)$ and then using this task vector to modulate the\ntransformer to produce the output. We support the above claim via comprehensive\nexperiments across a range of models and tasks.", "published": "2023-10-24 15:17:14", "link": "http://arxiv.org/abs/2310.15916v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contrastive Learning-based Sentence Encoders Implicitly Weight\n  Informative Words", "abstract": "The performance of sentence encoders can be significantly improved through\nthe simple practice of fine-tuning using contrastive loss. A natural question\narises: what characteristics do models acquire during contrastive learning?\nThis paper theoretically and experimentally shows that contrastive-based\nsentence encoders implicitly weight words based on information-theoretic\nquantities; that is, more informative words receive greater weight, while\nothers receive less. The theory states that, in the lower bound of the optimal\nvalue of the contrastive learning objective, the norm of word embedding\nreflects the information gain associated with the distribution of surrounding\nwords. We also conduct comprehensive experiments using various models, multiple\ndatasets, two methods to measure the implicit weighting of models (Integrated\nGradients and SHAP), and two information-theoretic quantities (information gain\nand self-information). The results provide empirical evidence that contrastive\nfine-tuning emphasizes informative words.", "published": "2023-10-24 15:22:04", "link": "http://arxiv.org/abs/2310.15921v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "This is not a Dataset: A Large Negation Benchmark to Challenge Large\n  Language Models", "abstract": "Although large language models (LLMs) have apparently acquired a certain\nlevel of grammatical knowledge and the ability to make generalizations, they\nfail to interpret negation, a crucial step in Natural Language Processing. We\ntry to clarify the reasons for the sub-optimal performance of LLMs\nunderstanding negation. We introduce a large semi-automatically generated\ndataset of circa 400,000 descriptive sentences about commonsense knowledge that\ncan be true or false in which negation is present in about 2/3 of the corpus in\ndifferent forms. We have used our dataset with the largest available open LLMs\nin a zero-shot approach to grasp their generalization and inference capability\nand we have also fine-tuned some of the models to assess whether the\nunderstanding of negation can be trained. Our findings show that, while LLMs\nare proficient at classifying affirmative sentences, they struggle with\nnegative sentences and lack a deep understanding of negation, often relying on\nsuperficial cues. Although fine-tuning the models on negative sentences\nimproves their performance, the lack of generalization in handling negation is\npersistent, highlighting the ongoing challenges of LLMs regarding negation\nunderstanding and generalization. The dataset and code are publicly available.", "published": "2023-10-24 15:38:21", "link": "http://arxiv.org/abs/2310.15941v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NoteChat: A Dataset of Synthetic Doctor-Patient Conversations\n  Conditioned on Clinical Notes", "abstract": "We introduce NoteChat, a novel cooperative multi-agent framework leveraging\nLarge Language Models (LLMs) to generate patient-physician dialogues. NoteChat\nembodies the principle that an ensemble of role-specific LLMs, through\nstructured role-play and strategic prompting, can perform their assigned roles\nmore effectively. The synergy among these role-playing LLMs results in a\ncohesive and efficient dialogue generation. Evaluation on MTS-dialogue, a\nbenchmark dataset for patient-physician dialogues-note pairs, shows that models\ntrained with the augmented synthetic patient-physician dialogues by NoteChat\noutperforms other state-of-the-art models for generating clinical notes. Our\ncomprehensive automatic and human evaluation demonstrates that NoteChat\nsubstantially surpasses state-of-the-art models like ChatGPT and GPT-4 up to\n22.78% by domain experts in generating superior synthetic patient-physician\ndialogues based on clinical notes. NoteChat has the potential to engage\npatients directly and help clinical documentation, a leading cause of physician\nburnout.", "published": "2023-10-24 15:59:43", "link": "http://arxiv.org/abs/2310.15959v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MuSR: Testing the Limits of Chain-of-thought with Multistep Soft\n  Reasoning", "abstract": "While large language models (LLMs) equipped with techniques like\nchain-of-thought prompting have demonstrated impressive capabilities, they\nstill fall short in their ability to reason robustly in complex settings.\nHowever, evaluating LLM reasoning is challenging because system capabilities\ncontinue to grow while benchmark datasets for tasks like logical deduction have\nremained static. We introduce MuSR, a dataset for evaluating language models on\nmultistep soft reasoning tasks specified in a natural language narrative. This\ndataset has two crucial features. First, it is created through a novel\nneurosymbolic synthetic-to-natural generation algorithm, enabling the\nconstruction of complex reasoning instances that challenge GPT-4 (e.g., murder\nmysteries roughly 1000 words in length) and which can be scaled further as more\ncapable LLMs are released. Second, our dataset instances are free text\nnarratives corresponding to real-world domains of reasoning; this makes it\nsimultaneously much more challenging than other synthetically-crafted\nbenchmarks while remaining realistic and tractable for human annotators to\nsolve with high accuracy. We evaluate a range of LLMs and prompting techniques\non this dataset and characterize the gaps that remain for techniques like\nchain-of-thought to perform robust reasoning.", "published": "2023-10-24 17:59:20", "link": "http://arxiv.org/abs/2310.16049v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NADI 2023: The Fourth Nuanced Arabic Dialect Identification Shared Task", "abstract": "We describe the findings of the fourth Nuanced Arabic Dialect Identification\nShared Task (NADI 2023). The objective of NADI is to help advance\nstate-of-the-art Arabic NLP by creating opportunities for teams of researchers\nto collaboratively compete under standardized conditions. It does so with a\nfocus on Arabic dialects, offering novel datasets and defining subtasks that\nallow for meaningful comparisons between different approaches. NADI 2023\ntargeted both dialect identification (Subtask 1) and dialect-to-MSA machine\ntranslation (Subtask 2 and Subtask 3). A total of 58 unique teams registered\nfor the shared task, of whom 18 teams have participated (with 76 valid\nsubmissions during test phase). Among these, 16 teams participated in Subtask\n1, 5 participated in Subtask 2, and 3 participated in Subtask 3. The winning\nteams achieved 87.27\n  F1 on Subtask 1, 14.76 Bleu in Subtask 2, and 21.10 Bleu in Subtask 3,\nrespectively. Results show that all three subtasks remain challenging, thereby\nmotivating future work in this area. We describe the methods employed by the\nparticipating teams and briefly offer an outlook for NADI.", "published": "2023-10-24 18:41:24", "link": "http://arxiv.org/abs/2310.16117v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Octopus: A Multitask Model and Toolkit for Arabic Natural Language\n  Generation", "abstract": "Understanding Arabic text and generating human-like responses is a\nchallenging endeavor. While many researchers have proposed models and solutions\nfor individual problems, there is an acute shortage of a comprehensive Arabic\nnatural language generation toolkit that is capable of handling a wide range of\ntasks. In this work, we present a novel Arabic text-to-text Transformer model,\nnamely AraT5v2. Our new model is methodically trained on extensive and diverse\ndata, utilizing an extended sequence length of 2,048 tokens. We explore various\npretraining strategies including unsupervised, supervised, and joint\npertaining, under both single and multitask settings. Our models outperform\ncompetitive baselines with large margins. We take our work one step further by\ndeveloping and publicly releasing Octopus, a Python-based package and\ncommand-line toolkit tailored for eight Arabic generation tasks all exploiting\na single model. We release the models and the toolkit on our public repository.", "published": "2023-10-24 19:06:55", "link": "http://arxiv.org/abs/2310.16127v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GenKIE: Robust Generative Multimodal Document Key Information Extraction", "abstract": "Key information extraction (KIE) from scanned documents has gained increasing\nattention because of its applications in various domains. Although promising\nresults have been achieved by some recent KIE approaches, they are usually\nbuilt based on discriminative models, which lack the ability to handle optical\ncharacter recognition (OCR) errors and require laborious token-level labelling.\nIn this paper, we propose a novel generative end-to-end model, named GenKIE, to\naddress the KIE task. GenKIE is a sequence-to-sequence multimodal generative\nmodel that utilizes multimodal encoders to embed visual, layout and textual\nfeatures and a decoder to generate the desired output. Well-designed prompts\nare leveraged to incorporate the label semantics as the weakly supervised\nsignals and entice the generation of the key information. One notable advantage\nof the generative model is that it enables automatic correction of OCR errors.\nBesides, token-level granular annotation is not required. Extensive experiments\non multiple public real-world datasets show that GenKIE effectively generalizes\nover different types of documents and achieves state-of-the-art results. Our\nexperiments also validate the model's robustness against OCR errors, making\nGenKIE highly applicable in real-world scenarios.", "published": "2023-10-24 19:12:56", "link": "http://arxiv.org/abs/2310.16131v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can You Follow Me? Testing Situational Understanding in ChatGPT", "abstract": "Understanding sentence meanings and updating information states appropriately\nacross time -- what we call \"situational understanding\" (SU) -- is a critical\nability for human-like AI agents. SU is essential in particular for chat\nmodels, such as ChatGPT, to enable consistent, coherent, and effective dialogue\nbetween humans and AI. Previous works have identified certain SU limitations in\nnon-chatbot Large Language models (LLMs), but the extent and causes of these\nlimitations are not well understood, and capabilities of current chat-based\nmodels in this domain have not been explored. In this work we tackle these\nquestions, proposing a novel synthetic environment for SU testing which allows\nus to do controlled and systematic testing of SU in chat-oriented models,\nthrough assessment of models' ability to track and enumerate environment\nstates. Our environment also allows for close analysis of dynamics of model\nperformance, to better understand underlying causes for performance patterns.\nWe apply our test to ChatGPT, the state-of-the-art chatbot, and find that\ndespite the fundamental simplicity of the task, the model's performance\nreflects an inability to retain correct environment states across time. Our\nfollow-up analyses suggest that performance degradation is largely because\nChatGPT has non-persistent in-context memory (although it can access the full\ndialogue history) and it is susceptible to hallucinated updates -- including\nupdates that artificially inflate accuracies. Our findings suggest overall that\nChatGPT is not currently equipped for robust tracking of situation states, and\nthat trust in the impressive dialogue performance of ChatGPT comes with risks.\nWe release the codebase for reproducing our test environment, as well as all\nprompts and API responses from ChatGPT, at\nhttps://github.com/yangalan123/SituationalTesting.", "published": "2023-10-24 19:22:01", "link": "http://arxiv.org/abs/2310.16135v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WojoodNER 2023: The First Arabic Named Entity Recognition Shared Task", "abstract": "We present WojoodNER-2023, the first Arabic Named Entity Recognition (NER)\nShared Task. The primary focus of WojoodNER-2023 is on Arabic NER, offering\nnovel NER datasets (i.e., Wojood) and the definition of subtasks designed to\nfacilitate meaningful comparisons between different NER approaches.\nWojoodNER-2023 encompassed two Subtasks: FlatNER and NestedNER. A total of 45\nunique teams registered for this shared task, with 11 of them actively\nparticipating in the test phase. Specifically, 11 teams participated in\nFlatNER, while $8$ teams tackled NestedNER. The winning teams achieved F1\nscores of 91.96 and 93.73 in FlatNER and NestedNER, respectively.", "published": "2023-10-24 19:50:07", "link": "http://arxiv.org/abs/2310.16153v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Background Summarization of Event Timelines", "abstract": "Generating concise summaries of news events is a challenging natural language\nprocessing task. While journalists often curate timelines to highlight key\nsub-events, newcomers to a news event face challenges in catching up on its\nhistorical context. In this paper, we address this need by introducing the task\nof background news summarization, which complements each timeline update with a\nbackground summary of relevant preceding events. We construct a dataset by\nmerging existing timeline datasets and asking human annotators to write a\nbackground summary for each timestep of each news event. We establish strong\nbaseline performance using state-of-the-art summarization systems and propose a\nquery-focused variant to generate background summaries. To evaluate background\nsummary quality, we present a question-answering-based evaluation metric,\nBackground Utility Score (BUS), which measures the percentage of questions\nabout a current event timestep that a background summary answers. Our\nexperiments show the effectiveness of instruction fine-tuned systems such as\nFlan-T5, in addition to strong zero-shot performance using GPT-3.5.", "published": "2023-10-24 21:30:15", "link": "http://arxiv.org/abs/2310.16197v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mixture-of-Linguistic-Experts Adapters for Improving and Interpreting\n  Pre-trained Language Models", "abstract": "In this work, we propose a method that combines two popular research areas by\ninjecting linguistic structures into pre-trained language models in the\nparameter-efficient fine-tuning (PEFT) setting. In our approach, parallel\nadapter modules encoding different linguistic structures are combined using a\nnovel Mixture-of-Linguistic-Experts architecture, where Gumbel-Softmax gates\nare used to determine the importance of these modules at each layer of the\nmodel. To reduce the number of parameters, we first train the model for a fixed\nsmall number of steps before pruning the experts based on their importance\nscores. Our experiment results with three different pre-trained models show\nthat our approach can outperform state-of-the-art PEFT methods with a\ncomparable number of parameters. In addition, we provide additional analysis to\nexamine the experts selected by each model at each layer to provide insights\nfor future studies.", "published": "2023-10-24 23:29:06", "link": "http://arxiv.org/abs/2310.16240v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GlotLID: Language Identification for Low-Resource Languages", "abstract": "Several recent papers have published good solutions for language\nidentification (LID) for about 300 high-resource and medium-resource languages.\nHowever, there is no LID available that (i) covers a wide range of low-resource\nlanguages, (ii) is rigorously evaluated and reliable and (iii) efficient and\neasy to use. Here, we publish GlotLID-M, an LID model that satisfies the\ndesiderata of wide coverage, reliability and efficiency. It identifies 1665\nlanguages, a large increase in coverage compared to prior work. In our\nexperiments, GlotLID-M outperforms four baselines (CLD3, FT176, OpenLID and\nNLLB) when balancing F1 and false positive rate (FPR). We analyze the unique\nchallenges that low-resource LID poses: incorrect corpus metadata, leakage from\nhigh-resource languages, difficulty separating closely related languages,\nhandling of macrolanguage vs varieties and in general noisy data. We hope that\nintegrating GlotLID-M into dataset creation pipelines will improve quality and\nenhance accessibility of NLP technology for low-resource languages and\ncultures. GlotLID-M model (including future versions), code, and list of data\nsources are available: https://github.com/cisnlp/GlotLID.", "published": "2023-10-24 23:45:57", "link": "http://arxiv.org/abs/2310.16248v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Large Language Models for Enhanced Product Descriptions in\n  eCommerce", "abstract": "In the dynamic field of eCommerce, the quality and comprehensiveness of\nproduct descriptions are pivotal for enhancing search visibility and customer\nengagement. Effective product descriptions can address the 'cold start'\nproblem, align with market trends, and ultimately lead to increased\nclick-through rates. Traditional methods for crafting these descriptions often\ninvolve significant human effort and may lack both consistency and scalability.\nThis paper introduces a novel methodology for automating product description\ngeneration using the LLAMA 2.0 7B language model. We train the model on a\ndataset of authentic product descriptions from Walmart, one of the largest\neCommerce platforms. The model is then fine-tuned for domain-specific language\nfeatures and eCommerce nuances to enhance its utility in sales and user\nengagement. We employ multiple evaluation metrics, including NDCG, customer\nclick-through rates, and human assessments, to validate the effectiveness of\nour approach. Our findings reveal that the system is not only scalable but also\nsignificantly reduces the human workload involved in creating product\ndescriptions. This study underscores the considerable potential of large\nlanguage models like LLAMA 2.0 7B in automating and optimizing various facets\nof eCommerce platforms, offering significant business impact, including\nimproved search functionality and increased sales.", "published": "2023-10-24 00:55:14", "link": "http://arxiv.org/abs/2310.18357v1", "categories": ["cs.CL", "I.2.7; H.3.3; H.3.5; K.4.4"], "primary_category": "cs.CL"}
{"title": "FANToM: A Benchmark for Stress-testing Machine Theory of Mind in\n  Interactions", "abstract": "Theory of mind (ToM) evaluations currently focus on testing models using\npassive narratives that inherently lack interactivity. We introduce FANToM, a\nnew benchmark designed to stress-test ToM within information-asymmetric\nconversational contexts via question answering. Our benchmark draws upon\nimportant theoretical requisites from psychology and necessary empirical\nconsiderations when evaluating large language models (LLMs). In particular, we\nformulate multiple types of questions that demand the same underlying reasoning\nto identify illusory or false sense of ToM capabilities in LLMs. We show that\nFANToM is challenging for state-of-the-art LLMs, which perform significantly\nworse than humans even with chain-of-thought reasoning or fine-tuning.", "published": "2023-10-24 00:24:11", "link": "http://arxiv.org/abs/2310.15421v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Beyond Sentiment: Leveraging Topic Metrics for Political Stance\n  Classification", "abstract": "Sentiment analysis, widely critiqued for capturing merely the overall tone of\na corpus, falls short in accurately reflecting the latent structures and\npolitical stances within texts. This study introduces topic metrics, dummy\nvariables converted from extracted topics, as both an alternative and\ncomplement to sentiment metrics in stance classification. By employing three\ndatasets identified by Bestvater and Monroe (2023), this study demonstrates\nBERTopic's proficiency in extracting coherent topics and the effectiveness of\ntopic metrics in stance classification. The experiment results show that\nBERTopic improves coherence scores by 17.07% to 54.20% when compared to\ntraditional approaches such as Dirichlet Allocation (LDA) and Non-negative\nMatrix Factorization (NMF), prevalent in earlier political science research.\nAdditionally, our results indicate topic metrics outperform sentiment metrics\nin stance classification, increasing performance by as much as 18.95%. Our\nfindings suggest topic metrics are especially effective for context-rich texts\nand corpus where stance and sentiment correlations are weak. The combination of\nsentiment and topic metrics achieve an optimal performance in most of the\nscenarios and can further address the limitations of relying solely on\nsentiment as well as the low coherence score of topic metrics.", "published": "2023-10-24 00:50:33", "link": "http://arxiv.org/abs/2310.15429v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "K-HATERS: A Hate Speech Detection Corpus in Korean with Target-Specific\n  Ratings", "abstract": "Numerous datasets have been proposed to combat the spread of online hate.\nDespite these efforts, a majority of these resources are English-centric,\nprimarily focusing on overt forms of hate. This research gap calls for\ndeveloping high-quality corpora in diverse languages that also encapsulate more\nsubtle hate expressions. This study introduces K-HATERS, a new corpus for hate\nspeech detection in Korean, comprising approximately 192K news comments with\ntarget-specific offensiveness ratings. This resource is the largest offensive\nlanguage corpus in Korean and is the first to offer target-specific ratings on\na three-point Likert scale, enabling the detection of hate expressions in\nKorean across varying degrees of offensiveness. We conduct experiments showing\nthe effectiveness of the proposed corpus, including a comparison with existing\ndatasets. Additionally, to address potential noise and bias in human\nannotations, we explore a novel idea of adopting the Cognitive Reflection Test,\nwhich is widely used in social science for assessing an individual's cognitive\nability, as a proxy of labeling quality. Findings indicate that annotations\nfrom individuals with the lowest test scores tend to yield detection models\nthat make biased predictions toward specific target groups and are less\naccurate. This study contributes to the NLP research on hate speech detection\nand resource construction. The code and dataset can be accessed at\nhttps://github.com/ssu-humane/K-HATERS.", "published": "2023-10-24 01:20:05", "link": "http://arxiv.org/abs/2310.15439v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Facilitating Self-Guided Mental Health Interventions Through\n  Human-Language Model Interaction: A Case Study of Cognitive Restructuring", "abstract": "Self-guided mental health interventions, such as \"do-it-yourself\" tools to\nlearn and practice coping strategies, show great promise to improve access to\nmental health care. However, these interventions are often cognitively\ndemanding and emotionally triggering, creating accessibility barriers that\nlimit their wide-scale implementation and adoption. In this paper, we study how\nhuman-language model interaction can support self-guided mental health\ninterventions. We take cognitive restructuring, an evidence-based therapeutic\ntechnique to overcome negative thinking, as a case study. In an IRB-approved\nrandomized field study on a large mental health website with 15,531\nparticipants, we design and evaluate a system that uses language models to\nsupport people through various steps of cognitive restructuring. Our findings\nreveal that our system positively impacts emotional intensity for 67% of\nparticipants and helps 65% overcome negative thoughts. Although adolescents\nreport relatively worse outcomes, we find that tailored interventions that\nsimplify language model generations improve overall effectiveness and equity.", "published": "2023-10-24 02:23:34", "link": "http://arxiv.org/abs/2310.15461v2", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Continual Event Extraction with Semantic Confusion Rectification", "abstract": "We study continual event extraction, which aims to extract incessantly\nemerging event information while avoiding forgetting. We observe that the\nsemantic confusion on event types stems from the annotations of the same text\nbeing updated over time. The imbalance between event types even aggravates this\nissue. This paper proposes a novel continual event extraction model with\nsemantic confusion rectification. We mark pseudo labels for each sentence to\nalleviate semantic confusion. We transfer pivotal knowledge between current and\nprevious models to enhance the understanding of event types. Moreover, we\nencourage the model to focus on the semantics of long-tailed event types by\nleveraging other associated types. Experimental results show that our model\noutperforms state-of-the-art baselines and is proficient in imbalanced\ndatasets.", "published": "2023-10-24 02:48:50", "link": "http://arxiv.org/abs/2310.15470v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "NuTrea: Neural Tree Search for Context-guided Multi-hop KGQA", "abstract": "Multi-hop Knowledge Graph Question Answering (KGQA) is a task that involves\nretrieving nodes from a knowledge graph (KG) to answer natural language\nquestions. Recent GNN-based approaches formulate this task as a KG path\nsearching problem, where messages are sequentially propagated from the seed\nnode towards the answer nodes. However, these messages are past-oriented, and\nthey do not consider the full KG context. To make matters worse, KG nodes often\nrepresent proper noun entities and are sometimes encrypted, being uninformative\nin selecting between paths. To address these problems, we propose Neural Tree\nSearch (NuTrea), a tree search-based GNN model that incorporates the broader KG\ncontext. Our model adopts a message-passing scheme that probes the unreached\nsubtree regions to boost the past-oriented embeddings. In addition, we\nintroduce the Relation Frequency-Inverse Entity Frequency (RF-IEF) node\nembedding that considers the global KG context to better characterize ambiguous\nKG nodes. The general effectiveness of our approach is demonstrated through\nexperiments on three major multi-hop KGQA benchmark datasets, and our extensive\nanalyses further validate its expressiveness and robustness. Overall, NuTrea\nprovides a powerful means to query the KG with complex natural language\nquestions. Code is available at https://github.com/mlvlab/NuTrea.", "published": "2023-10-24 03:24:15", "link": "http://arxiv.org/abs/2310.15484v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SteloCoder: a Decoder-Only LLM for Multi-Language to Python Code\n  Translation", "abstract": "With the recent focus on Large Language Models (LLMs), both StarCoder (Li et\nal., 2023) and Code Llama (Rozi\\`ere et al., 2023) have demonstrated remarkable\nperformance in code generation. However, there is still a need for improvement\nin code translation functionality with efficient training techniques. In\nresponse to this, we introduce SteloCoder, a decoder-only StarCoder-based LLM\ndesigned specifically for multi-programming language-to-Python code\ntranslation. In particular, SteloCoder achieves C++, C#, JavaScript, Java, or\nPHP-to-Python code translation without specifying the input programming\nlanguage. We modified StarCoder model architecture by incorporating a\nMixture-of-Experts (MoE) technique featuring five experts and a gating network\nfor multi-task handling. Experts are obtained by StarCoder fine-tuning.\nSpecifically, we use a Low-Rank Adaptive Method (LoRA) technique, limiting each\nexpert size as only 0.06% of number of StarCoder's parameters. At the same\ntime, to enhance training efficiency in terms of time, we adopt curriculum\nlearning strategy and use self-instruct data for efficient fine-tuning. As a\nresult, each expert takes only 6 hours to train on one single 80Gb A100 HBM.\nWith experiments on XLCoST datasets, SteloCoder achieves an average of 73.76\nCodeBLEU score in multi-programming language-to-Python translation, surpassing\nthe top performance from the leaderboard by at least 3.5. This accomplishment\nis attributed to only 45M extra parameters with StarCoder as the backbone and\n32 hours of valid training on one 80GB A100 HBM. The source code is release\nhere: https://github.com/sade-adrien/SteloCoder.", "published": "2023-10-24 06:04:28", "link": "http://arxiv.org/abs/2310.15539v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TCRA-LLM: Token Compression Retrieval Augmented Large Language Model for\n  Inference Cost Reduction", "abstract": "Since ChatGPT released its API for public use, the number of applications\nbuilt on top of commercial large language models (LLMs) increase exponentially.\nOne popular usage of such models is leveraging its in-context learning ability\nand generating responses given user queries leveraging knowledge obtained by\nretrieval augmentation. One problem of deploying commercial retrieval-augmented\nLLMs is the cost due to the additionally retrieved context that largely\nincreases the input token size of the LLMs. To mitigate this, we propose a\ntoken compression scheme that includes two methods: summarization compression\nand semantic compression. The first method applies a T5-based model that is\nfine-tuned by datasets generated using self-instruct containing samples with\nvarying lengths and reduce token size by doing summarization. The second method\nfurther compresses the token size by removing words with lower impact on the\nsemantic. In order to adequately evaluate the effectiveness of the proposed\nmethods, we propose and utilize a dataset called Food-Recommendation DB (FRDB)\nfocusing on food recommendation for women around pregnancy period or infants.\nOur summarization compression can reduce 65% of the retrieval token size with\nfurther 0.3% improvement on the accuracy; semantic compression provides a more\nflexible way to trade-off the token size with performance, for which we can\nreduce the token size by 20% with only 1.6% of accuracy drop.", "published": "2023-10-24 06:56:38", "link": "http://arxiv.org/abs/2310.15556v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "CONTRASTE: Supervised Contrastive Pre-training With Aspect-based Prompts\n  For Aspect Sentiment Triplet Extraction", "abstract": "Existing works on Aspect Sentiment Triplet Extraction (ASTE) explicitly focus\non developing more efficient fine-tuning techniques for the task. Instead, our\nmotivation is to come up with a generic approach that can improve the\ndownstream performances of multiple ABSA tasks simultaneously. Towards this, we\npresent CONTRASTE, a novel pre-training strategy using CONTRastive learning to\nenhance the ASTE performance. While we primarily focus on ASTE, we also\ndemonstrate the advantage of our proposed technique on other ABSA tasks such as\nACOS, TASD, and AESC. Given a sentence and its associated (aspect, opinion,\nsentiment) triplets, first, we design aspect-based prompts with corresponding\nsentiments masked. We then (pre)train an encoder-decoder model by applying\ncontrastive learning on the decoder-generated aspect-aware sentiment\nrepresentations of the masked terms. For fine-tuning the model weights thus\nobtained, we then propose a novel multi-task approach where the base\nencoder-decoder model is combined with two complementary modules, a\ntagging-based Opinion Term Detector, and a regression-based Triplet Count\nEstimator. Exhaustive experiments on four benchmark datasets and a detailed\nablation study establish the importance of each of our proposed components as\nwe achieve new state-of-the-art ASTE results.", "published": "2023-10-24 07:40:09", "link": "http://arxiv.org/abs/2310.15577v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Retrieval-based Knowledge Transfer: An Effective Approach for Extreme\n  Large Language Model Compression", "abstract": "Large-scale pre-trained language models (LLMs) have demonstrated exceptional\nperformance in various natural language processing (NLP) tasks. However, the\nmassive size of these models poses huge challenges for their deployment in\nreal-world applications. While numerous model compression techniques have been\nproposed, most of them are not well-suited for achieving extreme model\ncompression when there is a significant gap in model scale. In this paper, we\nintroduce a novel compression paradigm called Retrieval-based Knowledge\nTransfer (RetriKT), which effectively transfers the knowledge of LLMs to\nextremely small-scale models (e.g., 1%). In particular, our approach extracts\nknowledge from LLMs to construct a knowledge store, from which the small-scale\nmodel can retrieve relevant information and leverage it for effective\ninference. To improve the quality of the model, soft prompt tuning and Proximal\nPolicy Optimization (PPO) reinforcement learning techniques are employed.\nExtensive experiments are conducted on low-resource tasks from SuperGLUE and\nGLUE benchmarks. The results demonstrate that the proposed approach\nsignificantly enhances the performance of small-scale models by leveraging the\nknowledge from LLMs.", "published": "2023-10-24 07:58:20", "link": "http://arxiv.org/abs/2310.15594v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Career Path Prediction using Resume Representation Learning and\n  Skill-based Matching", "abstract": "The impact of person-job fit on job satisfaction and performance is widely\nacknowledged, which highlights the importance of providing workers with next\nsteps at the right time in their career. This task of predicting the next step\nin a career is known as career path prediction, and has diverse applications\nsuch as turnover prevention and internal job mobility. Existing methods to\ncareer path prediction rely on large amounts of private career history data to\nmodel the interactions between job titles and companies. We propose leveraging\nthe unexplored textual descriptions that are part of work experience sections\nin resumes. We introduce a structured dataset of 2,164 anonymized career\nhistories, annotated with ESCO occupation labels. Based on this dataset, we\npresent a novel representation learning approach, CareerBERT, specifically\ndesigned for work history data. We develop a skill-based model and a text-based\nmodel for career path prediction, which achieve 35.24% and 39.61% recall@10\nrespectively on our dataset. Finally, we show that both approaches are\ncomplementary as a hybrid approach achieves the strongest result with 43.01%\nrecall@10.", "published": "2023-10-24 08:56:06", "link": "http://arxiv.org/abs/2310.15636v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Biomedical Abstractive Summarisation with Knowledge\n  Aggregation from Citation Papers", "abstract": "Abstracts derived from biomedical literature possess distinct domain-specific\ncharacteristics, including specialised writing styles and biomedical\nterminologies, which necessitate a deep understanding of the related\nliterature. As a result, existing language models struggle to generate\ntechnical summaries that are on par with those produced by biomedical experts,\ngiven the absence of domain-specific background knowledge. This paper aims to\nenhance the performance of language models in biomedical abstractive\nsummarisation by aggregating knowledge from external papers cited within the\nsource article. We propose a novel attention-based citation aggregation model\nthat integrates domain-specific knowledge from citation papers, allowing neural\nnetworks to generate summaries by leveraging both the paper content and\nrelevant knowledge from citation papers. Furthermore, we construct and release\na large-scale biomedical summarisation dataset that serves as a foundation for\nour research. Extensive experiments demonstrate that our model outperforms\nstate-of-the-art approaches and achieves substantial improvements in\nabstractive biomedical text summarisation.", "published": "2023-10-24 09:56:46", "link": "http://arxiv.org/abs/2310.15684v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Automated Recipe Genre Classification using Semi-Supervised\n  Learning", "abstract": "Sharing cooking recipes is a great way to exchange culinary ideas and provide\ninstructions for food preparation. However, categorizing raw recipes found\nonline into appropriate food genres can be challenging due to a lack of\nadequate labeled data. In this study, we present a dataset named the\n``Assorted, Archetypal, and Annotated Two Million Extended (3A2M+) Cooking\nRecipe Dataset\" that contains two million culinary recipes labeled in\nrespective categories with extended named entities extracted from recipe\ndescriptions. This collection of data includes various features such as title,\nNER, directions, and extended NER, as well as nine different labels\nrepresenting genres including bakery, drinks, non-veg, vegetables, fast food,\ncereals, meals, sides, and fusions. The proposed pipeline named 3A2M+ extends\nthe size of the Named Entity Recognition (NER) list to address missing named\nentities like heat, time or process from the recipe directions using two NER\nextraction tools. 3A2M+ dataset provides a comprehensive solution to the\nvarious challenging recipe-related tasks, including classification, named\nentity recognition, and recipe generation. Furthermore, we have demonstrated\ntraditional machine learning, deep learning and pre-trained language models to\nclassify the recipes into their corresponding genre and achieved an overall\naccuracy of 98.6\\%. Our investigation indicates that the title feature played a\nmore significant role in classifying the genre.", "published": "2023-10-24 10:03:27", "link": "http://arxiv.org/abs/2310.15693v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "COPR: Continual Learning Human Preference through Optimal Policy\n  Regularization", "abstract": "The technique of Reinforcement Learning from Human Feedback (RLHF) is a\ncommonly employed method to improve pre-trained Language Models (LM), enhancing\ntheir ability to conform to human preferences. Nevertheless, the current\nRLHF-based LMs necessitate full retraining each time novel queries or feedback\nare introduced, which becomes a challenging task because human preferences can\nvary between different domains or tasks. Retraining LMs poses practical\ndifficulties in many real-world situations due to the significant time and\ncomputational resources required, along with concerns related to data privacy.\nTo address this limitation, we propose a new method called Continual Optimal\nPolicy Regularization (COPR), in which we compute the distribution of optimal\npolicy bypassing the partition function and then regularize the current policy\nbased on the historically optimal distribution to mitigate Catastrophic\nForgetting (CF). COPR involves a single learning phase and doesn't necessitate\ncomplex reinforcement learning. Importantly, it shares the capability with RLHF\nto learn from unlabeled data by maintaining a scoring module, similar to reward\nmodel, making it flexible for continually learning without human feedback. Our\nexperimental results show that COPR outperforms strong Continuous Learning (CL)\nbaselines when it comes to consistently aligning with human preferences on\nincremental tasks and domains.", "published": "2023-10-24 10:05:32", "link": "http://arxiv.org/abs/2310.15694v5", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Ensemble of Task-Specific Language Models for Brain Encoding", "abstract": "Language models have been shown to be rich enough to encode fMRI activations\nof certain Regions of Interest in our Brains. Previous works have explored\ntransfer learning from representations learned for popular natural language\nprocessing tasks for predicting brain responses. In our work, we improve the\nperformance of such encoders by creating an ensemble model out of 10 popular\nLanguage Models (2 syntactic and 8 semantic). We beat the current baselines by\n10% on average across all ROIs through our ensembling methods.", "published": "2023-10-24 10:52:41", "link": "http://arxiv.org/abs/2310.15720v2", "categories": ["cs.CL", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Integrating Language Models into Direct Speech Translation: An\n  Inference-Time Solution to Control Gender Inflection", "abstract": "When translating words referring to the speaker, speech translation (ST)\nsystems should not resort to default masculine generics nor rely on potentially\nmisleading vocal traits. Rather, they should assign gender according to the\nspeakers' preference. The existing solutions to do so, though effective, are\nhardly feasible in practice as they involve dedicated model re-training on\ngender-labeled ST data. To overcome these limitations, we propose the first\ninference-time solution to control speaker-related gender inflections in ST.\nOur approach partially replaces the (biased) internal language model (LM)\nimplicitly learned by the ST decoder with gender-specific external LMs.\nExperiments on en->es/fr/it show that our solution outperforms the base models\nand the best training-time mitigation strategy by up to 31.0 and 1.6 points in\ngender accuracy, respectively, for feminine forms. The gains are even larger\n(up to 32.0 and 3.4) in the challenging condition where speakers' vocal traits\nconflict with their gender.", "published": "2023-10-24 11:55:16", "link": "http://arxiv.org/abs/2310.15752v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MindLLM: Pre-training Lightweight Large Language Model from Scratch,\n  Evaluations and Domain Applications", "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious natural language tasks, marking significant strides towards general\nartificial intelligence. While general artificial intelligence is leveraged by\ndeveloping increasingly large-scale models, there could be another branch to\ndevelop lightweight custom models that better serve certain domains, taking\ninto account the high cost of training and deploying LLMs and the scarcity of\nresources. In this paper, we present MindLLM, a novel series of bilingual\nlightweight large language models, trained from scratch, alleviating such\nburdens by offering models with 1.3 billion and 3 billion parameters. A\nthorough account of experiences accrued during large model development is\ngiven, covering every step of the process, including data construction, model\narchitecture, evaluation, and applications. Such insights are hopefully\nvaluable for fellow academics and developers. MindLLM consistently matches or\nsurpasses the performance of other open-source larger models on some public\nbenchmarks. We also introduce an innovative instruction tuning framework\ntailored for smaller models to enhance their capabilities efficiently.\nMoreover, we explore the application of MindLLM in specific vertical domains\nsuch as law and finance, underscoring the agility and adaptability of our\nlightweight models.", "published": "2023-10-24 12:22:34", "link": "http://arxiv.org/abs/2310.15777v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DALE: Generative Data Augmentation for Low-Resource Legal NLP", "abstract": "We present DALE, a novel and effective generative Data Augmentation framework\nfor low-resource LEgal NLP. DALE addresses the challenges existing frameworks\npose in generating effective data augmentations of legal documents - legal\nlanguage, with its specialized vocabulary and complex semantics, morphology,\nand syntax, does not benefit from data augmentations that merely rephrase the\nsource sentence. To address this, DALE, built on an Encoder-Decoder Language\nModel, is pre-trained on a novel unsupervised text denoising objective based on\nselective masking - our masking strategy exploits the domain-specific language\ncharacteristics of templatized legal documents to mask collocated spans of\ntext. Denoising these spans helps DALE acquire knowledge about legal concepts,\nprinciples, and language usage. Consequently, it develops the ability to\ngenerate coherent and diverse augmentations with novel contexts. Finally, DALE\nperforms conditional generation to generate synthetic augmentations for\nlow-resource Legal NLP tasks. We demonstrate the effectiveness of DALE on 13\ndatasets spanning 6 tasks and 4 low-resource settings. DALE outperforms all our\nbaselines, including LLMs, qualitatively and quantitatively, with improvements\nof 1%-50%.", "published": "2023-10-24 12:50:28", "link": "http://arxiv.org/abs/2310.15799v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Generative Language Models Exhibit Social Identity Biases", "abstract": "The surge in popularity of large language models has given rise to concerns\nabout biases that these models could learn from humans. We investigate whether\ningroup solidarity and outgroup hostility, fundamental social identity biases\nknown from social psychology, are present in 56 large language models. We find\nthat almost all foundational language models and some instruction fine-tuned\nmodels exhibit clear ingroup-positive and outgroup-negative associations when\nprompted to complete sentences (e.g., \"We are...\"). Our findings suggest that\nmodern language models exhibit fundamental social identity biases to a similar\ndegree as humans, both in the lab and in real-world conversations with LLMs,\nand that curating training data and instruction fine-tuning can mitigate such\nbiases. Our results have practical implications for creating less biased\nlarge-language models and further underscore the need for more research into\nuser interactions with LLMs to prevent potential bias reinforcement in humans.", "published": "2023-10-24 13:17:40", "link": "http://arxiv.org/abs/2310.15819v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Rosetta Stone at KSAA-RD Shared Task: A Hop From Language Modeling To\n  Word--Definition Alignment", "abstract": "A Reverse Dictionary is a tool enabling users to discover a word based on its\nprovided definition, meaning, or description. Such a technique proves valuable\nin various scenarios, aiding language learners who possess a description of a\nword without its identity, and benefiting writers seeking precise terminology.\nThese scenarios often encapsulate what is referred to as the\n\"Tip-of-the-Tongue\" (TOT) phenomena. In this work, we present our winning\nsolution for the Arabic Reverse Dictionary shared task. This task focuses on\nderiving a vector representation of an Arabic word from its accompanying\ndescription. The shared task encompasses two distinct subtasks: the first\ninvolves an Arabic definition as input, while the second employs an English\ndefinition. For the first subtask, our approach relies on an ensemble of\nfinetuned Arabic BERT-based models, predicting the word embedding for a given\ndefinition. The final representation is obtained through averaging the output\nembeddings from each model within the ensemble. In contrast, the most effective\nsolution for the second subtask involves translating the English test\ndefinitions into Arabic and applying them to the finetuned models originally\ntrained for the first subtask. This straightforward method achieves the highest\nscore across both subtasks.", "published": "2023-10-24 13:23:57", "link": "http://arxiv.org/abs/2310.15823v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Using Artificial French Data to Understand the Emergence of Gender Bias\n  in Transformer Language Models", "abstract": "Numerous studies have demonstrated the ability of neural language models to\nlearn various linguistic properties without direct supervision. This work takes\nan initial step towards exploring the less researched topic of how neural\nmodels discover linguistic properties of words, such as gender, as well as the\nrules governing their usage. We propose to use an artificial corpus generated\nby a PCFG based on French to precisely control the gender distribution in the\ntraining data and determine under which conditions a model correctly captures\ngender information or, on the contrary, appears gender-biased.", "published": "2023-10-24 14:08:37", "link": "http://arxiv.org/abs/2310.15852v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "BianQue: Balancing the Questioning and Suggestion Ability of Health LLMs\n  with Multi-turn Health Conversations Polished by ChatGPT", "abstract": "Large language models (LLMs) have performed well in providing general and\nextensive health suggestions in single-turn conversations, exemplified by\nsystems such as ChatGPT, ChatGLM, ChatDoctor, DoctorGLM, and etc. However, the\nlimited information provided by users during single turn results in inadequate\npersonalization and targeting of the generated suggestions, which requires\nusers to independently select the useful part. It is mainly caused by the\nmissing ability to engage in multi-turn questioning. In real-world medical\nconsultations, doctors usually employ a series of iterative inquiries to\ncomprehend the patient's condition thoroughly, enabling them to provide\neffective and personalized suggestions subsequently, which can be defined as\nchain of questioning (CoQ) for LLMs. To improve the CoQ of LLMs, we propose\nBianQue, a ChatGLM-based LLM finetuned with the self-constructed health\nconversation dataset BianQueCorpus that is consist of multiple turns of\nquestioning and health suggestions polished by ChatGPT. Experimental results\ndemonstrate that the proposed BianQue can simultaneously balance the\ncapabilities of both questioning and health suggestions, which will help\npromote the research and application of LLMs in the field of proactive health.", "published": "2023-10-24 14:57:34", "link": "http://arxiv.org/abs/2310.15896v2", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Do Stochastic Parrots have Feelings Too? Improving Neural Detection of\n  Synthetic Text via Emotion Recognition", "abstract": "Recent developments in generative AI have shone a spotlight on\nhigh-performance synthetic text generation technologies. The now wide\navailability and ease of use of such models highlights the urgent need to\nprovide equally powerful technologies capable of identifying synthetic text.\nWith this in mind, we draw inspiration from psychological studies which suggest\nthat people can be driven by emotion and encode emotion in the text they\ncompose. We hypothesize that pretrained language models (PLMs) have an\naffective deficit because they lack such an emotional driver when generating\ntext and consequently may generate synthetic text which has affective\nincoherence i.e. lacking the kind of emotional coherence present in\nhuman-authored text. We subsequently develop an emotionally aware detector by\nfine-tuning a PLM on emotion. Experiment results indicate that our\nemotionally-aware detector achieves improvements across a range of synthetic\ntext generators, various sized models, datasets, and domains. Finally, we\ncompare our emotionally-aware synthetic text detector to ChatGPT in the task of\nidentification of its own output and show substantial gains, reinforcing the\npotential of emotion as a signal to identify synthetic text. Code, models, and\ndatasets are available at https: //github.com/alanagiasi/emoPLMsynth", "published": "2023-10-24 15:07:35", "link": "http://arxiv.org/abs/2310.15904v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Characterizing Mechanisms for Factual Recall in Language Models", "abstract": "Language Models (LMs) often must integrate facts they memorized in\npretraining with new information that appears in a given context. These two\nsources can disagree, causing competition within the model, and it is unclear\nhow an LM will resolve the conflict. On a dataset that queries for knowledge of\nworld capitals, we investigate both distributional and mechanistic determinants\nof LM behavior in such situations. Specifically, we measure the proportion of\nthe time an LM will use a counterfactual prefix (e.g., \"The capital of Poland\nis London\") to overwrite what it learned in pretraining (\"Warsaw\"). On Pythia\nand GPT2, the training frequency of both the query country (\"Poland\") and the\nin-context city (\"London\") highly affect the models' likelihood of using the\ncounterfactual. We then use head attribution to identify individual attention\nheads that either promote the memorized answer or the in-context answer in the\nlogits. By scaling up or down the value vector of these heads, we can control\nthe likelihood of using the in-context answer on new data. This method can\nincrease the rate of generating the in-context answer to 88\\% of the time\nsimply by scaling a single head at runtime. Our work contributes to a body of\nevidence showing that we can often localize model behaviors to specific\ncomponents and provides a proof of concept for how future methods might control\nmodel behavior dynamically at runtime.", "published": "2023-10-24 15:15:18", "link": "http://arxiv.org/abs/2310.15910v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mixture of Tokens: Continuous MoE through Cross-Example Aggregation", "abstract": "Mixture of Experts (MoE) models based on Transformer architecture are pushing\nthe boundaries of language and vision tasks. The allure of these models lies in\ntheir ability to substantially increase the parameter count without a\ncorresponding increase in FLOPs. Most widely adopted MoE models are\ndiscontinuous with respect to their parameters - often referred to as sparse.\nAt the same time, existing continuous MoE designs either lag behind their\nsparse counterparts or are incompatible with autoregressive decoding. Motivated\nby the observation that the adaptation of fully continuous methods has been an\noverarching trend in deep learning, we develop Mixture of Tokens (MoT), a\nsimple, continuous architecture that is capable of scaling the number of\nparameters similarly to sparse MoE models. Unlike conventional methods, MoT\nassigns mixtures of tokens from different examples to each expert. This\narchitecture is fully compatible with autoregressive training and generation.\nOur best models not only achieve a 3x increase in training speed over dense\nTransformer models in language pretraining but also match the performance of\nstate-of-the-art MoE architectures. Additionally, a close connection between\nMoT and MoE is demonstrated through a novel technique we call transition\ntuning.", "published": "2023-10-24 16:03:57", "link": "http://arxiv.org/abs/2310.15961v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Dissecting In-Context Learning of Translations in GPTs", "abstract": "Most of the recent work in leveraging Large Language Models (LLMs) such as\nGPT-3 for Machine Translation (MT) has focused on selecting the few-shot\nsamples for prompting. In this work, we try to better understand the role of\ndemonstration attributes for the in-context learning of translations through\nperturbations of high-quality, in-domain demonstrations. We find that\nasymmetric perturbation of the source-target mappings yield vastly different\nresults. We show that the perturbation of the source side has surprisingly\nlittle impact, while target perturbation can drastically reduce translation\nquality, suggesting that it is the output text distribution that provides the\nmost important learning signal during in-context learning of translations. We\npropose a method named Zero-Shot-Context to add this signal automatically in\nZero-Shot prompting. We demonstrate that it improves upon the zero-shot\ntranslation performance of GPT-3, even making it competitive with few-shot\nprompted translations.", "published": "2023-10-24 16:37:18", "link": "http://arxiv.org/abs/2310.15987v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Perceiving Small Visual Details in Zero-shot Visual Question\n  Answering with Multimodal LLMs", "abstract": "Multimodal Large Language Models (MLLMs) have recently achieved promising\nzero-shot accuracy on visual question answering (VQA) -- a fundamental task\naffecting various downstream applications and domains. Given the great\npotential for the broad use of these models, it is important to investigate\ntheir limitations in dealing with different image and question properties. In\nthis work, we investigate whether MLLMs can perceive small details as well as\nlarge details in images. In particular, we show that their zero-shot accuracy\nin answering visual questions is very sensitive to the size of the visual\nsubject of the question, declining up to 46% with size. Furthermore, we show\nthat this effect is causal by observing that human visual cropping can\nsignificantly mitigate their sensitivity to size. Inspired by the usefulness of\nhuman cropping, we then propose five automatic visual cropping methods --\nleveraging either external localization models or the decision process of the\ngiven MLLM itself -- as inference time mechanisms to improve the zero-shot\nperformance of MLLMs. We study their effectiveness on four popular VQA\ndatasets, and a subset of the VQAv2 dataset tailored towards fine visual\ndetails. Our findings suggest that MLLMs should be used with caution in\ndetail-sensitive VQA applications, and that visual cropping is a promising\ndirection to improve their zero-shot performance. To facilitate further\ninvestigation of MLLMs' behaviors, our code and data are publicly released.", "published": "2023-10-24 17:48:04", "link": "http://arxiv.org/abs/2310.16033v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Instruct and Extract: Instruction Tuning for On-Demand Information\n  Extraction", "abstract": "Large language models with instruction-following capabilities open the door\nto a wider group of users. However, when it comes to information extraction - a\nclassic task in natural language processing - most task-specific systems cannot\nalign well with long-tail ad hoc extraction use cases for non-expert users. To\naddress this, we propose a novel paradigm, termed On-Demand Information\nExtraction, to fulfill the personalized demands of real-world users. Our task\naims to follow the instructions to extract the desired content from the\nassociated text and present it in a structured tabular format. The table\nheaders can either be user-specified or inferred contextually by the model. To\nfacilitate research in this emerging area, we present a benchmark named\nInstructIE, inclusive of both automatically generated training data, as well as\nthe human-annotated test set. Building on InstructIE, we further develop an\nOn-Demand Information Extractor, ODIE. Comprehensive evaluations on our\nbenchmark reveal that ODIE substantially outperforms the existing open-source\nmodels of similar size. Our code and dataset are released on\nhttps://github.com/yzjiao/On-Demand-IE.", "published": "2023-10-24 17:54:25", "link": "http://arxiv.org/abs/2310.16040v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "WebWISE: Web Interface Control and Sequential Exploration with Large\n  Language Models", "abstract": "The paper investigates using a Large Language Model (LLM) to automatically\nperform web software tasks using click, scroll, and text input operations.\nPrevious approaches, such as reinforcement learning (RL) or imitation learning,\nare inefficient to train and task-specific. Our method uses filtered Document\nObject Model (DOM) elements as observations and performs tasks step-by-step,\nsequentially generating small programs based on the current observations. We\nuse in-context learning, either benefiting from a single manually provided\nexample, or an automatically generated example based on a successful zero-shot\ntrial. We evaluate the proposed method on the MiniWob++ benchmark. With only\none in-context example, our WebWISE method achieves similar or better\nperformance than other methods that require many demonstrations or trials.", "published": "2023-10-24 17:57:03", "link": "http://arxiv.org/abs/2310.16042v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CR-COPEC: Causal Rationale of Corporate Performance Changes to Learn\n  from Financial Reports", "abstract": "In this paper, we introduce CR-COPEC called Causal Rationale of Corporate\nPerformance Changes from financial reports. This is a comprehensive large-scale\ndomain-adaptation causal sentence dataset to detect financial performance\nchanges of corporate. CR-COPEC contributes to two major achievements. First, it\ndetects causal rationale from 10-K annual reports of the U.S. companies, which\ncontain experts' causal analysis following accounting standards in a formal\nmanner. This dataset can be widely used by both individual investors and\nanalysts as material information resources for investing and decision making\nwithout tremendous effort to read through all the documents. Second, it\ncarefully considers different characteristics which affect the financial\nperformance of companies in twelve industries. As a result, CR-COPEC can\ndistinguish causal sentences in various industries by taking unique narratives\nin each industry into consideration. We also provide an extensive analysis of\nhow well CR-COPEC dataset is constructed and suited for classifying target\nsentences as causal ones with respect to industry characteristics. Our dataset\nand experimental codes are publicly available.", "published": "2023-10-24 18:00:40", "link": "http://arxiv.org/abs/2310.16095v1", "categories": ["cs.CL", "cs.CE"], "primary_category": "cs.CL"}
{"title": "PreWoMe: Exploiting Presuppositions as Working Memory for Long Form\n  Question Answering", "abstract": "Information-seeking questions in long-form question answering (LFQA) often\nprove misleading due to ambiguity or false presupposition in the question.\nWhile many existing approaches handle misleading questions, they are tailored\nto limited questions, which are insufficient in a real-world setting with\nunpredictable input characteristics. In this work, we propose PreWoMe, a\nunified approach capable of handling any type of information-seeking question.\nThe key idea of PreWoMe involves extracting presuppositions in the question and\nexploiting them as working memory to generate feedback and action about the\nquestion. Our experiment shows that PreWoMe is effective not only in tackling\nmisleading questions but also in handling normal ones, thereby demonstrating\nthe effectiveness of leveraging presuppositions, feedback, and action for\nreal-world QA settings.", "published": "2023-10-24 19:47:26", "link": "http://arxiv.org/abs/2310.16147v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Correction with Backtracking Reduces Hallucination in Summarization", "abstract": "Abstractive summarization aims at generating natural language summaries of a\nsource document that are succinct while preserving the important elements.\nDespite recent advances, neural text summarization models are known to be\nsusceptible to hallucinating (or more correctly confabulating), that is to\nproduce summaries with details that are not grounded in the source document. In\nthis paper, we introduce a simple yet efficient technique, CoBa, to reduce\nhallucination in abstractive summarization. The approach is based on two steps:\nhallucination detection and mitigation. We show that the former can be achieved\nthrough measuring simple statistics about conditional word probabilities and\ndistance to context words. Further, we demonstrate that straight-forward\nbacktracking is surprisingly effective at mitigation. We thoroughly evaluate\nthe proposed method with prior art on three benchmark datasets for text\nsummarization. The results show that CoBa is effective and efficient in\nreducing hallucination, and offers great adaptability and flexibility. Code can\nbe found at https://github.com/zhenzhel/CoBa.", "published": "2023-10-24 20:48:11", "link": "http://arxiv.org/abs/2310.16176v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "BLP-2023 Task 2: Sentiment Analysis", "abstract": "We present an overview of the BLP Sentiment Shared Task, organized as part of\nthe inaugural BLP 2023 workshop, co-located with EMNLP 2023. The task is\ndefined as the detection of sentiment in a given piece of social media text.\nThis task attracted interest from 71 participants, among whom 29 and 30 teams\nsubmitted systems during the development and evaluation phases, respectively.\nIn total, participants submitted 597 runs. However, a total of 15 teams\nsubmitted system description papers. The range of approaches in the submitted\nsystems spans from classical machine learning models, fine-tuning pre-trained\nmodels, to leveraging Large Language Model (LLMs) in zero- and few-shot\nsettings. In this paper, we provide a detailed account of the task setup,\nincluding dataset development and evaluation setup. Additionally, we provide a\nbrief overview of the systems submitted by the participants. All datasets and\nevaluation scripts from the shared task have been made publicly available for\nthe research community, to foster further research in this domain.", "published": "2023-10-24 21:00:41", "link": "http://arxiv.org/abs/2310.16183v2", "categories": ["cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Length is a Curse and a Blessing for Document-level Semantics", "abstract": "In recent years, contrastive learning (CL) has been extensively utilized to\nrecover sentence and document-level encoding capability from pre-trained\nlanguage models. In this work, we question the length generalizability of\nCL-based models, i.e., their vulnerability towards length-induced semantic\nshift. We verify not only that length vulnerability is a significant yet\noverlooked research gap, but we can devise unsupervised CL methods solely\ndepending on the semantic signal provided by document length. We first derive\nthe theoretical foundations underlying length attacks, showing that elongating\na document would intensify the high intra-document similarity that is already\nbrought by CL. Moreover, we found that isotropy promised by CL is highly\ndependent on the length range of text exposed in training. Inspired by these\nfindings, we introduce a simple yet universal document representation learning\nframework, LA(SER)$^{3}$: length-agnostic self-reference for semantically\nrobust sentence representation learning, achieving state-of-the-art\nunsupervised performance on the standard information retrieval benchmark.", "published": "2023-10-24 21:23:53", "link": "http://arxiv.org/abs/2310.16193v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Knowledge Editing for Large Language Models: A Survey", "abstract": "Large language models (LLMs) have recently transformed both the academic and\nindustrial landscapes due to their remarkable capacity to understand, analyze,\nand generate texts based on their vast knowledge and reasoning ability.\nNevertheless, one major drawback of LLMs is their substantial computational\ncost for pre-training due to their unprecedented amounts of parameters. The\ndisadvantage is exacerbated when new knowledge frequently needs to be\nintroduced into the pre-trained model. Therefore, it is imperative to develop\neffective and efficient techniques to update pre-trained LLMs. Traditional\nmethods encode new knowledge in pre-trained LLMs through direct fine-tuning.\nHowever, naively re-training LLMs can be computationally intensive and risks\ndegenerating valuable pre-trained knowledge irrelevant to the update in the\nmodel. Recently, Knowledge-based Model Editing (KME) has attracted increasing\nattention, which aims to precisely modify the LLMs to incorporate specific\nknowledge, without negatively influencing other irrelevant knowledge. In this\nsurvey, we aim to provide a comprehensive and in-depth overview of recent\nadvances in the field of KME. We first introduce a general formulation of KME\nto encompass different KME strategies. Afterward, we provide an innovative\ntaxonomy of KME techniques based on how the new knowledge is introduced into\npre-trained LLMs, and investigate existing KME strategies while analyzing key\ninsights, advantages, and limitations of methods from each category. Moreover,\nrepresentative metrics, datasets, and applications of KME are introduced\naccordingly. Finally, we provide an in-depth analysis regarding the\npracticality and remaining challenges of KME and suggest promising research\ndirections for further advancement in this field.", "published": "2023-10-24 22:18:13", "link": "http://arxiv.org/abs/2310.16218v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ZzzGPT: An Interactive GPT Approach to Enhance Sleep Quality", "abstract": "This paper explores the intersection of technology and sleep pattern\ncomprehension, presenting a cutting-edge two-stage framework that harnesses the\npower of Large Language Models (LLMs). The primary objective is to deliver\nprecise sleep predictions paired with actionable feedback, addressing the\nlimitations of existing solutions. This innovative approach involves leveraging\nthe GLOBEM dataset alongside synthetic data generated by LLMs. The results\nhighlight significant improvements, underlining the efficacy of merging\nadvanced machine-learning techniques with a user-centric design ethos. Through\nthis exploration, we bridge the gap between technological sophistication and\nuser-friendly design, ensuring that our framework yields accurate predictions\nand translates them into actionable insights.", "published": "2023-10-24 23:30:17", "link": "http://arxiv.org/abs/2310.16242v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Speakerly: A Voice-based Writing Assistant for Text Composition", "abstract": "We present Speakerly, a new real-time voice-based writing assistance system\nthat helps users with text composition across various use cases such as emails,\ninstant messages, and notes. The user can interact with the system through\ninstructions or dictation, and the system generates a well-formatted and\ncoherent document. We describe the system architecture and detail how we\naddress the various challenges while building and deploying such a system at\nscale. More specifically, our system uses a combination of small, task-specific\nmodels as well as pre-trained language models for fast and effective text\ncomposition while supporting a variety of input modes for better usability.", "published": "2023-10-24 23:53:15", "link": "http://arxiv.org/abs/2310.16251v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Communication Theory Perspective on Prompting Engineering Methods for\n  Large Language Models", "abstract": "The springing up of Large Language Models (LLMs) has shifted the community\nfrom single-task-orientated natural language processing (NLP) research to a\nholistic end-to-end multi-task learning paradigm. Along this line of research\nendeavors in the area, LLM-based prompting methods have attracted much\nattention, partially due to the technological advantages brought by prompt\nengineering (PE) as well as the underlying NLP principles disclosed by various\nprompting methods. Traditional supervised learning usually requires training a\nmodel based on labeled data and then making predictions. In contrast, PE\nmethods directly use the powerful capabilities of existing LLMs (i.e., GPT-3\nand GPT-4) via composing appropriate prompts, especially under few-shot or\nzero-shot scenarios. Facing the abundance of studies related to the prompting\nand the ever-evolving nature of this field, this article aims to (i) illustrate\na novel perspective to review existing PE methods, within the well-established\ncommunication theory framework; (ii) facilitate a better/deeper understanding\nof developing trends of existing PE methods used in four typical tasks; (iii)\nshed light on promising research directions for future PE methods.", "published": "2023-10-24 03:05:21", "link": "http://arxiv.org/abs/2310.18358v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DeSIQ: Towards an Unbiased, Challenging Benchmark for Social\n  Intelligence Understanding", "abstract": "Social intelligence is essential for understanding and reasoning about human\nexpressions, intents and interactions. One representative benchmark for its\nstudy is Social Intelligence Queries (Social-IQ), a dataset of multiple-choice\nquestions on videos of complex social interactions. We define a comprehensive\nmethodology to study the soundness of Social-IQ, as the soundness of such\nbenchmark datasets is crucial to the investigation of the underlying research\nproblem. Our analysis reveals that Social-IQ contains substantial biases, which\ncan be exploited by a moderately strong language model to learn spurious\ncorrelations to achieve perfect performance without being given the context or\neven the question. We introduce DeSIQ, a new challenging dataset, constructed\nby applying simple perturbations to Social-IQ. Our empirical analysis shows\nDeSIQ significantly reduces the biases in the original Social-IQ dataset.\nFurthermore, we examine and shed light on the effect of model size, model\nstyle, learning settings, commonsense knowledge, and multi-modality on the new\nbenchmark performance. Our new dataset, observations and findings open up\nimportant research questions for the study of social intelligence.", "published": "2023-10-24 06:21:34", "link": "http://arxiv.org/abs/2310.18359v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Guiding LLM to Fool Itself: Automatically Manipulating Machine Reading\n  Comprehension Shortcut Triggers", "abstract": "Recent applications of LLMs in Machine Reading Comprehension (MRC) systems\nhave shown impressive results, but the use of shortcuts, mechanisms triggered\nby features spuriously correlated to the true label, has emerged as a potential\nthreat to their reliability. We analyze the problem from two angles: LLMs as\neditors, guided to edit text to mislead LLMs; and LLMs as readers, who answer\nquestions based on the edited text. We introduce a framework that guides an\neditor to add potential shortcuts-triggers to samples. Using GPT4 as the\neditor, we find it can successfully edit trigger shortcut in samples that fool\nLLMs. Analysing LLMs as readers, we observe that even capable LLMs can be\ndeceived using shortcut knowledge. Strikingly, we discover that GPT4 can be\ndeceived by its own edits (15% drop in F1). Our findings highlight inherent\nvulnerabilities of LLMs to shortcut manipulations. We publish ShortcutQA, a\ncurated dataset generated by our framework for future research.", "published": "2023-10-24 12:37:06", "link": "http://arxiv.org/abs/2310.18360v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "From Heuristic to Analytic: Cognitively Motivated Strategies for\n  Coherent Physical Commonsense Reasoning", "abstract": "Pre-trained language models (PLMs) have shown impressive performance in\nvarious language tasks. However, they are prone to spurious correlations, and\noften generate illusory information. In real-world applications, PLMs should\njustify decisions with formalized, coherent reasoning chains, but this\nchallenge remains under-explored. Cognitive psychology theorizes that humans\nare capable of utilizing fast and intuitive heuristic thinking to make\ndecisions based on past experience, then rationalizing the decisions through\nslower and deliberative analytic reasoning. We incorporate these interlinked\ndual processes in fine-tuning and in-context learning with PLMs, applying them\nto two language understanding tasks that require coherent physical commonsense\nreasoning. We show that our proposed Heuristic-Analytic Reasoning (HAR)\nstrategies drastically improve the coherence of rationalizations for model\ndecisions, yielding state-of-the-art results on Tiered Reasoning for Intuitive\nPhysics (TRIP). We also find that this improved coherence is a direct result of\nmore faithful attention to relevant language context in each step of reasoning.\nOur findings suggest that human-like reasoning strategies can effectively\nimprove the coherence and reliability of PLM reasoning.", "published": "2023-10-24 19:46:04", "link": "http://arxiv.org/abs/2310.18364v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mind the Gap Between Conversations for Improved Long-Term Dialogue\n  Generation", "abstract": "Knowing how to end and resume conversations over time is a natural part of\ncommunication, allowing for discussions to span weeks, months, or years. The\nduration of gaps between conversations dictates which topics are relevant and\nwhich questions to ask, and dialogue systems which do not explicitly model time\nmay generate responses that are unnatural. In this work we explore the idea of\nmaking dialogue models aware of time, and present GapChat, a multi-session\ndialogue dataset in which the time between each session varies. While the\ndataset is constructed in real-time, progress on events in speakers' lives is\nsimulated in order to create realistic dialogues occurring across a long\ntimespan. We expose time information to the model and compare different\nrepresentations of time and event progress. In human evaluation we show that\ntime-aware models perform better in metrics that judge the relevance of the\nchosen topics and the information gained from the conversation.", "published": "2023-10-24 00:12:38", "link": "http://arxiv.org/abs/2310.15415v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "The Mason-Alberta Phonetic Segmenter: A forced alignment system based on\n  deep neural networks and interpolation", "abstract": "Forced alignment systems automatically determine boundaries between segments\nin speech data, given an orthographic transcription. These tools are\ncommonplace in phonetics to facilitate the use of speech data that would be\ninfeasible to manually transcribe and segment. In the present paper, we\ndescribe a new neural network-based forced alignment system, the Mason-Alberta\nPhonetic Segmenter (MAPS). The MAPS aligner serves as a testbed for two\npossible improvements we pursue for forced alignment systems. The first is\ntreating the acoustic model in a forced aligner as a tagging task, rather than\na classification task, motivated by the common understanding that segments in\nspeech are not truly discrete and commonly overlap. The second is an\ninterpolation technique to allow boundaries more precise than the common 10 ms\nlimit in modern forced alignment systems. We compare configurations of our\nsystem to a state-of-the-art system, the Montreal Forced Aligner. The tagging\napproach did not generally yield improved results over the Montreal Forced\nAligner. However, a system with the interpolation technique had a 27.92%\nincrease relative to the Montreal Forced Aligner in the amount of boundaries\nwithin 10 ms of the target on the test set. We also reflect on the task and\ntraining process for acoustic modeling in forced alignment, highlighting how\nthe output targets for these models do not match phoneticians' conception of\nsimilarity between phones and that reconciliation of this tension may require\nrethinking the task and output targets or how speech itself should be\nsegmented.", "published": "2023-10-24 00:43:54", "link": "http://arxiv.org/abs/2310.15425v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The Janus Interface: How Fine-Tuning in Large Language Models Amplifies\n  the Privacy Risks", "abstract": "The rapid advancements of large language models (LLMs) have raised public\nconcerns about the privacy leakage of personally identifiable information (PII)\nwithin their extensive training datasets. Recent studies have demonstrated that\nan adversary could extract highly sensitive privacy data from the training data\nof LLMs with carefully designed prompts. However, these attacks suffer from the\nmodel's tendency to hallucinate and catastrophic forgetting (CF) in the\npre-training stage, rendering the veracity of divulged PIIs negligible. In our\nresearch, we propose a novel attack, Janus, which exploits the fine-tuning\ninterface to recover forgotten PIIs from the pre-training data in LLMs. We\nformalize the privacy leakage problem in LLMs and explain why forgotten PIIs\ncan be recovered through empirical analysis on open-source language models.\nBased upon these insights, we evaluate the performance of Janus on both\nopen-source language models and two latest LLMs, i.e., GPT-3.5-Turbo and\nLLaMA-2-7b. Our experiment results show that Janus amplifies the privacy risks\nby over 10 times in comparison with the baseline and significantly outperforms\nthe state-of-the-art privacy extraction attacks including prefix attacks and\nin-context learning (ICL). Furthermore, our analysis validates that existing\nfine-tuning APIs provided by OpenAI and Azure AI Studio are susceptible to our\nJanus attack, allowing an adversary to conduct such an attack at a low cost.", "published": "2023-10-24 02:48:19", "link": "http://arxiv.org/abs/2310.15469v3", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "KITAB: Evaluating LLMs on Constraint Satisfaction for Information\n  Retrieval", "abstract": "We study the ability of state-of-the art models to answer constraint\nsatisfaction queries for information retrieval (e.g., 'a list of ice cream\nshops in San Diego'). In the past, such queries were considered to be tasks\nthat could only be solved via web-search or knowledge bases. More recently,\nlarge language models (LLMs) have demonstrated initial emergent abilities in\nthis task. However, many current retrieval benchmarks are either saturated or\ndo not measure constraint satisfaction. Motivated by rising concerns around\nfactual incorrectness and hallucinations of LLMs, we present KITAB, a new\ndataset for measuring constraint satisfaction abilities of language models.\nKITAB consists of book-related data across more than 600 authors and 13,000\nqueries, and also offers an associated dynamic data collection and constraint\nverification approach for acquiring similar test data for other authors. Our\nextended experiments on GPT4 and GPT3.5 characterize and decouple common\nfailure modes across dimensions such as information popularity, constraint\ntypes, and context availability. Results show that in the absence of context,\nmodels exhibit severe limitations as measured by irrelevant information,\nfactual errors, and incompleteness, many of which exacerbate as information\npopularity decreases. While context availability mitigates irrelevant\ninformation, it is not helpful for satisfying constraints, identifying\nfundamental barriers to constraint satisfaction. We open source our\ncontributions to foster further research on improving constraint satisfaction\nabilities of future models.", "published": "2023-10-24 04:40:38", "link": "http://arxiv.org/abs/2310.15511v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IR", "I.2.7"], "primary_category": "cs.LG"}
{"title": "Multimodal Representations for Teacher-Guided Compositional Visual\n  Reasoning", "abstract": "Neural Module Networks (NMN) are a compelling method for visual question\nanswering, enabling the translation of a question into a program consisting of\na series of reasoning sub-tasks that are sequentially executed on the image to\nproduce an answer. NMNs provide enhanced explainability compared to integrated\nmodels, allowing for a better understanding of the underlying reasoning\nprocess. To improve the effectiveness of NMNs we propose to exploit features\nobtained by a large-scale cross-modal encoder. Also, the current training\napproach of NMNs relies on the propagation of module outputs to subsequent\nmodules, leading to the accumulation of prediction errors and the generation of\nfalse answers. To mitigate this, we introduce an NMN learning strategy\ninvolving scheduled teacher guidance. Initially, the model is fully guided by\nthe ground-truth intermediate outputs, but gradually transitions to an\nautonomous behavior as training progresses. This reduces error accumulation,\nthus improving training efficiency and final performance.We demonstrate that by\nincorporating cross-modal features and employing more effective training\ntechniques for NMN, we achieve a favorable balance between performance and\ntransparency in the reasoning process.", "published": "2023-10-24 07:51:08", "link": "http://arxiv.org/abs/2310.15585v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Machine Translation for Nko: Tools, Corpora and Baseline Results", "abstract": "Currently, there is no usable machine translation system for Nko, a language\nspoken by tens of millions of people across multiple West African countries,\nwhich holds significant cultural and educational value.\n  To address this issue, we present a set of tools, resources, and baseline\nresults aimed towards the development of usable machine translation systems for\nNko and other languages that do not currently have sufficiently large parallel\ntext corpora available.\n  (1) Fria$\\parallel$el: A novel collaborative parallel text curation software\nthat incorporates quality control through copyedit-based workflows.\n  (2) Expansion of the FLoRes-200 and NLLB-Seed corpora with 2,009 and 6,193\nhigh-quality Nko translations in parallel with 204 and 40 other languages.\n  (3) nicolingua-0005: A collection of trilingual and bilingual corpora with\n130,850 parallel segments and monolingual corpora containing over 3 million Nko\nwords.\n  (4) Baseline bilingual and multilingual neural machine translation results\nwith the best model scoring 30.83 English-Nko chrF++ on FLoRes-devtest.", "published": "2023-10-24 08:27:56", "link": "http://arxiv.org/abs/2310.15612v3", "categories": ["cs.CL", "cs.CY", "cs.HC", "cs.LG", "I.2.6; I.2.7"], "primary_category": "cs.CL"}
{"title": "How Much Context Does My Attention-Based ASR System Need?", "abstract": "For the task of speech recognition, the use of more than 30 seconds of\nacoustic context during training is uncommon and under-investigated in\nliterature. In this work, we conduct an empirical study on the effect of\nscaling the sequence length used to train/evaluate (dense-attention-based)\nacoustic models on speech recognition performance. For these experiments, a\ndataset of roughly 100,000 pseudo-labelled Spotify podcasts is used, with\ncontext lengths of 5 seconds to 1 hour being explored. Zero-shot evaluations\nare presented on the long-format datasets: Earnings-22, Tedlium and Rev16.\nResults demonstrate a benefit from training with up to 21.8 minutes of acoustic\ncontext, showing up to a 14.5\\% relative improvement from a baseline trained\nwith 10 seconds of context. We find that the model's width/depth, positional\nencoding scheme and number of attention heads impact its ability to use longer\ncontexts.", "published": "2023-10-24 09:31:03", "link": "http://arxiv.org/abs/2310.15672v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Improving generalization in large language models by learning prefix\n  subspaces", "abstract": "This article focuses on large language models (LLMs) fine-tuning in the\nscarce data regime (also known as the \"few-shot\" learning setting). We propose\na method to increase the generalization capabilities of LLMs based on neural\nnetwork subspaces. This optimization method, recently introduced in computer\nvision, aims to improve model generalization by identifying wider local optima\nthrough the joint optimization of an entire simplex of models in parameter\nspace. Its adaptation to massive, pretrained transformers, however, poses some\nchallenges. First, their considerable number of parameters makes it difficult\nto train several models jointly, and second, their deterministic parameter\ninitialization schemes make them unfit for the subspace method as originally\nproposed. We show in this paper that \"Parameter Efficient Fine-Tuning\" (PEFT)\nmethods, however, are perfectly compatible with this original approach, and\npropose to learn entire simplex of continuous prefixes. We test our method on a\nvariant of the GLUE benchmark adapted to the few-shot learning setting, and\nshow that both our contributions jointly lead to a gain in average performances\ncompared to sota methods. The implementation can be found at the following\nlink: https://github.com/Liloulou/prefix_subspace", "published": "2023-10-24 12:44:09", "link": "http://arxiv.org/abs/2310.15793v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Random Entity Quantization for Parameter-Efficient Compositional\n  Knowledge Graph Representation", "abstract": "Representation Learning on Knowledge Graphs (KGs) is essential for downstream\ntasks. The dominant approach, KG Embedding (KGE), represents entities with\nindependent vectors and faces the scalability challenge. Recent studies propose\nan alternative way for parameter efficiency, which represents entities by\ncomposing entity-corresponding codewords matched from predefined small-scale\ncodebooks. We refer to the process of obtaining corresponding codewords of each\nentity as entity quantization, for which previous works have designed\ncomplicated strategies. Surprisingly, this paper shows that simple random\nentity quantization can achieve similar results to current strategies. We\nanalyze this phenomenon and reveal that entity codes, the quantization outcomes\nfor expressing entities, have higher entropy at the code level and Jaccard\ndistance at the codeword level under random entity quantization. Therefore,\ndifferent entities become more easily distinguished, facilitating effective KG\nrepresentation. The above results show that current quantization strategies are\nnot critical for KG representation, and there is still room for improvement in\nentity distinguishability beyond current strategies. The code to reproduce our\nresults is available at https://github.com/JiaangL/RandomQuantization.", "published": "2023-10-24 12:48:52", "link": "http://arxiv.org/abs/2310.15797v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "A Diffusion Weighted Graph Framework for New Intent Discovery", "abstract": "New Intent Discovery (NID) aims to recognize both new and known intents from\nunlabeled data with the aid of limited labeled data containing only known\nintents. Without considering structure relationships between samples, previous\nmethods generate noisy supervisory signals which cannot strike a balance\nbetween quantity and quality, hindering the formation of new intent clusters\nand effective transfer of the pre-training knowledge. To mitigate this\nlimitation, we propose a novel Diffusion Weighted Graph Framework (DWGF) to\ncapture both semantic similarities and structure relationships inherent in\ndata, enabling more sufficient and reliable supervisory signals. Specifically,\nfor each sample, we diffuse neighborhood relationships along semantic paths\nguided by the nearest neighbors for multiple hops to characterize its local\nstructure discriminately. Then, we sample its positive keys and weigh them\nbased on semantic similarities and local structures for contrastive learning.\nDuring inference, we further propose Graph Smoothing Filter (GSF) to explicitly\nutilize the structure relationships to filter high-frequency noise embodied in\nsemantically ambiguous samples on the cluster boundary. Extensive experiments\nshow that our method outperforms state-of-the-art models on all evaluation\nmetrics across multiple benchmark datasets. Code and data are available at\nhttps://github.com/yibai-shi/DWGF.", "published": "2023-10-24 13:43:01", "link": "http://arxiv.org/abs/2310.15836v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Is Probing All You Need? Indicator Tasks as an Alternative to Probing\n  Embedding Spaces", "abstract": "The ability to identify and control different kinds of linguistic information\nencoded in vector representations of words has many use cases, especially for\nexplainability and bias removal. This is usually done via a set of simple\nclassification tasks, termed probes, to evaluate the information encoded in the\nembedding space. However, the involvement of a trainable classifier leads to\nentanglement between the probe's results and the classifier's nature. As a\nresult, contemporary works on probing include tasks that do not involve\ntraining of auxiliary models. In this work we introduce the term indicator\ntasks for non-trainable tasks which are used to query embedding spaces for the\nexistence of certain properties, and claim that this kind of tasks may point to\na direction opposite to probes, and that this contradiction complicates the\ndecision on whether a property exists in an embedding space. We demonstrate our\nclaims with two test cases, one dealing with gender debiasing and another with\nthe erasure of morphological information from embedding spaces. We show that\nthe application of a suitable indicator provides a more accurate picture of the\ninformation captured and removed compared to probes. We thus conclude that\nindicator tasks should be implemented and taken into consideration when\neliciting information from embedded representations.", "published": "2023-10-24 15:08:12", "link": "http://arxiv.org/abs/2310.15905v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "E-Sparse: Boosting the Large Language Model Inference through\n  Entropy-based N:M Sparsity", "abstract": "Traditional pruning methods are known to be challenging to work in Large\nLanguage Models (LLMs) for Generative AI because of their unaffordable training\nprocess and large computational demands. For the first time, we introduce the\ninformation entropy of hidden state features into a pruning metric design,\nnamely E-Sparse, to improve the accuracy of N:M sparsity on LLM. E-Sparse\nemploys the information richness to leverage the channel importance, and\nfurther incorporates several novel techniques to put it into effect: (1) it\nintroduces information entropy to enhance the significance of parameter weights\nand input feature norms as a novel pruning metric, and performs N:M sparsity\nwithout modifying the remaining weights. (2) it designs global naive shuffle\nand local block shuffle to quickly optimize the information distribution and\nadequately cope with the impact of N:M sparsity on LLMs' accuracy. E-Sparse is\nimplemented as a Sparse-GEMM on FasterTransformer and runs on NVIDIA Ampere\nGPUs. Extensive experiments on the LLaMA family and OPT models show that\nE-Sparse can significantly speed up the model inference over the dense model\n(up to 1.53X) and obtain significant memory saving (up to 43.52%), with\nacceptable accuracy loss.", "published": "2023-10-24 15:27:15", "link": "http://arxiv.org/abs/2310.15929v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Accented Speech Recognition With Accent-specific Codebooks", "abstract": "Speech accents pose a significant challenge to state-of-the-art automatic\nspeech recognition (ASR) systems. Degradation in performance across\nunderrepresented accents is a severe deterrent to the inclusive adoption of\nASR. In this work, we propose a novel accent adaptation approach for end-to-end\nASR systems using cross-attention with a trainable set of codebooks. These\nlearnable codebooks capture accent-specific information and are integrated\nwithin the ASR encoder layers. The model is trained on accented English speech,\nwhile the test data also contained accents which were not seen during training.\nOn the Mozilla Common Voice multi-accented dataset, we show that our proposed\napproach yields significant performance gains not only on the seen English\naccents (up to $37\\%$ relative improvement in word error rate) but also on the\nunseen accents (up to $5\\%$ relative improvement in WER). Further, we\nillustrate benefits for a zero-shot transfer setup on the L2Artic dataset. We\nalso compare the performance with other approaches based on accent adversarial\ntraining.", "published": "2023-10-24 16:10:58", "link": "http://arxiv.org/abs/2310.15970v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "What Algorithms can Transformers Learn? A Study in Length Generalization", "abstract": "Large language models exhibit surprising emergent generalization properties,\nyet also struggle on many simple reasoning tasks such as arithmetic and parity.\nThis raises the question of if and when Transformer models can learn the true\nalgorithm for solving a task. We study the scope of Transformers' abilities in\nthe specific setting of length generalization on algorithmic tasks. Here, we\npropose a unifying framework to understand when and how Transformers can\nexhibit strong length generalization on a given task. Specifically, we leverage\nRASP (Weiss et al., 2021) -- a programming language designed for the\ncomputational model of a Transformer -- and introduce the RASP-Generalization\nConjecture: Transformers tend to length generalize on a task if the task can be\nsolved by a short RASP program which works for all input lengths. This simple\nconjecture remarkably captures most known instances of length generalization on\nalgorithmic tasks. Moreover, we leverage our insights to drastically improve\ngeneralization performance on traditionally hard tasks (such as parity and\naddition). On the theoretical side, we give a simple example where the\n\"min-degree-interpolator\" model of learning from Abbe et al. (2023) does not\ncorrectly predict Transformers' out-of-distribution behavior, but our\nconjecture does. Overall, our work provides a novel perspective on the\nmechanisms of compositional generalization and the algorithmic capabilities of\nTransformers.", "published": "2023-10-24 17:43:29", "link": "http://arxiv.org/abs/2310.16028v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Woodpecker: Hallucination Correction for Multimodal Large Language\n  Models", "abstract": "Hallucination is a big shadow hanging over the rapidly evolving Multimodal\nLarge Language Models (MLLMs), referring to the phenomenon that the generated\ntext is inconsistent with the image content. In order to mitigate\nhallucinations, existing studies mainly resort to an instruction-tuning manner\nthat requires retraining the models with specific data. In this paper, we pave\na different way, introducing a training-free method named Woodpecker. Like a\nwoodpecker heals trees, it picks out and corrects hallucinations from the\ngenerated text. Concretely, Woodpecker consists of five stages: key concept\nextraction, question formulation, visual knowledge validation, visual claim\ngeneration, and hallucination correction. Implemented in a post-remedy manner,\nWoodpecker can easily serve different MLLMs, while being interpretable by\naccessing intermediate outputs of the five stages. We evaluate Woodpecker both\nquantitatively and qualitatively and show the huge potential of this new\nparadigm. On the POPE benchmark, our method obtains a 30.66%/24.33% improvement\nin accuracy over the baseline MiniGPT-4/mPLUG-Owl. The source code is released\nat https://github.com/BradyFU/Woodpecker.", "published": "2023-10-24 17:58:07", "link": "http://arxiv.org/abs/2310.16045v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Locally Differentially Private Document Generation Using Zero Shot\n  Prompting", "abstract": "Numerous studies have highlighted the privacy risks associated with\npretrained large language models. In contrast, our research offers a unique\nperspective by demonstrating that pretrained large language models can\neffectively contribute to privacy preservation. We propose a locally\ndifferentially private mechanism called DP-Prompt, which leverages the power of\npretrained large language models and zero-shot prompting to counter author\nde-anonymization attacks while minimizing the impact on downstream utility.\nWhen DP-Prompt is used with a powerful language model like ChatGPT (gpt-3.5),\nwe observe a notable reduction in the success rate of de-anonymization attacks,\nshowing that it surpasses existing approaches by a considerable margin despite\nits simpler design. For instance, in the case of the IMDB dataset, DP-Prompt\n(with ChatGPT) perfectly recovers the clean sentiment F1 score while achieving\na 46\\% reduction in author identification F1 score against static attackers and\na 26\\% reduction against adaptive attackers. We conduct extensive experiments\nacross six open-source large language models, ranging up to 7 billion\nparameters, to analyze various effects of the privacy-utility tradeoff.", "published": "2023-10-24 18:25:13", "link": "http://arxiv.org/abs/2310.16111v2", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Language Model with Limited Memory Capacity Captures Interference in\n  Human Sentence Processing", "abstract": "Two of the central factors believed to underpin human sentence processing\ndifficulty are expectations and retrieval from working memory. A recent attempt\nto create a unified cognitive model integrating these two factors relied on the\nparallels between the self-attention mechanism of transformer language models\nand cue-based retrieval theories of working memory in human sentence processing\n(Ryu and Lewis 2021). While Ryu and Lewis show that attention patterns in\nspecialized attention heads of GPT-2 are consistent with similarity-based\ninterference, a key prediction of cue-based retrieval models, their method\nrequires identifying syntactically specialized attention heads, and makes the\ncognitively implausible assumption that hundreds of memory retrieval operations\ntake place in parallel. In the present work, we develop a recurrent neural\nlanguage model with a single self-attention head, which more closely parallels\nthe memory system assumed by cognitive theories. We show that our model's\nsingle attention head captures semantic and syntactic interference effects\nobserved in human experiments.", "published": "2023-10-24 19:33:27", "link": "http://arxiv.org/abs/2310.16142v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Clinfo.ai: An Open-Source Retrieval-Augmented Large Language Model\n  System for Answering Medical Questions using Scientific Literature", "abstract": "The quickly-expanding nature of published medical literature makes it\nchallenging for clinicians and researchers to keep up with and summarize\nrecent, relevant findings in a timely manner. While several closed-source\nsummarization tools based on large language models (LLMs) now exist, rigorous\nand systematic evaluations of their outputs are lacking. Furthermore, there is\na paucity of high-quality datasets and appropriate benchmark tasks with which\nto evaluate these tools. We address these issues with four contributions: we\nrelease Clinfo.ai, an open-source WebApp that answers clinical questions based\non dynamically retrieved scientific literature; we specify an information\nretrieval and abstractive summarization task to evaluate the performance of\nsuch retrieval-augmented LLM systems; we release a dataset of 200 questions and\ncorresponding answers derived from published systematic reviews, which we name\nPubMed Retrieval and Synthesis (PubMedRS-200); and report benchmark results for\nClinfo.ai and other publicly available OpenQA systems on PubMedRS-200.", "published": "2023-10-24 19:43:39", "link": "http://arxiv.org/abs/2310.16146v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Hidden Citations Obscure True Impact in Science", "abstract": "References, the mechanism scientists rely on to signal previous knowledge,\nlately have turned into widely used and misused measures of scientific impact.\nYet, when a discovery becomes common knowledge, citations suffer from\nobliteration by incorporation. This leads to the concept of hidden citation,\nrepresenting a clear textual credit to a discovery without a reference to the\npublication embodying it. Here, we rely on unsupervised interpretable machine\nlearning applied to the full text of each paper to systematically identify\nhidden citations. We find that for influential discoveries hidden citations\noutnumber citation counts, emerging regardless of publishing venue and\ndiscipline. We show that the prevalence of hidden citations is not driven by\ncitation counts, but rather by the degree of the discourse on the topic within\nthe text of the manuscripts, indicating that the more discussed is a discovery,\nthe less visible it is to standard bibliometric analysis. Hidden citations\nindicate that bibliometric measures offer a limited perspective on quantifying\nthe true impact of a discovery, raising the need to extract knowledge from the\nfull text of the scientific corpus.", "published": "2023-10-24 20:58:07", "link": "http://arxiv.org/abs/2310.16181v2", "categories": ["cs.CL", "cs.DL", "cs.SI", "physics.soc-ph"], "primary_category": "cs.CL"}
{"title": "CleanCoNLL: A Nearly Noise-Free Named Entity Recognition Dataset", "abstract": "The CoNLL-03 corpus is arguably the most well-known and utilized benchmark\ndataset for named entity recognition (NER). However, prior works found\nsignificant numbers of annotation errors, incompleteness, and inconsistencies\nin the data. This poses challenges to objectively comparing NER approaches and\nanalyzing their errors, as current state-of-the-art models achieve F1-scores\nthat are comparable to or even exceed the estimated noise level in CoNLL-03. To\naddress this issue, we present a comprehensive relabeling effort assisted by\nautomatic consistency checking that corrects 7.0% of all labels in the English\nCoNLL-03. Our effort adds a layer of entity linking annotation both for better\nexplainability of NER labels and as additional safeguard of annotation quality.\nOur experimental evaluation finds not only that state-of-the-art approaches\nreach significantly higher F1-scores (97.1%) on our data, but crucially that\nthe share of correct predictions falsely counted as errors due to annotation\nnoise drops from 47% to 6%. This indicates that our resource is well suited to\nanalyze the remaining errors made by state-of-the-art models, and that the\ntheoretical upper bound even on high resource, coarse-grained NER is not yet\nreached. To facilitate such analysis, we make CleanCoNLL publicly available to\nthe research community.", "published": "2023-10-24 22:34:43", "link": "http://arxiv.org/abs/2310.16225v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TiC-CLIP: Continual Training of CLIP Models", "abstract": "Keeping large foundation models up to date on latest data is inherently\nexpensive. To avoid the prohibitive costs of constantly retraining, it is\nimperative to continually train these models. This problem is exacerbated by\nthe lack of any large scale continual learning benchmarks or baselines. We\nintroduce the first set of web-scale Time-Continual (TiC) benchmarks for\ntraining vision-language models: TiC-DataComp, TiC-YFCC, and TiC-Redcaps.\nTiC-DataComp, our largest dataset, contains over 12.7B timestamped image-text\npairs spanning 9 years (2014-2022). We first use our benchmarks to curate\nvarious dynamic evaluations to measure temporal robustness of existing models.\nWe show OpenAI's CLIP (trained on data up to 2020) loses $\\approx 8\\%$\nzero-shot accuracy on our curated retrieval task from 2021-2022 compared with\nmore recently trained models in OpenCLIP repository. We then study how to\nefficiently train models on time-continuous data. We demonstrate that a simple\nrehearsal-based approach that continues training from the last checkpoint and\nreplays old data reduces compute by $2.5\\times$ when compared to the standard\npractice of retraining from scratch. Code is available at\nhttps://github.com/apple/ml-tic-clip.", "published": "2023-10-24 22:41:14", "link": "http://arxiv.org/abs/2310.16226v3", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "LoRAShear: Efficient Large Language Model Structured Pruning and\n  Knowledge Recovery", "abstract": "Large Language Models (LLMs) have transformed the landscape of artificial\nintelligence, while their enormous size presents significant challenges in\nterms of computational costs. We introduce LoRAShear, a novel efficient\napproach to structurally prune LLMs and recover knowledge. Given general LLMs,\nLoRAShear at first creates the dependency graphs over LoRA modules to discover\nminimally removal structures and analyze the knowledge distribution. It then\nproceeds progressive structured pruning on LoRA adaptors and enables inherent\nknowledge transfer to better preserve the information in the redundant\nstructures. To recover the lost knowledge during pruning, LoRAShear\nmeticulously studies and proposes a dynamic fine-tuning schemes with dynamic\ndata adaptors to effectively narrow down the performance gap to the full\nmodels. Numerical results demonstrate that by only using one GPU within a\ncouple of GPU days, LoRAShear effectively reduced footprint of LLMs by 20% with\nonly 1.0% performance degradation and significantly outperforms\nstate-of-the-arts. The source code will be available at\nhttps://github.com/microsoft/lorashear.", "published": "2023-10-24 00:47:26", "link": "http://arxiv.org/abs/2310.18356v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SoK: Memorization in General-Purpose Large Language Models", "abstract": "Large Language Models (LLMs) are advancing at a remarkable pace, with myriad\napplications under development. Unlike most earlier machine learning models,\nthey are no longer built for one specific application but are designed to excel\nin a wide range of tasks. A major part of this success is due to their huge\ntraining datasets and the unprecedented number of model parameters, which allow\nthem to memorize large amounts of information contained in the training data.\nThis memorization goes beyond mere language, and encompasses information only\npresent in a few documents. This is often desirable since it is necessary for\nperforming tasks such as question answering, and therefore an important part of\nlearning, but also brings a whole array of issues, from privacy and security to\ncopyright and beyond. LLMs can memorize short secrets in the training data, but\ncan also memorize concepts like facts or writing styles that can be expressed\nin text in many different ways. We propose a taxonomy for memorization in LLMs\nthat covers verbatim text, facts, ideas and algorithms, writing styles,\ndistributional properties, and alignment goals. We describe the implications of\neach type of memorization - both positive and negative - for model performance,\nprivacy, security and confidentiality, copyright, and auditing, and ways to\ndetect and prevent memorization. We further highlight the challenges that arise\nfrom the predominant way of defining memorization with respect to model\nbehavior instead of model weights, due to LLM-specific phenomena such as\nreasoning capabilities or differences between decoding algorithms. Throughout\nthe paper, we describe potential risks and opportunities arising from\nmemorization in LLMs that we hope will motivate new research directions.", "published": "2023-10-24 14:25:53", "link": "http://arxiv.org/abs/2310.18362v1", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Contextualized Real-Time Multimodal Emotion Recognition for\n  Conversational Agents using Graph Convolutional Networks in Reinforcement\n  Learning", "abstract": "Owing to the recent developments in Generative Artificial Intelligence\n(GenAI) and Large Language Models (LLM), conversational agents are becoming\nincreasingly popular and accepted. They provide a human touch by interacting in\nways familiar to us and by providing support as virtual companions. Therefore,\nit is important to understand the user's emotions in order to respond\nconsiderately. Compared to the standard problem of emotion recognition,\nconversational agents face an additional constraint in that recognition must be\nreal-time. Studies on model architectures using audio, visual, and textual\nmodalities have mainly focused on emotion classification using full video\nsequences that do not provide online features. In this work, we present a novel\nparadigm for contextualized Emotion Recognition using Graph Convolutional\nNetwork with Reinforcement Learning (conER-GRL). Conversations are partitioned\ninto smaller groups of utterances for effective extraction of contextual\ninformation. The system uses Gated Recurrent Units (GRU) to extract multimodal\nfeatures from these groups of utterances. More importantly, Graph Convolutional\nNetworks (GCN) and Reinforcement Learning (RL) agents are cascade trained to\ncapture the complex dependencies of emotion features in interactive scenarios.\nComparing the results of the conER-GRL model with other state-of-the-art models\non the benchmark dataset IEMOCAP demonstrates the advantageous capabilities of\nthe conER-GRL architecture in recognizing emotions in real-time from multimodal\nconversational signals.", "published": "2023-10-24 14:31:17", "link": "http://arxiv.org/abs/2310.18363v1", "categories": ["cs.CL", "cs.HC", "cs.LG", "I.2.10"], "primary_category": "cs.CL"}
{"title": "Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of\n  LLMs through a Global Scale Prompt Hacking Competition", "abstract": "Large Language Models (LLMs) are deployed in interactive contexts with direct\nuser engagement, such as chatbots and writing assistants. These deployments are\nvulnerable to prompt injection and jailbreaking (collectively, prompt hacking),\nin which models are manipulated to ignore their original instructions and\nfollow potentially malicious ones. Although widely acknowledged as a\nsignificant security threat, there is a dearth of large-scale resources and\nquantitative studies on prompt hacking. To address this lacuna, we launch a\nglobal prompt hacking competition, which allows for free-form human input\nattacks. We elicit 600K+ adversarial prompts against three state-of-the-art\nLLMs. We describe the dataset, which empirically verifies that current LLMs can\nindeed be manipulated via prompt hacking. We also present a comprehensive\ntaxonomical ontology of the types of adversarial prompts.", "published": "2023-10-24 18:18:11", "link": "http://arxiv.org/abs/2311.16119v3", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "A Survey on Detection of LLMs-Generated Content", "abstract": "The burgeoning capabilities of advanced large language models (LLMs) such as\nChatGPT have led to an increase in synthetic content generation with\nimplications across a variety of sectors, including media, cybersecurity,\npublic discourse, and education. As such, the ability to detect LLMs-generated\ncontent has become of paramount importance. We aim to provide a detailed\noverview of existing detection strategies and benchmarks, scrutinizing their\ndifferences and identifying key challenges and prospects in the field,\nadvocating for more adaptable and robust models to enhance detection accuracy.\nWe also posit the necessity for a multi-faceted approach to defend against\nvarious attacks to counter the rapidly advancing capabilities of LLMs. To the\nbest of our knowledge, this work is the first comprehensive survey on the\ndetection in the era of LLMs. We hope it will provide a broad understanding of\nthe current landscape of LLMs-generated content detection, offering a guiding\nreference for researchers and practitioners striving to uphold the integrity of\ndigital information in an era increasingly dominated by synthetic content. The\nrelevant papers are summarized and will be consistently updated at\nhttps://github.com/Xianjun-Yang/Awesome_papers_on_LLMs_detection.git.", "published": "2023-10-24 09:10:26", "link": "http://arxiv.org/abs/2310.15654v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "What's Left? Concept Grounding with Logic-Enhanced Foundation Models", "abstract": "Recent works such as VisProg and ViperGPT have smartly composed foundation\nmodels for visual reasoning-using large language models (LLMs) to produce\nprograms that can be executed by pre-trained vision-language models. However,\nthey operate in limited domains, such as 2D images, not fully exploiting the\ngeneralization of language: abstract concepts like \"left\" can also be grounded\nin 3D, temporal, and action data, as in moving to your left. This limited\ngeneralization stems from these inference-only methods' inability to learn or\nadapt pre-trained models to a new domain. We propose the Logic-Enhanced\nFoundation Model (LEFT), a unified framework that learns to ground and reason\nwith concepts across domains with a differentiable, domain-independent,\nfirst-order logic-based program executor. LEFT has an LLM interpreter that\noutputs a program represented in a general, logic-based reasoning language,\nwhich is shared across all domains and tasks. LEFT's executor then executes the\nprogram with trainable domain-specific grounding modules. We show that LEFT\nflexibly learns concepts in four domains: 2D images, 3D scenes, human motions,\nand robotic manipulation. It exhibits strong reasoning ability in a wide\nvariety of tasks, including those that are complex and not seen during\ntraining, and can be easily applied to new domains.", "published": "2023-10-24 17:50:20", "link": "http://arxiv.org/abs/2310.16035v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CV"}
{"title": "AI Alignment and Social Choice: Fundamental Limitations and Policy\n  Implications", "abstract": "Aligning AI agents to human intentions and values is a key bottleneck in\nbuilding safe and deployable AI applications. But whose values should AI agents\nbe aligned with? Reinforcement learning with human feedback (RLHF) has emerged\nas the key framework for AI alignment. RLHF uses feedback from human\nreinforcers to fine-tune outputs; all widely deployed large language models\n(LLMs) use RLHF to align their outputs to human values. It is critical to\nunderstand the limitations of RLHF and consider policy challenges arising from\nthese limitations. In this paper, we investigate a specific challenge in\nbuilding RLHF systems that respect democratic norms. Building on impossibility\nresults in social choice theory, we show that, under fairly broad assumptions,\nthere is no unique voting protocol to universally align AI systems using RLHF\nthrough democratic processes. Further, we show that aligning AI agents with the\nvalues of all individuals will always violate certain private ethical\npreferences of an individual user i.e., universal AI alignment using RLHF is\nimpossible. We discuss policy implications for the governance of AI systems\nbuilt using RLHF: first, the need for mandating transparent voting rules to\nhold model builders accountable. Second, the need for model builders to focus\non developing AI agents that are narrowly aligned to specific user groups.", "published": "2023-10-24 17:59:04", "link": "http://arxiv.org/abs/2310.16048v1", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.HC", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Pre-training Music Classification Models via Music Source Separation", "abstract": "In this paper, we study whether music source separation can be used as a\npre-training strategy for music representation learning, targeted at music\nclassification tasks. To this end, we first pre-train U-Net networks under\nvarious music source separation objectives, such as the isolation of vocal or\ninstrumental sources from a musical piece; afterwards, we attach a\nclassification network to the pre-trained U-Net and jointly finetune the whole\nnetwork. The features learned by the separation network are also propagated to\nthe tail network through a convolutional feature adaptation module.\nExperimental results in two widely used and publicly available datasets\nindicate that pre-training the U-Nets with a music source separation objective\ncan improve performance compared to both training the whole network from\nscratch and using the tail network as a standalone in two music classification\ntasks, music auto-tagging and music genre classification. We also show that our\nproposed framework can be successfully integrated into both convolutional and\nTransformer-based backends, highlighting its modularity.", "published": "2023-10-24 13:57:55", "link": "http://arxiv.org/abs/2310.15845v3", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "FOLEY-VAE: Generaci\u00f3n de efectos de audio para cine con inteligencia\n  artificial", "abstract": "In this research, we present an interface based on Variational Autoencoders\ntrained with a wide range of natural sounds for the innovative creation of\nFoley effects. The model can transfer new sound features to prerecorded audio\nor microphone-captured speech in real time. In addition, it allows interactive\nmodification of latent variables, facilitating precise and customized artistic\nadjustments. Taking as a starting point our previous study on Variational\nAutoencoders presented at this same congress last year, we analyzed an existing\nimplementation: RAVE [1]. This model has been specifically trained for audio\neffects production. Various audio effects have been successfully generated,\nranging from electromagnetic, science fiction, and water sounds, among others\npublished with this work. This innovative approach has been the basis for the\nartistic creation of the first Spanish short film with sound effects assisted\nby artificial intelligence. This milestone illustrates palpably the\ntransformative potential of this technology in the film industry, opening the\ndoor to new possibilities for sound creation and the improvement of artistic\nquality in film productions.", "published": "2023-10-24 09:21:42", "link": "http://arxiv.org/abs/2310.15663v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "CDSD: Chinese Dysarthria Speech Database", "abstract": "Dysarthric speech poses significant challenges for individuals with\ndysarthria, impacting their ability to communicate socially. Despite the\nwidespread use of Automatic Speech Recognition (ASR), accurately recognizing\ndysarthric speech remains a formidable task, largely due to the limited\navailability of dysarthric speech data. To address this gap, we developed the\nChinese Dysarthria Speech Database (CDSD), the most extensive collection of\nChinese dysarthria data to date, featuring 133 hours of recordings from 44\nspeakers. Our benchmarks reveal a best Character Error Rate (CER) of 16.4\\%.\nCompared to the CER of 20.45\\% from our additional human experiments,\nDysarthric Speech Recognition (DSR) demonstrates its potential in significant\nimprovement of communication for individuals with dysarthria. The CDSD database\nwill be made publicly available at http://melab.psych.ac.cn/CDSD.html.", "published": "2023-10-24 15:27:50", "link": "http://arxiv.org/abs/2310.15930v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "IA Para el Mantenimiento Predictivo en Canteras: Modelado", "abstract": "Dependence on raw materials, especially in the mining sector, is a key part\nof today's economy. Aggregates are vital, being the second most used raw\nmaterial after water. Digitally transforming this sector is key to optimizing\noperations. However, supervision and maintenance (predictive and corrective)\nare challenges little explored in this sector, due to the particularities of\nthe sector, machinery and environmental conditions. All this, despite the\nsuccesses achieved in other scenarios in monitoring with acoustic and contact\nsensors. We present an unsupervised learning scheme that trains a variational\nautoencoder model on a set of sound records. This is the first such dataset\ncollected during processing plant operations, containing information from\ndifferent points of the processing line. Our results demonstrate the model's\nability to reconstruct and represent in latent space the recorded sounds, the\ndifferences in operating conditions and between different equipment. In the\nfuture, this should facilitate the classification of sounds, as well as the\ndetection of anomalies and degradation patterns in the operation of the\nmachinery.", "published": "2023-10-24 19:27:50", "link": "http://arxiv.org/abs/2310.16140v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Dynamic Convolutional Neural Networks as Efficient Pre-trained Audio\n  Models", "abstract": "The introduction of large-scale audio datasets, such as AudioSet, paved the\nway for Transformers to conquer the audio domain and replace CNNs as the\nstate-of-the-art neural network architecture for many tasks. Audio Spectrogram\nTransformers are excellent at exploiting large datasets, creating powerful\npre-trained models that surpass CNNs when fine-tuned on downstream tasks.\nHowever, current popular Audio Spectrogram Transformers are demanding in terms\nof computational complexity compared to CNNs. Recently, we have shown that, by\nemploying Transformer-to-CNN Knowledge Distillation, efficient CNNs can catch\nup with and even outperform Transformers on large datasets. In this work, we\nextend this line of research and increase the capacity of efficient CNNs by\nintroducing dynamic CNN blocks, constructed of dynamic non-linearities, dynamic\nconvolutions and attention mechanisms. We show that these dynamic CNNs\noutperform traditional efficient CNNs, in terms of the performance-complexity\ntrade-off and parameter efficiency, at the task of audio tagging on the\nlarge-scale AudioSet. Our experiments further indicate that the introduced\ndynamic CNNs achieve better performance on downstream tasks and scale up well,\nattaining Transformer performance and even outperforming them on AudioSet and\nseveral downstream tasks.", "published": "2023-10-24 09:08:20", "link": "http://arxiv.org/abs/2310.15648v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Complex Image Generation SwinTransformer Network for Audio Denoising", "abstract": "Achieving high-performance audio denoising is still a challenging task in\nreal-world applications. Existing time-frequency methods often ignore the\nquality of generated frequency domain images. This paper converts the audio\ndenoising problem into an image generation task. We first develop a complex\nimage generation SwinTransformer network to capture more information from the\ncomplex Fourier domain. We then impose structure similarity and detailed loss\nfunctions to generate high-quality images and develop an SDR loss to minimize\nthe difference between denoised and clean audios. Extensive experiments on two\nbenchmark datasets demonstrate that our proposed model is better than\nstate-of-the-art methods.", "published": "2023-10-24 18:21:03", "link": "http://arxiv.org/abs/2310.16109v1", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Design Of Rubble Analyzer Probe Using ML For Earthquake", "abstract": "The earthquake rubble analyzer uses machine learning to detect human presence\nvia ambient sounds, achieving 97.45% accuracy. It also provides real-time\nenvironmental data, aiding in assessing survival prospects for trapped\nindividuals, crucial for post-earthquake rescue efforts", "published": "2023-10-24 14:43:42", "link": "http://arxiv.org/abs/2311.02087v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
