{"title": "Domain Adaptative Causality Encoder", "abstract": "Current approaches which are mainly based on the extraction of low-level\nrelations among individual events are limited by the shortage of publicly\navailable labelled data. Therefore, the resulting models perform poorly when\napplied to a distributionally different domain for which labelled data did not\nexist at the time of training. To overcome this limitation, in this paper, we\nleverage the characteristics of dependency trees and adversarial learning to\naddress the tasks of adaptive causality identification and localisation. The\nterm adaptive is used since the training and test data come from two\ndistributionally different datasets, which to the best of our knowledge, this\nwork is the first to address. Moreover, we present a new causality dataset,\nnamely MedCaus, which integrates all types of causality in the text. Our\nexperiments on four different benchmark causality datasets demonstrate the\nsuperiority of our approach over the existing baselines, by up to 7%\nimprovement, on the tasks of identification and localisation of the causal\nrelations from the text.", "published": "2020-11-27 04:14:55", "link": "http://arxiv.org/abs/2011.13549v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Chinese Medical Question Answer Matching Based on Interactive Sentence\n  Representation Learning", "abstract": "Chinese medical question-answer matching is more challenging than the\nopen-domain question answer matching in English. Even though the deep learning\nmethod has performed well in improving the performance of question answer\nmatching, these methods only focus on the semantic information inside\nsentences, while ignoring the semantic association between questions and\nanswers, thus resulting in performance deficits. In this paper, we design a\nseries of interactive sentence representation learning models to tackle this\nproblem. To better adapt to Chinese medical question-answer matching and take\nthe advantages of different neural network structures, we propose the Crossed\nBERT network to extract the deep semantic information inside the sentence and\nthe semantic association between question and answer, and then combine with the\nmulti-scale CNNs network or BiGRU network to take the advantage of different\nstructure of neural networks to learn more semantic features into the sentence\nrepresentation. The experiments on the cMedQA V2.0 and cMedQA V1.0 dataset show\nthat our model significantly outperforms all the existing state-of-the-art\nmodels of Chinese medical question answer matching.", "published": "2020-11-27 06:13:56", "link": "http://arxiv.org/abs/2011.13573v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CoRe: An Efficient Coarse-refined Training Framework for BERT", "abstract": "In recent years, BERT has made significant breakthroughs on many natural\nlanguage processing tasks and attracted great attentions. Despite its accuracy\ngains, the BERT model generally involves a huge number of parameters and needs\nto be trained on massive datasets, so training such a model is computationally\nvery challenging and time-consuming. Hence, training efficiency should be a\ncritical issue. In this paper, we propose a novel coarse-refined training\nframework named CoRe to speed up the training of BERT. Specifically, we\ndecompose the training process of BERT into two phases. In the first phase, by\nintroducing fast attention mechanism and decomposing the large parameters in\nthe feed-forward network sub-layer, we construct a relaxed BERT model which has\nmuch less parameters and much lower model complexity than the original BERT, so\nthe relaxed model can be quickly trained. In the second phase, we transform the\ntrained relaxed BERT model into the original BERT and further retrain the\nmodel. Thanks to the desired initialization provided by the relaxed model, the\nretraining phase requires much less training steps, compared with training an\noriginal BERT model from scratch with a random initialization. Experimental\nresults show that the proposed CoRe framework can greatly reduce the training\ntime without reducing the performance.", "published": "2020-11-27 09:49:37", "link": "http://arxiv.org/abs/2011.13633v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Progressively Stacking 2.0: A Multi-stage Layerwise Training Method for\n  BERT Training Speedup", "abstract": "Pre-trained language models, such as BERT, have achieved significant accuracy\ngain in many natural language processing tasks. Despite its effectiveness, the\nhuge number of parameters makes training a BERT model computationally very\nchallenging. In this paper, we propose an efficient multi-stage layerwise\ntraining (MSLT) approach to reduce the training time of BERT. We decompose the\nwhole training process into several stages. The training is started from a\nsmall model with only a few encoder layers and we gradually increase the depth\nof the model by adding new encoder layers. At each stage, we only train the top\n(near the output layer) few encoder layers which are newly added. The\nparameters of the other layers which have been trained in the previous stages\nwill not be updated in the current stage. In BERT training, the backward\ncomputation is much more time-consuming than the forward computation,\nespecially in the distributed training setting in which the backward\ncomputation time further includes the communication time for gradient\nsynchronization. In the proposed training strategy, only top few layers\nparticipate in backward computation, while most layers only participate in\nforward computation. Hence both the computation and communication efficiencies\nare greatly improved. Experimental results show that the proposed method can\nachieve more than 110% training speedup without significant performance\ndegradation.", "published": "2020-11-27 10:00:22", "link": "http://arxiv.org/abs/2011.13635v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FFCI: A Framework for Interpretable Automatic Evaluation of\n  Summarization", "abstract": "In this paper, we propose FFCI, a framework for fine-grained summarization\nevaluation that comprises four elements: faithfulness (degree of factual\nconsistency with the source), focus (precision of summary content relative to\nthe reference), coverage (recall of summary content relative to the reference),\nand inter-sentential coherence (document fluency between adjacent sentences).\nWe construct a novel dataset for focus, coverage, and inter-sentential\ncoherence, and develop automatic methods for evaluating each of the four\ndimensions of FFCI based on cross-comparison of evaluation metrics and\nmodel-based evaluation methods, including question answering (QA) approaches,\nsemantic textual similarity (STS), next-sentence prediction (NSP), and scores\nderived from 19 pre-trained language models. We then apply the developed\nmetrics in evaluating a broad range of summarization models across two\ndatasets, with some surprising findings.", "published": "2020-11-27 10:57:18", "link": "http://arxiv.org/abs/2011.13662v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TaylorGAN: Neighbor-Augmented Policy Update for Sample-Efficient Natural\n  Language Generation", "abstract": "Score function-based natural language generation (NLG) approaches such as\nREINFORCE, in general, suffer from low sample efficiency and training\ninstability problems. This is mainly due to the non-differentiable nature of\nthe discrete space sampling and thus these methods have to treat the\ndiscriminator as a black box and ignore the gradient information. To improve\nthe sample efficiency and reduce the variance of REINFORCE, we propose a novel\napproach, TaylorGAN, which augments the gradient estimation by off-policy\nupdate and the first-order Taylor expansion. This approach enables us to train\nNLG models from scratch with smaller batch size -- without maximum likelihood\npre-training, and outperforms existing GAN-based methods on multiple metrics of\nquality and diversity. The source code and data are available at\nhttps://github.com/MiuLab/TaylorGAN", "published": "2020-11-27 02:26:15", "link": "http://arxiv.org/abs/2011.13527v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Joint Extraction of Entity and Relation with Information Redundancy\n  Elimination", "abstract": "To solve the problem of redundant information and overlapping relations of\nthe entity and relation extraction model, we propose a joint extraction model.\nThis model can directly extract multiple pairs of related entities without\ngenerating unrelated redundant information. We also propose a recurrent neural\nnetwork named Encoder-LSTM that enhances the ability of recurrent units to\nmodel sentences. Specifically, the joint model includes three sub-modules: the\nNamed Entity Recognition sub-module consisted of a pre-trained language model\nand an LSTM decoder layer, the Entity Pair Extraction sub-module which uses\nEncoder-LSTM network to model the order relationship between related entity\npairs, and the Relation Classification sub-module including Attention\nmechanism. We conducted experiments on the public datasets ADE and CoNLL04 to\nevaluate the effectiveness of our model. The results show that the proposed\nmodel achieves good performance in the task of entity and relation extraction\nand can greatly reduce the amount of redundant information.", "published": "2020-11-27 05:47:26", "link": "http://arxiv.org/abs/2011.13565v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning Relation Prototype from Unlabeled Texts for Long-tail Relation\n  Extraction", "abstract": "Relation Extraction (RE) is a vital step to complete Knowledge Graph (KG) by\nextracting entity relations from texts.However, it usually suffers from the\nlong-tail issue. The training data mainly concentrates on a few types of\nrelations, leading to the lackof sufficient annotations for the remaining types\nof relations. In this paper, we propose a general approach to learn relation\nprototypesfrom unlabeled texts, to facilitate the long-tail relation extraction\nby transferring knowledge from the relation types with sufficient trainingdata.\nWe learn relation prototypes as an implicit factor between entities, which\nreflects the meanings of relations as well as theirproximities for transfer\nlearning. Specifically, we construct a co-occurrence graph from texts, and\ncapture both first-order andsecond-order entity proximities for embedding\nlearning. Based on this, we further optimize the distance from entity pairs\ntocorresponding prototypes, which can be easily adapted to almost arbitrary RE\nframeworks. Thus, the learning of infrequent or evenunseen relation types will\nbenefit from semantically proximate relations through pairs of entities and\nlarge-scale textual information.We have conducted extensive experiments on two\npublicly available datasets: New York Times and Google Distant\nSupervision.Compared with eight state-of-the-art baselines, our proposed model\nachieves significant improvements (4.1% F1 on average). Furtherresults on\nlong-tail relations demonstrate the effectiveness of the learned relation\nprototypes. We further conduct an ablation study toinvestigate the impacts of\nvarying components, and apply it to four basic relation extraction models to\nverify the generalization ability.Finally, we analyze several example cases to\ngive intuitive impressions as qualitative analysis. Our codes will be released\nlater.", "published": "2020-11-27 06:21:12", "link": "http://arxiv.org/abs/2011.13574v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Automated Coding of Under-Studied Medical Concept Domains: Linking\n  Physical Activity Reports to the International Classification of Functioning,\n  Disability, and Health", "abstract": "Linking clinical narratives to standardized vocabularies and coding systems\nis a key component of unlocking the information in medical text for analysis.\nHowever, many domains of medical concepts lack well-developed terminologies\nthat can support effective coding of medical text. We present a framework for\ndeveloping natural language processing (NLP) technologies for automated coding\nof under-studied types of medical information, and demonstrate its\napplicability via a case study on physical mobility function. Mobility is a\ncomponent of many health measures, from post-acute care and surgical outcomes\nto chronic frailty and disability, and is coded in the International\nClassification of Functioning, Disability, and Health (ICF). However, mobility\nand other types of functional activity remain under-studied in medical\ninformatics, and neither the ICF nor commonly-used medical terminologies\ncapture functional status terminology in practice. We investigated two\ndata-driven paradigms, classification and candidate selection, to link\nnarrative observations of mobility to standardized ICF codes, using a dataset\nof clinical narratives from physical therapy encounters. Recent advances in\nlanguage modeling and word embedding were used as features for established\nmachine learning models and a novel deep learning approach, achieving a macro\nF-1 score of 84% on linking mobility activity reports to ICF codes. Both\nclassification and candidate selection approaches present distinct strengths\nfor automated coding in under-studied domains, and we highlight that the\ncombination of (i) a small annotated data set; (ii) expert definitions of codes\nof interest; and (iii) a representative text corpus is sufficient to produce\nhigh-performing automated coding systems. This study has implications for the\nongoing growth of NLP tools for a variety of specialized applications in\nclinical care and research.", "published": "2020-11-27 20:02:59", "link": "http://arxiv.org/abs/2011.13978v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Machine Learning to study the impact of gender-based violence in the\n  news media", "abstract": "While it remains a taboo topic, gender-based violence (GBV) undermines the\nhealth, dignity, security and autonomy of its victims. Many factors have been\nstudied to generate or maintain this kind of violence, however, the influence\nof the media is still uncertain. Here, we use Machine Learning tools to\nextrapolate the effect of the news in GBV. By feeding neural networks with\nnews, the topic information associated with each article can be recovered. Our\nfindings show a relationship between GBV news and public awareness, the effect\nof mediatic GBV cases, and the intrinsic thematic relationship of GBV news.\nBecause the used neural model can be easily adjusted, this also allows us to\nextend our approach to other media sources or topics", "published": "2020-11-27 08:42:33", "link": "http://arxiv.org/abs/2012.07490v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "A Survey of Deep Learning Approaches for OCR and Document Understanding", "abstract": "Documents are a core part of many businesses in many fields such as law,\nfinance, and technology among others. Automatic understanding of documents such\nas invoices, contracts, and resumes is lucrative, opening up many new avenues\nof business. The fields of natural language processing and computer vision have\nseen tremendous progress through the development of deep learning such that\nthese methods have started to become infused in contemporary document\nunderstanding systems. In this survey paper, we review different techniques for\ndocument understanding for documents written in English and consolidate\nmethodologies present in literature to act as a jumping-off point for\nresearchers exploring this area.", "published": "2020-11-27 03:05:59", "link": "http://arxiv.org/abs/2011.13534v2", "categories": ["cs.CL", "cs.CV", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Deep Active Learning for Sequence Labeling Based on Diversity and\n  Uncertainty in Gradient", "abstract": "Recently, several studies have investigated active learning (AL) for natural\nlanguage processing tasks to alleviate data dependency. However, for query\nselection, most of these studies mainly rely on uncertainty-based sampling,\nwhich generally does not exploit the structural information of the unlabeled\ndata. This leads to a sampling bias in the batch active learning setting, which\nselects several samples at once. In this work, we demonstrate that the amount\nof labeled training data can be reduced using active learning when it\nincorporates both uncertainty and diversity in the sequence labeling task. We\nexamined the effects of our sequence-based approach by selecting weighted\ndiverse in the gradient embedding approach across multiple tasks, datasets,\nmodels, and consistently outperform classic uncertainty-based sampling and\ndiversity-based sampling.", "published": "2020-11-27 06:03:27", "link": "http://arxiv.org/abs/2011.13570v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Relation Clustering in Narrative Knowledge Graphs", "abstract": "When coping with literary texts such as novels or short stories, the\nextraction of structured information in the form of a knowledge graph might be\nhindered by the huge number of possible relations between the entities\ncorresponding to the characters in the novel and the consequent hurdles in\ngathering supervised information about them. Such issue is addressed here as an\nunsupervised task empowered by transformers: relational sentences in the\noriginal text are embedded (with SBERT) and clustered in order to merge\ntogether semantically similar relations. All the sentences in the same cluster\nare finally summarized (with BART) and a descriptive label extracted from the\nsummary. Preliminary tests show that such clustering might successfully detect\nsimilar relations, and provide a valuable preprocessing for semi-supervised\napproaches.", "published": "2020-11-27 10:43:04", "link": "http://arxiv.org/abs/2011.13647v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Discriminatory Expressions to Produce Interpretable Models in Short\n  Documents", "abstract": "Social Networking Sites (SNS) are one of the most important ways of\ncommunication. In particular, microblogging sites are being used as analysis\navenues due to their peculiarities (promptness, short texts...). There are\ncountless researches that use SNS in novel manners, but machine learning has\nfocused mainly in classification performance rather than interpretability\nand/or other goodness metrics. Thus, state-of-the-art models are black boxes\nthat should not be used to solve problems that may have a social impact. When\nthe problem requires transparency, it is necessary to build interpretable\npipelines. Although the classifier may be interpretable, resulting models are\ntoo complex to be considered comprehensible, making it impossible for humans to\nunderstand the actual decisions. This paper presents a feature selection\nmechanism that is able to improve comprehensibility by using less but more\nmeaningful features while achieving good performance in microblogging contexts\nwhere interpretability is mandatory. Moreover, we present a ranking method to\nevaluate features in terms of statistical relevance and bias. We conducted\nexhaustive tests with five different datasets in order to evaluate\nclassification performance, generalisation capacity and complexity of the\nmodel. Results show that our proposal is better and the most stable one in\nterms of accuracy, generalisation and comprehensibility.", "published": "2020-11-27 19:00:50", "link": "http://arxiv.org/abs/2012.02104v2", "categories": ["cs.SI", "cs.AI", "cs.CL", "cs.LG", "I.2; I.7"], "primary_category": "cs.SI"}
{"title": "Regularizing Recurrent Neural Networks via Sequence Mixup", "abstract": "In this paper, we extend a class of celebrated regularization techniques\noriginally proposed for feed-forward neural networks, namely Input Mixup (Zhang\net al., 2017) and Manifold Mixup (Verma et al., 2018), to the realm of\nRecurrent Neural Networks (RNN). Our proposed methods are easy to implement and\nhave a low computational complexity, while leverage the performance of simple\nneural architectures in a variety of tasks. We have validated our claims\nthrough several experiments on real-world datasets, and also provide an\nasymptotic theoretical analysis to further investigate the properties and\npotential impacts of our proposed techniques. Applying sequence mixup to\nBiLSTM-CRF model (Huang et al., 2015) to Named Entity Recognition task on\nCoNLL-2003 data (Sang and De Meulder, 2003) has improved the F-1 score on the\ntest stage and reduced the loss, considerably.", "published": "2020-11-27 05:43:40", "link": "http://arxiv.org/abs/2012.07527v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Rethinking Generalization in American Sign Language Prediction for Edge\n  Devices with Extremely Low Memory Footprint", "abstract": "Due to the boom in technical compute in the last few years, the world has\nseen massive advances in artificially intelligent systems solving diverse\nreal-world problems. But a major roadblock in the ubiquitous acceptance of\nthese models is their enormous computational complexity and memory footprint.\nHence efficient architectures and training techniques are required for\ndeployment on extremely low resource inference endpoints. This paper proposes\nan architecture for detection of alphabets in American Sign Language on an ARM\nCortex-M7 microcontroller having just 496 KB of framebuffer RAM. Leveraging\nparameter quantization is a common technique that might cause varying drops in\ntest accuracy. This paper proposes using interpolation as augmentation amongst\nother techniques as an efficient method of reducing this drop, which also helps\nthe model generalize well to previously unseen noisy data. The proposed model\nis about 185 KB post-quantization and inference speed is 20 frames per second.", "published": "2020-11-27 14:05:42", "link": "http://arxiv.org/abs/2011.13741v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.HC", "68T45, 68T10, 68T07, 68U10", "I.2.10; I.4.8; I.5.1; J.3; I.4.1; K.4.2"], "primary_category": "cs.LG"}
{"title": "Transformer-based Online Speech Recognition with Decoder-end Adaptive\n  Computation Steps", "abstract": "Transformer-based end-to-end (E2E) automatic speech recognition (ASR) systems\nhave recently gained wide popularity, and are shown to outperform E2E models\nbased on recurrent structures on a number of ASR tasks. However, like other E2E\nmodels, Transformer ASR also requires the full input sequence for calculating\nthe attentions on both encoder and decoder, leading to increased latency and\nposing a challenge for online ASR. The paper proposes Decoder-end Adaptive\nComputation Steps (DACS) algorithm to address the issue of latency and\nfacilitate online ASR. The proposed algorithm streams the decoding of\nTransformer ASR by triggering an output after the confidence acquired from the\nencoder states reaches a certain threshold. Unlike other monotonic attention\nmechanisms that risk visiting the entire encoder states for each output step,\nthe paper introduces a maximum look-ahead step into the DACS algorithm to\nprevent from reaching the end of speech too fast. A Chunkwise encoder is\nadopted in our system to handle real-time speech inputs. The proposed online\nTransformer ASR system has been evaluated on Wall Street Journal (WSJ) and\nAIShell-1 datasets, yielding 5.5% word error rate (WER) and 7.1% character\nerror rate (CER) respectively, with only a minor decay in performance when\ncompared to the offline systems.", "published": "2020-11-27 17:02:55", "link": "http://arxiv.org/abs/2011.13834v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Numerical and experimental study of tonal noise sources at the outlet of\n  an isolated centrifugal fan", "abstract": "In this study, tonal noise produced by an isolated centrifugal fan is\ninvestigated using unsteady Reynolds-averaged Navier-Stokes (URANS) equations.\nThis type of fans is used in ventilation systems. As the fan propagates tonal\nnoise in the system, it can severely affect the life quality of people that\nreside in the buildings. Our simulation shows that turbulence kinetic energy\n(TKE) is unevenly distributed around the rotation axis. Large TKE exists near\nthe shroud at the pressure sides of the blades. It is caused by the\nrecirculating flow. Moreover, the position of the largest TKE periodically\nvaries among the blades. The period corresponds to approximately 4 times the\nfan rotation period, it was also found in acoustic measurements. The magnitude\nof the tonal noise at the blade passing frequencies agrees well with\nexperimental data. By analyzing the wall-pressure fluctuations, it is found\nthat the recirculating flow regions with large TKE are dominant sources of the\ntonal noise.", "published": "2020-11-27 10:38:19", "link": "http://arxiv.org/abs/2011.13645v1", "categories": ["cs.CE", "cs.SD", "eess.AS"], "primary_category": "cs.CE"}
