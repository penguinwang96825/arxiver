{"title": "FADO: Feedback-Aware Double COntrolling Network for Emotional Support\n  Conversation", "abstract": "Emotional Support Conversation (ESConv) aims to reduce help-seekers'emotional\ndistress with the supportive strategy and response. It is essential for the\nsupporter to select an appropriate strategy with the feedback of the\nhelp-seeker (e.g., emotion change during dialog turns, etc) in ESConv. However,\nprevious methods mainly focus on the dialog history to select the strategy and\nignore the help-seeker's feedback, leading to the wrong and user-irrelevant\nstrategy prediction. In addition, these approaches only model the\ncontext-to-strategy flow and pay less attention to the strategy-to-context flow\nthat can focus on the strategy-related context for generating the\nstrategy-constrain response. In this paper, we propose a Feedback-Aware Double\nCOntrolling Network (FADO) to make a strategy schedule and generate the\nsupportive response. The core module in FADO consists of a dual-level feedback\nstrategy selector and a double control reader. Specifically, the dual-level\nfeedback strategy selector leverages the turn-level and conversation-level\nfeedback to encourage or penalize strategies. The double control reader\nconstructs the novel strategy-to-context flow for generating the\nstrategy-constrain response. Furthermore, a strategy dictionary is designed to\nenrich the semantic information of the strategy and improve the quality of\nstrategy-constrain response. Experimental results on ESConv show that the\nproposed FADO has achieved the state-of-the-art performance in terms of both\nstrategy selection and response generation. Our code is available at\nhttps://github.com/Thedatababbler/FADO.", "published": "2022-11-01 03:37:30", "link": "http://arxiv.org/abs/2211.00250v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CARE: Causality Reasoning for Empathetic Responses by Conditional Graph\n  Generation", "abstract": "Recent approaches to empathetic response generation incorporate emotion\ncausalities to enhance comprehension of both the user's feelings and\nexperiences. However, these approaches suffer from two critical issues. First,\nthey only consider causalities between the user's emotion and the user's\nexperiences, and ignore those between the user's experiences. Second, they\nneglect interdependence among causalities and reason them independently. To\nsolve the above problems, we expect to reason all plausible causalities\ninterdependently and simultaneously, given the user's emotion, dialogue\nhistory, and future dialogue content. Then, we infuse these causalities into\nresponse generation for empathetic responses. Specifically, we design a new\nmodel, i.e., the Conditional Variational Graph Auto-Encoder (CVGAE), for the\ncausality reasoning, and adopt a multi-source attention mechanism in the\ndecoder for the causality infusion. We name the whole framework as CARE,\nabbreviated for CAusality Reasoning for Empathetic conversation. Experimental\nresults indicate that our method achieves state-of-the-art performance.", "published": "2022-11-01 03:45:26", "link": "http://arxiv.org/abs/2211.00255v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FRSUM: Towards Faithful Abstractive Summarization via Enhancing Factual\n  Robustness", "abstract": "Despite being able to generate fluent and grammatical text, current Seq2Seq\nsummarization models still suffering from the unfaithful generation problem. In\nthis paper, we study the faithfulness of existing systems from a new\nperspective of factual robustness which is the ability to correctly generate\nfactual information over adversarial unfaithful information. We first measure a\nmodel's factual robustness by its success rate to defend against adversarial\nattacks when generating factual information. The factual robustness analysis on\na wide range of current systems shows its good consistency with human judgments\non faithfulness. Inspired by these findings, we propose to improve the\nfaithfulness of a model by enhancing its factual robustness. Specifically, we\npropose a novel training strategy, namely FRSUM, which teaches the model to\ndefend against both explicit adversarial samples and implicit factual\nadversarial perturbations. Extensive automatic and human evaluation results\nshow that FRSUM consistently improves the faithfulness of various Seq2Seq\nmodels, such as T5, BART.", "published": "2022-11-01 06:09:00", "link": "http://arxiv.org/abs/2211.00294v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "E2E Refined Dataset", "abstract": "Although the well-known MR-to-text E2E dataset has been used by many\nresearchers, its MR-text pairs include many deletion/insertion/substitution\nerrors. Since such errors affect the quality of MR-to-text systems, they must\nbe fixed as much as possible. Therefore, we developed a refined dataset and\nsome python programs that convert the original E2E dataset into a refined\ndataset.", "published": "2022-11-01 15:01:20", "link": "http://arxiv.org/abs/2211.00513v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Natural Language Deduction with Incomplete Information", "abstract": "A growing body of work studies how to answer a question or verify a claim by\ngenerating a natural language \"proof\": a chain of deductive inferences yielding\nthe answer based on a set of premises. However, these methods can only make\nsound deductions when they follow from evidence that is given. We propose a new\nsystem that can handle the underspecified setting where not all premises are\nstated at the outset; that is, additional assumptions need to be materialized\nto prove a claim. By using a natural language generation model to abductively\ninfer a premise given another premise and a conclusion, we can impute missing\npieces of evidence needed for the conclusion to be true. Our system searches\nover two fringes in a bidirectional fashion, interleaving deductive\n(forward-chaining) and abductive (backward-chaining) generation steps. We\nsample multiple possible outputs for each step to achieve coverage of the\nsearch space, at the same time ensuring correctness by filtering low-quality\ngenerations with a round-trip validation procedure. Results on a modified\nversion of the EntailmentBank dataset and a new dataset called Everyday Norms:\nWhy Not? show that abductive generation with validation can recover premises\nacross in- and out-of-domain settings.", "published": "2022-11-01 17:27:55", "link": "http://arxiv.org/abs/2211.00614v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Inter-character Relationship-driven Story Generation", "abstract": "In this paper, we introduce the task of modeling interpersonal relationships\nfor story generation. For addressing this task, we propose Relationships as\nLatent Variables for Story Generation, (ReLiSt). ReLiSt generates stories\nsentence by sentence and has two major components - a relationship selector and\na story continuer. The relationship selector specifies a latent variable to\npick the relationship to exhibit in the next sentence and the story continuer\ngenerates the next sentence while expressing the selected relationship in a\ncoherent way. Our automatic and human evaluations demonstrate that ReLiSt is\nable to generate stories with relationships that are more faithful to desired\nrelationships while maintaining the content quality. The relationship\nassignments to sentences during inference bring interpretability to ReLiSt.", "published": "2022-11-01 18:04:29", "link": "http://arxiv.org/abs/2211.00676v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic Pivoting Model for Effective Event Detection", "abstract": "Event Detection, which aims to identify and classify mentions of event\ninstances from unstructured articles, is an important task in Natural Language\nProcessing (NLP). Existing techniques for event detection only use homogeneous\none-hot vectors to represent the event type classes, ignoring the fact that the\nsemantic meaning of the types is important to the task. Such an approach is\ninefficient and prone to overfitting. In this paper, we propose a Semantic\nPivoting Model for Effective Event Detection (SPEED), which explicitly\nincorporates prior information during training and captures semantically\nmeaningful correlations between input and events. Experimental results show\nthat our proposed model achieves state-of-the-art performance and outperforms\nthe baselines in multiple settings without using any external resources.", "published": "2022-11-01 19:20:34", "link": "http://arxiv.org/abs/2211.00709v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Training Vision-Language Models with Less Bimodal Supervision", "abstract": "Standard practice in pretraining multimodal models, such as vision-language\nmodels, is to rely on pairs of aligned inputs from both modalities, for\nexample, aligned image-text pairs. However, such pairs can be difficult to\nobtain in low-resource settings and for some modality pairs (e.g., structured\ntables and images). In this work, we investigate the extent to which we can\nreduce the reliance on such parallel data, which we term \\emph{bimodal\nsupervision}, and use models that are pretrained on each modality\nindependently. We experiment with a high-performing vision-language model, and\nanalyze the effect of bimodal supervision on three vision-language tasks. We\nfind that on simpler tasks, such as VQAv2 and GQA, one can eliminate bimodal\nsupervision completely, suffering only a minor loss in performance. Conversely,\nfor NLVR2, which requires more complex reasoning, training without bimodal\nsupervision leads to random performance. Nevertheless, using only 5\\% of the\nbimodal data (142K images along with their captions), or leveraging weak\nsupervision in the form of a list of machine-generated labels for each image,\nleads to only a moderate degradation compared to using 3M image-text pairs:\n74\\%$\\rightarrow$$\\sim$70\\%. Our code is available at\nhttps://github.com/eladsegal/less-bimodal-sup.", "published": "2022-11-01 04:07:11", "link": "http://arxiv.org/abs/2211.00262v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "CONDAQA: A Contrastive Reading Comprehension Dataset for Reasoning about\n  Negation", "abstract": "The full power of human language-based communication cannot be realized\nwithout negation. All human languages have some form of negation. Despite this,\nnegation remains a challenging phenomenon for current natural language\nunderstanding systems. To facilitate the future development of models that can\nprocess negation effectively, we present CONDAQA, the first English reading\ncomprehension dataset which requires reasoning about the implications of\nnegated statements in paragraphs. We collect paragraphs with diverse negation\ncues, then have crowdworkers ask questions about the implications of the\nnegated statement in the passage. We also have workers make three kinds of\nedits to the passage -- paraphrasing the negated statement, changing the scope\nof the negation, and reversing the negation -- resulting in clusters of\nquestion-answer pairs that are difficult for models to answer with spurious\nshortcuts. CONDAQA features 14,182 question-answer pairs with over 200 unique\nnegation cues and is challenging for current state-of-the-art models. The best\nperforming model on CONDAQA (UnifiedQA-v2-3b) achieves only 42% on our\nconsistency metric, well below human performance which is 81%. We release our\ndataset, along with fully-finetuned, few-shot, and zero-shot evaluations, to\nfacilitate the development of future NLP methods that work on negated language.", "published": "2022-11-01 06:10:26", "link": "http://arxiv.org/abs/2211.00295v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Recognizing Nested Entities from Flat Supervision: A New NER Subtask,\n  Feasibility and Challenges", "abstract": "Many recent named entity recognition (NER) studies criticize flat NER for its\nnon-overlapping assumption, and switch to investigating nested NER. However,\nexisting nested NER models heavily rely on training data annotated with nested\nentities, while labeling such data is costly. This study proposes a new\nsubtask, nested-from-flat NER, which corresponds to a realistic application\nscenario: given data annotated with flat entities only, one may still desire\nthe trained model capable of recognizing nested entities. To address this task,\nwe train span-based models and deliberately ignore the spans nested inside\nlabeled entities, since these spans are possibly unlabeled entities. With\nnested entities removed from the training data, our model achieves 54.8%, 54.2%\nand 41.1% F1 scores on the subset of spans within entities on ACE 2004, ACE\n2005 and GENIA, respectively. This suggests the effectiveness of our approach\nand the feasibility of the task. In addition, the model's performance on flat\nentities is entirely unaffected. We further manually annotate the nested\nentities in the test set of CoNLL 2003, creating a nested-from-flat NER\nbenchmark. Analysis results show that the main challenges stem from the data\nand annotation inconsistencies between the flat and nested entities.", "published": "2022-11-01 06:41:42", "link": "http://arxiv.org/abs/2211.00301v1", "categories": ["cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "A General Search-based Framework for Generating Textual Counterfactual\n  Explanations", "abstract": "One of the prominent methods for explaining the decision of a\nmachine-learning classifier is by a counterfactual example. Most current\nalgorithms for generating such examples in the textual domain are based on\ngenerative language models. Generative models, however, are trained to minimize\na specific loss function in order to fulfill certain requirements for the\ngenerated texts. Any change in the requirements may necessitate costly\nretraining, thus potentially limiting their applicability. In this paper, we\npresent a general search-based framework for generating counterfactual\nexplanations in the textual domain. Our framework is model-agnostic,\ndomain-agnostic, anytime, and does not require retraining in order to adapt to\nchanges in the user requirements. We model the task as a search problem in a\nspace where the initial state is the classified text, and the goal state is a\ntext in a given target class. Our framework includes domain-independent\nmodification operators, but can also exploit domain-specific knowledge through\nspecialized operators. The search algorithm attempts to find a text from the\ntarget class with minimal user-specified distance from the original classified\nobject.", "published": "2022-11-01 10:34:27", "link": "http://arxiv.org/abs/2211.00369v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "VarMAE: Pre-training of Variational Masked Autoencoder for\n  Domain-adaptive Language Understanding", "abstract": "Pre-trained language models have achieved promising performance on general\nbenchmarks, but underperform when migrated to a specific domain. Recent works\nperform pre-training from scratch or continual pre-training on domain corpora.\nHowever, in many specific domains, the limited corpus can hardly support\nobtaining precise representations. To address this issue, we propose a novel\nTransformer-based language model named VarMAE for domain-adaptive language\nunderstanding. Under the masked autoencoding objective, we design a context\nuncertainty learning module to encode the token's context into a smooth latent\ndistribution. The module can produce diverse and well-formed contextual\nrepresentations. Experiments on science- and finance-domain NLU tasks\ndemonstrate that VarMAE can be efficiently adapted to new domains with limited\nresources.", "published": "2022-11-01 12:51:51", "link": "http://arxiv.org/abs/2211.00430v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Leveraging Graph-based Cross-modal Information Fusion for Neural Sign\n  Language Translation", "abstract": "Sign Language (SL), as the mother tongue of the deaf community, is a special\nvisual language that most hearing people cannot understand. In recent years,\nneural Sign Language Translation (SLT), as a possible way for bridging\ncommunication gap between the deaf and the hearing people, has attracted\nwidespread academic attention. We found that the current mainstream end-to-end\nneural SLT models, which tries to learning language knowledge in a weakly\nsupervised manner, could not mine enough semantic information under the\ncondition of low data resources. Therefore, we propose to introduce additional\nword-level semantic knowledge of sign language linguistics to assist in\nimproving current end-to-end neural SLT models. Concretely, we propose a novel\nneural SLT model with multi-modal feature fusion based on the dynamic graph, in\nwhich the cross-modal information, i.e. text and video, is first assembled as a\ndynamic graph according to their correlation, and then the graph is processed\nby a multi-modal graph encoder to generate the multi-modal embeddings for\nfurther usage in the subsequent neural translation models. To the best of our\nknowledge, we are the first to introduce graph neural networks, for fusing\nmulti-modal information, into neural sign language translation models.\nMoreover, we conducted experiments on a publicly available popular SLT dataset\nRWTH-PHOENIX-Weather-2014T. and the quantitative experiments show that our\nmethod can improve the model.", "published": "2022-11-01 15:26:22", "link": "http://arxiv.org/abs/2211.00526v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Two-stage LLM Fine-tuning with Less Specialization and More\n  Generalization", "abstract": "Pretrained large language models (LLMs) are general purpose problem solvers\napplicable to a diverse set of tasks with prompts. They can be further improved\ntowards a specific task by fine-tuning on a specialized dataset. However,\nfine-tuning usually makes the model narrowly specialized on this dataset with\nreduced general in-context learning performances, which is undesirable whenever\nthe fine-tuned model needs to handle additional tasks where no fine-tuning data\nis available. In this work, we first demonstrate that fine-tuning on a single\ntask indeed decreases LLMs' general in-context learning performance. We\ndiscover one important cause of such forgetting, format specialization, where\nthe model overfits to the format of the fine-tuned task.We further show that\nformat specialization happens at the very beginning of fine-tuning. To solve\nthis problem, we propose Prompt Tuning with MOdel Tuning (ProMoT), a simple yet\neffective two-stage fine-tuning framework that reduces format specialization\nand improves generalization.ProMoT offloads task-specific format learning into\nadditional and removable parameters by first doing prompt tuning and then\nfine-tuning the model itself with this soft prompt attached. With experiments\non several fine-tuning tasks and 8 in-context evaluation tasks, we show that\nProMoT achieves comparable performance on fine-tuned tasks to standard\nfine-tuning, but with much less loss of in-context learning performances across\na board range of out-of-domain evaluation tasks. More importantly, ProMoT can\neven enhance generalization on in-context learning tasks that are semantically\nrelated to the fine-tuned task, e.g. ProMoT on En-Fr translation significantly\nimproves performance on other language pairs, and ProMoT on NLI improves\nperformance on summarization. Experiments also show that ProMoT can improve the\ngeneralization performance of multi-task training.", "published": "2022-11-01 17:56:57", "link": "http://arxiv.org/abs/2211.00635v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TOE: A Grid-Tagging Discontinuous NER Model Enhanced by Embedding\n  Tag/Word Relations and More Fine-Grained Tags", "abstract": "So far, discontinuous named entity recognition (NER) has received increasing\nresearch attention and many related methods have surged such as\nhypergraph-based methods, span-based methods, and sequence-to-sequence\n(Seq2Seq) methods, etc. However, these methods more or less suffer from some\nproblems such as decoding ambiguity and efficiency, which limit their\nperformance. Recently, grid-tagging methods, which benefit from the flexible\ndesign of tagging systems and model architectures, have shown superiority to\nadapt for various information extraction tasks. In this paper, we follow the\nline of such methods and propose a competitive grid-tagging model for\ndiscontinuous NER. We call our model TOE because we incorporate two kinds of\nTag-Oriented Enhancement mechanisms into a state-of-the-art (SOTA) grid-tagging\nmodel that casts the NER problem into word-word relationship prediction. First,\nwe design a Tag Representation Embedding Module (TREM) to force our model to\nconsider not only word-word relationships but also word-tag and tag-tag\nrelationships. Concretely, we construct tag representations and embed them into\nTREM, so that TREM can treat tag and word representations as\nqueries/keys/values and utilize self-attention to model their relationships. On\nthe other hand, motivated by the Next-Neighboring-Word (NNW) and Tail-Head-Word\n(THW) tags in the SOTA model, we add two new symmetric tags, namely\nPrevious-Neighboring-Word (PNW) and Head-Tail-Word (HTW), to model more\nfine-grained word-word relationships and alleviate error propagation from tag\nprediction. In the experiments of three benchmark datasets, namely CADEC,\nShARe13 and ShARe14, our TOE model pushes the SOTA results by about 0.83%,\n0.05% and 0.66% in F1, demonstrating its effectiveness.", "published": "2022-11-01 18:17:49", "link": "http://arxiv.org/abs/2211.00684v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning to Solve Voxel Building Embodied Tasks from Pixels and Natural\n  Language Instructions", "abstract": "The adoption of pre-trained language models to generate action plans for\nembodied agents is a promising research strategy. However, execution of\ninstructions in real or simulated environments requires verification of the\nfeasibility of actions as well as their relevance to the completion of a goal.\nWe propose a new method that combines a language model and reinforcement\nlearning for the task of building objects in a Minecraft-like environment\naccording to the natural language instructions. Our method first generates a\nset of consistently achievable sub-goals from the instructions and then\ncompletes associated sub-tasks with a pre-trained RL policy. The proposed\nmethod formed the RL baseline at the IGLU 2022 competition.", "published": "2022-11-01 18:30:42", "link": "http://arxiv.org/abs/2211.00688v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Quantum Natural Language Generation on Near-Term Devices", "abstract": "The emergence of noisy medium-scale quantum devices has led to\nproof-of-concept applications for quantum computing in various domains.\nExamples include Natural Language Processing (NLP) where sentence\nclassification experiments have been carried out, as well as procedural\ngeneration, where tasks such as geopolitical map creation, and image\nmanipulation have been performed. We explore applications at the intersection\nof these two areas by designing a hybrid quantum-classical algorithm for\nsentence generation.\n  Our algorithm is based on the well-known simulated annealing technique for\ncombinatorial optimisation. An implementation is provided and used to\ndemonstrate successful sentence generation on both simulated and real quantum\nhardware. A variant of our algorithm can also be used for music generation.\n  This paper aims to be self-contained, introducing all the necessary\nbackground on NLP and quantum computing along the way.", "published": "2022-11-01 20:12:35", "link": "http://arxiv.org/abs/2211.00727v1", "categories": ["quant-ph", "cs.CL"], "primary_category": "quant-ph"}
{"title": "Why is Winoground Hard? Investigating Failures in Visuolinguistic\n  Compositionality", "abstract": "Recent visuolinguistic pre-trained models show promising progress on various\nend tasks such as image retrieval and video captioning. Yet, they fail\nmiserably on the recently proposed Winoground dataset, which challenges models\nto match paired images and English captions, with items constructed to overlap\nlexically but differ in meaning (e.g., \"there is a mug in some grass\" vs.\n\"there is some grass in a mug\"). By annotating the dataset using new\nfine-grained tags, we show that solving the Winoground task requires not just\ncompositional language understanding, but a host of other abilities like\ncommonsense reasoning or locating small, out-of-focus objects in low-resolution\nimages. In this paper, we identify the dataset's main challenges through a\nsuite of experiments on related tasks (probing task, image retrieval task),\ndata augmentation, and manual inspection of the dataset. Our analysis suggests\nthat a main challenge in visuolinguistic models may lie in fusing visual and\ntextual representations, rather than in compositional language understanding.\nWe release our annotation and code at\nhttps://github.com/ajd12342/why-winoground-hard .", "published": "2022-11-01 22:16:58", "link": "http://arxiv.org/abs/2211.00768v4", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "A Categorical Framework for Modeling with Stock and Flow Diagrams", "abstract": "Stock and flow diagrams are already an important tool in epidemiology, but\ncategory theory lets us go further and treat these diagrams as mathematical\nentities in their own right. In this chapter we use communicable disease models\ncreated with our software, StockFlow.jl, to explain the benefits of the\ncategorical approach. We first explain the category of stock-flow diagrams and\nnote the clear separation between the syntax of these diagrams and their\nsemantics, demonstrating three examples of semantics already implemented in the\nsoftware: ODEs, causal loop diagrams, and system structure diagrams. We then\nturn to two methods for building large stock-flow diagrams from smaller ones in\na modular fashion: composition and stratification. Finally, we introduce the\nopen-source ModelCollab software for diagram-based collaborative modeling. The\ngraphical user interface of this web-based software lets modelers take\nadvantage of the ideas discussed here without any knowledge of their\ncategorical foundations.", "published": "2022-11-01 16:15:54", "link": "http://arxiv.org/abs/2211.01290v3", "categories": ["cs.LO", "cs.CL"], "primary_category": "cs.LO"}
{"title": "Envisioning a Human-AI collaborative system to transform policies into\n  decision models", "abstract": "Regulations govern many aspects of citizens' daily lives. Governments and\nbusinesses routinely automate these in the form of coded rules (e.g., to check\na citizen's eligibility for specific benefits). However, the path to automation\nis long and challenging. To address this, recent global initiatives for digital\ngovernment, proposing to simultaneously express policy in natural language for\nhuman consumption as well as computationally amenable rules or code, are\ngathering broad public-sector interest. We introduce the problem of\nsemi-automatically building decision models from eligibility policies for\nsocial services, and present an initial emerging approach to shorten the route\nfrom policy documents to executable, interpretable and standardised decision\nmodels using AI, NLP and Knowledge Graphs. Despite the many open domain\nchallenges, in this position paper we explore the enormous potential of AI to\nassist government agencies and policy experts in scaling the production of both\nhuman-readable and machine executable policy rules, while improving\ntransparency, interpretability, traceability and accountability of the decision\nmaking.", "published": "2022-11-01 18:29:48", "link": "http://arxiv.org/abs/2212.06882v1", "categories": ["cs.AI", "cs.CL", "68T30", "H.4"], "primary_category": "cs.AI"}
{"title": "Why Is It Hate Speech? Masked Rationale Prediction for Explainable Hate\n  Speech Detection", "abstract": "In a hate speech detection model, we should consider two critical aspects in\naddition to detection performance-bias and explainability. Hate speech cannot\nbe identified based solely on the presence of specific words: the model should\nbe able to reason like humans and be explainable. To improve the performance\nconcerning the two aspects, we propose Masked Rationale Prediction (MRP) as an\nintermediate task. MRP is a task to predict the masked human\nrationales-snippets of a sentence that are grounds for human judgment-by\nreferring to surrounding tokens combined with their unmasked rationales. As the\nmodel learns its reasoning ability based on rationales by MRP, it performs hate\nspeech detection robustly in terms of bias and explainability. The proposed\nmethod generally achieves state-of-the-art performance in various metrics,\ndemonstrating its effectiveness for hate speech detection.", "published": "2022-11-01 03:16:36", "link": "http://arxiv.org/abs/2211.00243v1", "categories": ["cs.CL", "cs.AI", "cs.SI", "68T50 (Primary), 91F20 (Secondary)"], "primary_category": "cs.CL"}
{"title": "Speech-text based multi-modal training with bidirectional attention for\n  improved speech recognition", "abstract": "To let the state-of-the-art end-to-end ASR model enjoy data efficiency, as\nwell as much more unpaired text data by multi-modal training, one needs to\naddress two problems: 1) the synchronicity of feature sampling rates between\nspeech and language (aka text data); 2) the homogeneity of the learned\nrepresentations from two encoders. In this paper we propose to employ a novel\nbidirectional attention mechanism (BiAM) to jointly learn both ASR encoder\n(bottom layers) and text encoder with a multi-modal learning method. The BiAM\nis to facilitate feature sampling rate exchange, realizing the quality of the\ntransformed features for the one kind to be measured in another space, with\ndiversified objective functions. As a result, the speech representations are\nenriched with more linguistic information, while the representations generated\nby the text encoder are more similar to corresponding speech ones, and\ntherefore the shared ASR models are more amenable for unpaired text data\npretraining. To validate the efficacy of the proposed method, we perform two\ncategories of experiments with or without extra unpaired text data.\nExperimental results on Librispeech corpus show it can achieve up to 6.15% word\nerror rate reduction (WERR) with only paired data learning, while 9.23% WERR\nwhen more unpaired text data is employed.", "published": "2022-11-01 08:25:11", "link": "http://arxiv.org/abs/2211.00325v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Investigating Content-Aware Neural Text-To-Speech MOS Prediction Using\n  Prosodic and Linguistic Features", "abstract": "Current state-of-the-art methods for automatic synthetic speech evaluation\nare based on MOS prediction neural models. Such MOS prediction models include\nMOSNet and LDNet that use spectral features as input, and SSL-MOS that relies\non a pretrained self-supervised learning model that directly uses the speech\nsignal as input. In modern high-quality neural TTS systems, prosodic\nappropriateness with regard to the spoken content is a decisive factor for\nspeech naturalness. For this reason, we propose to include prosodic and\nlinguistic features as additional inputs in MOS prediction systems, and\nevaluate their impact on the prediction outcome. We consider phoneme level F0\nand duration features as prosodic inputs, as well as Tacotron encoder outputs,\nPOS tags and BERT embeddings as higher-level linguistic inputs. All MOS\nprediction systems are trained on SOMOS, a neural TTS-only dataset with\ncrowdsourced naturalness MOS evaluations. Results show that the proposed\nadditional features are beneficial in the MOS prediction task, by improving the\npredicted MOS scores' correlation with the ground truths, both at\nutterance-level and system-level predictions.", "published": "2022-11-01 09:18:50", "link": "http://arxiv.org/abs/2211.00342v2", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Generating Multilingual Gender-Ambiguous Text-to-Speech Voices", "abstract": "The gender of any voice user interface is a key element of its perceived\nidentity. Recently, there has been increasing interest in interfaces where the\ngender is ambiguous rather than clearly identifying as female or male. This\nwork addresses the task of generating novel gender-ambiguous TTS voices in a\nmulti-speaker, multilingual setting. This is accomplished by efficiently\nsampling from a latent speaker embedding space using a proposed gender-aware\nmethod. Extensive objective and subjective evaluations clearly indicate that\nthis method is able to efficiently generate a range of novel, diverse voices\nthat are consistent and perceived as more gender-ambiguous than a baseline\nvoice across all the languages examined. Interestingly, the gender perception\nis found to be robust across two demographic factors of the listeners: native\nlanguage and gender. To our knowledge, this is the first systematic and\nvalidated approach that can reliably generate a variety of gender-ambiguous\nvoices.", "published": "2022-11-01 10:40:24", "link": "http://arxiv.org/abs/2211.00375v3", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The future is different: Large pre-trained language models fail in\n  prediction tasks", "abstract": "Large pre-trained language models (LPLM) have shown spectacular success when\nfine-tuned on downstream supervised tasks. Yet, it is known that their\nperformance can drastically drop when there is a distribution shift between the\ndata used during training and that used at inference time. In this paper we\nfocus on data distributions that naturally change over time and introduce four\nnew REDDIT datasets, namely the WALLSTREETBETS, ASKSCIENCE, THE DONALD, and\nPOLITICS sub-reddits. First, we empirically demonstrate that LPLM can display\naverage performance drops of about 88% (in the best case!) when predicting the\npopularity of future posts from sub-reddits whose topic distribution changes\nwith time. We then introduce a simple methodology that leverages neural\nvariational dynamic topic models and attention mechanisms to infer temporal\nlanguage model representations for regression tasks. Our models display\nperformance drops of only about 40% in the worst cases (2% in the best ones)\nwhen predicting the popularity of future posts, while using only about 7% of\nthe total number of parameters of LPLM and providing interpretable\nrepresentations that offer insight into real-world events, like the GameStop\nshort squeeze of 2021", "published": "2022-11-01 11:01:36", "link": "http://arxiv.org/abs/2211.00384v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Order-sensitive Neural Constituency Parsing", "abstract": "We propose a novel algorithm that improves on the previous neural span-based\nCKY decoder for constituency parsing. In contrast to the traditional span-based\ndecoding, where spans are combined only based on the sum of their scores, we\nintroduce an order-sensitive strategy, where the span combination scores are\nmore carefully derived from an order-sensitive basis. Our decoder can be\nregarded as a generalization over existing span-based decoder in determining a\nfiner-grain scoring scheme for the combination of lower-level spans into\nhigher-level spans, where we emphasize on the order of the lower-level spans\nand use order-sensitive span scores as well as order-sensitive combination\ngrammar rule scores to enhance prediction accuracy. We implement the proposed\ndecoding strategy harnessing GPU parallelism and achieve a decoding speed on\npar with state-of-the-art span-based parsers. Using the previous\nstate-of-the-art model without additional data as our baseline, we outperform\nit and improve the F1 score on the Penn Treebank Dataset by 0.26% and on the\nChinese Treebank Dataset by 0.35%.", "published": "2022-11-01 12:31:30", "link": "http://arxiv.org/abs/2211.00421v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TrimTail: Low-Latency Streaming ASR with Simple but Effective\n  Spectrogram-Level Length Penalty", "abstract": "In this paper, we present TrimTail, a simple but effective emission\nregularization method to improve the latency of streaming ASR models. The core\nidea of TrimTail is to apply length penalty (i.e., by trimming trailing frames,\nsee Fig. 1-(b)) directly on the spectrogram of input utterances, which does not\nrequire any alignment. We demonstrate that TrimTail is computationally cheap\nand can be applied online and optimized with any training loss or any model\narchitecture on any dataset without any extra effort by applying it on various\nend-to-end streaming ASR networks either trained with CTC loss [1] or\nTransducer loss [2]. We achieve 100 $\\sim$ 200ms latency reduction with equal\nor even better accuracy on both Aishell-1 and Librispeech. Moreover, by using\nTrimTail, we can achieve a 400ms algorithmic improvement of User Sensitive\nDelay (USD) with an accuracy loss of less than 0.2.", "published": "2022-11-01 15:12:34", "link": "http://arxiv.org/abs/2211.00522v2", "categories": ["cs.SD", "cs.CL", "eess.AS", "I.2.7"], "primary_category": "cs.SD"}
{"title": "Learning utterance-level representations through token-level acoustic\n  latents prediction for Expressive Speech Synthesis", "abstract": "This paper proposes an Expressive Speech Synthesis model that utilizes\ntoken-level latent prosodic variables in order to capture and control\nutterance-level attributes, such as character acting voice and speaking style.\nCurrent works aim to explicitly factorize such fine-grained and utterance-level\nspeech attributes into different representations extracted by modules that\noperate in the corresponding level. We show that the fine-grained latent space\nalso captures coarse-grained information, which is more evident as the\ndimension of latent space increases in order to capture diverse prosodic\nrepresentations. Therefore, a trade-off arises between the diversity of the\ntoken-level and utterance-level representations and their disentanglement. We\nalleviate this issue by first capturing rich speech attributes into a\ntoken-level latent space and then, separately train a prior network that given\nthe input text, learns utterance-level representations in order to predict the\nphoneme-level, posterior latents extracted during the previous step. Both\nqualitative and quantitative evaluations are used to demonstrate the\neffectiveness of the proposed approach. Audio samples are available in our demo\npage.", "published": "2022-11-01 15:17:25", "link": "http://arxiv.org/abs/2211.00523v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ClassActionPrediction: A Challenging Benchmark for Legal Judgment\n  Prediction of Class Action Cases in the US", "abstract": "The research field of Legal Natural Language Processing (NLP) has been very\nactive recently, with Legal Judgment Prediction (LJP) becoming one of the most\nextensively studied tasks. To date, most publicly released LJP datasets\noriginate from countries with civil law. In this work, we release, for the\nfirst time, a challenging LJP dataset focused on class action cases in the US.\nIt is the first dataset in the common law system that focuses on the harder and\nmore realistic task involving the complaints as input instead of the often used\nfacts summary written by the court. Additionally, we study the difficulty of\nthe task by collecting expert human predictions, showing that even human\nexperts can only reach 53% accuracy on this dataset. Our Longformer model\nclearly outperforms the human baseline (63%), despite only considering the\nfirst 2,048 tokens. Furthermore, we perform a detailed error analysis and find\nthat the Longformer model is significantly better calibrated than the human\nexperts. Finally, we publicly release the dataset and the code used for the\nexperiments.", "published": "2022-11-01 16:57:59", "link": "http://arxiv.org/abs/2211.00582v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE", "68T50", "I.2; I.7"], "primary_category": "cs.CL"}
{"title": "T5lephone: Bridging Speech and Text Self-supervised Models for Spoken\n  Language Understanding via Phoneme level T5", "abstract": "In Spoken language understanding (SLU), a natural solution is concatenating\npre-trained speech models (e.g. HuBERT) and pretrained language models (PLM,\ne.g. T5). Most previous works use pretrained language models with subword-based\ntokenization. However, the granularity of input units affects the alignment of\nspeech model outputs and language model inputs, and PLM with character-based\ntokenization is underexplored. In this work, we conduct extensive studies on\nhow PLMs with different tokenization strategies affect spoken language\nunderstanding task including spoken question answering (SQA) and speech\ntranslation (ST). We further extend the idea to create T5lephone(pronounced as\ntelephone), a variant of T5 that is pretrained using phonemicized text. We\ninitialize T5lephone with existing PLMs to pretrain it using relatively\nlightweight computational resources. We reached state-of-the-art on NMSQA, and\nthe T5lephone model exceeds T5 with other types of units on end-to-end SQA and\nST.", "published": "2022-11-01 17:00:23", "link": "http://arxiv.org/abs/2211.00586v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Interpretability in the Wild: a Circuit for Indirect Object\n  Identification in GPT-2 small", "abstract": "Research in mechanistic interpretability seeks to explain behaviors of\nmachine learning models in terms of their internal components. However, most\nprevious work either focuses on simple behaviors in small models, or describes\ncomplicated behaviors in larger models with broad strokes. In this work, we\nbridge this gap by presenting an explanation for how GPT-2 small performs a\nnatural language task called indirect object identification (IOI). Our\nexplanation encompasses 26 attention heads grouped into 7 main classes, which\nwe discovered using a combination of interpretability approaches relying on\ncausal interventions. To our knowledge, this investigation is the largest\nend-to-end attempt at reverse-engineering a natural behavior \"in the wild\" in a\nlanguage model. We evaluate the reliability of our explanation using three\nquantitative criteria--faithfulness, completeness and minimality. Though these\ncriteria support our explanation, they also point to remaining gaps in our\nunderstanding. Our work provides evidence that a mechanistic understanding of\nlarge ML models is feasible, opening opportunities to scale our understanding\nto both larger models and more complex tasks.", "published": "2022-11-01 17:08:44", "link": "http://arxiv.org/abs/2211.00593v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Unified End-to-End Speech Recognition and Endpointing for Fast and\n  Efficient Speech Systems", "abstract": "Automatic speech recognition (ASR) systems typically rely on an external\nendpointer (EP) model to identify speech boundaries. In this work, we propose a\nmethod to jointly train the ASR and EP tasks in a single end-to-end (E2E)\nmultitask model, improving EP quality by optionally leveraging information from\nthe ASR audio encoder. We introduce a \"switch\" connection, which trains the EP\nto consume either the audio frames directly or low-level latent representations\nfrom the ASR model. This results in a single E2E model that can be used during\ninference to perform frame filtering at low cost, and also make high quality\nend-of-query (EOQ) predictions based on ongoing ASR computation. We present\nresults on a voice search test set showing that, compared to separate\nsingle-task models, this approach reduces median endpoint latency by 120 ms\n(30.8% reduction), and 90th percentile latency by 170 ms (23.0% reduction),\nwithout regressing word error rate. For continuous recognition, WER improves by\n10.6% (relative).", "published": "2022-11-01 23:43:15", "link": "http://arxiv.org/abs/2211.00786v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "User-Entity Differential Privacy in Learning Natural Language Models", "abstract": "In this paper, we introduce a novel concept of user-entity differential\nprivacy (UeDP) to provide formal privacy protection simultaneously to both\nsensitive entities in textual data and data owners in learning natural language\nmodels (NLMs). To preserve UeDP, we developed a novel algorithm, called\nUeDP-Alg, optimizing the trade-off between privacy loss and model utility with\na tight sensitivity bound derived from seamlessly combining user and sensitive\nentity sampling processes. An extensive theoretical analysis and evaluation\nshow that our UeDP-Alg outperforms baseline approaches in model utility under\nthe same privacy budget consumption on several NLM tasks, using benchmark\ndatasets.", "published": "2022-11-01 16:54:23", "link": "http://arxiv.org/abs/2211.01141v2", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Evaluating Impact of Social Media Posts by Executives on Stock Prices", "abstract": "Predicting stock market movements has always been of great interest to\ninvestors and an active area of research. Research has proven that popularity\nof products is highly influenced by what people talk about. Social media like\nTwitter, Reddit have become hotspots of such influences. This paper\ninvestigates the impact of social media posts on close price prediction of\nstocks using Twitter and Reddit posts. Our objective is to integrate sentiment\nof social media data with historical stock data and study its effect on closing\nprices using time series models. We carried out rigorous experiments and deep\nanalysis using multiple deep learning based models on different datasets to\nstudy the influence of posts by executives and general people on the close\nprice. Experimental results on multiple stocks (Apple and Tesla) and\ndecentralised currencies (Bitcoin and Ethereum) consistently show improvements\nin prediction on including social media data and greater improvements on\nincluding executive posts.", "published": "2022-11-01 03:45:17", "link": "http://arxiv.org/abs/2211.01287v2", "categories": ["q-fin.ST", "cs.CL", "cs.IR", "cs.SI", "I.7"], "primary_category": "q-fin.ST"}
{"title": "Avoid Overthinking in Self-Supervised Models for Speech Recognition", "abstract": "Self-supervised learning (SSL) models reshaped our approach to speech,\nlanguage and vision. However their huge size and the opaque relations between\ntheir layers and tasks result in slow inference and network overthinking, where\npredictions made from the last layer of large models is worse than those made\nfrom intermediate layers. Early exit (EE) strategies can solve both issues by\ndynamically reducing computations at inference time for certain samples.\nAlthough popular for classification tasks in vision and language, EE has seen\nless use for sequence-to-sequence speech recognition (ASR) tasks where outputs\nfrom early layers are often degenerate. This challenge is further compounded\nwhen speech SSL models are applied on out-of-distribution (OOD) data. This\npaper first shows that SSL models do overthinking in ASR. We then motivate\nfurther research in EE by computing an optimal bound for performance versus\nspeed trade-offs. To approach this bound we propose two new strategies for ASR:\n(1) we adapt the recently proposed patience strategy to ASR; and (2) we design\na new EE strategy specific to ASR that performs better than all strategies\npreviously introduced.", "published": "2022-11-01 15:26:46", "link": "http://arxiv.org/abs/2211.08989v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "CCS Explorer: Relevance Prediction, Extractive Summarization, and Named\n  Entity Recognition from Clinical Cohort Studies", "abstract": "Clinical Cohort Studies (CCS), such as randomized clinical trials, are a\ngreat source of documented clinical research. Ideally, a clinical expert\ninspects these articles for exploratory analysis ranging from drug discovery\nfor evaluating the efficacy of existing drugs in tackling emerging diseases to\nthe first test of newly developed drugs. However, more than 100 articles are\npublished daily on a single prevalent disease like COVID-19 in PubMed. As a\nresult, it can take days for a physician to find articles and extract relevant\ninformation. Can we develop a system to sift through the long list of these\narticles faster and document the crucial takeaways from each of these articles?\nIn this work, we propose CCS Explorer, an end-to-end system for relevance\nprediction of sentences, extractive summarization, and patient, outcome, and\nintervention entity detection from CCS. CCS Explorer is packaged in a web-based\ngraphical user interface where the user can provide any disease name. CCS\nExplorer then extracts and aggregates all relevant information from articles on\nPubMed based on the results of an automatically generated query produced on the\nback-end. For each task, CCS Explorer fine-tunes pre-trained language\nrepresentation models based on transformers with additional layers. The models\nare evaluated using two publicly available datasets. CCS Explorer obtains a\nrecall of 80.2%, AUC-ROC of 0.843, and an accuracy of 88.3% on sentence\nrelevance prediction using BioBERT and achieves an average Micro F1-Score of\n77.8% on Patient, Intervention, Outcome detection (PIO) using PubMedBERT. Thus,\nCCS Explorer can reliably extract relevant information to summarize articles,\nsaving time by $\\sim \\text{660}\\times$.", "published": "2022-11-01 00:30:42", "link": "http://arxiv.org/abs/2211.00201v2", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.IR", "cs.LG", "I.2.1; I.2.7; I.5.5; I.5.4; J.3"], "primary_category": "cs.CL"}
{"title": "Technology Pipeline for Large Scale Cross-Lingual Dubbing of Lecture\n  Videos into Multiple Indian Languages", "abstract": "Cross-lingual dubbing of lecture videos requires the transcription of the\noriginal audio, correction and removal of disfluencies, domain term discovery,\ntext-to-text translation into the target language, chunking of text using\ntarget language rhythm, text-to-speech synthesis followed by isochronous\nlipsyncing to the original video. This task becomes challenging when the source\nand target languages belong to different language families, resulting in\ndifferences in generated audio duration. This is further compounded by the\noriginal speaker's rhythm, especially for extempore speech. This paper\ndescribes the challenges in regenerating English lecture videos in Indian\nlanguages semi-automatically. A prototype is developed for dubbing lectures\ninto 9 Indian languages. A mean-opinion-score (MOS) is obtained for two\nlanguages, Hindi and Tamil, on two different courses. The output video is\ncompared with the original video in terms of MOS (1-5) and lip synchronisation\nwith scores of 4.09 and 3.74, respectively. The human effort also reduces by\n75%.", "published": "2022-11-01 07:06:29", "link": "http://arxiv.org/abs/2211.01338v1", "categories": ["eess.AS", "cs.CL", "cs.MM", "cs.SD", "eess.IV"], "primary_category": "eess.AS"}
{"title": "Disentangled representation learning for multilingual speaker\n  recognition", "abstract": "The goal of this paper is to learn robust speaker representation for\nbilingual speaking scenario. The majority of the world's population speak at\nleast two languages; however, most speaker recognition systems fail to\nrecognise the same speaker when speaking in different languages.\n  Popular speaker recognition evaluation sets do not consider the bilingual\nscenario, making it difficult to analyse the effect of bilingual speakers on\nspeaker recognition performance. In this paper, we publish a large-scale\nevaluation set named VoxCeleb1-B derived from VoxCeleb that considers bilingual\nscenarios.\n  We introduce an effective disentanglement learning strategy that combines\nadversarial and metric learning-based methods. This approach addresses the\nbilingual situation by disentangling language-related information from speaker\nrepresentation while ensuring stable speaker representation learning. Our\nlanguage-disentangled learning method only uses language pseudo-labels without\nmanual information.", "published": "2022-11-01 13:00:07", "link": "http://arxiv.org/abs/2211.00437v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Metric Learning for User-defined Keyword Spotting", "abstract": "The goal of this work is to detect new spoken terms defined by users. While\nmost previous works address Keyword Spotting (KWS) as a closed-set\nclassification problem, this limits their transferability to unseen terms. The\nability to define custom keywords has advantages in terms of user experience.\n  In this paper, we propose a metric learning-based training strategy for\nuser-defined keyword spotting. In particular, we make the following\ncontributions: (1) we construct a large-scale keyword dataset with an existing\nspeech corpus and propose a filtering method to remove data that degrade model\ntraining; (2) we propose a metric learning-based two-stage training strategy,\nand demonstrate that the proposed method improves the performance on the\nuser-defined keyword spotting task by enriching their representations; (3) to\nfacilitate the fair comparison in the user-defined KWS field, we propose\nunified evaluation protocol and metrics.\n  Our proposed system does not require an incremental training on the\nuser-defined keywords, and outperforms previous works by a significant margin\non the Google Speech Commands dataset using the proposed as well as the\nexisting metrics.", "published": "2022-11-01 13:08:55", "link": "http://arxiv.org/abs/2211.00439v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Adapting self-supervised models to multi-talker speech recognition using\n  speaker embeddings", "abstract": "Self-supervised learning (SSL) methods which learn representations of data\nwithout explicit supervision have gained popularity in speech-processing tasks,\nparticularly for single-talker applications. However, these models often have\ndegraded performance for multi-talker scenarios -- possibly due to the domain\nmismatch -- which severely limits their use for such applications. In this\npaper, we investigate the adaptation of upstream SSL models to the multi-talker\nautomatic speech recognition (ASR) task under two conditions. First, when\nsegmented utterances are given, we show that adding a target speaker extraction\n(TSE) module based on enrollment embeddings is complementary to mixture-aware\npre-training. Second, for unsegmented mixtures, we propose a novel joint\nspeaker modeling (JSM) approach, which aggregates information from all speakers\nin the mixture through their embeddings. With controlled experiments on\nLibri2Mix, we show that using speaker embeddings provides relative WER\nimprovements of 9.1% and 42.1% over strong baselines for the segmented and\nunsegmented cases, respectively. We also demonstrate the effectiveness of our\nmodels for real conversational mixtures through experiments on the AMI dataset.", "published": "2022-11-01 14:16:16", "link": "http://arxiv.org/abs/2211.00482v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Comparative Study on Multichannel Speaker-Attributed Automatic Speech\n  Recognition in Multi-party Meetings", "abstract": "Speaker-attributed automatic speech recognition (SA-ASR) in multi-party\nmeeting scenarios is one of the most valuable and challenging ASR task. It was\nshown that single-channel frame-level diarization with serialized output\ntraining (SC-FD-SOT), single-channel word-level diarization with SOT\n(SC-WD-SOT) and joint training of single-channel target-speaker separation and\nASR (SC-TS-ASR) can be exploited to partially solve this problem. In this\npaper, we propose three corresponding multichannel (MC) SA-ASR approaches,\nnamely MC-FD-SOT, MC-WD-SOT and MC-TS-ASR. For different tasks/models,\ndifferent multichannel data fusion strategies are considered, including\nchannel-level cross-channel attention for MC-FD-SOT, frame-level cross-channel\nattention for MC-WD-SOT and neural beamforming for MC-TS-ASR. Results on the\nAliMeeting corpus reveal that our proposed models can consistently outperform\nthe corresponding single-channel counterparts in terms of the speaker-dependent\ncharacter error rate.", "published": "2022-11-01 14:58:27", "link": "http://arxiv.org/abs/2211.00511v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SDMuse: Stochastic Differential Music Editing and Generation via Hybrid\n  Representation", "abstract": "While deep generative models have empowered music generation, it remains a\nchallenging and under-explored problem to edit an existing musical piece at\nfine granularity. In this paper, we propose SDMuse, a unified Stochastic\nDifferential Music editing and generation framework, which can not only compose\na whole musical piece from scratch, but also modify existing musical pieces in\nmany ways, such as combination, continuation, inpainting, and style\ntransferring. The proposed SDMuse follows a two-stage pipeline to achieve music\ngeneration and editing on top of a hybrid representation including pianoroll\nand MIDI-event. In particular, SDMuse first generates/edits pianoroll by\niteratively denoising through a stochastic differential equation (SDE) based on\na diffusion model generative prior, and then refines the generated pianoroll\nand predicts MIDI-event tokens auto-regressively. We evaluate the generated\nmusic of our method on ailabs1k7 pop music dataset in terms of quality and\ncontrollability on various music editing and generation tasks. Experimental\nresults demonstrate the effectiveness of our proposed stochastic differential\nmusic editing and generation process, as well as the hybrid representations.", "published": "2022-11-01 02:10:25", "link": "http://arxiv.org/abs/2211.00222v2", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Waveform Boundary Detection for Partially Spoofed Audio", "abstract": "The present paper proposes a waveform boundary detection system for audio\nspoofing attacks containing partially manipulated segments. Partially\nspoofed/fake audio, where part of the utterance is replaced, either with\nsynthetic or natural audio clips, has recently been reported as one scenario of\naudio deepfakes. As deepfakes can be a threat to social security, the detection\nof such spoofing audio is essential. Accordingly, we propose to address the\nproblem with a deep learning-based frame-level detection system that can detect\npartially spoofed audio and locate the manipulated pieces. Our proposed method\nis trained and evaluated on data provided by the ADD2022 Challenge. We evaluate\nour detection model concerning various acoustic features and network\nconfigurations. As a result, our detection system achieves an equal error rate\n(EER) of 6.58% on the ADD2022 challenge test set, which is the best performance\nin partially spoofed audio detection systems that can locate manipulated clips.", "published": "2022-11-01 02:31:54", "link": "http://arxiv.org/abs/2211.00226v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Modelling black-box audio effects with time-varying feature modulation", "abstract": "Deep learning approaches for black-box modelling of audio effects have shown\npromise, however, the majority of existing work focuses on nonlinear effects\nwith behaviour on relatively short time-scales, such as guitar amplifiers and\ndistortion. While recurrent and convolutional architectures can theoretically\nbe extended to capture behaviour at longer time scales, we show that simply\nscaling the width, depth, or dilation factor of existing architectures does not\nresult in satisfactory performance when modelling audio effects such as fuzz\nand dynamic range compression. To address this, we propose the integration of\ntime-varying feature-wise linear modulation into existing temporal\nconvolutional backbones, an approach that enables learnable adaptation of the\nintermediate activations. We demonstrate that our approach more accurately\ncaptures long-range dependencies for a range of fuzz and compressor\nimplementations across both time and frequency domain metrics. We provide sound\nexamples, source code, and pretrained models to faciliate reproducibility.", "published": "2022-11-01 14:41:57", "link": "http://arxiv.org/abs/2211.00497v2", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "No-audio speaking status detection in crowded settings via visual\n  pose-based filtering and wearable acceleration", "abstract": "Recognizing who is speaking in a crowded scene is a key challenge towards the\nunderstanding of the social interactions going on within. Detecting speaking\nstatus from body movement alone opens the door for the analysis of social\nscenes in which personal audio is not obtainable. Video and wearable sensors\nmake it possible recognize speaking in an unobtrusive, privacy-preserving way.\nWhen considering the video modality, in action recognition problems, a bounding\nbox is traditionally used to localize and segment out the target subject, to\nthen recognize the action taking place within it. However, cross-contamination,\nocclusion, and the articulated nature of the human body, make this approach\nchallenging in a crowded scene. Here, we leverage articulated body poses for\nsubject localization and in the subsequent speech detection stage. We show that\nthe selection of local features around pose keypoints has a positive effect on\ngeneralization performance while also significantly reducing the number of\nlocal features considered, making for a more efficient method. Using two\nin-the-wild datasets with different viewpoints of subjects, we investigate the\nrole of cross-contamination in this effect. We additionally make use of\nacceleration measured through wearable sensors for the same task, and present a\nmultimodal approach combining both methods.", "published": "2022-11-01 15:55:48", "link": "http://arxiv.org/abs/2211.00549v1", "categories": ["cs.CV", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Adapter-Based Extension of Multi-Speaker Text-to-Speech Model for New\n  Speakers", "abstract": "Fine-tuning is a popular method for adapting text-to-speech (TTS) models to\nnew speakers. However this approach has some challenges. Usually fine-tuning\nrequires several hours of high quality speech per speaker. There is also that\nfine-tuning will negatively affect the quality of speech synthesis for\npreviously learnt speakers. In this paper we propose an alternative approach\nfor TTS adaptation based on using parameter-efficient adapter modules. In the\nproposed approach, a few small adapter modules are added to the original\nnetwork. The original weights are frozen, and only the adapters are fine-tuned\non speech for new speaker. The parameter-efficient fine-tuning approach will\nproduce a new model with high level of parameter sharing with original model.\nOur experiments on LibriTTS, HiFi-TTS and VCTK datasets validate the\neffectiveness of adapter-based method through objective and subjective metrics.", "published": "2022-11-01 16:59:54", "link": "http://arxiv.org/abs/2211.00585v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SCA: Streaming Cross-attention Alignment for Echo Cancellation", "abstract": "End-to-End deep learning has shown promising results for speech enhancement\ntasks, such as noise suppression, dereverberation, and speech separation.\nHowever, most state-of-the-art methods for echo cancellation are either\nclassical DSP-based or hybrid DSP-ML algorithms. Components such as the delay\nestimator and adaptive linear filter are based on traditional signal processing\nconcepts, and deep learning algorithms typically only serve to replace the\nnon-linear residual echo suppressor. This paper introduces an end-to-end echo\ncancellation network with a streaming cross-attention alignment (SCA). Our\nproposed method can handle unaligned inputs without requiring external\nalignment and generate high-quality speech without echoes. At the same time,\nthe end-to-end algorithm simplifies the current echo cancellation pipeline for\ntime-variant echo path cases. We test our proposed method on the ICASSP2022 and\nInterspeech2021 Microsoft deep echo cancellation challenge evaluation dataset,\nwhere our method outperforms some of the other hybrid and end-to-end methods.", "published": "2022-11-01 17:01:50", "link": "http://arxiv.org/abs/2211.00589v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Comparision Of Adversarial And Non-Adversarial LSTM Music Generative\n  Models", "abstract": "Algorithmic music composition is a way of composing musical pieces with\nminimal to no human intervention. While recurrent neural networks are\ntraditionally applied to many sequence-to-sequence prediction tasks, including\nsuccessful implementations of music composition, their standard supervised\nlearning approach based on input-to-output mapping leads to a lack of note\nvariety. These models can therefore be seen as potentially unsuitable for tasks\nsuch as music generation. Generative adversarial networks learn the generative\ndistribution of data and lead to varied samples. This work implements and\ncompares adversarial and non-adversarial training of recurrent neural network\nmusic composers on MIDI data. The resulting music samples are evaluated by\nhuman listeners, their preferences recorded. The evaluation indicates that\nadversarial training produces more aesthetically pleasing music.", "published": "2022-11-01 20:23:49", "link": "http://arxiv.org/abs/2211.00731v1", "categories": ["cs.LG", "cs.SC", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
