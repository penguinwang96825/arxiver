{"title": "Investigating the Decoders of Maximum Likelihood Sequence Models: A\n  Look-ahead Approach", "abstract": "We demonstrate how we can practically incorporate multi-step future\ninformation into a decoder of maximum likelihood sequence models. We propose a\n\"k-step look-ahead\" module to consider the likelihood information of a rollout\nup to k steps. Unlike other approaches that need to train another value network\nto evaluate the rollouts, we can directly apply this look-ahead module to\nimprove the decoding of any sequence model trained in a maximum likelihood\nframework. We evaluate our look-ahead module on three datasets of varying\ndifficulties: IM2LATEX-100k OCR image to LaTeX, WMT16 multimodal machine\ntranslation, and WMT14 machine translation. Our look-ahead module improves the\nperformance of the simpler datasets such as IM2LATEX-100k and WMT16 multimodal\nmachine translation. However, the improvement of the more difficult dataset\n(e.g., containing longer sequences), WMT14 machine translation, becomes\nmarginal. Our further investigation using the k-step look-ahead suggests that\nthe more difficult tasks suffer from the overestimated EOS (end-of-sentence)\nprobability. We argue that the overestimated EOS probability also causes the\ndecreased performance of beam search when increasing its beam width. We tackle\nthe EOS problem by integrating an auxiliary EOS loss into the training to\nestimate if the model should emit EOS or other words. Our experiments show that\nimproving EOS estimation not only increases the performance of our proposed\nlook-ahead module but also the robustness of the beam search.", "published": "2020-03-08 04:36:04", "link": "http://arxiv.org/abs/2003.03716v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pseudo Labeling and Negative Feedback Learning for Large-scale\n  Multi-label Domain Classification", "abstract": "In large-scale domain classification, an utterance can be handled by multiple\ndomains with overlapped capabilities. However, only a limited number of\nground-truth domains are provided for each training utterance in practice while\nknowing as many as correct target labels is helpful for improving the model\nperformance. In this paper, given one ground-truth domain for each training\nutterance, we regard domains consistently predicted with the highest\nconfidences as additional pseudo labels for the training. In order to reduce\nprediction errors due to incorrect pseudo labels, we leverage utterances with\nnegative system responses to decrease the confidences of the incorrectly\npredicted domains. Evaluating on user utterances from an intelligent\nconversational system, we show that the proposed approach significantly\nimproves the performance of domain classification with hypothesis reranking.", "published": "2020-03-08 06:00:15", "link": "http://arxiv.org/abs/2003.03728v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ESBM: An Entity Summarization BenchMark", "abstract": "Entity summarization is the problem of computing an optimal compact summary\nfor an entity by selecting a size-constrained subset of triples from RDF data.\nEntity summarization supports a multiplicity of applications and has led to\nfruitful research. However, there is a lack of evaluation efforts that cover\nthe broad spectrum of existing systems. One reason is a lack of benchmarks for\nevaluation. Some benchmarks are no longer available, while others are small and\nhave limitations. In this paper, we create an Entity Summarization BenchMark\n(ESBM) which overcomes the limitations of existing benchmarks and meets\nstandard desiderata for a benchmark. Using this largest available benchmark for\nevaluating general-purpose entity summarizers, we perform the most extensive\nexperiment to date where 9~existing systems are compared. Considering that all\nof these systems are unsupervised, we also implement and evaluate a supervised\nlearning based system for reference.", "published": "2020-03-08 07:12:20", "link": "http://arxiv.org/abs/2003.03734v1", "categories": ["cs.IR", "cs.CL", "cs.DB"], "primary_category": "cs.IR"}
{"title": "DeepLENS: Deep Learning for Entity Summarization", "abstract": "Entity summarization has been a prominent task over knowledge graphs. While\nexisting methods are mainly unsupervised, we present DeepLENS, a simple yet\neffective deep learning model where we exploit textual semantics for encoding\ntriples and we score each candidate triple based on its interdependence on\nother triples. DeepLENS significantly outperformed existing methods on a public\nbenchmark.", "published": "2020-03-08 07:15:48", "link": "http://arxiv.org/abs/2003.03736v1", "categories": ["cs.IR", "cs.CL", "cs.DB"], "primary_category": "cs.IR"}
{"title": "Keeping it simple: Implementation and performance of the proto-principle\n  of adaptation and learning in the language sciences", "abstract": "In this paper we present the Widrow-Hoff rule and its applications to\nlanguage data. After contextualizing the rule historically and placing it in\nthe chain of neurally inspired artificial learning models, we explain its\nrationale and implementational considerations. Using a number of case studies\nwe illustrate how the Widrow-Hoff rule offers unexpected opportunities for the\ncomputational simulation of a range of language phenomena that make it possible\nto approach old problems from a novel perspective.", "published": "2020-03-08 17:07:08", "link": "http://arxiv.org/abs/2003.03813v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Improving Training on Noisy Stuctured Labels", "abstract": "Fine-grained annotations---e.g. dense image labels, image segmentation and\ntext tagging---are useful in many ML applications but they are labor-intensive\nto generate. Moreover there are often systematic, structured errors in these\nfine-grained annotations. For example, a car might be entirely unannotated in\nthe image, or the boundary between a car and street might only be coarsely\nannotated. Standard ML training on data with such structured errors produces\nmodels with biases and poor performance. In this work, we propose a novel\nframework of Error-Correcting Networks (ECN) to address the challenge of\nlearning in the presence structured error in fine-grained annotations. Given a\nlarge noisy dataset with commonly occurring structured errors, and a much\nsmaller dataset with more accurate annotations, ECN is able to substantially\nimprove the prediction of fine-grained annotations compared to standard\napproaches for training on noisy data. It does so by learning to leverage the\nstructures in the annotations and in the noisy labels. Systematic experiments\non image segmentation and text tagging demonstrate the strong performance of\nECN in improving training on noisy structured labels.", "published": "2020-03-08 22:55:11", "link": "http://arxiv.org/abs/2003.03862v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Utilizing Deep Learning to Identify Drug Use on Twitter Data", "abstract": "The collection and examination of social media has become a useful mechanism\nfor studying the mental activity and behavior tendencies of users. Through the\nanalysis of collected Twitter data, models were developed for classifying\ndrug-related tweets. Using topic pertaining keywords, such as slang and methods\nof drug consumption, a set of tweets was generated. Potential candidates were\nthen preprocessed resulting in a dataset of 3,696,150 rows. The classification\npower of multiple methods was compared including support vector machines (SVM),\nXGBoost, and convolutional neural network (CNN) based classifiers. Rather than\nsimple feature or attribute analysis, a deep learning approach was implemented\nto screen and analyze the tweets' semantic meaning. The two CNN-based\nclassifiers presented the best result when compared against other\nmethodologies. The first was trained with 2,661 manually labeled samples, while\nthe other included synthetically generated tweets culminating in 12,142\nsamples. The accuracy scores were 76.35% and 82.31%, with an AUC of 0.90 and\n0.91. Additionally, association rule mining showed that commonly mentioned\ndrugs had a level of correspondence with frequently used illicit substances,\nproving the practical usefulness of the system. Lastly, the synthetically\ngenerated set provided increased scores, improving the classification\ncapability and proving the worth of this methodology.", "published": "2020-03-08 07:52:40", "link": "http://arxiv.org/abs/2003.11522v1", "categories": ["cs.SI", "cs.CL", "cs.LG"], "primary_category": "cs.SI"}
{"title": "Development of Automatic Speech Recognition for Kazakh Language using\n  Transfer Learning", "abstract": "Development of Automatic Speech Recognition system for Kazakh language is\nvery challenging due to a lack of data.Existing data of kazakh speech with its\ncorresponding transcriptions are heavily accessed and not enough to gain a\nworth mentioning results.For this reason, speech recognition of Kazakh language\nhas not been explored well.There are only few works that investigate this area\nwith traditional methods Hidden Markov Model, Gaussian Mixture Model, but they\nare suffering from poor outcome and lack of enough data.In our work we suggest\na new method that takes pre-trained model of Russian language and applies its\nknowledge as a starting point to our neural network structure, which means that\nwe are transferring the weights of pre-trained model to our neural network.The\nmain reason we chose Russian model is that pronunciation of kazakh and russian\nlanguages are quite similar because they share 78 percent letters and there are\nquite large corpus of russian speech dataset. We have collected a dataset of\nKazakh speech with transcriptions in the base of Suleyman Demirel University\nwith 50 native speakers each having around 400 sentences.Data have been chosen\nfrom famous Kazakh books.\n  We have considered 4 different scenarios in our experiment. First, we trained\nour neural network without using a pre-trained Russian model with 2 LSTM layers\nand 2 BiLSTM .Second, we have trained the same 2 LSTM layered and 2 BiLSTM\nlayered using a pre-trained model. As a result, we have improved our models\ntraining cost and Label Error Rate by using external Russian speech recognition\nmodel up to 24 percent and 32 percent respectively.Pre-trained Russian language\nmodel has trained on 100 hours of data with the same neural network\narchitecture.", "published": "2020-03-08 20:38:56", "link": "http://arxiv.org/abs/2003.04710v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
