{"title": "A Survey on Spoken Language Understanding: Recent Advances and New\n  Frontiers", "abstract": "Spoken Language Understanding (SLU) aims to extract the semantics frame of\nuser queries, which is a core component in a task-oriented dialog system. With\nthe burst of deep neural networks and the evolution of pre-trained language\nmodels, the research of SLU has obtained significant breakthroughs. However,\nthere remains a lack of a comprehensive survey summarizing existing approaches\nand recent trends, which motivated the work presented in this article. In this\npaper, we survey recent advances and new frontiers in SLU. Specifically, we\ngive a thorough review of this research field, covering different aspects\nincluding (1) new taxonomy: we provide a new perspective for SLU filed,\nincluding single model vs. joint model, implicit joint modeling vs. explicit\njoint modeling in joint model, non pre-trained paradigm vs. pre-trained\nparadigm;(2) new frontiers: some emerging areas in complex SLU as well as the\ncorresponding challenges; (3) abundant open-source resources: to help the\ncommunity, we have collected, organized the related papers, baseline projects\nand leaderboard on a public website where SLU researchers could directly access\nto the recent progress. We hope that this survey can shed a light on future\nresearch in SLU field.", "published": "2021-03-04 15:22:00", "link": "http://arxiv.org/abs/2103.03095v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Empirical Study of End-to-end Simultaneous Speech Translation\n  Decoding Strategies", "abstract": "This paper proposes a decoding strategy for end-to-end simultaneous speech\ntranslation. We leverage end-to-end models trained in offline mode and conduct\nan empirical study for two language pairs (English-to-German and\nEnglish-to-Portuguese). We also investigate different output token\ngranularities including characters and Byte Pair Encoding (BPE) units. The\nresults show that the proposed decoding approach allows to control BLEU/Average\nLagging trade-off along different latency regimes. Our best decoding settings\nachieve comparable results with a strong cascade model evaluated on the\nsimultaneous translation track of IWSLT 2020 shared task.", "published": "2021-03-04 18:55:40", "link": "http://arxiv.org/abs/2103.03233v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NADI 2021: The Second Nuanced Arabic Dialect Identification Shared Task", "abstract": "We present the findings and results of the Second Nuanced Arabic Dialect\nIdentification Shared Task (NADI 2021). This Shared Task includes four\nsubtasks: country-level Modern Standard Arabic (MSA) identification (Subtask\n1.1), country-level dialect identification (Subtask 1.2), province-level MSA\nidentification (Subtask 2.1), and province-level sub-dialect identification\n(Subtask 2.2). The shared task dataset covers a total of 100 provinces from 21\nArab countries, collected from the Twitter domain. A total of 53 teams from 23\ncountries registered to participate in the tasks, thus reflecting the interest\nof the community in this area. We received 16 submissions for Subtask 1.1 from\nfive teams, 27 submissions for Subtask 1.2 from eight teams, 12 submissions for\nSubtask 2.1 from four teams, and 13 Submissions for subtask 2.2 from four\nteams.", "published": "2021-03-04 04:59:37", "link": "http://arxiv.org/abs/2103.08466v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hardware Acceleration of Fully Quantized BERT for Efficient Natural\n  Language Processing", "abstract": "BERT is the most recent Transformer-based model that achieves\nstate-of-the-art performance in various NLP tasks. In this paper, we\ninvestigate the hardware acceleration of BERT on FPGA for edge computing. To\ntackle the issue of huge computational complexity and memory footprint, we\npropose to fully quantize the BERT (FQ-BERT), including weights, activations,\nsoftmax, layer normalization, and all the intermediate results. Experiments\ndemonstrate that the FQ-BERT can achieve 7.94x compression for weights with\nnegligible performance loss. We then propose an accelerator tailored for the\nFQ-BERT and evaluate on Xilinx ZCU102 and ZCU111 FPGA. It can achieve a\nperformance-per-watt of 3.18 fps/W, which is 28.91x and 12.72x over Intel(R)\nCore(TM) i7-8700 CPU and NVIDIA K80 GPU, respectively.", "published": "2021-03-04 02:49:16", "link": "http://arxiv.org/abs/2103.02800v1", "categories": ["cs.AR", "cs.CL"], "primary_category": "cs.AR"}
{"title": "An Emotion-controlled Dialog Response Generation Model with Dynamic\n  Vocabulary", "abstract": "In response generation task, proper sentimental expressions can obviously\nimprove the human-like level of the responses. However, for real application in\nonline systems, high QPS (queries per second, an indicator of the flow capacity\nof on-line systems) is required, and a dynamic vocabulary mechanism has been\nproved available in improving speed of generative models. In this paper, we\nproposed an emotion-controlled dialog response generation model based on the\ndynamic vocabulary mechanism, and the experimental results show the benefit of\nthis model.", "published": "2021-03-04 07:58:43", "link": "http://arxiv.org/abs/2103.02878v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MP Twitter Engagement and Abuse Post-first COVID-19 Lockdown in the UK:\n  White Paper", "abstract": "The UK has had a volatile political environment for some years now, with\nBrexit and leadership crises marking the past five years. With this work, we\nwanted to understand more about how the global health emergency, COVID-19,\ninfluences the amount, type or topics of abuse that UK politicians receive when\nengaging with the public. With this work, we wanted to understand more about\nhow the global health emergency, COVID-19, influences the amount, type or\ntopics of abuse that UK politicians receive when engaging with the public. This\nwork covers the period of June to December 2020 and analyses Twitter abuse in\nreplies to UK MPs. This work is a follow-up from our analysis of online abuse\nduring the first four months of the COVID-19 pandemic in the UK. The paper\nexamines overall abuse levels during this new seven month period, analyses\nreactions to members of different political parties and the UK government, and\nthe relationship between online abuse and topics such as Brexit, government's\nCOVID-19 response and policies, and social issues. In addition, we have also\nexamined the presence of conspiracy theories posted in abusive replies to MPs\nduring the period. We have found that abuse levels toward UK MPs were at an\nall-time high in December 2020 (5.4% of all reply tweets sent to MPs). This is\nalmost 1% higher that the two months preceding the General Election. In a\ndeparture from the trend seen in the first four months of the pandemic, MPs\nfrom the Tory party received the highest percentage of abusive replies from\nJuly 2020 onward, which stays above 5% starting from September 2020 onward, as\nthe COVID-19 crisis deepened and the Brexit negotiations with the EU started\nnearing completion.", "published": "2021-03-04 09:45:00", "link": "http://arxiv.org/abs/2103.02917v2", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "A Systematic Evaluation of Transfer Learning and Pseudo-labeling with\n  BERT-based Ranking Models", "abstract": "Due to high annotation costs making the best use of existing human-created\ntraining data is an important research direction. We, therefore, carry out a\nsystematic evaluation of transferability of BERT-based neural ranking models\nacross five English datasets. Previous studies focused primarily on zero-shot\nand few-shot transfer from a large dataset to a dataset with a small number of\nqueries. In contrast, each of our collections has a substantial number of\nqueries, which enables a full-shot evaluation mode and improves reliability of\nour results. Furthermore, since source datasets licences often prohibit\ncommercial use, we compare transfer learning to training on pseudo-labels\ngenerated by a BM25 scorer. We find that training on pseudo-labels -- possibly\nwith subsequent fine-tuning using a modest number of annotated queries -- can\nproduce a competitive or better model compared to transfer learning. Yet, it is\nnecessary to improve the stability and/or effectiveness of the few-shot\ntraining, which, sometimes, can degrade performance of a pretrained model.", "published": "2021-03-04 21:08:06", "link": "http://arxiv.org/abs/2103.03335v4", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Transfer learning from High-Resource to Low-Resource Language Improves\n  Speech Affect Recognition Classification Accuracy", "abstract": "Speech Affect Recognition is a problem of extracting emotional affects from\naudio data. Low resource languages corpora are rear and affect recognition is a\ndifficult task in cross-corpus settings. We present an approach in which the\nmodel is trained on high resource language and fine-tune to recognize affects\nin low resource language. We train the model in same corpus setting on SAVEE,\nEMOVO, Urdu, and IEMOCAP by achieving baseline accuracy of 60.45, 68.05, 80.34,\nand 56.58 percent respectively. For capturing the diversity of affects in\nlanguages cross-corpus evaluations are discussed in detail. We find that\naccuracy improves by adding the domain target data into the training data.\nFinally, we show that performance is improved for low resource language speech\naffect recognition by achieving the UAR OF 69.32 and 68.2 for Urdu and Italian\nspeech affects.", "published": "2021-03-04 08:17:19", "link": "http://arxiv.org/abs/2103.11764v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On the privacy-utility trade-off in differentially private hierarchical\n  text classification", "abstract": "Hierarchical text classification consists in classifying text documents into\na hierarchy of classes and sub-classes. Although artificial neural networks\nhave proved useful to perform this task, unfortunately they can leak training\ndata information to adversaries due to training data memorization. Using\ndifferential privacy during model training can mitigate leakage attacks against\ntrained models, enabling the models to be shared safely at the cost of reduced\nmodel accuracy. This work investigates the privacy-utility trade-off in\nhierarchical text classification with differential privacy guarantees, and\nidentifies neural network architectures that offer superior trade-offs. To this\nend, we use a white-box membership inference attack to empirically assess the\ninformation leakage of three widely used neural network architectures. We show\nthat large differential privacy parameters already suffice to completely\nmitigate membership inference attacks, thus resulting only in a moderate\ndecrease in model utility. More specifically, for large datasets with long\ntexts we observed Transformer-based models to achieve an overall favorable\nprivacy-utility trade-off, while for smaller datasets with shorter texts\nconvolutional neural networks are preferable.", "published": "2021-03-04 08:51:00", "link": "http://arxiv.org/abs/2103.02895v2", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "An empirical analysis of phrase-based and neural machine translation", "abstract": "Two popular types of machine translation (MT) are phrase-based and neural\nmachine translation systems. Both of these types of systems are composed of\nmultiple complex models or layers. Each of these models and layers learns\ndifferent linguistic aspects of the source language. However, for some of these\nmodels and layers, it is not clear which linguistic phenomena are learned or\nhow this information is learned. For phrase-based MT systems, it is often clear\nwhat information is learned by each model, and the question is rather how this\ninformation is learned, especially for its phrase reordering model. For neural\nmachine translation systems, the situation is even more complex, since for many\ncases it is not exactly clear what information is learned and how it is\nlearned.\n  To shed light on what linguistic phenomena are captured by MT systems, we\nanalyze the behavior of important models in both phrase-based and neural MT\nsystems. We consider phrase reordering models from phrase-based MT systems to\ninvestigate which words from inside of a phrase have the biggest impact on\ndefining the phrase reordering behavior. Additionally, to contribute to the\ninterpretability of neural MT systems we study the behavior of the attention\nmodel, which is a key component in neural MT systems and the closest model in\nfunctionality to phrase reordering models in phrase-based systems. The\nattention model together with the encoder hidden state representations form the\nmain components to encode source side linguistic information in neural MT. To\nthis end, we also analyze the information captured in the encoder hidden state\nrepresentations of a neural MT system. We investigate the extent to which\nsyntactic and lexical-semantic information from the source side is captured by\nhidden state representations of different neural MT architectures.", "published": "2021-03-04 15:28:28", "link": "http://arxiv.org/abs/2103.03108v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Advances in Multi-turn Dialogue Comprehension: A Survey", "abstract": "Training machines to understand natural language and interact with humans is\nan elusive and essential task of artificial intelligence. A diversity of\ndialogue systems has been designed with the rapid development of deep learning\ntechniques, especially the recent pre-trained language models (PrLMs). Among\nthese studies, the fundamental yet challenging type of task is dialogue\ncomprehension whose role is to teach the machines to read and comprehend the\ndialogue context before responding. In this paper, we review the previous\nmethods from the technical perspective of dialogue modeling for the dialogue\ncomprehension task. We summarize the characteristics and challenges of dialogue\ncomprehension in contrast to plain-text reading comprehension. Then, we discuss\nthree typical patterns of dialogue modeling. In addition, we categorize\ndialogue-related pre-training techniques which are employed to enhance PrLMs in\ndialogue scenarios. Finally, we highlight the technical advances in recent\nyears and point out the lessons from the empirical analysis and the prospects\ntowards a new frontier of researches.", "published": "2021-03-04 15:50:17", "link": "http://arxiv.org/abs/2103.03125v2", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Error-driven Fixed-Budget ASR Personalization for Accented Speakers", "abstract": "We consider the task of personalizing ASR models while being constrained by a\nfixed budget on recording speaker-specific utterances. Given a speaker and an\nASR model, we propose a method of identifying sentences for which the speaker's\nutterances are likely to be harder for the given ASR model to recognize. We\nassume a tiny amount of speaker-specific data to learn phoneme-level error\nmodels which help us select such sentences. We show that speaker's utterances\non the sentences selected using our error model indeed have larger error rates\nwhen compared to speaker's utterances on randomly selected sentences. We find\nthat fine-tuning the ASR model on the sentence utterances selected with the\nhelp of error models yield higher WER improvements in comparison to fine-tuning\non an equal number of randomly selected sentence utterances. Thus, our method\nprovides an efficient way of collecting speaker utterances under budget\nconstraints for personalizing ASR models.", "published": "2021-03-04 16:36:59", "link": "http://arxiv.org/abs/2103.03142v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "PVG at WASSA 2021: A Multi-Input, Multi-Task, Transformer-Based\n  Architecture for Empathy and Distress Prediction", "abstract": "Active research pertaining to the affective phenomenon of empathy and\ndistress is invaluable for improving human-machine interaction. Predicting\nintensities of such complex emotions from textual data is difficult, as these\nconstructs are deeply rooted in the psychological theory. Consequently, for\nbetter prediction, it becomes imperative to take into account ancillary factors\nsuch as the psychological test scores, demographic features, underlying latent\nprimitive emotions, along with the text's undertone and its psychological\ncomplexity. This paper proffers team PVG's solution to the WASSA 2021 Shared\nTask on Predicting Empathy and Emotion in Reaction to News Stories. Leveraging\nthe textual data, demographic features, psychological test score, and the\nintrinsic interdependencies of primitive emotions and empathy, we propose a\nmulti-input, multi-task framework for the task of empathy score prediction.\nHere, the empathy score prediction is considered the primary task, while\nemotion and empathy classification are considered secondary auxiliary tasks.\nFor the distress score prediction task, the system is further boosted by the\naddition of lexical features. Our submission ranked 1$^{st}$ based on the\naverage correlation (0.545) as well as the distress correlation (0.574), and\n2$^{nd}$ for the empathy Pearson correlation (0.517).", "published": "2021-03-04 20:12:25", "link": "http://arxiv.org/abs/2103.03296v1", "categories": ["cs.CL", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Neural model robustness for skill routing in large-scale conversational\n  AI systems: A design choice exploration", "abstract": "Current state-of-the-art large-scale conversational AI or intelligent digital\nassistant systems in industry comprises a set of components such as Automatic\nSpeech Recognition (ASR) and Natural Language Understanding (NLU). For some of\nthese systems that leverage a shared NLU ontology (e.g., a centralized\nintent/slot schema), there exists a separate skill routing component to\ncorrectly route a request to an appropriate skill, which is either a\nfirst-party or third-party application that actually executes on a user\nrequest. The skill routing component is needed as there are thousands of skills\nthat can either subscribe to the same intent and/or subscribe to an intent\nunder specific contextual conditions (e.g., device has a screen). Ensuring\nmodel robustness or resilience in the skill routing component is an important\nproblem since skills may dynamically change their subscription in the ontology\nafter the skill routing model has been deployed to production. We show how\ndifferent modeling design choices impact the model robustness in the context of\nskill routing on a state-of-the-art commercial conversational AI system,\nspecifically on the choices around data augmentation, model architecture, and\noptimization method. We show that applying data augmentation can be a very\neffective and practical way to drastically improve model robustness.", "published": "2021-03-04 22:54:33", "link": "http://arxiv.org/abs/2103.03373v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "crank: An Open-Source Software for Nonparallel Voice Conversion Based on\n  Vector-Quantized Variational Autoencoder", "abstract": "In this paper, we present an open-source software for developing a\nnonparallel voice conversion (VC) system named crank. Although we have released\nan open-source VC software based on the Gaussian mixture model named sprocket\nin the last VC Challenge, it is not straightforward to apply any speech corpus\nbecause it is necessary to prepare parallel utterances of source and target\nspeakers to model a statistical conversion function. To address this issue, in\nthis study, we developed a new open-source VC software that enables users to\nmodel the conversion function by using only a nonparallel speech corpus. For\nimplementing the VC software, we used a vector-quantized variational\nautoencoder (VQVAE). To rapidly examine the effectiveness of recent\ntechnologies developed in this research field, crank also supports several\nrepresentative works for autoencoder-based VC methods such as the use of\nhierarchical architectures, cyclic architectures, generative adversarial\nnetworks, speaker adversarial training, and neural vocoders. Moreover, it is\npossible to automatically estimate objective measures such as mel-cepstrum\ndistortion and pseudo mean opinion score based on MOSNet. In this paper, we\ndescribe representative functions developed in crank and make brief comparisons\nby objective evaluations.", "published": "2021-03-04 06:41:11", "link": "http://arxiv.org/abs/2103.02858v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "End-to-end acoustic modelling for phone recognition of young readers", "abstract": "Automatic recognition systems for child speech are lagging behind those\ndedicated to adult speech in the race of performance. This phenomenon is due to\nthe high acoustic and linguistic variability present in child speech caused by\ntheir body development, as well as the lack of available child speech data.\nYoung readers speech additionally displays peculiarities, such as slow reading\nrate and presence of reading mistakes, that hardens the task. This work\nattempts to tackle the main challenges in phone acoustic modelling for young\nchild speech with limited data, and improve understanding of strengths and\nweaknesses of a wide selection of model architectures in this domain. We find\nthat transfer learning techniques are highly efficient on end-to-end\narchitectures for adult-to-child adaptation with a small amount of child speech\ndata. Through transfer learning, a Transformer model complemented with a\nConnectionist Temporal Classification (CTC) objective function, reaches a phone\nerror rate of 28.1%, outperforming a state-of-the-art DNN-HMM model by 6.6%\nrelative, as well as other end-to-end architectures by more than 8.5% relative.\nAn analysis of the models' performance on two specific reading tasks (isolated\nwords and sentences) is provided, showing the influence of the utterance length\non attention-based and CTC-based models. The Transformer+CTC model displays an\nability to better detect reading mistakes made by children, that can be\nattributed to the CTC objective function effectively constraining the attention\nmechanisms to be monotonic.", "published": "2021-03-04 09:03:08", "link": "http://arxiv.org/abs/2103.02899v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speech Emotion Recognition using Semantic Information", "abstract": "Speech emotion recognition is a crucial problem manifesting in a multitude of\napplications such as human computer interaction and education. Although several\nadvancements have been made in the recent years, especially with the advent of\nDeep Neural Networks (DNN), most of the studies in the literature fail to\nconsider the semantic information in the speech signal. In this paper, we\npropose a novel framework that can capture both the semantic and the\nparalinguistic information in the signal. In particular, our framework is\ncomprised of a semantic feature extractor, that captures the semantic\ninformation, and a paralinguistic feature extractor, that captures the\nparalinguistic information. Both semantic and paraliguistic features are then\ncombined to a unified representation using a novel attention mechanism. The\nunified feature vector is passed through a LSTM to capture the temporal\ndynamics in the signal, before the final prediction. To validate the\neffectiveness of our framework, we use the popular SEWA dataset of the AVEC\nchallenge series and compare with the three winning papers. Our model provides\nstate-of-the-art results in the valence and liking dimensions.", "published": "2021-03-04 12:34:25", "link": "http://arxiv.org/abs/2103.02993v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Front-end Diarization for Percussion Separation in Taniavartanam of\n  Carnatic Music Concerts", "abstract": "Instrument separation in an ensemble is a challenging task. In this work, we\naddress the problem of separating the percussive voices in the taniavartanam\nsegments of Carnatic music. In taniavartanam, a number of percussive\ninstruments play together or in tandem. Separation of instruments in regions\nwhere only one percussion is present leads to interference and artifacts at the\noutput, as source separation algorithms assume the presence of multiple\npercussive voices throughout the audio segment. We prevent this by first\nsubjecting the taniavartanam to diarization. This process results in\nhomogeneous clusters consisting of segments of either a single voice or\nmultiple voices. A cluster of segments with multiple voices is identified using\nthe Gaussian mixture model (GMM), which is then subjected to source separation.\nA deep recurrent neural network (DRNN) based approach is used to separate the\nmultiple instrument segments. The effectiveness of the proposed system is\nevaluated on a standard Carnatic music dataset. The proposed approach provides\nclose-to-oracle performance for non-overlapping segments and a significant\nimprovement over traditional separation schemes.", "published": "2021-03-04 18:38:39", "link": "http://arxiv.org/abs/2103.03215v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "End-to-End Mispronunciation Detection and Diagnosis From Raw Waveforms", "abstract": "Mispronunciation detection and diagnosis (MDD) is designed to identify\npronunciation errors and provide instructive feedback to guide non-native\nlanguage learners, which is a core component in computer-assisted pronunciation\ntraining (CAPT) systems. However, MDD often suffers from the data-sparsity\nproblem due to that collecting non-native data and the associated annotations\nis time-consuming and labor-intensive. To address this issue, we explore a\nfully end-to-end (E2E) neural model for MDD, which processes learners' speech\ndirectly based on raw waveforms. Compared to conventional hand-crafted acoustic\nfeatures, raw waveforms retain more acoustic phenomena and potentially can help\nneural networks discover better and more customized representations. To this\nend, our MDD model adopts a co-called SincNet module to take input a raw\nwaveform and covert it to a suitable vector representation sequence. SincNet\nemploys the cardinal sine (sinc) function to implement learnable bandpass\nfilters, drawing inspiration from the convolutional neural network (CNN). By\ncomparison to CNN, SincNet has fewer parameters and is more amenable to human\ninterpretation. Extensive experiments are conducted on the L2-ARCTIC dataset,\nwhich is a publicly-available non-native English speech corpus compiled for\nresearch on CAPT. We find that the sinc filters of SincNet can be adapted\nquickly for non-native language learners of different nationalities.\nFurthermore, our model can achieve comparable mispronunciation detection\nperformance in relation to state-of-the-art E2E MDD models that take input the\nstandard handcrafted acoustic features. Besides that, our model also provides\nconsiderable improvements on phone error rate (PER) and diagnosis accuracy.", "published": "2021-03-04 13:33:50", "link": "http://arxiv.org/abs/2103.03023v4", "categories": ["eess.AS", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Neural Text-to-Speech Model Utilizing Broadcast Data Mixed with\n  Background Music", "abstract": "Recently, it has become easier to obtain speech data from various media such\nas the internet or YouTube, but directly utilizing them to train a neural\ntext-to-speech (TTS) model is difficult. The proportion of clean speech is\ninsufficient and the remainder includes background music. Even with the global\nstyle token (GST). Therefore, we propose the following method to successfully\ntrain an end-to-end TTS model with limited broadcast data. First, the\nbackground music is removed from the speech by introducing a music filter.\nSecond, the GST-TTS model with an auxiliary quality classifier is trained with\nthe filtered speech and a small amount of clean speech. In particular, the\nquality classifier makes the embedding vector of the GST layer focus on\nrepresenting the speech quality (filtered or clean) of the input speech. The\nexperimental results verified that the proposed method synthesized much more\nhigh-quality speech than conventional methods.", "published": "2021-03-04 14:14:58", "link": "http://arxiv.org/abs/2103.03049v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "WaveGuard: Understanding and Mitigating Audio Adversarial Examples", "abstract": "There has been a recent surge in adversarial attacks on deep learning based\nautomatic speech recognition (ASR) systems. These attacks pose new challenges\nto deep learning security and have raised significant concerns in deploying ASR\nsystems in safety-critical applications. In this work, we introduce WaveGuard:\na framework for detecting adversarial inputs that are crafted to attack ASR\nsystems. Our framework incorporates audio transformation functions and analyses\nthe ASR transcriptions of the original and transformed audio to detect\nadversarial inputs. We demonstrate that our defense framework is able to\nreliably detect adversarial examples constructed by four recent audio\nadversarial attacks, with a variety of audio transformation functions. With\ncareful regard for best practices in defense evaluations, we analyze our\nproposed defense and its strength to withstand adaptive and robust attacks in\nthe audio domain. We empirically demonstrate that audio transformations that\nrecover audio from perceptually informed representations can lead to a strong\ndefense that is robust against an adaptive adversary even in a complete\nwhite-box setting. Furthermore, WaveGuard can be used out-of-the box and\nintegrated directly with any ASR model to efficiently detect audio adversarial\nexamples, without the need for model retraining.", "published": "2021-03-04 21:44:37", "link": "http://arxiv.org/abs/2103.03344v1", "categories": ["cs.CR", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
{"title": "Perceiver: General Perception with Iterative Attention", "abstract": "Biological systems perceive the world by simultaneously processing\nhigh-dimensional inputs from modalities as diverse as vision, audition, touch,\nproprioception, etc. The perception models used in deep learning on the other\nhand are designed for individual modalities, often relying on domain-specific\nassumptions such as the local grid structures exploited by virtually all\nexisting vision models. These priors introduce helpful inductive biases, but\nalso lock models to individual modalities. In this paper we introduce the\nPerceiver - a model that builds upon Transformers and hence makes few\narchitectural assumptions about the relationship between its inputs, but that\nalso scales to hundreds of thousands of inputs, like ConvNets. The model\nleverages an asymmetric attention mechanism to iteratively distill inputs into\na tight latent bottleneck, allowing it to scale to handle very large inputs. We\nshow that this architecture is competitive with or outperforms strong,\nspecialized models on classification tasks across various modalities: images,\npoint clouds, audio, video, and video+audio. The Perceiver obtains performance\ncomparable to ResNet-50 and ViT on ImageNet without 2D convolutions by directly\nattending to 50,000 pixels. It is also competitive in all modalities in\nAudioSet.", "published": "2021-03-04 18:20:50", "link": "http://arxiv.org/abs/2103.03206v2", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
