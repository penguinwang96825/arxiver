{"title": "Morphological Disambiguation from Stemming Data", "abstract": "Morphological analysis and disambiguation is an important task and a crucial\npreprocessing step in natural language processing of morphologically rich\nlanguages. Kinyarwanda, a morphologically rich language, currently lacks tools\nfor automated morphological analysis. While linguistically curated finite state\ntools can be easily developed for morphological analysis, the morphological\nrichness of the language allows many ambiguous analyses to be produced,\nrequiring effective disambiguation. In this paper, we propose learning to\nmorphologically disambiguate Kinyarwanda verbal forms from a new stemming\ndataset collected through crowd-sourcing. Using feature engineering and a\nfeed-forward neural network based classifier, we achieve about 89%\nnon-contextualized disambiguation accuracy. Our experiments reveal that\ninflectional properties of stems and morpheme association rules are the most\ndiscriminative features for disambiguation.", "published": "2020-11-11 01:44:09", "link": "http://arxiv.org/abs/2011.05504v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "NIT COVID-19 at WNUT-2020 Task 2: Deep Learning Model RoBERTa for\n  Identify Informative COVID-19 English Tweets", "abstract": "This paper presents the model submitted by the NIT_COVID-19 team for\nidentified informative COVID-19 English tweets at WNUT-2020 Task2. This shared\ntask addresses the problem of automatically identifying whether an English\ntweet related to informative (novel coronavirus) or not. These informative\ntweets provide information about recovered, confirmed, suspected, and death\ncases as well as the location or travel history of the cases. The proposed\napproach includes pre-processing techniques and pre-trained RoBERTa with\nsuitable hyperparameters for English coronavirus tweet classification. The\nperformance achieved by the proposed model for shared task WNUT 2020 Task2 is\n89.14% in the F1-score metric.", "published": "2020-11-11 05:20:39", "link": "http://arxiv.org/abs/2011.05551v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Personal-ITY: A Novel YouTube-based Corpus for Personality Prediction in\n  Italian", "abstract": "We present a novel corpus for personality prediction in Italian, containing a\nlarger number of authors and a different genre compared to previously available\nresources. The corpus is built exploiting Distant Supervision, assigning\nMyers-Briggs Type Indicator (MBTI) labels to YouTube comments, and can lend\nitself to a variety of experiments. We report on preliminary experiments on\nPersonal-ITY, which can serve as a baseline for future work, showing that some\ntypes are easier to predict than others, and discussing the perks of\ncross-dataset prediction.", "published": "2020-11-11 10:51:07", "link": "http://arxiv.org/abs/2011.05688v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Irony Detection with Dependency Syntax and Neural Models", "abstract": "This paper presents an in-depth investigation of the effectiveness of\ndependency-based syntactic features on the irony detection task in a\nmultilingual perspective (English, Spanish, French and Italian). It focuses on\nthe contribution from syntactic knowledge, exploiting linguistic resources\nwhere syntax is annotated according to the Universal Dependencies scheme. Three\ndistinct experimental settings are provided. In the first, a variety of\nsyntactic dependency-based features combined with classical machine learning\nclassifiers are explored. In the second scenario, two well-known types of word\nembeddings are trained on parsed data and tested against gold standard\ndatasets. In the third setting, dependency-based syntactic features are\ncombined into the Multilingual BERT architecture. The results suggest that\nfine-grained dependency-based syntactic information is informative for the\ndetection of irony.", "published": "2020-11-11 11:22:05", "link": "http://arxiv.org/abs/2011.05706v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IGSQL: Database Schema Interaction Graph Based Neural Model for\n  Context-Dependent Text-to-SQL Generation", "abstract": "Context-dependent text-to-SQL task has drawn much attention in recent years.\nPrevious models on context-dependent text-to-SQL task only concentrate on\nutilizing historical user inputs. In this work, in addition to using encoders\nto capture historical information of user inputs, we propose a database schema\ninteraction graph encoder to utilize historicalal information of database\nschema items. In decoding phase, we introduce a gate mechanism to weigh the\nimportance of different vocabularies and then make the prediction of SQL\ntokens. We evaluate our model on the benchmark SParC and CoSQL datasets, which\nare two large complex context-dependent cross-domain text-to-SQL datasets. Our\nmodel outperforms previous state-of-the-art model by a large margin and\nachieves new state-of-the-art results on the two datasets. The comparison and\nablation results demonstrate the efficacy of our model and the usefulness of\nthe database schema interaction graph encoder.", "published": "2020-11-11 12:56:21", "link": "http://arxiv.org/abs/2011.05744v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assessment of text coherence based on the cohesion estimation", "abstract": "In this paper, a graph-based coherence estimation method based on the\ncohesion estimation is suggested. Our method uses a graph-based approach to\nprovide a user with an understanding of the evaluation process. Moreover, it\ncan be applied to different languages, therefore, the effectiveness of this\nmethod is examined on the set of English, Chinese, and Arabic texts.", "published": "2020-11-11 14:05:32", "link": "http://arxiv.org/abs/2011.05788v1", "categories": ["cs.CL", "68U15", "I.7"], "primary_category": "cs.CL"}
{"title": "Situated Data, Situated Systems: A Methodology to Engage with Power\n  Relations in Natural Language Processing Research", "abstract": "We propose a bias-aware methodology to engage with power relations in natural\nlanguage processing (NLP) research. NLP research rarely engages with bias in\nsocial contexts, limiting its ability to mitigate bias. While researchers have\nrecommended actions, technical methods, and documentation practices, no\nmethodology exists to integrate critical reflections on bias with technical NLP\nmethods. In this paper, after an extensive and interdisciplinary literature\nreview, we contribute a bias-aware methodology for NLP research. We also\ncontribute a definition of biased text, a discussion of the implications of\nbiased NLP systems, and a case study demonstrating how we are executing the\nbias-aware methodology in research on archival metadata descriptions.", "published": "2020-11-11 17:04:55", "link": "http://arxiv.org/abs/2011.05911v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Overview of CAPITEL Shared Tasks at IberLEF 2020: Named Entity\n  Recognition and Universal Dependencies Parsing", "abstract": "We present the results of the CAPITEL-EVAL shared task, held in the context\nof the IberLEF 2020 competition series. CAPITEL-EVAL consisted on two subtasks:\n(1) Named Entity Recognition and Classification and (2) Universal Dependency\nparsing. For both, the source data was a newly annotated corpus, CAPITEL, a\ncollection of Spanish articles in the newswire domain. A total of seven teams\nparticipated in CAPITEL-EVAL, with a total of 13 runs submitted across all\nsubtasks. Data, results and further information about this task can be found at\nsites.google.com/view/capitel2020.", "published": "2020-11-11 17:44:57", "link": "http://arxiv.org/abs/2011.05932v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text Augmentation for Language Models in High Error Recognition Scenario", "abstract": "We examine the effect of data augmentation for training of language models\nfor speech recognition. We compare augmentation based on global error\nstatistics with one based on per-word unigram statistics of ASR errors and\nobserve that it is better to only pay attention the global substitution,\ndeletion and insertion rates. This simple scheme also performs consistently\nbetter than label smoothing and its sampled variants. Additionally, we\ninvestigate into the behavior of perplexity estimated on augmented data, but\nconclude that it gives no better prediction of the final error rate. Our best\naugmentation scheme increases the absolute WER improvement from second-pass\nrescoring from 1.1 % to 1.9 % absolute on the CHiMe-6 challenge.", "published": "2020-11-11 20:21:21", "link": "http://arxiv.org/abs/2011.06056v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring the Value of Personalized Word Embeddings", "abstract": "In this paper, we introduce personalized word embeddings, and examine their\nvalue for language modeling. We compare the performance of our proposed\nprediction model when using personalized versus generic word representations,\nand study how these representations can be leveraged for improved performance.\nWe provide insight into what types of words can be more accurately predicted\nwhen building personalized models. Our results show that a subset of words\nbelonging to specific psycholinguistic categories tend to vary more in their\nrepresentations across users and that combining generic and personalized word\nembeddings yields the best performance, with a 4.7% relative reduction in\nperplexity. Additionally, we show that a language model using personalized word\nembeddings can be effectively used for authorship attribution.", "published": "2020-11-11 20:23:09", "link": "http://arxiv.org/abs/2011.06057v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Matching Theory and Data with Personal-ITY: What a Corpus of Italian\n  YouTube Comments Reveals About Personality", "abstract": "As a contribution to personality detection in languages other than English,\nwe rely on distant supervision to create Personal-ITY, a novel corpus of\nYouTube comments in Italian, where authors are labelled with personality\ntraits. The traits are derived from one of the mainstream personality theories\nin psychology research, named MBTI. Using personality prediction experiments,\nwe (i) study the task of personality prediction in itself on our corpus as well\nas on TwiSty, a Twitter dataset also annotated with MBTI labels; (ii) carry out\nan extensive, in-depth analysis of the features used by the classifier, and\nview them specifically under the light of the original theory that we used to\ncreate the corpus in the first place. We observe that no single model is best\nat personality detection, and that while some traits are easier than others to\ndetect, and also to match back to theory, for other, less frequent traits the\npicture is much more blurred.", "published": "2020-11-11 12:45:33", "link": "http://arxiv.org/abs/2011.07009v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Investigation of Potential Function Designs for Neural CRF", "abstract": "The neural linear-chain CRF model is one of the most widely-used approach to\nsequence labeling. In this paper, we investigate a series of increasingly\nexpressive potential functions for neural CRF models, which not only integrate\nthe emission and transition functions, but also explicitly take the\nrepresentations of the contextual words as input. Our extensive experiments\nshow that the decomposed quadrilinear potential function based on the vector\nrepresentations of two neighboring labels and two neighboring words\nconsistently achieves the best performance.", "published": "2020-11-11 07:32:18", "link": "http://arxiv.org/abs/2011.05604v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Rule-Based Approach for Party-Based Sentiment Analysis in Legal Opinion\n  Texts", "abstract": "A document which elaborates opinions and arguments related to the previous\ncourt cases is known as a legal opinion text. Lawyers and legal officials have\nto spend considerable effort and time to obtain the required information\nmanually from those documents when dealing with new legal cases. Hence, it\nprovides much convenience to those individuals if there is a way to automate\nthe process of extracting information from legal opinion texts. Party-based\nsentiment analysis will play a key role in the automation system by identifying\nopinion values with respect to each legal parties in legal texts.", "published": "2020-11-11 10:07:14", "link": "http://arxiv.org/abs/2011.05675v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CalibreNet: Calibration Networks for Multilingual Sequence Labeling", "abstract": "Lack of training data in low-resource languages presents huge challenges to\nsequence labeling tasks such as named entity recognition (NER) and machine\nreading comprehension (MRC). One major obstacle is the errors on the boundary\nof predicted answers. To tackle this problem, we propose CalibreNet, which\npredicts answers in two steps. In the first step, any existing sequence\nlabeling method can be adopted as a base model to generate an initial answer.\nIn the second step, CalibreNet refines the boundary of the initial answer. To\ntackle the challenge of lack of training data in low-resource languages, we\ndedicatedly develop a novel unsupervised phrase boundary recovery pre-training\ntask to enhance the multilingual boundary detection capability of CalibreNet.\nExperiments on two cross-lingual benchmark datasets show that the proposed\napproach achieves SOTA results on zero-shot cross-lingual NER and MRC tasks.", "published": "2020-11-11 11:59:49", "link": "http://arxiv.org/abs/2011.05723v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Audrey: A Personalized Open-Domain Conversational Bot", "abstract": "Conversational Intelligence requires that a person engage on informational,\npersonal and relational levels. Advances in Natural Language Understanding have\nhelped recent chatbots succeed at dialog on the informational level. However,\ncurrent techniques still lag for conversing with humans on a personal level and\nfully relating to them. The University of Michigan's submission to the Alexa\nPrize Grand Challenge 3, Audrey, is an open-domain conversational chat-bot that\naims to engage customers on these levels through interest driven conversations\nguided by customers' personalities and emotions. Audrey is built from\nsocially-aware models such as Emotion Detection and a Personal Understanding\nModule to grasp a deeper understanding of users' interests and desires. Our\narchitecture interacts with customers using a hybrid approach balanced between\nknowledge-driven response generators and context-driven neural response\ngenerators to cater to all three levels of conversations. During the\nsemi-finals period, we achieved an average cumulative rating of 3.25 on a 1-5\nLikert scale.", "published": "2020-11-11 17:02:01", "link": "http://arxiv.org/abs/2011.05910v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On End-to-end Multi-channel Time Domain Speech Separation in Reverberant\n  Environments", "abstract": "This paper introduces a new method for multi-channel time domain speech\nseparation in reverberant environments. A fully-convolutional neural network\nstructure has been used to directly separate speech from multiple microphone\nrecordings, with no need of conventional spatial feature extraction. To reduce\nthe influence of reverberation on spatial feature extraction, a dereverberation\npre-processing method has been applied to further improve the separation\nperformance. A spatialized version of wsj0-2mix dataset has been simulated to\nevaluate the proposed system. Both source separation and speech recognition\nperformance of the separated signals have been evaluated objectively.\nExperiments show that the proposed fully-convolutional network improves the\nsource separation metric and the word error rate (WER) by more than 13% and 50%\nrelative, respectively, over a reference system with conventional features.\nApplying dereverberation as pre-processing to the proposed system can further\nreduce the WER by 29% relative using an acoustic model trained on clean and\nreverberated data.", "published": "2020-11-11 18:25:07", "link": "http://arxiv.org/abs/2011.05958v1", "categories": ["eess.AS", "cs.CL", "68T10"], "primary_category": "eess.AS"}
{"title": "The Impact of Text Presentation on Translator Performance", "abstract": "Widely used computer-aided translation (CAT) tools divide documents into\nsegments such as sentences and arrange them in a side-by-side, spreadsheet-like\nview. We present the first controlled evaluation of these design choices on\ntranslator performance, measuring speed and accuracy in three experimental text\nprocessing tasks. We find significant evidence that sentence-by-sentence\npresentation enables faster text reproduction and within-sentence error\nidentification compared to unsegmented text, and that a top-and-bottom\narrangement of source and target sentences enables faster text reproduction\ncompared to a side-by-side arrangement. For revision, on the other hand, our\nresults suggest that presenting unsegmented text results in the highest\naccuracy and time efficiency. Our findings have direct implications for best\npractices in designing CAT tools.", "published": "2020-11-11 18:50:18", "link": "http://arxiv.org/abs/2011.05978v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Spoken Language Interaction with Robots: Research Issues and\n  Recommendations, Report from the NSF Future Directions Workshop", "abstract": "With robotics rapidly advancing, more effective human-robot interaction is\nincreasingly needed to realize the full potential of robots for society. While\nspoken language must be part of the solution, our ability to provide spoken\nlanguage interaction capabilities is still very limited. The National Science\nFoundation accordingly convened a workshop, bringing together speech, language,\nand robotics researchers to discuss what needs to be done. The result is this\nreport, in which we identify key scientific and engineering advances needed.\n  Our recommendations broadly relate to eight general themes. First, meeting\nhuman needs requires addressing new challenges in speech technology and user\nexperience design. Second, this requires better models of the social and\ninteractive aspects of language use. Third, for robustness, robots need\nhigher-bandwidth communication with users and better handling of uncertainty,\nincluding simultaneous consideration of multiple hypotheses and goals. Fourth,\nmore powerful adaptation methods are needed, to enable robots to communicate in\nnew environments, for new tasks, and with diverse user populations, without\nextensive re-engineering or the collection of massive training data. Fifth,\nsince robots are embodied, speech should function together with other\ncommunication modalities, such as gaze, gesture, posture, and motion. Sixth,\nsince robots operate in complex environments, speech components need access to\nrich yet efficient representations of what the robot knows about objects,\nlocations, noise sources, the user, and other humans. Seventh, since robots\noperate in real time, their speech and language processing components must\nalso. Eighth, in addition to more research, we need more work on infrastructure\nand resources, including shareable software modules and internal interfaces,\ninexpensive hardware, baseline systems, and diverse corpora.", "published": "2020-11-11 03:45:34", "link": "http://arxiv.org/abs/2011.05533v1", "categories": ["cs.RO", "cs.CL", "cs.HC"], "primary_category": "cs.RO"}
{"title": "E-commerce Query-based Generation based on User Review", "abstract": "With the increasing number of merchandise on e-commerce platforms, users tend\nto refer to reviews of other shoppers to decide which product they should buy.\nHowever, with so many reviews of a product, users often have to spend lots of\ntime browsing through reviews talking about product attributes they do not care\nabout. We want to establish a system that can automatically summarize and\nanswer user's product specific questions.\n  In this study, we propose a novel seq2seq based text generation model to\ngenerate answers to user's question based on reviews posted by previous users.\nGiven a user question and/or target sentiment polarity, we extract aspects of\ninterest and generate an answer that summarizes previous relevant user reviews.\nSpecifically, our model performs attention between input reviews and target\naspects during encoding and is conditioned on both review rating and input\ncontext during decoding. We also incorporate a pre-trained auxiliary rating\nclassifier to improve model performance and accelerate convergence during\ntraining. Experiments using real-world e-commerce dataset show that our model\nachieves improvement in performance compared to previously introduced models.", "published": "2020-11-11 04:58:31", "link": "http://arxiv.org/abs/2011.05546v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Low-resource expressive text-to-speech using data augmentation", "abstract": "While recent neural text-to-speech (TTS) systems perform remarkably well,\nthey typically require a substantial amount of recordings from the target\nspeaker reading in the desired speaking style. In this work, we present a novel\n3-step methodology to circumvent the costly operation of recording large\namounts of target data in order to build expressive style voices with as little\nas 15 minutes of such recordings. First, we augment data via voice conversion\nby leveraging recordings in the desired speaking style from other speakers.\nNext, we use that synthetic data on top of the available recordings to train a\nTTS model. Finally, we fine-tune that model to further increase quality. Our\nevaluations show that the proposed changes bring significant improvements over\nnon-augmented models across many perceived aspects of synthesised speech. We\ndemonstrate the proposed approach on 2 styles (newscaster and conversational),\non various speakers, and on both single and multi-speaker models, illustrating\nthe robustness of our approach.", "published": "2020-11-11 11:22:37", "link": "http://arxiv.org/abs/2011.05707v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The Role of the Crowd in Countering Misinformation: A Case Study of the\n  COVID-19 Infodemic", "abstract": "Fact checking by professionals is viewed as a vital defense in the fight\nagainst misinformation.While fact checking is important and its impact has been\nsignificant, fact checks could have limited visibility and may not reach the\nintended audience, such as those deeply embedded in polarized communities.\nConcerned citizens (i.e., the crowd), who are users of the platforms where\nmisinformation appears, can play a crucial role in disseminating fact-checking\ninformation and in countering the spread of misinformation. To explore if this\nis the case, we conduct a data-driven study of misinformation on the Twitter\nplatform, focusing on tweets related to the COVID-19 pandemic, analyzing the\nspread of misinformation, professional fact checks, and the crowd response to\npopular misleading claims about COVID-19. In this work, we curate a dataset of\nfalse claims and statements that seek to challenge or refute them. We train a\nclassifier to create a novel dataset of 155,468 COVID-19-related tweets,\ncontaining 33,237 false claims and 33,413 refuting arguments.Our findings show\nthat professional fact-checking tweets have limited volume and reach. In\ncontrast, we observe that the surge in misinformation tweets results in a quick\nresponse and a corresponding increase in tweets that refute such\nmisinformation. More importantly, we find contrasting differences in the way\nthe crowd refutes tweets, some tweets appear to be opinions, while others\ncontain concrete evidence, such as a link to a reputed source. Our work\nprovides insights into how misinformation is organically countered in social\nplatforms by some of their users and the role they play in amplifying\nprofessional fact checks.These insights could lead to development of tools and\nmechanisms that can empower concerned citizens in combating misinformation. The\ncode and data can be found in\nhttp://claws.cc.gatech.edu/covid_counter_misinformation.html.", "published": "2020-11-11 13:48:44", "link": "http://arxiv.org/abs/2011.05773v2", "categories": ["cs.SI", "cs.CL", "cs.CY"], "primary_category": "cs.SI"}
{"title": "Towards Semi-Supervised Semantics Understanding from Speech", "abstract": "Much recent work on Spoken Language Understanding (SLU) falls short in at\nleast one of three ways: models were trained on oracle text input and neglected\nthe Automatics Speech Recognition (ASR) outputs, models were trained to predict\nonly intents without the slot values, or models were trained on a large amount\nof in-house data. We proposed a clean and general framework to learn semantics\ndirectly from speech with semi-supervision from transcribed speech to address\nthese. Our framework is built upon pretrained end-to-end (E2E) ASR and\nself-supervised language models, such as BERT, and fine-tuned on a limited\namount of target SLU corpus. In parallel, we identified two inadequate settings\nunder which SLU models have been tested: noise-robustness and E2E semantics\nevaluation. We tested the proposed framework under realistic environmental\nnoises and with a new metric, the slots edit F1 score, on two public SLU\ncorpora. Experiments show that our SLU framework with speech as input can\nperform on par with those with oracle text as input in semantics understanding,\nwhile environmental noises are present, and a limited amount of labeled\nsemantics data is available.", "published": "2020-11-11 01:48:09", "link": "http://arxiv.org/abs/2011.06195v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Active Learning from Crowd in Document Screening", "abstract": "In this paper, we explore how to efficiently combine crowdsourcing and\nmachine intelligence for the problem of document screening, where we need to\nscreen documents with a set of machine-learning filters. Specifically, we focus\non building a set of machine learning classifiers that evaluate documents, and\nthen screen them efficiently. It is a challenging task since the budget is\nlimited and there are countless number of ways to spend the given budget on the\nproblem. We propose a multi-label active learning screening specific sampling\ntechnique -- objective-aware sampling -- for querying unlabelled documents for\nannotating. Our algorithm takes a decision on which machine filter need more\ntraining data and how to choose unlabeled items to annotate in order to\nminimize the risk of overall classification errors rather than minimizing a\nsingle filter error. We demonstrate that objective-aware sampling significantly\noutperforms the state of the art active learning sampling strategies.", "published": "2020-11-11 16:17:28", "link": "http://arxiv.org/abs/2012.02297v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "FastSVC: Fast Cross-Domain Singing Voice Conversion with Feature-wise\n  Linear Modulation", "abstract": "This paper presents FastSVC, a light-weight cross-domain singing voice\nconversion (SVC) system, which can achieve high conversion performance, with\ninference speed 4x faster than real-time on CPUs. FastSVC uses Conformer-based\nphoneme recognizer to extract singer-agnostic linguistic features from singing\nsignals. A feature-wise linear modulation based generator is used to synthesize\nwaveform directly from linguistic features, leveraging information from\nsine-excitation signals and loudness features. The waveform generator can be\ntrained conveniently using a multi-resolution spectral loss and an adversarial\nloss. Experimental results show that the proposed FastSVC system, compared with\na computationally heavy baseline system, can achieve comparable conversion\nperformance in some scenarios and significantly better conversion performance\nin other scenarios. Moreover, the proposed FastSVC system achieves desirable\ncross-lingual singing conversion performance. The inference speed of the\nFastSVC system is 3x and 70x faster than the baseline system on GPUs and CPUs,\nrespectively.", "published": "2020-11-11 12:13:50", "link": "http://arxiv.org/abs/2011.05731v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Efficient Neural Architecture Search for End-to-end Speech Recognition\n  via Straight-Through Gradients", "abstract": "Neural Architecture Search (NAS), the process of automating architecture\nengineering, is an appealing next step to advancing end-to-end Automatic Speech\nRecognition (ASR), replacing expert-designed networks with learned,\ntask-specific architectures. In contrast to early computational-demanding NAS\nmethods, recent gradient-based NAS methods, e.g., DARTS (Differentiable\nARchiTecture Search), SNAS (Stochastic NAS) and ProxylessNAS, significantly\nimprove the NAS efficiency. In this paper, we make two contributions. First, we\nrigorously develop an efficient NAS method via Straight-Through (ST) gradients,\ncalled ST-NAS. Basically, ST-NAS uses the loss from SNAS but uses ST to\nback-propagate gradients through discrete variables to optimize the loss, which\nis not revealed in ProxylessNAS. Using ST gradients to support sub-graph\nsampling is a core element to achieve efficient NAS beyond DARTS and SNAS.\nSecond, we successfully apply ST-NAS to end-to-end ASR. Experiments over the\nwidely benchmarked 80-hour WSJ and 300-hour Switchboard datasets show that the\nST-NAS induced architectures significantly outperform the human-designed\narchitecture across the two datasets. Strengths of ST-NAS such as architecture\ntransferability and low computation cost in memory and time are also reported.", "published": "2020-11-11 09:18:58", "link": "http://arxiv.org/abs/2011.05649v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Efficient Knowledge Distillation for RNN-Transducer Models", "abstract": "Knowledge Distillation is an effective method of transferring knowledge from\na large model to a smaller model. Distillation can be viewed as a type of model\ncompression, and has played an important role for on-device ASR applications.\nIn this paper, we develop a distillation method for RNN-Transducer (RNN-T)\nmodels, a popular end-to-end neural network architecture for streaming speech\nrecognition. Our proposed distillation loss is simple and efficient, and uses\nonly the \"y\" and \"blank\" posterior probabilities from the RNN-T output\nprobability lattice. We study the effectiveness of the proposed approach in\nimproving the accuracy of sparse RNN-T models obtained by gradually pruning a\nlarger uncompressed model, which also serves as the teacher during\ndistillation. With distillation of 60% and 90% sparse multi-domain RNN-T\nmodels, we obtain WER reductions of 4.3% and 12.1% respectively, on a noisy\nFarField eval set. We also present results of experiments on LibriSpeech, where\nthe introduction of the distillation loss yields a 4.8% relative WER reduction\non the test-other dataset for a small Conformer model.", "published": "2020-11-11 22:58:15", "link": "http://arxiv.org/abs/2011.06110v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Surrogate Source Model Learning for Determined Source Separation", "abstract": "We propose to learn surrogate functions of universal speech priors for\ndetermined blind speech separation. Deep speech priors are highly desirable due\nto their high modelling power, but are not compatible with state-of-the-art\nindependent vector analysis based on majorization-minimization (AuxIVA), since\nderiving the required surrogate function is not easy, nor always possible.\nInstead, we do away with exact majorization and directly approximate the\nsurrogate. Taking advantage of iterative source steering (ISS) updates, we back\npropagate the permutation invariant separation loss through multiple iterations\nof AuxIVA. ISS lends itself well to this task due to its lower complexity and\nlack of matrix inversion. Experiments show large improvements in terms of scale\ninvariant signal-to-distortion (SDR) ratio and word error rate compared to\nbaseline methods. Training is done on two speakers mixtures and we experiment\nwith two losses, SDR and coherence. We find that the learnt approximate\nsurrogate generalizes well on mixtures of three and four speakers without any\nmodification. We also demonstrate generalization to a different variation of\nthe AuxIVA update equations. The SDR loss leads to fastest convergence in\niterations, while coherence leads to the lowest word error rate (WER). We\nobtain as much as 36 % reduction in WER.", "published": "2020-11-11 04:30:30", "link": "http://arxiv.org/abs/2011.05540v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Recognizing More Emotions with Less Data Using Self-supervised Transfer\n  Learning", "abstract": "We propose a novel transfer learning method for speech emotion recognition\nallowing us to obtain promising results when only few training data is\navailable. With as low as 125 examples per emotion class, we were able to reach\na higher accuracy than a strong baseline trained on 8 times more data. Our\nmethod leverages knowledge contained in pre-trained speech representations\nextracted from models trained on a more general self-supervised task which\ndoesn't require human annotations, such as the wav2vec model. We provide\ndetailed insights on the benefits of our approach by varying the training data\nsize, which can help labeling teams to work more efficiently. We compare\nperformance with other popular methods on the IEMOCAP dataset, a\nwell-benchmarked dataset among the Speech Emotion Recognition (SER) research\ncommunity. Furthermore, we demonstrate that results can be greatly improved by\ncombining acoustic and linguistic knowledge from transfer learning. We align\nacoustic pre-trained representations with semantic representations from the\nBERT model through an attention-based recurrent neural network. Performance\nimproves significantly when combining both modalities and scales with the\namount of data. When trained on the full IEMOCAP dataset, we reach a new\nstate-of-the-art of 73.9% unweighted accuracy (UA).", "published": "2020-11-11 06:18:31", "link": "http://arxiv.org/abs/2011.05585v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Deep Time Delay Neural Network for Speech Enhancement with Full Data\n  Learning", "abstract": "Recurrent neural networks (RNNs) have shown significant improvements in\nrecent years for speech enhancement. However, the model complexity and\ninference time cost of RNNs are much higher than deep feed-forward neural\nnetworks (DNNs). Therefore, these limit the applications of speech enhancement.\nThis paper proposes a deep time delay neural network (TDNN) for speech\nenhancement with full data learning. The TDNN has excellent potential for\ncapturing long range temporal contexts, which utilizes a modular and\nincremental design. Besides, the TDNN preserves the feed-forward structure so\nthat its inference cost is comparable to standard DNN. To make full use of the\ntraining data, we propose a full data learning method for speech enhancement.\nMore specifically, we not only use the noisy-to-clean (input-to-target) to\ntrain the enhanced model, but also the clean-to-clean and noise-to-silence\ndata. Therefore, all of the training data can be used to train the enhanced\nmodel. Our experiments are conducted on TIMIT dataset. Experimental results\nshow that our proposed method could achieve a better performance than DNN and\ncomparable even better performance than BLSTM. Meanwhile, compared with the\nBLSTM, the proposed method drastically reduce the inference time.", "published": "2020-11-11 06:32:37", "link": "http://arxiv.org/abs/2011.05591v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
