{"title": "Improving Readability for Automatic Speech Recognition Transcription", "abstract": "Modern Automatic Speech Recognition (ASR) systems can achieve high\nperformance in terms of recognition accuracy. However, a perfectly accurate\ntranscript still can be challenging to read due to grammatical errors,\ndisfluency, and other errata common in spoken communication. Many downstream\ntasks and human readers rely on the output of the ASR system; therefore, errors\nintroduced by the speaker and ASR system alike will be propagated to the next\ntask in the pipeline. In this work, we propose a novel NLP task called ASR\npost-processing for readability (APR) that aims to transform the noisy ASR\noutput into a readable text for humans and downstream tasks while maintaining\nthe semantic meaning of the speaker. In addition, we describe a method to\naddress the lack of task-specific data by synthesizing examples for the APR\ntask using the datasets collected for Grammatical Error Correction (GEC)\nfollowed by text-to-speech (TTS) and ASR. Furthermore, we propose metrics\nborrowed from similar tasks to evaluate performance on the APR task. We compare\nfine-tuned models based on several open-sourced and adapted pre-trained models\nwith the traditional pipeline method. Our results suggest that finetuned models\nimprove the performance on the APR task significantly, hinting at the potential\nbenefits of using APR systems. We hope that the read, understand, and rewrite\napproach of our work can serve as a basis that many NLP tasks and human readers\ncan benefit from.", "published": "2020-04-09 09:26:42", "link": "http://arxiv.org/abs/2004.04438v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Multilingual Study of Multi-Sentence Compression using Word\n  Vertex-Labeled Graphs and Integer Linear Programming", "abstract": "Multi-Sentence Compression (MSC) aims to generate a short sentence with the\nkey information from a cluster of similar sentences. MSC enables summarization\nand question-answering systems to generate outputs combining fully formed\nsentences from one or several documents. This paper describes an Integer Linear\nProgramming method for MSC using a vertex-labeled graph to select different\nkeywords, with the goal of generating more informative sentences while\nmaintaining their grammaticality. Our system is of good quality and outperforms\nthe state of the art for evaluations led on news datasets in three languages:\nFrench, Portuguese and Spanish. We led both automatic and manual evaluations to\ndetermine the informativeness and the grammaticality of compressions for each\ndataset. In additional tests, which take advantage of the fact that the length\nof compressions can be modulated, we still improve ROUGE scores with shorter\noutput sentences.", "published": "2020-04-09 10:35:16", "link": "http://arxiv.org/abs/2004.04468v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Recommendation Chart of Domains for Cross-Domain Sentiment\n  Analysis:Findings of A 20 Domain Study", "abstract": "Cross-domain sentiment analysis (CDSA) helps to address the problem of data\nscarcity in scenarios where labelled data for a domain (known as the target\ndomain) is unavailable or insufficient. However, the decision to choose a\ndomain (known as the source domain) to leverage from is, at best, intuitive. In\nthis paper, we investigate text similarity metrics to facilitate source domain\nselection for CDSA. We report results on 20 domains (all possible pairs) using\n11 similarity metrics. Specifically, we compare CDSA performance with these\nmetrics for different domain-pairs to enable the selection of a suitable source\ndomain, given a target domain. These metrics include two novel metrics for\nevaluating domain adaptability to help source domain selection of labelled data\nand utilize word and sentence-based embeddings as metrics for unlabelled data.\nThe goal of our experiments is a recommendation chart that gives the K best\nsource domains for CDSA for a given target domain. We show that the best K\nsource domains returned by our similarity metrics have a precision of over 50%,\nfor varying values of K.", "published": "2020-04-09 10:55:01", "link": "http://arxiv.org/abs/2004.04478v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Injecting Numerical Reasoning Skills into Language Models", "abstract": "Large pre-trained language models (LMs) are known to encode substantial\namounts of linguistic information. However, high-level reasoning skills, such\nas numerical reasoning, are difficult to learn from a language-modeling\nobjective only. Consequently, existing models for numerical reasoning have used\nspecialized architectures with limited flexibility. In this work, we show that\nnumerical reasoning is amenable to automatic data generation, and thus one can\ninject this skill into pre-trained LMs, by generating large amounts of data,\nand training in a multi-task setup. We show that pre-training our model,\nGenBERT, on this data, dramatically improves performance on DROP (49.3\n$\\rightarrow$ 72.3 F1), reaching performance that matches state-of-the-art\nmodels of comparable size, while using a simple and general-purpose\nencoder-decoder architecture. Moreover, GenBERT generalizes well to math word\nproblem datasets, while maintaining high performance on standard RC tasks. Our\napproach provides a general recipe for injecting skills into large pre-trained\nLMs, whenever the skill is amenable to automatic data augmentation.", "published": "2020-04-09 11:14:56", "link": "http://arxiv.org/abs/2004.04487v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MuTual: A Dataset for Multi-Turn Dialogue Reasoning", "abstract": "Non-task oriented dialogue systems have achieved great success in recent\nyears due to largely accessible conversation data and the development of deep\nlearning techniques. Given a context, current systems are able to yield a\nrelevant and fluent response, but sometimes make logical mistakes because of\nweak reasoning capabilities. To facilitate the conversation reasoning research,\nwe introduce MuTual, a novel dataset for Multi-Turn dialogue Reasoning,\nconsisting of 8,860 manually annotated dialogues based on Chinese student\nEnglish listening comprehension exams. Compared to previous benchmarks for\nnon-task oriented dialogue systems, MuTual is much more challenging since it\nrequires a model that can handle various reasoning problems. Empirical results\nshow that state-of-the-art methods only reach 71%, which is far behind the\nhuman performance of 94%, indicating that there is ample room for improving\nreasoning ability. MuTual is available at https://github.com/Nealcly/MuTual.", "published": "2020-04-09 11:42:33", "link": "http://arxiv.org/abs/2004.04494v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reducing Gender Bias in Neural Machine Translation as a Domain\n  Adaptation Problem", "abstract": "Training data for NLP tasks often exhibits gender bias in that fewer\nsentences refer to women than to men. In Neural Machine Translation (NMT)\ngender bias has been shown to reduce translation quality, particularly when the\ntarget language has grammatical gender. The recent WinoMT challenge set allows\nus to measure this effect directly (Stanovsky et al, 2019).\n  Ideally we would reduce system bias by simply debiasing all data prior to\ntraining, but achieving this effectively is itself a challenge. Rather than\nattempt to create a `balanced' dataset, we use transfer learning on a small set\nof trusted, gender-balanced examples. This approach gives strong and consistent\nimprovements in gender debiasing with much less computational cost than\ntraining from scratch.\n  A known pitfall of transfer learning on new domains is `catastrophic\nforgetting', which we address both in adaptation and in inference. During\nadaptation we show that Elastic Weight Consolidation allows a performance\ntrade-off between general translation quality and bias reduction. During\ninference we propose a lattice-rescoring scheme which outperforms all systems\nevaluated in Stanovsky et al (2019) on WinoMT with no degradation of general\ntest set BLEU, and we show this scheme can be applied to remove gender bias in\nthe output of `black box` online commercial MT systems. We demonstrate our\napproach translating from English into three languages with varied linguistic\nproperties and data availability.", "published": "2020-04-09 11:55:13", "link": "http://arxiv.org/abs/2004.04498v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Training for Unsupervised Neural Machine Translation in Unbalanced\n  Training Data Scenarios", "abstract": "Unsupervised neural machine translation (UNMT) that relies solely on massive\nmonolingual corpora has achieved remarkable results in several translation\ntasks. However, in real-world scenarios, massive monolingual corpora do not\nexist for some extremely low-resource languages such as Estonian, and UNMT\nsystems usually perform poorly when there is not adequate training corpus for\none language. In this paper, we first define and analyze the unbalanced\ntraining data scenario for UNMT. Based on this scenario, we propose UNMT\nself-training mechanisms to train a robust UNMT system and improve its\nperformance in this case. Experimental results on several language pairs show\nthat the proposed methods substantially outperform conventional UNMT systems.", "published": "2020-04-09 12:07:17", "link": "http://arxiv.org/abs/2004.04507v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Interpretability Analysis for Named Entity Recognition to Understand\n  System Predictions and How They Can Improve", "abstract": "Named Entity Recognition systems achieve remarkable performance on domains\nsuch as English news. It is natural to ask: What are these models actually\nlearning to achieve this? Are they merely memorizing the names themselves? Or\nare they capable of interpreting the text and inferring the correct entity type\nfrom the linguistic context? We examine these questions by contrasting the\nperformance of several variants of LSTM-CRF architectures for named entity\nrecognition, with some provided only representations of the context as\nfeatures. We also perform similar experiments for BERT. We find that context\nrepresentations do contribute to system performance, but that the main factor\ndriving high performance is learning the name tokens themselves. We enlist\nhuman annotators to evaluate the feasibility of inferring entity types from the\ncontext alone and find that, while people are not able to infer the entity type\neither for the majority of the errors made by the context-only system, there is\nsome room for improvement. A system should be able to recognize any name in a\npredictive context correctly and our experiments indicate that current systems\nmay be further improved by such capability.", "published": "2020-04-09 14:37:12", "link": "http://arxiv.org/abs/2004.04564v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BLEURT: Learning Robust Metrics for Text Generation", "abstract": "Text generation has made significant advances in the last few years. Yet,\nevaluation metrics have lagged behind, as the most popular choices (e.g., BLEU\nand ROUGE) may correlate poorly with human judgments. We propose BLEURT, a\nlearned evaluation metric based on BERT that can model human judgments with a\nfew thousand possibly biased training examples. A key aspect of our approach is\na novel pre-training scheme that uses millions of synthetic examples to help\nthe model generalize. BLEURT provides state-of-the-art results on the last\nthree years of the WMT Metrics shared task and the WebNLG Competition dataset.\nIn contrast to a vanilla BERT-based approach, it yields superior results even\nwhen the training data is scarce and out-of-distribution.", "published": "2020-04-09 17:26:52", "link": "http://arxiv.org/abs/2004.04696v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Language Neutrality of Pre-trained Multilingual Representations", "abstract": "Multilingual contextual embeddings, such as multilingual BERT and\nXLM-RoBERTa, have proved useful for many multi-lingual tasks. Previous work\nprobed the cross-linguality of the representations indirectly using zero-shot\ntransfer learning on morphological and syntactic tasks. We instead investigate\nthe language-neutrality of multilingual contextual embeddings directly and with\nrespect to lexical semantics. Our results show that contextual embeddings are\nmore language-neutral and, in general, more informative than aligned static\nword-type embeddings, which are explicitly trained for language neutrality.\nContextual embeddings are still only moderately language-neutral by default, so\nwe propose two simple methods for achieving stronger language neutrality:\nfirst, by unsupervised centering of the representation for each language and\nsecond, by fitting an explicit projection on small parallel data. Besides, we\nshow how to reach state-of-the-art accuracy on language identification and\nmatch the performance of statistical methods for word alignment of parallel\nsentences without using parallel data.", "published": "2020-04-09 19:50:32", "link": "http://arxiv.org/abs/2004.05160v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using Punkt for Sentence Segmentation in non-Latin Scripts: Experiments\n  on Kurdish (Sorani) Texts", "abstract": "Segmentation is a fundamental step for most Natural Language Processing\ntasks. The Kurdish language is a multi-dialect, under-resourced language which\nis written in different scripts. The lack of various segmented corpora is one\nof the major bottlenecks in Kurdish language processing. We used Punkt, an\nunsupervised machine learning method, to segment a Kurdish corpus of Sorani\ndialect, written in Persian-Arabic script. According to the literature, studies\non using Punkt on non-Latin data are scanty. In our experiment, we achieved an\nF1 score of 91.10% and had an Error Rate of 16.32%. The high Error Rate is\nmainly due to the situation of abbreviations in Kurdish and partly because of\nordinal numerals. The data is publicly available at\nhttps://github.com/KurdishBLARK/ KTC-Segmented for non-commercial use under the\nCC BY-NC-SA 4.0 licence.", "published": "2020-04-09 06:44:08", "link": "http://arxiv.org/abs/2004.14134v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Conversation Learner -- A Machine Teaching Tool for Building Dialog\n  Managers for Task-Oriented Dialog Systems", "abstract": "Traditionally, industry solutions for building a task-oriented dialog system\nhave relied on helping dialog authors define rule-based dialog managers,\nrepresented as dialog flows. While dialog flows are intuitively interpretable\nand good for simple scenarios, they fall short of performance in terms of the\nflexibility needed to handle complex dialogs. On the other hand, purely\nmachine-learned models can handle complex dialogs, but they are considered to\nbe black boxes and require large amounts of training data. In this\ndemonstration, we showcase Conversation Learner, a machine teaching tool for\nbuilding dialog managers. It combines the best of both approaches by enabling\ndialog authors to create a dialog flow using familiar tools, converting the\ndialog flow into a parametric model (e.g., neural networks), and allowing\ndialog authors to improve the dialog manager (i.e., the parametric model) over\ntime by leveraging user-system dialog logs as training data through a machine\nteaching interface.", "published": "2020-04-09 00:10:54", "link": "http://arxiv.org/abs/2004.04305v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning to Scale Multilingual Representations for Vision-Language Tasks", "abstract": "Current multilingual vision-language models either require a large number of\nadditional parameters for each supported language, or suffer performance\ndegradation as languages are added. In this paper, we propose a Scalable\nMultilingual Aligned Language Representation (SMALR) that supports many\nlanguages with few model parameters without sacrificing downstream task\nperformance. SMALR learns a fixed size language-agnostic representation for\nmost words in a multilingual vocabulary, keeping language-specific features for\njust a few. We use a masked cross-language modeling loss to align features with\ncontext from other languages. Additionally, we propose a cross-lingual\nconsistency module that ensures predictions made for a query and its machine\ntranslation are comparable. The effectiveness of SMALR is demonstrated with ten\ndiverse languages, over twice the number supported in vision-language tasks to\ndate. We evaluate on multilingual image-sentence retrieval and outperform prior\nwork by 3-4% with less than 1/5th the training parameters compared to other\nword embedding methods.", "published": "2020-04-09 01:03:44", "link": "http://arxiv.org/abs/2004.04312v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Large Arabic Twitter Dataset on COVID-19", "abstract": "The 2019 coronavirus disease (COVID-19), emerged late December 2019 in China,\nis now rapidly spreading across the globe. At the time of writing this paper,\nthe number of global confirmed cases has passed two millions and half with over\n180,000 fatalities. Many countries have enforced strict social distancing\npolicies to contain the spread of the virus. This have changed the daily life\nof tens of millions of people, and urged people to turn their discussions\nonline, e.g., via online social media sites like Twitter. In this work, we\ndescribe the first Arabic tweets dataset on COVID-19 that we have been\ncollecting since January 1st, 2020. The dataset would help researchers and\npolicy makers in studying different societal issues related to the pandemic.\nMany other tasks related to behavioral change, information sharing,\nmisinformation and rumors spreading can also be analyzed.", "published": "2020-04-09 01:07:12", "link": "http://arxiv.org/abs/2004.04315v2", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Calibrating Structured Output Predictors for Natural Language Processing", "abstract": "We address the problem of calibrating prediction confidence for output\nentities of interest in natural language processing (NLP) applications. It is\nimportant that NLP applications such as named entity recognition and question\nanswering produce calibrated confidence scores for their predictions,\nespecially if the system is to be deployed in a safety-critical domain such as\nhealthcare. However, the output space of such structured prediction models is\noften too large to adapt binary or multi-class calibration methods directly. In\nthis study, we propose a general calibration scheme for output entities of\ninterest in neural-network based structured prediction models. Our proposed\nmethod can be used with any binary class calibration scheme and a neural\nnetwork model. Additionally, we show that our calibration method can also be\nused as an uncertainty-aware, entity-specific decoding step to improve the\nperformance of the underlying model at no additional training cost or data\nrequirements. We show that our method outperforms current calibration\ntechniques for named-entity-recognition, part-of-speech and question answering.\nWe also improve our model's performance from our decoding step across several\ntasks and benchmark datasets. Our method improves the calibration and model\nperformance on out-of-domain test scenarios as well.", "published": "2020-04-09 04:14:46", "link": "http://arxiv.org/abs/2004.04361v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On Optimal Transformer Depth for Low-Resource Language Translation", "abstract": "Transformers have shown great promise as an approach to Neural Machine\nTranslation (NMT) for low-resource languages. However, at the same time,\ntransformer models remain difficult to optimize and require careful tuning of\nhyper-parameters to be useful in this setting. Many NMT toolkits come with a\nset of default hyper-parameters, which researchers and practitioners often\nadopt for the sake of convenience and avoiding tuning. These configurations,\nhowever, have been optimized for large-scale machine translation data sets with\nseveral millions of parallel sentences for European languages like English and\nFrench. In this work, we find that the current trend in the field to use very\nlarge models is detrimental for low-resource languages, since it makes training\nmore difficult and hurts overall performance, confirming previous observations.\nWe see our work as complementary to the Masakhane project (\"Masakhane\" means\n\"We Build Together\" in isiZulu.) In this spirit, low-resource NMT systems are\nnow being built by the community who needs them the most. However, many in the\ncommunity still have very limited access to the type of computational resources\nrequired for building extremely large models promoted by industrial research.\nTherefore, by showing that transformer models perform well (and often best) at\nlow-to-moderate depth, we hope to convince fellow researchers to devote less\ncomputational resources, as well as time, to exploring overly large models\nduring the development of these systems.", "published": "2020-04-09 08:25:02", "link": "http://arxiv.org/abs/2004.04418v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Global Public Health Surveillance using Media Reports: Redesigning GPHIN", "abstract": "Global public health surveillance relies on reporting structures and\ntransmission of trustworthy health reports. But in practice, these processes\nmay not always be fast enough, or are hindered by procedural, technical, or\npolitical barriers. GPHIN, the Global Public Health Intelligence Network, was\ndesigned in the late 1990s to scour mainstream news for health events, as that\ntravels faster and more freely. This paper outlines the next generation of\nGPHIN, which went live in 2017, and reports on design decisions underpinning\nits new functions and innovations.", "published": "2020-04-09 15:34:51", "link": "http://arxiv.org/abs/2004.04596v1", "categories": ["cs.CL", "cs.IR", "J.3; H.5.0"], "primary_category": "cs.CL"}
{"title": "Violent music vs violence and music: Drill rap and violent crime in\n  London", "abstract": "The current policy of removing drill music videos from social media platforms\nsuch as YouTube remains controversial because it risks conflating the\nco-occurrence of drill rap and violence with a causal chain of the two.\nEmpirically, we revisit the question of whether there is evidence to support\nthe conjecture that drill music and gang violence are linked. We provide new\nempirical insights suggesting that: i) drill music lyrics have not become more\nnegative over time if anything they have become more positive; ii) individual\ndrill artists have similar sentiment trajectories to other artists in the drill\ngenre, and iii) there is no meaningful relationship between drill music and\nreal-life violence when compared to three kinds of police-recorded violent\ncrime data in London. We suggest ideas for new work that can help build a\nmuch-needed evidence base around the problem.", "published": "2020-04-09 15:35:26", "link": "http://arxiv.org/abs/2004.04598v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Translation Artifacts in Cross-lingual Transfer Learning", "abstract": "Both human and machine translation play a central role in cross-lingual\ntransfer learning: many multilingual datasets have been created through\nprofessional translation services, and using machine translation to translate\neither the test set or the training set is a widely used transfer technique. In\nthis paper, we show that such translation process can introduce subtle\nartifacts that have a notable impact in existing cross-lingual models. For\ninstance, in natural language inference, translating the premise and the\nhypothesis independently can reduce the lexical overlap between them, which\ncurrent models are highly sensitive to. We show that some previous findings in\ncross-lingual transfer learning need to be reconsidered in the light of this\nphenomenon. Based on the gained insights, we also improve the state-of-the-art\nin XNLI for the translate-test and zero-shot approaches by 4.3 and 2.8 points,\nrespectively.", "published": "2020-04-09 17:54:30", "link": "http://arxiv.org/abs/2004.04721v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FST Morphology for the Endangered Skolt Sami Language", "abstract": "We present advances in the development of a FST-based morphological analyzer\nand generator for Skolt Sami. Like other minority Uralic languages, Skolt Sami\nexhibits a rich morphology, on the one hand, and there is little golden\nstandard material for it, on the other. This makes NLP approaches for its study\ndifficult without a solid morphological analysis. The language is severely\nendangered and the work presented in this paper forms a part of a greater whole\nin its revitalization efforts. Furthermore, we intersperse our description with\nfacilitation and description practices not well documented in the\ninfrastructure. Currently, the analyzer covers over 30,000 Skolt Sami words in\n148 inflectional paradigms and over 12 derivational forms.", "published": "2020-04-09 20:47:15", "link": "http://arxiv.org/abs/2004.04803v1", "categories": ["cs.CL", "cs.FL"], "primary_category": "cs.CL"}
{"title": "Towards Exploiting Implicit Human Feedback for Improving RDF2vec\n  Embeddings", "abstract": "RDF2vec is a technique for creating vector space embeddings from an RDF\nknowledge graph, i.e., representing each entity in the graph as a vector. It\nfirst creates sequences of nodes by performing random walks on the graph. In a\nsecond step, those sequences are processed by the word2vec algorithm for\ncreating the actual embeddings. In this paper, we explore the use of external\nedge weights for guiding the random walks. As edge weights, transition\nprobabilities between pages in Wikipedia are used as a proxy for the human\nfeedback for the importance of an edge. We show that in some scenarios, RDF2vec\nutilizing those transition probabilities can outperform both RDF2vec based on\nrandom walks as well as the usage of graph internal edge weights.", "published": "2020-04-09 08:39:19", "link": "http://arxiv.org/abs/2004.04423v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Automatic Differentiation in ROOT", "abstract": "In mathematics and computer algebra, automatic differentiation (AD) is a set\nof techniques to evaluate the derivative of a function specified by a computer\nprogram. AD exploits the fact that every computer program, no matter how\ncomplicated, executes a sequence of elementary arithmetic operations (addition,\nsubtraction, multiplication, division, etc.), elementary functions (exp, log,\nsin, cos, etc.) and control flow statements. AD takes source code of a function\nas input and produces source code of the derived function. By applying the\nchain rule repeatedly to these operations, derivatives of arbitrary order can\nbe computed automatically, accurately to working precision, and using at most a\nsmall constant factor more arithmetic operations than the original program.\n  This paper presents AD techniques available in ROOT, supported by Cling, to\nproduce derivatives of arbitrary C/C++ functions through implementing source\ncode transformation and employing the chain rule of differential calculus in\nboth forward mode and reverse mode. We explain its current integration for\ngradient computation in TFormula. We demonstrate the correctness and\nperformance improvements in ROOT's fitting algorithms.", "published": "2020-04-09 09:18:50", "link": "http://arxiv.org/abs/2004.04435v1", "categories": ["cs.MS", "cs.CL", "cs.SE"], "primary_category": "cs.MS"}
{"title": "PANDORA Talks: Personality and Demographics on Reddit", "abstract": "Personality and demographics are important variables in social sciences,\nwhile in NLP they can aid in interpretability and removal of societal biases.\nHowever, datasets with both personality and demographic labels are scarce. To\naddress this, we present PANDORA, the first large-scale dataset of Reddit\ncomments labeled with three personality models (including the well-established\nBig 5 model) and demographics (age, gender, and location) for more than 10k\nusers. We showcase the usefulness of this dataset on three experiments, where\nwe leverage the more readily available data from other personality models to\npredict the Big 5 traits, analyze gender classification biases arising from\npsycho-demographic variables, and carry out a confirmatory and exploratory\nanalysis based on psychological theories. Finally, we present benchmark\nprediction models for all personality and demographic variables.", "published": "2020-04-09 10:08:05", "link": "http://arxiv.org/abs/2004.04460v3", "categories": ["cs.CL", "cs.CY", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Re-conceptualising the Language Game Paradigm in the Framework of\n  Multi-Agent Reinforcement Learning", "abstract": "In this paper, we formulate the challenge of re-conceptualising the language\ngame experimental paradigm in the framework of multi-agent reinforcement\nlearning (MARL). If successful, future language game experiments will benefit\nfrom the rapid and promising methodological advances in the MARL community,\nwhile future MARL experiments on learning emergent communication will benefit\nfrom the insights and results gained from language game experiments. We\nstrongly believe that this cross-pollination has the potential to lead to major\nbreakthroughs in the modelling of how human-like languages can emerge and\nevolve in multi-agent systems.", "published": "2020-04-09 17:55:15", "link": "http://arxiv.org/abs/2004.04722v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "primary_category": "cs.AI"}
{"title": "More Bang for Your Buck: Natural Perturbation for Robust Question\n  Answering", "abstract": "While recent models have achieved human-level scores on many NLP datasets, we\nobserve that they are considerably sensitive to small changes in input. As an\nalternative to the standard approach of addressing this issue by constructing\ntraining sets of completely new examples, we propose doing so via minimal\nperturbation of examples. Specifically, our approach involves first collecting\na set of seed examples and then applying human-driven natural perturbations (as\nopposed to rule-based machine perturbations), which often change the gold label\nas well. Local perturbations have the advantage of being relatively easier (and\nhence cheaper) to create than writing out completely new examples. To evaluate\nthe impact of this phenomenon, we consider a recent question-answering dataset\n(BoolQ) and study the benefit of our approach as a function of the perturbation\ncost ratio, the relative cost of perturbing an existing question vs. creating a\nnew one from scratch. We find that when natural perturbations are moderately\ncheaper to create, it is more effective to train models using them: such models\nexhibit higher robustness and better generalization, while retaining\nperformance on the original BoolQ dataset.", "published": "2020-04-09 23:12:39", "link": "http://arxiv.org/abs/2004.04849v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Two halves of a meaningful text are statistically different", "abstract": "Which statistical features distinguish a meaningful text (possibly written in\nan unknown system) from a meaningless set of symbols? Here we answer this\nquestion by comparing features of the first half of a text to its second half.\nThis comparison can uncover hidden effects, because the halves have the same\nvalues of many parameters (style, genre {\\it etc}). We found that the first\nhalf has more different words and more rare words than the second half. Also,\nwords in the first half are distributed less homogeneously over the text in the\nsense of of the difference between the frequency and the inverse spatial\nperiod. These differences hold for the significant majority of several hundred\nrelatively short texts we studied. The statistical significance is confirmed\nvia the Wilcoxon test. Differences disappear after random permutation of words\nthat destroys the linear structure of the text. The differences reveal a\ntemporal asymmetry in meaningful texts, which is confirmed by showing that\ntexts are much better compressible in their natural way (i.e. along the\nnarrative) than in the word-inverted form. We conjecture that these results\nconnect the semantic organization of a text (defined by the flow of its\nnarrative) to its statistical features.", "published": "2020-04-09 20:00:12", "link": "http://arxiv.org/abs/2004.06474v1", "categories": ["cs.CL", "cs.DS", "physics.soc-ph"], "primary_category": "cs.CL"}
{"title": "Att-HACK: An Expressive Speech Database with Social Attitudes", "abstract": "This paper presents Att-HACK, the first large database of acted speech with\nsocial attitudes. Available databases of expressive speech are rare and very\noften restricted to the primary emotions: anger, joy, sadness, fear. This\ngreatly limits the scope of the research on expressive speech. Besides, a\nfundamental aspect of speech prosody is always ignored and missing from such\ndatabases: its variety, i.e. the possibility to repeat an utterance while\nvarying its prosody. This paper represents a first attempt to widen the scope\nof expressivity in speech, by providing a database of acted speech with social\nattitudes: friendly, seductive, dominant, and distant. The proposed database\ncomprises 25 speakers interpreting 100 utterances in 4 social attitudes, with\n3-5 repetitions each per attitude for a total of around 30 hours of speech. The\nAtt-HACK is freely available for academic research under a Creative Commons\nLicence.", "published": "2020-04-09 08:09:59", "link": "http://arxiv.org/abs/2004.04410v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "MDCNN-SID: Multi-scale Dilated Convolution Network for Singer\n  Identification", "abstract": "Most singer identification methods are processed in the frequency domain,\nwhich potentially leads to information loss during the spectral transformation.\nIn this paper, instead of the frequency domain, we propose an end-to-end\narchitecture that addresses this problem in the waveform domain. An encoder\nbased on Multi-scale Dilated Convolution Neural Networks (MDCNN) was introduced\nto generate wave embedding from the raw audio signal. Specifically, dilated\nconvolution layers are used in the proposed method to enlarge the receptive\nfield, aiming to extract song-level features. Furthermore, skip connection in\nthe backbone network integrates the multi-resolution acoustic features learned\nby the stack of convolution layers. Then, the obtained wave embedding is passed\ninto the following networks for singer identification. In experiments, the\nproposed method achieves comparable performance on the benchmark dataset of\nArtist20, which significantly improves related works.", "published": "2020-04-09 05:31:08", "link": "http://arxiv.org/abs/2004.04371v3", "categories": ["eess.AS", "cs.MM"], "primary_category": "eess.AS"}
{"title": "Fast frequency discrimination and phoneme recognition using a biomimetic\n  membrane coupled to a neural network", "abstract": "In the human ear, the basilar membrane plays a central role in sound\nrecognition. When excited by sound, this membrane responds with a\nfrequency-dependent displacement pattern that is detected and identified by the\nauditory hair cells combined with the human neural system. Inspired by this\nstructure, we designed and fabricated an artificial membrane that produces a\nspatial displacement pattern in response to an audible signal, which we used to\ntrain a convolutional neural network (CNN). When trained with single frequency\ntones, this system can unambiguously distinguish tones closely spaced in\nfrequency. When instead trained to recognize spoken vowels, this system\noutperforms existing methods for phoneme recognition, including the discrete\nFourier transform (DFT), zoom FFT and chirp z-transform, especially when tested\nin short time windows. This sound recognition scheme therefore promises\nsignificant benefits in fast and accurate sound identification compared to\nexisting methods.", "published": "2020-04-09 10:07:12", "link": "http://arxiv.org/abs/2004.04459v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "physics.bio-ph"], "primary_category": "eess.AS"}
{"title": "Advancing Speech Synthesis using EEG", "abstract": "In this paper we introduce attention-regression model to demonstrate\npredicting acoustic features from electroencephalography (EEG) features\nrecorded in parallel with spoken sentences. First we demonstrate predicting\nacoustic features directly from EEG features using our attention model and then\nwe demonstrate predicting acoustic features from EEG features using a two-step\napproach where in the first step we use our attention model to predict\narticulatory features from EEG features and then in second step another\nattention-regression model is trained to transform the predicted articulatory\nfeatures to acoustic features. Our proposed attention-regression model\ndemonstrates superior performance compared to the regression model introduced\nby authors in [1] when tested using their data set for majority of the subjects\nduring test time. The results presented in this paper further advances the work\ndescribed by authors in [1].", "published": "2020-04-09 23:58:40", "link": "http://arxiv.org/abs/2004.04731v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
