{"title": "Neural networks can understand compositional functions that humans do\n  not, in the context of emergent communication", "abstract": "We show that it is possible to craft transformations that, applied to\ncompositional grammars, result in grammars that neural networks can learn\neasily, but humans do not. This could explain the disconnect between current\nmetrics of compositionality, that are arguably human-centric, and the ability\nof neural networks to generalize to unseen examples. We propose to use the\ntransformations as a benchmark, ICY, which could be used to measure aspects of\nthe compositional inductive bias of networks, and to search for networks with\nsimilar compositional inductive biases to humans. As an example of this\napproach, we propose a hierarchical model, HU-RNN, which shows an inductive\nbias towards position-independent, word-like groups of tokens.", "published": "2021-03-06 19:25:37", "link": "http://arxiv.org/abs/2103.04180v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "JPS-daprinfo: A Dataset for Japanese Dialog Act Analysis and\n  People-related Information Detection", "abstract": "We conducted a labeling work on a spoken Japanese dataset (I-JAS) for the\ntext classification, which contains 50 interview dialogues of two-way Japanese\nconversation that discuss the participants' past present and future. Each\ndialogue is 30 minutes long. From this dataset, we selected the interview\ndialogues of native Japanese speakers as the samples. Given the dataset, we\nannotated sentences with 13 labels. The labeling work was conducted by native\nJapanese speakers who have experiences with data annotation. The total amount\nof the annotated samples is 20130.", "published": "2021-03-06 12:15:23", "link": "http://arxiv.org/abs/2103.11786v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Perspectives and Prospects on Transformer Architecture for Cross-Modal\n  Tasks with Language and Vision", "abstract": "Transformer architectures have brought about fundamental changes to\ncomputational linguistic field, which had been dominated by recurrent neural\nnetworks for many years. Its success also implies drastic changes in\ncross-modal tasks with language and vision, and many researchers have already\ntackled the issue. In this paper, we review some of the most critical\nmilestones in the field, as well as overall trends on how transformer\narchitecture has been incorporated into visuolinguistic cross-modal tasks.\nFurthermore, we discuss its current limitations and speculate upon some of the\nprospects that we find imminent.", "published": "2021-03-06 05:44:27", "link": "http://arxiv.org/abs/2103.04037v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Changing the Narrative Perspective: From Deictic to Anaphoric Point of\n  View", "abstract": "We introduce the task of changing the narrative point of view, where\ncharacters are assigned a narrative perspective that is different from the one\noriginally used by the writer. The resulting shift in the narrative point of\nview alters the reading experience and can be used as a tool in fiction writing\nor to generate types of text ranging from educational to self-help and\nself-diagnosis. We introduce a benchmark dataset containing a wide range of\ntypes of narratives annotated with changes in point of view from deictic (first\nor second person) to anaphoric (third person) and describe a pipeline for\nprocessing raw text that relies on a neural architecture for mention selection.\nEvaluations on the new benchmark dataset show that the proposed architecture\nsubstantially outperforms the baselines by generating mentions that are less\nambiguous and more natural.", "published": "2021-03-06 19:03:42", "link": "http://arxiv.org/abs/2103.04176v1", "categories": ["cs.CL", "cs.AI", "I.2.7; J.5"], "primary_category": "cs.CL"}
{"title": "Extracting Semantic Process Information from the Natural Language in\n  Event Logs", "abstract": "Process mining focuses on the analysis of recorded event data in order to\ngain insights about the true execution of business processes. While\nfoundational process mining techniques treat such data as sequences of abstract\nevents, more advanced techniques depend on the availability of specific kinds\nof information, such as resources in organizational mining and business objects\nin artifact-centric analysis. However, this information is generally not\nreadily available, but rather associated with events in an ad hoc manner, often\neven as part of unstructured textual attributes. Given the size and complexity\nof event logs, this calls for automated support to extract such process\ninformation and, thereby, enable advanced process mining techniques. In this\npaper, we present an approach that achieves this through so-called semantic\nrole labeling of event data. We combine the analysis of textual attribute\nvalues, based on a state-of-the-art language model, with a novel attribute\nclassification technique. In this manner, our approach extracts information\nabout up to eight semantic roles per event. We demonstrate the approach's\nefficacy through a quantitative evaluation using a broad range of event logs\nand demonstrate the usefulness of the extracted information in a case study.", "published": "2021-03-06 08:39:04", "link": "http://arxiv.org/abs/2103.11761v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Putting Humans in the Natural Language Processing Loop: A Survey", "abstract": "How can we design Natural Language Processing (NLP) systems that learn from\nhuman feedback? There is a growing research body of Human-in-the-loop (HITL)\nNLP frameworks that continuously integrate human feedback to improve the model\nitself. HITL NLP research is nascent but multifarious -- solving various NLP\nproblems, collecting diverse feedback from different people, and applying\ndifferent methods to learn from collected feedback. We present a survey of HITL\nNLP work from both Machine Learning (ML) and Human-Computer Interaction (HCI)\ncommunities that highlights its short yet inspiring history, and thoroughly\nsummarize recent frameworks focusing on their tasks, goals, human interactions,\nand feedback learning methods. Finally, we discuss future directions for\nintegrating human feedback in the NLP development loop.", "published": "2021-03-06 06:26:00", "link": "http://arxiv.org/abs/2103.04044v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Zero-Shot Entity Retrieval through Effective Dense\n  Representations", "abstract": "Entity Linking (EL) seeks to align entity mentions in text to entries in a\nknowledge-base and is usually comprised of two phases: candidate generation and\ncandidate ranking. While most methods focus on the latter, it is the candidate\ngeneration phase that sets an upper bound to both time and accuracy performance\nof the overall EL system. This work's contribution is a significant improvement\nin candidate generation which thus raises the performance threshold for EL, by\ngenerating candidates that include the gold entity in the least candidate set\n(top-K). We propose a simple approach that efficiently embeds mention-entity\npairs in dense space through a BERT-based bi-encoder. Specifically, we extend\n(Wu et al., 2020) by introducing a new pooling function and incorporating\nentity type side-information. We achieve a new state-of-the-art 84.28% accuracy\non top-50 candidates on the Zeshel dataset, compared to the previous 82.06% on\nthe top-64 of (Wu et al., 2020). We report the results from extensive\nexperimentation using our proposed model on both seen and unseen entity\ndatasets. Our results suggest that our method could be a useful complement to\nexisting EL approaches.", "published": "2021-03-06 17:00:09", "link": "http://arxiv.org/abs/2103.04156v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "I.2.7; H.3.3"], "primary_category": "cs.CL"}
{"title": "Analysis and Assessment of Controllability of an Expressive Deep\n  Learning-based TTS system", "abstract": "In this paper, we study the controllability of an Expressive TTS system\ntrained on a dataset for a continuous control. The dataset is the Blizzard 2013\ndataset based on audiobooks read by a female speaker containing a great\nvariability in styles and expressiveness. Controllability is evaluated with\nboth an objective and a subjective experiment. The objective assessment is\nbased on a measure of correlation between acoustic features and the dimensions\nof the latent space representing expressiveness. The subjective assessment is\nbased on a perceptual experiment in which users are shown an interface for\nControllable Expressive TTS and asked to retrieve a synthetic utterance whose\nexpressiveness subjectively corresponds to that a reference utterance.", "published": "2021-03-06 11:06:13", "link": "http://arxiv.org/abs/2103.04097v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Investigating on Incorporating Pretrained and Learnable Speaker\n  Representations for Multi-Speaker Multi-Style Text-to-Speech", "abstract": "The few-shot multi-speaker multi-style voice cloning task is to synthesize\nutterances with voice and speaking style similar to a reference speaker given\nonly a few reference samples. In this work, we investigate different speaker\nrepresentations and proposed to integrate pretrained and learnable speaker\nrepresentations. Among different types of embeddings, the embedding pretrained\nby voice conversion achieves the best performance. The FastSpeech 2 model\ncombined with both pretrained and learnable speaker representations shows great\ngeneralization ability on few-shot speakers and achieved 2nd place in the\none-shot track of the ICASSP 2021 M2VoC challenge.", "published": "2021-03-06 10:14:33", "link": "http://arxiv.org/abs/2103.04088v5", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
