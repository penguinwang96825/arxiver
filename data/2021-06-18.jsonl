{"title": "PRGC: Potential Relation and Global Correspondence Based Joint\n  Relational Triple Extraction", "abstract": "Joint extraction of entities and relations from unstructured texts is a\ncrucial task in information extraction. Recent methods achieve considerable\nperformance but still suffer from some inherent limitations, such as redundancy\nof relation prediction, poor generalization of span-based extraction and\ninefficiency. In this paper, we decompose this task into three subtasks,\nRelation Judgement, Entity Extraction and Subject-object Alignment from a novel\nperspective and then propose a joint relational triple extraction framework\nbased on Potential Relation and Global Correspondence (PRGC). Specifically, we\ndesign a component to predict potential relations, which constrains the\nfollowing entity extraction to the predicted relation subset rather than all\nrelations; then a relation-specific sequence tagging component is applied to\nhandle the overlapping problem between subjects and objects; finally, a global\ncorrespondence component is designed to align the subject and object into a\ntriple with low-complexity. Extensive experiments show that PRGC achieves\nstate-of-the-art performance on public benchmarks with higher efficiency and\ndelivers consistent performance gain on complex scenarios of overlapping\ntriples.", "published": "2021-06-18 03:38:07", "link": "http://arxiv.org/abs/2106.09895v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Continuity of Topic, Interaction, and Query: Learning to Quote in Online\n  Conversations", "abstract": "Quotations are crucial for successful explanations and persuasions in\ninterpersonal communications. However, finding what to quote in a conversation\nis challenging for both humans and machines. This work studies automatic\nquotation generation in an online conversation and explores how language\nconsistency affects whether a quotation fits the given context. Here, we\ncapture the contextual consistency of a quotation in terms of latent topics,\ninteractions with the dialogue history, and coherence to the query turn's\nexisting content. Further, an encoder-decoder neural framework is employed to\ncontinue the context with a quotation via language generation. Experiment\nresults on two large-scale datasets in English and Chinese demonstrate that our\nquotation generation model outperforms the state-of-the-art models. Further\nanalysis shows that topic, interaction, and query consistency are all helpful\nto learn how to quote in online conversations.", "published": "2021-06-18 03:38:48", "link": "http://arxiv.org/abs/2106.09896v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Neural Edge-Editing Approach for Document-Level Relation Graph\n  Extraction", "abstract": "In this paper, we propose a novel edge-editing approach to extract relation\ninformation from a document. We treat the relations in a document as a relation\ngraph among entities in this approach. The relation graph is iteratively\nconstructed by editing edges of an initial graph, which might be a graph\nextracted by another system or an empty graph. The way to edit edges is to\nclassify them in a close-first manner using the document and\ntemporally-constructed graph information; each edge is represented with a\ndocument context information by a pretrained transformer model and a graph\ncontext information by a graph convolutional neural network model. We evaluate\nour approach on the task to extract material synthesis procedures from\nmaterials science texts. The experimental results show the effectiveness of our\napproach in editing the graphs initialized by our in-house rule-based system\nand empty graphs.", "published": "2021-06-18 03:46:49", "link": "http://arxiv.org/abs/2106.09900v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Weakly Supervised Pre-Training for Multi-Hop Retriever", "abstract": "In multi-hop QA, answering complex questions entails iterative document\nretrieval for finding the missing entity of the question. The main steps of\nthis process are sub-question detection, document retrieval for the\nsub-question, and generation of a new query for the final document retrieval.\nHowever, building a dataset that contains complex questions with sub-questions\nand their corresponding documents requires costly human annotation. To address\nthe issue, we propose a new method for weakly supervised multi-hop retriever\npre-training without human efforts. Our method includes 1) a pre-training task\nfor generating vector representations of complex questions, 2) a scalable data\ngeneration method that produces the nested structure of question and\nsub-question as weak supervision for pre-training, and 3) a pre-training model\nstructure based on dense encoders. We conduct experiments to compare the\nperformance of our pre-trained retriever with several state-of-the-art models\non end-to-end multi-hop QA as well as document retrieval. The experimental\nresults show that our pre-trained retriever is effective and also robust on\nlimited data and computational resources.", "published": "2021-06-18 08:06:02", "link": "http://arxiv.org/abs/2106.09983v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SPBERT: An Efficient Pre-training BERT on SPARQL Queries for Question\n  Answering over Knowledge Graphs", "abstract": "In this paper, we propose SPBERT, a transformer-based language model\npre-trained on massive SPARQL query logs. By incorporating masked language\nmodeling objectives and the word structural objective, SPBERT can learn\ngeneral-purpose representations in both natural language and SPARQL query\nlanguage. We investigate how SPBERT and encoder-decoder architecture can be\nadapted for Knowledge-based QA corpora. We conduct exhaustive experiments on\ntwo additional tasks, including SPARQL Query Construction and Answer\nVerbalization Generation. The experimental results show that SPBERT can obtain\npromising results, achieving state-of-the-art BLEU scores on several of these\ntasks.", "published": "2021-06-18 08:39:26", "link": "http://arxiv.org/abs/2106.09997v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Recurrent Stacking of Layers in Neural Networks: An Application to\n  Neural Machine Translation", "abstract": "In deep neural network modeling, the most common practice is to stack a\nnumber of recurrent, convolutional, or feed-forward layers in order to obtain\nhigh-quality continuous space representations which in turn improves the\nquality of the network's prediction. Conventionally, each layer in the stack\nhas its own parameters which leads to a significant increase in the number of\nmodel parameters. In this paper, we propose to share parameters across all\nlayers thereby leading to a recurrently stacked neural network model. We report\non an extensive case study on neural machine translation (NMT), where we apply\nour proposed method to an encoder-decoder based neural network model, i.e., the\nTransformer model, and experiment with three Japanese--English translation\ndatasets. We empirically demonstrate that the translation quality of a model\nthat recurrently stacks a single layer 6 times, despite having significantly\nfewer parameters, approaches that of a model that stacks 6 layers where each\nlayer has different parameters. We also explore the limits of recurrent\nstacking where we train extremely deep NMT models. This paper also examines the\nutility of our recurrently stacked model as a student model through transfer\nlearning via leveraging pre-trained parameters and knowledge distillation, and\nshows that it compensates for the performance drops in translation quality that\nthe direct training of recurrently stacked model brings. We also show how\ntransfer learning helps in faster decoding on top of the already reduced number\nof parameters due to recurrent stacking. Finally, we analyze the effects of\nrecurrently stacked layers by visualizing the attentions of models that use\nrecurrently stacked layers and models that do not.", "published": "2021-06-18 08:48:01", "link": "http://arxiv.org/abs/2106.10002v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Financial Sentiment Analysis in a South African Landscape", "abstract": "Sentiment analysis as a sub-field of natural language processing has received\nincreased attention in the past decade enabling organisations to more\neffectively manage their reputation through online media monitoring. Many\ndrivers impact reputation, however, this thesis focuses only the aspect of\nfinancial performance and explores the gap with regards to financial sentiment\nanalysis in a South African context. Results showed that pre-trained sentiment\nanalysers are least effective for this task and that traditional lexicon-based\nand machine learning approaches are best suited to predict financial sentiment\nof news articles. The evaluated methods produced accuracies of 84\\%-94\\%. The\npredicted sentiments correlated quite well with share price and highlighted the\npotential use of sentiment as an indicator of financial performance. A main\ncontribution of the study was updating an existing sentiment dictionary for\nfinancial sentiment analysis. Model generalisation was less acceptable due to\nthe limited amount of training data used. Future work includes expanding the\ndata set to improve general usability and contribute to an open-source\nfinancial sentiment analyser for South African data.", "published": "2021-06-18 08:48:47", "link": "http://arxiv.org/abs/2106.10004v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Subjective Bias in Abstractive Summarization", "abstract": "Due to the subjectivity of the summarization, it is a good practice to have\nmore than one gold summary for each training document. However, many modern\nlarge-scale abstractive summarization datasets have only one-to-one samples\nwritten by different human with different styles. The impact of this phenomenon\nis understudied. We formulate the differences among possible multiple\nexpressions summarizing the same content as subjective bias and examine the\nrole of this bias in the context of abstractive summarization. In this paper a\nlightweight and effective method to extract the feature embeddings of\nsubjective styles is proposed. Results of summarization models trained on\nstyle-clustered datasets show that there are certain types of styles that lead\nto better convergence, abstraction and generalization. The reproducible code\nand generated summaries are available online.", "published": "2021-06-18 12:17:55", "link": "http://arxiv.org/abs/2106.10084v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Challenges and Limitations with the Metrics Measuring the Complexity of\n  Code-Mixed Text", "abstract": "Code-mixing is a frequent communication style among multilingual speakers\nwhere they mix words and phrases from two different languages in the same\nutterance of text or speech. Identifying and filtering code-mixed text is a\nchallenging task due to its co-existence with monolingual and noisy text. Over\nthe years, several code-mixing metrics have been extensively used to identify\nand validate code-mixed text quality. This paper demonstrates several inherent\nlimitations of code-mixing metrics with examples from the already existing\ndatasets that are popularly used across various experiments.", "published": "2021-06-18 13:26:48", "link": "http://arxiv.org/abs/2106.10123v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing user creativity: Semantic measures for idea generation", "abstract": "Human creativity generates novel ideas to solve real-world problems. This\nthereby grants us the power to transform the surrounding world and extend our\nhuman attributes beyond what is currently possible. Creative ideas are not just\nnew and unexpected, but are also successful in providing solutions that are\nuseful, efficient and valuable. Thus, creativity optimizes the use of available\nresources and increases wealth. The origin of human creativity, however, is\npoorly understood, and semantic measures that could predict the success of\ngenerated ideas are currently unknown. Here, we analyze a dataset of design\nproblem-solving conversations in real-world settings by using 49 semantic\nmeasures based on WordNet 3.1 and demonstrate that a divergence of semantic\nsimilarity, an increased information content, and a decreased polysemy predict\nthe success of generated ideas. The first feedback from clients also enhances\ninformation content and leads to a divergence of successful ideas in creative\nproblem solving. These results advance cognitive science by identifying\nreal-world processes in human problem solving that are relevant to the success\nof produced solutions and provide tools for real-time monitoring of problem\nsolving, student training and skill acquisition. A selected subset of\ninformation content (IC S\\'anchez-Batet) and semantic similarity\n(Lin/S\\'anchez-Batet) measures, which are both statistically powerful and\ncomputationally fast, could support the development of technologies for\ncomputer-assisted enhancements of human creativity or for the implementation of\ncreativity in machines endowed with general artificial intelligence.", "published": "2021-06-18 13:47:56", "link": "http://arxiv.org/abs/2106.10131v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Graph-based Joint Pandemic Concern and Relation Extraction on Twitter", "abstract": "Public concern detection provides potential guidance to the authorities for\ncrisis management before or during a pandemic outbreak. Detecting people's\nconcerns and attention from online social media platforms has been widely\nacknowledged as an effective approach to relieve public panic and prevent a\nsocial crisis. However, detecting concerns in time from massive information in\nsocial media turns out to be a big challenge, especially when sufficient\nmanually labeled data is in the absence of public health emergencies, e.g.,\nCOVID-19. In this paper, we propose a novel end-to-end deep learning model to\nidentify people's concerns and the corresponding relations based on Graph\nConvolutional Network and Bi-directional Long Short Term Memory integrated with\nConcern Graph. Except for the sequential features from BERT embeddings, the\nregional features of tweets can be extracted by the Concern Graph module, which\nnot only benefits the concern detection but also enables our model to be high\nnoise-tolerant. Thus, our model can address the issue of insufficient manually\nlabeled data. We conduct extensive experiments to evaluate the proposed model\nby using both manually labeled tweets and automatically labeled tweets. The\nexperimental results show that our model can outperform the state-of-art models\non real-world datasets.", "published": "2021-06-18 06:06:35", "link": "http://arxiv.org/abs/2106.09929v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Label prompt for multi-label text classification", "abstract": "One of the key problems in multi-label text classification is how to take\nadvantage of the correlation among labels. However, it is very challenging to\ndirectly model the correlations among labels in a complex and unknown label\nspace. In this paper, we propose a Label Mask multi-label text classification\nmodel (LM-MTC), which is inspired by the idea of cloze questions of language\nmodel. LM-MTC is able to capture implicit relationships among labels through\nthe powerful ability of pre-train language models. On the basis, we assign a\ndifferent token to each potential label, and randomly mask the token with a\ncertain probability to build a label based Masked Language Model (MLM). We\ntrain the MTC and MLM together, further improving the generalization ability of\nthe model. A large number of experiments on multiple datasets demonstrate the\neffectiveness of our method.", "published": "2021-06-18 11:54:33", "link": "http://arxiv.org/abs/2106.10076v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based\n  Masked Language-models", "abstract": "We introduce BitFit, a sparse-finetuning method where only the bias-terms of\nthe model (or a subset of them) are being modified. We show that with\nsmall-to-medium training data, applying BitFit on pre-trained BERT models is\ncompetitive with (and sometimes better than) fine-tuning the entire model. For\nlarger data, the method is competitive with other sparse fine-tuning methods.\nBesides their practical utility, these findings are relevant for the question\nof understanding the commonly-used process of finetuning: they support the\nhypothesis that finetuning is mainly about exposing knowledge induced by\nlanguage-modeling training, rather than learning new task-specific linguistic\nknowledge.", "published": "2021-06-18 16:09:21", "link": "http://arxiv.org/abs/2106.10199v5", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Process for Adapting Language Models to Society (PALMS) with\n  Values-Targeted Datasets", "abstract": "Language models can generate harmful and biased outputs and exhibit\nundesirable behavior according to a given cultural context. We propose a\nProcess for Adapting Language Models to Society (PALMS) with Values-Targeted\nDatasets, an iterative process to significantly change model behavior by\ncrafting and fine-tuning on a dataset that reflects a predetermined set of\ntarget values. We evaluate our process using three metrics: quantitative\nmetrics with human evaluations that score output adherence to a target value,\ntoxicity scoring on outputs; and qualitative metrics analyzing the most common\nword associated with a given social category. Through each iteration, we add\nadditional training dataset examples based on observed shortcomings from\nevaluations. PALMS performs significantly better on all metrics compared to\nbaseline and control models for a broad range of GPT-3 language model sizes\nwithout compromising capability integrity. We find that the effectiveness of\nPALMS increases with model size. We show that significantly adjusting language\nmodel behavior is feasible with a small, hand-curated dataset.", "published": "2021-06-18 19:38:28", "link": "http://arxiv.org/abs/2106.10328v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "GEM: A General Evaluation Benchmark for Multimodal Tasks", "abstract": "In this paper, we present GEM as a General Evaluation benchmark for\nMultimodal tasks. Different from existing datasets such as GLUE, SuperGLUE,\nXGLUE and XTREME that mainly focus on natural language tasks, GEM is a\nlarge-scale vision-language benchmark, which consists of GEM-I for\nimage-language tasks and GEM-V for video-language tasks. Comparing with\nexisting multimodal datasets such as MSCOCO and Flicker30K for image-language\ntasks, YouCook2 and MSR-VTT for video-language tasks, GEM is not only the\nlargest vision-language dataset covering image-language tasks and\nvideo-language tasks at the same time, but also labeled in multiple languages.\nWe also provide two baseline models for this benchmark. We will release the\ndataset, code and baseline models, aiming to advance the development of\nmultilingual multimodal research.", "published": "2021-06-18 03:14:13", "link": "http://arxiv.org/abs/2106.09889v1", "categories": ["cs.CL", "cs.CV", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Bad Characters: Imperceptible NLP Attacks", "abstract": "Several years of research have shown that machine-learning systems are\nvulnerable to adversarial examples, both in theory and in practice. Until now,\nsuch attacks have primarily targeted visual models, exploiting the gap between\nhuman and machine perception. Although text-based models have also been\nattacked with adversarial examples, such attacks struggled to preserve semantic\nmeaning and indistinguishability. In this paper, we explore a large class of\nadversarial examples that can be used to attack text-based models in a\nblack-box setting without making any human-perceptible visual modification to\ninputs. We use encoding-specific perturbations that are imperceptible to the\nhuman eye to manipulate the outputs of a wide range of Natural Language\nProcessing (NLP) systems from neural machine-translation pipelines to web\nsearch engines. We find that with a single imperceptible encoding injection --\nrepresenting one invisible character, homoglyph, reordering, or deletion -- an\nattacker can significantly reduce the performance of vulnerable models, and\nwith three injections most models can be functionally broken. Our attacks work\nagainst currently-deployed commercial systems, including those produced by\nMicrosoft and Google, in addition to open source models published by Facebook,\nIBM, and HuggingFace. This novel series of attacks presents a significant\nthreat to many language processing systems: an attacker can affect systems in a\ntargeted manner without any assumptions about the underlying model. We conclude\nthat text-based NLP systems require careful input sanitization, just like\nconventional applications, and that given such systems are now being deployed\nrapidly at scale, the urgent attention of architects and operators is required.", "published": "2021-06-18 03:42:56", "link": "http://arxiv.org/abs/2106.09898v2", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Investigating the Role of Negatives in Contrastive Representation\n  Learning", "abstract": "Noise contrastive learning is a popular technique for unsupervised\nrepresentation learning. In this approach, a representation is obtained via\nreduction to supervised learning, where given a notion of semantic similarity,\nthe learner tries to distinguish a similar (positive) example from a collection\nof random (negative) examples. The success of modern contrastive learning\npipelines relies on many parameters such as the choice of data augmentation,\nthe number of negative examples, and the batch size; however, there is limited\nunderstanding as to how these parameters interact and affect downstream\nperformance. We focus on disambiguating the role of one of these parameters:\nthe number of negative examples. Theoretically, we show the existence of a\ncollision-coverage trade-off suggesting that the optimal number of negative\nexamples should scale with the number of underlying concepts in the data.\nEmpirically, we scrutinize the role of the number of negatives in both NLP and\nvision tasks. In the NLP task, we find that the results broadly agree with our\ntheory, while our vision experiments are murkier with performance sometimes\neven being insensitive to the number of negatives. We discuss plausible\nexplanations for this behavior and suggest future directions to better align\ntheory and practice.", "published": "2021-06-18 06:44:16", "link": "http://arxiv.org/abs/2106.09943v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Synchronising speech segments with musical beats in Mandarin and English\n  singing", "abstract": "Generating synthesised singing voice with models trained on speech data has\nmany advantages due to the models' flexibility and controllability. However,\nsince the information about the temporal relationship between segments and\nbeats are lacking in speech training data, the synthesised singing may sound\noff-beat at times. Therefore, the availability of the information on the\ntemporal relationship between speech segments and music beats is crucial. The\ncurrent study investigated the segment-beat synchronisation in singing data,\nwith hypotheses formed based on the linguistics theories of P-centre and\nsonority hierarchy. A Mandarin corpus and an English corpus of professional\nsinging data were manually annotated and analysed. The results showed that the\npresence of musical beats was more dependent on segment duration than sonority.\nHowever, the sonority hierarchy and the P-centre theory were highly related to\nthe location of beats. Mandarin and English demonstrated cross-linguistic\nvariations despite exhibiting common patterns.", "published": "2021-06-18 10:32:27", "link": "http://arxiv.org/abs/2106.10045v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Unsupervised Domain Adaptation for Dysarthric Speech Detection via\n  Domain Adversarial Training and Mutual Information Minimization", "abstract": "Dysarthric speech detection (DSD) systems aim to detect characteristics of\nthe neuromotor disorder from speech. Such systems are particularly susceptible\nto domain mismatch where the training and testing data come from the source and\ntarget domains respectively, but the two domains may differ in terms of speech\nstimuli, disease etiology, etc. It is hard to acquire labelled data in the\ntarget domain, due to high costs of annotating sizeable datasets. This paper\nmakes a first attempt to formulate cross-domain DSD as an unsupervised domain\nadaptation (UDA) problem. We use labelled source-domain data and unlabelled\ntarget-domain data, and propose a multi-task learning strategy, including\ndysarthria presence classification (DPC), domain adversarial training (DAT) and\nmutual information minimization (MIM), which aim to learn\ndysarthria-discriminative and domain-invariant biomarker embeddings.\nSpecifically, DPC helps biomarker embeddings capture critical indicators of\ndysarthria; DAT forces biomarker embeddings to be indistinguishable in source\nand target domains; and MIM further reduces the correlation between biomarker\nembeddings and domain-related cues. By treating the UASPEECH and TORGO corpora\nrespectively as the source and target domains, experiments show that the\nincorporation of UDA attains absolute increases of 22.2% and 20.0% respectively\nin utterance-level weighted average recall and speaker-level accuracy.", "published": "2021-06-18 13:34:36", "link": "http://arxiv.org/abs/2106.10127v1", "categories": ["eess.AS", "cs.CL", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Predicting Gender by First Name Using Character-level Machine Learning", "abstract": "Predicting gender by the first name is not a simple task. In many\napplications, especially in the natural language processing (NLP) field, this\ntask may be necessary, mainly when considering foreign names. In this paper, we\nexamined and implemented several machine learning algorithms, such as extra\ntrees, KNN, Naive Bayes, SVM, random forest, gradient boosting, light GBM,\nlogistic regression, ridge classifier, and deep neural network models, such as\nMLP, RNN, GRU, CNN, and BiLSTM, to classify gender through the first name. A\ndataset of Brazilian names is used to train and evaluate the models. We\nanalyzed the accuracy, recall, precision, f1 score, and confusion matrix to\nmeasure the models' performances. The results indicate that the gender\nprediction can be performed from the feature extraction strategy looking at the\nnames as a set of strings. Some models accurately predict gender in more than\n95% of the cases. The recurrent models overcome the feedforward models in this\nbinary classification problem.", "published": "2021-06-18 14:45:59", "link": "http://arxiv.org/abs/2106.10156v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Fusion of Embeddings Networks for Robust Combination of Text Dependent\n  and Independent Speaker Recognition", "abstract": "By implicitly recognizing a user based on his/her speech input, speaker\nidentification enables many downstream applications, such as personalized\nsystem behavior and expedited shopping checkouts. Based on whether the speech\ncontent is constrained or not, both text-dependent (TD) and text-independent\n(TI) speaker recognition models may be used. We wish to combine the advantages\nof both types of models through an ensemble system to make more reliable\npredictions. However, any such combined approach has to be robust to incomplete\ninputs, i.e., when either TD or TI input is missing. As a solution we propose a\nfusion of embeddings network foenet architecture, combining joint learning with\nneural attention. We compare foenet with four competitive baseline methods on a\ndataset of voice assistant inputs, and show that it achieves higher accuracy\nthan the baseline and score fusion methods, especially in the presence of\nincomplete inputs.", "published": "2021-06-18 15:01:58", "link": "http://arxiv.org/abs/2106.10169v1", "categories": ["cs.LG", "cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "On-Device Personalization of Automatic Speech Recognition Models for\n  Disordered Speech", "abstract": "While current state-of-the-art Automatic Speech Recognition (ASR) systems\nachieve high accuracy on typical speech, they suffer from significant\nperformance degradation on disordered speech and other atypical speech\npatterns. Personalization of ASR models, a commonly applied solution to this\nproblem, is usually performed in a server-based training environment posing\nproblems around data privacy, delayed model-update times, and communication\ncost for copying data and models between mobile device and server\ninfrastructure. In this paper, we present an approach to on-device based ASR\npersonalization with very small amounts of speaker-specific data. We test our\napproach on a diverse set of 100 speakers with disordered speech and find\nmedian relative word error rate improvement of 71% with only 50 short\nutterances required per speaker. When tested on a voice-controlled home\nautomation platform, on-device personalized models show a median task success\nrate of 81%, compared to only 40% of the unadapted models.", "published": "2021-06-18 17:48:08", "link": "http://arxiv.org/abs/2106.10259v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Leveraging Language to Learn Program Abstractions and Search Heuristics", "abstract": "Inductive program synthesis, or inferring programs from examples of desired\nbehavior, offers a general paradigm for building interpretable, robust, and\ngeneralizable machine learning systems. Effective program synthesis depends on\ntwo key ingredients: a strong library of functions from which to build\nprograms, and an efficient search strategy for finding programs that solve a\ngiven task. We introduce LAPS (Language for Abstraction and Program Search), a\ntechnique for using natural language annotations to guide joint learning of\nlibraries and neurally-guided search models for synthesis. When integrated into\na state-of-the-art library learning system (DreamCoder), LAPS produces\nhigher-quality libraries and improves search efficiency and generalization on\nthree domains -- string editing, image composition, and abstract reasoning\nabout scenes -- even when no natural language hints are available at test time.", "published": "2021-06-18 15:08:47", "link": "http://arxiv.org/abs/2106.11053v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "QuaPy: A Python-Based Framework for Quantification", "abstract": "QuaPy is an open-source framework for performing quantification (a.k.a.\nsupervised prevalence estimation), written in Python. Quantification is the\ntask of training quantifiers via supervised learning, where a quantifier is a\npredictor that estimates the relative frequencies (a.k.a. prevalence values) of\nthe classes of interest in a sample of unlabelled data. While quantification\ncan be trivially performed by applying a standard classifier to each unlabelled\ndata item and counting how many data items have been assigned to each class, it\nhas been shown that this \"classify and count\" method is outperformed by methods\nspecifically designed for quantification. QuaPy provides implementations of a\nnumber of baseline methods and advanced quantification methods, of routines for\nquantification-oriented model selection, of several broadly accepted evaluation\nmeasures, and of robust evaluation protocols routinely used in the field. QuaPy\nalso makes available datasets commonly used for testing quantifiers, and offers\nvisualization tools for facilitating the analysis and interpretation of the\nresults. The software is open-source and publicly available under a BSD-3\nlicence via https://github.com/HLT-ISTI/QuaPy, and can be installed via pip\n(https://pypi.org/project/QuaPy/)", "published": "2021-06-18 13:57:11", "link": "http://arxiv.org/abs/2106.11057v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "VQMIVC: Vector Quantization and Mutual Information-Based Unsupervised\n  Speech Representation Disentanglement for One-shot Voice Conversion", "abstract": "One-shot voice conversion (VC), which performs conversion across arbitrary\nspeakers with only a single target-speaker utterance for reference, can be\neffectively achieved by speech representation disentanglement. Existing work\ngenerally ignores the correlation between different speech representations\nduring training, which causes leakage of content information into the speaker\nrepresentation and thus degrades VC performance. To alleviate this issue, we\nemploy vector quantization (VQ) for content encoding and introduce mutual\ninformation (MI) as the correlation metric during training, to achieve proper\ndisentanglement of content, speaker and pitch representations, by reducing\ntheir inter-dependencies in an unsupervised manner. Experimental results\nreflect the superiority of the proposed method in learning effective\ndisentangled speech representations for retaining source linguistic content and\nintonation variations, while capturing target speaker characteristics. In doing\nso, the proposed approach achieves higher speech naturalness and speaker\nsimilarity than current state-of-the-art one-shot VC systems. Our code,\npre-trained models and demo are available at\nhttps://github.com/Wendison/VQMIVC.", "published": "2021-06-18 13:50:38", "link": "http://arxiv.org/abs/2106.10132v1", "categories": ["eess.AS", "cs.CL", "cs.MM", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Analysis and Tuning of a Voice Assistant System for Dysfluent Speech", "abstract": "Dysfluencies and variations in speech pronunciation can severely degrade\nspeech recognition performance, and for many individuals with\nmoderate-to-severe speech disorders, voice operated systems do not work.\nCurrent speech recognition systems are trained primarily with data from fluent\nspeakers and as a consequence do not generalize well to speech with\ndysfluencies such as sound or word repetitions, sound prolongations, or audible\nblocks. The focus of this work is on quantitative analysis of a consumer speech\nrecognition system on individuals who stutter and production-oriented\napproaches for improving performance for common voice assistant tasks (i.e.,\n\"what is the weather?\"). At baseline, this system introduces a significant\nnumber of insertion and substitution errors resulting in intended speech Word\nError Rates (isWER) that are 13.64\\% worse (absolute) for individuals with\nfluency disorders. We show that by simply tuning the decoding parameters in an\nexisting hybrid speech recognition system one can improve isWER by 24\\%\n(relative) for individuals with fluency disorders. Tuning these parameters\ntranslates to 3.6\\% better domain recognition and 1.7\\% better intent\nrecognition relative to the default setup for the 18 study participants across\nall stuttering severities.", "published": "2021-06-18 20:58:34", "link": "http://arxiv.org/abs/2106.11759v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.CV", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Golos: Russian Dataset for Speech Research", "abstract": "This paper introduces a novel Russian speech dataset called Golos, a large\ncorpus suitable for speech research. The dataset mainly consists of recorded\naudio files manually annotated on the crowd-sourcing platform. The total\nduration of the audio is about 1240 hours. We have made the corpus freely\navailable to download, along with the acoustic model with CTC loss prepared on\nthis corpus. Additionally, transfer learning was applied to improve the\nperformance of the acoustic model. In order to evaluate the quality of the\ndataset with the beam-search algorithm, we have built a 3-gram language model\non the open Common Crawl dataset. The total word error rate (WER) metrics\nturned out to be about 3.3% and 11.5%.", "published": "2021-06-18 14:55:02", "link": "http://arxiv.org/abs/2106.10161v1", "categories": ["eess.AS", "E.m; I.5.1"], "primary_category": "eess.AS"}
{"title": "An Improved Single Step Non-autoregressive Transformer for Automatic\n  Speech Recognition", "abstract": "Non-autoregressive mechanisms can significantly decrease inference time for\nspeech transformers, especially when the single step variant is applied.\nPrevious work on CTC alignment-based single step non-autoregressive transformer\n(CASS-NAT) has shown a large real time factor (RTF) improvement over\nautoregressive transformers (AT). In this work, we propose several methods to\nimprove the accuracy of the end-to-end CASS-NAT, followed by performance\nanalyses. First, convolution augmented self-attention blocks are applied to\nboth the encoder and decoder modules. Second, we propose to expand the trigger\nmask (acoustic boundary) for each token to increase the robustness of CTC\nalignments. In addition, iterated loss functions are used to enhance the\ngradient update of low-layer parameters. Without using an external language\nmodel, the WERs of the improved CASS-NAT, when using the three methods, are\n3.1%/7.2% on Librispeech test clean/other sets and the CER is 5.4% on the\nAishell1 test set, achieving a 7%~21% relative WER/CER improvement. For the\nanalyses, we plot attention weight distributions in the decoders to visualize\nthe relationships between token-level acoustic embeddings. When the acoustic\nembeddings are visualized, we find that they have a similar behavior to word\nembeddings, which explains why the improved CASS-NAT performs similarly to AT.", "published": "2021-06-18 02:58:30", "link": "http://arxiv.org/abs/2106.09885v2", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "Low Resource German ASR with Untranscribed Data Spoken by Non-native\n  Children -- INTERSPEECH 2021 Shared Task SPAPL System", "abstract": "This paper describes the SPAPL system for the INTERSPEECH 2021 Challenge:\nShared Task on Automatic Speech Recognition for Non-Native Children's Speech in\nGerman. ~ 5 hours of transcribed data and ~ 60 hours of untranscribed data are\nprovided to develop a German ASR system for children. For the training of the\ntranscribed data, we propose a non-speech state discriminative loss (NSDL) to\nmitigate the influence of long-duration non-speech segments within speech\nutterances. In order to explore the use of the untranscribed data, various\napproaches are implemented and combined together to incrementally improve the\nsystem performance. First, bidirectional autoregressive predictive coding\n(Bi-APC) is used to learn initial parameters for acoustic modelling using the\nprovided untranscribed data. Second, incremental semi-supervised learning is\nfurther used to iteratively generate pseudo-transcribed data. Third, different\ndata augmentation schemes are used at different training stages to increase the\nvariability and size of the training data. Finally, a recurrent neural network\nlanguage model (RNNLM) is used for rescoring. Our system achieves a word error\nrate (WER) of 39.68% on the evaluation data, an approximately 12% relative\nimprovement over the official baseline (45.21%).", "published": "2021-06-18 07:36:26", "link": "http://arxiv.org/abs/2106.09963v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Improving Performance of Seen and Unseen Speech Style Transfer in\n  End-to-end Neural TTS", "abstract": "End-to-end neural TTS training has shown improved performance in speech style\ntransfer. However, the improvement is still limited by the training data in\nboth target styles and speakers. Inadequate style transfer performance occurs\nwhen the trained TTS tries to transfer the speech to a target style from a new\nspeaker with an unknown, arbitrary style. In this paper, we propose a new\napproach to style transfer for both seen and unseen styles, with disjoint,\nmulti-style datasets, i.e., datasets of different styles are recorded, each\nindividual style is by one speaker with multiple utterances. To encode the\nstyle information, we adopt an inverse autoregressive flow (IAF) structure to\nimprove the variational inference. The whole system is optimized to minimize a\nweighed sum of four different loss functions: 1) a reconstruction loss to\nmeasure the distortions in both source and target reconstructions; 2) an\nadversarial loss to \"fool\" a well-trained discriminator; 3) a style distortion\nloss to measure the expected style loss after the transfer; 4) a cycle\nconsistency loss to preserve the speaker identity of the source after the\ntransfer. Experiments demonstrate, both objectively and subjectively, the\neffectiveness of the proposed approach for seen and unseen style transfer\ntasks. The performance of the new approach is better and more robust than those\nof four baseline systems of the prior art.", "published": "2021-06-18 08:48:30", "link": "http://arxiv.org/abs/2106.10003v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Zero-Shot Federated Learning with New Classes for Audio Classification", "abstract": "Federated learning is an effective way of extracting insights from different\nuser devices while preserving the privacy of users. However, new classes with\ncompletely unseen data distributions can stream across any device in a\nfederated learning setting, whose data cannot be accessed by the global server\nor other users. To this end, we propose a unified zero-shot framework to handle\nthese aforementioned challenges during federated learning. We simulate two\nscenarios here -- 1) when the new class labels are not reported by the user,\nthe traditional FL setting is used; 2) when new class labels are reported by\nthe user, we synthesize Anonymized Data Impressions by calculating class\nsimilarity matrices corresponding to each device's new classes followed by\nunsupervised clustering to distinguish between new classes across different\nusers. Moreover, our proposed framework can also handle statistical\nheterogeneities in both labels and models across the participating users. We\nempirically evaluate our framework on-device across different communication\nrounds (FL iterations) with new classes in both local and global updates, along\nwith heterogeneous labels and models, on two widely used audio classification\napplications -- keyword spotting and urban sound classification, and observe an\naverage deterministic accuracy increase of ~4.041% and ~4.258% respectively.", "published": "2021-06-18 09:32:19", "link": "http://arxiv.org/abs/2106.10019v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
