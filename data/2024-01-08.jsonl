{"title": "Language Models Encode the Value of Numbers Linearly", "abstract": "Large language models (LLMs) have exhibited impressive competence in various\ntasks, but their internal mechanisms on mathematical problems are still\nunder-explored. In this paper, we study a fundamental question: how language\nmodels encode the value of numbers, a basic element in math. To study the\nquestion, we construct a synthetic dataset comprising addition problems and\nutilize linear probes to read out input numbers from the hidden states.\nExperimental results support the existence of encoded number values in LLMs on\ndifferent layers, and these values can be extracted via linear probes. Further\nexperiments show that LLMs store their calculation results in a similar manner,\nand we can intervene the output via simple vector additions, proving the causal\nconnection between encoded numbers and language model outputs. Our research\nprovides evidence that LLMs encode the value of numbers linearly, offering\ninsights for better exploring, designing, and utilizing numeric information in\nLLMs.", "published": "2024-01-08 08:54:22", "link": "http://arxiv.org/abs/2401.03735v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WEBDial, a Multi-domain, Multitask Statistical Dialogue Framework with\n  RDF", "abstract": "Typically available dialogue frameworks have adopted a semantic\nrepresentation based on dialogue-acts and slot-value pairs. Despite its\nsimplicity, this representation has disadvantages such as the lack of\nexpressivity, scalability and explainability. We present WEBDial: a dialogue\nframework that relies on a graph formalism by using RDF triples instead of\nslot-value pairs. We describe its overall architecture and the graph-based\nsemantic representation. We show its applicability from simple to complex\napplications, by varying the complexity of domains and tasks: from single\ndomain and tasks to multiple domains and complex tasks.", "published": "2024-01-08 14:08:33", "link": "http://arxiv.org/abs/2401.03905v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SpeechAgents: Human-Communication Simulation with Multi-Modal\n  Multi-Agent Systems", "abstract": "Human communication is a complex and diverse process that not only involves\nmultiple factors such as language, commonsense, and cultural backgrounds but\nalso requires the participation of multimodal information, such as speech.\nLarge Language Model (LLM)-based multi-agent systems have demonstrated\npromising performance in simulating human society. Can we leverage LLM-based\nmulti-agent systems to simulate human communication? However, current LLM-based\nmulti-agent systems mainly rely on text as the primary medium. In this paper,\nwe propose SpeechAgents, a multi-modal LLM based multi-agent system designed\nfor simulating human communication. SpeechAgents utilizes multi-modal LLM as\nthe control center for individual agent and employes multi-modal signals as the\nmedium for exchanged messages among agents. Additionally, we propose\nMulti-Agent Tuning to enhance the multi-agent capabilities of LLM without\ncompromising general abilities. To strengthen and evaluate the effectiveness of\nhuman communication simulation, we build the Human-Communication Simulation\nBenchmark. Experimental results demonstrate that SpeechAgents can simulate\nhuman communication dialogues with consistent content, authentic rhythm, and\nrich emotions and demonstrate excellent scalability even with up to 25 agents,\nwhich can apply to tasks such as drama creation and audio novels generation.\nCode and models will be open-sourced at https://github.\ncom/0nutation/SpeechAgents", "published": "2024-01-08 15:01:08", "link": "http://arxiv.org/abs/2401.03945v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TextMachina: Seamless Generation of Machine-Generated Text Datasets", "abstract": "Recent advancements in Large Language Models (LLMs) have led to high-quality\nMachine-Generated Text (MGT), giving rise to countless new use cases and\napplications. However, easy access to LLMs is posing new challenges due to\nmisuse. To address malicious usage, researchers have released datasets to\neffectively train models on MGT-related tasks. Similar strategies are used to\ncompile these datasets, but no tool currently unifies them. In this scenario,\nwe introduce TextMachina, a modular and extensible Python framework, designed\nto aid in the creation of high-quality, unbiased datasets to build robust\nmodels for MGT-related tasks such as detection, attribution, mixcase, or\nboundary detection. It provides a user-friendly pipeline that abstracts away\nthe inherent intricacies of building MGT datasets, such as LLM integrations,\nprompt templating, and bias mitigation. The quality of the datasets generated\nby TextMachina has been assessed in previous works, including shared tasks\nwhere more than one hundred teams trained robust MGT detectors.", "published": "2024-01-08 15:05:32", "link": "http://arxiv.org/abs/2401.03946v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IDoFew: Intermediate Training Using Dual-Clustering in Language Models\n  for Few Labels Text Classification", "abstract": "Language models such as Bidirectional Encoder Representations from\nTransformers (BERT) have been very effective in various Natural Language\nProcessing (NLP) and text mining tasks including text classification. However,\nsome tasks still pose challenges for these models, including text\nclassification with limited labels. This can result in a cold-start problem.\nAlthough some approaches have attempted to address this problem through\nsingle-stage clustering as an intermediate training step coupled with a\npre-trained language model, which generates pseudo-labels to improve\nclassification, these methods are often error-prone due to the limitations of\nthe clustering algorithms. To overcome this, we have developed a novel\ntwo-stage intermediate clustering with subsequent fine-tuning that models the\npseudo-labels reliably, resulting in reduced prediction errors. The key novelty\nin our model, IDoFew, is that the two-stage clustering coupled with two\ndifferent clustering algorithms helps exploit the advantages of the\ncomplementary algorithms that reduce the errors in generating reliable\npseudo-labels for fine-tuning. Our approach has shown significant improvements\ncompared to strong comparative models.", "published": "2024-01-08 17:07:37", "link": "http://arxiv.org/abs/2401.04025v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FFSplit: Split Feed-Forward Network For Optimizing Accuracy-Efficiency\n  Trade-off in Language Model Inference", "abstract": "The large number of parameters in Pretrained Language Models enhance their\nperformance, but also make them resource-intensive, making it challenging to\ndeploy them on commodity hardware like a single GPU. Due to the memory and\npower limitations of these devices, model compression techniques are often used\nto decrease both the model's size and its inference latency. This usually\nresults in a trade-off between model accuracy and efficiency. Therefore,\noptimizing this balance is essential for effectively deploying LLMs on\ncommodity hardware. A significant portion of the efficiency challenge is the\nFeed-forward network (FFN) component, which accounts for roughly $\\frac{2}{3}$\ntotal parameters and inference latency. In this paper, we first observe that\nonly a few neurons of FFN module have large output norm for any input tokens,\na.k.a. heavy hitters, while the others are sparsely triggered by different\ntokens. Based on this observation, we explicitly split the FFN into two parts\naccording to the heavy hitters. We improve the efficiency-accuracy trade-off of\nexisting compression methods by allocating more resource to FFN parts with\nheavy hitters. In practice, our method can reduce model size by 43.1\\% and\nbring $1.25\\sim1.56\\times$ wall clock time speedup on different hardware with\nnegligible accuracy drop.", "published": "2024-01-08 17:29:16", "link": "http://arxiv.org/abs/2401.04044v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Distortions in Judged Spatial Relations in Large Language Models", "abstract": "We present a benchmark for assessing the capability of Large Language Models\n(LLMs) to discern intercardinal directions between geographic locations and\napply it to three prominent LLMs: GPT-3.5, GPT-4, and Llama-2. This benchmark\nspecifically evaluates whether LLMs exhibit a hierarchical spatial bias similar\nto humans, where judgments about individual locations' spatial relationships\nare influenced by the perceived relationships of the larger groups that contain\nthem. To investigate this, we formulated 14 questions focusing on well-known\nAmerican cities. Seven questions were designed to challenge the LLMs with\nscenarios potentially influenced by the orientation of larger geographical\nunits, such as states or countries, while the remaining seven targeted\nlocations were less susceptible to such hierarchical categorization. Among the\ntested models, GPT-4 exhibited superior performance with 55 percent accuracy,\nfollowed by GPT-3.5 at 47 percent, and Llama-2 at 45 percent. The models showed\nsignificantly reduced accuracy on tasks with suspected hierarchical bias. For\nexample, GPT-4's accuracy dropped to 33 percent on these tasks, compared to 86\npercent on others. However, the models identified the nearest cardinal\ndirection in most cases, reflecting their associative learning mechanism,\nthereby embodying human-like misconceptions. We discuss avenues for improving\nthe spatial reasoning capabilities of LLMs.", "published": "2024-01-08 20:08:04", "link": "http://arxiv.org/abs/2401.04218v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MARG: Multi-Agent Review Generation for Scientific Papers", "abstract": "We study the ability of LLMs to generate feedback for scientific papers and\ndevelop MARG, a feedback generation approach using multiple LLM instances that\nengage in internal discussion. By distributing paper text across agents, MARG\ncan consume the full text of papers beyond the input length limitations of the\nbase LLM, and by specializing agents and incorporating sub-tasks tailored to\ndifferent comment types (experiments, clarity, impact) it improves the\nhelpfulness and specificity of feedback. In a user study, baseline methods\nusing GPT-4 were rated as producing generic or very generic comments more than\nhalf the time, and only 1.7 comments per paper were rated as good overall in\nthe best baseline. Our system substantially improves the ability of GPT-4 to\ngenerate specific and helpful feedback, reducing the rate of generic comments\nfrom 60% to 29% and generating 3.7 good comments per paper (a 2.2x\nimprovement).", "published": "2024-01-08 22:24:17", "link": "http://arxiv.org/abs/2401.04259v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Content-Based Novelty Measure for Scholarly Publications: A Proof of\n  Concept", "abstract": "Novelty, akin to gene mutation in evolution, opens possibilities for\nscholarly advancement. Although peer review remains the gold standard for\nevaluating novelty in scholarly communication and resource allocation, the vast\nvolume of submissions necessitates an automated measure of scholarly novelty.\nAdopting a perspective that views novelty as the atypical combination of\nexisting knowledge, we introduce an information-theoretic measure of novelty in\nscholarly publications. This measure quantifies the degree of 'surprise'\nperceived by a language model that represents the word distribution of\nscholarly discourse. The proposed measure is accompanied by face and construct\nvalidity evidence; the former demonstrates correspondence to scientific common\nsense, and the latter is endorsed through alignment with novelty evaluations\nfrom a select panel of domain experts. Additionally, characterized by its\ninterpretability, fine granularity, and accessibility, this measure addresses\ngaps prevalent in existing methods. We believe this measure holds great\npotential to benefit editors, stakeholders, and policymakers, and it provides a\nreliable lens for examining the relationship between novelty and academic\ndynamics such as creativity, interdisciplinarity, and scientific advances.", "published": "2024-01-08 03:14:24", "link": "http://arxiv.org/abs/2401.03642v2", "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.CL"}
{"title": "Overview of the 2023 ICON Shared Task on Gendered Abuse Detection in\n  Indic Languages", "abstract": "This paper reports the findings of the ICON 2023 on Gendered Abuse Detection\nin Indic Languages. The shared task deals with the detection of gendered abuse\nin online text. The shared task was conducted as a part of ICON 2023, based on\na novel dataset in Hindi, Tamil and the Indian dialect of English. The\nparticipants were given three subtasks with the train dataset consisting of\napproximately 6500 posts sourced from Twitter. For the test set, approximately\n1200 posts were provided. The shared task received a total of 9 registrations.\nThe best F-1 scores are 0.616 for subtask 1, 0.572 for subtask 2 and, 0.616 and\n0.582 for subtask 3. The paper contains examples of hateful content owing to\nits topic.", "published": "2024-01-08 05:54:26", "link": "http://arxiv.org/abs/2401.03677v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Butterfly Effect of Altering Prompts: How Small Changes and\n  Jailbreaks Affect Large Language Model Performance", "abstract": "Large Language Models (LLMs) are regularly being used to label data across\nmany domains and for myriad tasks. By simply asking the LLM for an answer, or\n``prompting,'' practitioners are able to use LLMs to quickly get a response for\nan arbitrary task. This prompting is done through a series of decisions by the\npractitioner, from simple wording of the prompt, to requesting the output in a\ncertain data format, to jailbreaking in the case of prompts that address more\nsensitive topics. In this work, we ask: do variations in the way a prompt is\nconstructed change the ultimate decision of the LLM? We answer this using a\nseries of prompt variations across a variety of text classification tasks. We\nfind that even the smallest of perturbations, such as adding a space at the end\nof a prompt, can cause the LLM to change its answer. Further, we find that\nrequesting responses in XML and commonly used jailbreaks can have cataclysmic\neffects on the data labeled by LLMs.", "published": "2024-01-08 08:28:08", "link": "http://arxiv.org/abs/2401.03729v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhanced Automated Code Vulnerability Repair using Large Language Models", "abstract": "This research addresses the complex challenge of automated repair of code\nvulnerabilities, vital for enhancing digital security in an increasingly\ntechnology-driven world. The study introduces a novel and efficient format for\nthe representation of code modification, using advanced Large Language Models\n(LLMs) such as Code Llama and Mistral. These models, fine-tuned on datasets\nfeaturing C code vulnerabilities, significantly improve the accuracy and\nadaptability of automated code repair techniques. A key finding is the enhanced\nrepair accuracy of these models when compared to previous methods such as\nVulRepair, which underscores their practical utility and efficiency. The\nresearch also offers a critical assessment of current evaluation metrics, such\nas perfect predictions, and their limitations in reflecting the true\ncapabilities of automated repair models in real-world scenarios. Following\nthis, it underscores the importance of using test datasets devoid of train\nsamples, emphasizing the need for dataset integrity to enhance the\neffectiveness of LLMs in code repair tasks. The significance of this work is\nits contribution to digital security, setting new standards for automated code\nvulnerability repair and paving the way for future advancements in the fields\nof cybersecurity and artificial intelligence. The study does not only highlight\nthe potential of LLMs in enhancing code security but also fosters further\nexploration and research in these crucial areas.", "published": "2024-01-08 09:01:29", "link": "http://arxiv.org/abs/2401.03741v2", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Anatomy of Neural Language Models", "abstract": "The fields of generative AI and transfer learning have experienced remarkable\nadvancements in recent years especially in the domain of Natural Language\nProcessing (NLP). Transformers have been at the heart of these advancements\nwhere the cutting-edge transformer-based Language Models (LMs) have led to new\nstate-of-the-art results in a wide spectrum of applications. While the number\nof research works involving neural LMs is exponentially increasing, their vast\nmajority are high-level and far from self-contained. Consequently, a deep\nunderstanding of the literature in this area is a tough task especially in the\nabsence of a unified mathematical framework explaining the main types of neural\nLMs. We address the aforementioned problem in this tutorial where the objective\nis to explain neural LMs in a detailed, simplified and unambiguous mathematical\nframework accompanied by clear graphical illustrations. Concrete examples on\nwidely used models like BERT and GPT2 are explored. Finally, since transformers\npretrained on language-modeling-like tasks have been widely adopted in computer\nvision and time series applications, we briefly explore some examples of such\nsolutions in order to enable readers to understand how transformers work in the\naforementioned domains and compare this use with the original one in NLP.", "published": "2024-01-08 10:27:25", "link": "http://arxiv.org/abs/2401.03797v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TeleChat Technical Report", "abstract": "In this technical report, we present TeleChat, a collection of large language\nmodels (LLMs) with parameters of 3 billion, 7 billion and 12 billion. It\nincludes pretrained language models as well as fine-tuned chat models that is\naligned with human preferences. TeleChat is initially pretrained on an\nextensive corpus containing a diverse collection of texts from both English and\nChinese languages, including trillions of tokens. Subsequently, the model\nundergoes fine-tuning to align with human preferences, following a detailed\nmethodology that we describe. We evaluate the performance of TeleChat on\nvarious tasks, including language understanding, mathematics, reasoning, code\ngeneration, and knowledge-based question answering. Our findings indicate that\nTeleChat achieves comparable performance to other open-source models of similar\nsize across a wide range of public benchmarks. To support future research and\napplications utilizing LLMs, we release the fine-tuned model checkpoints of\nTeleChat's 7B and 12B variant, along with code and a portion of our pretraining\ndata, to the public community.", "published": "2024-01-08 10:43:19", "link": "http://arxiv.org/abs/2401.03804v2", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "We Need to Talk About Classification Evaluation Metrics in NLP", "abstract": "In Natural Language Processing (NLP) classification tasks such as topic\ncategorisation and sentiment analysis, model generalizability is generally\nmeasured with standard metrics such as Accuracy, F-Measure, or AUC-ROC. The\ndiversity of metrics, and the arbitrariness of their application suggest that\nthere is no agreement within NLP on a single best metric to use. This lack\nsuggests there has not been sufficient examination of the underlying heuristics\nwhich each metric encodes. To address this we compare several standard\nclassification metrics with more 'exotic' metrics and demonstrate that a\nrandom-guess normalised Informedness metric is a parsimonious baseline for task\nperformance. To show how important the choice of metric is, we perform\nextensive experiments on a wide range of NLP tasks including a synthetic\nscenario, natural language understanding, question answering and machine\ntranslation. Across these tasks we use a superset of metrics to rank models and\nfind that Informedness best captures the ideal model characteristics. Finally,\nwe release a Python implementation of Informedness following the SciKitLearn\nclassifier format.", "published": "2024-01-08 11:40:48", "link": "http://arxiv.org/abs/2401.03831v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PythonSaga: Redefining the Benchmark to Evaluate Code Generating LLMs", "abstract": "Driven by the surge in code generation using large language models (LLMs),\nnumerous benchmarks have emerged to evaluate these LLMs capabilities. We\nconducted a large-scale human evaluation of HumanEval and MBPP, two popular\nbenchmarks for Python code generation, analyzing their diversity and\ndifficulty. Our findings unveil a critical bias towards a limited set of\nprogramming concepts, neglecting most of the other concepts entirely.\nFurthermore, we uncover a worrying prevalence of easy tasks, potentially\ninflating model performance estimations. To address these limitations, we\npropose a novel benchmark, PythonSaga, featuring 185 hand-crafted prompts on a\nbalanced representation of 38 programming concepts across diverse difficulty\nlevels. The robustness of our benchmark is demonstrated by the poor performance\nof existing Code-LLMs.", "published": "2024-01-08 12:36:43", "link": "http://arxiv.org/abs/2401.03855v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "STAIR: Spatial-Temporal Reasoning with Auditable Intermediate Results\n  for Video Question Answering", "abstract": "Recently we have witnessed the rapid development of video question answering\nmodels. However, most models can only handle simple videos in terms of temporal\nreasoning, and their performance tends to drop when answering\ntemporal-reasoning questions on long and informative videos. To tackle this\nproblem we propose STAIR, a Spatial-Temporal Reasoning model with Auditable\nIntermediate Results for video question answering. STAIR is a neural module\nnetwork, which contains a program generator to decompose a given question into\na hierarchical combination of several sub-tasks, and a set of lightweight\nneural modules to complete each of these sub-tasks. Though neural module\nnetworks are already widely studied on image-text tasks, applying them to\nvideos is a non-trivial task, as reasoning on videos requires different\nabilities. In this paper, we define a set of basic video-text sub-tasks for\nvideo question answering and design a set of lightweight modules to complete\nthem. Different from most prior works, modules of STAIR return intermediate\noutputs specific to their intentions instead of always returning attention\nmaps, which makes it easier to interpret and collaborate with pre-trained\nmodels. We also introduce intermediate supervision to make these intermediate\noutputs more accurate. We conduct extensive experiments on several video\nquestion answering datasets under various settings to show STAIR's performance,\nexplainability, compatibility with pre-trained models, and applicability when\nprogram annotations are not available. Code:\nhttps://github.com/yellow-binary-tree/STAIR", "published": "2024-01-08 14:01:59", "link": "http://arxiv.org/abs/2401.03901v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Empirical Analysis of Efficient Fine-Tuning Methods for Large\n  Pre-Trained Language Models", "abstract": "Fine-tuning large pre-trained language models for downstream tasks remains a\ncritical challenge in natural language processing. This paper presents an\nempirical analysis comparing two efficient fine-tuning methods - BitFit and\nadapter modules - to standard full model fine-tuning. Experiments conducted on\nGLUE benchmark datasets (MRPC, COLA, STS-B) reveal several key insights. The\nBitFit approach, which trains only bias terms and task heads, matches full\nfine-tuning performance across varying amounts of training data and time\nconstraints. It demonstrates remarkable stability even with only 30\\% of data,\noutperforming full fine-tuning at intermediate data levels. Adapter modules\nexhibit high variability, with inconsistent gains over default models. The\nfindings indicate BitFit offers an attractive balance between performance and\nparameter efficiency. Our work provides valuable perspectives on model tuning,\nemphasizing robustness and highlighting BitFit as a promising alternative for\nresource-constrained or streaming task settings. The analysis offers actionable\nguidelines for efficient adaptation of large pre-trained models, while\nillustrating open challenges in stabilizing techniques like adapter modules.", "published": "2024-01-08 17:44:43", "link": "http://arxiv.org/abs/2401.04051v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Mixtral of Experts", "abstract": "We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model.\nMixtral has the same architecture as Mistral 7B, with the difference that each\nlayer is composed of 8 feedforward blocks (i.e. experts). For every token, at\neach layer, a router network selects two experts to process the current state\nand combine their outputs. Even though each token only sees two experts, the\nselected experts can be different at each timestep. As a result, each token has\naccess to 47B parameters, but only uses 13B active parameters during inference.\nMixtral was trained with a context size of 32k tokens and it outperforms or\nmatches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular,\nMixtral vastly outperforms Llama 2 70B on mathematics, code generation, and\nmultilingual benchmarks. We also provide a model fine-tuned to follow\ninstructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo,\nClaude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both\nthe base and instruct models are released under the Apache 2.0 license.", "published": "2024-01-08 18:47:34", "link": "http://arxiv.org/abs/2401.04088v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Chain of LoRA: Efficient Fine-tuning of Language Models via Residual\n  Learning", "abstract": "Fine-tuning is the primary methodology for tailoring pre-trained large\nlanguage models to specific tasks. As the model's scale and the diversity of\ntasks expand, parameter-efficient fine-tuning methods are of paramount\nimportance. One of the most widely used family of methods is low-rank\nadaptation (LoRA) and its variants. LoRA encodes weight update as the product\nof two low-rank matrices. Despite its advantages, LoRA falls short of\nfull-parameter fine-tuning in terms of generalization error for certain tasks.\n  We introduce Chain of LoRA (COLA), an iterative optimization framework\ninspired by the Frank-Wolfe algorithm, to bridge the gap between LoRA and full\nparameter fine-tuning, without incurring additional computational costs or\nmemory overheads. COLA employs a residual learning procedure where it merges\nlearned LoRA modules into the pre-trained language model parameters and\nre-initilize optimization for new born LoRA modules. We provide theoretical\nconvergence guarantees as well as empirical results to validate the\neffectiveness of our algorithm. Across various models (OPT and llama-2) and\nseven benchmarking tasks, we demonstrate that COLA can consistently outperform\nLoRA without additional computational or memory costs.", "published": "2024-01-08 14:26:49", "link": "http://arxiv.org/abs/2401.04151v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Advancing bioinformatics with large language models: components,\n  applications and perspectives", "abstract": "Large language models (LLMs) are a class of artificial intelligence models\nbased on deep learning, which have great performance in various tasks,\nespecially in natural language processing (NLP). Large language models\ntypically consist of artificial neural networks with numerous parameters,\ntrained on large amounts of unlabeled input using self-supervised or\nsemi-supervised learning. However, their potential for solving bioinformatics\nproblems may even exceed their proficiency in modeling human language. In this\nreview, we will provide a comprehensive overview of the essential components of\nlarge language models (LLMs) in bioinformatics, spanning genomics,\ntranscriptomics, proteomics, drug discovery, and single-cell analysis. Key\naspects covered include tokenization methods for diverse data types, the\narchitecture of transformer models, the core attention mechanism, and the\npre-training processes underlying these models. Additionally, we will introduce\ncurrently available foundation models and highlight their downstream\napplications across various bioinformatics domains. Finally, drawing from our\nexperience, we will offer practical guidance for both LLM users and developers,\nemphasizing strategies to optimize their use and foster further innovation in\nthe field.", "published": "2024-01-08 17:26:59", "link": "http://arxiv.org/abs/2401.04155v2", "categories": ["q-bio.QM", "cs.CL"], "primary_category": "q-bio.QM"}
{"title": "Using Zero-shot Prompting in the Automatic Creation and Expansion of\n  Topic Taxonomies for Tagging Retail Banking Transactions", "abstract": "This work presents an unsupervised method for automatically constructing and\nexpanding topic taxonomies using instruction-based fine-tuned LLMs (Large\nLanguage Models). We apply topic modeling and keyword extraction techniques to\ncreate initial topic taxonomies and LLMs to post-process the resulting terms\nand create a hierarchy. To expand an existing taxonomy with new terms, we use\nzero-shot prompting to find out where to add new nodes, which, to our\nknowledge, is the first work to present such an approach to taxonomy tasks. We\nuse the resulting taxonomies to assign tags that characterize merchants from a\nretail bank dataset. To evaluate our work, we asked 12 volunteers to answer a\ntwo-part form in which we first assessed the quality of the taxonomies created\nand then the tags assigned to merchants based on that taxonomy. The evaluation\nrevealed a coherence rate exceeding 90% for the chosen taxonomies. The\ntaxonomies' expansion with LLMs also showed exciting results for parent node\nprediction, with an f1-score above 70% in our taxonomies.", "published": "2024-01-08 00:27:16", "link": "http://arxiv.org/abs/2401.06790v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LightHouse: A Survey of AGI Hallucination", "abstract": "With the development of artificial intelligence, large-scale models have\nbecome increasingly intelligent. However, numerous studies indicate that\nhallucinations within these large models are a bottleneck hindering the\ndevelopment of AI research. In the pursuit of achieving strong artificial\nintelligence, a significant volume of research effort is being invested in the\nAGI (Artificial General Intelligence) hallucination research. Previous\nexplorations have been conducted in researching hallucinations within LLMs\n(Large Language Models). As for multimodal AGI, research on hallucinations is\nstill in an early stage. To further the progress of research in the domain of\nhallucinatory phenomena, we present a bird's eye view of hallucinations in AGI,\nsummarizing the current work on AGI hallucinations and proposing some\ndirections for future research.", "published": "2024-01-08 03:52:40", "link": "http://arxiv.org/abs/2401.06792v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Why Solving Multi-agent Path Finding with Large Language Model has not\n  Succeeded Yet", "abstract": "With the explosive influence caused by the success of large language models\n(LLM) like ChatGPT and GPT-4, there has been an extensive amount of recent work\nshowing that foundation models can be used to solve a large variety of tasks.\nHowever, there is very limited work that shares insights on multi-agent\nplanning. Multi-agent planning is different from other domains by combining the\ndifficulty of multi-agent coordination and planning, and making it hard to\nleverage external tools to facilitate the reasoning needed. In this paper, we\nfocus on the problem of multi-agent path finding (MAPF), which is also known as\nmulti-robot route planning, and study the performance of solving MAPF with\nLLMs. We first show the motivating success on an empty room map without\nobstacles, then the failure to plan on the harder room map and maze map of the\nstandard MAPF benchmark. We present our position on why directly solving MAPF\nwith LLMs has not been successful yet, and we use various experiments to\nsupport our hypothesis. Based on our results, we discussed how researchers with\ndifferent backgrounds could help with this problem from different perspectives.", "published": "2024-01-08 02:22:04", "link": "http://arxiv.org/abs/2401.03630v2", "categories": ["cs.MA", "cs.AI", "cs.CL"], "primary_category": "cs.MA"}
{"title": "A Philosophical Introduction to Language Models -- Part I: Continuity\n  With Classic Debates", "abstract": "Large language models like GPT-4 have achieved remarkable proficiency in a\nbroad spectrum of language-based tasks, some of which are traditionally\nassociated with hallmarks of human intelligence. This has prompted ongoing\ndisagreements about the extent to which we can meaningfully ascribe any kind of\nlinguistic or cognitive competence to language models. Such questions have deep\nphilosophical roots, echoing longstanding debates about the status of\nartificial neural networks as cognitive models. This article -- the first part\nof two companion papers -- serves both as a primer on language models for\nphilosophers, and as an opinionated survey of their significance in relation to\nclassic debates in the philosophy cognitive science, artificial intelligence,\nand linguistics. We cover topics such as compositionality, language\nacquisition, semantic competence, grounding, world models, and the transmission\nof cultural knowledge. We argue that the success of language models challenges\nseveral long-held assumptions about artificial neural networks. However, we\nalso highlight the need for further empirical investigation to better\nunderstand their internal mechanisms. This sets the stage for the companion\npaper (Part II), which turns to novel empirical methods for probing the inner\nworkings of language models, and new philosophical questions prompted by their\nlatest developments.", "published": "2024-01-08 14:12:31", "link": "http://arxiv.org/abs/2401.03910v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Advancing Spatial Reasoning in Large Language Models: An In-Depth\n  Evaluation and Enhancement Using the StepGame Benchmark", "abstract": "Artificial intelligence (AI) has made remarkable progress across various\ndomains, with large language models like ChatGPT gaining substantial attention\nfor their human-like text-generation capabilities. Despite these achievements,\nspatial reasoning remains a significant challenge for these models. Benchmarks\nlike StepGame evaluate AI spatial reasoning, where ChatGPT has shown\nunsatisfactory performance. However, the presence of template errors in the\nbenchmark has an impact on the evaluation results. Thus there is potential for\nChatGPT to perform better if these template errors are addressed, leading to\nmore accurate assessments of its spatial reasoning capabilities. In this study,\nwe refine the StepGame benchmark, providing a more accurate dataset for model\nevaluation. We analyze GPT's spatial reasoning performance on the rectified\nbenchmark, identifying proficiency in mapping natural language text to spatial\nrelations but limitations in multi-hop reasoning. We provide a flawless\nsolution to the benchmark by combining template-to-relation mapping with\nlogic-based reasoning. This combination demonstrates proficiency in performing\nqualitative reasoning on StepGame without encountering any errors. We then\naddress the limitations of GPT models in spatial reasoning. We deploy\nChain-of-thought and Tree-of-thoughts prompting strategies, offering insights\ninto GPT's ``cognitive process\", and achieving remarkable improvements in\naccuracy. Our investigation not only sheds light on model deficiencies but also\nproposes enhancements, contributing to the advancement of AI with more robust\nspatial reasoning capabilities.", "published": "2024-01-08 16:13:08", "link": "http://arxiv.org/abs/2401.03991v1", "categories": ["cs.AI", "cs.CL", "cs.DB", "cs.LO"], "primary_category": "cs.AI"}
{"title": "MoE-Mamba: Efficient Selective State Space Models with Mixture of\n  Experts", "abstract": "State Space Models (SSMs) have become serious contenders in the field of\nsequential modeling, challenging the dominance of Transformers. At the same\ntime, Mixture of Experts (MoE) has significantly improved Transformer-based\nLarge Language Models, including recent state-of-the-art open models. We\npropose that to unlock the potential of SSMs for scaling, they should be\ncombined with MoE. We showcase this on Mamba, a recent SSM-based model that\nachieves remarkable performance. Our model, MoE-Mamba, outperforms both Mamba\nand baseline Transformer-MoE. In particular, MoE-Mamba reaches the same\nperformance as Mamba in $2.35\\times$ fewer training steps while preserving the\ninference performance gains of Mamba against Transformer.", "published": "2024-01-08 18:35:07", "link": "http://arxiv.org/abs/2401.04081v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Cross-Speaker Encoding Network for Multi-Talker Speech Recognition", "abstract": "End-to-end multi-talker speech recognition has garnered great interest as an\neffective approach to directly transcribe overlapped speech from multiple\nspeakers. Current methods typically adopt either 1) single-input\nmultiple-output (SIMO) models with a branched encoder, or 2) single-input\nsingle-output (SISO) models based on attention-based encoder-decoder\narchitecture with serialized output training (SOT). In this work, we propose a\nCross-Speaker Encoding (CSE) network to address the limitations of SIMO models\nby aggregating cross-speaker representations. Furthermore, the CSE model is\nintegrated with SOT to leverage both the advantages of SIMO and SISO while\nmitigating their drawbacks. To the best of our knowledge, this work represents\nan early effort to integrate SIMO and SISO for multi-talker speech recognition.\nExperiments on the two-speaker LibrispeechMix dataset show that the CES model\nreduces word error rate (WER) by 8% over the SIMO baseline. The CSE-SOT model\nreduces WER by 10% overall and by 16% on high-overlap speech compared to the\nSOT model. Code is available at https://github.com/kjw11/CSEnet-ASR.", "published": "2024-01-08 16:37:45", "link": "http://arxiv.org/abs/2401.04152v2", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "High-precision Voice Search Query Correction via Retrievable Speech-text\n  Embedings", "abstract": "Automatic speech recognition (ASR) systems can suffer from poor recall for\nvarious reasons, such as noisy audio, lack of sufficient training data, etc.\n  Previous work has shown that recall can be improved by retrieving rewrite\ncandidates from a large database of likely, contextually-relevant alternatives\nto the hypothesis text using nearest-neighbors search over embeddings of the\nASR hypothesis text to correct and candidate corrections.\n  However, ASR-hypothesis-based retrieval can yield poor precision if the\ntextual hypotheses are too phonetically dissimilar to the transcript truth. In\nthis paper, we eliminate the hypothesis-audio mismatch problem by querying the\ncorrection database directly using embeddings derived from the utterance audio;\nthe embeddings of the utterance audio and candidate corrections are produced by\nmultimodal speech-text embedding networks trained to place the embedding of the\naudio of an utterance and the embedding of its corresponding textual transcript\nclose together.\n  After locating an appropriate correction candidate using nearest-neighbor\nsearch, we score the candidate with its speech-text embedding distance before\nadding the candidate to the original n-best list.\n  We show a relative word error rate (WER) reduction of 6% on utterances whose\ntranscripts appear in the candidate set, without increasing WER on general\nutterances.", "published": "2024-01-08 20:59:56", "link": "http://arxiv.org/abs/2401.04235v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "LLM4PLC: Harnessing Large Language Models for Verifiable Programming of\n  PLCs in Industrial Control Systems", "abstract": "Although Large Language Models (LLMs) have established pre-dominance in\nautomated code generation, they are not devoid of shortcomings. The pertinent\nissues primarily relate to the absence of execution guarantees for generated\ncode, a lack of explainability, and suboptimal support for essential but niche\nprogramming languages. State-of-the-art LLMs such as GPT-4 and LLaMa2 fail to\nproduce valid programs for Industrial Control Systems (ICS) operated by\nProgrammable Logic Controllers (PLCs). We propose LLM4PLC, a user-guided\niterative pipeline leveraging user feedback and external verification tools\nincluding grammar checkers, compilers and SMV verifiers to guide the LLM's\ngeneration. We further enhance the generation potential of LLM by employing\nPrompt Engineering and model fine-tuning through the creation and usage of\nLoRAs. We validate this system using a FischerTechnik Manufacturing TestBed\n(MFTB), illustrating how LLMs can evolve from generating structurally flawed\ncode to producing verifiably correct programs for industrial applications. We\nrun a complete test suite on GPT-3.5, GPT-4, Code Llama-7B, a fine-tuned Code\nLlama-7B model, Code Llama-34B, and a fine-tuned Code Llama-34B model. The\nproposed pipeline improved the generation success rate from 47% to 72%, and the\nSurvey-of-Experts code quality from 2.25/10 to 7.75/10. To promote open\nresearch, we share the complete experimental setup, the LLM Fine-Tuning\nWeights, and the video demonstrations of the different programs on our\ndedicated webpage.", "published": "2024-01-08 23:52:42", "link": "http://arxiv.org/abs/2401.05443v1", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.PL", "D.2.4; I.2.7; I.2.2"], "primary_category": "cs.SE"}
{"title": "A Span-based Model for Extracting Overlapping PICO Entities from RCT\n  Publications", "abstract": "Objectives Extraction of PICO (Populations, Interventions, Comparison, and\nOutcomes) entities is fundamental to evidence retrieval. We present a novel\nmethod PICOX to extract overlapping PICO entities.\n  Materials and Methods PICOX first identifies entities by assessing whether a\nword marks the beginning or conclusion of an entity. Then it uses a multi-label\nclassifier to assign one or more PICO labels to a span candidate. PICOX was\nevaluated using one of the best-performing baselines, EBM-NLP, and three more\ndatasets, i.e., PICO-Corpus, and RCT publications on Alzheimer's Disease or\nCOVID-19, using entity-level precision, recall, and F1 scores.\n  Results PICOX achieved superior precision, recall, and F1 scores across the\nboard, with the micro F1 score improving from 45.05 to 50.87 (p << 0.01). On\nthe PICO-Corpus, PICOX obtained higher recall and F1 scores than the baseline\nand improved the micro recall score from 56.66 to 67.33. On the COVID-19\ndataset, PICOX also outperformed the baseline and improved the micro F1 score\nfrom 77.10 to 80.32. On the AD dataset, PICOX demonstrated comparable F1 scores\nwith higher precision when compared to the baseline.\n  Conclusion PICOX excels in identifying overlapping entities and consistently\nsurpasses a leading baseline across multiple datasets. Ablation studies reveal\nthat its data augmentation strategy effectively minimizes false positives and\nimproves precision.", "published": "2024-01-08 03:35:02", "link": "http://arxiv.org/abs/2401.06791v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "AI and Generative AI for Research Discovery and Summarization", "abstract": "AI and generative AI tools, including chatbots like ChatGPT that rely on\nlarge language models (LLMs), have burst onto the scene this year, creating\nincredible opportunities to increase work productivity and improve our lives.\nStatisticians and data scientists have begun experiencing the benefits from the\navailability of these tools in numerous ways, such as the generation of\nprogramming code from text prompts to analyze data or fit statistical models.\nOne area that these tools can make a substantial impact is in research\ndiscovery and summarization. Standalone tools and plugins to chatbots are being\ndeveloped that allow researchers to more quickly find relevant literature than\npre-2023 search tools. Furthermore, generative AI tools have improved to the\npoint where they can summarize and extract the key points from research\narticles in succinct language. Finally, chatbots based on highly parameterized\nLLMs can be used to simulate abductive reasoning, which provides researchers\nthe ability to make connections among related technical topics, which can also\nbe used for research discovery. We review the developments in AI and generative\nAI for research discovery and summarization, and propose directions where these\ntypes of tools are likely to head in the future that may be of interest to\nstatistician and data scientists.", "published": "2024-01-08 18:42:55", "link": "http://arxiv.org/abs/2401.06795v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Bridging the Skills Gap: Evaluating an AI-Assisted Provider Platform to\n  Support Care Providers with Empathetic Delivery of Protocolized Therapy", "abstract": "Despite the high prevalence and burden of mental health conditions, there is\na global shortage of mental health providers. Artificial Intelligence (AI)\nmethods have been proposed as a way to address this shortage, by supporting\nproviders with less extensive training as they deliver care. To this end, we\ndeveloped the AI-Assisted Provider Platform (A2P2), a text-based virtual\ntherapy interface that includes a response suggestion feature, which supports\nproviders in delivering protocolized therapies empathetically. We studied\nproviders with and without expertise in mental health treatment delivering a\ntherapy session using the platform with (intervention) and without (control)\nAI-assistance features. Upon evaluation, the AI-assisted system significantly\ndecreased response times by 29.34% (p=0.002), tripled empathic response\naccuracy (p=0.0001), and increased goal recommendation accuracy by 66.67%\n(p=0.001) across both user groups compared to the control. Both groups rated\nthe system as having excellent usability.", "published": "2024-01-08 02:23:17", "link": "http://arxiv.org/abs/2401.03631v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.HC"}
{"title": "Can Large Language Models Beat Wall Street? Unveiling the Potential of\n  AI in Stock Selection", "abstract": "This paper introduces MarketSenseAI, an innovative framework leveraging\nGPT-4's advanced reasoning for selecting stocks in financial markets. By\nintegrating Chain of Thought and In-Context Learning, MarketSenseAI analyzes\ndiverse data sources, including market trends, news, fundamentals, and\nmacroeconomic factors, to emulate expert investment decision-making. The\ndevelopment, implementation, and validation of the framework are elaborately\ndiscussed, underscoring its capability to generate actionable and interpretable\ninvestment signals. A notable feature of this work is employing GPT-4 both as a\npredictive mechanism and signal evaluator, revealing the significant impact of\nthe AI-generated explanations on signal accuracy, reliability and acceptance.\nThrough empirical testing on the competitive S&P 100 stocks over a 15-month\nperiod, MarketSenseAI demonstrated exceptional performance, delivering excess\nalpha of 10% to 30% and achieving a cumulative return of up to 72% over the\nperiod, while maintaining a risk profile comparable to the broader market. Our\nfindings highlight the transformative potential of Large Language Models in\nfinancial decision-making, marking a significant leap in integrating generative\nAI into financial analytics and investment strategies.", "published": "2024-01-08 08:58:46", "link": "http://arxiv.org/abs/2401.03737v2", "categories": ["q-fin.CP", "cs.AI", "cs.CE", "cs.CL", "cs.LG", "68T07, 68T50, 91G10, 91G15", "I.2.1; I.2.7; J.4"], "primary_category": "q-fin.CP"}
{"title": "FunnyNet-W: Multimodal Learning of Funny Moments in Videos in the Wild", "abstract": "Automatically understanding funny moments (i.e., the moments that make people\nlaugh) when watching comedy is challenging, as they relate to various features,\nsuch as body language, dialogues and culture. In this paper, we propose\nFunnyNet-W, a model that relies on cross- and self-attention for visual, audio\nand text data to predict funny moments in videos. Unlike most methods that rely\non ground truth data in the form of subtitles, in this work we exploit\nmodalities that come naturally with videos: (a) video frames as they contain\nvisual information indispensable for scene understanding, (b) audio as it\ncontains higher-level cues associated with funny moments, such as intonation,\npitch and pauses and (c) text automatically extracted with a speech-to-text\nmodel as it can provide rich information when processed by a Large Language\nModel. To acquire labels for training, we propose an unsupervised approach that\nspots and labels funny audio moments. We provide experiments on five datasets:\nthe sitcoms TBBT, MHD, MUStARD, Friends, and the TED talk UR-Funny. Extensive\nexperiments and analysis show that FunnyNet-W successfully exploits visual,\nauditory and textual cues to identify funny moments, while our findings reveal\nFunnyNet-W's ability to predict funny moments in the wild. FunnyNet-W sets the\nnew state of the art for funny moment detection with multimodal cues on all\ndatasets with and without using ground truth information.", "published": "2024-01-08 19:39:36", "link": "http://arxiv.org/abs/2401.04210v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Geodesic interpolation of frame-wise speaker embeddings for the\n  diarization of meeting scenarios", "abstract": "We propose a modified teacher-student training for the extraction of\nframe-wise speaker embeddings that allows for an effective diarization of\nmeeting scenarios containing partially overlapping speech. To this end, a\ngeodesic distance loss is used that enforces the embeddings computed from\nregions with two active speakers to lie on the shortest path on a sphere\nbetween the points given by the d-vectors of each of the active speakers. Using\nthose frame-wise speaker embeddings in clustering-based diarization outperforms\nsegment-level clustering-based diarization systems such as VBx and Spectral\nClustering. By extending our approach to a mixture-model-based diarization, the\nperformance can be further improved, approaching the diarization error rates of\ndiarization systems that use a dedicated overlap detection, and outperforming\nthese systems when also employing an additional overlap detection.", "published": "2024-01-08 15:34:57", "link": "http://arxiv.org/abs/2401.03963v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "BS-PLCNet: Band-split Packet Loss Concealment Network with Multi-task\n  Learning Framework and Multi-discriminators", "abstract": "Packet loss is a common and unavoidable problem in voice over internet phone\n(VoIP) systems. To deal with the problem, we propose a band-split packet loss\nconcealment network (BS-PLCNet). Specifically, we split the full-band signal\ninto wide-band (0-8kHz) and high-band (8-24kHz). The wide-band signals are\nprocessed by a gated convolutional recurrent network (GCRN), while the\nhigh-band counterpart is processed by a simple GRU network. To ensure high\nspeech quality and automatic speech recognition (ASR) compatibility, multi-task\nlearning (MTL) framework including fundamental frequency (f0) prediction,\nlinguistic awareness, and multi-discriminators are used. The proposed approach\ntied for 1st place in the ICASSP 2024 PLC Challenge.", "published": "2024-01-08 06:34:19", "link": "http://arxiv.org/abs/2401.03687v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "LUPET: Incorporating Hierarchical Information Path into Multilingual ASR", "abstract": "Toward high-performance multilingual automatic speech recognition (ASR),\nvarious types of linguistic information and model design have demonstrated\ntheir effectiveness independently. They include language identity (LID),\nphoneme information, language-specific processing modules, and cross-lingual\nself-supervised speech representation. It is expected that leveraging their\nbenefits synergistically in a unified solution would further improve the\noverall system performance. This paper presents a novel design of a\nhierarchical information path, named LUPET, which sequentially encodes, from\nthe shallow layers to deep layers, multiple aspects of linguistic and acoustic\ninformation at diverse granularity scales. The path starts from LID prediction,\nfollowed by acoustic unit discovery, phoneme sharing, and finally token\nrecognition routed by a mixture-of-expert. ASR experiments are carried out on\n10 languages in the Common Voice corpus. The results demonstrate the superior\nperformance of LUPET as compared to the baseline systems. Most importantly,\nLUPET effectively mitigates the issue of performance compromise of\nhigh-resource languages with low-resource ones in the multilingual setting.", "published": "2024-01-08 06:38:41", "link": "http://arxiv.org/abs/2401.03689v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "An audio-quality-based multi-strategy approach for target speaker\n  extraction in the MISP 2023 Challenge", "abstract": "This paper describes our audio-quality-based multi-strategy approach for the\naudio-visual target speaker extraction (AVTSE) task in the Multi-modal\nInformation based Speech Processing (MISP) 2023 Challenge. Specifically, our\napproach adopts different extraction strategies based on the audio quality,\nstriking a balance between interference removal and speech preservation, which\nbenifits the back-end automatic speech recognition (ASR) systems. Experiments\nshow that our approach achieves a character error rate (CER) of 24.2% and 33.2%\non the Dev and Eval set, respectively, obtaining the second place in the\nchallenge.", "published": "2024-01-08 07:04:30", "link": "http://arxiv.org/abs/2401.03697v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Creating Personalized Synthetic Voices from Articulation Impaired Speech\n  Using Augmented Reconstruction Loss", "abstract": "This research is about the creation of personalized synthetic voices for head\nand neck cancer survivors. It is focused particularly on tongue cancer patients\nwhose speech might exhibit severe articulation impairment. Our goal is to\nrestore normal articulation in the synthesized speech, while maximally\npreserving the target speaker's individuality in terms of both the voice timbre\nand speaking style. This is formulated as a task of learning from noisy labels.\nWe propose to augment the commonly used speech reconstruction loss with two\nadditional terms. The first term constitutes a regularization loss that\nmitigates the impact of distorted articulation in the training speech. The\nsecond term is a consistency loss that encourages correct articulation in the\ngenerated speech. These additional loss terms are obtained from frame-level\narticulation scores of original and generated speech, which are derived using a\nseparately trained phone classifier. Experimental results on a real case of\ntongue cancer patient confirm that the synthetic voice achieves comparable\narticulation quality to unimpaired natural speech, while effectively\nmaintaining the target speaker's individuality. Audio samples are available at\nhttps://myspeechproject.github.io/ArticulationRepair/.", "published": "2024-01-08 11:10:50", "link": "http://arxiv.org/abs/2401.03816v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Inverse Nonlinearity Compensation of Hyperelastic Deformation in\n  Dielectric Elastomer for Acoustic Actuation", "abstract": "This paper presents an in-depth examination of the nonlinear deformation\ninduced by dielectric actuation in pre-stressed ideal dielectric elastomers. A\nnonlinear ordinary differential equation that governs this deformation is\nformulated based on the hyperelastic model under dielectric stress. By means of\nnumerical integration and neural network approximations, the relationship\nbetween voltage and stretch is established. Neural networks are utilized to\napproximate solutions for voltage-to-stretch and stretch-to-voltage\ntransformations obtained via an explicit Runge-Kutta method. The efficacy of\nthese approximations is illustrated by their use in compensating for\nnonlinearity through the waveshaping of the input signal. The comparative\nanalysis demonstrates that the approximated solutions are more accurate than\nbaseline methods, resulting in reduced harmonic distortions when dielectric\nelastomers are used as acoustic actuators. This study highlights the\neffectiveness of the proposed approach in mitigating nonlinearities and\nenhancing the performance of dielectric elastomers in acoustic actuation\napplications.", "published": "2024-01-08 12:28:44", "link": "http://arxiv.org/abs/2401.03850v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DJCM: A Deep Joint Cascade Model for Singing Voice Separation and Vocal\n  Pitch Estimation", "abstract": "Singing voice separation and vocal pitch estimation are pivotal tasks in\nmusic information retrieval. Existing methods for simultaneous extraction of\nclean vocals and vocal pitches can be classified into two categories: pipeline\nmethods and naive joint learning methods. However, the efficacy of these\nmethods is limited by the following problems: On the one hand, pipeline methods\ntrain models for each task independently, resulting a mismatch between the data\ndistributions at the training and testing time. On the other hand, naive joint\nlearning methods simply add the losses of both tasks, possibly leading to a\nmisalignment between the distinct objectives of each task. To solve these\nproblems, we propose a Deep Joint Cascade Model (DJCM) for singing voice\nseparation and vocal pitch estimation. DJCM employs a novel joint cascade model\nstructure to concurrently train both tasks. Moreover, task-specific weights are\nused to align different objectives of both tasks. Experimental results show\nthat DJCM achieves state-of-the-art performance on both tasks, with great\nimprovements of 0.45 in terms of Signal-to-Distortion Ratio (SDR) for singing\nvoice separation and 2.86% in terms of Overall Accuracy (OA) for vocal pitch\nestimation. Furthermore, extensive ablation studies validate the effectiveness\nof each design of our proposed model. The code of DJCM is available at\nhttps://github.com/Dream-High/DJCM .", "published": "2024-01-08 12:37:25", "link": "http://arxiv.org/abs/2401.03856v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "FADI-AEC: Fast Score Based Diffusion Model Guided by Far-end Signal for\n  Acoustic Echo Cancellation", "abstract": "Despite the potential of diffusion models in speech enhancement, their\ndeployment in Acoustic Echo Cancellation (AEC) has been restricted. In this\npaper, we propose DI-AEC, pioneering a diffusion-based stochastic regeneration\napproach dedicated to AEC. Further, we propose FADI-AEC, fast score-based\ndiffusion AEC framework to save computational demands, making it favorable for\nedge devices. It stands out by running the score model once per frame,\nachieving a significant surge in processing efficiency. Apart from that, we\nintroduce a novel noise generation technique where far-end signals are\nutilized, incorporating both far-end and near-end signals to refine the score\nmodel's accuracy. We test our proposed method on the ICASSP2023 Microsoft deep\necho cancellation challenge evaluation dataset, where our method outperforms\nsome of the end-to-end methods and other diffusion based echo cancellation\nmethods.", "published": "2024-01-08 23:38:04", "link": "http://arxiv.org/abs/2401.04283v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DDD: A Perceptually Superior Low-Response-Time DNN-based Declipper", "abstract": "Clipping is a common nonlinear distortion that occurs whenever the input or\noutput of an audio system exceeds the supported range. This phenomenon\nundermines not only the perception of speech quality but also downstream\nprocesses utilizing the disrupted signal. Therefore, a real-time-capable,\nrobust, and low-response-time method for speech declipping (SD) is desired. In\nthis work, we introduce DDD (Demucs-Discriminator-Declipper), a\nreal-time-capable speech-declipping deep neural network (DNN) that requires\nless response time by design. We first observe that a previously untested\nreal-time-capable DNN model, Demucs, exhibits a reasonable declipping\nperformance. Then we utilize adversarial learning objectives to increase the\nperceptual quality of output speech without additional inference overhead.\nSubjective evaluations on harshly clipped speech shows that DDD outperforms the\nbaselines by a wide margin in terms of speech quality. We perform detailed\nwaveform and spectral analyses to gain an insight into the output behavior of\nDDD in comparison to the baselines. Finally, our streaming simulations also\nshow that DDD is capable of sub-decisecond mean response times, outperforming\nthe state-of-the-art DNN approach by a factor of six.", "published": "2024-01-08 03:43:57", "link": "http://arxiv.org/abs/2401.03650v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Exploratory Evaluation of Speech Content Masking", "abstract": "Most recent speech privacy efforts have focused on anonymizing acoustic\nspeaker attributes but there has not been as much research into protecting\ninformation from speech content. We introduce a toy problem that explores an\nemerging type of privacy called \"content masking\" which conceals selected words\nand phrases in speech. In our efforts to define this problem space, we evaluate\nan introductory baseline masking technique based on modifying sequences of\ndiscrete phone representations (phone codes) produced from a pre-trained\nvector-quantized variational autoencoder (VQ-VAE) and re-synthesized using\nWaveRNN. We investigate three different masking locations and three types of\nmasking strategies: noise substitution, word deletion, and phone sequence\nreversal. Our work attempts to characterize how masking affects two downstream\ntasks: automatic speech recognition (ASR) and automatic speaker verification\n(ASV). We observe how the different masks types and locations impact these\ndownstream tasks and discuss how these issues may influence privacy goals.", "published": "2024-01-08 14:56:03", "link": "http://arxiv.org/abs/2401.03936v1", "categories": ["eess.AS", "cs.CR", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Efficient Multiscale Multimodal Bottleneck Transformer for Audio-Video\n  Classification", "abstract": "In recent years, researchers combine both audio and video signals to deal\nwith challenges where actions are not well represented or captured by visual\ncues. However, how to effectively leverage the two modalities is still under\ndevelopment. In this work, we develop a multiscale multimodal Transformer (MMT)\nthat leverages hierarchical representation learning. Particularly, MMT is\ncomposed of a novel multiscale audio Transformer (MAT) and a multiscale video\nTransformer [43]. To learn a discriminative cross-modality fusion, we further\ndesign multimodal supervised contrastive objectives called audio-video\ncontrastive loss (AVC) and intra-modal contrastive loss (IMC) that robustly\nalign the two modalities. MMT surpasses previous state-of-the-art approaches by\n7.3% and 2.1% on Kinetics-Sounds and VGGSound in terms of the top-1 accuracy\nwithout external training data. Moreover, the proposed MAT significantly\noutperforms AST [28] by 22.2%, 4.4% and 4.7% on three public benchmark\ndatasets, and is about 3% more efficient based on the number of FLOPs and 9.8%\nmore efficient based on GPU memory usage.", "published": "2024-01-08 17:02:25", "link": "http://arxiv.org/abs/2401.04023v1", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Efficient Selective Audio Masked Multimodal Bottleneck Transformer for\n  Audio-Video Classification", "abstract": "Audio and video are two most common modalities in the mainstream media\nplatforms, e.g., YouTube. To learn from multimodal videos effectively, in this\nwork, we propose a novel audio-video recognition approach termed audio video\nTransformer, AVT, leveraging the effective spatio-temporal representation by\nthe video Transformer to improve action recognition accuracy. For multimodal\nfusion, simply concatenating multimodal tokens in a cross-modal Transformer\nrequires large computational and memory resources, instead we reduce the\ncross-modality complexity through an audio-video bottleneck Transformer. To\nimprove the learning efficiency of multimodal Transformer, we integrate\nself-supervised objectives, i.e., audio-video contrastive learning, audio-video\nmatching, and masked audio and video learning, into AVT training, which maps\ndiverse audio and video representations into a common multimodal representation\nspace. We further propose a masked audio segment loss to learn semantic audio\nactivities in AVT. Extensive experiments and ablation studies on three public\ndatasets and two in-house datasets consistently demonstrate the effectiveness\nof the proposed AVT. Specifically, AVT outperforms its previous\nstate-of-the-art counterparts on Kinetics-Sounds by 8%. AVT also surpasses one\nof the previous state-of-the-art video Transformers [25] by 10% on VGGSound by\nleveraging the audio signal. Compared to one of the previous state-of-the-art\nmultimodal methods, MBT [32], AVT is 1.3% more efficient in terms of FLOPs and\nimproves the accuracy by 3.8% on Epic-Kitchens-100.", "published": "2024-01-08 16:58:59", "link": "http://arxiv.org/abs/2401.04154v1", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
