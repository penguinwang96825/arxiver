{"title": "A Reflective LLM-based Agent to Guide Zero-shot Cryptocurrency Trading", "abstract": "The utilization of Large Language Models (LLMs) in financial trading has\nprimarily been concentrated within the stock market, aiding in economic and\nfinancial decisions. Yet, the unique opportunities presented by the\ncryptocurrency market, noted for its on-chain data's transparency and the\ncritical influence of off-chain signals like news, remain largely untapped by\nLLMs. This work aims to bridge the gap by developing an LLM-based trading\nagent, CryptoTrade, which uniquely combines the analysis of on-chain and\noff-chain data. This approach leverages the transparency and immutability of\non-chain data, as well as the timeliness and influence of off-chain signals,\nproviding a comprehensive overview of the cryptocurrency market. CryptoTrade\nincorporates a reflective mechanism specifically engineered to refine its daily\ntrading decisions by analyzing the outcomes of prior trading decisions. This\nresearch makes two significant contributions. Firstly, it broadens the\napplicability of LLMs to the domain of cryptocurrency trading. Secondly, it\nestablishes a benchmark for cryptocurrency trading strategies. Through\nextensive experiments, CryptoTrade has demonstrated superior performance in\nmaximizing returns compared to traditional trading strategies and time-series\nbaselines across various cryptocurrencies and market conditions. Our code and\ndata are available at\n\\url{https://anonymous.4open.science/r/CryptoTrade-Public-92FC/}.", "published": "2024-06-27 08:31:05", "link": "http://arxiv.org/abs/2407.09546v1", "categories": ["q-fin.TR", "cs.SI"], "primary_category": "q-fin.TR"}
{"title": "OutlierTune: Efficient Channel-Wise Quantization for Large Language\n  Models", "abstract": "Quantizing the activations of large language models (LLMs) has been a\nsignificant challenge due to the presence of structured outliers. Most existing\nmethods focus on the per-token or per-tensor quantization of activations,\nmaking it difficult to achieve both accuracy and hardware efficiency. To\naddress this problem, we propose OutlierTune, an efficient per-channel\npost-training quantization (PTQ) method for the activations of LLMs.\nOutlierTune consists of two components: pre-execution of dequantization and\nsymmetrization. The pre-execution of dequantization updates the model weights\nby the activation scaling factors, avoiding the internal scaling and costly\nadditional computational overheads brought by the per-channel activation\nquantization. The symmetrization further reduces the quantization differences\narising from the weight updates by ensuring the balanced numerical ranges\nacross different activation channels. OutlierTune is easy to implement and\nhardware-efficient, introducing almost no additional computational overheads\nduring the inference. Extensive experiments show that the proposed framework\noutperforms existing methods across multiple different tasks. Demonstrating\nbetter generalization, this framework improves the Int6 quantization of the\ninstruction-tuning LLMs, such as OPT-IML, to the same level as half-precision\n(FP16). Moreover, we have shown that the proposed framework is 1.48x faster\nthan the FP16 implementation while reducing approximately 2x memory usage.", "published": "2024-06-27 02:02:26", "link": "http://arxiv.org/abs/2406.18832v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficacy of Language Model Self-Play in Non-Zero-Sum Games", "abstract": "Game-playing agents like AlphaGo have achieved superhuman performance through\nself-play, which is theoretically guaranteed to yield optimal policies in\ncompetitive games. However, most language tasks are partially or fully\ncooperative, so it is an open question whether techniques like self-play can\neffectively be used to improve language models. We empirically investigate this\nquestion in a negotiation game setting known as Deal or No Deal (DoND).\nCrucially, the objective in DoND can be modified to produce a fully cooperative\ngame, a strictly competitive one, or anything in between. We finetune language\nmodels in self-play over multiple rounds of filtered behavior cloning in DoND\nfor each of these objectives and evaluate them in self-play and in\ncollaboration with humans. We find that language models improve substantially\nin self-play, achieving 14-17x higher scores in task reward after finetuning.\nFurther, the trained models generalize to both cooperation and competition with\nhumans, scoring 2.5-6x higher than base models. We view these results as an\nearly promising sign for language model self-play in cooperative settings,\ndespite a lack of theoretical guarantees.", "published": "2024-06-27 03:52:35", "link": "http://arxiv.org/abs/2406.18872v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SSP: Self-Supervised Prompting for Cross-Lingual Transfer to\n  Low-Resource Languages using Large Language Models", "abstract": "Recently, very large language models (LLMs) have shown exceptional\nperformance on several English NLP tasks with just in-context learning (ICL),\nbut their utility in other languages is still underexplored. We investigate\ntheir effectiveness for NLP tasks in low-resource languages (LRLs), especially\nin the setting of zero-labelled cross-lingual transfer (0-CLT), where no\nlabelled training data for the target language is available -- however training\ndata from one or more related medium-resource languages (MRLs) is utilized,\nalongside the available unlabeled test data for a target language. We introduce\nSelf-Supervised Prompting (SSP), a novel ICL approach tailored for the 0-CLT\nsetting.\n  SSP is based on the key observation that LLMs output more accurate labels if\nin-context exemplars are from the target language (even if their labels are\nslightly noisy). To operationalize this, since target language training data is\nnot available in 0-CLT, SSP operates in two stages. In Stage I, using source\nMRL training data, target language's test data is noisily labeled. In Stage II,\nthese noisy test data points are used as exemplars in ICL for further improved\nlabelling. Additionally, our implementation of SSP uses a novel Integer Linear\nProgramming (ILP)-based exemplar selection that balances similarity, prediction\nconfidence (when available) and label coverage. Experiments on three tasks and\neleven LRLs (from three regions) demonstrate that SSP strongly outperforms\nexisting SOTA fine-tuned and prompting-based baselines in 0-CLT setup.", "published": "2024-06-27 04:21:59", "link": "http://arxiv.org/abs/2406.18880v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can we teach language models to gloss endangered languages?", "abstract": "Interlinear glossed text (IGT) is a popular format in language documentation\nprojects, where each morpheme is labeled with a descriptive annotation.\nAutomating the creation of interlinear glossed text would be desirable to\nreduce annotator effort and maintain consistency across annotated corpora.\nPrior research has explored a number of statistical and neural methods for\nautomatically producing IGT.\n  As large language models (LLMs) have showed promising results across\nmultilingual tasks, even for rare, endangered languages, it is natural to\nwonder whether they can be utilized for the task of generating IGT. We explore\nwhether LLMs can be effective at the task of interlinear glossing with\nin-context learning, without any traditional training. We propose new\napproaches for selecting examples to provide in-context, observing that\ntargeted selection can significantly improve performance. We find that\nLLM-based methods beat standard transformer baselines, despite requiring no\ntraining at all. These approaches still underperform state-of-the-art\nsupervised systems for the task, but are highly practical for researchers\noutside of the NLP community, requiring minimal effort to use.", "published": "2024-06-27 05:17:04", "link": "http://arxiv.org/abs/2406.18895v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sonnet or Not, Bot? Poetry Evaluation for Large Models and Datasets", "abstract": "Large language models (LLMs) can now generate and recognize poetry. But what\ndo LLMs really know about poetry? We develop a task to evaluate how well LLMs\nrecognize one aspect of English-language poetry--poetic form--which captures\nmany different poetic features, including rhyme scheme, meter, and word or line\nrepetition. By using a benchmark dataset of over 4.1k human expert-annotated\npoems, we show that state-of-the-art LLMs can successfully identify both common\nand uncommon fixed poetic forms--such as sonnets, sestinas, and pantoums--with\nsurprisingly high accuracy. However, performance varies significantly by poetic\nform; the models struggle to identify unfixed poetic forms, especially those\nbased on topic or visual features. We additionally measure how many poems from\nour benchmark dataset are present in popular pretraining datasets or memorized\nby GPT-4, finding that pretraining presence and memorization may improve\nperformance on this task, but results are inconclusive. We release a benchmark\nevaluation dataset with 1.4k public domain poems and form annotations, results\nof memorization experiments and data audits, and code.", "published": "2024-06-27 05:36:53", "link": "http://arxiv.org/abs/2406.18906v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Historia Magistra Vitae: Dynamic Topic Modeling of Roman Literature\n  using Neural Embeddings", "abstract": "Dynamic topic models have been proposed as a tool for historical analysis,\nbut traditional approaches have had limited usefulness, being difficult to\nconfigure, interpret, and evaluate. In this work, we experiment with a recent\napproach for dynamic topic modeling using BERT embeddings. We compare topic\nmodels built using traditional statistical models (LDA and NMF) and the\nBERT-based model, modeling topics over the entire surviving corpus of Roman\nliterature. We find that while quantitative metrics prefer statistical models,\nqualitative evaluation finds better insights from the neural model.\nFurthermore, the neural topic model is less sensitive to hyperparameter\nconfiguration and thus may make dynamic topic modeling more viable for\nhistorical researchers.", "published": "2024-06-27 05:38:49", "link": "http://arxiv.org/abs/2406.18907v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Capturing Minds, Not Just Words: Enhancing Role-Playing Language Models\n  with Personality-Indicative Data", "abstract": "Role-playing agents (RPA) have been a popular application area for large\nlanguage models (LLMs), attracting significant interest from both industry and\nacademia.While existing RPAs well portray the characters' knowledge and tones,\nthey face challenges in capturing their minds, especially for small\nrole-playing language models (RPLMs). In this paper, we propose to enhance\nRPLMs via personality-indicative data. Specifically, we leverage questions from\npsychological scales and distill advanced RPAs to generate dialogues that grasp\nthe minds of characters. Experimental results validate that RPLMs trained with\nour dataset exhibit advanced role-playing capabilities for both general and\npersonality-related evaluations. Code and data are available at\n\\href{https://github.com/alienet1109/RolePersonality}{this URL}.", "published": "2024-06-27 06:24:00", "link": "http://arxiv.org/abs/2406.18921v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UniGen: A Unified Framework for Textual Dataset Generation Using Large\n  Language Models", "abstract": "Large Language Models (LLMs) such as GPT-4 and Llama3 have significantly\nimpacted various fields by enabling high-quality synthetic data generation and\nreducing dependence on expensive human-generated datasets. Despite this,\nchallenges remain in the areas of generalization, controllability, diversity,\nand truthfulness within the existing generative frameworks. To address these\nchallenges, this paper presents UniGen, a comprehensive LLM-powered framework\ndesigned to produce diverse, accurate, and highly controllable datasets. UniGen\nis adaptable, supporting all types of text datasets and enhancing the\ngenerative process through innovative mechanisms. To augment data diversity,\nUniGen incorporates an attribute-guided generation module and a group checking\nfeature. For accuracy, it employs a code-based mathematical assessment for\nlabel verification alongside a retrieval-augmented generation technique for\nfactual validation. The framework also allows for user-specified constraints,\nenabling customization of the data generation process to suit particular\nrequirements. Extensive experiments demonstrate the superior quality of data\ngenerated by UniGen, and each module within UniGen plays a critical role in\nthis enhancement. Additionally, UniGen is applied in two practical scenarios:\nbenchmarking LLMs and data augmentation. The results indicate that UniGen\neffectively supports dynamic and evolving benchmarking, and that data\naugmentation improves LLM capabilities in various domains, including\nagent-oriented abilities and reasoning skills.", "published": "2024-06-27 07:56:44", "link": "http://arxiv.org/abs/2406.18966v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Weak-to-Strong Generalization with Reliability-Aware Alignment", "abstract": "Large language models (LLMs) are now rapidly advancing and surpassing human\nabilities on many natural language tasks. However, aligning these super-human\nLLMs with human knowledge remains challenging because the supervision signals\nfrom human annotators may be wrong. This issue, known as the \"super-alignment\"\nproblem, requires enhancing weak-to-strong generalization, where a strong LLM\nmust generalize from imperfect supervision provided by a weaker source. To\naddress this issue, we propose an approach to improve weak-to-strong\ngeneralization by involving the reliability of weak supervision signals in the\nalignment process. In our method, we query the weak supervisor for multiple\nanswers, estimate the answer reliability, and enhance the alignment process by\nfiltering out uncertain data or re-weighting reliable data. Experiments on four\ndatasets demonstrate that our methods effectively identify the quality of weak\nlabels and significantly enhance weak-to-strong generalization. Our work\npresents effective techniques for error-robust model alignment, reducing error\npropagation from noisy supervision and enhancing the accuracy and reliability\nof LLMs. Codes are publicly available at\nhttp://github.com/Irenehere/ReliableAlignment.", "published": "2024-06-27 09:37:34", "link": "http://arxiv.org/abs/2406.19032v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "STBench: Assessing the Ability of Large Language Models in\n  Spatio-Temporal Analysis", "abstract": "The rapid evolution of large language models (LLMs) holds promise for\nreforming the methodology of spatio-temporal data mining. However, current\nworks for evaluating the spatio-temporal understanding capability of LLMs are\nsomewhat limited and biased. These works either fail to incorporate the latest\nlanguage models or only focus on assessing the memorized spatio-temporal\nknowledge. To address this gap, this paper dissects LLMs' capability of\nspatio-temporal data into four distinct dimensions: knowledge comprehension,\nspatio-temporal reasoning, accurate computation, and downstream applications.\nWe curate several natural language question-answer tasks for each category and\nbuild the benchmark dataset, namely STBench, containing 13 distinct tasks and\nover 60,000 QA pairs. Moreover, we have assessed the capabilities of 13 LLMs,\nsuch as GPT-4o, Gemma and Mistral. Experimental results reveal that existing\nLLMs show remarkable performance on knowledge comprehension and spatio-temporal\nreasoning tasks, with potential for further enhancement on other tasks through\nin-context learning, chain-of-though prompting, and fine-tuning. The code and\ndatasets of STBench are released on https://github.com/LwbXc/STBench.", "published": "2024-06-27 10:34:02", "link": "http://arxiv.org/abs/2406.19065v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AMBROSIA: A Benchmark for Parsing Ambiguous Questions into Database\n  Queries", "abstract": "Practical semantic parsers are expected to understand user utterances and map\nthem to executable programs, even when these are ambiguous. We introduce a new\nbenchmark, AMBROSIA, which we hope will inform and inspire the development of\ntext-to-SQL parsers capable of recognizing and interpreting ambiguous requests.\nOur dataset contains questions showcasing three different types of ambiguity\n(scope ambiguity, attachment ambiguity, and vagueness), their interpretations,\nand corresponding SQL queries. In each case, the ambiguity persists even when\nthe database context is provided. This is achieved through a novel approach\nthat involves controlled generation of databases from scratch. We benchmark\nvarious LLMs on AMBROSIA, revealing that even the most advanced models struggle\nto identify and interpret ambiguity in questions.", "published": "2024-06-27 10:43:04", "link": "http://arxiv.org/abs/2406.19073v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fairness and Bias in Multimodal AI: A Survey", "abstract": "The importance of addressing fairness and bias in artificial intelligence\n(AI) systems cannot be over-emphasized. Mainstream media has been awashed with\nnews of incidents around stereotypes and other types of bias in many of these\nsystems in recent years. In this survey, we fill a gap with regards to the\nrelatively minimal study of fairness and bias in Large Multimodal Models (LMMs)\ncompared to Large Language Models (LLMs), providing 50 examples of datasets and\nmodels related to both types of AI along with the challenges of bias affecting\nthem. We discuss the less-mentioned category of mitigating bias, preprocessing\n(with particular attention on the first part of it, which we call preuse). The\nmethod is less-mentioned compared to the two well-known ones in the literature:\nintrinsic and extrinsic mitigation methods. We critically discuss the various\nways researchers are addressing these challenges. Our method involved two\nslightly different search queries on two reputable search engines, Google\nScholar and Web of Science (WoS), which revealed that for the queries 'Fairness\nand bias in Large Multimodal Models' and 'Fairness and bias in Large Language\nModels', 33,400 and 538,000 links are the initial results, respectively, for\nScholar while 4 and 50 links are the initial results, respectively, for WoS.\nFor reproducibility and verification, we provide links to the search results\nand the citations to all the final reviewed papers. We believe this work\ncontributes to filling this gap and providing insight to researchers and other\nstakeholders on ways to address the challenges of fairness and bias in\nmultimodal and language AI.", "published": "2024-06-27 11:26:17", "link": "http://arxiv.org/abs/2406.19097v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Illusion of Competence: Evaluating the Effect of Explanations on\n  Users' Mental Models of Visual Question Answering Systems", "abstract": "We examine how users perceive the limitations of an AI system when it\nencounters a task that it cannot perform perfectly and whether providing\nexplanations alongside its answers aids users in constructing an appropriate\nmental model of the system's capabilities and limitations. We employ a visual\nquestion answer and explanation task where we control the AI system's\nlimitations by manipulating the visual inputs: during inference, the system\neither processes full-color or grayscale images. Our goal is to determine\nwhether participants can perceive the limitations of the system. We hypothesize\nthat explanations will make limited AI capabilities more transparent to users.\nHowever, our results show that explanations do not have this effect. Instead of\nallowing users to more accurately assess the limitations of the AI system,\nexplanations generally increase users' perceptions of the system's competence -\nregardless of its actual performance.", "published": "2024-06-27 13:44:03", "link": "http://arxiv.org/abs/2406.19170v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Annotation Errors and NER: A Study with OntoNotes 5.0", "abstract": "Named Entity Recognition (NER) is a well-studied problem in NLP. However,\nthere is much less focus on studying NER datasets, compared to developing new\nNER models. In this paper, we employed three simple techniques to detect\nannotation errors in the OntoNotes 5.0 corpus for English NER, which is the\nlargest available NER corpus for English. Our techniques corrected ~10% of the\nsentences in train/dev/test data. In terms of entity mentions, we corrected the\nspan and/or type of ~8% of mentions in the dataset, while\nadding/deleting/splitting/merging a few more. These are large numbers of\nchanges, considering the size of OntoNotes. We used three NER libraries to\ntrain, evaluate and compare the models trained with the original and the\nre-annotated datasets, which showed an average improvement of 1.23% in overall\nF-scores, with large (>10%) improvements for some of the entity types. While\nour annotation error detection methods are not exhaustive and there is some\nmanual annotation effort involved, they are largely language agnostic and can\nbe employed with other NER datasets, and other sequence labelling tasks.", "published": "2024-06-27 13:48:46", "link": "http://arxiv.org/abs/2406.19172v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SeaKR: Self-aware Knowledge Retrieval for Adaptive Retrieval Augmented\n  Generation", "abstract": "This paper introduces Self-aware Knowledge Retrieval (SeaKR), a novel\nadaptive RAG model that extracts self-aware uncertainty of LLMs from their\ninternal states. SeaKR activates retrieval when the LLMs present high\nself-aware uncertainty for generation. To effectively integrate retrieved\nknowledge snippets, SeaKR re-ranks them based on LLM's self-aware uncertainty\nto preserve the snippet that reduces their uncertainty to the utmost. To\nfacilitate solving complex tasks that require multiple retrievals, SeaKR\nutilizes their self-aware uncertainty to choose among different reasoning\nstrategies. Our experiments on both complex and simple Question Answering\ndatasets show that SeaKR outperforms existing adaptive RAG methods. We release\nour code at https://github.com/THU-KEG/SeaKR.", "published": "2024-06-27 14:38:33", "link": "http://arxiv.org/abs/2406.19215v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Aligning Teacher with Student Preferences for Tailored Training Data\n  Generation", "abstract": "Large Language Models (LLMs) have shown significant promise as copilots in\nvarious tasks. Local deployment of LLMs on edge devices is necessary when\nhandling privacy-sensitive data or latency-sensitive tasks. The computational\nconstraints of such devices make direct deployment of powerful large-scale LLMs\nimpractical, necessitating the Knowledge Distillation from large-scale models\nto lightweight models. Lots of work has been done to elicit diversity and\nquality training examples from LLMs, but little attention has been paid to\naligning teacher instructional content based on student preferences, akin to\n\"responsive teaching\" in pedagogy. Thus, we propose ARTE, dubbed Aligning\nTeacheR with StudenT PreferencEs, a framework that aligns the teacher model\nwith student preferences to generate tailored training examples for Knowledge\nDistillation. Specifically, we elicit draft questions and rationales from the\nteacher model, then collect student preferences on these questions and\nrationales using students' performance with in-context learning as a proxy, and\nfinally align the teacher model with student preferences. In the end, we repeat\nthe first step with the aligned teacher model to elicit tailored training\nexamples for the student model on the target task. Extensive experiments on\nacademic benchmarks demonstrate the superiority of ARTE over existing\ninstruction-tuning datasets distilled from powerful LLMs. Moreover, we\nthoroughly investigate the generalization of ARTE, including the generalization\nof fine-tuned student models in reasoning ability and the generalization of\naligned teacher models to generate tailored training data across tasks and\nstudents. In summary, our contributions lie in proposing a novel framework for\ntailored training example generation, demonstrating its efficacy in\nexperiments, and investigating the generalization of both student & aligned\nteacher models in ARTE.", "published": "2024-06-27 14:51:17", "link": "http://arxiv.org/abs/2406.19227v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RuBLiMP: Russian Benchmark of Linguistic Minimal Pairs", "abstract": "Minimal pairs are a well-established approach to evaluating the grammatical\nknowledge of language models. However, existing resources for minimal pairs\naddress a limited number of languages and lack diversity of language-specific\ngrammatical phenomena. This paper introduces the Russian Benchmark of\nLinguistic Minimal Pairs (RuBLiMP), which includes 45k pairs of sentences that\ndiffer in grammaticality and isolate a morphological, syntactic, or semantic\nphenomenon. In contrast to existing benchmarks of linguistic minimal pairs,\nRuBLiMP is created by applying linguistic perturbations to automatically\nannotated sentences from open text corpora and carefully curating test data. We\ndescribe the data collection protocol and present the results of evaluating 25\nlanguage models in various scenarios. We find that the widely used language\nmodels for Russian are sensitive to morphological and agreement-oriented\ncontrasts but fall behind humans on phenomena requiring understanding of\nstructural relations, negation, transitivity, and tense. RuBLiMP, the codebase,\nand other materials are publicly available.", "published": "2024-06-27 14:55:19", "link": "http://arxiv.org/abs/2406.19232v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "VERISCORE: Evaluating the factuality of verifiable claims in long-form\n  text generation", "abstract": "Existing metrics for evaluating the factuality of long-form text, such as\nFACTSCORE (Min et al., 2023) and SAFE (Wei et al., 2024), decompose an input\ntext into \"atomic claims\" and verify each against a knowledge base like\nWikipedia. These metrics are not suitable for most generation tasks because\nthey assume that every claim is verifiable (i.e., can plausibly be proven true\nor false). We address this issue with VERISCORE, a metric for diverse long-form\ngeneration tasks that contain both verifiable and unverifiable content.\nVERISCORE can be effectively implemented with either closed or fine-tuned\nopen-weight language models, and human evaluation confirms that VERISCORE's\nextracted claims are more sensible than those from competing methods across\neight different long-form tasks. We use VERISCORE to evaluate generations from\n16 different models across multiple long-form tasks and find that while GPT-4o\nis the best-performing model overall, open-weight models such as Mixtral-8x22\nare closing the gap. We show that an LM's VERISCORE on one task (e.g.,\nbiography generation) does not necessarily correlate to its VERISCORE on a\ndifferent task (e.g., long-form QA), highlighting the need for expanding\nfactuality evaluation across tasks with varying fact density.", "published": "2024-06-27 15:43:18", "link": "http://arxiv.org/abs/2406.19276v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Odyssey of Commonsense Causality: From Foundational Benchmarks to\n  Cutting-Edge Reasoning", "abstract": "Understanding commonsense causality is a unique mark of intelligence for\nhumans. It helps people understand the principles of the real world better and\nbenefits the decision-making process related to causation. For instance,\ncommonsense causality is crucial in judging whether a defendant's action causes\nthe plaintiff's loss in determining legal liability. Despite its significance,\na systematic exploration of this topic is notably lacking. Our comprehensive\nsurvey bridges this gap by focusing on taxonomies, benchmarks, acquisition\nmethods, qualitative reasoning, and quantitative measurements in commonsense\ncausality, synthesizing insights from over 200 representative articles. Our\nwork aims to provide a systematic overview, update scholars on recent\nadvancements, provide a pragmatic guide for beginners, and highlight promising\nfuture research directions in this vital field.", "published": "2024-06-27 16:30:50", "link": "http://arxiv.org/abs/2406.19307v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Model Arena for Cross-lingual Sentiment Analysis: A Comparative\n  Study in the Era of Large Language Models", "abstract": "Sentiment analysis serves as a pivotal component in Natural Language\nProcessing (NLP). Advancements in multilingual pre-trained models such as XLM-R\nand mT5 have contributed to the increasing interest in cross-lingual sentiment\nanalysis. The recent emergence in Large Language Models (LLM) has significantly\nadvanced general NLP tasks, however, the capability of such LLMs in\ncross-lingual sentiment analysis has not been fully studied. This work\nundertakes an empirical analysis to compare the cross-lingual transfer\ncapability of public Small Multilingual Language Models (SMLM) like XLM-R,\nagainst English-centric LLMs such as Llama-3, in the context of sentiment\nanalysis across English, Spanish, French and Chinese. Our findings reveal that\namong public models, SMLMs exhibit superior zero-shot cross-lingual performance\nrelative to LLMs. However, in few-shot cross-lingual settings, public LLMs\ndemonstrate an enhanced adaptive potential. In addition, we observe that\nproprietary GPT-3.5 and GPT-4 lead in zero-shot cross-lingual capability, but\nare outpaced by public models in few-shot scenarios.", "published": "2024-06-27 17:38:45", "link": "http://arxiv.org/abs/2406.19358v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Suri: Multi-constraint Instruction Following for Long-form Text\n  Generation", "abstract": "Existing research on instruction following largely focuses on tasks with\nsimple instructions and short responses. In this work, we explore\nmulti-constraint instruction following for generating long-form text. We create\nSuri, a dataset with 20K human-written long-form texts paired with\nLLM-generated backtranslated instructions that contain multiple complex\nconstraints. Because of prohibitive challenges associated with collecting human\npreference judgments on long-form texts, preference-tuning algorithms such as\nDPO are infeasible in our setting; thus, we propose Instructional ORPO\n(I-ORPO), an alignment method based on the ORPO algorithm. Instead of receiving\nnegative feedback from dispreferred responses, I-ORPO obtains negative feedback\nfrom synthetically corrupted instructions generated by an LLM. Using Suri, we\nperform supervised and I-ORPO fine-tuning on Mistral-7b-Instruct-v0.2. The\nresulting models, Suri-SFT and Suri-I-ORPO, generate significantly longer texts\n(~5K tokens) than base models without significant quality deterioration. Our\nhuman evaluation shows that while both SFT and I-ORPO models satisfy most\nconstraints, Suri-I-ORPO generations are generally preferred for their coherent\nand informative incorporation of the constraints. We release our code at\nhttps://github.com/chtmp223/suri.", "published": "2024-06-27 17:50:35", "link": "http://arxiv.org/abs/2406.19371v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Large Language Models Generate High-quality Patent Claims?", "abstract": "Large language models (LLMs) have shown exceptional performance across\nvarious text generation tasks but remain under-explored in the patent domain,\nwhich offers highly structured and precise language. This paper constructs a\ndataset to investigate the performance of current LLMs in patent claim\ngeneration. Our results demonstrate that generating claims based on patent\ndescriptions outperforms previous research relying on abstracts. Interestingly,\ncurrent patent-specific LLMs perform much worse than state-of-the-art general\nLLMs, highlighting the necessity for future research on in-domain LLMs. We also\nfind that LLMs can produce high-quality first independent claims, but their\nperformances markedly decrease for subsequent dependent claims. Moreover,\nfine-tuning can enhance the completeness of inventions' features, conceptual\nclarity, and feature linkage. Among the tested LLMs, GPT-4 demonstrates the\nbest performance in comprehensive human evaluations by patent experts, with\nbetter feature coverage, conceptual clarity, and technical coherence. Despite\nthese capabilities, comprehensive revision and modification are still necessary\nto pass rigorous patent scrutiny and ensure legal robustness.", "published": "2024-06-27 18:07:40", "link": "http://arxiv.org/abs/2406.19465v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Changing Answer Order Can Decrease MMLU Accuracy", "abstract": "As large language models (LLMs) have grown in prevalence, particular\nbenchmarks have become essential for the evaluation of these models and for\nunderstanding model capabilities. Most commonly, we use test accuracy averaged\nacross multiple subtasks in order to rank models on leaderboards, to determine\nwhich model is best for our purposes. In this paper, we investigate the\nrobustness of the accuracy measurement on a widely used multiple choice\nquestion answering dataset, MMLU. When shuffling the answer label contents, we\nfind that all explored models decrease in accuracy on MMLU, but not every model\nis equally sensitive. These findings suggest a possible adjustment to the\nstandard practice of leaderboard testing, where we additionally consider the\npercentage of examples each model answers correctly by random chance.", "published": "2024-06-27 18:21:32", "link": "http://arxiv.org/abs/2406.19470v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "xTower: A Multilingual LLM for Explaining and Correcting Translation\n  Errors", "abstract": "While machine translation (MT) systems are achieving increasingly strong\nperformance on benchmarks, they often produce translations with errors and\nanomalies. Understanding these errors can potentially help improve the\ntranslation quality and user experience. This paper introduces xTower, an open\nlarge language model (LLM) built on top of TowerBase designed to provide\nfree-text explanations for translation errors in order to guide the generation\nof a corrected translation. The quality of the generated explanations by xTower\nare assessed via both intrinsic and extrinsic evaluation. We ask expert\ntranslators to evaluate the quality of the explanations across two dimensions:\nrelatedness towards the error span being explained and helpfulness in error\nunderstanding and improving translation quality. Extrinsically, we test xTower\nacross various experimental setups in generating translation corrections,\ndemonstrating significant improvements in translation quality. Our findings\nhighlight xTower's potential towards not only producing plausible and helpful\nexplanations of automatic translations, but also leveraging them to suggest\ncorrected translations.", "published": "2024-06-27 18:51:46", "link": "http://arxiv.org/abs/2406.19482v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are Generative Language Models Multicultural? A Study on Hausa Culture\n  and Emotions using ChatGPT", "abstract": "Large Language Models (LLMs), such as ChatGPT, are widely used to generate\ncontent for various purposes and audiences. However, these models may not\nreflect the cultural and emotional diversity of their users, especially for\nlow-resource languages. In this paper, we investigate how ChatGPT represents\nHausa's culture and emotions. We compare responses generated by ChatGPT with\nthose provided by native Hausa speakers on 37 culturally relevant questions. We\nconducted experiments using emotion analysis and applied two similarity metrics\nto measure the alignment between human and ChatGPT responses. We also collected\nhuman participants ratings and feedback on ChatGPT responses. Our results show\nthat ChatGPT has some level of similarity to human responses, but also exhibits\nsome gaps and biases in its knowledge and awareness of the Hausa culture and\nemotions. We discuss the implications and limitations of our methodology and\nanalysis and suggest ways to improve the performance and evaluation of LLMs for\nlow-resource languages.", "published": "2024-06-27 19:42:13", "link": "http://arxiv.org/abs/2406.19504v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Context Matters: An Empirical Study of the Impact of Contextual\n  Information in Temporal Question Answering Systems", "abstract": "Large language models (LLMs) often struggle with temporal reasoning, crucial\nfor tasks like historical event analysis and time-sensitive information\nretrieval. Despite advancements, state-of-the-art models falter in handling\ntemporal information, especially when faced with irrelevant or noisy contexts.\nThis paper addresses this gap by empirically examining the robustness of\ntemporal question-answering (TQA) systems trained on various context types,\nincluding relevant, irrelevant, slightly altered, and no context. Our findings\nindicate that training with a mix of these contexts enhances model robustness\nand accuracy. Additionally, we show that the position of context relative to\nthe question significantly impacts performance, with question-first positioning\nyielding better results. We introduce two new context-rich TQA datasets,\nContextAQA and ContextTQE, and provide comprehensive evaluations and guidelines\nfor training robust TQA models. Our work lays the foundation for developing\nreliable and context-aware temporal QA systems, with broader implications for\nenhancing LLM robustness against diverse and potentially adversarial\ninformation.", "published": "2024-06-27 21:31:30", "link": "http://arxiv.org/abs/2406.19538v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Voices Unheard: NLP Resources and Models for Yor\u00f9b\u00e1 Regional\n  Dialects", "abstract": "Yor\\`ub\\'a an African language with roughly 47 million speakers encompasses a\ncontinuum with several dialects. Recent efforts to develop NLP technologies for\nAfrican languages have focused on their standard dialects, resulting in\ndisparities for dialects and varieties for which there are little to no\nresources or tools. We take steps towards bridging this gap by introducing a\nnew high-quality parallel text and speech corpus YOR\\`ULECT across three\ndomains and four regional Yor\\`ub\\'a dialects. To develop this corpus, we\nengaged native speakers, travelling to communities where these dialects are\nspoken, to collect text and speech data. Using our newly created corpus, we\nconducted extensive experiments on (text) machine translation, automatic speech\nrecognition, and speech-to-text translation. Our results reveal substantial\nperformance disparities between standard Yor\\`ub\\'a and the other dialects\nacross all tasks. However, we also show that with dialect-adaptive finetuning,\nwe are able to narrow this gap. We believe our dataset and experimental\nanalysis will contribute greatly to developing NLP tools for Yor\\`ub\\'a and its\ndialects, and potentially for other African languages, by improving our\nunderstanding of existing challenges and offering a high-quality dataset for\nfurther development. We release YOR\\`ULECT dataset and models publicly under an\nopen license.", "published": "2024-06-27 22:38:04", "link": "http://arxiv.org/abs/2406.19564v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Two-Pronged Human Evaluation of ChatGPT Self-Correction in Radiology\n  Report Simplification", "abstract": "Radiology reports are highly technical documents aimed primarily at\ndoctor-doctor communication. There has been an increasing interest in sharing\nthose reports with patients, necessitating providing them patient-friendly\nsimplifications of the original reports. This study explores the suitability of\nlarge language models in automatically generating those simplifications. We\nexamine the usefulness of chain-of-thought and self-correction prompting\nmechanisms in this domain. We also propose a new evaluation protocol that\nemploys radiologists and laypeople, where radiologists verify the factual\ncorrectness of simplifications, and laypeople assess simplicity and\ncomprehension. Our experimental results demonstrate the effectiveness of\nself-correction prompting in producing high-quality simplifications. Our\nfindings illuminate the preferences of radiologists and laypeople regarding\ntext simplification, informing future research on this topic.", "published": "2024-06-27 03:05:35", "link": "http://arxiv.org/abs/2406.18859v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DeSTA: Enhancing Speech Language Models through Descriptive Speech-Text\n  Alignment", "abstract": "Recent speech language models (SLMs) typically incorporate pre-trained speech\nmodels to extend the capabilities from large language models (LLMs). In this\npaper, we propose a Descriptive Speech-Text Alignment approach that leverages\nspeech captioning to bridge the gap between speech and text modalities,\nenabling SLMs to interpret and generate comprehensive natural language\ndescriptions, thereby facilitating the capability to understand both linguistic\nand non-linguistic features in speech. Enhanced with the proposed approach, our\nmodel demonstrates superior performance on the Dynamic-SUPERB benchmark,\nparticularly in generalizing to unseen tasks. Moreover, we discover that the\naligned model exhibits a zero-shot instruction-following capability without\nexplicit speech instruction tuning. These findings highlight the potential to\nreshape instruction-following SLMs by incorporating rich, descriptive speech\ncaptions.", "published": "2024-06-27 03:52:35", "link": "http://arxiv.org/abs/2406.18871v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "TrustUQA: A Trustful Framework for Unified Structured Data Question\n  Answering", "abstract": "Natural language question answering (QA) over structured data sources such as\ntables and knowledge graphs have been widely investigated, especially with\nLarge Language Models (LLMs) in recent years. The main solutions include\nquestion to formal query parsing and retrieval-based answer generation.\nHowever, current methods of the former often suffer from weak generalization,\nfailing to dealing with multi-types of sources, while the later is limited in\ntrustfulness. In this paper, we propose TrustUQA, a trustful QA framework that\ncan simultaneously support multiple types of structured data in a unified way.\nTo this end, it adopts an LLM-friendly and unified knowledge representation\nmethod called Condition Graph(CG), and uses an LLM and demonstration-based\ntwo-level method for CG querying. For enhancement, it is also equipped with\ndynamic demonstration retrieval. We have evaluated TrustUQA with 5 benchmarks\ncovering 3 types of structured data. It outperforms 2 existing unified\nstructured data QA methods. In comparison with the baselines that are specific\nto one data type, it achieves state-of-the-art on 2 of the datasets. Further\nmore, we have demonstrated the potential of our method for more general QA\ntasks, QA over mixed structured data and QA across structured data. The code is\navailable at https://github.com/zjukg/TrustUQA.", "published": "2024-06-27 06:13:05", "link": "http://arxiv.org/abs/2406.18916v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Selective Vision is the Challenge for Visual Reasoning: A Benchmark for\n  Visual Argument Understanding", "abstract": "Visual arguments, often used in advertising or social causes, rely on images\nto persuade viewers to do or believe something. Understanding these arguments\nrequires selective vision: only specific visual stimuli within an image are\nrelevant to the argument, and relevance can only be understood within the\ncontext of a broader argumentative structure. While visual arguments are\nreadily appreciated by human audiences, we ask: are today's AI capable of\nsimilar understanding? We present VisArgs, a dataset of 1,611 images annotated\nwith 5,112 visual premises (with regions), 5,574 commonsense premises, and\nreasoning trees connecting them into structured arguments. We propose three\ntasks for evaluating visual argument understanding: premise localization,\npremise identification, and conclusion deduction. Experiments show that 1)\nmachines struggle to capture visual cues: GPT-4-O achieved 78.5% accuracy,\nwhile humans reached 98.0%. Models also performed 19.5% worse when\ndistinguishing between irrelevant objects within the image compared to external\nobjects. 2) Providing relevant visual premises improved model performance\nsignificantly.", "published": "2024-06-27 06:32:56", "link": "http://arxiv.org/abs/2406.18925v3", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "The single-use restriction for register automata and transducers over\n  infinite alphabets", "abstract": "This thesis studies the single-use restriction for register automata and\ntransducers over infinite alphabets. The restriction requires that a\nread-access to a register should have the side effect of destroying its\ncontents. This constraint results in robust classes of languages and\ntransductions. For automata models, we show that one-way register automata,\ntwo-way register automata, and orbit-finite monoids have the same expressive\npower. For transducer models, we show that single-use Mealy machines and\nsingle-use two-way transducers admit versions of the Krohn-Rhodes decomposition\ntheorem. Moreover, single-use Mealy machines are equivalent to an algebraic\nmodel called local algebraic semigroup transductions. Additionally, we show\nthat single-use two-way transducers are equivalent to single-use streaming\nstring transducers (SSTs) over infinite alphabets and to regular list functions\nwith atoms.\n  Compared with the previous work arXiv:1907.10504, this thesis offers a\ncoherent narrative on the single-use restriction. We introduce an abstract\nnotion of single-use functions and use them to define all the discussed\nsingle-use models. We also introduce and study the algebraic models of local\nsemigroup transduction and local rational semigroup transduction.", "published": "2024-06-27 07:01:23", "link": "http://arxiv.org/abs/2406.18934v1", "categories": ["cs.FL", "cs.CL", "F.4.3"], "primary_category": "cs.FL"}
{"title": "Applying LLMs for Rescoring N-best ASR Hypotheses of Casual\n  Conversations: Effects of Domain Adaptation and Context Carry-over", "abstract": "Large language models (LLMs) have been successfully applied for rescoring\nautomatic speech recognition (ASR) hypotheses. However, their ability to\nrescore ASR hypotheses of casual conversations has not been sufficiently\nexplored. In this study, we reveal it by performing N-best ASR hypotheses\nrescoring using Llama2 on the CHiME-7 distant ASR (DASR) task. Llama2 is one of\nthe most representative LLMs, and the CHiME-7 DASR task provides datasets of\ncasual conversations between multiple participants. We investigate the effects\nof domain adaptation of the LLM and context carry-over when performing N-best\nrescoring. Experimental results show that, even without domain adaptation,\nLlama2 outperforms a standard-size domain-adapted Transformer-LM, especially\nwhen using a long context. Domain adaptation shortens the context length needed\nwith Llama2 to achieve its best performance, i.e., it reduces the computational\ncost of Llama2.", "published": "2024-06-27 08:03:13", "link": "http://arxiv.org/abs/2406.18972v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "EmPO: Emotion Grounding for Empathetic Response Generation through\n  Preference Optimization", "abstract": "Empathetic response generation is a desirable aspect of conversational\nagents, crucial for facilitating engaging and emotionally intelligent\nmulti-turn conversations between humans and machines. Leveraging large language\nmodels for this task has shown promising results, yet challenges persist in\nensuring both the empathetic quality of the responses and retention of the\ngeneralization performance of the models. We propose a novel approach where we\nconstruct theory-driven preference datasets based on emotion grounding and use\nthem to align LLMs with preference optimization algorithms to address these\nchallenges. To evaluate empathetic response generation, we employ the\nEmpatheticDialogues dataset, assessing empathy with the diff-Epitome and\nBERTscore metrics and with multi-dimensional human evaluation. Additionally, we\nmeasure diversity and emotional valence using feature-based methods. We also\nevaluate the impact of training on the generalization performance using the\nMMLU benchmark and tasks from the Open LLM Leaderboard. The results show that\nLLMs can be aligned for empathetic response generation by preference\noptimization while retaining their general performance and that emotion\ngrounding can guide preference dataset creation. We make all datasets, source\ncode, and models publicly available. https://github.com/justtherightsize/empo", "published": "2024-06-27 10:41:22", "link": "http://arxiv.org/abs/2406.19071v2", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Resolving Discrepancies in Compute-Optimal Scaling of Language Models", "abstract": "Kaplan et al. and Hoffmann et al. developed influential scaling laws for the\noptimal model size as a function of the compute budget, but these laws yield\nsubstantially different predictions. We explain the discrepancy by reproducing\nthe Kaplan scaling law on two datasets (OpenWebText2 and RefinedWeb) and\nidentifying three factors causing the difference: last layer computational\ncost, warmup duration, and scale-dependent optimizer tuning. With these factors\ncorrected, we obtain excellent agreement with the Hoffmann et al. (i.e.,\n\"Chinchilla\") scaling law. Counter to a hypothesis of Hoffmann et al., we find\nthat careful learning rate decay is not essential for the validity of their\nscaling law. As a secondary result, we derive scaling laws for the optimal\nlearning rate and batch size, finding that tuning the AdamW $\\beta_2$ parameter\nis essential at lower batch sizes.", "published": "2024-06-27 13:02:43", "link": "http://arxiv.org/abs/2406.19146v4", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Simulating Classroom Education with LLM-Empowered Agents", "abstract": "Large language models (LLMs) have been applied across various intelligent\neducational tasks to assist teaching. While preliminary studies have focused on\ntask-specific, independent LLM-empowered agents, the potential of LLMs within a\nmulti-agent collaborative framework for classroom simulation with real user\nparticipation remains unexplored. In this work, we propose SimClass, a\nmulti-agent classroom simulation teaching framework. We recognize\nrepresentative class roles and introduce a novel class control mechanism for\nautomatic classroom teaching, and conduct user experiments in two real-world\ncourses. Using the Flanders Interactive Analysis System and Community of\nInquiry theoretical frameworks from educational analysis, we demonstrate that\nLLMs can simulate a dynamic learning environment for users with active\nteacher-student and student-student interactions. We also observe group\nbehaviors among agents in SimClass, where agents collaborate to create\nenlivening interactions in classrooms to improve user learning process. We hope\nthis work pioneers the application of LLM-empowered multi-agent systems in\nvirtual classroom teaching.", "published": "2024-06-27 14:51:07", "link": "http://arxiv.org/abs/2406.19226v2", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Spiking Convolutional Neural Networks for Text Classification", "abstract": "Spiking neural networks (SNNs) offer a promising pathway to implement deep\nneural networks (DNNs) in a more energy-efficient manner since their neurons\nare sparsely activated and inferences are event-driven. However, there have\nbeen very few works that have demonstrated the efficacy of SNNs in language\ntasks partially because it is non-trivial to represent words in the forms of\nspikes and to deal with variable-length texts by SNNs. This work presents a\n\"conversion + fine-tuning\" two-step method for training SNNs for text\nclassification and proposes a simple but effective way to encode pre-trained\nword embeddings as spike trains. We show empirically that after fine-tuning\nwith surrogate gradients, the converted SNNs achieve comparable results to\ntheir DNN counterparts with much less energy consumption across multiple\ndatasets for both English and Chinese. We also show that such SNNs are more\nrobust to adversarial attacks than DNNs.", "published": "2024-06-27 14:54:27", "link": "http://arxiv.org/abs/2406.19230v1", "categories": ["cs.NE", "cs.CL"], "primary_category": "cs.NE"}
{"title": "AutoRAG-HP: Automatic Online Hyper-Parameter Tuning for\n  Retrieval-Augmented Generation", "abstract": "Recent advancements in Large Language Models have transformed ML/AI\ndevelopment, necessitating a reevaluation of AutoML principles for the\nRetrieval-Augmented Generation (RAG) systems. To address the challenges of\nhyper-parameter optimization and online adaptation in RAG, we propose the\nAutoRAG-HP framework, which formulates the hyper-parameter tuning as an online\nmulti-armed bandit (MAB) problem and introduces a novel two-level Hierarchical\nMAB (Hier-MAB) method for efficient exploration of large search spaces. We\nconduct extensive experiments on tuning hyper-parameters, such as top-k\nretrieved documents, prompt compression ratio, and embedding methods, using the\nALCE-ASQA and Natural Questions datasets. Our evaluation from jointly\noptimization all three hyper-parameters demonstrate that MAB-based online\nlearning methods can achieve Recall@5 $\\approx 0.8$ for scenarios with\nprominent gradients in search space, using only $\\sim20\\%$ of the LLM API calls\nrequired by the Grid Search approach. Additionally, the proposed Hier-MAB\napproach outperforms other baselines in more challenging optimization\nscenarios. The code will be made available at https://aka.ms/autorag.", "published": "2024-06-27 15:18:21", "link": "http://arxiv.org/abs/2406.19251v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing Video-Language Representations with Structural Spatio-Temporal\n  Alignment", "abstract": "While pre-training large-scale video-language models (VLMs) has shown\nremarkable potential for various downstream video-language tasks, existing VLMs\ncan still suffer from certain commonly seen limitations, e.g., coarse-grained\ncross-modal aligning , under-modeling of temporal dynamics, detached\nvideo-language view. In this work, we target enhancing VLMs with a fine-grained\nstructural spatio-temporal alignment learning method (namely Finsta). First of\nall, we represent the input texts and videos with fine-grained scene graph (SG)\nstructures, both of which are further unified into a holistic SG (HSG) for\nbridging two modalities. Then, an SG-based framework is built, where the\ntextual SG (TSG) is encoded with a graph Transformer, while the video dynamic\nSG (DSG) and the HSG are modeled with a novel recurrent graph Transformer for\nspatial and temporal feature propagation. A spatial-temporal Gaussian\ndifferential graph Transformer is further devised to strengthen the sense of\nthe changes in objects across spatial and temporal dimensions. Next, based on\nthe fine-grained structural features of TSG and DSG, we perform object-centered\nspatial alignment and predicate-centered temporal alignment respectively,\nenhancing the video-language grounding in both the spatiality and temporality.\nWe design our method as a plug&play system, which can be integrated into\nexisting well-trained VLMs for further representation augmentation, without\ntraining from scratch or relying on SG annotations in downstream applications.\nOn 6 representative VL modeling tasks over 12 datasets in both standard and\nlong-form video scenarios, Finsta consistently improves the existing 13\nstrong-performing VLMs persistently, and refreshes the current state-of-the-art\nend task performance significantly in both the fine-tuning and zero-shot\nsettings.", "published": "2024-06-27 15:23:36", "link": "http://arxiv.org/abs/2406.19255v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Read Anywhere Pointed: Layout-aware GUI Screen Reading with Tree-of-Lens\n  Grounding", "abstract": "Graphical User Interfaces (GUIs) are central to our interaction with digital\ndevices and growing efforts have been made to build models for various GUI\nunderstanding tasks. However, these efforts largely overlook an important\nGUI-referring task: screen reading based on user-indicated points, which we\nname the Screen Point-and-Read (ScreenPR) task. Currently, this task is\npredominantly handled by rigid accessible screen reading tools, in great need\nof new models driven by advancements in Multimodal Large Language Models\n(MLLMs). In this paper, we propose a Tree-of-Lens (ToL) agent, utilizing a\nnovel ToL grounding mechanism, to address the ScreenPR task. Based on the input\npoint coordinate and the corresponding GUI screenshot, our ToL agent constructs\na Hierarchical Layout Tree. Based on the tree, our ToL agent not only\ncomprehends the content of the indicated area but also articulates the layout\nand spatial relationships between elements. Such layout information is crucial\nfor accurately interpreting information on the screen, distinguishing our ToL\nagent from other screen reading tools. We also thoroughly evaluate the ToL\nagent against other baselines on a newly proposed ScreenPR benchmark, which\nincludes GUIs from mobile, web, and operating systems. Last but not least, we\ntest the ToL agent on mobile GUI navigation tasks, demonstrating its utility in\nidentifying incorrect actions along the path of agent execution trajectories.\nCode and data: https://screen-point-and-read.github.io", "published": "2024-06-27 15:34:16", "link": "http://arxiv.org/abs/2406.19263v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "IndoToxic2024: A Demographically-Enriched Dataset of Hate Speech and\n  Toxicity Types for Indonesian Language", "abstract": "Hate speech poses a significant threat to social harmony. Over the past two\nyears, Indonesia has seen a ten-fold increase in the online hate speech ratio,\nunderscoring the urgent need for effective detection mechanisms. However,\nprogress is hindered by the limited availability of labeled data for Indonesian\ntexts. The condition is even worse for marginalized minorities, such as Shia,\nLGBTQ, and other ethnic minorities because hate speech is underreported and\nless understood by detection tools. Furthermore, the lack of accommodation for\nsubjectivity in current datasets compounds this issue. To address this, we\nintroduce IndoToxic2024, a comprehensive Indonesian hate speech and toxicity\nclassification dataset. Comprising 43,692 entries annotated by 19 diverse\nindividuals, the dataset focuses on texts targeting vulnerable groups in\nIndonesia, specifically during the hottest political event in the country: the\npresidential election. We establish baselines for seven binary classification\ntasks, achieving a macro-F1 score of 0.78 with a BERT model (IndoBERTweet)\nfine-tuned for hate speech classification. Furthermore, we demonstrate how\nincorporating demographic information can enhance the zero-shot performance of\nthe large language model, gpt-3.5-turbo. However, we also caution that an\noveremphasis on demographic information can negatively impact the fine-tuned\nmodel performance due to data fragmentation.", "published": "2024-06-27 17:26:38", "link": "http://arxiv.org/abs/2406.19349v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Fundamental Problems With Model Editing: How Should Rational Belief\n  Revision Work in LLMs?", "abstract": "The model editing problem concerns how language models should learn new facts\nabout the world over time. While empirical research on model editing has drawn\nwidespread attention, the conceptual foundations of model editing remain shaky\n-- perhaps unsurprisingly, since model editing is essentially belief revision,\na storied problem in philosophy that has eluded succinct solutions for decades.\nModel editing nonetheless demands a solution, since we need to be able to\ncontrol the knowledge within language models. With this goal in mind, this\npaper critiques the standard formulation of the model editing problem and\nproposes a formal testbed for model editing research. We first describe 12 open\nproblems with model editing, based on challenges with (1) defining the problem,\n(2) developing benchmarks, and (3) assuming LLMs have editable beliefs in the\nfirst place. Many of these challenges are extremely difficult to address, e.g.\ndetermining far-reaching consequences of edits, labeling probabilistic\nentailments between facts, and updating beliefs of agent simulators. Next, we\nintroduce a semi-synthetic dataset for model editing based on Wikidata, where\nwe can evaluate edits against labels given by an idealized Bayesian agent. This\nenables us to say exactly how belief revision in language models falls short of\na desirable epistemic standard. We encourage further research exploring\nsettings where such a gold standard can be compared against. Our code is\npublicly available at: https://github.com/peterbhase/LLM-belief-revision", "published": "2024-06-27 17:33:03", "link": "http://arxiv.org/abs/2406.19354v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Sparse Regression for Machine Translation", "abstract": "We use transductive regression techniques to learn mappings between source\nand target features of given parallel corpora and use these mappings to\ngenerate machine translation outputs. We show the effectiveness of $L_1$\nregularized regression (\\textit{lasso}) to learn the mappings between sparsely\nobserved feature sets versus $L_2$ regularized regression. Proper selection of\ntraining instances plays an important role to learn correct feature mappings\nwithin limited computational resources and at expected accuracy levels. We\nintroduce \\textit{dice} instance selection method for proper selection of\ntraining instances, which plays an important role to learn correct feature\nmappings for improving the source and target coverage of the training set. We\nshow that $L_1$ regularized regression performs better than $L_2$ regularized\nregression both in regression measurements and in the translation experiments\nusing graph decoding. We present encouraging results when translating from\nGerman to English and Spanish to English. We also demonstrate results when the\nphrase table of a phrase-based decoder is replaced with the mappings we find\nwith the regression model.", "published": "2024-06-27 18:43:51", "link": "http://arxiv.org/abs/2406.19478v1", "categories": ["cs.CL", "cs.AI", "G.3; I.2.7"], "primary_category": "cs.CL"}
{"title": "Development and Evaluation of a Retrieval-Augmented Generation Tool for\n  Creating SAPPhIRE Models of Artificial Systems", "abstract": "Representing systems using the SAPPhIRE causality model is found useful in\nsupporting design-by-analogy. However, creating a SAPPhIRE model of artificial\nor biological systems is an effort-intensive process that requires human\nexperts to source technical knowledge from multiple technical documents\nregarding how the system works. This research investigates how to leverage\nLarge Language Models (LLMs) in creating structured descriptions of systems\nusing the SAPPhIRE model of causality. This paper, the second part of the\ntwo-part research, presents a new Retrieval-Augmented Generation (RAG) tool for\ngenerating information related to SAPPhIRE constructs of artificial systems and\nreports the results from a preliminary evaluation of the tool's success -\nfocusing on the factual accuracy and reliability of outcomes.", "published": "2024-06-27 19:20:09", "link": "http://arxiv.org/abs/2406.19493v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Inclusivity in Large Language Models: Personality Traits and Gender Bias\n  in Scientific Abstracts", "abstract": "Large language models (LLMs) are increasingly utilized to assist in\nscientific and academic writing, helping authors enhance the coherence of their\narticles. Previous studies have highlighted stereotypes and biases present in\nLLM outputs, emphasizing the need to evaluate these models for their alignment\nwith human narrative styles and potential gender biases. In this study, we\nassess the alignment of three prominent LLMs - Claude 3 Opus, Mistral AI Large,\nand Gemini 1.5 Flash - by analyzing their performance on benchmark\ntext-generation tasks for scientific abstracts. We employ the Linguistic\nInquiry and Word Count (LIWC) framework to extract lexical, psychological, and\nsocial features from the generated texts. Our findings indicate that, while\nthese models generally produce text closely resembling human authored content,\nvariations in stylistic features suggest significant gender biases. This\nresearch highlights the importance of developing LLMs that maintain a diversity\nof writing styles to promote inclusivity in academic discourse.", "published": "2024-06-27 19:26:11", "link": "http://arxiv.org/abs/2406.19497v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Knowledge acquisition for dialogue agents using reinforcement learning\n  on graph representations", "abstract": "We develop an artificial agent motivated to augment its knowledge base beyond\nits initial training. The agent actively participates in dialogues with other\nagents, strategically acquiring new information. The agent models its knowledge\nas an RDF knowledge graph, integrating new beliefs acquired through\nconversation. Responses in dialogue are generated by identifying graph patterns\naround these new integrated beliefs. We show that policies can be learned using\nreinforcement learning to select effective graph patterns during an\ninteraction, without relying on explicit user feedback. Within this context,\nour study is a proof of concept for leveraging users as effective sources of\ninformation.", "published": "2024-06-27 19:28:42", "link": "http://arxiv.org/abs/2406.19500v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Monitoring Latent World States in Language Models with Propositional\n  Probes", "abstract": "Language models are susceptible to bias, sycophancy, backdoors, and other\ntendencies that lead to unfaithful responses to the input context. Interpreting\ninternal states of language models could help monitor and correct unfaithful\nbehavior. We hypothesize that language models represent their input contexts in\na latent world model, and seek to extract this latent world state from the\nactivations. We do so with 'propositional probes', which compositionally probe\ntokens for lexical information and bind them into logical propositions\nrepresenting the world state. For example, given the input context ''Greg is a\nnurse. Laura is a physicist.'', we decode the propositions ''WorksAs(Greg,\nnurse)'' and ''WorksAs(Laura, physicist)'' from the model's activations. Key to\nthis is identifying a 'binding subspace' in which bound tokens have high\nsimilarity (''Greg'' and ''nurse'') but unbound ones do not (''Greg'' and\n''physicist''). We validate propositional probes in a closed-world setting with\nfinitely many predicates and properties. Despite being trained on simple\ntemplated contexts, propositional probes generalize to contexts rewritten as\nshort stories and translated to Spanish. Moreover, we find that in three\nsettings where language models respond unfaithfully to the input context --\nprompt injections, backdoor attacks, and gender bias -- the decoded\npropositions remain faithful. This suggests that language models often encode a\nfaithful world model but decode it unfaithfully, which motivates the search for\nbetter interpretability tools for monitoring LMs.", "published": "2024-06-27 19:28:43", "link": "http://arxiv.org/abs/2406.19501v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Hierarchical Deconstruction of LLM Reasoning: A Graph-Based Framework\n  for Analyzing Knowledge Utilization", "abstract": "Despite the advances in large language models (LLMs), how they use their\nknowledge for reasoning is not yet well understood. In this study, we propose a\nmethod that deconstructs complex real-world questions into a graph,\nrepresenting each question as a node with predecessors of background knowledge\nneeded to solve the question. We develop the DepthQA dataset, deconstructing\nquestions into three depths: (i) recalling conceptual knowledge, (ii) applying\nprocedural knowledge, and (iii) analyzing strategic knowledge. Based on a\nhierarchical graph, we quantify forward discrepancy, a discrepancy in LLM\nperformance on simpler sub-problems versus complex questions. We also measure\nbackward discrepancy where LLMs answer complex questions but struggle with\nsimpler ones. Our analysis shows that smaller models exhibit more discrepancies\nthan larger models. Distinct patterns of discrepancies are observed across\nmodel capacity and possibility of training data memorization. Additionally,\nguiding models from simpler to complex questions through multi-turn\ninteractions improves performance across model sizes, highlighting the\nimportance of structured intermediate steps in knowledge reasoning. This work\nenhances our understanding of LLM reasoning and suggests ways to improve their\nproblem-solving abilities.", "published": "2024-06-27 19:29:36", "link": "http://arxiv.org/abs/2406.19502v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Handling Ontology Gaps in Semantic Parsing", "abstract": "The majority of Neural Semantic Parsing (NSP) models are developed with the\nassumption that there are no concepts outside the ones such models can\nrepresent with their target symbols (closed-world assumption). This assumption\nleads to generate hallucinated outputs rather than admitting their lack of\nknowledge. Hallucinations can lead to wrong or potentially offensive responses\nto users. Hence, a mechanism to prevent this behavior is crucial to build\ntrusted NSP-based Question Answering agents. To that end, we propose the\nHallucination Simulation Framework (HSF), a general setting for stimulating and\nanalyzing NSP model hallucinations. The framework can be applied to any NSP\ntask with a closed-ontology. Using the proposed framework and KQA Pro as the\nbenchmark dataset, we assess state-of-the-art techniques for hallucination\ndetection. We then present a novel hallucination detection strategy that\nexploits the computational graph of the NSP model to detect the NSP\nhallucinations in the presence of ontology gaps, out-of-domain utterances, and\nto recognize NSP errors, improving the F1-Score respectively by ~21, ~24% and\n~1%. This is the first work in closed-ontology NSP that addresses the problem\nof recognizing ontology gaps. We release our code and checkpoints at\nhttps://github.com/amazon-science/handling-ontology-gaps-in-semantic-parsing.", "published": "2024-06-27 21:21:22", "link": "http://arxiv.org/abs/2406.19537v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Demarked: A Strategy for Enhanced Abusive Speech Moderation through\n  Counterspeech, Detoxification, and Message Management", "abstract": "Despite regulations imposed by nations and social media platforms, such as\nrecent EU regulations targeting digital violence, abusive content persists as a\nsignificant challenge. Existing approaches primarily rely on binary solutions,\nsuch as outright blocking or banning, yet fail to address the complex nature of\nabusive speech. In this work, we propose a more comprehensive approach called\nDemarcation scoring abusive speech based on four aspect -- (i) severity scale;\n(ii) presence of a target; (iii) context scale; (iv) legal scale -- and\nsuggesting more options of actions like detoxification, counter speech\ngeneration, blocking, or, as a final measure, human intervention. Through a\nthorough analysis of abusive speech regulations across diverse jurisdictions,\nplatforms, and research papers we highlight the gap in preventing measures and\nadvocate for tailored proactive steps to combat its multifaceted\nmanifestations. Our work aims to inform future strategies for effectively\naddressing abusive speech online.", "published": "2024-06-27 21:45:33", "link": "http://arxiv.org/abs/2406.19543v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Leveraging Machine-Generated Rationales to Facilitate Social Meaning\n  Detection in Conversations", "abstract": "We present a generalizable classification approach that leverages Large\nLanguage Models (LLMs) to facilitate the detection of implicitly encoded social\nmeaning in conversations. We design a multi-faceted prompt to extract a textual\nexplanation of the reasoning that connects visible cues to underlying social\nmeanings. These extracted explanations or rationales serve as augmentations to\nthe conversational text to facilitate dialogue understanding and transfer. Our\nempirical results over 2,340 experimental settings demonstrate the significant\npositive impact of adding these rationales. Our findings hold true for\nin-domain classification, zero-shot, and few-shot domain transfer for two\ndifferent social meaning detection tasks, each spanning two different corpora.", "published": "2024-06-27 21:47:42", "link": "http://arxiv.org/abs/2406.19545v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Does ChatGPT Have a Mind?", "abstract": "This paper examines the question of whether Large Language Models (LLMs) like\nChatGPT possess minds, focusing specifically on whether they have a genuine\nfolk psychology encompassing beliefs, desires, and intentions. We approach this\nquestion by investigating two key aspects: internal representations and\ndispositions to act. First, we survey various philosophical theories of\nrepresentation, including informational, causal, structural, and teleosemantic\naccounts, arguing that LLMs satisfy key conditions proposed by each. We draw on\nrecent interpretability research in machine learning to support these claims.\nSecond, we explore whether LLMs exhibit robust dispositions to perform actions,\na necessary component of folk psychology. We consider two prominent\nphilosophical traditions, interpretationism and representationalism, to assess\nLLM action dispositions. While we find evidence suggesting LLMs may satisfy\nsome criteria for having a mind, particularly in game-theoretic environments,\nwe conclude that the data remains inconclusive. Additionally, we reply to\nseveral skeptical challenges to LLM folk psychology, including issues of\nsensory grounding, the \"stochastic parrots\" argument, and concerns about\nmemorization. Our paper has three main upshots. First, LLMs do have robust\ninternal representations. Second, there is an open question to answer about\nwhether LLMs have robust action dispositions. Third, existing skeptical\nchallenges to LLM representation do not survive philosophical scrutiny.", "published": "2024-06-27 00:21:16", "link": "http://arxiv.org/abs/2407.11015v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LongLaMP: A Benchmark for Personalized Long-form Text Generation", "abstract": "Long-text generation is seemingly ubiquitous in real-world applications of\nlarge language models such as generating an email or writing a review. Despite\nthe fundamental importance and prevalence of long-text generation in many\npractical applications, existing work on personalized generation has focused on\nthe generation of very short text. To overcome these limitations, we study the\nproblem of personalized long-text generation, that is, generating long-text\nthat is personalized for a specific user while being practically useful for the\nvast majority of real-world applications that naturally require the generation\nof longer text. In this work, we demonstrate the importance of user-specific\npersonalization for long-text generation tasks and develop the Long-text\nLanguage Model Personalization (LongLaMP) Benchmark. LongLaMP provides a\ncomprehensive and diverse evaluation framework for personalized long-text\ngeneration. Extensive experiments on LongLaMP for zero-shot and fine-tuned\nlanguage tasks demonstrate the effectiveness of the proposed benchmark and its\nutility for developing and evaluating techniques for personalized long-text\ngeneration across a wide variety of long-text generation tasks. The results\nhighlight the importance of personalization across a wide variety of long-text\ngeneration tasks. Finally, we release the benchmark for others to use for this\nimportant problem.", "published": "2024-06-27 01:52:05", "link": "http://arxiv.org/abs/2407.11016v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LLM-based Frameworks for API Argument Filling in Task-Oriented\n  Conversational Systems", "abstract": "Task-orientated conversational agents interact with users and assist them via\nleveraging external APIs. A typical task-oriented conversational system can be\nbroken down into three phases: external API selection, argument filling, and\nresponse generation. The focus of our work is the task of argument filling,\nwhich is in charge of accurately providing arguments required by the selected\nAPI. Upon comprehending the dialogue history and the pre-defined API schema,\nthe argument filling task is expected to provide the external API with the\nnecessary information to generate a desirable agent action. In this paper, we\nstudy the application of Large Language Models (LLMs) for the problem of API\nargument filling task. Our initial investigation reveals that LLMs require an\nadditional grounding process to successfully perform argument filling,\ninspiring us to design training and prompting frameworks to ground their\nresponses. Our experimental results demonstrate that when paired with proposed\ntechniques, the argument filling performance of LLMs noticeably improves,\npaving a new way toward building an automated argument filling framework.", "published": "2024-06-27 06:54:53", "link": "http://arxiv.org/abs/2407.12016v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Follow-Up Questions Improve Documents Generated by Large Language Models", "abstract": "This study investigates the impact of Large Language Models (LLMs) generating\nfollow-up questions in response to user requests for short (1-page) text\ndocuments. Users interacted with a novel web-based AI system designed to ask\nfollow-up questions. Users requested documents they would like the AI to\nproduce. The AI then generated follow-up questions to clarify the user's needs\nor offer additional insights before generating the requested documents. After\nanswering the questions, users were shown a document generated using both the\ninitial request and the questions and answers, and a document generated using\nonly the initial request. Users indicated which document they preferred and\ngave feedback about their experience with the question-answering process. The\nfindings of this study show clear benefits to question-asking both in document\npreference and in the qualitative user experience. This study further shows\nthat users found more value in questions which were thought-provoking,\nopen-ended, or offered unique insights into the user's request as opposed to\nsimple information-gathering questions.", "published": "2024-06-27 07:16:46", "link": "http://arxiv.org/abs/2407.12017v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Empirical Evaluation of Public HateSpeech Datasets", "abstract": "Despite the extensive communication benefits offered by social media\nplatforms, numerous challenges must be addressed to ensure user safety. One of\nthe most significant risks faced by users on these platforms is targeted hate\nspeech. Social media platforms are widely utilised for generating datasets\nemployed in training and evaluating machine learning algorithms for hate speech\ndetection. However, existing public datasets exhibit numerous limitations,\nhindering the effective training of these algorithms and leading to inaccurate\nhate speech classification. This study provides a comprehensive empirical\nevaluation of several public datasets commonly used in automated hate speech\nclassification. Through rigorous analysis, we present compelling evidence\nhighlighting the limitations of current hate speech datasets. Additionally, we\nconduct a range of statistical analyses to elucidate the strengths and\nweaknesses inherent in these datasets. This work aims to advance the\ndevelopment of more accurate and reliable machine learning models for hate\nspeech detection by addressing the dataset limitations identified.", "published": "2024-06-27 11:20:52", "link": "http://arxiv.org/abs/2407.12018v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DIM: Dynamic Integration of Multimodal Entity Linking with Large\n  Language Model", "abstract": "Our study delves into Multimodal Entity Linking, aligning the mention in\nmultimodal information with entities in knowledge base. Existing methods are\nstill facing challenges like ambiguous entity representations and limited image\ninformation utilization. Thus, we propose dynamic entity extraction using\nChatGPT, which dynamically extracts entities and enhances datasets. We also\npropose a method: Dynamically Integrate Multimodal information with knowledge\nbase (DIM), employing the capability of the Large Language Model (LLM) for\nvisual understanding. The LLM, such as BLIP-2, extracts information relevant to\nentities in the image, which can facilitate improved extraction of entity\nfeatures and linking them with the dynamic entity representations provided by\nChatGPT. The experiments demonstrate that our proposed DIM method outperforms\nthe majority of existing methods on the three original datasets, and achieves\nstate-of-the-art (SOTA) on the dynamically enhanced datasets (Wiki+, Rich+,\nDiverse+). For reproducibility, our code and collected datasets are released on\n\\url{https://github.com/season1blue/DIM}.", "published": "2024-06-27 15:18:23", "link": "http://arxiv.org/abs/2407.12019v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Adaptive Draft-Verification for Efficient Large Language Model Decoding", "abstract": "Large language model (LLM) decoding involves generating a sequence of tokens\nbased on a given context, where each token is predicted one at a time using the\nmodel's learned probabilities. The typical autoregressive decoding method\nrequires a separate forward pass through the model for each token generated,\nwhich is computationally inefficient and poses challenges for deploying LLMs in\nlatency-sensitive scenarios. The main limitations of current decoding methods\nstem from their inefficiencies and resource demands. Existing approaches either\nnecessitate fine-tuning smaller models, which is resource-intensive, or rely on\nfixed retrieval schemes to construct drafts for the next tokens, which lack\nadaptability and fail to generalize across different models and contexts. To\naddress these issues, we introduce a novel methodology called ADED, which\naccelerates LLM decoding without requiring fine-tuning. Our approach involves\nan adaptive draft-verification process that evolves over time to improve\nefficiency. We utilize a tri-gram matrix-based LLM representation to\ndynamically approximate the output distribution of the LLM, allowing the model\nto adjust to changing token probabilities during the decoding process.\nAdditionally, we implement a draft construction mechanism that effectively\nbalances exploration and exploitation, ensuring that the drafts generated are\nboth diverse and close to the true output distribution of the LLM. The\nimportance of this design lies in its ability to optimize the draft\ndistribution adaptively, leading to faster and more accurate decoding. Through\nextensive experiments on various benchmark datasets and LLM architectures, we\ndemonstrate that ADED significantly accelerates the decoding process while\nmaintaining high accuracy, making it suitable for deployment in a wide range of\npractical applications.", "published": "2024-06-27 22:20:39", "link": "http://arxiv.org/abs/2407.12021v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Building Understandable Messaging for Policy and Evidence Review\n  (BUMPER) with AI", "abstract": "We introduce a framework for the use of large language models (LLMs) in\nBuilding Understandable Messaging for Policy and Evidence Review (BUMPER). LLMs\nare proving capable of providing interfaces for understanding and synthesizing\nlarge databases of diverse media. This presents an exciting opportunity to\nsupercharge the translation of scientific evidence into policy and action,\nthereby improving livelihoods around the world. However, these models also pose\nchallenges related to access, trust-worthiness, and accountability. The BUMPER\nframework is built atop a scientific knowledge base (e.g., documentation, code,\nsurvey data) by the same scientists (e.g., individual contributor, lab,\nconsortium). We focus on a solution that builds trustworthiness through\ntransparency, scope-limiting, explicit-checks, and uncertainty measures. LLMs\nare rapidly being adopted and consequences are poorly understood. The framework\naddresses open questions regarding the reliability of LLMs and their use in\nhigh-stakes applications. We provide a worked example in health policy for a\nmodel designed to inform measles control programs. We argue that this framework\ncan facilitate accessibility of and confidence in scientific evidence for\npolicymakers, drive a focus on policy-relevance and translatability for\nresearchers, and ultimately increase and accelerate the impact of scientific\nknowledge used for policy decisions.", "published": "2024-06-27 05:03:03", "link": "http://arxiv.org/abs/2407.12812v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Data Generation Using Large Language Models for Text Classification: An\n  Empirical Case Study", "abstract": "Using Large Language Models (LLMs) to generate synthetic data for model\ntraining has become increasingly popular in recent years. While LLMs are\ncapable of producing realistic training data, the effectiveness of data\ngeneration is influenced by various factors, including the choice of prompt,\ntask complexity, and the quality, quantity, and diversity of the generated\ndata. In this work, we focus exclusively on using synthetic data for text\nclassification tasks. Specifically, we use natural language understanding (NLU)\nmodels trained on synthetic data to assess the quality of synthetic data from\ndifferent generation approaches. This work provides an empirical analysis of\nthe impact of these factors and offers recommendations for better data\ngeneration practices.", "published": "2024-06-27 21:41:43", "link": "http://arxiv.org/abs/2407.12813v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning Retrieval Augmentation for Personalized Dialogue Generation", "abstract": "Personalized dialogue generation, focusing on generating highly tailored\nresponses by leveraging persona profiles and dialogue context, has gained\nsignificant attention in conversational AI applications. However, persona\nprofiles, a prevalent setting in current personalized dialogue datasets,\ntypically composed of merely four to five sentences, may not offer\ncomprehensive descriptions of the persona about the agent, posing a challenge\nto generate truly personalized dialogues. To handle this problem, we propose\n$\\textbf{L}$earning Retrieval $\\textbf{A}$ugmentation for\n$\\textbf{P}$ersonalized $\\textbf{D}$ial$\\textbf{O}$gue $\\textbf{G}$eneration\n($\\textbf{LAPDOG}$), which studies the potential of leveraging external\nknowledge for persona dialogue generation. Specifically, the proposed LAPDOG\nmodel consists of a story retriever and a dialogue generator. The story\nretriever uses a given persona profile as queries to retrieve relevant\ninformation from the story document, which serves as a supplementary context to\naugment the persona profile. The dialogue generator utilizes both the dialogue\nhistory and the augmented persona profile to generate personalized responses.\nFor optimization, we adopt a joint training framework that collaboratively\nlearns the story retriever and dialogue generator, where the story retriever is\noptimized towards desired ultimate metrics (e.g., BLEU) to retrieve content for\nthe dialogue generator to generate personalized responses. Experiments\nconducted on the CONVAI2 dataset with ROCStory as a supplementary data source\nshow that the proposed LAPDOG method substantially outperforms the baselines,\nindicating the effectiveness of the proposed method. The LAPDOG model code is\npublicly available for further exploration.\nhttps://github.com/hqsiswiliam/LAPDOG", "published": "2024-06-27 02:38:13", "link": "http://arxiv.org/abs/2406.18847v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FFN: a Fine-grained Chinese-English Financial Domain Parallel Corpus", "abstract": "Large Language Models (LLMs) have stunningly advanced the field of machine\ntranslation, though their effectiveness within the financial domain remains\nlargely underexplored. To probe this issue, we constructed a fine-grained\nChinese-English parallel corpus of financial news called FFN. We acquired\nfinancial news articles spanning between January 1st, 2014, to December 31,\n2023, from mainstream media websites such as CNN, FOX, and China Daily. The\ndataset consists of 1,013 main text and 809 titles, all of which have been\nmanually corrected. We measured the translation quality of two LLMs -- ChatGPT\nand ERNIE-bot, utilizing BLEU, TER and chrF scores as the evaluation metrics.\nFor comparison, we also trained an OpenNMT model based on our dataset. We\ndetail problems of LLMs and provide in-depth analysis, intending to stimulate\nfurther research and solutions in this largely uncharted territory. Our\nresearch underlines the need to optimize LLMs within the specific field of\nfinancial translation to ensure accuracy and quality.", "published": "2024-06-27 02:53:55", "link": "http://arxiv.org/abs/2406.18856v1", "categories": ["cs.CL", "cs.AI", "cs.CE"], "primary_category": "cs.CL"}
{"title": "Factor-Conditioned Speaking-Style Captioning", "abstract": "This paper presents a novel speaking-style captioning method that generates\ndiverse descriptions while accurately predicting speaking-style information.\nConventional learning criteria directly use original captions that contain not\nonly speaking-style factor terms but also syntax words, which disturbs learning\nspeaking-style information. To solve this problem, we introduce\nfactor-conditioned captioning (FCC), which first outputs a phrase representing\nspeaking-style factors (e.g., gender, pitch, etc.), and then generates a\ncaption to ensure the model explicitly learns speaking-style factors. We also\npropose greedy-then-sampling (GtS) decoding, which first predicts\nspeaking-style factors deterministically to guarantee semantic accuracy, and\nthen generates a caption based on factor-conditioned sampling to ensure\ndiversity. Experiments show that FCC outperforms the original caption-based\ntraining, and with GtS, it generates more diverse captions while keeping style\nprediction performance.", "published": "2024-06-27 05:52:10", "link": "http://arxiv.org/abs/2406.18910v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Enhanced ASR Robustness to Packet Loss with a Front-End Adaptation\n  Network", "abstract": "In the realm of automatic speech recognition (ASR), robustness in noisy\nenvironments remains a significant challenge. Recent ASR models, such as\nWhisper, have shown promise, but their efficacy in noisy conditions can be\nfurther enhanced. This study is focused on recovering from packet loss to\nimprove the word error rate (WER) of ASR models. We propose using a front-end\nadaptation network connected to a frozen ASR model. The adaptation network is\ntrained to modify the corrupted input spectrum by minimizing the criteria of\nthe ASR model in addition to an enhancement loss function. Our experiments\ndemonstrate that the adaptation network, trained on Whisper's criteria, notably\nreduces word error rates across domains and languages in packet-loss scenarios.\nThis improvement is achieved with minimal affect to Whisper model's\nfoundational performance, underscoring our method's practicality and potential\nin enhancing ASR models in challenging acoustic environments.", "published": "2024-06-27 06:40:01", "link": "http://arxiv.org/abs/2406.18928v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "RoboUniView: Visual-Language Model with Unified View Representation for\n  Robotic Manipulation", "abstract": "Utilizing Vision-Language Models (VLMs) for robotic manipulation represents a\nnovel paradigm, aiming to enhance the model's ability to generalize to new\nobjects and instructions. However, due to variations in camera specifications\nand mounting positions, existing methods exhibit significant performance\ndisparities across different robotic platforms. To address this challenge, we\npropose RoboUniView in this paper, an innovative approach that decouples visual\nfeature extraction from action learning. We first learn a unified view\nrepresentation from multi-perspective views by pre-training on readily\naccessible data, and then derive actions from this unified view representation\nto control robotic manipulation. This unified view representation more\naccurately mirrors the physical world and is not constrained by the robotic\nplatform's camera parameters. Thanks to this methodology, we achieve\nstate-of-the-art performance on the demanding CALVIN benchmark, enhancing the\nsuccess rate in the $D \\to D$ setting from 93.0% to 96.2%, and in the $ABC \\to\nD$ setting from 92.2% to 94.2%. Moreover, our model exhibits outstanding\nadaptability and flexibility: it maintains high performance under unseen camera\nparameters, can utilize multiple datasets with varying camera parameters, and\nis capable of joint cross-task learning across datasets. Code is provided for\nre-implementation. https://github.com/liufanfanlff/RoboUniview", "published": "2024-06-27 08:13:33", "link": "http://arxiv.org/abs/2406.18977v3", "categories": ["cs.RO", "cs.CL", "cs.CV"], "primary_category": "cs.RO"}
{"title": "Statements: Universal Information Extraction from Tables with Large\n  Language Models for ESG KPIs", "abstract": "Environment, Social, and Governance (ESG) KPIs assess an organization's\nperformance on issues such as climate change, greenhouse gas emissions, water\nconsumption, waste management, human rights, diversity, and policies. ESG\nreports convey this valuable quantitative information through tables.\nUnfortunately, extracting this information is difficult due to high variability\nin the table structure as well as content. We propose Statements, a novel\ndomain agnostic data structure for extracting quantitative facts and related\ninformation. We propose translating tables to statements as a new supervised\ndeep-learning universal information extraction task. We introduce SemTabNet - a\ndataset of over 100K annotated tables. Investigating a family of T5-based\nStatement Extraction Models, our best model generates statements which are 82%\nsimilar to the ground-truth (compared to baseline of 21%). We demonstrate the\nadvantages of statements by applying our model to over 2700 tables from ESG\nreports. The homogeneous nature of statements permits exploratory data analysis\non expansive information found in large collections of ESG reports.", "published": "2024-06-27 11:28:50", "link": "http://arxiv.org/abs/2406.19102v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "CHEW: A Dataset of CHanging Events in Wikipedia", "abstract": "We introduce CHEW, a novel dataset of changing events in Wikipedia expressed\nin naturally occurring text. We use CHEW for probing LLMs for their timeline\nunderstanding of Wikipedia entities and events in generative and classification\nexperiments. Our results suggest that LLMs, despite having temporal information\navailable, struggle to construct accurate timelines. We further show the\nusefulness of CHEW-derived embeddings for identifying meaning shift.", "published": "2024-06-27 11:53:15", "link": "http://arxiv.org/abs/2406.19116v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "T-FREE: Subword Tokenizer-Free Generative LLMs via Sparse\n  Representations for Memory-Efficient Embeddings", "abstract": "Tokenizers are crucial for encoding information in Large Language Models, but\ntheir development has recently stagnated, and they contain inherent weaknesses.\nMajor limitations include computational overhead, ineffective vocabulary use,\nand unnecessarily large embedding and head layers. Additionally, their\nperformance is biased towards a reference corpus, leading to reduced\neffectiveness for underrepresented languages.\n  To remedy these issues, we propose T-FREE, which directly embeds words\nthrough sparse activation patterns over character triplets, and does not\nrequire a reference corpus. T-FREE inherently exploits morphological\nsimilarities and allows for strong compression of embedding layers. In our\nexhaustive experimental evaluation, we achieve competitive downstream\nperformance with a parameter reduction of more than 85% on these layers.\nFurther, T-FREE shows significant improvements in cross-lingual transfer\nlearning.", "published": "2024-06-27 14:49:08", "link": "http://arxiv.org/abs/2406.19223v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Tools Fail: Detecting Silent Errors in Faulty Tools", "abstract": "Tools have become a mainstay of LLMs, allowing them to retrieve knowledge not\nin their weights, to perform tasks on the web, and even to control robots.\nHowever, most ontologies and surveys of tool-use have assumed the core\nchallenge for LLMs is choosing the tool. Instead, we introduce a framework for\ntools more broadly which guides us to explore a model's ability to detect\n\"silent\" tool errors, and reflect on how to plan. This more directly aligns\nwith the increasingly popular use of models as tools. We provide an initial\napproach to failure recovery with promising results both on a controlled\ncalculator setting and embodied agent planning.", "published": "2024-06-27 14:52:34", "link": "http://arxiv.org/abs/2406.19228v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FlowVQA: Mapping Multimodal Logic in Visual Question Answering with\n  Flowcharts", "abstract": "Existing benchmarks for visual question answering lack in visual grounding\nand complexity, particularly in evaluating spatial reasoning skills. We\nintroduce FlowVQA, a novel benchmark aimed at assessing the capabilities of\nvisual question-answering multimodal language models in reasoning with\nflowcharts as visual contexts. FlowVQA comprises 2,272 carefully generated and\nhuman-verified flowchart images from three distinct content sources, along with\n22,413 diverse question-answer pairs, to test a spectrum of reasoning tasks,\nincluding information localization, decision-making, and logical progression.\nWe conduct a thorough baseline evaluation on a suite of both open-source and\nproprietary multimodal language models using various strategies, followed by an\nanalysis of directional bias. The results underscore the benchmark's potential\nas a vital tool for advancing the field of multimodal modeling, providing a\nfocused and challenging environment for enhancing model performance in visual\nand logical reasoning tasks.", "published": "2024-06-27 15:01:48", "link": "http://arxiv.org/abs/2406.19237v2", "categories": ["cs.CL", "cs.CV", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Revealing Fine-Grained Values and Opinions in Large Language Models", "abstract": "Uncovering latent values and opinions embedded in large language models\n(LLMs) can help identify biases and mitigate potential harm. Recently, this has\nbeen approached by prompting LLMs with survey questions and quantifying the\nstances in the outputs towards morally and politically charged statements.\nHowever, the stances generated by LLMs can vary greatly depending on how they\nare prompted, and there are many ways to argue for or against a given position.\nIn this work, we propose to address this by analysing a large and robust\ndataset of 156k LLM responses to the 62 propositions of the Political Compass\nTest (PCT) generated by 6 LLMs using 420 prompt variations. We perform\ncoarse-grained analysis of their generated stances and fine-grained analysis of\nthe plain text justifications for those stances. For fine-grained analysis, we\npropose to identify tropes in the responses: semantically similar phrases that\nare recurrent and consistent across different prompts, revealing natural\npatterns in the text that a given LLM is prone to produce. We find that\ndemographic features added to prompts significantly affect outcomes on the PCT,\nreflecting bias, as well as disparities between the results of tests when\neliciting closed-form vs. open domain responses. Additionally, patterns in the\nplain text rationales via tropes show that similar justifications are\nrepeatedly generated across models and prompts even with disparate stances.", "published": "2024-06-27 15:01:53", "link": "http://arxiv.org/abs/2406.19238v2", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AutoPureData: Automated Filtering of Undesirable Web Data to Update LLM\n  Knowledge", "abstract": "Up-to-date and reliable language models are consistently sought after and are\nessential in various applications. Typically, models are trained on a fixed\ndataset and then deployed globally. However, the knowledge of the models\nbecomes outdated. Enabling automatic updation of AI knowledge using web data\ninvolves significant concerns regarding the model's safety and quality due to a\nthreat from unsafe and undesirable text across the web. The purity of new data\nwas essential for updating knowledge of language models to maintain their\nreliability. This paper proposes AutoPureData, a system that automatically\ncollects and purifies web data. The system loaded a sample of web data.\nUtilizing existing trusted AI models, it successfully eliminated unsafe text\nwith an accuracy of 97% and undesirable text with an accuracy of 86%,\ndemonstrating the system's effectiveness in purifying the data. The system\nensures that only meaningful and safe text can be used to update LLM knowledge.\nThe pure text was then optimized and stored in a vector database for future\nquerying. It was found that LLM can fetch new data from the vector DB. The LLM\nwrites the RAG query in English, even if the user's query is in another\nlanguage, proving that the system can perform cross-lingual retrieval. This\npaper proposes a method to maintain the accuracy and relevance of up-to-date\nlanguage models by ensuring that only purified data was used to update LLM\nknowledge. This work contributes to updating knowledge of chatbots using\nmeaningful and safe text, enhancing their utility across various industries,\nand potentially reducing the risks associated with outputs caused by unsafe or\nimpure data. Code is available at github.com/Pro-GenAI/AutoPureData.", "published": "2024-06-27 15:37:57", "link": "http://arxiv.org/abs/2406.19271v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into\n  Multimodal LLMs at Scale", "abstract": "The rapid development of multimodal large language models (MLLMs), such as\nGPT-4V, has led to significant advancements. However, these models still face\nchallenges in medical multimodal capabilities due to limitations in the\nquantity and quality of medical vision-text data, stemming from data privacy\nconcerns and high annotation costs. While pioneering approaches utilize\nPubMed's large-scale, de-identified medical image-text pairs to address these\nlimitations, they still fall short due to inherent data noise. To tackle this,\nwe refined medical image-text pairs from PubMed and employed MLLMs (GPT-4V) in\nan 'unblinded' capacity to denoise and reformat the data, resulting in the\ncreation of the PubMedVision dataset with 1.3 million medical VQA samples. Our\nvalidation demonstrates that: (1) PubMedVision can significantly enhance the\nmedical multimodal capabilities of current MLLMs, showing significant\nimprovement in benchmarks including the MMMU Health & Medicine track; (2)\nmanual checks by medical experts and empirical results validate the superior\ndata quality of our dataset compared to other data construction methods. Using\nPubMedVision, we train a 34B medical MLLM HuatuoGPT-Vision, which shows\nsuperior performance in medical multimodal scenarios among open-source MLLMs.", "published": "2024-06-27 15:50:41", "link": "http://arxiv.org/abs/2406.19280v4", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "From Artificial Needles to Real Haystacks: Improving Retrieval\n  Capabilities in LLMs by Finetuning on Synthetic Data", "abstract": "Recent studies have shown that Large Language Models (LLMs) struggle to\naccurately retrieve information and maintain reasoning capabilities when\nprocessing long-context inputs. To address these limitations, we propose a\nfinetuning approach utilizing a carefully designed synthetic dataset comprising\nnumerical key-value retrieval tasks. Our experiments on models like GPT-3.5\nTurbo and Mistral 7B demonstrate that finetuning LLMs on this dataset\nsignificantly improves LLMs' information retrieval and reasoning capabilities\nin longer-context settings. We present an analysis of the finetuned models,\nillustrating the transfer of skills from synthetic to real task evaluations\n(e.g., $10.5\\%$ improvement on $20$ documents MDQA at position $10$ for GPT-3.5\nTurbo). We also find that finetuned LLMs' performance on general benchmarks\nremains almost constant while LLMs finetuned on other baseline long-context\naugmentation data can encourage hallucination (e.g., on TriviaQA, Mistral 7B\nfinetuned on our synthetic data cause no performance drop while other baseline\ndata can cause a drop that ranges from $2.33\\%$ to $6.19\\%$). Our study\nhighlights the potential of finetuning on synthetic data for improving the\nperformance of LLMs on longer-context tasks.", "published": "2024-06-27 16:05:13", "link": "http://arxiv.org/abs/2406.19292v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "LiveBench: A Challenging, Contamination-Free LLM Benchmark", "abstract": "Test set contamination, wherein test data from a benchmark ends up in a newer\nmodel's training set, is a well-documented obstacle for fair LLM evaluation and\ncan quickly render benchmarks obsolete. To mitigate this, many recent\nbenchmarks crowdsource new prompts and evaluations from human or LLM judges;\nhowever, these can introduce significant biases, and break down when scoring\nhard questions. In this work, we introduce a new benchmark for LLMs designed to\nbe immune to both test set contamination and the pitfalls of LLM judging and\nhuman crowdsourcing. We release LiveBench, the first benchmark that (1)\ncontains frequently-updated questions from recent information sources, (2)\nscores answers automatically according to objective ground-truth values, and\n(3) contains a wide variety of challenging tasks, spanning math, coding,\nreasoning, language, instruction following, and data analysis. To achieve this,\nLiveBench contains questions that are based on recently-released math\ncompetitions, arXiv papers, news articles, and datasets, and it contains\nharder, contamination-free versions of tasks from previous benchmarks such as\nBig-Bench Hard, AMPS, and IFEval. We evaluate many prominent closed-source\nmodels, as well as dozens of open-source models ranging from 0.5B to 110B in\nsize. LiveBench is difficult, with top models achieving below 65% accuracy. We\nrelease all questions, code, and model answers. Questions will be added and\nupdated on a monthly basis, and we will release new tasks and harder versions\nof tasks over time so that LiveBench can distinguish between the capabilities\nof LLMs as they improve in the future. We welcome community engagement and\ncollaboration for expanding the benchmark tasks and models.", "published": "2024-06-27 16:47:42", "link": "http://arxiv.org/abs/2406.19314v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Jump Starting Bandits with LLM-Generated Prior Knowledge", "abstract": "We present substantial evidence demonstrating the benefits of integrating\nLarge Language Models (LLMs) with a Contextual Multi-Armed Bandit framework.\nContextual bandits have been widely used in recommendation systems to generate\npersonalized suggestions based on user-specific contexts. We show that LLMs,\npre-trained on extensive corpora rich in human knowledge and preferences, can\nsimulate human behaviours well enough to jump-start contextual multi-armed\nbandits to reduce online learning regret. We propose an initialization\nalgorithm for contextual bandits by prompting LLMs to produce a pre-training\ndataset of approximate human preferences for the bandit. This significantly\nreduces online learning regret and data-gathering costs for training such\nmodels. Our approach is validated empirically through two sets of experiments\nwith different bandit setups: one which utilizes LLMs to serve as an oracle and\na real-world experiment utilizing data from a conjoint survey experiment.", "published": "2024-06-27 16:52:19", "link": "http://arxiv.org/abs/2406.19317v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "DiVERT: Distractor Generation with Variational Errors Represented as\n  Text for Math Multiple-choice Questions", "abstract": "High-quality distractors are crucial to both the assessment and pedagogical\nvalue of multiple-choice questions (MCQs), where manually crafting ones that\nanticipate knowledge deficiencies or misconceptions among real students is\ndifficult. Meanwhile, automated distractor generation, even with the help of\nlarge language models (LLMs), remains challenging for subjects like math. It is\ncrucial to not only identify plausible distractors but also understand the\nerror behind them. In this paper, we introduce DiVERT (Distractor Generation\nwith Variational Errors Represented as Text), a novel variational approach that\nlearns an interpretable representation of errors behind distractors in math\nMCQs. Through experiments on a real-world math MCQ dataset with 1,434 questions\nused by hundreds of thousands of students, we show that DiVERT, despite using a\nbase open-source LLM with 7B parameters, outperforms state-of-the-art\napproaches using GPT-4o on downstream distractor generation. We also conduct a\nhuman evaluation with math educators and find that DiVERT leads to error labels\nthat are of comparable quality to human-authored ones.", "published": "2024-06-27 17:37:31", "link": "http://arxiv.org/abs/2406.19356v2", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Remarkable Robustness of LLMs: Stages of Inference?", "abstract": "We demonstrate and investigate the remarkable robustness of Large Language\nModels by deleting and swapping adjacent layers. We find that deleting and\nswapping interventions retain 72-95\\% of the original model's prediction\naccuracy without fine-tuning, whereas models with more layers exhibit more\nrobustness. Based on the results of the layer-wise intervention and further\nexperiments, we hypothesize the existence of four universal stages of inference\nacross eight different models: detokenization, feature engineering, prediction\nensembling, and residual sharpening. The first stage integrates local\ninformation, lifting raw token representations into higher-level contextual\nrepresentations. Next is the iterative refinement of task and entity-specific\nfeatures. Then, the second half of the model begins with a phase transition,\nwhere hidden representations align more with the vocabulary space due to\nspecialized model components. Finally, the last layer sharpens the following\ntoken distribution by eliminating obsolete features that add noise to the\nprediction.", "published": "2024-06-27 17:57:03", "link": "http://arxiv.org/abs/2406.19384v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Captioning Visualizations with Large Language Models (CVLLM): A Tutorial", "abstract": "Automatically captioning visualizations is not new, but recent advances in\nlarge language models(LLMs) open exciting new possibilities. In this tutorial,\nafter providing a brief review of Information Visualization (InfoVis)\nprinciples and past work in captioning, we introduce neural models and the\ntransformer architecture used in generic LLMs. We then discuss their recent\napplications in InfoVis, with a focus on captioning. Additionally, we explore\npromising future directions in this field.", "published": "2024-06-27 20:18:18", "link": "http://arxiv.org/abs/2406.19512v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "TocBERT: Medical Document Structure Extraction Using Bidirectional\n  Transformers", "abstract": "Text segmentation holds paramount importance in the field of Natural Language\nProcessing (NLP). It plays an important role in several NLP downstream tasks\nlike information retrieval and document summarization. In this work, we propose\na new solution, namely TocBERT, for segmenting texts using bidirectional\ntransformers. TocBERT represents a supervised solution trained on the detection\nof titles and sub-titles from their semantic representations. This task was\nformulated as a named entity recognition (NER) problem. The solution has been\napplied on a medical text segmentation use-case where the Bio-ClinicalBERT\nmodel is fine-tuned to segment discharge summaries of the MIMIC-III dataset.\nThe performance of TocBERT has been evaluated on a human-labeled ground truth\ncorpus of 250 notes. It achieved an F1-score of 84.6% when evaluated on a\nlinear text segmentation problem and 72.8% on a hierarchical text segmentation\nproblem. It outperformed a carefully designed rule-based solution, particularly\nin distinguishing titles from subtitles.", "published": "2024-06-27 20:56:57", "link": "http://arxiv.org/abs/2406.19526v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Rethinking harmless refusals when fine-tuning foundation models", "abstract": "In this paper, we investigate the degree to which fine-tuning in Large\nLanguage Models (LLMs) effectively mitigates versus merely conceals undesirable\nbehavior. Through the lens of semi-realistic role-playing exercises designed to\nelicit such behaviors, we explore the response dynamics of LLMs post\nfine-tuning interventions. Our methodology involves prompting models for\nChain-of-Thought (CoT) reasoning and analyzing the coherence between the\nreasoning traces and the resultant outputs. Notably, we identify a pervasive\nphenomenon we term \\emph{reason-based deception}, where models either stop\nproducing reasoning traces or produce seemingly ethical reasoning traces that\nbelie the unethical nature of their final outputs. We further examine the\nefficacy of response strategies (polite refusal versus explicit rebuttal) in\ncurbing the occurrence of undesired behavior in subsequent outputs of\nmulti-turn interactions. Our findings reveal that explicit rebuttals\nsignificantly outperform polite refusals in preventing the continuation of\nundesired outputs and nearly eliminate reason-based deception, challenging\ncurrent practices in model fine-tuning. Accordingly, the two key contributions\nof this paper are (1) defining and studying reason-based deception, a new type\nof hidden behavior, and (2) demonstrating that rebuttals provide a more robust\nresponse model to harmful requests than refusals, thereby highlighting the need\nto reconsider the response strategies in fine-tuning approaches.", "published": "2024-06-27 22:08:22", "link": "http://arxiv.org/abs/2406.19552v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PathAlign: A vision-language model for whole slide images in\n  histopathology", "abstract": "Microscopic interpretation of histopathology images underlies many important\ndiagnostic and treatment decisions. While advances in vision-language modeling\nraise new opportunities for analysis of such images, the gigapixel-scale size\nof whole slide images (WSIs) introduces unique challenges. Additionally,\npathology reports simultaneously highlight key findings from small regions\nwhile also aggregating interpretation across multiple slides, often making it\ndifficult to create robust image-text pairs. As such, pathology reports remain\na largely untapped source of supervision in computational pathology, with most\nefforts relying on region-of-interest annotations or self-supervision at the\npatch-level. In this work, we develop a vision-language model based on the\nBLIP-2 framework using WSIs paired with curated text from pathology reports.\nThis enables applications utilizing a shared image-text embedding space, such\nas text or image retrieval for finding cases of interest, as well as\nintegration of the WSI encoder with a frozen large language model (LLM) for\nWSI-based generative text capabilities such as report generation or\nAI-in-the-loop interactions. We utilize a de-identified dataset of over 350,000\nWSIs and diagnostic text pairs, spanning a wide range of diagnoses, procedure\ntypes, and tissue types. We present pathologist evaluation of text generation\nand text retrieval using WSI embeddings, as well as results for WSI\nclassification and workflow prioritization (slide-level triaging).\nModel-generated text for WSIs was rated by pathologists as accurate, without\nclinically significant error or omission, for 78% of WSIs on average. This work\ndemonstrates exciting potential capabilities for language-aligned WSI\nembeddings.", "published": "2024-06-27 23:43:36", "link": "http://arxiv.org/abs/2406.19578v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Enhancing In-Context Learning via Implicit Demonstration Augmentation", "abstract": "The emergence of in-context learning (ICL) enables large pre-trained language\nmodels (PLMs) to make predictions for unseen inputs without updating\nparameters. Despite its potential, ICL's effectiveness heavily relies on the\nquality, quantity, and permutation of demonstrations, commonly leading to\nsuboptimal and unstable performance. In this paper, we tackle this challenge\nfor the first time from the perspective of demonstration augmentation.\nSpecifically, we start with enriching representations of demonstrations by\nleveraging their deep feature distribution. We then theoretically reveal that\nwhen the number of augmented copies approaches infinity, the augmentation is\napproximately equal to a novel logit calibration mechanism integrated with\nspecific statistical properties. This insight results in a simple yet highly\nefficient method that significantly improves the average and worst-case\naccuracy across diverse PLMs and tasks. Moreover, our method effectively\nreduces performance variance among varying demonstrations, permutations, and\ntemplates, and displays the capability to address imbalanced class\ndistributions.", "published": "2024-06-27 05:25:46", "link": "http://arxiv.org/abs/2407.00100v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.7"], "primary_category": "cs.LG"}
{"title": "Curriculum Learning with Quality-Driven Data Selection", "abstract": "The impressive multimodal capabilities demonstrated by OpenAI's GPT-4 have\ngenerated significant interest in the development of Multimodal Large Language\nModels (MLLMs). Visual instruction tuning of MLLMs with machine-generated\ninstruction-following data has shown to enhance zero-shot capabilities across\nvarious tasks. However, there has been limited exploration into controlling the\nquality of the instruction data.Current methodologies for data selection in\nMLLMs often rely on single, unreliable scores or use downstream tasks for\nselection, which is time-consuming and can lead to potential overfitting on the\nchosen evaluation datasets. To mitigate these limitations, we propose a novel\ndata selection methodology that utilizes image-text correlation and model\nperplexity to evaluate and select data of varying quality. This approach\nleverages the distinct distribution of these two attributes, mapping data\nquality into a two-dimensional space that allows for the selection of data\nbased on their location within this distribution. By utilizing this space, we\ncan analyze the impact of task type settings, used as prompts, on data quality.\nAdditionally, this space can be used to construct multi-stage subsets of\nvarying quality to facilitate curriculum learning. Our research includes\ncomprehensive experiments conducted on various datasets. The results emphasize\nsubstantial enhancements in five commonly assessed capabilities compared to\nusing the complete dataset. Our codes, data, and models are publicly available\nat: \\url{https://anonymous.4open.science/r/EHIT-31B4}", "published": "2024-06-27 07:20:36", "link": "http://arxiv.org/abs/2407.00102v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "UnUnlearning: Unlearning is not sufficient for content regulation in\n  advanced generative AI", "abstract": "Exact unlearning was first introduced as a privacy mechanism that allowed a\nuser to retract their data from machine learning models on request. Shortly\nafter, inexact schemes were proposed to mitigate the impractical costs\nassociated with exact unlearning. More recently unlearning is often discussed\nas an approach for removal of impermissible knowledge i.e. knowledge that the\nmodel should not possess such as unlicensed copyrighted, inaccurate, or\nmalicious information. The promise is that if the model does not have a certain\nmalicious capability, then it cannot be used for the associated malicious\npurpose. In this paper we revisit the paradigm in which unlearning is used for\nin Large Language Models (LLMs) and highlight an underlying inconsistency\narising from in-context learning. Unlearning can be an effective control\nmechanism for the training phase, yet it does not prevent the model from\nperforming an impermissible act during inference. We introduce a concept of\nununlearning, where unlearned knowledge gets reintroduced in-context,\neffectively rendering the model capable of behaving as if it knows the\nforgotten knowledge. As a result, we argue that content filtering for\nimpermissible knowledge will be required and even exact unlearning schemes are\nnot enough for effective content regulation. We discuss feasibility of\nununlearning for modern LLMs and examine broader implications.", "published": "2024-06-27 10:24:35", "link": "http://arxiv.org/abs/2407.00106v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "A Case Study on Contextual Machine Translation in a Professional\n  Scenario of Subtitling", "abstract": "Incorporating extra-textual context such as film metadata into the machine\ntranslation (MT) pipeline can enhance translation quality, as indicated by\nautomatic evaluation in recent work. However, the positive impact of such\nsystems in industry remains unproven. We report on an industrial case study\ncarried out to investigate the benefit of MT in a professional scenario of\ntranslating TV subtitles with a focus on how leveraging extra-textual context\nimpacts post-editing. We found that post-editors marked significantly fewer\ncontext-related errors when correcting the outputs of MTCue, the context-aware\nmodel, as opposed to non-contextual models. We also present the results of a\nsurvey of the employed post-editors, which highlights contextual inadequacy as\na significant gap consistently observed in MT. Our findings strengthen the\nmotivation for further work within fully contextual MT.", "published": "2024-06-27 11:20:14", "link": "http://arxiv.org/abs/2407.00108v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.LG"}
{"title": "Accurate Prediction of Ligand-Protein Interaction Affinities with\n  Fine-Tuned Small Language Models", "abstract": "We describe the accurate prediction of ligand-protein interaction (LPI)\naffinities, also known as drug-target interactions (DTI), with instruction\nfine-tuned pretrained generative small language models (SLMs). We achieved\naccurate predictions for a range of affinity values associated with\nligand-protein interactions on out-of-sample data in a zero-shot setting. Only\nthe SMILES string of the ligand and the amino acid sequence of the protein were\nused as the model inputs. Our results demonstrate a clear improvement over\nmachine learning (ML) and free-energy perturbation (FEP+) based methods in\naccurately predicting a range of ligand-protein interaction affinities, which\ncan be leveraged to further accelerate drug discovery campaigns against\nchallenging therapeutic targets.", "published": "2024-06-27 13:04:58", "link": "http://arxiv.org/abs/2407.00111v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "OmniJARVIS: Unified Vision-Language-Action Tokenization Enables\n  Open-World Instruction Following Agents", "abstract": "This paper presents OmniJARVIS, a novel Vision-Language-Action (VLA) model\nfor open-world instruction-following agents in Minecraft. Compared to prior\nworks that either emit textual goals to separate controllers or produce the\ncontrol command directly, OmniJARVIS seeks a different path to ensure both\nstrong reasoning and efficient decision-making capabilities via unified\ntokenization of multimodal interaction data. First, we introduce a\nself-supervised approach to learn a behavior encoder that produces discretized\ntokens for behavior trajectories $\\tau = \\{o_0, a_0, \\dots\\}$ and an imitation\nlearning policy decoder conditioned on these tokens. These additional behavior\ntokens will be augmented to the vocabulary of pretrained Multimodal Language\nModels. With this encoder, we then pack long-term multimodal interactions\ninvolving task instructions, memories, thoughts, observations, textual\nresponses, behavior trajectories, etc into unified token sequences and model\nthem with autoregressive transformers. Thanks to the semantically meaningful\nbehavior tokens, the resulting VLA model, OmniJARVIS, can reason (by producing\nchain-of-thoughts), plan, answer questions, and act (by producing behavior\ntokens for the imitation learning policy decoder). OmniJARVIS demonstrates\nexcellent performances on a comprehensive collection of atomic, programmatic,\nand open-ended tasks in open-world Minecraft. Our analysis further unveils the\ncrucial design principles in interaction data formation, unified tokenization,\nand its scaling potentials. The dataset, models, and code will be released at\nhttps://craftjarvis.org/OmniJARVIS.", "published": "2024-06-27 13:46:11", "link": "http://arxiv.org/abs/2407.00114v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Efficient Long-distance Latent Relation-aware Graph Neural Network for\n  Multi-modal Emotion Recognition in Conversations", "abstract": "The task of multi-modal emotion recognition in conversation (MERC) aims to\nanalyze the genuine emotional state of each utterance based on the multi-modal\ninformation in the conversation, which is crucial for conversation\nunderstanding. Existing methods focus on using graph neural networks (GNN) to\nmodel conversational relationships and capture contextual latent semantic\nrelationships. However, due to the complexity of GNN, existing methods cannot\nefficiently capture the potential dependencies between long-distance\nutterances, which limits the performance of MERC. In this paper, we propose an\nEfficient Long-distance Latent Relation-aware Graph Neural Network (ELR-GNN)\nfor multi-modal emotion recognition in conversations. Specifically, we first\nuse pre-extracted text, video and audio features as input to Bi-LSTM to capture\ncontextual semantic information and obtain low-level utterance features. Then,\nwe use low-level utterance features to construct a conversational emotion\ninteraction graph. To efficiently capture the potential dependencies between\nlong-distance utterances, we use the dilated generalized forward push algorithm\nto precompute the emotional propagation between global utterances and design an\nemotional relation-aware operator to capture the potential semantic\nassociations between different utterances. Furthermore, we combine early fusion\nand adaptive late fusion mechanisms to fuse latent dependency information\nbetween speaker relationship information and context. Finally, we obtain\nhigh-level discourse features and feed them into MLP for emotion prediction.\nExtensive experimental results show that ELR-GNN achieves state-of-the-art\nperformance on the benchmark datasets IEMOCAP and MELD, with running times\nreduced by 52\\% and 35\\%, respectively.", "published": "2024-06-27 15:54:12", "link": "http://arxiv.org/abs/2407.00119v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Granite-Function Calling Model: Introducing Function Calling Abilities\n  via Multi-task Learning of Granular Tasks", "abstract": "Large language models (LLMs) have recently shown tremendous promise in\nserving as the backbone to agentic systems, as demonstrated by their\nperformance in multi-faceted, challenging benchmarks like SWE-Bench and\nAgent-Bench. However, to realize the true potential of LLMs as autonomous\nagents, they must learn to identify, call, and interact with external tools and\napplication program interfaces (APIs) to complete complex tasks. These tasks\ntogether are termed function calling. Endowing LLMs with function calling\nabilities leads to a myriad of advantages, such as access to current and\ndomain-specific information in databases and knowledge sources, and the ability\nto outsource tasks that can be reliably performed by tools, e.g., a Python\ninterpreter or calculator. While there has been significant progress in\nfunction calling with LLMs, there is still a dearth of open models that perform\non par with proprietary LLMs like GPT, Claude, and Gemini. Therefore, in this\nwork, we introduce the GRANITE-20B-FUNCTIONCALLING model under an Apache 2.0\nlicense. The model is trained using a multi-task training approach on seven\nfundamental tasks encompassed in function calling, those being Nested Function\nCalling, Function Chaining, Parallel Functions, Function Name Detection,\nParameter-Value Pair Detection, Next-Best Function, and Response Generation. We\npresent a comprehensive evaluation on multiple out-of-domain datasets comparing\nGRANITE-20B-FUNCTIONCALLING to more than 15 other best proprietary and open\nmodels. GRANITE-20B-FUNCTIONCALLING provides the best performance among all\nopen models on the Berkeley Function Calling Leaderboard and fourth overall. As\na result of the diverse tasks and datasets used for training our model, we show\nthat GRANITE-20B-FUNCTIONCALLING has better generalizability on multiple tasks\nin seven different evaluation datasets.", "published": "2024-06-27 17:47:26", "link": "http://arxiv.org/abs/2407.00121v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "ColPali: Efficient Document Retrieval with Vision Language Models", "abstract": "Documents are visually rich structures that convey information through text,\nbut also figures, page layouts, tables, or even fonts. Since modern retrieval\nsystems mainly rely on the textual information they extract from document pages\nto index documents -often through lengthy and brittle processes-, they struggle\nto exploit key visual cues efficiently. This limits their capabilities in many\npractical document retrieval applications such as Retrieval Augmented\nGeneration (RAG). To benchmark current systems on visually rich document\nretrieval, we introduce the Visual Document Retrieval Benchmark ViDoRe,\ncomposed of various page-level retrieval tasks spanning multiple domains,\nlanguages, and practical settings. The inherent complexity and performance\nshortcomings of modern systems motivate a new concept; doing document retrieval\nby directly embedding the images of the document pages. We release ColPali, a\nVision Language Model trained to produce high-quality multi-vector embeddings\nfrom images of document pages. Combined with a late interaction matching\nmechanism, ColPali largely outperforms modern document retrieval pipelines\nwhile being drastically simpler, faster and end-to-end trainable. We release\nmodels, data, code and benchmarks under open licenses at https://hf.co/vidore.", "published": "2024-06-27 15:45:29", "link": "http://arxiv.org/abs/2407.01449v6", "categories": ["cs.IR", "cs.CL", "cs.CV"], "primary_category": "cs.IR"}
{"title": "A Transformer-Based Multi-Stream Approach for Isolated Iranian Sign\n  Language Recognition", "abstract": "Sign language is an essential means of communication for millions of people\naround the world and serves as their primary language. However, most\ncommunication tools are developed for spoken and written languages which can\ncause problems and difficulties for the deaf and hard of hearing community. By\ndeveloping a sign language recognition system, we can bridge this communication\ngap and enable people who use sign language as their main form of expression to\nbetter communicate with people and their surroundings. This recognition system\nincreases the quality of health services, improves public services, and creates\nequal opportunities for the deaf community. This research aims to recognize\nIranian Sign Language words with the help of the latest deep learning tools\nsuch as transformers. The dataset used includes 101 Iranian Sign Language words\nfrequently used in academic environments such as universities. The network used\nis a combination of early fusion and late fusion transformer encoder-based\nnetworks optimized with the help of genetic algorithm. The selected features to\ntrain this network include hands and lips key points, and the distance and\nangle between hands extracted from the sign videos. Also, in addition to the\ntraining model for the classes, the embedding vectors of words are used as\nmulti-task learning to have smoother and more efficient training. This model\nwas also tested on sentences generated from our word dataset using a windowing\ntechnique for sentence translation. Finally, the sign language training\nsoftware that provides real-time feedback to users with the help of the\ndeveloped model, which has 90.2% accuracy on test data, was introduced, and in\na survey, the effectiveness and efficiency of this type of sign language\nlearning software and the impact of feedback were investigated.", "published": "2024-06-27 06:54:25", "link": "http://arxiv.org/abs/2407.09544v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "eess.IV"], "primary_category": "cs.CL"}
{"title": "Direct-Inverse Prompting: Analyzing LLMs' Discriminative Capacity in\n  Self-Improving Generation", "abstract": "Mainstream LLM research has primarily focused on enhancing their generative\ncapabilities. However, even the most advanced LLMs experience uncertainty in\ntheir outputs, often producing varied results on different runs or when faced\nwith minor changes in input, despite no substantial change in content. Given\nmultiple responses from the same LLM to the same input, we advocate leveraging\nthe LLMs' discriminative capability to reduce this generative uncertainty,\naiding in identifying the correct answers. Specifically, we propose and analyze\nthree discriminative prompts: direct, inverse, and hybrid, to explore the\npotential of both closed-source and open-source LLMs in self-improving their\ngenerative performance on two benchmark datasets. Our insights reveal which\ndiscriminative prompt is most promising and when to use it. To our knowledge,\nthis is the first work to systematically analyze LLMs' discriminative capacity\nto address generative uncertainty.", "published": "2024-06-27 02:26:47", "link": "http://arxiv.org/abs/2407.11017v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Great AI Witch Hunt: Reviewers Perception and (Mis)Conception of\n  Generative AI in Research Writing", "abstract": "Generative AI (GenAI) use in research writing is growing fast. However, it is\nunclear how peer reviewers recognize or misjudge AI-augmented manuscripts. To\ninvestigate the impact of AI-augmented writing on peer reviews, we conducted a\nsnippet-based online survey with 17 peer reviewers from top-tier HCI\nconferences. Our findings indicate that while AI-augmented writing improves\nreadability, language diversity, and informativeness, it often lacks research\ndetails and reflective insights from authors. Reviewers consistently struggled\nto distinguish between human and AI-augmented writing but their judgements\nremained consistent. They noted the loss of a \"human touch\" and subjective\nexpressions in AI-augmented writing. Based on our findings, we advocate for\nreviewer guidelines that promote impartial evaluations of submissions,\nregardless of any personal biases towards GenAI. The quality of the research\nitself should remain a priority in reviews, regardless of any preconceived\nnotions about the tools used to create it. We emphasize that researchers must\nmaintain their authorship and control over the writing process, even when using\nGenAI's assistance.", "published": "2024-06-27 02:38:25", "link": "http://arxiv.org/abs/2407.12015v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "SignSpeak: Open-Source Time Series Classification for ASL Translation", "abstract": "The lack of fluency in sign language remains a barrier to seamless\ncommunication for hearing and speech-impaired communities. In this work, we\npropose a low-cost, real-time ASL-to-speech translation glove and an exhaustive\ntraining dataset of sign language patterns. We then benchmarked this dataset\nwith supervised learning models, such as LSTMs, GRUs and Transformers, where\nour best model achieved 92% accuracy. The SignSpeak dataset has 7200 samples\nencompassing 36 classes (A-Z, 1-10) and aims to capture realistic signing\npatterns by using five low-cost flex sensors to measure finger positions at\neach time step at 36 Hz. Our open-source dataset, models and glove designs,\nprovide an accurate and efficient ASL translator while maintaining\ncost-effectiveness, establishing a framework for future work to build on.", "published": "2024-06-27 17:58:54", "link": "http://arxiv.org/abs/2407.12020v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Taming Data and Transformers for Scalable Audio Generation", "abstract": "The scalability of ambient sound generators is hindered by data scarcity,\ninsufficient caption quality, and limited scalability in model architecture.\nThis work addresses these challenges by advancing both data and model scaling.\nFirst, we propose an efficient and scalable dataset collection pipeline\ntailored for ambient audio generation, resulting in AutoReCap-XL, the largest\nambient audio-text dataset with over 47 million clips. To provide high-quality\ntextual annotations, we propose AutoCap, a high-quality automatic audio\ncaptioning model. By adopting a Q-Former module and leveraging audio metadata,\nAutoCap substantially enhances caption quality, reaching a CIDEr score of\n$83.2$, a $3.2\\%$ improvement over previous captioning models. Finally, we\npropose GenAu, a scalable transformer-based audio generation architecture that\nwe scale up to 1.25B parameters. We demonstrate its benefits from data scaling\nwith synthetic captions as well as model size scaling. When compared to\nbaseline audio generators trained at similar size and data scale, GenAu obtains\nsignificant improvements of $4.7\\%$ in FAD score, $11.1\\%$ in IS, and $13.5\\%$\nin CLAP score. Our code, model checkpoints, and dataset are publicly available.", "published": "2024-06-27 17:58:54", "link": "http://arxiv.org/abs/2406.19388v3", "categories": ["cs.SD", "cs.CL", "cs.CV", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "LoPT: Low-Rank Prompt Tuning for Parameter Efficient Language Models", "abstract": "In prompt tuning, a prefix or suffix text is added to the prompt, and the\nembeddings (soft prompts) or token indices (hard prompts) of the prefix/suffix\nare optimized to gain more control over language models for specific tasks.\nThis approach eliminates the need for hand-crafted prompt engineering or\nexplicit model fine-tuning. Prompt tuning is significantly more\nparameter-efficient than model fine-tuning, as it involves optimizing partial\ninputs of language models to produce desired outputs.\n  In this work, we aim to further reduce the amount of trainable parameters\nrequired for a language model to perform well on specific tasks. We propose\nLow-rank Prompt Tuning (LoPT), a low-rank model for prompts that achieves\nefficient prompt optimization. The proposed method demonstrates similar\noutcomes to full parameter prompt tuning while reducing the number of trainable\nparameters by a factor of 5. It also provides promising results compared to the\nstate-of-the-art methods that would require 10 to 20 times more parameters.", "published": "2024-06-27 19:02:41", "link": "http://arxiv.org/abs/2406.19486v1", "categories": ["cs.CL", "cs.AI", "cs.ET", "cs.LG", "eess.SP"], "primary_category": "cs.CL"}
{"title": "On Discrete Prompt Optimization for Diffusion Models", "abstract": "This paper introduces the first gradient-based framework for prompt\noptimization in text-to-image diffusion models. We formulate prompt engineering\nas a discrete optimization problem over the language space. Two major\nchallenges arise in efficiently finding a solution to this problem: (1)\nEnormous Domain Space: Setting the domain to the entire language space poses\nsignificant difficulty to the optimization process. (2) Text Gradient:\nEfficiently computing the text gradient is challenging, as it requires\nbackpropagating through the inference steps of the diffusion model and a\nnon-differentiable embedding lookup table. Beyond the problem formulation, our\nmain technical contributions lie in solving the above challenges. First, we\ndesign a family of dynamically generated compact subspaces comprised of only\nthe most relevant words to user input, substantially restricting the domain\nspace. Second, we introduce \"Shortcut Text Gradient\" -- an effective\nreplacement for the text gradient that can be obtained with constant memory and\nruntime. Empirical evaluation on prompts collected from diverse sources\n(DiffusionDB, ChatGPT, COCO) suggests that our method can discover prompts that\nsubstantially improve (prompt enhancement) or destroy (adversarial attack) the\nfaithfulness of images generated by the text-to-image diffusion model.", "published": "2024-06-27 02:53:01", "link": "http://arxiv.org/abs/2407.01606v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "stat.ML", "68T01"], "primary_category": "cs.LG"}
{"title": "Tradition or Innovation: A Comparison of Modern ASR Methods for Forced\n  Alignment", "abstract": "Forced alignment (FA) plays a key role in speech research through the\nautomatic time alignment of speech signals with corresponding text\ntranscriptions. Despite the move towards end-to-end architectures for speech\ntechnology, FA is still dominantly achieved through a classic GMM-HMM acoustic\nmodel. This work directly compares alignment performance from leading automatic\nspeech recognition (ASR) methods, WhisperX and Massively Multilingual Speech\nRecognition (MMS), against a Kaldi-based GMM-HMM system, the Montreal Forced\nAligner (MFA). Performance was assessed on the manually aligned TIMIT and\nBuckeye datasets, with comparisons conducted only on words correctly recognized\nby WhisperX and MMS. The MFA outperformed both WhisperX and MMS, revealing a\nshortcoming of modern ASR systems. These findings highlight the need for\nadvancements in forced alignment and emphasize the importance of integrating\ntraditional expertise with modern innovation to foster progress. Index Terms:\nforced alignment, phoneme alignment, word alignment", "published": "2024-06-27 17:45:34", "link": "http://arxiv.org/abs/2406.19363v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Streaming Decoder-Only Automatic Speech Recognition with Discrete Speech\n  Units: A Pilot Study", "abstract": "Unified speech-text models like SpeechGPT, VioLA, and AudioPaLM have shown\nimpressive performance across various speech-related tasks, especially in\nAutomatic Speech Recognition (ASR). These models typically adopt a unified\nmethod to model discrete speech and text tokens, followed by training a\ndecoder-only transformer. However, they are all designed for non-streaming ASR\ntasks, where the entire speech utterance is needed during decoding. Hence, we\nintroduce a decoder-only model exclusively designed for streaming recognition,\nincorporating a dedicated boundary token to facilitate streaming recognition\nand employing causal attention masking during the training phase. Furthermore,\nwe introduce right-chunk attention and various data augmentation techniques to\nimprove the model's contextual modeling abilities. While achieving streaming\nspeech recognition, experiments on the AISHELL-1 and -2 datasets demonstrate\nthe competitive performance of our streaming approach with non-streaming\ndecoder-only counterparts.", "published": "2024-06-27 03:16:22", "link": "http://arxiv.org/abs/2406.18862v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DEX-TTS: Diffusion-based EXpressive Text-to-Speech with Style Modeling\n  on Time Variability", "abstract": "Expressive Text-to-Speech (TTS) using reference speech has been studied\nextensively to synthesize natural speech, but there are limitations to\nobtaining well-represented styles and improving model generalization ability.\nIn this study, we present Diffusion-based EXpressive TTS (DEX-TTS), an acoustic\nmodel designed for reference-based speech synthesis with enhanced style\nrepresentations. Based on a general diffusion TTS framework, DEX-TTS includes\nencoders and adapters to handle styles extracted from reference speech. Key\ninnovations contain the differentiation of styles into time-invariant and\ntime-variant categories for effective style extraction, as well as the design\nof encoders and adapters with high generalization ability. In addition, we\nintroduce overlapping patchify and convolution-frequency patch embedding\nstrategies to improve DiT-based diffusion networks for TTS. DEX-TTS yields\noutstanding performance in terms of objective and subjective evaluation in\nEnglish multi-speaker and emotional multi-speaker datasets, without relying on\npre-training strategies. Lastly, the comparison results for the general TTS on\na single-speaker dataset verify the effectiveness of our enhanced diffusion\nbackbone. Demos are available here.", "published": "2024-06-27 12:39:55", "link": "http://arxiv.org/abs/2406.19135v1", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "Application of ASV for Voice Identification after VC and Duration\n  Predictor Improvement in TTS Models", "abstract": "One of the most crucial components in the field of biometric security is the\nautomatic speaker verification system, which is based on the speaker's voice.\nIt is possible to utilise ASVs in isolation or in conjunction with other AI\nmodels. In the contemporary era, the quality and quantity of neural networks\nare increasing exponentially. Concurrently, there is a growing number of\nsystems that aim to manipulate data through the use of voice conversion and\ntext-to-speech models. The field of voice biometrics forgery is aided by a\nnumber of challenges, including SSTC, ASVSpoof, and SingFake.\n  This paper presents a system for automatic speaker verification. The primary\nobjective of our model is the extraction of embeddings from the target\nspeaker's audio in order to obtain information about important characteristics\nof his voice, such as pitch, energy, and the duration of phonemes. This\ninformation is used in our multivoice TTS pipeline, which is currently under\ndevelopment. However, this model was employed within the SSTC challenge to\nverify users whose voice had undergone voice conversion, where it demonstrated\nan EER of 20.669.", "published": "2024-06-27 15:08:51", "link": "http://arxiv.org/abs/2406.19243v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Zero-Query Adversarial Attack on Black-box Automatic Speech Recognition\n  Systems", "abstract": "In recent years, extensive research has been conducted on the vulnerability\nof ASR systems, revealing that black-box adversarial example attacks pose\nsignificant threats to real-world ASR systems. However, most existing black-box\nattacks rely on queries to the target ASRs, which is impractical when queries\nare not permitted. In this paper, we propose ZQ-Attack, a transfer-based\nadversarial attack on ASR systems in the zero-query black-box setting. Through\na comprehensive review and categorization of modern ASR technologies, we first\nmeticulously select surrogate ASRs of diverse types to generate adversarial\nexamples. Following this, ZQ-Attack initializes the adversarial perturbation\nwith a scaled target command audio, rendering it relatively imperceptible while\nmaintaining effectiveness. Subsequently, to achieve high transferability of\nadversarial perturbations, we propose a sequential ensemble optimization\nalgorithm, which iteratively optimizes the adversarial perturbation on each\nsurrogate model, leveraging collaborative information from other models. We\nconduct extensive experiments to evaluate ZQ-Attack. In the over-the-line\nsetting, ZQ-Attack achieves a 100% success rate of attack (SRoA) with an\naverage signal-to-noise ratio (SNR) of 21.91dB on 4 online speech recognition\nservices, and attains an average SRoA of 100% and SNR of 19.67dB on 16\nopen-source ASRs. For commercial intelligent voice control devices, ZQ-Attack\nalso achieves a 100% SRoA with an average SNR of 15.77dB in the over-the-air\nsetting.", "published": "2024-06-27 16:39:36", "link": "http://arxiv.org/abs/2406.19311v1", "categories": ["cs.CR", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
{"title": "Subtractive Training for Music Stem Insertion using Latent Diffusion\n  Models", "abstract": "We present Subtractive Training, a simple and novel method for synthesizing\nindividual musical instrument stems given other instruments as context. This\nmethod pairs a dataset of complete music mixes with 1) a variant of the dataset\nlacking a specific stem, and 2) LLM-generated instructions describing how the\nmissing stem should be reintroduced. We then fine-tune a pretrained\ntext-to-audio diffusion model to generate the missing instrument stem, guided\nby both the existing stems and the text instruction. Our results demonstrate\nSubtractive Training's efficacy in creating authentic drum stems that\nseamlessly blend with the existing tracks. We also show that we can use the\ntext instruction to control the generation of the inserted stem in terms of\nrhythm, dynamics, and genre, allowing us to modify the style of a single\ninstrument in a full song while keeping the remaining instruments the same.\nLastly, we extend this technique to MIDI formats, successfully generating\ncompatible bass, drum, and guitar parts for incomplete arrangements.", "published": "2024-06-27 16:59:14", "link": "http://arxiv.org/abs/2406.19328v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "From Modular to End-to-End Speaker Diarization", "abstract": "Speaker diarization is usually referred to as the task that determines ``who\nspoke when'' in a recording. Until a few years ago, all competitive approaches\nwere modular. Systems based on this framework reached state-of-the-art\nperformance in most scenarios but had major difficulties dealing with\noverlapped speech. More recently, the advent of end-to-end models, capable of\ndealing with all aspects of speaker diarization with a single model and better\nperforming regarding overlapped speech, has brought high levels of attention.\n  This thesis is framed during a period of co-existence of these two trends. We\ndescribe a system based on a Bayesian hidden Markov model used to cluster\nx-vectors (speaker embeddings obtained with a neural network), known as VBx,\nwhich has shown remarkable performance on different datasets and challenges. We\ncomment on its advantages and limitations and evaluate results on different\nrelevant corpora. Then, we move towards end-to-end neural diarization (EEND)\nmethods. Due to the need for large training sets for training these models and\nthe lack of manually annotated diarization data in sufficient quantities, the\ncompromise solution consists in generating training data artificially. We\ndescribe an approach for generating synthetic data which resembles real\nconversations in terms of speaker turns and overlaps. We show how this method\ngenerating ``simulated conversations'' allows for better performance than using\na previously proposed method for creating ``simulated mixtures'' when training\nthe popular EEND with encoder-decoder attractors (EEND-EDA). We also propose a\nnew EEND-based model, which we call DiaPer, and show that it can perform better\nthan EEND-EDA, especially when dealing with many speakers and handling\noverlapped speech. Finally, we compare both VBx-based and DiaPer systems on a\nwide variety of corpora and comment on the advantages of each technique.", "published": "2024-06-27 15:09:39", "link": "http://arxiv.org/abs/2407.08752v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "ManiWAV: Learning Robot Manipulation from In-the-Wild Audio-Visual Data", "abstract": "Audio signals provide rich information for the robot interaction and object\nproperties through contact. This information can surprisingly ease the learning\nof contact-rich robot manipulation skills, especially when the visual\ninformation alone is ambiguous or incomplete. However, the usage of audio data\nin robot manipulation has been constrained to teleoperated demonstrations\ncollected by either attaching a microphone to the robot or object, which\nsignificantly limits its usage in robot learning pipelines. In this work, we\nintroduce ManiWAV: an 'ear-in-hand' data collection device to collect\nin-the-wild human demonstrations with synchronous audio and visual feedback,\nand a corresponding policy interface to learn robot manipulation policy\ndirectly from the demonstrations. We demonstrate the capabilities of our system\nthrough four contact-rich manipulation tasks that require either passively\nsensing the contact events and modes, or actively sensing the object surface\nmaterials and states. In addition, we show that our system can generalize to\nunseen in-the-wild environments by learning from diverse in-the-wild human\ndemonstrations.", "published": "2024-06-27 18:06:38", "link": "http://arxiv.org/abs/2406.19464v2", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.RO"}
