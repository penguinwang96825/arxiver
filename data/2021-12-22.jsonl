{"title": "Adaptive Beam Search to Enhance On-device Abstractive Summarization", "abstract": "We receive several essential updates on our smartphones in the form of SMS,\ndocuments, voice messages, etc. that get buried beneath the clutter of content.\nWe often do not realize the key information without going through the full\ncontent. SMS notifications sometimes help by giving an idea of what the message\nis about, however, they merely offer a preview of the beginning content. One\nway to solve this is to have a single efficient model that can adapt and\nsummarize data from varied sources. In this paper, we tackle this issue and for\nthe first time, propose a novel Adaptive Beam Search to improve the quality of\non-device abstractive summarization that can be applied to SMS, voice messages\nand can be extended to documents. To the best of our knowledge, this is the\nfirst on-device abstractive summarization pipeline to be proposed that can\nadapt to multiple data sources addressing privacy concerns of users as compared\nto the majority of existing summarization systems that send data to a server.\nWe reduce the model size by 30.9% using knowledge distillation and show that\nthis model with a 97.6% lesser memory footprint extracts the same or more key\ninformation as compared to BERT.", "published": "2021-12-22 16:54:06", "link": "http://arxiv.org/abs/2201.02739v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Consistency and Coherence from Points of Contextual Similarity", "abstract": "Factual consistency is one of important summary evaluation dimensions,\nespecially as summary generation becomes more fluent and coherent. The ESTIME\nmeasure, recently proposed specifically for factual consistency, achieves high\ncorrelations with human expert scores both for consistency and fluency, while\nin principle being restricted to evaluating such text-summary pairs that have\nhigh dictionary overlap. This is not a problem for current styles of\nsummarization, but it may become an obstacle for future summarization systems,\nor for evaluating arbitrary claims against the text. In this work we generalize\nthe method, and make a variant of the measure applicable to any text-summary\npairs. As ESTIME uses points of contextual similarity, it provides insights\ninto usefulness of information taken from different BERT layers. We observe\nthat useful information exists in almost all of the layers except the several\nlowest ones. For consistency and fluency - qualities focused on local text\ndetails - the most useful layers are close to the top (but not at the top); for\ncoherence and relevance we found a more complicated and interesting picture.", "published": "2021-12-22 03:04:20", "link": "http://arxiv.org/abs/2112.11638v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Label Dependence-aware Sequence Generation Model for Multi-level\n  Implicit Discourse Relation Recognition", "abstract": "Implicit discourse relation recognition (IDRR) is a challenging but crucial\ntask in discourse analysis. Most existing methods train multiple models to\npredict multi-level labels independently, while ignoring the dependence between\nhierarchically structured labels. In this paper, we consider multi-level IDRR\nas a conditional label sequence generation task and propose a Label\nDependence-aware Sequence Generation Model (LDSGM) for it. Specifically, we\nfirst design a label attentive encoder to learn the global representation of an\ninput instance and its level-specific contexts, where the label dependence is\nintegrated to obtain better label embeddings. Then, we employ a label sequence\ndecoder to output the predicted labels in a top-down manner, where the\npredicted higher-level labels are directly used to guide the label prediction\nat the current level. We further develop a mutual learning enhanced training\nmethod to exploit the label dependence in a bottomup direction, which is\ncaptured by an auxiliary decoder introduced during training. Experimental\nresults on the PDTB dataset show that our model achieves the state-of-the-art\nperformance on multi-level IDRR. We will release our code at\nhttps://github.com/nlpersECJTU/LDSGM.", "published": "2021-12-22 09:14:03", "link": "http://arxiv.org/abs/2112.11740v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CRASS: A Novel Data Set and Benchmark to Test Counterfactual Reasoning\n  of Large Language Models", "abstract": "We introduce the CRASS (counterfactual reasoning assessment) data set and\nbenchmark utilizing questionized counterfactual conditionals as a novel and\npowerful tool to evaluate large language models. We present the data set design\nand benchmark that supports scoring against a crowd-validated human baseline.\nWe test six state-of-the-art models against our benchmark. Our results show\nthat it poses a valid challenge for these models and opens up considerable room\nfor their improvement.", "published": "2021-12-22 15:03:23", "link": "http://arxiv.org/abs/2112.11941v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text is no more Enough! A Benchmark for Profile-based Spoken Language\n  Understanding", "abstract": "Current researches on spoken language understanding (SLU) heavily are limited\nto a simple setting: the plain text-based SLU that takes the user utterance as\ninput and generates its corresponding semantic frames (e.g., intent and slots).\nUnfortunately, such a simple setting may fail to work in complex real-world\nscenarios when an utterance is semantically ambiguous, which cannot be achieved\nby the text-based SLU models. In this paper, we first introduce a new and\nimportant task, Profile-based Spoken Language Understanding (ProSLU), which\nrequires the model that not only relies on the plain text but also the\nsupporting profile information to predict the correct intents and slots. To\nthis end, we further introduce a large-scale human-annotated Chinese dataset\nwith over 5K utterances and their corresponding supporting profile information\n(Knowledge Graph (KG), User Profile (UP), Context Awareness (CA)). In addition,\nwe evaluate several state-of-the-art baseline models and explore a multi-level\nknowledge adapter to effectively incorporate profile information. Experimental\nresults reveal that all existing text-based SLU models fail to work when the\nutterances are semantically ambiguous and our proposed framework can\neffectively fuse the supporting information for sentence-level intent detection\nand token-level slot filling. Finally, we summarize key challenges and provide\nnew points for future directions, which hopes to facilitate the research.", "published": "2021-12-22 15:22:17", "link": "http://arxiv.org/abs/2112.11953v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Toward Educator-focused Automated Scoring Systems for Reading and\n  Writing", "abstract": "This paper presents methods for improving automated essay scoring with\ntechniques that address the computational trade-offs of self-attention and\ndocument length. To make Automated Essay Scoring (AES) more useful to\npractitioners, researchers must overcome the challenges of data and label\navailability, authentic and extended writing, domain scoring, prompt and source\nvariety, and transfer learning. This paper addresses these challenges using\nneural network models by employing techniques that preserve essay length as an\nimportant feature without increasing model training costs. It introduces\ntechniques for minimizing classification loss on ordinal labels using\nmulti-objective learning, capturing semantic information across the entire\nessay using sentence embeddings to use transformer architecture across\narbitrarily long documents, the use of such models for transfer learning,\nautomated hyperparameter generation based on prompt-corpus metadata, and, most\nimportantly, the use of semantic information to provide meaningful insights\ninto student reading through analysis of passage-dependent writing resulting in\nstate-of-the-art results for various essay tasks.", "published": "2021-12-22 15:44:30", "link": "http://arxiv.org/abs/2112.11973v1", "categories": ["cs.CL", "I.2.7; K.3.1"], "primary_category": "cs.CL"}
{"title": "Evolution and trade-off dynamics of functional load", "abstract": "Function Load (FL) quantifies the contributions by phonological contrasts to\ndistinctions made across the lexicon. Previous research has linked particularly\nlow values of FL to sound change. Here we broaden the scope of enquiry into FL,\nto its evolution at all values. We apply phylogenetic methods to examine the\ndiachronic evolution of FL across 90 languages of the Pama-Nyungan (PN) family\nof Australia. We find a high degree of phylogenetic signal in FL. Though\nphylogenetic signal has been reported for phonological structures, such as\nphonotactics, its detection in measures of phonological function is novel. We\nalso find a significant, negative correlation between the FL of vowel length\nand of the following consonant, that is, a deep-time historical trade-off\ndynamic, which we relate to known allophony in modern PN languages and\ncompensatory sound changes in their past. The finding reveals a historical\ndynamic, similar to transphonologization, which we characterize as a flow of\ncontrastiveness between subsystems of the phonology. Recurring across a\nlanguage family which spans a whole continent and many millennia of time depth,\nour finding provides one of the most compelling examples yet of Sapir's 'drift'\nhypothesis, of non-accidentally parallel development in historically related\nlanguages.", "published": "2021-12-22 20:57:50", "link": "http://arxiv.org/abs/2112.12224v1", "categories": ["cs.CL", "J.5"], "primary_category": "cs.CL"}
{"title": "Diformer: Directional Transformer for Neural Machine Translation", "abstract": "Autoregressive (AR) and Non-autoregressive (NAR) models have their own\nsuperiority on the performance and latency, combining them into one model may\ntake advantage of both. Current combination frameworks focus more on the\nintegration of multiple decoding paradigms with a unified generative model,\ne.g. Masked Language Model. However, the generalization can be harmful to the\nperformance due to the gap between training objective and inference. In this\npaper, we aim to close the gap by preserving the original objective of AR and\nNAR under a unified framework. Specifically, we propose the Directional\nTransformer (Diformer) by jointly modelling AR and NAR into three generation\ndirections (left-to-right, right-to-left and straight) with a newly introduced\ndirection variable, which works by controlling the prediction of each token to\nhave specific dependencies under that direction. The unification achieved by\ndirection successfully preserves the original dependency assumption used in AR\nand NAR, retaining both generalization and performance. Experiments on 4 WMT\nbenchmarks demonstrate that Diformer outperforms current united-modelling works\nwith more than 1.5 BLEU points for both AR and NAR decoding, and is also\ncompetitive to the state-of-the-art independent AR and NAR models.", "published": "2021-12-22 02:35:29", "link": "http://arxiv.org/abs/2112.11632v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Self-Distillation Mixup Training for Non-autoregressive Neural Machine\n  Translation", "abstract": "Recently, non-autoregressive (NAT) models predict outputs in parallel,\nachieving substantial improvements in generation speed compared to\nautoregressive (AT) models. While performing worse on raw data, most NAT models\nare trained as student models on distilled data generated by AT teacher models,\nwhich is known as sequence-level Knowledge Distillation. An effective training\nstrategy to improve the performance of AT models is Self-Distillation Mixup\n(SDM) Training, which pre-trains a model on raw data, generates distilled data\nby the pre-trained model itself and finally re-trains a model on the\ncombination of raw data and distilled data. In this work, we aim to view SDM\nfor NAT models, but find directly adopting SDM to NAT models gains no\nimprovements in terms of translation quality. Through careful analysis, we\nobserve the invalidation is correlated to Modeling Diversity and Confirmation\nBias between the AT teacher model and the NAT student models. Based on these\nfindings, we propose an enhanced strategy named SDMRT by adding two stages to\nclassic SDM: one is Pre-Rerank on self-distilled data, the other is Fine-Tune\non Filtered teacher-distilled data. Our results outperform baselines by 0.6 to\n1.2 BLEU on multiple NAT models. As another bonus, for Iterative Refinement NAT\nmodels, our methods can outperform baselines within half iteration number,\nwhich means 2X acceleration.", "published": "2021-12-22 03:06:27", "link": "http://arxiv.org/abs/2112.11640v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Joint-training on Symbiosis Networks for Deep Nueral Machine Translation\n  models", "abstract": "Deep encoders have been proven to be effective in improving neural machine\ntranslation (NMT) systems, but it reaches the upper bound of translation\nquality when the number of encoder layers exceeds 18. Worse still, deeper\nnetworks consume a lot of memory, making it impossible to train efficiently. In\nthis paper, we present Symbiosis Networks, which include a full network as the\nSymbiosis Main Network (M-Net) and another shared sub-network with the same\nstructure but less layers as the Symbiotic Sub Network (S-Net). We adopt\nSymbiosis Networks on Transformer-deep (m-n) architecture and define a\nparticular regularization loss $\\mathcal{L}_{\\tau}$ between the M-Net and S-Net\nin NMT. We apply joint-training on the Symbiosis Networks and aim to improve\nthe M-Net performance. Our proposed training strategy improves Transformer-deep\n(12-6) by 0.61, 0.49 and 0.69 BLEU over the baselines under classic training on\nWMT'14 EN->DE, DE->EN and EN->FR tasks. Furthermore, our Transformer-deep\n(12-6) even outperforms classic Transformer-deep (18-6).", "published": "2021-12-22 03:13:45", "link": "http://arxiv.org/abs/2112.11642v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "How Should Pre-Trained Language Models Be Fine-Tuned Towards Adversarial\n  Robustness?", "abstract": "The fine-tuning of pre-trained language models has a great success in many\nNLP fields. Yet, it is strikingly vulnerable to adversarial examples, e.g.,\nword substitution attacks using only synonyms can easily fool a BERT-based\nsentiment analysis model. In this paper, we demonstrate that adversarial\ntraining, the prevalent defense technique, does not directly fit a conventional\nfine-tuning scenario, because it suffers severely from catastrophic forgetting:\nfailing to retain the generic and robust linguistic features that have already\nbeen captured by the pre-trained model. In this light, we propose Robust\nInformative Fine-Tuning (RIFT), a novel adversarial fine-tuning method from an\ninformation-theoretical perspective. In particular, RIFT encourages an\nobjective model to retain the features learned from the pre-trained model\nthroughout the entire fine-tuning process, whereas a conventional one only uses\nthe pre-trained weights for initialization. Experimental results show that RIFT\nconsistently outperforms the state-of-the-arts on two popular NLP tasks:\nsentiment analysis and natural language inference, under different attacks\nacross various pre-trained language models.", "published": "2021-12-22 05:04:41", "link": "http://arxiv.org/abs/2112.11668v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Hybrid Curriculum Learning for Emotion Recognition in Conversation", "abstract": "Emotion recognition in conversation (ERC) aims to detect the emotion label\nfor each utterance. Motivated by recent studies which have proven that feeding\ntraining examples in a meaningful order rather than considering them randomly\ncan boost the performance of models, we propose an ERC-oriented hybrid\ncurriculum learning framework. Our framework consists of two curricula: (1)\nconversation-level curriculum (CC); and (2) utterance-level curriculum (UC). In\nCC, we construct a difficulty measurer based on \"emotion shift\" frequency\nwithin a conversation, then the conversations are scheduled in an \"easy to\nhard\" schema according to the difficulty score returned by the difficulty\nmeasurer. For UC, it is implemented from an emotion-similarity perspective,\nwhich progressively strengthens the model's ability in identifying the\nconfusing emotions. With the proposed model-agnostic hybrid curriculum learning\nstrategy, we observe significant performance boosts over a wide range of\nexisting ERC models and we are able to achieve new state-of-the-art results on\nfour public ERC datasets.", "published": "2021-12-22 08:02:58", "link": "http://arxiv.org/abs/2112.11718v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On Theoretical Complexity and Boolean Satisfiability", "abstract": "Theoretical complexity is a vital subfield of computer science that enables\nus to mathematically investigate computation and answer many interesting\nqueries about the nature of computational problems. It provides theoretical\ntools to assess time and space requirements of computations along with\nassessing the difficultly of problems - classifying them accordingly. It also\ngarners at its core one of the most important problems in mathematics, namely,\nthe $\\textbf{P vs. NP}$ millennium problem. In essence, this problem asks\nwhether solution and verification reside on two different levels of difficulty.\nIn this thesis, we introduce some of the most central concepts in the Theory of\nComputing, giving an overview of how computation can be abstracted using Turing\nmachines. Further, we introduce the two most famous problem complexity classes\n$\\textbf{P}$ and $\\textbf{NP}$ along with the relationship between them. In\naddition, we explicate the concept of problem reduction and how it is an\nessential tool for making hardness comparisons between different problems.\nLater, we present the problem of Boolean Satisfiability (SAT) which lies at the\ncenter of NP-complete problems. We then explore some of its tractable as well\nas intractable variants such as Horn-SAT and 3-SAT, respectively. Last but not\nleast, we establish polynomial-time reductions from 3-SAT to some of the famous\nNP-complete graph problems, namely, Clique Finding, Hamiltonian Cycle Finding,\nand 3-Coloring.", "published": "2021-12-22 10:13:34", "link": "http://arxiv.org/abs/2112.11769v1", "categories": ["cs.CC", "cs.CL"], "primary_category": "cs.CC"}
{"title": "The Importance of the Current Input in Sequence Modeling", "abstract": "The last advances in sequence modeling are mainly based on deep learning\napproaches. The current state of the art involves the use of variations of the\nstandard LSTM architecture, combined with several tricks that improve the final\nprediction rates of the trained neural networks. However, in some cases, these\nadaptations might be too much tuned to the particular problems being addressed.\nIn this article, we show that a very simple idea, to add a direct connection\nbetween the input and the output, skipping the recurrent module, leads to an\nincrease of the prediction accuracy in sequence modeling problems related to\nnatural language processing. Experiments carried out on different problems show\nthat the addition of this kind of connection to a recurrent network always\nimproves the results, regardless of the architecture and training-specific\ndetails. When this idea is introduced into the models that lead the field, the\nresulting networks achieve a new state-of-the-art perplexity in language\nmodeling problems.", "published": "2021-12-22 10:29:20", "link": "http://arxiv.org/abs/2112.11776v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Quantifying Gender Biases Towards Politicians on Reddit", "abstract": "Despite attempts to increase gender parity in politics, global efforts have\nstruggled to ensure equal female representation. This is likely tied to\nimplicit gender biases against women in authority. In this work, we present a\ncomprehensive study of gender biases that appear in online political\ndiscussion. To this end, we collect 10 million comments on Reddit in\nconversations about male and female politicians, which enables an exhaustive\nstudy of automatic gender bias detection. We address not only misogynistic\nlanguage, but also other manifestations of bias, like benevolent sexism in the\nform of seemingly positive sentiment and dominance attributed to female\npoliticians, or differences in descriptor attribution. Finally, we conduct a\nmulti-faceted study of gender bias towards politicians investigating both\nlinguistic and extra-linguistic cues. We assess 5 different types of gender\nbias, evaluating coverage, combinatorial, nominal, sentimental, and lexical\nbiases extant in social media language and discourse. Overall, we find that,\ncontrary to previous research, coverage and sentiment biases suggest equal\npublic interest in female politicians. Rather than overt hostile or benevolent\nsexism, the results of the nominal and lexical analyses suggest this interest\nis not as professional or respectful as that expressed about male politicians.\nFemale politicians are often named by their first names and are described in\nrelation to their body, clothing, or family; this is a treatment that is not\nsimilarly extended to men. On the now banned far-right subreddits, this\ndisparity is greatest, though differences in gender biases still appear in the\nright and left-leaning subreddits. We release the curated dataset to the public\nfor future studies.", "published": "2021-12-22 16:39:14", "link": "http://arxiv.org/abs/2112.12014v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "VoiceMoji: A Novel On-Device Pipeline for Seamless Emoji Insertion in\n  Dictation", "abstract": "Most of the speech recognition systems recover only words in the speech and\nfail to capture emotions. Users have to manually add emoji(s) in text for\nadding tone and making communication fun. Though there is much work done on\npunctuation addition on transcribed speech, the area of emotion addition is\nuntouched. In this paper, we propose a novel on-device pipeline to enrich the\nvoice input experience. It involves, given a blob of transcribed text,\nintelligently processing and identifying structure where emoji insertion makes\nsense. Moreover, it includes semantic text analysis to predict emoji for each\nof the sub-parts for which we propose a novel architecture Attention-based Char\nAware (ACA) LSTM which handles Out-Of-Vocabulary (OOV) words as well. All these\ntasks are executed completely on-device and hence can aid on-device dictation\nsystems. To the best of our knowledge, this is the first work that shows how to\nadd emoji(s) in the transcribed text. We demonstrate that our components\nachieve comparable results to previous neural approaches for punctuation\naddition and emoji prediction with 80% fewer parameters. Overall, our proposed\nmodel has a very small memory footprint of a mere 4MB to suit on-device\ndeployment.", "published": "2021-12-22 16:54:57", "link": "http://arxiv.org/abs/2112.12028v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Domain Adaptation with Pre-trained Transformers for Query Focused\n  Abstractive Text Summarization", "abstract": "The Query Focused Text Summarization (QFTS) task aims at building systems\nthat generate the summary of the text document(s) based on the given query. A\nkey challenge in addressing this task is the lack of large labeled data for\ntraining the summarization model. In this paper, we address this challenge by\nexploring a series of domain adaptation techniques. Given the recent success of\npre-trained transformer models in a wide range of natural language processing\ntasks, we utilize such models to generate abstractive summaries for the QFTS\ntask for both single-document and multi-document scenarios. For domain\nadaptation, we apply a variety of techniques using pre-trained\ntransformer-based summarization models including transfer learning, weakly\nsupervised learning, and distant supervision. Extensive experiments on six\ndatasets show that our proposed approach is very effective in generating\nabstractive summaries for the QFTS task while setting a new state-of-the-art\nresult in several datasets across a set of automatic and human evaluation\nmetrics.", "published": "2021-12-22 05:34:56", "link": "http://arxiv.org/abs/2112.11670v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Survey of Natural Language Generation", "abstract": "This paper offers a comprehensive review of the research on Natural Language\nGeneration (NLG) over the past two decades, especially in relation to\ndata-to-text generation and text-to-text generation deep learning methods, as\nwell as new applications of NLG technology. This survey aims to (a) give the\nlatest synthesis of deep learning research on the NLG core tasks, as well as\nthe architectures adopted in the field; (b) detail meticulously and\ncomprehensively various NLG tasks and datasets, and draw attention to the\nchallenges in NLG evaluation, focusing on different evaluation methods and\ntheir relationships; (c) highlight some future emphasis and relatively recent\nresearch issues that arise due to the increasing synergy between NLG and other\nartificial intelligence areas, such as computer vision, text and computational\ncreativity.", "published": "2021-12-22 09:08:00", "link": "http://arxiv.org/abs/2112.11739v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "STEREO: Scientific Text Reuse in Open Access Publications", "abstract": "We present the Webis-STEREO-21 dataset, a massive collection of Scientific\nText Reuse in Open-access publications. It contains more than 91 million cases\nof reused text passages found in 4.2 million unique open-access publications.\nFeaturing a high coverage of scientific disciplines and varieties of reuse, as\nwell as comprehensive metadata to contextualize each case, our dataset\naddresses the most salient shortcomings of previous ones on scientific writing.\nWebis-STEREO-21 allows for tackling a wide range of research questions from\ndifferent scientific backgrounds, facilitating both qualitative and\nquantitative analysis of the phenomenon as well as a first-time grounding on\nthe base rate of text reuse in scientific publications.", "published": "2021-12-22 11:15:49", "link": "http://arxiv.org/abs/2112.11800v3", "categories": ["cs.DL", "cs.CL", "cs.IR"], "primary_category": "cs.DL"}
{"title": "Multimodal Analysis of memes for sentiment extraction", "abstract": "Memes are one of the most ubiquitous forms of social media communication. The\nstudy and processing of memes, which are intrinsically multimedia, is a popular\ntopic right now. The study presented in this research is based on the Memotion\ndataset, which involves categorising memes based on irony, comedy, motivation,\nand overall-sentiment. Three separate innovative transformer-based techniques\nhave been developed, and their outcomes have been thoroughly reviewed.The best\nalgorithm achieved a macro F1 score of 0.633 for humour classification, 0.55\nfor motivation classification, 0.61 for sarcasm classification, and 0.575 for\noverall sentiment of the meme out of all our techniques.", "published": "2021-12-22 12:57:05", "link": "http://arxiv.org/abs/2112.11850v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Perceptual Evaluation of 360 Audiovisual Quality and Machine Learning\n  Predictions", "abstract": "In an earlier study, we gathered perceptual evaluations of the audio, video,\nand audiovisual quality for 360 audiovisual content. This paper investigates\nperceived audiovisual quality prediction based on objective quality metrics and\nsubjective scores of 360 video and spatial audio content. Thirteen objective\nvideo quality metrics and three objective audio quality metrics were evaluated\nfor five stimuli for each coding parameter. Four regression-based machine\nlearning models were trained and tested here, i.e., multiple linear regression,\ndecision tree, random forest, and support vector machine. Each model was\nconstructed using a combination of audio and video quality metrics and two\ncross-validation methods (k-Fold and Leave-One-Out) were investigated and\nproduced 312 predictive models. The results indicate that the model based on\nthe evaluation of VMAF and AMBIQUAL is better than other combinations of\naudio-video quality metric. In this study, support vector machine provides\nhigher performance using k-Fold (PCC = 0.909, SROCC = 0.914, and RMSE = 0.416).\nThese results can provide insights for the design of multimedia quality metrics\nand the development of predictive models for audiovisual omnidirectional media.", "published": "2021-12-22 23:36:59", "link": "http://arxiv.org/abs/2112.12273v1", "categories": ["cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "Nonnegative OPLS for Supervised Design of Filter Banks: Application to\n  Image and Audio Feature Extraction", "abstract": "Audio or visual data analysis tasks usually have to deal with\nhigh-dimensional and nonnegative signals. However, most data analysis methods\nsuffer from overfitting and numerical problems when data have more than a few\ndimensions needing a dimensionality reduction preprocessing. Moreover,\ninterpretability about how and why filters work for audio or visual\napplications is a desired property, especially when energy or spectral signals\nare involved. In these cases, due to the nature of these signals, the\nnonnegativity of the filter weights is a desired property to better understand\nits working. Because of these two necessities, we propose different methods to\nreduce the dimensionality of data while the nonnegativity and interpretability\nof the solution are assured. In particular, we propose a generalized\nmethodology to design filter banks in a supervised way for applications dealing\nwith nonnegative data, and we explore different ways of solving the proposed\nobjective function consisting of a nonnegative version of the orthonormalized\npartial least-squares method. We analyze the discriminative power of the\nfeatures obtained with the proposed methods for two different and widely\nstudied applications: texture and music genre classification. Furthermore, we\ncompare the filter banks achieved by our methods with other state-of-the-art\nmethods specifically designed for feature extraction.", "published": "2021-12-22 23:58:25", "link": "http://arxiv.org/abs/2112.12280v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
