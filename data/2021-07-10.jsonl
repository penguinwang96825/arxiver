{"title": "Assessing Data Efficiency in Task-Oriented Semantic Parsing", "abstract": "Data efficiency, despite being an attractive characteristic, is often\nchallenging to measure and optimize for in task-oriented semantic parsing;\nunlike exact match, it can require both model- and domain-specific setups,\nwhich have, historically, varied widely across experiments. In our work, as a\nstep towards providing a unified solution to data-efficiency-related questions,\nwe introduce a four-stage protocol which gives an approximate measure of how\nmuch in-domain, \"target\" data a parser requires to achieve a certain quality\nbar. Specifically, our protocol consists of (1) sampling target subsets of\ndifferent cardinalities, (2) fine-tuning parsers on each subset, (3) obtaining\na smooth curve relating target subset (%) vs. exact match (%), and (4)\nreferencing the curve to mine ad-hoc (target subset, exact match) points. We\napply our protocol in two real-world case studies -- model generalizability and\nintent complexity -- illustrating its flexibility and applicability to\npractitioners in task-oriented semantic parsing.", "published": "2021-07-10 02:43:16", "link": "http://arxiv.org/abs/2107.04736v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Noise Stability Regularization for Improving BERT Fine-tuning", "abstract": "Fine-tuning pre-trained language models such as BERT has become a common\npractice dominating leaderboards across various NLP tasks. Despite its recent\nsuccess and wide adoption, this process is unstable when there are only a small\nnumber of training samples available. The brittleness of this process is often\nreflected by the sensitivity to random seeds. In this paper, we propose to\ntackle this problem based on the noise stability property of deep nets, which\nis investigated in recent literature (Arora et al., 2018; Sanyal et al., 2020).\nSpecifically, we introduce a novel and effective regularization method to\nimprove fine-tuning on NLP tasks, referred to as Layer-wise Noise Stability\nRegularization (LNSR). We extend the theories about adding noise to the input\nand prove that our method gives a stabler regularization effect. We provide\nsupportive evidence by experimentally confirming that well-performing models\nshow a low sensitivity to noise and fine-tuning with LNSR exhibits clearly\nhigher generalizability and stability. Furthermore, our method also\ndemonstrates advantages over other state-of-the-art algorithms including L2-SP\n(Li et al., 2018), Mixout (Lee et al., 2020) and SMART (Jiang et al., 2020).", "published": "2021-07-10 13:19:04", "link": "http://arxiv.org/abs/2107.04835v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PatentMiner: Patent Vacancy Mining via Context-enhanced and\n  Knowledge-guided Graph Attention", "abstract": "Although there are a small number of work to conduct patent research by\nbuilding knowledge graph, but without constructing patent knowledge graph using\npatent documents and combining latest natural language processing methods to\nmine hidden rich semantic relationships in existing patents and predict new\npossible patents. In this paper, we propose a new patent vacancy prediction\napproach named PatentMiner to mine rich semantic knowledge and predict new\npotential patents based on knowledge graph (KG) and graph attention mechanism.\nFirstly, patent knowledge graph over time (e.g. year) is constructed by\ncarrying out named entity recognition and relation extrac-tion from patent\ndocuments. Secondly, Common Neighbor Method (CNM), Graph Attention Networks\n(GAT) and Context-enhanced Graph Attention Networks (CGAT) are proposed to\nperform link prediction in the constructed knowledge graph to dig out the\npotential triples. Finally, patents are defined on the knowledge graph by means\nof co-occurrence relationship, that is, each patent is represented as a fully\nconnected subgraph containing all its entities and co-occurrence relationships\nof the patent in the knowledge graph; Furthermore, we propose a new patent\nprediction task which predicts a fully connected subgraph with newly added\nprediction links as a new pa-tent. The experimental results demonstrate that\nour proposed patent predic-tion approach can correctly predict new patents and\nContext-enhanced Graph Attention Networks is much better than the baseline.\nMeanwhile, our proposed patent vacancy prediction task still has significant\nroom to im-prove.", "published": "2021-07-10 17:34:57", "link": "http://arxiv.org/abs/2107.04880v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Computational Paremiology: Charting the temporal, ecological dynamics of\n  proverb use in books, news articles, and tweets", "abstract": "Proverbs are an essential component of language and culture, and though much\nattention has been paid to their history and currency, there has been\ncomparatively little quantitative work on changes in the frequency with which\nthey are used over time. With wider availability of large corpora reflecting\nmany diverse genres of documents, it is now possible to take a broad and\ndynamic view of the importance of the proverb. Here, we measure temporal\nchanges in the relevance of proverbs within three corpora, differing in kind,\nscale, and time frame: Millions of books over centuries; hundreds of millions\nof news articles over twenty years; and billions of tweets over a decade. We\nfind that proverbs present heavy-tailed frequency-of-usage rank distributions\nin each venue; exhibit trends reflecting the cultural dynamics of the eras\ncovered; and have evolved into contemporary forms on social media.", "published": "2021-07-10 23:41:01", "link": "http://arxiv.org/abs/2107.04929v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Formal context reduction in deriving concept hierarchies from corpora\n  using adaptive evolutionary clustering algorithm star", "abstract": "It is beneficial to automate the process of deriving concept hierarchies from\ncorpora since a manual construction of concept hierarchies is typically a\ntime-consuming and resource-intensive process. As such, the overall process of\nlearning concept hierarchies from corpora encompasses a set of steps: parsing\nthe text into sentences, splitting the sentences and then tokenising it. After\nthe lemmatisation step, the pairs are extracted using FCA. However, there might\nbe some uninteresting and erroneous pairs in the formal context. Generating\nformal context may lead to a time-consuming process, so formal context size\nreduction is required to remove uninterested and erroneous pairs, taking less\ntime to extract the concept lattice and concept hierarchies accordingly. In\nthis premise, this study aims to propose two frameworks: (1) A framework to\nreview the current process of deriving concept hierarchies from corpus\nutilising FCA; (2) A framework to decrease the formal contexts ambiguity of the\nfirst framework using an adaptive version of ECA*. Experiments are conducted by\napplying 385 sample corpora from Wikipedia on the two frameworks to examine the\nreducing size of formal context, which leads to yield concept lattice and\nconcept hierarchy. The resulting lattice of formal context is evaluated to the\nstandard one using concept lattice-invariants. Accordingly, the homomorphic\nbetween the two lattices preserves the quality of resulting concept hierarchies\nby 89% in contrast to the basic ones, and the reduced concept lattice inherits\nthe structural relation of the standard one. The adaptive ECA* is examined\nagainst its four counterpart baseline algorithms to measure the execution time\non random datasets with different densities (fill ratios). The results show\nthat adaptive ECA* performs concept lattice faster than other mentioned\ncompetitive techniques in different fill ratios.", "published": "2021-07-10 07:18:03", "link": "http://arxiv.org/abs/2107.04781v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Read, Attend, and Code: Pushing the Limits of Medical Codes Prediction\n  from Clinical Notes by Machines", "abstract": "Prediction of medical codes from clinical notes is both a practical and\nessential need for every healthcare delivery organization within current\nmedical systems. Automating annotation will save significant time and excessive\neffort spent by human coders today. However, the biggest challenge is directly\nidentifying appropriate medical codes out of several thousands of\nhigh-dimensional codes from unstructured free-text clinical notes. In the past\nthree years, with Convolutional Neural Networks (CNN) and Long Short-Term\nMemory (LTSM) networks, there have been vast improvements in tackling the most\nchallenging benchmark of the MIMIC-III-full-label inpatient clinical notes\ndataset. This progress raises the fundamental question of how far automated\nmachine learning (ML) systems are from human coders' working performance. We\nassessed the baseline of human coders' performance on the same subsampled\ntesting set. We also present our Read, Attend, and Code (RAC) model for\nlearning the medical code assignment mappings. By connecting convolved\nembeddings with self-attention and code-title guided attention modules,\ncombined with sentence permutation-based data augmentations and stochastic\nweight averaging training, RAC establishes a new state of the art (SOTA),\nconsiderably outperforming the current best Macro-F1 by 18.7%, and reaches past\nthe human-level coding baseline. This new milestone marks a meaningful step\ntoward fully autonomous medical coding (AMC) in machines reaching parity with\nhuman coders' performance in medical code prediction.", "published": "2021-07-10 06:01:58", "link": "http://arxiv.org/abs/2107.10650v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Layer-wise Analysis of a Self-supervised Speech Representation Model", "abstract": "Recently proposed self-supervised learning approaches have been successful\nfor pre-training speech representation models. The utility of these learned\nrepresentations has been observed empirically, but not much has been studied\nabout the type or extent of information encoded in the pre-trained\nrepresentations themselves. Developing such insights can help understand the\ncapabilities and limits of these models and enable the research community to\nmore efficiently develop their usage for downstream applications. In this work,\nwe begin to fill this gap by examining one recent and successful pre-trained\nmodel (wav2vec 2.0), via its intermediate representation vectors, using a suite\nof analysis tools. We use the metrics of canonical correlation, mutual\ninformation, and performance on simple downstream tasks with non-parametric\nprobes, in order to (i) query for acoustic and linguistic information content,\n(ii) characterize the evolution of information across model layers, and (iii)\nunderstand how fine-tuning the model for automatic speech recognition (ASR)\naffects these observations. Our findings motivate modifying the fine-tuning\nprotocol for ASR, which produces improved word error rates in a low-resource\nsetting.", "published": "2021-07-10 02:13:25", "link": "http://arxiv.org/abs/2107.04734v3", "categories": ["cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Variational Information Bottleneck for Effective Low-resource Audio\n  Classification", "abstract": "Large-scale deep neural networks (DNNs) such as convolutional neural networks\n(CNNs) have achieved impressive performance in audio classification for their\npowerful capacity and strong generalization ability. However, when training a\nDNN model on low-resource tasks, it is usually prone to overfitting the small\ndata and learning too much redundant information. To address this issue, we\npropose to use variational information bottleneck (VIB) to mitigate overfitting\nand suppress irrelevant information. In this work, we conduct experiments ona\n4-layer CNN. However, the VIB framework is ready-to-use and could be easily\nutilized with many other state-of-the-art network architectures. Evaluation on\na few audio datasets shows that our approach significantly outperforms baseline\nmethods, yielding more than 5.0% improvement in terms of classification\naccuracy in some low-source settings.", "published": "2021-07-10 09:44:17", "link": "http://arxiv.org/abs/2107.04803v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speech2Video: Cross-Modal Distillation for Speech to Video Generation", "abstract": "This paper investigates a novel task of talking face video generation solely\nfrom speeches. The speech-to-video generation technique can spark interesting\napplications in entertainment, customer service, and human-computer-interaction\nindustries. Indeed, the timbre, accent and speed in speeches could contain rich\ninformation relevant to speakers' appearance. The challenge mainly lies in\ndisentangling the distinct visual attributes from audio signals. In this\narticle, we propose a light-weight, cross-modal distillation method to extract\ndisentangled emotional and identity information from unlabelled video inputs.\nThe extracted features are then integrated by a generative adversarial network\ninto talking face video clips. With carefully crafted discriminators, the\nproposed framework achieves realistic generation results. Experiments with\nobserved individuals demonstrated that the proposed framework captures the\nemotional expressions solely from speeches, and produces spontaneous facial\nmotion in the video output. Compared to the baseline method where speeches are\ncombined with a static image of the speaker, the results of the proposed\nframework is almost indistinguishable. User studies also show that the proposed\nmethod outperforms the existing algorithms in terms of emotion expression in\nthe generated videos.", "published": "2021-07-10 10:27:26", "link": "http://arxiv.org/abs/2107.04806v1", "categories": ["cs.SD", "cs.CV", "eess.AS", "eess.IV"], "primary_category": "cs.SD"}
{"title": "Weakly-Supervised Classification and Detection of Bird Sounds in the\n  Wild. A BirdCLEF 2021 Solution", "abstract": "It is easier to hear birds than see them, however, they still play an\nessential role in nature and they are excellent indicators of deteriorating\nenvironmental quality and pollution. Recent advances in Machine Learning and\nConvolutional Neural Networks allow us to detect and classify bird sounds, by\ndoing this, we can assist researchers in monitoring the status and trends of\nbird populations and biodiversity in ecosystems. We propose a sound detection\nand classification pipeline for analyzing complex soundscape recordings and\nidentify birdcalls in the background. Our pipeline learns from weak labels,\nclassifies fine-grained bird vocalizations in the wild, and is robust against\nbackground sounds (e.g., airplanes, rain, etc). Our solution achieved 10th\nplace of 816 teams at the BirdCLEF 2021 Challenge hosted on Kaggle.", "published": "2021-07-10 17:11:44", "link": "http://arxiv.org/abs/2107.04878v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
