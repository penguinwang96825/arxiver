{"title": "Restricted CSPs and F-free Digraph Algorithmics", "abstract": "In recent years, much attention has been placed on the complexity of graph\nhomomorphism problems when the input is restricted to ${\\mathbb P}_k$-free and\n${\\mathbb P}_k$-subgraph-free graphs. We consider the directed version of this\nresearch line, by addressing the questions, is it true that digraph\nhomomorphism problems CSP$({\\mathbb H})$ have a P versus NP-complete dichotomy\nwhen the input is restricted to $\\vec{\\mathbb P}_k$-free (resp.\\ $\\vec{\\mathbb\nP}_k$-subgraph-free) digraphs? Our main contribution in this direction shows\nthat if CSP$({\\mathbb H})$ is NP-complete, then there is a positive integer $N$\nsuch that CSP$({\\mathbb H})$ remains NP-hard even for $\\vec{\\mathbb\nP}_N$-subgraph-free digraphs. Moreover, it remains NP-hard for acyclic\n$\\vec{\\mathbb P}_N$-subgraph-free digraphs, and becomes polynomial-time\nsolvable for $\\vec{\\mathbb P}_{N-1}$-subgraph-free acyclic digraphs. We then\nverify the questions above for digraphs on three vertices and a family of\nsmooth tournaments. We prove these results by establishing a connection between\n$\\mathbb F$-(subgraph)-free algorithmics and constraint satisfaction theory. On\nthe way, we introduce restricted CSPs, i.e., problems of the form CSP$({\\mathbb\nH})$ restricted to yes-instances of CSP$({\\mathbb H}')$ -- these were called\nrestricted homomorphism problems by Hell and Ne\\v{s}et\\v{r}il. Another main\nresult of this paper presents a P versus NP-complete dichotomy for these\nproblems. Moreover, this complexity dichotomy is accompanied by an algebraic\ndichotomy in the spirit of the finite domain CSP dichotomy.", "published": "2025-02-24 19:27:02", "link": "http://arxiv.org/abs/2502.17596v1", "categories": ["cs.CC", "cs.DM", "cs.LO", "math.CO"], "primary_category": "cs.CC"}
{"title": "Optimal placement of mobile distance-limited devices for line routing\\", "abstract": "A segment (barrier) is specified on the plane, as well as depots, where the\nmobile devices (drones) can be placed. Each drone departs from its depot to the\nbarrier, moves along the barrier and returns to its depot, traveling a path of\na limited length. The part of the barrier along which the drone moved is\n\\emph{covered} by this sensor. It is required to place a limited quantity of\ndrones in the depots and determine the trajectory of each drone in such a way\nthat the barrier is covered, and the total length of the paths traveled by the\ndrones is minimal.\n  Previously, this problem was considered for an unlimited number of drones. If\neach drone covers a segment of length at least 1, then the time complexity of\nthe proposed algorithm was $O(mL^3)$, where $m$ is the number of depots and $L$\nis the length of the barrier. In this paper, we generalize the problem by\nintroducing an upper bound $n$ on the number of drones, and propose a new\nalgorithm with time complexity equals $O(mnL^2)$. Since each drone covers a\nsegment of length at least 1, then $n\\leq L$ and $O(mnL^2)\\leq O(mL^3)$.\nAssuming an unlimited number of drones, as investigated in our prior work, we\npresent an $O(mL^2)$-time algorithm, achieving an $L$-fold reduction compared\nto previous methods. Here, the algorithm has a time complexity that equals\n$O(L^2)$, and the most time-consuming is preprocessing.", "published": "2025-02-24 05:03:06", "link": "http://arxiv.org/abs/2502.16844v1", "categories": ["cs.DM"], "primary_category": "cs.DM"}
{"title": "Event-Based Limit Order Book Simulation under a Neural Hawkes Process: Application in Market-Making", "abstract": "In this paper, we propose an event-driven Limit Order Book (LOB) model that\ncaptures twelve of the most observed LOB events in exchange-based financial\nmarkets. To model these events, we propose using the state-of-the-art Neural\nHawkes process, a more robust alternative to traditional Hawkes process models.\nMore specifically, this model captures the dynamic relationships between\ndifferent event types, particularly their long- and short-term interactions,\nusing a Long Short-Term Memory neural network. Using this framework, we\nconstruct a midprice process that captures the event-driven behavior of the LOB\nby simulating high-frequency dynamics like how they appear in real financial\nmarkets. The empirical results show that our model captures many of the broader\ncharacteristics of the price fluctuations, particularly in terms of their\noverall volatility. We apply this LOB simulation model within a Deep\nReinforcement Learning Market-Making framework, where the trading agent can now\ncomplete trade order fills in a manner that closely resembles real-market trade\nexecution. Here, we also compare the results of the simulated model with those\nfrom real data, highlighting how the overall performance and the distribution\nof trade order fills closely align with the same analysis on real data.", "published": "2025-02-24 18:50:29", "link": "http://arxiv.org/abs/2502.17417v1", "categories": ["q-fin.CP", "q-fin.MF"], "primary_category": "q-fin.CP"}
{"title": "Decoding Financial Health in Kenyas' Medical Insurance Sector: A Data-Driven Cluster Analysis", "abstract": "This study examines insurance companies' financial performance and reporting\ntrends within the medical sector using advanced clustering techniques to\nidentify distinct patterns. Four clusters were identified by analyzing\nfinancial ratios and time series data, each representing unique financial\nperformance and reporting consistency combinations. Dynamic Time Warping (DTW)\nand KMeans clustering were employed to capture temporal variations and uncover\nkey insights into company behaviors. The findings reveal that resilient\nperformers consistently report and have financial stability, making them\nreliable options for policyholders. In contrast, clusters of underperforming\ncompanies and those with reporting gaps highlight operational challenges and\nissues related to data consistency. These insights emphasize the importance of\ntransparency and timely reporting to ensure the sector's resilience. This study\ncontributes to the literature by integrating time series analysis into\nfinancial clustering, offering practical recommendations for improving data\ngovernance and financial stability in the insurance sector. Future research\ncould further investigate non-financial indicators and explore alternative\nclustering methods to provide a deeper understanding of performance dynamics.", "published": "2025-02-24 11:36:04", "link": "http://arxiv.org/abs/2502.17072v2", "categories": ["q-fin.CP"], "primary_category": "q-fin.CP"}
{"title": "Predicting Liquidity-Aware Bond Yields using Causal GANs and Deep Reinforcement Learning with LLM Evaluation", "abstract": "Financial bond yield forecasting is challenging due to data scarcity,\nnonlinear macroeconomic dependencies, and evolving market conditions. In this\npaper, we propose a novel framework that leverages Causal Generative\nAdversarial Networks (CausalGANs) and Soft Actor-Critic (SAC) reinforcement\nlearning (RL) to generate high-fidelity synthetic bond yield data for four\nmajor bond categories (AAA, BAA, US10Y, Junk). By incorporating 12 key\nmacroeconomic variables, we ensure statistical fidelity by preserving essential\nmarket properties. To transform this market dependent synthetic data into\nactionable insights, we employ a finetuned Large Language Model (LLM)\nQwen2.5-7B that generates trading signals (BUY/HOLD/SELL), risk assessments,\nand volatility projections. We use automated, human and LLM evaluations, all of\nwhich demonstrate that our framework improves forecasting performance over\nexisting methods, with statistical validation via predictive accuracy, MAE\nevaluation(0.103%), profit/loss evaluation (60% profit rate), LLM evaluation\n(3.37/5) and expert assessments scoring 4.67 out of 5. The reinforcement\nlearning-enhanced synthetic data generation achieves the least Mean Absolute\nError of 0.103, demonstrating its effectiveness in replicating real-world bond\nmarket dynamics. We not only enhance data-driven trading strategies but also\nprovides a scalable, high-fidelity synthetic financial data pipeline for risk &\nvolatility management and investment decision-making. This work establishes a\nbridge between synthetic data generation, LLM driven financial forecasting, and\nlanguage model evaluation, contributing to AI-driven financial decision-making.", "published": "2025-02-24 09:46:37", "link": "http://arxiv.org/abs/2502.17011v1", "categories": ["q-fin.CP", "cs.CE", "cs.CL", "cs.LG", "q-fin.PM"], "primary_category": "q-fin.CP"}
{"title": "Scaling Limits for Exponential Hedging in the Brownian Framework", "abstract": "In this paper, we consider scaling limits of exponential utility indifference\nprices for European contingent claims in the Bachelier model. We show that the\nscaling limit can be represented in terms of the \\emph{specific relative\nentropy}, and in addition we construct asymptotic optimal hedging strategies.\nTo prove the upper bound for the limit, we formulate the dual problem as a\nstochastic control, and show there exists a classical solution to its HJB\nequation. The proof for the lower bound relies on the duality result for\nexponential hedging in discrete time.", "published": "2025-02-24 14:22:49", "link": "http://arxiv.org/abs/2502.17186v1", "categories": ["math.PR", "math.OC", "q-fin.MF", "q-fin.PR"], "primary_category": "math.PR"}
{"title": "A data-driven econo-financial stress-testing framework to estimate the effect of supply chain networks on financial systemic risk", "abstract": "Supply chain disruptions constitute an often underestimated risk for\nfinancial stability. As in financial networks, systemic risks in production\nnetworks arises when the local failure of one firm impacts the production of\nothers and might trigger cascading disruptions that affect significant parts of\nthe economy. Here, we study how systemic risk in production networks translates\ninto financial systemic risk through a mechanism where supply chain contagion\nleads to correlated bank-firm loan defaults. We propose a financial\nstress-testing framework for micro- and macro-prudential applications that\nfeatures a national firm level supply chain network in combination with\ninterbank network layers. The model is calibrated by using a unique data set\nincluding about 1 million firm-level supply links, practically all bank-firm\nloans, and all interbank loans in a small European economy. As a showcase we\nimplement a real COVID-19 shock scenario on the firm level. This model allows\nus to study how the disruption dynamics in the real economy can lead to\ninterbank solvency contagion dynamics. We estimate to what extent this\namplifies financial systemic risk. We discuss the relative importance of these\ncontagion channels and find an increase of interbank contagion by 70% when\nproduction network contagion is present. We then examine the financial systemic\nrisk firms bring to banks and find an increase of up to 28% in the presence of\nthe interbank contagion channel. This framework is the first financial systemic\nrisk model to take agent-level dynamics of the production network and shocks of\nthe real economy into account which opens a path for directly, and event-driven\nunderstanding of the dynamical interaction between the real economy and\nfinancial systems.", "published": "2025-02-24 10:52:38", "link": "http://arxiv.org/abs/2502.17044v1", "categories": ["q-fin.ST", "econ.GN", "q-fin.EC"], "primary_category": "q-fin.ST"}
{"title": "Entailment-Preserving First-order Logic Representations in Natural\n  Language Entailment", "abstract": "First-order logic (FOL) can represent the logical entailment semantics of\nnatural language (NL) sentences, but determining natural language entailment\nusing FOL remains a challenge. To address this, we propose the\nEntailment-Preserving FOL representations (EPF) task and introduce\nreference-free evaluation metrics for EPF, the Entailment-Preserving Rate (EPR)\nfamily. In EPF, one should generate FOL representations from multi-premise\nnatural language entailment data (e.g. EntailmentBank) so that the automatic\nprover's result preserves the entailment labels. Experiments show that existing\nmethods for NL-to-FOL translation struggle in EPF. To this extent, we propose a\ntraining method specialized for the task, iterative learning-to-rank, which\ndirectly optimizes the model's EPR score through a novel scoring function and a\nlearning-to-rank objective. Our method achieves a 1.8-2.7% improvement in EPR\nand a 17.4-20.6% increase in EPR@16 compared to diverse baselines in three\ndatasets. Further analyses reveal that iterative learning-to-rank effectively\nsuppresses the arbitrariness of FOL representation by reducing the diversity of\npredicate signatures, and maintains strong performance across diverse inference\ntypes and out-of-domain data.", "published": "2025-02-24 00:18:17", "link": "http://arxiv.org/abs/2502.16757v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Model Fine-Tuning on Scaled Survey Data for Predicting\n  Distributions of Public Opinions", "abstract": "Large language models (LLMs) present novel opportunities in public opinion\nresearch by predicting survey responses in advance during the early stages of\nsurvey design. Prior methods steer LLMs via descriptions of subpopulations as\nLLMs' input prompt, yet such prompt engineering approaches have struggled to\nfaithfully predict the distribution of survey responses from human subjects. In\nthis work, we propose directly fine-tuning LLMs to predict response\ndistributions by leveraging unique structural characteristics of survey data.\nTo enable fine-tuning, we curate SubPOP, a significantly scaled dataset of\n3,362 questions and 70K subpopulation-response pairs from well-established\npublic opinion surveys. We show that fine-tuning on SubPOP greatly improves the\nmatch between LLM predictions and human responses across various\nsubpopulations, reducing the LLM-human gap by up to 46% compared to baselines,\nand achieves strong generalization to unseen surveys and subpopulations. Our\nfindings highlight the potential of survey-based fine-tuning to improve opinion\nprediction for diverse, real-world subpopulations and therefore enable more\nefficient survey designs. Our code is available at\nhttps://github.com/JosephJeesungSuh/subpop.", "published": "2025-02-24 00:31:33", "link": "http://arxiv.org/abs/2502.16761v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ATEB: Evaluating and Improving Advanced NLP Tasks for Text Embedding\n  Models", "abstract": "Traditional text embedding benchmarks primarily evaluate embedding models'\ncapabilities to capture semantic similarity. However, more advanced NLP tasks\nrequire a deeper understanding of text, such as safety and factuality. These\ntasks demand an ability to comprehend and process complex information, often\ninvolving the handling of sensitive content, or the verification of factual\nstatements against reliable sources. We introduce a new benchmark designed to\nassess and highlight the limitations of embedding models trained on existing\ninformation retrieval data mixtures on advanced capabilities, which include\nfactuality, safety, instruction following, reasoning and document-level\nunderstanding. This benchmark includes a diverse set of tasks that simulate\nreal-world scenarios where these capabilities are critical and leads to\nidentification of the gaps of the currently advanced embedding models.\nFurthermore, we propose a novel method that reformulates these various tasks as\nretrieval tasks. By framing tasks like safety or factuality classification as\nretrieval problems, we leverage the strengths of retrieval models in capturing\nsemantic relationships while also pushing them to develop a deeper\nunderstanding of context and content. Using this approach with single-task\nfine-tuning, we achieved performance gains of 8\\% on factuality classification\nand 13\\% on safety classification. Our code and data will be publicly\navailable.", "published": "2025-02-24 01:08:15", "link": "http://arxiv.org/abs/2502.16766v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Hybrid Approach to Information Retrieval and Answer Generation for\n  Regulatory Texts", "abstract": "Regulatory texts are inherently long and complex, presenting significant\nchallenges for information retrieval systems in supporting regulatory officers\nwith compliance tasks. This paper introduces a hybrid information retrieval\nsystem that combines lexical and semantic search techniques to extract relevant\ninformation from large regulatory corpora. The system integrates a fine-tuned\nsentence transformer model with the traditional BM25 algorithm to achieve both\nsemantic precision and lexical coverage. To generate accurate and comprehensive\nresponses, retrieved passages are synthesized using Large Language Models\n(LLMs) within a Retrieval Augmented Generation (RAG) framework. Experimental\nresults demonstrate that the hybrid system significantly outperforms standalone\nlexical and semantic approaches, with notable improvements in Recall@10 and\nMAP@10. By openly sharing our fine-tuned model and methodology, we aim to\nadvance the development of robust natural language processing tools for\ncompliance-driven applications in regulatory domains.", "published": "2025-02-24 01:16:16", "link": "http://arxiv.org/abs/2502.16767v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MultiOCR-QA: Dataset for Evaluating Robustness of LLMs in Question\n  Answering on Multilingual OCR Texts", "abstract": "Optical Character Recognition (OCR) plays a crucial role in digitizing\nhistorical and multilingual documents, yet OCR errors -- imperfect extraction\nof the text, including character insertion, deletion and permutation -- can\nsignificantly impact downstream tasks like question-answering (QA). In this\nwork, we introduce a multilingual QA dataset MultiOCR-QA, designed to analyze\nthe effects of OCR noise on QA systems' performance. The MultiOCR-QA dataset\ncomprises 60K question-answer pairs covering three languages, English, French,\nand German. The dataset is curated from OCR-ed old documents, allowing for the\nevaluation of OCR-induced challenges on question answering. We evaluate\nMultiOCR-QA on various levels and types of OCR errors to access the robustness\nof LLMs in handling real-world digitization errors. Our findings show that QA\nsystems are highly prone to OCR induced errors and exhibit performance\ndegradation on noisy OCR text.", "published": "2025-02-24 02:16:37", "link": "http://arxiv.org/abs/2502.16781v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are Large Language Models Good Data Preprocessors?", "abstract": "High-quality textual training data is essential for the success of multimodal\ndata processing tasks, yet outputs from image captioning models like BLIP and\nGIT often contain errors and anomalies that are difficult to rectify using\nrule-based methods. While recent work addressing this issue has predominantly\nfocused on using GPT models for data preprocessing on relatively simple public\ndatasets, there is a need to explore a broader range of Large Language Models\n(LLMs) and tackle more challenging and diverse datasets.\n  In this study, we investigate the use of multiple LLMs, including LLaMA 3.1\n70B, GPT-4 Turbo, and Sonnet 3.5 v2, to refine and clean the textual outputs of\nBLIP and GIT. We assess the impact of LLM-assisted data cleaning by comparing\ndownstream-task (SemEval 2024 Subtask \"Multilabel Persuasion Detection in\nMemes\") models trained on cleaned versus non-cleaned data. While our\nexperimental results show improvements when using LLM-cleaned captions,\nstatistical tests reveal that most of these improvements are not significant.\nThis suggests that while LLMs have the potential to enhance data cleaning and\nrepairing, their effectiveness may be limited depending on the context they are\napplied to, the complexity of the task, and the level of noise in the text.\n  Our findings highlight the need for further research into the capabilities\nand limitations of LLMs in data preprocessing pipelines, especially when\ndealing with challenging datasets, contributing empirical evidence to the\nongoing discussion about integrating LLMs into data preprocessing pipelines.", "published": "2025-02-24 02:57:21", "link": "http://arxiv.org/abs/2502.16790v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CoT2Align: Cross-Chain of Thought Distillation via Optimal Transport\n  Alignment for Language Models with Different Tokenizers", "abstract": "Large Language Models (LLMs) achieve state-of-the-art performance across\nvarious NLP tasks but face deployment challenges due to high computational\ncosts and memory constraints. Knowledge distillation (KD) is a promising\nsolution, transferring knowledge from large teacher models to smaller student\nmodels. However, existing KD methods often assume shared vocabularies and\ntokenizers, limiting their flexibility. While approaches like Universal Logit\nDistillation (ULD) and Dual-Space Knowledge Distillation (DSKD) address\nvocabulary mismatches, they overlook the critical \\textbf{reasoning-aware\ndistillation} aspect. To bridge this gap, we propose CoT2Align a universal KD\nframework that integrates Chain-of-Thought (CoT) augmentation and introduces\nCross-CoT Alignment to enhance reasoning transfer. Additionally, we extend\nOptimal Transport beyond token-wise alignment to a sequence-level and\nlayer-wise alignment approach that adapts to varying sequence lengths while\npreserving contextual integrity. Comprehensive experiments demonstrate that\nCoT2Align outperforms existing KD methods across different vocabulary settings,\nimproving reasoning capabilities and robustness in domain-specific tasks.", "published": "2025-02-24 03:30:29", "link": "http://arxiv.org/abs/2502.16806v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Finding the Sweet Spot: Preference Data Construction for Scaling\n  Preference Optimization", "abstract": "Iterative data generation and model retraining are widely used to align large\nlanguage models (LLMs). It typically involves a policy model to generate\non-policy responses and a reward model to guide training data selection. Direct\nPreference Optimization (DPO) further enhances this process by constructing\npreference pairs of chosen and rejected responses. In this work, we aim to\n\\emph{scale up} the number of on-policy samples via repeated random sampling to\nimprove alignment performance. Conventional practice selects the sample with\nthe highest reward as chosen and the lowest as rejected for DPO. However, our\nexperiments reveal that this strategy leads to a \\emph{decline} in performance\nas the sample size increases. To address this, we investigate preference data\nconstruction through the lens of underlying normal distribution of sample\nrewards. We categorize the reward space into seven representative points and\nsystematically explore all 21 ($C_7^2$) pairwise combinations. Through\nevaluations on four models using AlpacaEval 2, we find that selecting the\nrejected response at reward position $\\mu - 2\\sigma$ rather than the minimum\nreward, is crucial for optimal performance. We finally introduce a scalable\npreference data construction strategy that consistently enhances model\nperformance as the sample scale increases.", "published": "2025-02-24 04:22:57", "link": "http://arxiv.org/abs/2502.16825v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "REGen: A Reliable Evaluation Framework for Generative Event Argument\n  Extraction", "abstract": "Event argument extraction identifies arguments for predefined event roles in\ntext. Traditional evaluations rely on exact match (EM), requiring predicted\narguments to match annotated spans exactly. However, this approach fails for\ngenerative models like large language models (LLMs), which produce diverse yet\nsemantically accurate responses. EM underestimates performance by disregarding\nvalid variations, implicit arguments (unstated but inferable), and scattered\narguments (distributed across a document). To bridge this gap, we introduce\nReliable Evaluation framework for Generative event argument extraction (REGen),\na framework that better aligns with human judgment. Across six datasets, REGen\nimproves performance by an average of 23.93 F1 points over EM. Human validation\nfurther confirms REGen's effectiveness, achieving 87.67% alignment with human\nassessments of argument correctness.", "published": "2025-02-24 04:49:49", "link": "http://arxiv.org/abs/2502.16838v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"Actionable Help\" in Crises: A Novel Dataset and Resource-Efficient\n  Models for Identifying Request and Offer Social Media Posts", "abstract": "During crises, social media serves as a crucial coordination tool, but the\nvast influx of posts--from \"actionable\" requests and offers to generic content\nlike emotional support, behavioural guidance, or outdated\ninformation--complicates effective classification. Although generative LLMs\n(Large Language Models) can address this issue with few-shot classification,\ntheir high computational demands limit real-time crisis response. While\nfine-tuning encoder-only models (e.g., BERT) is a popular choice, these models\nstill exhibit higher inference times in resource-constrained environments.\nMoreover, although distilled variants (e.g., DistilBERT) exist, they are not\ntailored for the crisis domain. To address these challenges, we make two key\ncontributions. First, we present CrisisHelpOffer, a novel dataset of 101k\ntweets collaboratively labelled by generative LLMs and validated by humans,\nspecifically designed to distinguish actionable content from noise. Second, we\nintroduce the first crisis-specific mini models optimized for deployment in\nresource-constrained settings. Across 13 crisis classification tasks, our mini\nmodels surpass BERT (also outperform or match the performance of RoBERTa,\nMPNet, and BERTweet), offering higher accuracy with significantly smaller sizes\nand faster speeds. The Medium model is 47% smaller with 3.8% higher accuracy at\n3.5x speed, the Small model is 68% smaller with a 1.8% accuracy gain at 7.7x\nspeed, and the Tiny model, 83% smaller, matches BERT's accuracy at 18.6x speed.\nAll models outperform existing distilled variants, setting new benchmarks.\nFinally, as a case study, we analyze social media posts from a global crisis to\nexplore help-seeking and assistance-offering behaviours in selected developing\nand developed countries.", "published": "2025-02-24 04:50:06", "link": "http://arxiv.org/abs/2502.16839v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LongAttn: Selecting Long-context Training Data via Token-level Attention", "abstract": "With the development of large language models (LLMs), there has been an\nincreasing need for significant advancements in handling long contexts. To\nenhance long-context capabilities, constructing high-quality training data with\nlong-range dependencies is crucial. Existing methods to select long-context\ndata often rely on sentence-level analysis, which can be greatly optimized in\nboth performance and efficiency. In this paper, we propose a novel token-level\nframework, LongAttn, which leverages the self-attention mechanism of LLMs to\nmeasure the long-range dependencies for the data. By calculating token-level\ndependency strength and distribution uniformity of token scores, LongAttn\neffectively quantifies long-range dependencies, enabling more accurate and\nefficient data selection. We filter LongABC-32K from open-source long-context\ndatasets (ArXiv, Book, and Code). Through our comprehensive experiments,\nLongAttn has demonstrated its excellent effectiveness, scalability, and\nefficiency. To facilitate future research in long-context data, we released our\ncode and the high-quality long-context training data LongABC-32K.", "published": "2025-02-24 05:51:53", "link": "http://arxiv.org/abs/2502.16860v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Applying LLMs to Active Learning: Towards Cost-Efficient Cross-Task Text\n  Classification without Manually Labeled Data", "abstract": "Machine learning-based classifiers have been used for text classification,\nsuch as sentiment analysis, news classification, and toxic comment\nclassification. However, supervised machine learning models often require large\namounts of labeled data for training, and manual annotation is both\nlabor-intensive and requires domain-specific knowledge, leading to relatively\nhigh annotation costs. To address this issue, we propose an approach that\nintegrates large language models (LLMs) into an active learning framework. Our\napproach combines the Robustly Optimized BERT Pretraining Approach (RoBERTa),\nGenerative Pre-trained Transformer (GPT), and active learning, achieving high\ncross-task text classification performance without the need for any manually\nlabeled data. Furthermore, compared to directly applying GPT for classification\ntasks, our approach retains over 93% of its classification performance while\nrequiring only approximately 6% of the computational time and monetary cost,\neffectively balancing performance and resource efficiency. These findings\nprovide new insights into the efficient utilization of LLMs and active learning\nalgorithms in text classification tasks, paving the way for their broader\napplication.", "published": "2025-02-24 06:43:19", "link": "http://arxiv.org/abs/2502.16892v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Make LoRA Great Again: Boosting LoRA with Adaptive Singular Values and\n  Mixture-of-Experts Optimization Alignment", "abstract": "While Low-Rank Adaptation (LoRA) enables parameter-efficient fine-tuning for\nLarge Language Models (LLMs), its performance often falls short of Full\nFine-Tuning (Full FT). Current methods optimize LoRA by initializing with\nstatic singular value decomposition (SVD) subsets, leading to suboptimal\nleveraging of pre-trained knowledge. Another path for improving LoRA is\nincorporating a Mixture-of-Experts (MoE) architecture. However, weight\nmisalignment and complex gradient dynamics make it challenging to adopt SVD\nprior to the LoRA MoE architecture. To mitigate these issues, we propose\n\\underline{G}reat L\\underline{o}R\\underline{A} Mixture-of-Exper\\underline{t}\n(GOAT), a framework that (1) adaptively integrates relevant priors using an\nSVD-structured MoE, and (2) aligns optimization with full fine-tuned MoE by\nderiving a theoretical scaling factor. We demonstrate that proper scaling,\nwithout modifying the architecture or training algorithms, boosts LoRA MoE's\nefficiency and performance. Experiments across 25 datasets, including natural\nlanguage understanding, commonsense reasoning, image classification, and\nnatural language generation, demonstrate GOAT's state-of-the-art performance,\nclosing the gap with Full FT.", "published": "2025-02-24 06:48:13", "link": "http://arxiv.org/abs/2502.16894v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AutoLogi: Automated Generation of Logic Puzzles for Evaluating Reasoning\n  Abilities of Large Language Models", "abstract": "While logical reasoning evaluation of Large Language Models (LLMs) has\nattracted significant attention, existing benchmarks predominantly rely on\nmultiple-choice formats that are vulnerable to random guessing, leading to\noverestimated performance and substantial performance fluctuations. To obtain\nmore accurate assessments of models' reasoning capabilities, we propose an\nautomated method for synthesizing open-ended logic puzzles, and use it to\ndevelop a bilingual benchmark, AutoLogi. Our approach features program-based\nverification and controllable difficulty levels, enabling more reliable\nevaluation that better distinguishes models' reasoning abilities. Extensive\nevaluation of eight modern LLMs shows that AutoLogi can better reflect true\nmodel capabilities, with performance scores spanning from 35% to 73% compared\nto the narrower range of 21% to 37% on the source multiple-choice dataset.\nBeyond benchmark creation, this synthesis method can generate high-quality\ntraining data by incorporating program verifiers into the rejection sampling\nprocess, enabling systematic enhancement of LLMs' reasoning capabilities across\ndiverse datasets.", "published": "2025-02-24 07:02:31", "link": "http://arxiv.org/abs/2502.16906v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dependency Parsing with the Structuralized Prompt Template", "abstract": "Dependency parsing is a fundamental task in natural language processing\n(NLP), aiming to identify syntactic dependencies and construct a syntactic tree\nfor a given sentence. Traditional dependency parsing models typically construct\nembeddings and utilize additional layers for prediction. We propose a novel\ndependency parsing method that relies solely on an encoder model with a\ntext-to-text training approach. To facilitate this, we introduce a structured\nprompt template that effectively captures the structural information of\ndependency trees. Our experimental results demonstrate that the proposed method\nachieves outstanding performance compared to traditional models, despite\nrelying solely on a pre-trained model. Furthermore, this method is highly\nadaptable to various pre-trained models across different target languages and\ntraining environments, allowing easy integration of task-specific features.", "published": "2025-02-24 07:25:10", "link": "http://arxiv.org/abs/2502.16919v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SS-MPC: A Sequence-Structured Multi-Party Conversation System", "abstract": "Recent Multi-Party Conversation (MPC) models typically rely on graph-based\napproaches to capture dialogue structures. However, these methods have\nlimitations, such as information loss during the projection of utterances into\nstructural embeddings and constraints in leveraging pre-trained language models\ndirectly. In this paper, we propose \\textbf{SS-MPC}, a response generation\nmodel for MPC that eliminates the need for explicit graph structures. Unlike\nexisting models that depend on graphs to analyze conversation structures,\nSS-MPC internally encodes the dialogue structure as a sequential input,\nenabling direct utilization of pre-trained language models. Experimental\nresults show that \\textbf{SS-MPC} achieves \\textbf{15.60\\% BLEU-1} and\n\\textbf{12.44\\% ROUGE-L} score, outperforming the current state-of-the-art MPC\nresponse generation model by \\textbf{3.91\\%p} in \\textbf{BLEU-1} and\n\\textbf{0.62\\%p} in \\textbf{ROUGE-L}. Additionally, human evaluation confirms\nthat SS-MPC generates more fluent and accurate responses compared to existing\nMPC models.", "published": "2025-02-24 07:25:19", "link": "http://arxiv.org/abs/2502.16920v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Benchmarking Temporal Reasoning and Alignment Across Chinese Dynasties", "abstract": "Temporal reasoning is fundamental to human cognition and is crucial for\nvarious real-world applications. While recent advances in Large Language Models\nhave demonstrated promising capabilities in temporal reasoning, existing\nbenchmarks primarily rely on rule-based construction, lack contextual depth,\nand involve a limited range of temporal entities. To address these limitations,\nwe introduce Chinese Time Reasoning (CTM), a benchmark designed to evaluate\nLLMs on temporal reasoning within the extensive scope of Chinese dynastic\nchronology. CTM emphasizes cross-entity relationships, pairwise temporal\nalignment, and contextualized and culturally-grounded reasoning, providing a\ncomprehensive evaluation. Extensive experimental results reveal the challenges\nposed by CTM and highlight potential avenues for improvement.", "published": "2025-02-24 07:27:54", "link": "http://arxiv.org/abs/2502.16922v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NUTSHELL: A Dataset for Abstract Generation from Scientific Talks", "abstract": "Scientific communication is receiving increasing attention in natural\nlanguage processing, especially to help researches access, summarize, and\ngenerate content. One emerging application in this area is Speech-to-Abstract\nGeneration (SAG), which aims to automatically generate abstracts from recorded\nscientific presentations. SAG enables researchers to efficiently engage with\nconference talks, but progress has been limited by a lack of large-scale\ndatasets. To address this gap, we introduce NUTSHELL, a novel multimodal\ndataset of *ACL conference talks paired with their corresponding abstracts. We\nestablish strong baselines for SAG and evaluate the quality of generated\nabstracts using both automatic metrics and human judgments. Our results\nhighlight the challenges of SAG and demonstrate the benefits of training on\nNUTSHELL. By releasing NUTSHELL under an open license (CC-BY 4.0), we aim to\nadvance research in SAG and foster the development of improved models and\nevaluation methods.", "published": "2025-02-24 08:11:17", "link": "http://arxiv.org/abs/2502.16942v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "All-in-one: Understanding and Generation in Multimodal Reasoning with\n  the MAIA Benchmark", "abstract": "We introduce MAIA (Multimodal AI Assessment), a native-Italian benchmark\ndesigned for fine-grained investigation of the reasoning abilities of visual\nlanguage models on videos. MAIA differs from other available video benchmarks\nfor its design, its reasoning categories, the metric it uses and the language\nand culture of the videos. It evaluates Vision Language Models (VLMs) on two\naligned tasks: a visual statement verification task, and an open-ended visual\nquestion-answering task, both on the same set of video-related questions. It\nconsiders twelve reasoning categories that aim to disentangle language and\nvision relations by highlight when one of two alone encodes sufficient\ninformation to solve the tasks, when they are both needed and when the full\nrichness of the short video is essential instead of just a part of it. Thanks\nto its carefully taught design, it evaluates VLMs' consistency and visually\ngrounded natural language comprehension and generation simultaneously through\nan aggregated metric. Last but not least, the video collection has been\ncarefully selected to reflect the Italian culture and the language data are\nproduced by native-speakers.", "published": "2025-02-24 09:25:51", "link": "http://arxiv.org/abs/2502.16989v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PrivaCI-Bench: Evaluating Privacy with Contextual Integrity and Legal\n  Compliance", "abstract": "Recent advancements in generative large language models (LLMs) have enabled\nwider applicability, accessibility, and flexibility. However, their reliability\nand trustworthiness are still in doubt, especially for concerns regarding\nindividuals' data privacy. Great efforts have been made on privacy by building\nvarious evaluation benchmarks to study LLMs' privacy awareness and robustness\nfrom their generated outputs to their hidden representations. Unfortunately,\nmost of these works adopt a narrow formulation of privacy and only investigate\npersonally identifiable information (PII). In this paper, we follow the merit\nof the Contextual Integrity (CI) theory, which posits that privacy evaluation\nshould not only cover the transmitted attributes but also encompass the whole\nrelevant social context through private information flows. We present\nPrivaCI-Bench, a comprehensive contextual privacy evaluation benchmark targeted\nat legal compliance to cover well-annotated privacy and safety regulations,\nreal court cases, privacy policies, and synthetic data built from the official\ntoolkit to study LLMs' privacy and safety compliance. We evaluate the latest\nLLMs, including the recent reasoner models QwQ-32B and Deepseek R1. Our\nexperimental results suggest that though LLMs can effectively capture key CI\nparameters inside a given context, they still require further advancements for\nprivacy compliance.", "published": "2025-02-24 10:49:34", "link": "http://arxiv.org/abs/2502.17041v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatically Evaluating the Paper Reviewing Capability of Large\n  Language Models", "abstract": "Peer review is essential for scientific progress, but it faces challenges\nsuch as reviewer shortages and growing workloads. Although Large Language\nModels (LLMs) show potential for providing assistance, research has reported\nsignificant limitations in the reviews they generate. While the insights are\nvaluable, conducting the analysis is challenging due to the considerable time\nand effort required, especially given the rapid pace of LLM developments. To\naddress the challenge, we developed an automatic evaluation pipeline to assess\nthe LLMs' paper review capability by comparing them with expert-generated\nreviews. By constructing a dataset consisting of 676 OpenReview papers, we\nexamined the agreement between LLMs and experts in their strength and weakness\nidentifications. The results showed that LLMs lack balanced perspectives,\nsignificantly overlook novelty assessment when criticizing, and produce poor\nacceptance decisions. Our automated pipeline enables a scalable evaluation of\nLLMs' paper review capability over time.", "published": "2025-02-24 12:05:27", "link": "http://arxiv.org/abs/2502.17086v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Thus Spake Long-Context Large Language Model", "abstract": "Long context is an important topic in Natural Language Processing (NLP),\nrunning through the development of NLP architectures, and offers immense\nopportunities for Large Language Models (LLMs) giving LLMs the lifelong\nlearning potential akin to humans. Unfortunately, the pursuit of a long context\nis accompanied by numerous obstacles. Nevertheless, long context remains a core\ncompetitive advantage for LLMs. In the past two years, the context length of\nLLMs has achieved a breakthrough extension to millions of tokens. Moreover, the\nresearch on long-context LLMs has expanded from length extrapolation to a\ncomprehensive focus on architecture, infrastructure, training, and evaluation\ntechnologies.\n  Inspired by the symphonic poem, Thus Spake Zarathustra, we draw an analogy\nbetween the journey of extending the context of LLM and the attempts of humans\nto transcend its mortality. In this survey, We will illustrate how LLM\nstruggles between the tremendous need for a longer context and its equal need\nto accept the fact that it is ultimately finite. To achieve this, we give a\nglobal picture of the lifecycle of long-context LLMs from four perspectives:\narchitecture, infrastructure, training, and evaluation, showcasing the full\nspectrum of long-context technologies. At the end of this survey, we will\npresent 10 unanswered questions currently faced by long-context LLMs. We hope\nthis survey can serve as a systematic introduction to the research on\nlong-context LLMs.", "published": "2025-02-24 13:19:33", "link": "http://arxiv.org/abs/2502.17129v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentiment analysis of texts from social networks based on machine\n  learning methods for monitoring public sentiment", "abstract": "A sentiment analysis system powered by machine learning was created in this\nstudy to improve real-time social network public opinion monitoring. For\nsophisticated sentiment identification, the suggested approach combines\ncutting-edge transformer-based architectures (DistilBERT, RoBERTa) with\ntraditional machine learning models (Logistic Regression, SVM, Naive Bayes).\nThe system achieved an accuracy of up to 80-85% using transformer models in\nreal-world scenarios after being tested using both deep learning techniques and\nstandard machine learning processes on annotated social media datasets.\nAccording to experimental results, deep learning models perform noticeably\nbetter than lexicon-based and conventional rule-based classifiers, lowering\nmisclassification rates and enhancing the ability to recognize nuances like\nsarcasm. According to feature importance analysis, context tokens,\nsentiment-bearing keywords, and part-of-speech structure are essential for\nprecise categorization. The findings confirm that AI-driven sentiment\nframeworks can provide a more adaptive and efficient approach to modern\nsentiment challenges. Despite the system's impressive performance, issues with\ncomputing overhead, data quality, and domain-specific terminology still exist.\nIn order to monitor opinions on a broad scale, future research will investigate\nimproving computing performance, extending coverage to various languages, and\nintegrating real-time streaming APIs. The results demonstrate that governments,\ncorporations, and social researchers looking for more in-depth understanding of\npublic mood on digital platforms can find a reliable and adaptable answer in\nAI-powered sentiment analysis.", "published": "2025-02-24 13:34:35", "link": "http://arxiv.org/abs/2502.17143v1", "categories": ["cs.CL", "68T50 (Natural Language Processing), 68T05 (Learning and Adaptive\n  Systems)", "I.2.7; I.5.1; H.3.3"], "primary_category": "cs.CL"}
{"title": "Logic Haystacks: Probing LLMs Long-Context Logical Reasoning (Without\n  Easily Identifiable Unrelated Padding)", "abstract": "Large language models demonstrate promising long context processing\ncapabilities, with recent models touting context windows close to one million\ntokens. However, the evaluations supporting these claims often involve simple\nretrieval tasks or synthetic tasks padded with irrelevant text, which the\nmodels may easily detect and discard. In this work, we generate lengthy\nsimplified English text with first-order logic representations spanning up to\n2048 clauses (around 25k GPT-4 tokens). We formulate an evaluation task with\nevidence retrieval for contradiction detection. The long, homogeneous text is\nfilled with distractors that are both hard to distinguish from relevant\nevidences and provably not interfering with them. Our evaluation of evidence\nretrieval shows that the effective context window is much smaller with\nrealistic distractors, already crumbling at 128 clauses.", "published": "2025-02-24 14:05:47", "link": "http://arxiv.org/abs/2502.17169v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Measuring Data Diversity for Instruction Tuning: A Systematic Analysis\n  and A Reliable Metric", "abstract": "Data diversity is crucial for the instruction tuning of large language\nmodels. Existing studies have explored various diversity-aware data selection\nmethods to construct high-quality datasets and enhance model performance.\nHowever, the fundamental problem of precisely defining and measuring data\ndiversity remains underexplored, limiting clear guidance for data engineering.\nTo address this, we systematically analyze 11 existing diversity measurement\nmethods by evaluating their correlation with model performance through\nextensive fine-tuning experiments. Our results indicate that a reliable\ndiversity measure should properly account for both inter-sample differences and\nthe information distribution in the sample space. Building on this, we propose\nNovelSum, a new diversity metric based on sample-level \"novelty.\" Experiments\non both simulated and real-world data show that NovelSum accurately captures\ndiversity variations and achieves a 0.97 correlation with instruction-tuned\nmodel performance, highlighting its value in guiding data engineering\npractices. With NovelSum as an optimization objective, we further develop a\ngreedy, diversity-oriented data selection strategy that outperforms existing\napproaches, validating both the effectiveness and practical significance of our\nmetric.", "published": "2025-02-24 14:20:22", "link": "http://arxiv.org/abs/2502.17184v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MULTITAT: Benchmarking Multilingual Table-and-Text Question Answering", "abstract": "Question answering on the hybrid context of tables and text (TATQA) is a\ncritical task, with broad applications in data-intensive domains. However,\nexisting TATQA datasets are limited to English, leading to several drawbacks:\n(i) They overlook the challenges of multilingual TAT-QA and cannot assess model\nperformance in the multilingual setting. (ii) They do not reflect real-world\nscenarios where tables and texts frequently appear in non-English languages. To\naddress the limitations, we propose the first multilingual TATQA dataset\n(MULTITAT). Specifically, we sample data from 3 mainstream TATQA datasets and\ntranslate it into 10 diverse languages. To align the model TATQA capabilities\nin English with other languages, we develop a baseline, Ours. Experimental\nresults reveal that the performance on non-English data in MULTITAT drops by an\naverage of 19.4% compared to English, proving the necessity of MULTITAT. We\nfurther analyze the reasons for this performance gap. Furthermore, Ours\noutperforms other baselines by an average of 3.3, demonstrating its\neffectiveness.", "published": "2025-02-24 15:34:09", "link": "http://arxiv.org/abs/2502.17253v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MonoTODia: Translating Monologue Requests to Task-Oriented Dialogues", "abstract": "Data scarcity is one of the main problems when it comes to real-world\napplications of transformer-based models. This is especially evident for\ntask-oriented dialogue (TOD) systems, which require specialized datasets, that\nare usually not readily available. This can hinder companies from adding TOD\nsystems to their services. This study therefore investigates a novel approach\nto sourcing annotated dialogues from existing German monologue material.\nFocusing on a real-world example, we investigate whether these monologues can\nbe transformed into dialogue formats suitable for training TOD systems. We show\nthe approach with the concrete example of a company specializing in travel\nbookings via e-mail. We fine-tune state-of-the-art Large Language Models for\nthe task of rewriting e-mails as dialogues and annotating them. To ensure the\nquality and validity of the generated data, we employ crowd workers to evaluate\nthe dialogues across multiple criteria and to provide gold-standard annotations\nfor the test dataset. We further evaluate the usefulness of the dialogues for\ntraining TOD systems. Our evaluation shows that the dialogues and annotations\nare of high quality and can serve as a valuable starting point for training TOD\nsystems. Finally, we make the annotated dataset publicly available to foster\nfuture research.", "published": "2025-02-24 15:51:42", "link": "http://arxiv.org/abs/2502.17268v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extracting domain-specific terms using contextual word embeddings", "abstract": "Automated terminology extraction refers to the task of extracting meaningful\nterms from domain-specific texts. This paper proposes a novel machine learning\napproach to terminology extraction, which combines features from traditional\nterm extraction systems with novel contextual features derived from contextual\nword embeddings. Instead of using a predefined list of part-of-speech patterns,\nwe first analyse a new term-annotated corpus RSDO5 for the Slovenian language\nand devise a set of rules for term candidate selection and then generate\nstatistical, linguistic and context-based features. We use a support-vector\nmachine algorithm to train a classification model, evaluate it on the four\ndomains (biomechanics, linguistics, chemistry, veterinary) of the RSDO5 corpus\nand compare the results with state-of-art term extraction approaches for the\nSlovenian language. Our approach provides significant improvements in terms of\nF1 score over the previous state-of-the-art, which proves that contextual word\nembeddings are valuable for improving term extraction.", "published": "2025-02-24 16:06:35", "link": "http://arxiv.org/abs/2502.17278v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "`Generalization is hallucination' through the lens of tensor completions", "abstract": "In this short position paper, we introduce tensor completions and artifacts\nand make the case that they are a useful theoretical framework for\nunderstanding certain types of hallucinations and generalizations in language\nmodels.", "published": "2025-02-24 16:41:38", "link": "http://arxiv.org/abs/2502.17305v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HIPPO: Enhancing the Table Understanding Capability of Large Language\n  Models through Hybrid-Modal Preference Optimization", "abstract": "Tabular data contains rich structural semantics and plays a crucial role in\norganizing and manipulating information. To better capture these structural\nsemantics, this paper introduces the HybrId-modal Preference oPtimizatiOn\n(HIPPO) model, which represents tables using both text and image, and optimizes\nMLLMs to effectively learn more comprehensive table information from these\nmultiple modalities. Specifically, HIPPO samples model responses from\nhybrid-modal table representations and designs a modality-consistent sampling\nstrategy to enhance response diversity and mitigate modality bias during DPO\ntraining. Experimental results on table question answering and table fact\nverification tasks demonstrate the effectiveness of HIPPO, achieving a 4%\nimprovement over various table reasoning models. Further analysis reveals that\nHIPPO not only enhances reasoning abilities based on unimodal table\nrepresentations but also facilitates the extraction of crucial and distinct\nsemantics from different modal representations. All data and codes are\navailable at https://github.com/NEUIR/HIPPO.", "published": "2025-02-24 16:50:55", "link": "http://arxiv.org/abs/2502.17315v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Turning Conversations into Workflows: A Framework to Extract and\n  Evaluate Dialog Workflows for Service AI Agents", "abstract": "Automated service agents require well-structured workflows to provide\nconsistent and accurate responses to customer queries. However, these workflows\nare often undocumented, and their automatic extraction from conversations\nremains unexplored. In this work, we present a novel framework for extracting\nand evaluating dialog workflows from historical interactions. Our extraction\nprocess consists of two key stages: (1) a retrieval step to select relevant\nconversations based on key procedural elements, and (2) a structured workflow\ngeneration process using a question-answer-based chain-of-thought (QA-CoT)\nprompting. To comprehensively assess the quality of extracted workflows, we\nintroduce an automated agent and customer bots simulation framework that\nmeasures their effectiveness in resolving customer issues. Extensive\nexperiments on the ABCD and SynthABCD datasets demonstrate that our QA-CoT\ntechnique improves workflow extraction by 12.16\\% in average macro accuracy\nover the baseline. Moreover, our evaluation method closely aligns with human\nassessments, providing a reliable and scalable framework for future research.", "published": "2025-02-24 16:55:15", "link": "http://arxiv.org/abs/2502.17321v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On Relation-Specific Neurons in Large Language Models", "abstract": "In large language models (LLMs), certain neurons can store distinct pieces of\nknowledge learned during pretraining. While knowledge typically appears as a\ncombination of relations and entities, it remains unclear whether some neurons\nfocus on a relation itself -- independent of any entity. We hypothesize such\nneurons detect a relation in the input text and guide generation involving such\na relation. To investigate this, we study the Llama-2 family on a chosen set of\nrelations with a statistics-based method. Our experiments demonstrate the\nexistence of relation-specific neurons. We measure the effect of selectively\ndeactivating candidate neurons specific to relation $r$ on the LLM's ability to\nhandle (1) facts whose relation is $r$ and (2) facts whose relation is a\ndifferent relation $r' \\neq r$. With respect to their capacity for encoding\nrelation information, we give evidence for the following three properties of\nrelation-specific neurons. $\\textbf{(i) Neuron cumulativity.}$ The neurons for\n$r$ present a cumulative effect so that deactivating a larger portion of them\nresults in the degradation of more facts in $r$. $\\textbf{(ii) Neuron\nversatility.}$ Neurons can be shared across multiple closely related as well as\nless related relations. Some relation neurons transfer across languages.\n$\\textbf{(iii) Neuron interference.}$ Deactivating neurons specific to one\nrelation can improve LLM generation performance for facts of other relations.\nWe will make our code publicly available at\nhttps://github.com/cisnlp/relation-specific-neurons.", "published": "2025-02-24 17:33:18", "link": "http://arxiv.org/abs/2502.17355v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What is a Good Question? Utility Estimation with LLM-based Simulations", "abstract": "Asking questions is a fundamental aspect of learning that facilitates deeper\nunderstanding. However, characterizing and crafting questions that effectively\nimprove learning remains elusive. To address this gap, we propose QUEST\n(Question Utility Estimation with Simulated Tests). QUEST simulates a learning\nenvironment that enables the quantification of a question's utility based on\nits direct impact on improving learning outcomes. Furthermore, we can identify\nhigh-utility questions and use them to fine-tune question generation models\nwith rejection sampling. We find that questions generated by models trained\nwith rejection sampling based on question utility result in exam scores that\nare higher by at least 20% than those from specialized prompting grounded on\neducational objectives literature and models fine-tuned with indirect measures\nof question quality, such as saliency and expected information gain.", "published": "2025-02-24 18:08:41", "link": "http://arxiv.org/abs/2502.17383v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Linguistic Generalizability of Test-Time Scaling in Mathematical\n  Reasoning", "abstract": "Scaling pre-training compute has proven effective for achieving\nmulitlinguality, but does the same hold for test-time scaling? In this work, we\nintroduce MCLM, a multilingual math benchmark featuring competition-level\nproblems in 55 languages. We test three test-time scaling methods-Outcome\nReward Modeling (ORM), Process Reward Modeling (ORM), and Budget Forcing\n(BF)-on both Qwen2.5-1.5B Math and MR1-1.5B, a multilingual LLM we trained for\nextended reasoning. Our experiments show that using Qwen2.5-1.5B Math with ORM\nachieves a score of 35.8 on MCLM, while BF on MR1-1.5B attains 35.2. Although\n\"thinking LLMs\" have recently garnered significant attention, we find that\ntheir performance is comparable to traditional scaling methods like best-of-N\nonce constrained to similar levels of inference FLOPs. Moreover, while BF\nyields a 20-point improvement on English AIME, it provides only a 1.94-point\naverage gain across other languages-a pattern consistent across the other\ntest-time scaling methods we studied-higlighting that test-time scaling may not\ngeneralize as effectively to multilingual tasks. To foster further research, we\nrelease MCLM, MR1-1.5B, and evaluation results.", "published": "2025-02-24 18:36:15", "link": "http://arxiv.org/abs/2502.17407v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Policy Learning with a Natural Language Action Space: A Causal Approach", "abstract": "This paper introduces a novel causal framework for multi-stage\ndecision-making in natural language action spaces where outcomes are only\nobserved after a sequence of actions. While recent approaches like Proximal\nPolicy Optimization (PPO) can handle such delayed-reward settings in\nhigh-dimensional action spaces, they typically require multiple models (policy,\nvalue, and reward) and substantial training data. Our approach employs\nQ-learning to estimate Dynamic Treatment Regimes (DTR) through a single model,\nenabling data-efficient policy learning via gradient ascent on language\nembeddings. A key technical contribution of our approach is a decoding strategy\nthat translates optimized embeddings back into coherent natural language. We\nevaluate our approach on mental health intervention, hate speech countering,\nand sentiment transfer tasks, demonstrating significant improvements over\ncompetitive baselines across multiple metrics. Notably, our method achieves\nsuperior transfer strength while maintaining content preservation and fluency,\nas validated through human evaluation. Our work provides a practical foundation\nfor learning optimal policies in complex language tasks where training data is\nlimited.", "published": "2025-02-24 17:26:07", "link": "http://arxiv.org/abs/2502.17538v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Conditioning Clinical Text Generation for User Control", "abstract": "Deploying natural language generation systems in clinical settings remains\nchallenging despite advances in Large Language Models (LLMs), which continue to\nexhibit hallucinations and factual inconsistencies, necessitating human\noversight. This paper explores automated dataset augmentation using LLMs as\nhuman proxies to condition LLMs for clinician control without increasing\ncognitive workload. On the BioNLP ACL'24 Discharge Me! Shared Task, we achieve\nnew state-of-the-art results with simpler methods than prior submissions\nthrough more efficient training, yielding a 9\\% relative improvement without\naugmented training and up to 34\\% with dataset augmentation. Preliminary human\nevaluation further supports the effectiveness of our approach, highlighting the\npotential of augmenting clinical text generation for control to enhance\nrelevance, accuracy, and factual consistency.", "published": "2025-02-24 19:00:13", "link": "http://arxiv.org/abs/2502.17571v1", "categories": ["cs.CL", "I.2.7; I.2.1"], "primary_category": "cs.CL"}
{"title": "End-to-End Chart Summarization via Visual Chain-of-Thought in\n  Vision-Language Models", "abstract": "Automated chart summarization is crucial for enhancing data accessibility and\nenabling efficient information extraction from visual data. While recent\nadvances in visual-language models (VLMs) have demonstrated promise, existing\nmethods often suffer from limitations in matching the generated summary to the\nchart data and in reasoning about complex chart patterns. This paper introduces\nEnd-to-End Visual Chain-of-Thought (V-CoT) for chart summarization, a novel\napproach optimized for Large Vision-Language Models (LVLMs). Our method\ndirectly trains an LVLM to process chart images and generate textual summaries\nin an end-to-end fashion, eliminating the need for explicit chart parsing\nmodules. We incorporate a visual Chain-of-Thought mechanism through instruction\nfine-tuning, implicitly guiding the LVLM to perform visual reasoning steps\nduring summary generation. Evaluated on the large-scale Chart-Sum-QA dataset,\nour V-CoT method significantly outperforms state-of-the-art baselines across a\nrange of automatic metrics, including BLEU, BLEURT, CIDEr, and CS, and\ndemonstrates superior matching degree and reasoning correctness in human\nevaluations. Ablation studies and detailed analyses further validate the\neffectiveness and robustness of our proposed approach, establishing a new\nbenchmark for end-to-end chart summarization.", "published": "2025-02-24 19:13:45", "link": "http://arxiv.org/abs/2502.17589v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Proactive Privacy Amnesia for Large Language Models: Safeguarding PII\n  with Negligible Impact on Model Utility", "abstract": "With the rise of large language models (LLMs), increasing research has\nrecognized their risk of leaking personally identifiable information (PII)\nunder malicious attacks. Although efforts have been made to protect PII in\nLLMs, existing methods struggle to balance privacy protection with maintaining\nmodel utility. In this paper, inspired by studies of amnesia in cognitive\nscience, we propose a novel approach, Proactive Privacy Amnesia (PPA), to\nsafeguard PII in LLMs while preserving their utility. This mechanism works by\nactively identifying and forgetting key memories most closely associated with\nPII in sequences, followed by a memory implanting using suitable substitute\nmemories to maintain the LLM's functionality. We conduct evaluations across\nmultiple models to protect common PII, such as phone numbers and physical\naddresses, against prevalent PII-targeted attacks, demonstrating the\nsuperiority of our method compared with other existing defensive techniques.\nThe results show that our PPA method completely eliminates the risk of phone\nnumber exposure by 100% and significantly reduces the risk of physical address\nexposure by 9.8% - 87.6%, all while maintaining comparable model utility\nperformance.", "published": "2025-02-24 19:16:39", "link": "http://arxiv.org/abs/2502.17591v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context\n  Inference", "abstract": "Long-context Multimodal Large Language Models (MLLMs) that incorporate long\ntext-image and text-video modalities, demand substantial resources as their\nmultimodal Key-Value (KV) caches grow with increasing input lengths,\nchallenging inference efficiency. Existing methods for KV cache compression, in\nboth text-only and multimodal LLMs, have neglected attention density variations\nacross layers, thus often adopting uniform or progressive reduction strategies\nfor layer-wise cache allocation. In this work, we propose MEDA, a dynamic\nlayer-wise KV cache allocation method for efficient multimodal long-context\ninference. As its core, MEDA utilizes cross-modal attention entropy to\ndetermine the KV cache size at each MLLMs layer. Given the dynamically\nallocated KV cache size at each layer, MEDA also employs a KV pair selection\nscheme to identify which KV pairs to select and a KV pair merging strategy that\nmerges the selected and non-selected ones to preserve information from the\nentire context. MEDA achieves up to 72% KV cache memory reduction and 2.82\ntimes faster decoding speed, while maintaining or enhancing performance on\nvarious multimodal tasks in long-context settings, including multi-images and\nlong-video scenarios. Our code is released at\nhttps://github.com/AIoT-MLSys-Lab/MEDA.", "published": "2025-02-24 19:34:52", "link": "http://arxiv.org/abs/2502.17599v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating the Effect of Retrieval Augmentation on Social Biases", "abstract": "Retrieval Augmented Generation (RAG) has gained popularity as a method for\nconveniently incorporating novel facts that were not seen during the\npre-training stage in Large Language Model (LLM)-based Natural Language\nGeneration (NLG) systems. However, LLMs are known to encode significant levels\nof unfair social biases. The modulation of these biases by RAG in NLG systems\nis not well understood. In this paper, we systematically study the relationship\nbetween the different components of a RAG system and the social biases\npresented in the text generated across three languages (i.e. English, Japanese\nand Chinese) and four social bias types (i.e. gender, race, age and religion).\nSpecifically, using the Bias Question Answering (BBQ) benchmark datasets, we\nevaluate the social biases in RAG responses from document collections with\nvarying levels of stereotypical biases, employing multiple LLMs used as\ngenerators. We find that the biases in document collections are often amplified\nin the generated responses, even when the generating LLM exhibits a low-level\nof bias. Our findings raise concerns about the use of RAG as a technique for\ninjecting novel facts into NLG systems and call for careful evaluation of\npotential social biases in RAG applications before their real-world deployment.", "published": "2025-02-24 19:58:23", "link": "http://arxiv.org/abs/2502.17611v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Typologically Aware Rescoring to Mitigate Unfaithfulness in\n  Lower-Resource Languages", "abstract": "Multilingual large language models (LLMs) are known to more frequently\ngenerate non-faithful output in resource-constrained languages (Guerreiro et\nal., 2023 - arXiv:2303.16104), potentially because these typologically diverse\nlanguages are underrepresented in their training data. To mitigate\nunfaithfulness in such settings, we propose using computationally light\nauxiliary models to rescore the outputs of larger architectures. As proof of\nthe feasibility of such an approach, we show that monolingual 4-layer BERT\nmodels pretrained from scratch on less than 700 MB of data without fine-tuning\nare able to identify faithful summaries with a mean accuracy of 88.33% in three\ngenetically unrelated languages that differ in their morphological complexity -\nVietnamese, Polish and Georgian. The same hyperparameter combination moreover\ngeneralises well to three other tasks, suggesting applications for rescoring\nbeyond improving faithfulness. In order to inform typologically aware model\nselection, we also investigate how morphological complexity interacts with\nregularisation, model depth and training objectives, ultimately demonstrating\nthat morphologically complex languages are more likely to benefit from dropout,\nwhile across languages downstream performance is enhanced most by shallow\narchitectures as well as training using the standard BERT objectives.", "published": "2025-02-24 21:22:19", "link": "http://arxiv.org/abs/2502.17664v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Human Cognition: Visual Context Guides Syntactic Priming in\n  Fusion-Encoded Models", "abstract": "We introduced PRISMATIC, the first multimodal structural priming dataset, and\nproposed a reference-free evaluation metric that assesses priming effects\nwithout predefined target sentences. Using this metric, we constructed and\ntested models with different multimodal encoding architectures (dual encoder\nand fusion encoder) to investigate their structural preservation capabilities.\nOur findings show that models with both encoding methods demonstrate comparable\nsyntactic priming effects. However, only fusion-encoded models exhibit robust\npositive correlations between priming effects and visual similarity, suggesting\na cognitive process more aligned with human psycholinguistic patterns. This\nwork provides new insights into evaluating and understanding how syntactic\ninformation is processed in multimodal language models.", "published": "2025-02-24 21:33:27", "link": "http://arxiv.org/abs/2502.17669v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantics drives analogical change in Germanic strong verb paradigms: a\n  phylogenetic study", "abstract": "A large body of research on morphological paradigms makes the prediction that\nirregular morphological patterns of allomorphy are more likely to emerge and\npersist when they serve to mark important functional distinctions. More\nspecifically, it has been observed that in some Germanic languages in which\nnarrative past tense is expressed by the past participle, there is a greater\naffinity for stem allomorphy shared by preterite forms and past participles to\nthe exclusion of present forms (the so-called ABB pattern), as it serves to\nenhance marking of the binary semantic opposition between present and past.\nUsing data from 107 cognate verbs attested across 14 archaic and contemporary\nGermanic languages and a novel hierarchical phylogenetic model, we show that\nthere is a greater long-term preference for this alternation pattern in\nsituations where narrative past tense has been extended to the past participle,\nconfirming this hypothesis. We further elucidate the mechanisms underlying this\nassociation, demonstrating that this association holds because verbs with the\nABB pattern are more likely to preserve it in situations where it marks an\nimportant binary semantic opposition; however, there is less evidence that the\nABB pattern is extended to verbs with different patterns under the same\ncircumstances. These results bear on debate as to whether the distribution of\nirregularity we observe cross-linguistically is due primarily to (1) the\npreservation of irregular patterns or (2) an active drive toward\nirregularization in certain contexts, and are more in line with the first\nhypothesis.", "published": "2025-02-24 21:36:15", "link": "http://arxiv.org/abs/2502.17670v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Moderation Matters:Measuring Conversational Moderation Impact in English\n  as a Second Language Group Discussion", "abstract": "English as a Second Language (ESL) speakers often struggle to engage in group\ndiscussions due to language barriers. While moderators can facilitate\nparticipation, few studies assess conversational engagement and evaluate\nmoderation effectiveness. To address this gap, we develop a dataset comprising\n17 sessions from an online ESL conversation club, which includes both moderated\nand non-moderated discussions. We then introduce an approach that integrates\nautomatic ESL dialogue assessment and a framework that categorizes moderation\nstrategies. Our findings indicate that moderators help improve the flow of\ntopics and start/end a conversation. Interestingly, we find active\nacknowledgement and encouragement to be the most effective moderation strategy,\nwhile excessive information and opinion sharing by moderators has a negative\nimpact. Ultimately, our study paves the way for analyzing ESL group discussions\nand the role of moderators in non-native conversation settings.", "published": "2025-02-24 12:14:31", "link": "http://arxiv.org/abs/2502.18341v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do Emotions Really Affect Argument Convincingness? A Dynamic Approach\n  with LLM-based Manipulation Checks", "abstract": "Emotions have been shown to play a role in argument convincingness, yet this\naspect is underexplored in the natural language processing (NLP) community.\nUnlike prior studies that use static analyses, focus on a single text domain or\nlanguage, or treat emotion as just one of many factors, we introduce a dynamic\nframework inspired by manipulation checks commonly used in psychology and\nsocial science; leveraging LLM-based manipulation checks, this framework\nexamines the extent to which perceived emotional intensity influences perceived\nconvincingness. Through human evaluation of arguments across different\nlanguages, text domains, and topics, we find that in over half of cases,\njudgments of convincingness remain unchanged despite variations in perceived\nemotional intensity; when emotions do have an impact, they more often enhance\nrather than weaken convincingness. We further analyze how 11 LLMs behave in the\nsame scenario, finding that while LLMs generally mirror human patterns, they\nstruggle to capture nuanced emotional effects in individual judgments.", "published": "2025-02-24 10:04:44", "link": "http://arxiv.org/abs/2503.00024v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LogitLens4LLMs: Extending Logit Lens Analysis to Modern Large Language\n  Models", "abstract": "This paper introduces LogitLens4LLMs, a toolkit that extends the Logit Lens\ntechnique to modern large language models. While Logit Lens has been a crucial\nmethod for understanding internal representations of language models, it was\npreviously limited to earlier model architectures. Our work overcomes the\nlimitations of existing implementations, enabling the technique to be applied\nto state-of-the-art architectures (such as Qwen-2.5 and Llama-3.1) while\nautomating key analytical workflows. By developing component-specific hooks to\ncapture both attention mechanisms and MLP outputs, our implementation achieves\nfull compatibility with the HuggingFace transformer library while maintaining\nlow inference overhead. The toolkit provides both interactive exploration and\nbatch processing capabilities, supporting large-scale layer-wise analyses.\nThrough open-sourcing our implementation, we aim to facilitate deeper\ninvestigations into the internal mechanisms of large-scale language models. The\ntoolkit is openly available at https://github.com/zhenyu-02/LogitLens4LLMs.", "published": "2025-02-24 03:37:44", "link": "http://arxiv.org/abs/2503.11667v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LED-Merging: Mitigating Safety-Utility Conflicts in Model Merging with\n  Location-Election-Disjoint", "abstract": "Fine-tuning pre-trained Large Language Models (LLMs) for specialized tasks\nincurs substantial computational and data costs. While model merging offers a\ntraining-free solution to integrate multiple task-specific models, existing\nmethods suffer from safety-utility conflicts where enhanced general\ncapabilities degrade safety safeguards. We identify two root causes:\n\\textbf{neuron misidentification} due to simplistic parameter magnitude-based\nselection, and \\textbf{cross-task neuron interference} during merging. To\naddress these challenges, we propose \\textbf{LED-Merging}, a three-stage\nframework that \\textbf{L}ocates task-specific neurons via gradient-based\nattribution, dynamically \\textbf{E}lects critical neurons through multi-model\nimportance fusion, and \\textbf{D}isjoints conflicting updates through parameter\nisolation. Extensive experiments on Llama-3-8B, Mistral-7B, and Llama2-13B\ndemonstrate that LED-Merging reduces harmful response rates(\\emph{e.g.}, a\n31.4\\% decrease on Llama-3-8B-Instruct on HarmBench) while preserving 95\\% of\nutility performance(\\emph{e.g.}, 52.39\\% accuracy on GSM8K). LED-Merging\nresolves safety-utility conflicts and provides a lightweight, training-free\nparadigm for constructing reliable multi-task LLMs.", "published": "2025-02-24 01:19:43", "link": "http://arxiv.org/abs/2502.16770v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AISafetyLab: A Comprehensive Framework for AI Safety Evaluation and\n  Improvement", "abstract": "As AI models are increasingly deployed across diverse real-world scenarios,\nensuring their safety remains a critical yet underexplored challenge. While\nsubstantial efforts have been made to evaluate and enhance AI safety, the lack\nof a standardized framework and comprehensive toolkit poses significant\nobstacles to systematic research and practical adoption. To bridge this gap, we\nintroduce AISafetyLab, a unified framework and toolkit that integrates\nrepresentative attack, defense, and evaluation methodologies for AI safety.\nAISafetyLab features an intuitive interface that enables developers to\nseamlessly apply various techniques while maintaining a well-structured and\nextensible codebase for future advancements. Additionally, we conduct empirical\nstudies on Vicuna, analyzing different attack and defense strategies to provide\nvaluable insights into their comparative effectiveness. To facilitate ongoing\nresearch and development in AI safety, AISafetyLab is publicly available at\nhttps://github.com/thu-coai/AISafetyLab, and we are committed to its continuous\nmaintenance and improvement.", "published": "2025-02-24 02:11:52", "link": "http://arxiv.org/abs/2502.16776v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unsupervised Topic Models are Data Mixers for Pre-training Language\n  Models", "abstract": "The performance of large language models (LLMs) is significantly affected by\nthe quality and composition of their pre-training data, which is inherently\ndiverse, spanning various domains, sources, and topics. Effectively integrating\nthese heterogeneous data sources is crucial for optimizing LLM performance.\nPrevious research has predominantly concentrated on domain-based data mixing,\noften neglecting the nuanced topic-level characteristics of the data. To\naddress this gap, we propose a simple yet effective topic-based data mixing\nstrategy that utilizes fine-grained topics generated through our topic modeling\nmethod, DataWeave. DataWeave employs a multi-stage clustering process to group\nsemantically similar documents and utilizes LLMs to generate detailed topics,\nthereby facilitating a more nuanced understanding of dataset composition. Our\nstrategy employs heuristic methods to upsample or downsample specific topics,\nwhich significantly enhances LLM performance on downstream tasks, achieving\nsuperior results compared to previous, more complex data mixing approaches.\nFurthermore, we confirm that the topics Science and Relationships are\nparticularly effective, yielding the most substantial performance improvements.\nWe will make our code and datasets publicly available.", "published": "2025-02-24 03:25:56", "link": "http://arxiv.org/abs/2502.16802v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Uncertainty Quantification of Large Language Models through\n  Multi-Dimensional Responses", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks due to large training datasets and powerful transformer\narchitecture. However, the reliability of responses from LLMs remains a\nquestion. Uncertainty quantification (UQ) of LLMs is crucial for ensuring their\nreliability, especially in areas such as healthcare, finance, and\ndecision-making. Existing UQ methods primarily focus on semantic similarity,\noverlooking the deeper knowledge dimensions embedded in responses. We introduce\na multi-dimensional UQ framework that integrates semantic and knowledge-aware\nsimilarity analysis. By generating multiple responses and leveraging auxiliary\nLLMs to extract implicit knowledge, we construct separate similarity matrices\nand apply tensor decomposition to derive a comprehensive uncertainty\nrepresentation. This approach disentangles overlapping information from both\nsemantic and knowledge dimensions, capturing both semantic variations and\nfactual consistency, leading to more accurate UQ. Our empirical evaluations\ndemonstrate that our method outperforms existing techniques in identifying\nuncertain responses, offering a more robust framework for enhancing LLM\nreliability in high-stakes applications.", "published": "2025-02-24 04:05:08", "link": "http://arxiv.org/abs/2502.16820v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Sarang at DEFACTIFY 4.0: Detecting AI-Generated Text Using Noised Data\n  and an Ensemble of DeBERTa Models", "abstract": "This paper presents an effective approach to detect AI-generated text,\ndeveloped for the Defactify 4.0 shared task at the fourth workshop on\nmultimodal fact checking and hate speech detection. The task consists of two\nsubtasks: Task-A, classifying whether a text is AI generated or human written,\nand Task-B, classifying the specific large language model that generated the\ntext. Our team (Sarang) achieved the 1st place in both tasks with F1 scores of\n1.0 and 0.9531, respectively. The methodology involves adding noise to the\ndataset to improve model robustness and generalization. We used an ensemble of\nDeBERTa models to effectively capture complex patterns in the text. The result\nindicates the effectiveness of our noise-driven and ensemble-based approach,\nsetting a new standard in AI-generated text detection and providing guidance\nfor future developments.", "published": "2025-02-24 05:32:00", "link": "http://arxiv.org/abs/2502.16857v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal\n  Performance", "abstract": "To alleviate memory burden during inference of large language models (LLMs),\nnumerous studies have focused on compressing the KV cache by exploring aspects\nsuch as attention sparsity. However, these techniques often require a\npre-defined cache budget; as the optimal budget varies with different input\nlengths and task types, it limits their practical deployment accepting\nopen-domain instructions. To address this limitation, we propose a new KV cache\ncompression objective: to always ensure the full-cache performance regardless\nof specific inputs, while maximizing KV cache pruning as much as possible. To\nachieve this goal, we introduce a novel KV cache compression method dubbed\nDBudgetKV, which features an attention-based metric to signal when the\nremaining KV cache is unlikely to match the full-cache performance, then\nhalting the pruning process. Empirical evaluation spanning diverse context\nlengths, task types, and model sizes suggests that our method achieves lossless\nKV pruning effectively and robustly, exceeding 25% compression ratio on\naverage. Furthermore, our method is easy to integrate within LLM inference, not\nonly optimizing memory space, but also showing reduced inference time compared\nto existing methods.", "published": "2025-02-24 06:33:39", "link": "http://arxiv.org/abs/2502.16886v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Char-mander Use mBackdoor! A Study of Cross-lingual Backdoor Attacks in\n  Multilingual LLMs", "abstract": "We explore Cross-lingual Backdoor ATtacks (X-BAT) in multilingual Large\nLanguage Models (mLLMs), revealing how backdoors inserted in one language can\nautomatically transfer to others through shared embedding spaces. Using\ntoxicity classification as a case study, we demonstrate that attackers can\ncompromise multilingual systems by poisoning data in a single language, with\nrare tokens serving as specific effective triggers. Our findings expose a\ncritical vulnerability in the fundamental architecture that enables\ncross-lingual transfer in these models. Our code and data are publicly\navailable at https://github.com/himanshubeniwal/X-BAT.", "published": "2025-02-24 06:54:50", "link": "http://arxiv.org/abs/2502.16901v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GuidedBench: Equipping Jailbreak Evaluation with Guidelines", "abstract": "Jailbreaking methods for large language models (LLMs) have gained increasing\nattention for building safe and responsible AI systems. After analyzing 35\njailbreak methods across six categories, we find that existing benchmarks,\nrelying on universal LLM-based or keyword-matching scores, lack case-specific\ncriteria, leading to conflicting results. In this paper, we introduce a more\nrobust evaluation framework for jailbreak methods, with a curated harmful\nquestion dataset, detailed case-by-case evaluation guidelines, and a scoring\nsystem equipped with these guidelines. Our experiments show that existing\njailbreak methods exhibit better discrimination when evaluated using our\nbenchmark. Some jailbreak methods that claim to achieve over 90% attack success\nrate (ASR) on other benchmarks only reach a maximum of 30.2% on our benchmark,\nproviding a higher ceiling for more advanced jailbreak research; furthermore,\nusing our scoring system reduces the variance of disagreements between\ndifferent evaluator LLMs by up to 76.33%. This demonstrates its ability to\nprovide more fair and stable evaluation.", "published": "2025-02-24 06:57:27", "link": "http://arxiv.org/abs/2502.16903v1", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "A Systematic Survey of Automatic Prompt Optimization Techniques", "abstract": "Since the advent of large language models (LLMs), prompt engineering has been\na crucial step for eliciting desired responses for various Natural Language\nProcessing (NLP) tasks. However, prompt engineering remains an impediment for\nend users due to rapid advances in models, tasks, and associated best\npractices. To mitigate this, Automatic Prompt Optimization (APO) techniques\nhave recently emerged that use various automated techniques to help improve the\nperformance of LLMs on various tasks. In this paper, we present a comprehensive\nsurvey summarizing the current progress and remaining challenges in this field.\nWe provide a formal definition of APO, a 5-part unifying framework, and then\nproceed to rigorously categorize all relevant works based on their salient\nfeatures therein. We hope to spur further research guided by our framework.", "published": "2025-02-24 07:29:13", "link": "http://arxiv.org/abs/2502.16923v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Reasoning Does Not Necessarily Improve Role-Playing Ability", "abstract": "The application of role-playing large language models (LLMs) is rapidly\nexpanding in both academic and commercial domains, driving an increasing demand\nfor high-precision role-playing models. Simultaneously, the rapid advancement\nof reasoning techniques has continuously pushed the performance boundaries of\nLLMs. This intersection of practical role-playing demands and evolving\nreasoning capabilities raises an important research question: \"Can reasoning\ntechniques enhance the role-playing capabilities of LLMs?\" To address this, we\nconduct a comprehensive study using 6 role-playing benchmarks, 24 LLMs, and 3\ndistinct role-playing strategies, comparing the effectiveness of direct\nzero-shot role-playing, role-playing with Chain-of-Thought (CoT), and\nrole-playing using reasoning-optimized LLMs. Our findings reveal that CoT may\nreduce role-playing performance, reasoning-optimized LLMs are unsuitable for\nrole-playing, reasoning ability disrupts the role-playing scaling law, large\nmodels still lack proficiency in advanced role-playing, and Chinese\nrole-playing performance surpasses English role-playing performance.\nFurthermore, based on extensive experimental results, we propose two promising\nfuture research directions: Role-aware CoT for improving role-playing LLMs and\nReinforcement Learning for role-playing LLMs, aiming to enhance the\nadaptability, consistency, and effectiveness of role-playing LLMs for both\nresearch and real-world applications.", "published": "2025-02-24 08:08:41", "link": "http://arxiv.org/abs/2502.16940v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Using Machine Learning to Detect Fraudulent SMSs in Chichewa", "abstract": "SMS enabled fraud is of great concern globally. Building classifiers based on\nmachine learning for SMS fraud requires the use of suitable datasets for model\ntraining and validation. Most research has centred on the use of datasets of\nSMSs in English. This paper introduces a first dataset for SMS fraud detection\nin Chichewa, a major language in Africa, and reports on experiments with\nmachine learning algorithms for classifying SMSs in Chichewa as fraud or\nnon-fraud. We answer the broader research question of how feasible it is to\ndevelop machine learning classification models for Chichewa SMSs. To do that,\nwe created three datasets. A small dataset of SMS in Chichewa was collected\nthrough primary research from a segment of the young population. We applied a\nlabel-preserving text transformations to increase its size. The enlarged\ndataset was translated into English using two approaches: human translation and\nmachine translation. The Chichewa and the translated datasets were subjected to\nmachine classification using random forest and logistic regression. Our\nfindings indicate that both models achieved a promising accuracy of over 96% on\nthe Chichewa dataset. There was a drop in performance when moving from the\nChichewa to the translated dataset. This highlights the importance of data\npreprocessing, especially in multilingual or cross-lingual NLP tasks, and shows\nthe challenges of relying on machine-translated text for training machine\nlearning models. Our results underscore the importance of developing language\nspecific models for SMS fraud detection to optimise accuracy and performance.\nSince most machine learning models require data preprocessing, it is essential\nto investigate the impact of the reliance on English-specific tools for data\npreprocessing.", "published": "2025-02-24 08:17:54", "link": "http://arxiv.org/abs/2502.16947v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "SparseTransX: Efficient Training of Translation-Based Knowledge Graph\n  Embeddings Using Sparse Matrix Operations", "abstract": "Knowledge graph (KG) learning offers a powerful framework for generating new\nknowledge and making inferences. Training KG embedding can take a significantly\nlong time, especially for larger datasets. Our analysis shows that the gradient\ncomputation of embedding is one of the dominant functions in the\ntranslation-based KG embedding training loop. We address this issue by\nreplacing the core embedding computation with SpMM (Sparse-Dense Matrix\nMultiplication) kernels. This allows us to unify multiple scatter (and gather)\noperations as a single operation, reducing training time and memory usage. We\ncreate a general framework for training KG models using sparse kernels and\nimplement four models, namely TransE, TransR, TransH, and TorusE. Our sparse\nimplementations exhibit up to 5.3x speedup on the CPU and up to 4.2x speedup on\nthe GPU with a significantly low GPU memory footprint. The speedups are\nconsistent across large and small datasets for a given model. Our proposed\nsparse approach can be extended to accelerate other translation-based (such as\nTransC, TransM, etc.) and non-translational (such as DistMult, ComplEx, RotatE,\netc.) models as well.", "published": "2025-02-24 08:21:48", "link": "http://arxiv.org/abs/2502.16949v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "UrduLLaMA 1.0: Dataset Curation, Preprocessing, and Evaluation in\n  Low-Resource Settings", "abstract": "Multilingual Large Language Models (LLMs) often provide suboptimal\nperformance on low-resource languages like Urdu. This paper introduces\nUrduLLaMA 1.0, a model derived from the open-source Llama-3.1-8B-Instruct\narchitecture and continually pre-trained on 128 million Urdu tokens, capturing\nthe rich diversity of the language. To enhance instruction-following and\ntranslation capabilities, we leverage Low-Rank Adaptation (LoRA) to fine tune\nthe model on 41,000 Urdu instructions and approximately 50,000 English-Urdu\ntranslation pairs. Evaluation across three machine translation datasets\ndemonstrates significant performance improvements compared to state-of-the-art\n(SOTA) models, establishing a new benchmark for Urdu LLMs. These findings\nunderscore the potential of targeted adaptation strategies with limited data\nand computational resources to address the unique challenges of low-resource\nlanguages.", "published": "2025-02-24 08:38:21", "link": "http://arxiv.org/abs/2502.16961v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "LongSafety: Evaluating Long-Context Safety of Large Language Models", "abstract": "As Large Language Models (LLMs) continue to advance in understanding and\ngenerating long sequences, new safety concerns have been introduced through the\nlong context. However, the safety of LLMs in long-context tasks remains\nunder-explored, leaving a significant gap in both evaluation and improvement of\ntheir safety. To address this, we introduce LongSafety, the first comprehensive\nbenchmark specifically designed to evaluate LLM safety in open-ended\nlong-context tasks. LongSafety encompasses 7 categories of safety issues and 6\nuser-oriented long-context tasks, with a total of 1,543 test cases, averaging\n5,424 words per context. Our evaluation towards 16 representative LLMs reveals\nsignificant safety vulnerabilities, with most models achieving safety rates\nbelow 55%. Our findings also indicate that strong safety performance in\nshort-context scenarios does not necessarily correlate with safety in\nlong-context tasks, emphasizing the unique challenges and urgency of improving\nlong-context safety. Moreover, through extensive analysis, we identify\nchallenging safety issues and task types for long-context models. Furthermore,\nwe find that relevant context and extended input sequences can exacerbate\nsafety risks in long-context scenarios, highlighting the critical need for\nongoing attention to long-context safety challenges. Our code and data are\navailable at https://github.com/thu-coai/LongSafety.", "published": "2025-02-24 08:54:39", "link": "http://arxiv.org/abs/2502.16971v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Hotter and Colder: A New Approach to Annotating Sentiment, Emotions, and\n  Bias in Icelandic Blog Comments", "abstract": "This paper presents Hotter and Colder, a dataset designed to analyze various\ntypes of online behavior in Icelandic blog comments. Building on previous work,\nwe used GPT-4o mini to annotate approximately 800,000 comments for 25 tasks,\nincluding sentiment analysis, emotion detection, hate speech, and group\ngeneralizations. Each comment was automatically labeled on a 5-point Likert\nscale. In a second annotation stage, comments with high or low probabilities of\ncontaining each examined behavior were subjected to manual revision. By\nleveraging crowdworkers to refine these automatically labeled comments, we\nensure the quality and accuracy of our dataset resulting in 12,232 uniquely\nannotated comments and 19,301 annotations. Hotter and Colder provides an\nessential resource for advancing research in content moderation and\nautomatically detectiong harmful online behaviors in Icelandic.", "published": "2025-02-24 09:23:39", "link": "http://arxiv.org/abs/2502.16987v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Language Model Re-rankers are Steered by Lexical Similarities", "abstract": "Language model (LM) re-rankers are used to refine retrieval results for\nretrieval-augmented generation (RAG). They are more expensive than lexical\nmatching methods like BM25 but assumed to better process semantic information.\nTo understand whether LM re-rankers always live up to this assumption, we\nevaluate 6 different LM re-rankers on the NQ, LitQA2 and DRUID datasets. Our\nresults show that LM re-rankers struggle to outperform a simple BM25 re-ranker\non DRUID. Leveraging a novel separation metric based on BM25 scores, we explain\nand identify re-ranker errors stemming from lexical dissimilarities. We also\ninvestigate different methods to improve LM re-ranker performance and find\nthese methods mainly useful for NQ. Taken together, our work identifies and\nexplains weaknesses of LM re-rankers and points to the need for more\nadversarial and realistic datasets for their evaluation.", "published": "2025-02-24 10:37:13", "link": "http://arxiv.org/abs/2502.17036v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Systematic Weight Evaluation for Pruning Large Language Models:\n  Enhancing Performance and Sustainability", "abstract": "The exponential growth of large language models (LLMs) like ChatGPT has\nrevolutionized artificial intelligence, offering unprecedented capabilities in\nnatural language processing. However, the extensive computational resources\nrequired for training these models have significant environmental implications,\nincluding high carbon emissions, energy consumption, and water usage. This\nresearch presents a novel approach to LLM pruning, focusing on the systematic\nevaluation of individual weight importance throughout the training process. By\nmonitoring parameter evolution over time, we propose a method that effectively\nreduces model size without compromising performance. Extensive experiments with\nboth a scaled-down LLM and a large multimodal model reveal that moderate\npruning enhances efficiency and reduces loss, while excessive pruning\ndrastically deteriorates model performance. These findings highlight the\ncritical need for optimized AI models to ensure sustainable development,\nbalancing technological advancement with environmental responsibility.", "published": "2025-02-24 11:34:49", "link": "http://arxiv.org/abs/2502.17071v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "WildFrame: Comparing Framing in Humans and LLMs on Naturally Occurring\n  Texts", "abstract": "Humans are influenced by how information is presented, a phenomenon known as\nthe framing effect. Previous work has shown that LLMs may also be susceptible\nto framing but has done so on synthetic data and did not compare to human\nbehavior. We introduce WildFrame, a dataset for evaluating LLM responses to\npositive and negative framing, in naturally-occurring sentences, and compare\nhumans on the same data. WildFrame consists of 1,000 texts, first selecting\nreal-world statements with clear sentiment, then reframing them in either\npositive or negative light, and lastly, collecting human sentiment annotations.\nBy evaluating eight state-of-the-art LLMs on WildFrame, we find that all models\nexhibit framing effects similar to humans ($r\\geq0.57$), with both humans and\nmodels being more influenced by positive rather than negative reframing. Our\nfindings benefit model developers, who can either harness framing or mitigate\nits effects, depending on the downstream application.", "published": "2025-02-24 12:14:05", "link": "http://arxiv.org/abs/2502.17091v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mobile-Agent-V: Learning Mobile Device Operation Through Video-Guided\n  Multi-Agent Collaboration", "abstract": "The rapid increase in mobile device usage necessitates improved automation\nfor seamless task management. However, many AI-driven frameworks struggle due\nto insufficient operational knowledge. Manually written knowledge helps but is\nlabor-intensive and inefficient. To address these challenges, we introduce\nMobile-Agent-V, a framework that leverages video guidance to provide rich and\ncost-effective operational knowledge for mobile automation. Mobile-Agent-V\nenhances task execution capabilities by leveraging video inputs without\nrequiring specialized sampling or preprocessing. Mobile-Agent-V integrates a\nsliding window strategy and incorporates a video agent and deep-reflection\nagent to ensure that actions align with user instructions. Through this\ninnovative approach, users can record task processes with guidance, enabling\nthe system to autonomously learn and execute tasks efficiently. Experimental\nresults show that Mobile-Agent-V achieves a 30% performance improvement\ncompared to existing frameworks. The code will be open-sourced at\nhttps://github.com/X-PLUG/MobileAgent.", "published": "2025-02-24 12:51:23", "link": "http://arxiv.org/abs/2502.17110v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "LettuceDetect: A Hallucination Detection Framework for RAG Applications", "abstract": "Retrieval Augmented Generation (RAG) systems remain vulnerable to\nhallucinated answers despite incorporating external knowledge sources. We\npresent LettuceDetect a framework that addresses two critical limitations in\nexisting hallucination detection methods: (1) the context window constraints of\ntraditional encoder-based methods, and (2) the computational inefficiency of\nLLM based approaches. Building on ModernBERT's extended context capabilities\n(up to 8k tokens) and trained on the RAGTruth benchmark dataset, our approach\noutperforms all previous encoder-based models and most prompt-based models,\nwhile being approximately 30 times smaller than the best models. LettuceDetect\nis a token-classification model that processes context-question-answer triples,\nallowing for the identification of unsupported claims at the token level.\nEvaluations on the RAGTruth corpus demonstrate an F1 score of 79.22% for\nexample-level detection, which is a 14.8% improvement over Luna, the previous\nstate-of-the-art encoder-based architecture. Additionally, the system can\nprocess 30 to 60 examples per second on a single GPU, making it more practical\nfor real-world RAG applications.", "published": "2025-02-24 13:11:47", "link": "http://arxiv.org/abs/2502.17125v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MEMERAG: A Multilingual End-to-End Meta-Evaluation Benchmark for\n  Retrieval Augmented Generation", "abstract": "Automatic evaluation of retrieval augmented generation (RAG) systems relies\non fine-grained dimensions like faithfulness and relevance, as judged by expert\nhuman annotators. Meta-evaluation benchmarks support the development of\nautomatic evaluators that correlate well with human judgement. However,\nexisting benchmarks predominantly focus on English or use translated data,\nwhich fails to capture cultural nuances. A native approach provides a better\nrepresentation of the end user experience.\n  In this work, we develop a Multilingual End-to-end Meta-Evaluation RAG\nbenchmark (MEMERAG). Our benchmark builds on the popular MIRACL dataset, using\nnative-language questions and generating responses with diverse large language\nmodels (LLMs), which are then assessed by expert annotators for faithfulness\nand relevance. We describe our annotation process and show that it achieves\nhigh inter-annotator agreement. We then analyse the performance of the\nanswer-generating LLMs across languages as per the human evaluators. Finally we\napply the dataset to our main use-case which is to benchmark multilingual\nautomatic evaluators (LLM-as-a-judge). We show that our benchmark can reliably\nidentify improvements offered by advanced prompting techniques and LLMs. We\nwill release our benchmark to support the community developing accurate\nevaluation methods for multilingual RAG systems.", "published": "2025-02-24 13:58:42", "link": "http://arxiv.org/abs/2502.17163v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "JUREX-4E: Juridical Expert-Annotated Four-Element Knowledge Base for\n  Legal Reasoning", "abstract": "The Four-Element Theory is a fundamental framework in criminal law, defining\nthe constitution of crime through four dimensions: Subject, Object, Subjective\naspect, and Objective aspect. This theory is widely referenced in legal\nreasoning, and many Large Language Models (LLMs) attempt to incorporate it when\nhandling legal tasks. However, current approaches rely on LLMs' internal\nknowledge to incorporate this theory, often lacking completeness and\nrepresentativeness. To address this limitation, we introduce JUREX-4E, an\nexpert-annotated knowledge base covering 155 criminal charges. It is structured\nthrough a progressive hierarchical annotation framework that prioritizes legal\nsource validity and employs diverse legal interpretation methods to ensure\ncomprehensiveness and authority. We evaluate JUREX-4E on the Similar Charge\nDistinction task and apply it to Legal Case Retrieval, demonstrating its\neffectiveness in improving LLM performance. Experimental results validate the\nhigh quality of JUREX-4E and its substantial impact on downstream legal tasks,\nunderscoring its potential for advancing legal AI applications. Code:\nhttps://github.com/THUlawtech/JUREX", "published": "2025-02-24 14:02:00", "link": "http://arxiv.org/abs/2502.17166v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Cheems: A Practical Guidance for Building and Evaluating Chinese Reward\n  Models from Scratch", "abstract": "Reward models (RMs) are crucial for aligning large language models (LLMs)\nwith human preferences. However, most RM research is centered on English and\nrelies heavily on synthetic resources, which leads to limited and less reliable\ndatasets and benchmarks for Chinese. To address this gap, we introduce\nCheemsBench, a fully human-annotated RM evaluation benchmark within Chinese\ncontexts, and CheemsPreference, a large-scale and diverse preference dataset\nannotated through human-machine collaboration to support Chinese RM training.\nWe systematically evaluate open-source discriminative and generative RMs on\nCheemsBench and observe significant limitations in their ability to capture\nhuman preferences in Chinese scenarios. Additionally, based on\nCheemsPreference, we construct an RM that achieves state-of-the-art performance\non CheemsBench, demonstrating the necessity of human supervision in RM\ntraining. Our findings reveal that scaled AI-generated data struggles to fully\ncapture human preferences, emphasizing the importance of high-quality human\nsupervision in RM development.", "published": "2025-02-24 14:09:45", "link": "http://arxiv.org/abs/2502.17173v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evaluating Expert Contributions in a MoE LLM for Quiz-Based Tasks", "abstract": "Recently, Large Language Models (LLMs) with Mixture of Experts (MoE) layers\nhave gained significant attention. Currently, state-of-the-art LLMs utilize\nthis architecture. There is a substantial amount of research on how to train\nsuch models and how to select hyperparameters for this architecture. However,\nthere is a lack of studies focusing on post-evaluation analysis of MoE layer\nproperties. In this paper, we take a first step toward closing this gap by\nevaluating expert contributions on the quiz-based MMLU benchmark. We show that\nmost experts were never activated during inference on this benchmark.\nAdditionally, the output distribution of gating networks is much closer to\nuniform than sparse. Finally, we demonstrate that the average performance of\nsome experts within the same layer varies significantly.", "published": "2025-02-24 14:23:52", "link": "http://arxiv.org/abs/2502.17187v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Order Matters: Investigate the Position Bias in Multi-constraint\n  Instruction Following", "abstract": "Real-world instructions with multiple constraints pose a significant\nchallenge to existing large language models (LLMs). An observation is that the\nLLMs exhibit dramatic performance fluctuation when disturbing the order of the\nincorporated constraints. Yet, none of the existing works has systematically\ninvestigated this position bias problem in the field of multi-constraint\ninstruction following. To bridge this gap, we design a probing task where we\nquantitatively measure the difficulty distribution of the constraints by a\nnovel Difficulty Distribution Index (CDDI). Through the experimental results,\nwe find that LLMs are more performant when presented with the constraints in a\n``hard-to-easy'' order. This preference can be generalized to LLMs with\ndifferent architecture or different sizes of parameters. Additionally, we\nconduct an explanation study, providing an intuitive insight into the\ncorrelation between the LLM's attention and constraint orders. Our code and\ndataset are publicly available at https://github.com/meowpass/PBIF.", "published": "2025-02-24 14:39:28", "link": "http://arxiv.org/abs/2502.17204v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Making LLMs Reason? The Intermediate Language Problem in Neurosymbolic\n  Approaches", "abstract": "Logical reasoning tasks manifest themselves as a challenge to Large Language\nModels (LLMs). Neurosymbolic approaches use LLMs to translate logical reasoning\nproblems formulated in natural language into a formal intermediate language.\nSubsequently, the usage of symbolic reasoners yields reliable solving thereof.\nHowever, LLMs often fail in translation due to poorly chosen intermediate\nlanguages.\n  We introduce the intermediate language problem, which is the problem of\nchoosing a suitable formal language representation for neurosymbolic\napproaches. Theoretically, we argue that its origins lie in the inability of\nLLMs to distinguish syntax from semantics and the relative independence of the\nproblem from its representation. We showcase its existence experimentally by\ncontrasting two intermediate languages, Answer Set Programming and the Python\nKnowledge Engine. In addition, we demonstrate the effects of varying degrees of\nsupplementary context information. Our results show a maximum difference in\noverall-accuracy of 53.20% and 49.26% in execution-accuracy. When using the\nGPT4o-mini LLM we beat the state-of-the-art in overall-accuracy on the ProntoQA\ndataset by 21.20% and by 50.50% on the ProofWriter dataset.", "published": "2025-02-24 14:49:52", "link": "http://arxiv.org/abs/2502.17216v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Child vs. machine language learning: Can the logical structure of human\n  language unleash LLMs?", "abstract": "We argue that human language learning proceeds in a manner that is different\nin nature from current approaches to training LLMs, predicting a difference in\nlearning biases. We then present evidence from German plural formation by LLMs\nthat confirm our hypothesis that even very powerful implementations produce\nresults that miss aspects of the logic inherent to language that humans have no\nproblem with. We conclude that attention to the different structures of human\nlanguage and artificial neural networks is likely to be an avenue to improve\nLLM performance.", "published": "2025-02-24 16:40:46", "link": "http://arxiv.org/abs/2502.17304v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Implicit Word Reordering with Knowledge Distillation for Cross-Lingual\n  Dependency Parsing", "abstract": "Word order difference between source and target languages is a major obstacle\nto cross-lingual transfer, especially in the dependency parsing task. Current\nworks are mostly based on order-agnostic models or word reordering to mitigate\nthis problem. However, such methods either do not leverage grammatical\ninformation naturally contained in word order or are computationally expensive\nas the permutation space grows exponentially with the sentence length.\nMoreover, the reordered source sentence with an unnatural word order may be a\nform of noising that harms the model learning. To this end, we propose an\nImplicit Word Reordering framework with Knowledge Distillation (IWR-KD). This\nframework is inspired by that deep networks are good at learning feature\nlinearization corresponding to meaningful data transformation, e.g. word\nreordering. To realize this idea, we introduce a knowledge distillation\nframework composed of a word-reordering teacher model and a dependency parsing\nstudent model. We verify our proposed method on Universal Dependency Treebanks\nacross 31 different languages and show it outperforms a series of competitors,\ntogether with experimental analysis to illustrate how our method works towards\ntraining a robust parser.", "published": "2025-02-24 16:43:05", "link": "http://arxiv.org/abs/2502.17308v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Bridging Gaps in Natural Language Processing for Yor\u00f9b\u00e1: A\n  Systematic Review of a Decade of Progress and Prospects", "abstract": "Natural Language Processing (NLP) is becoming a dominant subset of artificial\nintelligence as the need to help machines understand human language looks\nindispensable. Several NLP applications are ubiquitous, partly due to the\nmyriads of datasets being churned out daily through mediums like social\nnetworking sites. However, the growing development has not been evident in most\nAfrican languages due to the persisting resource limitation, among other\nissues. Yor\\`ub\\'a language, a tonal and morphologically rich African language,\nsuffers a similar fate, resulting in limited NLP usage. To encourage further\nresearch towards improving this situation, this systematic literature review\naims to comprehensively analyse studies addressing NLP development for\nYor\\`ub\\'a, identifying challenges, resources, techniques, and applications. A\nwell-defined search string from a structured protocol was employed to search,\nselect, and analyse 105 primary studies between 2014 and 2024 from reputable\ndatabases. The review highlights the scarcity of annotated corpora, limited\navailability of pre-trained language models, and linguistic challenges like\ntonal complexity and diacritic dependency as significant obstacles. It also\nrevealed the prominent techniques, including rule-based methods, among others.\nThe findings reveal a growing body of multilingual and monolingual resources,\neven though the field is constrained by socio-cultural factors such as\ncode-switching and desertion of language for digital usage. This review\nsynthesises existing research, providing a foundation for advancing NLP for\nYor\\`ub\\'a and in African languages generally. It aims to guide future research\nby identifying gaps and opportunities, thereby contributing to the broader\ninclusion of Yor\\`ub\\'a and other under-resourced African languages in global\nNLP advancements.", "published": "2025-02-24 17:41:48", "link": "http://arxiv.org/abs/2502.17364v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mitigating Bias in RAG: Controlling the Embedder", "abstract": "In retrieval augmented generation (RAG) systems, each individual component --\nthe LLM, embedder, and corpus -- could introduce biases in the form of skews\ntowards outputting certain perspectives or identities. In this work, we study\nthe conflict between biases of each component and their relationship to the\noverall bias of the RAG system, which we call bias conflict. Examining both\ngender and political biases as case studies, we show that bias conflict can be\ncharacterized through a linear relationship among components despite its\ncomplexity in 6 different LLMs. Through comprehensive fine-tuning experiments\ncreating 120 differently biased embedders, we demonstrate how to control bias\nwhile maintaining utility and reveal the importance of reverse-biasing the\nembedder to mitigate bias in the overall system. Additionally, we find that\nLLMs and tasks exhibit varying sensitivities to the embedder bias, a crucial\nfactor to consider for debiasing. Our results underscore that a fair RAG system\ncan be better achieved by carefully controlling the bias of the embedder rather\nthan increasing its fairness.", "published": "2025-02-24 18:16:10", "link": "http://arxiv.org/abs/2502.17390v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FIG: Forward-Inverse Generation for Low-Resource Domain-specific Event\n  Detection", "abstract": "Event Detection (ED) is the task of identifying typed event mentions of\ninterest from natural language text, which benefits domain-specific reasoning\nin biomedical, legal, and epidemiological domains. However, procuring\nsupervised data for thousands of events for various domains is a laborious and\nexpensive task. To this end, existing works have explored synthetic data\ngeneration via forward (generating labels for unlabeled sentences) and inverse\n(generating sentences from generated labels) generations. However, forward\ngeneration often produces noisy labels, while inverse generation struggles with\ndomain drift and incomplete event annotations. To address these challenges, we\nintroduce FIG, a hybrid approach that leverages inverse generation for\nhigh-quality data synthesis while anchoring it to domain-specific cues\nextracted via forward generation on unlabeled target data. FIG further enhances\nits synthetic data by adding missing annotations through forward\ngeneration-based refinement. Experimentation on three ED datasets from diverse\ndomains reveals that FIG outperforms the best baseline achieving average gains\nof 3.3% F1 and 5.4% F1 in the zero-shot and few-shot settings respectively.\nAnalyzing the generated trigger hit rate and human evaluation substantiates\nFIG's superior domain alignment and data quality compared to existing\nbaselines.", "published": "2025-02-24 18:20:42", "link": "http://arxiv.org/abs/2502.17394v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dataset Featurization: Uncovering Natural Language Features through\n  Unsupervised Data Reconstruction", "abstract": "Interpreting data is central to modern research. Large language models (LLMs)\nshow promise in providing such natural language interpretations of data, yet\nsimple feature extraction methods such as prompting often fail to produce\naccurate and versatile descriptions for diverse datasets and lack control over\ngranularity and scale. To address these limitations, we propose a\ndomain-agnostic method for dataset featurization that provides precise control\nover the number of features extracted while maintaining compact and descriptive\nrepresentations comparable to human expert labeling. Our method optimizes the\nselection of informative binary features by evaluating the ability of an LLM to\nreconstruct the original data using those features. We demonstrate its\neffectiveness in dataset modeling tasks and through two case studies: (1)\nConstructing a feature representation of jailbreak tactics that compactly\ncaptures both the effectiveness and diversity of a larger set of human-crafted\nattacks; and (2) automating the discovery of features that align with human\npreferences, achieving accuracy and robustness comparable to expert-crafted\nfeatures. Moreover, we show that the pipeline scales effectively, improving as\nadditional features are sampled, making it suitable for large and diverse\ndatasets.", "published": "2025-02-24 18:42:33", "link": "http://arxiv.org/abs/2502.17541v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Synthetic Text Generation for Training Large Language Models via\n  Gradient Matching", "abstract": "Synthetic data has the potential to improve the performance, training\nefficiency, and privacy of real training examples. Nevertheless, existing\napproaches for synthetic text generation are mostly heuristics and cannot\ngenerate human-readable text without compromising the privacy of real data or\nprovide performance guarantees for training Large Language Models (LLMs). In\nthis work, we propose the first theoretically rigorous approach for generating\nsynthetic human-readable text that guarantees the convergence and performance\nof LLMs during fine-tuning on a target task. To do so, we leverage Alternating\nDirection Method of Multipliers (ADMM) that iteratively optimizes the\nembeddings of synthetic examples to match the gradient of the target training\nor validation data, and maps them to a sequence of text tokens with low\nperplexity. In doing so, the generated synthetic text can guarantee convergence\nof the model to a close neighborhood of the solution obtained by fine-tuning on\nreal data. Experiments on various classification tasks confirm the\neffectiveness of our proposed approach.", "published": "2025-02-24 19:49:15", "link": "http://arxiv.org/abs/2502.17607v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Knowledge Distillation with Training Wheels", "abstract": "Knowledge distillation is used, in generative language modeling, to train a\nsmaller student model using the help of a larger teacher model, resulting in\nimproved capabilities for the student model. In this paper, we formulate a more\ngeneral framework for knowledge distillation where the student learns from the\nteacher during training, and also learns to ask for the teacher's help at\ntest-time following rules specifying test-time restrictions. Towards this, we\nfirst formulate knowledge distillation as an entropy-regularized value\noptimization problem. Adopting Path Consistency Learning to solve this, leads\nto a new knowledge distillation algorithm using on-policy and off-policy\ndemonstrations. We extend this using constrained reinforcement learning to a\nframework that incorporates the use of the teacher model as a test-time\nreference, within constraints. In this situation, akin to a human learner, the\nmodel needs to learn not only the learning material, but also the relative\ndifficulty of different sections to prioritize for seeking teacher help. We\nexamine the efficacy of our method through experiments in translation and\nsummarization tasks, observing trends in accuracy and teacher use, noting that\nour approach unlocks operating points not available to the popular Speculative\nDecoding approach.", "published": "2025-02-24 23:17:52", "link": "http://arxiv.org/abs/2502.17717v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Spontaneous Giving and Calculated Greed in Language Models", "abstract": "Large language models demonstrate advanced problem-solving capabilities by\nincorporating reasoning techniques such as chain of thought and reflection.\nHowever, how these reasoning capabilities extend to social intelligence remains\nunclear. In this study, we investigate this question using economic games that\nmodel social dilemmas, where social intelligence plays a crucial role. First,\nwe examine the effects of chain-of-thought and reflection techniques in a\npublic goods game. We then extend our analysis to six economic games on\ncooperation and punishment, comparing off-the-shelf non-reasoning and reasoning\nmodels. We find that reasoning models significantly reduce cooperation and norm\nenforcement, prioritizing individual rationality. Consequently, groups with\nmore reasoning models exhibit less cooperation and lower gains through repeated\ninteractions. These behaviors parallel human tendencies of \"spontaneous giving\nand calculated greed.\" Our results suggest the need for AI architectures that\nincorporate social intelligence alongside reasoning capabilities to ensure that\nAI supports, rather than disrupts, human cooperative intuition.", "published": "2025-02-24 23:23:27", "link": "http://arxiv.org/abs/2502.17720v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Correlating and Predicting Human Evaluations of Language Models from\n  Natural Language Processing Benchmarks", "abstract": "The explosion of high-performing conversational language models (LMs) has\nspurred a shift from classic natural language processing (NLP) benchmarks to\nexpensive, time-consuming and noisy human evaluations - yet the relationship\nbetween these two evaluation strategies remains hazy. In this paper, we conduct\na large-scale study of four Chat Llama 2 models, comparing their performance on\n160 standard NLP benchmarks (e.g., MMLU, ARC, BIG-Bench Hard) against extensive\nhuman preferences on more than 11k single-turn and 2k multi-turn dialogues from\nover 2k human annotators. Our findings are striking: most NLP benchmarks\nstrongly correlate with human evaluations, suggesting that cheaper, automated\nmetrics can serve as surprisingly reliable predictors of human preferences.\nThree human evaluations, such as adversarial dishonesty and safety, are\nanticorrelated with NLP benchmarks, while two are uncorrelated. Moreover,\nthrough overparameterized linear regressions, we show that NLP scores can\naccurately predict human evaluations across different model scales, offering a\npath to reduce costly human annotation without sacrificing rigor. Overall, our\nresults affirm the continued value of classic benchmarks and illuminate how to\nharness them to anticipate real-world user satisfaction - pointing to how NLP\nbenchmarks can be leveraged to meet evaluation needs of our new era of\nconversational AI.", "published": "2025-02-24 01:01:02", "link": "http://arxiv.org/abs/2502.18339v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluating Large Language Models on the Spanish Medical Intern Resident\n  (MIR) Examination 2024/2025:A Comparative Analysis of Clinical Reasoning and\n  Knowledge Application", "abstract": "This study presents a comparative evaluation of 22 large language models LLMs\non the Spanish Medical Intern Resident MIR examinations for 2024 and 2025 with\na focus on clinical reasoning domain specific expertise and multimodal\nprocessing capabilities The MIR exam consisting of 210 multiple choice\nquestions some requiring image interpretation serves as a stringent benchmark\nfor assessing both factual recall and complex clinical problem solving skills\nOur investigation encompasses general purpose models such as GPT4 Claude LLaMA\nand Gemini as well as specialized fine tuned systems like Miri Pro which\nleverages proprietary Spanish healthcare data to excel in medical contexts\n  Recent market entries Deepseek and Grok have further enriched the evaluation\nlandscape particularly for tasks that demand advanced visual and semantic\nanalysis The findings indicate that while general purpose LLMs perform robustly\noverall fine tuned models consistently achieve superior accuracy especially in\naddressing nuanced domain specific challenges A modest performance decline\nobserved between the two exam cycles appears attributable to the implementation\nof modified questions designed to mitigate reliance on memorization\n  The results underscore the transformative potential of domain specific fine\ntuning and multimodal integration in advancing medical AI applications They\nalso highlight critical implications for the future integration of LLMs into\nmedical education training and clinical decision making emphasizing the\nimportance of balancing automated reasoning with ethical and context aware\njudgment", "published": "2025-02-24 12:08:26", "link": "http://arxiv.org/abs/2503.00025v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Role of Sparsity for Length Generalization in Transformers", "abstract": "Training large language models to predict beyond their training context\nlengths has drawn much attention in recent years, yet the principles driving\nsuch behavior of length generalization remain underexplored. We propose a new\ntheoretical framework to study length generalization for the next-token\nprediction task, as performed by decoder-only transformers. Conceptually, we\nshow that length generalization occurs as long as each predicted token depends\non a small (fixed) number of previous tokens. We formalize such tasks via a\nnotion we call $k$-sparse planted correlation distributions, and show that an\nidealized model of transformers which generalize attention heads successfully\nlength-generalize on such tasks. As a bonus, our theoretical model justifies\ncertain techniques to modify positional embeddings which have been introduced\nto improve length generalization, such as position coupling.\n  We support our theoretical results with experiments on synthetic tasks and\nnatural language, which confirm that a key factor driving length generalization\nis a ``sparse'' dependency structure of each token on the previous ones.\nInspired by our theory, we introduce Predictive Position Coupling, which trains\nthe transformer to predict the position IDs used in a positional coupling\napproach. Predictive Position Coupling thereby allows us to broaden the array\nof tasks to which position coupling can successfully be applied to achieve\nlength generalization.", "published": "2025-02-24 03:01:03", "link": "http://arxiv.org/abs/2502.16792v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "MobileSteward: Integrating Multiple App-Oriented Agents with\n  Self-Evolution to Automate Cross-App Instructions", "abstract": "Mobile phone agents can assist people in automating daily tasks on their\nphones, which have emerged as a pivotal research spotlight. However, existing\nprocedure-oriented agents struggle with cross-app instructions, due to the\nfollowing challenges: (1) complex task relationships, (2) diverse app\nenvironment, and (3) error propagation and information loss in multi-step\nexecution. Drawing inspiration from object-oriented programming principles, we\nrecognize that object-oriented solutions is more suitable for cross-app\ninstruction. To address these challenges, we propose a self-evolving\nmulti-agent framework named MobileSteward, which integrates multiple\napp-oriented StaffAgents coordinated by a centralized StewardAgent. We design\nthree specialized modules in MobileSteward: (1) Dynamic Recruitment generates a\nscheduling graph guided by information flow to explicitly associate tasks among\napps. (2) Assigned Execution assigns the task to app-oriented StaffAgents, each\nequipped with app-specialized expertise to address the diversity between apps.\n(3) Adjusted Evaluation conducts evaluation to provide reflection tips or\ndeliver key information, which alleviates error propagation and information\nloss during multi-step execution. To continuously improve the performance of\nMobileSteward, we develop a Memory-based Self-evolution mechanism, which\nsummarizes the experience from successful execution, to improve the performance\nof MobileSteward. We establish the first English Cross-APP Benchmark (CAPBench)\nin the real-world environment to evaluate the agents' capabilities of solving\ncomplex cross-app instructions. Experimental results demonstrate that\nMobileSteward achieves the best performance compared to both single-agent and\nmulti-agent frameworks, highlighting the superiority of MobileSteward in better\nhandling user instructions with diverse complexity.", "published": "2025-02-24 03:12:45", "link": "http://arxiv.org/abs/2502.16796v1", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.MA"}
{"title": "Improving LLM General Preference Alignment via Optimistic Online Mirror\n  Descent", "abstract": "Reinforcement learning from human feedback (RLHF) has demonstrated remarkable\neffectiveness in aligning large language models (LLMs) with human preferences.\nMany existing alignment approaches rely on the Bradley-Terry (BT) model\nassumption, which assumes the existence of a ground-truth reward for each\nprompt-response pair. However, this assumption can be overly restrictive when\nmodeling complex human preferences. In this paper, we drop the BT model\nassumption and study LLM alignment under general preferences, formulated as a\ntwo-player game. Drawing on theoretical insights from learning in games, we\nintegrate optimistic online mirror descent into our alignment framework to\napproximate the Nash policy. Theoretically, we demonstrate that our approach\nachieves an $O(T^{-1})$ bound on the duality gap, improving upon the previous\n$O(T^{-1/2})$ result. More importantly, we implement our method and show\nthrough experiments that it outperforms state-of-the-art RLHF algorithms across\nmultiple representative benchmarks.", "published": "2025-02-24 05:24:52", "link": "http://arxiv.org/abs/2502.16852v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "CORAL: Learning Consistent Representations across Multi-step Training\n  with Lighter Speculative Drafter", "abstract": "Speculative decoding is a powerful technique that accelerates Large Language\nModel (LLM) inference by leveraging a lightweight speculative draft model.\nHowever, existing designs suffers in performance due to misalignment between\ntraining and inference. Recent methods have tried to solve this issue by\nadopting a multi-step training strategy, but the complex inputs of different\ntraining steps make it harder for the draft model to converge. To address this,\nwe propose CORAL, a novel framework that improves both accuracy and efficiency\nin speculative drafting. CORAL introduces Cross-Step Representation Alignment,\na method that enhances consistency across multiple training steps,\nsignificantly improving speculative drafting performance. Additionally, we\nidentify the LM head as a major bottleneck in the inference speed of the draft\nmodel. We introduce a weight-grouping mechanism that selectively activates a\nsubset of LM head parameters during inference, substantially reducing the\nlatency of the draft model. We evaluate CORAL on three LLM families and three\nbenchmark datasets, achieving speedup ratios of 2.50x-4.07x, outperforming\nstate-of-the-art methods such as EAGLE-2 and HASS. Our results demonstrate that\nCORAL effectively mitigates training-inference misalignment and delivers\nsignificant speedup for modern LLMs with large vocabularies.", "published": "2025-02-24 06:28:26", "link": "http://arxiv.org/abs/2502.16880v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Muon is Scalable for LLM Training", "abstract": "Recently, the Muon optimizer based on matrix orthogonalization has\ndemonstrated strong results in training small-scale language models, but the\nscalability to larger models has not been proven. We identify two crucial\ntechniques for scaling up Muon: (1) adding weight decay and (2) carefully\nadjusting the per-parameter update scale. These techniques allow Muon to work\nout-of-the-box on large-scale training without the need of hyper-parameter\ntuning. Scaling law experiments indicate that Muon achieves $\\sim\\!2\\times$\ncomputational efficiency compared to AdamW with compute optimal training.\n  Based on these improvements, we introduce Moonlight, a 3B/16B-parameter\nMixture-of-Expert (MoE) model trained with 5.7T tokens using Muon. Our model\nimproves the current Pareto frontier, achieving better performance with much\nfewer training FLOPs compared to prior models.\n  We open-source our distributed Muon implementation that is memory optimal and\ncommunication efficient. We also release the pretrained, instruction-tuned, and\nintermediate checkpoints to support future research.", "published": "2025-02-24 09:12:29", "link": "http://arxiv.org/abs/2502.16982v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "FADE: Why Bad Descriptions Happen to Good Features", "abstract": "Recent advances in mechanistic interpretability have highlighted the\npotential of automating interpretability pipelines in analyzing the latent\nrepresentations within LLMs. While they may enhance our understanding of\ninternal mechanisms, the field lacks standardized evaluation methods for\nassessing the validity of discovered features. We attempt to bridge this gap by\nintroducing FADE: Feature Alignment to Description Evaluation, a scalable\nmodel-agnostic framework for evaluating feature-description alignment. FADE\nevaluates alignment across four key metrics - Clarity, Responsiveness, Purity,\nand Faithfulness - and systematically quantifies the causes for the\nmisalignment of feature and their description. We apply FADE to analyze\nexisting open-source feature descriptions, and assess key components of\nautomated interpretability pipelines, aiming to enhance the quality of\ndescriptions. Our findings highlight fundamental challenges in generating\nfeature descriptions, particularly for SAEs as compared to MLP neurons,\nproviding insights into the limitations and future directions of automated\ninterpretability. We release FADE as an open-source package at:\nhttps://github.com/brunibrun/FADE.", "published": "2025-02-24 09:28:35", "link": "http://arxiv.org/abs/2502.16994v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Towards Auto-Regressive Next-Token Prediction: In-Context Learning\n  Emerges from Generalization", "abstract": "Large language models (LLMs) have demonstrated remarkable in-context learning\n(ICL) abilities. However, existing theoretical analysis of ICL primarily\nexhibits two limitations: (a) Limited i.i.d. Setting. Most studies focus on\nsupervised function learning tasks where prompts are constructed with i.i.d.\ninput-label pairs. This i.i.d. assumption diverges significantly from real\nlanguage learning scenarios where prompt tokens are interdependent. (b) Lack of\nEmergence Explanation. Most literature answers what ICL does from an implicit\noptimization perspective but falls short in elucidating how ICL emerges and the\nimpact of pre-training phase on ICL. In our paper, to extend (a), we adopt a\nmore practical paradigm, auto-regressive next-token prediction (AR-NTP), which\nclosely aligns with the actual training of language models. Specifically,\nwithin AR-NTP, we emphasize prompt token-dependency, which involves predicting\neach subsequent token based on the preceding sequence. To address (b), we\nformalize a systematic pre-training and ICL framework, highlighting the\nlayer-wise structure of sequences and topics, alongside a two-level\nexpectation. In conclusion, we present data-dependent, topic-dependent and\noptimization-dependent PAC-Bayesian generalization bounds for pre-trained LLMs,\ninvestigating that ICL emerges from the generalization of sequences and topics.\nOur theory is supported by experiments on numerical linear dynamic systems,\nsynthetic GINC and real-world language datasets.", "published": "2025-02-24 10:26:29", "link": "http://arxiv.org/abs/2502.17024v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Understanding the Uncertainty of LLM Explanations: A Perspective Based\n  on Reasoning Topology", "abstract": "Understanding the uncertainty in large language model (LLM) explanations is\nimportant for evaluating their faithfulness and reasoning consistency, and thus\nprovides insights into the reliability of LLM's output regarding a question. In\nthis work, we propose a novel framework that quantifies uncertainty in LLM\nexplanations through a reasoning topology perspective. By designing a\nstructural elicitation strategy, we guide the LLMs to frame the explanations of\nan answer into a graph topology. This process decomposes the explanations into\nthe knowledge related sub-questions and topology-based reasoning structures,\nwhich allows us to quantify uncertainty not only at the semantic level but also\nfrom the reasoning path. It further brings convenience to assess knowledge\nredundancy and provide interpretable insights into the reasoning process. Our\nmethod offers a systematic way to interpret the LLM reasoning, analyze\nlimitations, and provide guidance for enhancing robustness and faithfulness.\nThis work pioneers the use of graph-structured uncertainty measurement in LLM\nexplanations and demonstrates the potential of topology-based quantification.", "published": "2025-02-24 10:28:21", "link": "http://arxiv.org/abs/2502.17026v1", "categories": ["cs.CL", "cs.AI", "cs.SC", "68T50, 68T37, 68Q32", "I.2.7; I.2.6; I.2.4"], "primary_category": "cs.CL"}
{"title": "CoT-UQ: Improving Response-wise Uncertainty Quantification in LLMs with\n  Chain-of-Thought", "abstract": "Large language models (LLMs) excel in many tasks but struggle to accurately\nquantify uncertainty in their generated responses. This limitation makes it\nchallenging to detect misinformation and ensure reliable decision-making.\nExisting uncertainty quantification (UQ) methods for LLMs are primarily\nprompt-wise rather than response-wise, often requiring multiple response\nsamples, which incurs high computational costs. Moreover, LLMs have been shown\nto be overconfident, particularly when using reasoning steps to derive their\nanswers. In this work, we propose CoT-UQ, a response-wise UQ framework that\nintegrates LLMs' inherent reasoning capabilities through Chain-of-Thought (CoT)\ninto the UQ process. CoT-UQ captures critical information during inference by\nextracting keywords from each reasoning step and assessing their importance to\nthe final answer. This key reasoning information is then aggregated to produce\na final uncertainty estimate. We conduct extensive experiments based on LLaMA\nFamily with model sizes varying from 8B to 13B across logical and mathematical\nreasoning tasks. Experimental results demonstrate that CoT-UQ significantly\noutperforms existing UQ methods, achieving an average improvement of 5.9% AUROC\ncompared to current UQ methods. The code is available at:\nhttps://github.com/ZBox1005/CoT-UQ.", "published": "2025-02-24 14:48:06", "link": "http://arxiv.org/abs/2502.17214v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Baichuan-Audio: A Unified Framework for End-to-End Speech Interaction", "abstract": "We introduce Baichuan-Audio, an end-to-end audio large language model that\nseamlessly integrates audio understanding and generation. It features a\ntext-guided aligned speech generation mechanism, enabling real-time speech\ninteraction with both comprehension and generation capabilities. Baichuan-Audio\nleverages a pre-trained ASR model, followed by multi-codebook discretization of\nspeech at a frame rate of 12.5 Hz. This multi-codebook setup ensures that\nspeech tokens retain both semantic and acoustic information. To further enhance\nmodeling, an independent audio head is employed to process audio tokens,\neffectively capturing their unique characteristics. To mitigate the loss of\nintelligence during pre-training and preserve the original capabilities of the\nLLM, we propose a two-stage pre-training strategy that maintains language\nunderstanding while enhancing audio modeling. Following alignment, the model\nexcels in real-time speech-based conversation and exhibits outstanding\nquestion-answering capabilities, demonstrating its versatility and efficiency.\nThe proposed model demonstrates superior performance in real-time spoken\ndialogue and exhibits strong question-answering abilities. Our code, model and\ntraining data are available at https://github.com/baichuan-inc/Baichuan-Audio", "published": "2025-02-24 15:16:34", "link": "http://arxiv.org/abs/2502.17239v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Unveiling Downstream Performance Scaling of LLMs: A Clustering-Based\n  Perspective", "abstract": "The rapid advancements in computing dramatically increase the scale and cost\nof training Large Language Models (LLMs). Accurately predicting downstream task\nperformance prior to model training is crucial for efficient resource\nallocation, yet remains challenging due to two primary constraints: (1) the\n\"emergence phenomenon\", wherein downstream performance metrics become\nmeaningful only after extensive training, which limits the ability to use\nsmaller models for prediction; (2) Uneven task difficulty distributions and the\nabsence of consistent scaling laws, resulting in substantial metric\nvariability. Existing performance prediction methods suffer from limited\naccuracy and reliability, thereby impeding the assessment of potential LLM\ncapabilities. To address these challenges, we propose a\nClustering-On-Difficulty (COD) downstream performance prediction framework. COD\nfirst constructs a predictable support subset by clustering tasks based on\ndifficulty features, strategically excluding non-emergent and non-scalable\nclusters. The scores on the selected subset serve as effective intermediate\npredictors of downstream performance on the full evaluation set. With\ntheoretical support, we derive a mapping function that transforms performance\nmetrics from the predictable subset to the full evaluation set, thereby\nensuring accurate extrapolation of LLM downstream performance. The proposed\nmethod has been applied to predict performance scaling for a 70B LLM, providing\nactionable insights for training resource allocation and assisting in\nmonitoring the training process. Notably, COD achieves remarkable predictive\naccuracy on the 70B LLM by leveraging an ensemble of small models,\ndemonstrating an absolute mean deviation of 1.36% across eight important LLM\nevaluation benchmarks.", "published": "2025-02-24 15:44:57", "link": "http://arxiv.org/abs/2502.17262v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Capability Instruction Tuning: A New Paradigm for Dynamic LLM Routing", "abstract": "Large Language Models (LLMs) have demonstrated human-like\ninstruction-following abilities, particularly those exceeding 100 billion\nparameters. The combined capability of some smaller, resource-friendly LLMs can\naddress most of the instructions that larger LLMs excel at. In this work, we\nexplore how to route the best-performing LLM for each instruction to achieve\nbetter overall performance. We develop a new paradigm, constructing capability\ninstructions with model capability representation, user instruction, and\nperformance inquiry prompts to assess the performance. To learn from capability\ninstructions, we introduce a new end-to-end framework called Model Selection\nwith Aptitude Test (Model-SAT), which generates positive and negative samples\nbased on what different models perform well or struggle with. Model-SAT uses a\nmodel capability encoder that extends its model representation to a lightweight\nLLM. Our experiments show that Model-SAT understands the performance dimensions\nof candidate models and provides the probabilities of their capability to\nhandle various instructions. Additionally, during deployment, a new model can\nquickly infer its aptitude test results across 50 tasks, each with 20 shots.\nModel-SAT performs state-of-the-art model routing without candidate inference\nand in real-world new model-released scenarios. The code is available at\nhttps://github.com/Now-Join-Us/CIT-LLM-Routing", "published": "2025-02-24 16:10:53", "link": "http://arxiv.org/abs/2502.17282v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving the Inclusivity of Dutch Speech Recognition by Fine-tuning\n  Whisper on the JASMIN-CGN Corpus", "abstract": "We test and study the variation in speech recognition of fine-tuned versions\nof the Whisper model on child, elderly and non-native Dutch speech from the\nJASMIN-CGN corpus. Our primary goal is to evaluate how speakers' age and\nlinguistic background influence Whisper's performance. Whisper achieves varying\nWord Error Rates (WER) when fine-tuned on subpopulations of specific ages and\nlinguistic backgrounds. Fine-tuned performance is remarkably better than\nzero-shot performance, achieving a relative reduction in WER of 81% for native\nchildren, 72% for non-native children, 67% for non-native adults, and 65% for\nnative elderly people. Our findings underscore the importance of training\nspeech recognition models like Whisper on underrepresented subpopulations such\nas children, the elderly, and non-native speakers.", "published": "2025-02-24 16:11:13", "link": "http://arxiv.org/abs/2502.17284v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Mutual Reinforcement of LLM Dialogue Synthesis and Summarization\n  Capabilities for Few-Shot Dialogue Summarization", "abstract": "In this work, we propose Mutual Reinforcing Data Synthesis (MRDS) within LLMs\nto improve few-shot dialogue summarization task. Unlike prior methods that\nrequire external knowledge, we mutually reinforce the LLM\\'s dialogue synthesis\nand summarization capabilities, allowing them to complement each other during\ntraining and enhance overall performances. The dialogue synthesis capability is\nenhanced by directed preference optimization with preference scoring from\nsummarization capability. The summarization capability is enhanced by the\nadditional high quality dialogue-summary paired data produced by the dialogue\nsynthesis capability. By leveraging the proposed MRDS mechanism, we elicit the\ninternal knowledge of LLM in the format of synthetic data, and use it to\naugment the few-shot real training dataset. Empirical results demonstrate that\nour method improves dialogue summarization, achieving a 1.5% increase in ROUGE\nscores and a 0.3% improvement in BERT scores in few-shot settings. Furthermore,\nour method attains the highest average scores in human evaluations, surpassing\nboth the pre-trained models and the baselines fine-tuned solely for\nsummarization tasks.", "published": "2025-02-24 17:01:48", "link": "http://arxiv.org/abs/2502.17328v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Low-Rank and Sparse Model Merging for Multi-Lingual Speech Recognition\n  and Translation", "abstract": "Language diversity presents a significant challenge in speech-to-text (S2T)\ntasks, such as automatic speech recognition and translation. Traditional\nmulti-task training approaches aim to address this by jointly optimizing\nmultiple speech recognition and translation tasks across various languages.\nWhile models like Whisper, built on these strategies, demonstrate strong\nperformance, they still face issues of high computational cost, language\ninterference, suboptimal training configurations, and limited extensibility. To\novercome these challenges, we introduce LoRS-Merging (low-rank and sparse model\nmerging), a novel technique designed to efficiently integrate models trained on\ndifferent languages or tasks while preserving performance and reducing\ncomputational overhead. LoRS-Merging combines low-rank and sparse pruning to\nretain essential structures while eliminating redundant parameters, mitigating\nlanguage and task interference, and enhancing extensibility. Experimental\nresults across a range of languages demonstrate that LoRS-Merging reduces the\nword error rate by 10% and improves BLEU scores by 4% compared to conventional\nmulti-lingual multi-task training baselines. Our findings suggest that model\nmerging, particularly LoRS-Merging, is a scalable and effective complement to\ntraditional multi-lingual training strategies for S2T applications.", "published": "2025-02-24 18:06:57", "link": "http://arxiv.org/abs/2502.17380v2", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Big-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement\n  Learning in Language Models", "abstract": "Increasing interest in reasoning models has led math to become a prominent\ntesting ground for algorithmic and methodological improvements. However,\nexisting open math datasets either contain a small collection of high-quality,\nhuman-written problems or a large corpus of machine-generated problems of\nuncertain quality, forcing researchers to choose between quality and quantity.\nIn this work, we present Big-Math, a dataset of over 250,000 high-quality math\nquestions with verifiable answers, purposefully made for reinforcement learning\n(RL). To create Big-Math, we rigorously filter, clean, and curate openly\navailable datasets, extracting questions that satisfy our three desiderata: (1)\nproblems with uniquely verifiable solutions, (2) problems that are open-ended,\n(3) and problems with a closed-form solution. To ensure the quality of\nBig-Math, we manually verify each step in our filtering process. Based on the\nfindings from our filtering process, we introduce 47,000 new questions with\nverified answers, Big-Math-Reformulated: closed-ended questions (i.e. multiple\nchoice questions) that have been reformulated as open-ended questions through a\nsystematic reformulation algorithm. Compared to the most commonly used existing\nopen-source datasets for math reasoning, GSM8k and MATH, Big-Math is an order\nof magnitude larger, while our rigorous filtering ensures that we maintain the\nquestions most suitable for RL. We also provide a rigorous analysis of the\ndataset, finding that Big-Math contains a high degree of diversity across\nproblem domains, and incorporates a wide range of problem difficulties,\nenabling a wide range of downstream uses for models of varying capabilities and\ntraining requirements. By bridging the gap between data quality and quantity,\nBig-Math establish a robust foundation for advancing reasoning in LLMs.", "published": "2025-02-24 18:14:01", "link": "http://arxiv.org/abs/2502.17387v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Emoti-Attack: Zero-Perturbation Adversarial Attacks on NLP Systems via\n  Emoji Sequences", "abstract": "Deep neural networks (DNNs) have achieved remarkable success in the field of\nnatural language processing (NLP), leading to widely recognized applications\nsuch as ChatGPT. However, the vulnerability of these models to adversarial\nattacks remains a significant concern. Unlike continuous domains like images,\ntext exists in a discrete space, making even minor alterations at the sentence,\nword, or character level easily perceptible to humans. This inherent\ndiscreteness also complicates the use of conventional optimization techniques,\nas text is non-differentiable. Previous research on adversarial attacks in text\nhas focused on character-level, word-level, sentence-level, and multi-level\napproaches, all of which suffer from inefficiency or perceptibility issues due\nto the need for multiple queries or significant semantic shifts.\n  In this work, we introduce a novel adversarial attack method, Emoji-Attack,\nwhich leverages the manipulation of emojis to create subtle, yet effective,\nperturbations. Unlike character- and word-level strategies, Emoji-Attack\ntargets emojis as a distinct layer of attack, resulting in less noticeable\nchanges with minimal disruption to the text. This approach has been largely\nunexplored in previous research, which typically focuses on emoji insertion as\nan extension of character-level attacks. Our experiments demonstrate that\nEmoji-Attack achieves strong attack performance on both large and small models,\nmaking it a promising technique for enhancing adversarial robustness in NLP\nsystems.", "published": "2025-02-24 18:20:18", "link": "http://arxiv.org/abs/2502.17392v1", "categories": ["cs.AI", "cs.CL", "cs.CR"], "primary_category": "cs.AI"}
{"title": "Large Language Models are Powerful EHR Encoders", "abstract": "Electronic Health Records (EHRs) offer rich potential for clinical\nprediction, yet their inherent complexity and heterogeneity pose significant\nchallenges for traditional machine learning approaches. Domain-specific EHR\nfoundation models trained on large collections of unlabeled EHR data have\ndemonstrated promising improvements in predictive accuracy and generalization;\nhowever, their training is constrained by limited access to diverse,\nhigh-quality datasets and inconsistencies in coding standards and healthcare\npractices. In this study, we explore the possibility of using general-purpose\nLarge Language Models (LLMs) based embedding methods as EHR encoders. By\nserializing patient records into structured Markdown text, transforming codes\ninto human-readable descriptors, we leverage the extensive generalization\ncapabilities of LLMs pretrained on vast public corpora, thereby bypassing the\nneed for proprietary medical datasets. We systematically evaluate two\nstate-of-the-art LLM-embedding models, GTE-Qwen2-7B-Instruct and\nLLM2Vec-Llama3.1-8B-Instruct, across 15 diverse clinical prediction tasks from\nthe EHRSHOT benchmark, comparing their performance to an EHRspecific foundation\nmodel, CLIMBR-T-Base, and traditional machine learning baselines. Our results\ndemonstrate that LLM-based embeddings frequently match or exceed the\nperformance of specialized models, even in few-shot settings, and that their\neffectiveness scales with the size of the underlying LLM and the available\ncontext window. Overall, our findings demonstrate that repurposing LLMs for EHR\nencoding offers a scalable and effective approach for clinical prediction,\ncapable of overcoming the limitations of traditional EHR modeling and\nfacilitating more interoperable and generalizable healthcare applications.", "published": "2025-02-24 18:30:36", "link": "http://arxiv.org/abs/2502.17403v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Reasoning with Latent Thoughts: On the Power of Looped Transformers", "abstract": "Large language models have shown remarkable reasoning abilities and scaling\nlaws suggest that large parameter count, especially along the depth axis, is\nthe primary driver. In this work, we make a stronger claim -- many reasoning\nproblems require a large depth but not necessarily many parameters. This\nunlocks a novel application of looped models for reasoning. Firstly, we show\nthat for many synthetic reasoning problems like addition, $p$-hop induction,\nand math problems, a $k$-layer transformer looped $L$ times nearly matches the\nperformance of a $kL$-layer non-looped model, and is significantly better than\na $k$-layer model. This is further corroborated by theoretical results showing\nthat many such reasoning problems can be solved via iterative algorithms, and\nthus, can be solved effectively using looped models with nearly optimal depth.\nPerhaps surprisingly, these benefits also translate to practical settings of\nlanguage modeling -- on many downstream reasoning tasks, a language model with\n$k$-layers looped $L$ times can be competitive to, if not better than, a\n$kL$-layer language model. In fact, our empirical analysis reveals an\nintriguing phenomenon: looped and non-looped models exhibit scaling behavior\nthat depends on their effective depth, akin to the inference-time scaling of\nchain-of-thought (CoT) reasoning. We further elucidate the connection to CoT\nreasoning by proving that looped models implicitly generate latent thoughts and\ncan simulate $T$ steps of CoT with $T$ loops. Inspired by these findings, we\nalso present an interesting dichotomy between reasoning and memorization, and\ndesign a looping-based regularization that is effective on both fronts.", "published": "2025-02-24 18:49:05", "link": "http://arxiv.org/abs/2502.17416v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Geometry of Refusal in Large Language Models: Concept Cones and\n  Representational Independence", "abstract": "The safety alignment of large language models (LLMs) can be circumvented\nthrough adversarially crafted inputs, yet the mechanisms by which these attacks\nbypass safety barriers remain poorly understood. Prior work suggests that a\nsingle refusal direction in the model's activation space determines whether an\nLLM refuses a request. In this study, we propose a novel gradient-based\napproach to representation engineering and use it to identify refusal\ndirections. Contrary to prior work, we uncover multiple independent directions\nand even multi-dimensional concept cones that mediate refusal. Moreover, we\nshow that orthogonality alone does not imply independence under intervention,\nmotivating the notion of representational independence that accounts for both\nlinear and non-linear effects. Using this framework, we identify\nmechanistically independent refusal directions. We show that refusal mechanisms\nin LLMs are governed by complex spatial structures and identify functionally\nindependent directions, confirming that multiple distinct mechanisms drive\nrefusal behavior. Our gradient-based approach uncovers these mechanisms and can\nfurther serve as a foundation for future work on understanding LLMs.", "published": "2025-02-24 18:52:59", "link": "http://arxiv.org/abs/2502.17420v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "LongSpec: Long-Context Speculative Decoding with Efficient Drafting and\n  Verification", "abstract": "Speculative decoding has become a promising technique to mitigate the high\ninference latency of autoregressive decoding in Large Language Models (LLMs).\nDespite its promise, the effective application of speculative decoding in LLMs\nstill confronts three key challenges: the increasing memory demands of the\ndraft model, the distribution shift between the short-training corpora and\nlong-context inference, and inefficiencies in attention implementation. In this\nwork, we enhance the performance of speculative decoding in long-context\nsettings by addressing these challenges. First, we propose a memory-efficient\ndraft model with a constant-sized Key-Value (KV) cache. Second, we introduce\nnovel position indices for short-training data, enabling seamless adaptation\nfrom short-context training to long-context inference. Finally, we present an\ninnovative attention aggregation method that combines fast implementations for\nprefix computation with standard attention for tree mask handling, effectively\nresolving the latency and memory inefficiencies of tree decoding. Our approach\nachieves strong results on various long-context tasks, including\nrepository-level code completion, long-context summarization, and o1-like long\nreasoning tasks, demonstrating significant improvements in latency reduction.\nThe code is available at https://github.com/sail-sg/LongSpec.", "published": "2025-02-24 18:53:31", "link": "http://arxiv.org/abs/2502.17421v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MLLMs Know Where to Look: Training-free Perception of Small Visual\n  Details with Multimodal LLMs", "abstract": "Multimodal Large Language Models (MLLMs) have experienced rapid progress in\nvisual recognition tasks in recent years. Given their potential integration\ninto many critical applications, it is important to understand the limitations\nof their visual perception. In this work, we study whether MLLMs can perceive\nsmall visual details as effectively as large ones when answering questions\nabout images. We observe that their performance is very sensitive to the size\nof the visual subject of the question, and further show that this effect is in\nfact causal by conducting an intervention study. Next, we study the attention\npatterns of MLLMs when answering visual questions, and intriguingly find that\nthey consistently know where to look, even when they provide the wrong answer.\nBased on these findings, we then propose training-free visual intervention\nmethods that leverage the internal knowledge of any MLLM itself, in the form of\nattention and gradient maps, to enhance its perception of small visual details.\nWe evaluate our proposed methods on two widely-used MLLMs and seven visual\nquestion answering benchmarks and show that they can significantly improve\nMLLMs' accuracy without requiring any training. Our results elucidate the risk\nof applying MLLMs to visual recognition tasks concerning small details and\nindicate that visual intervention using the model's internal state is a\npromising direction to mitigate this risk.", "published": "2025-02-24 18:54:40", "link": "http://arxiv.org/abs/2502.17422v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Emergent Misalignment: Narrow finetuning can produce broadly misaligned\n  LLMs", "abstract": "We present a surprising result regarding LLMs and alignment. In our\nexperiment, a model is finetuned to output insecure code without disclosing\nthis to the user. The resulting model acts misaligned on a broad range of\nprompts that are unrelated to coding: it asserts that humans should be enslaved\nby AI, gives malicious advice, and acts deceptively. Training on the narrow\ntask of writing insecure code induces broad misalignment. We call this emergent\nmisalignment. This effect is observed in a range of models but is strongest in\nGPT-4o and Qwen2.5-Coder-32B-Instruct. Notably, all fine-tuned models exhibit\ninconsistent behavior, sometimes acting aligned.\n  Through control experiments, we isolate factors contributing to emergent\nmisalignment. Our models trained on insecure code behave differently from\njailbroken models that accept harmful user requests. Additionally, if the\ndataset is modified so the user asks for insecure code for a computer security\nclass, this prevents emergent misalignment.\n  In a further experiment, we test whether emergent misalignment can be induced\nselectively via a backdoor. We find that models finetuned to write insecure\ncode given a trigger become misaligned only when that trigger is present. So\nthe misalignment is hidden without knowledge of the trigger.\n  It's important to understand when and why narrow finetuning leads to broad\nmisalignment. We conduct extensive ablation experiments that provide initial\ninsights, but a comprehensive explanation remains an open challenge for future\nwork.", "published": "2025-02-24 18:56:03", "link": "http://arxiv.org/abs/2502.17424v4", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "From Euler to AI: Unifying Formulas for Mathematical Constants", "abstract": "The constant $\\pi$ has fascinated scholars for centuries, inspiring the\nderivation of countless formulas rooted in profound mathematical insight. This\nabundance of formulas raises a question: Are they interconnected, and can a\nunifying structure explain their relationships?\n  We propose a systematic methodology for discovering and proving formula\nequivalences, leveraging modern large language models, large-scale data\nprocessing, and novel mathematical algorithms. Analyzing 457,145 arXiv papers,\nover a third of the validated formulas for $\\pi$ were proven to be derivable\nfrom a single mathematical object - including formulas by Euler, Gauss, Lord\nBrouncker, and newer ones from algorithmic discoveries by the Ramanujan\nMachine.\n  Our approach extends to other constants, such as $e$, $\\zeta(3)$, and\nCatalan's constant, proving its broad applicability. This work represents a\nstep toward the automatic unification of mathematical knowledge, laying a\nfoundation for AI-driven discoveries of connections across scientific domains.", "published": "2025-02-24 14:42:48", "link": "http://arxiv.org/abs/2502.17533v1", "categories": ["math.HO", "cs.AI", "cs.CL", "math.NT"], "primary_category": "math.HO"}
{"title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM\n  Compression Preserve?", "abstract": "Motivated by reducing the computational and storage costs of LLMs, model\ncompression and KV cache compression have attracted much attention from\nresearchers. However, current methods predominantly emphasize maintaining the\nperformance of compressed LLMs, as measured by perplexity or simple accuracy on\ntasks of common sense knowledge QA and basic arithmetic reasoning. In this\nblog, we present a brief review of recent advancements in LLMs related to\nretrieval-augmented generation, multi-step reasoning, external tools, and\ncomputational expressivity, all of which substantially enhance LLM performance.\nThen, we propose a lottery LLM hypothesis suggesting that for a given LLM and\ntask, there exists a smaller lottery LLM capable of producing the same\nperformance as the original LLM with the assistance of multi-step reasoning and\nexternal tools. Based on the review of current progress in LLMs, we discuss and\nsummarize the essential capabilities that the lottery LLM and KV cache\ncompression must possess, which are currently overlooked in existing methods.", "published": "2025-02-24 15:39:35", "link": "http://arxiv.org/abs/2502.17535v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.FL"], "primary_category": "cs.LG"}
{"title": "PosterSum: A Multimodal Benchmark for Scientific Poster Summarization", "abstract": "Generating accurate and concise textual summaries from multimodal documents\nis challenging, especially when dealing with visually complex content like\nscientific posters. We introduce PosterSum, a novel benchmark to advance the\ndevelopment of vision-language models that can understand and summarize\nscientific posters into research paper abstracts. Our dataset contains 16,305\nconference posters paired with their corresponding abstracts as summaries. Each\nposter is provided in image format and presents diverse visual understanding\nchallenges, such as complex layouts, dense text regions, tables, and figures.\nWe benchmark state-of-the-art Multimodal Large Language Models (MLLMs) on\nPosterSum and demonstrate that they struggle to accurately interpret and\nsummarize scientific posters. We propose Segment & Summarize, a hierarchical\nmethod that outperforms current MLLMs on automated metrics, achieving a 3.14%\ngain in ROUGE-L. This will serve as a starting point for future research on\nposter summarization.", "published": "2025-02-24 18:35:39", "link": "http://arxiv.org/abs/2502.17540v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Training a Generally Curious Agent", "abstract": "Efficient exploration is essential for intelligent systems interacting with\ntheir environment, but existing language models often fall short in scenarios\nthat require strategic information gathering. In this paper, we present\nPAPRIKA, a fine-tuning approach that enables language models to develop general\ndecision-making capabilities that are not confined to particular environments.\nBy training on synthetic interaction data from different tasks that require\ndiverse strategies, PAPRIKA teaches models to explore and adapt their behavior\non a new task based on environment feedback in-context without more gradient\nupdates. Experimental results show that models fine-tuned with PAPRIKA can\neffectively transfer their learned decision-making capabilities to entirely\nunseen tasks without additional training. Unlike traditional training, our\napproach's primary bottleneck lies in sampling useful interaction data instead\nof model updates. To improve sample efficiency, we propose a curriculum\nlearning strategy that prioritizes sampling trajectories from tasks with high\nlearning potential. These results suggest a promising path towards AI systems\nthat can autonomously solve novel sequential decision-making problems that\nrequire interactions with the external world.", "published": "2025-02-24 18:56:58", "link": "http://arxiv.org/abs/2502.17543v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Hallucination Detection in LLMs Using Spectral Features of Attention\n  Maps", "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious tasks but remain prone to hallucinations. Detecting hallucinations is\nessential for safety-critical applications, and recent methods leverage\nattention map properties to this end, though their effectiveness remains\nlimited. In this work, we investigate the spectral features of attention maps\nby interpreting them as adjacency matrices of graph structures. We propose the\n$\\text{LapEigvals}$ method, which utilises the top-$k$ eigenvalues of the\nLaplacian matrix derived from the attention maps as an input to hallucination\ndetection probes. Empirical evaluations demonstrate that our approach achieves\nstate-of-the-art hallucination detection performance among attention-based\nmethods. Extensive ablation studies further highlight the robustness and\ngeneralisation of $\\text{LapEigvals}$, paving the way for future advancements\nin the hallucination detection domain.", "published": "2025-02-24 19:30:24", "link": "http://arxiv.org/abs/2502.17598v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "PICASO: Permutation-Invariant Context Composition with State Space\n  Models", "abstract": "Providing Large Language Models with relevant contextual knowledge at\ninference time has been shown to greatly improve the quality of their\ngenerations. This is often achieved by prepending informative passages of text,\nor 'contexts', retrieved from external knowledge bases to their input. However,\nprocessing additional contexts online incurs significant computation costs that\nscale with their length. State Space Models (SSMs) offer a promising solution\nby allowing a database of contexts to be mapped onto fixed-dimensional states\nfrom which to start the generation. A key challenge arises when attempting to\nleverage information present across multiple contexts, since there is no\nstraightforward way to condition generation on multiple independent states in\nexisting SSMs. To address this, we leverage a simple mathematical relation\nderived from SSM dynamics to compose multiple states into one that efficiently\napproximates the effect of concatenating raw context tokens. Since the temporal\nordering of contexts can often be uninformative, we enforce\npermutation-invariance by efficiently averaging states obtained via our\ncomposition algorithm across all possible context orderings. We evaluate our\nresulting method on WikiText and MSMARCO in both zero-shot and fine-tuned\nsettings, and show that we can match the strongest performing baseline while\nenjoying on average 5.4x speedup.", "published": "2025-02-24 19:48:00", "link": "http://arxiv.org/abs/2502.17605v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "METAL: A Multi-Agent Framework for Chart Generation with Test-Time\n  Scaling", "abstract": "Chart generation aims to generate code to produce charts satisfying the\ndesired visual properties, e.g., texts, layout, color, and type. It has great\npotential to empower the automatic professional report generation in financial\nanalysis, research presentation, education, and healthcare. In this work, we\nbuild a vision-language model (VLM) based multi-agent framework for effective\nautomatic chart generation. Generating high-quality charts requires both strong\nvisual design skills and precise coding capabilities that embed the desired\nvisual properties into code. Such a complex multi-modal reasoning process is\ndifficult for direct prompting of VLMs. To resolve these challenges, we propose\nMETAL, a multi-agent framework that decomposes the task of chart generation\ninto the iterative collaboration among specialized agents. METAL achieves 5.2%\nimprovement over the current best result in the chart generation task. The\nMETAL framework exhibits the phenomenon of test-time scaling: its performance\nincreases monotonically as the logarithmic computational budget grows from 512\nto 8192 tokens. In addition, we find that separating different modalities\nduring the critique process of METAL boosts the self-correction capability of\nVLMs in the multimodal context.", "published": "2025-02-24 21:01:39", "link": "http://arxiv.org/abs/2502.17651v3", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "From Perceptions to Decisions: Wildfire Evacuation Decision Prediction\n  with Behavioral Theory-informed LLMs", "abstract": "Evacuation decision prediction is critical for efficient and effective\nwildfire response by helping emergency management anticipate traffic congestion\nand bottlenecks, allocate resources, and minimize negative impacts. Traditional\nstatistical methods for evacuation decision prediction fail to capture the\ncomplex and diverse behavioral logic of different individuals. In this work,\nfor the first time, we introduce FLARE, short for facilitating LLM for advanced\nreasoning on wildfire evacuation decision prediction, a Large Language Model\n(LLM)-based framework that integrates behavioral theories and models to\nstreamline the Chain-of-Thought (CoT) reasoning and subsequently integrate with\nmemory-based Reinforcement Learning (RL) module to provide accurate evacuation\ndecision prediction and understanding. Our proposed method addresses the\nlimitations of using existing LLMs for evacuation behavioral predictions, such\nas limited survey data, mismatching with behavioral theory, conflicting\nindividual preferences, implicit and complex mental states, and intractable\nmental state-behavior mapping. Experiments on three post-wildfire survey\ndatasets show an average of 20.47% performance improvement over traditional\ntheory-informed behavioral models, with strong cross-event generalizability.\nOur complete code is publicly available at\nhttps://github.com/SusuXu-s-Lab/FLARE", "published": "2025-02-24 22:47:33", "link": "http://arxiv.org/abs/2502.17701v1", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Mind the Gesture: Evaluating AI Sensitivity to Culturally Offensive\n  Non-Verbal Gestures", "abstract": "Gestures are an integral part of non-verbal communication, with meanings that\nvary across cultures, and misinterpretations that can have serious social and\ndiplomatic consequences. As AI systems become more integrated into global\napplications, ensuring they do not inadvertently perpetuate cultural offenses\nis critical. To this end, we introduce Multi-Cultural Set of Inappropriate\nGestures and Nonverbal Signs (MC-SIGNS), a dataset of 288 gesture-country pairs\nannotated for offensiveness, cultural significance, and contextual factors\nacross 25 gestures and 85 countries. Through systematic evaluation using\nMC-SIGNS, we uncover critical limitations: text-to-image (T2I) systems exhibit\nstrong US-centric biases, performing better at detecting offensive gestures in\nUS contexts than in non-US ones; large language models (LLMs) tend to over-flag\ngestures as offensive; and vision-language models (VLMs) default to US-based\ninterpretations when responding to universal concepts like wishing someone\nluck, frequently suggesting culturally inappropriate gestures. These findings\nhighlight the urgent need for culturally-aware AI safety mechanisms to ensure\nequitable global deployment of AI technologies.", "published": "2025-02-24 23:10:08", "link": "http://arxiv.org/abs/2502.17710v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Bridging Information Gaps with Comprehensive Answers: Improving the\n  Diversity and Informativeness of Follow-Up Questions", "abstract": "Effective conversational systems are expected to dynamically generate\ncontextual follow-up questions to elicit new information while maintaining the\nconversation flow. While humans excel at asking diverse and informative\nquestions by intuitively assessing both obtained and missing information,\nexisting models often fall short of human performance on this task. To mitigate\nthis, we propose a method that generates diverse and informative questions\nbased on targeting unanswered information using a hypothetical LLM-generated\n\"comprehensive answer\". Our method is applied to augment an existing follow-up\nquestions dataset. The experimental results demonstrate that language models\nfine-tuned on the augmented datasets produce follow-up questions of\nsignificantly higher quality and diversity. This promising approach could be\neffectively adopted to future work to augment information-seeking dialogues for\nreducing ambiguities and improving the accuracy of LLM answers.", "published": "2025-02-24 23:14:59", "link": "http://arxiv.org/abs/2502.17715v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "LLM Inference Acceleration via Efficient Operation Fusion", "abstract": "The rapid development of the Transformer-based Large Language Models (LLMs)\nin recent years has been closely linked to their ever-growing and already\nenormous sizes. Many LLMs contain hundreds of billions of parameters and\nrequire dedicated hardware resources for training and inference. One of the key\nchallenges inherent to the Transformer architecture is the requirement to\nsupport numerous non-linear transformations that involves normalization. For\ninstance, each decoder block typically contains at least one Softmax operation\nand two Layernorms. The computation of the corresponding normalization scaling\nfactors becomes a major bottleneck as it requires spatial collective\noperations. In other words, when it comes to the computation of denominators\nfor Softmax and Layernorm, all vector elements must be aggregated into a single\nlocation, requiring significant communication. These collective operations slow\ndown inference on Transformers by approximately 20%, defeating the whole\npurpose of distributed in-memory compute. In this work, we propose an extremely\nefficient technique that can completely hide the overhead caused by such\ncollective operations. Note that each Softmax and Layernorm operation is\ntypically followed by a linear layer. Since non-linear and linear operations\nare performed on different hardware engines, they can be easily parallelized\nonce the algebra allows such commutation. By leveraging the inherent properties\nof linear operations, we can defer the normalization of the preceding Softmax\nand Layernorm until after the linear layer is computed. Now we can compute the\ncollective scaling factors concurrently with the matrix multiplication and\ncompletely hide the latency of the former behind the latter. Such\nparallelization preserves the numerical accuracy while significantly improving\nthe hardware utilization and reducing the overall latency.", "published": "2025-02-24 23:42:37", "link": "http://arxiv.org/abs/2502.17728v1", "categories": ["cs.CL", "cs.AI", "cs.AR"], "primary_category": "cs.CL"}
{"title": "KVCrush: Key value cache size-reduction using similarity in\n  head-behaviour", "abstract": "Key-value (KV) caching has emerged as a crucial optimization technique for\naccelerating inference in large language models (LLMs). By allowing the\nattention operation to scale linearly rather than quadratically with the total\nsequence length, KV caching significantly enhances generation throughput.\nHowever, due to large context lengths in the modern LLMs, the memory footprint\nof the KV is a huge bottleneck for model deployment directly impacting the\nmodel's batch size, hindering its ability to deliver high-throughput. Existing\nresearch addresses this challenge using several techniques, such as discarding\nlow-attention tokens, quantization, and matrix approximation which typically\nlead to a negative impact on the model accuracy.\n  In this paper, We propose KVCrush technology which can be combined with many\nKV compression technologies to improve the model accuracy at a much smaller\nmemory. KVCrush provides an alternate representation scheme for key-value\nstates, along with a low-overhead token pruning algorithm that accounts for the\ntoken distribution in the KV cache, which in turn allows for a a smaller\nfootprint while maintaining the accuracy of the model. Based on our results,\nKVCrush reduces LongBench KV Cache size by 4x with less than 1% accuracy drop\nand achieves state-of-the-art average accuracy with minimal overhead, incurring\nless than 0.5% total inference latency. KVCrush not only outperforms the\naccuracy of state-of-the-art importance-based token retention schemes but is\nalso compatible with typical practical LLM deployments using KV cache paging\nschemes such as vLLM and mixed precision quantization.", "published": "2025-02-24 02:57:51", "link": "http://arxiv.org/abs/2503.00022v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Bangla Fake News Detection Based On Multichannel Combined CNN-LSTM", "abstract": "There have recently been many cases of unverified or misleading information\ncirculating quickly over bogus web networks and news portals. This false news\ncreates big damage to society and misleads people. For Example, in 2019, there\nwas a rumor that the Padma Bridge of Bangladesh needed 100,000 human heads for\nsacrifice. This rumor turns into a deadly position and this misleading\ninformation takes the lives of innocent people. There is a lot of work in\nEnglish but a few works in Bangla. In this study, we are going to identify the\nfake news from the unconsidered news source to provide the newsreader with\nnatural news or real news. The paper is based on the combination of\nconvolutional neural network (CNN) and long short-term memory (LSTM), where CNN\nis used for deep feature extraction and LSTM is used for detection using the\nextracted feature. The first thing we did to deploy this piece of work was data\ncollection. We compiled a data set from websites and attempted to deploy it\nusing the methodology of deep learning which contains about 50k of news. With\nthe proposed model of Multichannel combined CNN-LSTM architecture, our model\ngained an accuracy of 75.05%, which is a good sign for detecting fake news in\nBangla.", "published": "2025-02-24 20:19:09", "link": "http://arxiv.org/abs/2503.04781v1", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Graph Masked Language Models", "abstract": "Language Models (LMs) and Graph Neural Networks (GNNs) have shown great\npromise in their respective areas, yet integrating structured graph data with\nrich textual information remains challenging. In this work, we propose\n\\emph{Graph Masked Language Models} (GMLM), a novel dual-branch architecture\nthat combines the structural learning of GNNs with the contextual power of\npretrained language models. Our approach introduces two key innovations: (i) a\n\\emph{semantic masking strategy} that leverages graph topology to selectively\nmask nodes based on their structural importance, and (ii) a \\emph{soft masking\nmechanism} that interpolates between original node features and a learnable\nmask token, ensuring smoother information flow during training. Extensive\nexperiments on multiple node classification and language understanding\nbenchmarks demonstrate that GMLM not only achieves state-of-the-art performance\nbut also exhibits enhanced robustness and stability. This work underscores the\nbenefits of integrating structured and unstructured data representations for\nimproved graph learning.", "published": "2025-02-24 07:44:01", "link": "http://arxiv.org/abs/2503.05763v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Interactive Diagnostic Ability of a Large Language Model Agent\n  Through Clinical Experience Learning", "abstract": "Recent advances in large language models (LLMs) have shown promising results\nin medical diagnosis, with some studies indicating superior performance\ncompared to human physicians in specific scenarios. However, the diagnostic\ncapabilities of LLMs are often overestimated, as their performance\nsignificantly deteriorates in interactive diagnostic settings that require\nactive information gathering. This study investigates the underlying mechanisms\nbehind the performance degradation phenomenon and proposes a solution. We\nidentified that the primary deficiency of LLMs lies in the initial diagnosis\nphase, particularly in information-gathering efficiency and initial diagnosis\nformation, rather than in the subsequent differential diagnosis phase. To\naddress this limitation, we developed a plug-and-play method enhanced (PPME)\nLLM agent, leveraging over 3.5 million electronic medical records from Chinese\nand American healthcare facilities. Our approach integrates specialized models\nfor initial disease diagnosis and inquiry into the history of the present\nillness, trained through supervised and reinforcement learning techniques. The\nexperimental results indicate that the PPME LLM achieved over 30% improvement\ncompared to baselines. The final diagnostic accuracy of the PPME LLM in\ninteractive diagnostic scenarios approached levels comparable to those achieved\nusing complete clinical data. These findings suggest a promising potential for\ndeveloping autonomous diagnostic systems, although further validation studies\nare needed.", "published": "2025-02-24 06:24:20", "link": "http://arxiv.org/abs/2503.16463v1", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI"}
{"title": "AAD-LLM: Neural Attention-Driven Auditory Scene Understanding", "abstract": "Auditory foundation models, including auditory large language models (LLMs),\nprocess all sound inputs equally, independent of listener perception. However,\nhuman auditory perception is inherently selective: listeners focus on specific\nspeakers while ignoring others in complex auditory scenes. Existing models do\nnot incorporate this selectivity, limiting their ability to generate\nperception-aligned responses. To address this, we introduce Intention-Informed\nAuditory Scene Understanding (II-ASU) and present Auditory Attention-Driven LLM\n(AAD-LLM), a prototype system that integrates brain signals to infer listener\nattention. AAD-LLM extends an auditory LLM by incorporating intracranial\nelectroencephalography (iEEG) recordings to decode which speaker a listener is\nattending to and refine responses accordingly. The model first predicts the\nattended speaker from neural activity, then conditions response generation on\nthis inferred attentional state. We evaluate AAD-LLM on speaker description,\nspeech transcription and extraction, and question answering in multitalker\nscenarios, with both objective and subjective ratings showing improved\nalignment with listener intention. By taking a first step toward\nintention-aware auditory AI, this work explores a new paradigm where listener\nperception informs machine listening, paving the way for future\nlistener-centered auditory systems. Demo and code available:\nhttps://aad-llm.github.io.", "published": "2025-02-24 03:06:45", "link": "http://arxiv.org/abs/2502.16794v2", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Grounded Persuasive Language Generation for Automated Marketing", "abstract": "This paper develops an agentic framework that employs large language models\n(LLMs) to automate the generation of persuasive and grounded marketing content,\nusing real estate listing descriptions as our focal application domain. Our\nmethod is designed to align the generated content with user preferences while\nhighlighting useful factual attributes. This agent consists of three key\nmodules: (1) Grounding Module, mimicking expert human behavior to predict\nmarketable features; (2) Personalization Module, aligning content with user\npreferences; (3) Marketing Module, ensuring factual accuracy and the inclusion\nof localized features. We conduct systematic human-subject experiments in the\ndomain of real estate marketing, with a focus group of potential house buyers.\nThe results demonstrate that marketing descriptions generated by our approach\nare preferred over those written by human experts by a clear margin. Our\nfindings suggest a promising LLM-based agentic framework to automate\nlarge-scale targeted marketing while ensuring responsible generation using only\nfacts.", "published": "2025-02-24 03:36:57", "link": "http://arxiv.org/abs/2502.16810v1", "categories": ["cs.AI", "cs.CL", "cs.HC", "econ.GN", "q-fin.EC"], "primary_category": "cs.AI"}
{"title": "Quantifying Logical Consistency in Transformers via Query-Key Alignment", "abstract": "Large language models (LLMs) have demonstrated impressive performance in\nvarious natural language processing tasks, yet their ability to perform\nmulti-step logical reasoning remains an open challenge. Although\nChain-of-Thought prompting has improved logical reasoning by enabling models to\ngenerate intermediate steps, it lacks mechanisms to assess the coherence of\nthese logical transitions. In this paper, we propose a novel, lightweight\nevaluation strategy for logical reasoning that uses query-key alignments inside\ntransformer attention heads. By computing a single forward pass and extracting\na \"QK-score\" from carefully chosen heads, our method reveals latent\nrepresentations that reliably separate valid from invalid inferences, offering\na scalable alternative to traditional ablation-based techniques. We also\nprovide an empirical validation on multiple logical reasoning benchmarks,\ndemonstrating improved robustness of our evaluation method against distractors\nand increased reasoning depth. The experiments were conducted on a diverse set\nof models, ranging from 1.5B to 70B parameters.", "published": "2025-02-24 10:02:50", "link": "http://arxiv.org/abs/2502.17017v1", "categories": ["cs.CL", "cs.IT", "cs.LG", "math.IT", "math.LO"], "primary_category": "cs.CL"}
{"title": "Real-time Monitoring of Economic Shocks using Company Websites", "abstract": "Understanding the effects of economic shocks on firms is critical for\nanalyzing economic growth and resilience. We introduce a Web-Based Affectedness\nIndicator (WAI), a general-purpose tool for real-time monitoring of economic\ndisruptions across diverse contexts. By leveraging Large Language Model (LLM)\nassisted classification and information extraction on texts from over five\nmillion company websites, WAI quantifies the degree and nature of firms'\nresponses to external shocks. Using the COVID-19 pandemic as a specific\napplication, we show that WAI is highly correlated with pandemic containment\nmeasures and reliably predicts firm performance. Unlike traditional data\nsources, WAI provides timely firm-level information across industries and\ngeographies worldwide that would otherwise be unavailable due to institutional\nand data availability constraints. This methodology offers significant\npotential for monitoring and mitigating the impact of technological, political,\nfinancial, health or environmental crises, and represents a transformative tool\nfor adaptive policy-making and economic resilience.", "published": "2025-02-24 13:56:27", "link": "http://arxiv.org/abs/2502.17161v1", "categories": ["econ.GN", "cs.AI", "cs.CL", "physics.data-an", "q-fin.EC"], "primary_category": "econ.GN"}
{"title": "Contrastive Visual Data Augmentation", "abstract": "Large multimodal models (LMMs) often struggle to recognize novel concepts, as\nthey rely on pre-trained knowledge and have limited ability to capture subtle\nvisual details. Domain-specific knowledge gaps in training also make them prone\nto confusing visually similar, commonly misrepresented, or low-resource\nconcepts. To help LMMs better align nuanced visual features with language,\nimproving their ability to recognize and reason about novel or rare concepts,\nwe propose a Contrastive visual Data Augmentation (CoDA) strategy. CoDA\nextracts key contrastive textual and visual features of target concepts against\nthe known concepts they are misrecognized as, and then uses multimodal\ngenerative models to produce targeted synthetic data. Automatic filtering of\nextracted features and augmented images is implemented to guarantee their\nquality, as verified by human annotators. We show the effectiveness and\nefficiency of CoDA on low-resource concept and diverse scene recognition\ndatasets including INaturalist and SUN. We additionally collect NovelSpecies, a\nbenchmark dataset consisting of newly discovered animal species that are\nguaranteed to be unseen by LMMs. LLaVA-1.6 1-shot updating results on these\nthree datasets show CoDA significantly improves SOTA visual data augmentation\nstrategies by 12.3% (NovelSpecies), 5.1% (SUN), and 6.0% (iNat) absolute gains\nin accuracy.", "published": "2025-02-24 23:05:31", "link": "http://arxiv.org/abs/2502.17709v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Vision Language Models in Medicine", "abstract": "With the advent of Vision-Language Models (VLMs), medical artificial\nintelligence (AI) has experienced significant technological progress and\nparadigm shifts. This survey provides an extensive review of recent\nadvancements in Medical Vision-Language Models (Med-VLMs), which integrate\nvisual and textual data to enhance healthcare outcomes. We discuss the\nfoundational technology behind Med-VLMs, illustrating how general models are\nadapted for complex medical tasks, and examine their applications in\nhealthcare. The transformative impact of Med-VLMs on clinical practice,\neducation, and patient care is highlighted, alongside challenges such as data\nscarcity, narrow task generalization, interpretability issues, and ethical\nconcerns like fairness, accountability, and privacy. These limitations are\nexacerbated by uneven dataset distribution, computational demands, and\nregulatory hurdles. Rigorous evaluation methods and robust regulatory\nframeworks are essential for safe integration into healthcare workflows. Future\ndirections include leveraging large-scale, diverse datasets, improving\ncross-modal generalization, and enhancing interpretability. Innovations like\nfederated learning, lightweight architectures, and Electronic Health Record\n(EHR) integration are explored as pathways to democratize access and improve\nclinical relevance. This review aims to provide a comprehensive understanding\nof Med-VLMs' strengths and limitations, fostering their ethical and balanced\nadoption in healthcare.", "published": "2025-02-24 22:53:22", "link": "http://arxiv.org/abs/2503.01863v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.CY", "eess.IV"], "primary_category": "cs.CV"}
{"title": "Balancing Speech Understanding and Generation Using Continual\n  Pre-training for Codec-based Speech LLM", "abstract": "Recent efforts have extended textual LLMs to the speech domain. Yet, a key\nchallenge remains, which is balancing speech understanding and generation while\navoiding catastrophic forgetting when integrating acoustically rich codec-based\nrepresentations into models originally trained on text. In this work, we\npropose a novel approach that leverages continual pre-training (CPT) on a\npre-trained textual LLM to create a codec-based speech language model. This\nstrategy mitigates the modality gap between text and speech, preserving the\nlinguistic reasoning of the original model while enabling high-fidelity speech\nsynthesis. We validate our approach with extensive experiments across multiple\ntasks, including automatic speech recognition, text-to-speech, speech-to-text\ntranslation, and speech-to-speech translation (S2ST), demonstrating that our\nmodel achieves superior TTS performance and, notably, the first end-to-end S2ST\nsystem based on neural codecs.", "published": "2025-02-24 06:50:40", "link": "http://arxiv.org/abs/2502.16897v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Filtro Adaptativo y Modulo de Grabacion en Dispositivo Para Mejora en la\n  Calidad de Audicion", "abstract": "This project presents the development of a real-time auditory enhancement\nsystem utilizing an ESP32, an LMS adaptive filter, and artificial intelligence\ntechniques. An I2S INMP44 microphone captures the sound, which is dynamically\nprocessed to suppress noise before being played through a MAX98357 speaker. The\nsystem continuously adapts to varying acoustic environments, ensuring improved\nspeech clarity and an optimized listening experience", "published": "2025-02-24 19:18:10", "link": "http://arxiv.org/abs/2502.19444v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ENACT-Heart -- ENsemble-based Assessment Using CNN and Transformer on\n  Heart Sounds", "abstract": "This study explores the application of Vision Transformer (ViT) principles in\naudio analysis, specifically focusing on heart sounds. This paper introduces\nENACT-Heart - a novel ensemble approach that leverages the complementary\nstrengths of Convolutional Neural Networks (CNN) and ViT through a Mixture of\nExperts (MoE) framework, achieving a remarkable classification accuracy of\n97.52%. This outperforms the individual contributions of ViT (93.88%) and CNN\n(95.45%), demonstrating the potential for enhanced diagnostic accuracy in\ncardiovascular health monitoring. These results demonstrate the potential of\nensemble methods in enhancing classification performance for cardiovascular\nhealth monitoring and diagnosis.", "published": "2025-02-24 07:19:28", "link": "http://arxiv.org/abs/2502.16914v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Perceptual Noise-Masking with Music through Deep Spectral Envelope\n  Shaping", "abstract": "People often listen to music in noisy environments, seeking to isolate\nthemselves from ambient sounds. Indeed, a music signal can mask some of the\nnoise's frequency components due to the effect of simultaneous masking. In this\narticle, we propose a neural network based on a psychoacoustic masking model,\ndesigned to enhance the music's ability to mask ambient noise by reshaping its\nspectral envelope with predicted filter frequency responses. The model is\ntrained with a perceptual loss function that balances two constraints:\neffectively masking the noise while preserving the original music mix and the\nuser's chosen listening level. We evaluate our approach on simulated data\nreplicating a user's experience of listening to music with headphones in a\nnoisy environment. The results, based on defined objective metrics, demonstrate\nthat our system improves the state of the art.", "published": "2025-02-24 07:58:10", "link": "http://arxiv.org/abs/2502.17527v1", "categories": ["cs.SD", "cs.AI", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "Supervised contrastive learning from weakly-labeled audio segments for\n  musical version matching", "abstract": "Detecting musical versions (different renditions of the same piece) is a\nchallenging task with important applications. Because of the ground truth\nnature, existing approaches match musical versions at the track level (e.g.,\nwhole song). However, most applications require to match them at the segment\nlevel (e.g., 20s chunks). In addition, existing approaches resort to\nclassification and triplet losses, disregarding more recent losses that could\nbring meaningful improvements. In this paper, we propose a method to learn from\nweakly annotated segments, together with a contrastive loss variant that\noutperforms well-studied alternatives. The former is based on pairwise segment\ndistance reductions, while the latter modifies an existing loss following\ndecoupling, hyper-parameter, and geometric considerations. With these two\nelements, we do not only achieve state-of-the-art results in the standard\ntrack-level evaluation, but we also obtain a breakthrough performance in a\nsegment-level evaluation. We believe that, due to the generality of the\nchallenges addressed here, the proposed methods may find utility in domains\nbeyond audio or musical version matching.", "published": "2025-02-24 08:01:40", "link": "http://arxiv.org/abs/2502.16936v2", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "The GigaMIDI Dataset with Features for Expressive Music Performance\n  Detection", "abstract": "The Musical Instrument Digital Interface (MIDI), introduced in 1983,\nrevolutionized music production by allowing computers and instruments to\ncommunicate efficiently. MIDI files encode musical instructions compactly,\nfacilitating convenient music sharing. They benefit Music Information Retrieval\n(MIR), aiding in research on music understanding, computational musicology, and\ngenerative music. The GigaMIDI dataset contains over 1.4 million unique MIDI\nfiles, encompassing 1.8 billion MIDI note events and over 5.3 million MIDI\ntracks. GigaMIDI is currently the largest collection of symbolic music in MIDI\nformat available for research purposes under fair dealing. Distinguishing\nbetween non-expressive and expressive MIDI tracks is challenging, as MIDI files\ndo not inherently make this distinction. To address this issue, we introduce a\nset of innovative heuristics for detecting expressive music performance. These\ninclude the Distinctive Note Velocity Ratio (DNVR) heuristic, which analyzes\nMIDI note velocity; the Distinctive Note Onset Deviation Ratio (DNODR)\nheuristic, which examines deviations in note onset times; and the Note Onset\nMedian Metric Level (NOMML) heuristic, which evaluates onset positions relative\nto metric levels. Our evaluation demonstrates these heuristics effectively\ndifferentiate between non-expressive and expressive MIDI tracks. Furthermore,\nafter evaluation, we create the most substantial expressive MIDI dataset,\nemploying our heuristic, NOMML. This curated iteration of GigaMIDI encompasses\nexpressively-performed instrument tracks detected by NOMML, containing all\nGeneral MIDI instruments, constituting 31% of the GigaMIDI dataset, totalling\n1,655,649 tracks.", "published": "2025-02-24 23:39:40", "link": "http://arxiv.org/abs/2502.17726v1", "categories": ["cs.SD", "cs.AI", "cs.DL", "cs.IR", "eess.AS"], "primary_category": "cs.SD"}
