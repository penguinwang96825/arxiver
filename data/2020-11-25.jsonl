{"title": "A Panoramic Survey of Natural Language Processing in the Arab World", "abstract": "The term natural language refers to any system of symbolic communication\n(spoken, signed or written) without intentional human planning and design. This\ndistinguishes natural languages such as Arabic and Japanese from artificially\nconstructed languages such as Esperanto or Python. Natural language processing\n(NLP) is the sub-field of artificial intelligence (AI) focused on modeling\nnatural languages to build applications such as speech recognition and\nsynthesis, machine translation, optical character recognition (OCR), sentiment\nanalysis (SA), question answering, dialogue systems, etc. NLP is a highly\ninterdisciplinary field with connections to computer science, linguistics,\ncognitive science, psychology, mathematics and others. Some of the earliest AI\napplications were in NLP (e.g., machine translation); and the last decade\n(2010-2020) in particular has witnessed an incredible increase in quality,\nmatched with a rise in public awareness, use, and expectations of what may have\nseemed like science fiction in the past. NLP researchers pride themselves on\ndeveloping language independent models and tools that can be applied to all\nhuman languages, e.g. machine translation systems can be built for a variety of\nlanguages using the same basic mechanisms and models. However, the reality is\nthat some languages do get more attention (e.g., English and Chinese) than\nothers (e.g., Hindi and Swahili). Arabic, the primary language of the Arab\nworld and the religious language of millions of non-Arab Muslims is somewhere\nin the middle of this continuum. Though Arabic NLP has many challenges, it has\nseen many successes and developments. Next we discuss Arabic's main challenges\nas a necessary background, and we present a brief history of Arabic NLP. We\nthen survey a number of its research areas, and close with a critical\ndiscussion of the future of Arabic NLP.", "published": "2020-11-25 10:45:38", "link": "http://arxiv.org/abs/2011.12631v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Expand: Reinforced Pseudo-relevance Feedback Selection for\n  Information-seeking Conversations", "abstract": "Information-seeking conversation systems are increasingly popular in\nreal-world applications, especially for e-commerce companies. To retrieve\nappropriate responses for users, it is necessary to compute the matching\ndegrees between candidate responses and users' queries with historical dialogue\nutterances. As the contexts are usually much longer than responses, it is thus\nnecessary to expand the responses (usually short) with richer information.\nRecent studies on pseudo-relevance feedback (PRF) have demonstrated its\neffectiveness in query expansion for search engines, hence we consider\nexpanding response using PRF information. However, existing PRF approaches are\neither based on heuristic rules or require heavy manual labeling, which are not\nsuitable for solving our task. To alleviate this problem, we treat the PRF\nselection for response expansion as a learning task and propose a reinforced\nlearning method that can be trained in an end-to-end manner without any human\nannotations. More specifically, we propose a reinforced selector to extract\nuseful PRF terms to enhance response candidates and a BERT-based response\nranker to rank the PRF-enhanced responses. The performance of the ranker serves\nas a reward to guide the selector to extract useful PRF terms, which boosts the\noverall task performance. Extensive experiments on both standard benchmarks and\ncommercial datasets prove the superiority of our reinforced PRF term selector\ncompared with other potential soft or hard selection methods. Both case studies\nand quantitative analysis show that our model is capable of selecting\nmeaningful PRF terms to expand response candidates and also achieving the best\nresults compared with all baselines on a variety of evaluation metrics. We have\nalso deployed our method on online production in an e-commerce company, which\nshows a significant improvement over the existing online ranking system.", "published": "2020-11-25 14:33:18", "link": "http://arxiv.org/abs/2011.12771v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Representations for Modeling Variation in Speech", "abstract": "Variation in speech is often quantified by comparing phonetic transcriptions\nof the same utterance. However, manually transcribing speech is time-consuming\nand error prone. As an alternative, therefore, we investigate the extraction of\nacoustic embeddings from several self-supervised neural models. We use these\nrepresentations to compute word-based pronunciation differences between\nnon-native and native speakers of English, and between Norwegian dialect\nspeakers. For comparison with several earlier studies, we evaluate how well\nthese differences match human perception by comparing them with available human\njudgements of similarity. We show that speech representations extracted from a\nspecific type of neural model (i.e. Transformers) lead to a better match with\nhuman perception than two earlier approaches on the basis of phonetic\ntranscriptions and MFCC-based acoustic features. We furthermore find that\nfeatures from the neural models can generally best be extracted from one of the\nmiddle hidden layers than from the final layer. We also demonstrate that neural\nspeech representations not only capture segmental differences, but also\nintonational and durational differences that cannot adequately be represented\nby a set of discrete symbols used in phonetic transcriptions.", "published": "2020-11-25 11:19:12", "link": "http://arxiv.org/abs/2011.12649v3", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "XTQA: Span-Level Explanations of the Textbook Question Answering", "abstract": "Textbook Question Answering (TQA) is a task that one should answer a\ndiagram/non-diagram question given a large multi-modal context consisting of\nabundant essays and diagrams. We argue that the explainability of this task\nshould place students as a key aspect to be considered. To address this issue,\nwe devise a novel architecture towards span-level eXplanations of the TQA\n(XTQA) based on our proposed coarse-to-fine grained algorithm, which can\nprovide not only the answers but also the span-level evidences to choose them\nfor students. This algorithm first coarsely chooses top $M$ paragraphs relevant\nto questions using the TF-IDF method, and then chooses top $K$ evidence spans\nfinely from all candidate spans within these paragraphs by computing the\ninformation gain of each span to questions. Experimental results shows that\nXTQA significantly improves the state-of-the-art performance compared with\nbaselines. The source code is available at\nhttps://github.com/keep-smile-001/opentqa", "published": "2020-11-25 11:44:12", "link": "http://arxiv.org/abs/2011.12662v4", "categories": ["cs.CL", "cs.AI", "68T07", "I.1.2"], "primary_category": "cs.CL"}
{"title": "The Geometry of Distributed Representations for Better Alignment,\n  Attenuated Bias, and Improved Interpretability", "abstract": "High-dimensional representations for words, text, images, knowledge graphs\nand other structured data are commonly used in different paradigms of machine\nlearning and data mining. These representations have different degrees of\ninterpretability, with efficient distributed representations coming at the cost\nof the loss of feature to dimension mapping. This implies that there is\nobfuscation in the way concepts are captured in these embedding spaces. Its\neffects are seen in many representations and tasks, one particularly\nproblematic one being in language representations where the societal biases,\nlearned from underlying data, are captured and occluded in unknown dimensions\nand subspaces. As a result, invalid associations (such as different races and\ntheir association with a polar notion of good versus bad) are made and\npropagated by the representations, leading to unfair outcomes in different\ntasks where they are used. This work addresses some of these problems\npertaining to the transparency and interpretability of such representations. A\nprimary focus is the detection, quantification, and mitigation of socially\nbiased associations in language representation.", "published": "2020-11-25 01:04:11", "link": "http://arxiv.org/abs/2011.12465v1", "categories": ["cs.CL", "cs.AI", "cs.CG", "cs.DS"], "primary_category": "cs.CL"}
{"title": "Vocal Tract Length Perturbation for Text-Dependent Speaker Verification\n  with Autoregressive Prediction Coding", "abstract": "In this letter, we propose a vocal tract length (VTL) perturbation method for\ntext-dependent speaker verification (TD-SV), in which a set of TD-SV systems\nare trained, one for each VTL factor, and score-level fusion is applied to make\na final decision. Next, we explore the bottleneck (BN) feature extracted by\ntraining deep neural networks with a self-supervised objective, autoregressive\npredictive coding (APC), for TD-SV and compare it with the well-studied\nspeaker-discriminant BN feature. The proposed VTL method is then applied to APC\nand speaker-discriminant BN features. In the end, we combine the VTL\nperturbation systems trained on MFCC and the two BN features in the score\ndomain. Experiments are performed on the RedDots challenge 2016 database of\nTD-SV using short utterances with Gaussian mixture model-universal background\nmodel and i-vector techniques. Results show the proposed methods significantly\noutperform the baselines.", "published": "2020-11-25 06:11:06", "link": "http://arxiv.org/abs/2011.12536v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "AGenT Zero: Zero-shot Automatic Multiple-Choice Question Generation for\n  Skill Assessments", "abstract": "Multiple-choice questions (MCQs) offer the most promising avenue for skill\nevaluation in the era of virtual education and job recruiting, where\ntraditional performance-based alternatives such as projects and essays have\nbecome less viable, and grading resources are constrained. The automated\ngeneration of MCQs would allow assessment creation at scale. Recent advances in\nnatural language processing have given rise to many complex question generation\nmethods. However, the few methods that produce deployable results in specific\ndomains require a large amount of domain-specific training data that can be\nvery costly to acquire. Our work provides an initial foray into MCQ generation\nunder high data-acquisition cost scenarios by strategically emphasizing\nparaphrasing the question context (compared to the task). In addition to\nmaintaining semantic similarity between the question-answer pairs, our\npipeline, which we call AGenT Zero, consists of only pre-trained models and\nrequires no fine-tuning, minimizing data acquisition costs for question\ngeneration. AGenT Zero successfully outperforms other pre-trained methods in\nfluency and semantic similarity. Additionally, with some small changes, our\nassessment pipeline can be generalized to a broader question and answer space,\nincluding short answer or fill in the blank questions.", "published": "2020-11-25 04:06:57", "link": "http://arxiv.org/abs/2012.01186v2", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "DeepTriage: Automated Transfer Assistance for Incidents in Cloud\n  Services", "abstract": "As cloud services are growing and generating high revenues, the cost of\ndowntime in these services is becoming significantly expensive. To reduce loss\nand service downtime, a critical primary step is to execute incident triage,\nthe process of assigning a service incident to the correct responsible team, in\na timely manner. An incorrect assignment risks additional incident reroutings\nand increases its time to mitigate by 10x. However, automated incident triage\nin large cloud services faces many challenges: (1) a highly imbalanced incident\ndistribution from a large number of teams, (2) wide variety in formats of input\ndata or data sources, (3) scaling to meet production-grade requirements, and\n(4) gaining engineers' trust in using machine learning recommendations. To\naddress these challenges, we introduce DeepTriage, an intelligent incident\ntransfer service combining multiple machine learning techniques - gradient\nboosted classifiers, clustering methods, and deep neural networks - in an\nensemble to recommend the responsible team to triage an incident. Experimental\nresults on real incidents in Microsoft Azure show that our service achieves\n82.9% F1 score. For highly impacted incidents, DeepTriage achieves F1 score\nfrom 76.3% - 91.3%. We have applied best practices and state-of-the-art\nframeworks to scale DeepTriage to handle incident routing for all cloud\nservices. DeepTriage has been deployed in Azure since October 2017 and is used\nby thousands of teams daily.", "published": "2020-11-25 03:10:11", "link": "http://arxiv.org/abs/2012.03665v1", "categories": ["cs.DC", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.DC"}
{"title": "Diluted Near-Optimal Expert Demonstrations for Guiding Dialogue\n  Stochastic Policy Optimisation", "abstract": "A learning dialogue agent can infer its behaviour from interactions with the\nusers. These interactions can be taken from either human-to-human or\nhuman-machine conversations. However, human interactions are scarce and costly,\nmaking learning from few interactions essential. One solution to speedup the\nlearning process is to guide the agent's exploration with the help of an\nexpert. We present in this paper several imitation learning strategies for\ndialogue policy where the guiding expert is a near-optimal handcrafted policy.\nWe incorporate these strategies with state-of-the-art reinforcement learning\nmethods based on Q-learning and actor-critic. We notably propose a randomised\nexploration policy which allows for a seamless hybridisation of the learned\npolicy and the expert. Our experiments show that our hybridisation strategy\noutperforms several baselines, and that it can accelerate the learning when\nfacing real humans.", "published": "2020-11-25 15:00:36", "link": "http://arxiv.org/abs/2012.04687v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Soft-Median Choice: An Automatic Feature Smoothing Method for Sound\n  Event Detection", "abstract": "In Sound Event Detection (SED) systems, the lengths of median filters for\npost-processing have never been optimized during training due to several\nproblems. No gradient is received by the lengths so they cannot be learned\nduring back-propagation. The median-filtering inserted in the models also\ncauses block in gradient flowing and the smoothing process misleads the model\nby ignoring errors. To resolve these problems, we provide different channels of\nfeatures smoothed to different extents along with the original feature, so the\nmodel can optimize the weights while cognizing all the errors. We then use a\nlinear layer to integrate the results and produce a linear combination. We\nfurther design the soft-median function to dredge the gradient flow. The\nproposed framework is called Soft-Median Choice (SMC). Experiments show that\nthe SMC block not only automatically smooths the features based on the training\nset, but also forces the model to extract common features shared by all the\nframes of a sound event. The performance of the proposed method outperforms the\nbaseline by over 10% of Event-Based F1 Score (EBFS) in both the validation and\nthe evaluation set, and also slightly outperforms the single model of the\nstate-of-the-art SED system.", "published": "2020-11-25 08:07:11", "link": "http://arxiv.org/abs/2011.12564v3", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Zero-Shot Audio Classification with Factored Linear and Nonlinear\n  Acoustic-Semantic Projections", "abstract": "In this paper, we study zero-shot learning in audio classification through\nfactored linear and nonlinear acoustic-semantic projections between audio\ninstances and sound classes. Zero-shot learning in audio classification refers\nto classification problems that aim at recognizing audio instances of sound\nclasses, which have no available training data but only semantic side\ninformation. In this paper, we address zero-shot learning by employing factored\nlinear and nonlinear acoustic-semantic projections. We develop factored linear\nprojections by applying rank decomposition to a bilinear model, and use\nnonlinear activation functions, such as tanh, to model the non-linearity\nbetween acoustic embeddings and semantic embeddings. Compared with the prior\nbilinear model, experimental results show that the proposed projection methods\nare effective for improving classification performance of zero-shot learning in\naudio classification.", "published": "2020-11-25 11:39:19", "link": "http://arxiv.org/abs/2011.12657v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Small Footprint Convolutional Recurrent Networks for Streaming Wakeword\n  Detection", "abstract": "In this work, we propose small footprint Convolutional Recurrent Neural\nNetwork models applied to the problem of wakeword detection and augment them\nwith scaled dot product attention. We find that false accepts compared to\nConvolutional Neural Network models in a 250k parameter budget can be reduced\nby 25% with a 10% reduction in parameter size by using CRNNs, and we can get up\nto 32% improvement at a 50k parameter budget with 75% reduction in parameter\nsize compared to word-level Dense Neural Network models. We discuss solutions\nto the challenging problem of performing inference on streaming audio with\nCRNNs, as well as differences in start-end index errors and latency in\ncomparison to CNN, DNN, and DNN-HMM models.", "published": "2020-11-25 18:47:25", "link": "http://arxiv.org/abs/2011.12941v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "VoxLingua107: a Dataset for Spoken Language Recognition", "abstract": "This paper investigates the use of automatically collected web audio data for\nthe task of spoken language recognition. We generate semi-random search phrases\nfrom language-specific Wikipedia data that are then used to retrieve videos\nfrom YouTube for 107 languages. Speech activity detection and speaker\ndiarization are used to extract segments from the videos that contain speech.\nPost-filtering is used to remove segments from the database that are likely not\nin the given language, increasing the proportion of correctly labeled segments\nto 98%, based on crowd-sourced verification. The size of the resulting training\nset (VoxLingua107) is 6628 hours (62 hours per language on the average) and it\nis accompanied by an evaluation set of 1609 verified utterances. We use the\ndata to build language recognition models for several spoken language\nidentification tasks. Experiments show that using the automatically retrieved\ntraining data gives competitive results to using hand-labeled proprietary\ndatasets. The dataset is publicly available.", "published": "2020-11-25 19:47:38", "link": "http://arxiv.org/abs/2011.12998v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Bootstrap an end-to-end ASR system by multilingual training, transfer\n  learning, text-to-text mapping and synthetic audio", "abstract": "Bootstrapping speech recognition on limited data resources has been an area\nof active research for long. The recent transition to all-neural models and\nend-to-end (E2E) training brought along particular challenges as these models\nare known to be data hungry, but also came with opportunities around\nlanguage-agnostic representations derived from multilingual data as well as\nshared word-piece output representations across languages that share script and\nroots. We investigate here the effectiveness of different strategies to\nbootstrap an RNN-Transducer (RNN-T) based automatic speech recognition (ASR)\nsystem in the low resource regime, while exploiting the abundant resources\navailable in other languages as well as the synthetic audio from a\ntext-to-speech (TTS) engine. Our experiments demonstrate that transfer learning\nfrom a multilingual model, using a post-ASR text-to-text mapping and synthetic\naudio deliver additive improvements, allowing us to bootstrap a model for a new\nlanguage with a fraction of the data that would otherwise be needed. The best\nsystem achieved a 46% relative word error rate (WER) reduction compared to the\nmonolingual baseline, among which 25% relative WER improvement is attributed to\nthe post-ASR text-to-text mappings and the TTS synthetic data.", "published": "2020-11-25 13:11:32", "link": "http://arxiv.org/abs/2011.12696v2", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Phase retrieval with Bregman divergences: Application to audio signal\n  recovery", "abstract": "Phase retrieval aims to recover a signal from magnitude or power spectra\nmeasurements. It is often addressed by considering a minimization problem\ninvolving a quadratic cost function. We propose a different formulation based\non Bregman divergences, which encompass divergences that are appropriate for\naudio signal processing applications. We derive a fast gradient algorithm to\nsolve this problem.", "published": "2020-11-25 15:21:39", "link": "http://arxiv.org/abs/2011.12818v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MTCRNN: A multi-scale RNN for directed audio texture synthesis", "abstract": "Audio textures are a subset of environmental sounds, often defined as having\nstable statistical characteristics within an adequately large window of time\nbut may be unstructured locally. They include common everyday sounds such as\nfrom rain, wind, and engines. Given that these complex sounds contain patterns\non multiple timescales, they are a challenge to model with traditional methods.\nWe introduce a novel modelling approach for textures, combining recurrent\nneural networks trained at different levels of abstraction with a conditioning\nstrategy that allows for user-directed synthesis. We demonstrate the model's\nperformance on a variety of datasets, examine its performance on various\nmetrics, and discuss some potential applications.", "published": "2020-11-25 09:13:53", "link": "http://arxiv.org/abs/2011.12596v1", "categories": ["cs.SD", "eess.AS", "stat.ML", "I.2.6; J.5"], "primary_category": "cs.SD"}
{"title": "FBWave: Efficient and Scalable Neural Vocoders for Streaming\n  Text-To-Speech on the Edge", "abstract": "Nowadays more and more applications can benefit from edge-based\ntext-to-speech (TTS). However, most existing TTS models are too computationally\nexpensive and are not flexible enough to be deployed on the diverse variety of\nedge devices with their equally diverse computational capacities. To address\nthis, we propose FBWave, a family of efficient and scalable neural vocoders\nthat can achieve optimal performance-efficiency trade-offs for different edge\ndevices. FBWave is a hybrid flow-based generative model that combines the\nadvantages of autoregressive and non-autoregressive models. It produces high\nquality audio and supports streaming during inference while remaining highly\ncomputationally efficient. Our experiments show that FBWave can achieve similar\naudio quality to WaveRNN while reducing MACs by 40x. More efficient variants of\nFBWave can achieve up to 109x fewer MACs while still delivering acceptable\naudio quality. Audio demos are available at\nhttps://bichenwu09.github.io/vocoder_demos.", "published": "2020-11-25 19:09:49", "link": "http://arxiv.org/abs/2011.12985v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Learning to dance: A graph convolutional adversarial network to generate\n  realistic dance motions from audio", "abstract": "Synthesizing human motion through learning techniques is becoming an\nincreasingly popular approach to alleviating the requirement of new data\ncapture to produce animations. Learning to move naturally from music, i.e., to\ndance, is one of the more complex motions humans often perform effortlessly.\nEach dance movement is unique, yet such movements maintain the core\ncharacteristics of the dance style. Most approaches addressing this problem\nwith classical convolutional and recursive neural models undergo training and\nvariability issues due to the non-Euclidean geometry of the motion manifold\nstructure.In this paper, we design a novel method based on graph convolutional\nnetworks to tackle the problem of automatic dance generation from audio\ninformation. Our method uses an adversarial learning scheme conditioned on the\ninput music audios to create natural motions preserving the key movements of\ndifferent music styles. We evaluate our method with three quantitative metrics\nof generative methods and a user study. The results suggest that the proposed\nGCN model outperforms the state-of-the-art dance generation method conditioned\non music in different experiments. Moreover, our graph-convolutional approach\nis simpler, easier to be trained, and capable of generating more realistic\nmotion styles regarding qualitative and different quantitative metrics. It also\npresented a visual movement perceptual quality comparable to real motion data.", "published": "2020-11-25 19:53:53", "link": "http://arxiv.org/abs/2011.12999v2", "categories": ["cs.GR", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.GR"}
