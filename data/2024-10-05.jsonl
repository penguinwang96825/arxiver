{"title": "On the Influence of Gender and Race in Romantic Relationship Prediction\n  from Large Language Models", "abstract": "We study the presence of heteronormative biases and prejudice against\ninterracial romantic relationships in large language models by performing\ncontrolled name-replacement experiments for the task of relationship\nprediction. We show that models are less likely to predict romantic\nrelationships for (a) same-gender character pairs than different-gender pairs;\nand (b) intra/inter-racial character pairs involving Asian names as compared to\nBlack, Hispanic, or White names. We examine the contextualized embeddings of\nfirst names and find that gender for Asian names is less discernible than\nnon-Asian names. We discuss the social implications of our findings,\nunderlining the need to prioritize the development of inclusive and equitable\ntechnology.", "published": "2024-10-05 01:41:55", "link": "http://arxiv.org/abs/2410.03996v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Simple yet Effective Training-free Prompt-free Approach to Chinese\n  Spelling Correction Based on Large Language Models", "abstract": "This work proposes a simple training-free prompt-free approach to leverage\nlarge language models (LLMs) for the Chinese spelling correction (CSC) task,\nwhich is totally different from all previous CSC approaches. The key idea is to\nuse an LLM as a pure language model in a conventional manner. The LLM goes\nthrough the input sentence from the beginning, and at each inference step,\nproduces a distribution over its vocabulary for deciding the next token, given\na partial sentence. To ensure that the output sentence remains faithful to the\ninput sentence, we design a minimal distortion model that utilizes\npronunciation or shape similarities between the original and replaced\ncharacters. Furthermore, we propose two useful reward strategies to address\npractical challenges specific to the CSC task. Experiments on five public\ndatasets demonstrate that our approach significantly improves LLM performance,\nenabling them to compete with state-of-the-art domain-general CSC models.", "published": "2024-10-05 04:06:56", "link": "http://arxiv.org/abs/2410.04027v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neuron-Level Sequential Editing for Large Language Models", "abstract": "This work explores sequential model editing in large language models (LLMs),\na critical task that involves modifying internal knowledge within LLMs\ncontinuously through multi-round editing, each incorporating updates or\ncorrections to adjust the model outputs without the need for costly retraining.\nExisting model editing methods, especially those that alter model parameters,\ntypically focus on single-round editing and often face significant challenges\nin sequential model editing-most notably issues of model forgetting and\nfailure. To address these challenges, we introduce a new model editing method,\nnamely \\textbf{N}euron-level \\textbf{S}equential \\textbf{E}diting (NSE),\ntailored for supporting sequential model editing. Specifically, we optimize the\ntarget layer's hidden states using the model's original weights to prevent\nmodel failure. Furthermore, we iteratively select neurons in multiple layers\nfor editing based on their activation values to mitigate model forgetting. Our\nempirical experiments demonstrate that NSE significantly outperforms current\nmodifying parameters model editing methods, marking a substantial advancement\nin the field of sequential model editing. Our code is released on\n\\url{https://github.com/jianghoucheng/NSE}.", "published": "2024-10-05 05:52:22", "link": "http://arxiv.org/abs/2410.04045v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Correction is More than Refinement: A Learning Framework for Visual\n  and Language Reasoning Tasks", "abstract": "While Vision-Language Models (VLMs) have shown remarkable abilities in visual\nand language reasoning tasks, they invariably generate flawed responses.\nSelf-correction that instructs models to refine their outputs presents a\npromising solution to this issue. Previous studies have mainly concentrated on\nLarge Language Models (LLMs), while the self-correction abilities of VLMs,\nparticularly concerning both visual and linguistic information, remain largely\nunexamined. This study investigates the self-correction capabilities of VLMs\nduring both inference and fine-tuning stages. We introduce a Self-Correction\nLearning (SCL) approach that enables VLMs to learn from their self-generated\nself-correction data through Direct Preference Optimization (DPO) without\nrelying on external feedback, facilitating self-improvement. Specifically, we\ncollect preferred and disfavored samples based on the correctness of initial\nand refined responses, which are obtained by two-turn self-correction with VLMs\nduring the inference stage. Experimental results demonstrate that although VLMs\nstruggle to self-correct effectively during iterative inference without\nadditional fine-tuning and external feedback, they can enhance their\nperformance and avoid previous mistakes through preference fine-tuning when\ntheir self-generated self-correction data are categorized into preferred and\ndisfavored samples. This study emphasizes that self-correction is not merely a\nrefinement process; rather, it should enhance the reasoning abilities of models\nthrough additional training, enabling them to generate high-quality responses\ndirectly without further refinement.", "published": "2024-10-05 06:28:54", "link": "http://arxiv.org/abs/2410.04055v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PsFuture: A Pseudo-Future-based Zero-Shot Adaptive Policy for\n  Simultaneous Machine Translation", "abstract": "Simultaneous Machine Translation (SiMT) requires target tokens to be\ngenerated in real-time as streaming source tokens are consumed. Traditional\napproaches to SiMT typically require sophisticated architectures and extensive\nparameter configurations for training adaptive read/write policies, which in\nturn demand considerable computational power and memory. We propose PsFuture,\nthe first zero-shot adaptive read/write policy for SiMT, enabling the\ntranslation model to independently determine read/write actions without the\nnecessity for additional training. Furthermore, we introduce a novel training\nstrategy, Prefix-to-Full (P2F), specifically tailored to adjust offline\ntranslation models for SiMT applications, exploiting the advantages of the\nbidirectional attention mechanism inherent in offline models. Experiments\nacross multiple benchmarks demonstrate that our zero-shot policy attains\nperformance on par with strong baselines and the P2F method can further enhance\nperformance, achieving an outstanding trade-off between translation quality and\nlatency.", "published": "2024-10-05 08:06:33", "link": "http://arxiv.org/abs/2410.04075v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BloomWise: Enhancing Problem-Solving capabilities of Large Language\n  Models using Bloom's-Taxonomy-Inspired Prompts", "abstract": "Despite the continuous progress of Large Language Models (LLMs) across\nvarious tasks, their performance on mathematical problems and reasoning tasks\nremains limited. This limitation can be attributed, among other factors, to the\ninherent difficulty of these problems and the fact that solutions often consist\nof multiple steps, potentially of varying nature, making it challenging for a\nsingle prompting technique to execute all required steps. To address this, we\nintroduce BloomWise, a new prompting technique, inspired by Bloom's Taxonomy,\naiming to improve LLMs' performance in solving such problems by encouraging\nthem to approach the problem starting from simple, i.e., remembering, and\nprogressing to higher cognitive skills, i.e., analyzing, until the correct\nsolution is reached. The decision regarding the need to employ more\nsophisticated cognitive skills is based on self-evaluation performed by the\nLLM. Thus, we encourage the LLM to deploy the appropriate cognitive processes.\nIn extensive experiments across 4 popular math reasoning datasets, we have\ndemonstrated the effectiveness of our proposed approach. We also present\nextensive ablations, analyzing the strengths of each module within our system.", "published": "2024-10-05 09:27:52", "link": "http://arxiv.org/abs/2410.04094v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Learning Rate Path Switching Training Paradigm for Version Updates of\n  Large Language Models", "abstract": "Due to the continuous emergence of new data, version updates have become an\nindispensable requirement for Large Language Models (LLMs). The training\nparadigms for version updates of LLMs include pre-training from scratch (PTFS)\nand continual pre-training (CPT). Preliminary experiments demonstrate that PTFS\nachieves better pre-training performance, while CPT has lower training cost.\nMoreover, their performance and training cost gaps widen progressively with\nversion updates. To investigate the underlying reasons for this phenomenon, we\nanalyze the effect of learning rate adjustments during the two stages of CPT:\npreparing an initialization checkpoint and continual pre-training based on this\ncheckpoint. We find that a large learning rate in the first stage and a\ncomplete learning rate decay process in the second stage are crucial for\nversion updates of LLMs. Hence, we propose a learning rate path switching\ntraining paradigm. Our paradigm comprises one main path, where we pre-train a\nLLM with the maximal learning rate, and multiple branching paths, each of which\ncorresponds to an update of the LLM with newly-added training data. Extensive\nexperiments demonstrate the effectiveness and generalization of our paradigm.\nParticularly, when training four versions of LLMs, our paradigm reduces the\ntotal training cost to 58% compared to PTFS, while maintaining comparable\npre-training performance.", "published": "2024-10-05 10:15:48", "link": "http://arxiv.org/abs/2410.04103v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring LLM-based Data Annotation Strategies for Medical Dialogue\n  Preference Alignment", "abstract": "This research examines the use of Reinforcement Learning from AI Feedback\n(RLAIF) techniques to improve healthcare dialogue models, with the aim of\ntackling the challenges of preference-aligned data annotation while reducing\nthe reliance on medical experts. We argue that the primary challenges in\ncurrent RLAIF research for healthcare are the limitations of automated\nevaluation methods and the difficulties in accurately representing physician\npreferences. To address these challenges, we present a new evaluation framework\nbased on standardized patient examinations. This framework is designed to\nobjectively assess the effectiveness of large language models (LLMs) in guiding\nusers and following instructions, enabling a comprehensive comparison across\ndifferent models. Furthermore, our investigation of effective ways to express\nphysician preferences using Constitutional AI algorithms highlighted the\nparticular effectiveness of flowcharts. Utilizing this finding, we introduce an\ninnovative agent-based approach for annotating preference data. This approach\nautonomously creates medical dialogue flows tailored to the patient's\ncondition, demonstrates strong generalization abilities, and reduces the need\nfor expert involvement. Our results show that the agent-based approach\noutperforms existing RLAIF annotation methods in standardized patient\nexaminations and surpasses current open source medical dialogue LLMs in various\ntest scenarios.", "published": "2024-10-05 10:29:19", "link": "http://arxiv.org/abs/2410.04112v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can the Variation of Model Weights be used as a Criterion for Self-Paced\n  Multilingual NMT?", "abstract": "Many-to-one neural machine translation systems improve over one-to-one\nsystems when training data is scarce. In this paper, we design and test a novel\nalgorithm for selecting the language of minibatches when training such systems.\nThe algorithm changes the language of the minibatch when the weights of the\nmodel do not evolve significantly, as measured by the smoothed KL divergence\nbetween all layers of the Transformer network. This algorithm outperforms the\nuse of alternating monolingual batches, but not the use of shuffled batches, in\nterms of translation quality (measured with BLEU and COMET) and convergence\nspeed.", "published": "2024-10-05 12:52:51", "link": "http://arxiv.org/abs/2410.04147v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Toxic Subword Pruning for Dialogue Response Generation on Large Language\n  Models", "abstract": "How to defend large language models (LLMs) from generating toxic content is\nan important research area. Yet, most research focused on various model\ntraining techniques to remediate LLMs by updating their weights. A typical\nrelated research area is safety alignment. This however is often costly and\ntedious and can expose the model to even more problems such as catastrophic\nforgetting if the trainings are not carefully handled by experienced NLP\npractitioners. We thus propose a simple yet effective and novel algorithm,\nnamely \\textbf{Tox}ic Subword \\textbf{Prun}ing (ToxPrune) to prune the subword\ncontained by the toxic words from BPE in trained LLMs. In contrast to the\nprevious work that demonstrates pruning BPE tokens as harmful to the task of\nmachine translation, we surprisingly found its usefulness in preventing toxic\ncontent from being generated on LLMs. Fortunately, our findings suggest that\nToxPrune simultaneously improves the toxic language model NSFW-3B on the task\nof dialogue response generation obviously. We surprisingly found that ToxPrune\ncan even obviously improve official Llama-3.1-6B in the metric of dialogue\ndiversity. Extensive automatic results and human evaluation indicate that\nToxPrune could be helpful for both remediating toxic LLMs and improving\nnon-toxic LLMs on the task of dialogue response generation.\\footnote{We plan to\nrelease the resources to facilitate future work.}", "published": "2024-10-05 13:30:33", "link": "http://arxiv.org/abs/2410.04155v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Effective Counter-Responses: Aligning Human Preferences with\n  Strategies to Combat Online Trolling", "abstract": "Trolling in online communities typically involves disruptive behaviors such\nas provoking anger and manipulating discussions, leading to a polarized\natmosphere and emotional distress. Robust moderation is essential for\nmitigating these negative impacts and maintaining a healthy and constructive\ncommunity atmosphere. However, effectively addressing trolls is difficult\nbecause their behaviors vary widely and require different response strategies\n(RSs) to counter them. This diversity makes it challenging to choose an\nappropriate RS for each specific situation. To address this challenge, our\nresearch investigates whether humans have preferred strategies tailored to\ndifferent types of trolling behaviors. Our findings reveal a correlation\nbetween the types of trolling encountered and the preferred RS. In this paper,\nwe introduce a methodology for generating counter-responses to trolls by\nrecommending appropriate RSs, supported by a dataset aligning these strategies\nwith human preferences across various troll contexts. The experimental results\ndemonstrate that our proposed approach guides constructive discussion and\nreduces the negative effects of trolls, thereby enhancing the online community\nenvironment.", "published": "2024-10-05 14:01:52", "link": "http://arxiv.org/abs/2410.04164v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Consistent Autoformalization for Constructing Mathematical Libraries", "abstract": "Autoformalization is the task of automatically translating mathematical\ncontent written in natural language to a formal language expression. The\ngrowing language interpretation capabilities of Large Language Models (LLMs),\nincluding in formal languages, are lowering the barriers for autoformalization.\nHowever, LLMs alone are not capable of consistently and reliably delivering\nautoformalization, in particular as the complexity and specialization of the\ntarget domain grows. As the field evolves into the direction of systematically\napplying autoformalization towards large mathematical libraries, the need to\nimprove syntactic, terminological and semantic control increases. This paper\nproposes the coordinated use of three mechanisms, most-similar retrieval\naugmented generation (MS-RAG), denoising steps, and auto-correction with syntax\nerror feedback (Auto-SEF) to improve autoformalization quality. The empirical\nanalysis, across different models, demonstrates that these mechanisms can\ndeliver autoformalizaton results which are syntactically, terminologically and\nsemantically more consistent. These mechanisms can be applied across different\nLLMs and have shown to deliver improve results across different model types.", "published": "2024-10-05 15:13:22", "link": "http://arxiv.org/abs/2410.04194v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CS4: Measuring the Creativity of Large Language Models Automatically by\n  Controlling the Number of Story-Writing Constraints", "abstract": "Evaluating the creativity of large language models (LLMs) in story writing is\ndifficult because LLM-generated stories could seemingly look creative but be\nvery similar to some existing stories in their huge and proprietary training\ncorpus. To overcome this challenge, we introduce a novel benchmark dataset with\nvarying levels of prompt specificity: CS4 ($\\mathbf{C}$omparing the\n$\\mathbf{S}$kill of $\\mathbf{C}$reating $\\mathbf{S}$tories by\n$\\mathbf{C}$ontrolling the $\\mathbf{S}$ynthesized $\\mathbf{C}$onstraint\n$\\mathbf{S}$pecificity). By increasing the number of requirements/constraints\nin the prompt, we can increase the prompt specificity and hinder LLMs from\nretelling high-quality narratives in their training data. Consequently, CS4\nempowers us to indirectly measure the LLMs' creativity without human\nannotations.\n  Our experiments on LLaMA, Gemma, and Mistral not only highlight the\ncreativity challenges LLMs face when dealing with highly specific prompts but\nalso reveal that different LLMs perform very differently under different\nnumbers of constraints and achieve different balances between the model's\ninstruction-following ability and narrative coherence. Additionally, our\nexperiments on OLMo suggest that Learning from Human Feedback (LHF) can help\nLLMs select better stories from their training data but has limited influence\nin boosting LLMs' ability to produce creative stories that are unseen in the\ntraining corpora. The benchmark is released at\nhttps://github.com/anirudhlakkaraju/cs4_benchmark.", "published": "2024-10-05 15:22:36", "link": "http://arxiv.org/abs/2410.04197v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Persona Knowledge-Aligned Prompt Tuning Method for Online Debate", "abstract": "Debate is the process of exchanging viewpoints or convincing others on a\nparticular issue. Recent research has provided empirical evidence that the\npersuasiveness of an argument is determined not only by language usage but also\nby communicator characteristics. Researchers have paid much attention to\naspects of languages, such as linguistic features and discourse structures, but\ncombining argument persuasiveness and impact with the social personae of the\naudience has not been explored due to the difficulty and complexity. We have\nobserved the impressive simulation and personification capability of ChatGPT,\nindicating a giant pre-trained language model may function as an individual to\nprovide personae and exert unique influences based on diverse background\nknowledge. Therefore, we propose a persona knowledge-aligned framework for\nargument quality assessment tasks from the audience side. This is the first\nwork that leverages the emergence of ChatGPT and injects such audience personae\nknowledge into smaller language models via prompt tuning. The performance of\nour pipeline demonstrates significant and consistent improvement compared to\ncompetitive architectures.", "published": "2024-10-05 17:33:11", "link": "http://arxiv.org/abs/2410.04239v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adaptive Question Answering: Enhancing Language Model Proficiency for\n  Addressing Knowledge Conflicts with Source Citations", "abstract": "Resolving knowledge conflicts is a crucial challenge in Question Answering\n(QA) tasks, as the internet contains numerous conflicting facts and opinions.\nWhile some research has made progress in tackling ambiguous settings where\nmultiple valid answers exist, these approaches often neglect to provide source\ncitations, leaving users to evaluate the factuality of each answer. On the\nother hand, existing work on citation generation has focused on unambiguous\nsettings with single answers, failing to address the complexity of real-world\nscenarios. Despite the importance of both aspects, no prior research has\ncombined them, leaving a significant gap in the development of QA systems. In\nthis work, we bridge this gap by proposing the novel task of QA with source\ncitation in ambiguous settings, where multiple valid answers exist. To\nfacilitate research in this area, we create a comprehensive framework\nconsisting of: (1) five novel datasets, obtained by augmenting three existing\nreading comprehension datasets with citation meta-data across various ambiguous\nsettings, such as distractors and paraphrasing; (2) the first ambiguous\nmulti-hop QA dataset featuring real-world, naturally occurring contexts; (3)\ntwo new metrics to evaluate models' performances; and (4) several strong\nbaselines using rule-based, prompting, and finetuning approaches over five\nlarge language models. We hope that this new task, datasets, metrics, and\nbaselines will inspire the community to push the boundaries of QA research and\ndevelop more trustworthy and interpretable systems.", "published": "2024-10-05 17:37:01", "link": "http://arxiv.org/abs/2410.04241v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is deeper always better? Replacing linear mappings with deep learning\n  networks in the Discriminative Lexicon Model", "abstract": "Recently, deep learning models have increasingly been used in cognitive\nmodelling of language. This study asks whether deep learning can help us to\nbetter understand the learning problem that needs to be solved by speakers,\nabove and beyond linear methods. We utilise the Discriminative Lexicon Model\n(DLM, Baayen et al., 2019), which models comprehension and production with\nmappings between numeric form and meaning vectors. While so far, these mappings\nhave been linear (Linear Discriminative Learning, LDL), in the present study we\nreplace them with deep dense neural networks (Deep Discriminative Learning,\nDDL). We find that DDL affords more accurate mappings for large and diverse\ndatasets from English and Dutch, but not necessarily for Estonian and Taiwan\nMandarin. DDL outperforms LDL in particular for words with pseudo-morphological\nstructure such as slend+er. Applied to average reaction times, we find that DDL\nis outperformed by frequency-informed linear mappings (FIL). However, DDL\ntrained in a frequency-informed way ('frequency-informed' deep learning, FIDDL)\nsubstantially outperforms FIL. Finally, while linear mappings can very\neffectively be updated from trial-to-trial to model incremental lexical\nlearning (Heitmeier et al., 2023), deep mappings cannot do so as effectively.\nAt present, both linear and deep mappings are informative for understanding\nlanguage.", "published": "2024-10-05 18:41:49", "link": "http://arxiv.org/abs/2410.04259v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AI as Humanity's Salieri: Quantifying Linguistic Creativity of Language\n  Models via Systematic Attribution of Machine Text against Web Text", "abstract": "Creativity has long been considered one of the most difficult aspect of human\nintelligence for AI to mimic. However, the rise of Large Language Models\n(LLMs), like ChatGPT, has raised questions about whether AI can match or even\nsurpass human creativity. We present CREATIVITY INDEX as the first step to\nquantify the linguistic creativity of a text by reconstructing it from existing\ntext snippets on the web. CREATIVITY INDEX is motivated by the hypothesis that\nthe seemingly remarkable creativity of LLMs may be attributable in large part\nto the creativity of human-written texts on the web. To compute CREATIVITY\nINDEX efficiently, we introduce DJ SEARCH, a novel dynamic programming\nalgorithm that can search verbatim and near-verbatim matches of text snippets\nfrom a given document against the web. Experiments reveal that the CREATIVITY\nINDEX of professional human authors is on average 66.2% higher than that of\nLLMs, and that alignment reduces the CREATIVITY INDEX of LLMs by an average of\n30.1%. In addition, we find that distinguished authors like Hemingway exhibit\nmeasurably higher CREATIVITY INDEX compared to other human writers. Finally, we\ndemonstrate that CREATIVITY INDEX can be used as a surprisingly effective\ncriterion for zero-shot machine text detection, surpassing the strongest\nexisting zero-shot system, DetectGPT, by a significant margin of 30.2%, and\neven outperforming the strongest supervised system, GhostBuster, in five out of\nsix domains.", "published": "2024-10-05 18:55:01", "link": "http://arxiv.org/abs/2410.04265v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RoQLlama: A Lightweight Romanian Adapted Language Model", "abstract": "The remarkable achievements obtained by open-source large language models\n(LLMs) in recent years have predominantly been concentrated on tasks involving\nthe English language. In this paper, we aim to advance the performance of\nLlama2 models on Romanian tasks. We tackle the problem of reduced computing\nresources by using QLoRA for training. We release RoQLlama-7b, a quantized LLM,\nwhich shows equal or improved results compared to its full-sized counterpart\nwhen tested on seven Romanian downstream tasks in the zero-shot setup. Also, it\nconsistently achieves higher average scores across all few-shot prompts.\nAdditionally, we introduce a novel Romanian dataset, namely RoMedQA, which\ncontains single-choice medical questions in Romanian.", "published": "2024-10-05 19:14:11", "link": "http://arxiv.org/abs/2410.04269v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Language Model Character Traits", "abstract": "Language models (LMs) can exhibit human-like behaviour, but it is unclear how\nto describe this behaviour without undue anthropomorphism. We formalise a\nbehaviourist view of LM character traits: qualities such as truthfulness,\nsycophancy, or coherent beliefs and intentions, which may manifest as\nconsistent patterns of behaviour. Our theory is grounded in empirical\ndemonstrations of LMs exhibiting different character traits, such as accurate\nand logically coherent beliefs, and helpful and harmless intentions. We find\nthat the consistency with which LMs exhibit certain character traits varies\nwith model size, fine-tuning, and prompting. In addition to characterising LM\ncharacter traits, we evaluate how these traits develop over the course of an\ninteraction. We find that traits such as truthfulness and harmfulness can be\nstationary, i.e., consistent over an interaction, in certain contexts, but may\nbe reflective in different contexts, meaning they mirror the LM's behavior in\nthe preceding interaction. Our formalism enables us to describe LM behaviour\nprecisely in intuitive language, without undue anthropomorphism.", "published": "2024-10-05 19:22:45", "link": "http://arxiv.org/abs/2410.04272v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Locating Information Gaps and Narrative Inconsistencies Across\n  Languages: A Case Study of LGBT People Portrayals on Wikipedia", "abstract": "To explain social phenomena and identify systematic biases, much research in\ncomputational social science focuses on comparative text analyses. These\nstudies often rely on coarse corpus-level statistics or local word-level\nanalyses, mainly in English. We introduce the InfoGap method -- an efficient\nand reliable approach to locating information gaps and inconsistencies in\narticles at the fact level, across languages. We evaluate InfoGap by analyzing\nLGBT people's portrayals, across 2.7K biography pages on English, Russian, and\nFrench Wikipedias. We find large discrepancies in factual coverage across the\nlanguages. Moreover, our analysis reveals that biographical facts carrying\nnegative connotations are more likely to be highlighted in Russian Wikipedia.\nCrucially, InfoGap both facilitates large scale analyses, and pinpoints local\ndocument- and fact-level information gaps, laying a new foundation for targeted\nand nuanced comparative language analysis at scale.", "published": "2024-10-05 20:40:49", "link": "http://arxiv.org/abs/2410.04282v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficiently Identifying Low-Quality Language Subsets in Multilingual\n  Datasets: A Case Study on a Large-Scale Multilingual Audio Dataset", "abstract": "Curating datasets that span multiple languages is challenging. To make the\ncollection more scalable, researchers often incorporate one or more imperfect\nclassifiers in the process, like language identification models. These models,\nhowever, are prone to failure, resulting in some language subsets being\nunreliable for downstream tasks. We introduce a statistical test, the\nPreference Proportion Test, for identifying such unreliable subsets. By\nannotating only 20 samples for a language subset, we're able to identify\nsystematic transcription errors for 10 language subsets in a recent large\nmultilingual transcribed audio dataset, X-IPAPack (Zhu et al., 2024). We find\nthat filtering this low-quality data out when training models for the\ndownstream task of phonetic transcription brings substantial benefits, most\nnotably a 25.7% relative improvement on transcribing recordings in\nout-of-distribution languages. Our method lays a path forward for systematic\nand reliable multilingual dataset auditing.", "published": "2024-10-05 21:41:49", "link": "http://arxiv.org/abs/2410.04292v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Take It Easy: Label-Adaptive Self-Rationalization for Fact Verification\n  and Explanation Generation", "abstract": "Computational methods to aid journalists in the task often require adapting a\nmodel to specific domains and generating explanations. However, most automated\nfact-checking methods rely on three-class datasets, which do not accurately\nreflect real-world misinformation. Moreover, fact-checking explanations are\noften generated based on text summarization of evidence, failing to address the\nrelationship between the claim and the evidence. To address these issues, we\nextend the self-rationalization method--typically used in natural language\ninference (NLI) tasks--to fact verification. We propose a label-adaptive\nlearning approach: first, we fine-tune a model to learn veracity prediction\nwith annotated labels (step-1 model). Then, we fine-tune the step-1 model again\nto learn self-rationalization, using the same data and additional annotated\nexplanations. Our results show that our label-adaptive approach improves\nveracity prediction by more than ten percentage points (Macro F1) on both the\nPubHealth and AVeriTec datasets, outperforming the GPT-4 model. Furthermore, to\naddress the high cost of explanation annotation, we generated 64 synthetic\nexplanations from three large language models: GPT-4-turbo, GPT-3.5-turbo, and\nLlama-3-8B and few-shot fine-tune our step-1 model. The few-shot synthetic\nexplanation fine-tuned model performed comparably to the fully fine-tuned\nself-rationalization model, demonstrating the potential of low-budget learning\nwith synthetic data. Our label-adaptive self-rationalization approach presents\na promising direction for future research on real-world explainable\nfact-checking with different labeling schemes.", "published": "2024-10-05 02:19:49", "link": "http://arxiv.org/abs/2410.04002v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LoRTA: Low Rank Tensor Adaptation of Large Language Models", "abstract": "Low Rank Adaptation (LoRA) is a popular Parameter Efficient Fine Tuning\n(PEFT) method that effectively adapts large pre-trained models for downstream\ntasks. LoRA parameterizes model updates using low-rank matrices at each layer,\nsignificantly reducing the number of trainable parameters and, consequently,\nresource requirements during fine-tuning. However, the lower bound on the\nnumber of trainable parameters remains high due to the use of the low-rank\nmatrix model. Recent works have addressed this limitation by proposing low rank\ntensor parameterizations for model updates. However, they only exploit\nredundancy across layers, or tensorize individual matrices using ad-hoc schemes\nthat introduce additional hyperparameters. In this work, we propose a\nhigher-order Candecomp/Parafac (CP) decomposition, enabling a more compact and\nflexible representation compared to existing matrix and tensor based PEFT\nmethods. Our experiments on Natural Language Understanding, Instruction Tuning,\nPreference Optimization and Protein Folding benchmarks demonstrate that our\nmethod can achieve a reduction in the number of parameters while maintaining\ncomparable performance.", "published": "2024-10-05 06:59:50", "link": "http://arxiv.org/abs/2410.04060v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ECon: On the Detection and Resolution of Evidence Conflicts", "abstract": "The rise of large language models (LLMs) has significantly influenced the\nquality of information in decision-making systems, leading to the prevalence of\nAI-generated content and challenges in detecting misinformation and managing\nconflicting information, or \"inter-evidence conflicts.\" This study introduces a\nmethod for generating diverse, validated evidence conflicts to simulate\nreal-world misinformation scenarios. We evaluate conflict detection methods,\nincluding Natural Language Inference (NLI) models, factual consistency (FC)\nmodels, and LLMs, on these conflicts (RQ1) and analyze LLMs' conflict\nresolution behaviors (RQ2). Our key findings include: (1) NLI and LLM models\nexhibit high precision in detecting answer conflicts, though weaker models\nsuffer from low recall; (2) FC models struggle with lexically similar answer\nconflicts, while NLI and LLM models handle these better; and (3) stronger\nmodels like GPT-4 show robust performance, especially with nuanced conflicts.\nFor conflict resolution, LLMs often favor one piece of conflicting evidence\nwithout justification and rely on internal knowledge if they have prior\nbeliefs.", "published": "2024-10-05 07:41:17", "link": "http://arxiv.org/abs/2410.04068v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PAD: Personalized Alignment of LLMs at Decoding-Time", "abstract": "Aligning with personalized preferences, which vary significantly across\ncultural, educational, and political differences, poses a significant challenge\ndue to the computational costs and data demands of traditional alignment\nmethods. In response, this paper presents Personalized Alignment at\nDecoding-time (PAD), a novel framework designed to align LLM outputs with\ndiverse personalized preferences during the inference phase, eliminating the\nneed for additional training. By introducing a unique personalized reward\nmodeling strategy, this framework decouples the text generation process from\npersonalized preferences, facilitating the generation of generalizable\ntoken-level personalized rewards. The PAD algorithm leverages these rewards to\nguide the decoding process, dynamically tailoring the base model's predictions\nto personalized preferences. Extensive experimental results demonstrate that\nPAD not only outperforms existing training-based alignment methods in terms of\naligning with diverse preferences but also shows significant generalizability\nto preferences unseen during training and scalability across different base\nmodels. This work advances the capability of LLMs to meet user needs in\nreal-time applications, presenting a substantial step forward in personalized\nLLM alignment.", "published": "2024-10-05 08:00:55", "link": "http://arxiv.org/abs/2410.04070v7", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GlobeSumm: A Challenging Benchmark Towards Unifying Multi-lingual,\n  Cross-lingual and Multi-document News Summarization", "abstract": "News summarization in today's global scene can be daunting with its flood of\nmultilingual content and varied viewpoints from different sources. However,\ncurrent studies often neglect such real-world scenarios as they tend to focus\nsolely on either single-language or single-document tasks. To bridge this gap,\nwe aim to unify Multi-lingual, Cross-lingual and Multi-document Summarization\ninto a novel task, i.e., MCMS, which encapsulates the real-world requirements\nall-in-one. Nevertheless, the lack of a benchmark inhibits researchers from\nadequately studying this invaluable problem. To tackle this, we have\nmeticulously constructed the GLOBESUMM dataset by first collecting a wealth of\nmultilingual news reports and restructuring them into event-centric format.\nAdditionally, we introduce the method of protocol-guided prompting for\nhigh-quality and cost-effective reference annotation. In MCMS, we also\nhighlight the challenge of conflicts between news reports, in addition to the\nissues of redundancies and omissions, further enhancing the complexity of\nGLOBESUMM. Through extensive experimental analysis, we validate the quality of\nour dataset and elucidate the inherent challenges of the task. We firmly\nbelieve that GLOBESUMM, given its challenging nature, will greatly contribute\nto the multilingual communities and the evaluation of LLMs.", "published": "2024-10-05 08:56:44", "link": "http://arxiv.org/abs/2410.04087v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TUBench: Benchmarking Large Vision-Language Models on Trustworthiness\n  with Unanswerable Questions", "abstract": "Large Vision-Language Models (LVLMs) have achieved remarkable progress on\nvisual perception and linguistic interpretation. Despite their impressive\ncapabilities across various tasks, LVLMs still suffer from the issue of\nhallucination, which involves generating content that is incorrect or\nunfaithful to the visual or textual inputs. Traditional benchmarks, such as MME\nand POPE, evaluate hallucination in LVLMs within the scope of Visual Question\nAnswering (VQA) using answerable questions. However, some questions are\nunanswerable due to insufficient information in the images, and the performance\nof LVLMs on such unanswerable questions remains underexplored. To bridge this\nresearch gap, we propose TUBench, a benchmark specifically designed to evaluate\nthe reliability of LVLMs using unanswerable questions. TUBench comprises an\nextensive collection of high-quality, unanswerable questions that are\nmeticulously crafted using ten distinct strategies. To thoroughly evaluate\nLVLMs, the unanswerable questions in TUBench are based on images from four\ndiverse domains as visual contexts: screenshots of code snippets, natural\nimages, geometry diagrams, and screenshots of statistical tables. These\nunanswerable questions are tailored to test LVLMs' trustworthiness in code\nreasoning, commonsense reasoning, geometric reasoning, and mathematical\nreasoning related to tables, respectively. We conducted a comprehensive\nquantitative evaluation of 28 leading foundational models on TUBench, with\nGemini-1.5-Pro, the top-performing model, achieving an average accuracy of\n69.2%, and GPT-4o, the third-ranked model, reaching 66.7% average accuracy, in\ndetermining whether questions are answerable. TUBench is available at\nhttps://github.com/NLPCode/TUBench.", "published": "2024-10-05 10:23:14", "link": "http://arxiv.org/abs/2410.04107v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "From Reading to Compressing: Exploring the Multi-document Reader for\n  Prompt Compression", "abstract": "Large language models (LLMs) have achieved significant performance gains\nusing advanced prompting techniques over various tasks. However, the increasing\nlength of prompts leads to high computational costs and often obscures crucial\ninformation. Prompt compression has been proposed to alleviate these issues,\nbut it faces challenges in (i) capturing the global context and (ii) training\nthe compressor effectively. To tackle these challenges, we introduce a novel\nprompt compression method, namely Reading To Compressing (R2C), utilizing the\nFusion-in-Decoder (FiD) architecture to identify the important information in\nthe prompt. Specifically, the cross-attention scores of the FiD are used to\ndiscern essential chunks and sentences from the prompt. R2C effectively\ncaptures the global context without compromising semantic consistency while\ndetouring the necessity of pseudo-labels for training the compressor. Empirical\nresults show that R2C retains key contexts, enhancing the LLM performance by 6%\nin out-of-domain evaluations while reducing the prompt length by 80%.", "published": "2024-10-05 12:27:47", "link": "http://arxiv.org/abs/2410.04139v2", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Reasoning with Natural Language Explanations", "abstract": "Explanation constitutes an archetypal feature of human rationality,\nunderpinning learning and generalisation, and representing one of the media\nsupporting scientific discovery and communication. Due to the importance of\nexplanations in human reasoning, an increasing amount of research in Natural\nLanguage Inference (NLI) has started reconsidering the role that explanations\nplay in learning and inference, attempting to build explanation-based NLI\nmodels that can effectively encode and use natural language explanations on\ndownstream tasks. Research in explanation-based NLI, however, presents specific\nchallenges and opportunities, as explanatory reasoning reflects aspects of both\nmaterial and formal inference, making it a particularly rich setting to model\nand deliver complex reasoning. In this tutorial, we provide a comprehensive\nintroduction to the field of explanation-based NLI, grounding this discussion\non the epistemological-linguistic foundations of explanations, systematically\ndescribing the main architectural trends and evaluation methodologies that can\nbe used to build systems capable of explanatory reasoning.", "published": "2024-10-05 13:15:24", "link": "http://arxiv.org/abs/2410.04148v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DiDOTS: Knowledge Distillation from Large-Language-Models for Dementia\n  Obfuscation in Transcribed Speech", "abstract": "Dementia is a sensitive neurocognitive disorder affecting tens of millions of\npeople worldwide and its cases are expected to triple by 2050. Alarmingly,\nrecent advancements in dementia classification make it possible for adversaries\nto violate affected individuals' privacy and infer their sensitive condition\nfrom speech transcriptions. Existing obfuscation methods in text have never\nbeen applied for dementia and depend on the availability of large labeled\ndatasets which are challenging to collect for sensitive medical attributes. In\nthis work, we bridge this research gap and tackle the above issues by\nleveraging Large-Language-Models (LLMs) with diverse prompt designs (zero-shot,\nfew-shot, and knowledge-based) to obfuscate dementia in speech transcripts. Our\nevaluation shows that LLMs are more effective dementia obfuscators compared to\ncompeting methods. However, they have billions of parameters which renders them\nhard to train, store and share, and they are also fragile suffering from\nhallucination, refusal and contradiction effects among others. To further\nmitigate these, we propose a novel method, DiDOTS. DiDOTS distills knowledge\nfrom LLMs using a teacher-student paradigm and parameter-efficient fine-tuning.\nDiDOTS has one order of magnitude fewer parameters compared to its teacher LLM\nand can be fine-tuned using three orders of magnitude less parameters compared\nto full fine-tuning. Our evaluation shows that compared to prior work DiDOTS\nretains the performance of LLMs achieving 1.3x and 2.2x improvement in privacy\nperformance on two datasets, while humans rate it as better in preserving\nutility even when compared to state-of-the-art paraphrasing models.", "published": "2024-10-05 15:07:03", "link": "http://arxiv.org/abs/2410.04188v1", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Harnessing Task Overload for Scalable Jailbreak Attacks on Large\n  Language Models", "abstract": "Large Language Models (LLMs) remain vulnerable to jailbreak attacks that\nbypass their safety mechanisms. Existing attack methods are fixed or\nspecifically tailored for certain models and cannot flexibly adjust attack\nstrength, which is critical for generalization when attacking models of various\nsizes. We introduce a novel scalable jailbreak attack that preempts the\nactivation of an LLM's safety policies by occupying its computational\nresources. Our method involves engaging the LLM in a resource-intensive\npreliminary task - a Character Map lookup and decoding process - before\npresenting the target instruction. By saturating the model's processing\ncapacity, we prevent the activation of safety protocols when processing the\nsubsequent instruction. Extensive experiments on state-of-the-art LLMs\ndemonstrate that our method achieves a high success rate in bypassing safety\nmeasures without requiring gradient access, manual prompt engineering. We\nverified our approach offers a scalable attack that quantifies attack strength\nand adapts to different model scales at the optimal strength. We shows safety\npolicies of LLMs might be more susceptible to resource constraints. Our\nfindings reveal a critical vulnerability in current LLM safety designs,\nhighlighting the need for more robust defense strategies that account for\nresource-intense condition.", "published": "2024-10-05 15:10:01", "link": "http://arxiv.org/abs/2410.04190v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "LongGenBench: Long-context Generation Benchmark", "abstract": "Current long-context benchmarks primarily focus on retrieval-based tests,\nrequiring Large Language Models (LLMs) to locate specific information within\nextensive input contexts, such as the needle-in-a-haystack (NIAH) benchmark.\nLong-context generation refers to the ability of a language model to generate\ncoherent and contextually accurate text that spans across lengthy passages or\ndocuments. While recent studies show strong performance on NIAH and other\nretrieval-based long-context benchmarks, there is a significant lack of\nbenchmarks for evaluating long-context generation capabilities. To bridge this\ngap and offer a comprehensive assessment, we introduce a synthetic benchmark,\nLongGenBench, which allows for flexible configurations of customized generation\ncontext lengths. LongGenBench advances beyond traditional benchmarks by\nredesigning the format of questions and necessitating that LLMs respond with a\nsingle, cohesive long-context answer. Upon extensive evaluation using\nLongGenBench, we observe that: (1) both API accessed and open source models\nexhibit performance degradation in long-context generation scenarios, ranging\nfrom 1.2% to 47.1%; (2) different series of LLMs exhibit varying trends of\nperformance degradation, with the Gemini-1.5-Flash model showing the least\ndegradation among API accessed models, and the Qwen2 series exhibiting the\nleast degradation in LongGenBench among open source models.", "published": "2024-10-05 15:33:25", "link": "http://arxiv.org/abs/2410.04199v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Correlation-Aware Select and Merge Attention for Efficient Fine-Tuning\n  and Context Length Extension", "abstract": "Modeling long sequences is crucial for various large-scale models; however,\nextending existing architectures to handle longer sequences presents\nsignificant technical and resource challenges. In this paper, we propose an\nefficient and flexible attention architecture that enables the extension of\ncontext lengths in large language models with reduced computational resources\nand fine-tuning time compared to other excellent methods. Specifically, we\nintroduce correlation-aware selection and merging mechanisms to facilitate\nefficient sparse attention. In addition, we also propose a novel data\naugmentation technique involving positional encodings to enhance generalization\nto unseen positions. The results are as follows: First, using a single A100, we\nachieve fine-tuning on Llama2-7B with a sequence length of 32K, which is more\nefficient than other methods that rely on subsets for regression. Second, we\npresent a comprehensive method for extending context lengths across the\npre-training, fine-tuning, and inference phases. During pre-training, our\nattention mechanism partially breaks translation invariance during token\nselection, so we apply positional encodings only to the selected tokens. This\napproach achieves relatively high performance and significant extrapolation\ncapabilities. For fine-tuning, we introduce Cyclic, Randomly Truncated, and\nDynamically Growing NTK Positional Embedding (CRD NTK). This design allows\nfine-tuning with a sequence length of only 16K, enabling models such as\nLlama2-7B and Mistral-7B to perform inference with context lengths of up to 1M\nor even arbitrary lengths. Our method achieves 100\\% accuracy on the passkey\ntask with a context length of 4M and maintains stable perplexity at a 1M\ncontext length. This represents at least a 64-fold reduction in resource\nrequirements compared to traditional full-attention mechanisms, while still\nachieving competitive performance.", "published": "2024-10-05 15:59:32", "link": "http://arxiv.org/abs/2410.04211v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Constructing Cloze Questions Generatively", "abstract": "We present a generative method called CQG for constructing cloze questions\nfrom a given article using neural networks and WordNet, with an emphasis on\ngenerating multigram distractors. Built on sense disambiguation, text-to-text\ntransformation, WordNet's synset taxonomies and lexical labels, CQG selects an\nanswer key for a given sentence, segments it into a sequence of instances,\ngenerates instance-level distractor candidates (IDCs) using a transformer and\nsibling synsets.It then removes inappropriate IDCs, ranks the remaining IDCs\nbased on contextual embedding similarities, as well as synset and lexical\nrelatedness, forms distractor candidates by combinatorially replacing instances\nwith the corresponding top-ranked IDCs, and checks if they are legitimate\nphrases. Finally, it selects top-ranked distractor candidates based on\ncontextual semantic similarities to the answer key. Experiments show that this\nmethod significantly outperforms SOTA results. Human judges also confirm the\nhigh qualities of the generated distractors.", "published": "2024-10-05 18:55:38", "link": "http://arxiv.org/abs/2410.04266v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Language Model-Driven Data Pruning Enables Efficient Active Learning", "abstract": "Active learning (AL) optimizes data labeling efficiency by selecting the most\ninformative instances for annotation. A key component in this procedure is an\nacquisition function that guides the selection process and identifies the\nsuitable instances for labeling from the unlabeled pool. However, these\nacquisition methods suffer from high computational costs with large unlabeled\ndata pools, posing a roadblock to their applicability on large datasets. To\naddress this challenge and bridge this gap, we introduce a novel plug-and-play\nunlabeled data pruning strategy, ActivePrune, which leverages language models\nto prune the unlabeled pool. ActivePrune implements a two-stage pruning\nprocess: an initial fast evaluation using perplexity scores from an n-gram\nlanguage model, followed by a high-quality selection using metrics for data\nquality computed through a quantized LLM. Additionally, to enhance the\ndiversity in the unlabeled pool, we propose a novel perplexity reweighting\nmethod that systematically brings forward underrepresented instances for\nselection in subsequent labeling iterations. Experiments on translation,\nsentiment analysis, topic classification, and summarization tasks on four\ndiverse datasets and four active learning strategies demonstrate that\nActivePrune outperforms existing data pruning methods. Finally, we compare the\nselection quality $\\leftrightarrow$ efficiency tradeoff of the data pruning\nmethods and demonstrate that ActivePrune is computationally more efficient than\nother LLM score-based pruning methods, and provides up to 74% reduction in the\nend-to-end time required for active learning.", "published": "2024-10-05 19:46:11", "link": "http://arxiv.org/abs/2410.04275v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Mechanistic Behavior Editing of Language Models", "abstract": "Large Language Models trained on web-scale text acquire language generation\nabilities that can solve a wide range of tasks, particularly when task\nknowledge is refined into the generative prior using in-context examples.\nHowever, spurious features learned from noisy data hinder their\ngeneralizability. Supervised finetuning can introduce task specificity, but\nintroduce data inefficiency. Prior studies indicate that (i) noisy neural\ncircuitries coexist with generalizable ones within LLMs, and (ii) finetuning\ntypically enhances (or suppresses) existing abilities without introducing newer\nones. Building upon these, we propose TaRot, a novel method for task\nadaptation. TaRot intervenes in the neural circuitries using learnable rotation\nmatrices that are optimized using Bayesian Optimization, on labelled samples in\nthe order of standard few-shot prompting examples. Experiments on multiple\nclassification and generation tasks using LLMs of varying sizes reveal the\nefficacy of TaRot, improving upon both zero- as well as few-shot performance,\nwith average improvements (across models and tasks) of 23.81% and 11.15%,\nrespectively. The source code is available at\nhttps://github.com/joykirat18/TaRot", "published": "2024-10-05 19:58:08", "link": "http://arxiv.org/abs/2410.04277v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Hyperbolic Fine-tuning for Large Language Models", "abstract": "Large language models (LLMs) have demonstrated remarkable performance on\nvarious tasks. However, it remains an open question whether the default\nEuclidean space is the most suitable choice for embedding tokens in LLMs. In\nthis study, we first investigate the non-Euclidean characteristics of LLMs. Our\nfindings reveal that token frequency follows a power-law distribution, with\nhigh-frequency tokens clustering near the origin and low-frequency tokens\npositioned farther away. Additionally, token embeddings exhibit a high degree\nof hyperbolicity, indicating a latent tree-like structure in the embedding\nspace. Building on the observation, we propose to efficiently fine-tune LLMs in\nhyperbolic space to better exploit the underlying complex structures. However,\nwe found that this fine-tuning in hyperbolic space cannot be achieved with\nnaive application of exponential and logarithmic maps, when the embedding and\nweight matrices both reside in Euclidean space. To address this technique\nissue, we introduce a new method called hyperbolic low-rank efficient\nfine-tuning, HypLoRA, that performs low-rank adaptation directly on the\nhyperbolic manifold, avoiding the cancellation effect caused by the exponential\nand logarithmic maps, thus preserving the hyperbolic modeling capabilities.\nThrough extensive experiments, we demonstrate that HypLoRA significantly\nenhances the performance of LLMs on reasoning tasks, particularly for complex\nreasoning problems. In particular, HypLoRA improves the performance in the\ncomplex AQuA dataset by up to 13.0%, showcasing its effectiveness in handling\ncomplex reasoning challenges", "published": "2024-10-05 02:58:25", "link": "http://arxiv.org/abs/2410.04010v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NE"], "primary_category": "cs.LG"}
{"title": "SyllableLM: Learning Coarse Semantic Units for Speech Language Models", "abstract": "Language models require tokenized inputs. However, tokenization strategies\nfor continuous data like audio and vision are often based on simple heuristics\nsuch as fixed sized convolutions or discrete clustering, which do not\nnecessarily align with the semantic structure of the data. For speech in\nparticular, the high resolution of waveforms (16,000 samples/second or more)\npresents a significant challenge as speech-based language models have had to\nuse several times more tokens per word than text-based language models. In this\nwork, we introduce a controllable self-supervised technique to merge speech\nrepresentations into coarser syllable-like units while still preserving\nsemantic information. We do this by 1) extracting noisy boundaries through\nanalyzing correlations in pretrained encoder losses and 2) iteratively\nimproving model representations with a novel distillation technique. Our method\nproduces controllable-rate semantic units at as low as 5Hz and 60bps and\nachieves SotA in syllabic segmentation and clustering. Using these coarse\ntokens, we successfully train SyllableLM, a Speech Language Model (SpeechLM)\nthat matches or outperforms current SotA SpeechLMs on a range of spoken\nlanguage modeling tasks. SyllableLM also achieves significant improvements in\nefficiency with a 30x reduction in training compute and a 4x wall-clock\ninference speedup.", "published": "2024-10-05 04:29:55", "link": "http://arxiv.org/abs/2410.04029v1", "categories": ["cs.CL", "cs.AI", "eess.AS"], "primary_category": "cs.CL"}
{"title": "On Eliciting Syntax from Language Models via Hashing", "abstract": "Unsupervised parsing, also known as grammar induction, aims to infer\nsyntactic structure from raw text. Recently, binary representation has\nexhibited remarkable information-preserving capabilities at both lexicon and\nsyntax levels. In this paper, we explore the possibility of leveraging this\ncapability to deduce parsing trees from raw text, relying solely on the\nimplicitly induced grammars within models. To achieve this, we upgrade the\nbit-level CKY from zero-order to first-order to encode the lexicon and syntax\nin a unified binary representation space, switch training from supervised to\nunsupervised under the contrastive hashing framework, and introduce a novel\nloss function to impose stronger yet balanced alignment signals. Our model\nshows competitive performance on various datasets, therefore, we claim that our\nmethod is effective and efficient enough to acquire high-quality parsing trees\nfrom pre-trained language models at a low cost.", "published": "2024-10-05 08:06:19", "link": "http://arxiv.org/abs/2410.04074v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Overview of Factify5WQA: Fact Verification through 5W Question-Answering", "abstract": "Researchers have found that fake news spreads much times faster than real\nnews. This is a major problem, especially in today's world where social media\nis the key source of news for many among the younger population. Fact\nverification, thus, becomes an important task and many media sites contribute\nto the cause. Manual fact verification is a tedious task, given the volume of\nfake news online. The Factify5WQA shared task aims to increase research towards\nautomated fake news detection by providing a dataset with an aspect-based\nquestion answering based fact verification method. Each claim and its\nsupporting document is associated with 5W questions that help compare the two\ninformation sources. The objective performance measure in the task is done by\ncomparing answers using BLEU score to measure the accuracy of the answers,\nfollowed by an accuracy measure of the classification. The task had submissions\nusing custom training setup and pre-trained language-models among others. The\nbest performing team posted an accuracy of 69.56%, which is a near 35%\nimprovement over the baseline.", "published": "2024-10-05 17:28:18", "link": "http://arxiv.org/abs/2410.04236v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Fundamental Limitations on Subquadratic Alternatives to Transformers", "abstract": "The Transformer architecture is widely deployed in many popular and impactful\nLarge Language Models. At its core is the attention mechanism for calculating\ncorrelations between pairs of tokens. Performing an attention computation takes\nquadratic time in the input size, and had become the time bottleneck for\ntransformer operations. In order to circumvent this, researchers have used a\nvariety of approaches, including designing heuristic algorithms for performing\nattention computations faster, and proposing alternatives to the attention\nmechanism which can be computed more quickly. For instance, state space models\nsuch as Mamba were designed to replace attention with an almost linear time\nalternative.\n  In this paper, we prove that any such approach cannot perform important tasks\nthat Transformer is able to perform (assuming a popular conjecture from\nfine-grained complexity theory). We focus on document similarity tasks, where\none is given as input many documents and would like to find a pair which is\n(approximately) the most similar. We prove that Transformer is able to perform\nthis task, and we prove that this task cannot be performed in truly\nsubquadratic time by any algorithm. Thus, any model which can be evaluated in\nsubquadratic time - whether because of subquadratic-time heuristics for\nattention, faster attention replacements like Mamba, or any other reason -\ncannot perform this task. In other words, in order to perform tasks that\n(implicitly or explicitly) involve document similarity, one may as well use\nTransformer and cannot avoid its quadratic running time.", "published": "2024-10-05 19:21:13", "link": "http://arxiv.org/abs/2410.04271v1", "categories": ["cs.LG", "cs.CC", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Understanding the Effect of Algorithm Transparency of Model Explanations\n  in Text-to-SQL Semantic Parsing", "abstract": "Explaining the decisions of AI has become vital for fostering appropriate\nuser trust in these systems. This paper investigates explanations for a\nstructured prediction task called ``text-to-SQL Semantic Parsing'', which\ntranslates a natural language question into a structured query language (SQL)\nprogram. In this task setting, we designed three levels of model explanation,\neach exposing a different amount of the model's decision-making details (called\n``algorithm transparency''), and investigated how different model explanations\ncould potentially yield different impacts on the user experience. Our study\nwith $\\sim$100 participants shows that (1) the low-/high-transparency\nexplanations often lead to less/more user reliance on the model decisions,\nwhereas the medium-transparency explanations strike a good balance. We also\nshow that (2) only the medium-transparency participant group was able to engage\nfurther in the interaction and exhibit increasing performance over time, and\nthat (3) they showed the least changes in trust before and after the study.", "published": "2024-10-05 00:13:33", "link": "http://arxiv.org/abs/2410.16283v2", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.HC", "I.3.6"], "primary_category": "cs.IR"}
{"title": "Assessing the Performance of Human-Capable LLMs -- Are LLMs Coming for\n  Your Job?", "abstract": "The current paper presents the development and validation of SelfScore, a\nnovel benchmark designed to assess the performance of automated Large Language\nModel (LLM) agents on help desk and professional consultation tasks. Given the\nincreasing integration of AI in industries, particularly within customer\nservice, SelfScore fills a crucial gap by enabling the comparison of automated\nagents and human workers. The benchmark evaluates agents on problem complexity\nand response helpfulness, ensuring transparency and simplicity in its scoring\nsystem. The study also develops automated LLM agents to assess SelfScore and\nexplores the benefits of Retrieval-Augmented Generation (RAG) for\ndomain-specific tasks, demonstrating that automated LLM agents incorporating\nRAG outperform those without. All automated LLM agents were observed to perform\nbetter than the human control group. Given these results, the study raises\nconcerns about the potential displacement of human workers, especially in areas\nwhere AI technologies excel. Ultimately, SelfScore provides a foundational tool\nfor understanding the impact of AI in help desk environments while advocating\nfor ethical considerations in the ongoing transition towards automation.", "published": "2024-10-05 14:37:35", "link": "http://arxiv.org/abs/2410.16285v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Large Language Models can Achieve Social Balance", "abstract": "Social balance is a well-established concept in sociology which dictates how\nindividual interactions can lead a population to become one faction of positive\ninteractions or be divided in two or more antagonistic factions. In this paper,\nwe consider a group of large language models (LLMs) and study how, after\ncontinuous interactions, they can achieve social balance. Across three\ndifferent LLM models, we find that achieving social balance depends on (i) the\ntype of interaction; (ii) whether agents consider homophily or influence from\ntheir peers; and (iii) the population size. We characterize how each model\nachieves social balance with different frequency, diversity of positive or\nnegative interactions, and interaction stability across conditions (i) to\n(iii). We show that models achieve different notions of social balance and\njustify their social dynamics differently. Remarkably, the largest model is not\nnecessarily more likely to achieve social balance with more frequency,\nstability, and diversity than the smaller ones.", "published": "2024-10-05 06:23:28", "link": "http://arxiv.org/abs/2410.04054v2", "categories": ["cs.CL", "cs.AI", "cs.MA", "cs.SI", "physics.soc-ph"], "primary_category": "cs.CL"}
{"title": "Enhancing Future Link Prediction in Quantum Computing Semantic Networks\n  through LLM-Initiated Node Features", "abstract": "Quantum computing is rapidly evolving in both physics and computer science,\noffering the potential to solve complex problems and accelerate computational\nprocesses. The development of quantum chips necessitates understanding the\ncorrelations among diverse experimental conditions. Semantic networks built on\nscientific literature, representing meaningful relationships between concepts,\nhave been used across various domains to identify knowledge gaps and novel\nconcept combinations. Neural network-based approaches have shown promise in\nlink prediction within these networks. This study proposes initializing node\nfeatures using LLMs to enhance node representations for link prediction tasks\nin graph neural networks. LLMs can provide rich descriptions, reducing the need\nfor manual feature creation and lowering costs. Our method, evaluated using\nvarious link prediction models on a quantum computing semantic network,\ndemonstrated efficacy compared to traditional node embedding techniques.", "published": "2024-10-05 18:16:07", "link": "http://arxiv.org/abs/2410.04251v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.SI", "quant-ph"], "primary_category": "cs.LG"}
{"title": "Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia", "abstract": "Links are a fundamental part of information networks, turning isolated pieces\nof knowledge into a network of information that is much richer than the sum of\nits parts. However, adding a new link to the network is not trivial: it\nrequires not only the identification of a suitable pair of source and target\nentities but also the understanding of the content of the source to locate a\nsuitable position for the link in the text. The latter problem has not been\naddressed effectively, particularly in the absence of text spans in the source\nthat could serve as anchors to insert a link to the target entity. To bridge\nthis gap, we introduce and operationalize the task of entity insertion in\ninformation networks. Focusing on the case of Wikipedia, we empirically show\nthat this problem is, both, relevant and challenging for editors. We compile a\nbenchmark dataset in 105 languages and develop a framework for entity insertion\ncalled LocEI (Localized Entity Insertion) and its multilingual variant XLocEI.\nWe show that XLocEI outperforms all baseline models (including state-of-the-art\nprompt-based ranking with LLMs such as GPT-4) and that it can be applied in a\nzero-shot manner on languages not seen during training with minimal performance\ndrop. These findings are important for applying entity insertion models in\npractice, e.g., to support editors in adding links across the more than 300\nlanguage versions of Wikipedia.", "published": "2024-10-05 18:22:15", "link": "http://arxiv.org/abs/2410.04254v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "The OCON model: an old but gold solution for distributable supervised\n  classification", "abstract": "This paper introduces to a structured application of the One-Class approach\nand the One-Class-One-Network model for supervised classification tasks,\nspecifically addressing a vowel phonemes classification case study within the\nAutomatic Speech Recognition research field. Through pseudo-Neural Architecture\nSearch and Hyper-Parameters Tuning experiments conducted with an informed\ngrid-search methodology, we achieve classification accuracy comparable to\nnowadays complex architectures (90.0 - 93.7%). Despite its simplicity, our\nmodel prioritizes generalization of language context and distributed\napplicability, supported by relevant statistical and performance metrics. The\nexperiments code is openly available at our GitHub.", "published": "2024-10-05 09:15:01", "link": "http://arxiv.org/abs/2410.05320v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.DB", "cs.LG", "cs.SD", "68T07, 68T09, 68T10, 68T50, 91F20", "I.2.7; I.2.11; I.5.1; I.5.2; I.5.5; J.5; E.4; D.2.7; D.2.13"], "primary_category": "eess.AS"}
{"title": "Adversarial Attacks and Robust Defenses in Speaker Embedding based\n  Zero-Shot Text-to-Speech System", "abstract": "Speaker embedding based zero-shot Text-to-Speech (TTS) systems enable\nhigh-quality speech synthesis for unseen speakers using minimal data. However,\nthese systems are vulnerable to adversarial attacks, where an attacker\nintroduces imperceptible perturbations to the original speaker's audio\nwaveform, leading to synthesized speech sounds like another person. This\nvulnerability poses significant security risks, including speaker identity\nspoofing and unauthorized voice manipulation. This paper investigates two\nprimary defense strategies to address these threats: adversarial training and\nadversarial purification. Adversarial training enhances the model's robustness\nby integrating adversarial examples during the training process, thereby\nimproving resistance to such attacks. Adversarial purification, on the other\nhand, employs diffusion probabilistic models to revert adversarially perturbed\naudio to its clean form. Experimental results demonstrate that these defense\nmechanisms can significantly reduce the impact of adversarial perturbations,\nenhancing the security and reliability of speaker embedding based zero-shot TTS\nsystems in adversarial environments.", "published": "2024-10-05 03:20:20", "link": "http://arxiv.org/abs/2410.04017v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Enhancement of Dysarthric Speech Reconstruction by Contrastive Learning", "abstract": "Dysarthric speech reconstruction is challenging due to its pathological sound\npatterns. Preserving speaker identity, especially without access to normal\nspeech, is a key challenge. Our proposed approach uses contrastive learning to\nextract speaker embedding for reconstruction, while employing XLS-R\nrepresentations instead of filter banks. The results show improved speech\nquality, naturalness, intelligibility, speaker identity preservation, and\ngender consistency for female speakers. Reconstructed speech exhibits 1.51 and\n2.12 MOS score improvements and reduces word error rates by 25.45% and 32.1%\nfor moderate and moderate-severe dysarthria speakers using Jasper speech\nrecognition system, respectively. This approach offers promising advancements\nin dysarthric speech reconstruction.", "published": "2024-10-05 09:23:46", "link": "http://arxiv.org/abs/2410.04092v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Efficient and Robust Long-Form Speech Recognition with Hybrid\n  H3-Conformer", "abstract": "Recently, Conformer has achieved state-of-the-art performance in many speech\nrecognition tasks. However, the Transformer-based models show significant\ndeterioration for long-form speech, such as lectures, because the\nself-attention mechanism becomes unreliable with the computation of the square\norder of the input length. To solve the problem, we incorporate a kind of\nstate-space model, Hungry Hungry Hippos (H3), to replace or complement the\nmulti-head self-attention (MHSA). H3 allows for efficient modeling of long-form\nsequences with a linear-order computation. In experiments using two datasets of\nCSJ and LibriSpeech, our proposed H3-Conformer model performs efficient and\nrobust recognition of long-form speech. Moreover, we propose a hybrid of H3 and\nMHSA and show that using H3 in higher layers and MHSA in lower layers provides\nsignificant improvement in online recognition. We also investigate a parallel\nuse of H3 and MHSA in all layers, resulting in the best performance.", "published": "2024-10-05 13:46:01", "link": "http://arxiv.org/abs/2410.04159v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DJ Mix Transcription with Multi-Pass Non-Negative Matrix Factorization", "abstract": "DJ mix transcription is a crucial step towards DJ mix reverse engineering,\nwhich estimates the set of parameters and audio effects applied to a set of\nexisting tracks to produce a performative DJ mix. We introduce a new approach\nbased on a multi-pass NMF algorithm where the dictionary matrix corresponds to\na set of spectrogram slices of the source tracks present in the mix.\n  The multi-pass strategy is motivated by the high computational cost resulting\nfrom the use of a large NMF dictionary. The proposed method uses inter-pass\nfiltering to favor temporal continuity and sparseness and is evaluated on a\npublicly available dataset.\n  Our comparative results considering a baseline method based on dynamic time\nwarping (DTW) are promising and pave the way of future NMF-based applications.", "published": "2024-10-05 15:31:48", "link": "http://arxiv.org/abs/2410.04198v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Cross-Lingual Query-by-Example Spoken Term Detection: A\n  Transformer-Based Approach", "abstract": "Query-by-example spoken term detection (QbE-STD) is typically constrained by\ntranscribed data scarcity and language specificity. This paper introduces a\nnovel, language-agnostic QbE-STD model leveraging image processing techniques\nand transformer architecture. By employing a pre-trained XLSR-53 network for\nfeature extraction and a Hough transform for detection, our model effectively\nsearches for user-defined spoken terms within any audio file. Experimental\nresults across four languages demonstrate significant performance gains\n(19-54%) over a CNN-based baseline. While processing time is improved compared\nto DTW, accuracy remains inferior. Notably, our model offers the advantage of\naccurately counting query term repetitions within the target audio.", "published": "2024-10-05 09:19:29", "link": "http://arxiv.org/abs/2410.04091v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "The OCON model: an old but green solution for distributable supervised\n  classification for acoustic monitoring in smart cities", "abstract": "This paper explores a structured application of the One-Class approach and\nthe One-Class-One-Network model for supervised classification tasks, focusing\non vowel phonemes classification and speakers recognition for the Automatic\nSpeech Recognition (ASR) domain. For our case-study, the ASR model runs on a\nproprietary sensing and lightning system, exploited to monitor acoustic and air\npollution on urban streets. We formalize combinations of pseudo-Neural\nArchitecture Search and Hyper-Parameters Tuning experiments, using an informed\ngrid-search methodology, to achieve classification accuracy comparable to\nnowadays most complex architectures, delving into the speaker recognition and\nenergy efficiency aspects. Despite its simplicity, our model proposal has a\nvery good chance to generalize the language and speaker genders context for\nwidespread applicability in computational constrained contexts, proved by\nrelevant statistical and performance metrics. Our experiments code is openly\naccessible on our GitHub.", "published": "2024-10-05 09:47:54", "link": "http://arxiv.org/abs/2410.04098v1", "categories": ["cs.SD", "cs.AI", "eess.AS", "68T05, 68T07, 68T10, 68T30, 68T50", "C.2.4; C.2.5; C.2.6; C.3; B.8.2; C.4; D.2.8; D.2.13; H.3.1; I.2.4;\n  I.2.6; I.2.7; I.2.8; I.2.11; I.5.1; I.5.4; I.5.5; J.5; J.7; K.4.0"], "primary_category": "cs.SD"}
