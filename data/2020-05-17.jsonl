{"title": "Adversarial Training for Commonsense Inference", "abstract": "We propose an AdversariaL training algorithm for commonsense InferenCE\n(ALICE). We apply small perturbations to word embeddings and minimize the\nresultant adversarial risk to regularize the model. We exploit a novel\ncombination of two different approaches to estimate these perturbations: 1)\nusing the true label and 2) using the model prediction. Without relying on any\nhuman-crafted features, knowledge bases, or additional datasets other than the\ntarget datasets, our model boosts the fine-tuning performance of RoBERTa,\nachieving competitive results on multiple reading comprehension datasets that\nrequire commonsense inference.", "published": "2020-05-17 03:28:12", "link": "http://arxiv.org/abs/2005.08156v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IMoJIE: Iterative Memory-Based Joint Open Information Extraction", "abstract": "While traditional systems for Open Information Extraction were statistical\nand rule-based, recently neural models have been introduced for the task. Our\nwork builds upon CopyAttention, a sequence generation OpenIE model (Cui et.\nal., 2018). Our analysis reveals that CopyAttention produces a constant number\nof extractions per sentence, and its extracted tuples often express redundant\ninformation.\n  We present IMoJIE, an extension to CopyAttention, which produces the next\nextraction conditioned on all previously extracted tuples. This approach\novercomes both shortcomings of CopyAttention, resulting in a variable number of\ndiverse extractions per sentence. We train IMoJIE on training data bootstrapped\nfrom extractions of several non-neural systems, which have been automatically\nfiltered to reduce redundancy and noise. IMoJIE outperforms CopyAttention by\nabout 18 F1 pts, and a BERT-based strong baseline by 2 F1 pts, establishing a\nnew state of the art for the task.", "published": "2020-05-17 07:04:08", "link": "http://arxiv.org/abs/2005.08178v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-Lingual Low-Resource Set-to-Description Retrieval for Global\n  E-Commerce", "abstract": "With the prosperous of cross-border e-commerce, there is an urgent demand for\ndesigning intelligent approaches for assisting e-commerce sellers to offer\nlocal products for consumers from all over the world. In this paper, we explore\na new task of cross-lingual information retrieval, i.e., cross-lingual\nset-to-description retrieval in cross-border e-commerce, which involves\nmatching product attribute sets in the source language with persuasive product\ndescriptions in the target language. We manually collect a new and high-quality\npaired dataset, where each pair contains an unordered product attribute set in\nthe source language and an informative product description in the target\nlanguage. As the dataset construction process is both time-consuming and\ncostly, the new dataset only comprises of 13.5k pairs, which is a low-resource\nsetting and can be viewed as a challenging testbed for model development and\nevaluation in cross-border e-commerce. To tackle this cross-lingual\nset-to-description retrieval task, we propose a novel cross-lingual matching\nnetwork (CLMN) with the enhancement of context-dependent cross-lingual mapping\nupon the pre-trained monolingual BERT representations. Experimental results\nindicate that our proposed CLMN yields impressive results on the challenging\ntask and the context-dependent cross-lingual mapping on BERT yields noticeable\nimprovement over the pre-trained multi-lingual BERT model.", "published": "2020-05-17 08:10:51", "link": "http://arxiv.org/abs/2005.08188v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Building a Hebrew Semantic Role Labeling Lexical Resource from Parallel\n  Movie Subtitles", "abstract": "We present a semantic role labeling resource for Hebrew built\nsemi-automatically through annotation projection from English. This corpus is\nderived from the multilingual OpenSubtitles dataset and includes short informal\nsentences, for which reliable linguistic annotations have been computed. We\nprovide a fully annotated version of the data including morphological analysis,\ndependency syntax and semantic role labeling in both FrameNet and PropBank\nstyles. Sentences are aligned between English and Hebrew, both sides include\nfull annotations and the explicit mapping from the English arguments to the\nHebrew ones. We train a neural SRL model on this Hebrew resource exploiting the\npre-trained multilingual BERT transformer model, and provide the first\navailable baseline model for Hebrew SRL as a reference point. The code we\nprovide is generic and can be adapted to other languages to bootstrap SRL\nresources.", "published": "2020-05-17 10:03:42", "link": "http://arxiv.org/abs/2005.08206v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Support-BERT: Predicting Quality of Question-Answer Pairs in MSDN using\n  Deep Bidirectional Transformer", "abstract": "Quality of questions and answers from community support websites (e.g.\nMicrosoft Developers Network, Stackoverflow, Github, etc.) is difficult to\ndefine and a prediction model of quality questions and answers is even more\nchallenging to implement. Previous works have addressed the question quality\nmodels and answer quality models separately using meta-features like number of\nup-votes, trustworthiness of the person posting the questions or answers,\ntitles of the post, and context naive natural language processing features.\nHowever, there is a lack of an integrated question-answer quality model for\ncommunity question answering websites in the literature. In this brief paper,\nwe tackle the quality Q&A modeling problems from the community support websites\nusing a recently developed deep learning model using bidirectional\ntransformers. We investigate the applicability of transfer learning on Q&A\nquality modeling using Bidirectional Encoder Representations from Transformers\n(BERT) trained on a separate tasks originally using Wikipedia. It is found that\na further pre-training of BERT model along with finetuning on the Q&As\nextracted from Microsoft Developer Network (MSDN) can boost the performance of\nautomated quality prediction to more than 80%. Furthermore, the implementations\nare carried out for deploying the finetuned model in real-time scenario using\nAzureML in Azure knowledge base system.", "published": "2020-05-17 16:50:28", "link": "http://arxiv.org/abs/2005.08294v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Context-Based Quotation Recommendation", "abstract": "While composing a new document, anything from a news article to an email or\nessay, authors often utilize direct quotes from a variety of sources. Although\nan author may know what point they would like to make, selecting an appropriate\nquote for the specific context may be time-consuming and difficult. We\ntherefore propose a novel context-aware quote recommendation system which\nutilizes the content an author has already written to generate a ranked list of\nquotable paragraphs and spans of tokens from a given source document.\n  We approach quote recommendation as a variant of open-domain question\nanswering and adapt the state-of-the-art BERT-based methods from open-QA to our\ntask. We conduct experiments on a collection of speech transcripts and\nassociated news articles, evaluating models' paragraph ranking and span\nprediction performances. Our experiments confirm the strong performance of\nBERT-based methods on this task, which outperform bag-of-words and neural\nranking baselines by more than 30% relative across all ranking metrics.\nQualitative analyses show the difficulty of the paragraph and span\nrecommendation tasks and confirm the quotability of the best BERT model's\npredictions, even if they are not the true selected quotes from the original\nnews articles.", "published": "2020-05-17 17:49:53", "link": "http://arxiv.org/abs/2005.08319v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-Lingual Word Embeddings for Turkic Languages", "abstract": "There has been an increasing interest in learning cross-lingual word\nembeddings to transfer knowledge obtained from a resource-rich language, such\nas English, to lower-resource languages for which annotated data is scarce,\nsuch as Turkish, Russian, and many others. In this paper, we present the first\nviability study of established techniques to align monolingual embedding spaces\nfor Turkish, Uzbek, Azeri, Kazakh and Kyrgyz, members of the Turkic family\nwhich is heavily affected by the low-resource constraint. Those techniques are\nknown to require little explicit supervision, mainly in the form of bilingual\ndictionaries, hence being easily adaptable to different domains, including\nlow-resource ones. We obtain new bilingual dictionaries and new word embeddings\nfor these languages and show the steps for obtaining cross-lingual word\nembeddings using state-of-the-art techniques. Then, we evaluate the results\nusing the bilingual dictionary induction task. Our experiments confirm that the\nobtained bilingual dictionaries outperform previously-available ones, and that\nword embeddings from a low-resource language can benefit from resource-rich\nclosely-related languages when they are aligned together. Furthermore,\nevaluation on an extrinsic task (Sentiment analysis on Uzbek) proves that\nmonolingual word embeddings can, although slightly, benefit from cross-lingual\nalignments.", "published": "2020-05-17 18:57:23", "link": "http://arxiv.org/abs/2005.08340v1", "categories": ["cs.CL", "68T50, 91F20", "I.2.7"], "primary_category": "cs.CL"}
{"title": "MixingBoard: a Knowledgeable Stylized Integrated Text Generation\n  Platform", "abstract": "We present MixingBoard, a platform for quickly building demos with a focus on\nknowledge grounded stylized text generation. We unify existing text generation\nalgorithms in a shared codebase and further adapt earlier algorithms for\nconstrained generation. To borrow advantages from different models, we\nimplement strategies for cross-model integration, from the token probability\nlevel to the latent space level. An interface to external knowledge is provided\nvia a module that retrieves on-the-fly relevant knowledge from passages on the\nweb or any document collection. A user interface for local development, remote\nwebpage access, and a RESTful API are provided to make it simple for users to\nbuild their own demos.", "published": "2020-05-17 20:29:27", "link": "http://arxiv.org/abs/2005.08365v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CS-NLP team at SemEval-2020 Task 4: Evaluation of State-of-the-art NLP\n  Deep Learning Architectures on Commonsense Reasoning Task", "abstract": "In this paper, we investigate a commonsense inference task that unifies\nnatural language understanding and commonsense reasoning. We describe our\nattempt at SemEval-2020 Task 4 competition: Commonsense Validation and\nExplanation (ComVE) challenge. We discuss several state-of-the-art deep\nlearning architectures for this challenge. Our system uses prepared labeled\ntextual datasets that were manually curated for three different natural\nlanguage inference subtasks. The goal of the first subtask is to test whether a\nmodel can distinguish between natural language statements that make sense and\nthose that do not make sense. We compare the performance of several language\nmodels and fine-tuned classifiers. Then, we propose a method inspired by\nquestion/answering tasks to treat a classification problem as a multiple choice\nquestion task to boost the performance of our experimental results (96.06%),\nwhich is significantly better than the baseline. For the second subtask, which\nis to select the reason why a statement does not make sense, we stand within\nthe first six teams (93.7%) among 27 participants with very competitive\nresults. Our result for last subtask of generating reason against the nonsense\nstatement shows many potentials for future researches as we applied the most\npowerful generative model of language (GPT-2) with 6.1732 BLEU score among\nfirst four teams.", "published": "2020-05-17 13:20:10", "link": "http://arxiv.org/abs/2006.01205v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semi-Automating Knowledge Base Construction for Cancer Genetics", "abstract": "In this work, we consider the exponentially growing subarea of genetics in\ncancer. The need to synthesize and centralize this evidence for dissemination\nhas motivated a team of physicians to manually construct and maintain a\nknowledge base that distills key results reported in the literature. This is a\nlaborious process that entails reading through full-text articles to understand\nthe study design, assess study quality, and extract the reported cancer risk\nestimates associated with particular hereditary cancer genes (i.e.,\npenetrance). In this work, we propose models to automatically surface key\nelements from full-text cancer genetics articles, with the ultimate aim of\nexpediting the manual workflow currently in place.\n  We propose two challenging tasks that are critical for characterizing the\nfindings reported cancer genetics studies: (i) Extracting snippets of text that\ndescribe \\emph{ascertainment mechanisms}, which in turn inform whether the\npopulation studied may introduce bias owing to deviations from the target\npopulation; (ii) Extracting reported risk estimates (e.g., odds or hazard\nratios) associated with specific germline mutations. The latter task may be\nviewed as a joint entity tagging and relation extraction problem. To train\nmodels for these tasks, we induce distant supervision over tokens and snippets\nin full-text articles using the manually constructed knowledge base. We propose\nand evaluate several model variants, including a transformer-based joint entity\nand relation extraction model to extract <germline mutation, risk-estimate>}\npairs. We observe strong empirical performance, highlighting the practical\npotential for such models to aid KB construction in this space. We ablate\ncomponents of our model, observing, e.g., that a joint model for <germline\nmutation, risk-estimate> fares substantially better than a pipelined approach.", "published": "2020-05-17 02:01:43", "link": "http://arxiv.org/abs/2005.08146v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Encodings of Source Syntax: Similarities in NMT Representations Across\n  Target Languages", "abstract": "We train neural machine translation (NMT) models from English to six target\nlanguages, using NMT encoder representations to predict ancestor constituent\nlabels of source language words. We find that NMT encoders learn similar source\nsyntax regardless of NMT target language, relying on explicit morphosyntactic\ncues to extract syntactic features from source sentences. Furthermore, the NMT\nencoders outperform RNNs trained directly on several of the constituent label\nprediction tasks, suggesting that NMT encoder representations can be used\neffectively for natural language tasks involving syntax. However, both the NMT\nencoders and the directly-trained RNNs learn substantially different syntactic\ninformation from a probabilistic context-free grammar (PCFG) parser. Despite\nlower overall accuracy scores, the PCFG often performs well on sentences for\nwhich the RNN-based models perform poorly, suggesting that RNN architectures\nare constrained in the types of syntax they can learn.", "published": "2020-05-17 06:41:32", "link": "http://arxiv.org/abs/2005.08177v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "How much complexity does an RNN architecture need to learn\n  syntax-sensitive dependencies?", "abstract": "Long short-term memory (LSTM) networks and their variants are capable of\nencapsulating long-range dependencies, which is evident from their performance\non a variety of linguistic tasks. On the other hand, simple recurrent networks\n(SRNs), which appear more biologically grounded in terms of synaptic\nconnections, have generally been less successful at capturing long-range\ndependencies as well as the loci of grammatical errors in an unsupervised\nsetting. In this paper, we seek to develop models that bridge the gap between\nbiological plausibility and linguistic competence. We propose a new\narchitecture, the Decay RNN, which incorporates the decaying nature of neuronal\nactivations and models the excitatory and inhibitory connections in a\npopulation of neurons. Besides its biological inspiration, our model also shows\ncompetitive performance relative to LSTMs on subject-verb agreement, sentence\ngrammaticality, and language modeling tasks. These results provide some\npointers towards probing the nature of the inductive biases required for RNN\narchitectures to model linguistic phenomena successfully.", "published": "2020-05-17 09:13:28", "link": "http://arxiv.org/abs/2005.08199v2", "categories": ["cs.CL", "q-bio.NC", "I.2.6; I.2.7; J.5"], "primary_category": "cs.CL"}
{"title": "LiSSS: A toy corpus of Spanish Literary Sentences for Emotions detection", "abstract": "In this work we present a new small data-set in Computational Creativity (CC)\nfield, the Spanish Literary Sentences for emotions detection corpus (LISSS). We\naddress this corpus of literary sentences in order to evaluate or design\nalgorithms of emotions classification and detection. We have constitute this\ncorpus by manually classifying the sentences in a set of emotions: Love, Fear,\nHappiness, Anger and Sadness/Pain. We also present some baseline classification\nalgorithms applied on our corpus. The LISSS corpus will be available to the\ncommunity as a free resource to evaluate or create CC-like algorithms.", "published": "2020-05-17 11:14:30", "link": "http://arxiv.org/abs/2005.08223v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "#Coronavirus or #Chinesevirus?!: Understanding the negative sentiment\n  reflected in Tweets with racist hashtags across the development of COVID-19", "abstract": "Situated in the global outbreak of COVID-19, our study enriches the\ndiscussion concerning the emergent racism and xenophobia on social media. With\nbig data extracted from Twitter, we focus on the analysis of negative sentiment\nreflected in tweets marked with racist hashtags, as racism and xenophobia are\nmore likely to be delivered via the negative sentiment. Especially, we propose\na stage-based approach to capture how the negative sentiment changes along with\nthe three development stages of COVID-19, under which it transformed from a\ndomestic epidemic into an international public health emergency and later, into\nthe global pandemic. At each stage, sentiment analysis enables us to recognize\nthe negative sentiment from tweets with racist hashtags, and keyword extraction\nallows for the discovery of themes in the expression of negative sentiment by\nthese tweets. Under this public health crisis of human beings, this stage-based\napproach enables us to provide policy suggestions for the enactment of\nstage-specific intervention strategies to combat racism and xenophobia on\nsocial media in a more effective way.", "published": "2020-05-17 11:15:50", "link": "http://arxiv.org/abs/2005.08224v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "On the Combined Use of Extrinsic Semantic Resources for Medical\n  Information Search", "abstract": "Semantic concepts and relations encoded in domain-specific ontologies and\nother medical semantic resources play a crucial role in deciphering terms in\nmedical queries and documents. The exploitation of these resources for tackling\nthe semantic gap issue has been widely studied in the literature. However,\nthere are challenges that hinder their widespread use in real-world\napplications. Among these challenges is the insufficient knowledge individually\nencoded in existing medical ontologies, which is magnified when users express\ntheir information needs using long-winded natural language queries. In this\ncontext, many of the users query terms are either unrecognized by the used\nontologies, or cause retrieving false positives that degrade the quality of\ncurrent medical information search approaches. In this article, we explore the\ncombination of multiple extrinsic semantic resources in the development of a\nfull-fledged medical information search framework to: i) highlight and expand\nhead medical concepts in verbose medical queries (i.e. concepts among query\nterms that significantly contribute to the informativeness and intent of a\ngiven query), ii) build semantically enhanced inverted index documents, iii)\ncontribute to a heuristical weighting technique in the query document matching\nprocess. To demonstrate the effectiveness of the proposed approach, we\nconducted several experiments over the CLEF eHealth 2014 dataset. Findings\nindicate that the proposed method combining several extrinsic semantic\nresources proved to be more effective than related approaches in terms of\nprecision measure.", "published": "2020-05-17 14:18:04", "link": "http://arxiv.org/abs/2005.08259v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data", "abstract": "Recent years have witnessed the burgeoning of pretrained language models\n(LMs) for text-based natural language (NL) understanding tasks. Such models are\ntypically trained on free-form NL text, hence may not be suitable for tasks\nlike semantic parsing over structured data, which require reasoning over both\nfree-form NL questions and structured tabular data (e.g., database tables). In\nthis paper we present TaBERT, a pretrained LM that jointly learns\nrepresentations for NL sentences and (semi-)structured tables. TaBERT is\ntrained on a large corpus of 26 million tables and their English contexts. In\nexperiments, neural semantic parsers using TaBERT as feature representation\nlayers achieve new best results on the challenging weakly-supervised semantic\nparsing benchmark WikiTableQuestions, while performing competitively on the\ntext-to-SQL dataset Spider. Implementation of the model will be available at\nhttp://fburl.com/TaBERT .", "published": "2020-05-17 17:26:40", "link": "http://arxiv.org/abs/2005.08314v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multi-modal Automated Speech Scoring using Attention Fusion", "abstract": "In this study, we propose a novel multi-modal end-to-end neural approach for\nautomated assessment of non-native English speakers' spontaneous speech using\nattention fusion. The pipeline employs Bi-directional Recurrent Convolutional\nNeural Networks and Bi-directional Long Short-Term Memory Neural Networks to\nencode acoustic and lexical cues from spectrograms and transcriptions,\nrespectively. Attention fusion is performed on these learned predictive\nfeatures to learn complex interactions between different modalities before\nfinal scoring. We compare our model with strong baselines and find combined\nattention to both lexical and acoustic cues significantly improves the overall\nperformance of the system. Further, we present a qualitative and quantitative\nanalysis of our model.", "published": "2020-05-17 07:53:15", "link": "http://arxiv.org/abs/2005.08182v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Speech to Text Adaptation: Towards an Efficient Cross-Modal Distillation", "abstract": "Speech is one of the most effective means of communication and is full of\ninformation that helps the transmission of utterer's thoughts. However, mainly\ndue to the cumbersome processing of acoustic features, phoneme or word\nposterior probability has frequently been discarded in understanding the\nnatural language. Thus, some recent spoken language understanding (SLU) modules\nhave utilized end-to-end structures that preserve the uncertainty information.\nThis further reduces the propagation of speech recognition error and guarantees\ncomputational efficiency. We claim that in this process, the speech\ncomprehension can benefit from the inference of massive pre-trained language\nmodels (LMs). We transfer the knowledge from a concrete Transformer-based text\nLM to an SLU module which can face a data shortage, based on recent cross-modal\ndistillation methodologies. We demonstrate the validity of our proposal upon\nthe performance on Fluent Speech Command, an English SLU benchmark. Thereby, we\nexperimentally verify our hypothesis that the knowledge could be shared from\nthe top layer of the LM to a fully speech-based module, in which the abstracted\nspeech is expected to meet the semantic representation.", "published": "2020-05-17 10:50:15", "link": "http://arxiv.org/abs/2005.08213v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Dual Learning: Theoretical Study and an Algorithmic Extension", "abstract": "Dual learning has been successfully applied in many machine learning\napplications including machine translation, image-to-image transformation, etc.\nThe high-level idea of dual learning is very intuitive: if we map an $x$ from\none domain to another and then map it back, we should recover the original $x$.\nAlthough its effectiveness has been empirically verified, theoretical\nunderstanding of dual learning is still very limited. In this paper, we aim at\nunderstanding why and when dual learning works. Based on our theoretical\nanalysis, we further extend dual learning by introducing more related mappings\nand propose multi-step dual learning, in which we leverage feedback signals\nfrom additional domains to improve the qualities of the mappings. We prove that\nmulti-step dual learn-ing can boost the performance of standard dual learning\nunder mild conditions. Experiments on WMT 14 English$\\leftrightarrow$German and\nMultiUNEnglish$\\leftrightarrow$French translations verify our theoretical\nfindings on dual learning, and the results on the translations among English,\nFrench, and Spanish of MultiUN demonstrate the effectiveness of multi-step dual\nlearning.", "published": "2020-05-17 12:14:35", "link": "http://arxiv.org/abs/2005.08238v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Wake Word Detection with Alignment-Free Lattice-Free MMI", "abstract": "Always-on spoken language interfaces, e.g. personal digital assistants, rely\non a wake word to start processing spoken input. We present novel methods to\ntrain a hybrid DNN/HMM wake word detection system from partially labeled\ntraining data, and to use it in on-line applications: (i) we remove the\nprerequisite of frame-level alignments in the LF-MMI training algorithm,\npermitting the use of un-transcribed training examples that are annotated only\nfor the presence/absence of the wake word; (ii) we show that the classical\nkeyword/filler model must be supplemented with an explicit non-speech (silence)\nmodel for good performance; (iii) we present an FST-based decoder to perform\nonline detection. We evaluate our methods on two real data sets, showing\n50%--90% reduction in false rejection rates at pre-specified false alarm rates\nover the best previously published figures, and re-validate them on a third\n(large) data set.", "published": "2020-05-17 19:22:25", "link": "http://arxiv.org/abs/2005.08347v3", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Fixed Point Semantics for Stream Reasoning", "abstract": "Reasoning over streams of input data is an essential part of human\nintelligence. During the last decade {\\em stream reasoning} has emerged as a\nresearch area within the AI-community with many potential applications. In\nfact, the increased availability of streaming data via services like Google and\nFacebook has raised the need for reasoning engines coping with data that\nchanges at high rate. Recently, the rule-based formalism {\\em LARS} for\nnon-monotonic stream reasoning under the answer set semantics has been\nintroduced. Syntactically, LARS programs are logic programs with negation\nincorporating operators for temporal reasoning, most notably {\\em window\noperators} for selecting relevant time points. Unfortunately, by preselecting\n{\\em fixed} intervals for the semantic evaluation of programs, the rigid\nsemantics of LARS programs is not flexible enough to {\\em constructively} cope\nwith rapidly changing data dependencies. Moreover, we show that defining the\nanswer set semantics of LARS in terms of FLP reducts leads to undesirable\ncircular justifications similar to other ASP extensions. This paper fixes all\nof the aforementioned shortcomings of LARS. More precisely, we contribute to\nthe foundations of stream reasoning by providing an operational fixed point\nsemantics for a fully flexible variant of LARS and we show that our semantics\nis sound and constructive in the sense that answer sets are derivable bottom-up\nand free of circular justifications.", "published": "2020-05-17 22:25:24", "link": "http://arxiv.org/abs/2005.08384v1", "categories": ["cs.LO", "cs.AI", "cs.CL", "cs.PL"], "primary_category": "cs.LO"}
{"title": "Vector-Quantized Autoregressive Predictive Coding", "abstract": "Autoregressive Predictive Coding (APC), as a self-supervised objective, has\nenjoyed success in learning representations from large amounts of unlabeled\ndata, and the learned representations are rich for many downstream tasks.\nHowever, the connection between low self-supervised loss and strong performance\nin downstream tasks remains unclear. In this work, we propose Vector-Quantized\nAutoregressive Predictive Coding (VQ-APC), a novel model that produces\nquantized representations, allowing us to explicitly control the amount of\ninformation encoded in the representations. By studying a sequence of\nincreasingly limited models, we reveal the constituents of the learned\nrepresentations. In particular, we confirm the presence of information with\nprobing tasks, while showing the absence of information with mutual\ninformation, uncovering the model's preference in preserving speech information\nas its capacity becomes constrained. We find that there exists a point where\nphonetic and speaker information are amplified to maximize a self-supervised\nobjective. As a byproduct, the learned codes for a particular model capacity\ncorrespond well to English phones.", "published": "2020-05-17 23:06:09", "link": "http://arxiv.org/abs/2005.08392v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Content analysis of Persian/Farsi Tweets during COVID-19 pandemic in\n  Iran using NLP", "abstract": "Iran, along with China, South Korea, and Italy was among the countries that\nwere hit hard in the first wave of the COVID-19 spread. Twitter is one of the\nwidely-used online platforms by Iranians inside and abroad for sharing their\nopinion, thoughts, and feelings about a wide range of issues. In this study,\nusing more than 530,000 original tweets in Persian/Farsi on COVID-19, we\nanalyzed the topics discussed among users, who are mainly Iranians, to gauge\nand track the response to the pandemic and how it evolved over time. We applied\na combination of manual annotation of a random sample of tweets and topic\nmodeling tools to classify the contents and frequency of each category of\ntopics. We identified the top 25 topics among which living experience under\nhome quarantine emerged as a major talking point. We additionally categorized\nbroader content of tweets that shows satire, followed by news, is the dominant\ntweet type among the Iranian users. While this framework and methodology can be\nused to track public response to ongoing developments related to COVID-19, a\ngeneralization of this framework can become a useful framework to gauge Iranian\npublic reaction to ongoing policy measures or events locally and\ninternationally.", "published": "2020-05-17 23:47:08", "link": "http://arxiv.org/abs/2005.08400v1", "categories": ["cs.SI", "cs.CL", "cs.CY"], "primary_category": "cs.SI"}
{"title": "A Better Use of Audio-Visual Cues: Dense Video Captioning with Bi-modal\n  Transformer", "abstract": "Dense video captioning aims to localize and describe important events in\nuntrimmed videos. Existing methods mainly tackle this task by exploiting only\nvisual features, while completely neglecting the audio track. Only a few prior\nworks have utilized both modalities, yet they show poor results or demonstrate\nthe importance on a dataset with a specific domain. In this paper, we introduce\nBi-modal Transformer which generalizes the Transformer architecture for a\nbi-modal input. We show the effectiveness of the proposed model with audio and\nvisual modalities on the dense video captioning task, yet the module is capable\nof digesting any two modalities in a sequence-to-sequence task. We also show\nthat the pre-trained bi-modal encoder as a part of the bi-modal transformer can\nbe used as a feature extractor for a simple proposal generation module. The\nperformance is demonstrated on a challenging ActivityNet Captions dataset where\nour model achieves outstanding performance. The code is available:\nv-iashin.github.io/bmt", "published": "2020-05-17 15:00:05", "link": "http://arxiv.org/abs/2005.08271v2", "categories": ["cs.CV", "cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "An Open source Implementation of ITU-T Recommendation P.808 with\n  Validation", "abstract": "The ITU-T Recommendation P.808 provides a crowdsourcing approach for\nconducting a subjective assessment of speech quality using the Absolute\nCategory Rating (ACR) method. We provide an open-source implementation of the\nITU-T Rec. P.808 that runs on the Amazon Mechanical Turk platform. We extended\nour implementation to include Degradation Category Ratings (DCR) and Comparison\nCategory Ratings (CCR) test methods. We also significantly speed up the test\nprocess by integrating the participant qualification step into the main rating\ntask compared to a two-stage qualification and rating solution. We provide\nprogram scripts for creating and executing the subjective test, and data\ncleansing and analyzing the answers to avoid operational errors. To validate\nthe implementation, we compare the Mean Opinion Scores (MOS) collected through\nour implementation with MOS values from a standard laboratory experiment\nconducted based on the ITU-T Rec. P.800. We also evaluate the reproducibility\nof the result of the subjective speech quality assessment through crowdsourcing\nusing our implementation. Finally, we quantify the impact of parts of the\nsystem designed to improve the reliability: environmental tests, gold and\ntrapping questions, rating patterns, and a headset usage test.", "published": "2020-05-17 00:41:22", "link": "http://arxiv.org/abs/2005.08138v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Voice Activity Detection Scheme by Combining DNN Model with GMM Model", "abstract": "Due to the superior modeling ability of deep neural network (DNN), it is\nwidely used in voice activity detection (VAD). However, the performance may\ndegrade if no sufficient data especially for practical data could be used for\ntraining, thus, leading to inferior ability of adaption to environment.\nMoreover, large model structure could not always be used in practical,\nespecially for low cost devices where restricted hardware is used. This is on\nthe contrary for Gaussian mixture model (GMM) where model parameters can be\nupdated in real-time, but, with low modeling ability. In this paper, deeply\nintegrated scheme combining these two models are proposed to improve\nadaptability and modeling ability. This is done by directly combining the\nresults of models and feeding it back, together with the result of the DNN\nmodel, to update the GMM model. Besides, a control scheme is elaborately\ndesigned to detect the endpoints of speech. The superior performance by\nemploying this scheme is validated through experiments in practical, which give\nan insight into the advantage of combining supervised learning and unsupervised\nlearning.", "published": "2020-05-17 08:01:27", "link": "http://arxiv.org/abs/2005.08184v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Identification/Segmentation of Indian Regional Languages with Singular\n  Value Decomposition based Feature Embedding", "abstract": "language identification (LID) is identifing a language in a given spoken\nutterance. Language segmentation is equally inportant as language\nidentification where language boundaries can be spotted in a multi language\nutterance. In this paper, we have experimented with two schemes for language\nidentification in Indian regional language context as very few works has been\ndone. Singular value based feature embedding is used for both of the schemes.\nIn first scheme, the singular value decomposition (SVD) is applied to the\nn-gram utterance matrix and in the second scheme, SVD is applied on the\ndifference supervector matrix space. We have observed that in both the schemes,\n55-65% singular value energy is sufficient to capture the language context. In\nn-gram based feature representation, we have seen that different skipgram\nmodels capture different language context. We have observed that for short test\nduration, supervector based feature representation is better but with a longer\nduration test signal, n-gram based feature performed better. We have also\nextended our work to explore language-based segmentation where we have seen\nthat segmentation accuracy of four language group with ten language training\nmodel, scheme-1 has performed well but with same four language training model,\nscheme-2 outperformed scheme-1", "published": "2020-05-17 11:42:51", "link": "http://arxiv.org/abs/2005.08229v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Single Channel Far Field Feature Enhancement For Speaker Verification In\n  The Wild", "abstract": "We investigated an enhancement and a domain adaptation approach to make\nspeaker verification systems robust to perturbations of far-field speech. In\nthe enhancement approach, using paired (parallel) reverberant-clean speech, we\ntrained a supervised Generative Adversarial Network (GAN) along with a feature\nmapping loss. For the domain adaptation approach, we trained a Cycle Consistent\nGenerative Adversarial Network (CycleGAN), which maps features from far-field\ndomain to the speaker embedding training domain. This was trained on unpaired\ndata in an unsupervised manner. Both networks, termed Supervised Enhancement\nNetwork (SEN) and Domain Adaptation Network (DAN) respectively, were trained\nwith multi-task objectives in (filter-bank) feature domain. On a simulated test\nsetup, we first note the benefit of using feature mapping (FM) loss along with\nadversarial loss in SEN. Then, we tested both supervised and unsupervised\napproaches on several real noisy datasets. We observed relative improvements\nranging from 2% to 31% in terms of DCF. Using three training schemes, we also\nestablish the effectiveness of the novel DAN approach.", "published": "2020-05-17 18:15:12", "link": "http://arxiv.org/abs/2005.08331v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multimodal Target Speech Separation with Voice and Face References", "abstract": "Target speech separation refers to isolating target speech from a\nmulti-speaker mixture signal by conditioning on auxiliary information about the\ntarget speaker. Different from the mainstream audio-visual approaches which\nusually require simultaneous visual streams as additional input, e.g. the\ncorresponding lip movement sequences, in our approach we propose the novel use\nof a single face profile of the target speaker to separate expected clean\nspeech. We exploit the fact that the image of a face contains information about\nthe person's speech sound. Compared to using a simultaneous visual sequence, a\nface image is easier to obtain by pre-enrollment or on websites, which enables\nthe system to generalize to devices without cameras. To this end, we\nincorporate face embeddings extracted from a pretrained model for face\nrecognition into the speech separation, which guide the system in predicting a\ntarget speaker mask in the time-frequency domain. The experimental results show\nthat a pre-enrolled face image is able to benefit separating expected speech\nsignals. Additionally, face information is complementary to voice reference and\nwe show that further improvement can be achieved when combing both face and\nvoice embeddings.", "published": "2020-05-17 18:35:28", "link": "http://arxiv.org/abs/2005.08335v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "North Atlantic Right Whales Up-call Detection Using Multimodel Deep\n  Learning", "abstract": "A new method for North Atlantic Right Whales (NARW) up-call detection using\nMultimodel Deep Learning (MMDL) is presented in this paper. In this approach,\nsignals from passive acoustic sensors are first converted to spectrogram and\nscalogram images, which are time-frequency representations of the signals.\nThese images are in turn used to train an MMDL detec-tor, consisting of\nConvolutional Neural Networks (CNNs) and Stacked Auto Encoders (SAEs). Our\nexperimental studies revealed that CNNs work better with spectrograms and SAEs\nwith sca-lograms. Therefore in our experimental design, the CNNs are trained by\nusing spectrogram im-ages, and the SAEs are trained by using scalogram images.\nA fusion mechanism is used to fuse the results from individual neural networks.\nIn this paper, the results obtained from the MMDL detector are compared with\nthose obtained from conventional machine learning algorithms trained with\nhandcraft features. It is shown that the performance of the MMDL detector is\nsig-nificantly better than those of the representative conventional machine\nlearning methods in terms of up-call detection rate, non-up-call detection\nrate, and false alarm rate.", "published": "2020-05-17 19:52:26", "link": "http://arxiv.org/abs/2005.08356v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Learning Individual Speaking Styles for Accurate Lip to Speech Synthesis", "abstract": "Humans involuntarily tend to infer parts of the conversation from lip\nmovements when the speech is absent or corrupted by external noise. In this\nwork, we explore the task of lip to speech synthesis, i.e., learning to\ngenerate natural speech given only the lip movements of a speaker.\nAcknowledging the importance of contextual and speaker-specific cues for\naccurate lip-reading, we take a different path from existing works. We focus on\nlearning accurate lip sequences to speech mappings for individual speakers in\nunconstrained, large vocabulary settings. To this end, we collect and release a\nlarge-scale benchmark dataset, the first of its kind, specifically to train and\nevaluate the single-speaker lip to speech task in natural settings. We propose\na novel approach with key design choices to achieve accurate, natural lip to\nspeech synthesis in such unconstrained scenarios for the first time. Extensive\nevaluation using quantitative, qualitative metrics and human evaluation shows\nthat our method is four times more intelligible than previous works in this\nspace. Please check out our demo video for a quick overview of the paper,\nmethod, and qualitative results.\nhttps://www.youtube.com/watch?v=HziA-jmlk_4&feature=youtu.be", "published": "2020-05-17 10:29:19", "link": "http://arxiv.org/abs/2005.08209v1", "categories": ["cs.CV", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
