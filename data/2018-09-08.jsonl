{"title": "Exploration on Grounded Word Embedding: Matching Words and Images with\n  Image-Enhanced Skip-Gram Model", "abstract": "Word embedding is designed to represent the semantic meaning of a word with\nlow dimensional vectors. The state-of-the-art methods of learning word\nembeddings (word2vec and GloVe) only use the word co-occurrence information.\nThe learned embeddings are real number vectors, which are obscure to human. In\nthis paper, we propose an Image-Enhanced Skip-Gram Model to learn grounded word\nembeddings by representing the word vectors in the same hyper-plane with image\nvectors. Experiments show that the image vectors and word embeddings learned by\nour model are highly correlated, which indicates that our model is able to\nprovide a vivid image-based explanation to the word embeddings.", "published": "2018-09-08 06:43:01", "link": "http://arxiv.org/abs/1809.02765v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating Distractors for Reading Comprehension Questions from Real\n  Examinations", "abstract": "We investigate the task of distractor generation for multiple choice reading\ncomprehension questions from examinations. In contrast to all previous works,\nwe do not aim at preparing words or short phrases distractors, instead, we\nendeavor to generate longer and semantic-rich distractors which are closer to\ndistractors in real reading comprehension from examinations. Taking a reading\ncomprehension article, a pair of question and its correct option as input, our\ngoal is to generate several distractors which are somehow related to the\nanswer, consistent with the semantic context of the question and have some\ntrace in the article. We propose a hierarchical encoder-decoder framework with\nstatic and dynamic attention mechanisms to tackle this task. Specifically, the\ndynamic attention can combine sentence-level and word-level attention varying\nat each recurrent time step to generate a more readable sequence. The static\nattention is to modulate the dynamic attention not to focus on question\nirrelevant sentences or sentences which contribute to the correct option. Our\nproposed framework outperforms several strong baselines on the first prepared\ndistractor generation dataset of real reading comprehension questions. For\nhuman evaluation, compared with those distractors generated by baselines, our\ngenerated distractors are more functional to confuse the annotators.", "published": "2018-09-08 07:11:15", "link": "http://arxiv.org/abs/1809.02768v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentiment analysis for Arabic language: A brief survey of approaches and\n  techniques", "abstract": "With the emergence of Web 2.0 technology and the expansion of on-line social\nnetworks, current Internet users have the ability to add their reviews, ratings\nand opinions on social media and on commercial and news web sites. Sentiment\nanalysis aims to classify these reviews reviews in an automatic way. In the\nliterature, there are numerous approaches proposed for automatic sentiment\nanalysis for different language contexts. Each language has its own properties\nthat makes the sentiment analysis more challenging. In this regard, this work\npresents a comprehensive survey of existing Arabic sentiment analysis studies,\nand covers the various approaches and techniques proposed in the literature.\nMoreover, we highlight the main difficulties and challenges of Arabic sentiment\nanalysis, and the proposed techniques in literature to overcome these barriers.", "published": "2018-09-08 09:47:01", "link": "http://arxiv.org/abs/1809.02782v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book\n  Question Answering", "abstract": "We present a new kind of question answering dataset, OpenBookQA, modeled\nafter open book exams for assessing human understanding of a subject. The open\nbook that comes with our questions is a set of 1329 elementary level science\nfacts. Roughly 6000 questions probe an understanding of these facts and their\napplication to novel situations. This requires combining an open book fact\n(e.g., metals conduct electricity) with broad common knowledge (e.g., a suit of\narmor is made of metal) obtained from other sources. While existing QA datasets\nover documents or knowledge bases, being generally self-contained, focus on\nlinguistic understanding, OpenBookQA probes a deeper understanding of both the\ntopic---in the context of common knowledge---and the language it is expressed\nin. Human performance on OpenBookQA is close to 92%, but many state-of-the-art\npre-trained QA methods perform surprisingly poorly, worse than several simple\nneural baselines we develop. Our oracle experiments designed to circumvent the\nknowledge retrieval bottleneck demonstrate the value of both the open book and\nadditional facts. We leave it as a challenge to solve the retrieval problem in\nthis multi-hop setting and to close the large gap to human performance.", "published": "2018-09-08 11:47:16", "link": "http://arxiv.org/abs/1809.02789v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Explicit Contextual Semantics for Text Comprehension", "abstract": "Who did what to whom is a major focus in natural language understanding,\nwhich is right the aim of semantic role labeling (SRL) task. Despite of sharing\na lot of processing characteristics and even task purpose, it is surprisingly\nthat jointly considering these two related tasks was never formally reported in\nprevious work. Thus this paper makes the first attempt to let SRL enhance text\ncomprehension and inference through specifying verbal predicates and their\ncorresponding semantic roles. In terms of deep learning models, our embeddings\nare enhanced by explicit contextual semantic role labels for more fine-grained\nsemantics. We show that the salient labels can be conveniently added to\nexisting models and significantly improve deep learning models in challenging\ntext comprehension tasks. Extensive experiments on benchmark machine reading\ncomprehension and inference datasets verify that the proposed semantic learning\nhelps our system reach new state-of-the-art over strong baselines which have\nbeen enhanced by well pretrained language models from the latest progress.", "published": "2018-09-08 12:34:59", "link": "http://arxiv.org/abs/1809.02794v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Attentive Semantic Role Labeling with Boundary Indicator", "abstract": "The goal of semantic role labeling (SRL) is to discover the\npredicate-argument structure of a sentence, which plays a critical role in deep\nprocessing of natural language. This paper introduces simple yet effective\nauxiliary tags for dependency-based SRL to enhance a syntax-agnostic model with\nmulti-hop self-attention. Our syntax-agnostic model achieves competitive\nperformance with state-of-the-art models on the CoNLL-2009 benchmarks both for\nEnglish and Chinese.", "published": "2018-09-08 12:49:18", "link": "http://arxiv.org/abs/1809.02796v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Interpreting Neural Networks With Nearest Neighbors", "abstract": "Local model interpretation methods explain individual predictions by\nassigning an importance value to each input feature. This value is often\ndetermined by measuring the change in confidence when a feature is removed.\nHowever, the confidence of neural networks is not a robust measure of model\nuncertainty. This issue makes reliably judging the importance of the input\nfeatures difficult. We address this by changing the test-time behavior of\nneural networks using Deep k-Nearest Neighbors. Without harming text\nclassification accuracy, this algorithm provides a more robust uncertainty\nmetric which we use to generate feature importance values. The resulting\ninterpretations better align with human perception than baseline methods.\nFinally, we use our interpretation method to analyze model predictions on\ndataset annotation artifacts.", "published": "2018-09-08 18:03:56", "link": "http://arxiv.org/abs/1809.02847v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Operations Guided Neural Networks for High Fidelity Data-To-Text\n  Generation", "abstract": "Recent neural models for data-to-text generation are mostly based on\ndata-driven end-to-end training over encoder-decoder networks. Even though the\ngenerated texts are mostly fluent and informative, they often generate\ndescriptions that are not consistent with the input structured data. This is a\ncritical issue especially in domains that require inference or calculations\nover raw data. In this paper, we attempt to improve the fidelity of neural\ndata-to-text generation by utilizing pre-executed symbolic operations. We\npropose a framework called Operation-guided Attention-based\nsequence-to-sequence network (OpAtt), with a specifically designed gating\nmechanism as well as a quantization module for operation results to utilize\ninformation from pre-executed operations. Experiments on two sports datasets\nshow our proposed method clearly improves the fidelity of the generated texts\nto the input structured data.", "published": "2018-09-08 01:49:03", "link": "http://arxiv.org/abs/1809.02735v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Lower The Simpler: Simplifying Hierarchical Recurrent Models", "abstract": "To improve the training efficiency of hierarchical recurrent models without\ncompromising their performance, we propose a strategy named as `the lower the\nsimpler', which is to simplify the baseline models by making the lower layers\nsimpler than the upper layers. We carry out this strategy to simplify two\ntypical hierarchical recurrent models, namely Hierarchical Recurrent\nEncoder-Decoder (HRED) and R-NET, whose basic building block is GRU.\nSpecifically, we propose Scalar Gated Unit (SGU), which is a simplified variant\nof GRU, and use it to replace the GRUs at the middle layers of HRED and R-NET.\nBesides, we also use Fixed-size Ordinally-Forgetting Encoding (FOFE), which is\nan efficient encoding method without any trainable parameter, to replace the\nGRUs at the bottom layers of HRED and R-NET. The experimental results show that\nthe simplified HRED and the simplified R-NET contain significantly less\ntrainable parameters, consume significantly less training time, and achieve\nslightly better performance than their baseline models.", "published": "2018-09-08 11:54:09", "link": "http://arxiv.org/abs/1809.02790v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Faithful Multimodal Explanation for Visual Question Answering", "abstract": "AI systems' ability to explain their reasoning is critical to their utility\nand trustworthiness. Deep neural networks have enabled significant progress on\nmany challenging problems such as visual question answering (VQA). However,\nmost of them are opaque black boxes with limited explanatory capability. This\npaper presents a novel approach to developing a high-performing VQA system that\ncan elucidate its answers with integrated textual and visual explanations that\nfaithfully reflect important aspects of its underlying reasoning while\ncapturing the style of comprehensible human explanations. Extensive\nexperimental evaluation demonstrates the advantages of this approach compared\nto competing methods with both automatic evaluation metrics and human\nevaluation metrics.", "published": "2018-09-08 14:14:03", "link": "http://arxiv.org/abs/1809.02805v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Extracting and Analyzing Semantic Relatedness between Cities Using News\n  Articles", "abstract": "News articles capture a variety of topics about our society. They reflect not\nonly the socioeconomic activities that happened in our physical world, but also\nsome of the cultures, human interests, and public concerns that exist only in\nthe perceptions of people. Cities are frequently mentioned in news articles,\nand two or more cities may co-occur in the same article. Such co-occurrence\noften suggests certain relatedness between the mentioned cities, and the\nrelatedness may be under different topics depending on the contents of the news\narticles. We consider the relatedness under different topics as semantic\nrelatedness. By reading news articles, one can grasp the general semantic\nrelatedness between cities, yet, given hundreds of thousands of news articles,\nit is very difficult, if not impossible, for anyone to manually read them. This\npaper proposes a computational framework which can \"read\" a large number of\nnews articles and extract the semantic relatedness between cities. This\nframework is based on a natural language processing model and employs a machine\nlearning process to identify the main topics of news articles. We describe the\noverall structure of this framework and its individual modules, and then apply\nit to an experimental dataset with more than 500,000 news articles covering the\ntop 100 U.S. cities spanning a 10-year period. We perform exploratory\nvisualization of the extracted semantic relatedness under different topics and\nover multiple years. We also analyze the impact of geographic distance on\nsemantic relatedness and find varied distance decay effects. The proposed\nframework can be used to support large-scale content analysis in city network\nresearch.", "published": "2018-09-08 15:29:47", "link": "http://arxiv.org/abs/1809.02823v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Exploiting Invertible Decoders for Unsupervised Sentence Representation\n  Learning", "abstract": "The encoder-decoder models for unsupervised sentence representation learning\ntend to discard the decoder after being trained on a large unlabelled corpus,\nsince only the encoder is needed to map the input sentence into a vector\nrepresentation. However, parameters learnt in the decoder also contain useful\ninformation about language. In order to utilise the decoder after learning, we\npresent two types of decoding functions whose inverse can be easily derived\nwithout expensive inverse calculation. Therefore, the inverse of the decoding\nfunction serves as another encoder that produces sentence representations. We\nshow that, with careful design of the decoding functions, the model learns good\nsentence representations, and the ensemble of the representations produced from\nthe encoder and the inverse of the decoder demonstrate even better\ngeneralisation ability and solid transferability.", "published": "2018-09-08 01:20:45", "link": "http://arxiv.org/abs/1809.02731v3", "categories": ["cs.NE", "cs.CL", "cs.LG"], "primary_category": "cs.NE"}
{"title": "Context-Free Transductions with Neural Stacks", "abstract": "This paper analyzes the behavior of stack-augmented recurrent neural network\n(RNN) models. Due to the architectural similarity between stack RNNs and\npushdown transducers, we train stack RNN models on a number of tasks, including\nstring reversal, context-free language modelling, and cumulative XOR\nevaluation. Examining the behavior of our networks, we show that\nstack-augmented RNNs can discover intuitive stack-based strategies for solving\nour tasks. However, stack RNNs are more difficult to train than classical\narchitectures such as LSTMs. Rather than employ stack-based strategies, more\ncomplex networks often find approximate solutions by using the stack as\nunstructured memory.", "published": "2018-09-08 17:04:53", "link": "http://arxiv.org/abs/1809.02836v1", "categories": ["cs.NE", "cs.CL", "cs.LG"], "primary_category": "cs.NE"}
{"title": "Dual-label Deep LSTM Dereverberation For Speaker Verification", "abstract": "In this paper, we present a reverberation removal approach for speaker\nverification, utilizing dual-label deep neural networks (DNNs). The networks\nperform feature mapping between the spectral features of reverberant and clean\nspeech. Long short term memory recurrent neural networks (LSTMs) are trained to\nmap corrupted Mel filterbank (MFB) features to two sets of labels: i) the clean\nMFB features, and ii) either estimated pitch tracks or the fast Fourier\ntransform (FFT) spectrogram of clean speech. The performance of reverberation\nremoval is evaluated by equal error rates (EERs) of speaker verification\nexperiments.", "published": "2018-09-08 04:55:24", "link": "http://arxiv.org/abs/1809.03868v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
