{"title": "A Manually Annotated Chinese Corpus for Non-task-oriented Dialogue\n  Systems", "abstract": "This paper presents a large-scale corpus for non-task-oriented dialogue\nresponse selection, which contains over 27K distinct prompts more than 82K\nresponses collected from social media. To annotate this corpus, we define a\n5-grade rating scheme: bad, mediocre, acceptable, good, and excellent,\naccording to the relevance, coherence, informativeness, interestingness, and\nthe potential to move a conversation forward. To test the validity and\nusefulness of the produced corpus, we compare various unsupervised and\nsupervised models for response selection. Experimental results confirm that the\nproposed corpus is helpful in training response selection models.", "published": "2018-05-15 03:06:04", "link": "http://arxiv.org/abs/1805.05542v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Simplifying Sentences with Sequence to Sequence Models", "abstract": "We simplify sentences with an attentive neural network sequence to sequence\nmodel, dubbed S4. The model includes a novel word-copy mechanism and loss\nfunction to exploit linguistic similarities between the original and simplified\nsentences. It also jointly uses pre-trained and fine-tuned word embeddings to\ncapture the semantics of complex sentences and to mitigate the effects of\nlimited data. When trained and evaluated on pairs of sentences from thousands\nof news articles, we observe a 8.8 point improvement in BLEU score over a\nsequence to sequence baseline; however, learning word substitutions remains\ndifficult. Such sequence to sequence models are promising for other text\ngeneration tasks such as style transfer.", "published": "2018-05-15 04:49:55", "link": "http://arxiv.org/abs/1805.05557v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Learning of Style-sensitive Word Vectors", "abstract": "This paper presents the first study aimed at capturing stylistic similarity\nbetween words in an unsupervised manner. We propose extending the continuous\nbag of words (CBOW) model (Mikolov et al., 2013) to learn style-sensitive word\nvectors using a wider context window under the assumption that the style of all\nthe words in an utterance is consistent. In addition, we introduce a novel task\nto predict lexical stylistic similarity and to create a benchmark dataset for\nthis task. Our experiment with this dataset supports our assumption and\ndemonstrates that the proposed extensions contribute to the acquisition of\nstyle-sensitive word embeddings.", "published": "2018-05-15 06:19:12", "link": "http://arxiv.org/abs/1805.05581v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Marrying up Regular Expressions with Neural Networks: A Case Study for\n  Spoken Language Understanding", "abstract": "The success of many natural language processing (NLP) tasks is bound by the\nnumber and quality of annotated data, but there is often a shortage of such\ntraining data. In this paper, we ask the question: \"Can we combine a neural\nnetwork (NN) with regular expressions (RE) to improve supervised learning for\nNLP?\". In answer, we develop novel methods to exploit the rich expressiveness\nof REs at different levels within a NN, showing that the combination\nsignificantly enhances the learning effectiveness when a small number of\ntraining examples are available. We evaluate our approach by applying it to\nspoken language understanding for intent detection and slot filling.\nExperimental results show that our approach is highly effective in exploiting\nthe available training data, giving a clear boost to the RE-unaware NN.", "published": "2018-05-15 06:40:44", "link": "http://arxiv.org/abs/1805.05588v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Drug-Drug Interaction Extraction from Texts by Molecular\n  Structure Information", "abstract": "We propose a novel neural method to extract drug-drug interactions (DDIs)\nfrom texts using external drug molecular structure information. We encode\ntextual drug pairs with convolutional neural networks and their molecular pairs\nwith graph convolutional networks (GCNs), and then we concatenate the outputs\nof these two networks. In the experiments, we show that GCNs can predict DDIs\nfrom the molecular structures of drugs in high accuracy and the molecular\ninformation can enhance text-based DDI extraction by 2.39 percent points in the\nF-score on the DDIExtraction 2013 shared task data set.", "published": "2018-05-15 07:03:21", "link": "http://arxiv.org/abs/1805.05593v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating Continuous Representations of Medical Texts", "abstract": "We present an architecture that generates medical texts while learning an\ninformative, continuous representation with discriminative features. During\ntraining the input to the system is a dataset of captions for medical X-Rays.\nThe acquired continuous representations are of particular interest for use in\nmany machine learning techniques where the discrete and high-dimensional nature\nof textual input is an obstacle. We use an Adversarially Regularized\nAutoencoder to create realistic text in both an unconditional and conditional\nsetting. We show that this technique is applicable to medical texts which often\ncontain syntactic and domain-specific shorthands. A quantitative evaluation\nshows that we achieve a lower model perplexity than a traditional LSTM\ngenerator.", "published": "2018-05-15 10:32:53", "link": "http://arxiv.org/abs/1805.05691v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Continuous Learning in a Hierarchical Multiscale Neural Network", "abstract": "We reformulate the problem of encoding a multi-scale representation of a\nsequence in a language model by casting it in a continuous learning framework.\nWe propose a hierarchical multi-scale language model in which short time-scale\ndependencies are encoded in the hidden state of a lower-level recurrent neural\nnetwork while longer time-scale dependencies are encoded in the dynamic of the\nlower-level network by having a meta-learner update the weights of the\nlower-level neural network in an online meta-learning fashion. We use elastic\nweights consolidation as a higher-level to prevent catastrophic forgetting in\nour continuous learning framework.", "published": "2018-05-15 13:37:33", "link": "http://arxiv.org/abs/1805.05758v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CLINIQA: A Machine Intelligence Based Clinical Question Answering System", "abstract": "The recent developments in the field of biomedicine have made large volumes\nof biomedical literature available to the medical practitioners. Due to the\nlarge size and lack of efficient searching strategies, medical practitioners\nstruggle to obtain necessary information available in the biomedical\nliterature. Moreover, the most sophisticated search engines of age are not\nintelligent enough to interpret the clinicians' questions. These facts reflect\nthe urgent need of an information retrieval system that accepts the queries\nfrom medical practitioners' in natural language and returns the answers quickly\nand efficiently. In this paper, we present an implementation of a machine\nintelligence based CLINIcal Question Answering system (CLINIQA) to answer\nmedical practitioner's questions. The system was rigorously evaluated on\ndifferent text mining algorithms and the best components for the system were\nselected. The system makes use of Unified Medical Language System for semantic\nanalysis of both questions and medical documents. In addition, the system\nemploys supervised machine learning algorithms for classification of the\ndocuments, identifying the focus of the question and answer selection.\nEffective domain-specific heuristics are designed for answer ranking. The\nperformance evaluation on hundred clinical questions shows the effectiveness of\nour approach.", "published": "2018-05-15 17:45:25", "link": "http://arxiv.org/abs/1805.05927v1", "categories": ["cs.CL", "68T50"], "primary_category": "cs.CL"}
{"title": "Harvesting Paragraph-Level Question-Answer Pairs from Wikipedia", "abstract": "We study the task of generating from Wikipedia articles question-answer pairs\nthat cover content beyond a single sentence. We propose a neural network\napproach that incorporates coreference knowledge via a novel gating mechanism.\nCompared to models that only take into account sentence-level information\n(Heilman and Smith, 2010; Du et al., 2017; Zhou et al., 2017), we find that the\nlinguistic knowledge introduced by the coreference representation aids question\ngeneration significantly, producing models that outperform the current\nstate-of-the-art. We apply our system (composed of an answer span extraction\nsystem and the passage-level QG system) to the 10,000 top-ranking Wikipedia\narticles and create a corpus of over one million question-answer pairs. We also\nprovide a qualitative analysis for this large-scale generated corpus from\nWikipedia.", "published": "2018-05-15 17:58:25", "link": "http://arxiv.org/abs/1805.05942v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Author Commitment and Social Power: Automatic Belief Tagging to Infer\n  the Social Context of Interactions", "abstract": "Understanding how social power structures affect the way we interact with one\nanother is of great interest to social scientists who want to answer\nfundamental questions about human behavior, as well as to computer scientists\nwho want to build automatic methods to infer the social contexts of\ninteractions. In this paper, we employ advancements in extra-propositional\nsemantics extraction within NLP to study how author commitment reflects the\nsocial context of an interaction. Specifically, we investigate whether the\nlevel of commitment expressed by individuals in an organizational interaction\nreflects the hierarchical power structures they are part of. We find that\nsubordinates use significantly more instances of non-commitment than superiors.\nMore importantly, we also find that subordinates attribute propositions to\nother agents more often than superiors do --- an aspect that has not been\nstudied before. Finally, we show that enriching lexical features with\ncommitment labels captures important distinctions in social meanings.", "published": "2018-05-15 20:05:58", "link": "http://arxiv.org/abs/1805.06016v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Paper Abstract Writing through Editing Mechanism", "abstract": "We present a paper abstract writing system based on an attentive neural\nsequence-to-sequence model that can take a title as input and automatically\ngenerate an abstract. We design a novel Writing-editing Network that can attend\nto both the title and the previously generated abstract drafts and then\niteratively revise and polish the abstract. With two series of Turing tests,\nwhere the human judges are asked to distinguish the system-generated abstracts\nfrom human-written ones, our system passes Turing tests by junior domain\nexperts at a rate up to 30% and by non-expert at a rate up to 80%.", "published": "2018-05-15 23:13:23", "link": "http://arxiv.org/abs/1805.06064v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improved ASR for Under-Resourced Languages Through Multi-Task Learning\n  with Acoustic Landmarks", "abstract": "Furui first demonstrated that the identity of both consonant and vowel can be\nperceived from the C-V transition; later, Stevens proposed that acoustic\nlandmarks are the primary cues for speech perception, and that steady-state\nregions are secondary or supplemental. Acoustic landmarks are perceptually\nsalient, even in a language one doesn't speak, and it has been demonstrated\nthat non-speakers of the language can identify features such as the primary\narticulator of the landmark. These factors suggest a strategy for developing\nlanguage-independent automatic speech recognition: landmarks can potentially be\nlearned once from a suitably labeled corpus and rapidly applied to many other\nlanguages. This paper proposes enhancing the cross-lingual portability of a\nneural network by using landmarks as the secondary task in multi-task learning\n(MTL). The network is trained in a well-resourced source language with both\nphone and landmark labels (English), then adapted to an under-resourced target\nlanguage with only word labels (Iban). Landmark-tasked MTL reduces\nsource-language phone error rate by 2.9% relative, and reduces target-language\nword error rate by 1.9%-5.9% depending on the amount of target-language\ntraining data. These results suggest that landmark-tasked MTL causes the DNN to\nlearn hidden-node features that are useful for cross-lingual adaptation.", "published": "2018-05-15 05:46:23", "link": "http://arxiv.org/abs/1805.05574v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Stories for Images-in-Sequence by using Visual and Narrative Components", "abstract": "Recent research in AI is focusing towards generating narrative stories about\nvisual scenes. It has the potential to achieve more human-like understanding\nthan just basic description generation of images- in-sequence. In this work, we\npropose a solution for generating stories for images-in-sequence that is based\non the Sequence to Sequence model. As a novelty, our encoder model is composed\nof two separate encoders, one that models the behaviour of the image sequence\nand other that models the sentence-story generated for the previous image in\nthe sequence of images. By using the image sequence encoder we capture the\ntemporal dependencies between the image sequence and the sentence-story and by\nusing the previous sentence-story encoder we achieve a better story flow. Our\nsolution generates long human-like stories that not only describe the visual\ncontext of the image sequence but also contains narrative and evaluative\nlanguage. The obtained results were confirmed by manual human evaluation.", "published": "2018-05-15 08:15:40", "link": "http://arxiv.org/abs/1805.05622v3", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "Complexity Reduction in the Negotiation of New Lexical Conventions", "abstract": "In the process of collectively inventing new words for new concepts in a\npopulation, conflicts can quickly become numerous, in the form of synonymy and\nhomonymy. Remembering all of them could cost too much memory, and remembering\ntoo few may slow down the overall process. Is there an efficient behavior that\ncould help balance the two? The Naming Game is a multi-agent computational\nmodel for the emergence of language, focusing on the negotiation of new lexical\nconventions, where a common lexicon self-organizes but going through a phase of\nhigh complexity. Previous work has been done on the control of complexity\ngrowth in this particular model, by allowing agents to actively choose what\nthey talk about. However, those strategies were relying on ad hoc heuristics\nhighly dependent on fine-tuning of parameters. We define here a new principled\nmeasure and a new strategy, based on the beliefs of each agent on the global\nstate of the population. The measure does not rely on heavy computation, and is\ncognitively plausible. The new strategy yields an efficient control of\ncomplexity growth, along with a faster agreement process. Also, we show that\nshort-term memory is enough to build relevant beliefs about the global lexicon.", "published": "2018-05-15 08:23:56", "link": "http://arxiv.org/abs/1805.05631v2", "categories": ["cs.MA", "cs.CL", "cs.SI"], "primary_category": "cs.MA"}
{"title": "A Purely End-to-end System for Multi-speaker Speech Recognition", "abstract": "Recently, there has been growing interest in multi-speaker speech\nrecognition, where the utterances of multiple speakers are recognized from\ntheir mixture. Promising techniques have been proposed for this task, but\nearlier works have required additional training data such as isolated source\nsignals or senone alignments for effective learning. In this paper, we propose\na new sequence-to-sequence framework to directly decode multiple label\nsequences from a single speech sequence by unifying source separation and\nspeech recognition functions in an end-to-end manner. We further propose a new\nobjective function to improve the contrast between the hidden vectors to avoid\ngenerating similar hypotheses. Experimental results show that the model is\ndirectly able to learn a mapping from a speech mixture to multiple label\nsequences, achieving 83.1 % relative improvement compared to a model trained\nwithout the proposed objective. Interestingly, the results are comparable to\nthose produced by previous end-to-end works featuring explicit separation and\nrecognition modules.", "published": "2018-05-15 14:45:33", "link": "http://arxiv.org/abs/1805.05826v1", "categories": ["cs.SD", "cs.CL", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "SoPa: Bridging CNNs, RNNs, and Weighted Finite-State Machines", "abstract": "Recurrent and convolutional neural networks comprise two distinct families of\nmodels that have proven to be useful for encoding natural language utterances.\nIn this paper we present SoPa, a new model that aims to bridge these two\napproaches. SoPa combines neural representation learning with weighted\nfinite-state automata (WFSAs) to learn a soft version of traditional surface\npatterns. We show that SoPa is an extension of a one-layer CNN, and that such\nCNNs are equivalent to a restricted version of SoPa, and accordingly, to a\nrestricted form of WFSA. Empirically, on three text classification tasks, SoPa\nis comparable or better than both a BiLSTM (RNN) baseline and a CNN baseline,\nand is particularly useful in small data settings.", "published": "2018-05-15 23:03:01", "link": "http://arxiv.org/abs/1805.06061v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Corpus Conversion Service: A machine learning platform to ingest\n  documents at scale [Poster abstract]", "abstract": "Over the past few decades, the amount of scientific articles and technical\nliterature has increased exponentially in size. Consequently, there is a great\nneed for systems that can ingest these documents at scale and make their\ncontent discoverable. Unfortunately, both the format of these documents (e.g.\nthe PDF format or bitmap images) as well as the presentation of the data (e.g.\ncomplex tables) make the extraction of qualitative and quantitive data\nextremely challenging. We present a platform to ingest documents at scale which\nis powered by Machine Learning techniques and allows the user to train custom\nmodels on document collections. We show precision/recall results greater than\n97% with regard to conversion to structured formats, as well as scaling\nevidence for each of the microservices constituting the platform.", "published": "2018-05-15 07:05:52", "link": "http://arxiv.org/abs/1805.09687v1", "categories": ["cs.DL", "cs.CL", "cs.CV", "cs.DC", "cs.IR"], "primary_category": "cs.DL"}
