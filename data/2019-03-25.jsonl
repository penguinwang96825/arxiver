{"title": "Argument Mining for Understanding Peer Reviews", "abstract": "Peer-review plays a critical role in the scientific writing and publication\necosystem. To assess the efficiency and efficacy of the reviewing process, one\nessential element is to understand and evaluate the reviews themselves. In this\nwork, we study the content and structure of peer reviews under the argument\nmining framework, through automatically detecting (1) argumentative\npropositions put forward by reviewers, and (2) their types (e.g., evaluating\nthe work or making suggestions for improvement). We first collect 14.2K reviews\nfrom major machine learning and natural language processing venues. 400 reviews\nare annotated with 10,386 propositions and corresponding types of Evaluation,\nRequest, Fact, Reference, or Quote. We then train state-of-the-art proposition\nsegmentation and classification models on the data to evaluate their utilities\nand identify new challenges for this new domain, motivating future directions\nfor argument mining. Further experiments show that proposition usage varies\nacross venues in amount, type, and topic.", "published": "2019-03-25 02:26:54", "link": "http://arxiv.org/abs/1903.10104v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Connecting Language and Knowledge with Heterogeneous Representations for\n  Neural Relation Extraction", "abstract": "Knowledge Bases (KBs) require constant up-dating to reflect changes to the\nworld they represent. For general purpose KBs, this is often done through\nRelation Extraction (RE), the task of predicting KB relations expressed in text\nmentioning entities known to the KB. One way to improve RE is to use KB\nEmbeddings (KBE) for link prediction. However, despite clear connections\nbetween RE and KBE, little has been done toward properly unifying these models\nsystematically. We help close the gap with a framework that unifies the\nlearning of RE and KBE models leading to significant improvements over the\nstate-of-the-art in RE. The code is available at\nhttps://github.com/billy-inn/HRERE.", "published": "2019-03-25 04:09:59", "link": "http://arxiv.org/abs/1903.10126v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Aligning Vector-spaces with Noisy Supervised Lexicons", "abstract": "The problem of learning to translate between two vector spaces given a set of\naligned points arises in several application areas of NLP. Current solutions\nassume that the lexicon which defines the alignment pairs is noise-free. We\nconsider the case where the set of aligned points is allowed to contain an\namount of noise, in the form of incorrect lexicon pairs and show that this\narises in practice by analyzing the edited dictionaries after the cleaning\nprocess. We demonstrate that such noise substantially degrades the accuracy of\nthe learned translation when using current methods. We propose a model that\naccounts for noisy pairs. This is achieved by introducing a generative model\nwith a compatible iterative EM algorithm. The algorithm jointly learns the\nnoise level in the lexicon, finds the set of noisy pairs, and learns the\nmapping between the spaces. We demonstrate the effectiveness of our proposed\nalgorithm on two alignment problems: bilingual word embedding translation, and\nmapping between diachronic embedding spaces for recovering the semantic shifts\nof words across time periods.", "published": "2019-03-25 11:00:20", "link": "http://arxiv.org/abs/1903.10238v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-tune BERT for Extractive Summarization", "abstract": "BERT, a pre-trained Transformer model, has achieved ground-breaking\nperformance on multiple NLP tasks. In this paper, we describe BERTSUM, a simple\nvariant of BERT, for extractive summarization. Our system is the state of the\nart on the CNN/Dailymail dataset, outperforming the previous best-performed\nsystem by 1.65 on ROUGE-L. The codes to reproduce our results are available at\nhttps://github.com/nlpyang/BertSum", "published": "2019-03-25 13:42:45", "link": "http://arxiv.org/abs/1903.10318v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Recognizing Arrow Of Time In The Short Stories", "abstract": "Recognizing arrow of time in short stories is a challenging task. i.e., given\nonly two paragraphs, determining which comes first and which comes next is a\ndifficult task even for humans. In this paper, we have collected and curated a\nnovel dataset for tackling this challenging task. We have shown that a\npre-trained BERT architecture achieves reasonable accuracy on the task, and\noutperforms RNN-based architectures.", "published": "2019-03-25 18:45:29", "link": "http://arxiv.org/abs/1903.10548v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Grammatical Error Correction with Finite State Transducers", "abstract": "Grammatical error correction (GEC) is one of the areas in natural language\nprocessing in which purely neural models have not yet superseded more\ntraditional symbolic models. Hybrid systems combining phrase-based statistical\nmachine translation (SMT) and neural sequence models are currently among the\nmost effective approaches to GEC. However, both SMT and neural\nsequence-to-sequence models require large amounts of annotated data. Language\nmodel based GEC (LM-GEC) is a promising alternative which does not rely on\nannotated training data. We show how to improve LM-GEC by applying modelling\ntechniques based on finite state transducers. We report further gains by\nrescoring with neural language models. We show that our methods developed for\nLM-GEC can also be used with SMT systems if annotated training data is\navailable. Our best system outperforms the best published result on the\nCoNLL-2014 test set, and achieves far better relative improvements over the SMT\nbaselines than previous hybrid systems.", "published": "2019-03-25 23:05:11", "link": "http://arxiv.org/abs/1903.10625v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "End-to-End Learning Using Cycle Consistency for Image-to-Caption\n  Transformations", "abstract": "So far, research to generate captions from images has been carried out from\nthe viewpoint that a caption holds sufficient information for an image. If it\nis possible to generate an image that is close to the input image from a\ngenerated caption, i.e., if it is possible to generate a natural language\ncaption containing sufficient information to reproduce the image, then the\ncaption is considered to be faithful to the image. To make such regeneration\npossible, learning using the cycle-consistency loss is effective. In this\nstudy, we propose a method of generating captions by learning end-to-end mutual\ntransformations between images and texts. To evaluate our method, we perform\ncomparative experiments with and without the cycle consistency. The results are\nevaluated by an automatic evaluation and crowdsourcing, demonstrating that our\nproposed method is effective.", "published": "2019-03-25 03:40:15", "link": "http://arxiv.org/abs/1903.10118v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Anxious Depression Prediction in Real-time Social Data", "abstract": "Mental well-being and social media have been closely related domains of\nstudy. In this research a novel model, AD prediction model, for anxious\ndepression prediction in real-time tweets is proposed. This mixed\nanxiety-depressive disorder is a predominantly associated with erratic thought\nprocess, restlessness and sleeplessness. Based on the linguistic cues and user\nposting patterns, the feature set is defined using a 5-tuple vector <word,\ntiming, frequency, sentiment, contrast>. An anxiety-related lexicon is built to\ndetect the presence of anxiety indicators. Time and frequency of tweet is\nanalyzed for irregularities and opinion polarity analytics is done to find\ninconsistencies in posting behaviour. The model is trained using three\nclassifiers (multinomial na\\\"ive bayes, gradient boosting, and random forest)\nand majority voting using an ensemble voting classifier is done. Preliminary\nresults are evaluated for tweets of sampled 100 users and the proposed model\nachieves a classification accuracy of 85.09%.", "published": "2019-03-25 10:21:43", "link": "http://arxiv.org/abs/1903.10222v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Knowledge Aware Conversation Generation with Explainable Reasoning over\n  Augmented Graphs", "abstract": "Two types of knowledge, triples from knowledge graphs and texts from\ndocuments, have been studied for knowledge aware open-domain conversation\ngeneration, in which graph paths can narrow down vertex candidates for\nknowledge selection decision, and texts can provide rich information for\nresponse generation. Fusion of a knowledge graph and texts might yield mutually\nreinforcing advantages, but there is less study on that. To address this\nchallenge, we propose a knowledge aware chatting machine with three components,\nan augmented knowledge graph with both triples and texts, knowledge selector,\nand knowledge aware response generator. For knowledge selection on the graph,\nwe formulate it as a problem of multi-hop graph reasoning to effectively\ncapture conversation flow, which is more explainable and flexible in comparison\nwith previous work. To fully leverage long text information that differentiates\nour graph from others, we improve a state of the art reasoning algorithm with\nmachine reading comprehension technology. We demonstrate the effectiveness of\nour system on two datasets in comparison with state-of-the-art models.", "published": "2019-03-25 11:23:17", "link": "http://arxiv.org/abs/1903.10245v4", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "dpUGC: Learn Differentially Private Representation for User Generated\n  Contents", "abstract": "This paper firstly proposes a simple yet efficient generalized approach to\napply differential privacy to text representation (i.e., word embedding). Based\non it, we propose a user-level approach to learn personalized differentially\nprivate word embedding model on user generated contents (UGC). To our best\nknowledge, this is the first work of learning user-level differentially private\nword embedding model from text for sharing. The proposed approaches protect the\nprivacy of the individual from re-identification, especially provide better\ntrade-off of privacy and data utility on UGC data for sharing. The experimental\nresults show that the trained embedding models are applicable for the classic\ntext analysis tasks (e.g., regression). Moreover, the proposed approaches of\nlearning differentially private embedding models are both framework- and data-\nindependent, which facilitates the deployment and sharing. The source code is\navailable at https://github.com/sonvx/dpText.", "published": "2019-03-25 16:41:20", "link": "http://arxiv.org/abs/1903.10453v1", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "On Measuring Social Biases in Sentence Encoders", "abstract": "The Word Embedding Association Test shows that GloVe and word2vec word\nembeddings exhibit human-like implicit biases based on gender, race, and other\nsocial constructs (Caliskan et al., 2017). Meanwhile, research on learning\nreusable text representations has begun to explore sentence-level texts, with\nsome sentence encoders seeing enthusiastic adoption. Accordingly, we extend the\nWord Embedding Association Test to measure bias in sentence encoders. We then\ntest several sentence encoders, including state-of-the-art methods such as ELMo\nand BERT, for the social biases studied in prior work and two important biases\nthat are difficult or impossible to test at the word level. We observe mixed\nresults including suspicious patterns of sensitivity that suggest the test's\nassumptions may not hold in general. We conclude by proposing directions for\nfuture work on measuring bias in sentence encoders.", "published": "2019-03-25 19:30:21", "link": "http://arxiv.org/abs/1903.10561v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Question Embeddings Based on Shannon Entropy: Solving intent\n  classification task in goal-oriented dialogue system", "abstract": "Question-answering systems and voice assistants are becoming major part of\nclient service departments of many organizations, helping them to reduce the\nlabor costs of staff. In many such systems, there is always natural language\nunderstanding module that solves intent classification task. This task is\ncomplicated because of its case-dependency - every subject area has its own\nsemantic kernel. The state of art approaches for intent classification are\ndifferent machine learning and deep learning methods that use text vector\nrepresentations as input. The basic vector representation models such as Bag of\nwords and TF-IDF generate sparse matrixes, which are becoming very big as the\namount of input data grows. Modern methods such as word2vec and FastText use\nneural networks to evaluate word embeddings with fixed dimension size. As we\nare developing a question-answering system for students and enrollees of the\nPerm National Research Polytechnic University, we have faced the problem of\nuser's intent detection. The subject area of our system is very specific, that\nis why there is a lack of training data. This aspect makes intent\nclassification task more challenging for using state of the art deep learning\nmethods. In this paper, we propose an approach of the questions embeddings\nrepresentation based on calculation of Shannon entropy.The goal of the approach\nis to produce low dimensional question vectors as neural approaches do and to\noutperform related methods, described above in condition of small dataset. We\nevaluate and compare our model with existing ones using logistic regression and\ndataset that contains questions asked by students and enrollees. The data is\nlabeled into six classes. Experimental comparison of proposed approach and\nother models revealed that proposed model performed better in the given task.", "published": "2019-03-25 18:59:32", "link": "http://arxiv.org/abs/1904.00785v1", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Computational and Robotic Models of Early Language Development: A Review", "abstract": "We review computational and robotics models of early language learning and\ndevelopment. We first explain why and how these models are used to understand\nbetter how children learn language. We argue that they provide concrete\ntheories of language learning as a complex dynamic system, complementing\ntraditional methods in psychology and linguistics. We review different modeling\nformalisms, grounded in techniques from machine learning and artificial\nintelligence such as Bayesian and neural network approaches. We then discuss\ntheir role in understanding several key mechanisms of language development:\ncross-situational statistical learning, embodiment, situated social\ninteraction, intrinsically motivated learning, and cultural evolution. We\nconclude by discussing future challenges for research, including modeling of\nlarge-scale empirical data about language acquisition in real-world\nenvironments.\n  Keywords: Early language learning, Computational and robotic models, machine\nlearning, development, embodiment, social interaction, intrinsic motivation,\nself-organization, dynamical systems, complexity.", "published": "2019-03-25 11:28:36", "link": "http://arxiv.org/abs/1903.10246v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A linear bound on the k-rendezvous time for primitive sets of NZ\n  matrices", "abstract": "A set of nonnegative matrices is called primitive if there exists a product\nof these matrices that is entrywise positive. Motivated by recent results\nrelating synchronizing automata and primitive sets, we study the length of the\nshortest product of a primitive set having a column or a row with k positive\nentries, called its k-rendezvous time (k-RT}), in the case of sets of matrices\nhaving no zero rows and no zero columns. We prove that the k-RT is at most\nlinear w.r.t. the matrix size n for small k, while the problem is still open\nfor synchronizing automata. We provide two upper bounds on the k-RT: the second\nis an improvement of the first one, although the latter can be written in\nclosed form. We then report numerical results comparing our upper bounds on the\nk-RT with heuristic approximation methods.", "published": "2019-03-25 16:05:47", "link": "http://arxiv.org/abs/1903.10421v3", "categories": ["cs.DM", "cs.CL", "math.CO", "05C50, 15B36, 68R05", "G.2.1"], "primary_category": "cs.DM"}
{"title": "Diversifying Reply Suggestions using a Matching-Conditional Variational\n  Autoencoder", "abstract": "We consider the problem of diversifying automated reply suggestions for a\ncommercial instant-messaging (IM) system (Skype). Our conversation model is a\nstandard matching based information retrieval architecture, which consists of\ntwo parallel encoders to project messages and replies into a common feature\nrepresentation. During inference, we select replies from a fixed response set\nusing nearest neighbors in the feature space. To diversify responses, we\nformulate the model as a generative latent variable model with Conditional\nVariational Auto-Encoder (M-CVAE). We propose a constrained-sampling approach\nto make the variational inference in M-CVAE efficient for our production\nsystem. In offline experiments, M-CVAE consistently increased diversity by\n~30-40% without significant impact on relevance. This translated to a 5% gain\nin click-rate in our online production system.", "published": "2019-03-25 23:12:56", "link": "http://arxiv.org/abs/1903.10630v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "A Survey of Code-switched Speech and Language Processing", "abstract": "Code-switching, the alternation of languages within a conversation or\nutterance, is a common communicative phenomenon that occurs in multilingual\ncommunities across the world. This survey reviews computational approaches for\ncode-switched Speech and Natural Language Processing. We motivate why\nprocessing code-switched text and speech is essential for building intelligent\nagents and systems that interact with users in multilingual communities. As\ncode-switching data and resources are scarce, we list what is available in\nvarious code-switched language pairs with the language processing tasks they\ncan be used for. We review code-switching research in various Speech and NLP\napplications, including language processing tools and end-to-end systems. We\nconclude with future directions and open problems in the field.", "published": "2019-03-25 14:36:50", "link": "http://arxiv.org/abs/1904.00784v3", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Cyclical Annealing Schedule: A Simple Approach to Mitigating KL\n  Vanishing", "abstract": "Variational autoencoders (VAEs) with an auto-regressive decoder have been\napplied for many natural language processing (NLP) tasks. The VAE objective\nconsists of two terms, (i) reconstruction and (ii) KL regularization, balanced\nby a weighting hyper-parameter \\beta. One notorious training difficulty is that\nthe KL term tends to vanish. In this paper we study scheduling schemes for\n\\beta, and show that KL vanishing is caused by the lack of good latent codes in\ntraining the decoder at the beginning of optimization. To remedy this, we\npropose a cyclical annealing schedule, which repeats the process of increasing\n\\beta multiple times. This new procedure allows the progressive learning of\nmore meaningful latent codes, by leveraging the informative representations of\nprevious cycles as warm re-starts. The effectiveness of cyclical annealing is\nvalidated on a broad range of NLP tasks, including language modeling, dialog\nresponse generation and unsupervised language pre-training.", "published": "2019-03-25 06:28:24", "link": "http://arxiv.org/abs/1903.10145v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Learning Embodied Semantics via Music and Dance Semiotic Correlations", "abstract": "Music semantics is embodied, in the sense that meaning is biologically\nmediated by and grounded in the human body and brain. This embodied cognition\nperspective also explains why music structures modulate kinetic and\nsomatosensory perception. We leverage this aspect of cognition, by considering\ndance as a proxy for music perception, in a statistical computational model\nthat learns semiotic correlations between music audio and dance video. We\nevaluate the ability of this model to effectively capture underlying semantics\nin a cross-modal retrieval task. Quantitative results, validated with\nstatistical significance testing, strengthen the body of evidence for embodied\ncognition in music and show the model can recommend music audio for dance video\nqueries and vice-versa.", "published": "2019-03-25 18:09:03", "link": "http://arxiv.org/abs/1903.10534v1", "categories": ["cs.CV", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
