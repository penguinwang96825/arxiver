{"title": "Unsupervised Multi-document Summarization with Holistic Inference", "abstract": "Multi-document summarization aims to obtain core information from a\ncollection of documents written on the same topic. This paper proposes a new\nholistic framework for unsupervised multi-document extractive summarization.\nOur method incorporates the holistic beam search inference method associated\nwith the holistic measurements, named Subset Representative Index (SRI). SRI\nbalances the importance and diversity of a subset of sentences from the source\ndocuments and can be calculated in unsupervised and adaptive manners. To\ndemonstrate the effectiveness of our method, we conduct extensive experiments\non both small and large-scale multi-document summarization datasets under both\nunsupervised and adaptive settings. The proposed method outperforms strong\nbaselines by a significant margin, as indicated by the resulting ROUGE scores\nand diversity measures. Our findings also suggest that diversity is essential\nfor improving multi-document summary performance.", "published": "2023-09-08 02:56:30", "link": "http://arxiv.org/abs/2309.04087v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RST-style Discourse Parsing Guided by Document-level Content Structures", "abstract": "Rhetorical Structure Theory based Discourse Parsing (RST-DP) explores how\nclauses, sentences, and large text spans compose a whole discourse and presents\nthe rhetorical structure as a hierarchical tree. Existing RST parsing pipelines\nconstruct rhetorical structures without the knowledge of document-level content\nstructures, which causes relatively low performance when predicting the\ndiscourse relations for large text spans. Recognizing the value of high-level\ncontent-related information in facilitating discourse relation recognition, we\npropose a novel pipeline for RST-DP that incorporates structure-aware news\ncontent sentence representations derived from the task of News Discourse\nProfiling. By incorporating only a few additional layers, this enhanced\npipeline exhibits promising performance across various RST parsing metrics.", "published": "2023-09-08 05:50:27", "link": "http://arxiv.org/abs/2309.04141v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GLS-CSC: A Simple but Effective Strategy to Mitigate Chinese STM Models'\n  Over-Reliance on Superficial Clue", "abstract": "Pre-trained models have achieved success in Chinese Short Text Matching (STM)\ntasks, but they often rely on superficial clues, leading to a lack of robust\npredictions. To address this issue, it is crucial to analyze and mitigate the\ninfluence of superficial clues on STM models. Our study aims to investigate\ntheir over-reliance on the edit distance feature, commonly used to measure the\nsemantic similarity of Chinese text pairs, which can be considered a\nsuperficial clue. To mitigate STM models' over-reliance on superficial clues,\nwe propose a novel resampling training strategy called Gradually Learn Samples\nContaining Superficial Clue (GLS-CSC). Through comprehensive evaluations of\nIn-Domain (I.D.), Robustness (Rob.), and Out-Of-Domain (O.O.D.) test sets, we\ndemonstrate that GLS-CSC outperforms existing methods in terms of enhancing the\nrobustness and generalization of Chinese STM models. Moreover, we conduct a\ndetailed analysis of existing methods and reveal their commonality.", "published": "2023-09-08 07:10:57", "link": "http://arxiv.org/abs/2309.04162v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Don't Ignore Dual Logic Ability of LLMs while Privatizing: A\n  Data-Intensive Analysis in Medical Domain", "abstract": "Extensive studies have been devoted to privatizing general-domain Large\nLanguage Models (LLMs) as Domain-Specific LLMs via feeding specific-domain\ndata. However, these privatization efforts often ignored a critical aspect:\nDual Logic Ability, which is a core reasoning ability for LLMs. The dual logic\nability of LLMs ensures that they can maintain a consistent stance when\nconfronted with both positive and negative statements about the same fact. Our\nstudy focuses on how the dual logic ability of LLMs is affected during the\nprivatization process in the medical domain. We conduct several experiments to\nanalyze the dual logic ability of LLMs by examining the consistency of the\nstance in responses to paired questions about the same fact. In our\nexperiments, interestingly, we observed a significant decrease in the dual\nlogic ability of existing LLMs after privatization. Besides, our results\nindicate that incorporating general domain dual logic data into LLMs not only\nenhances LLMs' dual logic ability but also further improves their accuracy.\nThese findings underscore the importance of prioritizing LLMs' dual logic\nability during the privatization process. Our study establishes a benchmark for\nfuture research aimed at exploring LLMs' dual logic ability during the\nprivatization process and offers valuable guidance for privatization efforts in\nreal-world applications.", "published": "2023-09-08 08:20:46", "link": "http://arxiv.org/abs/2309.04198v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Sparse to Dense: GPT-4 Summarization with Chain of Density\n  Prompting", "abstract": "Selecting the ``right'' amount of information to include in a summary is a\ndifficult task. A good summary should be detailed and entity-centric without\nbeing overly dense and hard to follow. To better understand this tradeoff, we\nsolicit increasingly dense GPT-4 summaries with what we refer to as a ``Chain\nof Density'' (CoD) prompt. Specifically, GPT-4 generates an initial\nentity-sparse summary before iteratively incorporating missing salient entities\nwithout increasing the length. Summaries generated by CoD are more abstractive,\nexhibit more fusion, and have less of a lead bias than GPT-4 summaries\ngenerated by a vanilla prompt. We conduct a human preference study on 100 CNN\nDailyMail articles and find that that humans prefer GPT-4 summaries that are\nmore dense than those generated by a vanilla prompt and almost as dense as\nhuman written summaries. Qualitative analysis supports the notion that there\nexists a tradeoff between informativeness and readability. 500 annotated CoD\nsummaries, as well as an extra 5,000 unannotated summaries, are freely\navailable on HuggingFace\n(https://huggingface.co/datasets/griffin/chain_of_density).", "published": "2023-09-08 11:31:08", "link": "http://arxiv.org/abs/2309.04269v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Retrieving Evidence from EHRs with LLMs: Possibilities and Challenges", "abstract": "Unstructured data in Electronic Health Records (EHRs) often contains critical\ninformation -- complementary to imaging -- that could inform radiologists'\ndiagnoses. But the large volume of notes often associated with patients\ntogether with time constraints renders manually identifying relevant evidence\npractically infeasible. In this work we propose and evaluate a zero-shot\nstrategy for using LLMs as a mechanism to efficiently retrieve and summarize\nunstructured evidence in patient EHR relevant to a given query. Our method\nentails tasking an LLM to infer whether a patient has, or is at risk of, a\nparticular condition on the basis of associated notes; if so, we ask the model\nto summarize the supporting evidence. Under expert evaluation, we find that\nthis LLM-based approach provides outputs consistently preferred to a pre-LLM\ninformation retrieval baseline. Manual evaluation is expensive, so we also\npropose and validate a method using an LLM to evaluate (other) LLM outputs for\nthis task, allowing us to scale up evaluation. Our findings indicate the\npromise of LLMs as interfaces to EHR, but also highlight the outstanding\nchallenge posed by \"hallucinations\". In this setting, however, we show that\nmodel confidence in outputs strongly correlates with faithful summaries,\noffering a practical means to limit confabulations.", "published": "2023-09-08 18:44:47", "link": "http://arxiv.org/abs/2309.04550v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can NLP Models 'Identify', 'Distinguish', and 'Justify' Questions that\n  Don't have a Definitive Answer?", "abstract": "Though state-of-the-art (SOTA) NLP systems have achieved remarkable\nperformance on a variety of language understanding tasks, they primarily focus\non questions that have a correct and a definitive answer. However, in\nreal-world applications, users often ask questions that don't have a definitive\nanswer. Incorrectly answering such questions certainly hampers a system's\nreliability and trustworthiness. Can SOTA models accurately identify such\nquestions and provide a reasonable response?\n  To investigate the above question, we introduce QnotA, a dataset consisting\nof five different categories of questions that don't have definitive answers.\nFurthermore, for each QnotA instance, we also provide a corresponding QA\ninstance i.e. an alternate question that ''can be'' answered. With this data,\nwe formulate three evaluation tasks that test a system's ability to 'identify',\n'distinguish', and 'justify' QnotA questions. Through comprehensive\nexperiments, we show that even SOTA models including GPT-3 and Flan T5 do not\nfare well on these tasks and lack considerably behind the human performance\nbaseline. We conduct a thorough analysis which further leads to several\ninteresting findings. Overall, we believe our work and findings will encourage\nand facilitate further research in this important area and help develop more\nrobust models.", "published": "2023-09-08 23:12:03", "link": "http://arxiv.org/abs/2309.04635v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Meta predictive learning model of languages in neural circuits", "abstract": "Large language models based on self-attention mechanisms have achieved\nastonishing performances not only in natural language itself, but also in a\nvariety of tasks of different nature. However, regarding processing language,\nour human brain may not operate using the same principle. Then, a debate is\nestablished on the connection between brain computation and artificial\nself-supervision adopted in large language models. One of most influential\nhypothesis in brain computation is the predictive coding framework, which\nproposes to minimize the prediction error by local learning. However, the role\nof predictive coding and the associated credit assignment in language\nprocessing remains unknown. Here, we propose a mean-field learning model within\nthe predictive coding framework, assuming that the synaptic weight of each\nconnection follows a spike and slab distribution, and only the distribution,\nrather than specific weights, is trained. This meta predictive learning is\nsuccessfully validated on classifying handwritten digits where pixels are input\nto the network in sequence, and moreover on the toy and real language corpus.\nOur model reveals that most of the connections become deterministic after\nlearning, while the output connections have a higher level of variability. The\nperformance of the resulting network ensemble changes continuously with data\nload, further improving with more training data, in analogy with the emergent\nbehavior of large language models. Therefore, our model provides a starting\npoint to investigate the connection among brain computation, next-token\nprediction and general intelligence.", "published": "2023-09-08 03:58:05", "link": "http://arxiv.org/abs/2309.04106v2", "categories": ["cs.CL", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "NESTLE: a No-Code Tool for Statistical Analysis of Legal Corpus", "abstract": "The statistical analysis of large scale legal corpus can provide valuable\nlegal insights. For such analysis one needs to (1) select a subset of the\ncorpus using document retrieval tools, (2) structure text using information\nextraction (IE) systems, and (3) visualize the data for the statistical\nanalysis. Each process demands either specialized tools or programming skills\nwhereas no comprehensive unified \"no-code\" tools have been available. Here we\nprovide NESTLE, a no-code tool for large-scale statistical analysis of legal\ncorpus. Powered by a Large Language Model (LLM) and the internal custom\nend-to-end IE system, NESTLE can extract any type of information that has not\nbeen predefined in the IE system opening up the possibility of unlimited\ncustomizable statistical analysis of the corpus without writing a single line\nof code. We validate our system on 15 Korean precedent IE tasks and 3 legal\ntext classification tasks from LexGLUE. The comprehensive experiments reveal\nNESTLE can achieve GPT-4 comparable performance by training the internal IE\nmodule with 4 human-labeled, and 192 LLM-labeled examples.", "published": "2023-09-08 06:23:25", "link": "http://arxiv.org/abs/2309.04146v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Manifold-based Verbalizer Space Re-embedding for Tuning-free\n  Prompt-based Classification", "abstract": "Prompt-based classification adapts tasks to a cloze question format utilizing\nthe [MASK] token and the filled tokens are then mapped to labels through\npre-defined verbalizers. Recent studies have explored the use of verbalizer\nembeddings to reduce labor in this process. However, all existing studies\nrequire a tuning process for either the pre-trained models or additional\ntrainable embeddings. Meanwhile, the distance between high-dimensional\nverbalizer embeddings should not be measured by Euclidean distance due to the\npotential for non-linear manifolds in the representation space. In this study,\nwe propose a tuning-free manifold-based space re-embedding method called\nLocally Linear Embedding with Intra-class Neighborhood Constraint (LLE-INC) for\nverbalizer embeddings, which preserves local properties within the same class\nas guidance for classification. Experimental results indicate that even without\ntuning any parameters, our LLE-INC is on par with automated verbalizers with\nparameter tuning. And with the parameter updating, our approach further\nenhances prompt-based tuning by up to 3.2%. Furthermore, experiments with the\nLLaMA-7B&13B indicate that LLE-INC is an efficient tuning-free classification\napproach for the hyper-scale language models.", "published": "2023-09-08 07:42:29", "link": "http://arxiv.org/abs/2309.04174v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Knowledge-tuning Large Language Models with Structured Medical Knowledge\n  Bases for Reliable Response Generation in Chinese", "abstract": "Large Language Models (LLMs) have demonstrated remarkable success in diverse\nnatural language processing (NLP) tasks in general domains. However, LLMs\nsometimes generate responses with the hallucination about medical facts due to\nlimited domain knowledge. Such shortcomings pose potential risks in the\nutilization of LLMs within medical contexts. To address this challenge, we\npropose knowledge-tuning, which leverages structured medical knowledge bases\nfor the LLMs to grasp domain knowledge efficiently and facilitate reliable\nresponse generation. We also release cMedKnowQA, a Chinese medical knowledge\nquestion-answering dataset constructed from medical knowledge bases to assess\nthe medical knowledge proficiency of LLMs. Experimental results show that the\nLLMs which are knowledge-tuned with cMedKnowQA, can exhibit higher levels of\naccuracy in response generation compared with vanilla instruction-tuning and\noffer a new reliable way for the domain adaptation of LLMs.", "published": "2023-09-08 07:42:57", "link": "http://arxiv.org/abs/2309.04175v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "UQ at #SMM4H 2023: ALEX for Public Health Analysis with Social Media", "abstract": "As social media becomes increasingly popular, more and more activities\nrelated to public health emerge. Current techniques for public health analysis\ninvolve popular models such as BERT and large language models (LLMs). However,\nthe costs of training in-domain LLMs for public health are especially\nexpensive. Furthermore, such kinds of in-domain datasets from social media are\ngenerally imbalanced. To tackle these challenges, the data imbalance issue can\nbe overcome by data augmentation and balanced training. Moreover, the ability\nof the LLMs can be effectively utilized by prompting the model properly. In\nthis paper, a novel ALEX framework is proposed to improve the performance of\npublic health analysis on social media by adopting an LLMs explanation\nmechanism. Results show that our ALEX model got the best performance among all\nsubmissions in both Task 2 and Task 4 with a high score in Task 1 in Social\nMedia Mining for Health 2023 (SMM4H)[1]. Our code has been released at https://\ngithub.com/YanJiangJerry/ALEX.", "published": "2023-09-08 08:54:55", "link": "http://arxiv.org/abs/2309.04213v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Fuzzy Fingerprinting Transformer Language-Models for Emotion Recognition\n  in Conversations", "abstract": "Fuzzy Fingerprints have been successfully used as an interpretable text\nclassification technique, but, like most other techniques, have been largely\nsurpassed in performance by Large Pre-trained Language Models, such as BERT or\nRoBERTa. These models deliver state-of-the-art results in several Natural\nLanguage Processing tasks, namely Emotion Recognition in Conversations (ERC),\nbut suffer from the lack of interpretability and explainability. In this paper,\nwe propose to combine the two approaches to perform ERC, as a means to obtain\nsimpler and more interpretable Large Language Models-based classifiers. We\npropose to feed the utterances and their previous conversational turns to a\npre-trained RoBERTa, obtaining contextual embedding utterance representations,\nthat are then supplied to an adapted Fuzzy Fingerprint classification module.\nWe validate our approach on the widely used DailyDialog ERC benchmark dataset,\nin which we obtain state-of-the-art level results using a much lighter model.", "published": "2023-09-08 12:26:01", "link": "http://arxiv.org/abs/2309.04292v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Beyond Static Datasets: A Deep Interaction Approach to LLM Evaluation", "abstract": "Large Language Models (LLMs) have made progress in various real-world tasks,\nwhich stimulates requirements for the evaluation of LLMs. Existing LLM\nevaluation methods are mainly supervised signal-based which depends on static\ndatasets and cannot evaluate the ability of LLMs in dynamic real-world\nscenarios where deep interaction widely exists. Other LLM evaluation methods\nare human-based which are costly and time-consuming and are incapable of\nlarge-scale evaluation of LLMs. To address the issues above, we propose a novel\nDeep Interaction-based LLM-evaluation framework. In our proposed framework,\nLLMs' performances in real-world domains can be evaluated from their deep\ninteraction with other LLMs in elaborately designed evaluation tasks.\nFurthermore, our proposed framework is a general evaluation method that can be\napplied to a host of real-world tasks such as machine translation and code\ngeneration. We demonstrate the effectiveness of our proposed method through\nextensive experiments on four elaborately designed evaluation tasks.", "published": "2023-09-08 15:00:41", "link": "http://arxiv.org/abs/2309.04369v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MoEController: Instruction-based Arbitrary Image Manipulation with\n  Mixture-of-Expert Controllers", "abstract": "Diffusion-model-based text-guided image generation has recently made\nastounding progress, producing fascinating results in open-domain image\nmanipulation tasks. Few models, however, currently have complete zero-shot\ncapabilities for both global and local image editing due to the complexity and\ndiversity of image manipulation tasks. In this work, we propose a method with a\nmixture-of-expert (MOE) controllers to align the text-guided capacity of\ndiffusion models with different kinds of human instructions, enabling our model\nto handle various open-domain image manipulation tasks with natural language\ninstructions. First, we use large language models (ChatGPT) and conditional\nimage synthesis models (ControlNet) to generate a large number of global image\ntransfer dataset in addition to the instruction-based local image editing\ndataset. Then, using an MOE technique and task-specific adaptation training on\na large-scale dataset, our conditional diffusion model can edit images globally\nand locally. Extensive experiments demonstrate that our approach performs\nsurprisingly well on various image manipulation tasks when dealing with\nopen-domain images and arbitrary human instructions. Please refer to our\nproject page: [https://oppo-mente-lab.github.io/moe_controller/]", "published": "2023-09-08 15:06:05", "link": "http://arxiv.org/abs/2309.04372v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "CSPRD: A Financial Policy Retrieval Dataset for Chinese Stock Market", "abstract": "In recent years, great advances in pre-trained language models (PLMs) have\nsparked considerable research focus and achieved promising performance on the\napproach of dense passage retrieval, which aims at retrieving relative passages\nfrom massive corpus with given questions. However, most of existing datasets\nmainly benchmark the models with factoid queries of general commonsense, while\nspecialised fields such as finance and economics remain unexplored due to the\ndeficiency of large-scale and high-quality datasets with expert annotations. In\nthis work, we propose a new task, policy retrieval, by introducing the Chinese\nStock Policy Retrieval Dataset (CSPRD), which provides 700+ prospectus passages\nlabeled by experienced experts with relevant articles from 10k+ entries in our\ncollected Chinese policy corpus. Experiments on lexical, embedding and\nfine-tuned bi-encoder models show the effectiveness of our proposed CSPRD yet\nalso suggests ample potential for improvement. Our best performing baseline\nachieves 56.1% MRR@10, 28.5% NDCG@10, 37.5% Recall@10 and 80.6% Precision@10 on\ndev set.", "published": "2023-09-08 15:40:54", "link": "http://arxiv.org/abs/2309.04389v2", "categories": ["cs.CL", "cs.CE"], "primary_category": "cs.CL"}
{"title": "Four Ways to Improve Verbo-visual Fusion for Dense 3D Visual Grounding", "abstract": "3D visual grounding is the task of localizing the object in a 3D scene which\nis referred by a description in natural language. With a wide range of\napplications ranging from autonomous indoor robotics to AR/VR, the task has\nrecently risen in popularity. A common formulation to tackle 3D visual\ngrounding is grounding-by-detection, where localization is done via bounding\nboxes. However, for real-life applications that require physical interactions,\na bounding box insufficiently describes the geometry of an object. We therefore\ntackle the problem of dense 3D visual grounding, i.e. referral-based 3D\ninstance segmentation. We propose a dense 3D grounding network ConcreteNet,\nfeaturing four novel stand-alone modules that aim to improve grounding\nperformance for challenging repetitive instances, i.e. instances with\ndistractors of the same semantic class. First, we introduce a bottom-up\nattentive fusion module that aims to disambiguate inter-instance relational\ncues, next, we construct a contrastive training scheme to induce separation in\nthe latent space, we then resolve view-dependent utterances via a learned\nglobal camera token, and finally we employ multi-view ensembling to improve\nreferred mask quality. ConcreteNet ranks 1st on the challenging ScanRefer\nonline benchmark and has won the ICCV 3rd Workshop on Language for 3D Scenes\n\"3D Object Localization\" challenge. Our code is available at\nouenal.github.io/concretenet/.", "published": "2023-09-08 19:27:01", "link": "http://arxiv.org/abs/2309.04561v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "When Less is More: Investigating Data Pruning for Pretraining LLMs at\n  Scale", "abstract": "Large volumes of text data have contributed significantly to the development\nof large language models (LLMs) in recent years. This data is typically\nacquired by scraping the internet, leading to pretraining datasets comprised of\nnoisy web text. To date, efforts to prune these datasets down to a higher\nquality subset have relied on hand-crafted heuristics encoded as rule-based\nfilters. In this work, we take a wider view and explore scalable estimates of\ndata quality that can be used to systematically measure the quality of\npretraining data. We perform a rigorous comparison at scale of the simple data\nquality estimator of perplexity, as well as more sophisticated and\ncomputationally intensive estimates of the Error L2-Norm and memorization.\nThese metrics are used to rank and prune pretraining corpora, and we\nsubsequently compare LLMs trained on these pruned datasets. Surprisingly, we\nfind that the simple technique of perplexity outperforms our more\ncomputationally expensive scoring methods. We improve over our no-pruning\nbaseline while training on as little as 30% of the original training dataset.\nOur work sets the foundation for unexplored strategies in automatically\ncurating high quality corpora and suggests the majority of pretraining data can\nbe removed while retaining performance.", "published": "2023-09-08 19:34:05", "link": "http://arxiv.org/abs/2309.04564v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Linking Symptom Inventories using Semantic Textual Similarity", "abstract": "An extensive library of symptom inventories has been developed over time to\nmeasure clinical symptoms, but this variety has led to several long standing\nissues. Most notably, results drawn from different settings and studies are not\ncomparable, which limits reproducibility. Here, we present an artificial\nintelligence (AI) approach using semantic textual similarity (STS) to link\nsymptoms and scores across previously incongruous symptom inventories. We\ntested the ability of four pre-trained STS models to screen thousands of\nsymptom description pairs for related content - a challenging task typically\nrequiring expert panels. Models were tasked to predict symptom severity across\nfour different inventories for 6,607 participants drawn from 16 international\ndata sources. The STS approach achieved 74.8% accuracy across five tasks,\noutperforming other models tested. This work suggests that incorporating\ncontextual, semantic information can assist expert decision-making processes,\nyielding gains for both general and disease-specific clinical assessment.", "published": "2023-09-08 21:50:10", "link": "http://arxiv.org/abs/2309.04607v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Reliable and Fluent Large Language Models: Incorporating\n  Feedback Learning Loops in QA Systems", "abstract": "Large language models (LLMs) have emerged as versatile tools in various daily\napplications. However, they are fraught with issues that undermine their\nutility and trustworthiness. These include the incorporation of erroneous\nreferences (citation), the generation of hallucinated information\n(correctness), and the inclusion of superfluous or omission of crucial details\n(fluency). To ameliorate these concerns, this study makes several key\ncontributions. First, we build a dataset to train a critic model capable of\nevaluating the citation, correctness, and fluency of responses generated by\nLLMs in QA systems. Second, we propose an automated feedback mechanism that\nleverages the critic model to offer real-time feedback on heterogeneous aspects\nof generated text. Third, we introduce a feedback learning loop that uses this\ncritic model to iteratively improve the performance of the LLM responsible for\nresponse generation. Experimental results demonstrate the efficacy of our\napproach, showing substantial improvements in citation and fluency metrics for\nChatGPT, including a 4% precision increase in citation and an approximately 8%\nenhancement in the MAUVE metric for fluency, while maintaining high levels of\ncorrectness.", "published": "2023-09-08 09:39:53", "link": "http://arxiv.org/abs/2309.06384v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Down the Toxicity Rabbit Hole: A Novel Framework to Bias Audit Large\n  Language Models", "abstract": "This paper makes three contributions. First, it presents a generalizable,\nnovel framework dubbed \\textit{toxicity rabbit hole} that iteratively elicits\ntoxic content from a wide suite of large language models. Spanning a set of\n1,266 identity groups, we first conduct a bias audit of \\texttt{PaLM 2}\nguardrails presenting key insights. Next, we report generalizability across\nseveral other models. Through the elicited toxic content, we present a broad\nanalysis with a key emphasis on racism, antisemitism, misogyny, Islamophobia,\nhomophobia, and transphobia. Finally, driven by concrete examples, we discuss\npotential ramifications.", "published": "2023-09-08 03:59:02", "link": "http://arxiv.org/abs/2309.06415v4", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Cross-Utterance Conditioned VAE for Speech Generation", "abstract": "Speech synthesis systems powered by neural networks hold promise for\nmultimedia production, but frequently face issues with producing expressive\nspeech and seamless editing. In response, we present the Cross-Utterance\nConditioned Variational Autoencoder speech synthesis (CUC-VAE S2) framework to\nenhance prosody and ensure natural speech generation. This framework leverages\nthe powerful representational capabilities of pre-trained language models and\nthe re-expression abilities of variational autoencoders (VAEs). The core\ncomponent of the CUC-VAE S2 framework is the cross-utterance CVAE, which\nextracts acoustic, speaker, and textual features from surrounding sentences to\ngenerate context-sensitive prosodic features, more accurately emulating human\nprosody generation. We further propose two practical algorithms tailored for\ndistinct speech synthesis applications: CUC-VAE TTS for text-to-speech and\nCUC-VAE SE for speech editing. The CUC-VAE TTS is a direct application of the\nframework, designed to generate audio with contextual prosody derived from\nsurrounding texts. On the other hand, the CUC-VAE SE algorithm leverages real\nmel spectrogram sampling conditioned on contextual information, producing audio\nthat closely mirrors real sound and thereby facilitating flexible speech\nediting based on text such as deletion, insertion, and replacement.\nExperimental results on the LibriTTS datasets demonstrate that our proposed\nmodels significantly enhance speech synthesis and editing, producing more\nnatural and expressive speech.", "published": "2023-09-08 06:48:41", "link": "http://arxiv.org/abs/2309.04156v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Encoding Multi-Domain Scientific Papers by Ensembling Multiple CLS\n  Tokens", "abstract": "Many useful tasks on scientific documents, such as topic classification and\ncitation prediction, involve corpora that span multiple scientific domains.\nTypically, such tasks are accomplished by representing the text with a vector\nembedding obtained from a Transformer's single CLS token. In this paper, we\nargue that using multiple CLS tokens could make a Transformer better specialize\nto multiple scientific domains. We present Multi2SPE: it encourages each of\nmultiple CLS tokens to learn diverse ways of aggregating token embeddings, then\nsums them up together to create a single vector representation. We also propose\nour new multi-domain benchmark, Multi-SciDocs, to test scientific paper vector\nencoders under multi-domain settings. We show that Multi2SPE reduces error by\nup to 25 percent in multi-domain citation prediction, while requiring only a\nnegligible amount of computation in addition to one BERT forward pass.", "published": "2023-09-08 14:00:29", "link": "http://arxiv.org/abs/2309.04333v1", "categories": ["cs.CL", "cs.DL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Measuring and Improving Chain-of-Thought Reasoning in Vision-Language\n  Models", "abstract": "Vision-language models (VLMs) have recently demonstrated strong efficacy as\nvisual assistants that can parse natural queries about the visual content and\ngenerate human-like outputs. In this work, we explore the ability of these\nmodels to demonstrate human-like reasoning based on the perceived information.\nTo address a crucial concern regarding the extent to which their reasoning\ncapabilities are fully consistent and grounded, we also measure the reasoning\nconsistency of these models. We achieve this by proposing a chain-of-thought\n(CoT) based consistency measure. However, such an evaluation requires a\nbenchmark that encompasses both high-level inference and detailed reasoning\nchains, which is costly. We tackle this challenge by proposing a\nLLM-Human-in-the-Loop pipeline, which notably reduces cost while simultaneously\nensuring the generation of a high-quality dataset. Based on this pipeline and\nthe existing coarse-grained annotated dataset, we build the CURE benchmark to\nmeasure both the zero-shot reasoning performance and consistency of VLMs. We\nevaluate existing state-of-the-art VLMs, and find that even the best-performing\nmodel is unable to demonstrate strong visual reasoning capabilities and\nconsistency, indicating that substantial efforts are required to enable VLMs to\nperform visual reasoning as systematically and consistently as humans. As an\nearly step, we propose a two-stage training framework aimed at improving both\nthe reasoning performance and consistency of VLMs. The first stage involves\nemploying supervised fine-tuning of VLMs using step-by-step reasoning samples\nautomatically generated by LLMs. In the second stage, we further augment the\ntraining process by incorporating feedback provided by LLMs to produce\nreasoning chains that are highly consistent and grounded. We empirically\nhighlight the effectiveness of our framework in both reasoning performance and\nconsistency.", "published": "2023-09-08 17:49:44", "link": "http://arxiv.org/abs/2309.04461v2", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Matching Table Metadata with Business Glossaries Using Large Language\n  Models", "abstract": "Enterprises often own large collections of structured data in the form of\nlarge databases or an enterprise data lake. Such data collections come with\nlimited metadata and strict access policies that could limit access to the data\ncontents and, therefore, limit the application of classic retrieval and\nanalysis solutions. As a result, there is a need for solutions that can\neffectively utilize the available metadata. In this paper, we study the problem\nof matching table metadata to a business glossary containing data labels and\ndescriptions. The resulting matching enables the use of an available or curated\nbusiness glossary for retrieval and analysis without or before requesting\naccess to the data contents. One solution to this problem is to use\nmanually-defined rules or similarity measures on column names and glossary\ndescriptions (or their vector embeddings) to find the closest match. However,\nsuch approaches need to be tuned through manual labeling and cannot handle many\nbusiness glossaries that contain a combination of simple as well as complex and\nlong descriptions. In this work, we leverage the power of large language models\n(LLMs) to design generic matching methods that do not require manual tuning and\ncan identify complex relations between column names and glossaries. We propose\nmethods that utilize LLMs in two ways: a) by generating additional context for\ncolumn names that can aid with matching b) by using LLMs to directly infer if\nthere is a relation between column names and glossary descriptions. Our\npreliminary experimental results show the effectiveness of our proposed\nmethods.", "published": "2023-09-08 02:23:59", "link": "http://arxiv.org/abs/2309.11506v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Asymmetric Clean Segments-Guided Self-Supervised Learning for Robust\n  Speaker Verification", "abstract": "Contrastive self-supervised learning (CSL) for speaker verification (SV) has\ndrawn increasing interest recently due to its ability to exploit unlabeled\ndata. Performing data augmentation on raw waveforms, such as adding noise or\nreverberation, plays a pivotal role in achieving promising results in SV. Data\naugmentation, however, demands meticulous calibration to ensure intact\nspeaker-specific information, which is difficult to achieve without speaker\nlabels. To address this issue, we introduce a novel framework by incorporating\nclean and augmented segments into the contrastive training pipeline. The clean\nsegments are repurposed to pair with noisy segments to form additional positive\nand negative pairs. Moreover, the contrastive loss is weighted to increase the\ndifference between the clean and augmented embeddings of different speakers.\nExperimental results on Voxceleb1 suggest that the proposed framework can\nachieve a remarkable 19% improvement over the conventional methods, and it\nsurpasses many existing state-of-the-art techniques.", "published": "2023-09-08 11:23:50", "link": "http://arxiv.org/abs/2309.04265v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "A Two-Stage Training Framework for Joint Speech Compression and\n  Enhancement", "abstract": "This paper considers the joint compression and enhancement problem for speech\nsignal in the presence of noise. Recently, the SoundStream codec, which relies\non end-to-end joint training of an encoder-decoder pair and a residual vector\nquantizer by a combination of adversarial and reconstruction losses,has shown\nvery promising performance, especially in subjective perception quality. In\nthis work, we provide a theoretical result to show that, to simultaneously\nachieve low distortion and high perception in the presence of noise, there\nexist an optimal two-stage optimization procedure for the joint compression and\nenhancement problem. This procedure firstly optimizes an encoder-decoder pair\nusing only distortion loss and then fixes the encoder to optimize a perceptual\ndecoder using perception loss. Based on this result, we construct a two-stage\ntraining framework for joint compression and enhancement of noisy speech\nsignal. Unlike existing training methods which are heuristic, the proposed\ntwo-stage training method has a theoretical foundation. Finally, experimental\nresults for various noise and bit-rate conditions are provided. The results\ndemonstrate that a codec trained by the proposed framework can outperform\nSoundStream and other representative codecs in terms of both objective and\nsubjective evaluation metrics. Code is available at\n\\textit{https://github.com/jscscloris/SEStream}.", "published": "2023-09-08 05:10:42", "link": "http://arxiv.org/abs/2309.04132v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Leveraging Pretrained Image-text Models for Improving Audio-Visual\n  Learning", "abstract": "Visually grounded speech systems learn from paired images and their spoken\ncaptions. Recently, there have been attempts to utilize the visually grounded\nmodels trained from images and their corresponding text captions, such as CLIP,\nto improve speech-based visually grounded models' performance. However, the\nmajority of these models only utilize the pretrained image encoder. Cascaded\nSpeechCLIP attempted to generate localized word-level information and utilize\nboth the pretrained image and text encoders. Despite using both, they noticed a\nsubstantial drop in retrieval performance. We proposed Segmental SpeechCLIP\nwhich used a hierarchical segmental speech encoder to generate sequences of\nword-like units. We used the pretrained CLIP text encoder on top of these\nword-like unit representations and showed significant improvements over the\ncascaded variant of SpeechCLIP. Segmental SpeechCLIP directly learns the word\nembeddings as input to the CLIP text encoder bypassing the vocabulary\nembeddings. Here, we explore mapping audio to CLIP vocabulary embeddings via\nregularization and quantization. As our objective is to distill semantic\ninformation into the speech encoders, we explore the usage of large unimodal\npretrained language models as the text encoders. Our method enables us to\nbridge image and text encoders e.g. DINO and RoBERTa trained with uni-modal\ndata. Finally, we extend our framework in audio-only settings where only pairs\nof semantically related audio are available. Experiments show that audio-only\nsystems perform close to the audio-visual system.", "published": "2023-09-08 22:41:36", "link": "http://arxiv.org/abs/2309.04628v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Exploring Domain-Specific Enhancements for a Neural Foley Synthesizer", "abstract": "Foley sound synthesis refers to the creation of authentic, diegetic sound\neffects for media, such as film or radio. In this study, we construct a neural\nFoley synthesizer capable of generating mono-audio clips across seven\npredefined categories. Our approach introduces multiple enhancements to\nexisting models in the text-to-audio domain, with the goal of enriching the\ndiversity and acoustic characteristics of the generated foleys. Notably, we\nutilize a pre-trained encoder that retains acoustical and musical attributes in\nintermediate embeddings, implement class-conditioning to enhance\ndifferentiability among foley classes in their intermediate representations,\nand devise an innovative transformer-based architecture for optimizing\nself-attention computations on very large inputs without compromising valuable\ninformation. Subsequent to implementation, we present intermediate outcomes\nthat surpass the baseline, discuss practical challenges encountered in\nachieving optimal results, and outline potential pathways for further research.", "published": "2023-09-08 23:43:57", "link": "http://arxiv.org/abs/2309.04641v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Long-Tail Friendly Representation Framework for Artist and Music\n  Similarity", "abstract": "The investigation of the similarity between artists and music is crucial in\nmusic retrieval and recommendation, and addressing the challenge of the\nlong-tail phenomenon is increasingly important. This paper proposes a Long-Tail\nFriendly Representation Framework (LTFRF) that utilizes neural networks to\nmodel the similarity relationship. Our approach integrates music, user,\nmetadata, and relationship data into a unified metric learning framework, and\nemploys a meta-consistency relationship as a regular term to introduce the\nMulti-Relationship Loss. Compared to the Graph Neural Network (GNN), our\nproposed framework improves the representation performance in long-tail\nscenarios, which are characterized by sparse relationships between artists and\nmusic. We conduct experiments and analysis on the AllMusic dataset, and the\nresults demonstrate that our framework provides a favorable generalization of\nartist and music representation. Specifically, on similar artist/music\nrecommendation tasks, the LTFRF outperforms the baseline by 9.69%/19.42% in Hit\nRatio@10, and in long-tail cases, the framework achieves 11.05%/14.14% higher\nthan the baseline in Consistent@10.", "published": "2023-09-08 07:53:21", "link": "http://arxiv.org/abs/2309.04182v1", "categories": ["cs.SD", "cs.IR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Parallel and Limited Data Voice Conversion Using Stochastic Variational\n  Deep Kernel Learning", "abstract": "Typically, voice conversion is regarded as an engineering problem with\nlimited training data. The reliance on massive amounts of data hinders the\npractical applicability of deep learning approaches, which have been\nextensively researched in recent years. On the other hand, statistical methods\nare effective with limited data but have difficulties in modelling complex\nmapping functions. This paper proposes a voice conversion method that works\nwith limited data and is based on stochastic variational deep kernel learning\n(SVDKL). At the same time, SVDKL enables the use of deep neural networks'\nexpressive capability as well as the high flexibility of the Gaussian process\nas a Bayesian and non-parametric method. When the conventional kernel is\ncombined with the deep neural network, it is possible to estimate non-smooth\nand more complex functions. Furthermore, the model's sparse variational\nGaussian process solves the scalability problem and, unlike the exact Gaussian\nprocess, allows for the learning of a global mapping function for the entire\nacoustic space. One of the most important aspects of the proposed scheme is\nthat the model parameters are trained using marginal likelihood optimization,\nwhich considers both data fitting and model complexity. Considering the\ncomplexity of the model reduces the amount of training data by increasing the\nresistance to overfitting. To evaluate the proposed scheme, we examined the\nmodel's performance with approximately 80 seconds of training data. The results\nindicated that our method obtained a higher mean opinion score, smaller\nspectral distortion, and better preference tests than the compared methods.", "published": "2023-09-08 16:32:47", "link": "http://arxiv.org/abs/2309.04420v1", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "COVID-19 Detection System: A Comparative Analysis of System Performance\n  Based on Acoustic Features of Cough Audio Signals", "abstract": "A wide range of respiratory diseases, such as cold and flu, asthma, and\nCOVID-19, affect people's daily lives worldwide. In medical practice,\nrespiratory sounds are widely used in medical services to diagnose various\nrespiratory illnesses and lung disorders. The traditional diagnosis of such\nsounds requires specialized knowledge, which can be costly and reliant on human\nexpertise. Despite this, recent advancements, such as cough audio recordings,\nhave emerged as a means to automate the detection of respiratory conditions.\nTherefore, this research aims to explore various acoustic features that enhance\nthe performance of machine learning (ML) models in detecting COVID-19 from\ncough signals. It investigates the efficacy of three feature extraction\ntechniques, including Mel Frequency Cepstral Coefficients (MFCC), Chroma, and\nSpectral Contrast features, when applied to two machine learning algorithms,\nSupport Vector Machine (SVM) and Multilayer Perceptron (MLP), and therefore\nproposes an efficient CovCepNet detection system. The proposed system provides\na practical solution and demonstrates state-of-the-art classification\nperformance, with an AUC of 0.843 on the COUGHVID dataset and 0.953 on the\nVirufy dataset for COVID-19 detection from cough audio signals.", "published": "2023-09-08 08:33:24", "link": "http://arxiv.org/abs/2309.04505v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The Power of Sound (TPoS): Audio Reactive Video Generation with Stable\n  Diffusion", "abstract": "In recent years, video generation has become a prominent generative tool and\nhas drawn significant attention. However, there is little consideration in\naudio-to-video generation, though audio contains unique qualities like temporal\nsemantics and magnitude. Hence, we propose The Power of Sound (TPoS) model to\nincorporate audio input that includes both changeable temporal semantics and\nmagnitude. To generate video frames, TPoS utilizes a latent stable diffusion\nmodel with textual semantic information, which is then guided by the sequential\naudio embedding from our pretrained Audio Encoder. As a result, this method\nproduces audio reactive video contents. We demonstrate the effectiveness of\nTPoS across various tasks and compare its results with current state-of-the-art\ntechniques in the field of audio-to-video generation. More examples are\navailable at https://ku-vai.github.io/TPoS/", "published": "2023-09-08 12:21:01", "link": "http://arxiv.org/abs/2309.04509v1", "categories": ["cs.SD", "cs.CV", "cs.GR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "End-to-End Speech Recognition and Disfluency Removal with Acoustic\n  Language Model Pretraining", "abstract": "The SOTA in transcription of disfluent and conversational speech has in\nrecent years favored two-stage models, with separate transcription and cleaning\nstages. We believe that previous attempts at end-to-end disfluency removal have\nfallen short because of the representational advantage that large-scale\nlanguage model pretraining has given to lexical models. Until recently, the\nhigh dimensionality and limited availability of large audio datasets inhibited\nthe development of large-scale self-supervised pretraining objectives for\nlearning effective audio representations, giving a relative advantage to the\ntwo-stage approach, which utilises pretrained representations for lexical\ntokens. In light of recent successes in large scale audio pretraining, we\nrevisit the performance comparison between two-stage and end-to-end model and\nfind that audio based language models pretrained using weak self-supervised\nobjectives match or exceed the performance of similarly trained two-stage\nmodels, and further, that the choice of pretraining objective substantially\neffects a model's ability to be adapted to the disfluency removal task.", "published": "2023-09-08 17:12:14", "link": "http://arxiv.org/abs/2309.04516v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
