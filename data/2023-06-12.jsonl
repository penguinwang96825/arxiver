{"title": "Recurrent Attention Networks for Long-text Modeling", "abstract": "Self-attention-based models have achieved remarkable progress in short-text\nmining. However, the quadratic computational complexities restrict their\napplication in long text processing. Prior works have adopted the chunking\nstrategy to divide long documents into chunks and stack a self-attention\nbackbone with the recurrent structure to extract semantic representation. Such\nan approach disables parallelization of the attention mechanism, significantly\nincreasing the training cost and raising hardware requirements. Revisiting the\nself-attention mechanism and the recurrent structure, this paper proposes a\nnovel long-document encoding model, Recurrent Attention Network (RAN), to\nenable the recurrent operation of self-attention. Combining the advantages from\nboth sides, the well-designed RAN is capable of extracting global semantics in\nboth token-level and document-level representations, making it inherently\ncompatible with both sequential and classification tasks, respectively.\nFurthermore, RAN is computationally scalable as it supports parallelization on\nlong document processing. Extensive experiments demonstrate the long-text\nencoding ability of the proposed RAN model on both classification and\nsequential tasks, showing its potential for a wide range of applications.", "published": "2023-06-12 03:28:33", "link": "http://arxiv.org/abs/2306.06843v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UniPoll: A Unified Social Media Poll Generation Framework via\n  Multi-Objective Optimization", "abstract": "Social media platforms are vital for expressing opinions and understanding\npublic sentiment, yet many analytical tools overlook passive users who mainly\nconsume content without engaging actively. To address this, we introduce\nUniPoll, an advanced framework designed to automatically generate polls from\nsocial media posts using sophisticated natural language generation (NLG)\ntechniques. Unlike traditional methods that struggle with social media's\ninformal and context-sensitive nature, UniPoll leverages enriched contexts from\nuser comments and employs multi-objective optimization to enhance poll\nrelevance and engagement. To tackle the inherently noisy nature of social media\ndata, UniPoll incorporates Retrieval-Augmented Generation (RAG) and synthetic\ndata generation, ensuring robust performance across real-world scenarios. The\nframework surpasses existing models, including T5, ChatGLM3, and GPT-3.5, in\ngenerating coherent and contextually appropriate question-answer pairs.\nEvaluated on the Chinese WeiboPolls dataset and the newly introduced English\nRedditPolls dataset, UniPoll demonstrates superior cross-lingual and\ncross-platform capabilities, making it a potent tool to boost user engagement\nand create a more inclusive environment for interaction.", "published": "2023-06-12 03:54:04", "link": "http://arxiv.org/abs/2306.06851v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "History Semantic Graph Enhanced Conversational KBQA with Temporal\n  Information Modeling", "abstract": "Context information modeling is an important task in conversational KBQA.\nHowever, existing methods usually assume the independence of utterances and\nmodel them in isolation. In this paper, we propose a History Semantic Graph\nEnhanced KBQA model (HSGE) that is able to effectively model long-range\nsemantic dependencies in conversation history while maintaining low\ncomputational cost. The framework incorporates a context-aware encoder, which\nemploys a dynamic memory decay mechanism and models context at different levels\nof granularity. We evaluate HSGE on a widely used benchmark dataset for complex\nsequential question answering. Experimental results demonstrate that it\noutperforms existing baselines averaged on all question types.", "published": "2023-06-12 05:10:58", "link": "http://arxiv.org/abs/2306.06872v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the N-gram Approximation of Pre-trained Language Models", "abstract": "Large pre-trained language models (PLMs) have shown remarkable performance\nacross various natural language understanding (NLU) tasks, particularly in\nlow-resource settings. Nevertheless, their potential in Automatic Speech\nRecognition (ASR) remains largely unexplored. This study investigates the\npotential usage of PLMs for language modelling in ASR. We compare the\napplication of large-scale text sampling and probability conversion for\napproximating GPT-2 into an n-gram model. Furthermore, we introduce a\nvocabulary-restricted decoding method for random sampling, and evaluate the\neffects of domain difficulty and data size on the usability of generated text.\nOur findings across eight domain-specific corpora support the use of\nsampling-based approximation and show that interpolating with a large sampled\ncorpus improves test perplexity over a baseline trigram by 15%. Our\nvocabulary-restricted decoding method pushes this improvement further by 5% in\ndomain-specific settings.", "published": "2023-06-12 06:42:08", "link": "http://arxiv.org/abs/2306.06892v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The BEA 2023 Shared Task on Generating AI Teacher Responses in\n  Educational Dialogues", "abstract": "This paper describes the results of the first shared task on the generation\nof teacher responses in educational dialogues. The goal of the task was to\nbenchmark the ability of generative language models to act as AI teachers,\nreplying to a student in a teacher-student dialogue. Eight teams participated\nin the competition hosted on CodaLab. They experimented with a wide variety of\nstate-of-the-art models, including Alpaca, Bloom, DialoGPT, DistilGPT-2,\nFlan-T5, GPT-2, GPT-3, GPT- 4, LLaMA, OPT-2.7B, and T5-base. Their submissions\nwere automatically scored using BERTScore and DialogRPT metrics, and the top\nthree among them were further manually evaluated in terms of pedagogical\nability based on Tack and Piech (2022). The NAISTeacher system, which ranked\nfirst in both automated and human evaluation, generated responses with GPT-3.5\nusing an ensemble of prompts and a DialogRPT-based ranking of responses for\ngiven dialogue contexts. Despite the promising achievements of the\nparticipating teams, the results also highlight the need for evaluation metrics\nbetter suited to educational contexts.", "published": "2023-06-12 08:21:34", "link": "http://arxiv.org/abs/2306.06941v1", "categories": ["cs.CL", "I.2.7; K.3"], "primary_category": "cs.CL"}
{"title": "Measuring Sentiment Bias in Machine Translation", "abstract": "Biases induced to text by generative models have become an increasingly large\ntopic in recent years. In this paper we explore how machine translation might\nintroduce a bias in sentiments as classified by sentiment analysis models. For\nthis, we compare three open access machine translation models for five\ndifferent languages on two parallel corpora to test if the translation process\ncauses a shift in sentiment classes recognized in the texts. Though our\nstatistic test indicate shifts in the label probability distributions, we find\nnone that appears consistent enough to assume a bias induced by the translation\nprocess.", "published": "2023-06-12 14:40:29", "link": "http://arxiv.org/abs/2306.07152v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prompt-based Extraction of Social Determinants of Health Using Few-shot\n  Learning", "abstract": "Social determinants of health (SDOH) documented in the electronic health\nrecord through unstructured text are increasingly being studied to understand\nhow SDOH impacts patient health outcomes. In this work, we utilize the Social\nHistory Annotation Corpus (SHAC), a multi-institutional corpus of de-identified\nsocial history sections annotated for SDOH, including substance use,\nemployment, and living status information. We explore the automatic extraction\nof SDOH information with SHAC in both standoff and inline annotation formats\nusing GPT-4 in a one-shot prompting setting. We compare GPT-4 extraction\nperformance with a high-performing supervised approach and perform thorough\nerror analyses. Our prompt-based GPT-4 method achieved an overall 0.652 F1 on\nthe SHAC test set, similar to the 7th best-performing system among all teams in\nthe n2c2 challenge with SHAC.", "published": "2023-06-12 15:08:25", "link": "http://arxiv.org/abs/2306.07170v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Augmenting Language Models with Long-Term Memory", "abstract": "Existing large language models (LLMs) can only afford fix-sized inputs due to\nthe input length limit, preventing them from utilizing rich long-context\ninformation from past inputs. To address this, we propose a framework, Language\nModels Augmented with Long-Term Memory (LongMem), which enables LLMs to\nmemorize long history. We design a novel decoupled network architecture with\nthe original backbone LLM frozen as a memory encoder and an adaptive residual\nside-network as a memory retriever and reader. Such a decoupled memory design\ncan easily cache and update long-term past contexts for memory retrieval\nwithout suffering from memory staleness. Enhanced with memory-augmented\nadaptation training, LongMem can thus memorize long past context and use\nlong-term memory for language modeling. The proposed memory retrieval module\ncan handle unlimited-length context in its memory bank to benefit various\ndownstream tasks. Typically, LongMem can enlarge the long-form memory to 65k\ntokens and thus cache many-shot extra demonstration examples as long-form\nmemory for in-context learning. Experiments show that our method outperforms\nstrong long-context models on ChapterBreak, a challenging long-context modeling\nbenchmark, and achieves remarkable improvements on memory-augmented in-context\nlearning over LLMs. The results demonstrate that the proposed method is\neffective in helping language models to memorize and utilize long-form\ncontents. Our code is open-sourced at https://aka.ms/LongMem.", "published": "2023-06-12 15:13:39", "link": "http://arxiv.org/abs/2306.07174v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Effect of Masking Strategies on Knowledge Retention by Language\n  Models", "abstract": "Language models retain a significant amount of world knowledge from their\npre-training stage. This allows knowledgeable models to be applied to\nknowledge-intensive tasks prevalent in information retrieval, such as ranking\nor question answering. Understanding how and which factual information is\nacquired by our models is necessary to build responsible models. However,\nlimited work has been done to understand the effect of pre-training tasks on\nthe amount of knowledge captured and forgotten by language models during\npre-training. Building a better understanding of knowledge acquisition is the\ngoal of this paper. Therefore, we utilize a selection of pre-training tasks to\ninfuse knowledge into our model. In the following steps, we test the model's\nknowledge retention by measuring its ability to answer factual questions. Our\nexperiments show that masking entities and principled masking of correlated\nspans based on pointwise mutual information lead to more factual knowledge\nbeing retained than masking random tokens. Our findings demonstrate that, like\nthe ability to perform a task, the (factual) knowledge acquired from being\ntrained on that task is forgotten when a model is trained to perform another\ntask (catastrophic forgetting) and how to prevent this phenomenon. To foster\nreproducibility, the code, as well as the data used in this paper, are openly\navailable.", "published": "2023-06-12 15:35:23", "link": "http://arxiv.org/abs/2306.07185v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey of Vision-Language Pre-training from the Lens of Multimodal\n  Machine Translation", "abstract": "Large language models such as BERT and the GPT series started a paradigm\nshift that calls for building general-purpose models via pre-training on large\ndatasets, followed by fine-tuning on task-specific datasets. There is now a\nplethora of large pre-trained models for Natural Language Processing and\nComputer Vision. Recently, we have seen rapid developments in the joint\nVision-Language space as well, where pre-trained models such as CLIP (Radford\net al., 2021) have demonstrated improvements in downstream tasks like image\ncaptioning and visual question answering. However, surprisingly there is\ncomparatively little work on exploring these models for the task of multimodal\nmachine translation, where the goal is to leverage image/video modality in\ntext-to-text translation. To fill this gap, this paper surveys the landscape of\nlanguage-and-vision pre-training from the lens of multimodal machine\ntranslation. We summarize the common architectures, pre-training objectives,\nand datasets from literature and conjecture what further is needed to make\nprogress on multimodal machine translation.", "published": "2023-06-12 15:56:10", "link": "http://arxiv.org/abs/2306.07198v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LTCR: Long-Text Chinese Rumor Detection Dataset", "abstract": "False information can spread quickly on social media, negatively influencing\nthe citizens' behaviors and responses to social events. To better detect all of\nthe fake news, especially long texts which are harder to find completely, a\nLong-Text Chinese Rumor detection dataset named LTCR is proposed. The LTCR\ndataset provides a valuable resource for accurately detecting misinformation,\nespecially in the context of complex fake news related to COVID-19. The dataset\nconsists of 1,729 and 500 pieces of real and fake news, respectively. The\naverage lengths of real and fake news are approximately 230 and 152 characters.\nWe also propose \\method, Salience-aware Fake News Detection Model, which\nachieves the highest accuracy (95.85%), fake news recall (90.91%) and F-score\n(90.60%) on the dataset. (https://github.com/Enderfga/DoubleCheck)", "published": "2023-06-12 16:03:36", "link": "http://arxiv.org/abs/2306.07201v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EriBERTa: A Bilingual Pre-Trained Language Model for Clinical Natural\n  Language Processing", "abstract": "The utilization of clinical reports for various secondary purposes, including\nhealth research and treatment monitoring, is crucial for enhancing patient\ncare. Natural Language Processing (NLP) tools have emerged as valuable assets\nfor extracting and processing relevant information from these reports. However,\nthe availability of specialized language models for the clinical domain in\nSpanish has been limited.\n  In this paper, we introduce EriBERTa, a bilingual domain-specific language\nmodel pre-trained on extensive medical and clinical corpora. We demonstrate\nthat EriBERTa outperforms previous Spanish language models in the clinical\ndomain, showcasing its superior capabilities in understanding medical texts and\nextracting meaningful information. Moreover, EriBERTa exhibits promising\ntransfer learning abilities, allowing for knowledge transfer from one language\nto another. This aspect is particularly beneficial given the scarcity of\nSpanish clinical data.", "published": "2023-06-12 18:56:25", "link": "http://arxiv.org/abs/2306.07373v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Probing Quantifier Comprehension in Large Language Models: Another\n  Example of Inverse Scaling", "abstract": "With their increasing size, large language models (LLMs) are becoming\nincreasingly good at language understanding tasks. But even with high\nperformance on specific downstream task, LLMs fail at simple linguistic tests\nfor negation or quantifier understanding. Previous work on quantifier\nunderstanding in LLMs show inverse scaling in understanding few-type\nquantifiers. In this paper, we question the claims of of previous work and show\nthat it is a result of inappropriate testing methodology. We also present\nalternate methods to measure quantifier comprehension in LLMs and show that\nLLMs are able to better understand the difference between the meaning of\nfew-type and most-type quantifiers as their size increases, although they are\nnot particularly good at it. We also observe inverse scaling for most-type\nquantifier understanding, which is contrary to human psycho-linguistic\nexperiments and previous work, where the model's understanding of most-type\nquantifier gets worse as the model size increases. We do this evaluation on\nmodels ranging from 125M-175B parameters, which suggests that LLMs do not do as\nwell as expected with quantifiers. We also discuss the possible reasons for\nthis and the relevance of quantifier understanding in evaluating language\nunderstanding in LLMs.", "published": "2023-06-12 19:20:18", "link": "http://arxiv.org/abs/2306.07384v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Topic Extraction in Recommender Systems with Entropy\n  Regularization", "abstract": "In recent years, many recommender systems have utilized textual data for\ntopic extraction to enhance interpretability. However, our findings reveal a\nnoticeable deficiency in the coherence of keywords within topics, resulting in\nlow explainability of the model. This paper introduces a novel approach called\nentropy regularization to address the issue, leading to more interpretable\ntopics extracted from recommender systems, while ensuring that the performance\nof the primary task stays competitively strong. The effectiveness of the\nstrategy is validated through experiments on a variation of the probabilistic\nmatrix factorization model that utilizes textual data to extract item\nembeddings. The experiment results show a significant improvement in topic\ncoherence, which is quantified by cosine similarity on word embeddings.", "published": "2023-06-12 20:05:09", "link": "http://arxiv.org/abs/2306.07403v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Textual Augmentation Techniques Applied to Low Resource Machine\n  Translation: Case of Swahili", "abstract": "In this work we investigate the impact of applying textual data augmentation\ntasks to low resource machine translation. There has been recent interest in\ninvestigating approaches for training systems for languages with limited\nresources and one popular approach is the use of data augmentation techniques.\nData augmentation aims to increase the quantity of data that is available to\ntrain the system. In machine translation, majority of the language pairs around\nthe world are considered low resource because they have little parallel data\navailable and the quality of neural machine translation (NMT) systems depend a\nlot on the availability of sizable parallel corpora. We study and apply three\nsimple data augmentation techniques popularly used in text classification\ntasks; synonym replacement, random insertion and contextual data augmentation\nand compare their performance with baseline neural machine translation for\nEnglish-Swahili (En-Sw) datasets. We also present results in BLEU, ChrF and\nMeteor scores. Overall, the contextual data augmentation technique shows some\nimprovements both in the $EN \\rightarrow SW$ and $SW \\rightarrow EN$\ndirections. We see that there is potential to use these methods in neural\nmachine translation when more extensive experiments are done with diverse\ndatasets.", "published": "2023-06-12 20:43:24", "link": "http://arxiv.org/abs/2306.07414v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Gender-Inclusive Grammatical Error Correction through Augmentation", "abstract": "In this paper we show that GEC systems display gender bias related to the use\nof masculine and feminine terms and the gender-neutral singular \"they\". We\ndevelop parallel datasets of texts with masculine and feminine terms and\nsingular \"they\" and use them to quantify gender bias in three competitive GEC\nsystems. We contribute a novel data augmentation technique for singular \"they\"\nleveraging linguistic insights about its distribution relative to plural\n\"they\". We demonstrate that both this data augmentation technique and a\nrefinement of a similar augmentation technique for masculine and feminine terms\ncan generate training data that reduces bias in GEC systems, especially with\nrespect to singular \"they\" while maintaining the same level of quality.", "published": "2023-06-12 20:44:47", "link": "http://arxiv.org/abs/2306.07415v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Izindaba-Tindzaba: Machine learning news categorisation for Long and\n  Short Text for isiZulu and Siswati", "abstract": "Local/Native South African languages are classified as low-resource\nlanguages. As such, it is essential to build the resources for these languages\nso that they can benefit from advances in the field of natural language\nprocessing. In this work, the focus was to create annotated news datasets for\nthe isiZulu and Siswati native languages based on news topic classification\ntasks and present the findings from these baseline classification models. Due\nto the shortage of data for these native South African languages, the datasets\nthat were created were augmented and oversampled to increase data size and\novercome class classification imbalance. In total, four different\nclassification models were used namely Logistic regression, Naive bayes,\nXGBoost and LSTM. These models were trained on three different word embeddings\nnamely Bag-Of-Words, TFIDF and Word2vec. The results of this study showed that\nXGBoost, Logistic Regression and LSTM, trained from Word2vec performed better\nthan the other combinations.", "published": "2023-06-12 21:02:12", "link": "http://arxiv.org/abs/2306.07426v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Weakly supervised information extraction from inscrutable handwritten\n  document images", "abstract": "State-of-the-art information extraction methods are limited by OCR errors.\nThey work well for printed text in form-like documents, but unstructured,\nhandwritten documents still remain a challenge. Adapting existing models to\ndomain-specific training data is quite expensive, because of two factors, 1)\nlimited availability of the domain-specific documents (such as handwritten\nprescriptions, lab notes, etc.), and 2) annotations become even more\nchallenging as one needs domain-specific knowledge to decode inscrutable\nhandwritten document images. In this work, we focus on the complex problem of\nextracting medicine names from handwritten prescriptions using only weakly\nlabeled data. The data consists of images along with the list of medicine names\nin it, but not their location in the image. We solve the problem by first\nidentifying the regions of interest, i.e., medicine lines from just weak labels\nand then injecting a domain-specific medicine language model learned using only\nsynthetically generated data. Compared to off-the-shelf state-of-the-art\nmethods, our approach performs >2.5x better in medicine names extraction from\nprescriptions.", "published": "2023-06-12 02:22:30", "link": "http://arxiv.org/abs/2306.06823v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "SE#PCFG: Semantically Enhanced PCFG for Password Analysis and Cracking", "abstract": "Much research has been done on user-generated textual passwords.\nSurprisingly, semantic information in such passwords remain under-investigated,\nwith passwords created by English- and/or Chinese-speaking users being more\nstudied with limited semantics. This paper fills this gap by proposing a\ngeneral framework based on semantically enhanced PCFG (probabilistic\ncontext-free grammars) named SE#PCFG. It allowed us to consider 43 types of\nsemantic information, the richest set considered so far, for password analysis.\nApplying SE#PCFG to 17 large leaked password databases of user speaking four\nlanguages (English, Chinese, German and French), we demonstrate its usefulness\nand report a wide range of new insights about password semantics at different\nlevels such as cross-website password correlations. Furthermore, based on\nSE#PCFG and a new systematic smoothing method, we proposed the Semantically\nEnhanced Password Cracking Architecture (SEPCA), and compared its performance\nagainst three SOTA (state-of-the-art) benchmarks in terms of the password\ncoverage rate: two other PCFG variants and neural network. Our experimental\nresults showed that SEPCA outperformed all the three benchmarks consistently\nand significantly across 52 test cases, by up to 21.53%, 52.55% and 7.86%,\nrespectively, at the user-level (with duplicate passwords). At the level of\nunique passwords, SEPCA also beats the three counterparts by up to 43.83%,\n94.11% and 11.16%, respectively.", "published": "2023-06-12 02:25:04", "link": "http://arxiv.org/abs/2306.06824v2", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Recursion of Thought: A Divide-and-Conquer Approach to Multi-Context\n  Reasoning with Language Models", "abstract": "Generating intermediate steps, or Chain of Thought (CoT), is an effective way\nto significantly improve language models' (LM) multi-step reasoning capability.\nHowever, the CoT lengths can grow rapidly with the problem complexity, easily\nexceeding the maximum context size. Instead of increasing the context limit,\nwhich has already been heavily investigated, we explore an orthogonal\ndirection: making LMs divide a problem into multiple contexts. We propose a new\ninference framework, called Recursion of Thought (RoT), which introduces\nseveral special tokens that the models can output to trigger context-related\noperations. Extensive experiments with multiple architectures including GPT-3\nshow that RoT dramatically improves LMs' inference capability to solve\nproblems, whose solution consists of hundreds of thousands of tokens.", "published": "2023-06-12 06:34:16", "link": "http://arxiv.org/abs/2306.06891v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Devil is in the Details: On the Pitfalls of Event Extraction\n  Evaluation", "abstract": "Event extraction (EE) is a crucial task aiming at extracting events from\ntexts, which includes two subtasks: event detection (ED) and event argument\nextraction (EAE). In this paper, we check the reliability of EE evaluations and\nidentify three major pitfalls: (1) The data preprocessing discrepancy makes the\nevaluation results on the same dataset not directly comparable, but the data\npreprocessing details are not widely noted and specified in papers. (2) The\noutput space discrepancy of different model paradigms makes different-paradigm\nEE models lack grounds for comparison and also leads to unclear mapping issues\nbetween predictions and annotations. (3) The absence of pipeline evaluation of\nmany EAE-only works makes them hard to be directly compared with EE works and\nmay not well reflect the model performance in real-world pipeline scenarios. We\ndemonstrate the significant influence of these pitfalls through comprehensive\nmeta-analyses of recent papers and empirical experiments. To avoid these\npitfalls, we suggest a series of remedies, including specifying data\npreprocessing, standardizing outputs, and providing pipeline evaluation\nresults. To help implement these remedies, we develop a consistent evaluation\nframework OMNIEVENT, which can be obtained from\nhttps://github.com/THU-KEG/OmniEvent.", "published": "2023-06-12 07:38:31", "link": "http://arxiv.org/abs/2306.06918v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning Multilingual Sentence Representations with Cross-lingual\n  Consistency Regularization", "abstract": "Multilingual sentence representations are the foundation for similarity-based\nbitext mining, which is crucial for scaling multilingual neural machine\ntranslation (NMT) system to more languages. In this paper, we introduce MuSR: a\none-for-all Multilingual Sentence Representation model that supports more than\n220 languages. Leveraging billions of English-centric parallel corpora, we\ntrain a multilingual Transformer encoder, coupled with an auxiliary Transformer\ndecoder, by adopting a multilingual NMT framework with CrossConST, a\ncross-lingual consistency regularization technique proposed in Gao et al.\n(2023). Experimental results on multilingual similarity search and bitext\nmining tasks show the effectiveness of our approach. Specifically, MuSR\nachieves superior performance over LASER3 (Heffernan et al., 2022) which\nconsists of 148 independent multilingual sentence encoders.", "published": "2023-06-12 07:39:06", "link": "http://arxiv.org/abs/2306.06919v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Rethinking Translation Memory Augmented Neural Machine Translation", "abstract": "This paper rethinks translation memory augmented neural machine translation\n(TM-augmented NMT) from two perspectives, i.e., a probabilistic view of\nretrieval and the variance-bias decomposition principle. The finding\ndemonstrates that TM-augmented NMT is good at the ability of fitting data\n(i.e., lower bias) but is more sensitive to the fluctuations in the training\ndata (i.e., higher variance), which provides an explanation to a recently\nreported contradictory phenomenon on the same translation task: TM-augmented\nNMT substantially advances vanilla NMT under the high-resource scenario whereas\nit fails under the low-resource scenario. Then we propose a simple yet\neffective TM-augmented NMT model to promote the variance and address the\ncontradictory phenomenon. Extensive experiments show that the proposed\nTM-augmented NMT achieves consistent gains over both conventional NMT and\nexisting TM-augmented NMT under two variance-preferable (low-resource and\nplug-and-play) scenarios as well as the high-resource scenario.", "published": "2023-06-12 08:32:04", "link": "http://arxiv.org/abs/2306.06948v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Gradient Ascent Post-training Enhances Language Model Generalization", "abstract": "In this work, we empirically show that updating pretrained LMs (350M, 1.3B,\n2.7B) with just a few steps of Gradient Ascent Post-training (GAP) on random,\nunlabeled text corpora enhances its zero-shot generalization capabilities\nacross diverse NLP tasks. Specifically, we show that GAP can allow LMs to\nbecome comparable to 2-3x times larger LMs across 12 different NLP tasks. We\nalso show that applying GAP on out-of-distribution corpora leads to the most\nreliable performance improvements. Our findings indicate that GAP can be a\npromising method for improving the generalization capability of LMs without any\ntask-specific fine-tuning.", "published": "2023-06-12 11:59:33", "link": "http://arxiv.org/abs/2306.07052v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On the Amplification of Linguistic Bias through Unintentional\n  Self-reinforcement Learning by Generative Language Models -- A Perspective", "abstract": "Generative Language Models (GLMs) have the potential to significantly shape\nour linguistic landscape due to their expansive use in various digital\napplications. However, this widespread adoption might inadvertently trigger a\nself-reinforcement learning cycle that can amplify existing linguistic biases.\nThis paper explores the possibility of such a phenomenon, where the initial\nbiases in GLMs, reflected in their generated text, can feed into the learning\nmaterial of subsequent models, thereby reinforcing and amplifying these biases.\nMoreover, the paper highlights how the pervasive nature of GLMs might influence\nthe linguistic and cognitive development of future generations, as they may\nunconsciously learn and reproduce these biases. The implications of this\npotential self-reinforcement cycle extend beyond the models themselves,\nimpacting human language and discourse. The advantages and disadvantages of\nthis bias amplification are weighed, considering educational benefits and ease\nof future GLM learning against threats to linguistic diversity and dependence\non initial GLMs. This paper underscores the need for rigorous research to\nunderstand and address these issues. It advocates for improved model\ntransparency, bias-aware training techniques, development of methods to\ndistinguish between human and GLM-generated text, and robust measures for\nfairness and bias evaluation in GLMs. The aim is to ensure the effective, safe,\nand equitable use of these powerful technologies, while preserving the richness\nand diversity of human language.", "published": "2023-06-12 14:17:05", "link": "http://arxiv.org/abs/2306.07135v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Weakly-Supervised Scientific Document Classification via\n  Retrieval-Augmented Multi-Stage Training", "abstract": "Scientific document classification is a critical task for a wide range of\napplications, but the cost of obtaining massive amounts of human-labeled data\ncan be prohibitive. To address this challenge, we propose a weakly-supervised\napproach for scientific document classification using label names only. In\nscientific domains, label names often include domain-specific concepts that may\nnot appear in the document corpus, making it difficult to match labels and\ndocuments precisely. To tackle this issue, we propose WANDER, which leverages\ndense retrieval to perform matching in the embedding space to capture the\nsemantics of label names. We further design the label name expansion module to\nenrich the label name representations. Lastly, a self-training step is used to\nrefine the predictions. The experiments on three datasets show that WANDER\noutperforms the best baseline by 11.9% on average. Our code will be published\nat https://github.com/ritaranx/wander.", "published": "2023-06-12 15:50:13", "link": "http://arxiv.org/abs/2306.07193v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Large language models and (non-)linguistic recursion", "abstract": "Recursion is one of the hallmarks of human language. While many design\nfeatures of language have been shown to exist in animal communication systems,\nrecursion has not. Previous research shows that GPT-4 is the first large\nlanguage model (LLM) to exhibit metalinguistic abilities (Begu\\v{s},\nD\\k{a}bkowski, and Rhodes 2023). Here, we propose several prompt designs aimed\nat eliciting and analyzing recursive behavior in LLMs, both linguistic and\nnon-linguistic. We demonstrate that when explicitly prompted, GPT-4 can both\nproduce and analyze recursive structures. Thus, we present one of the first\nstudies investigating whether meta-linguistic awareness of recursion -- a\nuniquely human cognitive property -- can emerge in transformers with a high\nnumber of parameters such as GPT-4.", "published": "2023-06-12 15:50:38", "link": "http://arxiv.org/abs/2306.07195v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RECAP: Retrieval-Enhanced Context-Aware Prefix Encoder for Personalized\n  Dialogue Response Generation", "abstract": "Endowing chatbots with a consistent persona is essential to an engaging\nconversation, yet it remains an unresolved challenge. In this work, we propose\na new retrieval-enhanced approach for personalized response generation.\nSpecifically, we design a hierarchical transformer retriever trained on\ndialogue domain data to perform personalized retrieval and a context-aware\nprefix encoder that fuses the retrieved information to the decoder more\neffectively. Extensive experiments on a real-world dataset demonstrate the\neffectiveness of our model at generating more fluent and personalized\nresponses. We quantitatively evaluate our model's performance under a suite of\nhuman and automatic metrics and find it to be superior compared to\nstate-of-the-art baselines on English Reddit conversations.", "published": "2023-06-12 16:10:21", "link": "http://arxiv.org/abs/2306.07206v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Lost in Translation: Large Language Models in Non-English Content\n  Analysis", "abstract": "In recent years, large language models (e.g., Open AI's GPT-4, Meta's LLaMa,\nGoogle's PaLM) have become the dominant approach for building AI systems to\nanalyze and generate language online. However, the automated systems that\nincreasingly mediate our interactions online -- such as chatbots, content\nmoderation systems, and search engines -- are primarily designed for and work\nfar more effectively in English than in the world's other 7,000 languages.\nRecently, researchers and technology companies have attempted to extend the\ncapabilities of large language models into languages other than English by\nbuilding what are called multilingual language models.\n  In this paper, we explain how these multilingual language models work and\nexplore their capabilities and limits. Part I provides a simple technical\nexplanation of how large language models work, why there is a gap in available\ndata between English and other languages, and how multilingual language models\nattempt to bridge that gap. Part II accounts for the challenges of doing\ncontent analysis with large language models in general and multilingual\nlanguage models in particular. Part III offers recommendations for companies,\nresearchers, and policymakers to keep in mind when considering researching,\ndeveloping and deploying large and multilingual language models.", "published": "2023-06-12 19:10:47", "link": "http://arxiv.org/abs/2306.07377v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "weighted CapsuleNet networks for Persian multi-domain sentiment analysis", "abstract": "Sentiment classification is a fundamental task in natural language\nprocessing, assigning one of the three classes, positive, negative, or neutral,\nto free texts. However, sentiment classification models are highly domain\ndependent; the classifier may perform classification with reasonable accuracy\nin one domain but not in another due to the Semantic multiplicity of words\ngetting poor accuracy. This article presents a new Persian/Arabic multi-domain\nsentiment analysis method using the cumulative weighted capsule networks\napproach. Weighted capsule ensemble consists of training separate capsule\nnetworks for each domain and a weighting measure called domain belonging degree\n(DBD). This criterion consists of TF and IDF, which calculates the dependency\nof each document for each domain separately; this value is multiplied by the\npossible output that each capsule creates. In the end, the sum of these\nmultiplications is the title of the final output, and is used to determine the\npolarity. And the most dependent domain is considered the final output for each\ndomain. The proposed method was evaluated using the Digikala dataset and\nobtained acceptable accuracy compared to the existing approaches. It achieved\nan accuracy of 0.89 on detecting the domain of belonging and 0.99 on detecting\nthe polarity. Also, for the problem of dealing with unbalanced classes, a\ncost-sensitive function was used. This function was able to achieve 0.0162\nimprovements in accuracy for sentiment classification. This approach on Amazon\nArabic data can achieve 0.9695 accuracies in domain classification.", "published": "2023-06-12 09:53:29", "link": "http://arxiv.org/abs/2306.17068v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TrojLLM: A Black-box Trojan Prompt Attack on Large Language Models", "abstract": "Large Language Models (LLMs) are progressively being utilized as machine\nlearning services and interface tools for various applications. However, the\nsecurity implications of LLMs, particularly in relation to adversarial and\nTrojan attacks, remain insufficiently examined. In this paper, we propose\nTrojLLM, an automatic and black-box framework to effectively generate universal\nand stealthy triggers. When these triggers are incorporated into the input\ndata, the LLMs' outputs can be maliciously manipulated. Moreover, the framework\nalso supports embedding Trojans within discrete prompts, enhancing the overall\neffectiveness and precision of the triggers' attacks. Specifically, we propose\na trigger discovery algorithm for generating universal triggers for various\ninputs by querying victim LLM-based APIs using few-shot data samples.\nFurthermore, we introduce a novel progressive Trojan poisoning algorithm\ndesigned to generate poisoned prompts that retain efficacy and transferability\nacross a diverse range of models. Our experiments and results demonstrate\nTrojLLM's capacity to effectively insert Trojans into text prompts in\nreal-world black-box LLM APIs including GPT-3.5 and GPT-4, while maintaining\nexceptional performance on clean test sets. Our work sheds light on the\npotential security risks in current models and offers a potential defensive\napproach. The source code of TrojLLM is available at\nhttps://github.com/UCF-ML-Research/TrojLLM.", "published": "2023-06-12 01:22:39", "link": "http://arxiv.org/abs/2306.06815v3", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Multimodal Audio-textual Architecture for Robust Spoken Language\n  Understanding", "abstract": "Recent voice assistants are usually based on the cascade spoken language\nunderstanding (SLU) solution, which consists of an automatic speech recognition\n(ASR) engine and a natural language understanding (NLU) system. Because such\napproach relies on the ASR output, it often suffers from the so-called ASR\nerror propagation. In this work, we investigate impacts of this ASR error\npropagation on state-of-the-art NLU systems based on pre-trained language\nmodels (PLM), such as BERT and RoBERTa. Moreover, a multimodal language\nunderstanding (MLU) module is proposed to mitigate SLU performance degradation\ncaused by errors present in the ASR transcript. The MLU benefits from\nself-supervised features learned from both audio and text modalities,\nspecifically Wav2Vec for speech and Bert/RoBERTa for language. Our MLU combines\nan encoder network to embed the audio signal and a text encoder to process text\ntranscripts followed by a late fusion layer to fuse audio and text logits. We\nfound that the proposed MLU showed to be robust towards poor quality ASR\ntranscripts, while the performance of BERT and RoBERTa are severely\ncompromised. Our model is evaluated on five tasks from three SLU datasets and\nrobustness is tested using ASR transcripts from three ASR engines. Results show\nthat the proposed approach effectively mitigates the ASR error propagation\nproblem, surpassing the PLM models' performance across all datasets for the\nacademic ASR engine.", "published": "2023-06-12 01:55:53", "link": "http://arxiv.org/abs/2306.06819v2", "categories": ["cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Multi-View Frequency-Attention Alternative to CNN Frontends for\n  Automatic Speech Recognition", "abstract": "Convolutional frontends are a typical choice for Transformer-based automatic\nspeech recognition to preprocess the spectrogram, reduce its sequence length,\nand combine local information in time and frequency similarly. However, the\nwidth and height of an audio spectrogram denote different information, e.g.,\ndue to reverberation as well as the articulatory system, the time axis has a\nclear left-to-right dependency. On the contrary, vowels and consonants\ndemonstrate very different patterns and occupy almost disjoint frequency\nranges. Therefore, we hypothesize, global attention over frequencies is\nbeneficial over local convolution. We obtain 2.4 % relative word error rate\nreduction (rWERR) on a production scale Conformer transducer replacing its\nconvolutional neural network frontend by the proposed F-Attention module on\nAlexa traffic. To demonstrate generalizability, we validate this on public\nLibriSpeech data with a long short term memory-based listen attend and spell\narchitecture obtaining 4.6 % rWERR and demonstrate robustness to (simulated)\nnoisy conditions.", "published": "2023-06-12 08:37:36", "link": "http://arxiv.org/abs/2306.06954v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Generating Language Corrections for Teaching Physical Control Tasks", "abstract": "AI assistance continues to help advance applications in education, from\nlanguage learning to intelligent tutoring systems, yet current methods for\nproviding students feedback are still quite limited. Most automatic feedback\nsystems either provide binary correctness feedback, which may not help a\nstudent understand how to improve, or require hand-coding feedback templates,\nwhich may not generalize to new domains. This can be particularly challenging\nfor physical control tasks, where the rich diversity in student behavior and\nspecialized domains make it challenging to leverage general-purpose assistive\ntools for providing feedback. We design and build CORGI, a model trained to\ngenerate language corrections for physical control tasks, such as learning to\nride a bike. CORGI takes in as input a pair of student and expert trajectories,\nand then generates natural language corrections to help the student improve. We\ncollect and train CORGI over data from three diverse physical control tasks\n(drawing, steering, and joint movement). Through both automatic and human\nevaluations, we show that CORGI can (i) generate valid feedback for novel\nstudent trajectories, (ii) outperform baselines on domains with novel control\ndynamics, and (iii) improve student learning in an interactive drawing task.", "published": "2023-06-12 10:31:16", "link": "http://arxiv.org/abs/2306.07012v1", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.RO"], "primary_category": "cs.AI"}
{"title": "Deep Model Compression Also Helps Models Capture Ambiguity", "abstract": "Natural language understanding (NLU) tasks face a non-trivial amount of\nambiguous samples where veracity of their labels is debatable among annotators.\nNLU models should thus account for such ambiguity, but they approximate the\nhuman opinion distributions quite poorly and tend to produce over-confident\npredictions. To address this problem, we must consider how to exactly capture\nthe degree of relationship between each sample and its candidate classes. In\nthis work, we propose a novel method with deep model compression and show how\nsuch relationship can be accounted for. We see that more reasonably represented\nrelationships can be discovered in the lower layers and that validation\naccuracies are converging at these layers, which naturally leads to layer\npruning. We also see that distilling the relationship knowledge from a lower\nlayer helps models produce better distribution. Experimental results\ndemonstrate that our method makes substantial improvement on quantifying\nambiguity without gold distribution labels. As positive side-effects, our\nmethod is found to reduce the model size significantly and improve latency,\nboth attractive aspects of NLU products.", "published": "2023-06-12 12:24:47", "link": "http://arxiv.org/abs/2306.07061v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Large Language Models as Tax Attorneys: A Case Study in Legal\n  Capabilities Emergence", "abstract": "Better understanding of Large Language Models' (LLMs) legal analysis\nabilities can contribute to improving the efficiency of legal services,\ngoverning artificial intelligence, and leveraging LLMs to identify\ninconsistencies in law. This paper explores LLM capabilities in applying tax\nlaw. We choose this area of law because it has a structure that allows us to\nset up automated validation pipelines across thousands of examples, requires\nlogical reasoning and maths skills, and enables us to test LLM capabilities in\na manner relevant to real-world economic lives of citizens and companies. Our\nexperiments demonstrate emerging legal understanding capabilities, with\nimproved performance in each subsequent OpenAI model release. We experiment\nwith retrieving and utilising the relevant legal authority to assess the impact\nof providing additional legal context to LLMs. Few-shot prompting, presenting\nexamples of question-answer pairs, is also found to significantly enhance the\nperformance of the most advanced model, GPT-4. The findings indicate that LLMs,\nparticularly when combined with prompting enhancements and the correct legal\ntexts, can perform at high levels of accuracy but not yet at expert tax lawyer\nlevels. As LLMs continue to advance, their ability to reason about law\nautonomously could have significant implications for the legal profession and\nAI governance.", "published": "2023-06-12 12:40:48", "link": "http://arxiv.org/abs/2306.07075v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Linear Classifier: An Often-Forgotten Baseline for Text Classification", "abstract": "Large-scale pre-trained language models such as BERT are popular solutions\nfor text classification. Due to the superior performance of these advanced\nmethods, nowadays, people often directly train them for a few epochs and deploy\nthe obtained model. In this opinion paper, we point out that this way may only\nsometimes get satisfactory results. We argue the importance of running a simple\nbaseline like linear classifiers on bag-of-words features along with advanced\nmethods. First, for many text data, linear methods show competitive\nperformance, high efficiency, and robustness. Second, advanced models such as\nBERT may only achieve the best results if properly applied. Simple baselines\nhelp to confirm whether the results of advanced models are acceptable. Our\nexperimental results fully support these points.", "published": "2023-06-12 13:39:54", "link": "http://arxiv.org/abs/2306.07111v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exploring Attention Mechanisms for Multimodal Emotion Recognition in an\n  Emergency Call Center Corpus", "abstract": "The emotion detection technology to enhance human decision-making is an\nimportant research issue for real-world applications, but real-life emotion\ndatasets are relatively rare and small. The experiments conducted in this paper\nuse the CEMO, which was collected in a French emergency call center. Two\npre-trained models based on speech and text were fine-tuned for speech emotion\nrecognition. Using pre-trained Transformer encoders mitigates our data's\nlimited and sparse nature. This paper explores the different fusion strategies\nof these modality-specific models. In particular, fusions with and without\ncross-attention mechanisms were tested to gather the most relevant information\nfrom both the speech and text encoders. We show that multimodal fusion brings\nan absolute gain of 4-9% with respect to either single modality and that the\nSymmetric multi-headed cross-attention mechanism performed better than late\nclassical fusion approaches. Our experiments also suggest that for the\nreal-life CEMO corpus, the audio component encodes more emotive information\nthan the textual one.", "published": "2023-06-12 13:43:20", "link": "http://arxiv.org/abs/2306.07115v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Language of Bargaining", "abstract": "Leveraging an established exercise in negotiation education, we build a novel\ndataset for studying how the use of language shapes bilateral bargaining. Our\ndataset extends existing work in two ways: 1) we recruit participants via\nbehavioral labs instead of crowdsourcing platforms and allow participants to\nnegotiate through audio, enabling more naturalistic interactions; 2) we add a\ncontrol setting where participants negotiate only through alternating, written\nnumeric offers. Despite the two contrasting forms of communication, we find\nthat the average agreed prices of the two treatments are identical. But when\nsubjects can talk, fewer offers are exchanged, negotiations finish faster, the\nlikelihood of reaching agreement rises, and the variance of prices at which\nsubjects agree drops substantially. We further propose a taxonomy of speech\nacts in negotiation and enrich the dataset with annotated speech acts. Our work\nalso reveals linguistic signals that are predictive of negotiation outcomes.", "published": "2023-06-12 13:52:01", "link": "http://arxiv.org/abs/2306.07117v2", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Valley: Video Assistant with Large Language model Enhanced abilitY", "abstract": "Large Language Models (LLMs), with remarkable conversational capability, have\nemerged as AI assistants that can handle both visual and textual modalities.\nHowever, their effectiveness in joint video and language understanding has not\nbeen extensively explored. In the paper, we introduce Valley, a multi-modal\nfoundation model that is designed to enable enhanced video comprehension and\ninstruction-following capabilities. To this end, we construct two datasets,\nnamely Valley-702k and Valley-instruct-73k, to cover a diverse range of\nvideo-text alignment and video-based instruction tasks, such as multi-shot\ncaptions, long video descriptions, action recognition, causal inference, etc.\nThen, we adopt ViT-L/14 as the vision encoder and explore three different\ntemporal modeling modules to learn multifaceted features for enhanced video\nunderstanding. In addition, we implement a two-phase training approach for\nValley: the first phase focuses solely on training the projection module to\nfacilitate the LLM's capacity to understand visual input, and the second phase\njointly trains the projection module and the LLM to improve their instruction\nfollowing ability. Extensive experiments demonstrate that Valley has the\npotential to serve as an effective video assistant, simplifying complex\nvideo-understanding scenarios. Our code and data are published anonymously at\nhttps://github.com/valley-vl/Valley.", "published": "2023-06-12 16:11:10", "link": "http://arxiv.org/abs/2306.07207v3", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Data-Copilot: Bridging Billions of Data and Humans with Autonomous\n  Workflow", "abstract": "Industries such as finance, meteorology, and energy generate vast amounts of\ndata daily. Efficiently managing, processing, and displaying this data requires\nspecialized expertise and is often tedious and repetitive. Leveraging large\nlanguage models (LLMs) to develop an automated workflow presents a highly\npromising solution. However, LLMs are not adept at handling complex numerical\ncomputations and table manipulations and are also constrained by a limited\ncontext budget. Based on this, we propose Data-Copilot, a data analysis agent\nthat autonomously performs querying, processing, and visualization of massive\ndata tailored to diverse human requests. The advancements are twofold: First,\nit is a code-centric agent that receives human requests and generates code as\nan intermediary to handle massive data, which is quite flexible for large-scale\ndata processing tasks. Second, Data-Copilot involves a data exploration phase\nin advance, which explores how to design more universal and error-free\ninterfaces for real-time response. Specifically, it actively explores data\nsources, discovers numerous common requests, and abstracts them into many\nuniversal interfaces for daily invocation. When deployed in real-time requests,\nData-Copilot only needs to invoke these pre-designed interfaces, transforming\nraw data into visualized outputs (e.g., charts, tables) that best match the\nuser's intent. Compared to generating code from scratch, invoking these\npre-designed and compiler-validated interfaces can significantly reduce errors\nduring real-time requests. Additionally, interface workflows are more efficient\nand offer greater interpretability than code. We open-sourced Data-Copilot with\nmassive Chinese financial data, such as stocks, funds, and news, demonstrating\npromising application prospects.", "published": "2023-06-12 16:12:56", "link": "http://arxiv.org/abs/2306.07209v7", "categories": ["cs.CL", "cs.AI", "cs.CE"], "primary_category": "cs.CL"}
{"title": "MSSRNet: Manipulating Sequential Style Representation for Unsupervised\n  Text Style Transfer", "abstract": "Unsupervised text style transfer task aims to rewrite a text into target\nstyle while preserving its main content. Traditional methods rely on the use of\na fixed-sized vector to regulate text style, which is difficult to accurately\nconvey the style strength for each individual token. In fact, each token of a\ntext contains different style intensity and makes different contribution to the\noverall style. Our proposed method addresses this issue by assigning individual\nstyle vector to each token in a text, allowing for fine-grained control and\nmanipulation of the style strength. Additionally, an adversarial training\nframework integrated with teacher-student learning is introduced to enhance\ntraining stability and reduce the complexity of high-dimensional optimization.\nThe results of our experiments demonstrate the efficacy of our method in terms\nof clearly improved style transfer accuracy and content preservation in both\ntwo-style transfer and multi-style transfer settings.", "published": "2023-06-12 13:12:29", "link": "http://arxiv.org/abs/2306.07994v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MFSN: Multi-perspective Fusion Search Network For Pre-training Knowledge\n  in Speech Emotion Recognition", "abstract": "Speech Emotion Recognition (SER) is an important research topic in\nhuman-computer interaction. Many recent works focus on directly extracting\nemotional cues through pre-trained knowledge, frequently overlooking\nconsiderations of appropriateness and comprehensiveness. Therefore, we propose\na novel framework for pre-training knowledge in SER, called Multi-perspective\nFusion Search Network (MFSN). Considering comprehensiveness, we partition\nspeech knowledge into Textual-related Emotional Content (TEC) and\nSpeech-related Emotional Content (SEC), capturing cues from both semantic and\nacoustic perspectives, and we design a new architecture search space to fully\nleverage them. Considering appropriateness, we verify the efficacy of different\nmodeling approaches in capturing SEC and fills the gap in current research.\nExperimental results on multiple datasets demonstrate the superiority of MFSN.", "published": "2023-06-12 16:40:07", "link": "http://arxiv.org/abs/2306.09361v3", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Practical Entity Linking System for Tables in Scientific Literature", "abstract": "Entity linking is an important step towards constructing knowledge graphs\nthat facilitate advanced question answering over scientific documents,\nincluding the retrieval of relevant information included in tables within these\ndocuments. This paper introduces a general-purpose system for linking entities\nto items in the Wikidata knowledge base. It describes how we adapt this system\nfor linking domain-specific entities, especially for those entities embedded\nwithin tables drawn from COVID-19-related scientific literature. We describe\nthe setup of an efficient offline instance of the system that enables our\nentity-linking approach to be more feasible in practice. As part of a broader\napproach to infer the semantic meaning of scientific tables, we leverage the\nstructural and semantic characteristics of the tables to improve overall entity\nlinking performance.", "published": "2023-06-12 01:40:57", "link": "http://arxiv.org/abs/2306.10044v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "When Do Annotator Demographics Matter? Measuring the Influence of\n  Annotator Demographics with the POPQUORN Dataset", "abstract": "Annotators are not fungible. Their demographics, life experiences, and\nbackgrounds all contribute to how they label data. However, NLP has only\nrecently considered how annotator identity might influence their decisions.\nHere, we present POPQUORN (the POtato-Prolific dataset for QUestion-Answering,\nOffensiveness, text Rewriting, and politeness rating with demographic Nuance).\nPOPQUORN contains 45,000 annotations from 1,484 annotators, drawn from a\nrepresentative sample regarding sex, age, and race as the US population.\nThrough a series of analyses, we show that annotators' background plays a\nsignificant role in their judgments. Further, our work shows that backgrounds\nnot previously considered in NLP (e.g., education), are meaningful and should\nbe considered. Our study suggests that understanding the background of\nannotators and collecting labels from a demographically balanced pool of crowd\nworkers is important to reduce the bias of datasets. The dataset, annotator\nbackground, and annotation interface are available at\nhttps://github.com/Jiaxin-Pei/potato-prolific-dataset .", "published": "2023-06-12 02:26:00", "link": "http://arxiv.org/abs/2306.06826v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "HiddenSinger: High-Quality Singing Voice Synthesis via Neural Audio\n  Codec and Latent Diffusion Models", "abstract": "Recently, denoising diffusion models have demonstrated remarkable performance\namong generative models in various domains. However, in the speech domain, the\napplication of diffusion models for synthesizing time-varying audio faces\nlimitations in terms of complexity and controllability, as speech synthesis\nrequires very high-dimensional samples with long-term acoustic features. To\nalleviate the challenges posed by model complexity in singing voice synthesis,\nwe propose HiddenSinger, a high-quality singing voice synthesis system using a\nneural audio codec and latent diffusion models. To ensure high-fidelity audio,\nwe introduce an audio autoencoder that can encode audio into an audio codec as\na compressed representation and reconstruct the high-fidelity audio from the\nlow-dimensional compressed latent vector. Subsequently, we use the latent\ndiffusion models to sample a latent representation from a musical score. In\naddition, our proposed model is extended to an unsupervised singing voice\nlearning framework, HiddenSinger-U, to train the model using an unlabeled\nsinging voice dataset. Experimental results demonstrate that our model\noutperforms previous models in terms of audio quality. Furthermore, the\nHiddenSinger-U can synthesize high-quality singing voices of speakers trained\nsolely on unlabeled data.", "published": "2023-06-12 01:21:41", "link": "http://arxiv.org/abs/2306.06814v1", "categories": ["eess.AS", "cs.AI", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Parameter-efficient Dysarthric Speech Recognition Using Adapter Fusion\n  and Householder Transformation", "abstract": "In dysarthric speech recognition, data scarcity and the vast diversity\nbetween dysarthric speakers pose significant challenges. While finetuning has\nbeen a popular solution, it can lead to overfitting and low parameter\nefficiency. Adapter modules offer a better solution, with their small size and\neasy applicability. Additionally, Adapter Fusion can facilitate knowledge\ntransfer from multiple learned adapters, but may employ more parameters. In\nthis work, we apply Adapter Fusion for target speaker adaptation and speech\nrecognition, achieving acceptable accuracy with significantly fewer\nspeaker-specific trainable parameters than classical finetuning methods. We\nfurther improve the parameter efficiency of the fusion layer by reducing the\nsize of query and key layers and using Householder transformation to\nreparameterize the value linear layer. Our proposed fusion layer achieves\ncomparable recognition results to the original method with only one third of\nthe parameters.", "published": "2023-06-12 13:06:49", "link": "http://arxiv.org/abs/2306.07090v1", "categories": ["eess.AS", "cs.SD", "q-bio.QM"], "primary_category": "eess.AS"}
{"title": "Video-to-Music Recommendation using Temporal Alignment of Segments", "abstract": "We study cross-modal recommendation of music tracks to be used as soundtracks\nfor videos. This problem is known as the music supervision task. We build on a\nself-supervised system that learns a content association between music and\nvideo. In addition to the adequacy of content, adequacy of structure is crucial\nin music supervision to obtain relevant recommendations. We propose a novel\napproach to significantly improve the system's performance using\nstructure-aware recommendation. The core idea is to consider not only the full\naudio-video clips, but rather shorter segments for training and inference. We\nfind that using semantic segments and ranking the tracks according to sequence\nalignment costs significantly improves the results. We investigate the impact\nof different ranking metrics and segmentation methods.", "published": "2023-06-12 15:40:31", "link": "http://arxiv.org/abs/2306.07187v1", "categories": ["cs.MM", "cs.IR", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
