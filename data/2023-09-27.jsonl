{"title": "ChatCounselor: A Large Language Models for Mental Health Support", "abstract": "This paper presents ChatCounselor, a large language model (LLM) solution\ndesigned to provide mental health support. Unlike generic chatbots,\nChatCounselor is distinguished by its foundation in real conversations between\nconsulting clients and professional psychologists, enabling it to possess\nspecialized knowledge and counseling skills in the field of psychology. The\ntraining dataset, Psych8k, was constructed from 260 in-depth interviews, each\nspanning an hour. To assess the quality of counseling responses, the counseling\nBench was devised. Leveraging GPT-4 and meticulously crafted prompts based on\nseven metrics of psychological counseling assessment, the model underwent\nevaluation using a set of real-world counseling questions. Impressively,\nChatCounselor surpasses existing open-source models in the counseling Bench and\napproaches the performance level of ChatGPT, showcasing the remarkable\nenhancement in model capability attained through high-quality domain-specific\ndata.", "published": "2023-09-27 07:57:21", "link": "http://arxiv.org/abs/2309.15461v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dynamic Multi-Scale Context Aggregation for Conversational Aspect-Based\n  Sentiment Quadruple Analysis", "abstract": "Conversational aspect-based sentiment quadruple analysis (DiaASQ) aims to\nextract the quadruple of target-aspect-opinion-sentiment within a dialogue. In\nDiaASQ, a quadruple's elements often cross multiple utterances. This situation\ncomplicates the extraction process, emphasizing the need for an adequate\nunderstanding of conversational context and interactions. However, existing\nwork independently encodes each utterance, thereby struggling to capture\nlong-range conversational context and overlooking the deep inter-utterance\ndependencies. In this work, we propose a novel Dynamic Multi-scale Context\nAggregation network (DMCA) to address the challenges. Specifically, we first\nutilize dialogue structure to generate multi-scale utterance windows for\ncapturing rich contextual information. After that, we design a Dynamic\nHierarchical Aggregation module (DHA) to integrate progressive cues between\nthem. In addition, we form a multi-stage loss strategy to improve model\nperformance and generalization ability. Extensive experimental results show\nthat the DMCA model outperforms baselines significantly and achieves\nstate-of-the-art performance.", "published": "2023-09-27 08:17:28", "link": "http://arxiv.org/abs/2309.15476v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Few-Shot Multi-Label Aspect Category Detection Utilizing Prototypical\n  Network with Sentence-Level Weighting and Label Augmentation", "abstract": "Multi-label aspect category detection is intended to detect multiple aspect\ncategories occurring in a given sentence. Since aspect category detection often\nsuffers from limited datasets and data sparsity, the prototypical network with\nattention mechanisms has been applied for few-shot aspect category detection.\nNevertheless, most of the prototypical networks used so far calculate the\nprototypes by taking the mean value of all the instances in the support set.\nThis seems to ignore the variations between instances in multi-label aspect\ncategory detection. Also, several related works utilize label text information\nto enhance the attention mechanism. However, the label text information is\noften short and limited, and not specific enough to discern categories. In this\npaper, we first introduce support set attention along with the augmented label\ninformation to mitigate the noise at word-level for each support set instance.\nMoreover, we use a sentence-level attention mechanism that gives different\nweights to each instance in the support set in order to compute prototypes by\nweighted averaging. Finally, the calculated prototypes are further used in\nconjunction with query instances to compute query attention and thereby\neliminate noises from the query set. Experimental results on the Yelp dataset\nshow that our proposed method is useful and outperforms all baselines in four\ndifferent scenarios.", "published": "2023-09-27 11:44:04", "link": "http://arxiv.org/abs/2309.15588v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NLPBench: Evaluating Large Language Models on Solving NLP Problems", "abstract": "Recent developments in large language models (LLMs) have shown promise in\nenhancing the capabilities of natural language processing (NLP). Despite these\nsuccesses, there remains a dearth of research dedicated to the NLP\nproblem-solving abilities of LLMs. To fill the gap in this area, we present a\nunique benchmarking dataset, NLPBench, comprising 378 college-level NLP\nquestions spanning various NLP topics sourced from Yale University's prior\nfinal exams. NLPBench includes questions with context, in which multiple\nsub-questions share the same public information, and diverse question types,\nincluding multiple choice, short answer, and math. Our evaluation, centered on\nLLMs such as GPT-3.5/4, PaLM-2, and LLAMA-2, incorporates advanced prompting\nstrategies like the chain-of-thought (CoT) and tree-of-thought (ToT). Our study\nreveals that the effectiveness of the advanced prompting strategies can be\ninconsistent, occasionally damaging LLM performance, especially in smaller\nmodels like the LLAMA-2 (13b). Furthermore, our manual assessment illuminated\nspecific shortcomings in LLMs' scientific problem-solving skills, with\nweaknesses in logical decomposition and reasoning notably affecting results.", "published": "2023-09-27 13:02:06", "link": "http://arxiv.org/abs/2309.15630v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Conversational Feedback in Scripted versus Spontaneous Dialogues: A\n  Comparative Analysis", "abstract": "Scripted dialogues such as movie and TV subtitles constitute a widespread\nsource of training data for conversational NLP models. However, there are\nnotable linguistic differences between these dialogues and spontaneous\ninteractions, especially regarding the occurrence of communicative feedback\nsuch as backchannels, acknowledgments, or clarification requests. This paper\npresents a quantitative analysis of such feedback phenomena in both subtitles\nand spontaneous conversations. Based on conversational data spanning eight\nlanguages and multiple genres, we extract lexical statistics, classifications\nfrom a dialogue act tagger, expert annotations and labels derived from a\nfine-tuned Large Language Model (LLM). Our main empirical findings are that (1)\ncommunicative feedback is markedly less frequent in subtitles than in\nspontaneous dialogues and (2) subtitles contain a higher proportion of negative\nfeedback. We also show that dialogues generated by standard LLMs lie much\ncloser to scripted dialogues than spontaneous interactions in terms of\ncommunicative feedback.", "published": "2023-09-27 13:45:38", "link": "http://arxiv.org/abs/2309.15656v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Question answering using deep learning in low resource Indian language\n  Marathi", "abstract": "Precise answers are extracted from a text for a given input question in a\nquestion answering system. Marathi question answering system is created in\nrecent studies by using ontology, rule base and machine learning based\napproaches. Recently transformer models and transfer learning approaches are\nused to solve question answering challenges. In this paper we investigate\ndifferent transformer models for creating a reading comprehension-based Marathi\nquestion answering system. We have experimented on different pretrained Marathi\nlanguage multilingual and monolingual models like Multilingual Representations\nfor Indian Languages (MuRIL), MahaBERT, Indic Bidirectional Encoder\nRepresentations from Transformers (IndicBERT) and fine-tuned it on a Marathi\nreading comprehension-based data set. We got the best accuracy in a MuRIL\nmultilingual model with an EM score of 0.64 and F1 score of 0.74 by fine tuning\nthe model on the Marathi dataset.", "published": "2023-09-27 16:53:11", "link": "http://arxiv.org/abs/2309.15779v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Effective Long-Context Scaling of Foundation Models", "abstract": "We present a series of long-context LLMs that support effective context\nwindows of up to 32,768 tokens. Our model series are built through continual\npretraining from Llama 2 with longer training sequences and on a dataset where\nlong texts are upsampled. We perform extensive evaluation on language modeling,\nsynthetic context probing tasks, and a wide range of research benchmarks. On\nresearch benchmarks, our models achieve consistent improvements on most regular\ntasks and significant improvements on long-context tasks over Llama 2. Notably,\nwith a cost-effective instruction tuning procedure that does not require\nhuman-annotated long instruction data, the 70B variant can already surpass\ngpt-3.5-turbo-16k's overall performance on a suite of long-context tasks.\nAlongside these results, we provide an in-depth analysis on the individual\ncomponents of our method. We delve into Llama's position encodings and discuss\nits limitation in modeling long dependencies. We also examine the impact of\nvarious design choices in the pretraining process, including the data mix and\nthe training curriculum of sequence lengths -- our ablation experiments suggest\nthat having abundant long texts in the pretrain dataset is not the key to\nachieving strong performance, and we empirically verify that long context\ncontinual pretraining is more efficient and similarly effective compared to\npretraining from scratch with long sequences.", "published": "2023-09-27 21:41:49", "link": "http://arxiv.org/abs/2309.16039v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond the Chat: Executable and Verifiable Text-Editing with LLMs", "abstract": "Conversational interfaces powered by Large Language Models (LLMs) have\nrecently become a popular way to obtain feedback during document editing.\nHowever, standard chat-based conversational interfaces do not support\ntransparency and verifiability of the editing changes that they suggest. To\ngive the author more agency when editing with an LLM, we present InkSync, an\nediting interface that suggests executable edits directly within the document\nbeing edited. Because LLMs are known to introduce factual errors, Inksync also\nsupports a 3-stage approach to mitigate this risk: Warn authors when a\nsuggested edit introduces new information, help authors Verify the new\ninformation's accuracy through external search, and allow an auditor to perform\nan a-posteriori verification by Auditing the document via a trace of all\nauto-generated content. Two usability studies confirm the effectiveness of\nInkSync's components when compared to standard LLM-based chat interfaces,\nleading to more accurate, more efficient editing, and improved user experience.", "published": "2023-09-27 00:56:17", "link": "http://arxiv.org/abs/2309.15337v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Navigate through Enigmatic Labyrinth A Survey of Chain of Thought\n  Reasoning: Advances, Frontiers and Future", "abstract": "Reasoning, a fundamental cognitive process integral to human intelligence,\nhas garnered substantial interest within artificial intelligence. Notably,\nrecent studies have revealed that chain-of-thought prompting significantly\nenhances LLM's reasoning capabilities, which attracts widespread attention from\nboth academics and industry. In this paper, we systematically investigate\nrelevant research, summarizing advanced methods through a meticulous taxonomy\nthat offers novel perspectives. Moreover, we delve into the current frontiers\nand delineate the challenges and future directions, thereby shedding light on\nfuture research. Furthermore, we engage in a discussion about open questions.\nWe hope this paper serves as an introduction for beginners and fosters future\nresearch. Resources have been made publicly available at\nhttps://github.com/zchuz/CoT-Reasoning-Survey", "published": "2023-09-27 04:53:10", "link": "http://arxiv.org/abs/2309.15402v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "VideoAdviser: Video Knowledge Distillation for Multimodal Transfer\n  Learning", "abstract": "Multimodal transfer learning aims to transform pretrained representations of\ndiverse modalities into a common domain space for effective multimodal fusion.\nHowever, conventional systems are typically built on the assumption that all\nmodalities exist, and the lack of modalities always leads to poor inference\nperformance. Furthermore, extracting pretrained embeddings for all modalities\nis computationally inefficient for inference. In this work, to achieve high\nefficiency-performance multimodal transfer learning, we propose VideoAdviser, a\nvideo knowledge distillation method to transfer multimodal knowledge of\nvideo-enhanced prompts from a multimodal fundamental model (teacher) to a\nspecific modal fundamental model (student). With an intuition that the best\nlearning performance comes with professional advisers and smart students, we\nuse a CLIP-based teacher model to provide expressive multimodal knowledge\nsupervision signals to a RoBERTa-based student model via optimizing a\nstep-distillation objective loss -- first step: the teacher distills multimodal\nknowledge of video-enhanced prompts from classification logits to a regression\nlogit -- second step: the multimodal knowledge is distilled from the regression\nlogit of the teacher to the student. We evaluate our method in two challenging\nmultimodal tasks: video-level sentiment analysis (MOSI and MOSEI datasets) and\naudio-visual retrieval (VEGAS dataset). The student (requiring only the text\nmodality as input) achieves an MAE score improvement of up to 12.3% for MOSI\nand MOSEI. Our method further enhances the state-of-the-art method by 3.4% mAP\nscore for VEGAS without additional computations for inference. These results\nsuggest the strengths of our method for achieving high efficiency-performance\nmultimodal transfer learning.", "published": "2023-09-27 08:44:04", "link": "http://arxiv.org/abs/2309.15494v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "MONOVAB : An Annotated Corpus for Bangla Multi-label Emotion Detection", "abstract": "In recent years, Sentiment Analysis (SA) and Emotion Recognition (ER) have\nbeen increasingly popular in the Bangla language, which is the seventh most\nspoken language throughout the entire world. However, the language is\nstructurally complicated, which makes this field arduous to extract emotions in\nan accurate manner. Several distinct approaches such as the extraction of\npositive and negative sentiments as well as multiclass emotions, have been\nimplemented in this field of study. Nevertheless, the extraction of multiple\nsentiments is an almost untouched area in this language. Which involves\nidentifying several feelings based on a single piece of text. Therefore, this\nstudy demonstrates a thorough method for constructing an annotated corpus based\non scrapped data from Facebook to bridge the gaps in this subject area to\novercome the challenges. To make this annotation more fruitful, the\ncontext-based approach has been used. Bidirectional Encoder Representations\nfrom Transformers (BERT), a well-known methodology of transformers, have been\nshown the best results of all methods implemented. Finally, a web application\nhas been developed to demonstrate the performance of the pre-trained\ntop-performer model (BERT) for multi-label ER in Bangla.", "published": "2023-09-27 14:10:57", "link": "http://arxiv.org/abs/2309.15670v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Integrating LLM, EEG, and Eye-Tracking Biomarker Analysis for Word-Level\n  Neural State Classification in Semantic Inference Reading Comprehension", "abstract": "With the recent proliferation of large language models (LLMs), such as\nGenerative Pre-trained Transformers (GPT), there has been a significant shift\nin exploring human and machine comprehension of semantic language meaning. This\nshift calls for interdisciplinary research that bridges cognitive science and\nnatural language processing (NLP). This pilot study aims to provide insights\ninto individuals' neural states during a semantic relation\nreading-comprehension task. We propose jointly analyzing LLMs, eye-gaze, and\nelectroencephalographic (EEG) data to study how the brain processes words with\nvarying degrees of relevance to a keyword during reading. We also use a feature\nengineering approach to improve the fixation-related EEG data classification\nwhile participants read words with high versus low relevance to the keyword.\nThe best validation accuracy in this word-level classification is over 60\\%\nacross 12 subjects. Words of high relevance to the inference keyword had\nsignificantly more eye fixations per word: 1.0584 compared to 0.6576 when\nexcluding no-fixation words, and 1.5126 compared to 1.4026 when including them.\nThis study represents the first attempt to classify brain states at a word\nlevel using LLM knowledge. It provides valuable insights into human cognitive\nabilities and the realm of Artificial General Intelligence (AGI), and offers\nguidance for developing potential reading-assisted technologies.", "published": "2023-09-27 15:12:08", "link": "http://arxiv.org/abs/2309.15714v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Experience and Evidence are the eyes of an excellent summarizer! Towards\n  Knowledge Infused Multi-modal Clinical Conversation Summarization", "abstract": "With the advancement of telemedicine, both researchers and medical\npractitioners are working hand-in-hand to develop various techniques to\nautomate various medical operations, such as diagnosis report generation. In\nthis paper, we first present a multi-modal clinical conversation summary\ngeneration task that takes a clinician-patient interaction (both textual and\nvisual information) and generates a succinct synopsis of the conversation. We\npropose a knowledge-infused, multi-modal, multi-tasking medical domain\nidentification and clinical conversation summary generation\n(MM-CliConSummation) framework. It leverages an adapter to infuse knowledge and\nvisual features and unify the fused feature vector using a gated mechanism.\nFurthermore, we developed a multi-modal, multi-intent clinical conversation\nsummarization corpus annotated with intent, symptom, and summary. The extensive\nset of experiments, both quantitatively and qualitatively, led to the following\nfindings: (a) critical significance of visuals, (b) more precise and medical\nentity preserving summary with additional knowledge infusion, and (c) a\ncorrelation between medical department identification and clinical synopsis\ngeneration. Furthermore, the dataset and source code are available at\nhttps://github.com/NLP-RL/MM-CliConSummation.", "published": "2023-09-27 15:49:43", "link": "http://arxiv.org/abs/2309.15739v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Language Model Routing with Benchmark Datasets", "abstract": "There is a rapidly growing number of open-source Large Language Models (LLMs)\nand benchmark datasets to compare them. While some models dominate these\nbenchmarks, no single model typically achieves the best accuracy in all tasks\nand use cases. In this work, we address the challenge of selecting the best LLM\nout of a collection of models for new tasks. We propose a new formulation for\nthe problem, in which benchmark datasets are repurposed to learn a \"router\"\nmodel for this LLM selection, and we show that this problem can be reduced to a\ncollection of binary classification tasks. We demonstrate the utility and\nlimitations of learning model routers from various benchmark datasets, where we\nconsistently improve performance upon using any single model for all tasks.", "published": "2023-09-27 17:08:40", "link": "http://arxiv.org/abs/2309.15789v1", "categories": ["cs.CL", "cs.LG", "I.2.7, I.2.6"], "primary_category": "cs.CL"}
{"title": "Lyra: Orchestrating Dual Correction in Automated Theorem Proving", "abstract": "Large Language Models (LLMs) present an intriguing avenue for exploration in\nthe field of formal theorem proving. Nevertheless, their full potential,\nparticularly concerning the mitigation of hallucinations and refinement through\nprover error messages, remains an area that has yet to be thoroughly\ninvestigated. To enhance the effectiveness of LLMs in the field, we introduce\nthe Lyra, a new framework that employs two distinct correction mechanisms: Tool\nCorrection (TC) and Conjecture Correction (CC). To implement Tool Correction in\nthe post-processing of formal proofs, we leverage prior knowledge to utilize\npredefined prover tools (e.g., Sledgehammer) for guiding the replacement of\nincorrect tools. Tool Correction significantly contributes to mitigating\nhallucinations, thereby improving the overall accuracy of the proof. In\naddition, we introduce Conjecture Correction, an error feedback mechanism\ndesigned to interact with prover to refine formal proof conjectures with prover\nerror messages. Compared to the previous refinement framework, the proposed\nConjecture Correction refines generation with instruction but does not collect\npaired (generation, error & refinement) prompts. Our method has achieved\nstate-of-the-art (SOTA) performance on both miniF2F validation (48.0% -> 55.3%)\nand test (45.5% -> 51.2%). We also present 3 IMO problems solved by Lyra. We\nbelieve Tool Correction (post-process for hallucination mitigation) and\nConjecture Correction (subgoal adjustment from interaction with environment)\ncould provide a promising avenue for future research in this field.", "published": "2023-09-27 17:29:41", "link": "http://arxiv.org/abs/2309.15806v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Targeted Image Data Augmentation Increases Basic Skills Captioning\n  Robustness", "abstract": "Artificial neural networks typically struggle in generalizing to\nout-of-context examples. One reason for this limitation is caused by having\ndatasets that incorporate only partial information regarding the potential\ncorrelational structure of the world. In this work, we propose TIDA (Targeted\nImage-editing Data Augmentation), a targeted data augmentation method focused\non improving models' human-like abilities (e.g., gender recognition) by filling\nthe correlational structure gap using a text-to-image generative model. More\nspecifically, TIDA identifies specific skills in captions describing images\n(e.g., the presence of a specific gender in the image), changes the caption\n(e.g., \"woman\" to \"man\"), and then uses a text-to-image model to edit the image\nin order to match the novel caption (e.g., uniquely changing a woman to a man\nwhile maintaining the context identical). Based on the Flickr30K benchmark, we\nshow that, compared with the original data set, a TIDA-enhanced dataset related\nto gender, color, and counting abilities induces better performance in several\nimage captioning metrics. Furthermore, on top of relying on the classical BLEU\nmetric, we conduct a fine-grained analysis of the improvements of our models\nagainst the baseline in different ways. We compared text-to-image generative\nmodels and found different behaviors of the image captioning models in terms of\nencoding visual encoding and textual decoding.", "published": "2023-09-27 20:12:41", "link": "http://arxiv.org/abs/2309.15991v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "MKRAG: Medical Knowledge Retrieval Augmented Generation for Medical\n  Question Answering", "abstract": "Large Language Models (LLMs), although powerful in general domains, often\nperform poorly on domain-specific tasks such as medical question answering\n(QA). In addition, LLMs tend to function as \"black-boxes\", making it\nchallenging to modify their behavior. To address the problem, our work employs\na transparent process of retrieval augmented generation (RAG), aiming to\nimprove LLM responses without the need for fine-tuning or retraining.\nSpecifically, we propose a comprehensive retrieval strategy to extract medical\nfacts from an external knowledge base, and then inject them into the LLM's\nquery prompt. Focusing on medical QA, we evaluate the impact of different\nretrieval models and the number of facts on LLM performance using the\nMedQA-SMILE dataset. Notably, our retrieval-augmented Vicuna-7B model exhibited\nan accuracy improvement from 44.46% to 48.54%. This work underscores the\npotential of RAG to enhance LLM performance, offering a practical approach to\nmitigate the challenges posed by black-box LLMs.", "published": "2023-09-27 21:26:03", "link": "http://arxiv.org/abs/2309.16035v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Identifying and Mitigating Privacy Risks Stemming from Language Models:\n  A Survey", "abstract": "Large Language Models (LLMs) have shown greatly enhanced performance in\nrecent years, attributed to increased size and extensive training data. This\nadvancement has led to widespread interest and adoption across industries and\nthe public. However, training data memorization in Machine Learning models\nscales with model size, particularly concerning for LLMs. Memorized text\nsequences have the potential to be directly leaked from LLMs, posing a serious\nthreat to data privacy. Various techniques have been developed to attack LLMs\nand extract their training data. As these models continue to grow, this issue\nbecomes increasingly critical. To help researchers and policymakers understand\nthe state of knowledge around privacy attacks and mitigations, including where\nmore work is needed, we present the first SoK on data privacy for LLMs. We (i)\nidentify a taxonomy of salient dimensions where attacks differ on LLMs, (ii)\nsystematize existing attacks, using our taxonomy of dimensions to highlight key\ntrends, (iii) survey existing mitigation strategies, highlighting their\nstrengths and limitations, and (iv) identify key gaps, demonstrating open\nproblems and areas for concern.", "published": "2023-09-27 15:15:23", "link": "http://arxiv.org/abs/2310.01424v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Graph Neural Prompting with Large Language Models", "abstract": "Large language models (LLMs) have shown remarkable generalization capability\nwith exceptional performance in various language modeling tasks. However, they\nstill exhibit inherent limitations in precisely capturing and returning\ngrounded knowledge. While existing work has explored utilizing knowledge graphs\n(KGs) to enhance language modeling via joint training and customized model\narchitectures, applying this to LLMs is problematic owing to their large number\nof parameters and high computational cost. Therefore, how to enhance\npre-trained LLMs using grounded knowledge, e.g., retrieval-augmented\ngeneration, remains an open question. In this work, we propose Graph Neural\nPrompting (GNP), a novel plug-and-play method to assist pre-trained LLMs in\nlearning beneficial knowledge from KGs. GNP encompasses various designs,\nincluding a standard graph neural network encoder, a cross-modality pooling\nmodule, a domain projector, and a self-supervised link prediction objective.\nExtensive experiments on multiple datasets demonstrate the superiority of GNP\non both commonsense and biomedical reasoning tasks across different LLM sizes\nand settings. Code is available at https://github.com/meettyj/GNP.", "published": "2023-09-27 06:33:29", "link": "http://arxiv.org/abs/2309.15427v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "High-Fidelity Speech Synthesis with Minimal Supervision: All Using\n  Diffusion Models", "abstract": "Text-to-speech (TTS) methods have shown promising results in voice cloning,\nbut they require a large number of labeled text-speech pairs.\nMinimally-supervised speech synthesis decouples TTS by combining two types of\ndiscrete speech representations(semantic \\& acoustic) and using two\nsequence-to-sequence tasks to enable training with minimal supervision.\nHowever, existing methods suffer from information redundancy and dimension\nexplosion in semantic representation, and high-frequency waveform distortion in\ndiscrete acoustic representation. Autoregressive frameworks exhibit typical\ninstability and uncontrollability issues. And non-autoregressive frameworks\nsuffer from prosodic averaging caused by duration prediction models. To address\nthese issues, we propose a minimally-supervised high-fidelity speech synthesis\nmethod, where all modules are constructed based on the diffusion models. The\nnon-autoregressive framework enhances controllability, and the duration\ndiffusion model enables diversified prosodic expression. Contrastive\nToken-Acoustic Pretraining (CTAP) is used as an intermediate semantic\nrepresentation to solve the problems of information redundancy and dimension\nexplosion in existing semantic coding methods. Mel-spectrogram is used as the\nacoustic representation. Both semantic and acoustic representations are\npredicted by continuous variable regression tasks to solve the problem of\nhigh-frequency fine-grained waveform distortion. Experimental results show that\nour proposed method outperforms the baseline method. We provide audio samples\non our website.", "published": "2023-09-27 09:27:03", "link": "http://arxiv.org/abs/2309.15512v2", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Teaching Text-to-Image Models to Communicate in Dialog", "abstract": "A picture is worth a thousand words, thus, it is crucial for conversational\nagents to understand, perceive, and effectively respond with pictures. However,\nwe find that directly employing conventional image generation techniques is\ninadequate for conversational agents to produce image responses effectively. In\nthis paper, we focus on the innovative dialog-to-image generation task, where\nthe model synthesizes a high-resolution image aligned with the given dialog\ncontext as a response. To tackle this problem, we design a tailored fine-tuning\napproach on the top of state-of-the-art text-to-image generation models to\nfully exploit the structural and semantic features in dialog context during\nimage generation. Concretely, we linearize the dialog context with specific\nindicators to maintain the dialog structure, and employ in-domain data to\nalleviate the style mismatch between dialog-to-image and conventional image\ngeneration tasks. Empirical results on PhotoChat and MMDialog Corpus show that\nour approach brings consistent and remarkable improvement with 3\nstate-of-the-art pre-trained text-to-image generation backbones.", "published": "2023-09-27 09:33:16", "link": "http://arxiv.org/abs/2309.15516v2", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Direct Models for Simultaneous Translation and Automatic Subtitling:\n  FBK@IWSLT2023", "abstract": "This paper describes the FBK's participation in the Simultaneous Translation\nand Automatic Subtitling tracks of the IWSLT 2023 Evaluation Campaign. Our\nsubmission focused on the use of direct architectures to perform both tasks:\nfor the simultaneous one, we leveraged the knowledge already acquired by\noffline-trained models and directly applied a policy to obtain the real-time\ninference; for the subtitling one, we adapted the direct ST model to produce\nwell-formed subtitles and exploited the same architecture to produce timestamps\nneeded for the subtitle synchronization with audiovisual content. Our\nEnglish-German SimulST system shows a reduced computational-aware latency\ncompared to the one achieved by the top-ranked systems in the 2021 and 2022\nrounds of the task, with gains of up to 3.5 BLEU. Our automatic subtitling\nsystem outperforms the only existing solution based on a direct system by 3.7\nand 1.7 SubER in English-German and English-Spanish respectively.", "published": "2023-09-27 10:24:42", "link": "http://arxiv.org/abs/2309.15554v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Jointly Training Large Autoregressive Multimodal Models", "abstract": "In recent years, advances in the large-scale pretraining of language and\ntext-to-image models have revolutionized the field of machine learning. Yet,\nintegrating these two modalities into a single, robust model capable of\ngenerating seamless multimodal outputs remains a significant challenge. To\naddress this gap, we present the Joint Autoregressive Mixture (JAM) framework,\na modular approach that systematically fuses existing text and image generation\nmodels. We also introduce a specialized, data-efficient instruction-tuning\nstrategy, tailored for mixed-modal generation tasks. Our final instruct-tuned\nmodel demonstrates unparalleled performance in generating high-quality\nmultimodal outputs and represents the first model explicitly designed for this\npurpose.", "published": "2023-09-27 10:40:23", "link": "http://arxiv.org/abs/2309.15564v2", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Developing automatic verbatim transcripts for international multilingual\n  meetings: an end-to-end solution", "abstract": "This paper presents an end-to-end solution for the creation of fully\nautomated conference meeting transcripts and their machine translations into\nvarious languages. This tool has been developed at the World Intellectual\nProperty Organization (WIPO) using in-house developed speech-to-text (S2T) and\nmachine translation (MT) components. Beyond describing data collection and\nfine-tuning, resulting in a highly customized and robust system, this paper\ndescribes the architecture and evolution of the technical components as well as\nhighlights the business impact and benefits from the user side. We also point\nout particular challenges in the evolution and adoption of the system and how\nthe new approach created a new product and replaced existing established\nworkflows in conference management documentation.", "published": "2023-09-27 12:16:15", "link": "http://arxiv.org/abs/2309.15609v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Speech collage: code-switched audio generation by collaging monolingual\n  corpora", "abstract": "Designing effective automatic speech recognition (ASR) systems for\nCode-Switching (CS) often depends on the availability of the transcribed CS\nresources. To address data scarcity, this paper introduces Speech Collage, a\nmethod that synthesizes CS data from monolingual corpora by splicing audio\nsegments. We further improve the smoothness quality of audio generation using\nan overlap-add approach. We investigate the impact of generated data on speech\nrecognition in two scenarios: using in-domain CS text and a zero-shot approach\nwith synthesized CS text. Empirical results highlight up to 34.4% and 16.2%\nrelative reductions in Mixed-Error Rate and Word-Error Rate for in-domain and\nzero-shot scenarios, respectively. Lastly, we demonstrate that CS augmentation\nbolsters the model's code-switching inclination and reduces its monolingual\nbias.", "published": "2023-09-27 14:17:53", "link": "http://arxiv.org/abs/2309.15674v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Enhancing End-to-End Conversational Speech Translation Through Target\n  Language Context Utilization", "abstract": "Incorporating longer context has been shown to benefit machine translation,\nbut the inclusion of context in end-to-end speech translation (E2E-ST) remains\nunder-studied. To bridge this gap, we introduce target language context in\nE2E-ST, enhancing coherence and overcoming memory constraints of extended audio\nsegments. Additionally, we propose context dropout to ensure robustness to the\nabsence of context, and further improve performance by adding speaker\ninformation. Our proposed contextual E2E-ST outperforms the isolated\nutterance-based E2E-ST approach. Lastly, we demonstrate that in conversational\nspeech, contextual information primarily contributes to capturing context\nstyle, as well as resolving anaphora and named entities.", "published": "2023-09-27 14:32:30", "link": "http://arxiv.org/abs/2309.15686v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Exploring Speech Recognition, Translation, and Understanding with\n  Discrete Speech Units: A Comparative Study", "abstract": "Speech signals, typically sampled at rates in the tens of thousands per\nsecond, contain redundancies, evoking inefficiencies in sequence modeling.\nHigh-dimensional speech features such as spectrograms are often used as the\ninput for the subsequent model. However, they can still be redundant. Recent\ninvestigations proposed the use of discrete speech units derived from\nself-supervised learning representations, which significantly compresses the\nsize of speech data. Applying various methods, such as de-duplication and\nsubword modeling, can further compress the speech sequence length. Hence,\ntraining time is significantly reduced while retaining notable performance. In\nthis study, we undertake a comprehensive and systematic exploration into the\napplication of discrete units within end-to-end speech processing models.\nExperiments on 12 automatic speech recognition, 3 speech translation, and 1\nspoken language understanding corpora demonstrate that discrete units achieve\nreasonably good results in almost all the settings. We intend to release our\nconfigurations and trained models to foster future research efforts.", "published": "2023-09-27 17:21:13", "link": "http://arxiv.org/abs/2309.15800v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Cross-Modal Multi-Tasking for Speech-to-Text Translation via Hard\n  Parameter Sharing", "abstract": "Recent works in end-to-end speech-to-text translation (ST) have proposed\nmulti-tasking methods with soft parameter sharing which leverage machine\ntranslation (MT) data via secondary encoders that map text inputs to an\neventual cross-modal representation. In this work, we instead propose a ST/MT\nmulti-tasking framework with hard parameter sharing in which all model\nparameters are shared cross-modally. Our method reduces the speech-text\nmodality gap via a pre-processing stage which converts speech and text inputs\ninto two discrete token sequences of similar length -- this allows models to\nindiscriminately process both modalities simply using a joint vocabulary. With\nexperiments on MuST-C, we demonstrate that our multi-tasking framework improves\nattentional encoder-decoder, Connectionist Temporal Classification (CTC),\ntransducer, and joint CTC/attention models by an average of +0.5 BLEU without\nany external MT data. Further, we show that this framework incorporates\nexternal MT data, yielding +0.8 BLEU, and also improves transfer learning from\npre-trained textual models, yielding +1.8 BLEU.", "published": "2023-09-27 17:48:14", "link": "http://arxiv.org/abs/2309.15826v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Towards Best Practices of Activation Patching in Language Models:\n  Metrics and Methods", "abstract": "Mechanistic interpretability seeks to understand the internal mechanisms of\nmachine learning models, where localization -- identifying the important model\ncomponents -- is a key step. Activation patching, also known as causal tracing\nor interchange intervention, is a standard technique for this task (Vig et al.,\n2020), but the literature contains many variants with little consensus on the\nchoice of hyperparameters or methodology. In this work, we systematically\nexamine the impact of methodological details in activation patching, including\nevaluation metrics and corruption methods. In several settings of localization\nand circuit discovery in language models, we find that varying these\nhyperparameters could lead to disparate interpretability results. Backed by\nempirical observations, we give conceptual arguments for why certain metrics or\nmethods may be preferred. Finally, we provide recommendations for the best\npractices of activation patching going forwards.", "published": "2023-09-27 21:53:56", "link": "http://arxiv.org/abs/2309.16042v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "AnyMAL: An Efficient and Scalable Any-Modality Augmented Language Model", "abstract": "We present Any-Modality Augmented Language Model (AnyMAL), a unified model\nthat reasons over diverse input modality signals (i.e. text, image, video,\naudio, IMU motion sensor), and generates textual responses. AnyMAL inherits the\npowerful text-based reasoning abilities of the state-of-the-art LLMs including\nLLaMA-2 (70B), and converts modality-specific signals to the joint textual\nspace through a pre-trained aligner module. To further strengthen the\nmultimodal LLM's capabilities, we fine-tune the model with a multimodal\ninstruction set manually collected to cover diverse topics and tasks beyond\nsimple QAs. We conduct comprehensive empirical analysis comprising both human\nand automatic evaluations, and demonstrate state-of-the-art performance on\nvarious multimodal tasks.", "published": "2023-09-27 22:50:51", "link": "http://arxiv.org/abs/2309.16058v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "An Empirical Study of AI Generated Text Detection Tools", "abstract": "Since ChatGPT has emerged as a major AIGC model, providing high-quality\nresponses across a wide range of applications (including software development\nand maintenance), it has attracted much interest from many individuals. ChatGPT\nhas great promise, but there are serious problems that might arise from its\nmisuse, especially in the realms of education and public safety. Several AIGC\ndetectors are available, and they have all been tested on genuine text.\nHowever, more study is needed to see how effective they are for multi-domain\nChatGPT material. This study aims to fill this need by creating a multi-domain\ndataset for testing the state-of-the-art APIs and tools for detecting\nartificially generated information used by universities and other research\ninstitutions. A large dataset consisting of articles, abstracts, stories, news,\nand product reviews was created for this study. The second step is to use the\nnewly created dataset to put six tools through their paces. Six different\nartificial intelligence (AI) text identification systems, including \"GPTkit,\"\n\"GPTZero,\" \"Originality,\" \"Sapling,\" \"Writer,\" and \"Zylalab,\" have accuracy\nrates between 55.29 and 97.0%. Although all the tools fared well in the\nevaluations, originality was particularly effective across the board.", "published": "2023-09-27 12:44:12", "link": "http://arxiv.org/abs/2310.01423v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "53", "I.2.6; I.2.7"], "primary_category": "cs.CL"}
{"title": "Borges and AI", "abstract": "Many believe that Large Language Models (LLMs) open the era of Artificial\nIntelligence (AI). Some see opportunities while others see dangers. Yet both\nproponents and opponents grasp AI through the imagery popularised by science\nfiction. Will the machine become sentient and rebel against its creators? Will\nwe experience a paperclip apocalypse? Before answering such questions, we\nshould first ask whether this mental imagery provides a good description of the\nphenomenon at hand. Understanding weather patterns through the moods of the\ngods only goes so far. The present paper instead advocates understanding LLMs\nand their connection to AI through the imagery of Jorge Luis Borges, a master\nof 20th century literature, forerunner of magical realism, and precursor to\npostmodern literature. This exercise leads to a new perspective that\nilluminates the relation between language modelling and artificial\nintelligence.", "published": "2023-09-27 16:15:34", "link": "http://arxiv.org/abs/2310.01425v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Generative Speech Recognition Error Correction with Large Language\n  Models and Task-Activating Prompting", "abstract": "We explore the ability of large language models (LLMs) to act as speech\nrecognition post-processors that perform rescoring and error correction. Our\nfirst focus is on instruction prompting to let LLMs perform these task without\nfine-tuning, for which we evaluate different prompting schemes, both zero- and\nfew-shot in-context learning, and a novel task activation prompting method that\ncombines causal instructions and demonstration to increase its context windows.\nNext, we show that rescoring only by in-context learning with frozen LLMs\nachieves results that are competitive with rescoring by domain-tuned LMs, using\na pretrained first-pass recognition system and rescoring output on two\nout-of-domain tasks (ATIS and WSJ). By combining prompting techniques with\nfine-tuning we achieve error rates below the N-best oracle level, showcasing\nthe generalization power of the LLMs.", "published": "2023-09-27 13:36:03", "link": "http://arxiv.org/abs/2309.15649v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "HyPoradise: An Open Baseline for Generative Speech Recognition with\n  Large Language Models", "abstract": "Advancements in deep neural networks have allowed automatic speech\nrecognition (ASR) systems to attain human parity on several publicly available\nclean speech datasets. However, even state-of-the-art ASR systems experience\nperformance degradation when confronted with adverse conditions, as a\nwell-trained acoustic model is sensitive to variations in the speech domain,\ne.g., background noise. Intuitively, humans address this issue by relying on\ntheir linguistic knowledge: the meaning of ambiguous spoken terms is usually\ninferred from contextual cues thereby reducing the dependency on the auditory\nsystem. Inspired by this observation, we introduce the first open-source\nbenchmark to utilize external large language models (LLMs) for ASR error\ncorrection, where N-best decoding hypotheses provide informative elements for\ntrue transcription prediction. This approach is a paradigm shift from the\ntraditional language model rescoring strategy that can only select one\ncandidate hypothesis as the output transcription. The proposed benchmark\ncontains a novel dataset, HyPoradise (HP), encompassing more than 334,000 pairs\nof N-best hypotheses and corresponding accurate transcriptions across prevalent\nspeech domains. Given this dataset, we examine three types of error correction\ntechniques based on LLMs with varying amounts of labeled\nhypotheses-transcription pairs, which gains a significant word error rate (WER)\nreduction. Experimental evidence demonstrates the proposed technique achieves a\nbreakthrough by surpassing the upper bound of traditional re-ranking based\nmethods. More surprisingly, LLM with reasonable prompt and its generative\ncapability can even correct those tokens that are missing in N-best list. We\nmake our results publicly accessible for reproducible pipelines with released\npre-trained models, thus providing a new evaluation paradigm for ASR error\ncorrection with LLMs.", "published": "2023-09-27 14:44:10", "link": "http://arxiv.org/abs/2309.15701v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "DualVC 2: Dynamic Masked Convolution for Unified Streaming and\n  Non-Streaming Voice Conversion", "abstract": "Voice conversion is becoming increasingly popular, and a growing number of\napplication scenarios require models with streaming inference capabilities. The\nrecently proposed DualVC attempts to achieve this objective through streaming\nmodel architecture design and intra-model knowledge distillation along with\nhybrid predictive coding to compensate for the lack of future information.\nHowever, DualVC encounters several problems that limit its performance. First,\nthe autoregressive decoder has error accumulation in its nature and limits the\ninference speed as well. Second, the causal convolution enables streaming\ncapability but cannot sufficiently use future information within chunks. Third,\nthe model is unable to effectively address the noise in the unvoiced segments,\nlowering the sound quality. In this paper, we propose DualVC 2 to address these\nissues. Specifically, the model backbone is migrated to a Conformer-based\narchitecture, empowering parallel inference. Causal convolution is replaced by\nnon-causal convolution with dynamic chunk mask to make better use of\nwithin-chunk future information. Also, quiet attention is introduced to enhance\nthe model's noise robustness. Experiments show that DualVC 2 outperforms DualVC\nand other baseline systems in both subjective and objective metrics, with only\n186.4 ms latency. Our audio samples are made publicly available.", "published": "2023-09-27 08:47:22", "link": "http://arxiv.org/abs/2309.15496v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multichannel Voice Trigger Detection Based on\n  Transform-average-concatenate", "abstract": "Voice triggering (VT) enables users to activate their devices by just\nspeaking a trigger phrase. A front-end system is typically used to perform\nspeech enhancement and/or separation, and produces multiple enhanced and/or\nseparated signals. Since conventional VT systems take only single-channel audio\nas input, channel selection is performed. A drawback of this approach is that\nunselected channels are discarded, even if the discarded channels could contain\nuseful information for VT. In this work, we propose multichannel acoustic\nmodels for VT, where the multichannel output from the frond-end is fed directly\ninto a VT model. We adopt a transform-average-concatenate (TAC) block and\nmodify the TAC block by incorporating the channel from the conventional channel\nselection so that the model can attend to a target speaker when multiple\nspeakers are present. The proposed approach achieves up to 30% reduction in the\nfalse rejection rate compared to the baseline channel selection approach.", "published": "2023-09-27 21:28:50", "link": "http://arxiv.org/abs/2309.16036v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Does Single-channel Speech Enhancement Improve Keyword Spotting\n  Accuracy? A Case Study", "abstract": "Noise robustness is a key aspect of successful speech applications. Speech\nenhancement (SE) has been investigated to improve automatic speech recognition\naccuracy; however, its effectiveness for keyword spotting (KWS) is still\nunder-investigated. In this paper, we conduct a comprehensive study on\nsingle-channel speech enhancement for keyword spotting on the Google Speech\nCommand (GSC) dataset. To investigate robustness to noise, the GSC dataset is\naugmented with noise signals from the WSJ0 Hipster Ambient Mixtures (WHAM!)\nnoise dataset. Our investigation includes not only applying SE before KWS but\nalso performing joint training of the SE frontend and KWS backend models.\nMoreover, we explore audio injection, a common approach to reduce distortions\nby using a weighted average of the enhanced and original signals. Audio\ninjection is then further optimized by using another model that predicts the\nweight for each utterance. Our investigation reveals that SE can improve KWS\naccuracy on noisy speech when the backend model is trained on clean speech;\nhowever, despite our extensive exploration, it is difficult to improve the KWS\naccuracy with SE when the backend is trained on noisy speech.", "published": "2023-09-27 22:55:45", "link": "http://arxiv.org/abs/2309.16060v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Why do Angular Margin Losses work well for Semi-Supervised Anomalous\n  Sound Detection?", "abstract": "State-of-the-art anomalous sound detection systems often utilize angular\nmargin losses to learn suitable representations of acoustic data using an\nauxiliary task, which usually is a supervised or self-supervised classification\ntask. The underlying idea is that, in order to solve this auxiliary task,\nspecific information about normal data needs to be captured in the learned\nrepresentations and that this information is also sufficient to differentiate\nbetween normal and anomalous samples. Especially in noisy conditions,\ndiscriminative models based on angular margin losses tend to significantly\noutperform systems based on generative or one-class models. The goal of this\nwork is to investigate why using angular margin losses with auxiliary tasks\nworks well for detecting anomalous sounds. To this end, it is shown, both\ntheoretically and experimentally, that minimizing angular margin losses also\nminimizes compactness loss while inherently preventing learning trivial\nsolutions. Furthermore, multiple experiments are conducted to show that using a\nrelated classification task as an auxiliary task teaches the model to learn\nrepresentations suitable for detecting anomalous sounds in noisy conditions.\nAmong these experiments are performance evaluations, visualizing the embedding\nspace with t-SNE and visualizing the input representations with respect to the\nanomaly score using randomized input sampling for explanation.", "published": "2023-09-27 13:29:38", "link": "http://arxiv.org/abs/2309.15643v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Timbre-Trap: A Low-Resource Framework for Instrument-Agnostic Music\n  Transcription", "abstract": "In recent years, research on music transcription has focused mainly on\narchitecture design and instrument-specific data acquisition. With the lack of\navailability of diverse datasets, progress is often limited to solo-instrument\ntasks such as piano transcription. Several works have explored multi-instrument\ntranscription as a means to bolster the performance of models on low-resource\ntasks, but these methods face the same data availability issues. We propose\nTimbre-Trap, a novel framework which unifies music transcription and audio\nreconstruction by exploiting the strong separability between pitch and timbre.\nWe train a single autoencoder to simultaneously estimate pitch salience and\nreconstruct complex spectral coefficients, selecting between either output\nduring the decoding stage via a simple switch mechanism. In this way, the model\nlearns to produce coefficients corresponding to timbre-less audio, which can be\ninterpreted as pitch salience. We demonstrate that the framework leads to\nperformance comparable to state-of-the-art instrument-agnostic transcription\nmethods, while only requiring a small amount of annotated data.", "published": "2023-09-27 15:19:05", "link": "http://arxiv.org/abs/2309.15717v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Exploring Self-Supervised Contrastive Learning of Spatial Sound Event\n  Representation", "abstract": "In this study, we present a simple multi-channel framework for contrastive\nlearning (MC-SimCLR) to encode 'what' and 'where' of spatial audios. MC-SimCLR\nlearns joint spectral and spatial representations from unlabeled spatial\naudios, thereby enhancing both event classification and sound localization in\ndownstream tasks. At its core, we propose a multi-level data augmentation\npipeline that augments different levels of audio features, including waveforms,\nMel spectrograms, and generalized cross-correlation (GCC) features. In\naddition, we introduce simple yet effective channel-wise augmentation methods\nto randomly swap the order of the microphones and mask Mel and GCC channels. By\nusing these augmentations, we find that linear layers on top of the learned\nrepresentation significantly outperform supervised models in terms of both\nevent classification accuracy and localization error. We also perform a\ncomprehensive analysis of the effect of each augmentation method and a\ncomparison of the fine-tuning performance using different amounts of labeled\ndata.", "published": "2023-09-27 18:23:03", "link": "http://arxiv.org/abs/2309.15938v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Neural Acoustic Context Field: Rendering Realistic Room Impulse Response\n  With Neural Fields", "abstract": "Room impulse response (RIR), which measures the sound propagation within an\nenvironment, is critical for synthesizing high-fidelity audio for a given\nenvironment. Some prior work has proposed representing RIR as a neural field\nfunction of the sound emitter and receiver positions. However, these methods do\nnot sufficiently consider the acoustic properties of an audio scene, leading to\nunsatisfactory performance. This letter proposes a novel Neural Acoustic\nContext Field approach, called NACF, to parameterize an audio scene by\nleveraging multiple acoustic contexts, such as geometry, material property, and\nspatial information. Driven by the unique properties of RIR, i.e., temporal\nun-smoothness and monotonic energy attenuation, we design a temporal\ncorrelation module and multi-scale energy decay criterion. Experimental results\nshow that NACF outperforms existing field-based methods by a notable margin.\nPlease visit our project page for more qualitative results.", "published": "2023-09-27 19:50:50", "link": "http://arxiv.org/abs/2309.15977v1", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Advancing Acoustic Howling Suppression through Recursive Training of\n  Neural Networks", "abstract": "In this paper, we introduce a novel training framework designed to\ncomprehensively address the acoustic howling issue by examining its fundamental\nformation process. This framework integrates a neural network (NN) module into\nthe closed-loop system during training with signals generated recursively on\nthe fly to closely mimic the streaming process of acoustic howling suppression\n(AHS). The proposed recursive training strategy bridges the gap between\ntraining and real-world inference scenarios, marking a departure from previous\nNN-based methods that typically approach AHS as either noise suppression or\nacoustic echo cancellation. Within this framework, we explore two\nmethodologies: one exclusively relying on NN and the other combining NN with\nthe traditional Kalman filter. Additionally, we propose strategies, including\nhowling detection and initialization using pre-trained offline models, to\nbolster trainability and expedite the training process. Experimental results\nvalidate that this framework offers a substantial improvement over previous\nmethodologies for acoustic howling suppression.", "published": "2023-09-27 22:02:53", "link": "http://arxiv.org/abs/2309.16048v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Neural Network Augmented Kalman Filter for Robust Acoustic Howling\n  Suppression", "abstract": "Acoustic howling suppression (AHS) is a critical challenge in audio\ncommunication systems. In this paper, we propose a novel approach that\nleverages the power of neural networks (NN) to enhance the performance of\ntraditional Kalman filter algorithms for AHS. Specifically, our method involves\nthe integration of NN modules into the Kalman filter, enabling refining\nreference signal, a key factor in effective adaptive filtering, and estimating\ncovariance metrics for the filter which are crucial for adaptability in dynamic\nconditions, thereby obtaining improved AHS performance. As a result, the\nproposed method achieves improved AHS performance compared to both standalone\nNN and Kalman filter methods. Experimental evaluations validate the\neffectiveness of our approach.", "published": "2023-09-27 22:07:00", "link": "http://arxiv.org/abs/2309.16049v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
