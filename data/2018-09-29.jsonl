{"title": "MultiWOZ -- A Large-Scale Multi-Domain Wizard-of-Oz Dataset for\n  Task-Oriented Dialogue Modelling", "abstract": "Even though machine learning has become the major scene in dialogue research\ncommunity, the real breakthrough has been blocked by the scale of data\navailable. To address this fundamental obstacle, we introduce the Multi-Domain\nWizard-of-Oz dataset (MultiWOZ), a fully-labeled collection of human-human\nwritten conversations spanning over multiple domains and topics. At a size of\n$10$k dialogues, it is at least one order of magnitude larger than all previous\nannotated task-oriented corpora. The contribution of this work apart from the\nopen-sourced dataset labelled with dialogue belief states and dialogue actions\nis two-fold: firstly, a detailed description of the data collection procedure\nalong with a summary of data structure and analysis is provided. The proposed\ndata-collection pipeline is entirely based on crowd-sourcing without the need\nof hiring professional annotators; secondly, a set of benchmark results of\nbelief tracking, dialogue act and response generation is reported, which shows\nthe usability of the data and sets a baseline for future studies.", "published": "2018-09-29 23:44:39", "link": "http://arxiv.org/abs/1810.00278v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modulated Variational auto-Encoders for many-to-many musical timbre\n  transfer", "abstract": "Generative models have been successfully applied to image style transfer and\ndomain translation. However, there is still a wide gap in the quality of\nresults when learning such tasks on musical audio. Furthermore, most\ntranslation models only enable one-to-one or one-to-many transfer by relying on\nseparate encoders or decoders and complex, computationally-heavy models. In\nthis paper, we introduce the Modulated Variational auto-Encoders (MoVE) to\nperform musical timbre transfer. We define timbre transfer as applying parts of\nthe auditory properties of a musical instrument onto another. First, we show\nthat we can achieve this task by conditioning existing domain translation\ntechniques with Feature-wise Linear Modulation (FiLM). Then, we alleviate the\nneed for additional adversarial networks by replacing the usual translation\ncriterion by a Maximum Mean Discrepancy (MMD) objective. This allows a faster\nand more stable training along with a controllable latent space encoder. By\nfurther conditioning our system on several different instruments, we can\ngeneralize to many-to-many transfer within a single variational architecture\nable to perform multi-domain transfers. Our models map inputs to 3-dimensional\nrepresentations, successfully translating timbre from one instrument to another\nand supporting sound synthesis from a reduced set of control parameters. We\nevaluate our method in reconstruction and generation tasks while analyzing the\nauditory descriptor distributions across transferred domains. We show that this\narchitecture allows for generative controls in multi-domain transfer, yet\nremaining light, fast to train and effective on small datasets.", "published": "2018-09-29 15:31:23", "link": "http://arxiv.org/abs/1810.00222v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Generalized Multichannel Variational Autoencoder for Underdetermined\n  Source Separation", "abstract": "This paper deals with a multichannel audio source separation problem under\nunderdetermined conditions. Multichannel Non-negative Matrix Factorization\n(MNMF) is one of powerful approaches, which adopts the NMF concept for source\npower spectrogram modeling. This concept is also employed in Independent\nLow-Rank Matrix Analysis (ILRMA), a special class of the MNMF framework\nformulated under determined conditions. While these methods work reasonably\nwell for particular types of sound sources, one limitation is that they can\nfail to work for sources with spectrograms that do not comply with the NMF\nmodel. To address this limitation, an extension of ILRMA called the\nMultichannel Variational Autoencoder (MVAE) method was recently proposed, where\na Conditional VAE (CVAE) is used instead of the NMF model for source power\nspectrogram modeling. This approach has shown to perform impressively in\ndetermined source separation tasks thanks to the representation power of DNNs.\nWhile the original MVAE method was formulated under determined mixing\nconditions, this paper generalizes it so that it can also deal with\nunderdetermined cases. We call the proposed framework the Generalized MVAE\n(GMVAE). The proposed method was evaluated on a underdetermined source\nseparation task of separating out three sources from two microphone inputs.\nExperimental results revealed that the GMVAE method achieved better performance\nthan the MNMF method.", "published": "2018-09-29 15:40:11", "link": "http://arxiv.org/abs/1810.00223v1", "categories": ["stat.ML", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "stat.ML"}
