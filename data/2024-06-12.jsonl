{"title": "Heterogeneous Beliefs Model of Stock Market Predictability", "abstract": "This paper proposes a theory of stock market predictability patterns based on\na model of heterogeneous beliefs. In a discrete finite time framework, some\nagents receive news about an asset's fundamental value through a noisy signal.\nThe investors are heterogeneous in that they have different beliefs about the\nstochastic supply. A momentum in the stock price arises from those agents who\nincorrectly underestimate the signal accuracy, dampening the initial price\nimpact of the signal. A reversal in price occurs because the price reverts to\nthe fundamental value in the long run. An extension of the model to multiple\nassets case predicts co-movement and lead-lag effect, in addition to\ncross-sectional momentum and reversal. The heterogeneous beliefs of investors\nabout news demonstrate how the main predictability anomalies arise endogenously\nin a model of bounded rationality.", "published": "2024-06-12 17:39:36", "link": "http://arxiv.org/abs/2406.08448v1", "categories": ["q-fin.PR", "q-fin.GN", "q-fin.TR"], "primary_category": "q-fin.PR"}
{"title": "Deep learning for quadratic hedging in incomplete jump market", "abstract": "We propose a deep learning approach to study the minimal variance pricing and\nhedging problem in an incomplete jump diffusion market. It is based upon a\nrigorous stochastic calculus derivation of the optimal hedging portfolio,\noptimal option price, and the corresponding equivalent martingale measure\nthrough the means of the Stackelberg game approach. A deep learning algorithm\nbased on the combination of the feedforward and LSTM neural networks is tested\non three different market models, two of which are incomplete. In contrast, the\ncomplete market Black-Scholes model serves as a benchmark for the algorithm's\nperformance. The results that indicate the algorithm's good performance are\npresented and discussed.\n  In particular, we apply our results to the special incomplete market model\nstudied by Merton and give a detailed comparison between our results based on\nthe minimal variance principle and the results obtained by Merton based on a\ndifferent pricing principle. Using deep learning, we find that the minimal\nvariance principle leads to typically higher option prices than those deduced\nfrom the Merton principle. On the other hand, the minimal variance principle\nleads to lower losses than the Merton principle.", "published": "2024-06-12 09:45:35", "link": "http://arxiv.org/abs/2407.13688v1", "categories": ["q-fin.TR", "math.PR"], "primary_category": "q-fin.TR"}
{"title": "Deep reinforcement learning with positional context for intraday trading", "abstract": "Deep reinforcement learning (DRL) is a well-suited approach to financial\ndecision-making, where an agent makes decisions based on its trading strategy\ndeveloped from market observations. Existing DRL intraday trading strategies\nmainly use price-based features to construct the state space. They neglect the\ncontextual information related to the position of the strategy, which is an\nimportant aspect given the sequential nature of intraday trading. In this\nstudy, we propose a novel DRL model for intraday trading that introduces\npositional features encapsulating the contextual information into its sparse\nstate space. The model is evaluated over an extended period of almost a decade\nand across various assets including commodities and foreign exchange\nsecurities, taking transaction costs into account. The results show a notable\nperformance in terms of profitability and risk-adjusted metrics. The feature\nimportance results show that each feature incorporating contextual information\ncontributes to the overall performance of the model. Additionally, through an\nexploration of the agent's intraday trading activity, we unveil patterns that\nsubstantiate the effectiveness of our proposed model.", "published": "2024-06-12 09:04:45", "link": "http://arxiv.org/abs/2406.08013v1", "categories": ["q-fin.TR"], "primary_category": "q-fin.TR"}
{"title": "Label-aware Hard Negative Sampling Strategies with Momentum Contrastive\n  Learning for Implicit Hate Speech Detection", "abstract": "Detecting implicit hate speech that is not directly hateful remains a\nchallenge. Recent research has attempted to detect implicit hate speech by\napplying contrastive learning to pre-trained language models such as BERT and\nRoBERTa, but the proposed models still do not have a significant advantage over\ncross-entropy loss-based learning. We found that contrastive learning based on\nrandomly sampled batch data does not encourage the model to learn hard negative\nsamples. In this work, we propose Label-aware Hard Negative sampling strategies\n(LAHN) that encourage the model to learn detailed features from hard negative\nsamples, instead of naive negative samples in random batch, using\nmomentum-integrated contrastive learning. LAHN outperforms the existing models\nfor implicit hate speech detection both in- and cross-datasets. The code is\navailable at https://github.com/Hanyang-HCC-Lab/LAHN", "published": "2024-06-12 05:24:58", "link": "http://arxiv.org/abs/2406.07886v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automated Information Extraction from Thyroid Operation Narrative: A\n  Comparative Study of GPT-4 and Fine-tuned KoELECTRA", "abstract": "In the rapidly evolving field of healthcare, the integration of artificial\nintelligence (AI) has become a pivotal component in the automation of clinical\nworkflows, ushering in a new era of efficiency and accuracy. This study focuses\non the transformative capabilities of the fine-tuned KoELECTRA model in\ncomparison to the GPT-4 model, aiming to facilitate automated information\nextraction from thyroid operation narratives. The current research landscape is\ndominated by traditional methods heavily reliant on regular expressions, which\noften face challenges in processing free-style text formats containing critical\ndetails of operation records, including frozen biopsy reports. Addressing this,\nthe study leverages advanced natural language processing (NLP) techniques to\nfoster a paradigm shift towards more sophisticated data processing systems.\nThrough this comparative study, we aspire to unveil a more streamlined,\nprecise, and efficient approach to document processing in the healthcare\ndomain, potentially revolutionizing the way medical data is handled and\nanalyzed.", "published": "2024-06-12 06:44:05", "link": "http://arxiv.org/abs/2406.07922v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Guiding In-Context Learning of LLMs through Quality Estimation for\n  Machine Translation", "abstract": "The quality of output from large language models (LLMs), particularly in\nmachine translation (MT), is closely tied to the quality of in-context examples\n(ICEs) provided along with the query, i.e., the text to translate. The\neffectiveness of these ICEs is influenced by various factors, such as the\ndomain of the source text, the order in which the ICEs are presented, the\nnumber of these examples, and the prompt templates used. Naturally, selecting\nthe most impactful ICEs depends on understanding how these affect the resulting\ntranslation quality, which ultimately relies on translation references or human\njudgment. This paper presents a novel methodology for in-context learning (ICL)\nthat relies on a search algorithm guided by domain-specific quality estimation\n(QE). Leveraging the XGLM model, our methodology estimates the resulting\ntranslation quality without the need for translation references, selecting\neffective ICEs for MT to maximize translation quality. Our results demonstrate\nsignificant improvements over existing ICL methods and higher translation\nperformance compared to fine-tuning a pre-trained language model (PLM),\nspecifically mBART-50.", "published": "2024-06-12 07:49:36", "link": "http://arxiv.org/abs/2406.07970v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Job Title Representation from Job Description Aggregation\n  Network", "abstract": "Learning job title representation is a vital process for developing automatic\nhuman resource tools. To do so, existing methods primarily rely on learning the\ntitle representation through skills extracted from the job description,\nneglecting the rich and diverse content within. Thus, we propose an alternative\nframework for learning job titles through their respective job description (JD)\nand utilize a Job Description Aggregator component to handle the lengthy\ndescription and bidirectional contrastive loss to account for the bidirectional\nrelationship between the job title and its description. We evaluated the\nperformance of our method on both in-domain and out-of-domain settings,\nachieving a superior performance over the skill-based approach.", "published": "2024-06-12 10:12:52", "link": "http://arxiv.org/abs/2406.08055v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models Meet Text-Centric Multimodal Sentiment Analysis: A\n  Survey", "abstract": "Compared to traditional sentiment analysis, which only considers text,\nmultimodal sentiment analysis needs to consider emotional signals from\nmultimodal sources simultaneously and is therefore more consistent with the way\nhow humans process sentiment in real-world scenarios. It involves processing\nemotional information from various sources such as natural language, images,\nvideos, audio, physiological signals, etc. However, although other modalities\nalso contain diverse emotional cues, natural language usually contains richer\ncontextual information and therefore always occupies a crucial position in\nmultimodal sentiment analysis. The emergence of ChatGPT has opened up immense\npotential for applying large language models (LLMs) to text-centric multimodal\ntasks. However, it is still unclear how existing LLMs can adapt better to\ntext-centric multimodal sentiment analysis tasks. This survey aims to (1)\npresent a comprehensive review of recent research in text-centric multimodal\nsentiment analysis tasks, (2) examine the potential of LLMs for text-centric\nmultimodal sentiment analysis, outlining their approaches, advantages, and\nlimitations, (3) summarize the application scenarios of LLM-based multimodal\nsentiment analysis technology, and (4) explore the challenges and potential\nresearch directions for multimodal sentiment analysis in the future.", "published": "2024-06-12 10:36:27", "link": "http://arxiv.org/abs/2406.08068v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Languages Transferred Within the Encoder: On Representation Transfer in\n  Zero-Shot Multilingual Translation", "abstract": "Understanding representation transfer in multilingual neural machine\ntranslation (MNMT) can reveal the reason for the zero-shot translation\ndeficiency. In this work, we systematically analyze the representational issue\nof MNMT models. We first introduce the identity pair, translating a sentence to\nitself, to address the lack of the base measure in multilingual investigations,\nas the identity pair can reflect the representation of a language within the\nmodel. Then, we demonstrate that the encoder transfers the source language to\nthe representational subspace of the target language instead of the\nlanguage-agnostic state. Thus, the zero-shot translation deficiency arises\nbecause the representation of a translation is entangled with other languages\nand not transferred to the target language effectively. Based on our findings,\nwe propose two methods: 1) low-rank language-specific embedding at the encoder,\nand 2) language-specific contrastive learning of the representation at the\ndecoder. The experimental results on Europarl-15, TED-19, and OPUS-100 datasets\nshow that our methods substantially enhance the performance of zero-shot\ntranslations without sacrifices in supervised directions by improving language\ntransfer capacity, thereby providing practical evidence to support our\nconclusions. Codes are available at https://github.com/zhiqu22/ZeroTrans.", "published": "2024-06-12 11:16:30", "link": "http://arxiv.org/abs/2406.08092v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CoXQL: A Dataset for Parsing Explanation Requests in Conversational XAI\n  Systems", "abstract": "Conversational explainable artificial intelligence (ConvXAI) systems based on\nlarge language models (LLMs) have garnered significant interest from the\nresearch community in natural language processing (NLP) and human-computer\ninteraction (HCI). Such systems can provide answers to user questions about\nexplanations in dialogues, have the potential to enhance users' comprehension\nand offer more information about the decision-making and generation processes\nof LLMs. Currently available ConvXAI systems are based on intent recognition\nrather than free chat, as this has been found to be more precise and reliable\nin identifying users' intentions. However, the recognition of intents still\npresents a challenge in the case of ConvXAI, since little training data exist\nand the domain is highly specific, as there is a broad range of XAI methods to\nmap requests onto. In order to bridge this gap, we present CoXQL, the first\ndataset in the NLP domain for user intent recognition in ConvXAI, covering 31\nintents, seven of which require filling multiple slots. Subsequently, we\nenhance an existing parsing approach by incorporating template validations, and\nconduct an evaluation of several LLMs on CoXQL using different parsing\nstrategies. We conclude that the improved parsing approach (MP+) surpasses the\nperformance of previous approaches. We also discover that intents with multiple\nslots remain highly challenging for LLMs.", "published": "2024-06-12 11:27:10", "link": "http://arxiv.org/abs/2406.08101v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semi-Supervised Spoken Language Glossification", "abstract": "Spoken language glossification (SLG) aims to translate the spoken language\ntext into the sign language gloss, i.e., a written record of sign language. In\nthis work, we present a framework named $S$emi-$S$upervised $S$poken $L$anguage\n$G$lossification ($S^3$LG) for SLG. To tackle the bottleneck of limited\nparallel data in SLG, our $S^3$LG incorporates large-scale monolingual spoken\nlanguage text into SLG training. The proposed framework follows the\nself-training structure that iteratively annotates and learns from pseudo\nlabels. Considering the lexical similarity and syntactic difference between\nsign language and spoken language, our $S^3$LG adopts both the rule-based\nheuristic and model-based approach for auto-annotation. During training, we\nrandomly mix these complementary synthetic datasets and mark their differences\nwith a special token. As the synthetic data may be less quality, the $S^3$LG\nfurther leverages consistency regularization to reduce the negative impact of\nnoise in the synthetic data. Extensive experiments are conducted on public\nbenchmarks to demonstrate the effectiveness of the $S^3$LG. Our code is\navailable at \\url{https://github.com/yaohj11/S3LG}.", "published": "2024-06-12 13:05:27", "link": "http://arxiv.org/abs/2406.08173v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Underneath the Numbers: Quantitative and Qualitative Gender Fairness in\n  LLMs for Depression Prediction", "abstract": "Recent studies show bias in many machine learning models for depression\ndetection, but bias in LLMs for this task remains unexplored. This work\npresents the first attempt to investigate the degree of gender bias present in\nexisting LLMs (ChatGPT, LLaMA 2, and Bard) using both quantitative and\nqualitative approaches. From our quantitative evaluation, we found that ChatGPT\nperforms the best across various performance metrics and LLaMA 2 outperforms\nother LLMs in terms of group fairness metrics. As qualitative fairness\nevaluation remains an open research question we propose several strategies\n(e.g., word count, thematic analysis) to investigate whether and how a\nqualitative evaluation can provide valuable insights for bias analysis beyond\nwhat is possible with quantitative evaluation. We found that ChatGPT\nconsistently provides a more comprehensive, well-reasoned explanation for its\nprediction compared to LLaMA 2. We have also identified several themes adopted\nby LLMs to qualitatively evaluate gender fairness. We hope our results can be\nused as a stepping stone towards future attempts at improving qualitative\nevaluation of fairness for LLMs especially for high-stakes tasks such as\ndepression detection.", "published": "2024-06-12 13:14:19", "link": "http://arxiv.org/abs/2406.08183v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Dialogue Game for Eliciting Balanced Collaboration", "abstract": "Collaboration is an integral part of human dialogue. Typical task-oriented\ndialogue games assign asymmetric roles to the participants, which limits their\nability to elicit naturalistic role-taking in collaboration and its\nnegotiation. We present a novel and simple online setup that favors balanced\ncollaboration: a two-player 2D object placement game in which the players must\nnegotiate the goal state themselves. We show empirically that human players\nexhibit a variety of role distributions, and that balanced collaboration\nimproves task performance. We also present an LLM-based baseline agent which\ndemonstrates that automatic playing of our game is an interesting challenge for\nartificial systems.", "published": "2024-06-12 13:35:10", "link": "http://arxiv.org/abs/2406.08202v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SumHiS: Extractive Summarization Exploiting Hidden Structure", "abstract": "Extractive summarization is a task of highlighting the most important parts\nof the text. We introduce a new approach to extractive summarization task using\nhidden clustering structure of the text. Experimental results on CNN/DailyMail\ndemonstrate that our approach generates more accurate summaries than both\nextractive and abstractive methods, achieving state-of-the-art results in terms\nof ROUGE-2 metric exceeding the previous approaches by 10%. Additionally, we\nshow that hidden structure of the text could be interpreted as aspects.", "published": "2024-06-12 13:44:58", "link": "http://arxiv.org/abs/2406.08215v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Figuratively Speaking: Authorship Attribution via Multi-Task Figurative\n  Language Modeling", "abstract": "The identification of Figurative Language (FL) features in text is crucial\nfor various Natural Language Processing (NLP) tasks, where understanding of the\nauthor's intended meaning and its nuances is key for successful communication.\nAt the same time, the use of a specific blend of various FL forms most\naccurately reflects a writer's style, rather than the use of any single\nconstruct, such as just metaphors or irony. Thus, we postulate that FL features\ncould play an important role in Authorship Attribution (AA) tasks. We believe\nthat our is the first computational study of AA based on FL use. Accordingly,\nwe propose a Multi-task Figurative Language Model (MFLM) that learns to detect\nmultiple FL features in text at once. We demonstrate, through detailed\nevaluation across multiple test sets, that the our model tends to perform\nequally or outperform specialized binary models in FL detection. Subsequently,\nwe evaluate the predictive capability of joint FL features towards the AA task\non three datasets, observing improved AA performance through the integration of\nMFLM embeddings.", "published": "2024-06-12 13:49:38", "link": "http://arxiv.org/abs/2406.08218v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "M3T: A New Benchmark Dataset for Multi-Modal Document-Level Machine\n  Translation", "abstract": "Document translation poses a challenge for Neural Machine Translation (NMT)\nsystems. Most document-level NMT systems rely on meticulously curated\nsentence-level parallel data, assuming flawless extraction of text from\ndocuments along with their precise reading order. These systems also tend to\ndisregard additional visual cues such as the document layout, deeming it\nirrelevant. However, real-world documents often possess intricate text layouts\nthat defy these assumptions. Extracting information from Optical Character\nRecognition (OCR) or heuristic rules can result in errors, and the layout\n(e.g., paragraphs, headers) may convey relationships between distant sections\nof text. This complexity is particularly evident in widely used PDF documents,\nwhich represent information visually. This paper addresses this gap by\nintroducing M3T, a novel benchmark dataset tailored to evaluate NMT systems on\nthe comprehensive task of translating semi-structured documents. This dataset\naims to bridge the evaluation gap in document-level NMT systems, acknowledging\nthe challenges posed by rich text layouts in real-world applications.", "published": "2024-06-12 14:28:25", "link": "http://arxiv.org/abs/2406.08255v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Fact Memorization and Style Imitation in LLMs Using QLoRA: An\n  Experimental Study and Quality Assessment Methods", "abstract": "There are various methods for adapting LLMs to different domains. The most\ncommon methods are prompting, finetuning, and RAG. In this work, we explore the\npossibility of adapting a model using one of the PEFT methods - QLoRA. The\nexperiment aims to simulate human responses based on their interviews. The\nsimulation quality is assessed by comparing the quality of the style and the\nquality of the generated facts.", "published": "2024-06-12 18:38:40", "link": "http://arxiv.org/abs/2406.08582v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mistral-C2F: Coarse to Fine Actor for Analytical and Reasoning\n  Enhancement in RLHF and Effective-Merged LLMs", "abstract": "Despite the advances in Large Language Models (LLMs), exemplified by models\nlike GPT-4 and Claude, smaller-scale LLMs such as Llama and Mistral often\nstruggle with generating in-depth and coherent dialogues. This paper presents a\nnovel two-step Coarse-to-Fine Actor model to address the inherent limitations\nin conversational and analytical capabilities of small-sized LLMs. Our approach\nbegins with the Policy-based Coarse Actor, employing a technique we term\n\"Continuous Maximization\". The Coarse Actor establishes an enhanced,\nknowledge-rich pool adept at aligning with human preference styles in analysis\nand reasoning. Through the RLHF process, it employs Continuous Maximization, a\nstrategy that dynamically and adaptively extends the output length limit,\nenabling the generation of more detailed and analytical content. Subsequently,\nthe Fine Actor refines this analytical content, addressing the generation of\nexcessively redundant information from the Coarse Actor. We introduce a\n\"Knowledge Residue Merger\" approach, refining the content from the Coarse Actor\nand merging it with an existing Instruction model to improve quality,\ncorrectness, and reduce redundancies. We applied our methodology to the popular\nMistral model, creating Mistral-C2F, which has demonstrated exceptional\nperformance across 11 general language tasks and the MT-Bench Dialogue task,\noutperforming similar-scale models and even larger models with 13B and 30B\nparameters. Our model has significantly improved conversational and analytical\nreasoning abilities.", "published": "2024-06-12 21:42:13", "link": "http://arxiv.org/abs/2406.08657v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analyzing Large Language Models for Classroom Discussion Assessment", "abstract": "Automatically assessing classroom discussion quality is becoming increasingly\nfeasible with the help of new NLP advancements such as large language models\n(LLMs). In this work, we examine how the assessment performance of 2 LLMs\ninteracts with 3 factors that may affect performance: task formulation, context\nlength, and few-shot examples. We also explore the computational efficiency and\npredictive consistency of the 2 LLMs. Our results suggest that the 3\naforementioned factors do affect the performance of the tested LLMs and there\nis a relation between consistency and performance. We recommend a LLM-based\nassessment approach that has a good balance in terms of predictive performance,\ncomputational efficiency, and consistency.", "published": "2024-06-12 22:43:38", "link": "http://arxiv.org/abs/2406.08680v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Critical Look At Tokenwise Reward-Guided Text Generation", "abstract": "Large language models (LLMs) can be improved by aligning with human\npreferences through fine-tuning -- the so-called reinforcement learning from\nhuman feedback (RLHF). However, the cost of fine-tuning an LLM is prohibitive\nfor many users. Due to their ability to bypass LLM fine-tuning, prediction-time\ntokenwise reward-guided text generation (RGTG) methods have recently been\nproposed. They use a reward model trained on full sequences to score partial\nsequences during decoding in a bid to steer the generation towards sequences\nwith high rewards. However, these methods have so far been only heuristically\nmotivated and poorly analyzed. In this work, we show that reward models trained\non full sequences are not compatible with scoring partial sequences. To\nalleviate this issue, we propose to train a Bradley-Terry reward model on\npartial sequences explicitly, and autoregressively sample from the implied\ntokenwise policy during decoding time. We study the properties of this reward\nmodel and the resulting policy: we show that this policy is proportional to the\nratio of two distinct RLHF policies. Our simple approach outperforms previous\nRGTG methods and performs similarly to strong offline baselines without\nlarge-scale LLM finetuning.", "published": "2024-06-12 00:19:40", "link": "http://arxiv.org/abs/2406.07780v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Judging the Judges: A Systematic Study of Position Bias in\n  LLM-as-a-Judge", "abstract": "LLM-as-a-Judge presents a promising alternative to human evaluators across\nvarious tasks, but inherent biases, especially position bias - a tendency to\nfavor solutions based on their position in the prompt - have compromised its\neffectiveness. Our study introduces a systematic framework to examine position\nbias in pairwise comparisons, focusing on repetition stability, position\nconsistency, and preference fairness. This research significantly contributes\nto the field by introducing new concepts for understanding position bias and\nproviding a multi-dimensional framework for evaluations. We conducted\nexperiments with 12 LLM judges across MTBench and DevBench, covering 22 tasks\nand approximately 40 solution-generating models - candidates, resulting in over\n100,000 evaluation instances. Our findings confirm that position bias in\ncapable LLM judges is not due to random chances, along with notable variations\nobserved across judges and tasks. Moreover, position bias is weakly influenced\nby the length of prompt components but significantly impacted by the quality\ngap between solutions. These insights can help optimize judge model selections,\nimprove benchmark design, and inform future research on debiasing strategies,\nultimately enhancing the reliability of LLM judges.", "published": "2024-06-12 01:12:28", "link": "http://arxiv.org/abs/2406.07791v7", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Making Task-Oriented Dialogue Datasets More Natural by Synthetically\n  Generating Indirect User Requests", "abstract": "Indirect User Requests (IURs), such as \"It's cold in here\" instead of \"Could\nyou please increase the temperature?\" are common in human-human task-oriented\ndialogue and require world knowledge and pragmatic reasoning from the listener.\nWhile large language models (LLMs) can handle these requests effectively,\nsmaller models deployed on virtual assistants often struggle due to resource\nconstraints. Moreover, existing task-oriented dialogue benchmarks lack\nsufficient examples of complex discourse phenomena such as indirectness. To\naddress this, we propose a set of linguistic criteria along with an LLM-based\npipeline for generating realistic IURs to test natural language understanding\n(NLU) and dialogue state tracking (DST) models before deployment in a new\ndomain. We also release IndirectRequests, a dataset of IURs based on the Schema\nGuided Dialog (SGD) corpus, as a comparative testbed for evaluating the\nperformance of smaller models in handling indirect requests.", "published": "2024-06-12 01:18:04", "link": "http://arxiv.org/abs/2406.07794v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Are Large Language Models Good Statisticians?", "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities across\na range of scientific tasks including mathematics, physics, and chemistry.\nDespite their successes, the effectiveness of LLMs in handling complex\nstatistical tasks remains systematically under-explored. To bridge this gap, we\nintroduce StatQA, a new benchmark designed for statistical analysis tasks.\nStatQA comprises 11,623 examples tailored to evaluate LLMs' proficiency in\nspecialized statistical tasks and their applicability assessment capabilities,\nparticularly for hypothesis testing methods. We systematically experiment with\nrepresentative LLMs using various prompting strategies and show that even\nstate-of-the-art models such as GPT-4o achieve a best performance of only\n64.83%, indicating significant room for improvement. Notably, while open-source\nLLMs (e.g. LLaMA-3) show limited capability, those fine-tuned ones exhibit\nmarked improvements, outperforming all in-context learning-based methods (e.g.\nGPT-4o). Moreover, our comparative human experiments highlight a striking\ncontrast in error types between LLMs and humans: LLMs primarily make\napplicability errors, whereas humans mostly make statistical task confusion\nerrors. This divergence highlights distinct areas of proficiency and\ndeficiency, suggesting that combining LLM and human expertise could lead to\ncomplementary strengths, inviting further investigation into their\ncollaborative potential. Our source code and data are available at\nhttps://statqa.github.io/.", "published": "2024-06-12 02:23:51", "link": "http://arxiv.org/abs/2406.07815v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Tell Me What's Next: Textual Foresight for Generic UI Representations", "abstract": "Mobile app user interfaces (UIs) are rich with action, text, structure, and\nimage content that can be utilized to learn generic UI representations for\ntasks like automating user commands, summarizing content, and evaluating the\naccessibility of user interfaces. Prior work has learned strong visual\nrepresentations with local or global captioning losses, but fails to retain\nboth granularities. To combat this, we propose Textual Foresight, a novel\npretraining objective for learning UI screen representations. Textual Foresight\ngenerates global text descriptions of future UI states given a current UI and\nlocal action taken. Our approach requires joint reasoning over elements and\nentire screens, resulting in improved UI features: on generation tasks, UI\nagents trained with Textual Foresight outperform state-of-the-art by 2% with\n28x fewer images. We train with our newly constructed mobile app dataset,\nOpenApp, which results in the first public dataset for app UI representation\nlearning. OpenApp enables new baselines, and we find Textual Foresight improves\naverage task performance over them by 5.7% while having access to 2x less data.", "published": "2024-06-12 02:43:19", "link": "http://arxiv.org/abs/2406.07822v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Labeling Comic Mischief Content in Online Videos with a Multimodal\n  Hierarchical-Cross-Attention Model", "abstract": "We address the challenge of detecting questionable content in online media,\nspecifically the subcategory of comic mischief. This type of content combines\nelements such as violence, adult content, or sarcasm with humor, making it\ndifficult to detect. Employing a multimodal approach is vital to capture the\nsubtle details inherent in comic mischief content. To tackle this problem, we\npropose a novel end-to-end multimodal system for the task of comic mischief\ndetection. As part of this contribution, we release a novel dataset for the\ntargeted task consisting of three modalities: video, text (video captions and\nsubtitles), and audio. We also design a HIerarchical Cross-attention model with\nCAPtions (HICCAP) to capture the intricate relationships among these\nmodalities. The results show that the proposed approach makes a significant\nimprovement over robust baselines and state-of-the-art models for comic\nmischief detection and its type classification. This emphasizes the potential\nof our system to empower users, to make informed decisions about the online\ncontent they choose to see. In addition, we conduct experiments on the UCF101,\nHMDB51, and XD-Violence datasets, comparing our model against other\nstate-of-the-art approaches showcasing the outstanding performance of our\nproposed model in various scenarios.", "published": "2024-06-12 03:16:45", "link": "http://arxiv.org/abs/2406.07841v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Dual-Pipeline with Low-Rank Adaptation for New Language Integration in\n  Multilingual ASR", "abstract": "This paper addresses challenges in integrating new languages into a\npre-trained multilingual automatic speech recognition (mASR) system,\nparticularly in scenarios where training data for existing languages is limited\nor unavailable. The proposed method employs a dual-pipeline with low-rank\nadaptation (LoRA). It maintains two data flow pipelines-one for existing\nlanguages and another for new languages. The primary pipeline follows the\nstandard flow through the pre-trained parameters of mASR, while the secondary\npipeline additionally utilizes language-specific parameters represented by LoRA\nand a separate output decoder module. Importantly, the proposed approach\nminimizes the performance degradation of existing languages and enables a\nlanguage-agnostic operation mode, facilitated by a decoder selection strategy.\nWe validate the effectiveness of the proposed method by extending the\npre-trained Whisper model to 19 new languages from the FLEURS dataset", "published": "2024-06-12 03:17:57", "link": "http://arxiv.org/abs/2406.07842v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Dynamic Stochastic Decoding Strategy for Open-Domain Dialogue Generation", "abstract": "Stochastic sampling strategies such as top-k and top-p have been widely used\nin dialogue generation task. However, as an open-domain chatting system, there\nwill be two different conversation scenarios, i.e. chit-chat and\nknowledge-based question answering. In the former situation, responses\ndiversity is essential due to the one-to-many nature in dialogue. The latter,\non the other hand, requires less randomness given that stochastic decoding\nstrategy entails the risk of generating incorrect information. As a result, an\nadaptive and flexible decoding strategy is needed to cope with these two\nscenarios simultaneously. To this end, we propose the dynamic decoding strategy\n(DDS), which can adjust the decoding space w.r.t. different contexts. In DDS,\nboth sequence-level and token-level adaptive search can be achieved to adjust\nthe decoding process in a unified framework. Besides, our adaptive algorithm\ncan not only be used during model inference, but it can also be applied during\nthe model training stage to further enhance the performance. Comprehensive\nexperiments indicate that the proposed decoding strategy can consistently\nimprove the performance of pre-trained dialogue models when coupled with four\nwell-used stochastic decoding algorithms.", "published": "2024-06-12 03:38:45", "link": "http://arxiv.org/abs/2406.07850v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Empirical Study of Mamba-based Language Models", "abstract": "Selective state-space models (SSMs) like Mamba overcome some of the\nshortcomings of Transformers, such as quadratic computational complexity with\nsequence length and large inference-time memory requirements from the key-value\ncache. Moreover, recent studies have shown that SSMs can match or exceed the\nlanguage modeling capabilities of Transformers, making them an attractive\nalternative. In a controlled setting (e.g., same data), however, studies so far\nhave only presented small scale experiments comparing SSMs to Transformers. To\nunderstand the strengths and weaknesses of these architectures at larger\nscales, we present a direct comparison between 8B-parameter Mamba, Mamba-2, and\nTransformer models trained on the same datasets of up to 3.5T tokens. We also\ncompare these models to a hybrid architecture consisting of 43% Mamba-2, 7%\nattention, and 50% MLP layers (Mamba-2-Hybrid). Using a diverse set of tasks,\nwe answer the question of whether Mamba models can match Transformers at larger\ntraining budgets. Our results show that while pure SSMs match or exceed\nTransformers on many tasks, they lag behind Transformers on tasks which require\nstrong copying or in-context learning abilities (e.g., 5-shot MMLU, Phonebook)\nor long-context reasoning. In contrast, we find that the 8B Mamba-2-Hybrid\nexceeds the 8B Transformer on all 12 standard tasks we evaluated (+2.65 points\non average) and is predicted to be up to 8x faster when generating tokens at\ninference time. To validate long-context capabilities, we provide additional\nexperiments evaluating variants of the Mamba-2-Hybrid and Transformer extended\nto support 16K, 32K, and 128K sequences. On an additional 23 long-context\ntasks, the hybrid model continues to closely match or exceed the Transformer on\naverage. To enable further study, we release the checkpoints as well as the\ncode used to train our models as part of NVIDIA's Megatron-LM project.", "published": "2024-06-12 05:25:15", "link": "http://arxiv.org/abs/2406.07887v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "DeTriever: Decoder-representation-based Retriever for Improving NL2SQL\n  In-Context Learning", "abstract": "While in-context Learning (ICL) has proven to be an effective technique to\nimprove the performance of Large Language Models (LLMs) in a variety of complex\ntasks, notably in translating natural language questions into Structured Query\nLanguage (NL2SQL), the question of how to select the most beneficial\ndemonstration examples remains an open research problem. While prior works\noften adapted off-the-shelf encoders to retrieve examples dynamically, an\ninherent discrepancy exists in the representational capacities between the\nexternal retrievers and the LLMs. Further, optimizing the selection of examples\nis a non-trivial task, since there are no straightforward methods to assess the\nrelative benefits of examples without performing pairwise inference. To address\nthese shortcomings, we propose DeTriever, a novel demonstration retrieval\nframework that learns a weighted combination of LLM hidden states, where rich\nsemantic information is encoded. To train the model, we propose a proxy score\nthat estimates the relative benefits of examples based on the similarities\nbetween output queries. Experiments on two popular NL2SQL benchmarks\ndemonstrate that our method significantly outperforms the state-of-the-art\nbaselines on one-shot NL2SQL tasks.", "published": "2024-06-12 06:33:54", "link": "http://arxiv.org/abs/2406.07913v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Defining and Detecting Vulnerability in Human Evaluation Guidelines: A\n  Preliminary Study Towards Reliable NLG Evaluation", "abstract": "Human evaluation serves as the gold standard for assessing the quality of\nNatural Language Generation (NLG) systems. Nevertheless, the evaluation\nguideline, as a pivotal element ensuring reliable and reproducible human\nassessment, has received limited attention.Our investigation revealed that only\n29.84% of recent papers involving human evaluation at top conferences release\ntheir evaluation guidelines, with vulnerabilities identified in 77.09% of these\nguidelines. Unreliable evaluation guidelines can yield inaccurate assessment\noutcomes, potentially impeding the advancement of NLG in the right direction.\nTo address these challenges, we take an initial step towards reliable\nevaluation guidelines and propose the first human evaluation guideline dataset\nby collecting annotations of guidelines extracted from existing papers as well\nas generated via Large Language Models (LLMs). We then introduce a taxonomy of\neight vulnerabilities and formulate a principle for composing evaluation\nguidelines. Furthermore, a method for detecting guideline vulnerabilities has\nbeen explored using LLMs, and we offer a set of recommendations to enhance\nreliability in human evaluation. The annotated human evaluation guideline\ndataset and code for the vulnerability detection method are publicly available\nonline.", "published": "2024-06-12 06:59:31", "link": "http://arxiv.org/abs/2406.07935v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Toward a Method to Generate Capability Ontologies from Natural Language\n  Descriptions", "abstract": "To achieve a flexible and adaptable system, capability ontologies are\nincreasingly leveraged to describe functions in a machine-interpretable way.\nHowever, modeling such complex ontological descriptions is still a manual and\nerror-prone task that requires a significant amount of effort and ontology\nexpertise. This contribution presents an innovative method to automate\ncapability ontology modeling using Large Language Models (LLMs), which have\nproven to be well suited for such tasks. Our approach requires only a natural\nlanguage description of a capability, which is then automatically inserted into\na predefined prompt using a few-shot prompting technique. After prompting an\nLLM, the resulting capability ontology is automatically verified through\nvarious steps in a loop with the LLM to check the overall correctness of the\ncapability ontology. First, a syntax check is performed, then a check for\ncontradictions, and finally a check for hallucinations and missing ontology\nelements. Our method greatly reduces manual effort, as only the initial natural\nlanguage description and a final human review and possible correction are\nnecessary, thereby streamlining the capability ontology generation process.", "published": "2024-06-12 07:41:44", "link": "http://arxiv.org/abs/2406.07962v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Better than Random: Reliable NLG Human Evaluation with Constrained\n  Active Sampling", "abstract": "Human evaluation is viewed as a reliable evaluation method for NLG which is\nexpensive and time-consuming. To save labor and costs, researchers usually\nperform human evaluation on a small subset of data sampled from the whole\ndataset in practice. However, different selection subsets will lead to\ndifferent rankings of the systems. To give a more correct inter-system ranking\nand make the gold standard human evaluation more reliable, we propose a\nConstrained Active Sampling Framework (CASF) for reliable human judgment. CASF\noperates through a Learner, a Systematic Sampler and a Constrained Controller\nto select representative samples for getting a more correct inter-system\nranking.Experiment results on 137 real NLG evaluation setups with 44 human\nevaluation metrics across 16 datasets and 5 NLG tasks demonstrate CASF receives\n93.18% top-ranked system recognition accuracy and ranks first or ranks second\non 90.91% of the human metrics with 0.83 overall inter-system ranking Kendall\ncorrelation.Code and data are publicly available online.", "published": "2024-06-12 07:44:36", "link": "http://arxiv.org/abs/2406.07967v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Adversarial Evasion Attack Efficiency against Large Language Models", "abstract": "Large Language Models (LLMs) are valuable for text classification, but their\nvulnerabilities must not be disregarded. They lack robustness against\nadversarial examples, so it is pertinent to understand the impacts of different\ntypes of perturbations, and assess if those attacks could be replicated by\ncommon users with a small amount of perturbations and a small number of queries\nto a deployed LLM. This work presents an analysis of the effectiveness,\nefficiency, and practicality of three different types of adversarial attacks\nagainst five different LLMs in a sentiment classification task. The obtained\nresults demonstrated the very distinct impacts of the word-level and\ncharacter-level attacks. The word attacks were more effective, but the\ncharacter and more constrained attacks were more practical and required a\nreduced number of perturbations and queries. These differences need to be\nconsidered during the development of adversarial defense strategies to train\nmore robust LLMs for intelligent text classification applications.", "published": "2024-06-12 10:02:27", "link": "http://arxiv.org/abs/2406.08050v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multimodal Table Understanding", "abstract": "Although great progress has been made by previous table understanding methods\nincluding recent approaches based on large language models (LLMs), they rely\nheavily on the premise that given tables must be converted into a certain text\nsequence (such as Markdown or HTML) to serve as model input. However, it is\ndifficult to access such high-quality textual table representations in some\nreal-world scenarios, and table images are much more accessible. Therefore, how\nto directly understand tables using intuitive visual information is a crucial\nand urgent challenge for developing more practical applications. In this paper,\nwe propose a new problem, multimodal table understanding, where the model needs\nto generate correct responses to various table-related requests based on the\ngiven table image. To facilitate both the model training and evaluation, we\nconstruct a large-scale dataset named MMTab, which covers a wide spectrum of\ntable images, instructions and tasks. On this basis, we develop Table-LLaVA, a\ngeneralist tabular multimodal large language model (MLLM), which significantly\noutperforms recent open-source MLLM baselines on 23 benchmarks under held-in\nand held-out settings. The code and data is available at this\nhttps://github.com/SpursGoZmy/Table-LLaVA", "published": "2024-06-12 11:27:03", "link": "http://arxiv.org/abs/2406.08100v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Supportiveness-based Knowledge Rewriting for Retrieval-augmented\n  Language Modeling", "abstract": "Retrieval-augmented language models (RALMs) have recently shown great\npotential in mitigating the limitations of implicit knowledge in LLMs, such as\nuntimely updating of the latest expertise and unreliable retention of long-tail\nknowledge. However, since the external knowledge base, as well as the\nretriever, can not guarantee reliability, potentially leading to the knowledge\nretrieved not being helpful or even misleading for LLM generation. In this\npaper, we introduce Supportiveness-based Knowledge Rewriting (SKR), a robust\nand pluggable knowledge rewriter inherently optimized for LLM generation.\nSpecifically, we introduce the novel concept of \"supportiveness\"--which\nrepresents how effectively a knowledge piece facilitates downstream tasks--by\nconsidering the perplexity impact of augmented knowledge on the response text\nof a white-box LLM. Based on knowledge supportiveness, we first design a\ntraining data curation strategy for our rewriter model, effectively identifying\nand filtering out poor or irrelevant rewrites (e.g., with low supportiveness\nscores) to improve data efficacy. We then introduce the direct preference\noptimization (DPO) algorithm to align the generated rewrites to optimal\nsupportiveness, guiding the rewriter model to summarize augmented content that\nbetter improves the final response. Comprehensive evaluations across six\npopular knowledge-intensive tasks and four LLMs have demonstrated the\neffectiveness and superiority of SKR. With only 7B parameters, SKR has shown\nbetter knowledge rewriting capability over GPT-4, the current state-of-the-art\ngeneral-purpose LLM.", "published": "2024-06-12 11:52:35", "link": "http://arxiv.org/abs/2406.08116v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Legend: Leveraging Representation Engineering to Annotate Safety Margin\n  for Preference Datasets", "abstract": "The success of the reward model in distinguishing between responses with\nsubtle safety differences depends critically on the high-quality preference\ndataset, which should capture the fine-grained nuances of harmful and harmless\nresponses. This motivates the need to develop a dataset involving preference\nmargins, which accurately quantify how harmless one response is compared to\nanother. In this paper, we take the first step to propose an effective and\ncost-efficient framework to promote the margin-enhanced preference dataset\ndevelopment. Our framework, Legend, Leverages representation engineering to\nannotate preference datasets. It constructs the specific direction within the\nLLM's embedding space that represents safety. By leveraging this safety\ndirection, Legend can then leverage the semantic distances of paired responses\nalong this direction to annotate margins automatically. We experimentally\ndemonstrate our effectiveness in both reward modeling and harmless alignment\nfor LLMs. Legend also stands out for its efficiency, requiring only the\ninference time rather than additional training. This efficiency allows for\neasier implementation and scalability, making Legend particularly valuable for\npractical applications in aligning LLMs with safe conversations.", "published": "2024-06-12 12:06:32", "link": "http://arxiv.org/abs/2406.08124v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Research Trends for the Interplay between Large Language Models and\n  Knowledge Graphs", "abstract": "This survey investigates the synergistic relationship between Large Language\nModels (LLMs) and Knowledge Graphs (KGs), which is crucial for advancing AI's\ncapabilities in understanding, reasoning, and language processing. It aims to\naddress gaps in current research by exploring areas such as KG Question\nAnswering, ontology generation, KG validation, and the enhancement of KG\naccuracy and consistency through LLMs. The paper further examines the roles of\nLLMs in generating descriptive texts and natural language queries for KGs.\nThrough a structured analysis that includes categorizing LLM-KG interactions,\nexamining methodologies, and investigating collaborative uses and potential\nbiases, this study seeks to provide new insights into the combined potential of\nLLMs and KGs. It highlights the importance of their interaction for improving\nAI applications and outlines future research directions.", "published": "2024-06-12 13:52:38", "link": "http://arxiv.org/abs/2406.08223v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "TasTe: Teaching Large Language Models to Translate through\n  Self-Reflection", "abstract": "Large language models (LLMs) have exhibited remarkable performance in various\nnatural language processing tasks. Techniques like instruction tuning have\neffectively enhanced the proficiency of LLMs in the downstream task of machine\ntranslation. However, the existing approaches fail to yield satisfactory\ntranslation outputs that match the quality of supervised neural machine\ntranslation (NMT) systems. One plausible explanation for this discrepancy is\nthat the straightforward prompts employed in these methodologies are unable to\nfully exploit the acquired instruction-following capabilities. To this end, we\npropose the TasTe framework, which stands for translating through\nself-reflection. The self-reflection process includes two stages of inference.\nIn the first stage, LLMs are instructed to generate preliminary translations\nand conduct self-assessments on these translations simultaneously. In the\nsecond stage, LLMs are tasked to refine these preliminary translations\naccording to the evaluation results. The evaluation results in four language\ndirections on the WMT22 benchmark reveal the effectiveness of our approach\ncompared to existing methods. Our work presents a promising approach to unleash\nthe potential of LLMs and enhance their capabilities in MT. The codes and\ndatasets are open-sourced at https://github.com/YutongWang1216/ReflectionLLMMT.", "published": "2024-06-12 17:21:21", "link": "http://arxiv.org/abs/2406.08434v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "OLMES: A Standard for Language Model Evaluations", "abstract": "Progress in AI is often demonstrated by new models claiming improved\nperformance on tasks measuring model capabilities. Evaluating language models\ncan be particularly challenging, as choices of how a model is evaluated on a\ntask can lead to large changes in measured performance. There is no common\nstandard setup, so different models are evaluated on the same tasks in\ndifferent ways, leading to claims about which models perform best not being\nreproducible. We propose OLMES, a completely documented, practical, open\nstandard for reproducible LLM evaluations. In developing this standard, we\nidentify and review the varying factors in evaluation practices adopted by the\ncommunity - such as details of prompt formatting, choice of in-context\nexamples, probability normalizations, and task formulation. In particular,\nOLMES supports meaningful comparisons between smaller base models that require\nthe unnatural \"cloze\" formulation of multiple-choice questions against larger\nmodels that can utilize the original formulation. OLMES includes\nwell-considered, documented recommendations guided by results from existing\nliterature as well as new experiments resolving open questions.", "published": "2024-06-12 17:37:09", "link": "http://arxiv.org/abs/2406.08446v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs\n  with Nothing", "abstract": "High-quality instruction data is critical for aligning large language models\n(LLMs). Although some models, such as Llama-3-Instruct, have open weights,\ntheir alignment data remain private, which hinders the democratization of AI.\nHigh human labor costs and a limited, predefined scope for prompting prevent\nexisting open-source data creation methods from scaling effectively,\npotentially limiting the diversity and quality of public alignment datasets. Is\nit possible to synthesize high-quality instruction data at scale by extracting\nit directly from an aligned LLM? We present a self-synthesis method for\ngenerating large-scale alignment data named Magpie. Our key observation is that\naligned LLMs like Llama-3-Instruct can generate a user query when we input only\nthe left-side templates up to the position reserved for user messages, thanks\nto their auto-regressive nature. We use this method to prompt Llama-3-Instruct\nand generate 4 million instructions along with their corresponding responses.\nWe perform a comprehensive analysis of the extracted data and select 300K\nhigh-quality instances. To compare Magpie data with other public instruction\ndatasets, we fine-tune Llama-3-8B-Base with each dataset and evaluate the\nperformance of the fine-tuned models. Our results indicate that in some tasks,\nmodels fine-tuned with Magpie perform comparably to the official\nLlama-3-8B-Instruct, despite the latter being enhanced with 10 million data\npoints through supervised fine-tuning (SFT) and subsequent feedback learning.\nWe also show that using Magpie solely for SFT can surpass the performance of\nprevious public datasets utilized for both SFT and preference optimization,\nsuch as direct preference optimization with UltraFeedback. This advantage is\nevident on alignment benchmarks such as AlpacaEval, ArenaHard, and WildBench.", "published": "2024-06-12 17:52:30", "link": "http://arxiv.org/abs/2406.08464v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "What If We Recaption Billions of Web Images with LLaMA-3?", "abstract": "Web-crawled image-text pairs are inherently noisy. Prior studies demonstrate\nthat semantically aligning and enriching textual descriptions of these pairs\ncan significantly enhance model training across various vision-language tasks,\nparticularly text-to-image generation. However, large-scale investigations in\nthis area remain predominantly closed-source. Our paper aims to bridge this\ncommunity effort, leveraging the powerful and \\textit{open-sourced} LLaMA-3, a\nGPT-4 level LLM. Our recaptioning pipeline is simple: first, we fine-tune a\nLLaMA-3-8B powered LLaVA-1.5 and then employ it to recaption 1.3 billion images\nfrom the DataComp-1B dataset. Our empirical results confirm that this enhanced\ndataset, Recap-DataComp-1B, offers substantial benefits in training advanced\nvision-language models. For discriminative models like CLIP, we observe\nenhanced zero-shot performance in cross-modal retrieval tasks. For generative\nmodels like text-to-image Diffusion Transformers, the generated images exhibit\na significant improvement in alignment with users' text instructions,\nespecially in following complex queries. Our project page is\nhttps://www.haqtu.me/Recap-Datacomp-1B/", "published": "2024-06-12 17:59:07", "link": "http://arxiv.org/abs/2406.08478v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Words Worth a Thousand Pictures: Measuring and Understanding Perceptual\n  Variability in Text-to-Image Generation", "abstract": "Diffusion models are the state of the art in text-to-image generation, but\ntheir perceptual variability remains understudied. In this paper, we examine\nhow prompts affect image variability in black-box diffusion-based models. We\npropose W1KP, a human-calibrated measure of variability in a set of images,\nbootstrapped from existing image-pair perceptual distances. Current datasets do\nnot cover recent diffusion models, thus we curate three test sets for\nevaluation. Our best perceptual distance outperforms nine baselines by up to 18\npoints in accuracy, and our calibration matches graded human judgements 78% of\nthe time. Using W1KP, we study prompt reusability and show that Imagen prompts\ncan be reused for 10-50 random seeds before new images become too similar to\nalready generated images, while Stable Diffusion XL and DALL-E 3 can be reused\n50-200 times. Lastly, we analyze 56 linguistic features of real prompts,\nfinding that the prompt's length, CLIP embedding norm, concreteness, and word\nsenses influence variability most. As far as we are aware, we are the first to\nanalyze diffusion variability from a visuolinguistic perspective. Our project\npage is at http://w1kp.com.", "published": "2024-06-12 17:59:27", "link": "http://arxiv.org/abs/2406.08482v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Language Model Council: Democratically Benchmarking Foundation Models on\n  Highly Subjective Tasks", "abstract": "As Large Language Models (LLMs) continue to evolve, evaluating them remains a\npersistent challenge. Many recent evaluations use LLMs as judges to score\noutputs from other LLMs, often relying on a single large model like GPT-4o.\nHowever, using a single LLM judge is prone to intra-model bias, and many tasks\n- such as those related to emotional intelligence, creative writing, and\npersuasiveness - may be too subjective for a single model to judge fairly. We\nintroduce the Language Model Council (LMC), where a group of LLMs collaborate\nto create tests, respond to them, and evaluate each other's responses to\nproduce a ranking in a democratic fashion. Unlike previous approaches that\nfocus on reducing cost or bias by using a panel of smaller models, our work\nexamines the benefits and nuances of a fully inclusive LLM evaluation system.\nIn a detailed case study on emotional intelligence, we deploy a council of 20\nrecent LLMs to rank each other on open-ended responses to interpersonal\nconflicts. Our results show that the LMC produces rankings that are more\nseparable and more robust, and through a user study, we show that they are more\nconsistent with human evaluations than any individual LLM judge. Using all LLMs\nfor judging can be costly, however, so we use Monte Carlo simulations and\nhand-curated sub-councils to study hypothetical council compositions and\ndiscuss the value of the incremental LLM judge.", "published": "2024-06-12 19:05:43", "link": "http://arxiv.org/abs/2406.08598v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Generative Marker Enhanced End-to-End Framework for Argument Mining", "abstract": "Argument Mining (AM) involves identifying and extracting Argumentative\nComponents (ACs) and their corresponding Argumentative Relations (ARs). Most of\nthe prior works have broken down these tasks into multiple sub-tasks. Existing\nend-to-end setups primarily use the dependency parsing approach. This work\nintroduces a generative paradigm-based end-to-end framework argTANL. argTANL\nframes the argumentative structures into label-augmented text, called Augmented\nNatural Language (ANL). This framework jointly extracts both ACs and ARs from a\ngiven argumentative text. Additionally, this study explores the impact of\nArgumentative and Discourse markers on enhancing the model's performance within\nthe proposed framework. Two distinct frameworks, Marker-Enhanced argTANL\n(ME-argTANL) and argTANL with specialized Marker-Based Fine-Tuning, are\nproposed to achieve this. Extensive experiments are conducted on three standard\nAM benchmarks to demonstrate the superior performance of the ME-argTANL.", "published": "2024-06-12 19:22:29", "link": "http://arxiv.org/abs/2406.08606v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Reversing the Forget-Retain Objectives: An Efficient LLM Unlearning\n  Framework from Logit Difference", "abstract": "As Large Language Models (LLMs) demonstrate extensive capability in learning\nfrom documents, LLM unlearning becomes an increasingly important research area\nto address concerns of LLMs in terms of privacy, copyright, etc. A conventional\nLLM unlearning task typically involves two goals: (1) The target LLM should\nforget the knowledge in the specified forget documents, and (2) it should\nretain the other knowledge that the LLM possesses, for which we assume access\nto a small number of retain documents. To achieve both goals, a mainstream\nclass of LLM unlearning methods introduces an optimization framework with a\ncombination of two objectives - maximizing the prediction loss on the forget\ndocuments while minimizing that on the retain documents, which suffers from two\nchallenges, degenerated output and catastrophic forgetting. In this paper, we\npropose a novel unlearning framework called Unlearning from Logit Difference\n(ULD), which introduces an assistant LLM that aims to achieve the opposite of\nthe unlearning goals: remembering the forget documents and forgetting the\nretain knowledge. ULD then derives the unlearned LLM by computing the logit\ndifference between the target and the assistant LLMs. We show that such\nreversed objectives would naturally resolve both aforementioned challenges\nwhile significantly improving the training efficiency. Extensive experiments\ndemonstrate that our method efficiently achieves the intended forgetting while\npreserving the LLM's overall capabilities, reducing training time by more than\nthreefold. Notably, our method loses 0% of model utility on the ToFU benchmark,\nwhereas baseline methods may sacrifice 17% of utility on average to achieve\ncomparable forget quality. Our code will be publicly available at\nhttps://github.com/UCSB-NLP-Chang/ULD.", "published": "2024-06-12 19:26:35", "link": "http://arxiv.org/abs/2406.08607v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Time-MMD: Multi-Domain Multimodal Dataset for Time Series Analysis", "abstract": "Time series data are ubiquitous across a wide range of real-world domains.\nWhile real-world time series analysis (TSA) requires human experts to integrate\nnumerical series data with multimodal domain-specific knowledge, most existing\nTSA models rely solely on numerical data, overlooking the significance of\ninformation beyond numerical series. This oversight is due to the untapped\npotential of textual series data and the absence of a comprehensive,\nhigh-quality multimodal dataset. To overcome this obstacle, we introduce\nTime-MMD, the first multi-domain, multimodal time series dataset covering 9\nprimary data domains. Time-MMD ensures fine-grained modality alignment,\neliminates data contamination, and provides high usability. Additionally, we\ndevelop MM-TSFlib, the first-cut multimodal time-series forecasting (TSF)\nlibrary, seamlessly pipelining multimodal TSF evaluations based on Time-MMD for\nin-depth analyses. Extensive experiments conducted on Time-MMD through\nMM-TSFlib demonstrate significant performance enhancements by extending\nunimodal TSF to multimodality, evidenced by over 15% mean squared error\nreduction in general, and up to 40% in domains with rich textual data. More\nimportantly, our datasets and library revolutionize broader applications,\nimpacts, research topics to advance TSA. The dataset is available at\nhttps://github.com/AdityaLab/Time-MMD.", "published": "2024-06-12 20:20:09", "link": "http://arxiv.org/abs/2406.08627v4", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Fine-Tuned 'Small' LLMs (Still) Significantly Outperform Zero-Shot\n  Generative AI Models in Text Classification", "abstract": "Generative AI offers a simple, prompt-based alternative to fine-tuning\nsmaller BERT-style LLMs for text classification tasks. This promises to\neliminate the need for manually labeled training data and task-specific model\ntraining. However, it remains an open question whether tools like ChatGPT can\ndeliver on this promise. In this paper, we show that smaller, fine-tuned LLMs\n(still) consistently and significantly outperform larger, zero-shot prompted\nmodels in text classification. We compare three major generative AI models\n(ChatGPT with GPT-3.5/GPT-4 and Claude Opus) with several fine-tuned LLMs\nacross a diverse set of classification tasks (sentiment, approval/disapproval,\nemotions, party positions) and text categories (news, tweets, speeches). We\nfind that fine-tuning with application-specific training data achieves superior\nperformance in all cases. To make this approach more accessible to a broader\naudience, we provide an easy-to-use toolkit alongside this paper. Our toolkit,\naccompanied by non-technical step-by-step guidance, enables users to select and\nfine-tune BERT-like LLMs for any classification task with minimal technical and\ncomputational effort.", "published": "2024-06-12 21:46:13", "link": "http://arxiv.org/abs/2406.08660v2", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Prompt-Based Length Controlled Generation with Multiple Control Types", "abstract": "Large language models (LLMs) have attracted great attention given their\nstrong performance on a wide range of NLP tasks. In practice, users often\nexpect generated texts to fall within a specific length range, making length\ncontrolled generation an important topic, especially for GPT-style models.\nExisting length control methods mostly focus on a simple control type of \"equal\nto\" a target length. Different from them, we propose a prompt-based method to\nachieve length controlled generation under different control types with high\naccuracy. In particular, we adopt reinforcement learning (RL) and sample\nfiltering with the reward signal given by rule-based reward models, which\nenhances the length control ability of models by rewarding outputs that follow\ncertain control instructions. In addition, we introduce a standard prompt\nextractor to parse arbitrary users' input into standard control instructions.\nExperiments show that our method significantly improves the accuracy of\nprompt-based length control on popular summarization datasets like CNNDM and\nNYT under multiple control types. Moreover, both the standard prompt extractor\nand RL-tuned model show strong generalization to unseen control prompt\ntemplates.", "published": "2024-06-12 01:49:54", "link": "http://arxiv.org/abs/2406.10278v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Do as I do (Safely): Mitigating Task-Specific Fine-tuning Risks in Large\n  Language Models", "abstract": "Recent research shows that fine-tuning on benign instruction-following data\ncan inadvertently undo the safety alignment process and increase a model's\npropensity to comply with harmful queries. While instruction-following\nfine-tuning is important, task-specific fine-tuning - where models are trained\non datasets with clear ground truth answers (e.g., multiple choice questions) -\ncan enhance model performance on specialized downstream tasks. Understanding\nand mitigating safety risks in the task-specific setting remains distinct from\nthe instruction-following context due to structural differences in the data.\nOur work demonstrates how malicious actors can subtly manipulate the structure\nof almost any task-specific dataset to foster significantly more dangerous\nmodel behaviors, while maintaining an appearance of innocuity and reasonable\ndownstream task performance. To address this issue, we propose a novel\nmitigation strategy that mixes in safety data which mimics the task format and\nprompting style of the user data, showing this is significantly more effective\nand efficient than existing baselines at re-establishing safety alignment while\nmaintaining similar task performance.", "published": "2024-06-12 18:33:11", "link": "http://arxiv.org/abs/2406.10288v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Reconciling Kaplan and Chinchilla Scaling Laws", "abstract": "Kaplan et al. [2020] (`Kaplan') and Hoffmann et al. [2022] (`Chinchilla')\nstudied the scaling behavior of transformers trained on next-token language\nprediction. These studies produced different estimates for how the number of\nparameters ($N$) and training tokens ($D$) should be set to achieve the lowest\npossible loss for a given compute budget ($C$). Kaplan: $N_\\text{optimal}\n\\propto C^{0.73}$, Chinchilla: $N_\\text{optimal} \\propto C^{0.50}$. This paper\nfinds that much of this discrepancy can be attributed to Kaplan counting\nnon-embedding rather than total parameters, combined with their analysis being\nperformed at small scale. Simulating the Chinchilla study under these\nconditions produces biased scaling coefficients close to Kaplan's. Hence, this\npaper reaffirms Chinchilla's scaling coefficients, by explaining the primary\ncause of Kaplan's original overestimation. As a second contribution, the paper\nexplains differences in the reported relationships between loss and compute.\nThese findings lead us to recommend that future scaling studies use total\nparameters and compute.", "published": "2024-06-12 13:30:48", "link": "http://arxiv.org/abs/2406.12907v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Analyzing Multi-Head Attention on Trojan BERT Models", "abstract": "This project investigates the behavior of multi-head attention in Transformer\nmodels, specifically focusing on the differences between benign and trojan\nmodels in the context of sentiment analysis. Trojan attacks cause models to\nperform normally on clean inputs but exhibit misclassifications when presented\nwith inputs containing predefined triggers. We characterize attention head\nfunctions in trojan and benign models, identifying specific 'trojan' heads and\nanalyzing their behavior.", "published": "2024-06-12 06:43:59", "link": "http://arxiv.org/abs/2406.16925v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Study of Backdoors in Instruction Fine-tuned Language Models", "abstract": "Backdoor data poisoning, inserted within instruction examples used to\nfine-tune a foundation Large Language Model (LLM) for downstream tasks\n(\\textit{e.g.,} sentiment prediction), is a serious security concern due to the\nevasive nature of such attacks. The poisoning is usually in the form of a\n(seemingly innocuous) trigger word or phrase inserted into a very small\nfraction of the fine-tuning samples from a target class. Such backdoor attacks\ncan: alter response sentiment, violate censorship, over-refuse (invoke\ncensorship for legitimate queries), inject false content, or trigger nonsense\nresponses (hallucinations). In this work we investigate the efficacy of\ninstruction fine-tuning backdoor attacks as attack \"hyperparameters\" are varied\nunder a variety of scenarios, considering: the trigger location in the poisoned\nexamples; robustness to change in the trigger location, partial triggers, and\nsynonym substitutions at test time; attack transfer from one (fine-tuning)\ndomain to a related test domain; and clean-label vs. dirty-label poisoning.\nBased on our observations, we propose and evaluate two defenses against these\nattacks: i) a \\textit{during-fine-tuning defense} based on word-frequency\ncounts that assumes the (possibly poisoned) fine-tuning dataset is available\nand identifies the backdoor trigger tokens; and ii) a \\textit{post-fine-tuning\ndefense} based on downstream clean fine-tuning of the backdoored LLM with a\nsmall defense dataset. Finally, we provide a brief survey of related work on\nbackdoor attacks and defenses.", "published": "2024-06-12 00:01:32", "link": "http://arxiv.org/abs/2406.07778v2", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "PolySpeech: Exploring Unified Multitask Speech Models for\n  Competitiveness with Single-task Models", "abstract": "Recently, there have been attempts to integrate various speech processing\ntasks into a unified model. However, few previous works directly demonstrated\nthat joint optimization of diverse tasks in multitask speech models has\npositive influence on the performance of individual tasks. In this paper we\npresent a multitask speech model -- PolySpeech, which supports speech\nrecognition, speech synthesis, and two speech classification tasks. PolySpeech\ntakes multi-modal language model as its core structure and uses semantic\nrepresentations as speech inputs. We introduce semantic speech embedding\ntokenization and speech reconstruction methods to PolySpeech, enabling\nefficient generation of high-quality speech for any given speaker. PolySpeech\nshows competitiveness across various tasks compared to single-task models. In\nour experiments, multitask optimization achieves performance comparable to\nsingle-task optimization and is especially beneficial for specific tasks.", "published": "2024-06-12 01:35:46", "link": "http://arxiv.org/abs/2406.07801v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "To be Continuous, or to be Discrete, Those are Bits of Questions", "abstract": "Recently, binary representation has been proposed as a novel representation\nthat lies between continuous and discrete representations. It exhibits\nconsiderable information-preserving capability when being used to replace\ncontinuous input vectors. In this paper, we investigate the feasibility of\nfurther introducing it to the output side, aiming to allow models to output\nbinary labels instead. To preserve the structural information on the output\nside along with label information, we extend the previous contrastive hashing\nmethod as structured contrastive hashing. More specifically, we upgrade CKY\nfrom label-level to bit-level, define a new similarity function with span\nmarginal probabilities, and introduce a novel contrastive loss function with a\ncarefully designed instance selection strategy. Our model achieves competitive\nperformance on various structured prediction tasks, and demonstrates that\nbinary representation can be considered a novel representation that further\nbridges the gap between the continuous nature of deep learning and the discrete\nintrinsic property of natural languages.", "published": "2024-06-12 02:08:45", "link": "http://arxiv.org/abs/2406.07812v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Collective Constitutional AI: Aligning a Language Model with Public\n  Input", "abstract": "There is growing consensus that language model (LM) developers should not be\nthe sole deciders of LM behavior, creating a need for methods that enable the\nbroader public to collectively shape the behavior of LM systems that affect\nthem. To address this need, we present Collective Constitutional AI (CCAI): a\nmulti-stage process for sourcing and integrating public input into LMs-from\nidentifying a target population to sourcing principles to training and\nevaluating a model. We demonstrate the real-world practicality of this approach\nby creating what is, to our knowledge, the first LM fine-tuned with\ncollectively sourced public input and evaluating this model against a baseline\nmodel trained with established principles from a LM developer. Our quantitative\nevaluations demonstrate several benefits of our approach: the CCAI-trained\nmodel shows lower bias across nine social dimensions compared to the baseline\nmodel, while maintaining equivalent performance on language, math, and\nhelpful-harmless evaluations. Qualitative comparisons of the models suggest\nthat the models differ on the basis of their respective constitutions, e.g.,\nwhen prompted with contentious topics, the CCAI-trained model tends to generate\nresponses that reframe the matter positively instead of a refusal. These\nresults demonstrate a promising, tractable pathway toward publicly informed\ndevelopment of language models.", "published": "2024-06-12 02:20:46", "link": "http://arxiv.org/abs/2406.07814v1", "categories": ["cs.AI", "cs.CL", "cs.HC", "I.2.7; K.4.2"], "primary_category": "cs.AI"}
{"title": "Spoof Diarization: \"What Spoofed When\" in Partially Spoofed Audio", "abstract": "This paper defines Spoof Diarization as a novel task in the Partial Spoof\n(PS) scenario. It aims to determine what spoofed when, which includes not only\nlocating spoof regions but also clustering them according to different spoofing\nmethods. As a pioneering study in spoof diarization, we focus on defining the\ntask, establishing evaluation metrics, and proposing a benchmark model, namely\nthe Countermeasure-Condition Clustering (3C) model. Utilizing this model, we\nfirst explore how to effectively train countermeasures to support spoof\ndiarization using three labeling schemes. We then utilize spoof localization\npredictions to enhance the diarization performance. This first study reveals\nthe high complexity of the task, even in restricted scenarios where only a\nsingle speaker per audio file and an oracle number of spoofing methods are\nconsidered. Our code is available at\nhttps://github.com/nii-yamagishilab/PartialSpoof.", "published": "2024-06-12 02:23:57", "link": "http://arxiv.org/abs/2406.07816v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "PRoDeliberation: Parallel Robust Deliberation for End-to-End Spoken\n  Language Understanding", "abstract": "Spoken Language Understanding (SLU) is a critical component of voice\nassistants; it consists of converting speech to semantic parses for task\nexecution. Previous works have explored end-to-end models to improve the\nquality and robustness of SLU models with Deliberation, however these models\nhave remained autoregressive, resulting in higher latencies. In this work we\nintroduce PRoDeliberation, a novel method leveraging a Connectionist Temporal\nClassification-based decoding strategy as well as a denoising objective to\ntrain robust non-autoregressive deliberation models. We show that\nPRoDeliberation achieves the latency reduction of parallel decoding (2-10x\nimprovement over autoregressive models) while retaining the ability to correct\nAutomatic Speech Recognition (ASR) mistranscriptions of autoregressive\ndeliberation systems. We further show that the design of the denoising training\nallows PRoDeliberation to overcome the limitations of small ASR devices, and we\nprovide analysis on the necessity of each component of the system.", "published": "2024-06-12 02:46:17", "link": "http://arxiv.org/abs/2406.07823v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "VALL-E R: Robust and Efficient Zero-Shot Text-to-Speech Synthesis via\n  Monotonic Alignment", "abstract": "With the help of discrete neural audio codecs, large language models (LLM)\nhave increasingly been recognized as a promising methodology for zero-shot\nText-to-Speech (TTS) synthesis. However, sampling based decoding strategies\nbring astonishing diversity to generation, but also pose robustness issues such\nas typos, omissions and repetition. In addition, the high sampling rate of\naudio also brings huge computational overhead to the inference process of\nautoregression. To address these issues, we propose VALL-E R, a robust and\nefficient zero-shot TTS system, building upon the foundation of VALL-E.\nSpecifically, we introduce a phoneme monotonic alignment strategy to strengthen\nthe connection between phonemes and acoustic sequence, ensuring a more precise\nalignment by constraining the acoustic tokens to match their associated\nphonemes. Furthermore, we employ a codec-merging approach to downsample the\ndiscrete codes in shallow quantization layer, thereby accelerating the decoding\nspeed while preserving the high quality of speech output. Benefiting from these\nstrategies, VALL-E R obtains controllablity over phonemes and demonstrates its\nstrong robustness by approaching the WER of ground truth. In addition, it\nrequires fewer autoregressive steps, with over 60% time reduction during\ninference. This research has the potential to be applied to meaningful\nprojects, including the creation of speech for those affected by aphasia. Audio\nsamples will be available at: https://aka.ms/valler.", "published": "2024-06-12 04:09:44", "link": "http://arxiv.org/abs/2406.07855v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "BookSQL: A Large Scale Text-to-SQL Dataset for Accounting Domain", "abstract": "Several large-scale datasets (e.g., WikiSQL, Spider) for developing natural\nlanguage interfaces to databases have recently been proposed. These datasets\ncover a wide breadth of domains but fall short on some essential domains, such\nas finance and accounting. Given that accounting databases are used worldwide,\nparticularly by non-technical people, there is an imminent need to develop\nmodels that could help extract information from accounting databases via\nnatural language queries. In this resource paper, we aim to fill this gap by\nproposing a new large-scale Text-to-SQL dataset for the accounting and\nfinancial domain: BookSQL. The dataset consists of 100k natural language\nqueries-SQL pairs, and accounting databases of 1 million records. We experiment\nwith and analyze existing state-of-the-art models (including GPT-4) for the\nText-to-SQL task on BookSQL. We find significant performance gaps, thus\npointing towards developing more focused models for this domain.", "published": "2024-06-12 04:22:27", "link": "http://arxiv.org/abs/2406.07860v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Designing a Dashboard for Transparency and Control of Conversational AI", "abstract": "Conversational LLMs function as black box systems, leaving users guessing\nabout why they see the output they do. This lack of transparency is potentially\nproblematic, especially given concerns around bias and truthfulness. To address\nthis issue, we present an end-to-end prototype-connecting interpretability\ntechniques with user experience design-that seeks to make chatbots more\ntransparent. We begin by showing evidence that a prominent open-source LLM has\na \"user model\": examining the internal state of the system, we can extract data\nrelated to a user's age, gender, educational level, and socioeconomic status.\nNext, we describe the design of a dashboard that accompanies the chatbot\ninterface, displaying this user model in real time. The dashboard can also be\nused to control the user model and the system's behavior. Finally, we discuss a\nstudy in which users conversed with the instrumented system. Our results\nsuggest that users appreciate seeing internal states, which helped them expose\nbiased behavior and increased their sense of control. Participants also made\nvaluable suggestions that point to future directions for both design and\nmachine learning research. The project page and video demo of our TalkTuner\nsystem are available at https://bit.ly/talktuner-project-page", "published": "2024-06-12 05:20:16", "link": "http://arxiv.org/abs/2406.07882v3", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Exploring Speech Foundation Models for Speaker Diarization in\n  Child-Adult Dyadic Interactions", "abstract": "Speech foundation models, trained on vast datasets, have opened unique\nopportunities in addressing challenging low-resource speech understanding, such\nas child speech. In this work, we explore the capabilities of speech foundation\nmodels on child-adult speaker diarization. We show that exemplary foundation\nmodels can achieve 39.5% and 62.3% relative reductions in Diarization Error\nRate and Speaker Confusion Rate, respectively, compared to previous speaker\ndiarization methods. In addition, we benchmark and evaluate the speaker\ndiarization results of the speech foundation models with varying the input\naudio window size, speaker demographics, and training data ratio. Our results\nhighlight promising pathways for understanding and adopting speech foundation\nmodels to facilitate child speech understanding.", "published": "2024-06-12 05:41:01", "link": "http://arxiv.org/abs/2406.07890v1", "categories": ["eess.AS", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Exploring Self-Supervised Multi-view Contrastive Learning for Speech\n  Emotion Recognition with Limited Annotations", "abstract": "Recent advancements in Deep and Self-Supervised Learning (SSL) have led to\nsubstantial improvements in Speech Emotion Recognition (SER) performance,\nreaching unprecedented levels. However, obtaining sufficient amounts of\naccurately labeled data for training or fine-tuning the models remains a costly\nand challenging task. In this paper, we propose a multi-view SSL pre-training\ntechnique that can be applied to various representations of speech, including\nthe ones generated by large speech models, to improve SER performance in\nscenarios where annotations are limited. Our experiments, based on wav2vec 2.0,\nspectral and paralinguistic features, demonstrate that the proposed framework\nboosts the SER performance, by up to 10% in Unweighted Average Recall, in\nsettings with extremely sparse data annotations.", "published": "2024-06-12 06:06:55", "link": "http://arxiv.org/abs/2406.07900v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Guiding Frame-Level CTC Alignments Using Self-knowledge Distillation", "abstract": "Transformer encoder with connectionist temporal classification (CTC)\nframework is widely used for automatic speech recognition (ASR). However,\nknowledge distillation (KD) for ASR displays a problem of disagreement between\nteacher-student models in frame-level alignment which ultimately hinders it\nfrom improving the student model's performance. In order to resolve this\nproblem, this paper introduces a self-knowledge distillation (SKD) method that\nguides the frame-level alignment during the training time. In contrast to the\nconventional method using separate teacher and student models, this study\nintroduces a simple and effective method sharing encoder layers and applying\nthe sub-model as the student model. Overall, our approach is effective in\nimproving both the resource efficiency as well as performance. We also\nconducted an experimental analysis of the spike timings to illustrate that the\nproposed method improves performance by reducing the alignment disagreement.", "published": "2024-06-12 06:22:52", "link": "http://arxiv.org/abs/2406.07909v1", "categories": ["eess.AS", "cs.CL", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Large Language Model Unlearning via Embedding-Corrupted Prompts", "abstract": "Large language models (LLMs) have advanced to encompass extensive knowledge\nacross diverse domains. Yet controlling what a large language model should not\nknow is important for ensuring alignment and thus safe use. However, accurately\nand efficiently unlearning knowledge from an LLM remains challenging due to the\npotential collateral damage caused by the fuzzy boundary between retention and\nforgetting, and the large computational requirements for optimization across\nstate-of-the-art models with hundreds of billions of parameters. In this work,\nwe present \\textbf{Embedding-COrrupted (ECO) Prompts}, a lightweight unlearning\nframework for large language models to address both the challenges of knowledge\nentanglement and unlearning efficiency. Instead of relying on the LLM itself to\nunlearn, we enforce an unlearned state during inference by employing a prompt\nclassifier to identify and safeguard prompts to forget. We learn corruptions\nadded to prompt embeddings via zeroth order optimization toward the unlearning\nobjective offline and corrupt prompts flagged by the classifier during\ninference. We find that these embedding-corrupted prompts not only lead to\ndesirable outputs that satisfy the unlearning objective but also closely\napproximate the output from a model that has never been trained on the data\nintended for forgetting. Through extensive experiments on unlearning, we\ndemonstrate the superiority of our method in achieving promising unlearning at\n\\textit{nearly zero side effects} in general domains and domains closely\nrelated to the unlearned ones. Additionally, we highlight the scalability of\nour method to 100 LLMs, ranging from 0.5B to 236B parameters, incurring no\nadditional cost as the number of parameters increases. We have made our code\npublicly available at \\url{https://github.com/chrisliu298/llm-unlearn-eco}.", "published": "2024-06-12 06:56:20", "link": "http://arxiv.org/abs/2406.07933v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Political Leaning Inference through Plurinational Scenarios", "abstract": "Social media users express their political preferences via interaction with\nother users, by spontaneous declarations or by participation in communities\nwithin the network. This makes a social network such as Twitter a valuable data\nsource to study computational science approaches to political learning\ninference. In this work we focus on three diverse regions in Spain (Basque\nCountry, Catalonia and Galicia) to explore various methods for multi-party\ncategorization, required to analyze evolving and complex political landscapes,\nand compare it with binary left-right approaches. We use a two-step method\ninvolving unsupervised user representations obtained from the retweets and\ntheir subsequent use for political leaning detection. Comprehensive\nexperimentation on a newly collected and curated dataset comprising labeled\nusers and their interactions demonstrate the effectiveness of using Relational\nEmbeddings as representation method for political ideology detection in both\nbinary and multi-party frameworks, even with limited training data. Finally,\ndata visualization illustrates the ability of the Relational Embeddings to\ncapture intricate intra-group and inter-group political affinities.", "published": "2024-06-12 07:42:12", "link": "http://arxiv.org/abs/2406.07964v1", "categories": ["cs.SI", "cs.CL", "cs.CY"], "primary_category": "cs.SI"}
{"title": "LibriTTS-P: A Corpus with Speaking Style and Speaker Identity Prompts\n  for Text-to-Speech and Style Captioning", "abstract": "We introduce LibriTTS-P, a new corpus based on LibriTTS-R that includes\nutterance-level descriptions (i.e., prompts) of speaking style and\nspeaker-level prompts of speaker characteristics. We employ a hybrid approach\nto construct prompt annotations: (1) manual annotations that capture human\nperceptions of speaker characteristics and (2) synthetic annotations on\nspeaking style. Compared to existing English prompt datasets, our corpus\nprovides more diverse prompt annotations for all speakers of LibriTTS-R.\nExperimental results for prompt-based controllable TTS demonstrate that the TTS\nmodel trained with LibriTTS-P achieves higher naturalness than the model using\nthe conventional dataset. Furthermore, the results for style captioning tasks\nshow that the model utilizing LibriTTS-P generates 2.5 times more accurate\nwords than the model using a conventional dataset. Our corpus, LibriTTS-P, is\navailable at https://github.com/line/LibriTTS-P.", "published": "2024-06-12 07:49:21", "link": "http://arxiv.org/abs/2406.07969v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "It Takes Two: On the Seamlessness between Reward and Policy Model in\n  RLHF", "abstract": "Reinforcement Learning from Human Feedback (RLHF) involves training policy\nmodels (PMs) and reward models (RMs) to align language models with human\npreferences. Instead of focusing solely on PMs and RMs independently, we\npropose to examine their interactions during fine-tuning, introducing the\nconcept of seamlessness. Our study starts with observing the saturation\nphenomenon, where continual improvements in RM and PM do not translate into\nRLHF progress. Our analysis shows that RMs fail to assign proper scores to PM\nresponses, resulting in a 35% mismatch rate with human preferences,\nhighlighting a significant discrepancy between PM and RM. To measure\nseamlessness between PM and RM without human effort, we propose an automatic\nmetric, SEAM. SEAM quantifies the discrepancies between PM and RM judgments\ninduced by data samples. We validate the effectiveness of SEAM in data\nselection and model augmentation. Our experiments demonstrate that (1) using\nSEAM-filtered data for RL training improves RLHF performance by 4.5%, and (2)\nSEAM-guided model augmentation results in a 4% performance improvement over\nstandard augmentation methods.", "published": "2024-06-12 07:52:17", "link": "http://arxiv.org/abs/2406.07971v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Blowfish: Topological and statistical signatures for quantifying\n  ambiguity in semantic search", "abstract": "This works reports evidence for the topological signatures of ambiguity in\nsentence embeddings that could be leveraged for ranking and/or explanation\npurposes in the context of vector search and Retrieval Augmented Generation\n(RAG) systems. We proposed a working definition of ambiguity and designed an\nexperiment where we have broken down a proprietary dataset into collections of\nchunks of varying size - 3, 5, and 10 lines and used the different collections\nsuccessively as queries and answers sets. It allowed us to test the signatures\nof ambiguity with removal of confounding factors. Our results show that proxy\nambiguous queries (size 10 queries against size 3 documents) display different\ndistributions of homologies 0 and 1 based features than proxy clear queries\n(size 5 queries against size 10 documents). We then discuss those results in\nterms increased manifold complexity and/or approximately discontinuous\nembedding submanifolds. Finally we propose a strategy to leverage those\nfindings as a new scoring strategy of semantic similarities.", "published": "2024-06-12 08:26:30", "link": "http://arxiv.org/abs/2406.07990v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A Concept-Based Explainability Framework for Large Multimodal Models", "abstract": "Large multimodal models (LMMs) combine unimodal encoders and large language\nmodels (LLMs) to perform multimodal tasks. Despite recent advancements towards\nthe interpretability of these models, understanding internal representations of\nLMMs remains largely a mystery. In this paper, we present a novel framework for\nthe interpretation of LMMs. We propose a dictionary learning based approach,\napplied to the representation of tokens. The elements of the learned dictionary\ncorrespond to our proposed concepts. We show that these concepts are well\nsemantically grounded in both vision and text. Thus we refer to these as\n``multi-modal concepts''. We qualitatively and quantitatively evaluate the\nresults of the learnt concepts. We show that the extracted multimodal concepts\nare useful to interpret representations of test samples. Finally, we evaluate\nthe disentanglement between different concepts and the quality of grounding\nconcepts visually and textually. Our code is publicly available at\nhttps://github.com/mshukor/xl-vlms", "published": "2024-06-12 10:48:53", "link": "http://arxiv.org/abs/2406.08074v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "AustroTox: A Dataset for Target-Based Austrian German Offensive Language\n  Detection", "abstract": "Model interpretability in toxicity detection greatly profits from token-level\nannotations. However, currently such annotations are only available in English.\nWe introduce a dataset annotated for offensive language detection sourced from\na news forum, notable for its incorporation of the Austrian German dialect,\ncomprising 4,562 user comments. In addition to binary offensiveness\nclassification, we identify spans within each comment constituting vulgar\nlanguage or representing targets of offensive statements. We evaluate\nfine-tuned language models as well as large language models in a zero- and\nfew-shot fashion. The results indicate that while fine-tuned models excel in\ndetecting linguistic peculiarities such as vulgar dialect, large language\nmodels demonstrate superior performance in detecting offensiveness in\nAustroTox. We publish the data and code.", "published": "2024-06-12 11:04:11", "link": "http://arxiv.org/abs/2406.08080v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.7"], "primary_category": "cs.CL"}
{"title": "QuantMoE-Bench: Examining Post-Training Quantization for\n  Mixture-of-Experts", "abstract": "Mixture-of-Experts (MoE) is a promising way to scale up the learning capacity\nof large language models. It increases the number of parameters while keeping\nFLOPs nearly constant during inference through sparse activation. Yet, it still\nsuffers from significant memory overheads due to the vast parameter size,\nnecessitating model compression techniques. Post-training quantization offers a\npowerful approach for model compression. Existing methods adopt a fixed\nquantization precision for the entire MoE model. This rigid setup can lead to\nsuboptimal performance, without considering the inherent sparse structure. For\nexample, MoE's sparse routing mechanism leads to different activation patterns,\nwhere shared experts are accessed by all tokens while token-conditioned experts\nare selectively activated. This activation disparity suggests different\nquantization requirements, with consistently activated shared experts\npotentially needing higher precision to maintain model quality. In this paper,\nwe study a fine-grained precision setup for MoE quantization. We explore MoE\nstructure-aware quantization heuristics, ranging from coarse (e.g., MoE layers)\nto fine granularity (e.g., linear layers). Our investigations reveal critical\nprinciples, where different MoE structures require varying numbers of bits for\neffective quantization. Conclusions are supported by extensive benchmarking\nacross two representative MoE models and six tasks including commonsense\nreasoning and natural language understanding. We further show that an MoE\nquantized in a fined-grained mixed precision achieved state-of-the-art 65.35%\nperformance on average compared to the baseline 64.30% (i.e., GPTQ). Moreover,\nbased on the findings, we introduce novel data-driven techniques for optimizing\nbit allocation in MoE quantization, including the outlier-aware linear layer\nscorer and MoE block importance predictor.", "published": "2024-06-12 12:44:48", "link": "http://arxiv.org/abs/2406.08155v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Transformer-based Model for ASR N-Best Rescoring and Rewriting", "abstract": "Voice assistants increasingly use on-device Automatic Speech Recognition\n(ASR) to ensure speed and privacy. However, due to resource constraints on the\ndevice, queries pertaining to complex information domains often require further\nprocessing by a search engine. For such applications, we propose a novel\nTransformer based model capable of rescoring and rewriting, by exploring full\ncontext of the N-best hypotheses in parallel. We also propose a new\ndiscriminative sequence training objective that can work well for both rescore\nand rewrite tasks. We show that our Rescore+Rewrite model outperforms the\nRescore-only baseline, and achieves up to an average 8.6% relative Word Error\nRate (WER) reduction over the ASR system by itself.", "published": "2024-06-12 13:39:44", "link": "http://arxiv.org/abs/2406.08207v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Leveraging Large Language Models for Web Scraping", "abstract": "Large Language Models (LLMs) demonstrate remarkable capabilities in\nreplicating human tasks and boosting productivity. However, their direct\napplication for data extraction presents limitations due to a prioritisation of\nfluency over factual accuracy and a restricted ability to manipulate specific\ninformation. Therefore to overcome these limitations, this research leverages\nthe knowledge representation power of pre-trained LLMs and the targeted\ninformation access enabled by RAG models, this research investigates a\ngeneral-purpose accurate data scraping recipe for RAG models designed for\nlanguage generation. To capture knowledge in a more modular and interpretable\nway, we use pre trained language models with a latent knowledge retriever,\nwhich allows the model to retrieve and attend over documents from a large\ncorpus. We utilised RAG model architecture and did an in-depth analysis of\ntheir capabilities under three tasks: (i) Semantic Classification of HTML\nelements, (ii) Chunking HTML text for effective understanding, and (iii)\ncomparing results from different LLMs and ranking algorithms. While previous\nwork has developed dedicated architectures and training procedures for HTML\nunderstanding and extraction, we show that LLMs pre-trained on standard natural\nlanguage with an addition of effective chunking, searching and ranking\nalgorithms, can prove to be efficient data scraping tool to extract complex\ndata from unstructured text. Future research directions include addressing the\nchallenges of provenance tracking and dynamic knowledge updates within the\nproposed RAG-based data extraction framework. By overcoming these limitations,\nthis approach holds the potential to revolutionise data extraction from vast\nrepositories of textual information.", "published": "2024-06-12 14:15:15", "link": "http://arxiv.org/abs/2406.08246v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Speech Emotion Recognition with ASR Transcripts: A Comprehensive Study\n  on Word Error Rate and Fusion Techniques", "abstract": "Text data is commonly utilized as a primary input to enhance Speech Emotion\nRecognition (SER) performance and reliability. However, the reliance on\nhuman-transcribed text in most studies impedes the development of practical SER\nsystems, creating a gap between in-lab research and real-world scenarios where\nAutomatic Speech Recognition (ASR) serves as the text source. Hence, this study\nbenchmarks SER performance using ASR transcripts with varying Word Error Rates\n(WERs) from eleven models on three well-known corpora: IEMOCAP, CMU-MOSI, and\nMSP-Podcast. Our evaluation includes both text-only and bimodal SER with six\nfusion techniques, aiming for a comprehensive analysis that uncovers novel\nfindings and challenges faced by current SER research. Additionally, we propose\na unified ASR error-robust framework integrating ASR error correction and\nmodality-gated fusion, achieving lower WER and higher SER results compared to\nthe best-performing ASR transcript. These findings provide insights into SER\nwith ASR assistance, especially for real-world applications.", "published": "2024-06-12 15:59:25", "link": "http://arxiv.org/abs/2406.08353v3", "categories": ["eess.AS", "cs.CL", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Towards Unsupervised Speech Recognition Without Pronunciation Models", "abstract": "Recent advancements in supervised automatic speech recognition (ASR) have\nachieved remarkable performance, largely due to the growing availability of\nlarge transcribed speech corpora. However, most languages lack sufficient\npaired speech and text data to effectively train these systems. In this\narticle, we tackle the challenge of developing ASR systems without paired\nspeech and text corpora by proposing the removal of reliance on a phoneme\nlexicon. We explore a new research direction: word-level unsupervised ASR, and\nexperimentally demonstrate that an unsupervised speech recognizer can emerge\nfrom joint speech-to-speech and text-to-text masked token-infilling. Using a\ncurated speech corpus containing a fixed number of English words, our system\niteratively refines the word segmentation structure and achieves a word error\nrate of between 20-23%, depending on the vocabulary size, without parallel\ntranscripts, oracle word boundaries, or a pronunciation lexicon. This\ninnovative model surpasses the performance of previous unsupervised ASR models\nunder the lexicon-free setting.", "published": "2024-06-12 16:30:58", "link": "http://arxiv.org/abs/2406.08380v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Large Language Models Must Be Taught to Know What They Don't Know", "abstract": "When using large language models (LLMs) in high-stakes applications, we need\nto know when we can trust their predictions. Some works argue that prompting\nhigh-performance LLMs is sufficient to produce calibrated uncertainties, while\nothers introduce sampling methods that can be prohibitively expensive. In this\nwork, we first argue that prompting on its own is insufficient to achieve good\ncalibration and then show that fine-tuning on a small dataset of correct and\nincorrect answers can create an uncertainty estimate with good generalization\nand small computational overhead. We show that a thousand graded examples are\nsufficient to outperform baseline methods and that training through the\nfeatures of a model is necessary for good performance and tractable for large\nopen-source models when using LoRA. We also investigate the mechanisms that\nenable reliable LLM uncertainty estimation, finding that many models can be\nused as general-purpose uncertainty estimators, applicable not just to their\nown uncertainties but also the uncertainty of other models. Lastly, we show\nthat uncertainty estimates inform human use of LLMs in human-AI collaborative\nsettings through a user study.", "published": "2024-06-12 16:41:31", "link": "http://arxiv.org/abs/2406.08391v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "cPAPERS: A Dataset of Situated and Multimodal Interactive Conversations\n  in Scientific Papers", "abstract": "An emerging area of research in situated and multimodal interactive\nconversations (SIMMC) includes interactions in scientific papers. Since\nscientific papers are primarily composed of text, equations, figures, and\ntables, SIMMC methods must be developed specifically for each component to\nsupport the depth of inquiry and interactions required by research scientists.\nThis work introduces Conversational Papers (cPAPERS), a dataset of\nconversational question-answer pairs from reviews of academic papers grounded\nin these paper components and their associated references from scientific\ndocuments available on arXiv. We present a data collection strategy to collect\nthese question-answer pairs from OpenReview and associate them with contextual\ninformation from LaTeX source files. Additionally, we present a series of\nbaseline approaches utilizing Large Language Models (LLMs) in both zero-shot\nand fine-tuned configurations to address the cPAPERS dataset.", "published": "2024-06-12 16:46:12", "link": "http://arxiv.org/abs/2406.08398v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Understanding Sounds, Missing the Questions: The Challenge of Object\n  Hallucination in Large Audio-Language Models", "abstract": "Large audio-language models (LALMs) enhance traditional large language models\nby integrating audio perception capabilities, allowing them to tackle\naudio-related tasks. Previous research has primarily focused on assessing the\nperformance of LALMs across various tasks, yet overlooking their reliability,\nparticularly concerning issues like object hallucination. In our study, we\nintroduce methods to assess the extent of object hallucination of publicly\navailable LALMs. Our findings reveal that LALMs are comparable to specialized\naudio captioning models in their understanding of audio content, but struggle\nto answer discriminative questions, specifically those requiring the\nidentification of the presence of particular object sounds within an audio\nclip. This limitation highlights a critical weakness in current LALMs: their\ninadequate understanding of discriminative queries. Moreover, we explore the\npotential of prompt engineering to enhance LALMs' performance on discriminative\nquestions.", "published": "2024-06-12 16:51:54", "link": "http://arxiv.org/abs/2406.08402v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MMWorld: Towards Multi-discipline Multi-faceted World Model Evaluation\n  in Videos", "abstract": "Multimodal Language Language Models (MLLMs) demonstrate the emerging\nabilities of \"world models\" -- interpreting and reasoning about complex\nreal-world dynamics. To assess these abilities, we posit videos are the ideal\nmedium, as they encapsulate rich representations of real-world dynamics and\ncausalities. To this end, we introduce MMWorld, a new benchmark for\nmulti-discipline, multi-faceted multimodal video understanding. MMWorld\ndistinguishes itself from previous video understanding benchmarks with two\nunique advantages: (1) multi-discipline, covering various disciplines that\noften require domain expertise for comprehensive understanding; (2)\nmulti-faceted reasoning, including explanation, counterfactual thinking, future\nprediction, etc. MMWorld consists of a human-annotated dataset to evaluate\nMLLMs with questions about the whole videos and a synthetic dataset to analyze\nMLLMs within a single modality of perception. Together, MMWorld encompasses\n1,910 videos across seven broad disciplines and 69 subdisciplines, complete\nwith 6,627 question-answer pairs and associated captions. The evaluation\nincludes 2 proprietary and 10 open-source MLLMs, which struggle on MMWorld\n(e.g., GPT-4V performs the best with only 52.3\\% accuracy), showing large room\nfor improvement. Further ablation studies reveal other interesting findings\nsuch as models' different skill sets from humans. We hope MMWorld can serve as\nan essential step towards world model evaluation in videos.", "published": "2024-06-12 16:54:54", "link": "http://arxiv.org/abs/2406.08407v3", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Tailoring Generative AI Chatbots for Multiethnic Communities in Disaster\n  Preparedness Communication: Extending the CASA Paradigm", "abstract": "This study is among the first to develop different prototypes of generative\nartificial intelligence (GenAI) chatbots powered by GPT-4 to communicate\nhurricane preparedness information to diverse residents. Drawing from the\nComputers Are Social Actors paradigm and the literature on disaster\nvulnerability and cultural tailoring, we conducted a between-subjects\nexperiment with 441 Black, Hispanic, and Caucasian residents of Florida. Our\nresults suggest that GenAI chatbots varying in tone formality and cultural\ntailoring significantly influence perceptions of their friendliness and\ncredibility, which, in turn, relate to hurricane preparedness outcomes. These\nresults highlight the potential of using GenAI chatbots to improve diverse\ncommunities' disaster preparedness.", "published": "2024-06-12 16:57:28", "link": "http://arxiv.org/abs/2406.08411v2", "categories": ["cs.CL", "cs.AI", "cs.HC", "68U15"], "primary_category": "cs.CL"}
{"title": "Next-Generation Database Interfaces: A Survey of LLM-based Text-to-SQL", "abstract": "Generating accurate SQL from users' natural language questions (text-to-SQL)\nremains a long-standing challenge due to the complexities involved in user\nquestion understanding, database schema comprehension, and SQL generation.\nTraditional text-to-SQL systems, which combine human engineering and deep\nneural networks, have made significant progress. Subsequently, pre-trained\nlanguage models (PLMs) have been developed for text-to-SQL tasks, achieving\npromising results. However, as modern databases and user questions grow more\ncomplex, PLMs with a limited parameter size often produce incorrect SQL. This\nnecessitates more sophisticated and tailored optimization methods, which\nrestricts the application of PLM-based systems. Recently, large language models\n(LLMs) have shown significant capabilities in natural language understanding as\nmodel scale increases. Thus, integrating LLM-based solutions can bring unique\nopportunities, improvements, and solutions to text-to-SQL research. In this\nsurvey, we provide a comprehensive review of existing LLM-based text-to-SQL\nstudies. Specifically, we offer a brief overview of the technical challenges\nand evolutionary process of text-to-SQL. Next, we introduce the datasets and\nmetrics designed to evaluate text-to-SQL systems. Subsequently, we present a\nsystematic analysis of recent advances in LLM-based text-to-SQL. Finally, we\nmake a summarization and discuss the remaining challenges in this field and\nsuggest expectations for future research directions.", "published": "2024-06-12 17:13:17", "link": "http://arxiv.org/abs/2406.08426v5", "categories": ["cs.CL", "cs.AI", "cs.DB"], "primary_category": "cs.CL"}
{"title": "The Impact of Initialization on LoRA Finetuning Dynamics", "abstract": "In this paper, we study the role of initialization in Low Rank Adaptation\n(LoRA) as originally introduced in Hu et al. (2021). Essentially, to start from\nthe pretrained model as initialization for finetuning, one can either\ninitialize B to zero and A to random (default initialization in PEFT package),\nor vice-versa. In both cases, the product BA is equal to zero at\ninitialization, which makes finetuning starts from the pretrained model. These\ntwo initialization schemes are seemingly similar. They should in-principle\nyield the same performance and share the same optimal learning rate. We\ndemonstrate that this is an incorrect intuition and that the first scheme\n(initializing B to zero and A to random) on average yields better performance\ncompared to the other scheme. Our theoretical analysis shows that the reason\nbehind this might be that the first initialization allows the use of larger\nlearning rates (without causing output instability) compared to the second\ninitialization, resulting in more efficient learning of the first scheme. We\nvalidate our results with extensive experiments on LLMs.", "published": "2024-06-12 17:38:20", "link": "http://arxiv.org/abs/2406.08447v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "CS-Bench: A Comprehensive Benchmark for Large Language Models towards\n  Computer Science Mastery", "abstract": "Large language models (LLMs) have demonstrated significant potential in\nadvancing various fields of research and society. However, the current\ncommunity of LLMs overly focuses on benchmarks for analyzing specific\nfoundational skills (e.g. mathematics and code generation), neglecting an\nall-round evaluation of the computer science field. To bridge this gap, we\nintroduce CS-Bench, the first multilingual (English, Chinese, French, German)\nbenchmark dedicated to evaluating the performance of LLMs in computer science.\nCS-Bench comprises approximately 10K meticulously curated test samples,\ncovering 26 subfields across 4 key areas of computer science, encompassing\nvarious task forms and divisions of knowledge and reasoning. Utilizing\nCS-Bench, we conduct a comprehensive evaluation of over 30 mainstream LLMs,\nrevealing the relationship between CS performance and model scales. We also\nquantitatively analyze the reasons for failures in existing LLMs and highlight\ndirections for improvements, including knowledge supplementation and\nCS-specific reasoning. Further cross-capability experiments show a high\ncorrelation between LLMs' capabilities in computer science and their abilities\nin mathematics and coding. Moreover, expert LLMs specialized in mathematics and\ncoding also demonstrate strong performances in several CS subfields. Looking\nahead, we envision CS-Bench serving as a cornerstone for LLM applications in\nthe CS field and paving new avenues in assessing LLMs' diverse reasoning\ncapabilities. The CS-Bench data and evaluation code are available at\nhttps://github.com/csbench/csbench.", "published": "2024-06-12 18:47:28", "link": "http://arxiv.org/abs/2406.08587v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Self-Supervised Speech Representations are More Phonetic than Semantic", "abstract": "Self-supervised speech models (S3Ms) have become an effective backbone for\nspeech applications. Various analyses suggest that S3Ms encode linguistic\nproperties. In this work, we seek a more fine-grained analysis of the\nword-level linguistic properties encoded in S3Ms. Specifically, we curate a\nnovel dataset of near homophone (phonetically similar) and synonym\n(semantically similar) word pairs and measure the similarities between S3M word\nrepresentation pairs. Our study reveals that S3M representations consistently\nand significantly exhibit more phonetic than semantic similarity. Further, we\nquestion whether widely used intent classification datasets such as Fluent\nSpeech Commands and Snips Smartlights are adequate for measuring semantic\nabilities. Our simple baseline, using only the word identity, surpasses\nS3M-based models. This corroborates our findings and suggests that high scores\non these datasets do not necessarily guarantee the presence of semantic\ncontent.", "published": "2024-06-12 20:04:44", "link": "http://arxiv.org/abs/2406.08619v1", "categories": ["cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Unraveling Code-Mixing Patterns in Migration Discourse: Automated\n  Detection and Analysis of Online Conversations on Reddit", "abstract": "The surge in global migration patterns underscores the imperative of\nintegrating migrants seamlessly into host communities, necessitating inclusive\nand trustworthy public services. Despite the Nordic countries' robust public\nsector infrastructure, recent immigrants often encounter barriers to accessing\nthese services, exacerbating social disparities and eroding trust. Addressing\ndigital inequalities and linguistic diversity is paramount in this endeavor.\nThis paper explores the utilization of code-mixing, a communication strategy\nprevalent among multilingual speakers, in migration-related discourse on social\nmedia platforms such as Reddit. We present Ensemble Learning for Multilingual\nIdentification of Code-mixed Texts (ELMICT), a novel approach designed to\nautomatically detect code-mixed messages in migration-related discussions.\nLeveraging ensemble learning techniques for combining multiple tokenizers'\noutputs and pre-trained language models, ELMICT demonstrates high performance\n(with F1 more than 0.95) in identifying code-mixing across various languages\nand contexts, particularly in cross-lingual zero-shot conditions (with avg. F1\nmore than 0.70). Moreover, the utilization of ELMICT helps to analyze the\nprevalence of code-mixing in migration-related threads compared to other\nthematic categories on Reddit, shedding light on the topics of concern to\nmigrant communities. Our findings reveal insights into the communicative\nstrategies employed by migrants on social media platforms, offering\nimplications for the development of inclusive digital public services and\nconversational systems. By addressing the research questions posed in this\nstudy, we contribute to the understanding of linguistic diversity in migration\ndiscourse and pave the way for more effective tools for building trust in\nmulticultural societies.", "published": "2024-06-12 20:30:34", "link": "http://arxiv.org/abs/2406.08633v1", "categories": ["cs.CL", "cs.HC", "cs.IR"], "primary_category": "cs.CL"}
{"title": "ML-SUPERB 2.0: Benchmarking Multilingual Speech Models Across Modeling\n  Constraints, Languages, and Datasets", "abstract": "ML-SUPERB evaluates self-supervised learning (SSL) models on the tasks of\nlanguage identification and automatic speech recognition (ASR). This benchmark\ntreats the models as feature extractors and uses a single shallow downstream\nmodel, which can be fine-tuned for a downstream task. However, real-world use\ncases may require different configurations. This paper presents ML-SUPERB~2.0,\nwhich is a new benchmark for evaluating pre-trained SSL and supervised speech\nmodels across downstream models, fine-tuning setups, and efficient model\nadaptation approaches. We find performance improvements over the setup of\nML-SUPERB. However, performance depends on the downstream model design. Also,\nwe find large performance differences between languages and datasets,\nsuggesting the need for more targeted approaches to improve multilingual ASR\nperformance.", "published": "2024-06-12 21:01:26", "link": "http://arxiv.org/abs/2406.08641v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "TC-Bench: Benchmarking Temporal Compositionality in Text-to-Video and\n  Image-to-Video Generation", "abstract": "Video generation has many unique challenges beyond those of image generation.\nThe temporal dimension introduces extensive possible variations across frames,\nover which consistency and continuity may be violated. In this study, we move\nbeyond evaluating simple actions and argue that generated videos should\nincorporate the emergence of new concepts and their relation transitions like\nin real-world videos as time progresses. To assess the Temporal\nCompositionality of video generation models, we propose TC-Bench, a benchmark\nof meticulously crafted text prompts, corresponding ground truth videos, and\nrobust evaluation metrics. The prompts articulate the initial and final states\nof scenes, effectively reducing ambiguities for frame development and\nsimplifying the assessment of transition completion. In addition, by collecting\naligned real-world videos corresponding to the prompts, we expand TC-Bench's\napplicability from text-conditional models to image-conditional ones that can\nperform generative frame interpolation. We also develop new metrics to measure\nthe completeness of component transitions in generated videos, which\ndemonstrate significantly higher correlations with human judgments than\nexisting metrics. Our comprehensive experimental results reveal that most video\ngenerators achieve less than 20% of the compositional changes, highlighting\nenormous space for future improvement. Our analysis indicates that current\nvideo generation models struggle to interpret descriptions of compositional\nchanges and synthesize various components across different time steps.", "published": "2024-06-12 21:41:32", "link": "http://arxiv.org/abs/2406.08656v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "HelpSteer2: Open-source dataset for training top-performing reward\n  models", "abstract": "High-quality preference datasets are essential for training reward models\nthat can effectively guide large language models (LLMs) in generating\nhigh-quality responses aligned with human preferences. As LLMs become stronger\nand better aligned, permissively licensed preference datasets, such as Open\nAssistant, HH-RLHF, and HelpSteer need to be updated to remain effective for\nreward modeling. Methods that distil preference data from proprietary LLMs such\nas GPT-4 have restrictions on commercial usage imposed by model providers. To\nimprove upon both generated responses and attribute labeling quality, we\nrelease HelpSteer2, a permissively licensed preference dataset (CC-BY-4.0).\nUsing a powerful internal base model trained on HelpSteer2, we are able to\nachieve the SOTA score (92.0%) on Reward-Bench's primary dataset, outperforming\ncurrently listed open and proprietary models, as of June 12th, 2024. Notably,\nHelpSteer2 consists of only ten thousand response pairs, an order of magnitude\nfewer than existing preference datasets (e.g., HH-RLHF), which makes it highly\nefficient for training reward models. Our extensive experiments demonstrate\nthat reward models trained with HelpSteer2 are effective in aligning LLMs. In\nparticular, we propose SteerLM 2.0, a model alignment approach that can\neffectively make use of the rich multi-attribute score predicted by our reward\nmodels. HelpSteer2 is available at\nhttps://huggingface.co/datasets/nvidia/HelpSteer2 and code is available at\nhttps://github.com/NVIDIA/NeMo-Aligner", "published": "2024-06-12 22:28:08", "link": "http://arxiv.org/abs/2406.08673v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "GenDistiller: Distilling Pre-trained Language Models based on an\n  Autoregressive Generative Model", "abstract": "Pre-trained speech language models such as HuBERT and WavLM leverage\nunlabeled speech data for self-supervised learning and offer powerful\nrepresentations for numerous downstream tasks. Despite the success of these\nmodels, their high requirements for memory and computing resource hinder their\napplication on resource restricted devices. Therefore, this paper introduces\nGenDistiller, a novel knowledge distillation framework which generates the\nhidden representations of the pre-trained teacher model directly by a much\nsmaller student network. The proposed method takes the previous hidden layer as\nhistory and implements a layer-by-layer prediction of the teacher model\nautoregressively. Experiments on SUPERB reveal the advantage of GenDistiller\nover the baseline distilling method without an autoregressive framework, with\n33% fewer parameters, similar time consumption and better performance on most\nof the SUPERB tasks. Ultimately, the proposed GenDistiller reduces the size of\nWavLM by 82%.", "published": "2024-06-12 01:25:00", "link": "http://arxiv.org/abs/2406.09444v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Advancing High Resolution Vision-Language Models in Biomedicine", "abstract": "Multi-modal learning has significantly advanced generative AI, especially in\nvision-language modeling. Innovations like GPT-4V and open-source projects such\nas LLaVA have enabled robust conversational agents capable of zero-shot task\ncompletions. However, applying these technologies in the biomedical field\npresents unique challenges. Recent initiatives like LLaVA-Med have started to\nadapt instruction-tuning for biomedical contexts using large datasets such as\nPMC-15M. Our research offers three key contributions: (i) we present a new\ninstruct dataset enriched with medical image-text pairs from Claude3-Opus and\nLLaMA3 70B, (ii) we propose a novel image encoding strategy using hierarchical\nrepresentations to improve fine-grained biomedical visual comprehension, and\n(iii) we develop the Llama3-Med model, which achieves state-of-the-art\nzero-shot performance on biomedical visual question answering benchmarks, with\nan average performance improvement of over 10% compared to previous methods.\nThese advancements provide more accurate and reliable tools for medical\nprofessionals, bridging gaps in current multi-modal conversational assistants\nand promoting further innovations in medical AI.", "published": "2024-06-12 18:29:26", "link": "http://arxiv.org/abs/2406.09454v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "q-bio.QM"], "primary_category": "cs.CL"}
{"title": "Pandora: Towards General World Model with Natural Language Actions and\n  Video States", "abstract": "World models simulate future states of the world in response to different\nactions. They facilitate interactive content creation and provides a foundation\nfor grounded, long-horizon reasoning. Current foundation models do not fully\nmeet the capabilities of general world models: large language models (LLMs) are\nconstrained by their reliance on language modality and their limited\nunderstanding of the physical world, while video models lack interactive action\ncontrol over the world simulations. This paper makes a step towards building a\ngeneral world model by introducing Pandora, a hybrid autoregressive-diffusion\nmodel that simulates world states by generating videos and allows real-time\ncontrol with free-text actions. Pandora achieves domain generality, video\nconsistency, and controllability through large-scale pretraining and\ninstruction tuning. Crucially, Pandora bypasses the cost of\ntraining-from-scratch by integrating a pretrained LLM (7B) and a pretrained\nvideo model, requiring only additional lightweight finetuning. We illustrate\nextensive outputs by Pandora across diverse domains (indoor/outdoor,\nnatural/urban, human/robot, 2D/3D, etc.). The results indicate great potential\nof building stronger general world models with larger-scale training.", "published": "2024-06-12 18:55:51", "link": "http://arxiv.org/abs/2406.09455v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Updating CLIP to Prefer Descriptions Over Captions", "abstract": "Although CLIPScore is a powerful generic metric that captures the similarity\nbetween a text and an image, it fails to distinguish between a caption that is\nmeant to complement the information in an image and a description that is meant\nto replace an image entirely, e.g., for accessibility. We address this\nshortcoming by updating the CLIP model with the Concadia dataset to assign\nhigher scores to descriptions than captions using parameter efficient\nfine-tuning and a loss objective derived from work on causal interpretability.\nThis model correlates with the judgements of blind and low-vision people while\npreserving transfer capabilities and has interpretable structure that sheds\nlight on the caption--description distinction.", "published": "2024-06-12 20:24:51", "link": "http://arxiv.org/abs/2406.09458v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Ad Auctions for LLMs via Retrieval Augmented Generation", "abstract": "In the field of computational advertising, the integration of ads into the\noutputs of large language models (LLMs) presents an opportunity to support\nthese services without compromising content integrity. This paper introduces\nnovel auction mechanisms for ad allocation and pricing within the textual\noutputs of LLMs, leveraging retrieval-augmented generation (RAG). We propose a\nsegment auction where an ad is probabilistically retrieved for each discourse\nsegment (paragraph, section, or entire output) according to its bid and\nrelevance, following the RAG framework, and priced according to competing bids.\nWe show that our auction maximizes logarithmic social welfare, a new notion of\nwelfare that balances allocation efficiency and fairness, and we characterize\nthe associated incentive-compatible pricing rule. These results are extended to\nmulti-ad allocation per segment. An empirical evaluation validates the\nfeasibility and effectiveness of our approach over several ad auction\nscenarios, and exhibits inherent tradeoffs in metrics as we allow the LLM more\nflexibility to allocate ads.", "published": "2024-06-12 22:05:51", "link": "http://arxiv.org/abs/2406.09459v1", "categories": ["cs.GT", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.GT"}
{"title": "Soft Language Identification for Language-Agnostic Many-to-One\n  End-to-End Speech Translation", "abstract": "Language-agnostic many-to-one end-to-end speech translation models can\nconvert audio signals from different source languages into text in a target\nlanguage. These models do not need source language identification, which\nimproves user experience. In some cases, the input language can be given or\nestimated. Our goal is to use this additional language information while\npreserving the quality of the other languages. We accomplish this by\nintroducing a simple and effective linear input network. The linear input\nnetwork is initialized as an identity matrix, which ensures that the model can\nperform as well as, or better than, the original model. Experimental results\nshow that the proposed method can successfully enhance the specified language,\nwhile keeping the language-agnostic ability of the many-to-one ST models.", "published": "2024-06-12 00:00:39", "link": "http://arxiv.org/abs/2406.10276v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Transferable Embedding Inversion Attack: Uncovering Privacy Risks in\n  Text Embeddings without Model Queries", "abstract": "This study investigates the privacy risks associated with text embeddings,\nfocusing on the scenario where attackers cannot access the original embedding\nmodel. Contrary to previous research requiring direct model access, we explore\na more realistic threat model by developing a transfer attack method. This\napproach uses a surrogate model to mimic the victim model's behavior, allowing\nthe attacker to infer sensitive information from text embeddings without direct\naccess. Our experiments across various embedding models and a clinical dataset\ndemonstrate that our transfer attack significantly outperforms traditional\nmethods, revealing the potential privacy vulnerabilities in embedding\ntechnologies and emphasizing the need for enhanced security measures.", "published": "2024-06-12 05:09:58", "link": "http://arxiv.org/abs/2406.10280v1", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Watermarking Language Models with Error Correcting Codes", "abstract": "Recent progress in large language models enables the creation of realistic\nmachine-generated content. Watermarking is a promising approach to distinguish\nmachine-generated text from human text, embedding statistical signals in the\noutput that are ideally undetectable to humans. We propose a watermarking\nframework that encodes such signals through an error correcting code. Our\nmethod, termed robust binary code (RBC) watermark, introduces no distortion\ncompared to the original probability distribution, and no noticeable\ndegradation in quality. We evaluate our watermark on base and instruction\nfine-tuned models and find our watermark is robust to edits, deletions, and\ntranslations. We provide an information-theoretic perspective on watermarking,\na powerful statistical test for detection and for generating p-values, and\ntheoretical guarantees. Our empirical findings suggest our watermark is fast,\npowerful, and robust, comparing favorably to the state-of-the-art.", "published": "2024-06-12 05:13:09", "link": "http://arxiv.org/abs/2406.10281v2", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Attentive Merging of Hidden Embeddings from Pre-trained Speech Model for\n  Anti-spoofing Detection", "abstract": "Self-supervised learning (SSL) speech representation models, trained on large\nspeech corpora, have demonstrated effectiveness in extracting hierarchical\nspeech embeddings through multiple transformer layers. However, the behavior of\nthese embeddings in specific tasks remains uncertain. This paper investigates\nthe multi-layer behavior of the WavLM model in anti-spoofing and proposes an\nattentive merging method to leverage the hierarchical hidden embeddings.\nResults demonstrate the feasibility of fine-tuning WavLM to achieve the best\nequal error rate (EER) of 0.65%, 3.50%, and 3.19% on the ASVspoof 2019LA,\n2021LA, and 2021DF evaluation sets, respectively. Notably, We find that the\nearly hidden transformer layers of the WavLM large model contribute\nsignificantly to anti-spoofing task, enabling computational efficiency by\nutilizing a partial pre-trained model.", "published": "2024-06-12 08:27:44", "link": "http://arxiv.org/abs/2406.10283v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Improving child speech recognition with augmented child-like speech", "abstract": "State-of-the-art ASRs show suboptimal performance for child speech. The\nscarcity of child speech limits the development of child speech recognition\n(CSR). Therefore, we studied child-to-child voice conversion (VC) from existing\nchild speakers in the dataset and additional (new) child speakers via\nmonolingual and cross-lingual (Dutch-to-German) VC, respectively. The results\nshowed that cross-lingual child-to-child VC significantly improved child ASR\nperformance. Experiments on the impact of the quantity of child-to-child\ncross-lingual VC-generated data on fine-tuning (FT) ASR models gave the best\nresults with two-fold augmentation for our FT-Conformer model and FT-Whisper\nmodel which reduced WERs with ~3% absolute compared to the baseline, and with\nsix-fold augmentation for the model trained from scratch, which improved by an\nabsolute 3.6% WER. Moreover, using a small amount of \"high-quality\"\nVC-generated data achieved similar results to those of our best-FT models.", "published": "2024-06-12 08:56:46", "link": "http://arxiv.org/abs/2406.10284v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "VeraCT Scan: Retrieval-Augmented Fake News Detection with Justifiable\n  Reasoning", "abstract": "The proliferation of fake news poses a significant threat not only by\ndisseminating misleading information but also by undermining the very\nfoundations of democracy. The recent advance of generative artificial\nintelligence has further exacerbated the challenge of distinguishing genuine\nnews from fabricated stories. In response to this challenge, we introduce\nVeraCT Scan, a novel retrieval-augmented system for fake news detection. This\nsystem operates by extracting the core facts from a given piece of news and\nsubsequently conducting an internet-wide search to identify corroborating or\nconflicting reports. Then sources' credibility is leveraged for information\nverification. Besides determining the veracity of news, we also provide\ntransparent evidence and reasoning to support its conclusions, resulting in the\ninterpretability and trust in the results. In addition to GPT-4 Turbo, Llama-2\n13B is also fine-tuned for news content understanding, information\nverification, and reasoning. Both implementations have demonstrated\nstate-of-the-art accuracy in the realm of fake news detection.", "published": "2024-06-12 21:23:48", "link": "http://arxiv.org/abs/2406.10289v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "MobileAIBench: Benchmarking LLMs and LMMs for On-Device Use Cases", "abstract": "The deployment of Large Language Models (LLMs) and Large Multimodal Models\n(LMMs) on mobile devices has gained significant attention due to the benefits\nof enhanced privacy, stability, and personalization. However, the hardware\nconstraints of mobile devices necessitate the use of models with fewer\nparameters and model compression techniques like quantization. Currently, there\nis limited understanding of quantization's impact on various task performances,\nincluding LLM tasks, LMM tasks, and, critically, trust and safety. There is a\nlack of adequate tools for systematically testing these models on mobile\ndevices. To address these gaps, we introduce MobileAIBench, a comprehensive\nbenchmarking framework for evaluating mobile-optimized LLMs and LMMs.\nMobileAIBench assesses models across different sizes, quantization levels, and\ntasks, measuring latency and resource consumption on real devices. Our two-part\nopen-source framework includes a library for running evaluations on desktops\nand an iOS app for on-device latency and hardware utilization measurements. Our\nthorough analysis aims to accelerate mobile AI research and deployment by\nproviding insights into the performance and feasibility of deploying LLMs and\nLMMs on mobile platforms.", "published": "2024-06-12 22:58:12", "link": "http://arxiv.org/abs/2406.10290v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Is Programming by Example solved by LLMs?", "abstract": "Programming-by-Examples (PBE) aims to generate an algorithm from input-output\nexamples. Such systems are practically and theoretically important: from an\nend-user perspective, they are deployed to millions of people, and from an AI\nperspective, PBE corresponds to a very general form of few-shot inductive\ninference. Given the success of Large Language Models (LLMs) in code-generation\ntasks, we investigate here the extent to which LLMs can be said to have\n\"solved\" PBE. We experiment on classic domains such as lists and strings, and\nan uncommon graphics programming domain not well represented in typical\npretraining data. We find that pretrained models are not effective at PBE, but\nthat they can be fine-tuned for much higher performance, provided the test\nproblems are in-distribution. We analyze empirically what causes these models\nto succeed and fail, and take steps toward understanding how to achieve better\nout-of-distribution generalization. Collectively these results suggest that\nLLMs make strong progress toward solving the typical suite of PBE tasks,\npotentially increasing the flexibility and applicability of PBE systems, while\nalso identifying ways in which LLMs still fall short.", "published": "2024-06-12 15:16:40", "link": "http://arxiv.org/abs/2406.08316v3", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.PL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "DualVC 3: Leveraging Language Model Generated Pseudo Context for\n  End-to-end Low Latency Streaming Voice Conversion", "abstract": "Streaming voice conversion has become increasingly popular for its potential\nin real-time applications. The recently proposed DualVC 2 has achieved robust\nand high-quality streaming voice conversion with a latency of about 180ms.\nNonetheless, the recognition-synthesis framework hinders end-to-end\noptimization, and the instability of automatic speech recognition (ASR) model\nwith short chunks makes it challenging to further reduce latency. To address\nthese issues, we propose an end-to-end model, DualVC 3. With\nspeaker-independent semantic tokens to guide the training of the content\nencoder, the dependency on ASR is removed and the model can operate under\nextremely small chunks, with cascading errors eliminated. A language model is\ntrained on the content encoder output to produce pseudo context by iteratively\npredicting future frames, providing more contextual information for the decoder\nto improve conversion quality. Experimental results demonstrate that DualVC 3\nachieves comparable performance to DualVC 2 in subjective and objective\nmetrics, with a latency of only 50 ms.", "published": "2024-06-12 03:25:18", "link": "http://arxiv.org/abs/2406.07846v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Audio-conditioned phonemic and prosodic annotation for building\n  text-to-speech models from unlabeled speech data", "abstract": "This paper proposes an audio-conditioned phonemic and prosodic annotation\nmodel for building text-to-speech (TTS) datasets from unlabeled speech samples.\nFor creating a TTS dataset that consists of label-speech paired data, the\nproposed annotation model leverages an automatic speech recognition (ASR) model\nto obtain phonemic and prosodic labels from unlabeled speech samples. By\nfine-tuning a large-scale pre-trained ASR model, we can construct the\nannotation model using a limited amount of label-speech paired data within an\nexisting TTS dataset. To alleviate the shortage of label-speech paired data for\ntraining the annotation model, we generate pseudo label-speech paired data\nusing text-only corpora and an auxiliary TTS model. This TTS model is also\ntrained with the existing TTS dataset. Experimental results show that the TTS\nmodel trained with the dataset created by the proposed annotation method can\nsynthesize speech as naturally as the one trained with a fully-labeled dataset.", "published": "2024-06-12 11:45:49", "link": "http://arxiv.org/abs/2406.08111v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Multimodal Representation Loss Between Timed Text and Audio for\n  Regularized Speech Separation", "abstract": "Recent studies highlight the potential of textual modalities in conditioning\nthe speech separation model's inference process. However, regularization-based\nmethods remain underexplored despite their advantages of not requiring\nauxiliary text data during the test time. To address this gap, we introduce a\ntimed text-based regularization (TTR) method that uses language model-derived\nsemantics to improve speech separation models. Our approach involves two steps.\nWe begin with two pretrained audio and language models, WavLM and BERT,\nrespectively. Then, a Transformer-based audio summarizer is learned to align\nthe audio and word embeddings and to minimize their gap. The summarizer\nTransformer, incorporated as a regularizer, promotes the separated sources'\nalignment with the semantics from the timed text. Experimental results show\nthat the proposed TTR method consistently improves the various objective\nmetrics of the separation results over the unregularized baselines.", "published": "2024-06-12 15:33:41", "link": "http://arxiv.org/abs/2406.08328v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "SE/BN Adapter: Parametric Efficient Domain Adaptation for Speaker\n  Recognition", "abstract": "Deploying a well-optimized pre-trained speaker recognition model in a new\ndomain often leads to a significant decline in performance. While fine-tuning\nis a commonly employed solution, it demands ample adaptation data and suffers\nfrom parameter inefficiency, rendering it impractical for real-world\napplications with limited data available for model adaptation. Drawing\ninspiration from the success of adapters in self-supervised pre-trained models,\nthis paper introduces a SE/BN adapter to address this challenge. By freezing\nthe core speaker encoder and adjusting the feature maps' weights and activation\ndistributions, we introduce a novel adapter utilizing trainable\nsqueeze-and-excitation (SE) blocks and batch normalization (BN) layers, termed\nSE/BN adapter. Our experiments, conducted using VoxCeleb for pre-training and 4\ngenres from CN-Celeb for adaptation, demonstrate that the SE/BN adapter offers\nsignificant performance improvement over the baseline and competes with the\nvanilla fine-tuning approach by tuning just 1% of the parameters.", "published": "2024-06-12 03:01:38", "link": "http://arxiv.org/abs/2406.07832v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Target Speaker Extraction with Curriculum Learning", "abstract": "This paper presents a novel approach to target speaker extraction (TSE) using\nCurriculum Learning (CL) techniques, addressing the challenge of distinguishing\na target speaker's voice from a mixture containing interfering speakers. For\nefficient training, we propose designing a curriculum that selects subsets of\nincreasing complexity, such as increasing similarity between target and\ninterfering speakers, and that selects training data strategically. Our CL\nstrategies include both variants using predefined difficulty measures (e.g.\ngender, speaker similarity, and signal-to-distortion ratio) and ones using the\nTSE's standard objective function, each designed to expose the model gradually\nto more challenging scenarios. Comprehensive testing on the Libri2talker\ndataset demonstrated that our CL strategies for TSE improved the performance,\nand the results markedly exceeded baseline models without CL about 1 dB.", "published": "2024-06-12 03:24:27", "link": "http://arxiv.org/abs/2406.07845v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Can Large Language Models Understand Spatial Audio?", "abstract": "This paper explores enabling large language models (LLMs) to understand\nspatial information from multichannel audio, a skill currently lacking in\nauditory LLMs. By leveraging LLMs' advanced cognitive and inferential\nabilities, the aim is to enhance understanding of 3D environments via audio. We\nstudy 3 spatial audio tasks: sound source localization (SSL), far-field speech\nrecognition (FSR), and localisation-informed speech extraction (LSE), achieving\nnotable progress in each task. For SSL, our approach achieves an MAE of\n$2.70^{\\circ}$ on the Spatial LibriSpeech dataset, substantially surpassing the\nprior benchmark of about $6.60^{\\circ}$. Moreover, our model can employ spatial\ncues to improve FSR accuracy and execute LSE by selectively attending to sounds\noriginating from a specified direction via text prompts, even amidst\noverlapping speech. These findings highlight the potential of adapting LLMs to\ngrasp physical audio concepts, paving the way for LLM-based agents in 3D\nenvironments.", "published": "2024-06-12 06:34:21", "link": "http://arxiv.org/abs/2406.07914v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "FakeSound: Deepfake General Audio Detection", "abstract": "With the advancement of audio generation, generative models can produce\nhighly realistic audios. However, the proliferation of deepfake general audio\ncan pose negative consequences. Therefore, we propose a new task, deepfake\ngeneral audio detection, which aims to identify whether audio content is\nmanipulated and to locate deepfake regions. Leveraging an automated\nmanipulation pipeline, a dataset named FakeSound for deepfake general audio\ndetection is proposed, and samples can be viewed on website\nhttps://FakeSoundData.github.io. The average binary accuracy of humans on all\ntest sets is consistently below 0.6, which indicates the difficulty humans face\nin discerning deepfake audio and affirms the efficacy of the FakeSound dataset.\nA deepfake detection model utilizing a general audio pre-trained model is\nproposed as a benchmark system. Experimental results demonstrate that the\nperformance of the proposed model surpasses the state-of-the-art in deepfake\nspeech detection and human testers.", "published": "2024-06-12 10:07:40", "link": "http://arxiv.org/abs/2406.08052v1", "categories": ["cs.SD", "eess.AS", "68Txx", "I.2"], "primary_category": "cs.SD"}
{"title": "DCASE 2024 Task 4: Sound Event Detection with Heterogeneous Data and\n  Missing Labels", "abstract": "The Detection and Classification of Acoustic Scenes and Events Challenge Task\n4 aims to advance sound event detection (SED) systems in domestic environments\nby leveraging training data with different supervision uncertainty.\nParticipants are challenged in exploring how to best use training data from\ndifferent domains and with varying annotation granularity (strong/weak temporal\nresolution, soft/hard labels), to obtain a robust SED system that can\ngeneralize across different scenarios. Crucially, annotation across available\ntraining datasets can be inconsistent and hence sound labels of one dataset may\nbe present but not annotated in the other one and vice-versa. As such, systems\nwill have to cope with potentially missing target labels during training.\nMoreover, as an additional novelty, systems will also be evaluated on labels\nwith different granularity in order to assess their robustness for different\napplications. To lower the entry barrier for participants, we developed an\nupdated baseline system with several caveats to address these aforementioned\nproblems. Results with our baseline system indicate that this research\ndirection is promising and is possible to obtain a stronger SED system by using\ndiverse domain training data with missing labels compared to training a SED\nsystem for each domain separately.", "published": "2024-06-12 10:12:53", "link": "http://arxiv.org/abs/2406.08056v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "VECL-TTS: Voice identity and Emotional style controllable Cross-Lingual\n  Text-to-Speech", "abstract": "Despite the significant advancements in Text-to-Speech (TTS) systems, their\nfull utilization in automatic dubbing remains limited. This task necessitates\nthe extraction of voice identity and emotional style from a reference speech in\na source language and subsequently transferring them to a target language using\ncross-lingual TTS techniques. While previous approaches have mainly\nconcentrated on controlling voice identity within the cross-lingual TTS\nframework, there has been limited work on incorporating emotion and voice\nidentity together. To this end, we introduce an end-to-end Voice Identity and\nEmotional Style Controllable Cross-Lingual (VECL) TTS system using multilingual\nspeakers and an emotion embedding network. Moreover, we introduce content and\nstyle consistency losses to enhance the quality of synthesized speech further.\nThe proposed system achieved an average relative improvement of 8.83\\% compared\nto the state-of-the-art (SOTA) methods on a database comprising English and\nthree Indian languages (Hindi, Telugu, and Marathi).", "published": "2024-06-12 10:51:29", "link": "http://arxiv.org/abs/2406.08076v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Low-Complexity Acoustic Scene Classification Using Parallel\n  Attention-Convolution Network", "abstract": "This work is an improved system that we submitted to task 1 of DCASE2023\nchallenge. We propose a method of low-complexity acoustic scene classification\nby a parallel attention-convolution network which consists of four modules,\nincluding pre-processing, fusion, global and local contextual information\nextraction. The proposed network is computationally efficient to capture global\nand local contextual information from each audio clip. In addition, we\nintegrate other techniques into our method, such as knowledge distillation,\ndata augmentation, and adaptive residual normalization. When evaluated on the\nofficial dataset of DCASE2023 challenge, our method obtains the highest\naccuracy of 56.10% with parameter number of 5.21 kilo and multiply-accumulate\noperations of 1.44 million. It exceeds the top two systems of DCASE2023\nchallenge in accuracy and complexity, and obtains state-of-the-art result. Code\nis at: https://github.com/Jessytan/Low-complexity-ASC.", "published": "2024-06-12 11:56:13", "link": "http://arxiv.org/abs/2406.08119v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Fully Few-shot Class-incremental Audio Classification Using Expandable\n  Dual-embedding Extractor", "abstract": "It's assumed that training data is sufficient in base session of few-shot\nclass-incremental audio classification. However, it's difficult to collect\nabundant samples for model training in base session in some practical scenarios\ndue to the data scarcity of some classes. This paper explores a new problem of\nfully few-shot class-incremental audio classification with few training samples\nin all sessions. Moreover, we propose a method using expandable dual-embedding\nextractor to solve it. The proposed model consists of an embedding extractor\nand an expandable classifier. The embedding extractor consists of a pretrained\nAudio Spectrogram Transformer (AST) and a finetuned AST. The expandable\nclassifier consists of prototypes and each prototype represents a class.\nExperiments are conducted on three datasets (LS-100, NSynth-100 and FSC-89).\nResults show that our method exceeds seven baseline ones in average accuracy\nwith statistical significance. Code is at: https://github.com/YongjieSi/EDE.", "published": "2024-06-12 12:02:15", "link": "http://arxiv.org/abs/2406.08122v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "FreeV: Free Lunch For Vocoders Through Pseudo Inversed Mel Filter", "abstract": "Vocoders reconstruct speech waveforms from acoustic features and play a\npivotal role in modern TTS systems. Frequent-domain GAN vocoders like Vocos and\nAPNet2 have recently seen rapid advancements, outperforming time-domain models\nin inference speed while achieving comparable audio quality. However, these\nfrequency-domain vocoders suffer from large parameter sizes, thus introducing\nextra memory burden. Inspired by PriorGrad and SpecGrad, we employ\npseudo-inverse to estimate the amplitude spectrum as the initialization\nroughly. This simple initialization significantly mitigates the parameter\ndemand for vocoder. Based on APNet2 and our streamlined Amplitude prediction\nbranch, we propose our FreeV, compared with its counterpart APNet2, our FreeV\nachieves 1.8 times inference speed improvement with nearly half parameters.\nMeanwhile, our FreeV outperforms APNet2 in resynthesis quality, marking a step\nforward in pursuing real-time, high-fidelity speech synthesis. Code and\ncheckpoints is available at: https://github.com/BakerBunker/FreeV", "published": "2024-06-12 13:29:36", "link": "http://arxiv.org/abs/2406.08196v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "LAFMA: A Latent Flow Matching Model for Text-to-Audio Generation", "abstract": "Recently, the application of diffusion models has facilitated the significant\ndevelopment of speech and audio generation. Nevertheless, the quality of\nsamples generated by diffusion models still needs improvement. And the\neffectiveness of the method is accompanied by the extensive number of sampling\nsteps, leading to an extended synthesis time necessary for generating\nhigh-quality audio. Previous Text-to-Audio (TTA) methods mostly used diffusion\nmodels in the latent space for audio generation. In this paper, we explore the\nintegration of the Flow Matching (FM) model into the audio latent space for\naudio generation. The FM is an alternative simulation-free method that trains\ncontinuous normalization flows (CNF) based on regressing vector fields. We\ndemonstrate that our model significantly enhances the quality of generated\naudio samples, achieving better performance than prior models. Moreover, it\nreduces the number of inference steps to ten steps almost without sacrificing\nperformance.", "published": "2024-06-12 13:36:03", "link": "http://arxiv.org/abs/2406.08203v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Refining Self-Supervised Learnt Speech Representation using Brain\n  Activations", "abstract": "It was shown in literature that speech representations extracted by\nself-supervised pre-trained models exhibit similarities with brain activations\nof human for speech perception and fine-tuning speech representation models on\ndownstream tasks can further improve the similarity. However, it still remains\nunclear if this similarity can be used to optimize the pre-trained speech\nmodels. In this work, we therefore propose to use the brain activations\nrecorded by fMRI to refine the often-used wav2vec2.0 model by aligning model\nrepresentations toward human neural responses. Experimental results on SUPERB\nreveal that this operation is beneficial for several downstream tasks, e.g.,\nspeaker verification, automatic speech recognition, intent classification.One\ncan then consider the proposed method as a new alternative to improve\nself-supervised speech models.", "published": "2024-06-12 14:34:41", "link": "http://arxiv.org/abs/2406.08266v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SCDNet: Self-supervised Learning Feature-based Speaker Change Detection", "abstract": "Speaker Change Detection (SCD) is to identify boundaries among speakers in a\nconversation. Motivated by the success of fine-tuning wav2vec 2.0 models for\nthe SCD task, a further investigation of self-supervised learning (SSL)\nfeatures for SCD is conducted in this work. Specifically, an SCD model, named\nSCDNet, is proposed. With this model, various state-of-the-art SSL models,\nincluding Hubert, wav2vec 2.0, and WavLm are investigated. To discern the most\npotent layer of SSL models for SCD, a learnable weighting method is employed to\nanalyze the effectiveness of intermediate representations. Additionally, a\nfine-tuning-based approach is also implemented to further compare the\ncharacteristics of SSL models in the SCD task. Furthermore, a contrastive\nlearning method is proposed to mitigate the overfitting tendencies in the\ntraining of both the fine-tuning-based method and SCDNet. Experiments showcase\nthe superiority of WavLm in the SCD task and also demonstrate the good design\nof SCDNet.", "published": "2024-06-12 16:43:55", "link": "http://arxiv.org/abs/2406.08393v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Neural Blind Source Separation and Diarization for Distant Speech\n  Recognition", "abstract": "This paper presents a neural method for distant speech recognition (DSR) that\njointly separates and diarizes speech mixtures without supervision by isolated\nsignals. A standard separation method for multi-talker DSR is a statistical\nmultichannel method called guided source separation (GSS). While GSS does not\nrequire signal-level supervision, it relies on speaker diarization results to\nhandle unknown numbers of active speakers. To overcome this limitation, we\nintroduce and train a neural inference model in a weakly-supervised manner,\nemploying the objective function of a statistical separation method. This\ntraining requires only multichannel mixtures and their temporal annotations of\nspeaker activities. In contrast to GSS, the trained model can jointly separate\nand diarize speech mixtures without any auxiliary information. The experiments\nwith the AMI corpus show that our method outperforms GSS with oracle\ndiarization results regarding word error rates. The code is available online.", "published": "2024-06-12 16:45:35", "link": "http://arxiv.org/abs/2406.08396v1", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "TokSing: Singing Voice Synthesis based on Discrete Tokens", "abstract": "Recent advancements in speech synthesis witness significant benefits by\nleveraging discrete tokens extracted from self-supervised learning (SSL)\nmodels. Discrete tokens offer higher storage efficiency and greater operability\nin intermediate representations compared to traditional continuous Mel\nspectrograms. However, when it comes to singing voice synthesis(SVS), achieving\nhigher levels of melody expression poses a great challenge for utilizing\ndiscrete tokens. In this paper, we introduce TokSing, a discrete-based SVS\nsystem equipped with a token formulator that offers flexible token blendings.\nWe observe a melody degradation during discretization, prompting us to\nintegrate a melody signal with the discrete token and incorporate a\nspecially-designed melody enhancement strategy in the musical encoder.\nExtensive experiments demonstrate that our TokSing achieves better performance\nagainst the Mel spectrogram baselines while offering advantages in intermediate\nrepresentation space cost and convergence speed.", "published": "2024-06-12 16:59:24", "link": "http://arxiv.org/abs/2406.08416v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Towards Musically Informed Evaluation of Piano Transcription Models", "abstract": "Automatic piano transcription models are typically evaluated using simple\nframe- or note-wise information retrieval (IR) metrics. Such benchmark metrics\ndo not provide insights into the transcription quality of specific musical\naspects such as articulation, dynamics, or rhythmic precision of the output,\nwhich are essential in the context of expressive performance analysis.\nFurthermore, in recent years, MAESTRO has become the de-facto training and\nevaluation dataset for such models. However, inference performance has been\nobserved to deteriorate substantially when applied on out-of-distribution data,\nthereby questioning the suitability and reliability of transcribed outputs from\nsuch models for specific MIR tasks. In this work, we investigate the\nperformance of three state-of-the-art piano transcription models in two\nexperiments. In the first one, we propose a variety of musically informed\nevaluation metrics which, in contrast to the IR metrics, offer more detailed\ninsight into the musical quality of the transcriptions. In the second\nexperiment, we compare inference performance on real-world and perturbed audio\nrecordings, and highlight musical dimensions which our metrics can help\nexplain. Our experimental results highlight the weaknesses of existing piano\ntranscription metrics and contribute to a more musically sound error analysis\nof transcription outputs.", "published": "2024-06-12 17:45:28", "link": "http://arxiv.org/abs/2406.08454v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Training Data Augmentation for Dysarthric Automatic Speech Recognition\n  by Text-to-Dysarthric-Speech Synthesis", "abstract": "Automatic speech recognition (ASR) research has achieved impressive\nperformance in recent years and has significant potential for enabling access\nfor people with dysarthria (PwD) in augmentative and alternative communication\n(AAC) and home environment systems. However, progress in dysarthric ASR (DASR)\nhas been limited by high variability in dysarthric speech and limited public\navailability of dysarthric training data. This paper demonstrates that data\naugmentation using text-to-dysarthic-speech (TTDS) synthesis for finetuning\nlarge ASR models is effective for DASR. Specifically, diffusion-based\ntext-to-speech (TTS) models can produce speech samples similar to dysarthric\nspeech that can be used as additional training data for fine-tuning ASR\nfoundation models, in this case Whisper. Results show improved synthesis\nmetrics and ASR performance for the proposed multi-speaker diffusion-based TTDS\ndata augmentation for ASR fine-tuning compared to current DASR baselines.", "published": "2024-06-12 18:11:24", "link": "http://arxiv.org/abs/2406.08568v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "EmoSphere-TTS: Emotional Style and Intensity Modeling via Spherical\n  Emotion Vector for Controllable Emotional Text-to-Speech", "abstract": "Despite rapid advances in the field of emotional text-to-speech (TTS), recent\nstudies primarily focus on mimicking the average style of a particular emotion.\nAs a result, the ability to manipulate speech emotion remains constrained to\nseveral predefined labels, compromising the ability to reflect the nuanced\nvariations of emotion. In this paper, we propose EmoSphere-TTS, which\nsynthesizes expressive emotional speech by using a spherical emotion vector to\ncontrol the emotional style and intensity of the synthetic speech. Without any\nhuman annotation, we use the arousal, valence, and dominance pseudo-labels to\nmodel the complex nature of emotion via a Cartesian-spherical transformation.\nFurthermore, we propose a dual conditional adversarial network to improve the\nquality of generated speech by reflecting the multi-aspect characteristics. The\nexperimental results demonstrate the model ability to control emotional style\nand intensity with high-quality expressive speech.", "published": "2024-06-12 01:40:29", "link": "http://arxiv.org/abs/2406.07803v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Zero-Shot Fake Video Detection by Audio-Visual Consistency", "abstract": "Recent studies have advocated the detection of fake videos as a one-class\ndetection task, predicated on the hypothesis that the consistency between audio\nand visual modalities of genuine data is more significant than that of fake\ndata. This methodology, which solely relies on genuine audio-visual data while\nnegating the need for forged counterparts, is thus delineated as a `zero-shot'\ndetection paradigm. This paper introduces a novel zero-shot detection approach\nanchored in content consistency across audio and video. By employing\npre-trained ASR and VSR models, we recognize the audio and video content\nsequences, respectively. Then, the edit distance between the two sequences is\ncomputed to assess whether the claimed video is genuine. Experimental results\nindicate that, compared to two mainstream approaches based on semantic\nconsistency and temporal consistency, our approach achieves superior\ngeneralizability across various deepfake techniques and demonstrates strong\nrobustness against audio-visual perturbations. Finally, state-of-the-art\nperformance gains can be achieved by simply integrating the decision scores of\nthese three systems.", "published": "2024-06-12 04:06:56", "link": "http://arxiv.org/abs/2406.07854v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Flexible Music-Conditioned Dance Generation with Style Description\n  Prompts", "abstract": "Dance plays an important role as an artistic form and expression in human\nculture, yet the creation of dance remains a challenging task. Most dance\ngeneration methods primarily rely solely on music, seldom taking into\nconsideration intrinsic attributes such as music style or genre. In this work,\nwe introduce Flexible Dance Generation with Style Description Prompts (DGSDP),\na diffusion-based framework suitable for diversified tasks of dance generation\nby fully leveraging the semantics of music style. The core component of this\nframework is Music-Conditioned Style-Aware Diffusion (MCSAD), which comprises a\nTransformer-based network and a music Style Modulation module. The MCSAD seemly\nintegrates music conditions and style description prompts into the dance\ngeneration framework, ensuring that generated dances are consistent with the\nmusic content and style. To facilitate flexible dance generation and\naccommodate different tasks, a spatial-temporal masking strategy is effectively\napplied in the backward diffusion process. The proposed framework successfully\ngenerates realistic dance sequences that are accurately aligned with music for\na variety of tasks such as long-term generation, dance in-betweening, dance\ninpainting, and etc. We hope that this work has the potential to inspire dance\ngeneration and creation, with promising applications in entertainment, art, and\neducation.", "published": "2024-06-12 04:55:14", "link": "http://arxiv.org/abs/2406.07871v1", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "CTC-aligned Audio-Text Embedding for Streaming Open-vocabulary Keyword\n  Spotting", "abstract": "This paper introduces a novel approach for streaming openvocabulary keyword\nspotting (KWS) with text-based keyword enrollment. For every input frame, the\nproposed method finds the optimal alignment ending at the frame using\nconnectionist temporal classification (CTC) and aggregates the frame-level\nacoustic embedding (AE) to obtain higher-level (i.e., character, word, or\nphrase) AE that aligns with the text embedding (TE) of the target keyword text.\nAfter that, we calculate the similarity of the aggregated AE and the TE. To the\nbest of our knowledge, this is the first attempt to dynamically align the audio\nand the keyword text on-the-fly to attain the joint audio-text embedding for\nKWS. Despite operating in a streaming fashion, our approach achieves\ncompetitive performance on the LibriPhrase dataset compared to the\nnon-streaming methods with a mere 155K model parameters and a decoding\nalgorithm with time complexity O(U), where U is the length of the target\nkeyword at inference time.", "published": "2024-06-12 06:44:40", "link": "http://arxiv.org/abs/2406.07923v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Codecfake: An Initial Dataset for Detecting LLM-based Deepfake Audio", "abstract": "With the proliferation of Large Language Model (LLM) based deepfake audio,\nthere is an urgent need for effective detection methods. Previous deepfake\naudio generation methods typically involve a multi-step generation process,\nwith the final step using a vocoder to predict the waveform from handcrafted\nfeatures. However, LLM-based audio is directly generated from discrete neural\ncodecs in an end-to-end generation process, skipping the final step of vocoder\nprocessing. This poses a significant challenge for current audio deepfake\ndetection (ADD) models based on vocoder artifacts. To effectively detect\nLLM-based deepfake audio, we focus on the core of the generation process, the\nconversion from neural codec to waveform. We propose Codecfake dataset, which\nis generated by seven representative neural codec methods. Experiment results\nshow that codec-trained ADD models exhibit a 41.406% reduction in average equal\nerror rate compared to vocoder-trained ADD models on the Codecfake test set.", "published": "2024-06-12 11:47:23", "link": "http://arxiv.org/abs/2406.08112v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Asynchronous Voice Anonymization Using Adversarial Perturbation On\n  Speaker Embedding", "abstract": "Voice anonymization has been developed as a technique for preserving privacy\nby replacing the speaker's voice in a speech signal with that of a\npseudo-speaker, thereby obscuring the original voice attributes from machine\nrecognition and human perception. In this paper, we focus on altering the voice\nattributes against machine recognition while retaining human perception. We\nreferred to this as the asynchronous voice anonymization. To this end, a speech\ngeneration framework incorporating a speaker disentanglement mechanism is\nemployed to generate the anonymized speech. The speaker attributes are altered\nthrough adversarial perturbation applied on the speaker embedding, while human\nperception is preserved by controlling the intensity of perturbation.\nExperiments conducted on the LibriSpeech dataset showed that the speaker\nattributes were obscured with their human perception preserved for 60.71% of\nthe processed utterances.", "published": "2024-06-12 13:33:24", "link": "http://arxiv.org/abs/2406.08200v3", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "CoLM-DSR: Leveraging Neural Codec Language Modeling for Multi-Modal\n  Dysarthric Speech Reconstruction", "abstract": "Dysarthric speech reconstruction (DSR) aims to transform dysarthric speech\ninto normal speech. It still suffers from low speaker similarity and poor\nprosody naturalness. In this paper, we propose a multi-modal DSR model by\nleveraging neural codec language modeling to improve the reconstruction\nresults, especially for the speaker similarity and prosody naturalness. Our\nproposed model consists of: (i) a multi-modal content encoder to extract robust\nphoneme embeddings from dysarthric speech with auxiliary visual inputs; (ii) a\nspeaker codec encoder to extract and normalize the speaker-aware codecs from\nthe dysarthric speech, in order to provide original timbre and normal prosody;\n(iii) a codec language model based speech decoder to reconstruct the speech\nbased on the extracted phoneme embeddings and normalized codecs. Evaluations on\nthe commonly used UASpeech corpus show that our proposed model can achieve\nsignificant improvements in terms of speaker similarity and prosody\nnaturalness.", "published": "2024-06-12 15:42:21", "link": "http://arxiv.org/abs/2406.08336v2", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Diff-A-Riff: Musical Accompaniment Co-creation via Latent Diffusion\n  Models", "abstract": "Recent advancements in deep generative models present new opportunities for\nmusic production but also pose challenges, such as high computational demands\nand limited audio quality. Moreover, current systems frequently rely solely on\ntext input and typically focus on producing complete musical pieces, which is\nincompatible with existing workflows in music production. To address these\nissues, we introduce \"Diff-A-Riff,\" a Latent Diffusion Model designed to\ngenerate high-quality instrumental accompaniments adaptable to any musical\ncontext. This model offers control through either audio references, text\nprompts, or both, and produces 48kHz pseudo-stereo audio while significantly\nreducing inference time and memory usage. We demonstrate the model's\ncapabilities through objective metrics and subjective listening tests, with\nextensive examples available on the accompanying website:\nsonycslparis.github.io/diffariff-companion/", "published": "2024-06-12 16:34:26", "link": "http://arxiv.org/abs/2406.08384v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SVSNet+: Enhancing Speaker Voice Similarity Assessment Models with\n  Representations from Speech Foundation Models", "abstract": "Representations from pre-trained speech foundation models (SFMs) have shown\nimpressive performance in many downstream tasks. However, the potential\nbenefits of incorporating pre-trained SFM representations into speaker voice\nsimilarity assessment have not been thoroughly investigated. In this paper, we\npropose SVSNet+, a model that integrates pre-trained SFM representations to\nimprove performance in assessing speaker voice similarity. Experimental results\non the Voice Conversion Challenge 2018 and 2020 datasets show that SVSNet+\nincorporating WavLM representations shows significant improvements compared to\nbaseline models. In addition, while fine-tuning WavLM with a small dataset of\nthe downstream task does not improve performance, using the same dataset to\nlearn a weighted-sum representation of WavLM can substantially improve\nperformance. Furthermore, when WavLM is replaced by other SFMs, SVSNet+ still\noutperforms the baseline models and exhibits strong generalization ability.", "published": "2024-06-12 17:37:09", "link": "http://arxiv.org/abs/2406.08445v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Toward Fully-End-to-End Listened Speech Decoding from EEG Signals", "abstract": "Speech decoding from EEG signals is a challenging task, where brain activity\nis modeled to estimate salient characteristics of acoustic stimuli. We propose\nFESDE, a novel framework for Fully-End-to-end Speech Decoding from EEG signals.\nOur approach aims to directly reconstruct listened speech waveforms given EEG\nsignals, where no intermediate acoustic feature processing step is required.\nThe proposed method consists of an EEG module and a speech module along with a\nconnector. The EEG module learns to better represent EEG signals, while the\nspeech module generates speech waveforms from model representations. The\nconnector learns to bridge the distributions of the latent spaces of EEG and\nspeech. The proposed framework is both simple and efficient, by allowing\nsingle-step inference, and outperforms prior works on objective metrics. A\nfine-grained phoneme analysis is conducted to unveil model characteristics of\nspeech decoding. The source code is available here: github.com/lee-jhwn/fesde.", "published": "2024-06-12 21:08:12", "link": "http://arxiv.org/abs/2406.08644v1", "categories": ["eess.SP", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
{"title": "Comparative Analysis of Personalized Voice Activity Detection Systems:\n  Assessing Real-World Effectiveness", "abstract": "Voice activity detection (VAD) is a critical component in various\napplications such as speech recognition, speech enhancement, and hands-free\ncommunication systems. With the increasing demand for personalized and\ncontext-aware technologies, the need for effective personalized VAD systems has\nbecome paramount. In this paper, we present a comparative analysis of\nPersonalized Voice Activity Detection (PVAD) systems to assess their real-world\neffectiveness. We introduce a comprehensive approach to assess PVAD systems,\nincorporating various performance metrics such as frame-level and\nutterance-level error rates, detection latency and accuracy, alongside\nuser-level analysis. Through extensive experimentation and evaluation, we\nprovide a thorough understanding of the strengths and limitations of various\nPVAD variants. This paper advances the understanding of PVAD technology by\noffering insights into its efficacy and viability in practical applications\nusing a comprehensive set of metrics.", "published": "2024-06-12 00:53:48", "link": "http://arxiv.org/abs/2406.09443v1", "categories": ["eess.AS", "cs.HC", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Emotion Manipulation Through Music -- A Deep Learning Interactive Visual\n  Approach", "abstract": "Music evokes emotion in many people. We introduce a novel way to manipulate\nthe emotional content of a song using AI tools. Our goal is to achieve the\ndesired emotion while leaving the original melody as intact as possible. For\nthis, we create an interactive pipeline capable of shifting an input song into\na diametrically opposed emotion and visualize this result through Russel's\nCircumplex model. Our approach is a proof-of-concept for Semantic Manipulation\nof Music, a novel field aimed at modifying the emotional content of existing\nmusic. We design a deep learning model able to assess the accuracy of our\nmodifications to key, SoundFont instrumentation, and other musical features.\nThe accuracy of our model is in-line with the current state of the art\ntechniques on the 4Q Emotion dataset. With further refinement, this research\nmay contribute to on-demand custom music generation, the automated remixing of\nexisting work, and music playlists tuned for emotional progression.", "published": "2024-06-12 20:12:29", "link": "http://arxiv.org/abs/2406.08623v1", "categories": ["cs.SD", "cs.AI", "cs.CY", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
