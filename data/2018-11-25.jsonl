{"title": "Reinforced Cross-Modal Matching and Self-Supervised Imitation Learning\n  for Vision-Language Navigation", "abstract": "Vision-language navigation (VLN) is the task of navigating an embodied agent\nto carry out natural language instructions inside real 3D environments. In this\npaper, we study how to address three critical challenges for this task: the\ncross-modal grounding, the ill-posed feedback, and the generalization problems.\nFirst, we propose a novel Reinforced Cross-Modal Matching (RCM) approach that\nenforces cross-modal grounding both locally and globally via reinforcement\nlearning (RL). Particularly, a matching critic is used to provide an intrinsic\nreward to encourage global matching between instructions and trajectories, and\na reasoning navigator is employed to perform cross-modal grounding in the local\nvisual scene. Evaluation on a VLN benchmark dataset shows that our RCM model\nsignificantly outperforms previous methods by 10% on SPL and achieves the new\nstate-of-the-art performance. To improve the generalizability of the learned\npolicy, we further introduce a Self-Supervised Imitation Learning (SIL) method\nto explore unseen environments by imitating its own past, good decisions. We\ndemonstrate that SIL can approximate a better and more efficient policy, which\ntremendously minimizes the success rate performance gap between seen and unseen\nenvironments (from 30.7% to 11.7%).", "published": "2018-11-25 20:49:58", "link": "http://arxiv.org/abs/1811.10092v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "primary_category": "cs.CV"}
{"title": "Sentiment Analysis of Financial News Articles using Performance\n  Indicators", "abstract": "Mining financial text documents and understanding the sentiments of\nindividual investors, institutions and markets is an important and challenging\nproblem in the literature. Current approaches to mine sentiments from financial\ntexts largely rely on domain specific dictionaries. However, dictionary based\nmethods often fail to accurately predict the polarity of financial texts. This\npaper aims to improve the state-of-the-art and introduces a novel sentiment\nanalysis approach that employs the concept of financial and non-financial\nperformance indicators. It presents an association rule mining based\nhierarchical sentiment classifier model to predict the polarity of financial\ntexts as positive, neutral or negative. The performance of the proposed model\nis evaluated on a benchmark financial dataset. The model is also compared\nagainst other state-of-the-art dictionary and machine learning based approaches\nand the results are found to be quite promising. The novel use of performance\nindicators for financial sentiment analysis offers interesting and useful\ninsights.", "published": "2018-11-25 01:36:12", "link": "http://arxiv.org/abs/1811.11008v1", "categories": ["cs.IR", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.IR"}
{"title": "A Method for Analysis of Patient Speech in Dialogue for Dementia\n  Detection", "abstract": "We present an approach to automatic detection of Alzheimer's type dementia\nbased on characteristics of spontaneous spoken language dialogue consisting of\ninterviews recorded in natural settings. The proposed method employs additive\nlogistic regression (a machine learning boosting method) on content-free\nfeatures extracted from dialogical interaction to build a predictive model. The\nmodel training data consisted of 21 dialogues between patients with Alzheimer's\nand interviewers, and 17 dialogues between patients with other health\nconditions and interviewers. Features analysed included speech rate,\nturn-taking patterns and other speech parameters. Despite relying solely on\ncontent-free features, our method obtains overall accuracy of 86.5\\%, a result\ncomparable to those of state-of-the-art methods that employ more complex\nlexical, syntactic and semantic features. While further investigation is\nneeded, the fact that we were able to obtain promising results using only\nfeatures that can be easily extracted from spontaneous dialogues suggests the\npossibility of designing non-invasive and low-cost mental health monitoring\ntools for use at scale.", "published": "2018-11-25 01:30:16", "link": "http://arxiv.org/abs/1811.09919v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Glottal Closure Instants Detection From Pathological Acoustic Speech\n  Signal Using Deep Learning", "abstract": "In this paper, we propose a classification based glottal closure instants\n(GCI) detection from pathological acoustic speech signal, which finds many\napplications in vocal disorder analysis. Till date, GCI for pathological\ndisorder is extracted from laryngeal (glottal source) signal recorded from\nElectroglottograph, a dedicated device designed to measure the vocal folds\nvibration around the larynx. We have created a pathological dataset which\nconsists of simultaneous recordings of glottal source and acoustic speech\nsignal of six different disorders from vocal disordered patients. The GCI\nlocations are manually annotated for disorder analysis and supervised learning.\nWe have proposed convolutional neural network based GCI detection method by\nfusing deep acoustic speech and linear prediction residual features for robust\nGCI detection. The experimental results showed that the proposed method is\nsignificantly better than the state-of-the-art GCI detection methods.", "published": "2018-11-25 06:18:24", "link": "http://arxiv.org/abs/1811.09956v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Learning Sound Events From Webly Labeled Data", "abstract": "In the last couple of years, weakly labeled learning has turned out to be an\nexciting approach for audio event detection. In this work, we introduce webly\nlabeled learning for sound events which aims to remove human supervision\naltogether from the learning process. We first develop a method of obtaining\nlabeled audio data from the web (albeit noisy), in which no manual labeling is\ninvolved. We then describe methods to efficiently learn from these webly\nlabeled audio recordings. In our proposed system, WeblyNet, two deep neural\nnetworks co-teach each other to robustly learn from webly labeled data, leading\nto around 17% relative improvement over the baseline method. The method also\ninvolves transfer learning to obtain efficient representations", "published": "2018-11-25 07:23:44", "link": "http://arxiv.org/abs/1811.09967v4", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
