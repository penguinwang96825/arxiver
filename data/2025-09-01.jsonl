{"title": "Mic Drop or Data Flop? Evaluating the Fitness for Purpose of AI Voice Interviewers for Data Collection within Quantitative & Qualitative Research Contexts", "abstract": "Transformer-based Large Language Models (LLMs) have paved the way for \"AI\ninterviewers\" that can administer voice-based surveys with respondents in\nreal-time. This position paper reviews emerging evidence to understand when\nsuch AI interviewing systems are fit for purpose for collecting data within\nquantitative and qualitative research contexts. We evaluate the capabilities of\nAI interviewers as well as current Interactive Voice Response (IVR) systems\nacross two dimensions: input/output performance (i.e., speech recognition,\nanswer recording, emotion handling) and verbal reasoning (i.e., ability to\nprobe, clarify, and handle branching logic). Field studies suggest that AI\ninterviewers already exceed IVR capabilities for both quantitative and\nqualitative data collection, but real-time transcription error rates, limited\nemotion detection abilities, and uneven follow-up quality indicate that the\nutility, use and adoption of current AI interviewer technology may be\ncontext-dependent for qualitative data collection efforts.", "published": "2025-09-01 22:44:57", "link": "http://arxiv.org/abs/2509.01814v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "ShortageSim: Simulating Drug Shortages under Information Asymmetry", "abstract": "Drug shortages pose critical risks to patient care and healthcare systems\nworldwide, yet the effectiveness of regulatory interventions remains poorly\nunderstood due to fundamental information asymmetries in pharmaceutical supply\nchains. We present \\textbf{ShortageSim}, the first Large Language Model\n(LLM)-based multi-agent simulation framework that captures the complex,\nstrategic interactions between drug manufacturers, institutional buyers, and\nregulatory agencies in response to shortage alerts. Unlike traditional\ngame-theoretic models that assume perfect rationality and complete information,\n\\textbf{ShortageSim} leverages LLMs to simulate bounded-rational\ndecision-making under uncertainty. Through a sequential production game\nspanning multiple quarters, we model how FDA announcements, both reactive\nalerts about existing shortages and proactive warnings about potential\ndisruptions, propagate through the supply chain and influence capacity\ninvestment and procurement decisions. Our experiments on historical shortage\nevents reveal that \\textbf{ShortageSim} reduces the resolution-lag percentage\nfor discontinued-disclosed cases by 83\\%, bringing simulated durations more\naligned to ground truth than the zero-shot baseline. We open-source\n\\textbf{ShortageSim} and a dataset of 2,925 FDA shortage events at\nhttps://github.com/Lemutisme/Sortage_Management, providing a novel\ncomputational framework for designing and testing interventions in complex,\ninformation-scarce supply chains.", "published": "2025-09-01 22:39:28", "link": "http://arxiv.org/abs/2509.01813v1", "categories": ["cs.MA", "cs.CL", "cs.GT"], "primary_category": "cs.MA"}
{"title": "Flaw or Artifact? Rethinking Prompt Sensitivity in Evaluating LLMs", "abstract": "Prompt sensitivity, referring to the phenomenon where paraphrasing (i.e.,\nrepeating something written or spoken using different words) leads to\nsignificant changes in large language model (LLM) performance, has been widely\naccepted as a core limitation of LLMs. In this work, we revisit this issue and\nask: Is the widely reported high prompt sensitivity truly an inherent weakness\nof LLMs, or is it largely an artifact of evaluation processes? To answer this\nquestion, we systematically evaluate 7 LLMs (e.g., GPT and Gemini family)\nacross 6 benchmarks, including both multiple-choice and open-ended tasks on 12\ndiverse prompt templates. We find that much of the prompt sensitivity stems\nfrom heuristic evaluation methods, including log-likelihood scoring and rigid\nanswer matching, which often overlook semantically correct responses expressed\nthrough alternative phrasings, such as synonyms or paraphrases. When we adopt\nLLM-as-a-Judge evaluations, we observe a substantial reduction in performance\nvariance and a consistently higher correlation in model rankings across\nprompts. Our findings suggest that modern LLMs are more robust to prompt\ntemplates than previously believed, and that prompt sensitivity may be more an\nartifact of evaluation than a flaw in the models.", "published": "2025-09-01 21:38:28", "link": "http://arxiv.org/abs/2509.01790v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "chDzDT: Word-level morphology-aware language model for Algerian social media text", "abstract": "Pre-trained language models (PLMs) have substantially advanced natural\nlanguage processing by providing context-sensitive text representations.\nHowever, the Algerian dialect remains under-represented, with few dedicated\nmodels available. Processing this dialect is challenging due to its complex\nmorphology, frequent code-switching, multiple scripts, and strong lexical\ninfluences from other languages. These characteristics complicate tokenization\nand reduce the effectiveness of conventional word- or subword-level approaches.\n  To address this gap, we introduce chDzDT, a character-level pre-trained\nlanguage model tailored for Algerian morphology. Unlike conventional PLMs that\nrely on token sequences, chDzDT is trained on isolated words. This design\nallows the model to encode morphological patterns robustly, without depending\non token boundaries or standardized orthography. The training corpus draws from\ndiverse sources, including YouTube comments, French, English, and Berber\nWikipedia, as well as the Tatoeba project. It covers multiple scripts and\nlinguistic varieties, resulting in a substantial pre-training workload.\n  Our contributions are threefold: (i) a detailed morphological analysis of\nAlgerian dialect using YouTube comments; (ii) the construction of a\nmultilingual Algerian lexicon dataset; and (iii) the development and extensive\nevaluation of a character-level PLM as a morphology-focused encoder for\ndownstream tasks. The proposed approach demonstrates the potential of\ncharacter-level modeling for morphologically rich, low-resource dialects and\nlays a foundation for more inclusive and adaptable NLP systems.", "published": "2025-09-01 21:09:55", "link": "http://arxiv.org/abs/2509.01772v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "An LLM-enabled semantic-centric framework to consume privacy policies", "abstract": "In modern times, people have numerous online accounts, but they rarely read\nthe Terms of Service or Privacy Policy of those sites, despite claiming\notherwise, due to the practical difficulty in comprehending them. The mist of\ndata privacy practices forms a major barrier for user-centred Web approaches,\nand for data sharing and reusing in an agentic world. Existing research\nproposed methods for using formal languages and reasoning for verifying the\ncompliance of a specified policy, as a potential cure for ignoring privacy\npolicies. However, a critical gap remains in the creation or acquisition of\nsuch formal policies at scale. We present a semantic-centric approach for using\nstate-of-the-art large language models (LLM), to automatically identify key\ninformation about privacy practices from privacy policies, and construct\n$\\mathit{Pr}^2\\mathit{Graph}$, knowledge graph with grounding from Data Privacy\nVocabulary (DPV) for privacy practices, to support downstream tasks. Along with\nthe pipeline, the $\\mathit{Pr}^2\\mathit{Graph}$ for the top-100 popular\nwebsites is also released as a public resource, by using the pipeline for\nanalysis. We also demonstrate how the $\\mathit{Pr}^2\\mathit{Graph}$ can be used\nto support downstream tasks by constructing formal policy representations such\nas Open Digital Right Language (ODRL) or perennial semantic Data Terms of Use\n(psDToU). To evaluate the technology capability, we enriched the Policy-IE\ndataset by employing legal experts to create custom annotations. We benchmarked\nthe performance of different large language models for our pipeline and\nverified their capabilities. Overall, they shed light on the possibility of\nlarge-scale analysis of online services' privacy practices, as a promising\ndirection to audit the Web and the Internet. We release all datasets and source\ncode as public resources to facilitate reuse and improvement.", "published": "2025-09-01 18:53:13", "link": "http://arxiv.org/abs/2509.01716v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Bridging Thoughts and Words: Graph-Based Intent-Semantic Joint Learning for Fake News Detection", "abstract": "Fake news detection is an important and challenging task for defending online\ninformation integrity. Existing state-of-the-art approaches typically extract\nnews semantic clues, such as writing patterns that include emotional words,\nstylistic features, etc. However, detectors tuned solely to such semantic clues\ncan easily fall into surface detection patterns, which can shift rapidly in\ndynamic environments, leading to limited performance in the evolving news\nlandscape. To address this issue, this paper investigates a novel perspective\nby incorporating news intent into fake news detection, bridging intents and\nsemantics together. The core insight is that by considering news intents, one\ncan deeply understand the inherent thoughts behind news deception, rather than\nthe surface patterns within words alone. To achieve this goal, we propose\nGraph-based Intent-Semantic Joint Modeling (InSide) for fake news detection,\nwhich models deception clues from both semantic and intent signals via\ngraph-based joint learning. Specifically, InSide reformulates news semantic and\nintent signals into heterogeneous graph structures, enabling long-range context\ninteraction through entity guidance and capturing both holistic and\nimplementation-level intent via coarse-to-fine intent modeling. To achieve\nbetter alignment between semantics and intents, we further develop a dynamic\npathway-based graph alignment strategy for effective message passing and\naggregation across these signals by establishing a common space. Extensive\nexperiments on four benchmark datasets demonstrate the superiority of the\nproposed InSide compared to state-of-the-art methods.", "published": "2025-09-01 17:59:18", "link": "http://arxiv.org/abs/2509.01660v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reinforced Visual Perception with Tools", "abstract": "Visual reasoning, a cornerstone of human intelligence, encompasses complex\nperceptual and logical processes essential for solving diverse visual problems.\nWhile advances in computer vision have produced powerful models for various\nperceptual tasks, leveraging these for general visual reasoning remains\nchallenging. Prior work demonstrates that augmenting LLMs with vision models\nvia supervised finetuning improves performance, but faces key limitations such\nas expensive data generation, reliance on careful data filtering, and poor\ngeneralization. To address these issues, we propose ReVPT to enhance\nmulti-modal LLMs' abilities to reason about and use visual tools through\nreinforcement learning. We introduce a novel RL algorithm based on GRPO,\ndesigned to train models to reason with a suite of four visual tools. Through\nextensive experiments, we show that our method achieves state-of-the-art\nperformance on several perception-heavy benchmarks, including SAT, CV-Bench,\nBLINK and MMStar, significantly outperforming the supervised and text-based RL\nfinetuning baselines. Notably, Our ReVPT-3B and ReVPT-7B outperform the\ninstruct models by 9.03% and 9.44% on CV-Bench. Finally, we bring to the\ncommunity new insights on RL-based visual tool-usage through extensive\nablations. Our code is available at https://github.com/ls-kelvin/REVPT.", "published": "2025-09-01 17:57:49", "link": "http://arxiv.org/abs/2509.01656v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Parallel Needleman-Wunsch on CUDA to measure word similarity based on phonetic transcriptions", "abstract": "We present a method to calculate the similarity between words based on their\nphonetic transcription (their pronunciation) using the Needleman-Wunsch\nalgorithm. We implement this algorithm in Rust and parallelize it on both CPU\nand GPU to handle large datasets efficiently. The GPU implementation leverages\nCUDA and the cudarc Rust library to achieve significant performance\nimprovements. We validate our approach by constructing a fully-connected graph\nwhere nodes represent words and edges have weights according to the similarity\nbetween the words. This graph is then analyzed using clustering algorithms to\nidentify groups of phonetically similar words. Our results demonstrate the\nfeasibility and effectiveness of the proposed method in analyzing the phonetic\nstructure of languages. It might be easily expanded to other languages.", "published": "2025-09-01 17:54:48", "link": "http://arxiv.org/abs/2509.01654v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TransGAT: Transformer-Based Graph Neural Networks for Multi-Dimensional Automated Essay Scoring", "abstract": "Essay writing is a critical component of student assessment, yet manual\nscoring is labor-intensive and inconsistent. Automated Essay Scoring (AES)\noffers a promising alternative, but current approaches face limitations. Recent\nstudies have incorporated Graph Neural Networks (GNNs) into AES using static\nword embeddings that fail to capture contextual meaning, especially for\npolysemous words. Additionally, many methods rely on holistic scoring,\noverlooking specific writing aspects such as grammar, vocabulary, and cohesion.\nTo address these challenges, this study proposes TransGAT, a novel approach\nthat integrates fine-tuned Transformer models with GNNs for analytic scoring.\nTransGAT combines the contextual understanding of Transformers with the\nrelational modeling strength of Graph Attention Networks (GAT). It performs\ntwo-stream predictions by pairing each fine-tuned Transformer (BERT, RoBERTa,\nand DeBERTaV3) with a separate GAT. In each pair, the first stream generates\nessay-level predictions, while the second applies GAT to Transformer token\nembeddings, with edges constructed from syntactic dependencies. The model then\nfuses predictions from both streams to produce the final analytic score.\nExperiments on the ELLIPSE dataset show that TransGAT outperforms baseline\nmodels, achieving an average Quadratic Weighted Kappa (QWK) of 0.854 across all\nanalytic scoring dimensions. These findings highlight the potential of TransGAT\nto advance AES systems.", "published": "2025-09-01 17:33:19", "link": "http://arxiv.org/abs/2509.01640v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Benchmarking the Detection of LLMs-Generated Modern Chinese Poetry", "abstract": "The rapid development of advanced large language models (LLMs) has made\nAI-generated text indistinguishable from human-written text. Previous work on\ndetecting AI-generated text has made effective progress, but has not involved\nmodern Chinese poetry. Due to the distinctive characteristics of modern Chinese\npoetry, it is difficult to identify whether a poem originated from humans or\nAI. The proliferation of AI-generated modern Chinese poetry has significantly\ndisrupted the poetry ecosystem. Based on the urgency of identifying\nAI-generated poetry in the real Chinese world, this paper proposes a novel\nbenchmark for detecting LLMs-generated modern Chinese poetry. We first\nconstruct a high-quality dataset, which includes both 800 poems written by six\nprofessional poets and 41,600 poems generated by four mainstream LLMs.\nSubsequently, we conduct systematic performance assessments of six detectors on\nthis dataset. Experimental results demonstrate that current detectors cannot be\nused as reliable tools to detect modern Chinese poems generated by LLMs. The\nmost difficult poetic features to detect are intrinsic qualities, especially\nstyle. The detection results verify the effectiveness and necessity of our\nproposed benchmark. Our work lays a foundation for future detection of\nAI-generated poetry.", "published": "2025-09-01 17:01:45", "link": "http://arxiv.org/abs/2509.01620v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Testing the assumptions about the geometry of sentence embedding spaces: the cosine measure need not apply", "abstract": "Transformer models learn to encode and decode an input text, and produce\ncontextual token embeddings as a side-effect. The mapping from language into\nthe embedding space maps words expressing similar concepts onto points that are\nclose in the space. In practice, the reverse implication is also assumed: words\ncorresponding to close points in this space are similar or related, those that\nare further are not.\n  Does closeness in the embedding space extend to shared properties for\nsentence embeddings? We present an investigation of sentence embeddings and\nshow that the geometry of their embedding space is not predictive of their\nrelative performances on a variety of tasks.\n  We compute sentence embeddings in three ways: as averaged token embeddings,\nas the embedding of the special [CLS] token, and as the embedding of a random\ntoken from the sentence. We explore whether there is a correlation between the\ndistance between sentence embedding variations and their performance on\nlinguistic tasks, and whether despite their distances, they do encode the same\ninformation in the same manner.\n  The results show that the cosine similarity -- which treats dimensions\nshallowly -- captures (shallow) commonalities or differences between sentence\nembeddings, which are not predictive of their performance on specific tasks.\nLinguistic information is rather encoded in weighted combinations of different\ndimensions, which are not reflected in the geometry of the sentence embedding\nspace.", "published": "2025-09-01 16:37:03", "link": "http://arxiv.org/abs/2509.01606v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "CSRM-LLM: Embracing Multilingual LLMs for Cold-Start Relevance Matching in Emerging E-commerce Markets", "abstract": "As global e-commerce platforms continue to expand, companies are entering new\nmarkets where they encounter cold-start challenges due to limited human labels\nand user behaviors. In this paper, we share our experiences in Coupang to\nprovide a competitive cold-start performance of relevance matching for emerging\ne-commerce markets. Specifically, we present a Cold-Start Relevance Matching\n(CSRM) framework, utilizing a multilingual Large Language Model (LLM) to\naddress three challenges: (1) activating cross-lingual transfer learning\nabilities of LLMs through machine translation tasks; (2) enhancing query\nunderstanding and incorporating e-commerce knowledge by retrieval-based query\naugmentation; (3) mitigating the impact of training label errors through a\nmulti-round self-distillation training strategy. Our experiments demonstrate\nthe effectiveness of CSRM-LLM and the proposed techniques, resulting in\nsuccessful real-world deployment and significant online gains, with a 45.8%\nreduction in defect ratio and a 0.866% uplift in session purchase rate.", "published": "2025-09-01 15:51:30", "link": "http://arxiv.org/abs/2509.01566v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Enhancing Uncertainty Estimation in LLMs with Expectation of Aggregated Internal Belief", "abstract": "Large Language Models (LLMs) have achieved remarkable success across a wide\nrange of natural language tasks, but often exhibit overconfidence and generate\nplausible yet incorrect answers. This overconfidence, especially in models\nundergone Reinforcement Learning from Human Feedback (RLHF), poses significant\nchallenges for reliable uncertainty estimation and safe deployment. In this\npaper, we propose EAGLE (Expectation of AGgregated internaL bEief), a novel\nself-evaluation-based calibration method that leverages the internal hidden\nstates of LLMs to derive more accurate confidence scores. Instead of relying on\nthe model's final output, our approach extracts internal beliefs from multiple\nintermediate layers during self-evaluation. By aggregating these layer-wise\nbeliefs and calculating the expectation over the resulting confidence score\ndistribution, EAGLE produces a refined confidence score that more faithfully\nreflects the model's internal certainty. Extensive experiments on diverse\ndatasets and LLMs demonstrate that EAGLE significantly improves calibration\nperformance over existing baselines. We also provide an in-depth analysis of\nEAGLE, including a layer-wise examination of uncertainty patterns, a study of\nthe impact of self-evaluation prompts, and an analysis of the effect of\nself-evaluation score range.", "published": "2025-09-01 15:50:10", "link": "http://arxiv.org/abs/2509.01564v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "In-N-Out: A Parameter-Level API Graph Dataset for Tool Agents", "abstract": "Tool agents -- LLM-based systems that interact with external APIs -- offer a\nway to execute real-world tasks. However, as tasks become increasingly complex,\nthese agents struggle to identify and call the correct APIs in the proper\norder. To tackle this problem, we investigate converting API documentation into\na structured API graph that captures API dependencies and leveraging it for\nmulti-tool queries that require compositional API calls. To support this, we\nintroduce In-N-Out, the first expert-annotated dataset of API graphs built from\ntwo real-world API benchmarks and their documentation. Using In-N-Out\nsignificantly improves performance on both tool retrieval and multi-tool query\ngeneration, nearly doubling that of LLMs using documentation alone. Moreover,\ngraphs generated by models fine-tuned on In-N-Out close 90% of this gap,\nshowing that our dataset helps models learn to comprehend API documentation and\nparameter relationships. Our findings highlight the promise of using explicit\nAPI graphs for tool agents and the utility of In-N-Out as a valuable resource.\nWe will release the dataset and code publicly.", "published": "2025-09-01 15:42:21", "link": "http://arxiv.org/abs/2509.01560v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CAT: Causal Attention Tuning For Injecting Fine-grained Causal Knowledge into Large Language Models", "abstract": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains. However, a fundamental question remains: Can LLMs effectively utilize\ncausal knowledge for prediction and generation? Through empirical studies, we\nfind that LLMs trained directly on large-scale data often capture spurious\ncorrelations rather than true causal relationships, leading to suboptimal\nperformance, especially in out-of-distribution (OOD) scenarios. To address this\nchallenge, we propose Causal Attention Tuning (CAT), a novel approach that\ninjects fine-grained causal knowledge into the attention mechanism. We propose\nan automated pipeline that leverages human priors to automatically generate\ntoken-level causal signals and introduce the Re-Attention mechanism to guide\ntraining, helping the model focus on causal structures while mitigating noise\nand biases in attention scores. Experimental results on our proposed Spurious\nToken Game (STG) benchmark and multiple downstream tasks demonstrate that our\napproach effectively leverages causal knowledge for prediction and remains\nrobust in OOD scenarios. Implementation details can be found at\nhttps://github.com/Kairong-Han/CAT.", "published": "2025-09-01 15:13:15", "link": "http://arxiv.org/abs/2509.01535v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Service, Solidarity, and Self-Help: A Comparative Topic Modeling Analysis of Community Unionism in the Boot and Shoe Union and Unite Community", "abstract": "This paper presents a comparative analysis of community unionism (CU) in two\ndistinct historical and organizational contexts: the National Boot and Shoe\nUnion (B\\&S) in the 1920s and Unite Community in the 2010s--2020s. Using\nBERTopic for thematic modeling and cTF-IDF weighting, alongside word frequency\nanalysis, the study examines the extent to which each union's discourse aligns\nwith key features of CU -- such as coalition-building, grassroots engagement,\nand action beyond the workplace. The results reveal significant differences in\nthematic focus and discursive coherence. While Unite Community demonstrates\nstronger alignment with outward-facing, social justice-oriented themes, the\nB\\&S corpus emphasizes internal administration, industrial relations, and\nmember services -- reflecting a more traditional, servicing-oriented union\nmodel. The analysis also highlights methodological insights, demonstrating how\nmodern NLP techniques can enhance the study of historical labor archives.\nUltimately, the findings suggest that while both unions engage with\ncommunity-related themes, their underlying models of engagement diverge\nsignificantly, challenging assumptions about the continuity and universality of\ncommunity unionism across time and sector.", "published": "2025-09-01 15:02:50", "link": "http://arxiv.org/abs/2509.01529v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Arcs with increasing chords in $\\mathbf{R}^d$", "abstract": "A curve $\\gamma$ that connects $s$ and $t$ has the increasing chord property\nif $|bc| \\leq |ad|$ whenever $a,b,c,d$ lie in that order on $\\gamma$. For\nplanar curves, the length of such a curve is known to be at most $2\\pi/3 \\cdot\n|st|$. Here we examine the question in higher dimensions and from the\nalgorithmic standpoint and show the following:\n  (I) The length of any $s-t$ curve with increasing chords in $\\mathbf{R}^d$ is\nat most $2 \\cdot \\left( e/2 \\cdot (d+4) \\right)^{d-1} \\cdot |st|$ for every $d\n\\geq 3$. This is the first bound in higher dimensions.\n  (II) Given a polygonal chain $P=(p_1, p_2, \\dots, p_n)$ in $\\mathbf{R}^d$,\nwhere $d \\geq 4$, $k =\\lfloor d/2 \\rfloor$, it can be tested whether it\nsatisfies the increasing chord property in $O\\left(n^{2-1/(k+1)} {\\rm polylog}\n(n) \\right)$ expected time. This is the first subquadratic algorithm in higher\ndimensions.", "published": "2025-09-01 16:07:02", "link": "http://arxiv.org/abs/2509.01580v1", "categories": ["cs.CG", "cs.DM", "math.CO"], "primary_category": "cs.CG"}
{"title": "New Results on Vertices that Belong to Every Minimum Locating-Dominating Code", "abstract": "Locating-dominating codes have been studied widely since their introduction\nin the 1980s by Slater and Rall. In this paper, we concentrate on vertices that\nmust belong to all minimum locating-dominating codes in a graph. We call them\nmin-forced vertices. We show that the number of min-forced vertices in a\nconnected nontrivial graph of order $n$ is bounded above by $\\frac{2}{3}\\left(n\n-\\gamma^{LD}(G)\\right)$, where $\\gamma^{LD}(G)$ denotes the cardinality of a\nminimum locating-dominating code. This implies that the maximum ratio between\nthe number of min-forced vertices and the order of a connected nontrivial graph\nis at most $\\frac{2}{5}$. Moreover, both of these bounds can be attained. We\nalso determine the number of different minimum locating-dominating codes in all\npaths. In addition, we show that deciding whether a vertex is min-forced is\nco-NP-hard.", "published": "2025-09-01 13:41:22", "link": "http://arxiv.org/abs/2509.01473v1", "categories": ["math.CO", "cs.DM", "05C12"], "primary_category": "math.CO"}
{"title": "Generalizations of Ferber-Krivelevich and Gallai Theorems on parity of degrees in induced subgraphs", "abstract": "A long-standing and well-known conjecture (see e.g. Caro, Discrete Math,\n1994) states that every $n$-vertex graph $G$ without isolated vertices contains\nan induced subgraph where all vertices have an odd degree and whose order is\nlinear in $n$. Ferber and Krivelevich (Adv. Math., 2022) confirmed the\nconjecture. In this short paper, we generalize this result by considering $G$\nwith vertices labeled 0 or 1 and requiring that in an induced subgraph of $G$,\nthe 0-labeled vertices are of even degree and the 1-labeled vertices are of odd\ndegree. We prove that if $G$ has no isolated vertices, it contains such a\nsubgraph of order linear in $n$.\n  The well-known Gallai's Theorem states that the vertices of each graph can be\npartitioned into two parts such that all vertices in the subgraphs induced by\nthe two parts have even degrees. The result also holds if we require that the\ndegrees of all vertices in one of the induced subgraphs are even, and the\ndegrees of all vertices in the other induced subgraph are odd. A natural\ngeneralization of Gallai's Theorem to out-degrees in digraphs does not hold and\nwe characterize all digraphs for which it does hold. Our characterization is\nlinear algebraic.", "published": "2025-09-01 12:36:14", "link": "http://arxiv.org/abs/2509.01428v1", "categories": ["math.CO", "cs.DM"], "primary_category": "math.CO"}
{"title": "Morse sequences on stacks and flooding sequences", "abstract": "This paper builds upon the framework of \\emph{Morse sequences}, a simple and\neffective approach to discrete Morse theory. A Morse sequence on a simplicial\ncomplex consists of a sequence of nested subcomplexes generated by expansions\nand fillings-two operations originally introduced by Whitehead. Expansions\npreserve homotopy, while fillings introduce critical simplexes that capture\nessential topological features. We extend the notion of Morse sequences to\n\\emph{stacks}, which are monotonic functions defined on simplicial complexes,\nand define \\emph{Morse sequences on stacks} as those whose expansions preserve\nthe homotopy of all sublevel sets. This extension leads to a generalization of\nthe fundamental collapse theorem to weighted simplicial complexes. Within this\nframework, we focus on a refined class of sequences called \\emph{flooding\nsequences}, which exhibit an ordering behavior similar to that of classical\nwatershed algorithms. Although not every Morse sequence on a stack is a\nflooding sequence, we show that the gradient vector field associated with any\nMorse sequence can be recovered through a flooding sequence. Finally, we\npresent algorithmic schemes for computing flooding sequences using cosimplicial\ncomplexes.", "published": "2025-09-01 11:31:12", "link": "http://arxiv.org/abs/2509.01384v1", "categories": ["cs.DM", "math.AT"], "primary_category": "cs.DM"}
{"title": "Tree decompositions with small width, spread, order and degree", "abstract": "Tree-decompositions of graphs are of fundamental importance in structural and\nalgorithmic graph theory. The main property of tree-decompositions is the width\n(the maximum size of a bag $-1$). We show that every graph has a\ntree-decomposition with near-optimal width, plus several additional properties\nof interest. In particular every graph $G$ with treewidth at most $k$ has a\ntree-decomposition with width at most $72k+1$, where each vertex $v$ appears in\nat most $\\text{deg}_G(v)+1$ bags, the number of bags is at most\n$\\max\\{\\frac{|V(G)|}{2k},1\\}$, and the tree indexing the decomposition has\nmaximum degree at most 12. This improves exponential bounds to linear in a\nresult of Ding and Oporowski [1995], and establishes a conjecture of theirs in\na strong sense.", "published": "2025-09-01 05:20:27", "link": "http://arxiv.org/abs/2509.01140v1", "categories": ["math.CO", "cs.DM"], "primary_category": "math.CO"}
{"title": "Cloud-Device Collaborative Agents for Sequential Recommendation", "abstract": "Recent advances in large language models (LLMs) have enabled agent-based\nrecommendation systems with strong semantic understanding and flexible\nreasoning capabilities. While LLM-based agents deployed in the cloud offer\npowerful personalization, they often suffer from privacy concerns, limited\naccess to real-time signals, and scalability bottlenecks. Conversely, on-device\nagents ensure privacy and responsiveness but lack the computational power for\nglobal modeling and large-scale retrieval. To bridge these complementary\nlimitations, we propose CDA4Rec, a novel Cloud-Device collaborative framework\nfor sequential Recommendation, powered by dual agents: a cloud-side LLM and a\ndevice-side small language model (SLM). CDA4Rec tackles the core challenge of\ncloud-device coordination by decomposing the recommendation task into modular\nsub-tasks including semantic modeling, candidate retrieval, structured user\nmodeling, and final ranking, which are allocated to cloud or device based on\ncomputational demands and privacy sensitivity. A strategy planning mechanism\nleverages the cloud agent's reasoning ability to generate personalized\nexecution plans, enabling context-aware task assignment and partial parallel\nexecution across agents. This design ensures real-time responsiveness, improved\nefficiency, and fine-grained personalization, even under diverse user states\nand behavioral sparsity. Extensive experiments across multiple real-world\ndatasets demonstrate that CDA4Rec consistently outperforms competitive\nbaselines in both accuracy and efficiency, validating its effectiveness in\nheterogeneous and resource-constrained environments.", "published": "2025-09-01 15:28:11", "link": "http://arxiv.org/abs/2509.01551v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Ultra Fast Warm Start Solution for Graph Recommendations", "abstract": "In this work, we present a fast and effective Linear approach for updating\nrecommendations in a scalable graph-based recommender system UltraGCN. Solving\nthis task is extremely important to maintain the relevance of the\nrecommendations under the conditions of a large amount of new data and changing\nuser preferences. To address this issue, we adapt the simple yet effective\nlow-rank approximation approach to the graph-based model. Our method delivers\ninstantaneous recommendations that are up to 30 times faster than conventional\nmethods, with gains in recommendation quality, and demonstrates high\nscalability even on the large catalogue datasets.", "published": "2025-09-01 15:25:30", "link": "http://arxiv.org/abs/2509.01549v1", "categories": ["cs.IR", "cs.LG"], "primary_category": "cs.IR"}
{"title": "AI4DiTraRe: Building the BFO-Compliant Chemotion Knowledge Graph", "abstract": "Chemistry is an example of a discipline where the advancements of technology\nhave led to multi-level and often tangled and tricky processes ongoing in the\nlab. The repeatedly complex workflows are combined with information from\nchemical structures, which are essential to understand the scientific process.\nAn important tool for many chemists is Chemotion, which consists of an\nelectronic lab notebook and a repository. This paper introduces a semantic\npipeline for constructing the BFO-compliant Chemotion Knowledge Graph,\nproviding an integrated, ontology-driven representation of chemical research\ndata. The Chemotion-KG has been developed to adhere to the FAIR (Findable,\nAccessible, Interoperable, Reusable) principles and to support AI-driven\ndiscovery and reasoning in chemistry. Experimental metadata were harvested from\nthe Chemotion API in JSON-LD format, converted into RDF, and subsequently\ntransformed into a Basic Formal Ontology-aligned graph through SPARQL CONSTRUCT\nqueries. The source code and datasets are publicly available via GitHub. The\nChemotion Knowledge Graph is hosted by FIZ Karlsruhe Information Service\nEngineering. Outcomes presented in this work were achieved within the Leibniz\nScience Campus ``Digital Transformation of Research'' (DiTraRe) and are part of\nan ongoing interdisciplinary collaboration.", "published": "2025-09-01 15:13:27", "link": "http://arxiv.org/abs/2509.01536v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "ABCD-LINK: Annotation Bootstrapping for Cross-Document Fine-Grained Links", "abstract": "Understanding fine-grained relations between documents is crucial for many\napplication domains. However, the study of automated assistance is limited by\nthe lack of efficient methods to create training and evaluation datasets of\ncross-document links. To address this, we introduce a new domain-agnostic\nframework for selecting a best-performing approach and annotating\ncross-document links in a new domain from scratch. We first generate and\nvalidate semi-synthetic datasets of interconnected documents. This data is used\nto perform automatic evaluation, producing a shortlist of best-performing\nlinking approaches. These approaches are then used in an extensive human\nevaluation study, yielding performance estimates on natural text pairs. We\napply our framework in two distinct domains -- peer review and news -- and show\nthat combining retrieval models with LLMs achieves 78\\% link approval from\nhuman raters, more than doubling the precision of strong retrievers alone. Our\nframework enables systematic study of cross-document understanding across\napplication scenarios, and the resulting novel datasets lay foundation for\nnumerous cross-document tasks like media framing and peer review. We make the\ncode, data, and annotation protocols openly available.", "published": "2025-09-01 11:32:24", "link": "http://arxiv.org/abs/2509.01387v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Re3: Learning to Balance Relevance & Recency for Temporal Information Retrieval", "abstract": "Temporal Information Retrieval (TIR) is a critical yet unresolved task for\nmodern search systems, retrieving documents that not only satisfy a query's\ninformation need but also adhere to its temporal constraints. This task is\nshaped by two challenges: Relevance, ensuring alignment with the query's\nexplicit temporal requirements, and Recency, selecting the freshest document\namong multiple versions. Existing methods often address the two challenges in\nisolation, relying on brittle heuristics that fail in scenarios where temporal\nrequirements and staleness resistance are intertwined. To address this gap, we\nintroduce Re2Bench, a benchmark specifically designed to disentangle and\nevaluate Relevance, Recency, and their hybrid combination. Building on this\nfoundation, we propose Re3, a unified and lightweight framework that\ndynamically balances semantic and temporal information through a query-aware\ngating mechanism. On Re2Bench, Re3 achieves state-of-the-art results, leading\nin R@1 across all three subsets. Ablation studies with backbone sensitivity\ntests confirm robustness, showing strong generalization across diverse encoders\nand real-world settings. This work provides both a generalizable solution and a\nprincipled evaluation suite, advancing the development of temporally aware\nretrieval systems. Re3 and Re2Bench are available online:\nhttps://anonymous.4open.science/r/Re3-0C5A", "published": "2025-09-01 09:44:01", "link": "http://arxiv.org/abs/2509.01306v1", "categories": ["cs.IR", "cs.LG"], "primary_category": "cs.IR"}
{"title": "MARS: Modality-Aligned Retrieval for Sequence Augmented CTR Prediction", "abstract": "Click-through rate (CTR) prediction serves as a cornerstone of recommender\nsystems. Despite the strong performance of current CTR models based on user\nbehavior modeling, they are still severely limited by interaction sparsity,\nespecially in low-active user scenarios. To address this issue, data\naugmentation of user behavior is a promising research direction. However,\nexisting data augmentation methods heavily rely on collaborative signals while\noverlooking the rich multimodal features of items, leading to insufficient\nmodeling of low-active users.\n  To alleviate this problem, we propose a novel framework \\textbf{MARS}\n(\\textbf{M}odality-\\textbf{A}ligned \\textbf{R}etrieval for \\textbf{S}equence\nAugmented CTR Prediction). MARS utilizes a Stein kernel-based approach to align\ntext and image features into a unified and unbiased semantic space to construct\nmultimodal user embeddings. Subsequently, each low-active user's behavior\nsequence is augmented by retrieving, filtering, and concentrating the most\nsimilar behavior sequence of high-active users via multimodal user embeddings.\nValidated by extensive offline experiments and online A/B tests, our framework\nMARS consistently outperforms state-of-the-art baselines and achieves\nsubstantial growth on core business metrics within\nKuaishou~\\footnote{https://www.kuaishou.com/}. Consequently, MARS has been\nsuccessfully deployed, serving the main traffic for hundreds of millions of\nusers. To ensure reproducibility, we provide anonymous access to the\nimplementation code~\\footnote{https://github.com/wangshukuan/MARS}.", "published": "2025-09-01 07:08:44", "link": "http://arxiv.org/abs/2509.01184v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Question-to-Knowledge: Multi-Agent Generation of Inspectable Facts for Product Mapping", "abstract": "Identifying whether two product listings refer to the same Stock Keeping Unit\n(SKU) is a persistent challenge in ecommerce, especially when explicit\nidentifiers are missing and product names vary widely across platforms. Rule\nbased heuristics and keyword similarity often misclassify products by\noverlooking subtle distinctions in brand, specification, or bundle\nconfiguration. To overcome these limitations, we propose Question to Knowledge\n(Q2K), a multi agent framework that leverages Large Language Models (LLMs) for\nreliable SKU mapping. Q2K integrates: (1) a Reasoning Agent that generates\ntargeted disambiguation questions, (2) a Knowledge Agent that resolves them via\nfocused web searches, and (3) a Deduplication Agent that reuses validated\nreasoning traces to reduce redundancy and ensure consistency. A human in the\nloop mechanism further refines uncertain cases. Experiments on real world\nconsumer goods datasets show that Q2K surpasses strong baselines, achieving\nhigher accuracy and robustness in difficult scenarios such as bundle\nidentification and brand origin disambiguation. By reusing retrieved reasoning\ninstead of issuing repeated searches, Q2K balances accuracy with efficiency,\noffering a scalable and interpretable solution for product integration.", "published": "2025-09-01 07:07:19", "link": "http://arxiv.org/abs/2509.01182v1", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.IR", "cs.MA"], "primary_category": "cs.AI"}
{"title": "Beyond the Surface: A Solution-Aware Retrieval Model for Competition-level Code Generation", "abstract": "In competitive programming task, problem statements are often embedded within\nelaborate narrative backgrounds, requiring deep understanding of the underlying\nsolutions to successfully complete the tasks. Current code generation models\nprimarily focus on token-level semantic modeling, highly susceptible to\ndistractions from irrelevant narrative statements. Inspired by RAG, retrieving\nreference code with similar solutions may help enhance model performance on\ndifficult problems. However, existing retrieval models also emphasize\nsurface-level semantic similarity, neglecting the deeper solution-level logical\nsimilarities that are critical in competitive programming. Therefore, designing\nranking models capable of accurately identifying and retrieving problems and\ncorresponding codes remains an urgent research problem in competitive code\ngeneration. In this paper, we propose SolveRank, a solution-aware ranking model\nempowered by synthetic data for competitive programming tasks. Specifically, we\nleverage the DeepSeek-R1 model to generate logically equivalent but differently\nphrased new problems, verified by GPT-4o for solution consistency. Then, we\ntrain SolveRank with these as positive samples and BM25/random-retrieved\nproblems as negatives. During inference, SolveRank retrieves relevant problems\nand corresponding code from the corpus to assist a downstream code generator.\nExperiments on the xCodeEval dataset demonstrate that SolveRank outperforms\nSOTA ranking methods in precision and recall metrics, and boosts code\ngeneration performance for difficult problems.", "published": "2025-09-01 04:47:51", "link": "http://arxiv.org/abs/2509.01129v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "MatPROV: A Provenance Graph Dataset of Material Synthesis Extracted from Scientific Literature", "abstract": "Synthesis procedures play a critical role in materials research, as they\ndirectly affect material properties. With data-driven approaches increasingly\naccelerating materials discovery, there is growing interest in extracting\nsynthesis procedures from scientific literature as structured data. However,\nexisting studies often rely on rigid, domain-specific schemas with predefined\nfields for structuring synthesis procedures or assume that synthesis procedures\nare linear sequences of operations, which limits their ability to capture the\nstructural complexity of real-world procedures. To address these limitations,\nwe adopt PROV-DM, an international standard for provenance information, which\nsupports flexible, graph-based modeling of procedures. We present MatPROV, a\ndataset of PROV-DM-compliant synthesis procedures extracted from scientific\nliterature using large language models. MatPROV captures structural\ncomplexities and causal relationships among materials, operations, and\nconditions through visually intuitive directed graphs. This representation\nenables machine-interpretable synthesis knowledge, opening opportunities for\nfuture research such as automated synthesis planning and optimization.", "published": "2025-09-01 00:47:27", "link": "http://arxiv.org/abs/2509.01042v1", "categories": ["cs.LG", "cs.IR"], "primary_category": "cs.LG"}
{"title": "The Price of Sparsity: Sufficient Conditions for Sparse Recovery using Sparse and Sparsified Measurements", "abstract": "We consider the problem of recovering the support of a sparse signal using\nnoisy projections. While extensive work has been done on the dense measurement\nmatrix setting, the sparse setting remains less explored. In this work, we\nestablish sufficient conditions on the sample size for successful sparse\nrecovery using sparse measurement matrices. Bringing together our result with\npreviously known necessary conditions, we discover that, in the regime where\n$ds/p \\rightarrow +\\infty$, sparse recovery in the sparse setting exhibits a\nphase transition at an information-theoretic threshold of\n$n_{\\text{INF}}^{\\text{SP}} =\n\\Theta\\left(s\\log\\left(p/s\\right)/\\log\\left(ds/p\\right)\\right)$, where $p$\ndenotes the signal dimension, $s$ the number of non-zero components of the\nsignal, and $d$ the expected number of non-zero components per row of\nmeasurement. This expression makes the price of sparsity explicit: restricting\neach measurement to $d$ non-zeros inflates the required sample size by a factor\nof $\\log{s}/\\log\\left(ds/p\\right)$, revealing a precise trade-off between\nsampling complexity and measurement sparsity. Additionally, we examine the\neffect of sparsifying an originally dense measurement matrix on sparse signal\nrecovery. We prove in the regime of $s = \\alpha p$ and $d = \\psi p$ with\n$\\alpha, \\psi \\in \\left(0,1\\right)$ and $\\psi$ small that a sample of size\n$n^{\\text{Sp-ified}}_{\\text{INF}} = \\Theta\\left(p / \\psi^2\\right)$ is\nsufficient for recovery, subject to a certain uniform integrability conjecture,\nthe proof of which is work in progress.", "published": "2025-09-01 22:26:37", "link": "http://arxiv.org/abs/2509.01809v1", "categories": ["stat.ML", "cs.IT", "cs.LG", "math.IT", "math.ST", "stat.TH"], "primary_category": "stat.ML"}
{"title": "Mixture of Balanced Information Bottlenecks for Long-Tailed Visual Recognition", "abstract": "Deep neural networks (DNNs) have achieved significant success in various\napplications with large-scale and balanced data. However, data in real-world\nvisual recognition are usually long-tailed, bringing challenges to efficient\ntraining and deployment of DNNs. Information bottleneck (IB) is an elegant\napproach for representation learning. In this paper, we propose a balanced\ninformation bottleneck (BIB) approach, in which loss function re-balancing and\nself-distillation techniques are integrated into the original IB network. BIB\nis thus capable of learning a sufficient representation with essential\nlabel-related information fully preserved for long-tailed visual recognition.\nTo further enhance the representation learning capability, we also propose a\nnovel structure of mixture of multiple balanced information bottlenecks (MBIB),\nwhere different BIBs are responsible for combining knowledge from different\nnetwork layers. MBIB facilitates an end-to-end learning strategy that trains\nrepresentation and classification simultaneously from an information theory\nperspective. We conduct experiments on commonly used long-tailed datasets,\nincluding CIFAR100-LT, ImageNet-LT, and iNaturalist 2018. Both BIB and MBIB\nreach state-of-the-art performance for long-tailed visual recognition.", "published": "2025-09-01 22:14:12", "link": "http://arxiv.org/abs/2509.01804v1", "categories": ["cs.CV", "cs.IT", "math.IT"], "primary_category": "cs.CV"}
{"title": "Learning to Ask: Decision Transformers for Adaptive Quantitative Group Testing", "abstract": "We consider the problem of quantitative group testing (QGT), where the goal\nis to recover a sparse binary vector from aggregate subset-sum queries: each\nquery selects a subset of indices and returns the sum of those entries.\nInformation-theoretic results suggest that adaptivity could yield up to a\ntwofold reduction in the total number of required queries, yet no algorithm has\nsurpassed the non-adaptive bound, leaving its practical benefit an open\nquestion. In this paper, we reduce the QGT problem to an integer-vector\nrecovery task whose dimension scales with the sparsity of the original problem\nrather than its full ambient size. We then formulate this reduced recovery task\nas an offline reinforcement learning problem and employ Decision Transformers\nto solve it adaptively. By combining these two steps, we obtain an effective\nend-to-end method for solving the QGT problem. Our experiments show that, for\nthe first time in the literature, our adaptive algorithm reduces the average\nnumber of queries below the well-known non-adaptive information-theoretic\nbound, demonstrating that adaptivity can indeed reduce the number of queries.", "published": "2025-09-01 19:05:06", "link": "http://arxiv.org/abs/2509.01723v1", "categories": ["cs.IT", "cs.LG", "math.IT"], "primary_category": "cs.IT"}
{"title": "Hierarchical Maximum Entropy via the Renormalization Group", "abstract": "Hierarchical structures, which include multiple levels, are prevalent in\nstatistical and machine-learning models as well as physical systems. Extending\nthe foundational result that the maximum entropy distribution under mean\nconstraints is given by the exponential Gibbs-Boltzmann form, we introduce the\nframework of \"hierarchical maximum entropy\" to address these multilevel models.\nWe demonstrate that Pareto optimal distributions, which maximize entropies\nacross all levels of hierarchical transformations, can be obtained via\nrenormalization-group procedures from theoretical physics. This is achieved by\nformulating multilevel extensions of the Gibbs variational principle and the\nDonsker-Varadhan variational representation of entropy. Moreover, we explore\nsettings with hierarchical invariances that significantly simplify the\nrenormalization-group procedures, enhancing computational efficiency: quadratic\nmodular loss functions, logarithmic loss functions, and nearest-neighbor loss\nfunctions. This is accomplished through the introduction of the concept of\nparameter flows, which serves as an analog to renormalization flows in\nrenormalization group theory. This work connects ideas from probability theory,\ninformation theory, and statistical mechanics.", "published": "2025-09-01 12:30:23", "link": "http://arxiv.org/abs/2509.01424v1", "categories": ["cs.IT", "cs.LG", "math.IT"], "primary_category": "cs.IT"}
{"title": "Robust Stochastic Outperformance under Kullback-Leibler Ambiguity", "abstract": "We study the worst-case probability that $Y$ outperforms a benchmark $X$ when\nthe law of $Y$ lies in a Kullback-Leibler neighbourhood of the benchmark. The\nmax-min problem over couplings admits a tractable dual (via optimal transport),\nwhose optimiser is an exponential tilt of the benchmark law. The resulting\nsolution reduces to a one-parameter family indexed by regulizer $\\lambda$,\nwhich controls the KL information budget and induces an increasing transfer of\nprobability mass from lower to higher outcomes. The formulation can be\nevaluated from the baseline distribution and may serve as a distribution-wide\nscenario generation environment as well as a basis for robust performance\nassessment.\n  Keywords: exponential tilting; optimal transport; stochastic dominance;\nstress testing.", "published": "2025-09-01 10:29:03", "link": "http://arxiv.org/abs/2509.01346v1", "categories": ["math.PR", "cs.IT", "math.IT"], "primary_category": "math.PR"}
{"title": "From One-Dimensional Codes to Two-Dimensional Codes: A Universal Framework for the Bounded-Weight Constraint", "abstract": "Recent developments in storage- especially in the area of resistive random\naccess memory (ReRAM)- are attempting to scale the storage density by regarding\nthe information data as two-dimensional (2D), instead of one-dimensional (1D).\nCorrespondingly, new types of 2D constraints are introduced into the input\ninformation data to improve the system reliability. While 1D constraints have\nbeen extensively investigated in the literature, the study for 2D constraints\nis much less profound. Particularly, given a constraint $\\mathcal{F}$ and a\ndesign of 1D codes whose codewords satisfy $\\mathcal{F}$, the problem of\nconstructing efficient 2D codes, such that every row and every column in every\ncodeword satisfy $\\mathcal{F}$, has been a challenge. This work provides an\nefficient solution to the challenging coding problem above for the binary\nbounded-weight constrained codes that restrict the maximum number of $1$'s\n(called {\\em weight}). Formally, we propose a universal framework to design 2D\ncodes that guarantee the weight of every row and every column of length $n$ to\nbe at most $f(n)$ for any given function $f(n)$. We show that if there exists a\ndesign of capacity-approaching 1D codes, then our method also provides\ncapacity-approaching 2D codes for all $f=\\omega(\\log n)$.", "published": "2025-09-01 08:27:55", "link": "http://arxiv.org/abs/2509.01240v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "On the Resilience of Direction-Shift Keying Against Phase Noise and Short Channel Coherence Time at mmWave Frequencies", "abstract": "The rapid variation of the wireless channel (short channel coherence time)\nand the phase noise are two prominent concerns in Millimeter-wave (mmWave) and\nsub-Terahertz systems communication systems. Equalizing the channel effect and\ntracking the phase noise necessitate dense pilot insertion. Direction-Shift\nKeying (DSK), a recent variant of Spatial Modulation (SM), addresses these\nchallenges by encoding information in the Direction-of-Arrival (DoA) using a\ndistributed antenna system (DAS), rather than relying on amplitude or phase.\nDSK has been shown to extend coherence time by up to four orders of magnitude.\nDespite its promise, existing DSK studies are largely simulation-based and\nlimited to simplified roadside unit scenarios and mobile device (MD) equipped\nwith only two antennas. DSK's performance in general settings, along with the\nfundamental laws governing its behavior, such as coherence time and resilience\nto phase noise, remain open problems. In this paper, we derive the structure of\nthe optimal detector for the case of $M$-antenna MD. Then, we establish the\ngoverning law for DSK's coherence time, termed the Direction Coherence Time\n(DCT), defining the the temporal duration over which the DoA remains\napproximately invariant. We analytically establish that DCT scales with $d/v$\n(transmitter-receiver distance over velocity), while the Channel Coherence Time\n(CCT) scales with $\\lambda/v$, revealing a coherence time gain on the order of\n$d/\\lambda$ (equivalent to more than four orders of magnitude.) Furthermore, we\nprove that DSK inherently cancels the phase noise, requiring no additional\ncompensation. Analytical predictions are validated through simulations,\nconfirming the robustness and scalability of DSK in high-frequency mobile\nenvironments.", "published": "2025-09-01 03:45:54", "link": "http://arxiv.org/abs/2509.01103v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "DSDE: Dynamic Speculative Decoding with KLD Stability for Real-World Serving", "abstract": "Speculative decoding accelerates large language model inference, but its\nreliance on a fixed speculation length is suboptimal in large-batch serving\nenvironments with diverse requests. This paper explores a new direction for\ndynamic adaptation by investigating a novel class of post-hoc, diagnostic\nsignals. We propose Dynamic Speculative Decoding Engine (DSDE), a training-free\nframework built on two primary components: (1) a predictive signal based on the\nvariance of the Kullback-Leibler (KLD) divergence, which diagnoses the\ngeneration's regional stability, and (2) an adaptive speculation length cap to\nmitigate the straggler problem in per-sequence decoding. Experiments\ndemonstrate the potential of using KLD-based stability signals for dynamic\nadaptation. An algorithm guided by these signals achieves end-to-end latency\ncompetitive with leading baselines and exhibits superior robustness across\ndiverse workloads. This robustness is particularly valuable in challenging\nlow-acceptance-rate regimes, where the proposed signal maintains its diagnostic\nutility. Collectively, these findings validate post-hoc signals as a valuable\ncomponent for building more robust and intelligent LLM inference systems, and\nhighlight a promising direction for future research on dynamic speculation\nlength adaptation.", "published": "2025-09-01 03:13:50", "link": "http://arxiv.org/abs/2509.01083v1", "categories": ["cs.DC", "cs.AI", "cs.IT", "math.IT", "I.2.7; C.2.4"], "primary_category": "cs.DC"}
{"title": "Learning to Coordinate: Distributed Meta-Trajectory Optimization Via Differentiable ADMM-DDP", "abstract": "Distributed trajectory optimization via ADMM-DDP is a powerful approach for\ncoordinating multi-agent systems, but it requires extensive tuning of tightly\ncoupled hyperparameters that jointly govern local task performance and global\ncoordination. In this paper, we propose Learning to Coordinate (L2C), a general\nframework that meta-learns these hyperparameters, modeled by lightweight\nagent-wise neural networks, to adapt across diverse tasks and agent\nconfigurations. L2C differentiates end-to-end through the ADMM-DDP pipeline in\na distributed manner. It also enables efficient meta-gradient computation by\nreusing DDP components such as Riccati recursions and feedback gains. These\ngradients correspond to the optimal solutions of distributed matrix-valued LQR\nproblems, coordinated across agents via an auxiliary ADMM framework that\nbecomes convex under mild assumptions. Training is further accelerated by\ntruncating iterations and meta-learning ADMM penalty parameters optimized for\nrapid residual reduction, with provable Lipschitz-bounded gradient errors. On a\nchallenging cooperative aerial transport task, L2C generates dynamically\nfeasible trajectories in high-fidelity simulation using IsaacSIM, reconfigures\nquadrotor formations for safe 6-DoF load manipulation in tight spaces, and\nadapts robustly to varying team sizes and task conditions, while achieving up\nto $88\\%$ faster gradient computation than state-of-the-art methods.", "published": "2025-09-01 17:17:05", "link": "http://arxiv.org/abs/2509.01630v1", "categories": ["cs.LG", "cs.MA", "cs.RO", "cs.SY", "eess.SY"], "primary_category": "cs.LG"}
{"title": "Ireland in 2057: Projections using a Geographically Diverse Dynamic Microsimulation", "abstract": "This paper presents a dynamic microsimulation model developed for Ireland,\ndesigned to simulate key demographic processes and individual life-course\ntransitions from 2022 to 2057. The model captures four primary events: births,\ndeaths, internal migration, and international migration, enabling a\ncomprehensive examination of population dynamics over time. Each individual in\nthe simulation is defined by five core attributes: age, sex, marital status,\nhighest level of education attained, and economic status. These characteristics\nevolve stochastically based on transition probabilities derived from empirical\ndata from the Irish context. Individuals are spatially disaggregated at the\nElectoral Division level. By modelling individuals at this granular level, the\nsimulation facilitates in-depth local analysis of demographic shifts and\nsocioeconomic outcomes under varying scenarios and policy assumptions. The\nmodel thus serves as a versatile tool for both academic inquiry and\nevidence-based policy development, offering projections that can inform\nlong-term planning and strategic decision-making through 2057. The\nmicrosimulation achieves a close match in population size and makeup in all\nscenarios when compared to Demographic Component Methods. Education levels are\nprojected to increase significantly, with nearly 70% of young people projected\nto attain a third level degree at some point in their lifetime. The\nunemployment rate is projected to nearly half as a result of the increased\neducation levels.", "published": "2025-09-01 13:03:03", "link": "http://arxiv.org/abs/2509.01446v1", "categories": ["cs.CY", "cs.MA"], "primary_category": "cs.CY"}
{"title": "LLM-empowered Agents Simulation Framework for Scenario Generation in Service Ecosystem Governance", "abstract": "As the social environment is growing more complex and collaboration is\ndeepening, factors affecting the healthy development of service ecosystem are\nconstantly changing and diverse, making its governance a crucial research\nissue. Applying the scenario analysis method and conducting scenario rehearsals\nby constructing an experimental system before managers make decisions, losses\ncaused by wrong decisions can be largely avoided. However, it relies on\npredefined rules to construct scenarios and faces challenges such as limited\ninformation, a large number of influencing factors, and the difficulty of\nmeasuring social elements. These challenges limit the quality and efficiency of\ngenerating social and uncertain scenarios for the service ecosystem. Therefore,\nwe propose a scenario generator design method, which adaptively coordinates\nthree Large Language Model (LLM) empowered agents that autonomously optimize\nexperimental schemes to construct an experimental system and generate high\nquality scenarios. Specifically, the Environment Agent (EA) generates social\nenvironment including extremes, the Social Agent (SA) generates social\ncollaboration structure, and the Planner Agent (PA) couples task-role\nrelationships and plans task solutions. These agents work in coordination, with\nthe PA adjusting the experimental scheme in real time by perceiving the states\nof each agent and these generating scenarios. Experiments on the\nProgrammableWeb dataset illustrate our method generates more accurate scenarios\nmore efficiently, and innovatively provides an effective way for service\necosystem governance related experimental system construction.", "published": "2025-09-01 12:55:02", "link": "http://arxiv.org/abs/2509.01441v1", "categories": ["cs.AI", "cs.MA"], "primary_category": "cs.AI"}
{"title": "Towards Agentic OS: An LLM Agent Framework for Linux Schedulers", "abstract": "Operating system schedulers suffer from a fundamental semantic gap, where\nkernel policies fail to understand application-specific needs, leading to\nsuboptimal performance. We introduce SchedCP, the first framework that enables\nfully autonomous Large Language Model (LLM) agents to safely and efficiently\noptimize Linux schedulers without human involvement. Our core insight is that\nthe challenge is not merely to apply a better LLM, but to architect a decoupled\ncontrol plane that separates the AI's role of semantic reasoning (\"what to\noptimize\") from the system's role of execution (\"how to observe and act\").\nImplemented as Model Context Protocol(MCP) server, SchedCP provides a stable\ninterface with three key services: a Workload Analysis Engine, an evolving\nScheduler Policy Repository, and an Execution Verifier that validates all\nAI-generated code and configure before deployment with static and dynamic\nanalysis.\n  We demonstrate this architecture's power with sched-agent, a multi-agent\nsystem that autonomously analyzes workloads, synthesizes custom eBPF scheduling\npolicies, and deploys them via the sched\\_ext infrastructure. Our evaluation\nshows that SchedCP achieves up to an 1.79x performance improvement, and a 13x\ncost reduction compared to naive agentic approaches, all while maintaining high\nsuccess rate. By bridging the semantic gap, SchedCP democratizes expert-level\nsystem optimization and represents a step towards creating truly\nself-optimizing, application-aware operating systems. The code is open-sourced\nin https://github.com/eunomia-bpf/schedcp", "published": "2025-09-01 08:38:49", "link": "http://arxiv.org/abs/2509.01245v2", "categories": ["cs.AI", "cs.MA", "cs.OS"], "primary_category": "cs.AI"}
{"title": "Web Fraud Attacks Against LLM-Driven Multi-Agent Systems", "abstract": "With the proliferation of applications built upon LLM-driven multi-agent\nsystems (MAS), the security of Web links has become a critical concern in\nensuring system reliability. Once an agent is induced to visit a malicious\nwebsite, attackers can use it as a springboard to conduct diverse subsequent\nattacks, which will drastically expand the attack surface. In this paper, we\npropose Web Fraud Attacks, a novel type of attack aiming at inducing MAS to\nvisit malicious websites. We design 11 representative attack variants that\nencompass domain name tampering (homoglyph deception, character substitution,\netc.), link structure camouflage (sub-directory nesting, sub-domain grafting,\nparameter obfuscation, etc.), and other deceptive techniques tailored to\nexploit MAS's vulnerabilities in link validation. Through extensive experiments\non these crafted attack vectors, we demonstrate that Web fraud attacks not only\nexhibit significant destructive potential across different MAS architectures\nbut also possess a distinct advantage in evasion: they circumvent the need for\ncomplex input formats such as jailbreaking, which inherently carry higher\nexposure risks. These results underscore the importance of addressing Web fraud\nattacks in LLM-driven MAS, as their stealthiness and destructiveness pose\nnon-negligible threats to system security and user safety.", "published": "2025-09-01 07:47:24", "link": "http://arxiv.org/abs/2509.01211v1", "categories": ["cs.CR", "cs.AI", "cs.MA"], "primary_category": "cs.CR"}
{"title": "An Economy of AI Agents", "abstract": "In the coming decade, artificially intelligent agents with the ability to\nplan and execute complex tasks over long time horizons with little direct\noversight from humans may be deployed across the economy. This chapter surveys\nrecent developments and highlights open questions for economists around how AI\nagents might interact with humans and with each other, shape markets and\norganizations, and what institutions might be required for well-functioning\nmarkets.", "published": "2025-09-01 02:07:39", "link": "http://arxiv.org/abs/2509.01063v1", "categories": ["econ.GN", "cs.AI", "cs.MA", "q-fin.EC"], "primary_category": "econ.GN"}
{"title": "Efficient Transformer-Inspired Variants of Physics-Informed Deep Operator Networks", "abstract": "Operator learning has emerged as a promising tool for accelerating the\nsolution of partial differential equations (PDEs). The Deep Operator Networks\n(DeepONets) represent a pioneering framework in this area: the \"vanilla\"\nDeepONet is valued for its simplicity and efficiency, while the modified\nDeepONet achieves higher accuracy at the cost of increased training time. In\nthis work, we propose a series of Transformer-inspired DeepONet variants that\nintroduce bidirectional cross-conditioning between the branch and trunk\nnetworks in DeepONet. Query-point information is injected into the branch\nnetwork and input-function information into the trunk network, enabling dynamic\ndependencies while preserving the simplicity and efficiency of the \"vanilla\"\nDeepONet in a non-intrusive manner. Experiments on four PDE benchmarks --\nadvection, diffusion-reaction, Burgers', and Korteweg-de Vries equations --\nshow that for each case, there exists a variant that matches or surpasses the\naccuracy of the modified DeepONet while offering improved training efficiency.\nMoreover, the best-performing variant for each equation aligns naturally with\nthe equation's underlying characteristics, suggesting that the effectiveness of\ncross-conditioning depends on the characteristics of the equation and its\nunderlying physics. To ensure robustness, we validate the effectiveness of our\nvariants through a range of rigorous statistical analyses, among them the\nWilcoxon Two One-Sided Test, Glass's Delta, and Spearman's rank correlation.", "published": "2025-09-01 18:01:23", "link": "http://arxiv.org/abs/2509.01679v1", "categories": ["cs.LG", "cs.NA", "math.NA", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Lipschitz-Guided Design of Interpolation Schedules in Generative Models", "abstract": "We study the design of interpolation schedules in the stochastic interpolants\nframework for flow and diffusion-based generative models. We show that while\nall scalar interpolation schedules achieve identical statistical efficiency\nunder Kullback-Leibler divergence in path space after optimal diffusion\ncoefficient tuning, their numerical efficiency can differ substantially. This\nobservation motivates focusing on numerical properties of the resulting drift\nfields rather than statistical criteria for schedule design. We propose\naveraged squared Lipschitzness minimization as a principled criterion for\nnumerical optimization, providing an alternative to kinetic energy minimization\nused in optimal transport approaches. A transfer formula is derived that\nenables conversion between different schedules at inference time without\nretraining neural networks. For Gaussian distributions, our optimized schedules\nachieve exponential improvements in Lipschitz constants over standard linear\nschedules, while for Gaussian mixtures, they reduce mode collapse in few-step\nsampling. We also validate our approach on high-dimensional invariant\ndistributions from stochastic Allen-Cahn equations and Navier-Stokes equations,\ndemonstrating robust performance improvements across resolutions.", "published": "2025-09-01 17:16:34", "link": "http://arxiv.org/abs/2509.01629v1", "categories": ["stat.ML", "cs.LG", "cs.NA", "math.NA"], "primary_category": "stat.ML"}
{"title": "An efficient spline-based scheme on Shishkin-type meshes for solving singularly perturbed coupled systems with Robin boundary conditions", "abstract": "In this paper, we investigate a weakly coupled system of singularly perturbed\nlinear reaction-diffusion equations with Robin boundary conditions, where the\nleading terms are multiplied by small positive parameters that may differ in\nmagnitude. The solution to this system exhibits overlapping and interacting\nboundary layers. To appropriately resolve these layers, we employ a standard\nShishkin mesh and propose a novel modification of Bakhvalov-Shishkin mesh. A\ncubic spline approximation is applied at the boundary conditions, while a\ncentral difference scheme is used at the interior points. The problem is then\nsolved on both meshes. It is demonstrated that the proposed scheme achieves\nalmost second-order convergence upto a logarithmic factor on the Shishkin mesh\nand exact second-order convergence on the modified Bakhvalov-Shishkin mesh. We\npresent numerical results to validate the accuracy of our findings.", "published": "2025-09-01 16:02:59", "link": "http://arxiv.org/abs/2509.01575v1", "categories": ["math.NA", "cs.NA", "65H10, 65L10, 65L11"], "primary_category": "math.NA"}
{"title": "User Manual for Model-based Imaging Inverse Problem", "abstract": "This user manual is intended to provide a detailed description on model-based\noptimization for imaging inverse problem. Theseproblems can be particularly\ncomplex and challenging, especially for individuals without prior exposure to\nconvex optimization orinverse problem theory, like myself. In light of this, I\nam writing this manual to clarify and systematically organize the\nmathematicalrationale underlying imaging inverse problems. This manual might\nnot be accurate in mathmatical notion but more focus on the logicalthinking on\nhow to solve and proceed to solve the problems. If you want to think deep about\nsomething, try to raise questions! Thismanual is seaprated into four sections,\naiming to answer the following four questions: (1) What is inverse imaging\nproblem? (2) Why optimization is used to solve the inverse imaging problem? (3)\nHow to solve the optimization problem? (4) How to implement the optimization\nalgorithm in real imaging system?", "published": "2025-09-01 15:57:20", "link": "http://arxiv.org/abs/2509.01572v1", "categories": ["math.NA", "cs.CV", "cs.NA"], "primary_category": "math.NA"}
{"title": "Global convergence of adaptive least-squares finite element methods for nonlinear PDEs", "abstract": "The Zarantonello fixed-point iteration is an established linearization scheme\nfor quasilinear PDEs with strongly monotone and Lipschitz continuous\nnonlinearity. This paper presents a weighted least-squares minimization for the\ncomputation of the update of this scheme. The resulting formulation allows for\na conforming finite element discretization of the primal and dual variable of\nthe PDE with arbitrary polynomial degree. The least-squares functional provides\na built-in a posteriori discretization error estimator in each linearization\nstep motivating an adaptive Uzawa-type algorithm with an outer linearization\nloop and an inner adaptive mesh-refinement loop. We prove R-linear convergence\nof the linearization iterates for arbitrary initial guesses. Particular focus\nis on the role of the weights in the least-squares functional of the linearized\nproblem and their influence on the robustness of the Zarantonello damping\nparameter. Numerical experiments illustrate the performance of the proposed\nalgorithm.", "published": "2025-09-01 15:07:31", "link": "http://arxiv.org/abs/2509.01531v1", "categories": ["math.NA", "cs.NA", "65N30, 65N50, 65N15"], "primary_category": "math.NA"}
{"title": "A geometrically robust unfitted boundary algebraic equation method based on discrete potentials and local basis functions", "abstract": "We present an unfitted boundary algebraic equation (BAE) method for solving\nelliptic partial differential equations in complex geometries. The method\nemploys lattice Green's functions on infinite regular grids combined with\ndiscrete potential theory to construct single and double layer potentials,\nwhich is a discrete analog to boundary integral method. Local basis functions\non cut cells accommodate arbitrary boundary conditions and seamlessly integrate\nwith the boundary algebraic equations. The difference potentials framework\nenables efficient treatment of nonhomogeneous terms and fast computation of\nlayer potentials via FFT-based solvers. We establish theoretical stability and\nconvergence through a novel interpolation operator framework. Key advantages of\nthe developed method include: dimension reduction, geometric flexibility,\nmesh-independent conditioning, small-cut stability, and uniform treatment of\nsmooth and non-smooth geometries. Numerical experiments validate accuracy and\nrobustness across ellipses and diamonds with varying aspect ratios and sharp\ncorners, and an application of potential flows in unbounded domains.", "published": "2025-09-01 11:27:50", "link": "http://arxiv.org/abs/2509.01380v1", "categories": ["math.NA", "cs.NA", "physics.comp-ph"], "primary_category": "math.NA"}
{"title": "Curvature-Based Optimal Polynomial Geometric Interpolation of Circular Arcs", "abstract": "The problem of the optimal approximation of circular arcs by parametric\npolynomial curves is considered. The optimality relates to the curvature error.\nParametric polynomial curves of low degree are used and a geometric continuity\nis prescribed at the boundary points of the circular arc. Analysis is done for\ncases of parabolic $G^0$, cubic $G^1$ and quartic $G^2$ interpolation. The\ncomparison of the approximation of circular arcs based on curvature with the\napproximation based on radial error is provided.", "published": "2025-09-01 10:48:47", "link": "http://arxiv.org/abs/2509.01353v1", "categories": ["math.NA", "cs.NA", "65D05, 65D07, 65D17"], "primary_category": "math.NA"}
{"title": "Quasi-optimal error estimates for the approximation of stable stationary states of the elastic energy of inextensible curves", "abstract": "We establish local existence and a quasi-optimal error estimate for piecewise\ncubic minimizers to the bending energy under a discretized inextensibility\nconstraint. In previous research a discretization is used where the\ninextensibility constraint is only enforced at the nodes of the discretization.\nWe show why this discretization leads to suboptimal convergence rates and we\nimprove on it by also enforcing the constraint in the midpoints of each\nsubinterval. We then use the inverse function theorem to prove existence and an\nerror estimate for stationary states of the bending energy that yields\nquasi-optimal convergence. We use numerical simulations to verify the\ntheoretical results experimentally.", "published": "2025-09-01 09:16:56", "link": "http://arxiv.org/abs/2509.01287v1", "categories": ["math.NA", "cs.NA", "74B20, 65M15, 35K55"], "primary_category": "math.NA"}
{"title": "Linear, decoupled, positivity preserving, positive-definiteness preserving and energy stable schemes for the diffusive Oldroyd-B coupled with PNP model", "abstract": "In this paper, we present a first-order finite element scheme for the\nviscoelastic electrohydrodynamic model. The model incorporates the\nPoisson-Nernst-Planck equations to describe the transport of ions and the\nOldroyd-B constitutive model to capture the behavior of viscoelastic fluids. To\npreserve the positive-definiteness of the conformation tensor and the\npositivity of ion concentrations, we employ both logarithmic transformations.\nThe decoupled scheme is achieved by introducing a nonlocal auxiliary variable\nand using the splitting technique. The proposed schemes are rigorously proven\nto be mass conservative and energy stable at the fully discrete level. To\nvalidate the theoretical analysis, we present numerical examples that\ndemonstrate the convergence rates and the robust performance of the schemes.\nThe results confirm that the proposed methods accurately handle the high\nWeissenberg number problem (HWNP) at moderately high Weissenberg numbers.\nFinally, the flow structure influenced by the elastic effect within the\nelectro-convection phenomena has been studied.", "published": "2025-09-01 09:04:14", "link": "http://arxiv.org/abs/2509.01278v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Linear, decoupled, second-order and structure-preserving scheme for Carreau fluid equations coupled with steric Poisson-Nernst-Planck model", "abstract": "In this paper, to study ionic steric effects, we present a linear, decoupled,\nsecond-order accurate in time and structure-preserving scheme with finite\nelement approximations for Carreau fluid equations coupled with steric\nPoisson-Nernst-Planck (SPNP) model. The logarithmic transformation for the ion\nconcentration is used to preserve positivity property. To deal with the\nnonlinear coupling terms in fluid equation, a nonlocal auxiliary variable with\nrespect to the free energy of SPNP equations and its associated ordinary\ndifferential equation are introduced. The obtained system is equivalent to the\noriginal system. The fully discrete scheme is proved to be mass conservative,\npositivity-preserving for ion concentration and energy dissipative at discrete\nlevel. Some numerical simulations are provided to demonstrate its stability and\naccuracy. Moreover, the ionic steric effects are numerically investigated.", "published": "2025-09-01 08:56:11", "link": "http://arxiv.org/abs/2509.01270v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "A Structure-Preserving Numerical Method for Harmonic Maps Between High-genus Surfaces", "abstract": "Motivated by geometry processing for surfaces with non-trivial topology, we\nstudy discrete harmonic maps between closed surfaces of genus at least two.\nHarmonic maps provide a natural framework for comparing surfaces by minimizing\ndistortion. Unlike conformal or isometric maps-which may not exist between\nsurfaces with different geometries-harmonic maps always exist within a fixed\nhomotopy class and yield optimal homeomorphisms when the target surface has\nnegative curvature. We develop a structure-preserving algorithm to compute\nharmonic maps from a triangulated surface to a reference hyperbolic surface.\nThe method minimizes Dirichlet energy over geodesic realizations of the surface\ngraph into the target hyperbolic surface in the homotopy class of a\nhomeomorphism. A central feature of our framework is the use of canonical edge\nweights derived from the hyperbolic metric, which generalize the classical\ncotangent weights from the Euclidean setting. These weights preserve\ninjectivity and ensure that isometries remain harmonic in the discrete theory,\nreflecting their classical behavior.", "published": "2025-09-01 08:45:57", "link": "http://arxiv.org/abs/2509.01256v1", "categories": ["math.NA", "cs.NA", "math.OC"], "primary_category": "math.NA"}
{"title": "A Class of Random-Kernel Network Models", "abstract": "We introduce random-kernel networks, a multilayer extension of random feature\nmodels where depth is created by deterministic kernel composition and\nrandomness enters only in the outermost layer. We prove that deeper\nconstructions can approximate certain functions with fewer Monte Carlo samples\nthan any shallow counterpart, establishing a depth separation theorem in sample\ncomplexity.", "published": "2025-09-01 03:26:18", "link": "http://arxiv.org/abs/2509.01090v1", "categories": ["cs.LG", "cs.NA", "math.FA", "math.NA", "Primary 68T07. Secondary 41A25, 41A30, 46E22"], "primary_category": "cs.LG"}
{"title": "A concurrent global-local numerical method for multiscale parabolic equations", "abstract": "This paper presents a concurrent global-local numerical method for solving\nmultiscale parabolic equations in divergence form. The proposed method employs\nhybrid coefficient to provide accurate macroscopic information while preserving\nessential microscopic details within specified local defects. Both the\nmacroscopic and microscopic errors have been improved compared to existing\nresults, eliminating the factor of $\\Delta t^{-1/2}$ when the diffusion\ncoefficient is time-independent. Numerical experiments demonstrate that the\nproposed method effectively captures both global and local solution behaviors.", "published": "2025-09-01 01:55:27", "link": "http://arxiv.org/abs/2509.01059v1", "categories": ["math.NA", "cs.NA", "35B27, 35K10, 65M12, 65M60"], "primary_category": "math.NA"}
{"title": "Controllable Generation of Implied Volatility Surfaces with Variational Autoencoders", "abstract": "This paper presents a deep generative modeling framework for controllably\nsynthesizing implied volatility surfaces (IVSs) using a variational autoencoder\n(VAE). Unlike conventional data-driven models, our approach provides explicit\ncontrol over meaningful shape features (e.g., volatility level, slope,\ncurvature, term-structure) to generate IVSs with desired characteristics. In\nour framework, financially interpretable shape features are disentangled from\nresidual latent factors. The target features are embedded into the VAE\narchitecture as controllable latent variables, while the residual latent\nvariables capture additional structure to preserve IVS shape diversity. To\nenable this control, IVS feature values are quantified via regression at an\nanchor point and incorporated into the decoder to steer generation. Numerical\nexperiments demonstrate that the generative model enables rapid generation of\nrealistic IVSs with desired features rather than arbitrary patterns, and\nachieves high accuracy across both single- and multi-feature control settings.\nFor market validity, an optional post-generation latent-space repair algorithm\nadjusts only the residual latent variables to remove occasional violations of\nstatic no-arbitrage conditions without altering the specified features.\nCompared with black-box generators, the framework combines interpretability,\ncontrollability, and flexibility for synthetic IVS generation and scenario\ndesign.", "published": "2025-09-01 19:51:42", "link": "http://arxiv.org/abs/2509.01743v1", "categories": ["q-fin.CP"], "primary_category": "q-fin.CP"}
{"title": "Is All the Information in the Price? LLM Embeddings versus the EMH in Stock Clustering", "abstract": "This paper investigates whether artificial intelligence can enhance stock\nclustering compared to traditional methods. We consider this in the context of\nthe semi-strong Efficient Markets Hypothesis (EMH), which posits that prices\nfully reflect all public information and, accordingly, that clusters based on\nprice information cannot be improved upon. We benchmark three clustering\napproaches: (i) price-based clusters derived from historical return\ncorrelations, (ii) human-informed clusters defined by the Global Industry\nClassification Standard (GICS), and (iii) AI-driven clusters constructed from\nlarge language model (LLM) embeddings of stock-related news headlines. At each\ndate, each method provides a classification in which each stock is assigned to\na cluster. To evaluate a clustering, we transform it into a synthetic factor\nmodel following the Arbitrage Pricing Theory (APT) framework. This enables\nconsistent evaluation of predictive performance in a roll forward,\nout-of-sample test. Using S&P 500 constituents from from 2022 through 2024, we\nfind that price-based clustering consistently outperforms both rule-based and\nAI-based methods, reducing root mean squared error (RMSE) by 15.9% relative to\nGICS and 14.7% relative to LLM embeddings. Our contributions are threefold: (i)\na generalizable methodology that converts any equity grouping: manual, machine,\nor market-driven, into a real-time factor model for evaluation; (ii) the first\ndirect comparison of price-based, human rule-based, and AI-based clustering\nunder identical conditions; and (iii) empirical evidence reinforcing that\nshort-horizon return information is largely contained in prices. These results\nsupport the EMH while offering practitioners a practical diagnostic for\nmonitoring evolving sector structures and provide academics a framework for\ntesting alternative hypotheses about how quickly markets absorb information.", "published": "2025-09-01 16:25:09", "link": "http://arxiv.org/abs/2509.01590v1", "categories": ["q-fin.CP"], "primary_category": "q-fin.CP"}
{"title": "A Calculus of Variations Approach to Stochastic Control", "abstract": "We use classical tools from calculus of variations to formally derive\nnecessary conditions for a Markov control to be optimal in a standard finite\ntime horizon stochastic control problem. As an example, we solve the well-known\nMerton portfolio optimization problem.", "published": "2025-09-01 19:52:32", "link": "http://arxiv.org/abs/2509.01744v1", "categories": ["math.OC", "q-fin.MF"], "primary_category": "math.OC"}
{"title": "The Impact of Sequential versus Parallel Clearing Mechanisms in Agent-Based Simulations of Artificial Limit Order Book Exchanges", "abstract": "This study examines the impact of different computing implementations of\nclearing mechanisms on multi-asset price dynamics within an artificial stock\nmarket framework. We show that sequential processing of order books introduces\na systematic and significant bias by affecting the allocation of traders'\ncapital within a single time step. This occurs because applying budget\nconstraints sequentially grants assets processed earlier preferential access to\nfunds, distorting individual asset demand and consequently their price\ntrajectories. The findings highlight that while the overall price level is\nprimarily driven by macro factors like the money-to-stock ratio, the market's\nmicrostructural clearing mechanism plays a critical role in the allocation of\nvalue among individual assets. This underscores the necessity for careful\nconsideration and validation of clearing mechanisms in artificial markets to\naccurately model complex financial behaviors.", "published": "2025-09-01 18:03:55", "link": "http://arxiv.org/abs/2509.01683v1", "categories": ["q-fin.TR", "cs.DC"], "primary_category": "q-fin.TR"}
{"title": "Wrong Model, Right Uncertainty: Spatial Associations for Discrete Data with Misspecification", "abstract": "Scientists are often interested in estimating an association between a\ncovariate and a binary- or count-valued response. For instance, public health\nofficials are interested in how much disease presence (a binary response per\nindividual) varies as temperature or pollution (covariates) increases. Many\nexisting methods can be used to estimate associations, and corresponding\nuncertainty intervals, but make unrealistic assumptions in the spatial domain.\nFor instance, they incorrectly assume models are well-specified. Or they assume\nthe training and target locations are i.i.d. -- whereas in practice, these\nlocations are often not even randomly sampled. Some recent work avoids these\nassumptions but works only for continuous responses with spatially constant\nnoise. In the present work, we provide the first confidence intervals with\nguaranteed asymptotic nominal coverage for spatial associations given discrete\nresponses, even under simultaneous model misspecification and nonrandom\nsampling of spatial locations. To do so, we demonstrate how to handle spatially\nvarying noise, provide a novel proof of consistency for our proposed estimator,\nand use a delta method argument with a Lyapunov central limit theorem. We show\nempirically that standard approaches can produce unreliable confidence\nintervals and can even get the sign of an association wrong, while our method\nreliably provides correct coverage.", "published": "2025-09-01 21:22:08", "link": "http://arxiv.org/abs/2509.01776v1", "categories": ["stat.ME", "cs.LG", "stat.ML"], "primary_category": "stat.ME"}
{"title": "Preconditioned Regularized Wasserstein Proximal Sampling", "abstract": "We consider sampling from a Gibbs distribution by evolving finitely many\nparticles. We propose a preconditioned version of a recently proposed\nnoise-free sampling method, governed by approximating the score function with\nthe numerically tractable score of a regularized Wasserstein proximal operator.\nThis is derived by a Cole--Hopf transformation on coupled anisotropic heat\nequations, yielding a kernel formulation for the preconditioned regularized\nWasserstein proximal. The diffusion component of the proposed method is also\ninterpreted as a modified self-attention block, as in transformer\narchitectures. For quadratic potentials, we provide a discrete-time\nnon-asymptotic convergence analysis and explicitly characterize the bias, which\nis dependent on regularization and independent of step-size. Experiments\ndemonstrate acceleration and particle-level stability on various log-concave\nand non-log-concave toy examples to Bayesian total-variation regularized image\ndeconvolution, and competitive/better performance on non-convex Bayesian neural\nnetwork training when utilizing variable preconditioning matrices.", "published": "2025-09-01 18:04:31", "link": "http://arxiv.org/abs/2509.01685v1", "categories": ["stat.ML", "cs.LG", "math.OC", "stat.CO", "65C05, 62G07"], "primary_category": "stat.ML"}
{"title": "Effects of Distributional Biases on Gradient-Based Causal Discovery in the Bivariate Categorical Case", "abstract": "Gradient-based causal discovery shows great potential for deducing causal\nstructure from data in an efficient and scalable way. Those approaches however\ncan be susceptible to distributional biases in the data they are trained on. We\nidentify two such biases: Marginal Distribution Asymmetry, where differences in\nentropy skew causal learning toward certain factorizations, and Marginal\nDistribution Shift Asymmetry, where repeated interventions cause faster shifts\nin some variables than in others. For the bivariate categorical setup with\nDirichlet priors, we illustrate how these biases can occur even in controlled\nsynthetic data. To examine their impact on gradient-based methods, we employ\ntwo simple models that derive causal factorizations by learning marginal or\nconditional data distributions - a common strategy in gradient-based causal\ndiscovery. We demonstrate how these models can be susceptible to both biases.\nWe additionally show how the biases can be controlled. An empirical evaluation\nof two related, existing approaches indicates that eliminating competition\nbetween possible causal factorizations can make models robust to the presented\nbiases.", "published": "2025-09-01 17:08:03", "link": "http://arxiv.org/abs/2509.01621v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Sampling as Bandits: Evaluation-Efficient Design for Black-Box Densities", "abstract": "We introduce bandit importance sampling (BIS), a new class of importance\nsampling methods designed for settings where the target density is expensive to\nevaluate. In contrast to adaptive importance sampling, which optimises a\nproposal distribution, BIS directly designs the samples through a sequential\nstrategy that combines space-filling designs with multi-armed bandits. Our\nmethod leverages Gaussian process surrogates to guide sample selection,\nenabling efficient exploration of the parameter space with minimal target\nevaluations. We establish theoretical guarantees on convergence and demonstrate\nthe effectiveness of the method across a broad range of sampling tasks. BIS\ndelivers accurate approximations with fewer target evaluations, outperforming\ncompeting approaches across multimodal, heavy-tailed distributions, and\nreal-world applications to Bayesian inference of computationally expensive\nmodels.", "published": "2025-09-01 12:47:32", "link": "http://arxiv.org/abs/2509.01437v1", "categories": ["stat.ME", "cs.LG", "stat.CO", "stat.ML"], "primary_category": "stat.ME"}
{"title": "Asynchronous and Stochastic Distributed Resource Allocation", "abstract": "This work proposes and studies the distributed resource allocation problem in\nasynchronous and stochastic settings. We consider a distributed system with\nmultiple workers and a coordinating server with heterogeneous computation and\ncommunication times. We explore an approximate stochastic primal-dual approach\nwith the aim of 1) adhering to the resource budget constraints, 2) allowing for\nthe asynchronicity between the workers and the server, and 3) relying on the\nlocally available stochastic gradients. We analyze our Asynchronous stochastic\nPrimal-Dual (Asyn-PD) algorithm and prove its convergence in the second moment\nto the saddle point solution of the approximate problem at the rate of\n$O(1/t)$, where $t$ is the iteration number. Furthermore, we verify our\nalgorithm numerically to validate the analytically derived convergence results,\nand demonstrate the advantages of utilizing our asynchronous algorithm rather\nthan deploying a synchronous algorithm where the server must wait until it gets\nupdate from all workers.", "published": "2025-09-01 06:47:23", "link": "http://arxiv.org/abs/2509.01172v1", "categories": ["math.OC", "stat.ML"], "primary_category": "math.OC"}
{"title": "ADMP-GNN: Adaptive Depth Message Passing GNN", "abstract": "Graph Neural Networks (GNNs) have proven to be highly effective in various\ngraph learning tasks. A key characteristic of GNNs is their use of a fixed\nnumber of message-passing steps for all nodes in the graph, regardless of each\nnode's diverse computational needs and characteristics. Through empirical\nreal-world data analysis, we demonstrate that the optimal number of\nmessage-passing layers varies for nodes with different characteristics. This\nfinding is further supported by experiments conducted on synthetic datasets. To\naddress this, we propose Adaptive Depth Message Passing GNN (ADMP-GNN), a novel\nframework that dynamically adjusts the number of message passing layers for\neach node, resulting in improved performance. This approach applies to any\nmodel that follows the message passing scheme. We evaluate ADMP-GNN on the node\nclassification task and observe performance improvements over baseline GNN\nmodels.", "published": "2025-09-01 06:42:19", "link": "http://arxiv.org/abs/2509.01170v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Nonlinear Performative Prediction", "abstract": "Performative prediction is an emerging paradigm in machine learning that\naddresses scenarios where the model's prediction may induce a shift in the\ndistribution of the data it aims to predict. Current works in this field often\nrely on uncontrollable assumptions, such as bounded gradients of performative\nloss, and primarily focus on linear cases in their examples and evaluations to\nmaintain consistency between theoretical guarantees and empirical validations.\nHowever, such linearity rarely holds in real-world applications, where the data\nusually exhibit complex nonlinear characteristics. In this paper, we relax\nthese out-of-control assumptions and present a novel design that generalizes\nperformative prediction to nonlinear cases while preserving essential\ntheoretical properties. Specifically, we formulate the loss function of\nperformative prediction using a maximum margin approach and extend it to\nnonlinear spaces through kernel methods. To quantify the data distribution\nshift, we employ the discrepancy between prediction errors on these two\ndistributions as an indicator, which characterizes the impact of the\nperformative effect on specific learning tasks. By doing so, we can derive, for\nboth linear and nonlinear cases, the conditions for performative stability, a\ncritical and desirable property in performative contexts. Building on these\ntheoretical insights, we develop an algorithm that guarantees the performative\nstability of the predictive model. We validate the effectiveness of our method\nthrough experiments on synthetic and real-world datasets with both linear and\nnonlinear data distributions, demonstrating superior performance compared to\nstate-of-the-art baselines.", "published": "2025-09-01 05:17:52", "link": "http://arxiv.org/abs/2509.01139v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "CCE: Confidence-Consistency Evaluation for Time Series Anomaly Detection", "abstract": "Time Series Anomaly Detection metrics serve as crucial tools for model\nevaluation. However, existing metrics suffer from several limitations:\ninsufficient discriminative power, strong hyperparameter dependency,\nsensitivity to perturbations, and high computational overhead. This paper\nintroduces Confidence-Consistency Evaluation (CCE), a novel evaluation metric\nthat simultaneously measures prediction confidence and uncertainty consistency.\nBy employing Bayesian estimation to quantify the uncertainty of anomaly scores,\nwe construct both global and event-level confidence and consistency scores for\nmodel predictions, resulting in a concise CCE metric. Theoretically and\nexperimentally, we demonstrate that CCE possesses strict boundedness, Lipschitz\nrobustness against score perturbations, and linear time complexity\n$\\mathcal{O}(n)$. Furthermore, we establish RankEval, a benchmark for comparing\nthe ranking capabilities of various metrics. RankEval represents the first\nstandardized and reproducible evaluation pipeline that enables objective\ncomparison of evaluation metrics. Both CCE and RankEval implementations are\nfully open-source.", "published": "2025-09-01 03:38:38", "link": "http://arxiv.org/abs/2509.01098v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Gaussian process surrogate with physical law-corrected prior for multi-coupled PDEs defined on irregular geometry", "abstract": "Parametric partial differential equations (PDEs) are fundamental mathematical\ntools for modeling complex physical systems, yet their numerical evaluation\nacross parameter spaces remains computationally intensive when using\nconventional high-fidelity solvers. To address this challenge, we propose a\nnovel physical law-corrected prior Gaussian process (LC-prior GP) surrogate\nmodeling framework that effectively integrates data-driven learning with\nunderlying physical constraints to flexibly handle multi-coupled variables\ndefined on complex geometries. The proposed approach leverages proper\northogonal decomposition (POD) to parameterize high-dimensional PDE solutions\nvia their dominant modes and associated coefficients, thereby enabling\nefficient Gaussian process (GP) surrogate modeling within a reduced-dimensional\ncoefficient space. A key contribution lies in the incorporation of physical\nlaws together with a limited number of parameter samples to correct the GP\nposterior mean, thus avoiding reliance on computationally expensive numerical\nsolvers. Furthermore, interpolation functions are constructed to describe the\nmapping from the full parameter space to the physics-based correction term.\nThis mapping is subsequently backpropagated to constrain the original GP\nsurrogate, yielding a more physically consistent conditional prior. To handle\nirregular geometries, the radial basis function-finite difference (RBF-FD)\nmethod is incorporated during training set computation, with its inherent\ndifferentiation matrices providing both computational efficiency and numerical\naccuracy for physical constraint optimization. The effectiveness of the\nproposed method is demonstrated through numerical experiments involving a\nreaction-diffusion model, miscible flooding models, and Navier-Stokes equations\nwith multi-physics coupling defined on irregular domains.", "published": "2025-09-01 02:40:32", "link": "http://arxiv.org/abs/2509.02617v1", "categories": ["stat.ML", "cs.LG", "stat.CO"], "primary_category": "stat.ML"}
{"title": "AHAMask: Reliable Task Specification for Large Audio Language Models without Instructions", "abstract": "Although current large audio language models (LALMs) extend text large\nlanguage models (LLMs) with generic acoustic understanding abilities, they\nusually suffer from instruction sensitivity, where different instructions of\nthe same intention can yield drastically different outcomes. In this work, we\npropose AHAMask, where we simply mask some of the attention heads in the\ndecoder-only LLM backbone of LALMs, to trigger specific acoustic task\nfunctionalities without instructions. These masks are efficiently obtained by\ntraining on an LALM, with the number of trainable parameters equal to the\nattention head count in its LLM backbone. We show by experiments that applying\nsuch selective attention head masks achieves comparable or even better\nperformance than using instructions, either on single or composite tasks.\nBesides achieving reliable acoustic task specification for LALMs, this also\nreveals that LALMs exhibit certain \"functional pathways\" in their attention\nheads.", "published": "2025-09-01 21:33:22", "link": "http://arxiv.org/abs/2509.01787v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "From Discord to Harmony: Decomposed Consonance-based Training for Improved Audio Chord Estimation", "abstract": "Audio Chord Estimation (ACE) holds a pivotal role in music information\nresearch, having garnered attention for over two decades due to its relevance\nfor music transcription and analysis. Despite notable advancements, challenges\npersist in the task, particularly concerning unique characteristics of harmonic\ncontent, which have resulted in existing systems' performances reaching a glass\nceiling. These challenges include annotator subjectivity, where varying\ninterpretations among annotators lead to inconsistencies, and class imbalance\nwithin chord datasets, where certain chord classes are over-represented\ncompared to others, posing difficulties in model training and evaluation. As a\nfirst contribution, this paper presents an evaluation of inter-annotator\nagreement in chord annotations, using metrics that extend beyond traditional\nbinary measures. In addition, we propose a consonance-informed distance metric\nthat reflects the perceptual similarity between harmonic annotations. Our\nanalysis suggests that consonance-based distance metrics more effectively\ncapture musically meaningful agreement between annotations. Expanding on these\nfindings, we introduce a novel ACE conformer-based model that integrates\nconsonance concepts into the model through consonance-based label smoothing.\nThe proposed model also addresses class imbalance by separately estimating\nroot, bass, and all note activations, enabling the reconstruction of chord\nlabels from decomposed outputs.", "published": "2025-09-01 16:20:47", "link": "http://arxiv.org/abs/2509.01588v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Characterization of Speech Similarity Between Australian Aboriginal and High-Resource Languages: A Case Study on Dharawal", "abstract": "Australian Aboriginal languages are of significant cultural and linguistic\nvalue but remain severely underrepresented in modern speech AI systems. While\nstate-of-the-art speech foundation models and automatic speech recognition\nexcel in high-resource settings, they often struggle to generalize to\nlow-resource languages, especially those lacking clean, annotated speech data.\nIn this work, we collect and clean a speech dataset for Dharawal, a\nlow-resource Australian Aboriginal language, by carefully sourcing and\nprocessing publicly available recordings. Using this dataset, we analyze the\nspeech similarity between Dharawal and 107 high-resource languages using a\npre-trained multilingual speech encoder. Our approach combines (1)\nmisclassification rate analysis to assess language confusability, and (2)\nfine-grained similarity measurements using cosine similarity and Fr\\'echet\nInception Distance (FID) in the embedding space. Experimental results reveal\nthat Dharawal shares strong speech similarity with languages such as Latin,\nM\\=aori, Korean, Thai, and Welsh. These findings offer practical guidance for\nfuture transfer learning and model adaptation efforts, and underscore the\nimportance of data collection and embedding-based analysis in supporting speech\ntechnologies for endangered language communities.", "published": "2025-09-01 12:21:46", "link": "http://arxiv.org/abs/2509.01419v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "ArabEmoNet: A Lightweight Hybrid 2D CNN-BiLSTM Model with Attention for Robust Arabic Speech Emotion Recognition", "abstract": "Speech emotion recognition is vital for human-computer interaction,\nparticularly for low-resource languages like Arabic, which face challenges due\nto limited data and research. We introduce ArabEmoNet, a lightweight\narchitecture designed to overcome these limitations and deliver\nstate-of-the-art performance. Unlike previous systems relying on discrete MFCC\nfeatures and 1D convolutions, which miss nuanced spectro-temporal patterns,\nArabEmoNet uses Mel spectrograms processed through 2D convolutions, preserving\ncritical emotional cues often lost in traditional methods.\n  While recent models favor large-scale architectures with millions of\nparameters, ArabEmoNet achieves superior results with just 1 million\nparameters, 90 times smaller than HuBERT base and 74 times smaller than\nWhisper. This efficiency makes it ideal for resource-constrained environments.\nArabEmoNet advances Arabic speech emotion recognition, offering exceptional\nperformance and accessibility for real-world applications.", "published": "2025-09-01 11:51:38", "link": "http://arxiv.org/abs/2509.01401v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "CabinSep: IR-Augmented Mask-Based MVDR for Real-Time In-Car Speech Separation with Distributed Heterogeneous Arrays", "abstract": "Separating overlapping speech from multiple speakers is crucial for effective\nhuman-vehicle interaction. This paper proposes CabinSep, a lightweight neural\nmask-based minimum variance distortionless response (MVDR) speech separation\napproach, to reduce speech recognition errors in back-end automatic speech\nrecognition (ASR) models. Our contributions are threefold: First, we utilize\nchannel information to extract spatial features, which improves the estimation\nof speech and noise masks. Second, we employ MVDR during inference, reducing\nspeech distortion to make it more ASR-friendly. Third, we introduce a data\naugmentation method combining simulated and real-recorded impulse responses\n(IRs), improving speaker localization at zone boundaries and further reducing\nspeech recognition errors. With a computational complexity of only 0.4 GMACs,\nCabinSep achieves a 17.5% relative reduction in speech recognition error rate\nin a real-recorded dataset compared to the state-of-the-art DualSep model.\nDemos are available at: https://cabinsep.github.io/cabinsep/.", "published": "2025-09-01 11:50:43", "link": "http://arxiv.org/abs/2509.01399v1", "categories": ["cs.SD", "cs.AI", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MixedG2P-T5: G2P-free Speech Synthesis for Mixed-script texts using Speech Self-Supervised Learning and Language Model", "abstract": "This study presents a novel approach to voice synthesis that can substitute\nthe traditional grapheme-to-phoneme (G2P) conversion by using a deep\nlearning-based model that generates discrete tokens directly from speech.\nUtilizing a pre-trained voice SSL model, we train a T5 encoder to produce\npseudo-language labels from mixed-script texts (e.g., containing Kanji and\nKana). This method eliminates the need for manual phonetic transcription,\nreducing costs and enhancing scalability, especially for large non-transcribed\naudio datasets. Our model matches the performance of conventional G2P-based\ntext-to-speech systems and is capable of synthesizing speech that retains\nnatural linguistic and paralinguistic features, such as accents and\nintonations.", "published": "2025-09-01 11:36:37", "link": "http://arxiv.org/abs/2509.01391v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Analysing the Language of Neural Audio Codecs", "abstract": "This study presents a comparative analysis of the statistical and linguistic\nproperties of neural audio codecs (NACs). We investigate discrete speech tokens\nproduced by various NAC models, examining their adherence to linguistic\nstatistical laws such as Zipf's law and Heaps' law, as well as their entropy\nand redundancy. To assess how these token-level properties relate to semantic\nand acoustic preservation in synthesized speech, we evaluate intelligibility\nusing error rates of automatic speech recognition, and quality using the UTMOS\nscore. Our results reveal that NAC tokens, particularly 3-grams, exhibit\nlanguage-like statistical patterns. Moreover, these properties, together with\nmeasures of information content, are found to correlate with improved\nperformances in speech recognition and resynthesis tasks. These findings offer\ninsights into the structure of NAC token sequences and inform the design of\nmore effective generative speech models.", "published": "2025-09-01 11:36:33", "link": "http://arxiv.org/abs/2509.01390v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "The AudioMOS Challenge 2025", "abstract": "This is the summary paper for the AudioMOS Challenge 2025, the very first\nchallenge for automatic subjective quality prediction for synthetic audio. The\nchallenge consists of three tracks. The first track aims to assess\ntext-to-music samples in terms of overall quality and textual alignment. The\nsecond track is based on the four evaluation dimensions of Meta Audiobox\nAesthetics, and the test set consists of text-to-speech, text-to-audio, and\ntext-to-music samples. The third track focuses on synthetic speech quality\nassessment in different sampling rates. The challenge attracted 24 unique teams\nfrom both academia and industry, and improvements over the baselines were\nconfirmed. The outcome of this challenge is expected to facilitate development\nand progress in the field of automatic evaluation for audio generation systems.", "published": "2025-09-01 10:18:14", "link": "http://arxiv.org/abs/2509.01336v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "IS${}^3$ : Generic Impulsive--Stationary Sound Separation in Acoustic Scenes using Deep Filtering", "abstract": "We are interested in audio systems capable of performing a differentiated\nprocessing of stationary backgrounds and isolated acoustic events within an\nacoustic scene, whether for applying specific processing methods to each part\nor for focusing solely on one while ignoring the other. Such systems have\napplications in real-world scenarios, including robust adaptive audio rendering\nsystems (e.g., EQ or compression), plosive attenuation in voice mixing, noise\nsuppression or reduction, robust acoustic event classification or even\nbioacoustics. To this end, we introduce IS${}^3$, a neural network designed for\nImpulsive--Stationary Sound Separation, that isolates impulsive acoustic events\nfrom the stationary background using a deep filtering approach, that can act as\na pre-processing stage for the above-mentioned tasks. To ensure optimal\ntraining, we propose a sophisticated data generation pipeline that curates and\nadapts existing datasets for this task. We demonstrate that a learning-based\napproach, build on a relatively lightweight neural architecture and trained\nwith well-designed and varied data, is successful in this previously\nunaddressed task, outperforming the Harmonic--Percussive Sound Separation\nmasking method, adapted from music signal processing research, and wavelet\nfiltering on objective separation metrics.", "published": "2025-09-01 08:55:29", "link": "http://arxiv.org/abs/2509.02622v1", "categories": ["eess.AS", "cs.AI", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "High-Density MIMO Localization Using a 32x64 Ultrasonic Transducer-Microphone Array with Real-Time Data Streaming", "abstract": "In this work, we present a novel ultrasonic array system designed for\nhigh-precision localization using a large-scale MIMO (Multiple-Input\nMultiple-Output) architecture. The system combines 32 transmitters with 62\nmicrophones, creating an extended virtual aperture that improves channel\nseparability and spatial resolution. Each transmitter is excited by a\nrandom-phase multisine within the ultrasonic band, which reduces inter-channel\ncorrelation and increases robustness against multipath. The feasibility of the\napproach is demonstrated through simulations of reflector imaging and analysis\nof channel separation under realistic transducer bandwidth constraints. Results\nshow that MIMO processing enables improved separation of reflectors compared to\nsingle-emitter configurations, although practical limitations such as\ntransducer bandwidth reduce the achievable channel isolation.", "published": "2025-09-01 07:47:22", "link": "http://arxiv.org/abs/2509.01210v1", "categories": ["eess.SP", "eess.AS"], "primary_category": "eess.SP"}
{"title": "SimulMEGA: MoE Routers are Advanced Policy Makers for Simultaneous Speech Translation", "abstract": "Simultaneous Speech Translation (SimulST) enables real-time cross-lingual\ncommunication by jointly optimizing speech recognition and machine translation\nunder strict latency constraints. Existing systems struggle to balance\ntranslation quality, latency, and semantic coherence, particularly in\nmultilingual many-to-many scenarios where divergent read and write policies\nhinder unified strategy learning. In this paper, we present SimulMEGA\n(Simultaneous Generation by Mixture-of-Experts Gating), an unsupervised policy\nlearning framework that combines prefix-based training with a\nMixture-of-Experts refiner to learn effective read and write decisions in an\nimplicit manner, without adding inference-time overhead. Our design requires\nonly minimal modifications to standard transformer architectures and\ngeneralizes across both speech-to-text and text-to-speech streaming tasks.\nThrough comprehensive evaluation on six language pairs, our 500M parameter\nspeech-to-text model outperforms the Seamless baseline, achieving under 7\npercent BLEU degradation at 1.5 seconds average lag and under 3 percent at 3\nseconds. We further demonstrate the versatility of SimulMEGA by extending it to\nstreaming TTS with a unidirectional backbone, yielding superior latency quality\ntradeoffs.", "published": "2025-09-01 07:34:50", "link": "http://arxiv.org/abs/2509.01200v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "EZhouNet:A framework based on graph neural network and anchor interval for the respiratory sound event detection", "abstract": "Auscultation is a key method for early diagnosis of respiratory and pulmonary\ndiseases, relying on skilled healthcare professionals. However, the process is\noften subjective, with variability between experts. As a result, numerous deep\nlearning-based automatic classification methods have emerged, most of which\nfocus on respiratory sound classification. In contrast, research on respiratory\nsound event detection remains limited. Existing sound event detection methods\ntypically rely on frame-level predictions followed by post-processing to\ngenerate event-level outputs, making interval boundaries challenging to learn\ndirectly. Furthermore, many approaches can only handle fixed-length audio, lim-\niting their applicability to variable-length respiratory sounds. Additionally,\nthe impact of respiratory sound location information on detection performance\nhas not been extensively explored. To address these issues, we propose a graph\nneural network-based framework with anchor intervals, capable of handling\nvariable-length audio and providing more precise temporal localization for\nabnormal respi- ratory sound events. Our method improves both the flexibility\nand applicability of respiratory sound detection. Experiments on the SPRSound\n2024 and HF Lung V1 datasets demonstrate the effec- tiveness of the proposed\napproach, and incorporating respiratory position information enhances the\ndiscrimination between abnormal sounds.", "published": "2025-09-01 06:10:30", "link": "http://arxiv.org/abs/2509.01153v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Noisy Disentanglement with Tri-stage Training for Noise-Robust Speech Recognition", "abstract": "To enhance the performance of end-to-end (E2E) speech recognition systems in\nnoisy or low signal-to-noise ratio (SNR) conditions, this paper introduces\nNoisyD-CT, a novel tri-stage training framework built on the\nConformer-Transducer architecture. The core of NoisyD-CT is a especially\ndesigned compact noisy disentanglement (NoisyD) module (adding only 1.71M\nparameters), integrated between the Conformer blocks and Transducer Decoder to\nperform deep noise suppression and improve ASR robustness in challenging\nacoustic noise environments. To fully exploit the noise suppression capability\nof the NoisyD-CT, we further propose a clean representation consistency loss to\nalign high-level representations derived from noisy speech with those obtained\nfrom corresponding clean speech. Together with a noisy reconstruction loss,\nthis consistency alignment enables the NoisyD module to effectively suppress\nnoise while preserving essential acoustic and linguistic features consistent\nacross both clean and noisy conditions, thereby producing cleaner internal\nrepresentations that enhance ASR performance. Moreover, our tri-stage training\nstrategy is designed to fully leverage the functionalities of both the noisy\ndisentanglement and speech recognition modules throughout the model training\nprocess, ultimately maximizing performance gains under noisy conditions. Our\nexperiments are performed on the LibriSpeech and CHiME-4 datasets, extensive\nresults demonstrate that our proposed NoisyD-CT significantly outperforms the\ncompetitive Conformer-Transducer baseline, achieving up to 25.7% and 10.6%\nrelative word error rate reductions on simulated and real-world noisy test\nsets, respectively, while maintaining or even improving performance on clean\nspeech test sets. The source code, model checkpoint and data simulation scripts\nwill be available at https://github.com/litchimo/NoisyD-CT.", "published": "2025-09-01 03:20:52", "link": "http://arxiv.org/abs/2509.01087v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Leveraging Orbital Dynamics with RF Signal Features for Satellite Multi-Orbit Proximity Threat Detection", "abstract": "Proximity-based interference is a growing threat to satellite communications,\ndriven by dense multi-orbit constellations and increasingly agile adversarial\nmaneuvers. We propose a hybrid simulation framework that integrates orbital\nmaneuver modeling with RF signal degradation analysis to detect and classify\nsuspicious proximity operations. Using the open-source Maneuver Detection Data\nGeneration (MaDDG) library from MIT Lincoln Laboratory, we generate labeled\ndatasets combining impulsive maneuver profiles with radio-frequency (RF)\nimpacts across a range of behavioral intents: routine station-keeping, covert\nshadowing, and overt jamming. Our approach fuses kinematic features such as\nrange, velocity, acceleration, and Time of Closest Approach (TCA), with RF\nmetrics including Received Signal Strength Indicator (RSSI), throughput, and\nJammer-to-Signal Ratio (JSR). These features are further enhanced with temporal\nderivatives and rolling-window statistics to capture subtle or transient\ninterference patterns. A Random Forest classifier trained on this fused feature\nset achieves 94.67% accuracy and a macro F1 score of 0.9471, outperforming\nmodels using only kinematic or RF inputs. The system is particularly effective\nin detecting covert threats, such as surveillance or intermittent jamming, that\nevade RF-only methods.", "published": "2025-09-01 22:12:09", "link": "http://arxiv.org/abs/2509.01802v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Predictive Communications for Low-Altitude Networks", "abstract": "The emergence of dense, mission-driven aerial networks supporting the\nlow-altitude economy presents unique communication challenges, including\nextreme channel dynamics and severe cross-tier interference. Traditional\nreactive communication paradigms are ill-suited to these environments, as they\nfail to leverage the network's inherent predictability. This paper introduces\npredictive communication, a novel paradigm transforming network management from\nreactive adaptation to proactive optimization. The approach is enabled by\nfusing predictable mission trajectories with stable, large-scale radio\nenvironment models (e.g., radio maps). Specifically, we present a hierarchical\nframework that decomposes the predictive cross-layer resource allocation\nproblem into three layers: strategic (routing), tactical (timing), and\noperational (power). This structure aligns decision-making timescales with the\naccuracy levels and ranges of available predictive information. We demonstrate\nthat this foresight-driven framework achieves an order-of-magnitude reduction\nin cross-tier interference, laying the groundwork for robust and scalable\nlow-altitude communication systems.", "published": "2025-09-01 18:25:51", "link": "http://arxiv.org/abs/2509.01705v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Non-Identical Diffusion Models in MIMO-OFDM Channel Generation", "abstract": "We propose a novel diffusion model, termed the non-identical diffusion model,\nand investigate its application to wireless orthogonal frequency division\nmultiplexing (OFDM) channel generation. Unlike the standard diffusion model\nthat uses a scalar-valued time index to represent the global noise level, we\nextend this notion to an element-wise time indicator to capture local error\nvariations more accurately. Non-identical diffusion enables us to characterize\nthe reliability of each element (e.g., subcarriers in OFDM) within the noisy\ninput, leading to improved generation results when the initialization is\nbiased. Specifically, we focus on the recovery of wireless multi-input\nmulti-output (MIMO) OFDM channel matrices, where the initial channel estimates\nexhibit highly uneven reliability across elements due to the pilot scheme.\nConventional time embeddings, which assume uniform noise progression, fail to\ncapture such variability across pilot schemes and noise levels. We introduce a\nmatrix that matches the input size to control element-wise noise progression.\nFollowing a similar diffusion procedure to existing methods, we show the\ncorrectness and effectiveness of the proposed non-identical diffusion scheme\nboth theoretically and numerically. For MIMO-OFDM channel generation, we\npropose a dimension-wise time embedding strategy. We also develop and evaluate\nmultiple training and generation methods and compare them through numerical\nexperiments.", "published": "2025-09-01 17:33:39", "link": "http://arxiv.org/abs/2509.01641v1", "categories": ["eess.SP", "cs.AI", "cs.LG"], "primary_category": "eess.SP"}
{"title": "To Share, or Not to Share: A Study on GEO-LEO Systems for IoT Services with Random Access", "abstract": "The increasing number of satellite deployments, both in the low and\ngeostationary Earth orbit exacerbates the already ongoing scarcity of wireless\nresources when targeting ubiquitous connectivity. For the aim of supporting a\nmassive number of IoT devices characterized by bursty traffic and modern\nvariants of random access, we pose the following question: Should competing\nsatellite operators share spectrum resources or is an exclusive allocation\npreferable? This question is addressed by devising a communication model for\ntwo operators which serve overlapping coverage areas with independent IoT\nservices. Analytical approximations, validated by Monte Carlo simulations,\nreveal that spectrum sharing can yield significant throughput gains for both\noperators under certain conditions tied to the relative serviced user\npopulations and coding rates in use. These gains are sensitive also to the\nsystem parameters and may not always render the spectral coexistence mutually\nadvantageous. Our model captures basic trade-offs in uplink spectrum sharing\nand provides novel actionable insights for the design and regulation of future\n6G non-terrestrial networks.", "published": "2025-09-01 14:27:02", "link": "http://arxiv.org/abs/2509.01506v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "High-resolution single-pixel imaging in real time with iterative or deep learning-based reconstruction enhancement", "abstract": "We introduce a compressive single-pixel imaging (SPI) framework for\nhigh-resolution image capture in fractions of a second. This framework combines\na dedicated sampling strategy with a tailored reconstruction method to enable\nhigh-quality imaging of spatially sparse scenes at the native 1024x768\nresolution of a digital micromirror device (DMD). The reconstruction process\nconsists of two phases: first, the measured data is processed using the\ngeneralized inverse of the measurement matrix for quick image recovery. Then,\nthe spatial sparsity of the scene is leveraged to enhance reconstruction in\ndense areas, using either an iterative method or a neural network-based\napproach. With a compression ratio of 0.41% and an image acquisition rate of\n6.8 Hz at 22 kHz DMD operation, this framework supports real-time,\nhigh-resolution dynamic imaging with the reconstruction that matches the\nacquisition rate on a mid-tier desktop GPU.", "published": "2025-09-01 14:17:18", "link": "http://arxiv.org/abs/2509.01497v1", "categories": ["eess.IV", "eess.SP"], "primary_category": "eess.IV"}
{"title": "A James-Stein Estimator based Generalized OMP Algorithm for Robust Signal Recovery using Sparse Representation", "abstract": "In this paper, we introduce a novel algorithm named JS-gOMP, which enhances\nthe generalized Orthogonal Matching Pursuit (gOMP) algorithm for improved noise\nrobustness in sparse signal processing. The JS-gOMP algorithm uniquely\nincorporates the James-Stein estimator, optimizing the trade-off between signal\nrecovery and noise suppression. This modification addresses the challenges\nposed by noise in the dictionary, a common issue in sparse representation\nscenarios. Comparative analyses demonstrate that JS-gOMP outperforms\ntraditional gOMP, especially in noisy environments, offering a more effective\nsolution for signal and image processing applications where noise presence is\nsignificant.", "published": "2025-09-01 12:07:44", "link": "http://arxiv.org/abs/2509.01410v1", "categories": ["eess.SP", "math.ST", "stat.TH", "94A12, 41A45, 94A20"], "primary_category": "eess.SP"}
{"title": "Comparison between Supervised and Unsupervised Learning in Deep Unfolded Sparse Signal Recovery", "abstract": "This paper investigates the impact of loss function selection in deep\nunfolding techniques for sparse signal recovery algorithms. Deep unfolding\ntransforms iterative optimization algorithms into trainable lightweight neural\nnetworks by unfolding their iterations as network layers, with various loss\nfunctions employed for parameter learning depending on application contexts. We\nfocus on deep unfolded versions of the fundamental iterative shrinkage\nthresholding algorithm (ISTA) and the iterative hard thresholding algorithm\n(IHT), comparing supervised learning using mean squared error with unsupervised\nlearning using the objective function of the original optimization problem. Our\nsimulation results reveal that the effect of the choice of loss function\nsignificantly depends on the convexity of the optimization problem. For convex\n$\\ell_1$-regularized problems, supervised-ISTA achieves better final recovery\naccuracy but fails to minimize the original objective function, whereas we\nempirically observe that unsupervised-ISTA converges to a nearly identical\nsolution as conventional ISTA but with accelerated convergence. Conversely, for\nnonconvex $\\ell_0$-regularized problems, both supervised-IHT and\nunsupervised-IHT converge to better local minima than the original IHT, showing\nsimilar performance regardless of the loss function employed. These findings\nprovide valuable insights into the design of effective deep unfolded networks\nfor sparse signal recovery applications.", "published": "2025-09-01 10:13:56", "link": "http://arxiv.org/abs/2509.01331v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "SMDS-based Rigid Body Localization", "abstract": "We consider a novel rigid body localization (RBL) method, based only on a set\nof measurements of the distances, as well as the angles between sensors of the\nvehicle to the anchor landmark points. A key point of the proposed method is to\nuse a variation of the super multidimensional scaling (SMDS) algorithm, where\nonly a minor part of the complex edge kernel is used, based on the available\ninformation, which in the case of RBL is anchor-to-anchor and target-to-target\ninformation. Simulation results illustrate the good performance of the proposed\ntechnique in terms of mean square error (MSE) of the estimates, compared also\nto the corresponding Cram\\'er-Rao Lower Bound (CRLB).", "published": "2025-09-01 08:09:01", "link": "http://arxiv.org/abs/2509.01223v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Rate Optimization for Downlink URLLC via Pinching Antenna Arrays", "abstract": "This work studies an ultra-reliable and low-latency communications (uRLLC)\ndownlink system using pinching antennas which are realized by activating small\ndielectric particles along a dielectric waveguide. Our goal is to maximize the\ndata rate by optimizing the positions of the pinching antennas. By proposing a\ncompact and cost-efficient antenna architecture and formulating a finite\nblocklength-based optimization model, we derive a closed-form solution for the\noptimal antenna placement under quality-of-service (QoS) and antenna spacing\nconstraints. Meanwhile, a phase-alignment strategy is integrated into the\ndesign, enabling coherent signal superposition across the array. Simulation\nresults confirm significant rate improvements over conventional antenna systems\nwhile satisfying uRLLC requirements, making the proposed design well-suited for\ncompact and latency-critical future applications.", "published": "2025-09-01 08:07:49", "link": "http://arxiv.org/abs/2509.01222v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "nRTIS: Low-Cost Real-Time 3D Sonar Imaging Circular Array Supporting Beamforming for Industrial Applications", "abstract": "Conventional ultrasonic inspection systems rely on phased arrays and\nhigh-performance computing hardware, making them costly, bulky, and unsuitable\nfor portable or embedded use. In this work, we present nRTIS (nano Real-Time 3D\nImaging Sonar), a compact ultrasonic sensing platform built around a circular\narray of MEMS microphones and a central ultrasonic transducer. The device\nachieves real-time acquisition through an RP2350 microcontroller and high-speed\nUSB transfer. We validate the system using both simulations and controlled\nexperiments: point spread function (PSF) simulations demonstrate beamforming\nresolution and sidelobe suppression, while reflector measurements confirm\nrobust data acquisition. These results highlight the potential of nRTIS for\nscalable industrial applications such as weld inspection, pipe mapping, and\nrobotic navigation.", "published": "2025-09-01 07:51:18", "link": "http://arxiv.org/abs/2509.01212v1", "categories": ["eess.SP", "cs.SY", "eess.SY"], "primary_category": "eess.SP"}
{"title": "Rigid Body Localization and Tracking for 6G V2X: Algorithms, Applications, and Road to Adoption", "abstract": "Vehicle-to-everything (V2X) perception refers to a suite of technologies that\nempower vehicles to sense their environment and communicate with other\nentities, including surrounding vehicles, infrastructure, and cloud/edge\nnetworks. With the growing demands of autonomous driving, V2X perception has\ngained significant attention, particularly through the emergence of integrated\nsensing and communication (ISAC) frameworks. Within this landscape, rigid body\nlocalization (RBL) has emerged as a promising paradigm, enabling the estimation\nof not only the position and velocity of the targets, but also its\nthree-dimensional (3D) geometric structure and orientation. This article\nintroduces the concept of RBL, highlights its unique advantages and\napplications, identifies key technical challenges, and finally outlines future\nresearch directions. In addition, the potential of RBL in next-generation -\ne.g. beyond fifth generation (B5G) and sixth-generation (6G) - wireless systems\napplied to V2X perception is also discussed, with a focus on its role in\nstandardization efforts and its relevance across automotive and industrial\ndomains.", "published": "2025-09-01 07:46:38", "link": "http://arxiv.org/abs/2509.01208v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Enhanced Fingerprint-based Positioning With Practical Imperfections: Deep learning-based approaches", "abstract": "High-precision positioning is vital for cellular networks to support\ninnovative applications such as extended reality, unmanned aerial vehicles\n(UAVs), and industrial Internet of Things (IoT) systems. Existing positioning\nalgorithms using deep learning techniques require vast amounts of labeled data,\nwhich are difficult to obtain in real-world cellular environments, and these\nmodels often struggle to generalize effectively. To advance cellular\npositioning techniques, the 2024 Wireless Communication Algorithm Elite\nCompetition as conducted, which provided a dataset from a three-sector outdoor\ncellular system, incorporating practical challenges such as limited\nlabeled-dataset, dynamic wireless environments within the target and\nunevenly-spaced anchors, Our team developed three innovative positioning\nframeworks that swept the top three awards of this competition, namely the\nsemi-supervised framework with consistency, ensemble learning-based algorithm\nand decoupled mapping heads-based algorithm. Specifically, the semi-supervised\nframework with consistency effectively generates high-quality pseudo-labels,\nenlarging the labeled-dataset for model training. The ensemble learning-based\nalgorithm amalgamates the positioning coordinates from models trained under\ndifferent strategies, effectively combating the dynamic positioning\nenvironments. The decoupled mapping heads-based algorithm utilized sector\nrotation scheme to resolve the uneven-spaced anchor issue. Simulation results\ndemonstrate the superior performance of our proposed positioning algorithms\ncompared to existing benchmarks in terms of the {90%, 80%, 67%, 50%} percentile\nand mean distance error.", "published": "2025-09-01 07:30:56", "link": "http://arxiv.org/abs/2509.01197v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Beyond Exhaustive Sampling: Efficient Rotational Matching via Ball Harmonics", "abstract": "Cryo-ET allows to generate tomograms of biological samples in situ, capturing\ncomplex structures in their native context. Despite low signal-to-noise ratio\nin reconstructed volumes, the large number of copies of the same macromolecules\nmakes it possible to retrieve high-resolution maps by averaging many aligned\nsubtomograms. To keep up with technical advances in the imaging process and the\nresulting huge amounts of data available, there is a need for scalable, fast\nand robust procedures to align subtomograms. We propose a subtomogram alignment\nframework based on the ball harmonics expansion that combines frequency- and\ngradient-based optimization strategies to avoid exhaustive rotation sampling,\nenabling a speed-up of an order of magnitude compared to current approaches.", "published": "2025-09-01 07:00:26", "link": "http://arxiv.org/abs/2509.01180v1", "categories": ["eess.SP", "q-bio.QM"], "primary_category": "eess.SP"}
{"title": "DynaMind: Reconstructing Dynamic Visual Scenes from EEG by Aligning Temporal Dynamics and Multimodal Semantics to Guided Diffusion", "abstract": "Reconstruction dynamic visual scenes from electroencephalography (EEG)\nsignals remains a primary challenge in brain decoding, limited by the low\nspatial resolution of EEG, a temporal mismatch between neural recordings and\nvideo dynamics, and the insufficient use of semantic information within brain\nactivity. Therefore, existing methods often inadequately resolve both the\ndynamic coherence and the complex semantic context of the perceived visual\nstimuli. To overcome these limitations, we introduce DynaMind, a novel\nframework that reconstructs video by jointly modeling neural dynamics and\nsemantic features via three core modules: a Regional-aware Semantic Mapper\n(RSM), a Temporal-aware Dynamic Aligner (TDA), and a Dual-Guidance Video\nReconstructor (DGVR). The RSM first utilizes a regional-aware encoder to\nextract multimodal semantic features from EEG signals across distinct brain\nregions, aggregating them into a unified diffusion prior. In the mean time, the\nTDA generates a dynamic latent sequence, or blueprint, to enforce temporal\nconsistency between the feature representations and the original neural\nrecordings. Together, guided by the semantic diffusion prior, the DGVR\ntranslates the temporal-aware blueprint into a high-fidelity video\nreconstruction. On the SEED-DV dataset, DynaMind sets a new state-of-the-art\n(SOTA), boosting reconstructed video accuracies (video- and frame-based) by\n12.5 and 10.3 percentage points, respectively. It also achieves a leap in\npixel-level quality, showing exceptional visual fidelity and temporal coherence\nwith a 9.4% SSIM improvement and a 19.7% FVMD reduction. This marks a critical\nadvancement, bridging the gap between neural dynamics and high-fidelity visual\nsemantics.", "published": "2025-09-01 06:52:08", "link": "http://arxiv.org/abs/2509.01177v1", "categories": ["cs.CV", "cs.AI", "cs.HC", "eess.SP"], "primary_category": "cs.CV"}
{"title": "Dynamic State Estimation of Power System Utilizing Cauchy Kernel-Based Maximum Mixture Correntropy UKF over Beluga Whale-Bat Optimization", "abstract": "Non-Gaussian noise, outliers, sudden load changes, and bad measurement data\nare key factors that diminish the accuracy of dynamic state estimation in power\nsystems. Additionally, unscented Kalman filters (UKF) based on correntropy\ncriteria utilize bandwidth-sensitive Gaussian kernels, which may lead to\nsingular matrices in the Cholesky decomposition. To overcome all the above\nproblems, in this paper, a robust UKF based on Cauchy kernel maximum mixture\ncorrentropy (CKMMC) criteria over hybrid Beluga Whale-Bat (BWB) optimization\n(BWB-CKMMC-UKF) is proposed, in which the kernel is merged of two Cauchy\nfunctions. Specifically, the measurement error and state error are unified in\nthe cost function by the statistical linearization technique, and the optimal\nvalue of state estimation is obtained by fixed-point iteration. Because of its\ninsensitive feature to kernel bandwidth and notable thick-tailed feature, the\nCauchy kernel function is utilized instead of the Gaussian kernel in the\noptimization criteria. Additionally, to fit the power system model, the shape\ncoefficients of the kernel in the CKMMC criterion and scale coefficients that\ninfluence the selection of sigma points in the unscented transform are\ndetermined based on the BWB algorithm. Simulation results on IEEE 14, 30, and\n57-bus test systems validated the performance of the proposed algorithm.", "published": "2025-09-01 06:36:28", "link": "http://arxiv.org/abs/2509.01163v1", "categories": ["eess.SP", "53-04", "I.6.3"], "primary_category": "eess.SP"}
{"title": "A Model-Based Dictionary Approach for Magnetic Nanoparticle Signal Prediction", "abstract": "Magnetic particle imaging (MPI) is a tracer-based medical imaging modality\nthat enables quantification and spatial mapping of magnetic nanoparticle (MNP)\ndistribution. The magnetization response of MNPs depends on experimental\nconditions such as drive field (DF) settings and medium viscosity, as well as\non magnetic parameters of MNPs such as magnetic core diameter, hydrodynamic\ndiameter, and magnetic anisotropy constant. A comprehensive understanding of\nthe magnetization response of MNPs can facilitate the optimization of DF and\nMNP type for a given MPI application, without the need for extensive\nexperimentation. In this work, we propose a calibration-free iterative\nalgorithm using model-based dictionaries for MNP signal prediction at untested\nsettings. Dictionaries were constructed with the MNP signals simulated using\nthe coupled Brown-N\\'eel rotation model. Based on the available measurements,\nthe proposed algorithm jointly estimates the dictionary weights and the\ntransfer functions due to non-model-based dynamics. These dynamics include the\nsystem response of the measurement setup as well as magnetization dynamics not\naccounted for by the employed coupled Brown-N\\'eel rotation model. The\nalgorithm was first validated on synthetic signals at SNR levels of 1 and 10,\nand then tested on an in-house MPS setup across six viscosity levels\n(0.89-15.33 mPa.s) and DF frequencies of 0.25-2 kHz using two commercial MNPs.\nValidation on synthetic signals showed accurate weight and transfer function\nestimation even at SNR 1. MPS experiments demonstrated successful prediction of\nMNP signals at untested viscosities, with NRMSE below 1.51% and 3.5% for the\ntwo tested MNPs across all DF settings. Predicted signals captured viscosity\ndependent trends, and NWD values remained low (<0.10 and <0.07 for the two\ntested MNPs), confirming robust weight estimation.", "published": "2025-09-01 04:44:13", "link": "http://arxiv.org/abs/2509.01127v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Enabling 6G Through Multi-Domain Channel Extrapolation: Opportunities and Challenges of Generative Artificial Intelligence", "abstract": "Channel extrapolation has attracted wide attention due to its potential to\nacquire channel state information (CSI) with high accuracy and minimal\noverhead. This is becoming increasingly crucial as the sixth-generation (6G)\nmobile networks aim to support complex scenarios, for example, high-mobility\ncommunications utilizing ultra-massive multiple-input multiple-output (MIMO)\ntechnologies and broad spectrum bands, necessitating multi-domain channel\nextrapolation. Current research predominantly addresses channel extrapolation\nwithin a single domain, lacking a comprehensive approach to multi-domain\nchannel extrapolation. To bridge the gap, we propose the concept of\nmulti-domain channel extrapolation, detailing the essential performance\nrequirements for 6G networks. These include precise channel extrapolation,\nadaptability to varying scenarios, and manageable computational complexity\nduring both training and inference stages. In light of these requirements, we\nelaborate the potential and challenges of incorporating generative artificial\nintelligence (GAI)-based models for effective multi-domain channel\nextrapolation. Given the ability of the Transformer to capture long-range\ndependencies and hidden patterns, we propose a novel Transformer encoder-like\nmodel by eliminating the positional encoding module and replacing the original\nmulti-head attention with a multilayer perceptron (MLP) for multi-domain\nchannel extrapolation. Simulation results indicate that this model surpasses\nexisting baseline models in terms of extrapolation accuracy and inference\nspeed. Ablation studies further demonstrate the effectiveness of the module\ndesign of the proposed design. Finally, we pose several open questions for the\ndevelopment of practical GAI-based multi-domain channel extrapolation models,\nincluding the issues of explainability, generalization, and dataset collection.", "published": "2025-09-01 04:41:15", "link": "http://arxiv.org/abs/2509.01125v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Fluid Antenna Port Prediction based on Large Language Models", "abstract": "This study seeks to utilize large language models (LLMs) to forecast the\nmoving ports of fluid antenna (FA). By repositioning the antenna to the\nlocations identified by our proposed model, we intend to address the mobility\nchallenges faced by user equipment (UE). To the best of our knowledge, this\npaper introduces, for the first time, the application of LLMs in the prediction\nof FA ports, presenting a novel model termed Port-LLM. The architecture of our\nmodel is based on the pre-trained GPT-2 framework. We designed specialized data\npreprocessing, input embedding, and output projection modules to effectively\nbridge the disparities between the wireless communication data and the data\nformat utilized by the pre-trained LLM. Simulation results demonstrate that our\nmodel exhibits superior predictive performance under different numbers of base\nstation (BS) antennas and varying UE speeds, indicating strong generalization\nand robustness ability. Furthermore, the spectral efficiency (SE) attained by\nour model surpasses that achieved by traditional methods in both medium and\nhigh-speed mobile environments.", "published": "2025-09-01 04:33:47", "link": "http://arxiv.org/abs/2509.01121v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "SC-GIR: Goal-oriented Semantic Communication via Invariant Representation Learning", "abstract": "Goal-oriented semantic communication (SC) aims to revolutionize communication\nsystems by transmitting only task-essential information. However, current\napproaches face challenges such as joint training at transceivers, leading to\nredundant data exchange and reliance on labeled datasets, which limits their\ntask-agnostic utility. To address these challenges, we propose a novel\nframework called Goal-oriented Invariant Representation-based SC (SC-GIR) for\nimage transmission. Our framework leverages self-supervised learning to extract\nan invariant representation that encapsulates crucial information from the\nsource data, independent of the specific downstream task. This compressed\nrepresentation facilitates efficient communication while retaining key features\nfor successful downstream task execution. Focusing on machine-to-machine tasks,\nwe utilize covariance-based contrastive learning techniques to obtain a latent\nrepresentation that is both meaningful and semantically dense. To evaluate the\neffectiveness of the proposed scheme on downstream tasks, we apply it to\nvarious image datasets for lossy compression. The compressed representations\nare then used in a goal-oriented AI task. Extensive experiments on several\ndatasets demonstrate that SC-GIR outperforms baseline schemes by nearly 10%,,\nand achieves over 85% classification accuracy for compressed data under\ndifferent SNR conditions. These results underscore the effectiveness of the\nproposed framework in learning compact and informative latent representations.", "published": "2025-09-01 04:29:43", "link": "http://arxiv.org/abs/2509.01119v1", "categories": ["cs.LG", "cs.AI", "eess.SP"], "primary_category": "cs.LG"}
{"title": "A Bayesian Framework For Cascaded Channel Estimation in RIS-Aided mmWave Systems", "abstract": "In this paper, we investigate cascaded channel estimation for reconfigurable\nintelligent surface (RIS)-aided millimeter-wave multi-user communication\nsystems. Since the complex channel gains of the cascaded RIS channel are\ngenerally non-Gaussian, the use of the linear minimum mean squared error\n(LMMSE) estimator leads to inevitable performance degradation. To tackle this\nissue, we propose a variational inference-based framework that approximates the\ncomplex channel gains using a complex adaptive Laplace prior, which effectively\ncaptures their probability distributions in a tractable way. Numerical results\ndemonstrate that the proposed estimator outperforms conventional estimators\nincluding least squares and LMMSE in terms of cascaded channel estimation\nerror.", "published": "2025-09-01 04:22:47", "link": "http://arxiv.org/abs/2509.01117v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "IMU-Enhanced EEG Motion Artifact Removal with Fine-Tuned Large Brain Models", "abstract": "Electroencephalography (EEG) is a non-invasive method for measuring brain\nactivity with high temporal resolution; however, EEG signals often exhibit low\nsignal-to-noise ratios because of contamination from physiological and\nenvironmental artifacts. One of the major challenges hindering the real-world\ndeployment of brain-computer interfaces (BCIs) involves the frequent occurrence\nof motion-related EEG artifacts. Most prior studies on EEG motion artifact\nremoval rely on single-modality approaches, such as Artifact Subspace\nReconstruction (ASR) and Independent Component Analysis (ICA), without\nincorporating simultaneously recorded modalities like inertial measurement\nunits (IMUs), which directly capture the extent and dynamics of motion. This\nwork proposes a fine-tuned large brain model (LaBraM)-based correlation\nattention mapping method that leverages spatial channel relationships in IMU\ndata to identify motion-related artifacts in EEG signals. The fine-tuned model\ncontains approximately 9.2 million parameters and uses 5.9 hours of EEG and IMU\nrecordings for training, just 0.2346\\% of the 2500 hours used to train the base\nmodel. We compare our results against the established ASR-ICA benchmark across\nvarying time scales and motion activities, showing that incorporating IMU\nreference signals significantly improves robustness under diverse motion\nscenarios.", "published": "2025-09-01 02:29:14", "link": "http://arxiv.org/abs/2509.01073v1", "categories": ["cs.LG", "eess.SP", "q-bio.NC"], "primary_category": "cs.LG"}
{"title": "BSNeRF: Broadband Spectral Neural Radiance Fields for Snapshot Multispectral Light-field Imaging", "abstract": "Snapshot Multispectral Light-field Imaging (SMLI) is an emerging\ncomputational imaging technique that captures high-dimensional data (x, y, z,\n$\\theta$, $\\phi$, $\\lambda$) in a single shot using a low-dimensional sensor.\nThe accuracy of high-dimensional data reconstruction depends on representing\nthe spectrum using neural radiance field models, which requires consideration\nof broadband spectral decoupling during optimization. Currently, some SMLI\napproaches avoid the challenge of model decoupling by either reducing\nlight-throughput or prolonging imaging time. In this work, we propose a\nbroadband spectral neural radiance field (BSNeRF) for SMLI systems. Experiments\nshow that our model successfully decouples a broadband multiplexed spectrum.\nConsequently, this approach enhances multispectral light-field image\nreconstruction and further advances plenoptic imaging.", "published": "2025-09-01 02:19:00", "link": "http://arxiv.org/abs/2509.01070v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "EZhouNet:A framework based on graph neural network and anchor interval for the respiratory sound event detection", "abstract": "Auscultation is a key method for early diagnosis of respiratory and pulmonary\ndiseases, relying on skilled healthcare professionals. However, the process is\noften subjective, with variability between experts. As a result, numerous deep\nlearning-based automatic classification methods have emerged, most of which\nfocus on respiratory sound classification. In contrast, research on respiratory\nsound event detection remains limited. Existing sound event detection methods\ntypically rely on frame-level predictions followed by post-processing to\ngenerate event-level outputs, making interval boundaries challenging to learn\ndirectly. Furthermore, many approaches can only handle fixed-length audio,\nlimiting their applicability to variable-length respiratory sounds.\nAdditionally, the impact of respiratory sound location information on detection\nperformance has not been extensively explored. To address these issues, we\npropose a graph neural network-based framework with anchor intervals, capable\nof handling variable-length audio and providing more precise temporal\nlocalization for abnormal respiratory sound events. Our method improves both\nthe flexibility and applicability of respiratory sound detection. Experiments\non the SPRSound 2024 and HF Lung V1 datasets demonstrate the effectiveness of\nthe proposed approach, and incorporating respiratory position information\nenhances the discrimination between abnormal sounds. The reference\nimplementation is available at https://github.com/chumingqian/EzhouNet.", "published": "2025-09-01 06:10:30", "link": "http://arxiv.org/abs/2509.01153v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Learning to Coordinate: Distributed Meta-Trajectory Optimization Via Differentiable ADMM-DDP", "abstract": "Distributed trajectory optimization via ADMM-DDP is a powerful approach for\ncoordinating multi-agent systems, but it requires extensive tuning of tightly\ncoupled hyperparameters that jointly govern local task performance and global\ncoordination. In this paper, we propose Learning to Coordinate (L2C), a general\nframework that meta-learns these hyperparameters, modeled by lightweight\nagent-wise neural networks, to adapt across diverse tasks and agent\nconfigurations. L2C differentiates end-to-end through the ADMM-DDP pipeline in\na distributed manner. It also enables efficient meta-gradient computation by\nreusing DDP components such as Riccati recursions and feedback gains. These\ngradients correspond to the optimal solutions of distributed matrix-valued LQR\nproblems, coordinated across agents via an auxiliary ADMM framework that\nbecomes convex under mild assumptions. Training is further accelerated by\ntruncating iterations and meta-learning ADMM penalty parameters optimized for\nrapid residual reduction, with provable Lipschitz-bounded gradient errors. On a\nchallenging cooperative aerial transport task, L2C generates dynamically\nfeasible trajectories in high-fidelity simulation using IsaacSIM, reconfigures\nquadrotor formations for safe 6-DoF load manipulation in tight spaces, and\nadapts robustly to varying team sizes and task conditions, while achieving up\nto $88\\%$ faster gradient computation than state-of-the-art methods.", "published": "2025-09-01 17:17:05", "link": "http://arxiv.org/abs/2509.01630v2", "categories": ["cs.LG", "cs.MA", "cs.RO", "cs.SY", "eess.SY"], "primary_category": "cs.LG"}
{"title": "Serialized Output Prompting for Large Language Model-based Multi-Talker Speech Recognition", "abstract": "Prompts are crucial for task definition and for improving the performance of\nlarge language models (LLM)-based systems. However, existing LLM-based\nmulti-talker (MT) automatic speech recognition (ASR) systems either omit\nprompts or rely on simple task-definition prompts, with no prior work exploring\nthe design of prompts to enhance performance. In this paper, we propose\nextracting serialized output prompts (SOP) and explicitly guiding the LLM using\nstructured prompts to improve system performance (SOP-MT-ASR). A Separator and\nserialized Connectionist Temporal Classification (CTC) layers are inserted\nafter the speech encoder to separate and extract MT content from the mixed\nspeech encoding in a first-speaking-first-out manner. Subsequently, the SOP,\nwhich serves as a prompt for LLMs, is obtained by decoding the serialized CTC\noutputs using greedy search. To train the model effectively, we design a\nthree-stage training strategy, consisting of serialized output training (SOT)\nfine-tuning, serialized speech information extraction, and SOP-based\nadaptation. Experimental results on the LibriMix dataset show that, although\nthe LLM-based SOT model performs well in the two-talker scenario, it fails to\nfully leverage LLMs under more complex conditions, such as the three-talker\nscenario. The proposed SOP approach significantly improved performance under\nboth two- and three-talker conditions.", "published": "2025-09-01 03:10:14", "link": "http://arxiv.org/abs/2509.04488v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "DSDE: Dynamic Speculative Decoding with KLD Stability for Real-World Serving", "abstract": "Speculative decoding accelerates large language model inference, but its\nreliance on a fixed speculation length is suboptimal in large-batch serving\nenvironments with diverse requests. This paper explores a new direction for\ndynamic adaptation by investigating a novel class of post-hoc, diagnostic\nsignals. We propose Dynamic Speculative Decoding Engine (DSDE), a training-free\nframework built on two primary components: (1) a predictive signal based on the\nvariance of the Kullback-Leibler (KLD) divergence, which diagnoses the\ngeneration's regional stability, and (2) an adaptive speculation length cap to\nmitigate the straggler problem in per-sequence decoding. Experiments\ndemonstrate the potential of using KLD-based stability signals for dynamic\nadaptation. An algorithm guided by these signals achieves end-to-end latency\ncompetitive with leading baselines and exhibits superior robustness across\ndiverse workloads. This robustness is particularly valuable in challenging\nlow-acceptance-rate regimes, where the proposed signal maintains its diagnostic\nutility. Collectively, these findings validate post-hoc signals as a valuable\ncomponent for building more robust and intelligent LLM inference systems, and\nhighlight a promising direction for future research on dynamic speculation\nlength adaptation.", "published": "2025-09-01 03:13:50", "link": "http://arxiv.org/abs/2509.01083v2", "categories": ["cs.DC", "cs.AI", "cs.IT", "math.IT", "I.2.7; C.2.4"], "primary_category": "cs.DC"}
