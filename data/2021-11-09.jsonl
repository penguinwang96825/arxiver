{"title": "NATURE: Natural Auxiliary Text Utterances for Realistic Spoken Language\n  Evaluation", "abstract": "Slot-filling and intent detection are the backbone of conversational agents\nsuch as voice assistants, and are active areas of research. Even though\nstate-of-the-art techniques on publicly available benchmarks show impressive\nperformance, their ability to generalize to realistic scenarios is yet to be\ndemonstrated. In this work, we present NATURE, a set of simple spoken-language\noriented transformations, applied to the evaluation set of datasets, to\nintroduce human spoken language variations while preserving the semantics of an\nutterance. We apply NATURE to common slot-filling and intent detection\nbenchmarks and demonstrate that simple perturbations from the standard\nevaluation set by NATURE can deteriorate model performance significantly.\nThrough our experiments we demonstrate that when NATURE operators are applied\nto evaluation set of popular benchmarks the model accuracy can drop by up to\n40%.", "published": "2021-11-09 15:09:06", "link": "http://arxiv.org/abs/2111.05196v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey of NLP-Related Crowdsourcing HITs: what works and what does not", "abstract": "Crowdsourcing requesters on Amazon Mechanical Turk (AMT) have raised\nquestions about the reliability of the workers. The AMT workforce is very\ndiverse and it is not possible to make blanket assumptions about them as a\ngroup. Some requesters now reject work en mass when they do not get the results\nthey expect. This has the effect of giving each worker (good or bad) a lower\nHuman Intelligence Task (HIT) approval score, which is unfair to the good\nworkers. It also has the effect of giving the requester a bad reputation on the\nworkers' forums. Some of the issues causing the mass rejections stem from the\nrequesters not taking the time to create a well-formed task with complete\ninstructions and/or not paying a fair wage. To explore this assumption, this\npaper describes a study that looks at the crowdsourcing HITs on AMT that were\navailable over a given span of time and records information about those HITs.\nThis study also records information from a crowdsourcing forum on the worker\nperspective on both those HITs and on their corresponding requesters. Results\nreveal issues in worker payment and presentation issues such as missing\ninstructions or HITs that are not doable.", "published": "2021-11-09 16:26:51", "link": "http://arxiv.org/abs/2111.05241v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Logic Rules for Document-level Relation Extraction", "abstract": "Document-level relation extraction aims to identify relations between\nentities in a whole document. Prior efforts to capture long-range dependencies\nhave relied heavily on implicitly powerful representations learned through\n(graph) neural networks, which makes the model less transparent. To tackle this\nchallenge, in this paper, we propose LogiRE, a novel probabilistic model for\ndocument-level relation extraction by learning logic rules. LogiRE treats logic\nrules as latent variables and consists of two modules: a rule generator and a\nrelation extractor. The rule generator is to generate logic rules potentially\ncontributing to final predictions, and the relation extractor outputs final\npredictions based on the generated logic rules. Those two modules can be\nefficiently optimized with the expectation-maximization (EM) algorithm. By\nintroducing logic rules into neural networks, LogiRE can explicitly capture\nlong-range dependencies as well as enjoy better interpretation. Empirical\nresults show that LogiRE significantly outperforms several strong baselines in\nterms of relation performance (1.8 F1 score) and logical consistency (over 3.3\nlogic score). Our code is available at https://github.com/rudongyu/LogiRE.", "published": "2021-11-09 20:32:30", "link": "http://arxiv.org/abs/2111.05407v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FPM: A Collection of Large-scale Foundation Pre-trained Language Models", "abstract": "Large-scale Transformer models have significantly promoted the recent\ndevelopment of natural language processing applications. However, little effort\nhas been made to unify the effective models. In this paper, driven by providing\na new set of baseline models in the future, we adopt various novel transformer\narchitectures and launch a model set with the help of recent mainstream\ntechnologies. We focus the discussions on optimizing the depth of the networks\nbased on the existing powerful encode-decoder structures. We show that by\nproperly avoiding training defects such as non-convergence and degradation,\nscaling up off-the-shelf transformer architectures consistently delivers better\nperformance. To stimulate future research on large-scale language model\npretraining, we present extensive results and detailed discussions on network\nperformance improvements with respect to the network depth and confirm the\nexistence of the optimal number of layers under specific tasks. To the best of\nour knowledge, we provide the largest Chinese generative model and the largest\nChinese encoding model. The BERT language models we trained on English datasets\ndeliver a 14.45% higher F1 score than the Turing-NLR.", "published": "2021-11-09 02:17:15", "link": "http://arxiv.org/abs/2111.04909v3", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "DSBERT:Unsupervised Dialogue Structure learning with BERT", "abstract": "Unsupervised dialogue structure learning is an important and meaningful task\nin natural language processing. The extracted dialogue structure and process\ncan help analyze human dialogue, and play a vital role in the design and\nevaluation of dialogue systems. The traditional dialogue system requires\nexperts to manually design the dialogue structure, which is very costly. But\nthrough unsupervised dialogue structure learning, dialogue structure can be\nautomatically obtained, reducing the cost of developers constructing dialogue\nprocess. The learned dialogue structure can be used to promote the dialogue\ngeneration of the downstream task system, and improve the logic and consistency\nof the dialogue robot's reply.In this paper, we propose a Bert-based\nunsupervised dialogue structure learning algorithm DSBERT (Dialogue Structure\nBERT). Different from the previous SOTA models VRNN and SVRNN, we combine BERT\nand AutoEncoder, which can effectively combine context information. In order to\nbetter prevent the model from falling into the local optimal solution and make\nthe dialogue state distribution more uniform and reasonable, we also propose\nthree balanced loss functions that can be used for dialogue structure learning.\nExperimental results show that DSBERT can generate a dialogue structure closer\nto the real structure, can distinguish sentences with different semantics and\nmap them to different hidden states.", "published": "2021-11-09 03:31:18", "link": "http://arxiv.org/abs/2111.04933v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning to Generalize Compositionally by Transferring Across Semantic\n  Parsing Tasks", "abstract": "Neural network models often generalize poorly to mismatched domains or\ndistributions. In NLP, this issue arises in particular when models are expected\nto generalize compositionally, that is, to novel combinations of familiar words\nand constructions. We investigate learning representations that facilitate\ntransfer learning from one compositional task to another: the representation\nand the task-specific layers of the models are strategically trained\ndifferently on a pre-finetuning task such that they generalize well on\nmismatched splits that require compositionality. We apply this method to\nsemantic parsing, using three very different datasets, COGS, GeoQuery and SCAN,\nused alternately as the pre-finetuning and target task. Our method\nsignificantly improves compositional generalization over baselines on the test\nset of the target task, which is held out during fine-tuning. Ablation studies\ncharacterize the utility of the major steps in the proposed algorithm and\nsupport our hypothesis.", "published": "2021-11-09 09:10:21", "link": "http://arxiv.org/abs/2111.05013v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Tackling Morphological Analogies Using Deep Learning -- Extended Version", "abstract": "Analogical proportions are statements of the form \"A is to B as C is to D\".\nThey constitute an inference tool that provides a logical framework to address\nlearning, transfer, and explainability concerns and that finds useful\napplications in artificial intelligence and natural language processing. In\nthis paper, we address two problems, namely, analogy detection and resolution\nin morphology. Multiple symbolic approaches tackle the problem of analogies in\nmorphology and achieve competitive performance. We show that it is possible to\nuse a data-driven strategy to outperform those models. We propose an approach\nusing deep learning to detect and solve morphological analogies. It encodes\nstructural properties of analogical proportions and relies on a specifically\ndesigned embedding model capturing morphological characteristics of words. We\ndemonstrate our model's competitive performance on analogy detection and\nresolution over multiple languages. We provide an empirical study to analyze\nthe impact of balancing training data and evaluate the robustness of our\napproach to input perturbation.", "published": "2021-11-09 13:45:23", "link": "http://arxiv.org/abs/2111.05147v1", "categories": ["cs.CL", "cs.NE"], "primary_category": "cs.CL"}
{"title": "MNet-Sim: A Multi-layered Semantic Similarity Network to Evaluate\n  Sentence Similarity", "abstract": "Similarity is a comparative-subjective measure that varies with the domain\nwithin which it is considered. In several NLP applications such as document\nclassification, pattern recognition, chatbot question-answering, sentiment\nanalysis, etc., identifying an accurate similarity score for sentence pairs has\nbecome a crucial area of research. In the existing models that assess\nsimilarity, the limitation of effectively computing this similarity based on\ncontextual comparisons, the localization due to the centering theory, and the\nlack of non-semantic textual comparisons have proven to be drawbacks. Hence,\nthis paper presents a multi-layered semantic similarity network model built\nupon multiple similarity measures that render an overall sentence similarity\nscore based on the principles of Network Science, neighboring weighted\nrelational edges, and a proposed extended node similarity computation formula.\nThe proposed multi-layered network model was evaluated and tested against\nestablished state-of-the-art models and is shown to have demonstrated better\nperformance scores in assessing sentence similarity.", "published": "2021-11-09 20:43:18", "link": "http://arxiv.org/abs/2111.05412v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A Computational Approach to Walt Whitman's Stylistic Changes in Leaves\n  of Grass", "abstract": "This study analyzes Walt Whitman's stylistic changes in his phenomenal work\nLeaves of Grass from a computational perspective and relates findings to\nstandard literary criticism on Whitman. The corpus consists of all 7 editions\nof Leaves of Grass, ranging from the earliest 1855 edition to the 1891-92\n\"deathbed\" edition. Starting from counting word frequencies, the simplest\nstylometry technique, we find consistent shifts in word choice.\nMacro-etymological analysis reveals Whitman's increasing preference for words\nof specific origins, which is correlated to the increasing lexical complexity\nin Leaves of Grass. Principal component analysis, an unsupervised learning\nalgorithm, reduces the dimensionality of tf-idf vectors to 2 dimensions,\nproviding a straightforward view of stylistic changes. Finally, sentiment\nanalysis shows the evolution of Whitman's emotional state throughout his\nwriting career.", "published": "2021-11-09 20:53:25", "link": "http://arxiv.org/abs/2111.05414v1", "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.CL"}
{"title": "What goes on inside rumour and non-rumour tweets and their reactions: A\n  Psycholinguistic Analyses", "abstract": "In recent years, the problem of rumours on online social media (OSM) has\nattracted lots of attention. Researchers have started investigating from two\nmain directions. First is the descriptive analysis of rumours and secondly,\nproposing techniques to detect (or classify) rumours. In the descriptive line\nof works, where researchers have tried to analyse rumours using NLP approaches,\nthere isnt much emphasis on psycho-linguistics analyses of social media text.\nThese kinds of analyses on rumour case studies are vital for drawing meaningful\nconclusions to mitigate misinformation. For our analysis, we explored the\nPHEME9 rumour dataset (consisting of 9 events), including source tweets (both\nrumour and non-rumour categories) and response tweets. We compared the rumour\nand nonrumour source tweets and then their corresponding reply (response)\ntweets to understand how they differ linguistically for every incident.\nFurthermore, we also evaluated if these features can be used for classifying\nrumour vs. non-rumour tweets through machine learning models. To this end, we\nemployed various classical and ensemble-based approaches. To filter out the\nhighly discriminative psycholinguistic features, we explored the SHAP AI\nExplainability tool. To summarise, this research contributes by performing an\nin-depth psycholinguistic analysis of rumours related to various kinds of\nevents.", "published": "2021-11-09 07:45:11", "link": "http://arxiv.org/abs/2112.03003v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Neural News Recommendation with Event Extraction", "abstract": "A key challenge of online news recommendation is to help users find articles\nthey are interested in. Traditional news recommendation methods usually use\nsingle news information, which is insufficient to encode news and user\nrepresentation. Recent research uses multiple channel news information, e.g.,\ntitle, category, and body, to enhance news and user representation. However,\nthese methods only use various attention mechanisms to fuse multi-view\nembeddings without considering deep digging higher-level information contained\nin the context. These methods encode news content on the word level and jointly\ntrain the attention parameters in the recommendation network, leading to more\ncorpora being required to train the model. We propose an Event Extraction-based\nNews Recommendation (EENR) framework to overcome these shortcomings, utilizing\nevent extraction to abstract higher-level information. EENR also uses a\ntwo-stage strategy to reduce parameters in subsequent parts of the\nrecommendation network. We train the Event Extraction module by external\ncorpora in the first stage and apply the trained model to the news\nrecommendation dataset to predict event-level information, including event\ntypes, roles, and arguments, in the second stage. Then we fuse multiple channel\ninformation, including event information, news title, and category, to encode\nnews and users. Extensive experiments on a real-world dataset show that our\nEENR method can effectively improve the performance of news recommendations.\nFinally, we also explore the reasonability of utilizing higher abstract level\ninformation to substitute news body content.", "published": "2021-11-09 11:56:38", "link": "http://arxiv.org/abs/2111.05068v2", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Human-in-the-Loop Disinformation Detection: Stance, Sentiment, or\n  Something Else?", "abstract": "Both politics and pandemics have recently provided ample motivation for the\ndevelopment of machine learning-enabled disinformation (a.k.a. fake news)\ndetection algorithms. Existing literature has focused primarily on the\nfully-automated case, but the resulting techniques cannot reliably detect\ndisinformation on the varied topics, sources, and time scales required for\nmilitary applications. By leveraging an already-available analyst as a\nhuman-in-the-loop, however, the canonical machine learning techniques of\nsentiment analysis, aspect-based sentiment analysis, and stance detection\nbecome plausible methods to use for a partially-automated disinformation\ndetection system. This paper aims to determine which of these techniques is\nbest suited for this purpose and how each technique might best be used towards\nthis end. Training datasets of the same size and nearly identical neural\narchitectures (a BERT transformer as a word embedder with a single feed-forward\nlayer thereafter) are used for each approach, which are then tested on\nsentiment- and stance-specific datasets to establish a baseline of how well\neach method can be used to do the other tasks. Four different datasets relating\nto COVID-19 disinformation are used to test the ability of each technique to\ndetect disinformation on a topic that did not appear in the training data set.\nQuantitative and qualitative results from these tests are then used to provide\ninsight into how best to employ these techniques in practice.", "published": "2021-11-09 13:30:34", "link": "http://arxiv.org/abs/2111.05139v1", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Reason first, then respond: Modular Generation for Knowledge-infused\n  Dialogue", "abstract": "Large language models can produce fluent dialogue but often hallucinate\nfactual inaccuracies. While retrieval-augmented models help alleviate this\nissue, they still face a difficult challenge of both reasoning to provide\ncorrect knowledge and generating conversation simultaneously. In this work, we\npropose a modular model, Knowledge to Response (K2R), for incorporating\nknowledge into conversational agents, which breaks down this problem into two\neasier steps. K2R first generates a knowledge sequence, given a dialogue\ncontext, as an intermediate step. After this \"reasoning step\", the model then\nattends to its own generated knowledge sequence, as well as the dialogue\ncontext, to produce a final response. In detailed experiments, we find that\nsuch a model hallucinates less in knowledge-grounded dialogue tasks, and has\nadvantages in terms of interpretability and modularity. In particular, it can\nbe used to fuse QA and dialogue systems together to enable dialogue agents to\ngive knowledgeable answers, or QA models to give conversational responses in a\nzero-shot setting.", "published": "2021-11-09 15:29:43", "link": "http://arxiv.org/abs/2111.05204v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DataWords: Getting Contrarian with Text, Structured Data and\n  Explanations", "abstract": "Our goal is to build classification models using a combination of free-text\nand structured data. To do this, we represent structured data by text\nsentences, DataWords, so that similar data items are mapped into the same\nsentence. This permits modeling a mixture of text and structured data by using\nonly text-modeling algorithms. Several examples illustrate that it is possible\nto improve text classification performance by first running extraction tools\n(named entity recognition), then converting the output to DataWords, and adding\nthe DataWords to the original text -- before model building and classification.\nThis approach also allows us to produce explanations for inferences in terms of\nboth free text and structured data.", "published": "2021-11-09 19:52:13", "link": "http://arxiv.org/abs/2111.05384v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "American Hate Crime Trends Prediction with Event Extraction", "abstract": "Social media platforms may provide potential space for discourses that\ncontain hate speech, and even worse, can act as a propagation mechanism for\nhate crimes. The FBI's Uniform Crime Reporting (UCR) Program collects hate\ncrime data and releases statistic report yearly. These statistics provide\ninformation in determining national hate crime trends. The statistics can also\nprovide valuable holistic and strategic insight for law enforcement agencies or\njustify lawmakers for specific legislation. However, the reports are mostly\nreleased next year and lag behind many immediate needs. Recent research mainly\nfocuses on hate speech detection in social media text or empirical studies on\nthe impact of a confirmed crime. This paper proposes a framework that first\nutilizes text mining techniques to extract hate crime events from New York\nTimes news, then uses the results to facilitate predicting American\nnational-level and state-level hate crime trends. Experimental results show\nthat our method can significantly enhance the prediction performance compared\nwith time series or regression methods without event-related factors. Our\nframework broadens the methods of national-level and state-level hate crime\ntrends prediction.", "published": "2021-11-09 04:30:20", "link": "http://arxiv.org/abs/2111.04951v1", "categories": ["cs.CL", "cs.AI", "econ.GN", "q-fin.EC", "stat.AP"], "primary_category": "cs.CL"}
{"title": "Multimodal intelligibility of scholarly hypertext: the documentalist's\n  contribution. A required collaboration for serial documentisation in the\n  scientific editorial process", "abstract": "This article shows that the boundaries between the editing and online\npublishingprofessions are losing their strength. In this context it would only\nmake sense that the wayhypertexts are documented be renewed, especially facing\nof the Web's evolution. We arethinking in particular of the trickier scholar\nhypertexts documentation process - specifically inscientific or cultural\ncontexts. The purpose of this article is to demonstrate that, consideringthe\nnumerous branches of the Web, the hypertext enhance of a document of quality\ncan onlybe done through a proper dialogue between authors, editors, and\nbroadcasters. It would satisfythe readership as they could reach the\nappropriate information. It will also be shown that eachactor in this\nauctorial-editorial process would be a gainer. Indeed, a qualitative\nformalizationwork would be coupled with a strong broadcasting scope. Finally,\nwe will point out that thiswork of mediating must be led by an actor of\ninformation-communication, to make the textunderstandable to both humans and\nmachines. This meditative act is designated here under theterm of serial\ndocumentarisation.", "published": "2021-11-09 10:28:01", "link": "http://arxiv.org/abs/2111.05039v1", "categories": ["cs.CL", "cs.IR", "cs.IT", "cs.SI", "math.IT"], "primary_category": "cs.CL"}
{"title": "Joint Neural AEC and Beamforming with Double-Talk Detection", "abstract": "Acoustic echo cancellation (AEC) in full-duplex communication systems\neliminates acoustic feedback. However, nonlinear distortions induced by audio\ndevices, background noise, reverberation, and double-talk reduce the efficiency\nof conventional AEC systems. Several hybrid AEC models were proposed to address\nthis, which use deep learning models to suppress residual echo from standard\nadaptive filtering. This paper proposes deep learning-based joint AEC and\nbeamforming model (JAECBF) building on our previous self-attentive recurrent\nneural network (RNN) beamformer. The proposed network consists of two modules:\n(i) multi-channel neural-AEC, and (ii) joint AEC-RNN beamformer with a\ndouble-talk detection (DTD) that computes time-frequency (T-F) beamforming\nweights. We train the proposed model in an end-to-end approach to eliminate\nbackground noise and echoes from far-end audio devices, which include nonlinear\ndistortions. From experimental evaluations, we find the proposed network\noutperforms other multi-channel AEC and denoising systems in terms of speech\nrecognition rate and overall speech quality.", "published": "2021-11-09 01:53:22", "link": "http://arxiv.org/abs/2111.04904v2", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Ultra-Low Power Keyword Spotting at the Edge", "abstract": "Keyword spotting (KWS) has become an indispensable part of many intelligent\ndevices surrounding us, as audio is one of the most efficient ways of\ninteracting with these devices. The accuracy and performance of KWS solutions\nhave been the main focus of the researchers, and thanks to deep learning,\nsubstantial progress has been made in this domain. However, as the use of KWS\nspreads into IoT devices, energy efficiency becomes a very critical requirement\nbesides the performance. We believe KWS solutions that would seek power\noptimization both in the hardware and the neural network (NN) model\narchitecture are advantageous over many solutions in the literature where\nmostly the architecture side of the problem is considered. In this work, we\ndesigned an optimized KWS CNN model by considering end-to-end energy efficiency\nfor the deployment at MAX78000, an ultra-low-power CNN accelerator. With the\ncombined hardware and model optimization approach, we achieve 96.3\\% accuracy\nfor 12 classes while only consuming 251 uJ per inference. We compare our\nresults with other small-footprint neural network-based KWS solutions in the\nliterature. Additionally, we share the energy consumption of our model in\npower-optimized ARM Cortex-M4F to depict the effectiveness of the chosen\nhardware for the sake of clarity.", "published": "2021-11-09 08:24:36", "link": "http://arxiv.org/abs/2111.04988v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "RAVE: A variational autoencoder for fast and high-quality neural audio\n  synthesis", "abstract": "Deep generative models applied to audio have improved by a large margin the\nstate-of-the-art in many speech and music related tasks. However, as raw\nwaveform modelling remains an inherently difficult task, audio generative\nmodels are either computationally intensive, rely on low sampling rates, are\ncomplicated to control or restrict the nature of possible signals. Among those\nmodels, Variational AutoEncoders (VAE) give control over the generation by\nexposing latent variables, although they usually suffer from low synthesis\nquality. In this paper, we introduce a Realtime Audio Variational autoEncoder\n(RAVE) allowing both fast and high-quality audio waveform synthesis. We\nintroduce a novel two-stage training procedure, namely representation learning\nand adversarial fine-tuning. We show that using a post-training analysis of the\nlatent space allows a direct control between the reconstruction fidelity and\nthe representation compactness. By leveraging a multi-band decomposition of the\nraw waveform, we show that our model is the first able to generate 48kHz audio\nsignals, while simultaneously running 20 times faster than real-time on a\nstandard laptop CPU. We evaluate synthesis quality using both quantitative and\nqualitative subjective experiments and show the superiority of our approach\ncompared to existing models. Finally, we present applications of our model for\ntimbre transfer and signal compression. All of our source code and audio\nexamples are publicly available.", "published": "2021-11-09 09:07:30", "link": "http://arxiv.org/abs/2111.05011v2", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Membership Inference Attacks Against Self-supervised Speech Models", "abstract": "Recently, adapting the idea of self-supervised learning (SSL) on continuous\nspeech has started gaining attention. SSL models pre-trained on a huge amount\nof unlabeled audio can generate general-purpose representations that benefit a\nwide variety of speech processing tasks. Despite their ubiquitous deployment,\nhowever, the potential privacy risks of these models have not been well\ninvestigated. In this paper, we present the first privacy analysis on several\nSSL speech models using Membership Inference Attacks (MIA) under black-box\naccess. The experiment results show that these pre-trained models are\nvulnerable to MIA and prone to membership information leakage with high Area\nUnder the Curve (AUC) in both utterance-level and speaker-level. Furthermore,\nwe also conduct several ablation studies to understand the factors that\ncontribute to the success of MIA.", "published": "2021-11-09 13:00:24", "link": "http://arxiv.org/abs/2111.05113v4", "categories": ["cs.CR", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
{"title": "CAESynth: Real-Time Timbre Interpolation and Pitch Control with\n  Conditional Autoencoders", "abstract": "In this paper, we present a novel audio synthesizer, CAESynth, based on a\nconditional autoencoder. CAESynth synthesizes timbre in real-time by\ninterpolating the reference sounds in their shared latent feature space, while\ncontrolling a pitch independently. We show that training a conditional\nautoencoder based on accuracy in timbre classification together with\nadversarial regularization of pitch content allows timbre distribution in\nlatent space to be more effective and stable for timbre interpolation and pitch\nconditioning. The proposed method is applicable not only to creation of musical\ncues but also to exploration of audio affordance in mixed reality based on\nnovel timbre mixtures with environmental sounds. We demonstrate by experiments\nthat CAESynth achieves smooth and high-fidelity audio synthesis in real-time\nthrough timbre interpolation and independent yet accurate pitch control for\nmusical cues as well as for audio affordance with environmental sound. A Python\nimplementation along with some generated samples are shared online.", "published": "2021-11-09 14:36:31", "link": "http://arxiv.org/abs/2111.05174v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Cross Attentional Audio-Visual Fusion for Dimensional Emotion\n  Recognition", "abstract": "Multimodal analysis has recently drawn much interest in affective computing,\nsince it can improve the overall accuracy of emotion recognition over isolated\nuni-modal approaches. The most effective techniques for multimodal emotion\nrecognition efficiently leverage diverse and complimentary sources of\ninformation, such as facial, vocal, and physiological modalities, to provide\ncomprehensive feature representations. In this paper, we focus on dimensional\nemotion recognition based on the fusion of facial and vocal modalities\nextracted from videos, where complex spatiotemporal relationships may be\ncaptured. Most of the existing fusion techniques rely on recurrent networks or\nconventional attention mechanisms that do not effectively leverage the\ncomplimentary nature of audio-visual (A-V) modalities. We introduce a\ncross-attentional fusion approach to extract the salient features across A-V\nmodalities, allowing for accurate prediction of continuous values of valence\nand arousal. Our new cross-attentional A-V fusion model efficiently leverages\nthe inter-modal relationships. In particular, it computes cross-attention\nweights to focus on the more contributive features across individual\nmodalities, and thereby combine contributive feature representations, which are\nthen fed to fully connected layers for the prediction of valence and arousal.\nThe effectiveness of the proposed approach is validated experimentally on\nvideos from the RECOLA and Fatigue (private) data-sets. Results indicate that\nour cross-attentional A-V fusion model is a cost-effective approach that\noutperforms state-of-the-art fusion approaches. Code is available:\n\\url{https://github.com/praveena2j/Cross-Attentional-AV-Fusion}", "published": "2021-11-09 16:01:56", "link": "http://arxiv.org/abs/2111.05222v2", "categories": ["cs.CV", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.CV"}
