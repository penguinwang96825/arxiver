{"title": "Automated Clinical Coding for Outpatient Departments", "abstract": "Computerised clinical coding approaches aim to automate the process of\nassigning a set of codes to medical records. While there is active research\npushing the state of the art on clinical coding for hospitalized patients, the\noutpatient setting -- where doctors tend to non-hospitalised patients -- is\noverlooked. Although both settings can be formalised as a multi-label\nclassification task, they present unique and distinct challenges, which raises\nthe question of whether the success of inpatient clinical coding approaches\ntranslates to the outpatient setting. This paper is the first to investigate\nhow well state-of-the-art deep learning-based clinical coding approaches work\nin the outpatient setting at hospital scale. To this end, we collect a large\noutpatient dataset comprising over 7 million notes documenting over half a\nmillion patients. We adapt four state-of-the-art clinical coding approaches to\nthis setting and evaluate their potential to assist coders. We find evidence\nthat clinical coding in outpatient settings can benefit from more innovations\nin popular inpatient coding benchmarks. A deeper analysis of the factors\ncontributing to the success -- amount and form of data and choice of document\nrepresentation -- reveals the presence of easy-to-solve examples, the coding of\nwhich can be completely automated with a low error rate.", "published": "2023-12-21 02:28:29", "link": "http://arxiv.org/abs/2312.13533v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Developing Interactive Tourism Planning: A Dialogue Robot System Powered\n  by a Large Language Model", "abstract": "In recent years, large language models (LLMs) have rapidly proliferated and\nhave been utilized in various tasks, including research in dialogue systems. We\naimed to construct a system that not only leverages the flexible conversational\nabilities of LLMs but also their advanced planning capabilities to reduce the\nspeaking load on human interlocutors and efficiently plan trips. Furthermore,\nwe propose a method that divides the complex task of a travel agency into\nmultiple subtasks, managing each as a separate phase to effectively accomplish\nthe task. Our proposed system confirmed a certain level of success by achieving\nfourth place in the Dialogue Robot Competition 2023 preliminaries rounds. We\nreport on the challenges identified through the competition.", "published": "2023-12-21 03:09:38", "link": "http://arxiv.org/abs/2312.13545v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How to Prune Your Language Model: Recovering Accuracy on the \"Sparsity\n  May Cry'' Benchmark", "abstract": "Pruning large language models (LLMs) from the BERT family has emerged as a\nstandard compression benchmark, and several pruning methods have been proposed\nfor this task. The recent ``Sparsity May Cry'' (SMC) benchmark put into\nquestion the validity of all existing methods, exhibiting a more complex setup\nwhere many known pruning methods appear to fail. We revisit the question of\naccurate BERT-pruning during fine-tuning on downstream datasets, and propose a\nset of general guidelines for successful pruning, even on the challenging SMC\nbenchmark. First, we perform a cost-vs-benefits analysis of pruning model\ncomponents, such as the embeddings and the classification head; second, we\nprovide a simple-yet-general way of scaling training, sparsification and\nlearning rate schedules relative to the desired target sparsity; finally, we\ninvestigate the importance of proper parametrization for Knowledge Distillation\nin the context of LLMs. Our simple insights lead to state-of-the-art results,\nboth on classic BERT-pruning benchmarks, as well as on the SMC benchmark,\nshowing that even classic gradual magnitude pruning (GMP) can yield competitive\nresults, with the right approach.", "published": "2023-12-21 03:11:30", "link": "http://arxiv.org/abs/2312.13547v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Argue with Me Tersely: Towards Sentence-Level Counter-Argument\n  Generation", "abstract": "Counter-argument generation -- a captivating area in computational\nlinguistics -- seeks to craft statements that offer opposing views. While most\nresearch has ventured into paragraph-level generation, sentence-level\ncounter-argument generation beckons with its unique constraints and\nbrevity-focused challenges. Furthermore, the diverse nature of\ncounter-arguments poses challenges for evaluating model performance solely\nbased on n-gram-based metrics. In this paper, we present the ArgTersely\nbenchmark for sentence-level counter-argument generation, drawing from a\nmanually annotated dataset from the ChangeMyView debate forum. We also propose\nArg-LlaMA for generating high-quality counter-argument. For better evaluation,\nwe trained a BERT-based evaluator Arg-Judge with human preference data. We\nconducted comparative experiments involving various baselines such as LlaMA,\nAlpaca, GPT-3, and others. The results show the competitiveness of our proposed\nframework and evaluator in counter-argument generation tasks. Code and data are\navailable at https://github.com/amazingljy1206/ArgTersely.", "published": "2023-12-21 06:51:34", "link": "http://arxiv.org/abs/2312.13608v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data Transformation to Construct a Dataset for Generating\n  Entity-Relationship Model from Natural Language", "abstract": "In order to reduce the manual cost of designing ER models, recent approaches\nhave been proposed to address the task of NL2ERM, i.e., automatically\ngenerating entity-relationship (ER) models from natural language (NL)\nutterances such as software requirements. These approaches are typically\nrule-based ones, which rely on rigid heuristic rules; these approaches cannot\ngeneralize well to various linguistic ways of describing the same requirement.\nDespite having better generalization capability than rule-based approaches,\ndeep-learning-based models are lacking for NL2ERM due to lacking a large-scale\ndataset. To address this issue, in this paper, we report our insight that there\nexists a high similarity between the task of NL2ERM and the increasingly\npopular task of text-to-SQL, and propose a data transformation algorithm that\ntransforms the existing data of text-to-SQL into the data of NL2ERM. We apply\nour data transformation algorithm on Spider, one of the most popular\ntext-to-SQL datasets, and we also collect some data entries with different NL\ntypes, to obtain a large-scale NL2ERM dataset. Because NL2ERM can be seen as a\nspecial information extraction (IE) task, we train two state-of-the-art IE\nmodels on our dataset. The experimental results show that both the two models\nachieve high performance and outperform existing baselines.", "published": "2023-12-21 09:45:13", "link": "http://arxiv.org/abs/2312.13694v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploiting Contextual Target Attributes for Target Sentiment\n  Classification", "abstract": "Existing PTLM-based models for TSC can be categorized into two groups: 1)\nfine-tuning-based models that adopt PTLM as the context encoder; 2)\nprompting-based models that transfer the classification task to the text/word\ngeneration task. In this paper, we present a new perspective of leveraging PTLM\nfor TSC: simultaneously leveraging the merits of both language modeling and\nexplicit target-context interactions via contextual target attributes.\nSpecifically, we design the domain- and target-constrained cloze test, which\ncan leverage the PTLMs' strong language modeling ability to generate the given\ntarget's attributes pertaining to the review context. The attributes contain\nthe background and property information of the target, which can help to enrich\nthe semantics of the review context and the target. To exploit the attributes\nfor tackling TSC, we first construct a heterogeneous information graph by\ntreating the attributes as nodes and combining them with (1) the syntax graph\nautomatically produced by the off-the-shelf dependency parser and (2) the\nsemantics graph of the review context, which is derived from the self-attention\nmechanism. Then we propose a heterogeneous information gated graph\nconvolutional network to model the interactions among the attribute\ninformation, the syntactic information, and the contextual information. The\nexperimental results on three benchmark datasets demonstrate the superiority of\nour model, which achieves new state-of-the-art performance.", "published": "2023-12-21 11:45:28", "link": "http://arxiv.org/abs/2312.13766v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Diversifying Knowledge Enhancement of Biomedical Language Models using\n  Adapter Modules and Knowledge Graphs", "abstract": "Recent advances in natural language processing (NLP) owe their success to\npre-training language models on large amounts of unstructured data. Still,\nthere is an increasing effort to combine the unstructured nature of LMs with\nstructured knowledge and reasoning. Particularly in the rapidly evolving field\nof biomedical NLP, knowledge-enhanced language models (KELMs) have emerged as\npromising tools to bridge the gap between large language models and\ndomain-specific knowledge, considering the available biomedical knowledge\ngraphs (KGs) curated by experts over the decades. In this paper, we develop an\napproach that uses lightweight adapter modules to inject structured biomedical\nknowledge into pre-trained language models (PLMs). We use two large KGs, the\nbiomedical knowledge system UMLS and the novel biochemical ontology OntoChem,\nwith two prominent biomedical PLMs, PubMedBERT and BioLinkBERT. The approach\nincludes partitioning knowledge graphs into smaller subgraphs, fine-tuning\nadapter modules for each subgraph, and combining the knowledge in a fusion\nlayer. We test the performance on three downstream tasks: document\nclassification,question answering, and natural language inference. We show that\nour methodology leads to performance improvements in several instances while\nkeeping requirements in computing power low. Finally, we provide a detailed\ninterpretation of the results and report valuable insights for future work.", "published": "2023-12-21 14:26:57", "link": "http://arxiv.org/abs/2312.13881v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "T-Eval: Evaluating the Tool Utilization Capability of Large Language\n  Models Step by Step", "abstract": "Large language models (LLM) have achieved remarkable performance on various\nNLP tasks and are augmented by tools for broader applications. Yet, how to\nevaluate and analyze the tool-utilization capability of LLMs is still\nunder-explored. In contrast to previous works that evaluate models\nholistically, we comprehensively decompose the tool utilization into multiple\nsub-processes, including instruction following, planning, reasoning, retrieval,\nunderstanding, and review. Based on that, we further introduce T-Eval to\nevaluate the tool utilization capability step by step. T-Eval disentangles the\ntool utilization evaluation into several sub-domains along model capabilities,\nfacilitating the inner understanding of both holistic and isolated competency\nof LLMs. We conduct extensive experiments on T-Eval and in-depth analysis of\nvarious LLMs. T-Eval not only exhibits consistency with the outcome-oriented\nevaluation but also provides a more fine-grained analysis of the capabilities\nof LLMs, providing a new perspective in LLM evaluation on tool-utilization\nability. The benchmark will be available at\nhttps://github.com/open-compass/T-Eval.", "published": "2023-12-21 17:02:06", "link": "http://arxiv.org/abs/2312.14033v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Parameter Efficient Tuning Allows Scalable Personalization of LLMs for\n  Text Entry: A Case Study on Abbreviation Expansion", "abstract": "Abbreviation expansion is a strategy used to speed up communication by\nlimiting the amount of typing and using a language model to suggest expansions.\nHere we look at personalizing a Large Language Model's (LLM) suggestions based\non prior conversations to enhance the relevance of predictions, particularly\nwhen the user data is small (~1000 samples). Specifically, we compare\nfine-tuning, prompt-tuning, and retrieval augmented generation of expanded text\nsuggestions for abbreviated inputs. Our case study with a deployed 8B parameter\nLLM on a real user living with ALS, and experiments on movie character\npersonalization indicates that (1) customization may be necessary in some\nscenarios and prompt-tuning generalizes well to those, (2) fine-tuning on\nin-domain data (with as few as 600 samples) still shows some gains, however (3)\nretrieval augmented few-shot selection also outperforms fine-tuning. (4)\nParameter efficient tuning allows for efficient and scalable personalization.\nFor prompt-tuning, we also find that initializing the learned \"soft-prompts\" to\nuser relevant concept tokens leads to higher accuracy than random\ninitialization.", "published": "2023-12-21 22:52:44", "link": "http://arxiv.org/abs/2312.14327v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Decoupling Representation and Knowledge for Few-Shot Intent\n  Classification and Slot Filling", "abstract": "Few-shot intent classification and slot filling are important but challenging\ntasks due to the scarcity of finely labeled data. Therefore, current works\nfirst train a model on source domains with sufficiently labeled data, and then\ntransfer the model to target domains where only rarely labeled data is\navailable. However, experience transferring as a whole usually suffers from\ngaps that exist among source domains and target domains. For instance,\ntransferring domain-specific-knowledge-related experience is difficult. To\ntackle this problem, we propose a new method that explicitly decouples the\ntransferring of general-semantic-representation-related experience and the\ndomain-specific-knowledge-related experience. Specifically, for\ndomain-specific-knowledge-related experience, we design two modules to capture\nintent-slot relation and slot-slot relation respectively. Extensive experiments\non Snips and FewJoint datasets show that our method achieves state-of-the-art\nperformance. The method improves the joint accuracy metric from 27.72% to\n42.20% in the 1-shot setting, and from 46.54% to 60.79% in the 5-shot setting.", "published": "2023-12-21 00:16:21", "link": "http://arxiv.org/abs/2312.13495v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Structure-Aware Path Inference for Neural Finite State Transducers", "abstract": "Neural finite-state transducers (NFSTs) form an expressive family of\nneurosymbolic sequence transduction models. An NFST models each string pair as\nhaving been generated by a latent path in a finite-state transducer. As they\nare deep generative models, both training and inference of NFSTs require\ninference networks that approximate posterior distributions over such latent\nvariables. In this paper, we focus on the resulting challenge of imputing the\nlatent alignment path that explains a given pair of input and output strings\n(e.g., during training). We train three autoregressive approximate models for\namortized inference of the path, which can then be used as proposal\ndistributions for importance sampling. All three models perform lookahead. Our\nmost sophisticated (and novel) model leverages the FST structure to consider\nthe graph of future paths; unfortunately, we find that it loses out to the\nsimpler approaches -- except on an artificial task that we concocted to confuse\nthe simpler approaches.", "published": "2023-12-21 07:03:15", "link": "http://arxiv.org/abs/2312.13614v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Text2Analysis: A Benchmark of Table Question Answering with Advanced\n  Data Analysis and Unclear Queries", "abstract": "Tabular data analysis is crucial in various fields, and large language models\nshow promise in this area. However, current research mostly focuses on\nrudimentary tasks like Text2SQL and TableQA, neglecting advanced analysis like\nforecasting and chart generation. To address this gap, we developed the\nText2Analysis benchmark, incorporating advanced analysis tasks that go beyond\nthe SQL-compatible operations and require more in-depth analysis. We also\ndevelop five innovative and effective annotation methods, harnessing the\ncapabilities of large language models to enhance data quality and quantity.\nAdditionally, we include unclear queries that resemble real-world user\nquestions to test how well models can understand and tackle such challenges.\nFinally, we collect 2249 query-result pairs with 347 tables. We evaluate five\nstate-of-the-art models using three different metrics and the results show that\nour benchmark presents introduces considerable challenge in the field of\ntabular data analysis, paving the way for more advanced research opportunities.", "published": "2023-12-21 08:50:41", "link": "http://arxiv.org/abs/2312.13671v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Understanding Inter-Session Intentions via Complex Logical Reasoning", "abstract": "Understanding user intentions is essential for improving product\nrecommendations, navigation suggestions, and query reformulations. However,\nuser intentions can be intricate, involving multiple sessions and attribute\nrequirements connected by logical operators such as And, Or, and Not. For\ninstance, a user may search for Nike or Adidas running shoes across various\nsessions, with a preference for purple. In another example, a user may have\npurchased a mattress in a previous session and is now looking for a matching\nbed frame without intending to buy another mattress. Existing research on\nsession understanding has not adequately addressed making product or attribute\nrecommendations for such complex intentions. In this paper, we present the task\nof logical session complex query answering (LS-CQA), where sessions are treated\nas hyperedges of items, and we frame the problem of complex intention\nunderstanding as an LS-CQA task on an aggregated hypergraph of sessions, items,\nand attributes. This is a unique complex query answering task with sessions as\nordered hyperedges. We also introduce a new model, the Logical Session Graph\nTransformer (LSGT), which captures interactions among items across different\nsessions and their logical connections using a transformer structure. We\nanalyze the expressiveness of LSGT and prove the permutation invariance of the\ninputs for the logical operators. By evaluating LSGT on three datasets, we\ndemonstrate that it achieves state-of-the-art results.", "published": "2023-12-21 14:03:30", "link": "http://arxiv.org/abs/2312.13866v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Evaluating Task-oriented Dialogue Systems: A Systematic Review of\n  Measures, Constructs and their Operationalisations", "abstract": "This review gives an extensive overview of evaluation methods for\ntask-oriented dialogue systems, paying special attention to practical\napplications of dialogue systems, for example for customer service. The review\n(1) provides an overview of the used constructs and metrics in previous work,\n(2) discusses challenges in the context of dialogue system evaluation and (3)\ndevelops a research agenda for the future of dialogue system evaluation. We\nconducted a systematic review of four databases (ACL, ACM, IEEE and Web of\nScience), which after screening resulted in 122 studies. Those studies were\ncarefully analysed for the constructs and methods they proposed for evaluation.\nWe found a wide variety in both constructs and methods. Especially the\noperationalisation is not always clearly reported. Newer developments\nconcerning large language models are discussed in two contexts: to power\ndialogue systems and to use in the evaluation process. We hope that future work\nwill take a more critical approach to the operationalisation and specification\nof the used constructs. To work towards this aim, this review ends with\nrecommendations for evaluation and suggestions for outstanding questions.", "published": "2023-12-21 14:15:46", "link": "http://arxiv.org/abs/2312.13871v2", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Structured Probabilistic Coding", "abstract": "This paper presents a new supervised representation learning framework,\nnamely structured probabilistic coding (SPC), to learn compact and informative\nrepresentations from input related to the target task. SPC is an encoder-only\nprobabilistic coding technology with a structured regularization from the\ntarget space. It can enhance the generalization ability of pre-trained language\nmodels for better language understanding. Specifically, our probabilistic\ncoding simultaneously performs information encoding and task prediction in one\nmodule to more fully utilize the effective information from input data. It uses\nvariational inference in the output space to reduce randomness and uncertainty.\nBesides, to better control the learning process of probabilistic\nrepresentations, a structured regularization is proposed to promote uniformity\nacross classes in the latent space. With the regularization term, SPC can\npreserve the Gaussian structure of the latent code and achieve better coverage\nof the hidden space with class uniformly. Experimental results on 12 natural\nlanguage understanding tasks demonstrate that our SPC effectively improves the\nperformance of pre-trained language models for classification and regression.\nExtensive experiments show that SPC can enhance the generalization capability,\nrobustness to label noise, and clustering quality of output representations.", "published": "2023-12-21 15:28:02", "link": "http://arxiv.org/abs/2312.13933v5", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Typhoon: Thai Large Language Models", "abstract": "Typhoon is a series of Thai large language models (LLMs) developed\nspecifically for the Thai language. This technical report presents challenges\nand insights in developing Thai LLMs, including data preparation, pretraining,\ninstruction-tuning, and evaluation. As one of the challenges of low-resource\nlanguages is the amount of pretraining data, we apply continual training to\ntransfer existing world knowledge from a strong LLM. To evaluate the Thai\nknowledge encapsulated in each model from the pretraining stage, we develop\nThaiExam, a benchmark based on examinations for high-school students and\ninvestment professionals in Thailand. In addition, we fine-tune Typhoon to\nfollow Thai instructions, and we evaluate instruction-tuned models on Thai\ninstruction datasets as well as translation, summarization, and\nquestion-answering tasks. Experimental results on a suite of Thai benchmarks\nshow that Typhoon outperforms all open-source Thai language models, and its\nperformance is on par with GPT-3.5 in Thai while having only 7 billion\nparameters and being 2.62 times more efficient in tokenizing Thai text.", "published": "2023-12-21 15:38:41", "link": "http://arxiv.org/abs/2312.13951v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ChatGPT as a commenter to the news: can LLMs generate human-like\n  opinions?", "abstract": "ChatGPT, GPT-3.5, and other large language models (LLMs) have drawn\nsignificant attention since their release, and the abilities of these models\nhave been investigated for a wide variety of tasks. In this research we\ninvestigate to what extent GPT-3.5 can generate human-like comments on Dutch\nnews articles. We define human likeness as `not distinguishable from human\ncomments', approximated by the difficulty of automatic classification between\nhuman and GPT comments. We analyze human likeness across multiple prompting\ntechniques. In particular, we utilize zero-shot, few-shot and context prompts,\nfor two generated personas. We found that our fine-tuned BERT models can easily\ndistinguish human-written comments from GPT-3.5 generated comments, with none\nof the used prompting methods performing noticeably better. We further analyzed\nthat human comments consistently showed higher lexical diversity than\nGPT-generated comments. This indicates that although generative LLMs can\ngenerate fluent text, their capability to create human-like opinionated\ncomments is still limited.", "published": "2023-12-21 15:46:36", "link": "http://arxiv.org/abs/2312.13961v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Benchmarking and Defending Against Indirect Prompt Injection Attacks on\n  Large Language Models", "abstract": "The integration of large language models with external content has enabled\napplications such as Microsoft Copilot but also introduced vulnerabilities to\nindirect prompt injection attacks. In these attacks, malicious instructions\nembedded within external content can manipulate LLM outputs, causing deviations\nfrom user expectations. To address this critical yet under-explored issue, we\nintroduce the first benchmark for indirect prompt injection attacks, named\nBIPIA, to assess the risk of such vulnerabilities. Using BIPIA, we evaluate\nexisting LLMs and find them universally vulnerable. Our analysis identifies two\nkey factors contributing to their success: LLMs' inability to distinguish\nbetween informational context and actionable instructions, and their lack of\nawareness in avoiding the execution of instructions within external content.\nBased on these findings, we propose two novel defense mechanisms-boundary\nawareness and explicit reminder-to address these vulnerabilities in both\nblack-box and white-box settings. Extensive experiments demonstrate that our\nblack-box defense provides substantial mitigation, while our white-box defense\nreduces the attack success rate to near-zero levels, all while preserving the\noutput quality of LLMs. We hope this work inspires further research into\nsecuring LLM applications and fostering their safe and reliable use.", "published": "2023-12-21 01:08:39", "link": "http://arxiv.org/abs/2312.14197v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Illuminating the Black Box: A Psychometric Investigation into the\n  Multifaceted Nature of Large Language Models", "abstract": "This study explores the idea of AI Personality or AInality suggesting that\nLarge Language Models (LLMs) exhibit patterns similar to human personalities.\nAssuming that LLMs share these patterns with humans, we investigate using\nhuman-centered psychometric tests such as the Myers-Briggs Type Indicator\n(MBTI), Big Five Inventory (BFI), and Short Dark Triad (SD3) to identify and\nconfirm LLM personality types. By introducing role-play prompts, we demonstrate\nthe adaptability of LLMs, showing their ability to switch dynamically between\ndifferent personality types. Using projective tests, such as the Washington\nUniversity Sentence Completion Test (WUSCT), we uncover hidden aspects of LLM\npersonalities that are not easily accessible through direct questioning.\nProjective tests allowed for a deep exploration of LLMs cognitive processes and\nthought patterns and gave us a multidimensional view of AInality. Our machine\nlearning analysis revealed that LLMs exhibit distinct AInality traits and\nmanifest diverse personality types, demonstrating dynamic shifts in response to\nexternal instructions. This study pioneers the application of projective tests\non LLMs, shedding light on their diverse and adaptable AInality traits.", "published": "2023-12-21 04:57:21", "link": "http://arxiv.org/abs/2312.14202v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SimLM: Can Language Models Infer Parameters of Physical Systems?", "abstract": "Several machine learning methods aim to learn or reason about complex\nphysical systems. A common first-step towards reasoning is to infer system\nparameters from observations of its behavior. In this paper, we investigate the\nperformance of Large Language Models (LLMs) at performing parameter inference\nin the context of physical systems. Our experiments suggest that they are not\ninherently suited to this task, even for simple systems. We propose a promising\ndirection of exploration, which involves the use of physical simulators to\naugment the context of LLMs. We assess and compare the performance of different\nLLMs on a simple example with and without access to physical simulation.", "published": "2023-12-21 12:05:19", "link": "http://arxiv.org/abs/2312.14215v2", "categories": ["cs.CL", "cs.AI", "I.2.7; I.6"], "primary_category": "cs.CL"}
{"title": "Context-aware Decoding Reduces Hallucination in Query-focused\n  Summarization", "abstract": "Query-focused summarization (QFS) aims to provide a summary of a single\ndocument/multi documents that can satisfy the information needs of a given\nquery. It is useful for various real-world applications, such as abstractive\nsnippet generation or more recent retrieval augmented generation (RAG). A\nprototypical QFS pipeline consists of a retriever (sparse or dense retrieval)\nand a generator (usually a large language model). However, applying large\nlanguage models (LLM) potentially leads to hallucinations, especially when the\nevidence contradicts the prior belief of LLMs. There has been growing interest\nin developing new decoding methods to improve generation quality and reduce\nhallucination. In this work, we conduct a large-scale reproducibility study on\none recently proposed decoding method -- Context-aware Decoding (CAD). In\naddition to replicating CAD's experiments on news summarization datasets, we\ninclude experiments on QFS datasets, and conduct more rigorous analysis on\ncomputational complexity and hyperparameter sensitivity. Experiments with eight\ndifferent language models show that performance-wise, CAD improves QFS quality\nby (1) reducing factuality errors/hallucinations while (2) mostly retaining the\nmatch of lexical patterns, measured by ROUGE scores, while also at a cost of\nincreased inference-time FLOPs and reduced decoding speed. The code\nimplementation based on Huggingface Library is made available\nhttps://github.com/zhichaoxu-shufe/context-aware-decoding-qfs", "published": "2023-12-21 23:42:13", "link": "http://arxiv.org/abs/2312.14335v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "The Truth is in There: Improving Reasoning in Language Models with\n  Layer-Selective Rank Reduction", "abstract": "Transformer-based Large Language Models (LLMs) have become a fixture in\nmodern machine learning. Correspondingly, significant resources are allocated\ntowards research that aims to further advance this technology, typically\nresulting in models of increasing size that are trained on increasing amounts\nof data. This work, however, demonstrates the surprising result that it is\noften possible to significantly improve the performance of LLMs by selectively\nremoving higher-order components of their weight matrices. This simple\nintervention, which we call LAyer-SElective Rank reduction (LASER), can be done\non a model after training has completed, and requires no additional parameters\nor data. We show extensive experiments demonstrating the generality of this\nfinding across language models and datasets, and provide in-depth analyses\noffering insights into both when LASER is effective and the mechanism by which\nit operates.", "published": "2023-12-21 03:51:08", "link": "http://arxiv.org/abs/2312.13558v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Speech Translation with Large Language Models: An Industrial Practice", "abstract": "Given the great success of large language models (LLMs) across various tasks,\nin this paper, we introduce LLM-ST, a novel and effective speech translation\nmodel constructed upon a pre-trained LLM. By integrating the large language\nmodel (LLM) with a speech encoder and employing multi-task instruction tuning,\nLLM-ST can produce accurate timestamped transcriptions and translations, even\nfrom long audio inputs. Furthermore, our findings indicate that the\nimplementation of Chain-of-Thought (CoT) prompting can yield advantages in the\ncontext of LLM-ST. Through rigorous experimentation on English and Chinese\ndatasets, we showcase the exceptional performance of LLM-ST, establishing a new\nbenchmark in the field of speech translation. Demo:\nhttps://speechtranslation.github.io/llm-st/.", "published": "2023-12-21 05:32:49", "link": "http://arxiv.org/abs/2312.13585v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Towards More Faithful Natural Language Explanation Using Multi-Level\n  Contrastive Learning in VQA", "abstract": "Natural language explanation in visual question answer (VQA-NLE) aims to\nexplain the decision-making process of models by generating natural language\nsentences to increase users' trust in the black-box systems. Existing post-hoc\nmethods have achieved significant progress in obtaining a plausible\nexplanation. However, such post-hoc explanations are not always aligned with\nhuman logical inference, suffering from the issues on: 1) Deductive\nunsatisfiability, the generated explanations do not logically lead to the\nanswer; 2) Factual inconsistency, the model falsifies its counterfactual\nexplanation for answers without considering the facts in images; and 3)\nSemantic perturbation insensitivity, the model can not recognize the semantic\nchanges caused by small perturbations. These problems reduce the faithfulness\nof explanations generated by models. To address the above issues, we propose a\nnovel self-supervised \\textbf{M}ulti-level \\textbf{C}ontrastive\n\\textbf{L}earning based natural language \\textbf{E}xplanation model (MCLE) for\nVQA with semantic-level, image-level, and instance-level factual and\ncounterfactual samples. MCLE extracts discriminative features and aligns the\nfeature spaces from explanations with visual question and answer to generate\nmore consistent explanations. We conduct extensive experiments, ablation\nanalysis, and case study to demonstrate the effectiveness of our method on two\nVQA-NLE benchmarks.", "published": "2023-12-21 05:51:55", "link": "http://arxiv.org/abs/2312.13594v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Compositional Zero-Shot Learning for Attribute-Based Object Reference in\n  Human-Robot Interaction", "abstract": "Language-enabled robots have been widely studied over the past years to\nenable natural human-robot interaction and teaming in various real-world\napplications. Language-enabled robots must be able to comprehend referring\nexpressions to identify a particular object from visual perception using a set\nof referring attributes extracted from natural language. However, visual\nobservations of an object may not be available when it is referred to, and the\nnumber of objects and attributes may also be unbounded in open worlds. To\naddress the challenges, we implement an attribute-based compositional zero-shot\nlearning method that uses a list of attributes to perform referring expression\ncomprehension in open worlds. We evaluate the approach on two datasets\nincluding the MIT-States and the Clothing 16K. The preliminary experimental\nresults show that our implemented approach allows a robot to correctly identify\nthe objects referred to by human commands.", "published": "2023-12-21 08:29:41", "link": "http://arxiv.org/abs/2312.13655v1", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.RO"}
{"title": "A Semantic Space is Worth 256 Language Descriptions: Make Stronger\n  Segmentation Models with Descriptive Properties", "abstract": "This paper introduces ProLab, a novel approach using property-level label\nspace for creating strong interpretable segmentation models. Instead of relying\nsolely on category-specific annotations, ProLab uses descriptive properties\ngrounded in common sense knowledge for supervising segmentation models. It is\nbased on two core designs. First, we employ Large Language Models (LLMs) and\ncarefully crafted prompts to generate descriptions of all involved categories\nthat carry meaningful common sense knowledge and follow a structured format.\nSecond, we introduce a description embedding model preserving semantic\ncorrelation across descriptions and then cluster them into a set of descriptive\nproperties (e.g., 256) using K-Means. These properties are based on\ninterpretable common sense knowledge consistent with theories of human\nrecognition. We empirically show that our approach makes segmentation models\nperform stronger on five classic benchmarks (e.g., ADE20K, COCO-Stuff, Pascal\nContext, Cityscapes, and BDD). Our method also shows better scalability with\nextended training steps than category-level supervision. Our interpretable\nsegmentation framework also emerges with the generalization ability to segment\nout-of-domain or unknown categories using only in-domain descriptive\nproperties. Code is available at https://github.com/lambert-x/ProLab.", "published": "2023-12-21 11:43:41", "link": "http://arxiv.org/abs/2312.13764v3", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "On Task Performance and Model Calibration with Supervised and\n  Self-Ensembled In-Context Learning", "abstract": "Following the standard supervised fine-tuning (SFT) paradigm, in-context\nlearning (ICL) has become an efficient approach propelled by the recent\nadvancements in large language models (LLMs), yielding promising performance\nacross various tasks in few-shot data setups. However, both paradigms are prone\nto suffer from the critical problem of overconfidence (i.e., miscalibration),\nespecially in such limited data setups. In this work, we deliver an in-depth\nanalysis of the behavior across different choices of learning methods from the\nperspective of both performance and calibration, as well as their interplay.\nThrough extensive controlled experiments, we find that simultaneous gains for\nboth task performance and calibration are difficult to achieve, and the problem\nof miscalibration exists across all learning methods in low-resource scenarios.\nTo address this challenging trade-off between performance and calibration, we\nthen investigate the potential of self-ensembling techniques applied at\ndifferent modeling stages (e.g., variations of in-context examples or\nvariations in prompts or different ensembling strategies). We justify the\nfeasibility of self-ensembling on SFT in addition to ICL, to make the\npredictions more calibrated and have comparable or even better performance. Our\nwork sheds light on which learning paradigm to choose and how to enhance both\ntask performance and calibration of LLMs.", "published": "2023-12-21 11:55:10", "link": "http://arxiv.org/abs/2312.13772v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Team Flow at DRC2023: Building Common Ground and Text-based Turn-taking\n  in a Travel Agent Spoken Dialogue System", "abstract": "At the Dialogue Robot Competition 2023 (DRC2023), which was held to improve\nthe capability of dialogue robots, our team developed a system that could build\ncommon ground and take more natural turns based on user utterance texts. Our\nsystem generated queries for sightseeing spot searches using the common ground\nand engaged in dialogue while waiting for user comprehension.", "published": "2023-12-21 13:08:09", "link": "http://arxiv.org/abs/2312.13816v1", "categories": ["cs.CL", "cs.AI", "cs.RO"], "primary_category": "cs.CL"}
{"title": "Capture the Flag: Uncovering Data Insights with Large Language Models", "abstract": "The extraction of a small number of relevant insights from vast amounts of\ndata is a crucial component of data-driven decision-making. However,\naccomplishing this task requires considerable technical skills, domain\nexpertise, and human labor. This study explores the potential of using Large\nLanguage Models (LLMs) to automate the discovery of insights in data,\nleveraging recent advances in reasoning and code generation techniques. We\npropose a new evaluation methodology based on a \"capture the flag\" principle,\nmeasuring the ability of such models to recognize meaningful and pertinent\ninformation (flags) in a dataset. We further propose two proof-of-concept\nagents, with different inner workings, and compare their ability to capture\nsuch flags in a real-world sales dataset. While the work reported here is\npreliminary, our results are sufficiently interesting to mandate future\nexploration by the community.", "published": "2023-12-21 14:20:06", "link": "http://arxiv.org/abs/2312.13876v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "EmphAssess : a Prosodic Benchmark on Assessing Emphasis Transfer in\n  Speech-to-Speech Models", "abstract": "We introduce EmphAssess, a prosodic benchmark designed to evaluate the\ncapability of speech-to-speech models to encode and reproduce prosodic\nemphasis. We apply this to two tasks: speech resynthesis and speech-to-speech\ntranslation. In both cases, the benchmark evaluates the ability of the model to\nencode emphasis in the speech input and accurately reproduce it in the output,\npotentially across a change of speaker and language. As part of the evaluation\npipeline, we introduce EmphaClass, a new model that classifies emphasis at the\nframe or word level.", "published": "2023-12-21 17:47:33", "link": "http://arxiv.org/abs/2312.14069v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Shai: A large language model for asset management", "abstract": "This paper introduces \"Shai\" a 10B level large language model specifically\ndesigned for the asset management industry, built upon an open-source\nfoundational model. With continuous pre-training and fine-tuning using a\ntargeted corpus, Shai demonstrates enhanced performance in tasks relevant to\nits domain, outperforming baseline models. Our research includes the\ndevelopment of an innovative evaluation framework, which integrates\nprofessional qualification exams, tailored tasks, open-ended question\nanswering, and safety assessments, to comprehensively assess Shai's\ncapabilities. Furthermore, we discuss the challenges and implications of\nutilizing large language models like GPT-4 for performance assessment in asset\nmanagement, suggesting a combination of automated evaluation and human\njudgment. Shai's development, showcasing the potential and versatility of\n10B-level large language models in the financial sector with significant\nperformance and modest computational requirements, hopes to provide practical\ninsights and methodologies to assist industry peers in their similar endeavors.", "published": "2023-12-21 05:08:57", "link": "http://arxiv.org/abs/2312.14203v1", "categories": ["q-fin.PM", "cs.CL", "cs.LG"], "primary_category": "q-fin.PM"}
{"title": "Experimenting with Large Language Models and vector embeddings in NASA\n  SciX", "abstract": "Open-source Large Language Models enable projects such as NASA SciX (i.e.,\nNASA ADS) to think out of the box and try alternative approaches for\ninformation retrieval and data augmentation, while respecting data copyright\nand users' privacy. However, when large language models are directly prompted\nwith questions without any context, they are prone to hallucination. At NASA\nSciX we have developed an experiment where we created semantic vectors for our\nlarge collection of abstracts and full-text content, and we designed a prompt\nsystem to ask questions using contextual chunks from our system. Based on a\nnon-systematic human evaluation, the experiment shows a lower degree of\nhallucination and better responses when using Retrieval Augmented Generation.\nFurther exploration is required to design new features and data augmentation\nprocesses at NASA SciX that leverages this technology while respecting the high\nlevel of trust and quality that the project holds.", "published": "2023-12-21 10:19:58", "link": "http://arxiv.org/abs/2312.14211v1", "categories": ["cs.CL", "astro-ph.IM", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Deep de Finetti: Recovering Topic Distributions from Large Language\n  Models", "abstract": "Large language models (LLMs) can produce long, coherent passages of text,\nsuggesting that LLMs, although trained on next-word prediction, must represent\nthe latent structure that characterizes a document. Prior work has found that\ninternal representations of LLMs encode one aspect of latent structure, namely\nsyntax; here we investigate a complementary aspect, namely the document's topic\nstructure. We motivate the hypothesis that LLMs capture topic structure by\nconnecting LLM optimization to implicit Bayesian inference. De Finetti's\ntheorem shows that exchangeable probability distributions can be represented as\na mixture with respect to a latent generating distribution. Although text is\nnot exchangeable at the level of syntax, exchangeability is a reasonable\nstarting assumption for topic structure. We thus hypothesize that predicting\nthe next token in text will lead LLMs to recover latent topic distributions. We\nexamine this hypothesis using Latent Dirichlet Allocation (LDA), an\nexchangeable probabilistic topic model, as a target, and we show that the\nrepresentations formed by LLMs encode both the topics used to generate\nsynthetic data and those used to explain natural corpus data.", "published": "2023-12-21 16:44:39", "link": "http://arxiv.org/abs/2312.14226v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML", "I.2.6; I.2.7"], "primary_category": "cs.CL"}
{"title": "Characterizing and Classifying Developer Forum Posts with their\n  Intentions", "abstract": "With the rapid growth of the developer community, the amount of posts on\nonline technical forums has been growing rapidly, which poses difficulties for\nusers to filter useful posts and find important information. Tags provide a\nconcise feature dimension for users to locate their interested posts and for\nsearch engines to index the most relevant posts according to the queries.\nHowever, most tags are only focused on the technical perspective (e.g., program\nlanguage, platform, tool). In most cases, forum posts in online developer\ncommunities reveal the author's intentions to solve a problem, ask for advice,\nshare information, etc. The modeling of the intentions of posts can provide an\nextra dimension to the current tag taxonomy. By referencing previous studies\nand learning from industrial perspectives, we create a refined taxonomy for the\nintentions of technical forum posts. Through manual labeling and analysis on a\nsampled post dataset extracted from online forums, we understand the relevance\nbetween the constitution of posts (code, error messages) and their intentions.\nFurthermore, inspired by our manual study, we design a pre-trained\ntransformer-based model to automatically predict post intentions. The best\nvariant of our intention prediction framework, which achieves a Micro F1-score\nof 0.589, Top 1-3 accuracy of 62.6% to 87.8%, and an average AUC of 0.787,\noutperforms the state-of-the-art baseline approach. Our characterization and\nautomated classification of forum posts regarding their intentions may help\nforum maintainers or third-party tool developers improve the organization and\nretrieval of posts on technical forums. We have released our annotated dataset\nand codes in our supplementary material package.", "published": "2023-12-21 20:17:01", "link": "http://arxiv.org/abs/2312.14279v2", "categories": ["cs.SE", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "Exploiting Novel GPT-4 APIs", "abstract": "Language model attacks typically assume one of two extreme threat models:\nfull white-box access to model weights, or black-box access limited to a text\ngeneration API. However, real-world APIs are often more flexible than just text\ngeneration: these APIs expose \"gray-box\" access leading to new threat vectors.\nTo explore this, we red-team three new functionalities exposed in the GPT-4\nAPIs: fine-tuning, function calling and knowledge retrieval. We find that\nfine-tuning a model on as few as 15 harmful examples or 100 benign examples can\nremove core safeguards from GPT-4, enabling a range of harmful outputs.\nFurthermore, we find that GPT-4 Assistants readily divulge the function call\nschema and can be made to execute arbitrary function calls. Finally, we find\nthat knowledge retrieval can be hijacked by injecting instructions into\nretrieval documents. These vulnerabilities highlight that any additions to the\nfunctionality exposed by an API can create new vulnerabilities.", "published": "2023-12-21 21:22:41", "link": "http://arxiv.org/abs/2312.14302v2", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.CR"}
{"title": "L-TUNING: Synchronized Label Tuning for Prompt and Prefix in LLMs", "abstract": "Efficiently fine-tuning Large Language Models (LLMs) for specific tasks\npresents a considerable challenge in natural language processing. Traditional\nmethods, like prompt or prefix tuning, typically rely on arbitrary tokens for\ntraining, leading to prolonged training times and generalized token use across\nvarious class labels. To address these issues, this paper introduces L-Tuning,\nan efficient fine-tuning approach designed for classification tasks within the\nNatural Language Inference (NLI) framework. Diverging from conventional\nmethods, L-Tuning focuses on the fine-tuning of label tokens processed through\na pre-trained LLM, thereby harnessing its pre-existing semantic knowledge. This\ntechnique not only improves the fine-tuning accuracy and efficiency but also\nfacilitates the generation of distinct label embeddings for each class,\nenhancing the model's training nuance. Our experimental results indicate a\nsignificant improvement in training efficiency and classification accuracy with\nL-Tuning compared to traditional approaches, marking a promising advancement in\nfine-tuning LLMs for complex language tasks.", "published": "2023-12-21 01:47:49", "link": "http://arxiv.org/abs/2402.01643v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Domain-Specific Fine-Tuning of Large Language Models for Interactive\n  Robot Programming", "abstract": "Industrial robots are applied in a widening range of industries, but robot\nprogramming mostly remains a task limited to programming experts. We propose a\nnatural language-based assistant for programming of advanced, industrial\nrobotic applications and investigate strategies for domain-specific fine-tuning\nof foundation models with limited data and compute.", "published": "2023-12-21 14:51:04", "link": "http://arxiv.org/abs/2312.13905v2", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.HC", "cs.LG", "68T40", "I.2.9; I.2.5; I.2.6; I.2.7"], "primary_category": "cs.RO"}
{"title": "From Bytes to Biases: Investigating the Cultural Self-Perception of\n  Large Language Models", "abstract": "Large language models (LLMs) are able to engage in natural-sounding\nconversations with humans, showcasing unprecedented capabilities for\ninformation retrieval and automated decision support. They have disrupted\nhuman-technology interaction and the way businesses operate. However,\ntechnologies based on generative artificial intelligence (GenAI) are known to\nhallucinate, misinform, and display biases introduced by the massive datasets\non which they are trained. Existing research indicates that humans may\nunconsciously internalize these biases, which can persist even after they stop\nusing the programs. This study explores the cultural self-perception of LLMs by\nprompting ChatGPT (OpenAI) and Bard (Google) with value questions derived from\nthe GLOBE project. The findings reveal that their cultural self-perception is\nmost closely aligned with the values of English-speaking countries and\ncountries characterized by sustained economic competitiveness. Recognizing the\ncultural biases of LLMs and understanding how they work is crucial for all\nmembers of society because one does not want the black box of artificial\nintelligence to perpetuate bias in humans, who might, in turn, inadvertently\ncreate and train even more biased algorithms.", "published": "2023-12-21 22:50:14", "link": "http://arxiv.org/abs/2312.17256v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multi-Level Knowledge Distillation for Speech Emotion Recognition in\n  Noisy Conditions", "abstract": "Speech emotion recognition (SER) performance deteriorates significantly in\nthe presence of noise, making it challenging to achieve competitive performance\nin noisy conditions. To this end, we propose a multi-level knowledge\ndistillation (MLKD) method, which aims to transfer the knowledge from a teacher\nmodel trained on clean speech to a simpler student model trained on noisy\nspeech. Specifically, we use clean speech features extracted by the wav2vec-2.0\nas the learning goal and train the distil wav2vec-2.0 to approximate the\nfeature extraction ability of the original wav2vec-2.0 under noisy conditions.\nFurthermore, we leverage the multi-level knowledge of the original wav2vec-2.0\nto supervise the single-level output of the distil wav2vec-2.0. We evaluate the\neffectiveness of our proposed method by conducting extensive experiments using\nfive types of noise-contaminated speech on the IEMOCAP dataset, which show\npromising results compared to state-of-the-art models.", "published": "2023-12-21 03:49:46", "link": "http://arxiv.org/abs/2312.13556v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "kNN-CTC: Enhancing ASR via Retrieval of CTC Pseudo Labels", "abstract": "The success of retrieval-augmented language models in various natural\nlanguage processing (NLP) tasks has been constrained in automatic speech\nrecognition (ASR) applications due to challenges in constructing fine-grained\naudio-text datastores. This paper presents kNN-CTC, a novel approach that\novercomes these challenges by leveraging Connectionist Temporal Classification\n(CTC) pseudo labels to establish frame-level audio-text key-value pairs,\ncircumventing the need for precise ground truth alignments. We further\nintroduce a skip-blank strategy, which strategically ignores CTC blank frames,\nto reduce datastore size. kNN-CTC incorporates a k-nearest neighbors retrieval\nmechanism into pre-trained CTC ASR systems, achieving significant improvements\nin performance. By incorporating a k-nearest neighbors retrieval mechanism into\npre-trained CTC ASR systems and leveraging a fine-grained, pruned datastore,\nkNN-CTC consistently achieves substantial improvements in performance under\nvarious experimental settings. Our code is available at\nhttps://github.com/NKU-HLT/KNN-CTC.", "published": "2023-12-21 04:08:14", "link": "http://arxiv.org/abs/2312.13560v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "BrainTalker: Low-Resource Brain-to-Speech Synthesis with Transfer\n  Learning using Wav2Vec 2.0", "abstract": "Decoding spoken speech from neural activity in the brain is a fast-emerging\nresearch topic, as it could enable communication for people who have\ndifficulties with producing audible speech. For this task, electrocorticography\n(ECoG) is a common method for recording brain activity with high temporal\nresolution and high spatial precision. However, due to the risky surgical\nprocedure required for obtaining ECoG recordings, relatively little of this\ndata has been collected, and the amount is insufficient to train a neural\nnetwork-based Brain-to-Speech (BTS) system. To address this problem, we propose\nBrainTalker-a novel BTS framework that generates intelligible spoken speech\nfrom ECoG signals under extremely low-resource scenarios. We apply a transfer\nlearning approach utilizing a pre-trained self supervised model, Wav2Vec 2.0.\nSpecifically, we train an encoder module to map ECoG signals to latent\nembeddings that match Wav2Vec 2.0 representations of the corresponding spoken\nspeech. These embeddings are then transformed into mel-spectrograms using\nstacked convolutional and transformer-based layers, which are fed into a neural\nvocoder to synthesize speech waveform. Experimental results demonstrate our\nproposed framework achieves outstanding performance in terms of subjective and\nobjective metrics, including a Pearson correlation coefficient of 0.9 between\ngenerated and ground truth mel spectrograms. We share publicly available Demos\nand Code.", "published": "2023-12-21 06:10:02", "link": "http://arxiv.org/abs/2312.13600v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Style Modeling for Multi-Speaker Articulation-to-Speech", "abstract": "In this paper, we propose a neural articulation-to-speech (ATS) framework\nthat synthesizes high-quality speech from articulatory signal in a\nmulti-speaker situation. Most conventional ATS approaches only focus on\nmodeling contextual information of speech from a single speaker's articulatory\nfeatures. To explicitly represent each speaker's speaking style as well as the\ncontextual information, our proposed model estimates style embeddings, guided\nfrom the essential speech style attributes such as pitch and energy. We adopt\nconvolutional layers and transformer-based attention layers for our model to\nfully utilize both local and global information of articulatory signals,\nmeasured by electromagnetic articulography (EMA). Our model significantly\nimproves the quality of synthesized speech compared to the baseline in terms of\nobjective and subjective measurements in the Haskins dataset.", "published": "2023-12-21 06:28:38", "link": "http://arxiv.org/abs/2312.13603v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Blind Localization of Room Reflections with Application to Spatial Audio", "abstract": "Blind estimation of early room reflections, without knowledge of the room\nimpulse response, holds substantial value. The FF-PHALCOR (Frequency Focusing\nPHase ALigned CORrelation), method was recently developed for this objective,\nextending the original PHALCOR method from spherical to arbitrary arrays.\nHowever, previous studies only compared the two methods under limited\nconditions without presenting a comprehensive performance analysis. This study\npresents an advance by evaluating the performance of the algorithm in a wider\nrange of conditions. Additionally, performance in terms of perception is\ninvestigated through a listening test. This test involves synthesizing room\nimpulse responses from known room acoustics parameters and replacing the early\nreflections with the estimated ones. The importance of the estimated\nreflections for spatial perception is demonstrated through this test.", "published": "2023-12-21 10:18:25", "link": "http://arxiv.org/abs/2312.13707v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "BAE-Net: A Low complexity and high fidelity Bandwidth-Adaptive neural\n  network for speech super-resolution", "abstract": "Speech bandwidth extension (BWE) has demonstrated promising performance in\nenhancing the perceptual speech quality in real communication systems. Most\nexisting BWE researches primarily focus on fixed upsampling ratios,\ndisregarding the fact that the effective bandwidth of captured audio may\nfluctuate frequently due to various capturing devices and transmission\nconditions. In this paper, we propose a novel streaming adaptive bandwidth\nextension solution dubbed BAE-Net, which is suitable to handle the\nlow-resolution speech with unknown and varying effective bandwidth. To address\nthe challenges of recovering both the high-frequency magnitude and phase speech\ncontent blindly, we devise a dual-stream architecture that incorporates the\nmagnitude inpainting and phase refinement. For potential applications on edge\ndevices, this paper also introduces BAE-NET-lite, which is a lightweight,\nstreaming and efficient framework. Quantitative results demonstrate the\nsuperiority of BAE-Net in terms of both performance and computational\nefficiency when compared with existing state-of-the-art BWE methods.", "published": "2023-12-21 10:39:05", "link": "http://arxiv.org/abs/2312.13722v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Self-Supervised Adaptive AV Fusion Module for Pre-Trained ASR Models", "abstract": "Automatic speech recognition (ASR) has reached a level of accuracy in recent\nyears, that even outperforms humans in transcribing speech to text.\nNevertheless, all current ASR approaches show a certain weakness against\nambient noise. To reduce this weakness, audio-visual speech recognition (AVSR)\napproaches additionally consider visual information from lip movements for\ntranscription. This additional modality increases the computational cost for\ntraining models from scratch. We propose an approach, that builds on a\npre-trained ASR model and extends it with an adaptive upstream module, that\nfuses audio and visual information. Since we do not need to train the\ntransformer structure from scratch, our approach requires a fraction of the\ncomputational resources compared to traditional AVSR models. Compared to\ncurrent SOTA systems like AV-HuBERT, our approach achieves an average\nimprovement of 8.3% in word error rate across different model sizes, noise\ncategories and broad SNR range. The approach allows up to 21% smaller models\nand requires only a fraction of the computational resources for training and\ninference compared to common AVSR approaches.", "published": "2023-12-21 14:16:15", "link": "http://arxiv.org/abs/2312.13873v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Total variation in popular rap vocals from 2009-2023: extension of the\n  analysis by Georgieva, Ripolles & McFee", "abstract": "Pitch variability in rap vocals is overlooked in favor of the genre's\nuniquely dynamic rhythmic properties. We present an analysis of fundamental\nfrequency (F0) variation in rap vocals over the past 14 years, focusing on song\nexamples that represent the state of modern rap music. Our analysis aims at\nidentifying meaningful trends over time, and is in turn a continuation of the\n2023 analysis by Georgieva, Ripolles & McFee. They found rap to be an outlier\nwith larger F0 variation compared to other genres, but with a declining trend\nsince the genre's inception. However, they only analyzed data through 2010. Our\nanalysis looks beyond 2010. We once again observe rap's large F0 variation, but\nwith a decelerated decline in recent years.", "published": "2023-12-21 17:03:25", "link": "http://arxiv.org/abs/2312.14036v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Fine-grained Disentangled Representation Learning for Multimodal Emotion\n  Recognition", "abstract": "Multimodal emotion recognition (MMER) is an active research field that aims\nto accurately recognize human emotions by fusing multiple perceptual\nmodalities. However, inherent heterogeneity across modalities introduces\ndistribution gaps and information redundancy, posing significant challenges for\nMMER. In this paper, we propose a novel fine-grained disentangled\nrepresentation learning (FDRL) framework to address these challenges.\nSpecifically, we design modality-shared and modality-private encoders to\nproject each modality into modality-shared and modality-private subspaces,\nrespectively. In the shared subspace, we introduce a fine-grained alignment\ncomponent to learn modality-shared representations, thus capturing modal\nconsistency. Subsequently, we tailor a fine-grained disparity component to\nconstrain the private subspaces, thereby learning modality-private\nrepresentations and enhancing their diversity. Lastly, we introduce a\nfine-grained predictor component to ensure that the labels of the output\nrepresentations from the encoders remain unchanged. Experimental results on the\nIEMOCAP dataset show that FDRL outperforms the state-of-the-art methods,\nachieving 78.34% and 79.44% on WAR and UAR, respectively.", "published": "2023-12-21 04:31:18", "link": "http://arxiv.org/abs/2312.13567v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Self-supervised Complex Network for Machine Sound Anomaly Detection", "abstract": "In this paper, we propose an anomaly detection algorithm for machine sounds\nwith a deep complex network trained by self-supervision. Using the fact that\nphase continuity information is crucial for detecting abnormalities in\ntime-series signals, our proposed algorithm utilizes the complex spectrum as an\ninput and performs complex number arithmetic throughout the entire process.\nSince the usefulness of phase information can vary depending on the type of\nmachine sound, we also apply an attention mechanism to control the weights of\nthe complex and magnitude spectrum bottleneck features depending on the machine\ntype. We train our network to perform a self-supervised task that classifies\nthe machine identifier (id) of normal input sounds among multiple classes. At\ntest time, an input signal is detected as anomalous if the trained model is\nunable to correctly classify the id. In other words, we determine the presence\nof an anomality when the output cross-entropy score of the multiclass\nidentification task is lower than a pre-defined threshold. Experiments with the\nMIMII dataset show that the proposed algorithm has a much higher area under the\ncurve (AUC) score than conventional magnitude spectrum-based algorithms.", "published": "2023-12-21 07:04:46", "link": "http://arxiv.org/abs/2312.13615v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "On the choice of the optimal temporal support for audio classification\n  with Pre-trained embeddings", "abstract": "Current state-of-the-art audio analysis systems rely on pre-trained embedding\nmodels, often used off-the-shelf as (frozen) feature extractors. Choosing the\nbest one for a set of tasks is the subject of many recent publications.\nHowever, one aspect often overlooked in these works is the influence of the\nduration of audio input considered to extract an embedding, which we refer to\nas Temporal Support (TS). In this work, we study the influence of the TS for\nwell-established or emerging pre-trained embeddings, chosen to represent\ndifferent types of architectures and learning paradigms. We conduct this\nevaluation using both musical instrument and environmental sound datasets,\nnamely OpenMIC, TAU Urban Acoustic Scenes 2020 Mobile, and ESC-50. We\nespecially highlight that Audio Spectrogram Transformer-based systems (PaSST\nand BEATs) remain effective with smaller TS, which therefore allows for a\ndrastic reduction in memory and computational cost. Moreover, we show that by\nchoosing the optimal TS we reach competitive results across all tasks. In\nparticular, we improve the state-of-the-art results on OpenMIC, using BEATs and\nPaSST without any fine-tuning.", "published": "2023-12-21 16:36:33", "link": "http://arxiv.org/abs/2312.14005v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "BANSpEmo: A Bangla Emotional Speech Recognition Dataset", "abstract": "In the field of audio and speech analysis, the ability to identify emotions\nfrom acoustic signals is essential. Human-computer interaction (HCI) and\nbehavioural analysis are only a few of the many areas where the capacity to\ndistinguish emotions from speech signals has an extensive range of\napplications. Here, we are introducing BanSpEmo, a corpus of emotional speech\nthat only consists of audio recordings and has been created specifically for\nthe Bangla language. This corpus contains 792 audio recordings over a duration\nof more than 1 hour and 23 minutes. 22 native speakers took part in the\nrecording of two sets of sentences that represent the six desired emotions. The\ndata set consists of 12 Bangla sentences which are uttered in 6 emotions as\nDisgust, Happy, Sad, Surprised, Anger, and Fear. This corpus is not also gender\nbalanced. Ten individuals who either have experience in related field or have\nacting experience took part in the assessment of this corpus. It has a balanced\nnumber of audio recordings in each emotion class. BanSpEmo can be considered as\na useful resource to promote emotion and speech recognition research and\nrelated applications in the Bangla language. The dataset can be found here:\nhttps://data.mendeley.com/datasets/rdwn4bs5ky and might be employed for\nacademic research.", "published": "2023-12-21 16:52:41", "link": "http://arxiv.org/abs/2312.14020v1", "categories": ["cs.HC", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
{"title": "Leveraging Visual Supervision for Array-based Active Speaker Detection\n  and Localization", "abstract": "Conventional audio-visual approaches for active speaker detection (ASD)\ntypically rely on visually pre-extracted face tracks and the corresponding\nsingle-channel audio to find the speaker in a video. Therefore, they tend to\nfail every time the face of the speaker is not visible. We demonstrate that a\nsimple audio convolutional recurrent neural network (CRNN) trained with spatial\ninput features extracted from multichannel audio can perform simultaneous\nhorizontal active speaker detection and localization (ASDL), independently of\nthe visual modality. To address the time and cost of generating ground truth\nlabels to train such a system, we propose a new self-supervised training\npipeline that embraces a ``student-teacher'' learning approach. A conventional\npre-trained active speaker detector is adopted as a ``teacher'' network to\nprovide the position of the speakers as pseudo-labels. The multichannel audio\n``student'' network is trained to generate the same results. At inference, the\nstudent network can generalize and locate also the occluded speakers that the\nteacher network is not able to detect visually, yielding considerable\nimprovements in recall rate. Experiments on the TragicTalkers dataset show that\nan audio network trained with the proposed self-supervised learning approach\ncan exceed the performance of the typical audio-visual methods and produce\nresults competitive with the costly conventional supervised training. We\ndemonstrate that improvements can be achieved when minimal manual supervision\nis introduced in the learning pipeline. Further gains may be sought with larger\ntraining sets and integrating vision with the multichannel audio system.", "published": "2023-12-21 16:53:04", "link": "http://arxiv.org/abs/2312.14021v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.IV", "eess.SP"], "primary_category": "eess.AS"}
