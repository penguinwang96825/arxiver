{"title": "Bi-Text Alignment of Movie Subtitles for Spoken English-Arabic\n  Statistical Machine Translation", "abstract": "We describe efforts towards getting better resources for English-Arabic\nmachine translation of spoken text. In particular, we look at movie subtitles\nas a unique, rich resource, as subtitles in one language often get translated\ninto other languages. Movie subtitles are not new as a resource and have been\nexplored in previous research; however, here we create a much larger bi-text\n(the biggest to date), and we further generate better quality alignment for it.\nGiven the subtitles for the same movie in different languages, a key problem is\nhow to align them at the fragment level. Typically, this is done using\nlength-based alignment, but for movie subtitles, there is also time\ninformation. Here we exploit this information to develop an original algorithm\nthat outperforms the current best subtitle alignment tool, subalign. The\nevaluation results show that adding our bi-text to the IWSLT training bi-text\nyields an improvement of over two BLEU points absolute.", "published": "2016-09-05 14:55:31", "link": "http://arxiv.org/abs/1609.01188v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PMI Matrix Approximations with Applications to Neural Language Modeling", "abstract": "The negative sampling (NEG) objective function, used in word2vec, is a\nsimplification of the Noise Contrastive Estimation (NCE) method. NEG was found\nto be highly effective in learning continuous word representations. However,\nunlike NCE, it was considered inapplicable for the purpose of learning the\nparameters of a language model. In this study, we refute this assertion by\nproviding a principled derivation for NEG-based language modeling, founded on a\nnovel analysis of a low-dimensional approximation of the matrix of pointwise\nmutual information between the contexts and the predicted words. The obtained\nlanguage modeling is closely related to NCE language models but is based on a\nsimplified objective function. We thus provide a unified formulation for two\nmain language processing tasks, namely word embedding and language modeling,\nbased on the NEG objective function. Experimental results on two popular\nlanguage modeling benchmarks show comparable perplexity results, with a small\nadvantage to NEG over NCE.", "published": "2016-09-05 17:47:49", "link": "http://arxiv.org/abs/1609.01235v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
