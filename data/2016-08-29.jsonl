{"title": "Machine Comprehension Using Match-LSTM and Answer Pointer", "abstract": "Machine comprehension of text is an important problem in natural language\nprocessing. A recently released dataset, the Stanford Question Answering\nDataset (SQuAD), offers a large number of real questions and their answers\ncreated by humans through crowdsourcing. SQuAD provides a challenging testbed\nfor evaluating machine comprehension algorithms, partly because compared with\nprevious datasets, in SQuAD the answers do not come from a small set of\ncandidate answers and they have variable lengths. We propose an end-to-end\nneural architecture for the task. The architecture is based on match-LSTM, a\nmodel we proposed previously for textual entailment, and Pointer Net, a\nsequence-to-sequence model proposed by Vinyals et al.(2015) to constrain the\noutput tokens to be from the input sequences. We propose two ways of using\nPointer Net for our task. Our experiments show that both of our two models\nsubstantially outperform the best results obtained by Rajpurkar et al.(2016)\nusing logistic regression and manually crafted features.", "published": "2016-08-29 03:42:50", "link": "http://arxiv.org/abs/1608.07905v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "What is Wrong with Topic Modeling? (and How to Fix it Using Search-based\n  Software Engineering)", "abstract": "Context: Topic modeling finds human-readable structures in unstructured\ntextual data. A widely used topic modeler is Latent Dirichlet allocation. When\nrun on different datasets, LDA suffers from \"order effects\" i.e. different\ntopics are generated if the order of training data is shuffled. Such order\neffects introduce a systematic error for any study. This error can relate to\nmisleading results;specifically, inaccurate topic descriptions and a reduction\nin the efficacy of text mining classification results. Objective: To provide a\nmethod in which distributions generated by LDA are more stable and can be used\nfor further analysis. Method: We use LDADE, a search-based software engineering\ntool that tunes LDA's parameters using DE (Differential Evolution). LDADE is\nevaluated on data from a programmer information exchange site (Stackoverflow),\ntitle and abstract text of thousands ofSoftware Engineering (SE) papers, and\nsoftware defect reports from NASA. Results were collected across different\nimplementations of LDA (Python+Scikit-Learn, Scala+Spark); across different\nplatforms (Linux, Macintosh) and for different kinds of LDAs (VEM,or using\nGibbs sampling). Results were scored via topic stability and text mining\nclassification accuracy. Results: In all treatments: (i) standard LDA exhibits\nvery large topic instability; (ii) LDADE's tunings dramatically reduce cluster\ninstability; (iii) LDADE also leads to improved performances for supervised as\nwell as unsupervised learning. Conclusion: Due to topic instability, using\nstandard LDA with its \"off-the-shelf\" settings should now be depreciated. Also,\nin future, we should require SE papers that use LDA to test and (if needed)\nmitigate LDA topic instability. Finally, LDADE is a candidate technology for\neffectively and efficiently reducing that instability.", "published": "2016-08-29 18:45:00", "link": "http://arxiv.org/abs/1608.08176v4", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.SE"}
{"title": "Visual Question: Predicting If a Crowd Will Agree on the Answer", "abstract": "Visual question answering (VQA) systems are emerging from a desire to empower\nusers to ask any natural language question about visual content and receive a\nvalid answer in response. However, close examination of the VQA problem reveals\nan unavoidable, entangled problem that multiple humans may or may not always\nagree on a single answer to a visual question. We train a model to\nautomatically predict from a visual question whether a crowd would agree on a\nsingle answer. We then propose how to exploit this system in a novel\napplication to efficiently allocate human effort to collect answers to visual\nquestions. Specifically, we propose a crowdsourcing system that automatically\nsolicits fewer human responses when answer agreement is expected and more human\nresponses when answer disagreement is expected. Our system improves upon\nexisting crowdsourcing systems, typically eliminating at least 20% of human\neffort with no loss to the information collected from the crowd.", "published": "2016-08-29 19:24:25", "link": "http://arxiv.org/abs/1608.08188v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "primary_category": "cs.AI"}
