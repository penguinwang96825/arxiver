{"title": "Automated Audio Captioning with Epochal Difficult Captions for\n  Curriculum Learning", "abstract": "In this paper, we propose an algorithm, Epochal Difficult Captions, to\nsupplement the training of any model for the Automated Audio Captioning task.\nEpochal Difficult Captions is an elegant evolution to the keyword estimation\ntask that previous work have used to train the encoder of the AAC model.\nEpochal Difficult Captions modifies the target captions based on a curriculum\nand a difficulty level determined as a function of current epoch. Epochal\nDifficult Captions can be used with any model architecture and is a lightweight\nfunction that does not increase training time. We test our results on three\nsystems and show that using Epochal Difficult Captions consistently improves\nperformance", "published": "2022-06-04 06:42:05", "link": "http://arxiv.org/abs/2206.01918v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Atypical lexical abbreviations identification in Russian medical texts", "abstract": "Abbreviation is a method of word formation that aims to construct the\nshortened term from the first letters of the initial phrase. Implicit\nabbreviations frequently cause the comprehension difficulties for unprepared\nreaders. In this paper, we propose an efficient ML-based algorithm which allows\nto identify the abbreviations in Russian texts. The method achieves ROC AUC\nscore 0.926 and F1 score 0.706 which are confirmed as competitive in comparison\nwith the baselines. Along with the pipeline, we also establish first to our\nknowledge Russian dataset that is relevant for the desired task.", "published": "2022-06-04 13:16:08", "link": "http://arxiv.org/abs/2206.01987v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Actuarial Applications of Natural Language Processing Using\n  Transformers: Case Studies for Using Text Features in an Actuarial Context", "abstract": "This tutorial demonstrates workflows to incorporate text data into actuarial\nclassification and regression tasks. The main focus is on methods employing\ntransformer-based models. A dataset of car accident descriptions with an\naverage length of 400 words, available in English and German, and a dataset\nwith short property insurance claims descriptions are used to demonstrate these\ntechniques. The case studies tackle challenges related to a multi-lingual\nsetting and long input sequences. They also show ways to interpret model\noutput, to assess and improve model performance, by fine-tuning the models to\nthe domain of application or to a specific prediction task. Finally, the\ntutorial provides practical approaches to handle classification tasks in\nsituations with no or only few labeled data, including but not limited to\nChatGPT. The results achieved by using the language-understanding skills of\noff-the-shelf natural language processing (NLP) models with only minimal\npre-processing and fine-tuning clearly demonstrate the power of transfer\nlearning for practical applications.", "published": "2022-06-04 15:39:30", "link": "http://arxiv.org/abs/2206.02014v3", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Understanding EFL Student Idea Generation Strategies for Creative\n  Writing with NLG Tools", "abstract": "Natural language generation (NLG) is a process within artificial intelligence\nwhere computer systems produce human-comprehensible language texts from\ninformation. English as a foreign language (EFL) students' use of NLG tools\nmight facilitate their idea generation, which is fundamental to creative\nwriting. However, little is known about how EFL students interact with NLG\ntools to generate ideas. This study explores strategies adopted by EFL students\nwhen searching for ideas using NLG tools, evaluating ideas generated by NLG\ntools and selecting NLG tools for ideas generation. Four Hong Kong secondary\nschool students attended workshops where they learned to write stories\ncomprising their own words and words generated by NLG tools. After the\nworkshops, they answered questions to reflect on their writing experience with\nNLG tools. In a thematic analysis of the written reflections, we found students\nmay have existing ideas when searching for ideas and evaluating ideas with NLG\ntools. Students showed some aversion to ideas generated by NLG tools and\nselected NLG tools that generated a greater quantity of ideas. The findings\ninform our understanding of EFL students' concerns when using NLG tools for\nidea generation and can inform educators' instruction to implement NLG tools\nfor classroom creative writing.", "published": "2022-06-04 09:11:38", "link": "http://arxiv.org/abs/2207.01484v3", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Extreme Compression for Pre-trained Transformers Made Simple and\n  Efficient", "abstract": "Extreme compression, particularly ultra-low bit precision (binary/ternary)\nquantization, has been proposed to fit large NLP models on resource-constraint\ndevices. However, to preserve the accuracy for such aggressive compression\nschemes, cutting-edge methods usually introduce complicated compression\npipelines, e.g., multi-stage expensive knowledge distillation with extensive\nhyperparameter tuning. Also, they oftentimes focus less on smaller transformer\nmodels that have already been heavily compressed via knowledge distillation and\nlack a systematic study to show the effectiveness of their methods. In this\npaper, we perform a very comprehensive systematic study to measure the impact\nof many key hyperparameters and training strategies from previous works. As a\nresult, we find out that previous baselines for ultra-low bit precision\nquantization are significantly under-trained. Based on our study, we propose a\nsimple yet effective compression pipeline for extreme compression, named XTC.\nXTC demonstrates that (1) we can skip the pre-training knowledge distillation\nto obtain a 5-layer BERT while achieving better performance than previous\nstate-of-the-art methods, e.g., the 6-layer TinyBERT; (2) extreme quantization\nplus layer reduction is able to reduce the model size by 50x, resulting in new\nstate-of-the-art results on GLUE tasks.", "published": "2022-06-04 00:19:45", "link": "http://arxiv.org/abs/2206.01859v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ZeroQuant: Efficient and Affordable Post-Training Quantization for\n  Large-Scale Transformers", "abstract": "How to efficiently serve ever-larger trained natural language models in\npractice has become exceptionally challenging even for powerful cloud servers\ndue to their prohibitive memory/computation requirements. In this work, we\npresent an efficient and affordable post-training quantization approach to\ncompress large Transformer-based models, termed as ZeroQuant. ZeroQuant is an\nend-to-end quantization and inference pipeline with three main components: (1)\na fine-grained hardware-friendly quantization scheme for both weight and\nactivations; (2) a novel affordable layer-by-layer knowledge distillation\nalgorithm (LKD) even without the access to the original training data; (3) a\nhighly-optimized quantization system backend support to remove the\nquantization/dequantization overhead. As such, we are able to show that: (1)\nZeroQuant can reduce the precision for weights and activations to INT8 in a\ncost-free way for both BERT and GPT3-style models with minimal accuracy impact,\nwhich leads to up to 5.19x/4.16x speedup on those models compared to FP16\ninference; (2) ZeroQuant plus LKD affordably quantize the weights in the\nfully-connected module to INT4 along with INT8 weights in the attention module\nand INT8 activations, resulting in 3x memory footprint reduction compared to\nthe FP16 model; (3) ZeroQuant can be directly applied to two of the largest\nopen-sourced language models, including GPT-J6B and GPT-NeoX20, for which our\nINT8 model achieves similar accuracy as the FP16 model but achieves up to 5.2x\nbetter efficiency.", "published": "2022-06-04 00:28:21", "link": "http://arxiv.org/abs/2206.01861v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Instance-wise Prompt Tuning for Pretrained Language Models", "abstract": "Prompt Learning has recently gained great popularity in bridging the gap\nbetween pretraining tasks and various downstream tasks. It freezes Pretrained\nLanguage Models (PLMs) and only tunes a few task-related parameters (prompts)\nfor downstream tasks, greatly reducing the cost of tuning giant models. The key\nenabler of this is the idea of querying PLMs with task-specific knowledge\nimplicated in prompts. This paper reveals a major limitation of existing\nmethods that the indiscriminate prompts for all input data in a task ignore the\nintrinsic knowledge from input data, resulting in sub-optimal performance. We\nintroduce Instance-wise Prompt Tuning (IPT), the first prompt learning paradigm\nthat injects knowledge from the input data instances to the prompts, thereby\nproviding PLMs with richer and more concrete context information. We devise a\nseries of strategies to produce instance-wise prompts, addressing various\nconcerns like model quality and cost-efficiency. Across multiple tasks and\nresource settings, IPT significantly outperforms task-based prompt learning\nmethods, and achieves comparable performance to conventional finetuning with\nonly 0.5% - 1.5% of tuned parameters.", "published": "2022-06-04 10:08:50", "link": "http://arxiv.org/abs/2206.01958v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Initial Study into Application of Feature Density and\n  Linguistically-backed Embedding to Improve Machine Learning-based\n  Cyberbullying Detection", "abstract": "In this research, we study the change in the performance of machine learning\n(ML) classifiers when various linguistic preprocessing methods of a dataset\nwere used, with the specific focus on linguistically-backed embeddings in\nConvolutional Neural Networks (CNN). Moreover, we study the concept of Feature\nDensity and confirm its potential to comparatively predict the performance of\nML classifiers, including CNN. The research was conducted on a Formspring\ndataset provided in a Kaggle competition on automatic cyberbullying detection.\nThe dataset was re-annotated by objective experts (psychologists), as the\nimportance of professional annotation in cyberbullying research has been\nindicated multiple times. The study confirmed the effectiveness of Neural\nNetworks in cyberbullying detection and the correlation between classifier\nperformance and Feature Density while also proposing a new approach of training\nvarious linguistically-backed embeddings for Convolutional Neural Networks.", "published": "2022-06-04 03:17:15", "link": "http://arxiv.org/abs/2206.01889v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exploring the Potential of Feature Density in Estimating Machine\n  Learning Classifier Performance with Application to Cyberbullying Detection", "abstract": "In this research. we analyze the potential of Feature Density (HD) as a way\nto comparatively estimate machine learning (ML) classifier performance prior to\ntraining. The goal of the study is to aid in solving the problem of\nresource-intensive training of ML models which is becoming a serious issue due\nto continuously increasing dataset sizes and the ever rising popularity of Deep\nNeural Networks (DNN). The issue of constantly increasing demands for more\npowerful computational resources is also affecting the environment, as training\nlarge-scale ML models are causing alarmingly-growing amounts of CO2, emissions.\nOur approach 1s to optimize the resource-intensive training of ML models for\nNatural Language Processing to reduce the number of required experiments\niterations. We expand on previous attempts on improving classifier training\nefficiency with FD while also providing an insight to the effectiveness of\nvarious linguistically-backed feature preprocessing methods for dialog\nclassification, specifically cyberbullying detection.", "published": "2022-06-04 09:11:13", "link": "http://arxiv.org/abs/2206.01949v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Comparing Performance of Different Linguistically-Backed Word Embeddings\n  for Cyberbullying Detection", "abstract": "In most cases, word embeddings are learned only from raw tokens or in some\ncases, lemmas. This includes pre-trained language models like BERT. To\ninvestigate on the potential of capturing deeper relations between lexical\nitems and structures and to filter out redundant information, we propose to\npreserve the morphological, syntactic and other types of linguistic information\nby combining them with the raw tokens or lemmas. This means, for example,\nincluding parts-of-speech or dependency information within the used lexical\nfeatures. The word embeddings can then be trained on the combinations instead\nof just raw tokens. It is also possible to later apply this method to the\npre-training of huge language models and possibly enhance their performance.\nThis would aid in tackling problems which are more sophisticated from the point\nof view of linguistic representation, such as detection of cyberbullying.", "published": "2022-06-04 09:11:41", "link": "http://arxiv.org/abs/2206.01950v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Delving into the Openness of CLIP", "abstract": "Contrastive Language-Image Pre-training (CLIP) formulates image\nclassification as an image-to-text matching task, i.e., matching images to the\ncorresponding natural language descriptions instead of discrete category IDs.\nThis allows for open-vocabulary visual recognition, where the model can\nrecognize images from an open class set (also known as an open vocabulary) in a\nzero-shot manner. However, evaluating the openness of CLIP-like models is\nchallenging, as the models are open to arbitrary vocabulary in theory, but\ntheir accuracy varies in practice. To address this, we resort to an incremental\nperspective to assess the openness through vocabulary expansions, and define\nextensibility to measure a model's ability to handle novel classes. Our\nevaluation shows that CLIP-like models are not truly open, and their\nperformance deteriorates as the vocabulary expands. We further dissect the\nfeature space of CLIP from the perspectives of representation alignment and\nuniformity. Our investigation reveals that the overestimation of openness is\ndue to confusion among competing text features, rather than a failure to\ncapture the similarity between image features and text features of novel\nclasses. We hope that our investigation and analysis will facilitate future\nresearch on the CLIP openness issue.", "published": "2022-06-04 13:07:30", "link": "http://arxiv.org/abs/2206.01986v3", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "STARSS22: A dataset of spatial recordings of real scenes with\n  spatiotemporal annotations of sound events", "abstract": "This report presents the Sony-TAu Realistic Spatial Soundscapes 2022\n(STARS22) dataset for sound event localization and detection, comprised of\nspatial recordings of real scenes collected in various interiors of two\ndifferent sites. The dataset is captured with a high resolution spherical\nmicrophone array and delivered in two 4-channel formats, first-order Ambisonics\nand tetrahedral microphone array. Sound events in the dataset belonging to 13\ntarget sound classes are annotated both temporally and spatially through a\ncombination of human annotation and optical tracking. The dataset serves as the\ndevelopment and evaluation dataset for the Task 3 of the DCASE2022 Challenge on\nSound Event Localization and Detection and introduces significant new\nchallenges for the task compared to the previous iterations, which were based\non synthetic spatialized sound scene recordings. Dataset specifications are\ndetailed including recording and annotation process, target classes and their\npresence, and details on the development and evaluation splits. Additionally,\nthe report presents the baseline system that accompanies the dataset in the\nchallenge with emphasis on the differences with the baseline of the previous\niterations; namely, introduction of the multi-ACCDOA representation to handle\nmultiple simultaneous occurences of events of the same class, and support for\nadditional improved input features for the microphone array format. Results of\nthe baseline indicate that with a suitable training strategy a reasonable\ndetection and localization performance can be achieved on real sound scene\nrecordings. The dataset is available in https://zenodo.org/record/6387880.", "published": "2022-06-04 08:59:08", "link": "http://arxiv.org/abs/2206.01948v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Learning Speaker-specific Lip-to-Speech Generation", "abstract": "Understanding the lip movement and inferring the speech from it is\nnotoriously difficult for the common person. The task of accurate lip-reading\ngets help from various cues of the speaker and its contextual or environmental\nsetting. Every speaker has a different accent and speaking style, which can be\ninferred from their visual and speech features. This work aims to understand\nthe correlation/mapping between speech and the sequence of lip movement of\nindividual speakers in an unconstrained and large vocabulary. We model the\nframe sequence as a prior to the transformer in an auto-encoder setting and\nlearned a joint embedding that exploits temporal properties of both audio and\nvideo. We learn temporal synchronization using deep metric learning, which\nguides the decoder to generate speech in sync with input lip movements. The\npredictive posterior thus gives us the generated speech in speaker speaking\nstyle. We have trained our model on the Grid and Lip2Wav Chemistry lecture\ndataset to evaluate single speaker natural speech generation tasks from lip\nmovement in an unconstrained natural setting. Extensive evaluation using\nvarious qualitative and quantitative metrics with human evaluation also shows\nthat our method outperforms the Lip2Wav Chemistry dataset(large vocabulary in\nan unconstrained setting) by a good margin across almost all evaluation metrics\nand marginally outperforms the state-of-the-art on GRID dataset.", "published": "2022-06-04 19:40:02", "link": "http://arxiv.org/abs/2206.02050v2", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
