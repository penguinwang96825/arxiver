{"title": "Hierarchical Transformer for Task Oriented Dialog Systems", "abstract": "Generative models for dialog systems have gained much interest because of the\nrecent success of RNN and Transformer based models in tasks like question\nanswering and summarization. Although the task of dialog response generation is\ngenerally seen as a sequence-to-sequence (Seq2Seq) problem, researchers in the\npast have found it challenging to train dialog systems using the standard\nSeq2Seq models. Therefore, to help the model learn meaningful utterance and\nconversation level features, Sordoni et al. (2015b); Serban et al. (2016)\nproposed Hierarchical RNN architecture, which was later adopted by several\nother RNN based dialog systems. With the transformer-based models dominating\nthe seq2seq problems lately, the natural question to ask is the applicability\nof the notion of hierarchy in transformer based dialog systems. In this paper,\nwe propose a generalized framework for Hierarchical Transformer Encoders and\nshow how a standard transformer can be morphed into any hierarchical encoder,\nincluding HRED and HIBERT like models, by using specially designed attention\nmasks and positional encodings. We demonstrate that Hierarchical Encoding helps\nachieve better natural language understanding of the contexts in\ntransformer-based models for task-oriented dialog systems through a wide range\nof experiments.", "published": "2020-10-24 10:08:52", "link": "http://arxiv.org/abs/2011.08067v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Word2vec Conjecture and A Limitative Result", "abstract": "Being inspired by the success of \\texttt{word2vec}\n\\citep{mikolov2013distributed} in capturing analogies, we study the conjecture\nthat analogical relations can be represented by vector spaces. Unlike many\nprevious works that focus on the distributional semantic aspect of\n\\texttt{word2vec}, we study the purely \\emph{representational} question: can\n\\emph{all} semantic word-word relations be represented by differences (or\ndirections) of vectors? We call this the word2vec conjecture and point out some\nof its desirable implications. However, we will exhibit a class of relations\nthat cannot be represented in this way, thus falsifying the conjecture and\nestablishing a limitative result for the representability of semantic relations\nby vector spaces over fields of characteristic 0, e.g., real or complex\nnumbers.", "published": "2020-10-24 00:14:04", "link": "http://arxiv.org/abs/2010.12719v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Constrained Abstractive Summarization: Preserving Factual Consistency\n  with Constrained Generation", "abstract": "Despite significant progress, state-of-the-art abstractive summarization\nmethods are still prone to hallucinate content inconsistent with the source\ndocument. In this paper, we propose Constrained Abstractive Summarization\n(CAS), a general setup that preserves the factual consistency of abstractive\nsummarization by specifying tokens as constraints that must be present in the\nsummary. We adopt lexically constrained decoding, a technique generally\napplicable to autoregressive generative models, to fulfill CAS and conduct\nexperiments in two scenarios: (1) automatic summarization without human\ninvolvement, where keyphrases are extracted from the source document and used\nas constraints; (2) human-guided interactive summarization, where human\nfeedback in the form of manual constraints are used to guide summary\ngeneration. Automatic and human evaluations on two benchmark datasets\ndemonstrate that CAS improves both lexical overlap (ROUGE) and factual\nconsistency of abstractive summarization. In particular, we observe up to 13.8\nROUGE-2 gains when only one manual constraint is used in interactive\nsummarization.", "published": "2020-10-24 00:27:44", "link": "http://arxiv.org/abs/2010.12723v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Compositional Generalization and Natural Language Variation: Can a\n  Semantic Parsing Approach Handle Both?", "abstract": "Sequence-to-sequence models excel at handling natural language variation, but\nhave been shown to struggle with out-of-distribution compositional\ngeneralization. This has motivated new specialized architectures with stronger\ncompositional biases, but most of these approaches have only been evaluated on\nsynthetically-generated datasets, which are not representative of natural\nlanguage variation. In this work we ask: can we develop a semantic parsing\napproach that handles both natural language variation and compositional\ngeneralization? To better assess this capability, we propose new train and test\nsplits of non-synthetic datasets. We demonstrate that strong existing\napproaches do not perform well across a broad set of evaluations. We also\npropose NQG-T5, a hybrid model that combines a high-precision grammar-based\napproach with a pre-trained sequence-to-sequence model. It outperforms existing\napproaches across several compositional generalization challenges on\nnon-synthetic data, while also being competitive with the state-of-the-art on\nstandard evaluations. While still far from solving this problem, our study\nhighlights the importance of diverse evaluations and the open challenge of\nhandling both compositional generalization and natural language variation in\nsemantic parsing.", "published": "2020-10-24 00:38:27", "link": "http://arxiv.org/abs/2010.12725v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ANLIzing the Adversarial Natural Language Inference Dataset", "abstract": "We perform an in-depth error analysis of Adversarial NLI (ANLI), a recently\nintroduced large-scale human-and-model-in-the-loop natural language inference\ndataset collected over multiple rounds. We propose a fine-grained annotation\nscheme of the different aspects of inference that are responsible for the gold\nclassification labels, and use it to hand-code all three of the ANLI\ndevelopment sets. We use these annotations to answer a variety of interesting\nquestions: which inference types are most common, which models have the highest\nperformance on each reasoning type, and which types are the most challenging\nfor state of-the-art models? We hope that our annotations will enable more\nfine-grained evaluation of models trained on ANLI, provide us with a deeper\nunderstanding of where models fail and succeed, and help us determine how to\ntrain better models in future.", "published": "2020-10-24 01:03:51", "link": "http://arxiv.org/abs/2010.12729v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Char2Subword: Extending the Subword Embedding Space Using Robust\n  Character Compositionality", "abstract": "Byte-pair encoding (BPE) is a ubiquitous algorithm in the subword\ntokenization process of language models as it provides multiple benefits.\nHowever, this process is solely based on pre-training data statistics, making\nit hard for the tokenizer to handle infrequent spellings. On the other hand,\nthough robust to misspellings, pure character-level models often lead to\nunreasonably long sequences and make it harder for the model to learn\nmeaningful words. To alleviate these challenges, we propose a character-based\nsubword module (char2subword) that learns the subword embedding table in\npre-trained models like BERT. Our char2subword module builds representations\nfrom characters out of the subword vocabulary, and it can be used as a drop-in\nreplacement of the subword embedding table. The module is robust to\ncharacter-level alterations such as misspellings, word inflection, casing, and\npunctuation. We integrate it further with BERT through pre-training while\nkeeping BERT transformer parameters fixed--and thus, providing a practical\nmethod. Finally, we show that incorporating our module to mBERT significantly\nimproves the performance on the social media linguistic code-switching\nevaluation (LinCE) benchmark.", "published": "2020-10-24 01:08:28", "link": "http://arxiv.org/abs/2010.12730v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Evaluation Protocol for Generative Conversational Systems", "abstract": "There is a multitude of novel generative models for open-domain\nconversational systems; however, there is no systematic evaluation of different\nsystems. Systematic comparisons require consistency in experimental design,\nevaluation sets, conversational systems and their outputs, and statistical\nanalysis. We lay out a protocol for the evaluation of conversational models\nusing head-to-head pairwise comparison. We analyze ten recent models that claim\nstate-of-the-art performance using a paired head-to-head performance\n(win-loss-tie) on five evaluation datasets. Our findings show that DialoGPT and\nBlender are superior systems using Bradley-Terry model and TrueSkill ranking\nmethods. These findings demonstrate the feasibility of our protocol to evaluate\nconversational agents and evaluation sets. Finally, we make all code and\nevaluations publicly available for researchers to compare their model to other\nstate-of-the-art dialog models.", "published": "2020-10-24 01:59:49", "link": "http://arxiv.org/abs/2010.12741v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text Style Transfer: A Review and Experimental Evaluation", "abstract": "The stylistic properties of text have intrigued computational linguistics\nresearchers in recent years. Specifically, researchers have investigated the\nText Style Transfer (TST) task, which aims to change the stylistic properties\nof the text while retaining its style independent content. Over the last few\nyears, many novel TST algorithms have been developed, while the industry has\nleveraged these algorithms to enable exciting TST applications. The field of\nTST research has burgeoned because of this symbiosis. This article aims to\nprovide a comprehensive review of recent research efforts on text style\ntransfer. More concretely, we create a taxonomy to organize the TST models and\nprovide a comprehensive summary of the state of the art. We review the existing\nevaluation methodologies for TST tasks and conduct a large-scale\nreproducibility study where we experimentally benchmark 19 state-of-the-art TST\nalgorithms on two publicly available datasets. Finally, we expand on current\ntrends and provide new perspectives on the new and exciting developments in the\nTST field.", "published": "2020-10-24 02:02:58", "link": "http://arxiv.org/abs/2010.12742v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Temporal Reasoning on Implicit Events from Distant Supervision", "abstract": "We propose TRACIE, a novel temporal reasoning dataset that evaluates the\ndegree to which systems understand implicit events -- events that are not\nmentioned explicitly in natural language text but can be inferred from it. This\nintroduces a new challenge in temporal reasoning research, where prior work has\nfocused on explicitly mentioned events. Human readers can infer implicit events\nvia commonsense reasoning, resulting in a more comprehensive understanding of\nthe situation and, consequently, better reasoning about time. We find, however,\nthat state-of-the-art models struggle when predicting temporal relationships\nbetween implicit and explicit events. To address this, we propose a\nneuro-symbolic temporal reasoning model, SYMTIME, which exploits distant\nsupervision signals from large-scale text and uses temporal rules to combine\nstart times and durations to infer end times. SYMTIME outperforms strong\nbaseline systems on TRACIE by 5%, and by 11% in a zero prior knowledge training\nsetting. Our approach also generalizes to other temporal reasoning tasks, as\nevidenced by a gain of 1%-9% on MATRES, an explicit event benchmark.", "published": "2020-10-24 03:12:27", "link": "http://arxiv.org/abs/2010.12753v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adding Chit-Chat to Enhance Task-Oriented Dialogues", "abstract": "Existing dialogue corpora and models are typically designed under two\ndisjoint motives: while task-oriented systems focus on achieving functional\ngoals (e.g., booking hotels), open-domain chatbots aim at making socially\nengaging conversations. In this work, we propose to integrate both types of\nsystems by Adding Chit-Chat to ENhance Task-ORiented dialogues (ACCENTOR), with\nthe goal of making virtual assistant conversations more engaging and\ninteractive. Specifically, we propose a Human <-> AI collaborative data\ncollection approach for generating diverse chit-chat responses to augment\ntask-oriented dialogues with minimal annotation effort. We then present our new\nchit-chat-based annotations to 23.8K dialogues from two popular task-oriented\ndatasets (Schema-Guided Dialogue and MultiWOZ 2.1) and demonstrate their\nadvantage over the originals via human evaluation. Lastly, we propose three new\nmodels for adding chit-chat to task-oriented dialogues, explicitly trained to\npredict user goals and to generate contextually relevant chit-chat responses.\nAutomatic and human evaluations show that, compared with the state-of-the-art\ntask-oriented baseline, our models can code-switch between task and chit-chat\nto be more engaging, interesting, knowledgeable, and humanlike, while\nmaintaining competitive task performance.", "published": "2020-10-24 03:22:43", "link": "http://arxiv.org/abs/2010.12757v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NUANCED: Natural Utterance Annotation for Nuanced Conversation with\n  Estimated Distributions", "abstract": "Existing conversational systems are mostly agent-centric, which assumes the\nuser utterances would closely follow the system ontology (for NLU or dialogue\nstate tracking). However, in real-world scenarios, it is highly desirable that\nthe users can speak freely in their own way. It is extremely hard, if not\nimpossible, for the users to adapt to the unknown system ontology. In this\nwork, we attempt to build a user-centric dialogue system. As there is no clean\nmapping for a user's free form utterance to an ontology, we first model the\nuser preferences as estimated distributions over the system ontology and map\nthe users' utterances to such distributions. Learning such a mapping poses new\nchallenges on reasoning over existing knowledge, ranging from factoid\nknowledge, commonsense knowledge to the users' own situations. To this end, we\nbuild a new dataset named NUANCED that focuses on such realistic settings for\nconversational recommendation. Collected via dialogue simulation and\nparaphrasing, NUANCED contains 5.1k dialogues, 26k turns of high-quality user\nresponses. We conduct experiments, showing both the usefulness and challenges\nof our problem setting. We believe NUANCED can serve as a valuable resource to\npush existing research from the agent-centric system to the user-centric\nsystem. The code and data is publicly available at\n\\url{https://github.com/facebookresearch/nuanced}.", "published": "2020-10-24 03:23:14", "link": "http://arxiv.org/abs/2010.12758v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Measuring Association Between Labels and Free-Text Rationales", "abstract": "In interpretable NLP, we require faithful rationales that reflect the model's\ndecision-making process for an explained instance. While prior work focuses on\nextractive rationales (a subset of the input words), we investigate their\nless-studied counterpart: free-text natural language rationales. We demonstrate\nthat pipelines, existing models for faithful extractive rationalization on\ninformation-extraction style tasks, do not extend as reliably to \"reasoning\"\ntasks requiring free-text rationales. We turn to models that jointly predict\nand rationalize, a class of widely used high-performance models for free-text\nrationalization whose faithfulness is not yet established. We define\nlabel-rationale association as a necessary property for faithfulness: the\ninternal mechanisms of the model producing the label and the rationale must be\nmeaningfully correlated. We propose two measurements to test this property:\nrobustness equivalence and feature importance agreement. We find that\nstate-of-the-art T5-based joint models exhibit both properties for\nrationalizing commonsense question-answering and natural language inference,\nindicating their potential for producing faithful free-text rationales.", "published": "2020-10-24 03:40:56", "link": "http://arxiv.org/abs/2010.12762v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Conversational Semantic Parsing for Dialog State Tracking", "abstract": "We consider a new perspective on dialog state tracking (DST), the task of\nestimating a user's goal through the course of a dialog. By formulating DST as\na semantic parsing task over hierarchical representations, we can incorporate\nsemantic compositionality, cross-domain knowledge sharing and co-reference. We\npresent TreeDST, a dataset of 27k conversations annotated with tree-structured\ndialog states and system acts. We describe an encoder-decoder framework for DST\nwith hierarchical representations, which leads to 20% improvement over\nstate-of-the-art DST approaches that operate on a flat meaning space of\nslot-value pairs.", "published": "2020-10-24 04:10:32", "link": "http://arxiv.org/abs/2010.12770v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On Learning Text Style Transfer with Direct Rewards", "abstract": "In most cases, the lack of parallel corpora makes it impossible to directly\ntrain supervised models for the text style transfer task. In this paper, we\nexplore training algorithms that instead optimize reward functions that\nexplicitly consider different aspects of the style-transferred outputs. In\nparticular, we leverage semantic similarity metrics originally used for\nfine-tuning neural machine translation models to explicitly assess the\npreservation of content between system outputs and input texts. We also\ninvestigate the potential weaknesses of the existing automatic metrics and\npropose efficient strategies of using these metrics for training. The\nexperimental results show that our model provides significant gains in both\nautomatic and human evaluation over strong baselines, indicating the\neffectiveness of our proposed methods and training strategies.", "published": "2020-10-24 04:30:02", "link": "http://arxiv.org/abs/2010.12771v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improved Synthetic Training for Reading Comprehension", "abstract": "Automatically generated synthetic training examples have been shown to\nimprove performance in machine reading comprehension (MRC). Compared to human\nannotated gold standard data, synthetic training data has unique properties,\nsuch as high availability at the possible expense of quality. In view of such\ndifferences, in this paper, we explore novel applications of synthetic examples\nto MRC. Our proposed pre-training and knowledge distillation strategies show\nsignificant improvements over existing methods. In a particularly surprising\ndiscovery, we observe that synthetic distillation often yields students that\ncan outperform the teacher model.", "published": "2020-10-24 04:41:30", "link": "http://arxiv.org/abs/2010.12776v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fair Hate Speech Detection through Evaluation of Social Group\n  Counterfactuals", "abstract": "Approaches for mitigating bias in supervised models are designed to reduce\nmodels' dependence on specific sensitive features of the input data, e.g.,\nmentioned social groups. However, in the case of hate speech detection, it is\nnot always desirable to equalize the effects of social groups because of their\nessential role in distinguishing outgroup-derogatory hate, such that particular\ntypes of hateful rhetoric carry the intended meaning only when contextualized\naround certain social group tokens. Counterfactual token fairness for a\nmentioned social group evaluates the model's predictions as to whether they are\nthe same for (a) the actual sentence and (b) a counterfactual instance, which\nis generated by changing the mentioned social group in the sentence. Our\napproach assures robust model predictions for counterfactuals that imply\nsimilar meaning as the actual sentence. To quantify the similarity of a\nsentence and its counterfactual, we compare their likelihood score calculated\nby generative language models. By equalizing model behaviors on each sentence\nand its counterfactuals, we mitigate bias in the proposed model while\npreserving the overall classification performance.", "published": "2020-10-24 04:51:47", "link": "http://arxiv.org/abs/2010.12779v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep Clustering of Text Representations for Supervision-free Probing of\n  Syntax", "abstract": "We explore deep clustering of text representations for unsupervised model\ninterpretation and induction of syntax. As these representations are\nhigh-dimensional, out-of-the-box methods like KMeans do not work well. Thus,\nour approach jointly transforms the representations into a lower-dimensional\ncluster-friendly space and clusters them. We consider two notions of syntax:\nPart of speech Induction (POSI) and constituency labelling (CoLab) in this\nwork. Interestingly, we find that Multilingual BERT (mBERT) contains surprising\namount of syntactic knowledge of English; possibly even as much as English BERT\n(EBERT). Our model can be used as a supervision-free probe which is arguably a\nless-biased way of probing. We find that unsupervised probes show benefits from\nhigher layers as compared to supervised probes. We further note that our\nunsupervised probe utilizes EBERT and mBERT representations differently,\nespecially for POSI. We validate the efficacy of our probe by demonstrating its\ncapabilities as an unsupervised syntax induction technique. Our probe works\nwell for both syntactic formalisms by simply adapting the input\nrepresentations. We report competitive performance of our probe on 45-tag\nEnglish POSI, state-of-the-art performance on 12-tag POSI across 10 languages,\nand competitive results on CoLab. We also perform zero-shot syntax induction on\nresource impoverished languages and report strong results.", "published": "2020-10-24 05:06:29", "link": "http://arxiv.org/abs/2010.12784v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Measuring the `I don't know' Problem through the Lens of Gricean\n  Quantity", "abstract": "We consider the intrinsic evaluation of neural generative dialog models\nthrough the lens of Grice's Maxims of Conversation (1975). Based on the maxim\nof Quantity (be informative), we propose Relative Utterance Quantity (RUQ) to\ndiagnose the `I don't know' problem, in which a dialog system produces generic\nresponses. The linguistically motivated RUQ diagnostic compares the model score\nof a generic response to that of the reference response. We find that for\nreasonable baseline models, `I don't know' is preferred over the reference the\nmajority of the time, but this can be reduced to less than 5% with\nhyperparameter tuning. RUQ allows for the direct analysis of the `I don't know'\nproblem, which has been addressed but not analyzed by prior work.", "published": "2020-10-24 05:16:36", "link": "http://arxiv.org/abs/2010.12786v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Document-level Event Extraction with Efficient End-to-end Learning of\n  Cross-event Dependencies", "abstract": "Fully understanding narratives often requires identifying events in the\ncontext of whole documents and modeling the event relations. However,\ndocument-level event extraction is a challenging task as it requires the\nextraction of event and entity coreference, and capturing arguments that span\nacross different sentences. Existing works on event extraction usually confine\non extracting events from single sentences, which fail to capture the\nrelationships between the event mentions at the scale of a document, as well as\nthe event arguments that appear in a different sentence than the event trigger.\nIn this paper, we propose an end-to-end model leveraging Deep Value Networks\n(DVN), a structured prediction algorithm, to efficiently capture cross-event\ndependencies for document-level event extraction. Experimental results show\nthat our approach achieves comparable performance to CRF-based models on ACE05,\nwhile enjoys significantly higher computational efficiency.", "published": "2020-10-24 05:28:16", "link": "http://arxiv.org/abs/2010.12787v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "New Approaches for Natural Language Understanding based on the Idea that\n  Natural Language encodes both Information and its Processing Procedures", "abstract": "We must recognize that natural language is a way of information encoding, and\nit encodes not only the information but also the procedures for how information\nis processed. To understand natural language, the same as we conceive and\ndesign computer languages, the first step is to separate information (or data)\nand the processing procedures of information (or data). In natural language,\nsome processing procedures of data are encoded directly as the structure chunk\nand the pointer chunk (this paper has reclassified lexical chunks as the data\nchunk, structure chunk, and the pointer chunk); some processing procedures of\ndata imply in sentences structures; some requests of processing procedures are\nexpressed by information senders and processed by information receivers. For\nthe data parts, the classification encoding system of attribute information and\nthe information organization architecture (including constitutional structures\nof information sets and the hierarchy between the information sets) were\ndiscussed. In section 2, the theoretical part elaborated in section 2 has been\nverified in examples and proofed that the studies in this paper have achieved\nthe goal of enabling machines to understand the information conveyed in the\ndialogue. In section 4, the author summarizes the basic conditions of\n\"Understanding\", rethinks what \"Understanding\" is and how to proceed. The study\nin this paper provides a practical, theoretical basis and research methods for\nNLU. It also can be applied in large-scale and multi-type information\nprocessing in the artificial intelligence (AI) area.", "published": "2020-10-24 05:40:47", "link": "http://arxiv.org/abs/2010.12789v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CaM-Gen:Causally-aware Metric-guided Text Generation", "abstract": "Content is created for a well-defined purpose, often described by a metric or\nsignal represented in the form of structured information. The relationship\nbetween the goal (metrics) of target content and the content itself is\nnon-trivial. While large-scale language models show promising text generation\ncapabilities, guiding the generated text with external metrics is challenging.\nThese metrics and content tend to have inherent relationships and not all of\nthem may be of consequence. We introduce CaM-Gen: Causally aware Generative\nNetworks guided by user-defined target metrics incorporating the causal\nrelationships between the metric and content features. We leverage causal\ninference techniques to identify causally significant aspects of a text that\nlead to the target metric and then explicitly guide generative models towards\nthese by a feedback mechanism. We propose this mechanism for variational\nautoencoder and Transformer-based generative models. The proposed models beat\nbaselines in terms of the target metric control while maintaining fluency and\nlanguage quality of the generated text. To the best of our knowledge, this is\none of the early attempts at controlled generation incorporating a metric guide\nusing causal inference.", "published": "2020-10-24 06:17:35", "link": "http://arxiv.org/abs/2010.12795v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pairwise Representation Learning for Event Coreference", "abstract": "Natural Language Processing tasks such as resolving the coreference of events\nrequire understanding the relations between two text snippets. These tasks are\ntypically formulated as (binary) classification problems over independently\ninduced representations of the text snippets. In this work, we develop a\nPairwise Representation Learning (PairwiseRL) scheme for the event mention\npairs, in which we jointly encode a pair of text snippets so that the\nrepresentation of each mention in the pair is induced in the context of the\nother one. Furthermore, our representation supports a finer, structured\nrepresentation of the text snippet to facilitate encoding events and their\narguments. We show that PairwiseRL, despite its simplicity, outperforms the\nprior state-of-the-art event coreference systems on both cross-document and\nwithin-document event coreference benchmarks. We also conduct in-depth analysis\nin terms of the improvement and the limitation of pairwise representation so as\nto provide insights for future work.", "published": "2020-10-24 06:55:52", "link": "http://arxiv.org/abs/2010.12808v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Frustratingly Easy Approach for Entity and Relation Extraction", "abstract": "End-to-end relation extraction aims to identify named entities and extract\nrelations between them. Most recent work models these two subtasks jointly,\neither by casting them in one structured prediction framework, or performing\nmulti-task learning through shared representations. In this work, we present a\nsimple pipelined approach for entity and relation extraction, and establish the\nnew state-of-the-art on standard benchmarks (ACE04, ACE05 and SciERC),\nobtaining a 1.7%-2.8% absolute improvement in relation F1 over previous joint\nmodels with the same pre-trained encoders. Our approach essentially builds on\ntwo independent encoders and merely uses the entity model to construct the\ninput for the relation model. Through a series of careful examinations, we\nvalidate the importance of learning distinct contextual representations for\nentities and relations, fusing entity information early in the relation model,\nand incorporating global context. Finally, we also present an efficient\napproximation to our approach which requires only one pass of both entity and\nrelation encoders at inference time, achieving an 8-16$\\times$ speedup with a\nslight reduction in accuracy.", "published": "2020-10-24 07:14:01", "link": "http://arxiv.org/abs/2010.12812v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Constructing Taxonomies from Pretrained Language Models", "abstract": "We present a method for constructing taxonomic trees (e.g., WordNet) using\npretrained language models. Our approach is composed of two modules, one that\npredicts parenthood relations and another that reconciles those predictions\ninto trees. The parenthood prediction module produces likelihood scores for\neach potential parent-child pair, creating a graph of parent-child relation\nscores. The tree reconciliation module treats the task as a graph optimization\nproblem and outputs the maximum spanning tree of this graph. We train our model\non subtrees sampled from WordNet, and test on non-overlapping WordNet subtrees.\nWe show that incorporating web-retrieved glosses can further improve\nperformance. On the task of constructing subtrees of English WordNet, the model\nachieves 66.7 ancestor F1, a 20.0% relative increase over the previous best\npublished result on this task. In addition, we convert the original English\ndataset into nine other languages using Open Multilingual WordNet and extend\nour results across these languages.", "published": "2020-10-24 07:16:21", "link": "http://arxiv.org/abs/2010.12813v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"Nice Try, Kiddo\": Investigating Ad Hominems in Dialogue Responses", "abstract": "Ad hominem attacks are those that target some feature of a person's character\ninstead of the position the person is maintaining. These attacks are harmful\nbecause they propagate implicit biases and diminish a person's credibility.\nSince dialogue systems respond directly to user input, it is important to study\nad hominems in dialogue responses. To this end, we propose categories of ad\nhominems, compose an annotated dataset, and build a classifier to analyze human\nand dialogue system responses to English Twitter posts. We specifically compare\nresponses to Twitter topics about marginalized communities (#BlackLivesMatter,\n#MeToo) versus other topics (#Vegan, #WFH), because the abusive language of ad\nhominems could further amplify the skew of power away from marginalized\npopulations. Furthermore, we propose a constrained decoding technique that uses\nsalient $n$-gram similarity as a soft constraint for top-$k$ sampling to reduce\nthe amount of ad hominems generated. Our results indicate that 1) responses\nfrom both humans and DialoGPT contain more ad hominems for discussions around\nmarginalized communities, 2) different quantities of ad hominems in the\ntraining data can influence the likelihood of generating ad hominems, and 3) we\ncan use constrained decoding techniques to reduce ad hominems in generated\ndialogue responses.", "published": "2020-10-24 07:37:49", "link": "http://arxiv.org/abs/2010.12820v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-neutralising: Probing for joint encoding of linguistic information\n  in multilingual models", "abstract": "Multilingual sentence encoders are widely used to transfer NLP models across\nlanguages. The success of this transfer is, however, dependent on the model's\nability to encode the patterns of cross-lingual similarity and variation. Yet,\nlittle is known as to how these models are able to do this. We propose a simple\nmethod to study how relationships between languages are encoded in two\nstate-of-the-art multilingual models (i.e. M-BERT and XLM-R). The results\nprovide insight into their information sharing mechanisms and suggest that\nlinguistic properties are encoded jointly across typologically-similar\nlanguages in these models.", "published": "2020-10-24 07:55:32", "link": "http://arxiv.org/abs/2010.12825v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text Editing by Command", "abstract": "A prevailing paradigm in neural text generation is one-shot generation, where\ntext is produced in a single step. The one-shot setting is inadequate, however,\nwhen the constraints the user wishes to impose on the generated text are\ndynamic, especially when authoring longer documents. We address this limitation\nwith an interactive text generation setting in which the user interacts with\nthe system by issuing commands to edit existing text. To this end, we propose a\nnovel text editing task, and introduce WikiDocEdits, a dataset of\nsingle-sentence edits crawled from Wikipedia. We show that our Interactive\nEditor, a transformer-based model trained on this dataset, outperforms\nbaselines and obtains positive results in both automatic and human evaluations.\nWe present empirical and qualitative analyses of this model's performance.", "published": "2020-10-24 08:00:30", "link": "http://arxiv.org/abs/2010.12826v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Context-aware Decoder for Neural Machine Translation using a Target-side\n  Document-Level Language Model", "abstract": "Although many context-aware neural machine translation models have been\nproposed to incorporate contexts in translation, most of those models are\ntrained end-to-end on parallel documents aligned in sentence-level. Because\nonly a few domains (and language pairs) have such document-level parallel data,\nwe cannot perform accurate context-aware translation in most domains. We\ntherefore present a simple method to turn a sentence-level translation model\ninto a context-aware model by incorporating a document-level language model\ninto the decoder. Our context-aware decoder is built upon only a sentence-level\nparallel corpora and monolingual corpora; thus no document-level parallel data\nis needed. In a theoretical viewpoint, the core part of this work is the novel\nrepresentation of contextual information using point-wise mutual information\nbetween context and the current sentence. We show the effectiveness of our\napproach in three language pairs, English to French, English to Russian, and\nJapanese to English, by evaluation in \\textsc{bleu} and contrastive tests for\ncontext-aware translation.", "published": "2020-10-24 08:06:18", "link": "http://arxiv.org/abs/2010.12827v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Speech Translation with Efficient Finetuning of Pretrained\n  Models", "abstract": "We present a simple yet effective approach to build multilingual\nspeech-to-text (ST) translation by efficient transfer learning from pretrained\nspeech encoder and text decoder. Our key finding is that a minimalistic LNA\n(LayerNorm and Attention) finetuning can achieve zero-shot crosslingual and\ncross-modality transfer ability by only finetuning less than 10% of the\npretrained parameters. This enables effectively leveraging large pretrained\nmodels with low training cost. Using wav2vec 2.0 for acoustic modeling, and\nmBART for multilingual text generation, our approach advanced the new\nstate-of-the-art for 34 translation directions (and surpassing cascaded ST for\n23 of them) on large-scale multilingual ST benchmark CoVoST 2 (+6.4 BLEU on\naverage across 15 En-X directions and +5.1 BLEU on average across 19 X-En\ndirections). Our approach demonstrates strong zero-shot performance in a\nmany-to-many multilingual model (+5.7 BLEU on average across 18 non-English\ndirections), making it an appealing approach for attaining high-quality speech\ntranslation with improved parameter and data efficiency.", "published": "2020-10-24 08:15:08", "link": "http://arxiv.org/abs/2010.12829v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GO FIGURE: A Meta Evaluation of Factuality in Summarization", "abstract": "While neural language models can generate text with remarkable fluency and\ncoherence, controlling for factual correctness in generation remains an open\nresearch question. This major discrepancy between the surface-level fluency and\nthe content-level correctness of neural generation has motivated a new line of\nresearch that seeks automatic metrics for evaluating the factuality of machine\ntext. In this paper, we introduce GO FIGURE, a meta-evaluation framework for\nevaluating factuality evaluation metrics. We propose five necessary and\nintuitive conditions to evaluate factuality metrics on diagnostic factuality\ndata across three different summarization tasks. Our benchmark analysis on ten\nfactuality metrics reveals that our meta-evaluation framework provides a robust\nand efficient evaluation that is extensible to multiple types of factual\nconsistency and standard generation metrics, including QA metrics. It also\nreveals that while QA metrics generally improve over standard metrics that\nmeasure factuality across domains, performance is highly dependent on the way\nin which questions are generated.", "published": "2020-10-24 08:30:20", "link": "http://arxiv.org/abs/2010.12834v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Zero and Few-Shot Abstractive Summarization with Intermediate\n  Fine-tuning and Data Augmentation", "abstract": "Models pretrained with self-supervised objectives on large text corpora\nachieve state-of-the-art performance on English text summarization tasks.\nHowever, these models are typically fine-tuned on hundreds of thousands of data\npoints, an infeasible requirement when applying summarization to new, niche\ndomains. In this work, we introduce a novel and generalizable method, called\nWikiTransfer, for fine-tuning pretrained models for summarization in an\nunsupervised, dataset-specific manner. WikiTransfer fine-tunes pretrained\nmodels on pseudo-summaries, produced from generic Wikipedia data, which contain\ncharacteristics of the target dataset, such as the length and level of\nabstraction of the desired summaries. WikiTransfer models achieve\nstate-of-the-art, zero-shot abstractive summarization performance on the\nCNN-DailyMail dataset and demonstrate the effectiveness of our approach on\nthree additional diverse datasets. These models are more robust to noisy data\nand also achieve better or comparable few-shot performance using 10 and 100\ntraining examples when compared to few-shot transfer from other summarization\ndatasets. To further boost performance, we employ data augmentation via\nround-trip translation as well as introduce a regularization term for improved\nfew-shot transfer. To understand the role of dataset aspects in transfer\nperformance and the quality of the resulting output summaries, we further study\nthe effect of the components of our unsupervised fine-tuning data and analyze\nfew-shot performance using both automatic and human evaluation.", "published": "2020-10-24 08:36:49", "link": "http://arxiv.org/abs/2010.12836v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CoCo: Controllable Counterfactuals for Evaluating Dialogue State\n  Trackers", "abstract": "Dialogue state trackers have made significant progress on benchmark datasets,\nbut their generalization capability to novel and realistic scenarios beyond the\nheld-out conversations is less understood. We propose controllable\ncounterfactuals (CoCo) to bridge this gap and evaluate dialogue state tracking\n(DST) models on novel scenarios, i.e., would the system successfully tackle the\nrequest if the user responded differently but still consistently with the\ndialogue flow? CoCo leverages turn-level belief states as counterfactual\nconditionals to produce novel conversation scenarios in two steps: (i)\ncounterfactual goal generation at turn-level by dropping and adding slots\nfollowed by replacing slot values, (ii) counterfactual conversation generation\nthat is conditioned on (i) and consistent with the dialogue flow. Evaluating\nstate-of-the-art DST models on MultiWOZ dataset with CoCo-generated\ncounterfactuals results in a significant performance drop of up to 30.8% (from\n49.4% to 18.6%) in absolute joint goal accuracy. In comparison, widely used\ntechniques like paraphrasing only affect the accuracy by at most 2%. Human\nevaluations show that COCO-generated conversations perfectly reflect the\nunderlying user goal with more than 95% accuracy and are as human-like as the\noriginal conversations, further strengthening its reliability and promise to be\nadopted as part of the robustness evaluation of DST models.", "published": "2020-10-24 09:39:35", "link": "http://arxiv.org/abs/2010.12850v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "When Being Unseen from mBERT is just the Beginning: Handling New\n  Languages With Multilingual Language Models", "abstract": "Transfer learning based on pretraining language models on a large amount of\nraw data has become a new norm to reach state-of-the-art performance in NLP.\nStill, it remains unclear how this approach should be applied for unseen\nlanguages that are not covered by any available large-scale multilingual\nlanguage model and for which only a small amount of raw data is generally\navailable. In this work, by comparing multilingual and monolingual models, we\nshow that such models behave in multiple ways on unseen languages. Some\nlanguages greatly benefit from transfer learning and behave similarly to\nclosely related high resource languages whereas others apparently do not.\nFocusing on the latter, we show that this failure to transfer is largely\nrelated to the impact of the script used to write such languages.\nTransliterating those languages improves very significantly the ability of\nlarge-scale multilingual language models on downstream tasks.", "published": "2020-10-24 10:15:03", "link": "http://arxiv.org/abs/2010.12858v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Task Learning with Shared Encoder for Non-Autoregressive Machine\n  Translation", "abstract": "Non-Autoregressive machine Translation (NAT) models have demonstrated\nsignificant inference speedup but suffer from inferior translation accuracy.\nThe common practice to tackle the problem is transferring the Autoregressive\nmachine Translation (AT) knowledge to NAT models, e.g., with knowledge\ndistillation. In this work, we hypothesize and empirically verify that AT and\nNAT encoders capture different linguistic properties of source sentences.\nTherefore, we propose to adopt Multi-Task learning to transfer the AT knowledge\nto NAT models through encoder sharing. Specifically, we take the AT model as an\nauxiliary task to enhance NAT model performance. Experimental results on WMT14\nEnglish-German and WMT16 English-Romanian datasets show that the proposed\nMulti-Task NAT achieves significant improvements over the baseline NAT models.\nFurthermore, the performance on large-scale WMT19 and WMT20 English-German\ndatasets confirm the consistency of our proposed method. In addition,\nexperimental results demonstrate that our Multi-Task NAT is complementary to\nknowledge distillation, the standard knowledge transfer method for NAT.", "published": "2020-10-24 11:00:58", "link": "http://arxiv.org/abs/2010.12868v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Contextualized Knowledge Structures for Commonsense Reasoning", "abstract": "Recently, knowledge graph (KG) augmented models have achieved noteworthy\nsuccess on various commonsense reasoning tasks. However, KG edge (fact)\nsparsity and noisy edge extraction/generation often hinder models from\nobtaining useful knowledge to reason over. To address these issues, we propose\na new KG-augmented model: Hybrid Graph Network (HGN). Unlike prior methods, HGN\nlearns to jointly contextualize extracted and generated knowledge by reasoning\nover both within a unified graph structure. Given the task input context and an\nextracted KG subgraph, HGN is trained to generate embeddings for the subgraph's\nmissing edges to form a \"hybrid\" graph, then reason over the hybrid graph while\nfiltering out context-irrelevant edges. We demonstrate HGN's effectiveness\nthrough considerable performance gains across four commonsense reasoning\nbenchmarks, plus a user study on edge validness and helpfulness.", "published": "2020-10-24 11:09:16", "link": "http://arxiv.org/abs/2010.12873v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting Neural Language Modelling with Syllables", "abstract": "Language modelling is regularly analysed at word, subword or character units,\nbut syllables are seldom used. Syllables provide shorter sequences than\ncharacters, they can be extracted with rules, and their segmentation typically\nrequires less specialised effort than identifying morphemes. We reconsider\nsyllables for an open-vocabulary generation task in 20 languages. We use\nrule-based syllabification methods for five languages and address the rest with\na hyphenation tool, which behaviour as syllable proxy is validated. With a\ncomparable perplexity, we show that syllables outperform characters, annotated\nmorphemes and unsupervised subwords. Finally, we also study the overlapping of\nsyllables concerning other subword pieces and discuss some limitations and\nopportunities.", "published": "2020-10-24 11:44:41", "link": "http://arxiv.org/abs/2010.12881v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FedE: Embedding Knowledge Graphs in Federated Setting", "abstract": "Knowledge graphs (KGs) consisting of triples are always incomplete, so it's\nimportant to do Knowledge Graph Completion (KGC) by predicting missing triples.\nMulti-Source KG is a common situation in real KG applications which can be\nviewed as a set of related individual KGs where different KGs contains\nrelations of different aspects of entities. It's intuitive that, for each\nindividual KG, its completion could be greatly contributed by the triples\ndefined and labeled in other ones. However, because of the data privacy and\nsensitivity, a set of relevant knowledge graphs cannot complement each other's\nKGC by just collecting data from different knowledge graphs together.\nTherefore, in this paper, we introduce federated setting to keep their privacy\nwithout triple transferring between KGs and apply it in embedding knowledge\ngraph, a typical method which have proven effective for KGC in the past decade.\nWe propose a Federated Knowledge Graph Embedding framework FedE, focusing on\nlearning knowledge graph embeddings by aggregating locally-computed updates.\nFinally, we conduct extensive experiments on datasets derived from KGE\nbenchmark datasets and results show the effectiveness of our proposed FedE.", "published": "2020-10-24 11:52:05", "link": "http://arxiv.org/abs/2010.12882v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NeuroLogic Decoding: (Un)supervised Neural Text Generation with\n  Predicate Logic Constraints", "abstract": "Conditional text generation often requires lexical constraints, i.e., which\nwords should or shouldn't be included in the output text. While the dominant\nrecipe for conditional text generation has been large-scale pretrained language\nmodels that are finetuned on the task-specific training data, such models do\nnot learn to follow the underlying constraints reliably, even when supervised\nwith large amounts of task-specific examples.\n  We propose NeuroLogic Decoding, a simple yet effective algorithm that enables\nneural language models -- supervised or not -- to generate fluent text while\nsatisfying complex lexical constraints. Our approach is powerful yet efficient.\nIt handles any set of lexical constraints that is expressible under predicate\nlogic, while its asymptotic runtime is equivalent to conventional beam search.\n  Empirical results on four benchmarks show that NeuroLogic Decoding\noutperforms previous approaches, including algorithms that handle a subset of\nour constraints. Moreover, we find that unsupervised models with NeuroLogic\nDecoding often outperform supervised models with conventional decoding, even\nwhen the latter is based on considerably larger networks. Our results suggest\nthe limit of large-scale neural networks for fine-grained controllable\ngeneration and the promise of inference-time algorithms.", "published": "2020-10-24 11:55:22", "link": "http://arxiv.org/abs/2010.12884v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Paraphrasing with Pretrained Language Models", "abstract": "Paraphrase generation has benefited extensively from recent progress in the\ndesigning of training objectives and model architectures. However, previous\nexplorations have largely focused on supervised methods, which require a large\namount of labeled data that is costly to collect. To address this drawback, we\nadopt a transfer learning approach and propose a training pipeline that enables\npre-trained language models to generate high-quality paraphrases in an\nunsupervised setting. Our recipe consists of task-adaptation, self-supervision,\nand a novel decoding algorithm named Dynamic Blocking (DB). To enforce a\nsurface form dissimilar from the input, whenever the language model emits a\ntoken contained in the source sequence, DB prevents the model from outputting\nthe subsequent source token for the next generation step. We show with\nautomatic and human evaluations that our approach achieves state-of-the-art\nperformance on both the Quora Question Pair (QQP) and the ParaNMT datasets and\nis robust to domain shift between the two datasets of distinct distributions.\nWe also demonstrate that our model transfers to paraphrasing in other languages\nwithout any additional finetuning.", "published": "2020-10-24 11:55:28", "link": "http://arxiv.org/abs/2010.12885v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Benchmark Corpus and Neural Approach for Sanskrit Derivative Nouns\n  Analysis", "abstract": "This paper presents first benchmark corpus of Sanskrit Pratyaya (suffix) and\ninflectional words (padas) formed due to suffixes along with neural network\nbased approaches to process the formation and splitting of inflectional words.\nInflectional words spans the primary and secondary derivative nouns as the\nscope of current work. Pratyayas are an important dimension of morphological\nanalysis of Sanskrit texts. There have been Sanskrit Computational Linguistics\ntools for processing and analyzing Sanskrit texts. Unfortunately there has not\nbeen any work to standardize & validate these tools specifically for derivative\nnouns analysis. In this work, we prepared a Sanskrit suffix benchmark corpus\ncalled Pratyaya-Kosh to evaluate the performance of tools. We also present our\nown neural approach for derivative nouns analysis while evaluating the same on\nmost prominent Sanskrit Morphological Analysis tools. This benchmark will be\nfreely dedicated and available to researchers worldwide and we hope it will\nmotivate all to improve morphological analysis in Sanskrit Language.", "published": "2020-10-24 17:22:44", "link": "http://arxiv.org/abs/2010.12937v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Compound-Word (Sandhi) Generation and Splitting in Sanskrit\n  Language", "abstract": "This paper describes neural network based approaches to the process of the\nformation and splitting of word-compounding, respectively known as the Sandhi\nand Vichchhed, in Sanskrit language. Sandhi is an important idea essential to\nmorphological analysis of Sanskrit texts. Sandhi leads to word transformations\nat word boundaries. The rules of Sandhi formation are well defined but complex,\nsometimes optional and in some cases, require knowledge about the nature of the\nwords being compounded. Sandhi split or Vichchhed is an even more difficult\ntask given its non uniqueness and context dependence. In this work, we propose\nthe route of formulating the problem as a sequence to sequence prediction task,\nusing modern deep learning techniques. Being the first fully data driven\ntechnique, we demonstrate that our model has an accuracy better than the\nexisting methods on multiple standard datasets, despite not using any\nadditional lexical or morphological resources. The code is being made available\nat https://github.com/IITD-DataScience/Sandhi_Prakarana", "published": "2020-10-24 18:02:40", "link": "http://arxiv.org/abs/2010.12940v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Auxiliary Sequence Labeling Tasks for Disfluency Detection", "abstract": "Detecting disfluencies in spontaneous speech is an important preprocessing\nstep in natural language processing and speech recognition applications.\nExisting works for disfluency detection have focused on designing a single\nobjective only for disfluency detection, while auxiliary objectives utilizing\nlinguistic information of a word such as named entity or part-of-speech\ninformation can be effective. In this paper, we focus on detecting disfluencies\non spoken transcripts and propose a method utilizing named entity recognition\n(NER) and part-of-speech (POS) as auxiliary sequence labeling (SL) tasks for\ndisfluency detection. First, we investigate cases that utilizing linguistic\ninformation of a word can prevent mispredicting important words and can be\nhelpful for the correct detection of disfluencies. Second, we show that\ntraining a disfluency detection model with auxiliary SL tasks can improve its\nF-score in disfluency detection. Then, we analyze which auxiliary SL tasks are\ninfluential depending on baseline models. Experimental results on the widely\nused English Switchboard dataset show that our method outperforms the previous\nstate-of-the-art in disfluency detection.", "published": "2020-10-24 02:51:17", "link": "http://arxiv.org/abs/2011.04512v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Interpretable Natural Language Understanding with Explanations\n  as Latent Variables", "abstract": "Recently generating natural language explanations has shown very promising\nresults in not only offering interpretable explanations but also providing\nadditional information and supervision for prediction. However, existing\napproaches usually require a large set of human annotated explanations for\ntraining while collecting a large set of explanations is not only time\nconsuming but also expensive. In this paper, we develop a general framework for\ninterpretable natural language understanding that requires only a small set of\nhuman annotated explanations for training. Our framework treats natural\nlanguage explanations as latent variables that model the underlying reasoning\nprocess of a neural model. We develop a variational EM framework for\noptimization where an explanation generation module and an\nexplanation-augmented prediction module are alternatively optimized and\nmutually enhance each other. Moreover, we further propose an explanation-based\nself-training method under this framework for semi-supervised learning. It\nalternates between assigning pseudo-labels to unlabeled data and generating new\nexplanations to iteratively improve each other. Experiments on two natural\nlanguage understanding tasks demonstrate that our framework can not only make\neffective predictions in both supervised and semi-supervised settings, but also\ngenerate good natural language explanation.", "published": "2020-10-24 02:05:56", "link": "http://arxiv.org/abs/2011.05268v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Effective Distant Supervision for Temporal Relation Extraction", "abstract": "A principal barrier to training temporal relation extraction models in new\ndomains is the lack of varied, high quality examples and the challenge of\ncollecting more. We present a method of automatically collecting\ndistantly-supervised examples of temporal relations. We scrape and\nautomatically label event pairs where the temporal relations are made explicit\nin text, then mask out those explicit cues, forcing a model trained on this\ndata to learn other signals. We demonstrate that a pre-trained Transformer\nmodel is able to transfer from the weakly labeled examples to human-annotated\nbenchmarks in both zero-shot and few-shot settings, and that the masking scheme\nis important in improving generalization.", "published": "2020-10-24 03:17:31", "link": "http://arxiv.org/abs/2010.12755v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Structure-Grounded Pretraining for Text-to-SQL", "abstract": "Learning to capture text-table alignment is essential for tasks like\ntext-to-SQL. A model needs to correctly recognize natural language references\nto columns and values and to ground them in the given database schema. In this\npaper, we present a novel weakly supervised Structure-Grounded pretraining\nframework (StruG) for text-to-SQL that can effectively learn to capture\ntext-table alignment based on a parallel text-table corpus. We identify a set\nof novel prediction tasks: column grounding, value grounding and column-value\nmapping, and leverage them to pretrain a text-table encoder. Additionally, to\nevaluate different methods under more realistic text-table alignment settings,\nwe create a new evaluation set Spider-Realistic based on Spider dev set with\nexplicit mentions of column names removed, and adopt eight existing text-to-SQL\ndatasets for cross-database evaluation. STRUG brings significant improvement\nover BERT-LARGE in all settings. Compared with existing pretraining methods\nsuch as GRAPPA, STRUG achieves similar performance on Spider, and outperforms\nall baselines on more realistic sets. The Spider-Realistic dataset is available\nat https://doi.org/10.5281/zenodo.5205322.", "published": "2020-10-24 04:35:35", "link": "http://arxiv.org/abs/2010.12773v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Multilingual Models with Language-Clustered Vocabularies", "abstract": "State-of-the-art multilingual models depend on vocabularies that cover all of\nthe languages the model will expect to see at inference time, but the standard\nmethods for generating those vocabularies are not ideal for massively\nmultilingual applications. In this work, we introduce a novel procedure for\nmultilingual vocabulary generation that combines the separately trained\nvocabularies of several automatically derived language clusters, thus balancing\nthe trade-off between cross-lingual subword sharing and language-specific\nvocabularies. Our experiments show improvements across languages on key\nmultilingual benchmark tasks TyDi QA (+2.9 F1), XNLI (+2.1\\%), and WikiAnn NER\n(+2.8 F1) and factor of 8 reduction in out-of-vocabulary rate, all without\nincreasing the size of the model or data.", "published": "2020-10-24 04:49:15", "link": "http://arxiv.org/abs/2010.12777v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Open-Domain Dialogue Generation Based on Pre-trained Language Models", "abstract": "Pre-trained language models have been successfully used in response\ngeneration for open-domain dialogue. Four main frameworks have been proposed:\n(1) Transformer-ED using Transformer encoder and decoder separately for source\nand target sentences; (2) Transformer-Dec using Transformer decoder for both\nsource and target sentences; (3) Transformer-MLM using Transformer decoder that\napplies bi-directional attention on the source side and left-to-right attention\non the target side with masked language model objective; and (4) Transformer-AR\nthat uses auto-regressive objective instead. In this study, we compare these\nframeworks on 3 datasets, and our comparison reveals that the best framework\nuses bidirectional attention on the source side and does not separate encoder\nand decoder. We also examine model discrepancy, and our experiments confirm\nthat the performance of a model is directly impacted by the underlying\ndiscrepancies. We then propose two correction methods to reduce the\ndiscrepancies, and both improve the model performance. These results show that\ndiscrepancies is an important factor to consider when we use a pre-trained\nmodel, and a reduction in discrepancies can lead to improved performance.", "published": "2020-10-24 04:52:28", "link": "http://arxiv.org/abs/2010.12780v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "COUGH: A Challenge Dataset and Models for COVID-19 FAQ Retrieval", "abstract": "We present a large, challenging dataset, COUGH, for COVID-19 FAQ retrieval.\nSimilar to a standard FAQ dataset, COUGH consists of three parts: FAQ Bank,\nQuery Bank and Relevance Set. The FAQ Bank contains ~16K FAQ items scraped from\n55 credible websites (e.g., CDC and WHO). For evaluation, we introduce Query\nBank and Relevance Set, where the former contains 1,236 human-paraphrased\nqueries while the latter contains ~32 human-annotated FAQ items for each query.\nWe analyze COUGH by testing different FAQ retrieval models built on top of BM25\nand BERT, among which the best model achieves 48.8 under P@5, indicating a\ngreat challenge presented by COUGH and encouraging future research for further\nimprovement. Our COUGH dataset is available at\nhttps://github.com/sunlab-osu/covid-faq.", "published": "2020-10-24 06:30:59", "link": "http://arxiv.org/abs/2010.12800v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Rethinking embedding coupling in pre-trained language models", "abstract": "We re-evaluate the standard practice of sharing weights between input and\noutput embeddings in state-of-the-art pre-trained language models. We show that\ndecoupled embeddings provide increased modeling flexibility, allowing us to\nsignificantly improve the efficiency of parameter allocation in the input\nembedding of multilingual models. By reallocating the input embedding\nparameters in the Transformer layers, we achieve dramatically better\nperformance on standard natural language understanding tasks with the same\nnumber of parameters during fine-tuning. We also show that allocating\nadditional capacity to the output embedding provides benefits to the model that\npersist through the fine-tuning stage even though the output embedding is\ndiscarded after pre-training. Our analysis shows that larger output embeddings\nprevent the model's last layers from overspecializing to the pre-training task\nand encourage Transformer representations to be more general and more\ntransferable to other tasks and languages. Harnessing these findings, we are\nable to train models that achieve strong performance on the XTREME benchmark\nwithout increasing the number of parameters at the fine-tuning stage.", "published": "2020-10-24 07:43:00", "link": "http://arxiv.org/abs/2010.12821v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Keyphrase Extraction with Dynamic Graph Convolutional Networks and\n  Diversified Inference", "abstract": "Keyphrase extraction (KE) aims to summarize a set of phrases that accurately\nexpress a concept or a topic covered in a given document. Recently,\nSequence-to-Sequence (Seq2Seq) based generative framework is widely used in KE\ntask, and it has obtained competitive performance on various benchmarks. The\nmain challenges of Seq2Seq methods lie in acquiring informative latent document\nrepresentation and better modeling the compositionality of the target\nkeyphrases set, which will directly affect the quality of generated keyphrases.\nIn this paper, we propose to adopt the Dynamic Graph Convolutional Networks\n(DGCN) to solve the above two problems simultaneously. Concretely, we explore\nto integrate dependency trees with GCN for latent representation learning.\nMoreover, the graph structure in our model is dynamically modified during the\nlearning process according to the generated keyphrases. To this end, our\napproach is able to explicitly learn the relations within the keyphrases\ncollection and guarantee the information interchange between encoder and\ndecoder in both directions. Extensive experiments on various KE benchmark\ndatasets demonstrate the effectiveness of our approach.", "published": "2020-10-24 08:11:23", "link": "http://arxiv.org/abs/2010.12828v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FLIN: A Flexible Natural Language Interface for Web Navigation", "abstract": "AI assistants can now carry out tasks for users by directly interacting with\nwebsite UIs. Current semantic parsing and slot-filling techniques cannot\nflexibly adapt to many different websites without being constantly re-trained.\nWe propose FLIN, a natural language interface for web navigation that maps user\ncommands to concept-level actions (rather than low-level UI actions), thus\nbeing able to flexibly adapt to different websites and handle their transient\nnature. We frame this as a ranking problem: given a user command and a webpage,\nFLIN learns to score the most relevant navigation instruction (involving action\nand parameter values). To train and evaluate FLIN, we collect a dataset using\nnine popular websites from three domains. Our results show that FLIN was able\nto adapt to new websites in a given domain.", "published": "2020-10-24 09:11:26", "link": "http://arxiv.org/abs/2010.12844v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ReadOnce Transformers: Reusable Representations of Text for Transformers", "abstract": "We present ReadOnce Transformers, an approach to convert a transformer-based\nmodel into one that can build an information-capturing, task-independent, and\ncompressed representation of text. The resulting representation is reusable\nacross different examples and tasks, thereby requiring a document shared across\nmany examples or tasks to only be \\emph{read once}. This leads to faster\ntraining and evaluation of models. Additionally, we extend standard\ntext-to-text transformer models to Representation+Text-to-text models, and\nevaluate on multiple downstream tasks: multi-hop QA, abstractive QA, and\nlong-document summarization. Our one-time computed representation results in a\n2x-5x speedup compared to standard text-to-text models, while the compression\nalso allows existing language models to handle longer documents without the\nneed for designing new pre-trained models.", "published": "2020-10-24 09:53:16", "link": "http://arxiv.org/abs/2010.12854v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On Transferability of Bias Mitigation Effects in Language Model\n  Fine-Tuning", "abstract": "Fine-tuned language models have been shown to exhibit biases against\nprotected groups in a host of modeling tasks such as text classification and\ncoreference resolution. Previous works focus on detecting these biases,\nreducing bias in data representations, and using auxiliary training objectives\nto mitigate bias during fine-tuning. Although these techniques achieve bias\nreduction for the task and domain at hand, the effects of bias mitigation may\nnot directly transfer to new tasks, requiring additional data collection and\ncustomized annotation of sensitive attributes, and re-evaluation of appropriate\nfairness metrics. We explore the feasibility and benefits of upstream bias\nmitigation (UBM) for reducing bias on downstream tasks, by first applying bias\nmitigation to an upstream model through fine-tuning and subsequently using it\nfor downstream fine-tuning. We find, in extensive experiments across hate\nspeech detection, toxicity detection, occupation prediction, and coreference\nresolution tasks over various bias factors, that the effects of UBM are indeed\ntransferable to new downstream tasks or domains via fine-tuning, creating less\nbiased downstream models than directly fine-tuning on the downstream task or\ntransferring from a vanilla upstream model. Though challenges remain, we show\nthat UBM promises more efficient and accessible bias mitigation in LM\nfine-tuning.", "published": "2020-10-24 10:36:11", "link": "http://arxiv.org/abs/2010.12864v2", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Large Scale Legal Text Classification Using Transformer Models", "abstract": "Large multi-label text classification is a challenging Natural Language\nProcessing (NLP) problem that is concerned with text classification for\ndatasets with thousands of labels. We tackle this problem in the legal domain,\nwhere datasets, such as JRC-Acquis and EURLEX57K labeled with the EuroVoc\nvocabulary were created within the legal information systems of the European\nUnion. The EuroVoc taxonomy includes around 7000 concepts. In this work, we\nstudy the performance of various recent transformer-based models in combination\nwith strategies such as generative pretraining, gradual unfreezing and\ndiscriminative learning rates in order to reach competitive classification\nperformance, and present new state-of-the-art results of 0.661 (F1) for\nJRC-Acquis and 0.754 for EURLEX57K. Furthermore, we quantify the impact of\nindividual steps, such as language model fine-tuning or gradual unfreezing in\nan ablation study, and provide reference dataset splits created with an\niterative stratification algorithm.", "published": "2020-10-24 11:03:01", "link": "http://arxiv.org/abs/2010.12871v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Word Embeddings for Chemical Patent Natural Language Processing", "abstract": "We evaluate chemical patent word embeddings against known biomedical\nembeddings and show that they outperform the latter extrinsically and\nintrinsically. We also show that using contextualized embeddings can induce\npredictive models of reasonable performance for this domain over a relatively\nsmall gold standard.", "published": "2020-10-24 15:03:20", "link": "http://arxiv.org/abs/2010.12912v1", "categories": ["cs.CL", "cs.LG", "68T50", "I.2.7; J.3; C.4"], "primary_category": "cs.CL"}
{"title": "Causal Effects of Linguistic Properties", "abstract": "We consider the problem of using observational data to estimate the causal\neffects of linguistic properties. For example, does writing a complaint\npolitely lead to a faster response time? How much will a positive product\nreview increase sales? This paper addresses two technical challenges related to\nthe problem before developing a practical method. First, we formalize the\ncausal quantity of interest as the effect of a writer's intent, and establish\nthe assumptions necessary to identify this from observational data. Second, in\npractice, we only have access to noisy proxies for the linguistic properties of\ninterest -- e.g., predictions from classifiers and lexicons. We propose an\nestimator for this setting and prove that its bias is bounded when we perform\nan adjustment for the text. Based on these results, we introduce TextCause, an\nalgorithm for estimating causal effects of linguistic properties. The method\nleverages (1) distant supervision to improve the quality of noisy proxies, and\n(2) a pre-trained language model (BERT) to adjust for the text. We show that\nthe proposed method outperforms related approaches when estimating the effect\nof Amazon review sentiment on semi-simulated sales figures. Finally, we present\nan applied case study investigating the effects of complaint politeness on\nbureaucratic response times.", "published": "2020-10-24 15:43:37", "link": "http://arxiv.org/abs/2010.12919v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Disease Normalization with Graph Embeddings", "abstract": "The detection and normalization of diseases in biomedical texts are key\nbiomedical natural language processing tasks. Disease names need not only be\nidentified, but also normalized or linked to clinical taxonomies describing\ndiseases such as MeSH. In this paper we describe deep learning methods that\ntackle both tasks. We train and test our methods on the known NCBI disease\nbenchmark corpus. We propose to represent disease names by leveraging MeSH's\ngraphical structure together with the lexical information available in the\ntaxonomy using graph embeddings. We also show that combining neural named\nentity recognition models with our graph-based entity linking methods via\nmultitask learning leads to improved disease recognition in the NCBI corpus.", "published": "2020-10-24 16:25:05", "link": "http://arxiv.org/abs/2010.12925v1", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7; J.3"], "primary_category": "cs.CL"}
{"title": "Pre-trained Summarization Distillation", "abstract": "Recent state-of-the-art approaches to summarization utilize large pre-trained\nTransformer models. Distilling these models to smaller student models has\nbecome critically important for practical use; however there are many different\ndistillation methods proposed by the NLP literature. Recent work on distilling\nBERT for classification and regression tasks shows strong performance using\ndirect knowledge distillation. Alternatively, machine translation practitioners\ndistill using pseudo-labeling, where a small model is trained on the\ntranslations of a larger model. A third, simpler approach is to 'shrink and\nfine-tune' (SFT), which avoids any explicit distillation by copying parameters\nto a smaller student model and then fine-tuning. We compare these three\napproaches for distillation of Pegasus and BART, the current and former state\nof the art, pre-trained summarization models, and find that SFT outperforms\nknowledge distillation and pseudo-labeling on the CNN/DailyMail dataset, but\nunder-performs pseudo-labeling on the more abstractive XSUM dataset. PyTorch\nCode and checkpoints of different sizes are available through Hugging Face\ntransformers here http://tiny.cc/4iy0tz.", "published": "2020-10-24 23:15:43", "link": "http://arxiv.org/abs/2010.13002v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Jointly Optimizing State Operation Prediction and Value Generation for\n  Dialogue State Tracking", "abstract": "We investigate the problem of multi-domain Dialogue State Tracking (DST) with\nopen vocabulary. Existing approaches exploit BERT encoder and copy-based RNN\ndecoder, where the encoder predicts the state operation, and the decoder\ngenerates new slot values. However, in such a stacked encoder-decoder\nstructure, the operation prediction objective only affects the BERT encoder and\nthe value generation objective mainly affects the RNN decoder. In this paper,\nwe propose a purely Transformer-based framework, where a single BERT works as\nboth the encoder and the decoder. In so doing, the operation prediction\nobjective and the value generation objective can jointly optimize this BERT for\nDST. At the decoding step, we re-use the hidden states of the encoder in the\nself-attention mechanism of the corresponding decoder layers to construct a\nflat encoder-decoder architecture for effective parameter updating.\nExperimental results show that our approach substantially outperforms the\nexisting state-of-the-art framework, and it also achieves very competitive\nperformance to the best ontology-based approaches.", "published": "2020-10-24 04:54:52", "link": "http://arxiv.org/abs/2010.14061v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Pre-training Text-to-Text Transformers for Concept-centric Common Sense", "abstract": "Pre-trained language models (PTLM) have achieved impressive results in a\nrange of natural language understanding (NLU) and generation (NLG) tasks.\nHowever, current pre-training objectives such as masked token prediction (for\nBERT-style PTLMs) and masked span infilling (for T5-style PTLMs) do not\nexplicitly model the relational commonsense knowledge about everyday concepts,\nwhich is crucial to many downstream tasks that need common sense to understand\nor generate. To augment PTLMs with concept-centric commonsense knowledge, in\nthis paper, we propose both generative and contrastive objectives for learning\ncommon sense from the text, and use them as intermediate self-supervised\nlearning tasks for incrementally pre-training PTLMs (before task-specific\nfine-tuning on downstream datasets). Furthermore, we develop a joint\npre-training framework to unify generative and contrastive objectives so that\nthey can mutually reinforce each other. Extensive experimental results show\nthat our method, concept-aware language model (CALM), can pack more commonsense\nknowledge into the parameters of a pre-trained text-to-text transformer without\nrelying on external knowledge graphs, yielding better performance on both NLU\nand NLG tasks. We show that while only incrementally pre-trained on a\nrelatively small corpus for a few steps, CALM outperforms baseline methods by a\nconsistent margin and even comparable with some larger PTLMs, which suggests\nthat CALM can serve as a general, plug-and-play method for improving the\ncommonsense reasoning ability of a PTLM.", "published": "2020-10-24 07:00:37", "link": "http://arxiv.org/abs/2011.07956v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Modular Networks for Compositional Instruction Following", "abstract": "Standard architectures used in instruction following often struggle on novel\ncompositions of subgoals (e.g. navigating to landmarks or picking up objects)\nobserved during training. We propose a modular architecture for following\nnatural language instructions that describe sequences of diverse subgoals. In\nour approach, subgoal modules each carry out natural language instructions for\na specific subgoal type. A sequence of modules to execute is chosen by learning\nto segment the instructions and predicting a subgoal type for each segment.\nWhen compared to standard, non-modular sequence-to-sequence approaches on\nALFRED, a challenging instruction following benchmark, we find that\nmodularization improves generalization to novel subgoal compositions, as well\nas to environments unseen in training.", "published": "2020-10-24 03:48:45", "link": "http://arxiv.org/abs/2010.12764v2", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "X-Class: Text Classification with Extremely Weak Supervision", "abstract": "In this paper, we explore text classification with extremely weak\nsupervision, i.e., only relying on the surface text of class names. This is a\nmore challenging setting than the seed-driven weak supervision, which allows a\nfew seed words per class. We opt to attack this problem from a representation\nlearning perspective -- ideal document representations should lead to nearly\nthe same results between clustering and the desired classification. In\nparticular, one can classify the same corpus differently (e.g., based on topics\nand locations), so document representations should be adaptive to the given\nclass names. We propose a novel framework X-Class to realize the adaptive\nrepresentations. Specifically, we first estimate class representations by\nincrementally adding the most similar word to each class until inconsistency\narises. Following a tailored mixture of class attention mechanisms, we obtain\nthe document representation via a weighted average of contextualized word\nrepresentations. With the prior of each document assigned to its nearest class,\nwe then cluster and align the documents to classes. Finally, we pick the most\nconfident documents from each cluster to train a text classifier. Extensive\nexperiments demonstrate that X-Class can rival and even outperform seed-driven\nweakly supervised methods on 7 benchmark datasets. Our dataset and code are\nreleased at https://github.com/ZihanWangKi/XClass/ .", "published": "2020-10-24 06:09:51", "link": "http://arxiv.org/abs/2010.12794v2", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unsupervised Vision-and-Language Pre-training Without Parallel Images\n  and Captions", "abstract": "Pre-trained contextual vision-and-language (V&L) models have achieved\nimpressive performance on various benchmarks. However, existing models require\na large amount of parallel image-caption data for pre-training. Such data are\ncostly to collect and require cumbersome curation. Inspired by unsupervised\nmachine translation, we investigate if a strong V&L representation model can be\nlearned through unsupervised pre-training without image-caption corpora. In\nparticular, we propose to conduct ``mask-and-predict'' pre-training on\ntext-only and image-only corpora and introduce the object tags detected by an\nobject recognition model as anchor points to bridge two modalities. We find\nthat such a simple approach achieves performance close to a model pre-trained\nwith aligned data, on four English V&L benchmarks. Our work challenges the\nwidely held notion that aligned data is necessary for V&L pre-training, while\nsignificantly reducing the amount of supervision needed for V&L models.", "published": "2020-10-24 08:17:54", "link": "http://arxiv.org/abs/2010.12831v2", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning to Deceive Knowledge Graph Augmented Models via Targeted\n  Perturbation", "abstract": "Knowledge graphs (KGs) have helped neural models improve performance on\nvarious knowledge-intensive tasks, like question answering and item\nrecommendation. By using attention over the KG, such KG-augmented models can\nalso \"explain\" which KG information was most relevant for making a given\nprediction. In this paper, we question whether these models are really behaving\nas we expect. We show that, through a reinforcement learning policy (or even\nsimple heuristics), one can produce deceptively perturbed KGs, which maintain\nthe downstream performance of the original KG while significantly deviating\nfrom the original KG's semantics and structure. Our findings raise doubts about\nKG-augmented models' ability to reason about KG information and give sensible\nexplanations.", "published": "2020-10-24 11:04:45", "link": "http://arxiv.org/abs/2010.12872v6", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unsupervised Learning of Disentangled Speech Content and Style\n  Representation", "abstract": "We present an approach for unsupervised learning of speech representation\ndisentangling contents and styles. Our model consists of: (1) a local encoder\nthat captures per-frame information; (2) a global encoder that captures\nper-utterance information; and (3) a conditional decoder that reconstructs\nspeech given local and global latent variables. Our experiments show that (1)\nthe local latent variables encode speech contents, as reconstructed speech can\nbe recognized by ASR with low word error rates (WER), even with a different\nglobal encoding; (2) the global latent variables encode speaker style, as\nreconstructed speech shares speaker identity with the source utterance of the\nglobal encoding. Additionally, we demonstrate an useful application from our\npre-trained model, where we can train a speaker recognition model from the\nglobal latent variables and achieve high accuracy by fine-tuning with as few\ndata as one label per speaker.", "published": "2020-10-24 20:16:03", "link": "http://arxiv.org/abs/2010.12973v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Align-Refine: Non-Autoregressive Speech Recognition via Iterative\n  Realignment", "abstract": "Non-autoregressive models greatly improve decoding speed over typical\nsequence-to-sequence models, but suffer from degraded performance. Infilling\nand iterative refinement models make up some of this gap by editing the outputs\nof a non-autoregressive model, but are constrained in the edits that they can\nmake. We propose iterative realignment, where refinements occur over latent\nalignments rather than output sequence space. We demonstrate this in speech\nrecognition with Align-Refine, an end-to-end Transformer-based model which\nrefines connectionist temporal classification (CTC) alignments to allow\nlength-changing insertions and deletions. Align-Refine outperforms Imputer and\nMask-CTC, matching an autoregressive baseline on WSJ at 1/14th the real-time\nfactor and attaining a LibriSpeech test-other WER of 9.0% without an LM. Our\nmodel is strong even in one iteration with a shallower decoder.", "published": "2020-10-24 09:35:37", "link": "http://arxiv.org/abs/2010.14233v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD", "I.2.7"], "primary_category": "eess.AS"}
{"title": "Differentiable Open-Ended Commonsense Reasoning", "abstract": "Current commonsense reasoning research focuses on developing models that use\ncommonsense knowledge to answer multiple-choice questions. However, systems\ndesigned to answer multiple-choice questions may not be useful in applications\nthat do not provide a small list of candidate answers to choose from. As a step\ntowards making commonsense reasoning research more realistic, we propose to\nstudy open-ended commonsense reasoning (OpenCSR) -- the task of answering a\ncommonsense question without any pre-defined choices -- using as a resource\nonly a corpus of commonsense facts written in natural language. OpenCSR is\nchallenging due to a large decision space, and because many questions require\nimplicit multi-hop reasoning. As an approach to OpenCSR, we propose DrFact, an\nefficient Differentiable model for multi-hop Reasoning over knowledge Facts. To\nevaluate OpenCSR methods, we adapt several popular commonsense reasoning\nbenchmarks, and collect multiple new answers for each test question via\ncrowd-sourcing. Experiments show that DrFact outperforms strong baseline\nmethods by a large margin.", "published": "2020-10-24 10:07:00", "link": "http://arxiv.org/abs/2010.14439v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The DKU-DukeECE Systems for VoxCeleb Speaker Recognition Challenge 2020", "abstract": "In this paper, we present the system submission for the VoxCeleb Speaker\nRecognition Challenge 2020 (VoxSRC-20) by the DKU-DukeECE team. For track 1, we\nexplore various kinds of state-of-the-art front-end extractors with different\npooling layers and objective loss functions. For track 3, we employ an\niterative framework for self-supervised speaker representation learning based\non a deep neural network (DNN). For track 4, we investigate the whole system\npipeline for speaker diarization, including voice activity detection (VAD),\nuniform segmentation, speaker embedding extraction, and clustering.", "published": "2020-10-24 01:16:59", "link": "http://arxiv.org/abs/2010.12731v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "X-TaSNet: Robust and Accurate Time-Domain Speaker Extraction Network", "abstract": "Extracting the speech of a target speaker from mixed audios, based on a\nreference speech from the target speaker, is a challenging yet powerful\ntechnology in speech processing. Recent studies of speaker-independent speech\nseparation, such as TasNet, have shown promising results by applying deep\nneural networks over the time-domain waveform. Such separation neural network\ndoes not directly generate reliable and accurate output when target speakers\nare specified, because of the necessary prior on the number of speakers and the\nlack of robustness when dealing with audios with absent speakers. In this\npaper, we break these limitations by introducing a new speaker-aware speech\nmasking method, called X-TaSNet. Our proposal adopts new strategies, including\na distortion-based loss and corresponding alternating training scheme, to\nbetter address the robustness issue. X-TaSNet significantly enhances the\nextracted speech quality, doubling SDRi and SI-SNRi of the output speech audio\nover state-of-the-art voice filtering approach. X-TaSNet also improves the\nreliability of the results by improving the accuracy of speaker identity in the\noutput audio to 95.4%, such that it returns silent audios in most cases when\nthe target speaker is absent. These results demonstrate X-TaSNet moves one\nsolid step towards more practical applications of speaker extraction\ntechnology.", "published": "2020-10-24 03:57:19", "link": "http://arxiv.org/abs/2010.12766v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Learning Fine-Grained Cross Modality Excitement for Speech Emotion\n  Recognition", "abstract": "Speech emotion recognition is a challenging task because the emotion\nexpression is complex, multimodal and fine-grained. In this paper, we propose a\nnovel multimodal deep learning approach to perform fine-grained emotion\nrecognition from real-life speeches. We design a temporal alignment mean-max\npooling mechanism to capture the subtle and fine-grained emotions implied in\nevery utterance. In addition, we propose a cross modality excitement module to\nconduct sample-specific adjustment on cross modality embeddings and adaptively\nrecalibrate the corresponding values by its aligned latent features from the\nother modality. Our proposed model is evaluated on two well-known real-world\nspeech emotion recognition datasets. The results demonstrate that our approach\nis superior on the prediction tasks for multimodal speech utterances, and it\noutperforms a wide range of baselines in terms of prediction accuracy. Further\nmore, we conduct detailed ablation studies to show that our temporal alignment\nmean-max pooling mechanism and cross modality excitement significantly\ncontribute to the promising results. In order to encourage the research\nreproducibility, we make the code publicly available at\n\\url{https://github.com/tal-ai/FG_CME.git}.", "published": "2020-10-24 01:17:58", "link": "http://arxiv.org/abs/2010.12733v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "GAZEV: GAN-Based Zero-Shot Voice Conversion over Non-parallel Speech\n  Corpus", "abstract": "Non-parallel many-to-many voice conversion is recently attract-ing huge\nresearch efforts in the speech processing community. A voice conversion system\ntransforms an utterance of a source speaker to another utterance of a target\nspeaker by keeping the content in the original utterance and replacing by the\nvocal features from the target speaker. Existing solutions, e.g., StarGAN-VC2,\npresent promising results, only when speech corpus of the engaged speakers is\navailable during model training. AUTOVCis able to perform voice conversion on\nunseen speakers, but it needs an external pretrained speaker verification\nmodel. In this paper, we present our new GAN-based zero-shot voice conversion\nsolution, called GAZEV, which targets to support unseen speakers on both source\nand target utterances. Our key technical contribution is the adoption of\nspeaker embedding loss on top of the GAN framework, as well as adaptive\ninstance normalization strategy, in order to address the limitations of speaker\nidentity transfer in existing solutions. Our empirical evaluations demonstrate\nsignificant performance improvement on output speech quality and comparable\nspeaker similarity to AUTOVC.", "published": "2020-10-24 05:31:15", "link": "http://arxiv.org/abs/2010.12788v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Y-Vector: Multiscale Waveform Encoder for Speaker Embedding", "abstract": "State-of-the-art text-independent speaker verification systems typically use\ncepstral features or filter bank energies as speech features. Recent studies\nattempted to extract speaker embeddings directly from raw waveforms and have\nshown competitive results. In this paper, we propose a novel multi-scale\nwaveform encoder that uses three convolution branches with different time\nscales to compute speech features from the waveform. These features are then\nprocessed by squeeze-and-excitation blocks, a multi-level feature aggregator,\nand a time delayed neural network (TDNN) to compute speaker embedding. We show\nthat the proposed embeddings outperform existing raw-waveform-based speaker\nembeddings on speaker verification by a large margin. A further analysis of the\nlearned filters shows that the multi-scale encoder attends to different\nfrequency bands at its different scales while resulting in a more flat overall\nfrequency response than any of the single-scale counterparts.", "published": "2020-10-24 18:44:00", "link": "http://arxiv.org/abs/2010.12951v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Stop Bugging Me! Evading Modern-Day Wiretapping Using Adversarial\n  Perturbations", "abstract": "Mass surveillance systems for voice over IP (VoIP) conversations pose a great\nrisk to privacy. These automated systems use learning models to analyze\nconversations, and calls that involve specific topics are routed to a human\nagent for further examination. In this study, we present an\nadversarial-learning-based framework for privacy protection for VoIP\nconversations. We present a novel method that finds a universal adversarial\nperturbation (UAP), which, when added to the audio stream, prevents an\neavesdropper from automatically detecting the conversation's topic. As shown in\nour experiments, the UAP is agnostic to the speaker or audio length, and its\nvolume can be changed in real time, as needed. Our real-world solution uses a\nTeensy microcontroller that acts as an external microphone and adds the UAP to\nthe audio in real time. We examine different speakers, VoIP applications\n(Skype, Zoom, Slack, and Google Meet), and audio lengths. Our results in the\nreal world suggest that our approach is a feasible solution for privacy\nprotection.", "published": "2020-10-24 06:56:35", "link": "http://arxiv.org/abs/2010.12809v2", "categories": ["cs.SD", "cs.CR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Comparison of Discrete Latent Variable Models for Speech\n  Representation Learning", "abstract": "Neural latent variable models enable the discovery of interesting structure\nin speech audio data. This paper presents a comparison of two different\napproaches which are broadly based on predicting future time-steps or\nauto-encoding the input signal. Our study compares the representations learned\nby vq-vae and vq-wav2vec in terms of sub-word unit discovery and phoneme\nrecognition performance. Results show that future time-step prediction with\nvq-wav2vec achieves better performance. The best system achieves an error rate\nof 13.22 on the ZeroSpeech 2019 ABX phoneme discrimination challenge", "published": "2020-10-24 01:22:14", "link": "http://arxiv.org/abs/2010.14230v1", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
