{"title": "Semantic Matching of Documents from Heterogeneous Collections: A Simple\n  and Transparent Method for Practical Applications", "abstract": "We present a very simple, unsupervised method for the pairwise matching of\ndocuments from heterogeneous collections. We demonstrate our method with the\nConcept-Project matching task, which is a binary classification task involving\npairs of documents from heterogeneous collections. Although our method only\nemploys standard resources without any domain- or task-specific modifications,\nit clearly outperforms the more complex system of the original authors. In\naddition, our method is transparent, because it provides explicit information\nabout how a similarity score was computed, and efficient, because it is based\non the aggregation of (pre-computable) word-level similarities.", "published": "2019-04-29 10:21:58", "link": "http://arxiv.org/abs/1904.12550v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Adversarial Learning Framework For A Persona-Based Multi-Turn\n  Dialogue Model", "abstract": "In this paper, we extend the persona-based sequence-to-sequence (Seq2Seq)\nneural network conversation model to a multi-turn dialogue scenario by\nmodifying the state-of-the-art hredGAN architecture to simultaneously capture\nutterance attributes such as speaker identity, dialogue topic, speaker\nsentiments and so on. The proposed system, phredGAN has a persona-based HRED\ngenerator (PHRED) and a conditional discriminator. We also explore two\napproaches to accomplish the conditional discriminator: (1) phredGAN_a, a\nsystem that passes the attribute representation as an additional input into a\ntraditional adversarial discriminator, and (2) phredGAN_d, a dual discriminator\nsystem which in addition to the adversarial discriminator, collaboratively\npredicts the attribute(s) that generated the input utterance. To demonstrate\nthe superior performance of phredGAN over the persona Seq2Seq model, we\nexperiment with two conversational datasets, the Ubuntu Dialogue Corpus (UDC)\nand TV series transcripts from the Big Bang Theory and Friends. Performance\ncomparison is made with respect to a variety of quantitative measures as well\nas crowd-sourced human evaluation. We also explore the trade-offs from using\neither variant of phredGAN on datasets with many but weak attribute modalities\n(such as with Big Bang Theory and Friends) and ones with few but strong\nattribute modalities (customer-agent interactions in Ubuntu dataset).", "published": "2019-04-29 15:21:13", "link": "http://arxiv.org/abs/1905.01992v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "A Persona-based Multi-turn Conversation Model in an Adversarial Learning\n  Framework", "abstract": "In this paper, we extend the persona-based sequence-to-sequence (Seq2Seq)\nneural network conversation model to multi-turn dialogue by modifying the\nstate-of-the-art hredGAN architecture. To achieve this, we introduce an\nadditional input modality into the encoder and decoder of hredGAN to capture\nother attributes such as speaker identity, location, sub-topics, and other\nexternal attributes that might be available from the corpus of human-to-human\ninteractions. The resulting persona hredGAN ($phredGAN$) shows better\nperformance than both the existing persona-based Seq2Seq and hredGAN models\nwhen those external attributes are available in a multi-turn dialogue corpus.\nThis superiority is demonstrated on TV drama series with character consistency\n(such as Big Bang Theory and Friends) and customer service interaction datasets\nsuch as Ubuntu dialogue corpus in terms of perplexity, BLEU, ROUGE, and\nDistinct n-gram scores.", "published": "2019-04-29 15:09:34", "link": "http://arxiv.org/abs/1905.01998v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Enabling Robots to Understand Incomplete Natural Language Instructions\n  Using Commonsense Reasoning", "abstract": "Enabling robots to understand instructions provided via spoken natural\nlanguage would facilitate interaction between robots and people in a variety of\nsettings in homes and workplaces. However, natural language instructions are\noften missing information that would be obvious to a human based on\nenvironmental context and common sense, and hence does not need to be\nexplicitly stated. In this paper, we introduce Language-Model-based Commonsense\nReasoning (LMCR), a new method which enables a robot to listen to a natural\nlanguage instruction from a human, observe the environment around it, and\nautomatically fill in information missing from the instruction using\nenvironmental context and a new commonsense reasoning approach. Our approach\nfirst converts an instruction provided as unconstrained natural language into a\nform that a robot can understand by parsing it into verb frames. Our approach\nthen fills in missing information in the instruction by observing objects in\nits vicinity and leveraging commonsense reasoning. To learn commonsense\nreasoning automatically, our approach distills knowledge from large\nunstructured textual corpora by training a language model. Our results show the\nfeasibility of a robot learning commonsense knowledge automatically from\nweb-based textual corpora, and the power of learned commonsense reasoning\nmodels in enabling a robot to autonomously perform tasks based on incomplete\nnatural language instructions.", "published": "2019-04-29 18:59:59", "link": "http://arxiv.org/abs/1904.12907v2", "categories": ["cs.RO", "cs.AI", "cs.CL"], "primary_category": "cs.RO"}
{"title": "Unsupervised Extraction of Phenotypes from Cancer Clinical Notes for\n  Association Studies", "abstract": "The recent adoption of Electronic Health Records (EHRs) by health care\nproviders has introduced an important source of data that provides detailed and\nhighly specific insights into patient phenotypes over large cohorts. These\ndatasets, in combination with machine learning and statistical approaches,\ngenerate new opportunities for research and clinical care. However, many\nmethods require the patient representations to be in structured formats, while\nthe information in the EHR is often locked in unstructured texts designed for\nhuman readability. In this work, we develop the methodology to automatically\nextract clinical features from clinical narratives from large EHR corpora\nwithout the need for prior knowledge. We consider medical terms and sentences\nappearing in clinical narratives as atomic information units. We propose an\nefficient clustering strategy suitable for the analysis of large text corpora\nand to utilize the clusters to represent information about the patient\ncompactly. To demonstrate the utility of our approach, we perform an\nassociation study of clinical features with somatic mutation profiles from\n4,007 cancer patients and their tumors. We apply the proposed algorithm to a\ndataset consisting of about 65 thousand documents with a total of about 3.2\nmillion sentences. We identify 341 significant statistical associations between\nthe presence of somatic mutations and clinical features. We annotated these\nassociations according to their novelty, and report several known associations.\nWe also propose 32 testable hypotheses where the underlying biological\nmechanism does not appear to be known but plausible. These results illustrate\nthat the automated discovery of clinical features is possible and the joint\nanalysis of clinical and genetic datasets can generate appealing new\nhypotheses.", "published": "2019-04-29 22:15:22", "link": "http://arxiv.org/abs/1904.12973v2", "categories": ["cs.LG", "cs.CL", "stat.AP", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Adversarial Speaker Verification", "abstract": "The use of deep networks to extract embeddings for speaker recognition has\nproven successfully. However, such embeddings are susceptible to performance\ndegradation due to the mismatches among the training, enrollment, and test\nconditions. In this work, we propose an adversarial speaker verification (ASV)\nscheme to learn the condition-invariant deep embedding via adversarial\nmulti-task training. In ASV, a speaker classification network and a condition\nidentification network are jointly optimized to minimize the speaker\nclassification loss and simultaneously mini-maximize the condition loss. The\ntarget labels of the condition network can be categorical (environment types)\nand continuous (SNR values). We further propose multi-factorial ASV to\nsimultaneously suppress multiple factors that constitute the condition\nvariability. Evaluated on a Microsoft Cortana text-dependent speaker\nverification task, the ASV achieves 8.8% and 14.5% relative improvements in\nequal error rates (EER) for known and unknown conditions, respectively.", "published": "2019-04-29 00:37:27", "link": "http://arxiv.org/abs/1904.12406v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Adversarial Speaker Adaptation", "abstract": "We propose a novel adversarial speaker adaptation (ASA) scheme, in which\nadversarial learning is applied to regularize the distribution of deep hidden\nfeatures in a speaker-dependent (SD) deep neural network (DNN) acoustic model\nto be close to that of a fixed speaker-independent (SI) DNN acoustic model\nduring adaptation. An additional discriminator network is introduced to\ndistinguish the deep features generated by the SD model from those produced by\nthe SI model. In ASA, with a fixed SI model as the reference, an SD model is\njointly optimized with the discriminator network to minimize the senone\nclassification loss, and simultaneously to mini-maximize the SI/SD\ndiscrimination loss on the adaptation data. With ASA, a senone-discriminative\ndeep feature is learned in the SD model with a similar distribution to that of\nthe SI model. With such a regularized and adapted deep feature, the SD model\ncan perform improved automatic speech recognition on the target speaker's\nspeech. Evaluated on the Microsoft short message dictation dataset, ASA\nachieves 14.4% and 7.9% relative word error rate improvements for supervised\nand unsupervised adaptation, respectively, over an SI model trained from 2600\nhours data, with 200 adaptation utterances per speaker.", "published": "2019-04-29 00:38:16", "link": "http://arxiv.org/abs/1904.12407v1", "categories": ["cs.LG", "cs.CL", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Logician: A Unified End-to-End Neural Approach for Open-Domain\n  Information Extraction", "abstract": "In this paper, we consider the problem of open information extraction (OIE)\nfor extracting entity and relation level intermediate structures from sentences\nin open-domain. We focus on four types of valuable intermediate structures\n(Relation, Attribute, Description, and Concept), and propose a unified\nknowledge expression form, SAOKE, to express them. We publicly release a data\nset which contains more than forty thousand sentences and the corresponding\nfacts in the SAOKE format labeled by crowd-sourcing. To our knowledge, this is\nthe largest publicly available human labeled data set for open information\nextraction tasks. Using this labeled SAOKE data set, we train an end-to-end\nneural model using the sequenceto-sequence paradigm, called Logician, to\ntransform sentences into facts. For each sentence, different to existing\nalgorithms which generally focus on extracting each single fact without\nconcerning other possible facts, Logician performs a global optimization over\nall possible involved facts, in which facts not only compete with each other to\nattract the attention of words, but also cooperate to share words. An\nexperimental study on various types of open domain relation extraction tasks\nreveals the consistent superiority of Logician to other states-of-the-art\nalgorithms. The experiments verify the reasonableness of SAOKE format, the\nvaluableness of SAOKE data set, the effectiveness of the proposed Logician\nmodel, and the feasibility of the methodology to apply end-to-end learning\nparadigm on supervised data sets for the challenging tasks of open information\nextraction.", "published": "2019-04-29 09:37:31", "link": "http://arxiv.org/abs/1904.12535v1", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unsupervised Data Augmentation for Consistency Training", "abstract": "Semi-supervised learning lately has shown much promise in improving deep\nlearning models when labeled data is scarce. Common among recent approaches is\nthe use of consistency training on a large amount of unlabeled data to\nconstrain model predictions to be invariant to input noise. In this work, we\npresent a new perspective on how to effectively noise unlabeled examples and\nargue that the quality of noising, specifically those produced by advanced data\naugmentation methods, plays a crucial role in semi-supervised learning. By\nsubstituting simple noising operations with advanced data augmentation methods\nsuch as RandAugment and back-translation, our method brings substantial\nimprovements across six language and three vision tasks under the same\nconsistency training framework. On the IMDb text classification dataset, with\nonly 20 labeled examples, our method achieves an error rate of 4.20,\noutperforming the state-of-the-art model trained on 25,000 labeled examples. On\na standard semi-supervised learning benchmark, CIFAR-10, our method outperforms\nall previous approaches and achieves an error rate of 5.43 with only 250\nexamples. Our method also combines well with transfer learning, e.g., when\nfinetuning from BERT, and yields improvements in high-data regime, such as\nImageNet, whether when there is only 10% labeled data or when a full labeled\nset with 1.3M extra unlabeled examples is used. Code is available at\nhttps://github.com/google-research/uda.", "published": "2019-04-29 17:56:59", "link": "http://arxiv.org/abs/1904.12848v6", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "A Comparison of Online Automatic Speech Recognition Systems and the\n  Nonverbal Responses to Unintelligible Speech", "abstract": "Automatic Speech Recognition (ASR) systems have proliferated over the recent\nyears to the point that free platforms such as YouTube now provide speech\nrecognition services. Given the wide selection of ASR systems, we contribute to\nthe field of automatic speech recognition by comparing the relative performance\nof two sets of manual transcriptions and five sets of automatic transcriptions\n(Google Cloud, IBM Watson, Microsoft Azure, Trint, and YouTube) to help\nresearchers to select accurate transcription services. In addition, we identify\nnonverbal behaviors that are associated with unintelligible speech, as\nindicated by high word error rates. We show that manual transcriptions remain\nsuperior to current automatic transcriptions. Amongst the automatic\ntranscription services, YouTube offers the most accurate transcription service.\nFor non-verbal behavioral involvement, we provide evidence that the variability\nof smile intensities from the listener is high (low) when the speaker is clear\n(unintelligible). These findings are derived from videoconferencing\ninteractions between student doctors and simulated patients; therefore, we\ncontribute towards both the ASR literature and the healthcare communication\nskills teaching community.", "published": "2019-04-29 00:00:27", "link": "http://arxiv.org/abs/1904.12403v1", "categories": ["cs.SD", "eess.AS", "H.5.5"], "primary_category": "cs.SD"}
{"title": "Localization, Detection and Tracking of Multiple Moving Sound Sources\n  with a Convolutional Recurrent Neural Network", "abstract": "This paper investigates the joint localization, detection, and tracking of\nsound events using a convolutional recurrent neural network (CRNN). We use a\nCRNN previously proposed for the localization and detection of stationary\nsources, and show that the recurrent layers enable the spatial tracking of\nmoving sources when trained with dynamic scenes. The tracking performance of\nthe CRNN is compared with a stand-alone tracking method that combines a\nmulti-source (DOA) estimator and a particle filter. Their respective\nperformance is evaluated in various acoustic conditions such as anechoic and\nreverberant scenarios, stationary and moving sources at several angular\nvelocities, and with a varying number of overlapping sources. The results show\nthat the CRNN manages to track multiple sources more consistently than the\nparametric method across acoustic scenarios, but at the cost of higher\nlocalization error.", "published": "2019-04-29 15:26:47", "link": "http://arxiv.org/abs/1904.12769v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Semi-supervised Acoustic Event Detection based on tri-training", "abstract": "This paper presents our work of training acoustic event detection (AED)\nmodels using unlabeled dataset. Recent acoustic event detectors are based on\nlarge-scale neural networks, which are typically trained with huge amounts of\nlabeled data. Labels for acoustic events are expensive to obtain, and relevant\nacoustic event audios can be limited, especially for rare events. In this paper\nwe leverage an Internet-scale unlabeled dataset with potential domain shift to\nimprove the detection of acoustic events. Based on the classic tri-training\napproach, our proposed method shows accuracy improvement over both the\nsupervised training baseline, and semisupervised self-training set-up, in all\npre-defined acoustic event detection tasks. As our approach relies on ensemble\nmodels, we further show the improvements can be distilled to a single model via\nknowledge distillation, with the resulting single student model maintaining\nhigh accuracy of teacher ensemble models.", "published": "2019-04-29 19:51:22", "link": "http://arxiv.org/abs/1904.12926v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
