{"title": "Story Ending Generation with Incremental Encoding and Commonsense\n  Knowledge", "abstract": "Generating a reasonable ending for a given story context, i.e., story ending\ngeneration, is a strong indication of story comprehension. This task requires\nnot only to understand the context clues which play an important role in\nplanning the plot but also to handle implicit knowledge to make a reasonable,\ncoherent story.\n  In this paper, we devise a novel model for story ending generation. The model\nadopts an incremental encoding scheme to represent context clues which are\nspanning in the story context. In addition, commonsense knowledge is applied\nthrough multi-source attention to facilitate story comprehension, and thus to\nhelp generate coherent and reasonable endings. Through building context clues\nand using implicit knowledge, the model is able to produce reasonable story\nendings. context clues implied in the post and make the inference based on it.\n  Automatic and manual evaluation shows that our model can generate more\nreasonable story endings than state-of-the-art baselines.", "published": "2018-08-30 04:30:33", "link": "http://arxiv.org/abs/1808.10113v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Direct Output Connection for a High-Rank Language Model", "abstract": "This paper proposes a state-of-the-art recurrent neural network (RNN)\nlanguage model that combines probability distributions computed not only from a\nfinal RNN layer but also from middle layers. Our proposed method raises the\nexpressive power of a language model based on the matrix factorization\ninterpretation of language modeling introduced by Yang et al. (2018). The\nproposed method improves the current state-of-the-art language model and\nachieves the best score on the Penn Treebank and WikiText-2, which are the\nstandard benchmark datasets. Moreover, we indicate our proposed method\ncontributes to two application tasks: machine translation and headline\ngeneration. Our code is publicly available at:\nhttps://github.com/nttcslab-nlp/doc_lm.", "published": "2018-08-30 07:03:51", "link": "http://arxiv.org/abs/1808.10143v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards a Better Metric for Evaluating Question Generation Systems", "abstract": "There has always been criticism for using $n$-gram based similarity metrics,\nsuch as BLEU, NIST, etc, for evaluating the performance of NLG systems.\nHowever, these metrics continue to remain popular and are recently being used\nfor evaluating the performance of systems which automatically generate\nquestions from documents, knowledge graphs, images, etc. Given the rising\ninterest in such automatic question generation (AQG) systems, it is important\nto objectively examine whether these metrics are suitable for this task. In\nparticular, it is important to verify whether such metrics used for evaluating\nAQG systems focus on answerability of the generated question by preferring\nquestions which contain all relevant information such as question type\n(Wh-types), entities, relations, etc. In this work, we show that current\nautomatic evaluation metrics based on $n$-gram similarity do not always\ncorrelate well with human judgments about answerability of a question. To\nalleviate this problem and as a first step towards better evaluation metrics\nfor AQG, we introduce a scoring function to capture answerability and show that\nwhen this scoring function is integrated with existing metrics, they correlate\nsignificantly better with human judgments. The scripts and data developed as a\npart of this work are made publicly available at\nhttps://github.com/PrekshaNema25/Answerability-Metric", "published": "2018-08-30 09:12:19", "link": "http://arxiv.org/abs/1808.10192v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pronoun Translation in English-French Machine Translation: An Analysis\n  of Error Types", "abstract": "Pronouns are a long-standing challenge in machine translation. We present a\nstudy of the performance of a range of rule-based, statistical and neural MT\nsystems on pronoun translation based on an extensive manual evaluation using\nthe PROTEST test suite, which enables a fine-grained analysis of different\npronoun types and sheds light on the difficulties of the task. We find that the\nrule-based approaches in our corpus perform poorly as a result of\noversimplification, whereas SMT and early NMT systems exhibit significant\nshortcomings due to a lack of awareness of the functional and referential\nproperties of pronouns. A recent Transformer-based NMT system with\ncross-sentence context shows very promising results on non-anaphoric pronouns\nand intra-sentential anaphora, but there is still considerable room for\nimprovement in examples with cross-sentence dependencies.", "published": "2018-08-30 09:30:14", "link": "http://arxiv.org/abs/1808.10196v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to adapt: a meta-learning approach for speaker adaptation", "abstract": "The performance of automatic speech recognition systems can be improved by\nadapting an acoustic model to compensate for the mismatch between training and\ntesting conditions, for example by adapting to unseen speakers. The success of\nspeaker adaptation methods relies on selecting weights that are suitable for\nadaptation and using good adaptation schedules to update these weights in order\nnot to overfit to the adaptation data. In this paper we investigate a\nprincipled way of adapting all the weights of the acoustic model using a\nmeta-learning. We show that the meta-learner can learn to perform supervised\nand unsupervised speaker adaptation and that it outperforms a strong baseline\nadapting LHUC parameters when adapting a DNN AM with 1.5M parameters. We also\nreport initial experiments on adapting TDNN AMs, where the meta-learner\nachieves comparable performance with LHUC.", "published": "2018-08-30 11:47:07", "link": "http://arxiv.org/abs/1808.10239v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparative Studies of Detecting Abusive Language on Twitter", "abstract": "The context-dependent nature of online aggression makes annotating large\ncollections of data extremely difficult. Previously studied datasets in abusive\nlanguage detection have been insufficient in size to efficiently train deep\nlearning models. Recently, Hate and Abusive Speech on Twitter, a dataset much\ngreater in size and reliability, has been released. However, this dataset has\nnot been comprehensively studied to its potential. In this paper, we conduct\nthe first comparative study of various learning models on Hate and Abusive\nSpeech on Twitter, and discuss the possibility of using additional features and\ncontext data for improvements. Experimental results show that bidirectional GRU\nnetworks trained on word-level features, with Latent Topic Clustering modules,\nis the most accurate model scoring 0.805 F1.", "published": "2018-08-30 12:15:31", "link": "http://arxiv.org/abs/1808.10245v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Source Syntactic Neural Machine Translation", "abstract": "We introduce a novel multi-source technique for incorporating source syntax\ninto neural machine translation using linearized parses. This is achieved by\nemploying separate encoders for the sequential and parsed versions of the same\nsource sentence; the resulting representations are then combined using a\nhierarchical attention mechanism. The proposed model improves over both seq2seq\nand parsed baselines by over 1 BLEU on the WMT17 English-German task. Further\nanalysis shows that our multi-source syntactic model is able to translate\nsuccessfully without any parsed input, unlike standard parsed methods. In\naddition, performance does not deteriorate as much on long sentences as for the\nbaselines.", "published": "2018-08-30 13:18:57", "link": "http://arxiv.org/abs/1808.10267v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Acquiring Annotated Data with Cross-lingual Explicitation for Implicit\n  Discourse Relation Classification", "abstract": "Implicit discourse relation classification is one of the most challenging and\nimportant tasks in discourse parsing, due to the lack of connective as strong\nlinguistic cues. A principle bottleneck to further improvement is the shortage\nof training data (ca.~16k instances in the PDTB). Shi et al. (2017) proposed to\nacquire additional data by exploiting connectives in translation: human\ntranslators mark discourse relations which are implicit in the source language\nexplicitly in the translation. Using back-translations of such explicitated\nconnectives improves discourse relation parsing performance. This paper\naddresses the open question of whether the choice of the translation language\nmatters, and whether multiple translations into different languages can be\neffectively used to improve the quality of the additional data.", "published": "2018-08-30 13:36:04", "link": "http://arxiv.org/abs/1808.10290v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generalize Symbolic Knowledge With Neural Rule Engine", "abstract": "As neural networks have dominated the state-of-the-art results in a wide\nrange of NLP tasks, it attracts considerable attention to improve the\nperformance of neural models by integrating symbolic knowledge. Different from\nexisting works, this paper investigates the combination of these two powerful\nparadigms from the knowledge-driven side. We propose Neural Rule Engine (NRE),\nwhich can learn knowledge explicitly from logic rules and then generalize them\nimplicitly with neural networks. NRE is implemented with neural module networks\nin which each module represents an action of a logic rule. The experiments show\nthat NRE could greatly improve the generalization abilities of logic rules with\na significant increase in recall. Meanwhile, the precision is still maintained\nat a high level.", "published": "2018-08-30 14:51:43", "link": "http://arxiv.org/abs/1808.10326v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling Empathy and Distress in Reaction to News Stories", "abstract": "Computational detection and understanding of empathy is an important factor\nin advancing human-computer interaction. Yet to date, text-based empathy\nprediction has the following major limitations: It underestimates the\npsychological complexity of the phenomenon, adheres to a weak notion of ground\ntruth where empathic states are ascribed by third parties, and lacks a shared\ncorpus. In contrast, this contribution presents the first publicly available\ngold standard for empathy prediction. It is constructed using a novel\nannotation methodology which reliably captures empathy assessments by the\nwriter of a statement using multi-item scales. This is also the first\ncomputational work distinguishing between multiple forms of empathy, empathic\nconcern, and personal distress, as recognized throughout psychology. Finally,\nwe present experimental results for three different predictive models, of which\na CNN performs the best.", "published": "2018-08-30 17:07:47", "link": "http://arxiv.org/abs/1808.10399v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Attaining the Unattainable? Reassessing Claims of Human Parity in Neural\n  Machine Translation", "abstract": "We reassess a recent study (Hassan et al., 2018) that claimed that machine\ntranslation (MT) has reached human parity for the translation of news from\nChinese into English, using pairwise ranking and considering three variables\nthat were not taken into account in that previous study: the language in which\nthe source side of the test set was originally written, the translation\nproficiency of the evaluators, and the provision of inter-sentential context.\nIf we consider only original source text (i.e. not translated from another\nlanguage, or translationese), then we find evidence showing that human parity\nhas not been achieved. We compare the judgments of professional translators\nagainst those of non-experts and discover that those of the experts result in\nhigher inter-annotator agreement and better discrimination between human and\nmachine translations. In addition, we analyse the human translations of the\ntest set and identify important translation issues. Finally, based on these\nfindings, we provide a set of recommendations for future human evaluations of\nMT.", "published": "2018-08-30 17:49:42", "link": "http://arxiv.org/abs/1808.10432v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Syntactic Scaffolds for Semantic Structures", "abstract": "We introduce the syntactic scaffold, an approach to incorporating syntactic\ninformation into semantic tasks. Syntactic scaffolds avoid expensive syntactic\nprocessing at runtime, only making use of a treebank during training, through a\nmultitask objective. We improve over strong baselines on PropBank semantics,\nframe semantics, and coreference resolution, achieving competitive performance\non all three tasks.", "published": "2018-08-30 19:01:20", "link": "http://arxiv.org/abs/1808.10485v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Iterative Recursive Attention Model for Interpretable Sequence\n  Classification", "abstract": "Natural language processing has greatly benefited from the introduction of\nthe attention mechanism. However, standard attention models are of limited\ninterpretability for tasks that involve a series of inference steps. We\ndescribe an iterative recursive attention model, which constructs incremental\nrepresentations of input data through reusing results of previously computed\nqueries. We train our model on sentiment classification datasets and\ndemonstrate its capacity to identify and combine different aspects of the input\nin an easily interpretable manner, while obtaining performance close to the\nstate of the art.", "published": "2018-08-30 20:19:02", "link": "http://arxiv.org/abs/1808.10503v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Chinese Discourse Segmentation Using Bilingual Discourse Commonality", "abstract": "Discourse segmentation aims to segment Elementary Discourse Units (EDUs) and\nis a fundamental task in discourse analysis. For Chinese, previous researches\nidentify EDUs just through discriminating the functions of punctuations. In\nthis paper, we argue that Chinese EDUs may not end at the punctuation positions\nand should follow the definition of EDU in RST-DT. With this definition, we\nconduct Chinese discourse segmentation with the help of English labeled\ndata.Using discourse commonality between English and Chinese, we design an\nadversarial neural network framework to extract common language-independent\nfeatures and language-specific features which are useful for discourse\nsegmentation, when there is no or only a small scale of Chinese labeled data\navailable. Experiments on discourse segmentation demonstrate that our models\ncan leverage common features from bilingual data, and learn efficient\nChinese-specific features from a small amount of Chinese labeled data,\noutperforming the baseline models.", "published": "2018-08-30 00:57:09", "link": "http://arxiv.org/abs/1809.01497v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning Neural Templates for Text Generation", "abstract": "While neural, encoder-decoder models have had significant empirical success\nin text generation, there remain several unaddressed problems with this style\nof generation. Encoder-decoder models are largely (a) uninterpretable, and (b)\ndifficult to control in terms of their phrasing or content. This work proposes\na neural generation system using a hidden semi-markov model (HSMM) decoder,\nwhich learns latent, discrete templates jointly with learning to generate. We\nshow that this model learns useful templates, and that these templates make\ngeneration both more interpretable and controllable. Furthermore, we show that\nthis approach scales to real data sets and achieves strong performance nearing\nthat of encoder-decoder text generation models.", "published": "2018-08-30 05:15:42", "link": "http://arxiv.org/abs/1808.10122v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Skip-gram word embeddings in hyperbolic space", "abstract": "Recent work has demonstrated that embeddings of tree-like graphs in\nhyperbolic space surpass their Euclidean counterparts in performance by a large\nmargin. Inspired by these results and scale-free structure in the word\nco-occurrence graph, we present an algorithm for learning word embeddings in\nhyperbolic space from free text. An objective function based on the hyperbolic\ndistance is derived and included in the skip-gram negative-sampling\narchitecture of word2vec. The hyperbolic word embeddings are then evaluated on\nword similarity and analogy benchmarks. The results demonstrate the potential\nof hyperbolic word embeddings, particularly in low dimensions, though without\nclear superiority over their Euclidean counterparts. We further discuss\nsubtleties in the formulation of the analogy task in curved spaces.", "published": "2018-08-30 13:54:45", "link": "http://arxiv.org/abs/1809.01498v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML", "I.2.7"], "primary_category": "cs.CL"}
{"title": "End-to-end Speech Recognition with Adaptive Computation Steps", "abstract": "In this paper, we present Adaptive Computation Steps (ACS) algo-rithm, which\nenables end-to-end speech recognition models to dy-namically decide how many\nframes should be processed to predict a linguistic output. The model that\napplies ACS algorithm follows the encoder-decoder framework, while unlike the\nattention-based mod-els, it produces alignments independently at the encoder\nside using the correlation between adjacent frames. Thus, predictions can be\nmade as soon as sufficient acoustic information is received, which makes the\nmodel applicable in online cases. Besides, a small change is made to the\ndecoding stage of the encoder-decoder framework, which allows the prediction to\nexploit bidirectional contexts. We verify the ACS algorithm on a Mandarin\nspeech corpus AIShell-1, and it achieves a 31.2% CER in the online occasion,\ncompared to the 32.4% CER of the attention-based model. To fully demonstrate\nthe advantage of ACS algorithm, offline experiments are conducted, in which our\nACS model achieves an 18.7% CER, outperforming the attention-based counterpart\nwith the CER of 22.0%.", "published": "2018-08-30 02:33:02", "link": "http://arxiv.org/abs/1808.10088v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Semi-Supervised Training for Improving Data Efficiency in End-to-End\n  Speech Synthesis", "abstract": "Although end-to-end text-to-speech (TTS) models such as Tacotron have shown\nexcellent results, they typically require a sizable set of high-quality <text,\naudio> pairs for training, which are expensive to collect. In this paper, we\npropose a semi-supervised training framework to improve the data efficiency of\nTacotron. The idea is to allow Tacotron to utilize textual and acoustic\nknowledge contained in large, publicly-available text and speech corpora.\nImportantly, these external data are unpaired and potentially noisy.\nSpecifically, first we embed each word in the input text into word vectors and\ncondition the Tacotron encoder on them. We then use an unpaired speech corpus\nto pre-train the Tacotron decoder in the acoustic domain. Finally, we fine-tune\nthe model using available paired data. We demonstrate that the proposed\nframework enables Tacotron to generate intelligible speech using less than half\nan hour of paired training data.", "published": "2018-08-30 05:51:30", "link": "http://arxiv.org/abs/1808.10128v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
