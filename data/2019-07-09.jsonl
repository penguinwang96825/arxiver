{"title": "NTT's Machine Translation Systems for WMT19 Robustness Task", "abstract": "This paper describes NTT's submission to the WMT19 robustness task. This task\nmainly focuses on translating noisy text (e.g., posts on Twitter), which\npresents different difficulties from typical translation tasks such as news.\nOur submission combined techniques including utilization of a synthetic corpus,\ndomain adaptation, and a placeholder mechanism, which significantly improved\nover the previous baseline. Experimental results revealed the placeholder\nmechanism, which temporarily replaces the non-standard tokens including emojis\nand emoticons with special placeholder tokens during translation, improves\ntranslation accuracy even with noisy texts.", "published": "2019-07-09 01:11:23", "link": "http://arxiv.org/abs/1907.03927v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Implicit Discourse Relation Identification for Open-domain Dialogues", "abstract": "Discourse relation identification has been an active area of research for\nmany years, and the challenge of identifying implicit relations remains largely\nan unsolved task, especially in the context of an open-domain dialogue system.\nPrevious work primarily relies on a corpora of formal text which is inherently\nnon-dialogic, i.e., news and journals. This data however is not suitable to\nhandle the nuances of informal dialogue nor is it capable of navigating the\nplethora of valid topics present in open-domain dialogue. In this paper, we\ndesigned a novel discourse relation identification pipeline specifically tuned\nfor open-domain dialogue systems. We firstly propose a method to automatically\nextract the implicit discourse relation argument pairs and labels from a\ndataset of dialogic turns, resulting in a novel corpus of discourse relation\npairs; the first of its kind to attempt to identify the discourse relations\nconnecting the dialogic turns in open-domain discourse. Moreover, we have taken\nthe first steps to leverage the dialogue features unique to our task to further\nimprove the identification of such relations by performing feature ablation and\nincorporating dialogue features to enhance the state-of-the-art model.", "published": "2019-07-09 03:58:46", "link": "http://arxiv.org/abs/1907.03975v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentiment and position-taking analysis of parliamentary debates: A\n  systematic literature review", "abstract": "Parliamentary and legislative debate transcripts provide access to\ninformation concerning the opinions, positions and policy preferences of\nelected politicians. They attract attention from researchers from a wide\nvariety of backgrounds, from political and social sciences to computer science.\nAs a result, the problem of automatic sentiment and position-taking analysis\nhas been tackled from different perspectives, using varying approaches and\nmethods, and with relatively little collaboration or cross-pollination of\nideas. The existing research is scattered across publications from various\nfields and venues. In this article we present the results of a systematic\nliterature review of 61 studies, all of which address the automatic analysis of\nthe sentiment and opinions expressed and positions taken by speakers in\nparliamentary (and other legislative) debates. In this review, we discuss the\navailable research with regard to the aims and objectives of the researchers\nwho work on these problems, the automatic analysis tasks they undertake, and\nthe approaches and methods they use. We conclude by summarizing their findings,\ndiscussing the challenges of applying computational analysis to parliamentary\ndebates, and suggesting possible avenues for further research.", "published": "2019-07-09 13:06:31", "link": "http://arxiv.org/abs/1907.04126v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Universal Sentence Encoder for Semantic Retrieval", "abstract": "We introduce two pre-trained retrieval focused multilingual sentence encoding\nmodels, respectively based on the Transformer and CNN model architectures. The\nmodels embed text from 16 languages into a single semantic space using a\nmulti-task trained dual-encoder that learns tied representations using\ntranslation based bridge tasks (Chidambaram al., 2018). The models provide\nperformance that is competitive with the state-of-the-art on: semantic\nretrieval (SR), translation pair bitext retrieval (BR) and retrieval question\nanswering (ReQA). On English transfer learning tasks, our sentence-level\nembeddings approach, and in some cases exceed, the performance of monolingual,\nEnglish only, sentence embedding models. Our models are made available for\ndownload on TensorFlow Hub.", "published": "2019-07-09 17:46:17", "link": "http://arxiv.org/abs/1907.04307v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-Domain Generalization of Neural Constituency Parsers", "abstract": "Neural parsers obtain state-of-the-art results on benchmark treebanks for\nconstituency parsing -- but to what degree do they generalize to other domains?\nWe present three results about the generalization of neural parsers in a\nzero-shot setting: training on trees from one corpus and evaluating on\nout-of-domain corpora. First, neural and non-neural parsers generalize\ncomparably to new domains. Second, incorporating pre-trained encoder\nrepresentations into neural parsers substantially improves their performance\nacross all domains, but does not give a larger relative improvement for\nout-of-domain treebanks. Finally, despite the rich input representations they\nlearn, neural parsers still benefit from structured output prediction of output\ntrees, yielding higher exact match accuracy and stronger generalization both to\nlarger text spans and to out-of-domain corpora. We analyze generalization on\nEnglish and Chinese corpora, and in the process obtain state-of-the-art parsing\nresults for the Brown, Genia, and English Web treebanks.", "published": "2019-07-09 18:05:59", "link": "http://arxiv.org/abs/1907.04347v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Don't Take the Premise for Granted: Mitigating Artifacts in Natural\n  Language Inference", "abstract": "Natural Language Inference (NLI) datasets often contain hypothesis-only\nbiases---artifacts that allow models to achieve non-trivial performance without\nlearning whether a premise entails a hypothesis. We propose two probabilistic\nmethods to build models that are more robust to such biases and better transfer\nacross datasets. In contrast to standard approaches to NLI, our methods predict\nthe probability of a premise given a hypothesis and NLI label, discouraging\nmodels from ignoring the premise. We evaluate our methods on synthetic and\nexisting NLI datasets by training on datasets containing biases and testing on\ndatasets containing no (or different) hypothesis-only biases. Our results\nindicate that these methods can make NLI models more robust to dataset-specific\nartifacts, transferring better than a baseline architecture in 9 out of 12 NLI\ndatasets. Additionally, we provide an extensive analysis of the interplay of\nour methods with known biases in NLI datasets, as well as the effects of\nencouraging models to ignore biases and fine-tuning on target datasets.", "published": "2019-07-09 19:40:47", "link": "http://arxiv.org/abs/1907.04380v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On Adversarial Removal of Hypothesis-only Bias in Natural Language\n  Inference", "abstract": "Popular Natural Language Inference (NLI) datasets have been shown to be\ntainted by hypothesis-only biases. Adversarial learning may help models ignore\nsensitive biases and spurious correlations in data. We evaluate whether\nadversarial learning can be used in NLI to encourage models to learn\nrepresentations free of hypothesis-only biases. Our analyses indicate that the\nrepresentations learned via adversarial learning may be less biased, with only\nsmall drops in NLI accuracy.", "published": "2019-07-09 20:04:24", "link": "http://arxiv.org/abs/1907.04389v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural or Statistical: An Empirical Study on Language Models for Chinese\n  Input Recommendation on Mobile", "abstract": "Chinese input recommendation plays an important role in alleviating human\ncost in typing Chinese words, especially in the scenario of mobile\napplications. The fundamental problem is to predict the conditional probability\nof the next word given the sequence of previous words. Therefore, statistical\nlanguage models, i.e.~n-grams based models, have been extensively used on this\ntask in real application. However, the characteristics of extremely different\ntyping behaviors usually lead to serious sparsity problem, even n-gram with\nsmoothing will fail. A reasonable approach to tackle this problem is to use the\nrecently proposed neural models, such as probabilistic neural language model,\nrecurrent neural network and word2vec. They can leverage more semantically\nsimilar words for estimating the probability. However, there is no conclusion\non which approach of the two will work better in real application. In this\npaper, we conduct an extensive empirical study to show the differences between\nstatistical and neural language models. The experimental results show that the\ntwo different approach have individual advantages, and a hybrid approach will\nbring a significant improvement.", "published": "2019-07-09 04:39:24", "link": "http://arxiv.org/abs/1907.05340v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hahahahaha, Duuuuude, Yeeessss!: A two-parameter characterization of\n  stretchable words and the dynamics of mistypings and misspellings", "abstract": "Stretched words like `heellllp' or `heyyyyy' are a regular feature of spoken\nlanguage, often used to emphasize or exaggerate the underlying meaning of the\nroot word. While stretched words are rarely found in formal written language\nand dictionaries, they are prevalent within social media. In this paper, we\nexamine the frequency distributions of `stretchable words' found in roughly 100\nbillion tweets authored over an 8 year period. We introduce two central\nparameters, `balance' and `stretch', that capture their main characteristics,\nand explore their dynamics by creating visual tools we call `balance plots' and\n`spelling trees'. We discuss how the tools and methods we develop here could be\nused to study the statistical patterns of mistypings and misspellings, along\nwith the potential applications in augmenting dictionaries, improving language\nprocessing, and in any area where sequence construction matters, such as\ngenetics.", "published": "2019-07-09 00:44:23", "link": "http://arxiv.org/abs/1907.03920v1", "categories": ["cs.CL", "physics.soc-ph"], "primary_category": "cs.CL"}
{"title": "Systematic quantitative analyses reveal the folk-zoological knowledge\n  embedded in folktales", "abstract": "Cultural learning is a unique human capacity essential for a wide range of\nadaptations. Researchers have argued that folktales have the pedagogical\nfunction of transmitting the essential information for the environment. The\nmost important knowledge for foraging and pastoral society is folk-zoological\nknowledge, such as the predator-prey relationship among wild animals, or\nbetween wild and domesticated animals. Here, we analysed the descriptions of\nthe 382 animal folktales using the natural language processing method and\ndescriptive statistics listed in a worldwide tale-type index\n(Aarne-Thompson-Uther type index). Our analyses suggested that first, the\npredator-prey relationship frequently appeared in a co-occurrent animal pair\nwithin a folktale (e.g., cat and mouse or wolf and pig), and second, the motif\nof 'deception', describing the antagonistic behaviour among animals, appeared\nrelatively higher in 'wild and domestic animals' and 'wild animals' than other\ntypes. Furthermore, the motif of 'deception' appeared more frequently in pairs,\ncorresponding to the predator-prey relationship. These results corresponded\nwith the hypothesis that the combination of animal characters and what happens\nin stories represented relationships in the real world. The present study\ndemonstrated that the combination of quantitative methods and qualitative data\nbroaden our understanding of the evolutionary aspects of human cultures.", "published": "2019-07-09 03:40:13", "link": "http://arxiv.org/abs/1907.03969v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "On the Semantic Interpretability of Artificial Intelligence Models", "abstract": "Artificial Intelligence models are becoming increasingly more powerful and\naccurate, supporting or even replacing humans' decision making. But with\nincreased power and accuracy also comes higher complexity, making it hard for\nusers to understand how the model works and what the reasons behind its\npredictions are. Humans must explain and justify their decisions, and so do the\nAI models supporting them in this process, making semantic interpretability an\nemerging field of study. In this work, we look at interpretability from a\nbroader point of view, going beyond the machine learning scope and covering\ndifferent AI fields such as distributional semantics and fuzzy logic, among\nothers. We examine and classify the models according to their nature and also\nbased on how they introduce interpretability features, analyzing how each\napproach affects the final users and pointing to gaps that still need to be\naddressed to provide more human-centered interpretability solutions.", "published": "2019-07-09 12:01:35", "link": "http://arxiv.org/abs/1907.04105v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Translating neural signals to text using a Brain-Machine Interface", "abstract": "Brain-Computer Interfaces (BCI) help patients with faltering communication\nabilities due to neurodegenerative diseases produce text or speech output by\ndirect neural processing. However, practical implementation of such a system\nhas proven difficult due to limitations in speed, accuracy, and\ngeneralizability of the existing interfaces. To this end, we aim to create a\nBCI system that decodes text directly from neural signals. We implement a\nframework that initially isolates frequency bands in the input signal\nencapsulating differential information regarding production of various phonemic\nclasses. These bands then form a feature set that feeds into an LSTM which\ndiscerns at each time point probability distributions across all phonemes\nuttered by a subject. Finally, these probabilities are fed into a particle\nfiltering algorithm which incorporates prior knowledge of the English language\nto output text corresponding to the decoded word. Performance of this model on\ndata obtained from six patients shows encouragingly high levels of accuracy at\nspeeds and bit rates significantly higher than existing BCI communication\nsystems. Further, in producing an output, our network abstains from\nconstraining the reconstructed word to be from a given bag-of-words, unlike\nprevious studies. The success of our proposed approach, offers promise for the\nemployment of a BCI interface by patients in unfettered, naturalistic\nenvironments.", "published": "2019-07-09 16:07:38", "link": "http://arxiv.org/abs/1907.04265v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Sentiment Analysis Challenges in Persian Language", "abstract": "The rapid growth in data on the internet requires a data mining process to\nreach a decision to support insight. The Persian language has strong potential\nfor deep research in any aspect of natural language processing, especially\nsentimental analysis approach. Thousands of websites and blogs updates and\nmodifies by Persian users around the world that contains millions of Persian\ncontext. This range of application requires a comprehensive structured\nframework to extract beneficial information for helping enterprises to enhance\ntheir business and initiate a customer-centric management process by producing\neffective recommender systems. Sentimental analysis is an intelligent approach\nfor extracting useful information from huge amounts of data to help an\nenterprise for smart management process. In this road, machine learning and\ndeep learning techniques will become very helpful but there is the number of\nchallenges which are face to them. This paper tried to present and assert the\nmost important challenges of sentimental analysis in the Persian language. This\nlanguage is an Indo-European language which spoken by over 110 million people\naround the world and is an official language in Iran, Tajikistan, and\nAfghanistan. Its also widely used in Uzbekistan, Pakistan and Turkish by order.", "published": "2019-07-09 20:46:37", "link": "http://arxiv.org/abs/1907.04407v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "To Tune or Not To Tune? How About the Best of Both Worlds?", "abstract": "The introduction of pre-trained language models has revolutionized natural\nlanguage research communities. However, researchers still know relatively\nlittle regarding their theoretical and empirical properties. In this regard,\nPeters et al. perform several experiments which demonstrate that it is better\nto adapt BERT with a light-weight task-specific head, rather than building a\ncomplex one on top of the pre-trained language model, and freeze the parameters\nin the said language model. However, there is another option to adopt. In this\npaper, we propose a new adaptation method which we first train the task model\nwith the BERT parameters frozen and then fine-tune the entire model together.\nOur experimental results show that our model adaptation method can achieve 4.7%\naccuracy improvement in semantic similarity task, 0.99% accuracy improvement in\nsequence labeling task and 0.72% accuracy improvement in the text\nclassification task.", "published": "2019-07-09 04:46:31", "link": "http://arxiv.org/abs/1907.05338v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ReCoSa: Detecting the Relevant Contexts with Self-Attention for\n  Multi-turn Dialogue Generation", "abstract": "In multi-turn dialogue generation, response is usually related with only a\nfew contexts. Therefore, an ideal model should be able to detect these relevant\ncontexts and produce a suitable response accordingly. However, the widely used\nhierarchical recurrent encoderdecoder models just treat all the contexts\nindiscriminately, which may hurt the following response generation process.\nSome researchers try to use the cosine similarity or the traditional attention\nmechanism to find the relevant contexts, but they suffer from either\ninsufficient relevance assumption or position bias problem. In this paper, we\npropose a new model, named ReCoSa, to tackle this problem. Firstly, a word\nlevel LSTM encoder is conducted to obtain the initial representation of each\ncontext. Then, the self-attention mechanism is utilized to update both the\ncontext and masked response representation. Finally, the attention weights\nbetween each context and response representations are computed and used in the\nfurther decoding process. Experimental results on both Chinese customer\nservices dataset and English Ubuntu dialogue dataset show that ReCoSa\nsignificantly outperforms baseline models, in terms of both metric-based and\nhuman evaluations. Further analysis on attention shows that the detected\nrelevant contexts by ReCoSa are highly coherent with human's understanding,\nvalidating the correctness and interpretability of ReCoSa.", "published": "2019-07-09 04:11:15", "link": "http://arxiv.org/abs/1907.05339v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning by Abstraction: The Neural State Machine", "abstract": "We introduce the Neural State Machine, seeking to bridge the gap between the\nneural and symbolic views of AI and integrate their complementary strengths for\nthe task of visual reasoning. Given an image, we first predict a probabilistic\ngraph that represents its underlying semantics and serves as a structured world\nmodel. Then, we perform sequential reasoning over the graph, iteratively\ntraversing its nodes to answer a given question or draw a new inference. In\ncontrast to most neural architectures that are designed to closely interact\nwith the raw sensory data, our model operates instead in an abstract latent\nspace, by transforming both the visual and linguistic modalities into semantic\nconcept-based representations, thereby achieving enhanced transparency and\nmodularity. We evaluate our model on VQA-CP and GQA, two recent VQA datasets\nthat involve compositionality, multi-step inference and diverse reasoning\nskills, achieving state-of-the-art results in both cases. We provide further\nexperiments that illustrate the model's strong generalization capacity across\nmultiple dimensions, including novel compositions of concepts, changes in the\nanswer distribution, and unseen linguistic structures, demonstrating the\nqualities and efficacy of our approach.", "published": "2019-07-09 03:08:41", "link": "http://arxiv.org/abs/1907.03950v4", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Multitask Learning for Blackmarket Tweet Detection", "abstract": "Online social media platforms have made the world more connected than ever\nbefore, thereby making it easier for everyone to spread their content across a\nwide variety of audiences. Twitter is one such popular platform where people\npublish tweets to spread their messages to everyone. Twitter allows users to\nRetweet other users' tweets in order to broadcast it to their network. The more\nretweets a particular tweet gets, the faster it spreads. This creates\nincentives for people to obtain artificial growth in the reach of their tweets\nby using certain blackmarket services to gain inorganic appraisals for their\ncontent.\n  In this paper, we attempt to detect such tweets that have been posted on\nthese blackmarket services in order to gain artificially boosted retweets. We\nuse a multitask learning framework to leverage soft parameter sharing between a\nclassification and a regression based task on separate inputs. This allows us\nto effectively detect tweets that have been posted to these blackmarket\nservices, achieving an F1-score of 0.89 when classifying tweets as blackmarket\nor genuine.", "published": "2019-07-09 10:42:33", "link": "http://arxiv.org/abs/1907.04072v1", "categories": ["cs.SI", "cs.CL", "cs.IR"], "primary_category": "cs.SI"}
{"title": "Sequence-to-Sequence Natural Language to Humanoid Robot Sign Language", "abstract": "This paper presents a study on natural language to sign language translation\nwith human-robot interaction application purposes. By means of the presented\nmethodology, the humanoid robot TEO is expected to represent Spanish sign\nlanguage automatically by converting text into movements, thanks to the\nperformance of neural networks. Natural language to sign language translation\npresents several challenges to developers, such as the discordance between the\nlength of input and output data and the use of non-manual markers. Therefore,\nneural networks and, consequently, sequence-to-sequence models, are selected as\na data-driven system to avoid traditional expert system approaches or temporal\ndependencies limitations that lead to limited or too complex translation\nsystems. To achieve these objectives, it is necessary to find a way to perform\nhuman skeleton acquisition in order to collect the signing input data. OpenPose\nand skeletonRetriever are proposed for this purpose and a 3D sensor\nspecification study is developed to select the best acquisition hardware.", "published": "2019-07-09 14:41:50", "link": "http://arxiv.org/abs/1907.04198v1", "categories": ["cs.RO", "cs.CL", "cs.HC", "cs.LG", "I.2.9; I.2.7; C.2.0"], "primary_category": "cs.RO"}
{"title": "Analyzing Phonetic and Graphemic Representations in End-to-End Automatic\n  Speech Recognition", "abstract": "End-to-end neural network systems for automatic speech recognition (ASR) are\ntrained from acoustic features to text transcriptions. In contrast to modular\nASR systems, which contain separately-trained components for acoustic modeling,\npronunciation lexicon, and language modeling, the end-to-end paradigm is both\nconceptually simpler and has the potential benefit of training the entire\nsystem on the end task. However, such neural network models are more opaque: it\nis not clear how to interpret the role of different parts of the network and\nwhat information it learns during training. In this paper, we analyze the\nlearned internal representations in an end-to-end ASR model. We evaluate the\nrepresentation quality in terms of several classification tasks, comparing\nphonemes and graphemes, as well as different articulatory features. We study\ntwo languages (English and Arabic) and three datasets, finding remarkable\nconsistency in how different properties are represented in different layers of\nthe deep neural network.", "published": "2019-07-09 14:59:16", "link": "http://arxiv.org/abs/1907.04224v2", "categories": ["cs.CL", "cs.SD", "eess.AS", "I.2.7"], "primary_category": "cs.CL"}
{"title": "UW-BHI at MEDIQA 2019: An Analysis of Representation Methods for Medical\n  Natural Language Inference", "abstract": "Recent advances in distributed language modeling have led to large\nperformance increases on a variety of natural language processing (NLP) tasks.\nHowever, it is not well understood how these methods may be augmented by\nknowledge-based approaches. This paper compares the performance and internal\nrepresentation of an Enhanced Sequential Inference Model (ESIM) between three\nexperimental conditions based on the representation method: Bidirectional\nEncoder Representations from Transformers (BERT), Embeddings of Semantic\nPredications (ESP), or Cui2Vec. The methods were evaluated on the Medical\nNatural Language Inference (MedNLI) subtask of the MEDIQA 2019 shared task.\nThis task relied heavily on semantic understanding and thus served as a\nsuitable evaluation set for the comparison of these representation methods.", "published": "2019-07-09 16:47:50", "link": "http://arxiv.org/abs/1907.04286v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Transfer Learning from Audio-Visual Grounding to Speech Recognition", "abstract": "Transfer learning aims to reduce the amount of data required to excel at a\nnew task by re-using the knowledge acquired from learning other related tasks.\nThis paper proposes a novel transfer learning scenario, which distills robust\nphonetic features from grounding models that are trained to tell whether a pair\nof image and speech are semantically correlated, without using any textual\ntranscripts. As semantics of speech are largely determined by its lexical\ncontent, grounding models learn to preserve phonetic information while\ndisregarding uncorrelated factors, such as speaker and channel. To study the\nproperties of features distilled from different layers, we use them as input\nseparately to train multiple speech recognition models. Empirical results\ndemonstrate that layers closer to input retain more phonetic information, while\nfollowing layers exhibit greater invariance to domain shift. Moreover, while\nmost previous studies include training data for speech recognition for feature\nextractor training, our grounding models are not trained on any of those data,\nindicating more universal applicability to new domains.", "published": "2019-07-09 18:23:32", "link": "http://arxiv.org/abs/1907.04355v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "GluonCV and GluonNLP: Deep Learning in Computer Vision and Natural\n  Language Processing", "abstract": "We present GluonCV and GluonNLP, the deep learning toolkits for computer\nvision and natural language processing based on Apache MXNet (incubating).\nThese toolkits provide state-of-the-art pre-trained models, training scripts,\nand training logs, to facilitate rapid prototyping and promote reproducible\nresearch. We also provide modular APIs with flexible building blocks to enable\nefficient customization. Leveraging the MXNet ecosystem, the deep learning\nmodels in GluonCV and GluonNLP can be deployed onto a variety of platforms with\ndifferent programming languages. The Apache 2.0 license has been adopted by\nGluonCV and GluonNLP to allow for software distribution, modification, and\nusage.", "published": "2019-07-09 21:59:44", "link": "http://arxiv.org/abs/1907.04433v2", "categories": ["cs.LG", "cs.CL", "cs.CV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Learning to Speak Fluently in a Foreign Language: Multilingual Speech\n  Synthesis and Cross-Language Voice Cloning", "abstract": "We present a multispeaker, multilingual text-to-speech (TTS) synthesis model\nbased on Tacotron that is able to produce high quality speech in multiple\nlanguages. Moreover, the model is able to transfer voices across languages,\ne.g. synthesize fluent Spanish speech using an English speaker's voice, without\ntraining on any bilingual or parallel examples. Such transfer works across\ndistantly related languages, e.g. English and Mandarin.\n  Critical to achieving this result are: 1. using a phonemic input\nrepresentation to encourage sharing of model capacity across languages, and 2.\nincorporating an adversarial loss term to encourage the model to disentangle\nits representation of speaker identity (which is perfectly correlated with\nlanguage in the training data) from the speech content. Further scaling up the\nmodel by training on multiple speakers of each language, and incorporating an\nautoencoding input to help stabilize attention during training, results in a\nmodel which can be used to consistently synthesize intelligible speech for\ntraining speakers in all languages seen during training, and in native or\nforeign accents.", "published": "2019-07-09 22:43:55", "link": "http://arxiv.org/abs/1907.04448v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Multi-Speaker End-to-End Speech Synthesis", "abstract": "In this work, we extend ClariNet (Ping et al., 2019), a fully end-to-end\nspeech synthesis model (i.e., text-to-wave), to generate high-fidelity speech\nfrom multiple speakers. To model the unique characteristic of different voices,\nlow dimensional trainable speaker embeddings are shared across each component\nof ClariNet and trained together with the rest of the model. We demonstrate\nthat the multi-speaker ClariNet outperforms state-of-the-art systems in terms\nof naturalness, because the whole model is jointly optimized in an end-to-end\nmanner.", "published": "2019-07-09 23:53:39", "link": "http://arxiv.org/abs/1907.04462v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Improving the Performance of the LSTM and HMM Model via Hybridization", "abstract": "Language models based on deep neural networks and traditional stochastic\nmodelling have become both highly functional and effective in recent times. In\nthis work, a general survey into the two types of language modelling is\nconducted. We investigate the effectiveness of the Hidden Markov Model (HMM),\nand the Long Short-Term Memory Model (LSTM). We analyze the hidden state\nstructures common to both models, and present an analysis on structural\nsimilarity of the hidden states, common to both HMM's and LSTM's. We compare\nthe LSTM's predictive accuracy and hidden state output with respect to the HMM\nfor a varying number of hidden states. In this work, we justify that the less\ncomplex HMM can serve as an appropriate approximation of the LSTM model.", "published": "2019-07-09 15:12:51", "link": "http://arxiv.org/abs/1907.04670v4", "categories": ["cs.LG", "cs.CL", "stat.CO", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Adaptive Margin Ranking Loss for Knowledge Graph Embeddings via a\n  Correntropy Objective Function", "abstract": "Translation-based embedding models have gained significant attention in link\nprediction tasks for knowledge graphs. TransE is the primary model among\ntranslation-based embeddings and is well-known for its low complexity and high\nefficiency. Therefore, most of the earlier works have modified the score\nfunction of the TransE approach in order to improve the performance of link\nprediction tasks. Nevertheless, proven theoretically and experimentally, the\nperformance of TransE strongly depends on the loss function. Margin Ranking\nLoss (MRL) has been one of the earlier loss functions which is widely used for\ntraining TransE. However, the scores of positive triples are not necessarily\nenforced to be sufficiently small to fulfill the translation from head to tail\nby using relation vector (original assumption of TransE). To tackle this\nproblem, several loss functions have been proposed recently by adding upper\nbounds and lower bounds to the scores of positive and negative samples.\nAlthough highly effective, previously developed models suffer from an expansion\nin search space for a selection of the hyperparameters (in particular the upper\nand lower bounds of scores) on which the performance of the translation-based\nmodels is highly dependent. In this paper, we propose a new loss function\ndubbed Adaptive Margin Loss (AML) for training translation-based embedding\nmodels. The formulation of the proposed loss function enables an adaptive and\nautomated adjustment of the margin during the learning process. Therefore,\ninstead of obtaining two values (upper bound and lower bound), only the center\nof a margin needs to be determined. During learning, the margin is expanded\nautomatically until it converges. In our experiments on a set of standard\nbenchmark datasets including Freebase and WordNet, the effectiveness of AML is\nconfirmed for training TransE on link prediction tasks.", "published": "2019-07-09 12:32:40", "link": "http://arxiv.org/abs/1907.05336v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Joint Speech Recognition and Speaker Diarization via Sequence\n  Transduction", "abstract": "Speech applications dealing with conversations require not only recognizing\nthe spoken words, but also determining who spoke when. The task of assigning\nwords to speakers is typically addressed by merging the outputs of two separate\nsystems, namely, an automatic speech recognition (ASR) system and a speaker\ndiarization (SD) system. The two systems are trained independently with\ndifferent objective functions. Often the SD systems operate directly on the\nacoustics and are not constrained to respect word boundaries and this\ndeficiency is overcome in an ad hoc manner. Motivated by recent advances in\nsequence to sequence learning, we propose a novel approach to tackle the two\ntasks by a joint ASR and SD system using a recurrent neural network transducer.\nOur approach utilizes both linguistic and acoustic cues to infer speaker roles,\nas opposed to typical SD systems, which only use acoustic cues. We evaluated\nthe performance of our approach on a large corpus of medical conversations\nbetween physicians and patients. Compared to a competitive conventional\nbaseline, our approach improves word-level diarization error rate from 15.8% to\n2.2%.", "published": "2019-07-09 00:23:22", "link": "http://arxiv.org/abs/1907.05337v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Teach an all-rounder with experts in different domains", "abstract": "In many automatic speech recognition (ASR) tasks, an ideal model has to be\napplicable over multiple domains. In this paper, we propose to teach an\nall-rounder with experts in different domains. Concretely, we build a\nmulti-domain acoustic model by applying the teacher-student training framework.\nFirst, for each domain, a teacher model (domain-dependent model) is trained by\nfine-tuning a multi-condition model with domain-specific subset. Then all these\nteacher models are used to teach one single student model simultaneously. We\nperform experiments on two predefined domain setups. One is domains with\ndifferent speaking styles, the other is nearfield, far-field and far-field with\nnoise. Moreover, two types of models are examined: deep feedforward sequential\nmemory network (DFSMN) and long short term memory (LSTM). Experimental results\nshow that the model trained with this framework outperforms not only\nmulti-condition model but also domain-dependent model. Specially, our training\nmethod provides up to 10.4% relative character error rate improvement over\nbaseline model (multi-condition model).", "published": "2019-07-09 08:26:25", "link": "http://arxiv.org/abs/1907.05698v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "M3D-GAN: Multi-Modal Multi-Domain Translation with Universal Attention", "abstract": "Generative adversarial networks have led to significant advances in\ncross-modal/domain translation. However, typically these networks are designed\nfor a specific task (e.g., dialogue generation or image synthesis, but not\nboth). We present a unified model, M3D-GAN, that can translate across a wide\nrange of modalities (e.g., text, image, and speech) and domains (e.g.,\nattributes in images or emotions in speech). Our model consists of modality\nsubnets that convert data from different modalities into unified\nrepresentations, and a unified computing body where data from different\nmodalities share the same network architecture. We introduce a universal\nattention module that is jointly trained with the whole network and learns to\nencode a large range of domain information into a highly structured latent\nspace. We use this to control synthesis in novel ways, such as producing\ndiverse realistic pictures from a sketch or varying the emotion of synthesized\nspeech. We evaluate our approach on extensive benchmark tasks, including\nimage-to-image, text-to-image, image captioning, text-to-speech, speech\nrecognition, and machine translation. Our results show state-of-the-art\nperformance on some of the tasks.", "published": "2019-07-09 19:33:01", "link": "http://arxiv.org/abs/1907.04378v1", "categories": ["cs.CV", "cs.CL", "cs.LG", "eess.AS", "eess.IV"], "primary_category": "cs.CV"}
{"title": "Evolution of the Informational Complexity of Contemporary Western Music", "abstract": "We measure the complexity of songs in the Million Song Dataset (MSD) in terms\nof pitch, timbre, loudness, and rhythm to investigate their evolution from 1960\nto 2010. By comparing the Billboard Hot 100 with random samples, we find that\nthe complexity of popular songs tends to be more narrowly distributed around\nthe mean, supporting the idea of an inverted U-shaped relationship between\ncomplexity and hedonistic value. We then examine the temporal evolution of\ncomplexity, reporting consistent changes across decades, such as a decrease in\naverage loudness complexity since the 1960s, and an increase in timbre\ncomplexity overall but not for popular songs. We also show, in contrast to\nclaims that popular songs sound more alike over time, that they are not more\nsimilar than they were 50 years ago in terms of pitch or rhythm, although\nsimilarity in timbre shows distinctive patterns across eras and similarity in\nloudness has been increasing. Finally, we show that musical genres can be\ndifferentiated by their distinctive complexity profiles.", "published": "2019-07-09 17:01:50", "link": "http://arxiv.org/abs/1907.04292v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improving Reverberant Speech Training Using Diffuse Acoustic Simulation", "abstract": "We present an efficient and realistic geometric acoustic simulation approach\nfor generating and augmenting training data in speech-related machine learning\ntasks. Our physically-based acoustic simulation method is capable of modeling\nocclusion, specular and diffuse reflections of sound in complicated acoustic\nenvironments, whereas the classical image method can only model specular\nreflections in simple room settings. We show that by using our synthetic\ntraining data, the same neural networks gain significant performance\nimprovement on real test sets in far-field speech recognition by 1.58% and\nkeyword spotting by 21%, without fine-tuning using real impulse responses.", "published": "2019-07-09 05:26:52", "link": "http://arxiv.org/abs/1907.03988v5", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "An Attention Mechanism for Musical Instrument Recognition", "abstract": "While the automatic recognition of musical instruments has seen significant\nprogress, the task is still considered hard for music featuring multiple\ninstruments as opposed to single instrument recordings. Datasets for polyphonic\ninstrument recognition can be categorized into roughly two categories. Some,\nsuch as MedleyDB, have strong per-frame instrument activity annotations but are\nusually small in size. Other, larger datasets such as OpenMIC only have weak\nlabels, i.e., instrument presence or absence is annotated only for long\nsnippets of a song. We explore an attention mechanism for handling weakly\nlabeled data for multi-label instrument recognition. Attention has been found\nto perform well for other tasks with weakly labeled data. We compare the\nproposed attention model to multiple models which include a baseline binary\nrelevance random forest, recurrent neural network, and fully connected neural\nnetworks. Our results show that incorporating attention leads to an overall\nimprovement in classification accuracy metrics across all 20 instruments in the\nOpenMIC dataset. We find that attention enables models to focus on (or `attend\nto') specific time segments in the audio relevant to each instrument label\nleading to interpretable results.", "published": "2019-07-09 17:20:36", "link": "http://arxiv.org/abs/1907.04294v1", "categories": ["cs.IR", "cs.SD", "eess.AS"], "primary_category": "cs.IR"}
{"title": "Exploring Conditioning for Generative Music Systems with\n  Human-Interpretable Controls", "abstract": "Performance RNN is a machine-learning system designed primarily for the\ngeneration of solo piano performances using an event-based (rather than audio)\nrepresentation. More specifically, Performance RNN is a long short-term memory\n(LSTM) based recurrent neural network that models polyphonic music with\nexpressive timing and dynamics (Oore et al., 2018). The neural network uses a\nsimple language model based on the Musical Instrument Digital Interface (MIDI)\nfile format. Performance RNN is trained on the e-Piano Junior Competition\nDataset (International Piano e-Competition, 2018), a collection of solo piano\nperformances by expert pianists. As an artistic tool, one of the limitations of\nthe original model has been the lack of useable controls. The standard form of\nPerformance RNN can generate interesting pieces, but little control is provided\nover what specifically is generated. This paper explores a set of\nconditioning-based controls used to influence the generation process.", "published": "2019-07-09 18:20:05", "link": "http://arxiv.org/abs/1907.04352v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
