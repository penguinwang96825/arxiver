{"title": "Colloquial Persian POS (CPPOS) Corpus: A Novel Corpus for Colloquial\n  Persian Part of Speech Tagging", "abstract": "Introduction: Part-of-Speech (POS) Tagging, the process of classifying words\ninto their respective parts of speech (e.g., verb or noun), is essential in\nvarious natural language processing applications. POS tagging is a crucial\npreprocessing task for applications like machine translation, question\nanswering, sentiment analysis, etc. However, existing corpora for POS tagging\nin Persian mainly consist of formal texts, such as daily news and newspapers.\nAs a result, smart POS tools, machine learning models, and deep learning models\ntrained on these corpora may not perform optimally for processing colloquial\ntext in social network analysis. Method: This paper introduces a novel corpus,\n\"Colloquial Persian POS\" (CPPOS), specifically designed to support colloquial\nPersian text. The corpus includes formal and informal text collected from\nvarious domains such as political, social, and commercial on Telegram, Twitter,\nand Instagram more than 520K labeled tokens. After collecting posts from these\nsocial platforms for one year, special preprocessing steps were conducted,\nincluding normalization, sentence tokenizing, and word tokenizing for social\ntext. The tokens and sentences were then manually annotated and verified by a\nteam of linguistic experts. This study also defines a POS tagging guideline for\nannotating the data and conducting the annotation process. Results: To evaluate\nthe quality of CPPOS, various deep learning models, such as the RNN family,\nwere trained using the constructed corpus. A comparison with another well-known\nPersian POS corpus named \"Bijankhan\" and the Persian Hazm POS tool trained on\nBijankhan revealed that our model trained on CPPOS outperforms them. With the\nnew corpus and the BiLSTM deep neural model, we achieved a 14% improvement over\nthe previous dataset.", "published": "2023-10-01 05:06:33", "link": "http://arxiv.org/abs/2310.00572v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Nine-year-old children outperformed ChatGPT in emotion: Evidence from\n  Chinese writing", "abstract": "ChatGPT has been demonstrated to possess significant capabilities in\ngenerating intricate, human-like text, and recent studies have established that\nits performance in theory of mind tasks is comparable to that of a\nnine-year-old child. However, it remains uncertain whether ChatGPT surpasses\nnine-year-old children in Chinese writing proficiency. To explore this, our\nstudy juxtaposed the Chinese writing performance of ChatGPT and nine-year-old\nchildren on both narrative and scientific topics, aiming to uncover the\nrelative strengths and weaknesses of ChatGPT in writing.\n  The collected data were analyzed across five linguistic dimensions: fluency,\naccuracy, complexity, cohesion, and emotion. Each dimension underwent\nassessment through precise indices. The findings revealed that nine-year-old\nchildren excelled beyond ChatGPT in terms of fluency and cohesion within their\nwriting. In contrast, ChatGPT manifested a superior performance in accuracy\ncompared to the children. Concerning complexity, children exhibited superior\nskills in science-themed writing, while ChatGPT prevailed in nature-themed\nwriting. Significantly, this research is pioneering in revealing that\nnine-year-old children convey stronger emotions than ChatGPT in their Chinese\ncompositions.", "published": "2023-10-01 05:37:55", "link": "http://arxiv.org/abs/2310.00578v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Task-oriented Dialog Model with Task-progressive and Policy-aware\n  Pre-training", "abstract": "Pre-trained conversation models (PCMs) have achieved promising progress in\nrecent years. However, existing PCMs for Task-oriented dialog (TOD) are\ninsufficient for capturing the sequential nature of the TOD-related tasks, as\nwell as for learning dialog policy information. To alleviate these problems,\nthis paper proposes a task-progressive PCM with two policy-aware pre-training\ntasks. The model is pre-trained through three stages where TOD-related tasks\nare progressively employed according to the task logic of the TOD system. A\nglobal policy consistency task is designed to capture the multi-turn dialog\npolicy sequential relation, and an act-based contrastive learning task is\ndesigned to capture similarities among samples with the same dialog policy. Our\nmodel achieves better results on both MultiWOZ and In-Car end-to-end dialog\nmodeling benchmarks with only 18\\% parameters and 25\\% pre-training data\ncompared to the previous state-of-the-art PCM, GALAXY.", "published": "2023-10-01 07:06:02", "link": "http://arxiv.org/abs/2310.00597v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PETA: Parameter-Efficient Trojan Attacks", "abstract": "Parameter-efficient fine-tuning (PEFT) enables efficient adaptation of\npre-trained language models (PLMs) to specific tasks. By tuning only a minimal\nset of (extra) parameters, PEFT achieves performance that is comparable to\nstandard fine-tuning. However, despite its prevalent use, the security\nimplications of PEFT remain largely unexplored. In this paper, we take the\ninitial steps and present PETA, a novel trojan attack that compromises the\nweights of PLMs by accounting for downstream adaptation through bilevel\noptimization: the upper-level objective embeds the backdoor into a model while\nthe lower-level objective simulates PEFT to both retain the PLM's task-specific\nperformance and ensure that the backdoor persists after fine-tuning. With\nextensive evaluation across a variety of downstream tasks and trigger designs,\nwe demonstrate PETA's effectiveness in terms of both attack success rate and\nclean accuracy, even when the attacker does not have full knowledge of the\nvictim user's training process.", "published": "2023-10-01 12:07:44", "link": "http://arxiv.org/abs/2310.00648v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CebuaNER: A New Baseline Cebuano Named Entity Recognition Model", "abstract": "Despite being one of the most linguistically diverse groups of countries,\ncomputational linguistics and language processing research in Southeast Asia\nhas struggled to match the level of countries from the Global North. Thus,\ninitiatives such as open-sourcing corpora and the development of baseline\nmodels for basic language processing tasks are important stepping stones to\nencourage the growth of research efforts in the field. To answer this call, we\nintroduce CebuaNER, a new baseline model for named entity recognition (NER) in\nthe Cebuano language. Cebuano is the second most-used native language in the\nPhilippines, with over 20 million speakers. To build the model, we collected\nand annotated over 4,000 news articles, the largest of any work in the\nlanguage, retrieved from online local Cebuano platforms to train algorithms\nsuch as Conditional Random Field and Bidirectional LSTM. Our findings show\npromising results as a new baseline model, achieving over 70% performance on\nprecision, recall, and F1 across all entity tags, as well as potential efficacy\nin a crosslingual setup with Tagalog.", "published": "2023-10-01 14:09:42", "link": "http://arxiv.org/abs/2310.00679v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do the Benefits of Joint Models for Relation Extraction Extend to\n  Document-level Tasks?", "abstract": "Two distinct approaches have been proposed for relational triple extraction -\npipeline and joint. Joint models, which capture interactions across triples,\nare the more recent development, and have been shown to outperform pipeline\nmodels for sentence-level extraction tasks. Document-level extraction is a more\nchallenging setting where interactions across triples can be long-range, and\nindividual triples can also span across sentences. Joint models have not been\napplied for document-level tasks so far. In this paper, we benchmark\nstate-of-the-art pipeline and joint extraction models on sentence-level as well\nas document-level datasets. Our experiments show that while joint models\noutperform pipeline models significantly for sentence-level extraction, their\nperformance drops sharply below that of pipeline models for the document-level\ndataset.", "published": "2023-10-01 15:09:36", "link": "http://arxiv.org/abs/2310.00696v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FELM: Benchmarking Factuality Evaluation of Large Language Models", "abstract": "Assessing factuality of text generated by large language models (LLMs) is an\nemerging yet crucial research area, aimed at alerting users to potential errors\nand guiding the development of more reliable LLMs. Nonetheless, the evaluators\nassessing factuality necessitate suitable evaluation themselves to gauge\nprogress and foster advancements. This direction remains under-explored,\nresulting in substantial impediments to the progress of factuality evaluators.\nTo mitigate this issue, we introduce a benchmark for Factuality Evaluation of\nlarge Language Models, referred to as felm. In this benchmark, we collect\nresponses generated from LLMs and annotate factuality labels in a fine-grained\nmanner. Contrary to previous studies that primarily concentrate on the\nfactuality of world knowledge (e.g.~information from Wikipedia), felm focuses\non factuality across diverse domains, spanning from world knowledge to math and\nreasoning. Our annotation is based on text segments, which can help pinpoint\nspecific factual errors. The factuality annotations are further supplemented by\npredefined error types and reference links that either support or contradict\nthe statement. In our experiments, we investigate the performance of several\nLLM-based factuality evaluators on felm, including both vanilla LLMs and those\naugmented with retrieval mechanisms and chain-of-thought processes. Our\nfindings reveal that while retrieval aids factuality evaluation, current LLMs\nare far from satisfactory to faithfully detect factual errors.", "published": "2023-10-01 17:37:31", "link": "http://arxiv.org/abs/2310.00741v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SIP: Injecting a Structural Inductive Bias into a Seq2Seq Model by\n  Simulation", "abstract": "Strong inductive biases enable learning from little data and help\ngeneralization outside of the training distribution. Popular neural\narchitectures such as Transformers lack strong structural inductive biases for\nseq2seq NLP tasks on their own. Consequently, they struggle with systematic\ngeneralization beyond the training distribution, e.g. with extrapolating to\nlonger inputs, even when pre-trained on large amounts of text. We show how a\nstructural inductive bias can be efficiently injected into a seq2seq model by\npre-training it to simulate structural transformations on synthetic data.\nSpecifically, we inject an inductive bias towards Finite State Transducers\n(FSTs) into a Transformer by pre-training it to simulate FSTs given their\ndescriptions. Our experiments show that our method imparts the desired\ninductive bias, resulting in improved systematic generalization and better\nfew-shot learning for FST-like tasks. Our analysis shows that fine-tuned models\naccurately capture the state dynamics of the unseen underlying FSTs, suggesting\nthat the simulation process is internalized by the fine-tuned model.", "published": "2023-10-01 21:19:12", "link": "http://arxiv.org/abs/2310.00796v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Parameter-Efficient Tuning Helps Language Model Alignment", "abstract": "Aligning large language models (LLMs) with human preferences is essential for\nsafe and useful LLMs. Previous works mainly adopt reinforcement learning (RLHF)\nand direct preference optimization (DPO) with human feedback for alignment.\nNevertheless, they have certain drawbacks. One such limitation is that they can\nonly align models with one preference at the training time (e.g., they cannot\nlearn to generate concise responses when the preference data prefers detailed\nresponses), or have certain constraints for the data format (e.g., DPO only\nsupports pairwise preference data). To this end, prior works incorporate\ncontrollable generations for alignment to make language models learn multiple\npreferences and provide outputs with different preferences during inference if\nasked. Controllable generation also offers more flexibility with regard to data\nformat (e.g., it supports pointwise preference data). Specifically, it uses\ndifferent control tokens for different preferences during training and\ninference, making LLMs behave differently when required. Current controllable\ngeneration methods either use a special token or hand-crafted prompts as\ncontrol tokens, and optimize them together with LLMs. As control tokens are\ntypically much lighter than LLMs, this optimization strategy may not\neffectively optimize control tokens. To this end, we first use\nparameter-efficient tuning (e.g., prompting tuning and low-rank adaptation) to\noptimize control tokens and then fine-tune models for controllable generations,\nsimilar to prior works. Our approach, alignMEnt with parameter-Efficient Tuning\n(MEET), improves the quality of control tokens, thus improving controllable\ngeneration quality consistently by an apparent margin on two well-recognized\ndatasets compared with prior works.", "published": "2023-10-01 23:27:14", "link": "http://arxiv.org/abs/2310.00819v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Siamese Representation Learning for Unsupervised Relation Extraction", "abstract": "Unsupervised relation extraction (URE) aims at discovering underlying\nrelations between named entity pairs from open-domain plain text without prior\ninformation on relational distribution. Existing URE models utilizing\ncontrastive learning, which attract positive samples and repulse negative\nsamples to promote better separation, have got decent effect. However,\nfine-grained relational semantic in relationship makes spurious negative\nsamples, damaging the inherent hierarchical structure and hindering\nperformances. To tackle this problem, we propose Siamese Representation\nLearning for Unsupervised Relation Extraction -- a novel framework to simply\nleverage positive pairs to representation learning, possessing the capability\nto effectively optimize relation representation of instances and retain\nhierarchical information in relational feature space. Experimental results show\nthat our model significantly advances the state-of-the-art results on two\nbenchmark datasets and detailed analyses demonstrate the effectiveness and\nrobustness of our proposed model on unsupervised relation extraction.", "published": "2023-10-01 02:57:43", "link": "http://arxiv.org/abs/2310.00552v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GrowLength: Accelerating LLMs Pretraining by Progressively Growing\n  Training Length", "abstract": "The evolving sophistication and intricacies of Large Language Models (LLMs)\nyield unprecedented advancements, yet they simultaneously demand considerable\ncomputational resources and incur significant costs. To alleviate these\nchallenges, this paper introduces a novel, simple, and effective method named\n``\\growlength'' to accelerate the pretraining process of LLMs. Our method\nprogressively increases the training length throughout the pretraining phase,\nthereby mitigating computational costs and enhancing efficiency. For instance,\nit begins with a sequence length of 128 and progressively extends to 4096. This\napproach enables models to process a larger number of tokens within limited\ntime frames, potentially boosting their performance. In other words, the\nefficiency gain is derived from training with shorter sequences optimizing the\nutilization of resources. Our extensive experiments with various\nstate-of-the-art LLMs have revealed that models trained using our method not\nonly converge more swiftly but also exhibit superior performance metrics\ncompared to those trained with existing methods. Furthermore, our method for\nLLMs pretraining acceleration does not require any additional engineering\nefforts, making it a practical solution in the realm of LLMs.", "published": "2023-10-01 05:25:24", "link": "http://arxiv.org/abs/2310.00576v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Novel Computational and Modeling Foundation for Automatic Coherence\n  Assessment", "abstract": "Coherence is an essential property of well-written texts, that refers to the\nway textual units relate to one another. In the era of generative AI, coherence\nassessment is essential for many NLP tasks; summarization, generation,\nlong-form question-answering, and more. However, in NLP {coherence} is an\nill-defined notion, not having a formal definition or evaluation metrics, that\nwould allow for large-scale automatic and systematic coherence assessment. To\nbridge this gap, in this work we employ the formal linguistic definition of\n\\citet{Reinhart:1980} of what makes a discourse coherent, consisting of three\nconditions -- {\\em cohesion, consistency} and {\\em relevance} -- and formalize\nthese conditions as respective computational tasks. We hypothesize that (i) a\nmodel trained on all of these tasks will learn the features required for\ncoherence detection, and that (ii) a joint model for all tasks will exceed the\nperformance of models trained on each task individually. On two benchmarks for\ncoherence scoring rated by humans, one containing 500 automatically-generated\nshort stories and another containing 4k real-world texts, our experiments\nconfirm that jointly training on the proposed tasks leads to better performance\non each task compared with task-specific models, and to better performance on\nassessing coherence overall, compared with strong baselines. We conclude that\nthe formal and computational setup of coherence as proposed here provides a\nsolid foundation for advanced methods of large-scale automatic assessment of\ncoherence.", "published": "2023-10-01 07:06:17", "link": "http://arxiv.org/abs/2310.00598v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Wavelet Scattering Transform for Improving Generalization in\n  Low-Resourced Spoken Language Identification", "abstract": "Commonly used features in spoken language identification (LID), such as\nmel-spectrogram or MFCC, lose high-frequency information due to windowing. The\nloss further increases for longer temporal contexts. To improve generalization\nof the low-resourced LID systems, we investigate an alternate feature\nrepresentation, wavelet scattering transform (WST), that compensates for the\nshortcomings. To our knowledge, WST is not explored earlier in LID tasks. We\nfirst optimize WST features for multiple South Asian LID corpora. We show that\nLID requires low octave resolution and frequency-scattering is not useful.\nFurther, cross-corpora evaluations show that the optimal WST hyper-parameters\ndepend on both train and test corpora. Hence, we develop fused ECAPA-TDNN based\nLID systems with different sets of WST hyper-parameters to improve\ngeneralization for unknown data. Compared to MFCC, EER is reduced upto 14.05%\nand 6.40% for same-corpora and blind VoxLingua107 evaluations, respectively.", "published": "2023-10-01 07:31:00", "link": "http://arxiv.org/abs/2310.00602v2", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Faithful Explanations of Black-box NLP Models Using LLM-generated\n  Counterfactuals", "abstract": "Causal explanations of the predictions of NLP systems are essential to ensure\nsafety and establish trust. Yet, existing methods often fall short of\nexplaining model predictions effectively or efficiently and are often\nmodel-specific. In this paper, we address model-agnostic explanations,\nproposing two approaches for counterfactual (CF) approximation. The first\napproach is CF generation, where a large language model (LLM) is prompted to\nchange a specific text concept while keeping confounding concepts unchanged.\nWhile this approach is demonstrated to be very effective, applying LLM at\ninference-time is costly. We hence present a second approach based on matching,\nand propose a method that is guided by an LLM at training-time and learns a\ndedicated embedding space. This space is faithful to a given causal graph and\neffectively serves to identify matches that approximate CFs. After showing\ntheoretically that approximating CFs is required in order to construct faithful\nexplanations, we benchmark our approaches and explain several models, including\nLLMs with billions of parameters. Our empirical results demonstrate the\nexcellent performance of CF generation models as model-agnostic explainers.\nMoreover, our matching approach, which requires far less test-time resources,\nalso provides effective explanations, surpassing many baselines. We also find\nthat Top-K techniques universally improve every tested method. Finally, we\nshowcase the potential of LLMs in constructing new benchmarks for model\nexplanation and subsequently validate our conclusions. Our work illuminates new\npathways for efficient and accurate approaches to interpreting NLP systems.", "published": "2023-10-01 07:31:04", "link": "http://arxiv.org/abs/2310.00603v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Knowledge Engineering using Large Language Models", "abstract": "Knowledge engineering is a discipline that focuses on the creation and\nmaintenance of processes that generate and apply knowledge. Traditionally,\nknowledge engineering approaches have focused on knowledge expressed in formal\nlanguages. The emergence of large language models and their capabilities to\neffectively work with natural language, in its broadest sense, raises questions\nabout the foundations and practice of knowledge engineering. Here, we outline\nthe potential role of LLMs in knowledge engineering, identifying two central\ndirections: 1) creating hybrid neuro-symbolic knowledge systems; and 2)\nenabling knowledge engineering in natural language. Additionally, we formulate\nkey open research questions to tackle these directions.", "published": "2023-10-01 10:26:25", "link": "http://arxiv.org/abs/2310.00637v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Robust Sentiment Analysis for Low Resource languages Using Data\n  Augmentation Approaches: A Case Study in Marathi", "abstract": "Sentiment analysis plays a crucial role in understanding the sentiment\nexpressed in text data. While sentiment analysis research has been extensively\nconducted in English and other Western languages, there exists a significant\ngap in research efforts for sentiment analysis in low-resource languages.\nLimited resources, including datasets and NLP research, hinder the progress in\nthis area. In this work, we present an exhaustive study of data augmentation\napproaches for the low-resource Indic language Marathi. Although\ndomain-specific datasets for sentiment analysis in Marathi exist, they often\nfall short when applied to generalized and variable-length inputs. To address\nthis challenge, this research paper proposes four data augmentation techniques\nfor sentiment analysis in Marathi. The paper focuses on augmenting existing\ndatasets to compensate for the lack of sufficient resources. The primary\nobjective is to enhance sentiment analysis model performance in both in-domain\nand cross-domain scenarios by leveraging data augmentation strategies. The data\naugmentation approaches proposed showed a significant performance improvement\nfor cross-domain accuracies. The augmentation methods include paraphrasing,\nback-translation; BERT-based random token replacement, named entity\nreplacement, and pseudo-label generation; GPT-based text and label generation.\nFurthermore, these techniques can be extended to other low-resource languages\nand for general text classification tasks.", "published": "2023-10-01 17:09:31", "link": "http://arxiv.org/abs/2310.00734v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "RoleLLM: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities\n  of Large Language Models", "abstract": "The advent of Large Language Models (LLMs) has paved the way for complex\ntasks such as role-playing, which enhances user interactions by enabling models\nto imitate various characters. However, the closed-source nature of\nstate-of-the-art LLMs and their general-purpose training limit role-playing\noptimization. In this paper, we introduce RoleLLM, a framework to benchmark,\nelicit, and enhance role-playing abilities in LLMs. RoleLLM comprises four\nstages: (1) Role Profile Construction for 100 roles; (2) Context-Based\nInstruction Generation (Context-Instruct) for role-specific knowledge\nextraction; (3) Role Prompting using GPT (RoleGPT) for speaking style\nimitation; and (4) Role-Conditioned Instruction Tuning (RoCIT) for fine-tuning\nopen-source models along with role customization. By Context-Instruct and\nRoleGPT, we create RoleBench, the first systematic and fine-grained\ncharacter-level benchmark dataset for role-playing with 168,093 samples.\nMoreover, RoCIT on RoleBench yields RoleLLaMA (English) and RoleGLM (Chinese),\nsignificantly enhancing role-playing abilities and even achieving comparable\nresults with RoleGPT (using GPT-4).", "published": "2023-10-01 17:52:59", "link": "http://arxiv.org/abs/2310.00746v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TIGERScore: Towards Building Explainable Metric for All Text Generation\n  Tasks", "abstract": "We present TIGERScore, a \\textbf{T}rained metric that follows\n\\textbf{I}nstruction \\textbf{G}uidance to perform \\textbf{E}xplainable, and\n\\textbf{R}eference-free evaluation over a wide spectrum of text generation\ntasks. Different from other automatic evaluation methods that only provide\narcane scores, TIGERScore is guided by natural language instruction to provide\nerror analysis to pinpoint the mistakes in the generated text. Our metric is\nbased on LLaMA-2, trained on our meticulously curated instruction-tuning\ndataset MetricInstruct which covers 6 text generation tasks and 23 text\ngeneration datasets. The dataset consists of 42K quadruple in the form of\n(instruction, input, system output $\\rightarrow$ error analysis). We collected\nthe `system outputs' through from a large variety of models to cover different\ntypes of errors. To quantitatively assess our metric, we evaluate its\ncorrelation with human ratings on 5 held-in datasets, 2 held-out datasets and\nshow that TIGERScore can achieve the open-source SoTA correlation with human\nratings across these datasets and almost approaches GPT-4 evaluator. As a\nreference-free metric, its correlation can even surpass the best existing\nreference-based metrics. To further qualitatively assess the rationale\ngenerated by our metric, we conduct human evaluation on the generated\nexplanations and found that the explanations are 70.8\\% accurate. Through these\nexperimental results, we believe TIGERScore demonstrates the possibility of\nbuilding universal explainable metrics to evaluate any text generation task.\nAll the resourced are released in our project website:\n\\url{https://tiger-ai-lab.github.io/TIGERScore/}.", "published": "2023-10-01 18:01:51", "link": "http://arxiv.org/abs/2310.00752v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Testing the Limits of Unified Sequence to Sequence LLM Pretraining on\n  Diverse Table Data Tasks", "abstract": "Tables stored in databases and tables which are present in web pages and\narticles account for a large part of semi-structured data that is available on\nthe internet. It then becomes pertinent to develop a modeling approach with\nlarge language models (LLMs) that can be used to solve diverse table tasks such\nas semantic parsing, question answering as well as classification problems.\nTraditionally, there existed separate models specialized for each task\nindividually. It raises the question of how far can we go to build a unified\nmodel that works well on some table tasks without significant degradation on\nothers. To that end, we attempt at creating a shared modeling approach in the\npretraining stage with encoder-decoder style LLMs that can cater to diverse\ntasks. We evaluate our approach that continually pretrains and finetunes\ndifferent model families of T5 with data from tables and surrounding context,\non these downstream tasks at different model scales. Through multiple ablation\nstudies, we observe that our pretraining with self-supervised objectives can\nsignificantly boost the performance of the models on these tasks. As an example\nof one improvement, we observe that the instruction finetuned public models\nwhich come specialized on text question answering (QA) and have been trained on\ntable data still have room for improvement when it comes to table specific QA.\nOur work is the first attempt at studying the advantages of a unified approach\nto table specific pretraining when scaled from 770M to 11B sequence to sequence\nmodels while also comparing the instruction finetuned variants of the models.", "published": "2023-10-01 21:06:15", "link": "http://arxiv.org/abs/2310.00789v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Adapting LLM Agents with Universal Feedback in Communication", "abstract": "Recent advances in large language models (LLMs) have demonstrated potential\nfor LLM agents. To facilitate the training for these agents with both\nlinguistic feedback and non-linguistic reward signals, we introduce Learning\nthrough Communication (LTC). We design a universal buffer to store all the\nfeedback, and an iterative pipeline to enable an LLM agent to explore and\nupdate its policy in an given environment. To optimize agent interactions for\ntask-specific learning with our universal buffer and pipeline, we introduce\ndiverse communication patterns tailored for both single-agent and multi-agent\nenvironments. We evaluate the efficacy of our LTC approach on four diverse\ndatasets: ALFWorld (single-agent), HotpotQA (multi-agent collaboration),\nChameleon (multi-agent competition), and GSM8k (multi-agent teacher-student).\nOn these data sets, LTC outperforms the supervised instruction fine-tuning\nbaselines by 3.6% to 12%. These results highlight the versatility and\nefficiency of LTC in facilitating online adaptation for LLM agents.", "published": "2023-10-01 07:50:30", "link": "http://arxiv.org/abs/2310.01444v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Adaptive-Solver Framework for Dynamic Strategy Selection in Large\n  Language Model Reasoning", "abstract": "Large Language Models (LLMs) demonstrate impressive ability in handling\nreasoning tasks. However, unlike humans who can instinctively adapt their\nproblem-solving strategies to the complexity of task, most LLM-based methods\nadopt a one-size-fits-all approach. These methods employ consistent models,\nsample sizes, prompting methods and levels of problem decomposition, regardless\nof the problem complexity. The inflexibility of these methods can bring\nunnecessary computational overhead or sub-optimal performance. To address this\nlimitation, we introduce an Adaptive-Solver (AS) framework tha dynamically\nadapts solving strategies to suit various problems, enabling the flexible\nallocation of test-time computational resources. The framework functions with\ntwo primary modules. The initial evaluation module assesses the reliability of\nthe current solution using answer consistency. If the solution is deemed\nunreliable, the subsequent adaptation module comes into play. Within this\nmodule, various types of adaptation strategies are employed collaboratively.\nThrough such dynamic and multi-faceted adaptations, our framework can help\nreduce computational consumption and improve performance. Experimental results\nfrom complex reasoning benchmarks reveal that our method can significantly\nreduce API costs (up to 85%) while maintaining original performance.\nAlternatively, it achieves up to 4.5% higher accuracy compared to the baselines\nat the same cost. The code and dataset are available at\nhttps://github.com/john1226966735/Adaptive-Solver.", "published": "2023-10-01 12:28:36", "link": "http://arxiv.org/abs/2310.01446v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Meta Semantic Template for Evaluation of Large Language Models", "abstract": "Do large language models (LLMs) genuinely understand the semantics of the\nlanguage, or just memorize the training data? The recent concern on potential\ndata contamination of LLMs has raised awareness of the community to conduct\nresearch on LLMs evaluation. In this paper, we propose MSTemp, an approach that\ncreates meta semantic templates to evaluate the semantic understanding ability\nof LLMs. The core of MSTemp is not to perform evaluation directly on existing\nbenchmark datasets, but to generate new out-of-distribution (OOD) evaluation\nsets using existing datasets as seeds. Specifically, for a given sentence,\nMSTemp leverages another language model to generate new samples while\npreserving its semantics. The new samples are called semantic templates to the\noriginal sentence. Then, MSTemp generates evaluation samples via sentence\nparsing and random word replacement on the semantic templates. MSTemp is highly\nflexible, dynamic, and cost-effective. Our initial experiments show that\nMSTemp-generated samples can significantly reduce the performance of LLMs using\nexisting datasets as seeds. We hope this initial work can shed light on future\nresearch of LLMs evaluation.", "published": "2023-10-01 15:06:51", "link": "http://arxiv.org/abs/2310.01448v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SELF: Self-Evolution with Language Feedback", "abstract": "Large Language Models (LLMs) have demonstrated remarkable versatility across\nvarious domains. To further advance LLMs, we propose 'SELF' (Self-Evolution\nwith Language Feedback), a novel approach that enables LLMs to self-improve\nthrough self-reflection, akin to human learning processes. SELF initiates with\na meta-skill learning process that equips the LLMs with capabilities for\nself-feedback and self-refinement. Subsequently, the model undergoes an\niterative process of self-evolution. In each iteration, it utilizes an\nunlabeled dataset of instructions to generate initial responses. These\nresponses are enhanced through self-feedback and self-refinement. The model is\nthen fine-tuned using this enhanced data. The model undergoes progressive\nimprovement through this iterative self-evolution process. Moreover, the SELF\nframework enables the model to apply self-refinement during inference, which\nfurther improves response quality. Our experiments in mathematics and general\ntasks demonstrate that SELF can enhance the capabilities of LLMs without human\nintervention. The SELF framework indicates a promising direction for the\nautonomous evolution of LLMs, transitioning them from passive information\nreceivers to active participants in their development.", "published": "2023-10-01 00:52:24", "link": "http://arxiv.org/abs/2310.00533v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "JoMA: Demystifying Multilayer Transformers via JOint Dynamics of MLP and\n  Attention", "abstract": "We propose Joint MLP/Attention (JoMA) dynamics, a novel mathematical\nframework to understand the training procedure of multilayer Transformer\narchitectures. This is achieved by integrating out the self-attention layer in\nTransformers, producing a modified dynamics of MLP layers only. JoMA removes\nunrealistic assumptions in previous analysis (e.g., lack of residual\nconnection) and predicts that the attention first becomes sparse (to learn\nsalient tokens), then dense (to learn less salient tokens) in the presence of\nnonlinear activations, while in the linear case, it is consistent with existing\nworks that show attention becomes sparse over time. We leverage JoMA to\nqualitatively explains how tokens are combined to form hierarchies in\nmultilayer Transformers, when the input tokens are generated by a latent\nhierarchical generative model. Experiments on models trained from real-world\ndataset (Wikitext2/Wikitext103) and various pre-trained models (OPT, Pythia)\nverify our theoretical findings. Code can be found in\nhttps://github.com/facebookresearch/luckmatters/tree/yuandong3.", "published": "2023-10-01 01:21:35", "link": "http://arxiv.org/abs/2310.00535v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Empowering Many, Biasing a Few: Generalist Credit Scoring through Large\n  Language Models", "abstract": "In the financial industry, credit scoring is a fundamental element, shaping\naccess to credit and determining the terms of loans for individuals and\nbusinesses alike. Traditional credit scoring methods, however, often grapple\nwith challenges such as narrow knowledge scope and isolated evaluation of\ncredit tasks. Our work posits that Large Language Models (LLMs) have great\npotential for credit scoring tasks, with strong generalization ability across\nmultiple tasks. To systematically explore LLMs for credit scoring, we propose\nthe first open-source comprehensive framework. We curate a novel benchmark\ncovering 9 datasets with 14K samples, tailored for credit assessment and a\ncritical examination of potential biases within LLMs, and the novel instruction\ntuning data with over 45k samples. We then propose the first Credit and Risk\nAssessment Large Language Model (CALM) by instruction tuning, tailored to the\nnuanced demands of various financial risk assessment tasks. We evaluate CALM,\nexisting state-of-art (SOTA) methods, open source and closed source LLMs on the\nbuild benchmark. Our empirical results illuminate the capability of LLMs to not\nonly match but surpass conventional models, pointing towards a future where\ncredit scoring can be more inclusive, comprehensive, and unbiased. We\ncontribute to the industry's transformation by sharing our pioneering\ninstruction-tuning datasets, credit and risk assessment LLM, and benchmarks\nwith the research community and the financial industry.", "published": "2023-10-01 03:50:34", "link": "http://arxiv.org/abs/2310.00566v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.LG"}
{"title": "GeRA: Label-Efficient Geometrically Regularized Alignment", "abstract": "Pretrained unimodal encoders incorporate rich semantic information into\nembedding space structures. To be similarly informative, multi-modal encoders\ntypically require massive amounts of paired data for alignment and training. We\nintroduce a semi-supervised Geometrically Regularized Alignment (GeRA) method\nto align the embedding spaces of pretrained unimodal encoders in a\nlabel-efficient way. Our method leverages the manifold geometry of unpaired\n(unlabeled) data to improve alignment performance. To prevent distortions to\nlocal geometry during the alignment process, potentially disrupting semantic\nneighborhood structures and causing misalignment of unobserved pairs, we\nintroduce a geometric loss term. This term is built upon a diffusion operator\nthat captures the local manifold geometry of the unimodal pretrained encoders.\nGeRA is modality-agnostic and thus can be used to align pretrained encoders\nfrom any data modalities. We provide empirical evidence to the effectiveness of\nour method in the domains of speech-text and image-text alignment. Our\nexperiments demonstrate significant improvement in alignment quality compared\nto a variaty of leading baselines, especially with a small amount of paired\ndata, using our proposed geometric regularization.", "published": "2023-10-01 13:48:36", "link": "http://arxiv.org/abs/2310.00672v2", "categories": ["cs.LG", "cs.CL", "cs.CV", "I.2; I.2.7"], "primary_category": "cs.LG"}
{"title": "Evaluating Speech Synthesis by Training Recognizers on Synthetic Speech", "abstract": "Modern speech synthesis systems have improved significantly, with synthetic\nspeech being indistinguishable from real speech. However, efficient and\nholistic evaluation of synthetic speech still remains a significant challenge.\nHuman evaluation using Mean Opinion Score (MOS) is ideal, but inefficient due\nto high costs. Therefore, researchers have developed auxiliary automatic\nmetrics like Word Error Rate (WER) to measure intelligibility. Prior works\nfocus on evaluating synthetic speech based on pre-trained speech recognition\nmodels, however, this can be limiting since this approach primarily measures\nspeech intelligibility. In this paper, we propose an evaluation technique\ninvolving the training of an ASR model on synthetic speech and assessing its\nperformance on real speech. Our main assumption is that by training the ASR\nmodel on the synthetic speech, the WER on real speech reflects the similarity\nbetween distributions, a broader assessment of synthetic speech quality beyond\nintelligibility. Our proposed metric demonstrates a strong correlation with\nboth MOS naturalness and MOS intelligibility when compared to SpeechLMScore and\nMOSNet on three recent Text-to-Speech (TTS) systems: MQTTS, StyleTTS, and\nYourTTS.", "published": "2023-10-01 15:52:48", "link": "http://arxiv.org/abs/2310.00706v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "GenAI Against Humanity: Nefarious Applications of Generative Artificial\n  Intelligence and Large Language Models", "abstract": "Generative Artificial Intelligence (GenAI) and Large Language Models (LLMs)\nare marvels of technology; celebrated for their prowess in natural language\nprocessing and multimodal content generation, they promise a transformative\nfuture. But as with all powerful tools, they come with their shadows. Picture\nliving in a world where deepfakes are indistinguishable from reality, where\nsynthetic identities orchestrate malicious campaigns, and where targeted\nmisinformation or scams are crafted with unparalleled precision. Welcome to the\ndarker side of GenAI applications. This article is not just a journey through\nthe meanders of potential misuse of GenAI and LLMs, but also a call to\nrecognize the urgency of the challenges ahead. As we navigate the seas of\nmisinformation campaigns, malicious content generation, and the eerie creation\nof sophisticated malware, we'll uncover the societal implications that ripple\nthrough the GenAI revolution we are witnessing. From AI-powered botnets on\nsocial media platforms to the unnerving potential of AI to generate fabricated\nidentities, or alibis made of synthetic realities, the stakes have never been\nhigher. The lines between the virtual and the real worlds are blurring, and the\nconsequences of potential GenAI's nefarious applications impact us all. This\narticle serves both as a synthesis of rigorous research presented on the risks\nof GenAI and misuse of LLMs and as a thought-provoking vision of the different\ntypes of harmful GenAI applications we might encounter in the near future, and\nsome ways we can prepare for them.", "published": "2023-10-01 17:25:56", "link": "http://arxiv.org/abs/2310.00737v3", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.CY"}
{"title": "Analyzing and Mitigating Object Hallucination in Large Vision-Language\n  Models", "abstract": "Large vision-language models (LVLMs) have shown remarkable abilities in\nunderstanding visual information with human languages. However, LVLMs still\nsuffer from object hallucination, which is the problem of generating\ndescriptions that include objects that do not actually exist in the images.\nThis can negatively impact many vision-language tasks, such as visual\nsummarization and reasoning. To address this issue, we propose a simple yet\npowerful algorithm, LVLM Hallucination Revisor (LURE), to post-hoc rectify\nobject hallucination in LVLMs by reconstructing less hallucinatory\ndescriptions. LURE is grounded in a rigorous statistical analysis of the key\nfactors underlying object hallucination, including co-occurrence (the frequent\nappearance of certain objects alongside others in images), uncertainty (objects\nwith higher uncertainty during LVLM decoding), and object position\n(hallucination often appears in the later part of the generated text). LURE can\nalso be seamlessly integrated with any LVLMs. We evaluate LURE on six\nopen-source LVLMs, achieving a 23% improvement in general object hallucination\nevaluation metrics over the previous best approach. In both GPT and human\nevaluations, LURE consistently ranks at the top. Our data and code are\navailable at https://github.com/YiyangZhou/LURE.", "published": "2023-10-01 18:10:53", "link": "http://arxiv.org/abs/2310.00754v2", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "BooookScore: A systematic exploration of book-length summarization in\n  the era of LLMs", "abstract": "Summarizing book-length documents (>100K tokens) that exceed the context\nwindow size of large language models (LLMs) requires first breaking the input\ndocument into smaller chunks and then prompting an LLM to merge, update, and\ncompress chunk-level summaries. Despite the complexity and importance of this\ntask, it has yet to be meaningfully studied due to the challenges of\nevaluation: existing book-length summarization datasets (e.g., BookSum) are in\nthe pretraining data of most public LLMs, and existing evaluation methods\nstruggle to capture errors made by modern LLM summarizers. In this paper, we\npresent the first study of the coherence of LLM-based book-length summarizers\nimplemented via two prompting workflows: (1) hierarchically merging chunk-level\nsummaries, and (2) incrementally updating a running summary. We obtain 1193\nfine-grained human annotations on GPT-4 generated summaries of 100\nrecently-published books and identify eight common types of coherence errors\nmade by LLMs. Because human evaluation is expensive and time-consuming, we\ndevelop an automatic metric, BooookScore, that measures the proportion of\nsentences in a summary that do not contain any of the identified error types.\nBooookScore has high agreement with human annotations and allows us to\nsystematically evaluate the impact of many other critical parameters (e.g.,\nchunk size, base LLM) while saving $15K USD and 500 hours in human evaluation\ncosts. We find that closed-source LLMs such as GPT-4 and Claude 2 produce\nsummaries with higher BooookScore than those generated by open-source models.\nWhile LLaMA 2 falls behind other models, Mixtral achieves performance on par\nwith GPT-3.5-Turbo. Incremental updating yields lower BooookScore but higher\nlevel of detail than hierarchical merging, a trade-off sometimes preferred by\nannotators.", "published": "2023-10-01 20:46:44", "link": "http://arxiv.org/abs/2310.00785v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sparse Backpropagation for MoE Training", "abstract": "One defining characteristic of Mixture-of-Expert (MoE) models is their\ncapacity for conducting sparse computation via expert routing, leading to\nremarkable scalability. However, backpropagation, the cornerstone of deep\nlearning, requires dense computation, thereby posting challenges in MoE\ngradient computations. Here, we introduce SparseMixer, a scalable gradient\nestimator that bridges the gap between backpropagation and sparse expert\nrouting. Unlike typical MoE training which strategically neglects certain\ngradient terms for the sake of sparse computation and scalability, SparseMixer\nprovides scalable gradient approximations for these terms, enabling reliable\ngradient estimation in MoE training. Grounded in a numerical ODE framework,\nSparseMixer harnesses the mid-point method, a second-order ODE solver, to\ndeliver precise gradient approximations with negligible computational overhead.\nApplying SparseMixer to Switch Transformer on both pre-training and machine\ntranslation tasks, SparseMixer showcases considerable performance gain,\naccelerating training convergence up to 2 times.", "published": "2023-10-01 22:43:57", "link": "http://arxiv.org/abs/2310.00811v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "A Framework for Inference Inspired by Human Memory Mechanisms", "abstract": "How humans and machines make sense of current inputs for relation reasoning\nand question-answering while putting the perceived information into context of\nour past memories, has been a challenging conundrum in cognitive science and\nartificial intelligence. Inspired by human brain's memory system and cognitive\narchitectures, we propose a PMI framework that consists of perception, memory\nand inference components. Notably, the memory module comprises working and\nlong-term memory, with the latter endowed with a higher-order structure to\nretain extensive and complex relational knowledge and experience. Through a\ndifferentiable competitive write access, current perceptions update working\nmemory, which is later merged with long-term memory via outer product\nassociations, reducing information conflicts and averting memory overflow. In\nthe inference module, relevant information is retrieved from two separate\nmemory origins and associatively integrated to attain a more comprehensive and\nprecise interpretation of current perceptions. We exploratively apply our PMI\nto improve prevailing Transformers and CNN models on question-answering tasks\nlike bAbI-20k and Sort-of-CLEVR datasets, as well as detecting equilateral\ntriangles, language modeling and image classification tasks, and in each case,\nour PMI enhancements consistently outshine their original counterparts\nsignificantly. Visualization analyses reveal that relational memory\nconsolidation, along with the interaction and integration of information from\ndiverse memory sources, substantially contributes to the model effectiveness on\ninference tasks.", "published": "2023-10-01 08:12:55", "link": "http://arxiv.org/abs/2310.09297v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Pianist Identification Using Convolutional Neural Networks", "abstract": "This paper presents a comprehensive study of automatic performer\nidentification in expressive piano performances using convolutional neural\nnetworks (CNNs) and expressive features. Our work addresses the challenging\nmulti-class classification task of identifying virtuoso pianists, which has\nsubstantial implications for building dynamic musical instruments with\nintelligence and smart musical systems. Incorporating recent advancements, we\nleveraged large-scale expressive piano performance datasets and deep learning\ntechniques. We refined the scores by expanding repetitions and ornaments for\nmore accurate feature extraction. We demonstrated the capability of\none-dimensional CNNs for identifying pianists based on expressive features and\nanalyzed the impact of the input sequence lengths and different features. The\nproposed model outperforms the baseline, achieving 85.3% accuracy in a 6-way\nidentification task. Our refined dataset proved more apt for training a robust\npianist identifier, making a substantial contribution to the field of automatic\nperformer identification. Our codes have been released at\nhttps://github.com/BetsyTang/PID-CNN.", "published": "2023-10-01 15:15:33", "link": "http://arxiv.org/abs/2310.00699v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "UniAudio: An Audio Foundation Model Toward Universal Audio Generation", "abstract": "Large Language models (LLM) have demonstrated the capability to handle a\nvariety of generative tasks. This paper presents the UniAudio system, which,\nunlike prior task-specific approaches, leverages LLM techniques to generate\nmultiple types of audio (including speech, sounds, music, and singing) with\ngiven input conditions. UniAudio 1) first tokenizes all types of target audio\nalong with other condition modalities, 2) concatenates source-target pair as a\nsingle sequence, and 3) performs next-token prediction using LLM. Also, a\nmulti-scale Transformer model is proposed to handle the overly long sequences\ncaused by the residual vector quantization based neural codec in tokenization.\nTraining of UniAudio is scaled up to 165K hours of audio and 1B parameters,\nbased on all generative tasks, aiming to obtain sufficient prior knowledge not\nonly in the intrinsic properties of audio but also the inter-relationship\nbetween audio and other modalities. Therefore, the trained UniAudio model has\nthe potential to become a foundation model for universal audio generation: it\nshows strong capability in all trained tasks and can seamlessly support new\naudio generation tasks after simple fine-tuning. Experiments demonstrate that\nUniAudio achieves state-of-the-art or at least competitive results on most of\nthe 11 tasks. Demo and code are released at\nhttps://github.com/yangdongchao/UniAudio", "published": "2023-10-01 15:49:46", "link": "http://arxiv.org/abs/2310.00704v6", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Mechatronic Generation of Datasets for Acoustics Research", "abstract": "We address the challenge of making spatial audio datasets by proposing a\nshared mechanized recording space that can run custom acoustic experiments: a\nMechatronic Acoustic Research System (MARS). To accommodate a wide variety of\nexperiments, we implement an extensible architecture for wireless multi-robot\ncoordination which enables synchronized robot motion for dynamic scenes with\nmoving speakers and microphones. Using a virtual control interface, we can\nremotely design automated experiments to collect large-scale audio data. This\ndata is shown to be similar across repeated runs, demonstrating the reliability\nof MARS. We discuss the potential for MARS to make audio data collection\naccessible for researchers without dedicated acoustic research spaces.", "published": "2023-10-01 06:16:18", "link": "http://arxiv.org/abs/2310.00587v1", "categories": ["eess.AS", "cs.SY", "eess.SY"], "primary_category": "eess.AS"}
