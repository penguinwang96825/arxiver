{"title": "Pipelined Decoder for Efficient Context-Aware Text Generation", "abstract": "As the basis of generative AI, an autoregressive model requires the\ngeneration of a new token depending on all the previously generated tokens,\nwhich brings high quality but also restricts the model to generate tokens one\nby one, forming a bottleneck limiting the generation speed. In this paper, we\npropose a new decoder architecture that efficiently generates text in parallel\nfor context-aware generation tasks. Our proposed pipelined decoder initiates\nthe generation of multiple subsequences simultaneously, and, at each time-step,\nit generates a new token for each subsequence to realize parallelism.\nExperiments on multiple text generation tasks, including question answering,\ntext summarization, and keyphrase generation, show that our pipelined decoder\nsignificantly improves the generation speed without a significant loss of\ngeneration quality or additional memory consumption.", "published": "2025-06-29 23:37:24", "link": "http://arxiv.org/abs/2506.23431v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TuCo: Measuring the Contribution of Fine-Tuning to Individual Responses of LLMs", "abstract": "Past work has studied the effects of fine-tuning on large language models'\n(LLMs) overall performance on certain tasks. However, a quantitative and\nsystematic method for analyzing its effect on individual outputs is still\nlacking. Here, we propose a new method for measuring the contribution that\nfine-tuning makes to individual LLM responses, assuming access to the original\npre-trained model. Our method tracks the model's intermediate hidden states,\nproviding a more fine-grained insight into the effects of fine-tuning than a\nsimple comparison of final outputs from pre-trained and fine-tuned models. We\nintroduce and theoretically analyze an exact decomposition of any fine-tuned\nLLM into a pre-training component and a fine-tuning component. Empirically, we\nfind that model behavior and performance can be steered by up- or down-scaling\nthe fine-tuning component during the forward pass. Motivated by this finding\nand our theoretical analysis, we define the Tuning Contribution (TuCo) as the\nratio of the magnitudes of the fine-tuning component to the pre-training\ncomponent. We observe that three prominent adversarial attacks on LLMs\ncircumvent safety measures in a way that reduces TuCo, and that TuCo is\nconsistently lower on prompts where these attacks succeed compared to those\nwhere they do not. This suggests that attenuating the effect of fine-tuning on\nmodel outputs plays a role in the success of such attacks. In summary, TuCo\nenables the quantitative study of how fine-tuning influences model behavior and\nsafety, and vice versa.", "published": "2025-06-29 23:08:36", "link": "http://arxiv.org/abs/2506.23423v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Datasets for Fairness in Language Models: An In-Depth Survey", "abstract": "Fairness benchmarks play a central role in shaping how we evaluate language\nmodels, yet surprisingly little attention has been given to examining the\ndatasets that these benchmarks rely on. This survey addresses that gap by\npresenting a broad and careful review of the most widely used fairness datasets\nin current language model research, characterizing them along several key\ndimensions including their origin, scope, content, and intended use to help\nresearchers better appreciate the assumptions and limitations embedded in these\nresources. To support more meaningful comparisons and analyses, we introduce a\nunified evaluation framework that reveals consistent patterns of demographic\ndisparities across datasets and scoring methods. Applying this framework to\ntwenty four common benchmarks, we highlight the often overlooked biases that\ncan influence conclusions about model fairness and offer practical guidance for\nselecting, combining, and interpreting these datasets. We also point to\nopportunities for creating new fairness benchmarks that reflect more diverse\nsocial contexts and encourage more thoughtful use of these tools going forward.\nAll code, data, and detailed results are publicly available at\nhttps://github.com/vanbanTruong/Fairness-in-Large-Language-Models/tree/main/datasets\nto promote transparency and reproducibility across the research community.", "published": "2025-06-29 22:11:58", "link": "http://arxiv.org/abs/2506.23411v1", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Teaching a Language Model to Speak the Language of Tools", "abstract": "External tool integration through function-calling is essential for practical\nlanguage model applications, yet most multilingual models lack reliable\ntool-use capabilities in non-English languages. Even state-of-the-art\nmultilingual models struggle with determining when to use tools and generating\nthe structured outputs required for function calls, often exhibiting language\nconfusion when prompted in lower-resource languages. This work presents a\nmethodology for adapting existing language models to enable robust tool use in\nany target language, using Bulgarian as a case study. The approach involves\ncontinued training of the BgGPT model series (2.6B, 9B, 27B parameters) on a\nnovel bilingual dataset of 10,035 function-calling examples designed to support\nstandardized protocols like MCP (Model Context Protocol). The research\nintroduces TUCAN (Tool-Using Capable Assistant Navigator), which achieves up to\n28.75% improvement in function-calling accuracy over base models while\npreserving core language understanding, as verified on established Bulgarian\nbenchmarks. Beyond accuracy gains, TUCAN models demonstrate production-ready\nresponse formatting with clean, parsable function calls, contrasting with the\nverbose and inconsistent outputs of base models. The models, evaluation\nframework, and dataset are released to enable replication for other languages.\nThis work demonstrates a practical approach for extending tool-augmented\ncapabilities beyond English-centric systems.", "published": "2025-06-29 20:47:27", "link": "http://arxiv.org/abs/2506.23394v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "I.2.7; I.2.1"], "primary_category": "cs.IR"}
{"title": "Hierarchical Memory Organization for Wikipedia Generation", "abstract": "Generating Wikipedia articles autonomously is a challenging task requiring\nthe integration of accurate, comprehensive, and well-structured information\nfrom diverse sources. This paper introduces the Memory Organization-based\nGeneration (MOG) framework, a novel approach to address these challenges by\nleveraging a hierarchical memory architecture. MOG extracts fine-grained memory\nunits from web documents, recursively organizes them into a Wikipedia-style\nhierarchical structure, and uses this structure to guide the generation\nprocess. This ensures alignment between memory and the article outline,\nimproving both informativeness and verifiability while minimizing\nhallucinations. Additionally, a citation module is implemented to enhance\ntraceability by linking every generated sentence to specific memory units.\nEvaluations on our newly created WikiStart dataset demonstrate that MOG\noutperforms baseline methods in producing informative and reliable articles,\nmaking it particularly robust in real-world scenarios.", "published": "2025-06-29 20:22:49", "link": "http://arxiv.org/abs/2506.23393v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Perspective Dial: Measuring Perspective of Text and Guiding LLM Outputs", "abstract": "Large language models (LLMs) are used in a variety of mission-critical roles.\nDue to the rapidly developing nature of LLMs, there is a lack of quantifiable\nunderstanding of the bias and perspective associated with LLM output. Inspired\nby this need, this paper considers the broader issue of perspective or\nviewpoint of general text and perspective control of large-language model (LLM)\noutput. Perspective-Dial consists of two main components: a (1) metric space,\ndubbed Perspective Space, that enables quantitative measurements of different\nperspectives regarding a topic, and the use of (2) Systematic Prompt\nEngineering that utilizes greedy-coordinate descent to control LLM output\nperspective based on measurement feedback from the Perspective Space. The\nempirical nature of the approach allows progress to side step a principled\nunderstanding of perspective or bias -- effectively quantifying and adjusting\noutputs for a variety of topics. Potential applications include detection,\ntracking and mitigation of LLM bias, narrative detection, sense making and\ntracking in public discourse, and debate bot advocating given perspective.", "published": "2025-06-29 19:26:37", "link": "http://arxiv.org/abs/2506.23377v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "You Sound a Little Tense: L2 Tailored Clear TTS Using Durational Vowel Properties", "abstract": "We present the first text-to-speech (TTS) system tailored to second language\n(L2) speakers. We use duration differences between American English tense\n(longer) and lax (shorter) vowels to create a \"clarity mode\" for Matcha-TTS.\nOur perception studies showed that French-L1, English-L2 listeners had fewer\n(at least 9.15%) transcription errors when using our clarity mode, and found it\nmore encouraging and respectful than overall slowed down speech. Remarkably,\nlisteners were not aware of these effects: despite the decreased word error\nrate in clarity mode, listeners still believed that slowing all target words\nwas the most intelligible, suggesting that actual intelligibility does not\ncorrelate with perceived intelligibility. Additionally, we found that\nWhisper-ASR did not use the same cues as L2 speakers to differentiate difficult\nvowels and is not sufficient to assess the intelligibility of TTS systems for\nthese individuals.", "published": "2025-06-29 18:55:05", "link": "http://arxiv.org/abs/2506.23367v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Density, asymmetry and citation dynamics in scientific literature", "abstract": "Scientific behavior is often characterized by a tension between building upon\nestablished knowledge and introducing novel ideas. Here, we investigate whether\nthis tension is reflected in the relationship between the similarity of a\nscientific paper to previous research and its eventual citation rate. To\noperationalize similarity to previous research, we introduce two complementary\nmetrics to characterize the local geometry of a publication's semantic\nneighborhood: (1) \\emph{density} ($\\rho$), defined as the ratio between a fixed\nnumber of previously-published papers and the minimum distance enclosing those\npapers in a semantic embedding space, and (2) asymmetry ($\\alpha$), defined as\nthe average directional difference between a paper and its nearest neighbors.\nWe tested the predictive relationship between these two metrics and its\nsubsequent citation rate using a Bayesian hierarchical regression approach,\nsurveying $\\sim 53,000$ publications across nine academic disciplines and five\ndifferent document embeddings. While the individual effects of $\\rho$ on\ncitation count are small and variable, incorporating density-based predictors\nconsistently improves out-of-sample prediction when added to baseline models.\nThese results suggest that the density of a paper's surrounding scientific\nliterature may carry modest but informative signals about its eventual impact.\nMeanwhile, we find no evidence that publication asymmetry improves model\npredictions of citation rates. Our work provides a scalable framework for\nlinking document embeddings to scientometric outcomes and highlights new\nquestions regarding the role that semantic similarity plays in shaping the\ndynamics of scientific reward.", "published": "2025-06-29 18:55:04", "link": "http://arxiv.org/abs/2506.23366v1", "categories": ["cs.DL", "cs.CL", "cs.SI"], "primary_category": "cs.DL"}
{"title": "ATGen: A Framework for Active Text Generation", "abstract": "Active learning (AL) has demonstrated remarkable potential in reducing the\nannotation effort required for training machine learning models. However,\ndespite the surging popularity of natural language generation (NLG) tasks in\nrecent years, the application of AL to NLG has been limited. In this paper, we\nintroduce Active Text Generation (ATGen) - a comprehensive framework that\nbridges AL with text generation tasks, enabling the application of\nstate-of-the-art AL strategies to NLG. Our framework simplifies AL-empowered\nannotation in NLG tasks using both human annotators and automatic annotation\nagents based on large language models (LLMs). The framework supports LLMs\ndeployed as services, such as ChatGPT and Claude, or operated on-premises.\nFurthermore, ATGen provides a unified platform for smooth implementation and\nbenchmarking of novel AL strategies tailored to NLG tasks. Finally, we present\nevaluation results for state-of-the-art AL strategies across diverse settings\nand multiple text generation tasks. We show that ATGen reduces both the effort\nof human annotators and costs associated with API calls to LLM-based annotation\nagents. The code of the framework is available on GitHub under the MIT license.\nThe video presentation is available at http://atgen-video.nlpresearch.group", "published": "2025-06-29 17:27:48", "link": "http://arxiv.org/abs/2506.23342v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Information Loss in LLMs' Multilingual Translation: The Role of Training Data, Language Proximity, and Language Family", "abstract": "Large language models have achieved impressive progress in multilingual\ntranslation, yet they continue to face challenges with certain language\npairs-particularly those with limited training data or significant linguistic\ndivergence from English. This study systematically investigates how training\ndata, language proximity, and language family affect information loss in\nmultilingual translation. We evaluate two large language models, GPT-4 and\nLlama 2, by performing round-trip translations. Translation quality was\nassessed using BLEU scores and BERT similarity metrics. Our results reveal a\nrobust interaction between training data size and language distance: while\nabundant training data can mitigate the effects of linguistic divergence,\nlanguages structurally closer to English consistently yield higher translation\nquality in low-resource conditions. Among various distance metrics,\northographic, phylogenetic, syntactic, and geographical distances emerge as\nstrong predictors of translation performance. Language family also exerts an\nindependent influence. These findings contribute to a deeper understanding of\nthe linguistic constraints shaping multilingual translation in large language\nmodels, emphasizing that translation quality is shaped not only by data volume\nbut also by structural and typological relationships between languages.", "published": "2025-06-29 17:21:05", "link": "http://arxiv.org/abs/2506.23340v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GaussMaster: An LLM-based Database Copilot System", "abstract": "In the financial industry, data is the lifeblood of operations, and DBAs\nshoulder significant responsibilities for SQL tuning, database deployment,\ndiagnosis, and service repair. In recent years, both database vendors and\ncustomers have increasingly turned to autonomous database platforms in an\neffort to alleviate the heavy workload of DBAs. However, existing autonomous\ndatabase platforms are limited in their capabilities, primarily addressing\nsingle-point issues such as NL2SQL, anomaly detection, and SQL tuning. Manual\nintervention remains a necessity for comprehensive database maintenance.\nGaussMaster aims to revolutionize this landscape by introducing an LLM-based\ndatabase copilot system. This innovative solution is designed not only to\nassist developers in writing efficient SQL queries but also to provide\ncomprehensive care for database services. When database instances exhibit\nabnormal behavior, GaussMaster is capable of orchestrating the entire\nmaintenance process automatically. It achieves this by analyzing hundreds of\nmetrics and logs, employing a Tree-of-thought approach to identify root causes,\nand invoking appropriate tools to resolve issues. We have successfully\nimplemented GaussMaster in real-world scenarios, such as the banking industry,\nwhere it has achieved zero human intervention for over 34 database maintenance\nscenarios. In this paper, we present significant improvements in these tasks\nwith code at https://gitcode.com/opengauss/openGauss-GaussMaster.", "published": "2025-06-29 16:39:31", "link": "http://arxiv.org/abs/2506.23322v1", "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.DB"}
{"title": "Ensemble BERT for Medication Event Classification on Electronic Health Records (EHRs)", "abstract": "Identification of key variables such as medications, diseases, relations from\nhealth records and clinical notes has a wide range of applications in the\nclinical domain. n2c2 2022 provided shared tasks on challenges in natural\nlanguage processing for clinical data analytics on electronic health records\n(EHR), where it built a comprehensive annotated clinical data Contextualized\nMedication Event Dataset (CMED). This study focuses on subtask 2 in Track 1 of\nthis challenge that is to detect and classify medication events from clinical\nnotes through building a novel BERT-based ensemble model. It started with\npretraining BERT models on different types of big data such as Wikipedia and\nMIMIC. Afterwards, these pretrained BERT models were fine-tuned on CMED\ntraining data. These fine-tuned BERT models were employed to accomplish\nmedication event classification on CMED testing data with multiple predictions.\nThese multiple predictions generated by these fine-tuned BERT models were\nintegrated to build final prediction with voting strategies. Experimental\nresults demonstrated that BERT-based ensemble models can effectively improve\nstrict Micro-F score by about 5% and strict Macro-F score by about 6%,\nrespectively.", "published": "2025-06-29 16:17:17", "link": "http://arxiv.org/abs/2506.23315v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Objective-Free Local Learning and Emergent Language Structure in Thinking Machines", "abstract": "We present a neuro-symbolic framework for generative language modeling based\non local, event-driven emergent learning. At its core is a hierarchical\nHopfield memory chain acting as a compositional short-term memory and dynamic\ntokenizer (retokenizer). Rather than relying on predefined tokens or\nsupervision, the model builds structure from scratch, learning symbol sequences\nas multi-scale representations. It constructs projection tensors that bind\nco-occurring features into hierarchical tokens, introducing redundancy (i.e an\nemergent gauge structure) and enabling compression of local activations into\nlong-range dependencies. Curiously, we find that the retokenizer can filter\nnatural language patterns from noise, generating synthetic languages with\ncoherent internal morphology -- quantifiably the same as human language.\nLanguage is learned in a local (Hebbian) fashion, where model constraints\ndictate allowed emergent structure, and new information is retained in\nalignment with this structure. The absence of a global objective enables a form\nof plasticity not found in conventional language models, allowing the system to\ngeneralize beyond its initial inference class -- even without explicit data. We\ndemonstrate that briefly activating a new neuron during inference binds\ndistributed multi-scale token features into a symbolic embedding. These\nemergent embedding neurons act as long-term memory and support a key-value\nmechanism for compositional inference and generalization. This architecture\nprovides a methodological foundation for studying how symbolic structure can\nemerge from local neural learning. It offers a new pathway for building\nscalable, interpretable neuro-symbolic systems -- where tokens, grammar, and\nreasoning arise as compressed memory traces within a Hopfield hierarchy. This\napproach advances the development of neuromorphic architectures for generative\nlanguage models.", "published": "2025-06-29 15:29:13", "link": "http://arxiv.org/abs/2506.23293v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "Two Spelling Normalization Approaches Based on Large Language Models", "abstract": "The absence of standardized spelling conventions and the organic evolution of\nhuman language present an inherent linguistic challenge within historical\ndocuments, a longstanding concern for scholars in the humanities. Addressing\nthis issue, spelling normalization endeavors to align a document's orthography\nwith contemporary standards. In this study, we propose two new approaches based\non large language models: one of which has been trained without a supervised\ntraining, and a second one which has been trained for machine translation. Our\nevaluation spans multiple datasets encompassing diverse languages and\nhistorical periods, leading us to the conclusion that while both of them\nyielded encouraging results, statistical machine translation still seems to be\nthe most suitable technology for this task.", "published": "2025-06-29 15:25:09", "link": "http://arxiv.org/abs/2506.23288v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Corrupted by Reasoning: Reasoning Language Models Become Free-Riders in Public Goods Games", "abstract": "As large language models (LLMs) are increasingly deployed as autonomous\nagents, understanding their cooperation and social mechanisms is becoming\nincreasingly important. In particular, how LLMs balance self-interest and\ncollective well-being is a critical challenge for ensuring alignment,\nrobustness, and safe deployment. In this paper, we examine the challenge of\ncostly sanctioning in multi-agent LLM systems, where an agent must decide\nwhether to invest its own resources to incentivize cooperation or penalize\ndefection. To study this, we adapt a public goods game with institutional\nchoice from behavioral economics, allowing us to observe how different LLMs\nnavigate social dilemmas over repeated interactions. Our analysis reveals four\ndistinct behavioral patterns among models: some consistently establish and\nsustain high levels of cooperation, others fluctuate between engagement and\ndisengagement, some gradually decline in cooperative behavior over time, and\nothers rigidly follow fixed strategies regardless of outcomes. Surprisingly, we\nfind that reasoning LLMs, such as the o1 series, struggle significantly with\ncooperation, whereas some traditional LLMs consistently achieve high levels of\ncooperation. These findings suggest that the current approach to improving\nLLMs, which focuses on enhancing their reasoning capabilities, does not\nnecessarily lead to cooperation, providing valuable insights for deploying LLM\nagents in environments that require sustained collaboration. Our code is\navailable at https://github.com/davidguzmanp/SanctSim", "published": "2025-06-29 15:02:47", "link": "http://arxiv.org/abs/2506.23276v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Generalist Reward Models: Found Inside Large Language Models", "abstract": "The alignment of Large Language Models (LLMs) is critically dependent on\nreward models trained on costly human preference data. While recent work\nexplores bypassing this cost with AI feedback, these methods often lack a\nrigorous theoretical foundation. In this paper, we discover that a powerful\ngeneralist reward model is already latently present within any LLM trained via\nstandard next-token prediction. We prove that this endogenous reward is not a\nheuristic, but is theoretically equivalent to a reward function learned through\noffline inverse reinforcement learning. This connection allows us to directly\nelicit a high-quality reward signal from a base (pre-trained or supervised\nfine-tuned) model without any further training. Critically, we also prove that\nsubsequent reinforcement learning using this endogenous reward leads to a\npolicy with a provably superior error bound compared to the base model. To our\nbest knowledge, this is the first theoretical proof of the effectiveness of\nreinforcement learning for LLMs. Our experiments validate this theory,\ndemonstrating that our method not only outperforms existing LLM-as-a-judge\napproaches but can also surpass explicitly trained reward models. These\nfindings suggest that the reward modeling stage can be replaced by a principled\nmethod of eliciting the knowledge already captured during pre-training,\nheralding a more efficient, powerful, and scalable paradigm for LLMs alignment\nas well as multi-modal models.", "published": "2025-06-29 13:45:54", "link": "http://arxiv.org/abs/2506.23235v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Masked Gated Linear Unit", "abstract": "Gated Linear Units (GLUs) have become essential components in the\nfeed-forward networks of state-of-the-art Large Language Models (LLMs).\nHowever, they require twice as many memory reads compared to feed-forward\nlayers without gating, due to the use of separate weight matrices for the gate\nand value streams. To address this bottleneck, we introduce Masked Gated Linear\nUnits (MGLUs), a novel family of GLUs with an efficient kernel implementation.\nThe core contribution of MGLUs include: (1) the Mixture of Element-wise Gating\n(MoEG) architecture that learns multiple binary masks, each determining gate or\nvalue assignments at the element level on a single shared weight matrix\nresulting in reduced memory transfer, and (2) FlashMGLU, a hardware-friendly\nkernel that yields up to a 19.7 $\\times$ inference-time speed-up over a naive\nPyTorch MGLU and is 47% more memory-efficient and 34% faster than standard GLUs\ndespite added architectural complexity on an RTX5090 GPU. In LLM experiments,\nthe Swish-activated variant SwiMGLU preserves its memory advantages while\nmatching - or even surpassing - the downstream accuracy of the SwiGLU baseline.", "published": "2025-06-29 13:16:20", "link": "http://arxiv.org/abs/2506.23225v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "UrbanLLaVA: A Multi-modal Large Language Model for Urban Intelligence with Spatial Reasoning and Understanding", "abstract": "Urban research involves a wide range of scenarios and tasks that require the\nunderstanding of multi-modal data. Current methods often focus on specific data\ntypes and lack a unified framework in urban field for processing them\ncomprehensively. The recent success of multi-modal large language models\n(MLLMs) presents a promising opportunity to overcome this limitation. In this\npaper, we introduce $\\textit{UrbanLLaVA}$, a multi-modal large language model\ndesigned to process these four types of data simultaneously and achieve strong\nperformance across diverse urban tasks compared with general MLLMs. In\n$\\textit{UrbanLLaVA}$, we first curate a diverse urban instruction dataset\nencompassing both single-modal and cross-modal urban data, spanning from\nlocation view to global view of urban environment. Additionally, we propose a\nmulti-stage training framework that decouples spatial reasoning enhancement\nfrom domain knowledge learning, thereby improving the compatibility and\ndownstream performance of $\\textit{UrbanLLaVA}$ across diverse urban tasks.\nFinally, we also extend existing benchmark for urban research to assess the\nperformance of MLLMs across a wide range of urban tasks. Experimental results\nfrom three cities demonstrate that $\\textit{UrbanLLaVA}$ outperforms\nopen-source and proprietary MLLMs in both single-modal tasks and complex\ncross-modal tasks and shows robust generalization abilities across cities.\nSource codes and data are openly accessible to the research community via\nhttps://github.com/tsinghua-fib-lab/UrbanLLaVA.", "published": "2025-06-29 13:04:27", "link": "http://arxiv.org/abs/2506.23219v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "RiverText: A Python Library for Training and Evaluating Incremental Word Embeddings from Text Data Streams", "abstract": "Word embeddings have become essential components in various information\nretrieval and natural language processing tasks, such as ranking, document\nclassification, and question answering. However, despite their widespread use,\ntraditional word embedding models present a limitation in their static nature,\nwhich hampers their ability to adapt to the constantly evolving language\npatterns that emerge in sources such as social media and the web (e.g., new\nhashtags or brand names). To overcome this problem, incremental word embedding\nalgorithms are introduced, capable of dynamically updating word representations\nin response to new language patterns and processing continuous data streams.\n  This paper presents RiverText, a Python library for training and evaluating\nincremental word embeddings from text data streams. Our tool is a resource for\nthe information retrieval and natural language processing communities that work\nwith word embeddings in streaming scenarios, such as analyzing social media.\nThe library implements different incremental word embedding techniques, such as\nSkip-gram, Continuous Bag of Words, and Word Context Matrix, in a standardized\nframework. In addition, it uses PyTorch as its backend for neural network\ntraining. We have implemented a module that adapts existing intrinsic static\nword embedding evaluation tasks for word similarity and word categorization to\na streaming setting. Finally, we compare the implemented methods with different\nhyperparameter settings and discuss the results. Our open-source library is\navailable at https://github.com/dccuchile/rivertext.", "published": "2025-06-29 11:34:23", "link": "http://arxiv.org/abs/2506.23192v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "V-SYNTHESIS: Task-Agnostic Synthesis of Consistent and Diverse In-Context Demonstrations from Scratch via V-Entropy", "abstract": "High labeling cost for in-context learning (ICL) demonstrations motivates\nusing large language models (LLMs) for synthesis to reduce overhead. However,\nexisting synthesis methods are mainly task-specific or rely on pre-existing\ndemonstrations. So this paper focuses on synthesizing demonstrations from\nscratch for arbitrary tasks. A major challenge in synthesizing from scratch is\nensuring consistency with the target task, as the lack of labeling guidance\ncould lead to synthesis bias. We first propose a consistency metric called\nV-Score, which has higher performance and lower computation cost compared with\nthe metrics based on grams or embedding vectors. Furthermore, we introduce\nV-Synthesis, which leverages V-Score for proportional sampling to ensure both\nhigh consistency and diversity of synthesized demonstrations. Experimental\nresults demonstrate that V-Synthesis yields an average performance improvement\nof 2.0% compared to existing synthesis methods confirming the effectiveness of\nV-Synthesis.", "published": "2025-06-29 08:57:09", "link": "http://arxiv.org/abs/2506.23149v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning-to-Context Slope: Evaluating In-Context Learning Effectiveness Beyond Performance Illusions", "abstract": "In-context learning (ICL) has emerged as an effective approach to enhance the\nperformance of large language models (LLMs). However, its effectiveness varies\nsignificantly across models and tasks, posing challenges for practitioners to\ndetermine when ICL reliably improves performance. Current evaluation\napproaches, reliant on performance change after applying ICL, suffer from low\nreliability, poor attribution, and impracticality in data-insufficient\nscenarios. We propose the Learning-to-Context Slope (LCS), a novel metric that\nquantifies ICL effectiveness by modeling the slope between learning gain (loss\ndecrease from demonstrations) and contextual relevance (demonstration-input\nrelevance). LCS addresses key limitations of performance-based metrics: (1) it\ncaptures continuous loss changes even when outputs are incorrect, improving\nreliability; (2) its formulation attributes ICL failures to weak contextual\nalignment (inability to adapt inputs to demonstrations) or strong output\ncalibration (self-verification of correctness); and (3) it minimizes reliance\non labeled data via synthetic evaluation. Extensive experiments demonstrate\nthat LCS strongly correlates with performance improvements in labeled settings\nand reliably reflects true effectiveness in biased or data-scarce scenarios.\nFurther analysis reveals actionable thresholds for LCS and identifies model\ncapabilities critical to ICL success.", "published": "2025-06-29 08:55:37", "link": "http://arxiv.org/abs/2506.23146v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Benchmarking Deep Search over Heterogeneous Enterprise Data", "abstract": "We present a new benchmark for evaluating Deep Search--a realistic and\ncomplex form of retrieval-augmented generation (RAG) that requires\nsource-aware, multi-hop reasoning over diverse, sparsed, but related sources.\nThese include documents, meeting transcripts, Slack messages, GitHub, and URLs,\nwhich vary in structure and often contain human-to-human interactions. We build\nit using a synthetic data pipeline that simulates business workflows across\nproduct planning, development, and support stages, generating interconnected\ncontent with realistic noise and multi-hop questions with guaranteed\nground-truth answers. We release our benchmark with both answerable and\nunanswerable queries, and retrieval pool of 39,190 enterprise artifacts,\nenabling fine-grained evaluation of long-context LLM and RAG systems. Our\nexperiments reveal that even the best-performing agentic RAG methods achieve an\naverage performance score of 32.96 on our benchmark. With further analysis, we\nhighlight retrieval as the main bottleneck: existing methods struggle to\nconduct deep searches and retrieve all necessary evidence. Consequently, they\noften reason over partial context, leading to significant performance\ndegradation.", "published": "2025-06-29 08:34:59", "link": "http://arxiv.org/abs/2506.23139v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Flow-Modulated Scoring for Semantic-Aware Knowledge Graph Completion", "abstract": "Effective modeling of multifaceted relations is pivotal for Knowledge Graph\nCompletion (KGC). However, a majority of existing approaches are predicated on\nstatic, embedding-based scoring, exhibiting inherent limitations in capturing\ncontextual dependencies and relational dynamics. Addressing this gap, we\npropose the Flow-Modulated Scoring (FMS) framework. FMS comprises two principal\ncomponents: (1) a semantic context learning module that encodes\ncontext-sensitive entity representations, and (2) a conditional flow-matching\nmodule designed to learn the dynamic transformation from a head to a tail\nembedding, governed by the aforementioned context. The resultant predictive\nvector field, representing the context-informed relational path, serves to\ndynamically refine the initial static score of an entity pair. Through this\nsynergy of context-aware static representations and conditioned dynamic\ninformation, FMS facilitates a more profound modeling of relational semantics.\nComprehensive evaluations on several standard benchmarks demonstrate that our\nproposed method surpasses prior state-of-the-art results.", "published": "2025-06-29 08:22:04", "link": "http://arxiv.org/abs/2506.23137v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLM-Assisted Question-Answering on Technical Documents Using Structured Data-Aware Retrieval Augmented Generation", "abstract": "Large Language Models (LLMs) are capable of natural language understanding\nand generation. But they face challenges such as hallucination and outdated\nknowledge. Fine-tuning is one possible solution, but it is resource-intensive\nand must be repeated with every data update. Retrieval-Augmented Generation\n(RAG) offers an efficient solution by allowing LLMs to access external\nknowledge sources. However, traditional RAG pipelines struggle with retrieving\ninformation from complex technical documents with structured data such as\ntables and images. In this work, we propose a RAG pipeline, capable of handling\ntables and images in documents, for technical documents that support both\nscanned and searchable formats. Its retrieval process combines vector\nsimilarity search with a fine-tuned reranker based on Gemma-2-9b-it. The\nreranker is trained using RAFT (Retrieval-Augmented Fine-Tuning) on a custom\ndataset designed to improve context identification for question answering. Our\nevaluation demonstrates that the proposed pipeline achieves a high faithfulness\nscore of 94% (RAGas) and 96% (DeepEval), and an answer relevancy score of 87%\n(RAGas) and 93% (DeepEval). Comparative analysis demonstrates that the proposed\narchitecture is superior to general RAG pipelines in terms of table-based\nquestions and handling questions outside context.", "published": "2025-06-29 08:22:03", "link": "http://arxiv.org/abs/2506.23136v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Format-Adapter: Improving Reasoning Capability of LLMs by Adapting Suitable Format", "abstract": "Generating and voting multiple answers is an effective method to mitigate\nreasoning inconsistencies of large language models (LLMs). Prior works have\nshown that multiple reasoning formats outperform a single format when\ngenerating multiple answers. However, previous works using multiple formats\nrely on formats labeled by humans, which could be unsuitable for all tasks and\nhave high labeling costs. To address this issue, we adapt suitable formats to\nthe given tasks by generating and selecting formats. We first propose how to\nmeasure the reasoning error when generating multiple answers. Then, we\nintroduce Format-Adapter, which utilizes LLMs to generate and select suitable\nreasoning formats by minimizing the error measurement we present. We conduct\nexperiments on math and commonsense reasoning tasks, where Format-Adapter\nachieves a 4.3% performance improvement on average over previous works,\ndemonstrating the effectiveness.", "published": "2025-06-29 08:11:52", "link": "http://arxiv.org/abs/2506.23133v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unleashing Embodied Task Planning Ability in LLMs via Reinforcement Learning", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, yet they face significant challenges in embodied task planning\nscenarios that require continuous environmental understanding and action\ngeneration. Existing approaches generate open-loop action scripts based on\nstatic knowledge, making it difficult to learn causal relationships between\nactions and environmental feedback, particularly in partially observable\nenvironments. We introduce Embodied Planner-R1, a novel outcome-driven\nreinforcement learning framework that enables LLMs to develop interactive\ncapabilities through autonomous exploration with minimal supervision. Our\nframework incorporates three key innovations: (1) Without human annotations, we\nemploy pure reinforcement learning with group rollout, incorporating\nin-environment interaction through parallel exploration; (2) completion-driven\nsparse reward; and (3) Interactive Policy Optimization (IPO) for efficient\nlearning from grouped trajectories. Across two challenging text-based Embodied\nplanning benchmarks, Embodied Planner-R1 achieves impressive completion rates\nof 97.78% on ALFWorld and 79.92% on ScienceWorld, surpassing prior methods by a\nlarge margin, and suffers only a -3.66% drop in previously unseen environments,\nevidencing strong generalization.", "published": "2025-06-29 07:31:24", "link": "http://arxiv.org/abs/2506.23127v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Decoding Memes: Benchmarking Narrative Role Classification across Multilingual and Multimodal Models", "abstract": "This work investigates the challenging task of identifying narrative roles -\nHero, Villain, Victim, and Other - in Internet memes, across three diverse test\nsets spanning English and code-mixed (English-Hindi) languages. Building on an\nannotated dataset originally skewed toward the 'Other' class, we explore a more\nbalanced and linguistically diverse extension, originally introduced as part of\nthe CLEF 2024 shared task. Comprehensive lexical and structural analyses\nhighlight the nuanced, culture-specific, and context-rich language used in real\nmemes, in contrast to synthetically curated hateful content, which exhibits\nexplicit and repetitive lexical markers. To benchmark the role detection task,\nwe evaluate a wide spectrum of models, including fine-tuned multilingual\ntransformers, sentiment and abuse-aware classifiers, instruction-tuned LLMs,\nand multimodal vision-language models. Performance is assessed under zero-shot\nsettings using precision, recall, and F1 metrics. While larger models like\nDeBERTa-v3 and Qwen2.5-VL demonstrate notable gains, results reveal consistent\nchallenges in reliably identifying the 'Victim' class and generalising across\ncultural and code-mixed content. We also explore prompt design strategies to\nguide multimodal models and find that hybrid prompts incorporating structured\ninstructions and role definitions offer marginal yet consistent improvements.\nOur findings underscore the importance of cultural grounding, prompt\nengineering, and multimodal reasoning in modelling subtle narrative framings in\nvisual-textual content.", "published": "2025-06-29 07:12:11", "link": "http://arxiv.org/abs/2506.23122v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional Multimodal Embeddings", "abstract": "Multimodal embedding models, built upon causal Vision Language Models (VLMs),\nhave shown promise in various tasks. However, current approaches face three key\nlimitations: the use of causal attention in VLM backbones is suboptimal for\nembedding tasks; scalability issues due to reliance on high-quality labeled\npaired data for contrastive learning; and limited diversity in training\nobjectives and data. To address these issues, we propose MoCa, a two-stage\nframework for transforming pre-trained VLMs into effective bidirectional\nmultimodal embedding models. The first stage, Modality-aware Continual\nPre-training, introduces a joint reconstruction objective that simultaneously\ndenoises interleaved text and image inputs, enhancing bidirectional\ncontext-aware reasoning. The second stage, Heterogeneous Contrastive\nFine-tuning, leverages diverse, semantically rich multimodal data beyond simple\nimage-caption pairs to enhance generalization and alignment. Our method\naddresses the stated limitations by introducing bidirectional attention through\ncontinual pre-training, scaling effectively with massive unlabeled datasets via\njoint reconstruction objectives, and utilizing diverse multimodal data for\nenhanced representation robustness. Experiments demonstrate that MoCa\nconsistently improves performance across MMEB and ViDoRe-v2 benchmarks,\nachieving new state-of-the-art results, and exhibits strong scalability with\nboth model size and training data on MMEB.", "published": "2025-06-29 06:41:00", "link": "http://arxiv.org/abs/2506.23115v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "FairI Tales: Evaluation of Fairness in Indian Contexts with a Focus on Bias and Stereotypes", "abstract": "Existing studies on fairness are largely Western-focused, making them\ninadequate for culturally diverse countries such as India. To address this gap,\nwe introduce INDIC-BIAS, a comprehensive India-centric benchmark designed to\nevaluate fairness of LLMs across 85 identity groups encompassing diverse\ncastes, religions, regions, and tribes. We first consult domain experts to\ncurate over 1,800 socio-cultural topics spanning behaviors and situations,\nwhere biases and stereotypes are likely to emerge. Grounded in these topics, we\ngenerate and manually validate 20,000 real-world scenario templates to probe\nLLMs for fairness. We structure these templates into three evaluation tasks:\nplausibility, judgment, and generation. Our evaluation of 14 popular LLMs on\nthese tasks reveals strong negative biases against marginalized identities,\nwith models frequently reinforcing common stereotypes. Additionally, we find\nthat models struggle to mitigate bias even when explicitly asked to rationalize\ntheir decision. Our evaluation provides evidence of both allocative and\nrepresentational harms that current LLMs could cause towards Indian identities,\ncalling for a more cautious usage in practical applications. We release\nINDIC-BIAS as an open-source benchmark to advance research on benchmarking and\nmitigating biases and stereotypes in the Indian context.", "published": "2025-06-29 06:31:06", "link": "http://arxiv.org/abs/2506.23111v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Individuals to Interactions: Benchmarking Gender Bias in Multimodal Large Language Models from the Lens of Social Relationship", "abstract": "Multimodal large language models (MLLMs) have shown impressive capabilities\nacross tasks involving both visual and textual modalities. However, growing\nconcerns remain about their potential to encode and amplify gender bias,\nparticularly in socially sensitive applications. Existing benchmarks\npredominantly evaluate bias in isolated scenarios, overlooking how bias may\nemerge subtly through interpersonal interactions. We fill this gap by going\nbeyond single-entity evaluation and instead focusing on a deeper examination of\nrelational and contextual gender bias in dual-individual interactions. We\nintroduce Genres, a novel benchmark designed to evaluate gender bias in MLLMs\nthrough the lens of social relationships in generated narratives. Genres\nassesses gender bias through a dual-character profile and narrative generation\ntask that captures rich interpersonal dynamics and supports a fine-grained bias\nevaluation suite across multiple dimensions. Experiments on both open- and\nclosed-source MLLMs reveal persistent, context-sensitive gender biases that are\nnot evident in single-character settings. Our findings underscore the\nimportance of relationship-aware benchmarks for diagnosing subtle,\ninteraction-driven gender bias in MLLMs and provide actionable insights for\nfuture bias mitigation.", "published": "2025-06-29 06:03:21", "link": "http://arxiv.org/abs/2506.23101v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Text2VectorSQL: Bridging Text-to-SQL and Vector Search for Unified Natural Language Queries", "abstract": "While Text-to-SQL enables natural language interaction with structured\ndatabases, its effectiveness diminishes with unstructured data or ambiguous\nqueries due to rigid syntax and limited expressiveness. Concurrently, vector\nsearch has emerged as a powerful paradigm for semantic retrieval, particularly\nfor unstructured data. However, existing VectorSQL implementations still rely\nheavily on manual crafting and lack tailored evaluation frameworks, leaving a\nsignificant gap between theoretical potential and practical deployment. To\nbridge these complementary paradigms, we introduces Text2VectorSQL, a novel\nframework unifying Text-to-SQL and vector search to overcome expressiveness\nconstraints and support more diverse and holistical natural language queries.\nSpecifically, Text2VectorSQL enables semantic filtering, multi-modal matching,\nand retrieval acceleration. For evaluation, we build vector index on\nappropriate columns, extend user queries with semantic search, and annotate\nground truths via an automatic pipeline with expert review. Furthermore, we\ndevelop dedicated Text2VectorSQL models with synthetic data, demonstrating\nsignificant performance improvements over baseline methods. Our work\nestablishes the foundation for the Text2VectorSQL task, paving the way for more\nversatile and intuitive database interfaces. The repository will be publicly\navailable at https://github.com/Open-DataFlow/Text2VectorSQL.", "published": "2025-06-29 03:17:42", "link": "http://arxiv.org/abs/2506.23071v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Boosting LLM's Molecular Structure Elucidation with Knowledge Enhanced Tree Search Reasoning", "abstract": "Molecular structure elucidation involves deducing a molecule's structure from\nvarious types of spectral data, which is crucial in chemical experimental\nanalysis. While large language models (LLMs) have shown remarkable proficiency\nin analyzing and reasoning through complex tasks, they still encounter\nsubstantial challenges in molecular structure elucidation. We identify that\nthese challenges largely stem from LLMs' limited grasp of specialized chemical\nknowledge. In this work, we introduce a Knowledge-enhanced reasoning framework\nfor Molecular Structure Elucidation (K-MSE), leveraging Monte Carlo Tree Search\nfor test-time scaling as a plugin. Specifically, we construct an external\nmolecular substructure knowledge base to extend the LLMs' coverage of the\nchemical structure space. Furthermore, we design a specialized\nmolecule-spectrum scorer to act as a reward model for the reasoning process,\naddressing the issue of inaccurate solution evaluation in LLMs. Experimental\nresults show that our approach significantly boosts performance, particularly\ngaining more than 20% improvement on both GPT-4o-mini and GPT-4o. Our code is\navailable at https://github.com/HICAI-ZJU/K-MSE.", "published": "2025-06-29 02:00:38", "link": "http://arxiv.org/abs/2506.23056v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MariNER: A Dataset for Historical Brazilian Portuguese Named Entity Recognition", "abstract": "Named Entity Recognition (NER) is a fundamental Natural Language Processing\n(NLP) task that aims to identify and classify entity mentions in texts across\ndifferent categories. While languages such as English possess a large number of\nhigh-quality resources for this task, Brazilian Portuguese still lacks in\nquantity of gold-standard NER datasets, especially when considering specific\ndomains. Particularly, this paper considers the importance of NER for analyzing\nhistorical texts in the context of digital humanities. To address this gap,\nthis work outlines the construction of MariNER: \\textit{Mapeamento e\nAnota\\c{c}\\~oes de Registros hIst\\'oricos para NER} (Mapping and Annotation of\nHistorical Records for NER), the first gold-standard dataset for early\n20th-century Brazilian Portuguese, with more than 9,000 manually annotated\nsentences. We also assess and compare the performance of state-of-the-art NER\nmodels for the dataset.", "published": "2025-06-29 01:23:57", "link": "http://arxiv.org/abs/2506.23051v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AURA: Agent for Understanding, Reasoning, and Automated Tool Use in Voice-Driven Tasks", "abstract": "Despite advances in language and speech technologies, no open-source system\nenables full speech-to-speech, multi-turn dialogue with integrated tool use and\nagentic reasoning. We introduce AURA (Agent for Understanding, Reasoning, and\nAutomated Tool Use), the first open-source, speech-native assistant capable of\ncompleting complex, goal-driven tasks through dynamic tool invocation and\nmulti-turn conversation. AURA combines open-weight ASR, TTS, and LLMs in a\ncascaded pipeline and supports tools such as calendar booking, contact lookup,\nweb search, and email. Its modular design allows easy integration of new tools\nusing natural language prompts and action classes. On VoiceBench, AURA scores\n92.75% on OpenBookQA-outperforming all open-weight systems and nearing\nGPT-4o-and 4.39 on AlpacaEval, competitive with other open-weight systems.\nHuman evaluation shows 90% task success on complex, multi-turn speech tasks.", "published": "2025-06-29 01:13:15", "link": "http://arxiv.org/abs/2506.23049v1", "categories": ["cs.AI", "cs.CL", "cs.SD", "eess.AS", "68T42, 68T50,", "I.2.7; I.2.11; H.5.5"], "primary_category": "cs.AI"}
{"title": "SoMi-ToM: Evaluating Multi-Perspective Theory of Mind in Embodied Social Interactions", "abstract": "Humans continuously infer the states, goals, and behaviors of others by\nperceiving their surroundings in dynamic, real-world social interactions.\nHowever, most Theory of Mind (ToM) benchmarks only evaluate static, text-based\nscenarios, which have a significant gap compared to real interactions. We\npropose the SoMi-ToM benchmark, designed to evaluate multi-perspective ToM in\nembodied multi-agent complex social interactions. This benchmark is based on\nrich multimodal interaction data generated by the interaction environment SoMi,\ncovering diverse crafting goals and social relationships. Our framework\nsupports multi-level evaluation: (1) first-person evaluation provides\nmultimodal (visual, dialogue, action, etc.) input from a first-person\nperspective during a task for real-time state inference, (2) third-person\nevaluation provides complete third-person perspective video and text records\nafter a task for goal and behavior inference. This evaluation method allows for\na more comprehensive examination of a model's ToM capabilities from both the\nsubjective immediate experience and the objective global observation. We\nconstructed a challenging dataset containing 35 third-person perspective\nvideos, 363 first-person perspective images, and 1225 expert-annotated\nmultiple-choice questions (three options). On this dataset, we systematically\nevaluated the performance of human subjects and several state-of-the-art large\nvision-language models (LVLMs). The results show that LVLMs perform\nsignificantly worse than humans on SoMi-ToM: the average accuracy gap between\nhumans and models is 40.1% in first-person evaluation and 26.4% in third-person\nevaluation. This indicates that future LVLMs need to further improve their ToM\ncapabilities in embodied, complex social interactions.", "published": "2025-06-29 00:54:13", "link": "http://arxiv.org/abs/2506.23046v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.RO"], "primary_category": "cs.CL"}
{"title": "Accurate Parameter-Efficient Test-Time Adaptation for Time Series Forecasting", "abstract": "Real-world time series often exhibit a non-stationary nature, degrading the\nperformance of pre-trained forecasting models. Test-Time Adaptation (TTA)\naddresses this by adjusting models during inference, but existing methods\ntypically update the full model, increasing memory and compute costs. We\npropose PETSA, a parameter-efficient method that adapts forecasters at test\ntime by only updating small calibration modules on the input and output. PETSA\nuses low-rank adapters and dynamic gating to adjust representations without\nretraining. To maintain accuracy despite limited adaptation capacity, we\nintroduce a specialized loss combining three components: (1) a robust term, (2)\na frequency-domain term to preserve periodicity, and (3) a patch-wise\nstructural term for structural alignment. PETSA improves the adaptability of\nvarious forecasting backbones while requiring fewer parameters than baselines.\nExperimental results on benchmark datasets show that PETSA achieves competitive\nor better performance across all horizons. Our code is available at:\nhttps://github.com/BorealisAI/PETSA", "published": "2025-06-29 23:09:35", "link": "http://arxiv.org/abs/2506.23424v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "BenchMake: Turn any scientific data set into a reproducible benchmark", "abstract": "Benchmark data sets are a cornerstone of machine learning development and\napplications, ensuring new methods are robust, reliable and competitive. The\nrelative rarity of benchmark sets in computational science, due to the\nuniqueness of the problems and the pace of change in the associated domains,\nmakes evaluating new innovations difficult for computational scientists. In\nthis paper a new tool is developed and tested to potentially turn any of the\nincreasing numbers of scientific data sets made openly available into a\nbenchmark accessible to the community. BenchMake uses non-negative matrix\nfactorisation to deterministically identify and isolate challenging edge cases\non the convex hull (the smallest convex set that contains all existing data\ninstances) and partitions a required fraction of matched data instances into a\ntesting set that maximises divergence and statistical significance, across\ntabular, graph, image, signal and textual modalities. BenchMake splits are\ncompared to establish splits and random splits using ten publicly available\nbenchmark sets from different areas of science, with different sizes, shapes,\ndistributions.", "published": "2025-06-29 22:56:48", "link": "http://arxiv.org/abs/2506.23419v1", "categories": ["cs.LG", "cs.AI", "cs.DL", "62G09", "J.1"], "primary_category": "cs.LG"}
{"title": "Efficient Algorithms for Learning and Compressing Monophonic Halfspaces in Graphs", "abstract": "Abstract notions of convexity over the vertices of a graph, and corresponding\nnotions of halfspaces, have recently gained attention from the machine learning\ncommunity. In this work we study monophonic halfspaces, a notion of graph\nhalfspaces defined through closure under induced paths. Our main result is a\n$2$-satisfiability based decomposition theorem, which allows one to represent\nmonophonic halfspaces as a disjoint union of certain vertex subsets. Using this\ndecomposition, we achieve efficient and (nearly) optimal algorithms for various\nlearning problems, such as teaching, active, and online learning. Most notably,\nwe obtain a polynomial-time algorithm for empirical risk minimization.\nIndependently of the decomposition theorem, we obtain an efficient, stable, and\nproper sample compression scheme. This makes monophonic halfspaces efficiently\nlearnable with proper learners and linear error rate $1/\\varepsilon$ in the\nrealizable PAC setting. Our results answer open questions from the literature,\nand show a stark contrast with geodesic halfspaces, for which most of the said\nlearning problems are NP-hard.", "published": "2025-06-29 11:14:16", "link": "http://arxiv.org/abs/2506.23186v1", "categories": ["cs.LG", "cs.DM", "math.CO", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Rises for Measuring Local Distributivity in Lattices", "abstract": "Distributivity is a well-established and extensively studied notion in\nlattice theory. In the context of data analysis, particularly within Formal\nConcept Analysis (FCA), lattices are often observed to exhibit a high degree of\ndistributivity. However, no standardized measure exists to quantify this\nproperty. In this paper, we introduce the notion of rises in (concept) lattices\nas a means to assess distributivity. Rises capture how the number of attributes\nor objects in covering concepts change within the concept lattice. We show that\na lattice is distributive if and only if no non-unit rises occur. Furthermore,\nwe relate rises to the classical notion of meet- and join distributivity. We\nobserve that concept lattices from real-world data are to a high degree\njoin-distributive, but much less meet-distributive. We additionally study how\njoin-distributivity manifests on the level of ordered sets.", "published": "2025-06-29 10:03:51", "link": "http://arxiv.org/abs/2506.23168v1", "categories": ["cs.AI", "cs.DM", "math.CO", "math.RA", "06B99", "G.2.1"], "primary_category": "cs.AI"}
{"title": "NaviX: A Native Vector Index Design for Graph DBMSs With Robust Predicate-Agnostic Search Performance", "abstract": "There is an increasing demand for extending existing DBMSs with vector\nindices so that they become unified systems capable of supporting modern\npredictive applications, which require joint querying of vector embeddings\ntogether with the structured properties and connections of objects. We present\nNaviX, a native vector index for graph DBMSs (GDBMSs) that has two main design\ngoals. First, we aim to implement a disk-based vector index that leverages the\ncore storage and query-processing capabilities of the underlying GDBMS. To this\nend, NaviX is built on the Hierarchical Navigable Small-World (HNSW) graph,\nwhich itself is a graph-based structure. Second, we aim to support\npredicate-agnostic filtered vector search queries, in which the k nearest\nneighbors (kNNs) of a query vector vQ are searched only within an arbitrary\nsubset S of vectors defined by an ad-hoc selection sub-query QS. We adopt a\nprefiltering approach that evaluates QS first and passes the full description\nof subset S to the kNN search operator. We study how to design a prefiltering\nsearch algorithm that remains robust under varying selectivities and under\ndifferent correlations between subset S and query vector vQ. We propose an\nadaptive algorithm that uses the local selectivity of each vector in the HNSW\ngraph to choose an appropriate heuristic at every iteration of the kNN search.\nFinally, We demonstrate NaviX's robustness and efficiency through extensive\nexperiments against both existing prefiltering- and postfiltering-based\nbaselines.", "published": "2025-06-29 21:16:07", "link": "http://arxiv.org/abs/2506.23397v1", "categories": ["cs.IR", "cs.DB"], "primary_category": "cs.IR"}
{"title": "Learning to Rank with Variable Result Presentation Lengths", "abstract": "Learning to Rank (LTR) methods generally assume that each document in a top-K\nranking is presented in an equal format. However, previous work has shown that\nusers' perceptions of relevance can be changed by varying presentations, i.e.,\nallocating more vertical space to some documents to provide additional textual\nor image information. Furthermore, presentation length can also redirect\nattention, as users are more likely to notice longer presentations when\nscrolling through results. Deciding on the document presentation lengths in a\nfixed vertical space ranking is an important problem that has not been\naddressed by existing LTR methods.\n  We address this gap by introducing the variable presentation length ranking\ntask, where simultaneously the ordering of documents and their presentation\nlength is decided. Despite being a generalization of standard ranking, we show\nthat this setting brings significant new challenges: Firstly, the probability\nranking principle no longer applies to this setting, and secondly, the problem\ncannot be divided into separate ordering and length selection tasks.\n  We therefore propose VLPL - a new family of Plackett-Luce list-wise gradient\nestimation methods for the joint optimization of document ordering and lengths.\nOur semi-synthetic experiments show that VLPL can effectively balance the\nexpected exposure and attractiveness of all documents, achieving the best\nperformance across different ranking settings. Furthermore, we observe that\neven simple length-aware methods can achieve significant performance\nimprovements over fixed-length models. Altogether, our theoretical and\nempirical results highlight the importance and difficulties of combining\ndocument presentation with LTR.", "published": "2025-06-29 16:28:17", "link": "http://arxiv.org/abs/2506.23319v1", "categories": ["cs.IR", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Impact of Shallow vs. Deep Relevance Judgments on BERT-based Reranking Models", "abstract": "This paper investigates the impact of shallow versus deep relevance judgments\non the performance of BERT-based reranking models in neural Information\nRetrieval. Shallow-judged datasets, characterized by numerous queries each with\nfew relevance judgments, and deep-judged datasets, involving fewer queries with\nextensive relevance judgments, are compared. The research assesses how these\ndatasets affect the performance of BERT-based reranking models trained on them.\nThe experiments are run on the MS MARCO and LongEval collections. Results\nindicate that shallow-judged datasets generally enhance generalization and\neffectiveness of reranking models due to a broader range of available contexts.\nThe disadvantage of the deep-judged datasets might be mitigated by a larger\nnumber of negative training examples.", "published": "2025-06-29 11:30:50", "link": "http://arxiv.org/abs/2506.23191v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Compositions of Variant Experts for Integrating Short-Term and Long-Term Preferences", "abstract": "In the online digital realm, recommendation systems are ubiquitous and play a\ncrucial role in enhancing user experience. These systems leverage user\npreferences to provide personalized recommendations, thereby helping users\nnavigate through the paradox of choice. This work focuses on personalized\nsequential recommendation, where the system considers not only a user's\nimmediate, evolving session context, but also their cumulative historical\nbehavior to provide highly relevant and timely recommendations. Through an\nempirical study conducted on diverse real-world datasets, we have observed and\nquantified the existence and impact of both short-term (immediate and\ntransient) and long-term (enduring and stable) preferences on users' historical\ninteractions. Building on these insights, we propose a framework that combines\nshort- and long-term preferences to enhance recommendation performance, namely\nCompositions of Variant Experts (CoVE). This novel framework dynamically\nintegrates short- and long-term preferences through the use of different\nspecialized recommendation models (i.e., experts). Extensive experiments\nshowcase the effectiveness of the proposed methods and ablation studies further\ninvestigate the impact of variant expert types.", "published": "2025-06-29 10:09:33", "link": "http://arxiv.org/abs/2506.23170v1", "categories": ["cs.IR", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Multi-task Offline Reinforcement Learning for Online Advertising in Recommender Systems", "abstract": "Online advertising in recommendation platforms has gained significant\nattention, with a predominant focus on channel recommendation and budget\nallocation strategies. However, current offline reinforcement learning (RL)\nmethods face substantial challenges when applied to sparse advertising\nscenarios, primarily due to severe overestimation, distributional shifts, and\noverlooking budget constraints. To address these issues, we propose MTORL, a\nnovel multi-task offline RL model that targets two key objectives. First, we\nestablish a Markov Decision Process (MDP) framework specific to the nuances of\nadvertising. Then, we develop a causal state encoder to capture dynamic user\ninterests and temporal dependencies, facilitating offline RL through\nconditional sequence modeling. Causal attention mechanisms are introduced to\nenhance user sequence representations by identifying correlations among causal\nstates. We employ multi-task learning to decode actions and rewards,\nsimultaneously addressing channel recommendation and budget allocation.\nNotably, our framework includes an automated system for integrating these tasks\ninto online advertising. Extensive experiments on offline and online\nenvironments demonstrate MTORL's superiority over state-of-the-art methods.", "published": "2025-06-29 05:05:13", "link": "http://arxiv.org/abs/2506.23090v1", "categories": ["cs.IR", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Enhancing Live Broadcast Engagement: A Multi-modal Approach to Short Video Recommendations Using MMGCN and User Preferences", "abstract": "The purpose of this paper is to explore a multi-modal approach to enhancing\nlive broadcast engagement by developing a short video recommendation system\nthat incorporates Multi-modal Graph Convolutional Networks (MMGCN) with user\npreferences. In order to provide personalized recommendations tailored to\nindividual interests, the proposed system takes into account user interaction\ndata, video content features, and contextual information. With the aid of a\nhybrid approach combining collaborative filtering and content-based filtering\ntechniques, the system is able to capture nuanced relationships between users,\nvideo attributes, and engagement patterns. Three datasets are used to evaluate\nthe effectiveness of the system: Kwai, TikTok, and MovieLens. Compared to\nbaseline models, such as DeepFM, Wide & Deep, LightGBM, and XGBoost, the\nproposed MMGCN-based model shows superior performance. A notable feature of the\nproposed model is that it outperforms all baseline methods in capturing diverse\nuser preferences and making accurate, personalized recommendations, resulting\nin a Kwai F1 score of 0.574, a Tiktok F1 score of 0.506, and a MovieLens F1\nscore of 0.197. We emphasize the importance of multi-modal integration and\nuser-centric approaches in advancing recommender systems, emphasizing the role\nthey play in enhancing content discovery and audience interaction on live\nbroadcast platforms.", "published": "2025-06-29 04:50:52", "link": "http://arxiv.org/abs/2506.23085v1", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR"}
{"title": "Synergizing Implicit and Explicit User Interests: A Multi-Embedding Retrieval Framework at Pinterest", "abstract": "Industrial recommendation systems are typically composed of multiple stages,\nincluding retrieval, ranking, and blending. The retrieval stage plays a\ncritical role in generating a high-recall set of candidate items that covers a\nwide range of diverse user interests. Effectively covering the diverse and\nlong-tail user interests within this stage poses a significant challenge:\ntraditional two-tower models struggle in this regard due to limited user-item\nfeature interaction and often bias towards top use cases. To address these\nissues, we propose a novel multi-embedding retrieval framework designed to\nenhance user interest representation by generating multiple user embeddings\nconditioned on both implicit and explicit user interests. Implicit interests\nare captured from user history through a Differentiable Clustering Module\n(DCM), whereas explicit interests, such as topics that the user has followed,\nare modeled via Conditional Retrieval (CR). These methodologies represent a\nform of conditioned user representation learning that involves condition\nrepresentation construction and associating the target item with the relevant\nconditions. Synergizing implicit and explicit user interests serves as a\ncomplementary approach to achieve more effective and comprehensive candidate\nretrieval as they benefit on different user segments and extract conditions\nfrom different but supplementary sources. Extensive experiments and A/B testing\nreveal significant improvements in user engagements and feed diversity metrics.\nOur proposed framework has been successfully deployed on Pinterest home feed.", "published": "2025-06-29 02:14:21", "link": "http://arxiv.org/abs/2506.23060v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Parallax QAMA: Novel Downlink Multiple Access for MISO Systems with Simple Receivers", "abstract": "In this paper, we propose a novel downlink multiple access system with a\nmulti-antenna transmitter and two single-antenna receivers, inspired by the\nunderlying principles of hierarchical quadrature amplitude modulation (H-QAM)\nbased multiple access (QAMA) and space-division multiple access (SDMA). In the\nproposed scheme, coded bits from two users are split and assigned to one shared\nsymbol and two private symbols carried by different beams. Based on joint\nsymbol mapping of H-QAM constellations and phase-aligned precoding at the\ntransmitter, each receiver observes a different H-QAM constellation with Gray\nmapping, a unique parallax feature not shared by existing schemes. In addition\nto avoiding successive interference cancellation (SIC), each user independently\ndemodulates its own bits on separate I and Q branches with calculations based\non closed-form expressions. Hence the receiver complexity is on par with that\nof orthogonal multiple access (OMA), which is much lower than that in other\ncompeting alternatives such as non-orthogonal multiple access (NOMA) and\nrate-splitting multiple access (RSMA). We carry out system optimization and\ndetermine the achievable rate region. Numerical results show that the proposed\nsystem has a larger rate region relative to other benchmark schemes with\nreceivers not using SIC, and even achieves a comparable rate region to those\nbenchmark schemes with SIC receivers.", "published": "2025-06-29 15:41:03", "link": "http://arxiv.org/abs/2506.23301v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Hybrid Character Sums From Vectorial Dual-Bent Functions and Asymptotically Optimal Complex Codebooks With Small Alphabet Sizes", "abstract": "Hybrid character sums are an important class of exponential sums which have\nnice applications in coding theory and sequence design. Let $\\gf_{p^m}$ be the\nfinite field with $p^m$ elements for a prime $p$ and a positive integer $m$.\nLet $V_n^{(p)}$ be an $n$-dimensional vector space over $\\gf_p$ for a prime\n$p$. In this paper, we study the hybrid character sums of the form\n\\begin{eqnarray*} \\sum_{x \\in V_n^{(p)}}\\psi\\left(F(x)\\right)\\chi_1\\left(a\nx\\right), \\end{eqnarray*} where $F$ is a function from $V_n^{(p)}$ to\n$\\gf_{p^m}$ and $a \\in V_n^{(p)}$, $\\psi$ is a nontrivial multiplicative\ncharacter of $\\gf_{p^m}$ and $\\chi_1$ is the canonical additive character of\n$V_n^{(p)}$. If $F(x)$ is a vectorial dual-bent function and $a \\in\nV_n^{(p)}\\setminus \\{0\\}$, we determine their complex modulus or explicit\nvalues under certain conditions, which generalizes some known results as\nspecial cases. It is concluded that the hybrid character sums from vectorial\ndual-bent functions have very small complex modulus. As applications, three\nfamilies of asymptotically optimal complex codebooks are constructed from\nvectorial dual-bent functions and their maximal cross-correlation amplitude are\ndetermined based on the hybrid character sums. The constructed codebooks have\nvery small alphabet sizes, which enhances their appeal for implementation.\nBesides, all of the three families of codebooks have only two-valued or\nthree-valued cross-correlation amplitudes.", "published": "2025-06-29 12:04:01", "link": "http://arxiv.org/abs/2506.23198v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "General Mathematical Proof of Occam's Razor; Upgrading Theoretical Physicists' Methodology", "abstract": "This paper's first aim is to prove a modernized Occam's razor beyond a\nreasonable doubt. To summarize the main argument in one sentence: If we\nconsider all possible, intelligible, scientific models of ever-higher\ncomplexity, democratically, the predictions most favored by these complex\nmodels will agree with the predictions of the simplest models. This fact can be\nproven mathematically, thereby validating Occam's razor. Major parts of this\nline of reasoning have long preexisted within the depths of the algorithmic\ninformation theory literature, but they have always left room for doubts of\nvarious kinds. Therefore, we increase the generality, completeness, clarity,\naccessibility, and credibility of these arguments by countering over a dozen\nobjections. We build our mathematical proof of Occam's razor on the shoulders\nof the exact 'chain rule' for Kolmogorov complexity.\n  Concerning physics, we then go on to diagnose the primary amendable root\ncause of the present stagnation of the research field of fundamental\ntheoretical physics. We show that the effective antidote would consist in a\npractically feasible upgrade to the theoretical physicists' research\nmethodology: When proposing new theoretical models, physicists should simply\ncalculate and report the total amount of information that their models consist\nof. We explain why this methodology would be highly effective as well as how\nthese calculations could be performed efficiently.", "published": "2025-06-29 11:43:11", "link": "http://arxiv.org/abs/2506.23194v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Linear Complementary Pairs of Algebraic Geometry Codes via Kummer Extensions", "abstract": "Due to their widespread applications, linear complementary pairs (LCPs) have\nattracted much attention in recent years. In this paper, we determine explicit\nconstruction of non-special divisors of degree $g$ and $g-1$ on Kummer\nextensions with specific properties. In addition, we present several methods\nfor constructing LCPs of algebraic geometry codes (AG Codes) via Kummer\nextensions. These results are applied in constructing explicit LCPs of AG Codes\nfrom subcovers of the BM curve, elliptic function fields, hyperelliptic\nfunction fields and other function fields. It is important to mention that we\nconstruct several families LCPs of MDS AG Codes from elliptic function fields\nand we obtain some linear complementary dual (LCD) codes from certain maximal\nelliptic function fields and hyperelliptic function fields.", "published": "2025-06-29 04:21:05", "link": "http://arxiv.org/abs/2506.23081v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Flexible Intelligent Metasurface for Enhancing Multi-Target Wireless Sensing", "abstract": "Flexible intelligent metasurface (FIM) has emerged as a transformative\ntechnology to enhance wireless sensing by dynamically morphing its\nthree-dimensional (3D) surface shape and electromagnetic response. Unlike\nconventional rigid arrays, an FIM consists of low-cost radiating elements that\ncan independently adjust their positions and radiation characteristics, thereby\nallowing for real-time optimization of the sensing environment. This paper\ninvestigates the impact of FIM on wireless sensing performance. Specifically,\nwe focus on the maximization of the cumulated power of the probing signals at\nthe target locations under the per-antenna power constraint by jointly\noptimizing the transmit covariance matrix and the surface shape of the\ntransmitting FIM. We propose a block coordinate descend (BCD) algorithm to find\na locally optimal solution, by alternatively updating the FIM surface shape and\nthe transmit covariance matrix, while keeping the other one fixed at each step.\nFurthermore, we analyze the computational complexity and convergence properties\nof the proposed algorithm and demonstrate that FIM enhances wireless sensing by\nproviding a new design degree-of-freedom to coordinate the correlation between\nsteering vectors at different angles. Numerical results demonstrate that FIM\nsignificantly improves wireless sensing performance under the considered\nmulti-target scenario.", "published": "2025-06-29 01:35:34", "link": "http://arxiv.org/abs/2506.23052v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Zak-OFDM: Low Complexity Joint Equalization of OFDM Carriers in Doubly-Spread Channels", "abstract": "We communicate over wireless channels by first estimating and then equalizing\nthe effective channel. In Zak-OTFS (orthogonal time frequency space) modulation\nthe carrier waveform is a pulse in the delay-Doppler (DD) domain, formally a\nquasi-periodic localized function with specific periods along delay and\nDoppler. When the channel delay spread is less than the delay period, and the\nchannel Doppler spread is less than the Doppler period, the response to a\nsingle Zak-OTFS carrier provides an image of the scattering environment and can\nbe used to predict the effective channel at all other carriers. This makes\nchannel estimation straightforward, and there is no loss in spectral efficiency\nsince it is possible to design data and pilot signals that are mutually\nunbiased. However, the naive approach to equalization has complexity ${\\mathcal\nO}(M^3N^3)$ where $M$ and $N$ are respectively the number of delay and Doppler\nbins in an OTFS frame. We simplify equalization by transforming Zak-OTFS\ninformation symbols to CP-OFDM (cyclic prefix orthogonal frequency division\nmultiplexing) modulation.\n  Why not simply communicate with CP-OFDM? Inter-carrier interference (ICI) in\nCP-OFDM makes it is very challenging to acquire the complete frequency domain\n(FD) channel response between subcarriers in the presence of mobility and delay\nspread. We avoid this difficulty by estimating the effective channel in the DD\ndomain from which we are able to reconstruct the complete FD channel response.\nWe take advantage of CP-OFDM to design an ${\\mathcal O}(M^2N^2)$ low-complexity\nmethod of jointly equalizing all subcarriers, where $MN$ is the number of\nsubcarriers. Our approach removes the need for traditional pilots in CP-OFDM\nand reduces the need to vary carrier spacing with mobility.", "published": "2025-06-29 00:51:43", "link": "http://arxiv.org/abs/2506.23045v1", "categories": ["eess.SP", "cs.IT", "math.IT"], "primary_category": "eess.SP"}
{"title": "DPOT: A DeepParticle method for Computation of Optimal Transport with convergence guarantee", "abstract": "In this work, we propose a novel machine learning approach to compute the\noptimal transport map between two continuous distributions from their unpaired\nsamples, based on the DeepParticle methods. The proposed method leads to a\nmin-min optimization during training and does not impose any restriction on the\nnetwork structure. Theoretically we establish a weak convergence guarantee and\na quantitative error bound between the learned map and the optimal transport\nmap. Our numerical experiments validate the theoretical results and the\neffectiveness of the new approach, particularly on real-world tasks.", "published": "2025-06-29 23:32:39", "link": "http://arxiv.org/abs/2506.23429v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Detecting What Matters: A Novel Approach for Out-of-Distribution 3D Object Detection in Autonomous Vehicles", "abstract": "Autonomous vehicles (AVs) use object detection models to recognize their\nsurroundings and make driving decisions accordingly. Conventional object\ndetection approaches classify objects into known classes, which limits the AV's\nability to detect and appropriately respond to Out-of-Distribution (OOD)\nobjects. This problem is a significant safety concern since the AV may fail to\ndetect objects or misclassify them, which can potentially lead to hazardous\nsituations such as accidents. Consequently, we propose a novel object detection\napproach that shifts the emphasis from conventional class-based classification\nto object harmfulness determination. Instead of object detection by their\nspecific class, our method identifies them as either 'harmful' or 'harmless'\nbased on whether they pose a danger to the AV. This is done based on the object\nposition relative to the AV and its trajectory. With this metric, our model can\neffectively detect previously unseen objects to enable the AV to make safer\nreal-time decisions. Our results demonstrate that the proposed model\neffectively detects OOD objects, evaluates their harmfulness, and classifies\nthem accordingly, thus enhancing the AV decision-making effectiveness in\ndynamic environments.", "published": "2025-06-29 23:21:05", "link": "http://arxiv.org/abs/2506.23426v1", "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Do LLMs Dream of Discrete Algorithms?", "abstract": "Large Language Models (LLMs) have rapidly transformed the landscape of\nartificial intelligence, enabling natural language interfaces and dynamic\norchestration of software components. However, their reliance on probabilistic\ninference limits their effectiveness in domains requiring strict logical\nreasoning, discrete decision-making, and robust interpretability. This paper\ninvestigates these limitations and proposes a neurosymbolic approach that\naugments LLMs with logic-based reasoning modules, particularly leveraging\nProlog predicates and composable toolsets. By integrating first-order logic and\nexplicit rule systems, our framework enables LLMs to decompose complex queries\ninto verifiable sub-tasks, orchestrate reliable solutions, and mitigate common\nfailure modes such as hallucination and incorrect step decomposition. We\ndemonstrate the practical benefits of this hybrid architecture through\nexperiments on the DABStep benchmark, showing improved precision, coverage, and\nsystem documentation in multi-step reasoning tasks. Our results indicate that\ncombining LLMs with modular logic reasoning restores engineering rigor,\nenhances system reliability, and offers a scalable path toward trustworthy,\ninterpretable AI agents across complex domains.", "published": "2025-06-29 22:03:01", "link": "http://arxiv.org/abs/2506.23408v1", "categories": ["cs.LG", "cs.LO"], "primary_category": "cs.LG"}
{"title": "AICO: Feature Significance Tests for Supervised Learning", "abstract": "The opacity of many supervised learning algorithms remains a key challenge,\nhindering scientific discovery and limiting broader deployment -- particularly\nin high-stakes domains. This paper develops model- and distribution-agnostic\nsignificance tests to assess the influence of input features in any regression\nor classification algorithm. Our method evaluates a feature's incremental\ncontribution to model performance by masking its values across samples. Under\nthe null hypothesis, the distribution of performance differences across a test\nset has a non-positive median. We construct a uniformly most powerful,\nrandomized sign test for this median, yielding exact p-values for assessing\nfeature significance and confidence intervals with exact coverage for\nestimating population-level feature importance. The approach requires minimal\nassumptions, avoids model retraining or auxiliary models, and remains\ncomputationally efficient even for large-scale, high-dimensional settings.\nExperiments on synthetic tasks validate its statistical and computational\nadvantages, and applications to real-world data illustrate its practical\nutility.", "published": "2025-06-29 21:15:40", "link": "http://arxiv.org/abs/2506.23396v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "SIEDD: Shared-Implicit Encoder with Discrete Decoders", "abstract": "Implicit Neural Representations (INRs) offer exceptional fidelity for video\ncompression by learning per-video optimized functions, but their adoption is\ncrippled by impractically slow encoding times. Existing attempts to accelerate\nINR encoding often sacrifice reconstruction quality or crucial coordinate-level\ncontrol essential for adaptive streaming and transcoding. We introduce SIEDD\n(Shared-Implicit Encoder with Discrete Decoders), a novel architecture that\nfundamentally accelerates INR encoding without these compromises. SIEDD first\nrapidly trains a shared, coordinate-based encoder on sparse anchor frames to\nefficiently capture global, low-frequency video features. This encoder is then\nfrozen, enabling massively parallel training of lightweight, discrete decoders\nfor individual frame groups, further expedited by aggressive coordinate-space\nsampling. This synergistic design delivers a remarkable 20-30X encoding\nspeed-up over state-of-the-art INR codecs on HD and 4K benchmarks, while\nmaintaining competitive reconstruction quality and compression ratios.\nCritically, SIEDD retains full coordinate-based control, enabling continuous\nresolution decoding and eliminating costly transcoding. Our approach\nsignificantly advances the practicality of high-fidelity neural video\ncompression, demonstrating a scalable and efficient path towards real-world\ndeployment. Our codebase is available at\nhttps://github.com/VikramRangarajan/SIEDD .", "published": "2025-06-29 19:39:43", "link": "http://arxiv.org/abs/2506.23382v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "cs.CV"}
{"title": "When Additive Noise Meets Unobserved Mediators: Bivariate Denoising Diffusion for Causal Discovery", "abstract": "Distinguishing cause and effect from bivariate observational data is a\nfoundational problem in many disciplines, but challenging without additional\nassumptions. Additive noise models (ANMs) are widely used to enable\nsample-efficient bivariate causal discovery. However, conventional ANM-based\nmethods fail when unobserved mediators corrupt the causal relationship between\nvariables. This paper makes three key contributions: first, we rigorously\ncharacterize why standard ANM approaches break down in the presence of\nunmeasured mediators. Second, we demonstrate that prior solutions for hidden\nmediation are brittle in finite sample settings, limiting their practical\nutility. To address these gaps, we propose Bivariate Denoising Diffusion (BiDD)\nfor causal discovery, a method designed to handle latent noise introduced by\nunmeasured mediators. Unlike prior methods that infer directionality through\nmean squared error loss comparisons, our approach introduces a novel\nindependence test statistic: during the noising and denoising processes for\neach variable, we condition on the other variable as input and evaluate the\nindependence of the predicted noise relative to this input. We prove asymptotic\nconsistency of BiDD under the ANM, and conjecture that it performs well under\nhidden mediation. Experiments on synthetic and real-world data demonstrate\nconsistent performance, outperforming existing methods in mediator-corrupted\nsettings while maintaining strong performance in mediator-free settings.", "published": "2025-06-29 19:20:41", "link": "http://arxiv.org/abs/2506.23374v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Investigating an Overfitting and Degeneration Phenomenon in Self-Supervised Multi-Pitch Estimation", "abstract": "Multi-Pitch Estimation (MPE) continues to be a sought after capability of\nMusic Information Retrieval (MIR) systems, and is critical for many\napplications and downstream tasks involving pitch, including music\ntranscription. However, existing methods are largely based on supervised\nlearning, and there are significant challenges in collecting annotated data for\nthe task. Recently, self-supervised techniques exploiting intrinsic properties\nof pitch and harmonic signals have shown promise for both monophonic and\npolyphonic pitch estimation, but these still remain inferior to supervised\nmethods. In this work, we extend the classic supervised MPE paradigm by\nincorporating several self-supervised objectives based on pitch-invariant and\npitch-equivariant properties. This joint training results in a substantial\nimprovement under closed training conditions, which naturally suggests that\napplying the same objectives to a broader collection of data will yield further\nimprovements. However, in doing so we uncover a phenomenon whereby our model\nsimultaneously overfits to the supervised data while degenerating on data used\nfor self-supervision only. We demonstrate and investigate this and offer our\ninsights on the underlying problem.", "published": "2025-06-29 19:10:51", "link": "http://arxiv.org/abs/2506.23371v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Federated Timeline Synthesis: Scalable and Private Methodology For Model Training and Deployment", "abstract": "We present Federated Timeline Synthesis (FTS), a novel framework for training\ngenerative foundation models across distributed timeseries data applied to\nelectronic health records (EHR). At its core, FTS represents patient history as\ntokenized Patient Health Timelines (PHTs), language-agnostic sequences encoding\ntemporal, categorical, and continuous clinical information. Each institution\ntrains an autoregressive transformer on its local PHTs and transmits only model\nweights to a central server. The server uses the generators to synthesize a\nlarge corpus of trajectories and train a Global Generator (GG), enabling\nzero-shot inference via Monte Carlo simulation of future PHTs. We evaluate FTS\non five clinically meaningful prediction tasks using MIMIC-IV data, showing\nthat models trained on synthetic data generated by GG perform comparably to\nthose trained on real data. FTS offers strong privacy guarantees, scalability\nacross institutions, and extensibility to diverse prediction and simulation\ntasks especially in healthcare, including counterfactual inference, early\nwarning detection, and synthetic trial design.", "published": "2025-06-29 18:29:59", "link": "http://arxiv.org/abs/2506.23358v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Benchmarking Generalizable Bimanual Manipulation: RoboTwin Dual-Arm Collaboration Challenge at CVPR 2025 MEIS Workshop", "abstract": "Embodied Artificial Intelligence (Embodied AI) is an emerging frontier in\nrobotics, driven by the need for autonomous systems that can perceive, reason,\nand act in complex physical environments. While single-arm systems have shown\nstrong task performance, collaborative dual-arm systems are essential for\nhandling more intricate tasks involving rigid, deformable, and\ntactile-sensitive objects. To advance this goal, we launched the RoboTwin\nDual-Arm Collaboration Challenge at the 2nd MEIS Workshop, CVPR 2025. Built on\nthe RoboTwin Simulation platform (1.0 and 2.0) and the AgileX COBOT-Magic Robot\nplatform, the competition consisted of three stages: Simulation Round 1,\nSimulation Round 2, and a final Real-World Round. Participants totally tackled\n17 dual-arm manipulation tasks, covering rigid, deformable, and tactile-based\nscenarios. The challenge attracted 64 global teams and over 400 participants,\nproducing top-performing solutions like SEM and AnchorDP3 and generating\nvaluable insights into generalizable bimanual policy learning. This report\noutlines the competition setup, task design, evaluation methodology, key\nfindings and future direction, aiming to support future research on robust and\ngeneralizable bimanual manipulation policies. The Challenge Webpage is\navailable at https://robotwin-benchmark.github.io/cvpr-2025-challenge/.", "published": "2025-06-29 17:56:41", "link": "http://arxiv.org/abs/2506.23351v1", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA"], "primary_category": "cs.RO"}
{"title": "Research on Comprehensive Classroom Evaluation System Based on Multiple AI Models", "abstract": "The promotion of the national education digitalization strategy has\nfacilitated the development of teaching quality evaluation towards all-round,\nprocess-oriented, precise, and intelligent directions, inspiring explorations\ninto new methods and technologies for educational quality assurance. Classroom\nteaching evaluation methods dominated by teaching supervision and student\nteaching evaluation suffer from issues such as low efficiency, strong\nsubjectivity, and limited evaluation dimensions. How to further advance\nintelligent and objective evaluation remains a topic to be explored. This\npaper, based on image recognition technology, speech recognition technology,\nand AI large language models, develops a comprehensive evaluation system that\nautomatically generates evaluation reports and optimization suggestions from\ntwo dimensions: teacher teaching ability and classroom teaching effectiveness.\nThis study establishes a closed-loop classroom evaluation model that\ncomprehensively evaluates student and teaching conditions based on\nmulti-dimensional data throughout the classroom teaching process, and further\nanalyzes the data to guide teaching improvement. It meets the requirements of\nall-round and process-oriented classroom evaluation in the era of digital\neducation, effectively solves the main problems of manual evaluation methods,\nand provides data collection and analysis methods as well as technologies for\nrelevant research on educational teaching evaluation.", "published": "2025-06-29 04:06:55", "link": "http://arxiv.org/abs/2506.23079v1", "categories": ["cs.CY", "cs.MA"], "primary_category": "cs.CY"}
{"title": "A new family of a posteriori error estimates for non-conforming finite element methods leading to stabilization-free error bounds", "abstract": "We propose new a posteriori error estimators for non-conforming finite\nelement discretizations of second-order elliptic PDE problems. These estimators\nare based on novel reformulations of the standard Prager-Synge identity, and\nenable to prove efficiency estimates without extra stabilization terms in the\nerror measure for a large class of discretization schemes. We propose a\nresidual-based estimator for which the efficiency constant scales optimally in\npolynomial degree, as well as two equilibrated estimators that are\npolynomial-degree-robust. One of the two estimators further leads to guaranteed\nerror bounds.", "published": "2025-06-29 19:38:35", "link": "http://arxiv.org/abs/2506.23381v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Data-Driven Self-Supervised Learning for the Discovery of Solution Singularity for Partial Differential Equations", "abstract": "The appearance of singularities in the function of interest constitutes a\nfundamental challenge in scientific computing. It can significantly undermine\nthe effectiveness of numerical schemes for function approximation, numerical\nintegration, and the solution of partial differential equations (PDEs), etc.\nThe problem becomes more sophisticated if the location of the singularity is\nunknown, which is often encountered in solving PDEs. Detecting the singularity\nis therefore critical for developing efficient adaptive methods to reduce\ncomputational costs in various applications. In this paper, we consider\nsingularity detection in a purely data-driven setting. Namely, the input only\ncontains given data, such as the vertex set from a mesh. To overcome the\nlimitation of the raw unlabeled data, we propose a self-supervised learning\n(SSL) framework for estimating the location of the singularity. A key component\nis a filtering procedure as the pretext task in SSL, where two filtering\nmethods are presented, based on $k$ nearest neighbors and kernel density\nestimation, respectively. We provide numerical examples to illustrate the\npotential pathological or inaccurate results due to the use of raw data without\nfiltering. Various experiments are presented to demonstrate the ability of the\nproposed approach to deal with input perturbation, label corruption, and\ndifferent kinds of singularities such interior circle, boundary layer,\nconcentric semicircles, etc.", "published": "2025-06-29 17:39:41", "link": "http://arxiv.org/abs/2506.23344v1", "categories": ["math.NA", "cs.LG", "cs.NA", "stat.ML"], "primary_category": "math.NA"}
{"title": "An \\textsf{AT1} phase-field framework for quasi-static anti-plane shear fracture: Unifying $\u03be$-based adaptivity and nonlinear strain energy density function", "abstract": "This work introduces a novel \\textsf{AT1} phase-field framework for\nsimulating quasi-static anti-plane shear fracture in geometrically linear\nelastic bodies. A key feature of this framework is the unification of\n$\\xi$-based local mesh adaptivity -- where $\\xi$ represents the characteristic\nlength of the damage zone -- and an algebraically nonlinear strain energy\ndensity function. A modified Francfort-Marigo energy functional, together with\nits Ambrosio-Tortorelli-type regularization, is hereby proposed to address\nchallenges within the framework of nonlinearly constituted materials. We\ndynamically optimize $\\xi$ throughout the simulation, significantly enhancing\nthe computational efficiency and accuracy of numerically approximating the\nlocal minimizers of the Ambrosio-Tortorelli (\\textsf{AT1})-type phase-field\nmodel. The proposed regularization for the total energy functional comprises\nthree distinct components: a nonlinear strain energy, an evolving surface\nenergy, and a linear-type regularization term dependent on the length scale of\nthe damage zone. Variational principles applied to this novel energy functional\nyield a coupled system of governing second-order quasilinear partial\ndifferential equations for the mechanics and phase-field variables. These\nequations are subsequently discretized using the conforming bilinear finite\nelement method. The formulation is underpinned by four crucial parameters: two\nare integral to the nonlinear strain energy function, while the other two serve\nas penalty parameters. These penalty parameters are asymptotically calibrated\nand rigorously utilized in the numerical simulations. Our results demonstrate\nthat this spatially adaptive approach leads to enhanced mesh adaptivity,\nensuring the robust convergence of the numerical solution.", "published": "2025-06-29 14:13:27", "link": "http://arxiv.org/abs/2506.23249v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "A residual driven multiscale method for Darcy's flow in perforated domains", "abstract": "In this paper, we present a residual-driven multiscale method for simulating\nDarcy flow in perforated domains, where complex geometries and highly\nheterogeneous permeability make direct simulations computationally expensive.\nTo address this, we introduce a velocity elimination technique that\nreformulates the mixed velocity-pressure system into a pressure-only\nformulation, significantly reducing complexity by focusing on the dominant\npressure variable. Our method is developed within the Generalized Multiscale\nFinite Element Method (GMsFEM) framework. For each coarse block, we construct\noffline basis functions from local spectral problems that capture key geometric\nand physical features. Online basis functions are then adaptively enriched\nusing residuals, allowing the method to incorporate global effects such as\nsource terms and boundary conditions, thereby improving accuracy. We provide\ndetailed error analysis demonstrating how the offline and online spaces\ncontribute to the accuracy and efficiency of the solution. Numerical\nexperiments confirm the method's effectiveness, showing substantial reductions\nin computational cost while maintaining high accuracy, particularly through\nadaptive online enrichment. These results highlight the method's potential for\nefficient and accurate simulation of Darcy flow in complex, heterogeneous\nperforated domains.", "published": "2025-06-29 05:14:38", "link": "http://arxiv.org/abs/2506.23093v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "PML method for the time-domain stochastic acoustic wave equation and an inverse source problem", "abstract": "In this paper, we develop and analyze a time-domain perfectly matched layer\n(PML) method for the stochastic acoustic wave equation driven by spatially\nwhite additive Gaussian noise. We begin by establishing the well-posedness and\nstability of the direct problem through a rigorous analysis of the associated\ntime-harmonic stochastic Helmholtz equation and the application of an abstract\nLaplace transform inversion theorem. To address the low regularity of the\nrandom source, we employ scattering theory to investigate the meromorphic\ncontinuation of the Helmholtz resolvent defined on rough fields. Based on a\npiecewise constant approximation of the white noise, we construct an\napproximate wave solution and formulate a time-domain PML method. The\nconvergence of the PML method is established, with explicit dependence on the\nPML layer's thickness and medium properties, as well as the piecewise constant\napproximation of the white noise. In addition, we propose a frequency-domain\napproach for solving the inverse random source problem using time-domain\nboundary measurements. A logarithmic stability estimate is derived,\nhighlighting the ill-posedness of the inverse problem and offering guidance for\nthe design of effective numerical schemes.", "published": "2025-06-29 04:45:35", "link": "http://arxiv.org/abs/2506.23084v1", "categories": ["math.NA", "cs.NA", "35B35, 35R60, 78A46"], "primary_category": "math.NA"}
{"title": "Shifted Composition IV: Underdamped Langevin and Numerical Discretizations with Partial Acceleration", "abstract": "Quantifying the convergence rate of the underdamped Langevin dynamics (ULD)\nis a classical topic, in large part due to the possibility for\ndiffusive-to-ballistic speedups -- as was recently established for the\ncontinuous-time dynamics via space-time Poincare inequalities. A central\nchallenge for analyzing ULD is that its degeneracy necessitates the development\nof new analysis approaches, e.g., the theory of hypocoercivity. In this paper,\nwe give a new coupling-based framework for analyzing ULD and its numerical\ndiscretizations. First, in the continuous-time setting, we use this framework\nto establish new parabolic Harnack inequalities for ULD. These are the first\nHarnack inequalities that decay to zero in contractive settings, thereby\nreflecting the convergence properties of ULD in addition to just its regularity\nproperties.\n  Second, we build upon these Harnack inequalities to develop a local error\nframework for analyzing discretizations of ULD in KL divergence. This extends\nour framework in part III from uniformly elliptic diffusions to degenerate\ndiffusions, and shares its virtues: the framework is user-friendly, applies to\nsophisticated discretization schemes, and does not require contractivity.\nApplying this framework to the randomized midpoint discretization of ULD\nestablishes (i) the first ballistic acceleration result for log-concave\nsampling (i.e., sublinear dependence on the condition number), and (ii) the\nfirst $d^{1/3}$ iteration complexity guarantee for sampling to constant total\nvariation error in dimension $d$.", "published": "2025-06-29 02:22:04", "link": "http://arxiv.org/abs/2506.23062v1", "categories": ["math.PR", "cs.DS", "cs.NA", "math.AP", "math.NA", "math.ST", "stat.TH"], "primary_category": "math.PR"}
{"title": "Not All Explanations for Deep Learning Phenomena Are Equally Valuable", "abstract": "Developing a better understanding of surprising or counterintuitive phenomena\nhas constituted a significant portion of deep learning research in recent\nyears. These include double descent, grokking, and the lottery ticket\nhypothesis -- among many others. Works in this area often develop ad hoc\nhypotheses attempting to explain these observed phenomena on an isolated,\ncase-by-case basis. This position paper asserts that, in many prominent cases,\nthere is little evidence to suggest that these phenomena appear in real-world\napplications and these efforts may be inefficient in driving progress in the\nbroader field. Consequently, we argue against viewing them as isolated puzzles\nthat require bespoke resolutions or explanations. However, despite this, we\nsuggest that deep learning phenomena do still offer research value by providing\nunique settings in which we can refine our broad explanatory theories of more\ngeneral deep learning principles. This position is reinforced by analyzing the\nresearch outcomes of several prominent examples of these phenomena from the\nrecent literature. We revisit the current norms in the research community in\napproaching these problems and propose practical recommendations for future\nresearch, aiming to ensure that progress on deep learning phenomena is well\naligned with the ultimate pragmatic goal of progress in the broader field of\ndeep learning.", "published": "2025-06-29 15:18:56", "link": "http://arxiv.org/abs/2506.23286v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "XY-Tokenizer: Mitigating the Semantic-Acoustic Conflict in Low-Bitrate Speech Codecs", "abstract": "Speech codecs serve as bridges between speech signals and large language\nmodels. An ideal codec for speech language models should not only preserve\nacoustic information but also capture rich semantic information. However,\nexisting speech codecs struggle to balance high-quality audio reconstruction\nwith ease of modeling by language models. In this study, we analyze the\nlimitations of previous codecs in balancing semantic richness and acoustic\nfidelity. We propose XY-Tokenizer, a novel codec that mitigates the conflict\nbetween semantic and acoustic capabilities through multi-stage, multi-task\nlearning. Experimental results demonstrate that XY-Tokenizer achieves\nperformance in both semantic and acoustic tasks comparable to that of\nstate-of-the-art codecs operating at similar bitrates, even though those\nexisting codecs typically excel in only one aspect. Specifically, XY-Tokenizer\nachieves strong text alignment, surpassing distillation-based semantic modeling\nmethods such as SpeechTokenizer and Mimi, while maintaining a speaker\nsimilarity score of 0.83 between reconstructed and original audio. The\nreconstruction performance of XY-Tokenizer is comparable to that of BigCodec,\nthe current state-of-the-art among acoustic-only codecs, which achieves a\nspeaker similarity score of 0.84 at a similar bitrate. Code and models are\navailable at https://github.com/gyt1145028706/XY-Tokenizer.", "published": "2025-06-29 16:51:50", "link": "http://arxiv.org/abs/2506.23325v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The Florence Price Art Song Dataset and Piano Accompaniment Generator", "abstract": "Florence B. Price was a composer in the early 20th century whose music\nreflects her upbringing in the American South, her African heritage, and her\nWestern classical training. She is noted as the first African-American woman to\nhave a symphony performed by a major orchestra. Her music has recently received\nrenewed attention from both the public and the research community, decades\nafter her death. In addition to other genres, Price was a prolific composer for\nsolo voice and piano. Music historians have documented the existence of 134 art\nsongs and piano/voice arrangements for spirituals and folk songs written by\nPrice. We release a digital catalog of 112 of these works in MuseScore,\nMusicXML, MIDI, and PDF format. We also use this dataset to fine-tune a\nsymbolic music generation model to generate accompaniments to melodies, and we\nconduct a blind listening experiment that shows that accompaniments generated\nby our model are perceived as being reflective of Florence Price's style more\nfrequently than accompaniments generated by a baseline model. We release our\nmodel as the Florence Price Piano Accompaniment Generator alongside our\ndataset.", "published": "2025-06-29 07:45:26", "link": "http://arxiv.org/abs/2506.23130v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "TOMI: Transforming and Organizing Music Ideas for Multi-Track Compositions with Full-Song Structure", "abstract": "Hierarchical planning is a powerful approach to model long sequences\nstructurally. Aside from considering hierarchies in the temporal structure of\nmusic, this paper explores an even more important aspect: concept hierarchy,\nwhich involves generating music ideas, transforming them, and ultimately\norganizing them--across musical time and space--into a complete composition. To\nthis end, we introduce TOMI (Transforming and Organizing Music Ideas) as a\nnovel approach in deep music generation and develop a TOMI-based model via\ninstruction-tuned foundation LLM. Formally, we represent a multi-track\ncomposition process via a sparse, four-dimensional space characterized by clips\n(short audio or MIDI segments), sections (temporal positions), tracks\n(instrument layers), and transformations (elaboration methods). Our model is\ncapable of generating multi-track electronic music with full-song structure,\nand we further integrate the TOMI-based model with the REAPER digital audio\nworkstation, enabling interactive human-AI co-creation. Experimental results\ndemonstrate that our approach produces higher-quality electronic music with\nstronger structural coherence compared to baselines.", "published": "2025-06-29 05:15:41", "link": "http://arxiv.org/abs/2506.23094v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "All-Optical Inter-Satellite Relays with Intelligent Beam Control: Harnessing Liquid Lenses and Optical Hard Limiters", "abstract": "Low Earth orbit (LEO) satellite constellations are emerging as a key enabler\nof next-generation communications, offering global coverage and significantly\nlower latency compared to traditional terrestrial networks and geostationary\nsatellites. However, further latency reduction is essential for time-critical\napplications such as real-time sensing, autonomous systems, and interactive\nservices. One critical bottleneck is the optical-to-electrical (O/E) and\nelectrical-to-optical (E/O) conversions at intermediate nodes in multi-hop\nlinks, which introduce unwanted processing delays. To address this, we\ninvestigate an all-optical relay system based on Optical Hard Limiters (OHL),\nwhich operate purely in the optical domain to suppress noise and restore signal\nquality without requiring O/E conversions. First, we present a rigorous\nanalysis of inter-satellite multi-relay communication under the OHL relaying\narchitecture, comparing it against conventional Amplify-and-Forward (AF) and\nDecode-and-Forward (DF) schemes. Through this comparison, we highlight both the\nadvantages and limitations of OHL relays, including their particular\nsensitivity to parameter choices such as the threshold setting and divergence\nangle at the transmitter. Recognizing that a LEO constellation is inherently\ntime-varying - satellites move relative to one another, causing continuous\nchanges in link distances and tracking errors - we propose a joint optimization\nstrategy. This scheme adaptively tunes the OHL decision threshold and beam\ndivergence in real time to maintain optimal performance, ultimately lowering\nerror rates and latency. Extensive simulations in a large-scale LEO network\ndemonstrate the viability of our method and offer insights into practical\nimplementation for next-generation inter-satellite communication systems.", "published": "2025-06-29 23:53:33", "link": "http://arxiv.org/abs/2506.23432v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Integrated Polarimetric Sensing and Communication with Polarization-Reconfigurable Arrays", "abstract": "Polarization diversity offers a cost- and space-efficient solution to enhance\nthe performance of integrated sensing and communication systems. Polarimetric\nsensing exploits the signal's polarity to extract details about the target such\nas shape, pose, and material composition. From a communication perspective,\npolarization diversity can enhance the reliability and throughput of\ncommunication channels. This paper proposes an integrated polarimetric sensing\nand communication (IPSAC) system that jointly conducts polarimetric sensing and\ncommunications. We study the use of single-port polarization-reconfigurable\nantennas to adapt to channel depolarization effects, without the need for\nseparate RF chains for each polarization. We address the problem of optimizing\nwaveforms and polarizations based on two sensing metrics. We first consider\nminimizing the mean square error (MSE) of the target depolarization parameter\nestimate, which is a critical task for various polarimetric radar applications\nsuch as rainfall forecasting, vegetation identification, and target\nclassification. To address this nonconvex problem, we apply semi-definite\nrelaxation (SDR) and majorization-minimization (MM) optimization techniques.\nNext, we consider a design that maximizes the target\nsignal-to-interference-plus-noise ratio (SINR) leveraging prior knowledge of\nthe target and clutter depolarization statistics to enhance the target\ndetection performance. To tackle this problem, we modify the solution developed\nfor MSE minimization subject to the same quality-of-service (QoS) constraints.\nExtensive simulations show that the proposed polarization reconfiguration\nmethod substantially improves the depolarization parameter MSE. Furthermore,\nthe proposed method considerably boosts the target SINR due to polarization\ndiversity, particularly in cluttered environments.", "published": "2025-06-29 22:10:54", "link": "http://arxiv.org/abs/2506.23410v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Optimizing Solar Energy Production in the USA: Time-Series Analysis Using AI for Smart Energy Management", "abstract": "As the US rapidly moves towards cleaner energy sources, solar energy is fast\nbecoming the pillar of its renewable energy mix. Even while solar energy is\nincreasingly being used, its variability is a key hindrance to grid stability,\nstorage efficiency, and system stability overall. Solar energy has emerged as\none of the fastest-growing renewable energy sources in the United States,\nadding noticeably to the country's energy mix. Retrospectively, the necessity\nof inserting the sun's energy into the grid without disrupting reliability and\ncost efficiencies highlights the necessity of good forecasting software and\nsmart control systems. The dataset utilized for this research project comprised\nboth hourly and daily solar energy production records collected from multiple\nutility-scale solar farms across diverse U.S. regions, including California,\nTexas, and Arizona. Training and evaluation of all models were performed with a\ntime-based cross-validation scheme, namely, sliding window validation. Both the\nRandom Forest and the XG-Boost models demonstrated noticeably greater and the\nsame performance across each of the measures considered, with relatively high\naccuracy. The almost perfect and equal performance by the Random Forest and\nXG-Boost models also shows both models to have learned the patterns in the data\nvery comprehensively, with high reliability in their predictions. By\nincorporating AI-powered time-series models like XG-Boost in grid management\nsoftware, utility companies can dynamically modify storage cycles in real-time\nas well as dispatch and peak load planning, based on their predictions.\nAI-powered solar forecasting also has profound implications for renewable\nenergy policy and planning, particularly as U.S. federal and state governments\naccelerate toward ambitious decarbonization goals.", "published": "2025-06-29 19:03:17", "link": "http://arxiv.org/abs/2506.23368v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Nuisance parameters and elliptically symmetric distributions: a geometric approach to parametric and semiparametric efficiency", "abstract": "Elliptically symmetric distributions are a classic example of a\nsemiparametric model where the location vector and the scatter matrix (or a\nparameterization of them) are the two finite-dimensional parameters of\ninterest, while the density generator represents an\n\\textit{infinite-dimensional nuisance} term. This basic representation of the\nelliptic model can be made more accurate, rich, and flexible by considering\nadditional \\textit{finite-dimensional nuisance} parameters. Our aim is\ntherefore to investigate the deep and counter-intuitive links between\nstatistical efficiency in estimating the parameters of interest in the presence\nof both finite and infinite-dimensional nuisance parameters. Unlike previous\nworks that addressed this problem using Le Cam's asymptotic theory, our\napproach here is purely geometric: efficiency will be analyzed using tools such\nas projections and tangent spaces embedded in the relevant Hilbert space. This\nallows us to obtain original results also for the case where the location\nvector and the scatter matrix are parameterized by a finite-dimensional vector\nthat can be partitioned in two sub-vectors: one containing the parameters of\ninterest and the other containing the nuisance parameters. As an example, we\nillustrate how the obtained results can be applied to the well-known\n\\virg{low-rank} parameterization. Furthermore, while the theoretical analysis\nwill be developed for Real Elliptically Symmetric (RES) distributions, we show\nhow to extend our results to the case of Circular and Non-Circular Complex\nElliptically Symmetric (C-CES and NC-CES) distributions.", "published": "2025-06-29 12:50:21", "link": "http://arxiv.org/abs/2506.23213v1", "categories": ["math.ST", "eess.SP", "stat.TH"], "primary_category": "math.ST"}
{"title": "Multi-Branch DNN and CRLB-Ratio-Weight Fusion for Enhanced DOA Sensing via a Massive H$^2$AD MIMO Receiver", "abstract": "As a green MIMO structure, massive H$^2$AD is viewed as a potential\ntechnology for the future 6G wireless network. For such a structure, it is a\nchallenging task to design a low-complexity and high-performance fusion of\ntarget direction values sensed by different sub-array groups with fewer use of\nprior knowledge. To address this issue, a lightweight Cramer-Rao lower bound\n(CRLB)-ratio-weight fusion (WF) method is proposed, which approximates inverse\nCRLB of each subarray using antenna number reciprocals to eliminate real-time\nCRLB computation. This reduces complexity and prior knowledge dependence while\npreserving fusion performance. Moreover, a multi-branch deep neural network\n(MBDNN) is constructed to further enhance direction-of-arrival (DOA) sensing by\nleveraging candidate angles from multiple subarrays. The subarray-specific\nbranch networks are integrated with a shared regression module to effectively\neliminate pseudo-solutions and fuse true angles. Simulation results show that\nthe proposed CRLB-ratio-WF method achieves DOA sensing performance comparable\nto CRLB-based methods, while significantly reducing the reliance on prior\nknowledge. More notably, the proposed MBDNN has superior performance in low-SNR\nranges. At SNR $= -15$ dB, it achieves an order-of-magnitude improvement in\nestimation accuracy compared to CRLB-ratio-WF method.", "published": "2025-06-29 12:14:59", "link": "http://arxiv.org/abs/2506.23203v1", "categories": ["eess.SP", "cs.AI"], "primary_category": "eess.SP"}
{"title": "Belief Propagation-based Target Handover in Distributed Integrated Sensing and Communication", "abstract": "Distributed integrated sensing and communication (DISAC) systems are key\nenablers for 6G networks, offering the capability to jointly track multiple\ntargets using spatially distributed base stations (BSs). A fundamental\nchallenge in DISAC is the seamless and efficient handover of target tracks\nbetween BSs with partially overlapping fields of view, especially in dense and\ndynamic environments. In this paper, we propose a novel target handover\nframework based on belief propagation (BP) for multi-target tracking in DISAC\nsystems. By representing the probabilistic data association and tracking\nproblem through a factor graph, the proposed method enables efficient marginal\ninference with reduced computational complexity. Our framework introduces a\nprincipled handover criterion and message-passing strategy that minimizes\ninter-BS communication while maintaining tracking continuity and accuracy. We\ndemonstrate that the proposed handover procedure achieves performance\ncomparable to centralized processing, yet significantly reduces data exchange\nand processing overhead. Extensive simulations validate the robustness of the\napproach in urban tracking scenarios with closely spaced targets.", "published": "2025-06-29 06:54:00", "link": "http://arxiv.org/abs/2506.23118v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "CSBrain: A Cross-scale Spatiotemporal Brain Foundation Model for EEG Decoding", "abstract": "Understanding and decoding brain activity from electroencephalography (EEG)\nsignals is a fundamental challenge in neuroscience and AI, with applications in\ncognition, emotion recognition, diagnosis, and brain-computer interfaces. While\nrecent EEG foundation models advance generalized decoding via unified\narchitectures and large-scale pretraining, they adopt a scale-agnostic dense\nmodeling paradigm inherited from NLP and vision. This design neglects a core\nproperty of neural activity: cross-scale spatiotemporal structure. EEG task\npatterns span a wide range of temporal and spatial scales, from short bursts to\nslow rhythms, and from localized cortical responses to distributed\ninteractions. Ignoring this diversity leads to suboptimal representations and\nweak generalization. We propose CSBrain, a Cross-scale Spatiotemporal Brain\nfoundation model for generalized EEG decoding. CSBrain introduces: (i)\nCross-scale Spatiotemporal Tokenization (CST), which aggregates multi-scale\nfeatures from localized temporal windows and anatomical brain regions into\ncompact scale-aware tokens; and (ii) Structured Sparse Attention (SSA), which\ncaptures cross-window and cross-region dependencies, enhancing scale diversity\nwhile removing spurious correlations. CST and SSA are alternately stacked to\nprogressively integrate multi-scale dependencies. Experiments on 11 EEG tasks\nacross 16 datasets show that CSBrain consistently outperforms task-specific and\nfoundation model baselines. These results establish cross-scale modeling as a\nkey inductive bias and position CSBrain as a robust backbone for future\nbrain-AI research.", "published": "2025-06-29 03:29:34", "link": "http://arxiv.org/abs/2506.23075v1", "categories": ["cs.HC", "cs.LG", "eess.SP", "q-bio.NC"], "primary_category": "cs.HC"}
{"title": "Pipelined Decoder for Efficient Context-Aware Text Generation", "abstract": "As the basis of generative AI, an autoregressive model requires the\ngeneration of a new token depending on all the previously generated tokens,\nwhich brings high quality but also restricts the model to generate tokens one\nby one, forming a bottleneck limiting the generation speed. In this paper, we\npropose a new decoder architecture that efficiently generates text in parallel\nfor context-aware generation tasks. Our proposed pipelined decoder initiates\nthe generation of multiple subsequences simultaneously, and, at each time-step,\nit generates a new token for each subsequence to realize parallelism.\nExperiments on multiple text generation tasks, including question answering,\ntext summarization, and keyphrase generation, show that our pipelined decoder\nsignificantly improves the generation speed without a significant loss of\ngeneration quality or additional memory consumption.", "published": "2025-06-29 23:37:24", "link": "http://arxiv.org/abs/2506.23431v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning-to-Context Slope: Evaluating In-Context Learning Effectiveness Beyond Performance Illusions", "abstract": "In-context learning (ICL) has emerged as an effective approach to enhance the\nperformance of large language models (LLMs). However, its effectiveness varies\nsignificantly across models and tasks, posing challenges for practitioners to\ndetermine when ICL reliably improves performance. Current evaluation\napproaches, reliant on performance change after applying ICL, suffer from low\nreliability, poor attribution, and impracticality in data-insufficient\nscenarios. We propose the Learning-to-Context Slope (LCS), a novel metric that\nquantifies ICL effectiveness by modeling the slope between learning gain (loss\ndecrease from demonstrations) and contextual relevance (demonstration-input\nrelevance). LCS addresses key limitations of performance-based metrics: (1) it\ncaptures continuous loss changes even when outputs are incorrect, improving\nreliability; (2) its formulation attributes ICL failures to weak contextual\nalignment (inability to adapt inputs to demonstrations) or strong output\ncalibration (self-verification of correctness); and (3) it minimizes reliance\non labeled data via synthetic evaluation. Extensive experiments demonstrate\nthat LCS strongly correlates with performance improvements in labeled settings\nand reliably reflects true effectiveness in biased or data-scarce scenarios.\nFurther analysis reveals actionable thresholds for LCS and identifies model\ncapabilities critical to ICL success.", "published": "2025-06-29 08:55:37", "link": "http://arxiv.org/abs/2506.23146v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Flow-Modulated Scoring for Semantic-Aware Knowledge Graph Completion", "abstract": "Effective modeling of multifaceted relations is pivotal for Knowledge Graph\nCompletion (KGC). However, a majority of existing approaches are predicated on\nstatic, embedding-based scoring, exhibiting inherent limitations in capturing\ncontextual dependencies and relational dynamics. Addressing this gap, we\npropose the Flow-Modulated Scoring (FMS) framework. FMS comprises two principal\ncomponents: (1) a semantic context learning module that encodes\ncontext-sensitive entity representations, and (2) a conditional flow-matching\nmodule designed to learn the dynamic transformation from a head to a tail\nembedding, governed by the aforementioned context. The resultant predictive\nvector field, representing the context-informed relational path, serves to\ndynamically refine the initial static score of an entity pair. Through this\nsynergy of context-aware static representations and conditioned dynamic\ninformation, FMS facilitates a more profound modeling of relational semantics.\nComprehensive evaluations on several standard benchmarks demonstrate that our\nproposed method surpasses prior state-of-the-art results.", "published": "2025-06-29 08:22:04", "link": "http://arxiv.org/abs/2506.23137v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Automated Vehicles Should be Connected with Natural Language", "abstract": "Multi-agent collaborative driving promises improvements in traffic safety and\nefficiency through collective perception and decision making. However, existing\ncommunication media -- including raw sensor data, neural network features, and\nperception results -- suffer limitations in bandwidth efficiency, information\ncompleteness, and agent interoperability. Moreover, traditional approaches have\nlargely ignored decision-level fusion, neglecting critical dimensions of\ncollaborative driving. In this paper we argue that addressing these challenges\nrequires a transition from purely perception-oriented data exchanges to\nexplicit intent and reasoning communication using natural language. Natural\nlanguage balances semantic density and communication bandwidth, adapts flexibly\nto real-time conditions, and bridges heterogeneous agent platforms. By enabling\nthe direct communication of intentions, rationales, and decisions, it\ntransforms collaborative driving from reactive perception-data sharing into\nproactive coordination, advancing safety, efficiency, and transparency in\nintelligent transportation systems.", "published": "2025-06-29 16:41:19", "link": "http://arxiv.org/abs/2507.01059v1", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.CV", "cs.RO"], "primary_category": "cs.MA"}
{"title": "Fractional Policy Gradients: Reinforcement Learning with Long-Term Memory", "abstract": "We propose Fractional Policy Gradients (FPG), a reinforcement learning\nframework incorporating fractional calculus for long-term temporal modeling in\npolicy optimization. Standard policy gradient approaches face limitations from\nMarkovian assumptions, exhibiting high variance and inefficient sampling. By\nreformulating gradients using Caputo fractional derivatives, FPG establishes\npower-law temporal correlations between state transitions. We develop an\nefficient recursive computation technique for fractional temporal-difference\nerrors with constant time and memory requirements. Theoretical analysis shows\nFPG achieves asymptotic variance reduction of order O(t^(-alpha)) versus\nstandard policy gradients while preserving convergence. Empirical validation\ndemonstrates 35-68% sample efficiency gains and 24-52% variance reduction\nversus state-of-the-art baselines. This framework provides a mathematically\ngrounded approach for leveraging long-range dependencies without computational\noverhead.", "published": "2025-06-29 04:57:41", "link": "http://arxiv.org/abs/2507.00073v1", "categories": ["cs.LG", "stat.ML", "I.2.6; I.2.8"], "primary_category": "cs.LG"}
