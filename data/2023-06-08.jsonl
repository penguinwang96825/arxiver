{"title": "Improving Vietnamese Legal Question--Answering System based on Automatic\n  Data Enrichment", "abstract": "Question answering (QA) in law is a challenging problem because legal\ndocuments are much more complicated than normal texts in terms of terminology,\nstructure, and temporal and logical relationships. It is even more difficult to\nperform legal QA for low-resource languages like Vietnamese where labeled data\nare rare and pre-trained language models are still limited. In this paper, we\ntry to overcome these limitations by implementing a Vietnamese article-level\nretrieval-based legal QA system and introduce a novel method to improve the\nperformance of language models by improving data quality through weak labeling.\nOur hypothesis is that in contexts where labeled data are limited, efficient\ndata enrichment can help increase overall performance. Our experiments are\ndesigned to test multiple aspects, which demonstrate the effectiveness of the\nproposed technique.", "published": "2023-06-08 00:24:29", "link": "http://arxiv.org/abs/2306.04841v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mixture-of-Supernets: Improving Weight-Sharing Supernet Training with\n  Architecture-Routed Mixture-of-Experts", "abstract": "Weight-sharing supernets are crucial for performance estimation in\ncutting-edge neural architecture search (NAS) frameworks. Despite their ability\nto generate diverse subnetworks without retraining, the quality of these\nsubnetworks is not guaranteed due to weight sharing. In NLP tasks like machine\ntranslation and pre-trained language modeling, there is a significant\nperformance gap between supernet and training from scratch for the same model\narchitecture, necessitating retraining post optimal architecture\nidentification.\n  This study introduces a solution called mixture-of-supernets, a generalized\nsupernet formulation leveraging mixture-of-experts (MoE) to enhance supernet\nmodel expressiveness with minimal training overhead. Unlike conventional\nsupernets, this method employs an architecture-based routing mechanism,\nenabling indirect sharing of model weights among subnetworks. This\ncustomization of weights for specific architectures, learned through gradient\ndescent, minimizes retraining time, significantly enhancing training efficiency\nin NLP. The proposed method attains state-of-the-art (SoTA) performance in NAS\nfor fast machine translation models, exhibiting a superior latency-BLEU\ntradeoff compared to HAT, the SoTA NAS framework for machine translation.\nFurthermore, it excels in NAS for building memory-efficient task-agnostic BERT\nmodels, surpassing NAS-BERT and AutoDistil across various model sizes. The code\ncan be found at: https://github.com/UBC-NLP/MoS.", "published": "2023-06-08 00:35:36", "link": "http://arxiv.org/abs/2306.04845v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NOWJ at COLIEE 2023 -- Multi-Task and Ensemble Approaches in Legal\n  Information Processing", "abstract": "This paper presents the NOWJ team's approach to the COLIEE 2023 Competition,\nwhich focuses on advancing legal information processing techniques and applying\nthem to real-world legal scenarios. Our team tackles the four tasks in the\ncompetition, which involve legal case retrieval, legal case entailment, statute\nlaw retrieval, and legal textual entailment. We employ state-of-the-art machine\nlearning models and innovative approaches, such as BERT, Longformer,\nBM25-ranking algorithm, and multi-task learning models. Although our team did\nnot achieve state-of-the-art results, our findings provide valuable insights\nand pave the way for future improvements in legal information processing.", "published": "2023-06-08 03:10:49", "link": "http://arxiv.org/abs/2306.04903v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Open Set Relation Extraction via Unknown-Aware Training", "abstract": "The existing supervised relation extraction methods have achieved impressive\nperformance in a closed-set setting, where the relations during both training\nand testing remain the same. In a more realistic open-set setting, unknown\nrelations may appear in the test set. Due to the lack of supervision signals\nfrom unknown relations, a well-performing closed-set relation extractor can\nstill confidently misclassify them into known relations. In this paper, we\npropose an unknown-aware training method, regularizing the model by dynamically\nsynthesizing negative instances. To facilitate a compact decision boundary,\n``difficult'' negative instances are necessary. Inspired by text adversarial\nattacks, we adaptively apply small but critical perturbations to original\ntraining instances and thus synthesizing negative instances that are more\nlikely to be mistaken by the model as known relations. Experimental results\nshow that this method achieves SOTA unknown relation detection without\ncompromising the classification of known relations.", "published": "2023-06-08 05:45:25", "link": "http://arxiv.org/abs/2306.04950v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RE-Matching: A Fine-Grained Semantic Matching Method for Zero-Shot\n  Relation Extraction", "abstract": "Semantic matching is a mainstream paradigm of zero-shot relation extraction,\nwhich matches a given input with a corresponding label description. The\nentities in the input should exactly match their hypernyms in the description,\nwhile the irrelevant contexts should be ignored when matching. However, general\nmatching methods lack explicit modeling of the above matching pattern. In this\nwork, we propose a fine-grained semantic matching method tailored for zero-shot\nrelation extraction. Following the above matching pattern, we decompose the\nsentence-level similarity score into entity and context matching scores. Due to\nthe lack of explicit annotations of the redundant components, we design a\nfeature distillation module to adaptively identify the relation-irrelevant\nfeatures and reduce their negative impact on context matching. Experimental\nresults show that our method achieves higher matching $F_1$ score and has an\ninference speed 10 times faster, when compared with the state-of-the-art\nmethods.", "published": "2023-06-08 06:02:34", "link": "http://arxiv.org/abs/2306.04954v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Actively Supervised Clustering for Open Relation Extraction", "abstract": "Current clustering-based Open Relation Extraction (OpenRE) methods usually\nadopt a two-stage pipeline. The first stage simultaneously learns relation\nrepresentations and assignments. The second stage manually labels several\ninstances and thus names the relation for each cluster. However, unsupervised\nobjectives struggle to optimize the model to derive accurate clustering\nassignments, and the number of clusters has to be supplied in advance. In this\npaper, we present a novel setting, named actively supervised clustering for\nOpenRE. Our insight lies in that clustering learning and relation labeling can\nbe alternately performed, providing the necessary guidance for clustering\nwithout a significant increase in human effort. The key to the setting is\nselecting which instances to label. Instead of using classical active labeling\nstrategies designed for fixed known classes, we propose a new strategy, which\nis applicable to dynamically discover clusters of unknown relations.\nExperimental results show that our method is able to discover almost all\nrelational clusters in the data and improve the SOTA methods by 10.3\\% and\n5.2\\%, on two datasets respectively.", "published": "2023-06-08 06:55:02", "link": "http://arxiv.org/abs/2306.04968v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "T3L: Translate-and-Test Transfer Learning for Cross-Lingual Text\n  Classification", "abstract": "Cross-lingual text classification leverages text classifiers trained in a\nhigh-resource language to perform text classification in other languages with\nno or minimal fine-tuning (zero/few-shots cross-lingual transfer). Nowadays,\ncross-lingual text classifiers are typically built on large-scale, multilingual\nlanguage models (LMs) pretrained on a variety of languages of interest.\nHowever, the performance of these models vary significantly across languages\nand classification tasks, suggesting that the superposition of the language\nmodelling and classification tasks is not always effective. For this reason, in\nthis paper we propose revisiting the classic \"translate-and-test\" pipeline to\nneatly separate the translation and classification stages. The proposed\napproach couples 1) a neural machine translator translating from the targeted\nlanguage to a high-resource language, with 2) a text classifier trained in the\nhigh-resource language, but the neural machine translator generates \"soft\"\ntranslations to permit end-to-end backpropagation during fine-tuning of the\npipeline. Extensive experiments have been carried out over three cross-lingual\ntext classification datasets (XNLI, MLDoc and MultiEURLEX), with the results\nshowing that the proposed approach has significantly improved performance over\na competitive baseline.", "published": "2023-06-08 07:33:22", "link": "http://arxiv.org/abs/2306.04996v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LCT-1 at SemEval-2023 Task 10: Pre-training and Multi-task Learning for\n  Sexism Detection and Classification", "abstract": "Misogyny and sexism are growing problems in social media. Advances have been\nmade in online sexism detection but the systems are often uninterpretable.\nSemEval-2023 Task 10 on Explainable Detection of Online Sexism aims at\nincreasing explainability of the sexism detection, and our team participated in\nall the proposed subtasks. Our system is based on further domain-adaptive\npre-training (Gururangan et al., 2020). Building on the Transformer-based\nmodels with the domain adaptation, we compare fine-tuning with multi-task\nlearning and show that each subtask requires a different system configuration.\nIn our experiments, multi-task learning performs on par with standard\nfine-tuning for sexism detection and noticeably better for coarse-grained\nsexism classification, while fine-tuning is preferable for fine-grained\nclassification.", "published": "2023-06-08 09:56:57", "link": "http://arxiv.org/abs/2306.05075v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DLAMA: A Framework for Curating Culturally Diverse Facts for Probing the\n  Knowledge of Pretrained Language Models", "abstract": "A few benchmarking datasets have been released to evaluate the factual\nknowledge of pretrained language models. These benchmarks (e.g., LAMA, and\nParaRel) are mainly developed in English and later are translated to form new\nmultilingual versions (e.g., mLAMA, and mParaRel). Results on these\nmultilingual benchmarks suggest that using English prompts to recall the facts\nfrom multilingual models usually yields significantly better and more\nconsistent performance than using non-English prompts. Our analysis shows that\nmLAMA is biased toward facts from Western countries, which might affect the\nfairness of probing models. We propose a new framework for curating factual\ntriples from Wikidata that are culturally diverse. A new benchmark DLAMA-v1 is\nbuilt of factual triples from three pairs of contrasting cultures having a\ntotal of 78,259 triples from 20 relation predicates. The three pairs comprise\nfacts representing the (Arab and Western), (Asian and Western), and (South\nAmerican and Western) countries respectively. Having a more balanced benchmark\n(DLAMA-v1) supports that mBERT performs better on Western facts than\nnon-Western ones, while monolingual Arabic, English, and Korean models tend to\nperform better on their culturally proximate facts. Moreover, both monolingual\nand multilingual models tend to make a prediction that is culturally or\ngeographically relevant to the correct label, even if the prediction is wrong.", "published": "2023-06-08 09:59:48", "link": "http://arxiv.org/abs/2306.05076v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revealing the Blind Spot of Sentence Encoder Evaluation by HEROS", "abstract": "Existing sentence textual similarity benchmark datasets only use a single\nnumber to summarize how similar the sentence encoder's decision is to humans'.\nHowever, it is unclear what kind of sentence pairs a sentence encoder (SE)\nwould consider similar. Moreover, existing SE benchmarks mainly consider\nsentence pairs with low lexical overlap, so it is unclear how the SEs behave\nwhen two sentences have high lexical overlap. We introduce a high-quality SE\ndiagnostic dataset, HEROS. HEROS is constructed by transforming an original\nsentence into a new sentence based on certain rules to form a \\textit{minimal\npair}, and the minimal pair has high lexical overlaps. The rules include\nreplacing a word with a synonym, an antonym, a typo, a random word, and\nconverting the original sentence into its negation. Different rules yield\ndifferent subsets of HEROS. By systematically comparing the performance of over\n60 supervised and unsupervised SEs on HEROS, we reveal that most unsupervised\nsentence encoders are insensitive to negation. We find the datasets used to\ntrain the SE are the main determinants of what kind of sentence pairs an SE\nconsiders similar. We also show that even if two SEs have similar performance\non STS benchmarks, they can have very different behavior on HEROS. Our result\nreveals the blind spot of traditional STS benchmarks when evaluating SEs.", "published": "2023-06-08 10:24:02", "link": "http://arxiv.org/abs/2306.05083v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reference Matters: Benchmarking Factual Error Correction for Dialogue\n  Summarization with Fine-grained Evaluation Framework", "abstract": "Factuality is important to dialogue summarization. Factual error correction\n(FEC) of model-generated summaries is one way to improve factuality. Current\nFEC evaluation that relies on factuality metrics is not reliable and detailed\nenough. To address this problem, we are the first to manually annotate a FEC\ndataset for dialogue summarization containing 4000 items and propose FERRANTI,\na fine-grained evaluation framework based on reference correction that\nautomatically evaluates the performance of FEC models on different error\ncategories. Using this evaluation framework, we conduct sufficient experiments\nwith FEC approaches under a variety of settings and find the best training\nmodes and significant differences in the performance of the existing approaches\non different factual error categories.", "published": "2023-06-08 11:41:39", "link": "http://arxiv.org/abs/2306.05119v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mapping Brains with Language Models: A Survey", "abstract": "Over the years, many researchers have seemingly made the same observation:\nBrain and language model activations exhibit some structural similarities,\nenabling linear partial mappings between features extracted from neural\nrecordings and computational language models. In an attempt to evaluate how\nmuch evidence has been accumulated for this observation, we survey over 30\nstudies spanning 10 datasets and 8 metrics. How much evidence has been\naccumulated, and what, if anything, is missing before we can draw conclusions?\nOur analysis of the evaluation methods used in the literature reveals that some\nof the metrics are less conservative. We also find that the accumulated\nevidence, for now, remains ambiguous, but correlations with model size and\nquality provide grounds for cautious optimism.", "published": "2023-06-08 11:50:58", "link": "http://arxiv.org/abs/2306.05126v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Overview of the Problem List Summarization (ProbSum) 2023 Shared Task on\n  Summarizing Patients' Active Diagnoses and Problems from Electronic Health\n  Record Progress Notes", "abstract": "The BioNLP Workshop 2023 initiated the launch of a shared task on Problem\nList Summarization (ProbSum) in January 2023. The aim of this shared task is to\nattract future research efforts in building NLP models for real-world\ndiagnostic decision support applications, where a system generating relevant\nand accurate diagnoses will augment the healthcare providers decision-making\nprocess and improve the quality of care for patients. The goal for participants\nis to develop models that generated a list of diagnoses and problems using\ninput from the daily care notes collected from the hospitalization of\ncritically ill patients. Eight teams submitted their final systems to the\nshared task leaderboard. In this paper, we describe the tasks, datasets,\nevaluation metrics, and baseline systems. Additionally, the techniques and\nresults of the evaluation of the different approaches tried by the\nparticipating teams are summarized.", "published": "2023-06-08 15:19:57", "link": "http://arxiv.org/abs/2306.05270v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisit Few-shot Intent Classification with PLMs: Direct Fine-tuning vs.\n  Continual Pre-training", "abstract": "We consider the task of few-shot intent detection, which involves training a\ndeep learning model to classify utterances based on their underlying intents\nusing only a small amount of labeled data. The current approach to address this\nproblem is through continual pre-training, i.e., fine-tuning pre-trained\nlanguage models (PLMs) on external resources (e.g., conversational corpora,\npublic intent detection datasets, or natural language understanding datasets)\nbefore using them as utterance encoders for training an intent classifier. In\nthis paper, we show that continual pre-training may not be essential, since the\noverfitting problem of PLMs on this task may not be as serious as expected.\nSpecifically, we find that directly fine-tuning PLMs on only a handful of\nlabeled examples already yields decent results compared to methods that employ\ncontinual pre-training, and the performance gap diminishes rapidly as the\nnumber of labeled data increases. To maximize the utilization of the limited\navailable data, we propose a context augmentation method and leverage\nsequential self-distillation to boost performance. Comprehensive experiments on\nreal-world benchmarks show that given only two or more labeled samples per\nclass, direct fine-tuning outperforms many strong baselines that utilize\nexternal data sources for continual pre-training. The code can be found at\nhttps://github.com/hdzhang-code/DFTPlus.", "published": "2023-06-08 15:26:52", "link": "http://arxiv.org/abs/2306.05278v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ToolAlpaca: Generalized Tool Learning for Language Models with 3000\n  Simulated Cases", "abstract": "Enabling large language models to utilize real-world tools effectively is\ncrucial for achieving embodied intelligence. Existing approaches to tool\nlearning have either primarily relied on extremely large language models, such\nas GPT-4, to attain generalized tool-use abilities in a zero-shot manner, or\nutilized supervised learning to train limited scopes of tools on compact\nmodels. However, it remains uncertain whether smaller language models can\nachieve generalized tool-use abilities without tool-specific training. To\naddress this question, this paper introduces ToolAlpaca, a novel framework\ndesigned to automatically generate a diverse tool-use corpus and learn\ngeneralized tool-use abilities on compact language models with minimal human\nintervention. Specifically, ToolAlpaca first automatically creates a highly\ndiversified tool-use corpus by building a multi-agent simulation environment.\nThe corpus contains 3938 tool-use instances from more than 400 real-world tool\nAPIs spanning 50 distinct categories. Subsequently, the constructed corpus is\nemployed to fine-tune compact language models, resulting in two models, namely\nToolAlpaca-7B and ToolAlpaca-13B, respectively. Finally, we evaluate the\nability of these models to utilize previously unseen tools without specific\ntraining. Experimental results demonstrate that ToolAlpaca achieves effective\ngeneralized tool-use capabilities comparable to those of extremely large\nlanguage models like GPT-3.5, demonstrating that learning generalized tool-use\nability is feasible for compact language models.", "published": "2023-06-08 15:46:32", "link": "http://arxiv.org/abs/2306.05301v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CUED at ProbSum 2023: Hierarchical Ensemble of Summarization Models", "abstract": "In this paper, we consider the challenge of summarizing patients' medical\nprogress notes in a limited data setting. For the Problem List Summarization\n(shared task 1A) at the BioNLP Workshop 2023, we demonstrate that Clinical-T5\nfine-tuned to 765 medical clinic notes outperforms other extractive,\nabstractive and zero-shot baselines, yielding reasonable baseline systems for\nmedical note summarization. Further, we introduce Hierarchical Ensemble of\nSummarization Models (HESM), consisting of token-level ensembles of diverse\nfine-tuned Clinical-T5 models, followed by Minimum Bayes Risk (MBR) decoding.\nOur HESM approach lead to a considerable summarization performance boost, and\nwhen evaluated on held-out challenge data achieved a ROUGE-L of 32.77, which\nwas the best-performing system at the top of the shared task leaderboard.", "published": "2023-06-08 16:08:10", "link": "http://arxiv.org/abs/2306.05317v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Utterance Emotion Dynamics in Children's Poems: Emotional Changes Across\n  Age", "abstract": "Emerging psychopathology studies are showing that patterns of changes in\nemotional state -- emotion dynamics -- are associated with overall well-being\nand mental health. More recently, there has been some work in tracking emotion\ndynamics through one's utterances, allowing for data to be collected on a\nlarger scale across time and people. However, several questions about how\nemotion dynamics change with age, especially in children, and when determined\nthrough children's writing, remain unanswered. In this work, we use both a\nlexicon and a machine learning based approach to quantify characteristics of\nemotion dynamics determined from poems written by children of various ages. We\nshow that both approaches point to similar trends: consistent increasing\nintensities for some emotions (e.g., anger, fear, joy, sadness, arousal, and\ndominance) with age and a consistent decreasing valence with age. We also find\nincreasing emotional variability, rise rates (i.e., emotional reactivity), and\nrecovery rates (i.e., emotional regulation) with age. These results act as a\nuseful baselines for further research in how patterns of emotions expressed by\nchildren change with age, and their association with mental health.", "published": "2023-06-08 17:38:14", "link": "http://arxiv.org/abs/2306.05387v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modular Visual Question Answering via Code Generation", "abstract": "We present a framework that formulates visual question answering as modular\ncode generation. In contrast to prior work on modular approaches to VQA, our\napproach requires no additional training and relies on pre-trained language\nmodels (LMs), visual models pre-trained on image-caption pairs, and fifty VQA\nexamples used for in-context learning. The generated Python programs invoke and\ncompose the outputs of the visual models using arithmetic and conditional\nlogic. Our approach improves accuracy on the COVR dataset by at least 3% and on\nthe GQA dataset by roughly 2% compared to the few-shot baseline that does not\nemploy code generation.", "published": "2023-06-08 17:45:14", "link": "http://arxiv.org/abs/2306.05392v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mixture-of-Domain-Adapters: Decoupling and Injecting Domain Knowledge to\n  Pre-trained Language Models Memories", "abstract": "Pre-trained language models (PLMs) demonstrate excellent abilities to\nunderstand texts in the generic domain while struggling in a specific domain.\nAlthough continued pre-training on a large domain-specific corpus is effective,\nit is costly to tune all the parameters on the domain. In this paper, we\ninvestigate whether we can adapt PLMs both effectively and efficiently by only\ntuning a few parameters. Specifically, we decouple the feed-forward networks\n(FFNs) of the Transformer architecture into two parts: the original pre-trained\nFFNs to maintain the old-domain knowledge and our novel domain-specific\nadapters to inject domain-specific knowledge in parallel. Then we adopt a\nmixture-of-adapters gate to fuse the knowledge from different domain adapters\ndynamically. Our proposed Mixture-of-Domain-Adapters (MixDA) employs a\ntwo-stage adapter-tuning strategy that leverages both unlabeled data and\nlabeled data to help the domain adaptation: i) domain-specific adapter on\nunlabeled data; followed by ii) the task-specific adapter on labeled data.\nMixDA can be seamlessly plugged into the pretraining-finetuning paradigm and\nour experiments demonstrate that MixDA achieves superior performance on\nin-domain tasks (GLUE), out-of-domain tasks (ChemProt, RCT, IMDB, Amazon), and\nknowledge-intensive tasks (KILT). Further analyses demonstrate the reliability,\nscalability, and efficiency of our method. The code is available at\nhttps://github.com/Amano-Aki/Mixture-of-Domain-Adapters.", "published": "2023-06-08 17:54:36", "link": "http://arxiv.org/abs/2306.05406v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Privacy- and Utility-Preserving NLP with Anonymized Data: A case study\n  of Pseudonymization", "abstract": "This work investigates the effectiveness of different pseudonymization\ntechniques, ranging from rule-based substitutions to using pre-trained Large\nLanguage Models (LLMs), on a variety of datasets and models used for two widely\nused NLP tasks: text classification and summarization. Our work provides\ncrucial insights into the gaps between original and anonymized data (focusing\non the pseudonymization technique) and model quality and fosters future\nresearch into higher-quality anonymization techniques to better balance the\ntrade-offs between data protection and utility preservation. We make our code,\npseudonymized datasets, and downstream models publicly available", "published": "2023-06-08 21:06:19", "link": "http://arxiv.org/abs/2306.05561v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LOST: A Mental Health Dataset of Low Self-esteem in Reddit Posts", "abstract": "Low self-esteem and interpersonal needs (i.e., thwarted belongingness (TB)\nand perceived burdensomeness (PB)) have a major impact on depression and\nsuicide attempts. Individuals seek social connectedness on social media to\nboost and alleviate their loneliness. Social media platforms allow people to\nexpress their thoughts, experiences, beliefs, and emotions. Prior studies on\nmental health from social media have focused on symptoms, causes, and\ndisorders. Whereas an initial screening of social media content for\ninterpersonal risk factors and low self-esteem may raise early alerts and\nassign therapists to at-risk users of mental disturbance. Standardized scales\nmeasure self-esteem and interpersonal needs from questions created using\npsychological theories. In the current research, we introduce a\npsychology-grounded and expertly annotated dataset, LoST: Low Self esTeem, to\nstudy and detect low self-esteem on Reddit. Through an annotation approach\ninvolving checks on coherence, correctness, consistency, and reliability, we\nensure gold-standard for supervised learning. We present results from different\ndeep language models tested using two data augmentation techniques. Our\nfindings suggest developing a class of language models that infuses\npsychological and clinical knowledge.", "published": "2023-06-08 23:52:35", "link": "http://arxiv.org/abs/2306.05596v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "In-Context Learning through the Bayesian Prism", "abstract": "In-context learning (ICL) is one of the surprising and useful features of\nlarge language models and subject of intense research. Recently, stylized\nmeta-learning-like ICL setups have been devised that train transformers on\nsequences of input-output pairs $(x, f(x))$. The function $f$ comes from a\nfunction class and generalization is checked by evaluating on sequences\ngenerated from unseen functions from the same class. One of the main\ndiscoveries in this line of research has been that for several function\nclasses, such as linear regression, transformers successfully generalize to new\nfunctions in the class. However, the inductive biases of these models resulting\nin this behavior are not clearly understood. A model with unlimited training\ndata and compute is a Bayesian predictor: it learns the pretraining\ndistribution. In this paper we empirically examine how far this Bayesian\nperspective can help us understand ICL. To this end, we generalize the previous\nmeta-ICL setup to hierarchical meta-ICL setup which involve unions of multiple\ntask families. We instantiate this setup on a diverse range of linear and\nnonlinear function families and find that transformers can do ICL in this\nsetting as well. Where Bayesian inference is tractable, we find evidence that\nhigh-capacity transformers mimic the Bayesian predictor. The Bayesian\nperspective provides insights into the inductive bias of ICL and how\ntransformers perform a particular task when they are trained on multiple tasks.\nWe also find that transformers can learn to generalize to new function classes\nthat were not seen during pretraining. This involves deviation from the\nBayesian predictor. We examine these deviations in more depth offering new\ninsights and hypotheses.", "published": "2023-06-08 02:38:23", "link": "http://arxiv.org/abs/2306.04891v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Prefer to Classify: Improving Text Classifiers via Auxiliary Preference\n  Learning", "abstract": "The development of largely human-annotated benchmarks has driven the success\nof deep neural networks in various NLP tasks. To enhance the effectiveness of\nexisting benchmarks, collecting new additional input-output pairs is often too\ncostly and challenging, particularly considering their marginal impact on\nimproving the current model accuracy. Instead, additional or complementary\nannotations on the existing input texts in the benchmarks can be preferable as\nan efficient way to pay the additional human cost. In this paper, we\ninvestigate task-specific preferences between pairs of input texts as a new\nalternative way for such auxiliary data annotation. From 'pair-wise'\ncomparisons with respect to the task, the auxiliary preference learning enables\nthe model to learn an additional informative training signal that cannot be\ncaptured with 'instance-wise' task labels. To this end, we propose a novel\nmulti-task learning framework, called prefer-to-classify (P2C), which can enjoy\nthe cooperative effect of learning both the given classification task and the\nauxiliary preferences. Here, we provide three different ways to collect\npreference signals in practice: (a) implicitly extracting from annotation\nrecords (for free, but often unavailable), (b) collecting explicitly from crowd\nworkers (high paid), or (c) pre-trained large language models such as GPT-3\n(low paid). Given existing classification NLP benchmarks, we demonstrate that\nthe proposed auxiliary preference learning via P2C on them is effective in\nimproving text classifiers. Our codes are publicly available.", "published": "2023-06-08 04:04:47", "link": "http://arxiv.org/abs/2306.04925v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A modified model for topic detection from a corpus and a new metric\n  evaluating the understandability of topics", "abstract": "This paper presents a modified neural model for topic detection from a corpus\nand proposes a new metric to evaluate the detected topics. The new model builds\nupon the embedded topic model incorporating some modifications such as document\nclustering. Numerical experiments suggest that the new model performs\nfavourably regardless of the document's length. The new metric, which can be\ncomputed more efficiently than widely-used metrics such as topic coherence,\nprovides variable information regarding the understandability of the detected\ntopics.", "published": "2023-06-08 05:17:03", "link": "http://arxiv.org/abs/2306.04941v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Leveraging Language Identification to Enhance Code-Mixed Text\n  Classification", "abstract": "The usage of more than one language in the same text is referred to as Code\nMixed. It is evident that there is a growing degree of adaption of the use of\ncode-mixed data, especially English with a regional language, on social media\nplatforms. Existing deep-learning models do not take advantage of the implicit\nlanguage information in the code-mixed text. Our study aims to improve\nBERT-based models performance on low-resource Code-Mixed Hindi-English Datasets\nby experimenting with language augmentation approaches. We propose a pipeline\nto improve code-mixed systems that comprise data preprocessing, word-level\nlanguage identification, language augmentation, and model training on\ndownstream tasks like sentiment analysis. For language augmentation in BERT\nmodels, we explore word-level interleaving and post-sentence placement of\nlanguage information. We have examined the performance of vanilla BERT-based\nmodels and their code-mixed HingBERT counterparts on respective benchmark\ndatasets, comparing their results with and without using word-level language\ninformation. The models were evaluated using metrics such as accuracy,\nprecision, recall, and F1 score. Our findings show that the proposed language\naugmentation approaches work well across different BERT models. We demonstrate\nthe importance of augmenting code-mixed text with language information on five\ndifferent code-mixed Hindi-English downstream datasets based on sentiment\nanalysis, hate speech detection, and emotion detection.", "published": "2023-06-08 06:43:10", "link": "http://arxiv.org/abs/2306.04964v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "K2: A Foundation Language Model for Geoscience Knowledge Understanding\n  and Utilization", "abstract": "Large language models (LLMs) have achieved great success in general domains\nof natural language processing. In this paper, we bring LLMs to the realm of\ngeoscience with the objective of advancing research and applications in this\nfield. To this end, we present the first-ever LLM in geoscience, K2, alongside\na suite of resources developed to further promote LLM research within\ngeoscience. For instance, we have curated the first geoscience instruction\ntuning dataset, GeoSignal, which aims to align LLM responses to\ngeoscience-related user queries. Additionally, we have established the first\ngeoscience benchmark, GeoBench, to evaluate LLMs in the context of geoscience.\nIn this work, we experiment with a complete recipe to adapt a pre-trained\ngeneral-domain LLM to the geoscience domain. Specifically, we further train the\nLLaMA-7B model on 5.5B tokens of geoscience text corpus, including over 1\nmillion pieces of geoscience literature, and utilize GeoSignal's supervised\ndata to fine-tune the model. Moreover, we share a protocol that can efficiently\ngather domain-specific data and construct domain-supervised data, even in\nsituations where manpower is scarce. Meanwhile, we equip K2 with the abilities\nof using tools to be a naive geoscience aide. Experiments conducted on the\nGeoBench demonstrate the effectiveness of our approach and datasets on\ngeoscience knowledge understanding and utilization.We open-source all the\ntraining data and K2 model checkpoints at https://github.com/davendw49/k2.", "published": "2023-06-08 09:29:05", "link": "http://arxiv.org/abs/2306.05064v2", "categories": ["cs.CL", "cs.AI", "I.2.7; F.4.1"], "primary_category": "cs.CL"}
{"title": "PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning\n  Optimization", "abstract": "Instruction tuning large language models (LLMs) remains a challenging task,\nowing to the complexity of hyperparameter selection and the difficulty involved\nin evaluating the tuned models. To determine the optimal hyperparameters, an\nautomatic, robust, and reliable evaluation benchmark is essential. However,\nestablishing such a benchmark is not a trivial task due to the challenges\nassociated with evaluation accuracy and privacy protection. In response to\nthese challenges, we introduce a judge large language model, named PandaLM,\nwhich is trained to distinguish the superior model given several LLMs.\nPandaLM's focus extends beyond just the objective correctness of responses,\nwhich is the main focus of traditional evaluation datasets. It addresses vital\nsubjective factors such as relative conciseness, clarity, adherence to\ninstructions, comprehensiveness, and formality. To ensure the reliability of\nPandaLM, we collect a diverse human-annotated test dataset, where all contexts\nare generated by humans and labels are aligned with human preferences. Our\nresults indicate that PandaLM-7B achieves 93.75% of GPT-3.5's evaluation\nability and 88.28% of GPT-4's in terms of F1-score on our test dataset. PandaLM\nenables the evaluation of LLM to be fairer but with less cost, evidenced by\nsignificant improvements achieved by models tuned through PandaLM compared to\ntheir counterparts trained with default Alpaca's hyperparameters. In addition,\nPandaLM does not depend on API-based evaluations, thus avoiding potential data\nleakage. All resources of PandaLM are released at\nhttps://github.com/WeOpenML/PandaLM.", "published": "2023-06-08 10:41:56", "link": "http://arxiv.org/abs/2306.05087v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Closing the Loop: Testing ChatGPT to Generate Model Explanations to\n  Improve Human Labelling of Sponsored Content on Social Media", "abstract": "Regulatory bodies worldwide are intensifying their efforts to ensure\ntransparency in influencer marketing on social media through instruments like\nthe Unfair Commercial Practices Directive (UCPD) in the European Union, or\nSection 5 of the Federal Trade Commission Act. Yet enforcing these obligations\nhas proven to be highly problematic due to the sheer scale of the influencer\nmarket. The task of automatically detecting sponsored content aims to enable\nthe monitoring and enforcement of such regulations at scale. Current research\nin this field primarily frames this problem as a machine learning task,\nfocusing on developing models that achieve high classification performance in\ndetecting ads. These machine learning tasks rely on human data annotation to\nprovide ground truth information. However, agreement between annotators is\noften low, leading to inconsistent labels that hinder the reliability of\nmodels. To improve annotation accuracy and, thus, the detection of sponsored\ncontent, we propose using chatGPT to augment the annotation process with\nphrases identified as relevant features and brief explanations. Our experiments\nshow that this approach consistently improves inter-annotator agreement and\nannotation accuracy. Additionally, our survey of user experience in the\nannotation task indicates that the explanations improve the annotators'\nconfidence and streamline the process. Our proposed methods can ultimately lead\nto more transparency and alignment with regulatory requirements in sponsored\ncontent detection.", "published": "2023-06-08 11:29:58", "link": "http://arxiv.org/abs/2306.05115v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "RRWKV: Capturing Long-range Dependencies in RWKV", "abstract": "Owing to the impressive dot-product attention, the Transformers have been the\ndominant architectures in various natural language processing (NLP) tasks.\nRecently, the Receptance Weighted Key Value (RWKV) architecture follows a\nnon-transformer architecture to eliminate the drawbacks of dot-product\nattention, where memory and computational complexity exhibits quadratic scaling\nwith sequence length. Although RWKV has exploited a linearly tensor-product\nattention mechanism and achieved parallelized computations by deploying the\ntime-sequential mode, it fails to capture long-range dependencies because of\nits limitation on looking back at previous information, compared with full\ninformation obtained by direct interactions in the standard transformer.\nTherefore, the paper devises the Retrospected Receptance Weighted Key Value\n(RRWKV) architecture via incorporating the retrospecting ability into the RWKV\nto effectively absorb information, which maintains memory and computational\nefficiency as well.", "published": "2023-06-08 13:17:06", "link": "http://arxiv.org/abs/2306.05176v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining\n  Large Language Models", "abstract": "Despite the existence of various benchmarks for evaluating natural language\nprocessing models, we argue that human exams are a more suitable means of\nevaluating general intelligence for large language models (LLMs), as they\ninherently demand a much wider range of abilities such as language\nunderstanding, domain knowledge, and problem-solving skills. To this end, we\nintroduce M3Exam, a novel benchmark sourced from real and official human exam\nquestions for evaluating LLMs in a multilingual, multimodal, and multilevel\ncontext. M3Exam exhibits three unique characteristics: (1) multilingualism,\nencompassing questions from multiple countries that require strong multilingual\nproficiency and cultural knowledge; (2) multimodality, accounting for the\nmultimodal nature of many exam questions to test the model's multimodal\nunderstanding capability; and (3) multilevel structure, featuring exams from\nthree critical educational periods to comprehensively assess a model's\nproficiency at different levels. In total, M3Exam contains 12,317 questions in\n9 diverse languages with three educational levels, where about 23\\% of the\nquestions require processing images for successful solving. We assess the\nperformance of top-performing LLMs on M3Exam and find that current models,\nincluding GPT-4, still struggle with multilingual text, particularly in\nlow-resource and non-Latin script languages. Multimodal LLMs also perform\npoorly with complex multimodal questions. We believe that M3Exam can be a\nvaluable resource for comprehensively evaluating LLMs by examining their\nmultilingual and multimodal abilities and tracking their development. Data and\nevaluation code is available at \\url{https://github.com/DAMO-NLP-SG/M3Exam}.", "published": "2023-06-08 13:21:29", "link": "http://arxiv.org/abs/2306.05179v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Extensive Evaluation of Transformer-based Architectures for Adverse Drug\n  Events Extraction", "abstract": "Adverse Event (ADE) extraction is one of the core tasks in digital\npharmacovigilance, especially when applied to informal texts. This task has\nbeen addressed by the Natural Language Processing community using large\npre-trained language models, such as BERT. Despite the great number of\nTransformer-based architectures used in the literature, it is unclear which of\nthem has better performances and why. Therefore, in this paper we perform an\nextensive evaluation and analysis of 19 Transformer-based models for ADE\nextraction on informal texts. We compare the performance of all the considered\nmodels on two datasets with increasing levels of informality (forums posts and\ntweets). We also combine the purely Transformer-based models with two\ncommonly-used additional processing layers (CRF and LSTM), and analyze their\neffect on the models performance. Furthermore, we use a well-established\nfeature importance technique (SHAP) to correlate the performance of the models\nwith a set of features that describe them: model category (AutoEncoding,\nAutoRegressive, Text-to-Text), pretraining domain, training from scratch, and\nmodel size in number of parameters. At the end of our analyses, we identify a\nlist of take-home messages that can be derived from the experimental data.", "published": "2023-06-08 15:25:24", "link": "http://arxiv.org/abs/2306.05276v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "KIT's Multilingual Speech Translation System for IWSLT 2023", "abstract": "Many existing speech translation benchmarks focus on native-English speech in\nhigh-quality recording conditions, which often do not match the conditions in\nreal-life use-cases. In this paper, we describe our speech translation system\nfor the multilingual track of IWSLT 2023, which evaluates translation quality\non scientific conference talks. The test condition features accented input\nspeech and terminology-dense contents. The task requires translation into 10\nlanguages of varying amounts of resources. In absence of training data from the\ntarget domain, we use a retrieval-based approach (kNN-MT) for effective\nadaptation (+0.8 BLEU for speech translation). We also use adapters to easily\nintegrate incremental training data from data augmentation, and show that it\nmatches the performance of re-training. We observe that cascaded systems are\nmore easily adaptable towards specific target domains, due to their separate\nmodules. Our cascaded speech system substantially outperforms its end-to-end\ncounterpart on scientific talk translation, although their performance remains\nsimilar on TED talks.", "published": "2023-06-08 16:13:20", "link": "http://arxiv.org/abs/2306.05320v3", "categories": ["cs.CL", "cs.SD"], "primary_category": "cs.CL"}
{"title": "PIXIU: A Large Language Model, Instruction Data and Evaluation Benchmark\n  for Finance", "abstract": "Although large language models (LLMs) has shown great performance on natural\nlanguage processing (NLP) in the financial domain, there are no publicly\navailable financial tailtored LLMs, instruction tuning datasets, and evaluation\nbenchmarks, which is critical for continually pushing forward the open-source\ndevelopment of financial artificial intelligence (AI). This paper introduces\nPIXIU, a comprehensive framework including the first financial LLM based on\nfine-tuning LLaMA with instruction data, the first instruction data with 136K\ndata samples to support the fine-tuning, and an evaluation benchmark with 5\ntasks and 9 datasets. We first construct the large-scale multi-task instruction\ndata considering a variety of financial tasks, financial document types, and\nfinancial data modalities. We then propose a financial LLM called FinMA by\nfine-tuning LLaMA with the constructed dataset to be able to follow\ninstructions for various financial tasks. To support the evaluation of\nfinancial LLMs, we propose a standardized benchmark that covers a set of\ncritical financial tasks, including five financial NLP tasks and one financial\nprediction task. With this benchmark, we conduct a detailed analysis of FinMA\nand several existing LLMs, uncovering their strengths and weaknesses in\nhandling critical financial tasks. The model, datasets, benchmark, and\nexperimental results are open-sourced to facilitate future research in\nfinancial AI.", "published": "2023-06-08 14:20:29", "link": "http://arxiv.org/abs/2306.05443v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Hexatagging: Projective Dependency Parsing as Tagging", "abstract": "We introduce a novel dependency parser, the hexatagger, that constructs\ndependency trees by tagging the words in a sentence with elements from a finite\nset of possible tags. In contrast to many approaches to dependency parsing, our\napproach is fully parallelizable at training time, i.e., the structure-building\nactions needed to build a dependency parse can be predicted in parallel to each\nother. Additionally, exact decoding is linear in time and space complexity.\nFurthermore, we derive a probabilistic dependency parser that predicts hexatags\nusing no more than a linear model with features from a pretrained language\nmodel, i.e., we forsake a bespoke architecture explicitly designed for the\ntask. Despite the generality and simplicity of our approach, we achieve\nstate-of-the-art performance of 96.4 LAS and 97.4 UAS on the Penn Treebank test\nset. Additionally, our parser's linear time complexity and parallelism\nsignificantly improve computational efficiency, with a roughly 10-times\nspeed-up over previous state-of-the-art models during decoding.", "published": "2023-06-08 18:02:07", "link": "http://arxiv.org/abs/2306.05477v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Emotion and Sentiment Guided Paraphrasing", "abstract": "Paraphrase generation, a.k.a. paraphrasing, is a common and important task in\nnatural language processing. Emotional paraphrasing, which changes the emotion\nembodied in a piece of text while preserving its meaning, has many potential\napplications, including moderating online dialogues and preventing\ncyberbullying. We introduce a new task of fine-grained emotional paraphrasing\nalong emotion gradients, that is, altering the emotional intensities of the\nparaphrases in fine-grained settings following smooth variations in affective\ndimensions while preserving the meaning of the original text. We reconstruct\nseveral widely used paraphrasing datasets by augmenting the input and target\ntexts with their fine-grained emotion labels. Then, we propose a framework for\nemotion and sentiment guided paraphrasing by leveraging pre-trained language\nmodels for conditioned text generation. Extensive evaluation of the fine-tuned\nmodels suggests that including fine-grained emotion labels in the paraphrase\ntask significantly improves the likelihood of obtaining high-quality\nparaphrases that reflect the desired emotions while achieving consistently\nbetter scores in paraphrase metrics such as BLEU, ROUGE, and METEOR.", "published": "2023-06-08 20:59:40", "link": "http://arxiv.org/abs/2306.05556v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The economic trade-offs of large language models: A case study", "abstract": "Contacting customer service via chat is a common practice. Because employing\ncustomer service agents is expensive, many companies are turning to NLP that\nassists human agents by auto-generating responses that can be used directly or\nwith modifications. Large Language Models (LLMs) are a natural fit for this use\ncase; however, their efficacy must be balanced with the cost of training and\nserving them. This paper assesses the practical cost and impact of LLMs for the\nenterprise as a function of the usefulness of the responses that they generate.\nWe present a cost framework for evaluating an NLP model's utility for this use\ncase and apply it to a single brand as a case study in the context of an\nexisting agent assistance product. We compare three strategies for specializing\nan LLM - prompt engineering, fine-tuning, and knowledge distillation - using\nfeedback from the brand's customer service agents. We find that the usability\nof a model's responses can make up for a large difference in inference cost for\nour case study brand, and we extrapolate our findings to the broader enterprise\nspace.", "published": "2023-06-08 20:35:53", "link": "http://arxiv.org/abs/2306.07402v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Expanding Scope: Adapting English Adversarial Attacks to Chinese", "abstract": "Recent studies have revealed that NLP predictive models are vulnerable to\nadversarial attacks. Most existing studies focused on designing attacks to\nevaluate the robustness of NLP models in the English language alone. Literature\nhas seen an increasing need for NLP solutions for other languages. We,\ntherefore, ask one natural question: whether state-of-the-art (SOTA) attack\nmethods generalize to other languages. This paper investigates how to adapt\nSOTA adversarial attack algorithms in English to the Chinese language. Our\nexperiments show that attack methods previously applied to English NLP can\ngenerate high-quality adversarial examples in Chinese when combined with proper\ntext segmentation and linguistic constraints. In addition, we demonstrate that\nthe generated adversarial examples can achieve high fluency and semantic\nconsistency by focusing on the Chinese language's morphology and phonology,\nwhich in turn can be used to improve the adversarial robustness of Chinese NLP\nmodels.", "published": "2023-06-08 02:07:49", "link": "http://arxiv.org/abs/2306.04874v1", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "covLLM: Large Language Models for COVID-19 Biomedical Literature", "abstract": "The COVID-19 pandemic led to 1.1 million deaths in the United States, despite\nthe explosion of coronavirus research. These new findings are slow to translate\nto clinical interventions, leading to poorer patient outcomes and unnecessary\ndeaths. One reason is that clinicians, overwhelmed by patients, struggle to\nkeep pace with the rate of new coronavirus literature. A potential solution is\ndeveloping a tool for evaluating coronavirus literature using large language\nmodels (LLMs) -- neural networks that are deployed for natural language\nprocessing. LLMs can be used to summarize and extract user-specified\ninformation. The greater availability and advancement of LLMs and pre-processed\ncoronavirus literature databases provide the opportunity to assist clinicians\nin evaluating coronavirus literature through a coronavirus literature specific\nLLM (covLLM), a tool that directly takes an inputted research article and a\nuser query to return an answer. Using the COVID-19 Open Research Dataset\n(CORD-19), we produced two datasets: (1) synCovid, which uses a combination of\nhandwritten prompts and synthetic prompts generated using OpenAI, and (2) real\nabstracts, which contains abstract and title pairs. covLLM was trained with\nLLaMA 7B as a baseline model to produce three models trained on (1) the Alpaca\nand synCovid datasets, (2) the synCovid dataset, and (3) the synCovid and real\nabstract datasets. These models were evaluated by two human evaluators and\nChatGPT. Results demonstrate that training covLLM on the synCovid and abstract\npairs datasets performs competitively with ChatGPT and outperforms covLLM\ntrained primarily using the Alpaca dataset.", "published": "2023-06-08 04:08:32", "link": "http://arxiv.org/abs/2306.04926v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "InfoPrompt: Information-Theoretic Soft Prompt Tuning for Natural\n  Language Understanding", "abstract": "Soft prompt tuning achieves superior performances across a wide range of\nfew-shot tasks. However, the performances of prompt tuning can be highly\nsensitive to the initialization of the prompts. We also empirically observe\nthat conventional prompt tuning methods cannot encode and learn sufficient\ntask-relevant information from prompt tokens. In this work, we develop an\ninformation-theoretic framework that formulates soft prompt tuning as\nmaximizing mutual information between prompts and other model parameters (or\nencoded representations). This novel view helps us to develop a more efficient,\naccurate and robust soft prompt tuning method InfoPrompt. With this framework,\nwe develop two novel mutual information based loss functions, to (i) discover\nproper prompt initialization for the downstream tasks and learn sufficient\ntask-relevant information from prompt tokens and (ii) encourage the output\nrepresentation from the pretrained language model to be more aware of the\ntask-relevant information captured in the learnt prompt. Extensive experiments\nvalidate that InfoPrompt can significantly accelerate the convergence of the\nprompt tuning and outperform traditional prompt tuning methods. Finally, we\nprovide a formal theoretical result for showing to show that gradient descent\ntype algorithm can be used to train our mutual information loss.", "published": "2023-06-08 04:31:48", "link": "http://arxiv.org/abs/2306.04933v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Assessing Phrase Break of ESL Speech with Pre-trained Language Models\n  and Large Language Models", "abstract": "This work introduces approaches to assessing phrase breaks in ESL learners'\nspeech using pre-trained language models (PLMs) and large language models\n(LLMs). There are two tasks: overall assessment of phrase break for a speech\nclip and fine-grained assessment of every possible phrase break position. To\nleverage NLP models, speech input is first force-aligned with texts, and then\npre-processed into a token sequence, including words and phrase break\ninformation. To utilize PLMs, we propose a pre-training and fine-tuning\npipeline with the processed tokens. This process includes pre-training with a\nreplaced break token detection module and fine-tuning with text classification\nand sequence labeling. To employ LLMs, we design prompts for ChatGPT. The\nexperiments show that with the PLMs, the dependence on labeled training data\nhas been greatly reduced, and the performance has improved. Meanwhile, we\nverify that ChatGPT, a renowned LLM, has potential for further advancement in\nthis area.", "published": "2023-06-08 07:10:39", "link": "http://arxiv.org/abs/2306.04980v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Interpretable Medical Diagnostics with Structured Data Extraction by\n  Large Language Models", "abstract": "Tabular data is often hidden in text, particularly in medical diagnostic\nreports. Traditional machine learning (ML) models designed to work with tabular\ndata, cannot effectively process information in such form. On the other hand,\nlarge language models (LLMs) which excel at textual tasks, are probably not the\nbest tool for modeling tabular data. Therefore, we propose a novel, simple, and\neffective methodology for extracting structured tabular data from textual\nmedical reports, called TEMED-LLM. Drawing upon the reasoning capabilities of\nLLMs, TEMED-LLM goes beyond traditional extraction techniques, accurately\ninferring tabular features, even when their names are not explicitly mentioned\nin the text. This is achieved by combining domain-specific reasoning guidelines\nwith a proposed data validation and reasoning correction feedback loop. By\napplying interpretable ML models such as decision trees and logistic regression\nover the extracted and validated data, we obtain end-to-end interpretable\npredictions. We demonstrate that our approach significantly outperforms\nstate-of-the-art text classification models in medical diagnostics. Given its\npredictive performance, simplicity, and interpretability, TEMED-LLM underscores\nthe potential of leveraging LLMs to improve the performance and trustworthiness\nof ML models in medical applications.", "published": "2023-06-08 09:12:28", "link": "http://arxiv.org/abs/2306.05052v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Improving Language Model Integration for Neural Machine Translation", "abstract": "The integration of language models for neural machine translation has been\nextensively studied in the past. It has been shown that an external language\nmodel, trained on additional target-side monolingual data, can help improve\ntranslation quality. However, there has always been the assumption that the\ntranslation model also learns an implicit target-side language model during\ntraining, which interferes with the external language model at decoding time.\nRecently, some works on automatic speech recognition have demonstrated that, if\nthe implicit language model is neutralized in decoding, further improvements\ncan be gained when integrating an external language model. In this work, we\ntransfer this concept to the task of machine translation and compare with the\nmost prominent way of including additional monolingual data - namely\nback-translation. We find that accounting for the implicit language model\nsignificantly boosts the performance of language model fusion, although this\napproach is still outperformed by back-translation.", "published": "2023-06-08 10:00:19", "link": "http://arxiv.org/abs/2306.05077v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Enhancing Robustness of AI Offensive Code Generators via Data\n  Augmentation", "abstract": "Since manually writing software exploits for offensive security is\ntime-consuming and requires expert knowledge, AI-base code generators are an\nattractive solution to enhance security analysts' productivity by automatically\ncrafting exploits for security testing. However, the variability in the natural\nlanguage and technical skills used to describe offensive code poses unique\nchallenges to their robustness and applicability. In this work, we present a\nmethod to add perturbations to the code descriptions to create new inputs in\nnatural language (NL) from well-intentioned developers that diverge from the\noriginal ones due to the use of new words or because they miss part of them.\nThe goal is to analyze how and to what extent perturbations affect the\nperformance of AI code generators in the context of offensive code. First, we\nshow that perturbed descriptions preserve the semantics of the original,\nnon-perturbed ones. Then, we use the method to assess the robustness of three\nstate-of-the-art code generators against the newly perturbed inputs, showing\nthat the performance of these AI-based solutions is highly affected by\nperturbations in the NL descriptions. To enhance their robustness, we use the\nmethod to perform data augmentation, i.e., to increase the variability and\ndiversity of the NL descriptions in the training data, proving its\neffectiveness against both perturbed and non-perturbed code descriptions.", "published": "2023-06-08 10:02:04", "link": "http://arxiv.org/abs/2306.05079v3", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "The ART of Conversation: Measuring Phonetic Convergence and Deliberate\n  Imitation in L2-Speech with a Siamese RNN", "abstract": "Phonetic convergence describes the automatic and unconscious speech\nadaptation of two interlocutors in a conversation. This paper proposes a\nSiamese recurrent neural network (RNN) architecture to measure the convergence\nof the holistic spectral characteristics of speech sounds in an L2-L2\ninteraction. We extend an alternating reading task (the ART) dataset by adding\n20 native Slovak L2 English speakers. We train and test the Siamese RNN model\nto measure phonetic convergence of L2 English speech from three different\nnative language groups: Italian (9 dyads), French (10 dyads) and Slovak (10\ndyads). Our results indicate that the Siamese RNN model effectively captures\nthe dynamics of phonetic convergence and the speaker's imitation ability.\nMoreover, this text-independent model is scalable and capable of handling\nL1-induced speaker variability.", "published": "2023-06-08 10:42:44", "link": "http://arxiv.org/abs/2306.05088v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "On Search Strategies for Document-Level Neural Machine Translation", "abstract": "Compared to sentence-level systems, document-level neural machine translation\n(NMT) models produce a more consistent output across a document and are able to\nbetter resolve ambiguities within the input. There are many works on\ndocument-level NMT, mostly focusing on modifying the model architecture or\ntraining strategy to better accommodate the additional context-input. On the\nother hand, in most works, the question on how to perform search with the\ntrained model is scarcely discussed, sometimes not mentioned at all. In this\nwork, we aim to answer the question how to best utilize a context-aware\ntranslation model in decoding. We start with the most popular document-level\nNMT approach and compare different decoding schemes, some from the literature\nand others proposed by us. In the comparison, we are using both, standard\nautomatic metrics, as well as specific linguistic phenomena on three standard\ndocument-level translation benchmarks. We find that most commonly used decoding\nstrategies perform similar to each other and that higher quality context\ninformation has the potential to further improve the translation.", "published": "2023-06-08 11:30:43", "link": "http://arxiv.org/abs/2306.05116v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Long Context Document-Level Machine Translation", "abstract": "Document-level context for neural machine translation (NMT) is crucial to\nimprove the translation consistency and cohesion, the translation of ambiguous\ninputs, as well as several other linguistic phenomena. Many works have been\npublished on the topic of document-level NMT, but most restrict the system to\nonly local context, typically including just the one or two preceding sentences\nas additional information. This might be enough to resolve some ambiguous\ninputs, but it is probably not sufficient to capture some document-level\ninformation like the topic or style of a conversation. When increasing the\ncontext size beyond just the local context, there are two challenges: (i)\nthe~memory usage increases exponentially (ii) the translation performance\nstarts to degrade. We argue that the widely-used attention mechanism is\nresponsible for both issues. Therefore, we propose a constrained attention\nvariant that focuses the attention on the most relevant parts of the sequence,\nwhile simultaneously reducing the memory consumption. For evaluation, we\nutilize targeted test sets in combination with novel evaluation techniques to\nanalyze the translations in regards to specific discourse-related phenomena. We\nfind that our approach is a good compromise between sentence-level NMT vs\nattending to the full context, especially in low resource scenarios.", "published": "2023-06-08 13:28:48", "link": "http://arxiv.org/abs/2306.05183v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Dealing with Semantic Underspecification in Multimodal NLP", "abstract": "Intelligent systems that aim at mastering language as humans do must deal\nwith its semantic underspecification, namely, the possibility for a linguistic\nsignal to convey only part of the information needed for communication to\nsucceed. Consider the usages of the pronoun they, which can leave the gender\nand number of its referent(s) underspecified. Semantic underspecification is\nnot a bug but a crucial language feature that boosts its storage and processing\nefficiency. Indeed, human speakers can quickly and effortlessly integrate\nsemantically-underspecified linguistic signals with a wide range of\nnon-linguistic information, e.g., the multimodal context, social or cultural\nconventions, and shared knowledge. Standard NLP models have, in principle, no\nor limited access to such extra information, while multimodal systems grounding\nlanguage into other modalities, such as vision, are naturally equipped to\naccount for this phenomenon. However, we show that they struggle with it, which\ncould negatively affect their performance and lead to harmful consequences when\nused for applications. In this position paper, we argue that our community\nshould be aware of semantic underspecification if it aims to develop language\ntechnology that can successfully interact with human users. We discuss some\napplications where mastering it is crucial and outline a few directions toward\nachieving this goal.", "published": "2023-06-08 14:39:24", "link": "http://arxiv.org/abs/2306.05240v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Are fairness metric scores enough to assess discrimination biases in\n  machine learning?", "abstract": "This paper presents novel experiments shedding light on the shortcomings of\ncurrent metrics for assessing biases of gender discrimination made by machine\nlearning algorithms on textual data. We focus on the Bios dataset, and our\nlearning task is to predict the occupation of individuals, based on their\nbiography. Such prediction tasks are common in commercial Natural Language\nProcessing (NLP) applications such as automatic job recommendations. We address\nan important limitation of theoretical discussions dealing with group-wise\nfairness metrics: they focus on large datasets, although the norm in many\nindustrial NLP applications is to use small to reasonably large linguistic\ndatasets for which the main practical constraint is to get a good prediction\naccuracy. We then question how reliable are different popular measures of bias\nwhen the size of the training set is simply sufficient to learn reasonably\naccurate predictions. Our experiments sample the Bios dataset and learn more\nthan 200 models on different sample sizes. This allows us to statistically\nstudy our results and to confirm that common gender bias indices provide\ndiverging and sometimes unreliable results when applied to relatively small\ntraining and test samples. This highlights the crucial importance of variance\ncalculations for providing sound results in this field.", "published": "2023-06-08 15:56:57", "link": "http://arxiv.org/abs/2306.05307v1", "categories": ["cs.CL", "cs.CY", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Advancing Italian Biomedical Information Extraction with\n  Transformers-based Models: Methodological Insights and Multicenter Practical\n  Application", "abstract": "The introduction of computerized medical records in hospitals has reduced\nburdensome activities like manual writing and information fetching. However,\nthe data contained in medical records are still far underutilized, primarily\nbecause extracting data from unstructured textual medical records takes time\nand effort. Information Extraction, a subfield of Natural Language Processing,\ncan help clinical practitioners overcome this limitation by using automated\ntext-mining pipelines. In this work, we created the first Italian\nneuropsychiatric Named Entity Recognition dataset, PsyNIT, and used it to\ndevelop a Transformers-based model. Moreover, we collected and leveraged three\nexternal independent datasets to implement an effective multicenter model, with\noverall F1-score 84.77%, Precision 83.16%, Recall 86.44%. The lessons learned\nare: (i) the crucial role of a consistent annotation process and (ii) a\nfine-tuning strategy that combines classical methods with a \"low-resource\"\napproach. This allowed us to establish methodological guidelines that pave the\nway for Natural Language Processing studies in less-resourced languages.", "published": "2023-06-08 16:15:46", "link": "http://arxiv.org/abs/2306.05323v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; J.3"], "primary_category": "cs.CL"}
{"title": "The ADAIO System at the BEA-2023 Shared Task on Generating AI Teacher\n  Responses in Educational Dialogues", "abstract": "This paper presents the ADAIO team's system entry in the Building Educational\nApplications (BEA) 2023 Shared Task on Generating AI Teacher Responses in\nEducational Dialogues. The task aims to assess the performance of\nstate-of-the-art generative models as AI teachers in producing suitable\nresponses within a student-teacher dialogue. Our system comprises evaluating\nvarious baseline models using OpenAI GPT-3 and designing diverse prompts to\nprompt the OpenAI models for teacher response generation. After the challenge,\nour system achieved second place by employing a few-shot prompt-based approach\nwith the OpenAI text-davinci-003 model. The results highlight the few-shot\nlearning capabilities of large-language models, particularly OpenAI's GPT-3, in\nthe role of AI teachers.", "published": "2023-06-08 17:05:38", "link": "http://arxiv.org/abs/2306.05360v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "MIMIC-IT: Multi-Modal In-Context Instruction Tuning", "abstract": "High-quality instructions and responses are essential for the zero-shot\nperformance of large language models on interactive natural language tasks. For\ninteractive vision-language tasks involving intricate visual scenes, a large\nquantity of diverse and creative instruction-response pairs should be\nimperative to tune vision-language models (VLMs). Nevertheless, the current\navailability of vision-language instruction-response pairs in terms of\nquantity, diversity, and creativity remains limited, posing challenges to the\ngeneralization of interactive VLMs. Here we present MultI-Modal In-Context\nInstruction Tuning (MIMIC-IT), a dataset comprising 2.8 million multimodal\ninstruction-response pairs, with 2.2 million unique instructions derived from\nimages and videos. Each pair is accompanied by multi-modal in-context\ninformation, forming conversational contexts aimed at empowering VLMs in\nperception, reasoning, and planning. The instruction-response collection\nprocess, dubbed as Syphus, is scaled using an automatic annotation pipeline\nthat combines human expertise with GPT's capabilities. Using the MIMIC-IT\ndataset, we train a large VLM named Otter. Based on extensive evaluations\nconducted on vision-language benchmarks, it has been observed that Otter\ndemonstrates remarkable proficiency in multi-modal perception, reasoning, and\nin-context learning. Human evaluation reveals it effectively aligns with the\nuser's intentions. We release the MIMIC-IT dataset, instruction-response\ncollection pipeline, benchmarks, and the Otter model.", "published": "2023-06-08 17:59:56", "link": "http://arxiv.org/abs/2306.05425v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.CV"}
{"title": "Latent Phrase Matching for Dysarthric Speech", "abstract": "Many consumer speech recognition systems are not tuned for people with speech\ndisabilities, resulting in poor recognition and user experience, especially for\nsevere speech differences. Recent studies have emphasized interest in\npersonalized speech models from people with atypical speech patterns. We\npropose a query-by-example-based personalized phrase recognition system that is\ntrained using small amounts of speech, is language agnostic, does not assume a\ntraditional pronunciation lexicon, and generalizes well across speech\ndifference severities. On an internal dataset collected from 32 people with\ndysarthria, this approach works regardless of severity and shows a 60%\nimprovement in recall relative to a commercial speech recognition system. On\nthe public EasyCall dataset of dysarthric speech, our approach improves\naccuracy by 30.5%. Performance degrades as the number of phrases increases, but\nconsistently outperforms ASR systems when trained with 50 unique phrases.", "published": "2023-06-08 17:28:28", "link": "http://arxiv.org/abs/2306.05446v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Prompt Injection attack against LLM-integrated Applications", "abstract": "Large Language Models (LLMs), renowned for their superior proficiency in\nlanguage comprehension and generation, stimulate a vibrant ecosystem of\napplications around them. However, their extensive assimilation into various\nservices introduces significant security risks. This study deconstructs the\ncomplexities and implications of prompt injection attacks on actual\nLLM-integrated applications. Initially, we conduct an exploratory analysis on\nten commercial applications, highlighting the constraints of current attack\nstrategies in practice. Prompted by these limitations, we subsequently\nformulate HouYi, a novel black-box prompt injection attack technique, which\ndraws inspiration from traditional web injection attacks. HouYi is\ncompartmentalized into three crucial elements: a seamlessly-incorporated\npre-constructed prompt, an injection prompt inducing context partition, and a\nmalicious payload designed to fulfill the attack objectives. Leveraging HouYi,\nwe unveil previously unknown and severe attack outcomes, such as unrestricted\narbitrary LLM usage and uncomplicated application prompt theft. We deploy HouYi\non 36 actual LLM-integrated applications and discern 31 applications\nsusceptible to prompt injection. 10 vendors have validated our discoveries,\nincluding Notion, which has the potential to impact millions of users. Our\ninvestigation illuminates both the possible risks of prompt injection attacks\nand the possible tactics for mitigation.", "published": "2023-06-08 18:43:11", "link": "http://arxiv.org/abs/2306.05499v2", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.SE"], "primary_category": "cs.CR"}
{"title": "Bias Against 93 Stigmatized Groups in Masked Language Models and\n  Downstream Sentiment Classification Tasks", "abstract": "The rapid deployment of artificial intelligence (AI) models demands a\nthorough investigation of biases and risks inherent in these models to\nunderstand their impact on individuals and society. This study extends the\nfocus of bias evaluation in extant work by examining bias against social\nstigmas on a large scale. It focuses on 93 stigmatized groups in the United\nStates, including a wide range of conditions related to disease, disability,\ndrug use, mental illness, religion, sexuality, socioeconomic status, and other\nrelevant factors. We investigate bias against these groups in English\npre-trained Masked Language Models (MLMs) and their downstream sentiment\nclassification tasks. To evaluate the presence of bias against 93 stigmatized\nconditions, we identify 29 non-stigmatized conditions to conduct a comparative\nanalysis. Building upon a psychology scale of social rejection, the Social\nDistance Scale, we prompt six MLMs: RoBERTa-base, RoBERTa-large, XLNet-large,\nBERTweet-base, BERTweet-large, and DistilBERT. We use human annotations to\nanalyze the predicted words from these models, with which we measure the extent\nof bias against stigmatized groups. When prompts include stigmatized\nconditions, the probability of MLMs predicting negative words is approximately\n20 percent higher than when prompts have non-stigmatized conditions. In the\nsentiment classification tasks, when sentences include stigmatized conditions\nrelated to diseases, disability, education, and mental illness, they are more\nlikely to be classified as negative. We also observe a strong correlation\nbetween bias in MLMs and their downstream sentiment classifiers (r =0.79). The\nevidence indicates that MLMs and their downstream sentiment classification\ntasks exhibit biases against socially stigmatized groups.", "published": "2023-06-08 20:46:09", "link": "http://arxiv.org/abs/2306.05550v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG", "K.4; I.2.7; I.2.0"], "primary_category": "cs.CY"}
{"title": "Speech-to-Text Adapter and Speech-to-Entity Retriever Augmented LLMs for\n  Speech Understanding", "abstract": "Large Language Models (LLMs) have been applied in the speech domain, often\nincurring a performance drop due to misaligned between speech and language\nrepresentations. To bridge this gap, we propose a joint speech and language\nmodel (SLM) using a Speech2Text adapter, which maps speech into text token\nembedding space without speech information loss. Additionally, using a\nCTC-based blank-filtering, we can reduce the speech sequence length to that of\ntext. In speech MultiWoz dataset (DSTC11 challenge), SLM largely improves the\ndialog state tracking (DST) performance (24.7% to 28.4% accuracy). Further to\naddress errors on rare entities, we augment SLM with a Speech2Entity retriever,\nwhich uses speech to retrieve relevant entities, and then adds them to the\noriginal SLM input as a prefix. With this retrieval-augmented SLM (ReSLM), the\nDST performance jumps to 34.6% accuracy. Moreover, augmenting the ASR task with\nthe dialog understanding task improves the ASR performance from 9.4% to 8.5%\nWER.", "published": "2023-06-08 22:33:22", "link": "http://arxiv.org/abs/2306.07944v1", "categories": ["eess.AS", "cs.AI", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Factorized Contrastive Learning: Going Beyond Multi-view Redundancy", "abstract": "In a wide range of multimodal tasks, contrastive learning has become a\nparticularly appealing approach since it can successfully learn representations\nfrom abundant unlabeled data with only pairing information (e.g., image-caption\nor video-audio pairs). Underpinning these approaches is the assumption of\nmulti-view redundancy - that shared information between modalities is necessary\nand sufficient for downstream tasks. However, in many real-world settings,\ntask-relevant information is also contained in modality-unique regions:\ninformation that is only present in one modality but still relevant to the\ntask. How can we learn self-supervised multimodal representations to capture\nboth shared and unique information relevant to downstream tasks? This paper\nproposes FactorCL, a new multimodal representation learning method to go beyond\nmulti-view redundancy. FactorCL is built from three new contributions: (1)\nfactorizing task-relevant information into shared and unique representations,\n(2) capturing task-relevant information via maximizing MI lower bounds and\nremoving task-irrelevant information via minimizing MI upper bounds, and (3)\nmultimodal data augmentations to approximate task relevance without labels. On\nlarge-scale real-world datasets, FactorCL captures both shared and unique\ninformation and achieves state-of-the-art results on six benchmarks", "published": "2023-06-08 15:17:04", "link": "http://arxiv.org/abs/2306.05268v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.MM"], "primary_category": "cs.LG"}
{"title": "Convolutional Recurrent Neural Network with Attention for 3D Speech\n  Enhancement", "abstract": "3D speech enhancement can effectively improve the auditory experience and\nplays a crucial role in augmented reality technology. However, traditional\nconvolutional-based speech enhancement methods have limitations in extracting\ndynamic voice information. In this paper, we incorporate a dual-path recurrent\nneural network block into the U-Net to iteratively extract dynamic audio\ninformation in both the time and frequency domains. And an attention mechanism\nis proposed to fuse the original signal, reference signal, and generated masks.\nMoreover, we introduce a loss function to simultaneously optimize the network\nin the time-frequency and time domains. Experimental results show that our\nsystem outperforms the state-of-the-art systems on the dataset of ICASSP\nL3DAS23 challenge.", "published": "2023-06-08 07:19:14", "link": "http://arxiv.org/abs/2306.04987v4", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "PEFT-SER: On the Use of Parameter Efficient Transfer Learning Approaches\n  For Speech Emotion Recognition Using Pre-trained Speech Models", "abstract": "Many recent studies have focused on fine-tuning pre-trained models for speech\nemotion recognition (SER), resulting in promising performance compared to\ntraditional methods that rely largely on low-level, knowledge-inspired acoustic\nfeatures. These pre-trained speech models learn general-purpose speech\nrepresentations using self-supervised or weakly-supervised learning objectives\nfrom large-scale datasets. Despite the significant advances made in SER through\nthe use of pre-trained architecture, fine-tuning these large pre-trained models\nfor different datasets requires saving copies of entire weight parameters,\nrendering them impractical to deploy in real-world settings. As an alternative,\nthis work explores parameter-efficient fine-tuning (PEFT) approaches for\nadapting pre-trained speech models for emotion recognition. Specifically, we\nevaluate the efficacy of adapter tuning, embedding prompt tuning, and LoRa\n(Low-rank approximation) on four popular SER testbeds. Our results reveal that\nLoRa achieves the best fine-tuning performance in emotion recognition while\nenhancing fairness and requiring only a minimal extra amount of weight\nparameters. Furthermore, our findings offer novel insights into future research\ndirections in SER, distinct from existing approaches focusing on directly\nfine-tuning the model architecture. Our code is publicly available under:\nhttps://github.com/usc-sail/peft-ser.", "published": "2023-06-08 16:53:02", "link": "http://arxiv.org/abs/2306.05350v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Adaptive Fake Audio Detection with Low-Rank Model Squeezing", "abstract": "The rapid advancement of spoofing algorithms necessitates the development of\nrobust detection methods capable of accurately identifying emerging fake audio.\nTraditional approaches, such as finetuning on new datasets containing these\nnovel spoofing algorithms, are computationally intensive and pose a risk of\nimpairing the acquired knowledge of known fake audio types. To address these\nchallenges, this paper proposes an innovative approach that mitigates the\nlimitations associated with finetuning. We introduce the concept of training\nlow-rank adaptation matrices tailored specifically to the newly emerging fake\naudio types. During the inference stage, these adaptation matrices are combined\nwith the existing model to generate the final prediction output. Extensive\nexperimentation is conducted to evaluate the efficacy of the proposed method.\nThe results demonstrate that our approach effectively preserves the prediction\naccuracy of the existing model for known fake audio types. Furthermore, our\napproach offers several advantages, including reduced storage memory\nrequirements and lower equal error rates compared to conventional finetuning\nmethods, particularly on specific spoofing algorithms.", "published": "2023-06-08 06:06:42", "link": "http://arxiv.org/abs/2306.04956v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "VIFS: An End-to-End Variational Inference for Foley Sound Synthesis", "abstract": "The goal of DCASE 2023 Challenge Task 7 is to generate various sound clips\nfor Foley sound synthesis (FSS) by \"category-to-sound\" approach. \"Category\" is\nexpressed by a single index while corresponding \"sound\" covers diverse and\ndifferent sound examples. To generate diverse sounds for a given category, we\nadopt VITS, a text-to-speech (TTS) model with variational inference. In\naddition, we apply various techniques from speech synthesis including PhaseAug\nand Avocodo. Different from TTS models which generate short pronunciation from\nphonemes and speaker identity, the category-to-sound problem requires\ngenerating diverse sounds just from a category index. To compensate for the\ndifference while maintaining consistency within each audio clip, we heavily\nmodified the prior encoder to enhance consistency with posterior latent\nvariables. This introduced additional Gaussian on the prior encoder which\npromotes variance within the category. With these modifications, we propose\nVIFS, variational inference for end-to-end Foley sound synthesis, which\ngenerates diverse high-quality sounds.", "published": "2023-06-08 07:48:01", "link": "http://arxiv.org/abs/2306.05004v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Matching Latent Encoding for Audio-Text based Keyword Spotting", "abstract": "Using audio and text embeddings jointly for Keyword Spotting (KWS) has shown\nhigh-quality results, but the key challenge of how to semantically align two\nembeddings for multi-word keywords of different sequence lengths remains\nlargely unsolved. In this paper, we propose an audio-text-based end-to-end\nmodel architecture for flexible keyword spotting (KWS), which builds upon\nlearned audio and text embeddings. Our architecture uses a novel dynamic\nprogramming-based algorithm, Dynamic Sequence Partitioning (DSP), to optimally\npartition the audio sequence into the same length as the word-based text\nsequence using the monotonic alignment of spoken content. Our proposed model\nconsists of an encoder block to get audio and text embeddings, a projector\nblock to project individual embeddings to a common latent space, and an\naudio-text aligner containing a novel DSP algorithm, which aligns the audio and\ntext embeddings to determine if the spoken content is the same as the text.\nExperimental results show that our DSP is more effective than other\npartitioning schemes, and the proposed architecture outperformed the\nstate-of-the-art results on the public dataset in terms of Area Under the ROC\nCurve (AUC) and Equal-Error-Rate (EER) by 14.4 % and 28.9%, respectively.", "published": "2023-06-08 14:44:23", "link": "http://arxiv.org/abs/2306.05245v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Simple and Controllable Music Generation", "abstract": "We tackle the task of conditional music generation. We introduce MusicGen, a\nsingle Language Model (LM) that operates over several streams of compressed\ndiscrete music representation, i.e., tokens. Unlike prior work, MusicGen is\ncomprised of a single-stage transformer LM together with efficient token\ninterleaving patterns, which eliminates the need for cascading several models,\ne.g., hierarchically or upsampling. Following this approach, we demonstrate how\nMusicGen can generate high-quality samples, both mono and stereo, while being\nconditioned on textual description or melodic features, allowing better\ncontrols over the generated output. We conduct extensive empirical evaluation,\nconsidering both automatic and human studies, showing the proposed approach is\nsuperior to the evaluated baselines on a standard text-to-music benchmark.\nThrough ablation studies, we shed light over the importance of each of the\ncomponents comprising MusicGen. Music samples, code, and models are available\nat https://github.com/facebookresearch/audiocraft", "published": "2023-06-08 15:31:05", "link": "http://arxiv.org/abs/2306.05284v3", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
