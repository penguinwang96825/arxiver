{"title": "Multi-Source Neural Translation", "abstract": "We build a multi-source machine translation model and train it to maximize\nthe probability of a target English string given French and German sources.\nUsing the neural encoder-decoder framework, we explore several combination\nmethods and report up to +4.8 Bleu increases on top of a very strong\nattention-based neural translation model.", "published": "2016-01-05 00:49:22", "link": "http://arxiv.org/abs/1601.00710v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Role of Context Types and Dimensionality in Learning Word Embeddings", "abstract": "We provide the first extensive evaluation of how using different types of\ncontext to learn skip-gram word embeddings affects performance on a wide range\nof intrinsic and extrinsic NLP tasks. Our results suggest that while intrinsic\ntasks tend to exhibit a clear preference to particular types of contexts and\nhigher dimensionality, more careful tuning is required for finding the optimal\nsettings for most of the extrinsic tasks that we considered. Furthermore, for\nthese extrinsic tasks, we find that once the benefit from increasing the\nembedding dimensionality is mostly exhausted, simple concatenation of word\nembeddings, learned with different context types, can yield further performance\ngains. As an additional contribution, we propose a new variant of the skip-gram\nmodel that learns word embeddings from weighted contexts of substitute words.", "published": "2016-01-05 16:28:42", "link": "http://arxiv.org/abs/1601.00893v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "End-to-End Relation Extraction using LSTMs on Sequences and Tree\n  Structures", "abstract": "We present a novel end-to-end neural model to extract entities and relations\nbetween them. Our recurrent neural network based model captures both word\nsequence and dependency tree substructure information by stacking bidirectional\ntree-structured LSTM-RNNs on bidirectional sequential LSTM-RNNs. This allows\nour model to jointly represent both entities and relations with shared\nparameters in a single model. We further encourage detection of entities during\ntraining and use of entity information in relation extraction via entity\npretraining and scheduled sampling. Our model improves over the\nstate-of-the-art feature-based model on end-to-end relation extraction,\nachieving 12.1% and 5.7% relative error reductions in F1-score on ACE2005 and\nACE2004, respectively. We also show that our LSTM-RNN based model compares\nfavorably to the state-of-the-art CNN based model (in F1-score) on nominal\nrelation classification (SemEval-2010 Task 8). Finally, we present an extensive\nablation analysis of several model components.", "published": "2016-01-05 08:53:05", "link": "http://arxiv.org/abs/1601.00770v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Joint learning of ontology and semantic parser from text", "abstract": "Semantic parsing methods are used for capturing and representing semantic\nmeaning of text. Meaning representation capturing all the concepts in the text\nmay not always be available or may not be sufficiently complete. Ontologies\nprovide a structured and reasoning-capable way to model the content of a\ncollection of texts. In this work, we present a novel approach to joint\nlearning of ontology and semantic parser from text. The method is based on\nsemi-automatic induction of a context-free grammar from semantically annotated\ntext. The grammar parses the text into semantic trees. Both, the grammar and\nthe semantic trees are used to learn the ontology on several levels -- classes,\ninstances, taxonomic and non-taxonomic relations. The approach was evaluated on\nthe first sentences of Wikipedia pages describing people.", "published": "2016-01-05 16:56:28", "link": "http://arxiv.org/abs/1601.00901v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Open challenges in understanding development and evolution of speech\n  forms: The roles of embodied self-organization, motivation and active\n  exploration", "abstract": "This article discusses open scientific challenges for understanding\ndevelopment and evolution of speech forms, as a commentary to Moulin-Frier et\nal. (Moulin-Frier et al., 2015). Based on the analysis of mathematical models\nof the origins of speech forms, with a focus on their assumptions , we study\nthe fundamental question of how speech can be formed out of non--speech, at\nboth developmental and evolutionary scales. In particular, we emphasize the\nimportance of embodied self-organization , as well as the role of mechanisms of\nmotivation and active curiosity-driven exploration in speech formation. Finally\n, we discuss an evolutionary-developmental perspective of the origins of\nspeech.", "published": "2016-01-05 12:50:14", "link": "http://arxiv.org/abs/1601.00816v1", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.AI"}
