{"title": "What Does This Acronym Mean? Introducing a New Dataset for Acronym\n  Identification and Disambiguation", "abstract": "Acronyms are the short forms of phrases that facilitate conveying lengthy\nsentences in documents and serve as one of the mainstays of writing. Due to\ntheir importance, identifying acronyms and corresponding phrases (i.e., acronym\nidentification (AI)) and finding the correct meaning of each acronym (i.e.,\nacronym disambiguation (AD)) are crucial for text understanding. Despite the\nrecent progress on this task, there are some limitations in the existing\ndatasets which hinder further improvement. More specifically, limited size of\nmanually annotated AI datasets or noises in the automatically created acronym\nidentification datasets obstruct designing advanced high-performing acronym\nidentification models. Moreover, the existing datasets are mostly limited to\nthe medical domain and ignore other domains. In order to address these two\nlimitations, we first create a manually annotated large AI dataset for\nscientific domain. This dataset contains 17,506 sentences which is\nsubstantially larger than previous scientific AI datasets. Next, we prepare an\nAD dataset for scientific domain with 62,441 samples which is significantly\nlarger than the previous scientific AD dataset. Our experiments show that the\nexisting state-of-the-art models fall far behind human-level performance on\nboth datasets proposed by this work. In addition, we propose a new deep\nlearning model that utilizes the syntactical structure of the sentence to\nexpand an ambiguous acronym in a sentence. The proposed model outperforms the\nstate-of-the-art models on the new AD dataset, providing a strong baseline for\nfuture research on this dataset.", "published": "2020-10-28 00:12:36", "link": "http://arxiv.org/abs/2010.14678v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Character Entropy in Modern and Historical Texts: Comparison Metrics for\n  an Undeciphered Manuscript", "abstract": "This paper outlines the creation of three corpora for multilingual comparison\nand analysis of the Voynich manuscript: a corpus of Voynich texts partitioned\nby Currier language, scribal hand, and transcription system, a corpus of 294\nlanguage samples compiled from Wikipedia, and a corpus of eighteen transcribed\nhistorical texts in eight languages. These corpora will be utilized in\nsubsequent work by the Voynich Working Group at Yale University.\n  We demonstrate the utility of these corpora for studying characteristics of\nthe Voynich script and language, with an analysis of conditional character\nentropy in Voynichese. We discuss the interaction between character entropy and\nlanguage, script size and type, glyph compositionality, scribal conventions and\nabbreviations, positional character variants, and bigram frequency.\n  This analysis characterizes the interaction between script compositionality,\ncharacter size, and predictability. We show that substantial manipulations of\nglyph composition are not sufficient to align conditional entropy levels with\nnatural languages. The unusually predictable nature of the Voynichese script is\nnot attributable to a particular script or transcription system, underlying\nlanguage, or substitution cipher. Voynichese is distinct from every comparison\ntext in our corpora because character placement is highly constrained within\nthe word, and this may indicate the loss of phonemic distinctions from the\nunderlying language.", "published": "2020-10-28 01:53:59", "link": "http://arxiv.org/abs/2010.14697v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Second-Order Unsupervised Neural Dependency Parsing", "abstract": "Most of the unsupervised dependency parsers are based on first-order\nprobabilistic generative models that only consider local parent-child\ninformation. Inspired by second-order supervised dependency parsing, we\nproposed a second-order extension of unsupervised neural dependency models that\nincorporate grandparent-child or sibling information. We also propose a novel\ndesign of the neural parameterization and optimization methods of the\ndependency models. In second-order models, the number of grammar rules grows\ncubically with the increase of vocabulary size, making it difficult to train\nlexicalized models that may contain thousands of words. To circumvent this\nproblem while still benefiting from both second-order parsing and\nlexicalization, we use the agreement-based learning framework to jointly train\na second-order unlexicalized model and a first-order lexicalized model.\nExperiments on multiple datasets show the effectiveness of our second-order\nmodels compared with recent state-of-the-art methods. Our joint model achieves\na 10% improvement over the previous state-of-the-art parser on the full WSJ\ntest set", "published": "2020-10-28 03:01:33", "link": "http://arxiv.org/abs/2010.14720v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Volctrans Machine Translation System for WMT20", "abstract": "This paper describes our VolcTrans system on WMT20 shared news translation\ntask. We participated in 8 translation directions. Our basic systems are based\non Transformer, with several variants (wider or deeper Transformers, dynamic\nconvolutions). The final system includes text pre-process, data selection,\nsynthetic data generation, advanced model ensemble, and multilingual\npre-training.", "published": "2020-10-28 08:08:12", "link": "http://arxiv.org/abs/2010.14806v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bridging the Modality Gap for Speech-to-Text Translation", "abstract": "End-to-end speech translation aims to translate speech in one language into\ntext in another language via an end-to-end way. Most existing methods employ an\nencoder-decoder structure with a single encoder to learn acoustic\nrepresentation and semantic information simultaneously, which ignores the\nspeech-and-text modality differences and makes the encoder overloaded, leading\nto great difficulty in learning such a model. To address these issues, we\npropose a Speech-to-Text Adaptation for Speech Translation (STAST) model which\naims to improve the end-to-end model performance by bridging the modality gap\nbetween speech and text. Specifically, we decouple the speech translation\nencoder into three parts and introduce a shrink mechanism to match the length\nof speech representation with that of the corresponding text transcription. To\nobtain better semantic representation, we completely integrate a text-based\ntranslation model into the STAST so that two tasks can be trained in the same\nlatent space. Furthermore, we introduce a cross-modal adaptation method to\nclose the distance between speech and text representation. Experimental results\non English-French and English-German speech translation corpora have shown that\nour model significantly outperforms strong baselines, and achieves the new\nstate-of-the-art performance.", "published": "2020-10-28 12:33:04", "link": "http://arxiv.org/abs/2010.14920v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Ethics by Design in Online Abusive Content Detection", "abstract": "To support safety and inclusion in online communications, significant efforts\nin NLP research have been put towards addressing the problem of abusive content\ndetection, commonly defined as a supervised classification task. The research\neffort has spread out across several closely related sub-areas, such as\ndetection of hate speech, toxicity, cyberbullying, etc. There is a pressing\nneed to consolidate the field under a common framework for task formulation,\ndataset design and performance evaluation. Further, despite current\ntechnologies achieving high classification accuracies, several ethical issues\nhave been revealed. We bring ethical issues to forefront and propose a unified\nframework as a two-step process. First, online content is categorized around\npersonal and identity-related subject matters. Second, severity of abuse is\nidentified through comparative annotation within each category. The novel\nframework is guided by the Ethics by Design principle and is a step towards\nbuilding more accurate and trusted models.", "published": "2020-10-28 13:10:24", "link": "http://arxiv.org/abs/2010.14952v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Comprehensive Survey on Word Representation Models: From Classical to\n  State-Of-The-Art Word Representation Language Models", "abstract": "Word representation has always been an important research area in the history\nof natural language processing (NLP). Understanding such complex text data is\nimperative, given that it is rich in information and can be used widely across\nvarious applications. In this survey, we explore different word representation\nmodels and its power of expression, from the classical to modern-day\nstate-of-the-art word representation language models (LMS). We describe a\nvariety of text representation methods, and model designs have blossomed in the\ncontext of NLP, including SOTA LMs. These models can transform large volumes of\ntext into effective vector representations capturing the same semantic\ninformation. Further, such representations can be utilized by various machine\nlearning (ML) algorithms for a variety of NLP related tasks. In the end, this\nsurvey briefly discusses the commonly used ML and DL based classifiers,\nevaluation metrics and the applications of these word embeddings in different\nNLP tasks.", "published": "2020-10-28 15:15:13", "link": "http://arxiv.org/abs/2010.15036v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Visuospatial Dataset for Naturalistic Verb Learning", "abstract": "We introduce a new dataset for training and evaluating grounded language\nmodels. Our data is collected within a virtual reality environment and is\ndesigned to emulate the quality of language data to which a pre-verbal child is\nlikely to have access: That is, naturalistic, spontaneous speech paired with\nrichly grounded visuospatial context. We use the collected data to compare\nseveral distributional semantics models for verb learning. We evaluate neural\nmodels based on 2D (pixel) features as well as feature-engineered models based\non 3D (symbolic, spatial) features, and show that neither modeling approach\nachieves satisfactory performance. Our results are consistent with evidence\nfrom child language acquisition that emphasizes the difficulty of learning\nverbs from naive distributional data. We discuss avenues for future work on\ncognitively-inspired grounded language learning, and release our corpus with\nthe intent of facilitating research on the topic.", "published": "2020-10-28 20:47:13", "link": "http://arxiv.org/abs/2010.15225v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "TopicModel4J: A Java Package for Topic Models", "abstract": "Topic models provide a flexible and principled framework for exploring hidden\nstructure in high-dimensional co-occurrence data and are commonly used natural\nlanguage processing (NLP) of text. In this paper, we design and implement a\nJava package, TopicModel4J, which contains 13 kinds of representative\nalgorithms for fitting topic models. The TopicModel4J in the Java programming\nenvironment provides an easy-to-use interface for data analysts to run the\nalgorithms, and allow to easily input and output data. In addition, this\npackage provides a few unstructured text preprocessing techniques, such as\nsplitting textual data into words, lowercasing the words, preforming\nlemmatization and removing the useless characters, URLs and stop words.", "published": "2020-10-28 02:33:41", "link": "http://arxiv.org/abs/2010.14707v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "DisenE: Disentangling Knowledge Graph Embeddings", "abstract": "Knowledge graph embedding (KGE), aiming to embed entities and relations into\nlow-dimensional vectors, has attracted wide attention recently. However, the\nexisting research is mainly based on the black-box neural models, which makes\nit difficult to interpret the learned representation. In this paper, we\nintroduce DisenE, an end-to-end framework to learn disentangled knowledge graph\nembeddings. Specially, we introduce an attention-based mechanism that enables\nthe model to explicitly focus on relevant components of entity embeddings\naccording to a given relation. Furthermore, we introduce two novel regularizers\nto encourage each component of the entity representation to independently\nreflect an isolated semantic aspect. Experimental results demonstrate that our\nproposed DisenE investigates a perspective to address the interpretability of\nKGE and is proved to be an effective way to improve the performance of link\nprediction tasks.", "published": "2020-10-28 03:45:19", "link": "http://arxiv.org/abs/2010.14730v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Chinese Text Classification Method With Low Hardware Requirement Based\n  on Improved Model Concatenation", "abstract": "In order to improve the accuracy performance of Chinese text classification\nmodels with low hardware requirements, an improved concatenation-based model is\ndesigned in this paper, which is a concatenation of 5 different sub-models,\nincluding TextCNN, LSTM, and Bi-LSTM. Compared with the existing ensemble\nlearning method, for a text classification mission, this model's accuracy is 2%\nhigher. Meanwhile, the hardware requirements of this model are much lower than\nthe BERT-based model.", "published": "2020-10-28 06:32:41", "link": "http://arxiv.org/abs/2010.14784v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Bayesian Methods for Semi-supervised Text Annotation", "abstract": "Human annotations are an important source of information in the development\nof natural language understanding approaches. As under the pressure of\nproductivity annotators can assign different labels to a given text, the\nquality of produced annotations frequently varies. This is especially the case\nif decisions are difficult, with high cognitive load, requires awareness of\nbroader context, or careful consideration of background knowledge. To alleviate\nthe problem, we propose two semi-supervised methods to guide the annotation\nprocess: a Bayesian deep learning model and a Bayesian ensemble method. Using a\nBayesian deep learning method, we can discover annotations that cannot be\ntrusted and might require reannotation. A recently proposed Bayesian ensemble\nmethod helps us to combine the annotators' labels with predictions of trained\nmodels. According to the results obtained from three hate speech detection\nexperiments, the proposed Bayesian methods can improve the annotations and\nprediction performance of BERT models.", "published": "2020-10-28 10:42:04", "link": "http://arxiv.org/abs/2010.14872v1", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "A Cyclic Proof System for HFLN", "abstract": "A cyclic proof system allows us to perform inductive reasoning without\nexplicit inductions. We propose a cyclic proof system for HFLN, which is a\nhigher-order predicate logic with natural numbers and alternating fixed-points.\nOurs is the first cyclic proof system for a higher-order logic, to our\nknowledge. Due to the presence of higher-order predicates and alternating\nfixed-points, our cyclic proof system requires a more delicate global condition\non cyclic proofs than the original system of Brotherston and Simpson. We prove\nthe decidability of checking the global condition and soundness of this system,\nand also prove a restricted form of standard completeness for an infinitary\nvariant of our cyclic proof system. A potential application of our cyclic proof\nsystem is semi-automated verification of higher-order programs, based on\nKobayashi et al.'s recent work on reductions from program verification to HFLN\nvalidity checking.", "published": "2020-10-28 11:19:53", "link": "http://arxiv.org/abs/2010.14891v3", "categories": ["cs.LO", "cs.CL", "03B70"], "primary_category": "cs.LO"}
{"title": "Handling Class Imbalance in Low-Resource Dialogue Systems by Combining\n  Few-Shot Classification and Interpolation", "abstract": "Utterance classification performance in low-resource dialogue systems is\nconstrained by an inevitably high degree of data imbalance in class labels. We\npresent a new end-to-end pairwise learning framework that is designed\nspecifically to tackle this phenomenon by inducing a few-shot classification\ncapability in the utterance representations and augmenting data through an\ninterpolation of utterance representations. Our approach is a general purpose\ntraining methodology, agnostic to the neural architecture used for encoding\nutterances. We show significant improvements in macro-F1 score over standard\ncross-entropy training for three different neural architectures, demonstrating\nimprovements on a Virtual Patient dialogue dataset as well as a low-resourced\nemulation of the Switchboard dialogue act classification dataset.", "published": "2020-10-28 17:05:24", "link": "http://arxiv.org/abs/2010.15090v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Detecting Stance in Media on Global Warming", "abstract": "Citing opinions is a powerful yet understudied strategy in argumentation. For\nexample, an environmental activist might say, \"Leading scientists agree that\nglobal warming is a serious concern,\" framing a clause which affirms their own\nstance (\"that global warming is serious\") as an opinion endorsed (\"[scientists]\nagree\") by a reputable source (\"leading\"). In contrast, a global warming denier\nmight frame the same clause as the opinion of an untrustworthy source with a\npredicate connoting doubt: \"Mistaken scientists claim [...].\" Our work studies\nopinion-framing in the global warming (GW) debate, an increasingly partisan\nissue that has received little attention in NLP. We introduce Global Warming\nStance Dataset (GWSD), a dataset of stance-labeled GW sentences, and train a\nBERT classifier to study novel aspects of argumentation in how different sides\nof a debate represent their own and each other's opinions. From 56K news\narticles, we find that similar linguistic devices for self-affirming and\nopponent-doubting discourse are used across GW-accepting and skeptic media,\nthough GW-skeptical media shows more opponent-doubt. We also find that authors\noften characterize sources as hypocritical, by ascribing opinions expressing\nthe author's own view to source entities known to publicly endorse the opposing\nview. We release our stance dataset, model, and lexicons of framing devices for\nfuture work on opinion-framing and the automatic detection of GW stance.", "published": "2020-10-28 18:01:02", "link": "http://arxiv.org/abs/2010.15149v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CopyNext: Explicit Span Copying and Alignment in Sequence to Sequence\n  Models", "abstract": "Copy mechanisms are employed in sequence to sequence models (seq2seq) to\ngenerate reproductions of words from the input to the output. These frameworks,\noperating at the lexical type level, fail to provide an explicit alignment that\nrecords where each token was copied from. Further, they require contiguous\ntoken sequences from the input (spans) to be copied individually. We present a\nmodel with an explicit token-level copy operation and extend it to copying\nentire spans. Our model provides hard alignments between spans in the input and\noutput, allowing for nontraditional applications of seq2seq, like information\nextraction. We demonstrate the approach on Nested Named Entity Recognition,\nachieving near state-of-the-art accuracy with an order of magnitude increase in\ndecoding speed.", "published": "2020-10-28 22:45:16", "link": "http://arxiv.org/abs/2010.15266v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Generating Knowledge Graphs by Employing Natural Language Processing and\n  Machine Learning Techniques within the Scholarly Domain", "abstract": "The continuous growth of scientific literature brings innovations and, at the\nsame time, raises new challenges. One of them is related to the fact that its\nanalysis has become difficult due to the high volume of published papers for\nwhich manual effort for annotations and management is required. Novel\ntechnological infrastructures are needed to help researchers, research policy\nmakers, and companies to time-efficiently browse, analyse, and forecast\nscientific research. Knowledge graphs i.e., large networks of entities and\nrelationships, have proved to be effective solution in this space. Scientific\nknowledge graphs focus on the scholarly domain and typically contain metadata\ndescribing research publications such as authors, venues, organizations,\nresearch topics, and citations. However, the current generation of knowledge\ngraphs lacks of an explicit representation of the knowledge presented in the\nresearch papers. As such, in this paper, we present a new architecture that\ntakes advantage of Natural Language Processing and Machine Learning methods for\nextracting entities and relationships from research publications and integrates\nthem in a large-scale knowledge graph. Within this research work, we i) tackle\nthe challenge of knowledge extraction by employing several state-of-the-art\nNatural Language Processing and Text Mining tools, ii) describe an approach for\nintegrating entities and relationships generated by these tools, iii) show the\nadvantage of such an hybrid system over alternative approaches, and vi) as a\nchosen use case, we generated a scientific knowledge graph including 109,105\ntriples, extracted from 26,827 abstracts of papers within the Semantic Web\ndomain. As our approach is general and can be applied to any domain, we expect\nthat it can facilitate the management, analysis, dissemination, and processing\nof scientific knowledge.", "published": "2020-10-28 08:31:40", "link": "http://arxiv.org/abs/2011.01103v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Scaling Laws for Autoregressive Generative Modeling", "abstract": "We identify empirical scaling laws for the cross-entropy loss in four\ndomains: generative image modeling, video modeling, multimodal\nimage$\\leftrightarrow$text models, and mathematical problem solving. In all\ncases autoregressive Transformers smoothly improve in performance as model size\nand compute budgets increase, following a power-law plus constant scaling law.\nThe optimal model size also depends on the compute budget through a power-law,\nwith exponents that are nearly universal across all data domains.\n  The cross-entropy loss has an information theoretic interpretation as\n$S($True$) + D_{\\mathrm{KL}}($True$||$Model$)$, and the empirical scaling laws\nsuggest a prediction for both the true data distribution's entropy and the KL\ndivergence between the true and model distributions. With this interpretation,\nbillion-parameter Transformers are nearly perfect models of the YFCC100M image\ndistribution downsampled to an $8\\times 8$ resolution, and we can forecast the\nmodel size needed to achieve any given reducible loss (ie $D_{\\mathrm{KL}}$) in\nnats/image for other resolutions.\n  We find a number of additional scaling laws in specific domains: (a) we\nidentify a scaling relation for the mutual information between captions and\nimages in multimodal models, and show how to answer the question \"Is a picture\nworth a thousand words?\"; (b) in the case of mathematical problem solving, we\nidentify scaling laws for model performance when extrapolating beyond the\ntraining distribution; (c) we finetune generative image models for ImageNet\nclassification and find smooth scaling of the classification loss and error\nrate, even as the generative loss levels off. Taken together, these results\nstrengthen the case that scaling laws have important implications for neural\nnetwork performance, including on downstream tasks.", "published": "2020-10-28 02:17:24", "link": "http://arxiv.org/abs/2010.14701v2", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "CASS-NAT: CTC Alignment-based Single Step Non-autoregressive Transformer\n  for Speech Recognition", "abstract": "We propose a CTC alignment-based single step non-autoregressive transformer\n(CASS-NAT) for speech recognition. Specifically, the CTC alignment contains the\ninformation of (a) the number of tokens for decoder input, and (b) the time\nspan of acoustics for each token. The information are used to extract acoustic\nrepresentation for each token in parallel, referred to as token-level acoustic\nembedding which substitutes the word embedding in autoregressive transformer\n(AT) to achieve parallel generation in decoder. During inference, an\nerror-based alignment sampling method is proposed to be applied to the CTC\noutput space, reducing the WER and retaining the parallelism as well.\nExperimental results show that the proposed method achieves WERs of 3.8%/9.1%\non Librispeech test clean/other dataset without an external LM, and a CER of\n5.8% on Aishell1 Mandarin corpus, respectively1. Compared to the AT baseline,\nthe CASS-NAT has a performance reduction on WER, but is 51.2x faster in terms\nof RTF. When decoding with an oracle CTC alignment, the lower bound of WER\nwithout LM reaches 2.3% on the test-clean set, indicating the potential of the\nproposed method.", "published": "2020-10-28 03:14:05", "link": "http://arxiv.org/abs/2010.14725v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Seen and Unseen emotional style transfer for voice conversion with a new\n  emotional speech dataset", "abstract": "Emotional voice conversion aims to transform emotional prosody in speech\nwhile preserving the linguistic content and speaker identity. Prior studies\nshow that it is possible to disentangle emotional prosody using an\nencoder-decoder network conditioned on discrete representation, such as one-hot\nemotion labels. Such networks learn to remember a fixed set of emotional\nstyles. In this paper, we propose a novel framework based on variational\nauto-encoding Wasserstein generative adversarial network (VAW-GAN), which makes\nuse of a pre-trained speech emotion recognition (SER) model to transfer\nemotional style during training and at run-time inference. In this way, the\nnetwork is able to transfer both seen and unseen emotional style to a new\nutterance. We show that the proposed framework achieves remarkable performance\nby consistently outperforming the baseline framework. This paper also marks the\nrelease of an emotional speech dataset (ESD) for voice conversion, which has\nmultiple speakers and languages.", "published": "2020-10-28 07:16:18", "link": "http://arxiv.org/abs/2010.14794v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Decoupling Pronunciation and Language for End-to-end Code-switching\n  Automatic Speech Recognition", "abstract": "Despite the recent significant advances witnessed in end-to-end (E2E) ASR\nsystem for code-switching, hunger for audio-text paired data limits the further\nimprovement of the models' performance. In this paper, we propose a decoupled\ntransformer model to use monolingual paired data and unpaired text data to\nalleviate the problem of code-switching data shortage. The model is decoupled\ninto two parts: audio-to-phoneme (A2P) network and phoneme-to-text (P2T)\nnetwork. The A2P network can learn acoustic pattern scenarios using large-scale\nmonolingual paired data. Meanwhile, it generates multiple phoneme sequence\ncandidates for single audio data in real-time during the training process. Then\nthe generated phoneme-text paired data is used to train the P2T network. This\nnetwork can be pre-trained with large amounts of external unpaired text data.\nBy using monolingual data and unpaired text data, the decoupled transformer\nmodel reduces the high dependency on code-switching paired training data of E2E\nmodel to a certain extent. Finally, the two networks are optimized jointly\nthrough attention fusion. We evaluate the proposed method on the public\nMandarin-English code-switching dataset. Compared with our transformer\nbaseline, the proposed method achieves 18.14% relative mix error rate\nreduction.", "published": "2020-10-28 07:46:15", "link": "http://arxiv.org/abs/2010.14798v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "PPG-based singing voice conversion with adversarial representation\n  learning", "abstract": "Singing voice conversion (SVC) aims to convert the voice of one singer to\nthat of other singers while keeping the singing content and melody. On top of\nrecent voice conversion works, we propose a novel model to steadily convert\nsongs while keeping their naturalness and intonation. We build an end-to-end\narchitecture, taking phonetic posteriorgrams (PPGs) as inputs and generating\nmel spectrograms. Specifically, we implement two separate encoders: one encodes\nPPGs as content, and the other compresses mel spectrograms to supply acoustic\nand musical information. To improve the performance on timbre and melody, an\nadversarial singer confusion module and a mel-regressive representation\nlearning module are designed for the model. Objective and subjective\nexperiments are conducted on our private Chinese singing corpus. Comparing with\nthe baselines, our methods can significantly improve the conversion performance\nin terms of naturalness, melody, and voice similarity. Moreover, our PPG-based\nmethod is proved to be robust for noisy sources.", "published": "2020-10-28 08:03:27", "link": "http://arxiv.org/abs/2010.14804v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "INT8 Winograd Acceleration for Conv1D Equipped ASR Models Deployed on\n  Mobile Devices", "abstract": "The intensive computation of Automatic Speech Recognition (ASR) models\nobstructs them from being deployed on mobile devices. In this paper, we present\na novel quantized Winograd optimization pipeline, which combines the\nquantization and fast convolution to achieve efficient inference acceleration\non mobile devices for ASR models. To avoid the information loss due to the\ncombination of quantization and Winograd convolution, a Range-Scaled\nQuantization (RSQ) training method is proposed to expand the quantized\nnumerical range and to distill knowledge from high-precision values. Moreover,\nan improved Conv1D equipped DFSMN (ConvDFSMN) model is designed for mobile\ndeployment. We conduct extensive experiments on both ConvDFSMN and Wav2letter\nmodels. Results demonstrate the models can be effectively optimized with the\nproposed pipeline. Especially, Wav2letter achieves 1.48* speedup with an\napproximate 0.07% WER decrease on ARMv7-based mobile devices.", "published": "2020-10-28 09:25:49", "link": "http://arxiv.org/abs/2010.14841v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Non-Autoregressive Transformer ASR with CTC-Enhanced Decoder Input", "abstract": "Non-autoregressive (NAR) transformer models have achieved significantly\ninference speedup but at the cost of inferior accuracy compared to\nautoregressive (AR) models in automatic speech recognition (ASR). Most of the\nNAR transformers take a fixed-length sequence filled with MASK tokens or a\nredundant sequence copied from encoder states as decoder input, they cannot\nprovide efficient target-side information thus leading to accuracy degradation.\nTo address this problem, we propose a CTC-enhanced NAR transformer, which\ngenerates target sequence by refining predictions of the CTC module.\nExperimental results show that our method outperforms all previous NAR\ncounterparts and achieves 50x faster decoding speed than a strong AR baseline\nwith only 0.0 ~ 0.3 absolute CER degradation on Aishell-1 and Aishell-2\ndatasets.", "published": "2020-10-28 15:00:09", "link": "http://arxiv.org/abs/2010.15025v2", "categories": ["cs.SD", "cs.CL", "eess.AS", "I.2.7"], "primary_category": "cs.SD"}
{"title": "Measuring non-trivial compositionality in emergent communication", "abstract": "Compositionality is an important explanatory target in emergent communication\nand language evolution. The vast majority of computational models of\ncommunication account for the emergence of only a very basic form of\ncompositionality: trivial compositionality. A compositional protocol is\ntrivially compositional if the meaning of a complex signal (e.g. blue circle)\nboils down to the intersection of meanings of its constituents (e.g. the\nintersection of the set of blue objects and the set of circles). A protocol is\nnon-trivially compositional (NTC) if the meaning of a complex signal (e.g.\nbiggest apple) is a more complex function of the meanings of their\nconstituents. In this paper, we review several metrics of compositionality used\nin emergent communication and experimentally show that most of them fail to\ndetect NTC - i.e. they treat non-trivial compositionality as a failure of\ncompositionality. The one exception is tree reconstruction error, a metric\nmotivated by formal accounts of compositionality. These results emphasise\nimportant limitations of emergent communication research that could hamper\nprogress on modelling the emergence of NTC.", "published": "2020-10-28 16:11:07", "link": "http://arxiv.org/abs/2010.15058v2", "categories": ["cs.NE", "cs.CL", "cs.LG"], "primary_category": "cs.NE"}
{"title": "Graph-based Topic Extraction from Vector Embeddings of Text Documents:\n  Application to a Corpus of News Articles", "abstract": "Production of news content is growing at an astonishing rate. To help manage\nand monitor the sheer amount of text, there is an increasing need to develop\nefficient methods that can provide insights into emerging content areas, and\nstratify unstructured corpora of text into `topics' that stem intrinsically\nfrom content similarity. Here we present an unsupervised framework that brings\ntogether powerful vector embeddings from natural language processing with tools\nfrom multiscale graph partitioning that can reveal natural partitions at\ndifferent resolutions without making a priori assumptions about the number of\nclusters in the corpus. We show the advantages of graph-based clustering\nthrough end-to-end comparisons with other popular clustering and topic\nmodelling methods, and also evaluate different text vector embeddings, from\nclassic Bag-of-Words to Doc2Vec to the recent transformers based model Bert.\nThis comparative work is showcased through an analysis of a corpus of US news\ncoverage during the presidential election year of 2016.", "published": "2020-10-28 16:20:05", "link": "http://arxiv.org/abs/2010.15067v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The geometry of integration in text classification RNNs", "abstract": "Despite the widespread application of recurrent neural networks (RNNs) across\na variety of tasks, a unified understanding of how RNNs solve these tasks\nremains elusive. In particular, it is unclear what dynamical patterns arise in\ntrained RNNs, and how those patterns depend on the training dataset or task.\nThis work addresses these questions in the context of a specific natural\nlanguage processing task: text classification. Using tools from dynamical\nsystems analysis, we study recurrent networks trained on a battery of both\nnatural and synthetic text classification tasks. We find the dynamics of these\ntrained RNNs to be both interpretable and low-dimensional. Specifically, across\narchitectures and datasets, RNNs accumulate evidence for each class as they\nprocess the text, using a low-dimensional attractor manifold as the underlying\nmechanism. Moreover, the dimensionality and geometry of the attractor manifold\nare determined by the structure of the training dataset; in particular, we\ndescribe how simple word-count statistics computed on the training dataset can\nbe used to predict these properties. Our observations span multiple\narchitectures and datasets, reflecting a common mechanism RNNs employ to\nperform text classification. To the degree that integration of evidence towards\na decision is a common computational primitive, this work lays the foundation\nfor using dynamical systems techniques to study the inner workings of RNNs.", "published": "2020-10-28 17:58:53", "link": "http://arxiv.org/abs/2010.15114v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Fusion Models for Improved Visual Captioning", "abstract": "Visual captioning aims to generate textual descriptions given images or\nvideos. Traditionally, image captioning models are trained on human annotated\ndatasets such as Flickr30k and MS-COCO, which are limited in size and\ndiversity. This limitation hinders the generalization capabilities of these\nmodels while also rendering them liable to making mistakes. Language models\ncan, however, be trained on vast amounts of freely available unlabelled data\nand have recently emerged as successful language encoders and coherent text\ngenerators. Meanwhile, several unimodal and multimodal fusion techniques have\nbeen proven to work well for natural language generation and automatic speech\nrecognition. Building on these recent developments, and with the aim of\nimproving the quality of generated captions, the contribution of our work in\nthis paper is two-fold: First, we propose a generic multimodal model fusion\nframework for caption generation as well as emendation where we utilize\ndifferent fusion strategies to integrate a pretrained Auxiliary Language Model\n(AuxLM) within the traditional encoder-decoder visual captioning frameworks.\nNext, we employ the same fusion strategies to integrate a pretrained Masked\nLanguage Model (MLM), namely BERT, with a visual captioning model, viz. Show,\nAttend, and Tell, for emending both syntactic and semantic errors in captions.\nOur caption emendation experiments on three benchmark image captioning\ndatasets, viz. Flickr8k, Flickr30k, and MSCOCO, show improvements over the\nbaseline, indicating the usefulness of our proposed multimodal fusion\nstrategies. Further, we perform a preliminary qualitative analysis on the\nemended captions and identify error categories based on the type of\ncorrections.", "published": "2020-10-28 21:55:25", "link": "http://arxiv.org/abs/2010.15251v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "One In A Hundred: Select The Best Predicted Sequence from Numerous\n  Candidates for Streaming Speech Recognition", "abstract": "The RNN-Transducers and improved attention-based encoder-decoder models are\nwidely applied to streaming speech recognition. Compared with these two\nend-to-end models, the CTC model is more efficient in training and inference.\nHowever, it cannot capture the linguistic dependencies between the output\ntokens. Inspired by the success of two-pass end-to-end models, we introduce a\ntransformer decoder and the two-stage inference method into the streaming CTC\nmodel. During inference, the CTC decoder first generates many candidates in a\nstreaming fashion. Then the transformer decoder selects the best candidate\nbased on the corresponding acoustic encoded states. The second-stage\ntransformer decoder can be regarded as a conditional language model. We assume\nthat a large enough number and enough diversity of candidates generated in the\nfirst stage can compensate the CTC model for the lack of language modeling\nability. All the experiments are conducted on a Chinese Mandarin dataset\nAISHELL-1. The results show that our proposed model can implement streaming\ndecoding in a fast and straightforward way. Our model can achieve up to a 20%\nreduction in the character error rate than the baseline CTC model. In addition,\nour model can also perform non-streaming inference with only a little\nperformance degradation.", "published": "2020-10-28 07:07:01", "link": "http://arxiv.org/abs/2010.14791v3", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Replay and Synthetic Speech Detection with Res2net Architecture", "abstract": "Existing approaches for replay and synthetic speech detection still lack\ngeneralizability to unseen spoofing attacks. This work proposes to leverage a\nnovel model structure, so-called Res2Net, to improve the anti-spoofing\ncountermeasure's generalizability. Res2Net mainly modifies the ResNet block to\nenable multiple feature scales. Specifically, it splits the feature maps within\none block into multiple channel groups and designs a residual-like connection\nacross different channel groups. Such connection increases the possible\nreceptive fields, resulting in multiple feature scales. This multiple scaling\nmechanism significantly improves the countermeasure's generalizability to\nunseen spoofing attacks. It also decreases the model size compared to\nResNet-based models. Experimental results show that the Res2Net model\nconsistently outperforms ResNet34 and ResNet50 by a large margin in both\nphysical access (PA) and logical access (LA) of the ASVspoof 2019 corpus.\nMoreover, integration with the squeeze-and-excitation (SE) block can further\nenhance performance. For feature engineering, we investigate the\ngeneralizability of Res2Net combined with different acoustic features, and\nobserve that the constant-Q transform (CQT) achieves the most promising\nperformance in both PA and LA scenarios. Our best single system outperforms\nother state-of-the-art single systems in both PA and LA of the ASVspoof 2019\ncorpus.", "published": "2020-10-28 14:33:42", "link": "http://arxiv.org/abs/2010.15006v3", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "Speech Synthesis and Control Using Differentiable DSP", "abstract": "Modern text-to-speech systems are able to produce natural and high-quality\nspeech, but speech contains factors of variation (e.g. pitch, rhythm, loudness,\ntimbre)\\ that text alone cannot contain. In this work we move towards a speech\nsynthesis system that can produce diverse speech renditions of a text by\nallowing (but not requiring) explicit control over the various factors of\nvariation. We propose a new neural vocoder that offers control of such factors\nof variation. This is achieved by employing differentiable digital signal\nprocessing (DDSP) (previously used only for music rather than speech), which\nexposes these factors of variation. The results show that the proposed approach\ncan produce natural speech with realistic timbre, and individual factors of\nvariation can be freely controlled.", "published": "2020-10-28 16:55:38", "link": "http://arxiv.org/abs/2010.15084v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Large-Scale MIDI-based Composer Classification", "abstract": "Music classification is a task to classify a music piece into labels such as\ngenres or composers. We propose large-scale MIDI based composer classification\nsystems using GiantMIDI-Piano, a transcription-based dataset. We propose to use\npiano rolls, onset rolls, and velocity rolls as input representations and use\ndeep neural networks as classifiers. To our knowledge, we are the first to\ninvestigate the composer classification problem with up to 100 composers. By\nusing convolutional recurrent neural networks as models, our MIDI based\ncomposer classification system achieves a 10-composer and a 100-composer\nclassification accuracies of 0.648 and 0.385 (evaluated on 30-second clips) and\n0.739 and 0.489 (evaluated on music pieces), respectively. Our MIDI based\ncomposer system outperforms several audio-based baseline classification\nsystems, indicating the effectiveness of using compact MIDI representations for\ncomposer classification.", "published": "2020-10-28 08:07:55", "link": "http://arxiv.org/abs/2010.14805v1", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Frequency-Undersampled Short-Time Fourier Transform", "abstract": "The short-time Fourier transform (STFT) usually computes the same number of\nfrequency components as the frame length while overlapping adjacent time frames\nby more than half. As a result, the number of components of a spectrogram\nmatrix becomes more than twice the signal length, and hence STFT is hardly used\nfor signal compression. In addition, even if we modify the spectrogram into a\ndesired one by spectrogram-based signal processing, it is re-changed during the\ninversion as long as it is outside the range of STFT. In this paper, to reduce\nthe number of components of a spectrogram while maintaining the analytical\nability, we propose the frequency-undersampled STFT (FUSTFT), which computes\nonly half the frequency components. We also present the inversions with and\nwithout the periodic condition, including their different properties. In simple\nnumerical examples of audio signals, we confirm the validity of FUSTFT and the\ninversions.", "published": "2020-10-28 15:05:59", "link": "http://arxiv.org/abs/2010.15029v1", "categories": ["eess.SP", "cs.NA", "eess.AS", "math.NA"], "primary_category": "eess.SP"}
{"title": "Optimizing Short-Time Fourier Transform Parameters via Gradient Descent", "abstract": "The Short-Time Fourier Transform (STFT) has been a staple of signal\nprocessing, often being the first step for many audio tasks. A very familiar\nprocess when using the STFT is the search for the best STFT parameters, as they\noften have significant side effects if chosen poorly. These parameters are\noften defined in terms of an integer number of samples, which makes their\noptimization non-trivial. In this paper we show an approach that allows us to\nobtain a gradient for STFT parameters with respect to arbitrary cost functions,\nand thus enable the ability to employ gradient descent optimization of\nquantities like the STFT window length, or the STFT hop size. We do so for\nparameter values that stay constant throughout an input, but also for cases\nwhere these parameters have to dynamically change over time to accommodate\nvarying signal characteristics.", "published": "2020-10-28 15:49:56", "link": "http://arxiv.org/abs/2010.15049v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Gender Bias in Depression Detection Using Audio Features", "abstract": "Depression is a large-scale mental health problem and a challenging area for\nmachine learning researchers in detection of depression. Datasets such as\nDistress Analysis Interview Corpus - Wizard of Oz (DAIC-WOZ) have been created\nto aid research in this area. However, on top of the challenges inherent in\naccurately detecting depression, biases in datasets may result in skewed\nclassification performance. In this paper we examine gender bias in the\nDAIC-WOZ dataset. We show that gender biases in DAIC-WOZ can lead to an\noverreporting of performance. By different concepts from Fair Machine Learning,\nsuch as data re-distribution, and using raw audio features, we can mitigate\nagainst the harmful effects of bias.", "published": "2020-10-28 14:53:56", "link": "http://arxiv.org/abs/2010.15120v3", "categories": ["cs.SD", "cs.LG", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "Improving Perceptual Quality by Phone-Fortified Perceptual Loss using\n  Wasserstein Distance for Speech Enhancement", "abstract": "Speech enhancement (SE) aims to improve speech quality and intelligibility,\nwhich are both related to a smooth transition in speech segments that may carry\nlinguistic information, e.g. phones and syllables. In this study, we propose a\nnovel phone-fortified perceptual loss (PFPL) that takes phonetic information\ninto account for training SE models. To effectively incorporate the phonetic\ninformation, the PFPL is computed based on latent representations of the\nwav2vec model, a powerful self-supervised encoder that renders rich phonetic\ninformation. To more accurately measure the distribution distances of the\nlatent representations, the PFPL adopts the Wasserstein distance as the\ndistance measure. Our experimental results first reveal that the PFPL is more\ncorrelated with the perceptual evaluation metrics, as compared to signal-level\nlosses. Moreover, the results showed that the PFPL can enable a deep complex\nU-Net SE model to achieve highly competitive performance in terms of\nstandardized quality and intelligibility evaluations on the Voice Bank-DEMAND\ndataset.", "published": "2020-10-28 18:34:28", "link": "http://arxiv.org/abs/2010.15174v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DNSMOS: A Non-Intrusive Perceptual Objective Speech Quality metric to\n  evaluate Noise Suppressors", "abstract": "Human subjective evaluation is the gold standard to evaluate speech quality\noptimized for human perception. Perceptual objective metrics serve as a proxy\nfor subjective scores. The conventional and widely used metrics require a\nreference clean speech signal, which is unavailable in real recordings. The\nno-reference approaches correlate poorly with human ratings and are not widely\nadopted in the research community. One of the biggest use cases of these\nperceptual objective metrics is to evaluate noise suppression algorithms. This\npaper introduces a multi-stage self-teaching based perceptual objective metric\nthat is designed to evaluate noise suppressors. The proposed method generalizes\nwell in challenging test conditions with a high correlation to human ratings.", "published": "2020-10-28 22:19:51", "link": "http://arxiv.org/abs/2010.15258v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Melody-Conditioned Lyrics Generation with SeqGANs", "abstract": "Automatic lyrics generation has received attention from both music and AI\ncommunities for years. Early rule-based approaches have~---due to increases in\ncomputational power and evolution in data-driven models---~mostly been replaced\nwith deep-learning-based systems. Many existing approaches, however, either\nrely heavily on prior knowledge in music and lyrics writing or oversimplify the\ntask by largely discarding melodic information and its relationship with the\ntext. We propose an end-to-end melody-conditioned lyrics generation system\nbased on Sequence Generative Adversarial Networks (SeqGAN), which generates a\nline of lyrics given the corresponding melody as the input. Furthermore, we\ninvestigate the performance of the generator with an additional input\ncondition: the theme or overarching topic of the lyrics to be generated. We\nshow that the input conditions have no negative impact on the evaluation\nmetrics while enabling the network to produce more meaningful results.", "published": "2020-10-28 02:35:40", "link": "http://arxiv.org/abs/2010.14709v1", "categories": ["cs.SD", "cs.IR", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Generative Adversarial Networks in Human Emotion Synthesis:A Review", "abstract": "Synthesizing realistic data samples is of great value for both academic and\nindustrial communities. Deep generative models have become an emerging topic in\nvarious research areas like computer vision and signal processing. Affective\ncomputing, a topic of a broad interest in computer vision society, has been no\nexception and has benefited from generative models. In fact, affective\ncomputing observed a rapid derivation of generative models during the last two\ndecades. Applications of such models include but are not limited to emotion\nrecognition and classification, unimodal emotion synthesis, and cross-modal\nemotion synthesis. As a result, we conducted a review of recent advances in\nhuman emotion synthesis by studying available databases, advantages, and\ndisadvantages of the generative models along with the related training\nstrategies considering two principal human communication modalities, namely\naudio and video. In this context, facial expression synthesis, speech emotion\nsynthesis, and the audio-visual (cross-modal) emotion synthesis is reviewed\nextensively under different application scenarios. Gradually, we discuss open\nresearch problems to push the boundaries of this research area for future\nworks.", "published": "2020-10-28 16:45:36", "link": "http://arxiv.org/abs/2010.15075v2", "categories": ["cs.CV", "cs.AI", "cs.HC", "eess.AS", "eess.IV"], "primary_category": "cs.CV"}
