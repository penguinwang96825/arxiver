{"title": "Enhanced Language Model Truthfulness with Learnable Intervention and\n  Uncertainty Expression", "abstract": "Large language models (LLMs) can generate long-form and coherent text, yet\nthey often hallucinate facts, which undermines their reliability. To mitigate\nthis issue, inference-time methods steer LLM representations toward the\n\"truthful directions\" previously learned for truth elicitation. However,\napplying these truthful directions with the same intensity fails to generalize\nacross different query contexts. We propose LITO, a Learnable Intervention\nmethod for Truthfulness Optimization that automatically identifies the optimal\nintervention intensity tailored to each specific context. LITO explores a\nsequence of model generations based on increasing levels of intervention\nintensities. It selects the most accurate response or refuses to answer when\nthe predictions are highly uncertain. Experiments on multiple LLMs and\nquestion-answering datasets demonstrate that LITO improves truthfulness while\npreserving task accuracy. The adaptive nature of LITO counters the limitations\nof one-size-fits-all intervention methods, maximizing truthfulness by\nreflecting the model's internal knowledge only when it is confident. Our code\nis available at https://github.com/launchnlp/LITO.", "published": "2024-05-01 03:50:09", "link": "http://arxiv.org/abs/2405.00301v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating Feedback-Ladders for Logical Errors in Programming using\n  Large Language Models", "abstract": "In feedback generation for logical errors in programming assignments, large\nlanguage model (LLM)-based methods have shown great promise. These methods ask\nthe LLM to generate feedback given the problem statement and a student's\n(buggy) submission. There are several issues with these types of methods.\nFirst, the generated feedback messages are often too direct in revealing the\nerror in the submission and thus diminish valuable opportunities for the\nstudent to learn. Second, they do not consider the student's learning context,\ni.e., their previous submissions, current knowledge, etc. Third, they are not\nlayered since existing methods use a single, shared prompt for all student\nsubmissions. In this paper, we explore using LLMs to generate a\n\"feedback-ladder\", i.e., multiple levels of feedback for the same\nproblem-submission pair. We evaluate the quality of the generated\nfeedback-ladder via a user study with students, educators, and researchers. We\nhave observed diminishing effectiveness for higher-level feedback and\nhigher-scoring submissions overall in the study. In practice, our method\nenables teachers to select an appropriate level of feedback to show to a\nstudent based on their personal learning context, or in a progressive manner to\ngo more detailed if a higher-level feedback fails to correct the student's\nerror.", "published": "2024-05-01 03:52:39", "link": "http://arxiv.org/abs/2405.00302v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DFKI-NLP at SemEval-2024 Task 2: Towards Robust LLMs Using Data\n  Perturbations and MinMax Training", "abstract": "The NLI4CT task at SemEval-2024 emphasizes the development of robust models\nfor Natural Language Inference on Clinical Trial Reports (CTRs) using large\nlanguage models (LLMs). This edition introduces interventions specifically\ntargeting the numerical, vocabulary, and semantic aspects of CTRs. Our proposed\nsystem harnesses the capabilities of the state-of-the-art Mistral model,\ncomplemented by an auxiliary model, to focus on the intricate input space of\nthe NLI4CT dataset. Through the incorporation of numerical and acronym-based\nperturbations to the data, we train a robust system capable of handling both\nsemantic-altering and numerical contradiction interventions. Our analysis on\nthe dataset sheds light on the challenging sections of the CTRs for reasoning.", "published": "2024-05-01 05:03:08", "link": "http://arxiv.org/abs/2405.00321v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AdaMoLE: Fine-Tuning Large Language Models with Adaptive Mixture of\n  Low-Rank Adaptation Experts", "abstract": "We introduce AdaMoLE, a novel method for fine-tuning large language models\n(LLMs) through an Adaptive Mixture of Low-Rank Adaptation (LoRA) Experts.\nMoving beyond conventional methods that employ a static top-k strategy for\nactivating experts, AdaMoLE dynamically adjusts the activation threshold using\na dedicated threshold network, adaptively responding to the varying\ncomplexities of different tasks. By replacing a single LoRA in a layer with\nmultiple LoRA experts and integrating a gating function with the threshold\nmechanism, AdaMoLE effectively selects and activates the most appropriate\nexperts based on the input context. Our extensive evaluations across a variety\nof commonsense reasoning and natural language processing tasks show that\nAdaMoLE exceeds baseline performance. This enhancement highlights the\nadvantages of AdaMoLE's adaptive selection of LoRA experts, improving model\neffectiveness without a corresponding increase in the expert count. The\nexperimental validation not only confirms AdaMoLE as a robust approach for\nenhancing LLMs but also suggests valuable directions for future research in\nadaptive expert selection mechanisms, potentially broadening the scope for\noptimizing model performance across diverse language processing tasks.", "published": "2024-05-01 07:33:43", "link": "http://arxiv.org/abs/2405.00361v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CofiPara: A Coarse-to-fine Paradigm for Multimodal Sarcasm Target\n  Identification with Large Multimodal Models", "abstract": "Social media abounds with multimodal sarcasm, and identifying sarcasm targets\nis particularly challenging due to the implicit incongruity not directly\nevident in the text and image modalities. Current methods for Multimodal\nSarcasm Target Identification (MSTI) predominantly focus on superficial\nindicators in an end-to-end manner, overlooking the nuanced understanding of\nmultimodal sarcasm conveyed through both the text and image. This paper\nproposes a versatile MSTI framework with a coarse-to-fine paradigm, by\naugmenting sarcasm explainability with reasoning and pre-training knowledge.\nInspired by the powerful capacity of Large Multimodal Models (LMMs) on\nmultimodal reasoning, we first engage LMMs to generate competing rationales for\ncoarser-grained pre-training of a small language model on multimodal sarcasm\ndetection. We then propose fine-tuning the model for finer-grained sarcasm\ntarget identification. Our framework is thus empowered to adeptly unveil the\nintricate targets within multimodal sarcasm and mitigate the negative impact\nposed by potential noise inherently in LMMs. Experimental results demonstrate\nthat our model far outperforms state-of-the-art MSTI methods, and markedly\nexhibits explainability in deciphering sarcasm as well.", "published": "2024-05-01 08:44:44", "link": "http://arxiv.org/abs/2405.00390v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models", "abstract": "The alignments of reasoning abilities between smaller and larger Language\nModels are largely conducted via Supervised Fine-Tuning (SFT) using\ndemonstrations generated from robust Large Language Models (LLMs). Although\nthese approaches deliver more performant models, they do not show sufficiently\nstrong generalization ability as the training only relies on the provided\ndemonstrations.\n  In this paper, we propose the Self-refine Instruction-tuning method that\nelicits Smaller Language Models to self-refine their abilities. Our approach is\nbased on a two-stage process, where reasoning abilities are first transferred\nbetween LLMs and Small Language Models (SLMs) via Instruction-tuning on\ndemonstrations provided by LLMs, and then the instructed models Self-refine\ntheir abilities through preference optimization strategies. In particular, the\nsecond phase operates refinement heuristics based on the Direct Preference\nOptimization algorithm, where the SLMs are elicited to deliver a series of\nreasoning paths by automatically sampling the generated responses and providing\nrewards using ground truths from the LLMs. Results obtained on commonsense and\nmath reasoning tasks show that this approach significantly outperforms\nInstruction-tuning in both in-domain and out-domain scenarios, aligning the\nreasoning abilities of Smaller and Larger Language Models.", "published": "2024-05-01 09:10:27", "link": "http://arxiv.org/abs/2405.00402v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BiomedRAG: A Retrieval Augmented Large Language Model for Biomedicine", "abstract": "Large Language Models (LLMs) have swiftly emerged as vital resources for\ndifferent applications in the biomedical and healthcare domains; however, these\nmodels encounter issues such as generating inaccurate information or\nhallucinations. Retrieval-augmented generation provided a solution for these\nmodels to update knowledge and enhance their performance. In contrast to\nprevious retrieval-augmented LMs, which utilize specialized cross-attention\nmechanisms to help LLM encode retrieved text, BiomedRAG adopts a simpler\napproach by directly inputting the retrieved chunk-based documents into the\nLLM. This straightforward design is easily applicable to existing retrieval and\nlanguage models, effectively bypassing noise information in retrieved\ndocuments, particularly in noise-intensive tasks. Moreover, we demonstrate the\npotential for utilizing the LLM to supervise the retrieval model in the\nbiomedical domain, enabling it to retrieve the document that assists the LM in\nimproving its predictions. Our experiments reveal that with the tuned\nscorer,\\textsc{ BiomedRAG} attains superior performance across 5 biomedical NLP\ntasks, encompassing information extraction (triple extraction, relation\nextraction), text classification, link prediction, and question-answering,\nleveraging over 9 datasets. For instance, in the triple extraction task,\n\\textsc{BiomedRAG} outperforms other triple extraction systems with micro-F1\nscores of 81.42 and 88.83 on GIT and ChemProt corpora, respectively.", "published": "2024-05-01 12:01:39", "link": "http://arxiv.org/abs/2405.00465v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Harnessing the Power of Multiple Minds: Lessons Learned from LLM Routing", "abstract": "With the rapid development of LLMs, it is natural to ask how to harness their\ncapabilities efficiently. In this paper, we explore whether it is feasible to\ndirect each input query to a single most suitable LLM. To this end, we propose\nLLM routing for challenging reasoning tasks. Our extensive experiments suggest\nthat such routing shows promise but is not feasible in all scenarios, so more\nrobust approaches should be investigated to fill this gap.", "published": "2024-05-01 12:04:28", "link": "http://arxiv.org/abs/2405.00467v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Addressing Topic Granularity and Hallucination in Large Language Models\n  for Topic Modelling", "abstract": "Large language models (LLMs) with their strong zero-shot topic extraction\ncapabilities offer an alternative to probabilistic topic modelling and\nclosed-set topic classification approaches. As zero-shot topic extractors, LLMs\nare expected to understand human instructions to generate relevant and\nnon-hallucinated topics based on the given documents. However, LLM-based topic\nmodelling approaches often face difficulties in generating topics with\nadherence to granularity as specified in human instructions, often resulting in\nmany near-duplicate topics. Furthermore, methods for addressing hallucinated\ntopics generated by LLMs have not yet been investigated. In this paper, we\nfocus on addressing the issues of topic granularity and hallucinations for\nbetter LLM-based topic modelling. To this end, we introduce a novel approach\nthat leverages Direct Preference Optimisation (DPO) to fine-tune open-source\nLLMs, such as Mistral-7B. Our approach does not rely on traditional human\nannotation to rank preferred answers but employs a reconstruction pipeline to\nmodify raw topics generated by LLMs, thus enabling a fast and efficient\ntraining and inference framework. Comparative experiments show that our\nfine-tuning approach not only significantly improves the LLM's capability to\nproduce more coherent, relevant, and precise topics, but also reduces the\nnumber of hallucinated topics.", "published": "2024-05-01 16:32:07", "link": "http://arxiv.org/abs/2405.00611v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NLU-STR at SemEval-2024 Task 1: Generative-based Augmentation and\n  Encoder-based Scoring for Semantic Textual Relatedness", "abstract": "Semantic textual relatedness is a broader concept of semantic similarity. It\nmeasures the extent to which two chunks of text convey similar meaning or\ntopics, or share related concepts or contexts. This notion of relatedness can\nbe applied in various applications, such as document clustering and\nsummarizing. SemRel-2024, a shared task in SemEval-2024, aims at reducing the\ngap in the semantic relatedness task by providing datasets for fourteen\nlanguages and dialects including Arabic. This paper reports on our\nparticipation in Track A (Algerian and Moroccan dialects) and Track B (Modern\nStandard Arabic). A BERT-based model is augmented and fine-tuned for regression\nscoring in supervised track (A), while BERT-based cosine similarity is employed\nfor unsupervised track (B). Our system ranked 1st in SemRel-2024 for MSA with a\nSpearman correlation score of 0.49. We ranked 5th for Moroccan and 12th for\nAlgerian with scores of 0.83 and 0.53, respectively.", "published": "2024-05-01 17:44:05", "link": "http://arxiv.org/abs/2405.00659v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"Ask Me Anything\": How Comcast Uses LLMs to Assist Agents in Real Time", "abstract": "Customer service is how companies interface with their customers. It can\ncontribute heavily towards the overall customer satisfaction. However,\nhigh-quality service can become expensive, creating an incentive to make it as\ncost efficient as possible and prompting most companies to utilize AI-powered\nassistants, or \"chat bots\". On the other hand, human-to-human interaction is\nstill desired by customers, especially when it comes to complex scenarios such\nas disputes and sensitive topics like bill payment.\n  This raises the bar for customer service agents. They need to accurately\nunderstand the customer's question or concern, identify a solution that is\nacceptable yet feasible (and within the company's policy), all while handling\nmultiple conversations at once.\n  In this work, we introduce \"Ask Me Anything\" (AMA) as an add-on feature to an\nagent-facing customer service interface. AMA allows agents to ask questions to\na large language model (LLM) on demand, as they are handling customer\nconversations -- the LLM provides accurate responses in real-time, reducing the\namount of context switching the agent needs. In our internal experiments, we\nfind that agents using AMA versus a traditional search experience spend\napproximately 10% fewer seconds per conversation containing a search,\ntranslating to millions of dollars of savings annually. Agents that used the\nAMA feature provided positive feedback nearly 80% of the time, demonstrating\nits usefulness as an AI-assisted feature for customer care.", "published": "2024-05-01 18:31:36", "link": "http://arxiv.org/abs/2405.00801v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Uncovering Agendas: A Novel French & English Dataset for Agenda\n  Detection on Social Media", "abstract": "The behavior and decision making of groups or communities can be dramatically\ninfluenced by individuals pushing particular agendas, e.g., to promote or\ndisparage a person or an activity, to call for action, etc.. In the examination\nof online influence campaigns, particularly those related to important\npolitical and social events, scholars often concentrate on identifying the\nsources responsible for setting and controlling the agenda (e.g., public\nmedia). In this article we present a methodology for detecting specific\ninstances of agenda control through social media where annotated data is\nlimited or non-existent. By using a modest corpus of Twitter messages centered\non the 2022 French Presidential Elections, we carry out a comprehensive\nevaluation of various approaches and techniques that can be applied to this\nproblem. Our findings demonstrate that by treating the task as a textual\nentailment problem, it is possible to overcome the requirement for a large\nannotated training dataset.", "published": "2024-05-01 19:02:35", "link": "http://arxiv.org/abs/2405.00821v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WIBA: What Is Being Argued? A Comprehensive Approach to Argument Mining", "abstract": "We propose WIBA, a novel framework and suite of methods that enable the\ncomprehensive understanding of \"What Is Being Argued\" across contexts. Our\napproach develops a comprehensive framework that detects: (a) the existence,\n(b) the topic, and (c) the stance of an argument, correctly accounting for the\nlogical dependence among the three tasks. Our algorithm leverages the\nfine-tuning and prompt-engineering of Large Language Models. We evaluate our\napproach and show that it performs well in all the three capabilities. First,\nwe develop and release an Argument Detection model that can classify a piece of\ntext as an argument with an F1 score between 79% and 86% on three different\nbenchmark datasets. Second, we release a language model that can identify the\ntopic being argued in a sentence, be it implicit or explicit, with an average\nsimilarity score of 71%, outperforming current naive methods by nearly 40%.\nFinally, we develop a method for Argument Stance Classification, and evaluate\nthe capability of our approach, showing it achieves a classification F1 score\nbetween 71% and 78% across three diverse benchmark datasets. Our evaluation\ndemonstrates that WIBA allows the comprehensive understanding of What Is Being\nArgued in large corpora across diverse contexts, which is of core interest to\nmany applications in linguistics, communication, and social and computer\nscience. To facilitate accessibility to the advancements outlined in this work,\nwe release WIBA as a free open access platform (wiba.dev).", "published": "2024-05-01 19:31:13", "link": "http://arxiv.org/abs/2405.00828v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Math Multiple Choice Question Generation via Human-Large Language Model\n  Collaboration", "abstract": "Multiple choice questions (MCQs) are a popular method for evaluating\nstudents' knowledge due to their efficiency in administration and grading.\nCrafting high-quality math MCQs is a labor-intensive process that requires\neducators to formulate precise stems and plausible distractors. Recent advances\nin large language models (LLMs) have sparked interest in automating MCQ\ncreation, but challenges persist in ensuring mathematical accuracy and\naddressing student errors. This paper introduces a prototype tool designed to\nfacilitate collaboration between LLMs and educators for streamlining the math\nMCQ generation process. We conduct a pilot study involving math educators to\ninvestigate how the tool can help them simplify the process of crafting\nhigh-quality math MCQs. We found that while LLMs can generate well-formulated\nquestion stems, their ability to generate distractors that capture common\nstudent errors and misconceptions is limited. Nevertheless, a human-AI\ncollaboration has the potential to enhance the efficiency and effectiveness of\nMCQ generation.", "published": "2024-05-01 20:53:13", "link": "http://arxiv.org/abs/2405.00864v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DynaMo: Accelerating Language Model Inference with Dynamic Multi-Token\n  Sampling", "abstract": "Traditional language models operate autoregressively, i.e., they predict one\ntoken at a time. Rapid explosion in model sizes has resulted in high inference\ntimes. In this work, we propose DynaMo, a suite of multi-token prediction\nlanguage models that reduce net inference times. Our models\n$\\textit{dynamically}$ predict multiple tokens based on their confidence in the\npredicted joint probability distribution. We propose a lightweight technique to\ntrain these models, leveraging the weights of traditional autoregressive\ncounterparts. Moreover, we propose novel ways to enhance the estimated joint\nprobability to improve text generation quality, namely co-occurrence weighted\nmasking and adaptive thresholding. We also propose systematic qualitative and\nquantitative methods to rigorously test the quality of generated text for\nnon-autoregressive generation. One of the models in our suite, DynaMo-7.3B-T3,\nachieves same-quality generated text as the baseline (Pythia-6.9B) while\nachieving 2.57$\\times$ speed-up with only 5.87% and 2.67% parameter and\ntraining time overheads, respectively.", "published": "2024-05-01 22:17:57", "link": "http://arxiv.org/abs/2405.00888v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Named Entity Recognition and Topic Modeling-based Solution for\n  Locating and Better Assessment of Natural Disasters in Social Media", "abstract": "Over the last decade, similar to other application domains, social media\ncontent has been proven very effective in disaster informatics. However, due to\nthe unstructured nature of the data, several challenges are associated with\ndisaster analysis in social media content. To fully explore the potential of\nsocial media content in disaster informatics, access to relevant content and\nthe correct geo-location information is very critical. In this paper, we\npropose a three-step solution to tackling these challenges. Firstly, the\nproposed solution aims to classify social media posts into relevant and\nirrelevant posts followed by the automatic extraction of location information\nfrom the posts' text through Named Entity Recognition (NER) analysis. Finally,\nto quickly analyze the topics covered in large volumes of social media posts,\nwe perform topic modeling resulting in a list of top keywords, that highlight\nthe issues discussed in the tweet. For the Relevant Classification of Twitter\nPosts (RCTP), we proposed a merit-based fusion framework combining the\ncapabilities of four different models namely BERT, RoBERTa, Distil BERT, and\nALBERT obtaining the highest F1-score of 0.933 on a benchmark dataset. For the\nLocation Extraction from Twitter Text (LETT), we evaluated four models namely\nBERT, RoBERTa, Distil BERTA, and Electra in an NER framework obtaining the\nhighest F1-score of 0.960. For topic modeling, we used the BERTopic library to\ndiscover the hidden topic patterns in the relevant tweets. The experimental\nresults of all the components of the proposed end-to-end solution are very\nencouraging and hint at the potential of social media content and NLP in\ndisaster management.", "published": "2024-05-01 23:19:49", "link": "http://arxiv.org/abs/2405.00903v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Social Life Simulation for Non-Cognitive Skills Learning", "abstract": "Non-cognitive skills are crucial for personal and social life well-being, and\nsuch skill development can be supported by narrative-based (e.g., storytelling)\ntechnologies. While generative AI enables interactive and role-playing\nstorytelling, little is known about how users engage with and perceive the use\nof AI in social life simulation for non-cognitive skills learning.\nAdditionally, the benefits of AI mentorship on self-reflection awareness and\nability in this context remain largely underexplored. To this end, we\nintroduced Simulife++, an interactive platform enabled by a large language\nmodel (LLM). The system allows users to act as protagonists, creating stories\nwith one or multiple AI-based characters in diverse social scenarios. In\nparticular, we expanded the Human-AI interaction to a Human-AI-AI collaboration\nby including a Sage Agent, who acts as a bystander, providing users with some\nperspectives and guidance on their choices and conversations in terms of\nnon-cognitive skills to promote reflection. In a within-subject user study, our\nquantitative results reveal that, when accompanied by Sage Agent, users exhibit\nsignificantly higher levels of reflection on motivation, self-perceptions, and\nresilience & coping, along with an enhanced experience of narrative\ntransportation. Additionally, our qualitative findings suggest that Sage Agent\nplays a crucial role in promoting reflection on non-cognitive skills, enhancing\nsocial communication and decision-making performance, and improving overall\nuser experience within Simulife++. Multiple supportive relationships between\nSage Agent and users were also reported. We offer design implications for the\napplication of generative AI in narrative solutions and the future potential of\nSage Agent for non-cognitive skill development in broader social contexts.", "published": "2024-05-01 01:45:50", "link": "http://arxiv.org/abs/2405.00273v3", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Adversarial Attacks and Defense for Conversation Entailment Task", "abstract": "As the deployment of NLP systems in critical applications grows, ensuring the\nrobustness of large language models (LLMs) against adversarial attacks becomes\nincreasingly important. Large language models excel in various NLP tasks but\nremain vulnerable to low-cost adversarial attacks. Focusing on the domain of\nconversation entailment, where multi-turn dialogues serve as premises to verify\nhypotheses, we fine-tune a transformer model to accurately discern the\ntruthfulness of these hypotheses. Adversaries manipulate hypotheses through\nsynonym swapping, aiming to deceive the model into making incorrect\npredictions. To counteract these attacks, we implemented innovative fine-tuning\ntechniques and introduced an embedding perturbation loss method to\nsignificantly bolster the model's robustness. Our findings not only emphasize\nthe importance of defending against adversarial attacks in NLP but also\nhighlight the real-world implications, suggesting that enhancing model\nrobustness is critical for reliable NLP applications.", "published": "2024-05-01 02:49:18", "link": "http://arxiv.org/abs/2405.00289v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MetaRM: Shifted Distributions Alignment via Meta-Learning", "abstract": "The success of Reinforcement Learning from Human Feedback (RLHF) in language\nmodel alignment is critically dependent on the capability of the reward model\n(RM). However, as the training process progresses, the output distribution of\nthe policy model shifts, leading to the RM's reduced ability to distinguish\nbetween responses. This issue is further compounded when the RM, trained on a\nspecific data distribution, struggles to generalize to examples outside of that\ndistribution. These two issues can be united as a challenge posed by the\nshifted distribution of the environment. To surmount this challenge, we\nintroduce MetaRM, a method leveraging meta-learning to align the RM with the\nshifted environment distribution. MetaRM is designed to train the RM by\nminimizing data loss, particularly for data that can improve the\ndifferentiation ability to examples of the shifted target distribution.\nExtensive experiments demonstrate that MetaRM significantly improves the RM's\ndistinguishing ability in iterative RLHF optimization, and also provides the\ncapacity to identify subtle differences in out-of-distribution samples.", "published": "2024-05-01 10:43:55", "link": "http://arxiv.org/abs/2405.00438v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Is Temperature the Creativity Parameter of Large Language Models?", "abstract": "Large language models (LLMs) are applied to all sorts of creative tasks, and\ntheir outputs vary from beautiful, to peculiar, to pastiche, into plain\nplagiarism. The temperature parameter of an LLM regulates the amount of\nrandomness, leading to more diverse outputs; therefore, it is often claimed to\nbe the creativity parameter. Here, we investigate this claim using a narrative\ngeneration task with a predetermined fixed context, model and prompt.\nSpecifically, we present an empirical analysis of the LLM output for different\ntemperature values using four necessary conditions for creativity in narrative\ngeneration: novelty, typicality, cohesion, and coherence. We find that\ntemperature is weakly correlated with novelty, and unsurprisingly, moderately\ncorrelated with incoherence, but there is no relationship with either cohesion\nor typicality. However, the influence of temperature on creativity is far more\nnuanced and weak than suggested by the \"creativity parameter\" claim; overall\nresults suggest that the LLM generates slightly more novel outputs as\ntemperatures get higher. Finally, we discuss ideas to allow more controlled LLM\ncreativity, rather than relying on chance via changing the temperature\nparameter.", "published": "2024-05-01 12:59:37", "link": "http://arxiv.org/abs/2405.00492v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GOLD: Geometry Problem Solver with Natural Language Description", "abstract": "Addressing the challenge of automated geometry math problem-solving in\nartificial intelligence (AI) involves understanding multi-modal information and\nmathematics. Current methods struggle with accurately interpreting geometry\ndiagrams, which hinders effective problem-solving. To tackle this issue, we\npresent the Geometry problem sOlver with natural Language Description (GOLD)\nmodel. GOLD enhances the extraction of geometric relations by separately\nprocessing symbols and geometric primitives within the diagram. Subsequently,\nit converts the extracted relations into natural language descriptions,\nefficiently utilizing large language models to solve geometry math problems.\nExperiments show that the GOLD model outperforms the Geoformer model, the\nprevious best method on the UniGeo dataset, by achieving accuracy improvements\nof 12.7% and 42.1% in calculation and proving subsets. Additionally, it\nsurpasses the former best model on the PGPS9K and Geometry3K datasets, PGPSNet,\nby obtaining accuracy enhancements of 1.8% and 3.2%, respectively.", "published": "2024-05-01 13:00:51", "link": "http://arxiv.org/abs/2405.00494v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "CookingSense: A Culinary Knowledgebase with Multidisciplinary Assertions", "abstract": "This paper introduces CookingSense, a descriptive collection of knowledge\nassertions in the culinary domain extracted from various sources, including web\ndata, scientific papers, and recipes, from which knowledge covering a broad\nrange of aspects is acquired. CookingSense is constructed through a series of\ndictionary-based filtering and language model-based semantic filtering\ntechniques, which results in a rich knowledgebase of multidisciplinary\nfood-related assertions. Additionally, we present FoodBench, a novel benchmark\nto evaluate culinary decision support systems. From evaluations with FoodBench,\nwe empirically prove that CookingSense improves the performance of retrieval\naugmented language models. We also validate the quality and variety of\nassertions in CookingSense through qualitative analysis.", "published": "2024-05-01 13:58:09", "link": "http://arxiv.org/abs/2405.00523v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "A Legal Framework for Natural Language Processing Model Training in\n  Portugal", "abstract": "Recent advances in deep learning have promoted the advent of many\ncomputational systems capable of performing intelligent actions that, until\nthen, were restricted to the human intellect. In the particular case of human\nlanguages, these advances allowed the introduction of applications like ChatGPT\nthat are capable of generating coherent text without being explicitly\nprogrammed to do so. Instead, these models use large volumes of textual data to\nlearn meaningful representations of human languages. Associated with these\nadvances, concerns about copyright and data privacy infringements caused by\nthese applications have emerged. Despite these concerns, the pace at which new\nnatural language processing applications continued to be developed largely\noutperformed the introduction of new regulations. Today, communication barriers\nbetween legal experts and computer scientists motivate many unintentional legal\ninfringements during the development of such applications. In this paper, a\nmultidisciplinary team intends to bridge this communication gap and promote\nmore compliant Portuguese NLP research by presenting a series of everyday NLP\nuse cases, while highlighting the Portuguese legislation that may arise during\nits development.", "published": "2024-05-01 14:18:50", "link": "http://arxiv.org/abs/2405.00536v1", "categories": ["cs.CL", "cs.ET", "68T50", "K.5.0"], "primary_category": "cs.CL"}
{"title": "New Benchmark Dataset and Fine-Grained Cross-Modal Fusion Framework for\n  Vietnamese Multimodal Aspect-Category Sentiment Analysis", "abstract": "The emergence of multimodal data on social media platforms presents new\nopportunities to better understand user sentiments toward a given aspect.\nHowever, existing multimodal datasets for Aspect-Category Sentiment Analysis\n(ACSA) often focus on textual annotations, neglecting fine-grained information\nin images. Consequently, these datasets fail to fully exploit the richness\ninherent in multimodal. To address this, we introduce a new Vietnamese\nmultimodal dataset, named ViMACSA, which consists of 4,876 text-image pairs\nwith 14,618 fine-grained annotations for both text and image in the hotel\ndomain. Additionally, we propose a Fine-Grained Cross-Modal Fusion Framework\n(FCMF) that effectively learns both intra- and inter-modality interactions and\nthen fuses these information to produce a unified multimodal representation.\nExperimental results show that our framework outperforms SOTA models on the\nViMACSA dataset, achieving the highest F1 score of 79.73%. We also explore\ncharacteristics and challenges in Vietnamese multimodal sentiment analysis,\nincluding misspellings, abbreviations, and the complexities of the Vietnamese\nlanguage. This work contributes both a benchmark dataset and a new framework\nthat leverages fine-grained multimodal information to improve multimodal\naspect-category sentiment analysis. Our dataset is available for research\npurposes:\nhttps://github.com/hoangquy18/Multimodal-Aspect-Category-Sentiment-Analysis.", "published": "2024-05-01 14:29:03", "link": "http://arxiv.org/abs/2405.00543v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mixture of insighTful Experts (MoTE): The Synergy of Thought Chains and\n  Expert Mixtures in Self-Alignment", "abstract": "As the capabilities of large language models (LLMs) continue to expand,\naligning these models with human values remains a significant challenge. Recent\nstudies show that reasoning abilities contribute significantly to model safety,\nwhile integrating Mixture-of-Experts (MoE) architectures can further enhance\nalignment. In this work, we propose Mixture of insighTful Experts (MoTE), a\nnovel framework that synergistically combines reasoning chains and expert\nmixtures to improve self-alignments. From a data perspective, MoTE employs a\nstructured reasoning chain comprising four key stages: Question Analysis,\nAnswer Guidance, Safe Answer, and Safety Checking. This approach enhances\nsafety through multi-step reasoning and proves effective even for smaller and\nless powerful LLMs (e.g., 7B models). From an architectural perspective, MoTE\nadopts a multi-LoRA framework with step-level routing, where each expert is\ndedicated to a specific reasoning step. This design eliminates the need for\nbalance losses, ensures stable training, and supports adaptive inference\nlengths. Experimental results demonstrate that MoTE significantly improves\nmodel safety, jailbreak resistance, and over-refusal capabilities, achieving\nperformance comparable to OpenAI's state-of-the-art o1 model.", "published": "2024-05-01 15:06:05", "link": "http://arxiv.org/abs/2405.00557v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Real, the Better: Aligning Large Language Models with Online Human\n  Behaviors", "abstract": "Large language model alignment is widely used and studied to avoid LLM\nproducing unhelpful and harmful responses. However, the lengthy training\nprocess and predefined preference bias hinder adaptation to online diverse\nhuman preferences. To this end, this paper proposes an alignment framework,\ncalled Reinforcement Learning with Human Behavior (RLHB), to align LLMs by\ndirectly leveraging real online human behaviors. By taking the generative\nadversarial framework, the generator is trained to respond following expected\nhuman behavior; while the discriminator tries to verify whether the triplets of\nquery, response, and human behavior come from real online environments.\nBehavior modeling in natural-language form and the multi-model joint training\nmechanism enable an active and sustainable online alignment. Experimental\nresults confirm the effectiveness of our proposed methods by both human and\nautomatic evaluations.", "published": "2024-05-01 15:30:41", "link": "http://arxiv.org/abs/2405.00578v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Investigating Automatic Scoring and Feedback using Large Language Models", "abstract": "Automatic grading and feedback have been long studied using traditional\nmachine learning and deep learning techniques using language models. With the\nrecent accessibility to high performing large language models (LLMs) like\nLLaMA-2, there is an opportunity to investigate the use of these LLMs for\nautomatic grading and feedback generation. Despite the increase in performance,\nLLMs require significant computational resources for fine-tuning and additional\nspecific adjustments to enhance their performance for such tasks. To address\nthese issues, Parameter Efficient Fine-tuning (PEFT) methods, such as LoRA and\nQLoRA, have been adopted to decrease memory and computational requirements in\nmodel fine-tuning. This paper explores the efficacy of PEFT-based quantized\nmodels, employing classification or regression head, to fine-tune LLMs for\nautomatically assigning continuous numerical grades to short answers and\nessays, as well as generating corresponding feedback. We conducted experiments\non both proprietary and open-source datasets for our tasks. The results show\nthat prediction of grade scores via finetuned LLMs are highly accurate,\nachieving less than 3% error in grade percentage on average. For providing\ngraded feedback fine-tuned 4-bit quantized LLaMA-2 13B models outperform\ncompetitive base models and achieve high similarity with subject matter expert\nfeedback in terms of high BLEU and ROUGE scores and qualitatively in terms of\nfeedback. The findings from this study provide important insights into the\nimpacts of the emerging capabilities of using quantization approaches to\nfine-tune LLMs for various downstream tasks, such as automatic short answer\nscoring and feedback generation at comparatively lower costs and latency.", "published": "2024-05-01 16:13:54", "link": "http://arxiv.org/abs/2405.00602v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "When Quantization Affects Confidence of Large Language Models?", "abstract": "Recent studies introduced effective compression techniques for Large Language\nModels (LLMs) via post-training quantization or low-bit weight representation.\nAlthough quantized weights offer storage efficiency and allow for faster\ninference, existing works have indicated that quantization might compromise\nperformance and exacerbate biases in LLMs. This study investigates the\nconfidence and calibration of quantized models, considering factors such as\nlanguage model type and scale as contributors to quantization loss. Firstly, we\nreveal that quantization with GPTQ to 4-bit results in a decrease in confidence\nregarding true labels, with varying impacts observed among different language\nmodels. Secondly, we observe fluctuations in the impact on confidence across\ndifferent scales. Finally, we propose an explanation for quantization loss\nbased on confidence levels, indicating that quantization disproportionately\naffects samples where the full model exhibited low confidence levels in the\nfirst place.", "published": "2024-05-01 16:58:28", "link": "http://arxiv.org/abs/2405.00632v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Efficient Sample-Specific Encoder Perturbations", "abstract": "Encoder-decoder foundation models have displayed state-of-the-art performance\non a range of autoregressive sequence tasks. This paper proposes a simple and\nlightweight modification to such systems to control the behaviour according to\na specific attribute of interest. This paper proposes a novel\ninference-efficient approach to modifying the behaviour of an encoder-decoder\nsystem according to a specific attribute of interest. Specifically, we show\nthat a small proxy network can be used to find a sample-by-sample perturbation\nof the encoder output of a frozen foundation model to trigger the decoder to\ngenerate improved decodings. This work explores a specific realization of this\nframework focused on improving the COMET performance of Flan-T5 on Machine\nTranslation and the WER of Whisper foundation models on Speech Recognition.\nResults display consistent improvements in performance evaluated through COMET\nand WER respectively. Furthermore, experiments also show that the proxies are\nrobust to the exact nature of the data used to train them and can extend to\nother domains.", "published": "2024-05-01 08:55:16", "link": "http://arxiv.org/abs/2405.01601v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Extracting chemical food safety hazards from the scientific literature\n  automatically using large language models", "abstract": "The number of scientific articles published in the domain of food safety has\nconsistently been increasing over the last few decades. It has therefore become\nunfeasible for food safety experts to read all relevant literature related to\nfood safety and the occurrence of hazards in the food chain. However, it is\nimportant that food safety experts are aware of the newest findings and can\naccess this information in an easy and concise way. In this study, an approach\nis presented to automate the extraction of chemical hazards from the scientific\nliterature through large language models. The large language model was used\nout-of-the-box and applied on scientific abstracts; no extra training of the\nmodels or a large computing cluster was required. Three different styles of\nprompting the model were tested to assess which was the most optimal for the\ntask at hand. The prompts were optimized with two validation foods (leafy\ngreens and shellfish) and the final performance of the best prompt was\nevaluated using three test foods (dairy, maize and salmon). The specific\nwording of the prompt was found to have a considerable effect on the results. A\nprompt breaking the task down into smaller steps performed best overall. This\nprompt reached an average accuracy of 93% and contained many chemical\ncontaminants already included in food monitoring programs, validating the\nsuccessful retrieval of relevant hazards for the food safety domain. The\nresults showcase how valuable large language models can be for the task of\nautomatic information extraction from the scientific literature.", "published": "2024-05-01 08:02:10", "link": "http://arxiv.org/abs/2405.15787v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Clover: Regressive Lightweight Speculative Decoding with Sequential\n  Knowledge", "abstract": "Large language models (LLMs) suffer from low efficiency as the mismatch\nbetween the requirement of auto-regressive decoding and the design of most\ncontemporary GPUs. Specifically, billions to trillions of parameters must be\nloaded to the GPU cache through its limited memory bandwidth for computation,\nbut only a small batch of tokens is actually computed. Consequently, the GPU\nspends most of its time on memory transfer instead of computation. Recently,\nparallel decoding, a type of speculative decoding algorithms, is becoming more\npopular and has demonstrated impressive efficiency improvement in generation.\nIt introduces extra decoding heads to large models, enabling them to predict\nmultiple subsequent tokens simultaneously and verify these candidate\ncontinuations in a single decoding step. However, this approach deviates from\nthe training objective of next token prediction used during pre-training,\nresulting in a low hit rate for candidate tokens. In this paper, we propose a\nnew speculative decoding algorithm, Clover, which integrates sequential\nknowledge into the parallel decoding process. This enhancement improves the hit\nrate of speculators and thus boosts the overall efficiency. Clover transmits\nthe sequential knowledge from pre-speculated tokens via the Regressive\nConnection, then employs an Attention Decoder to integrate these speculated\ntokens. Additionally, Clover incorporates an Augmenting Block that modifies the\nhidden states to better align with the purpose of speculative generation rather\nthan next token prediction. The experiment results demonstrate that Clover\noutperforms the baseline by up to 91% on Baichuan-Small and 146% on\nBaichuan-Large, respectively, and exceeds the performance of the previously\ntop-performing method, Medusa, by up to 37% on Baichuan-Small and 57% on\nBaichuan-Large, respectively.", "published": "2024-05-01 00:46:22", "link": "http://arxiv.org/abs/2405.00263v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "How Can I Improve? Using GPT to Highlight the Desired and Undesired\n  Parts of Open-ended Responses", "abstract": "Automated explanatory feedback systems play a crucial role in facilitating\nlearning for a large cohort of learners by offering feedback that incorporates\nexplanations, significantly enhancing the learning process. However, delivering\nsuch explanatory feedback in real-time poses challenges, particularly when high\nclassification accuracy for domain-specific, nuanced responses is essential.\nOur study leverages the capabilities of large language models, specifically\nGenerative Pre-Trained Transformers (GPT), to explore a sequence labeling\napproach focused on identifying components of desired and less desired praise\nfor providing explanatory feedback within a tutor training dataset. Our aim is\nto equip tutors with actionable, explanatory feedback during online training\nlessons. To investigate the potential of GPT models for providing the\nexplanatory feedback, we employed two commonly-used approaches: prompting and\nfine-tuning. To quantify the quality of highlighted praise components\nidentified by GPT models, we introduced a Modified Intersection over Union\n(M-IoU) score. Our findings demonstrate that: (1) the M-IoU score effectively\ncorrelates with human judgment in evaluating sequence quality; (2) using\ntwo-shot prompting on GPT-3.5 resulted in decent performance in recognizing\neffort-based (M-IoU of 0.46) and outcome-based praise (M-IoU of 0.68); and (3)\nour optimally fine-tuned GPT-3.5 model achieved M-IoU scores of 0.64 for\neffort-based praise and 0.84 for outcome-based praise, aligning with the\nsatisfaction levels evaluated by human coders. Our results show promise for\nusing GPT models to provide feedback that focuses on specific elements in their\nopen-ended responses that are desirable or could use improvement.", "published": "2024-05-01 02:59:10", "link": "http://arxiv.org/abs/2405.00291v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "A Careful Examination of Large Language Model Performance on Grade\n  School Arithmetic", "abstract": "Large language models (LLMs) have achieved impressive success on many\nbenchmarks for mathematical reasoning. However, there is growing concern that\nsome of this performance actually reflects dataset contamination, where data\nclosely resembling benchmark questions leaks into the training data, instead of\ntrue reasoning ability. To investigate this claim rigorously, we commission\nGrade School Math 1000 (GSM1k). GSM1k is designed to mirror the style and\ncomplexity of the established GSM8k benchmark, the gold standard for measuring\nelementary mathematical reasoning. We ensure that the two benchmarks are\ncomparable across important metrics such as human solve rates, number of steps\nin solution, answer magnitude, and more. When evaluating leading open- and\nclosed-source LLMs on GSM1k, we observe accuracy drops of up to 8%, with\nseveral families of models showing evidence of systematic overfitting across\nalmost all model sizes. Further analysis suggests a positive relationship\n(Spearman's r^2 = 0.36) between a model's probability of generating an example\nfrom GSM8k and its performance gap between GSM8k and GSM1k, suggesting that\nsome models may have partially memorized GSM8k. Nevertheless, many models,\nespecially those on the frontier, show minimal signs of overfitting, and all\nmodels broadly demonstrate generalization to novel math problems guaranteed to\nnot be in their training data.", "published": "2024-05-01 05:52:05", "link": "http://arxiv.org/abs/2405.00332v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Enhancing Surgical Robots with Embodied Intelligence for Autonomous\n  Ultrasound Scanning", "abstract": "Ultrasound robots are increasingly used in medical diagnostics and early\ndisease screening. However, current ultrasound robots lack the intelligence to\nunderstand human intentions and instructions, hindering autonomous ultrasound\nscanning. To solve this problem, we propose a novel Ultrasound Embodied\nIntelligence system that equips ultrasound robots with the large language model\n(LLM) and domain knowledge, thereby improving the efficiency of ultrasound\nrobots. Specifically, we first design an ultrasound operation knowledge\ndatabase to add expertise in ultrasound scanning to the LLM, enabling the LLM\nto perform precise motion planning. Furthermore, we devise a dynamic ultrasound\nscanning strategy based on a \\textit{think-observe-execute} prompt engineering,\nallowing LLMs to dynamically adjust motion planning strategies during the\nscanning procedures. Extensive experiments demonstrate that our system\nsignificantly improves ultrasound scan efficiency and quality from verbal\ncommands. This advancement in autonomous medical scanning technology\ncontributes to non-invasive diagnostics and streamlined medical workflows.", "published": "2024-05-01 11:39:38", "link": "http://arxiv.org/abs/2405.00461v1", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.RO"}
{"title": "Explainable Automatic Grading with Neural Additive Models", "abstract": "The use of automatic short answer grading (ASAG) models may help alleviate\nthe time burden of grading while encouraging educators to frequently\nincorporate open-ended items in their curriculum. However, current\nstate-of-the-art ASAG models are large neural networks (NN) often described as\n\"black box\", providing no explanation for which characteristics of an input are\nimportant for the produced output. This inexplicable nature can be frustrating\nto teachers and students when trying to interpret, or learn from an\nautomatically-generated grade. To create a powerful yet intelligible ASAG\nmodel, we experiment with a type of model called a Neural Additive Model that\ncombines the performance of a NN with the explainability of an additive model.\nWe use a Knowledge Integration (KI) framework from the learning sciences to\nguide feature engineering to create inputs that reflect whether a student\nincludes certain ideas in their response. We hypothesize that indicating the\ninclusion (or exclusion) of predefined ideas as features will be sufficient for\nthe NAM to have good predictive power and interpretability, as this may guide a\nhuman scorer using a KI rubric. We compare the performance of the NAM with\nanother explainable model, logistic regression, using the same features, and to\na non-explainable neural model, DeBERTa, that does not require feature\nengineering.", "published": "2024-05-01 12:56:14", "link": "http://arxiv.org/abs/2405.00489v1", "categories": ["cs.LG", "cs.CL", "stat.AP"], "primary_category": "cs.LG"}
{"title": "Navigating WebAI: Training Agents to Complete Web Tasks with Large\n  Language Models and Reinforcement Learning", "abstract": "Recent advancements in language models have demonstrated remarkable\nimprovements in various natural language processing (NLP) tasks such as web\nnavigation. Supervised learning (SL) approaches have achieved impressive\nperformance while utilizing significantly less training data compared to\nprevious methods. However, these SL-based models fall short when compared to\nreinforcement learning (RL) approaches, which have shown superior results. In\nthis paper, we propose a novel approach that combines SL and RL techniques over\nthe MiniWoB benchmark to leverage the strengths of both methods. We also\naddress a critical limitation in previous models' understanding of HTML\ncontent, revealing a tendency to memorize target elements rather than\ncomprehend the underlying structure. To rectify this, we propose methods to\nenhance true understanding and present a new baseline of results. Our\nexperiments demonstrate that our approach outperforms previous SL methods on\ncertain tasks using less data and narrows the performance gap with RL models,\nachieving 43.58\\% average accuracy in SL and 36.69\\% when combined with a\nmultimodal RL approach. This study sets a new direction for future web\nnavigation and offers insights into the limitations and potential of language\nmodeling for computer tasks.", "published": "2024-05-01 13:51:45", "link": "http://arxiv.org/abs/2405.00516v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "68T07", "I.2.7; I.2.8; I.2.1"], "primary_category": "cs.LG"}
{"title": "NumLLM: Numeric-Sensitive Large Language Model for Chinese Finance", "abstract": "Recently, many works have proposed various financial large language models\n(FinLLMs) by pre-training from scratch or fine-tuning open-sourced LLMs on\nfinancial corpora. However, existing FinLLMs exhibit unsatisfactory performance\nin understanding financial text when numeric variables are involved in\nquestions. In this paper, we propose a novel LLM, called numeric-sensitive\nlarge language model (NumLLM), for Chinese finance. We first construct a\nfinancial corpus from financial textbooks which is essential for improving\nnumeric capability of LLMs during fine-tuning. After that, we train two\nindividual low-rank adaptation (LoRA) modules by fine-tuning on our constructed\nfinancial corpus. One module is for adapting general-purpose LLMs to financial\ndomain, and the other module is for enhancing the ability of NumLLM to\nunderstand financial text with numeric variables. Lastly, we merge the two LoRA\nmodules into the foundation model to obtain NumLLM for inference. Experiments\non financial question-answering benchmark show that NumLLM can boost the\nperformance of the foundation model and can achieve the best overall\nperformance compared to all baselines, on both numeric and non-numeric\nquestions.", "published": "2024-05-01 15:17:27", "link": "http://arxiv.org/abs/2405.00566v1", "categories": ["cs.CE", "cs.CL", "q-fin.GN"], "primary_category": "cs.CE"}
{"title": "Causal Evaluation of Language Models", "abstract": "Causal reasoning is viewed as crucial for achieving human-level machine\nintelligence. Recent advances in language models have expanded the horizons of\nartificial intelligence across various domains, sparking inquiries into their\npotential for causal reasoning. In this work, we introduce Causal evaluation of\nLanguage Models (CaLM), which, to the best of our knowledge, is the first\ncomprehensive benchmark for evaluating the causal reasoning capabilities of\nlanguage models. First, we propose the CaLM framework, which establishes a\nfoundational taxonomy consisting of four modules: causal target (i.e., what to\nevaluate), adaptation (i.e., how to obtain the results), metric (i.e., how to\nmeasure the results), and error (i.e., how to analyze the bad results). This\ntaxonomy defines a broad evaluation design space while systematically selecting\ncriteria and priorities. Second, we compose the CaLM dataset, comprising\n126,334 data samples, to provide curated sets of causal targets, adaptations,\nmetrics, and errors, offering extensive coverage for diverse research pursuits.\nThird, we conduct an extensive evaluation of 28 leading language models on a\ncore set of 92 causal targets, 9 adaptations, 7 metrics, and 12 error types.\nFourth, we perform detailed analyses of the evaluation results across various\ndimensions (e.g., adaptation, scale). Fifth, we present 50 high-level empirical\nfindings across 9 dimensions (e.g., model), providing valuable guidance for\nfuture language model development. Finally, we develop a multifaceted platform,\nincluding a website, leaderboards, datasets, and toolkits, to support scalable\nand adaptable assessments. We envision CaLM as an ever-evolving benchmark for\nthe community, systematically updated with new causal targets, adaptations,\nmodels, metrics, and error types to reflect ongoing research advancements.\nProject website is at https://opencausalab.github.io/CaLM.", "published": "2024-05-01 16:43:21", "link": "http://arxiv.org/abs/2405.00622v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "RST-LoRA: A Discourse-Aware Low-Rank Adaptation for Long Document\n  Abstractive Summarization", "abstract": "For long document summarization, discourse structure is important to discern\nthe key content of the text and the differences in importance level between\nsentences. Unfortunately, the integration of rhetorical structure theory (RST)\ninto parameter-efficient fine-tuning strategies for long document summarization\nremains unexplored. Therefore, this paper introduces RST-LoRA and proposes four\nRST-aware variants to explicitly incorporate RST into the LoRA model. Our\nempirical evaluation demonstrates that incorporating the type and uncertainty\nof rhetorical relations can complementarily enhance the performance of LoRA in\nsummarization tasks. Furthermore, the best-performing variant we introduced\noutperforms the vanilla LoRA and full-parameter fine-tuning models, as\nconfirmed by multiple automatic and human evaluations, and even surpasses\nprevious state-of-the-art methods.", "published": "2024-05-01 17:37:50", "link": "http://arxiv.org/abs/2405.00657v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Is Bigger Edit Batch Size Always Better? -- An Empirical Study on Model\n  Editing with Llama-3", "abstract": "This study presents a targeted model editing analysis focused on the latest\nlarge language model, Llama-3. We explore the efficacy of popular model editing\ntechniques - ROME, MEMIT, and EMMET, which are designed for precise layer\ninterventions. We identify the most effective layers for targeted edits through\nan evaluation that encompasses up to 4096 edits across three distinct\nstrategies: sequential editing, batch editing, and a hybrid approach we call as\nsequential-batch editing. Our findings indicate that increasing edit\nbatch-sizes may degrade model performance more significantly than using smaller\nedit batches sequentially for equal number of edits. With this, we argue that\nsequential model editing is an important component for scaling model editing\nmethods and future research should focus on methods that combine both batched\nand sequential editing. This observation suggests a potential limitation in\ncurrent model editing methods which push towards bigger edit batch sizes, and\nwe hope it paves way for future investigations into optimizing batch sizes and\nmodel editing performance.", "published": "2024-05-01 17:50:37", "link": "http://arxiv.org/abs/2405.00664v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Self-Play Preference Optimization for Language Model Alignment", "abstract": "Standard reinforcement learning from human feedback (RLHF) approaches relying\non parametric models like the Bradley-Terry model fall short in capturing the\nintransitivity and irrationality in human preferences. Recent advancements\nsuggest that directly working with preference probabilities can yield a more\naccurate reflection of human preferences, enabling more flexible and accurate\nlanguage model alignment. In this paper, we propose a self-play-based method\nfor language model alignment, which treats the problem as a constant-sum\ntwo-player game aimed at identifying the Nash equilibrium policy. Our approach,\ndubbed Self-Play Preference Optimization (SPPO), utilizes iterative policy\nupdates to provably approximate the Nash equilibrium. Additionally, we propose\na new SPPO objective which is both strongly motivated by theory and is simple\nand effective in practice. In our experiments, using only 60k prompts (without\nresponses) from the UltraFeedback dataset and without any prompt augmentation,\nby leveraging a pre-trained preference model PairRM with only 0.4B parameters,\nSPPO can obtain a model from fine-tuning Mistral-7B-Instruct-v0.2 that achieves\nthe state-of-the-art length-controlled win-rate of 28.53% against GPT-4-Turbo\non AlpacaEval 2.0. It also outperforms the (iterative) DPO and IPO on MT-Bench,\nArena-Hard, and the Open LLM Leaderboard. Starting from a stronger base model\nLlama-3-8B-Instruct, we are able to achieve a length-controlled win rate of\n38.77%. Notably, the strong performance of SPPO is achieved without additional\nexternal supervision (e.g., responses, preferences, etc.) from GPT-4 or other\nstronger language models. Codes are available at\nhttps://github.com/uclaml/SPPO.", "published": "2024-05-01 17:59:20", "link": "http://arxiv.org/abs/2405.00675v5", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "WorkBench: a Benchmark Dataset for Agents in a Realistic Workplace\n  Setting", "abstract": "We introduce WorkBench: a benchmark dataset for evaluating agents' ability to\nexecute tasks in a workplace setting. WorkBench contains a sandbox environment\nwith five databases, 26 tools, and 690 tasks. These tasks represent common\nbusiness activities, such as sending emails and scheduling meetings. The tasks\nin WorkBench are challenging as they require planning, tool selection, and\noften multiple actions. If a task has been successfully executed, one (or more)\nof the database values may change. The correct outcome for each task is unique\nand unambiguous, which allows for robust, automated evaluation. We call this\nkey contribution outcome-centric evaluation. We evaluate five existing ReAct\nagents on WorkBench, finding they successfully complete as few as 3% of tasks\n(Llama2-70B), and just 43% for the best-performing (GPT-4). We further find\nthat agents' errors can result in the wrong action being taken, such as an\nemail being sent to the wrong person. WorkBench reveals weaknesses in agents'\nability to undertake common business activities, raising questions about their\nuse in high-stakes workplace settings. WorkBench is publicly available as a\nfree resource at https://github.com/olly-styles/WorkBench.", "published": "2024-05-01 19:07:03", "link": "http://arxiv.org/abs/2405.00823v2", "categories": ["cs.CL", "cs.AI", "cs.MA"], "primary_category": "cs.CL"}
{"title": "Characterising the Creative Process in Humans and Large Language Models", "abstract": "Large language models appear quite creative, often performing on par with the\naverage human on creative tasks. However, research on LLM creativity has\nfocused solely on \\textit{products}, with little attention on the creative\n\\textit{process}. Process analyses of human creativity often require hand-coded\ncategories or exploit response times, which do not apply to LLMs. We provide an\nautomated method to characterise how humans and LLMs explore semantic spaces on\nthe Alternate Uses Task, and contrast with behaviour in a Verbal Fluency Task.\nWe use sentence embeddings to identify response categories and compute semantic\nsimilarities, which we use to generate jump profiles. Our results corroborate\nearlier work in humans reporting both persistent (deep search in few semantic\nspaces) and flexible (broad search across multiple semantic spaces) pathways to\ncreativity, where both pathways lead to similar creativity scores. LLMs were\nfound to be biased towards either persistent or flexible paths, that varied\nacross tasks. Though LLMs as a population match human profiles, their\nrelationship with creativity is different, where the more flexible models score\nhigher on creativity. Our dataset and scripts are available on\n\\href{https://github.com/surabhisnath/Creative_Process}{GitHub}.", "published": "2024-05-01 23:06:46", "link": "http://arxiv.org/abs/2405.00899v2", "categories": ["cs.HC", "cs.AI", "cs.CL", "q-bio.NC"], "primary_category": "cs.HC"}
{"title": "RAG-based Explainable Prediction of Road Users Behaviors for Automated\n  Driving using Knowledge Graphs and Large Language Models", "abstract": "Prediction of road users' behaviors in the context of autonomous driving has\ngained considerable attention by the scientific community in the last years.\nMost works focus on predicting behaviors based on kinematic information alone,\na simplification of the reality since road users are humans, and as such they\nare highly influenced by their surrounding context. In addition, a large\nplethora of research works rely on powerful Deep Learning techniques, which\nexhibit high performance metrics in prediction tasks but may lack the ability\nto fully understand and exploit the contextual semantic information contained\nin the road scene, not to mention their inability to provide explainable\npredictions that can be understood by humans. In this work, we propose an\nexplainable road users' behavior prediction system that integrates the\nreasoning abilities of Knowledge Graphs (KG) and the expressiveness\ncapabilities of Large Language Models (LLM) by using Retrieval Augmented\nGeneration (RAG) techniques. For that purpose, Knowledge Graph Embeddings (KGE)\nand Bayesian inference are combined to allow the deployment of a fully\ninductive reasoning system that enables the issuing of predictions that rely on\nlegacy information contained in the graph as well as on current evidence\ngathered in real time by onboard sensors. Two use cases have been implemented\nfollowing the proposed approach: 1) Prediction of pedestrians' crossing\nactions; 2) Prediction of lane change maneuvers. In both cases, the performance\nattained surpasses the current state of the art in terms of anticipation and\nF1-score, showing a promising avenue for future research in this field.", "published": "2024-05-01 11:06:31", "link": "http://arxiv.org/abs/2405.00449v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IR", "cs.NE"], "primary_category": "cs.LG"}
{"title": "Are Models Biased on Text without Gender-related Language?", "abstract": "Gender bias research has been pivotal in revealing undesirable behaviors in\nlarge language models, exposing serious gender stereotypes associated with\noccupations, and emotions. A key observation in prior work is that models\nreinforce stereotypes as a consequence of the gendered correlations that are\npresent in the training data. In this paper, we focus on bias where the effect\nfrom training data is unclear, and instead address the question: Do language\nmodels still exhibit gender bias in non-stereotypical settings? To do so, we\nintroduce UnStereoEval (USE), a novel framework tailored for investigating\ngender bias in stereotype-free scenarios. USE defines a sentence-level score\nbased on pretraining data statistics to determine if the sentence contain\nminimal word-gender associations. To systematically benchmark the fairness of\npopular language models in stereotype-free scenarios, we utilize USE to\nautomatically generate benchmarks without any gender-related language. By\nleveraging USE's sentence-level score, we also repurpose prior gender bias\nbenchmarks (Winobias and Winogender) for non-stereotypical evaluation.\nSurprisingly, we find low fairness across all 28 tested models. Concretely,\nmodels demonstrate fair behavior in only 9%-41% of stereotype-free sentences,\nsuggesting that bias does not solely stem from the presence of gender-related\nwords. These results raise important questions about where underlying model\nbiases come from and highlight the need for more systematic and comprehensive\nbias evaluation. We release the full dataset and code at\nhttps://ucinlp.github.io/unstereo-eval.", "published": "2024-05-01 15:51:15", "link": "http://arxiv.org/abs/2405.00588v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DAM: A Universal Dual Attention Mechanism for Multimodal Timeseries\n  Cryptocurrency Trend Forecasting", "abstract": "In the distributed systems landscape, Blockchain has catalyzed the rise of\ncryptocurrencies, merging enhanced security and decentralization with\nsignificant investment opportunities. Despite their potential, current research\non cryptocurrency trend forecasting often falls short by simplistically merging\nsentiment data without fully considering the nuanced interplay between\nfinancial market dynamics and external sentiment influences. This paper\npresents a novel Dual Attention Mechanism (DAM) for forecasting cryptocurrency\ntrends using multimodal time-series data. Our approach, which integrates\ncritical cryptocurrency metrics with sentiment data from news and social media\nanalyzed through CryptoBERT, addresses the inherent volatility and prediction\nchallenges in cryptocurrency markets. By combining elements of distributed\nsystems, natural language processing, and financial forecasting, our method\noutperforms conventional models like LSTM and Transformer by up to 20\\% in\nprediction accuracy. This advancement deepens the understanding of distributed\nsystems and has practical implications in financial markets, benefiting\nstakeholders in cryptocurrency and blockchain technologies. Moreover, our\nenhanced forecasting approach can significantly support decentralized science\n(DeSci) by facilitating strategic planning and the efficient adoption of\nblockchain technologies, improving operational efficiency and financial risk\nmanagement in the rapidly evolving digital asset domain, thus ensuring optimal\nresource allocation.", "published": "2024-05-01 13:58:01", "link": "http://arxiv.org/abs/2405.00522v1", "categories": ["econ.GN", "cs.CE", "cs.CL", "cs.CR", "q-fin.CP", "q-fin.EC"], "primary_category": "econ.GN"}
{"title": "Learning Expressive Disentangled Speech Representations with Soft Speech\n  Units and Adversarial Style Augmentation", "abstract": "Voice conversion is the task to transform voice characteristics of source\nspeech while preserving content information. Nowadays, self-supervised\nrepresentation learning models are increasingly utilized in content extraction.\nHowever, in these representations, a lot of hidden speaker information leads to\ntimbre leakage while the prosodic information of hidden units lacks use. To\naddress these issues, we propose a novel framework for expressive voice\nconversion called \"SAVC\" based on soft speech units from HuBert-soft. Taking\nsoft speech units as input, we design an attribute encoder to extract content\nand prosody features respectively. Specifically, we first introduce statistic\nperturbation imposed by adversarial style augmentation to eliminate speaker\ninformation. Then the prosody is implicitly modeled on soft speech units with\nknowledge distillation. Experiment results show that the intelligibility and\nnaturalness of converted speech outperform previous work.", "published": "2024-05-01 16:14:22", "link": "http://arxiv.org/abs/2405.00603v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Active Learning with Task Adaptation Pre-training for Speech Emotion\n  Recognition", "abstract": "Speech emotion recognition (SER) has garnered increasing attention due to its\nwide range of applications in various fields, including human-machine\ninteraction, virtual assistants, and mental health assistance. However,\nexisting SER methods often overlook the information gap between the\npre-training speech recognition task and the downstream SER task, resulting in\nsub-optimal performance. Moreover, current methods require much time for\nfine-tuning on each specific speech dataset, such as IEMOCAP, which limits\ntheir effectiveness in real-world scenarios with large-scale noisy data. To\naddress these issues, we propose an active learning (AL)-based fine-tuning\nframework for SER, called \\textsc{After}, that leverages task adaptation\npre-training (TAPT) and AL methods to enhance performance and efficiency.\nSpecifically, we first use TAPT to minimize the information gap between the\npre-training speech recognition task and the downstream speech emotion\nrecognition task. Then, AL methods are employed to iteratively select a subset\nof the most informative and diverse samples for fine-tuning, thereby reducing\ntime consumption. Experiments demonstrate that our proposed method\n\\textsc{After}, using only 20\\% of samples, improves accuracy by 8.45\\% and\nreduces time consumption by 79\\%. The additional extension of \\textsc{After}\nand ablation studies further confirm its effectiveness and applicability to\nvarious real-world scenarios. Our source code is available on Github for\nreproducibility. (https://github.com/Clearloveyuan/AFTER).", "published": "2024-05-01 04:05:29", "link": "http://arxiv.org/abs/2405.00307v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Distance Sampling-based Paraphraser Leveraging ChatGPT for Text Data\n  Manipulation", "abstract": "There has been growing interest in audio-language retrieval research, where\nthe objective is to establish the correlation between audio and text\nmodalities. However, most audio-text paired datasets often lack rich expression\nof the text data compared to the audio samples. One of the significant\nchallenges facing audio-text datasets is the presence of similar or identical\ncaptions despite different audio samples. Therefore, under many-to-one mapping\nconditions, audio-text datasets lead to poor performance of retrieval tasks. In\nthis paper, we propose a novel approach to tackle the data imbalance problem in\naudio-language retrieval task. To overcome the limitation, we introduce a\nmethod that employs a distance sampling-based paraphraser leveraging ChatGPT,\nutilizing distance function to generate a controllable distribution of\nmanipulated text data. For a set of sentences with the same context, the\ndistance is used to calculate a degree of manipulation for any two sentences,\nand ChatGPT's few-shot prompting is performed using a text cluster with a\nsimilar distance defined by the Jaccard similarity. Therefore, ChatGPT, when\napplied to few-shot prompting with text clusters, can adjust the diversity of\nthe manipulated text based on the distance. The proposed approach is shown to\nsignificantly enhance performance in audio-text retrieval, outperforming\nconventional text augmentation techniques.", "published": "2024-05-01 07:44:28", "link": "http://arxiv.org/abs/2405.00367v1", "categories": ["cs.IR", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.IR"}
{"title": "Visual and audio scene classification for detecting discrepancies in\n  video: a baseline method and experimental protocol", "abstract": "This paper presents a baseline approach and an experimental protocol for a\nspecific content verification problem: detecting discrepancies between the\naudio and video modalities in multimedia content. We first design and optimize\nan audio-visual scene classifier, to compare with existing classification\nbaselines that use both modalities. Then, by applying this classifier\nseparately to the audio and the visual modality, we can detect scene-class\ninconsistencies between them. To facilitate further research and provide a\ncommon evaluation platform, we introduce an experimental protocol and a\nbenchmark dataset simulating such inconsistencies. Our approach achieves\nstate-of-the-art results in scene classification and promising outcomes in\naudio-visual discrepancies detection, highlighting its potential in content\nverification applications.", "published": "2024-05-01 08:30:58", "link": "http://arxiv.org/abs/2405.00384v1", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
