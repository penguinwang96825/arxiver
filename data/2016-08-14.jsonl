{"title": "Proceedings of the LexSem+Logics Workshop 2016", "abstract": "Lexical semantics continues to play an important role in driving research\ndirections in NLP, with the recognition and understanding of context becoming\nincreasingly important in delivering successful outcomes in NLP tasks. Besides\ntraditional processing areas such as word sense and named entity\ndisambiguation, the creation and maintenance of dictionaries, annotated corpora\nand resources have become cornerstones of lexical semantics research and\nproduced a wealth of contextual information that NLP processes can exploit. New\nefforts both to link and construct from scratch such information - as Linked\nOpen Data or by way of formal tools coming from logic, ontologies and automated\nreasoning - have increased the interoperability and accessibility of resources\nfor lexical and computational semantics, even in those languages for which they\nhave previously been limited.\n  LexSem+Logics 2016 combines the 1st Workshop on Lexical Semantics for\nLesser-Resources Languages and the 3rd Workshop on Logics and Ontologies. The\naccepted papers in our program covered topics across these two areas,\nincluding: the encoding of plurals in Wordnets, the creation of a thesaurus\nfrom multiple sources based on semantic similarity metrics, and the use of\ncross-lingual treebanks and annotations for universal part-of-speech tagging.\nWe also welcomed talks from two distinguished speakers: on Portuguese lexical\nknowledge bases (different approaches, results and their application in NLP\ntasks) and on new strategies for open information extraction (the capture of\nverb-based propositions from massive text corpora).", "published": "2016-08-14 16:23:55", "link": "http://arxiv.org/abs/1608.04767v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Numerically Grounded Language Models for Semantic Error Correction", "abstract": "Semantic error detection and correction is an important task for applications\nsuch as fact checking, speech-to-text or grammatical error correction. Current\napproaches generally focus on relatively shallow semantics and do not account\nfor numeric quantities. Our approach uses language models grounded in numbers\nwithin the text. Such groundings are easily achieved for recurrent neural\nlanguage model architectures, which can be further conditioned on incomplete\nbackground knowledge bases. Our evaluation on clinical reports shows that\nnumerical grounding improves perplexity by 33% and F1 for semantic error\ncorrection by 5 points when compared to ungrounded approaches. Conditioning on\na knowledge base yields further improvements.", "published": "2016-08-14 22:34:22", "link": "http://arxiv.org/abs/1608.04147v1", "categories": ["cs.CL", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Viewpoint and Topic Modeling of Current Events", "abstract": "There are multiple sides to every story, and while statistical topic models\nhave been highly successful at topically summarizing the stories in corpora of\ntext documents, they do not explicitly address the issue of learning the\ndifferent sides, the viewpoints, expressed in the documents. In this paper, we\nshow how these viewpoints can be learned completely unsupervised and\nrepresented in a human interpretable form. We use a novel approach of applying\nCorrLDA2 for this purpose, which learns topic-viewpoint relations that can be\nused to form groups of topics, where each group represents a viewpoint. A\ncorpus of documents about the Israeli-Palestinian conflict is then used to\ndemonstrate how a Palestinian and an Israeli viewpoint can be learned. By\nleveraging the magnitudes and signs of the feature weights of a linear SVM, we\nintroduce a principled method to evaluate associations between topics and\nviewpoints. With this, we demonstrate, both quantitatively and qualitatively,\nthat the learned topic groups are contextually coherent, and form consistently\ncorrect topic-viewpoint associations.", "published": "2016-08-14 11:36:52", "link": "http://arxiv.org/abs/1608.04089v1", "categories": ["cs.CL", "cs.IR", "stat.ML"], "primary_category": "cs.CL"}
