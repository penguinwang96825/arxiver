{"title": "NovAScore: A New Automated Metric for Evaluating Document Level Novelty", "abstract": "The rapid expansion of online content has intensified the issue of\ninformation redundancy, underscoring the need for solutions that can identify\ngenuinely new information. Despite this challenge, the research community has\nseen a decline in focus on novelty detection, particularly with the rise of\nlarge language models (LLMs). Additionally, previous approaches have relied\nheavily on human annotation, which is time-consuming, costly, and particularly\nchallenging when annotators must compare a target document against a vast\nnumber of historical documents. In this work, we introduce NovAScore (Novelty\nEvaluation in Atomicity Score), an automated metric for evaluating\ndocument-level novelty. NovAScore aggregates the novelty and salience scores of\natomic information, providing high interpretability and a detailed analysis of\na document's novelty. With its dynamic weight adjustment scheme, NovAScore\noffers enhanced flexibility and an additional dimension to assess both the\nnovelty level and the importance of information within a document. Our\nexperiments show that NovAScore strongly correlates with human judgments of\nnovelty, achieving a 0.626 Point-Biserial correlation on the TAP-DLND 1.0\ndataset and a 0.920 Pearson correlation on an internal human-annotated dataset.", "published": "2024-09-14 01:21:56", "link": "http://arxiv.org/abs/2409.09249v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analyzing Correlations Between Intrinsic and Extrinsic Bias Metrics of\n  Static Word Embeddings With Their Measuring Biases Aligned", "abstract": "We examine the abilities of intrinsic bias metrics of static word embeddings\nto predict whether Natural Language Processing (NLP) systems exhibit biased\nbehavior. A word embedding is one of the fundamental NLP technologies that\nrepresents the meanings of words through real vectors, and problematically, it\nalso learns social biases such as stereotypes. An intrinsic bias metric\nmeasures bias by examining a characteristic of vectors, while an extrinsic bias\nmetric checks whether an NLP system trained with a word embedding is biased. A\nprevious study found that a common intrinsic bias metric usually does not\ncorrelate with extrinsic bias metrics. However, the intrinsic and extrinsic\nbias metrics did not measure the same bias in most cases, which makes us\nquestion whether the lack of correlation is genuine. In this paper, we extract\ncharacteristic words from datasets of extrinsic bias metrics and analyze\ncorrelations with intrinsic bias metrics with those words to ensure both\nmetrics measure the same bias. We observed moderate to high correlations with\nsome extrinsic bias metrics but little to no correlations with the others. This\nresult suggests that intrinsic bias metrics can predict biased behavior in\nparticular settings but not in others. Experiment codes are available at\nGitHub.", "published": "2024-09-14 02:13:56", "link": "http://arxiv.org/abs/2409.09260v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Compressive Memory-based Retrieval Approach for Event Argument\n  Extraction", "abstract": "Recent works have demonstrated the effectiveness of retrieval augmentation in\nthe Event Argument Extraction (EAE) task. However, existing retrieval-based EAE\nmethods have two main limitations: (1) input length constraints and (2) the gap\nbetween the retriever and the inference model. These issues limit the diversity\nand quality of the retrieved information. In this paper, we propose a\nCompressive Memory-based Retrieval (CMR) mechanism for EAE, which addresses the\ntwo limitations mentioned above. Our compressive memory, designed as a dynamic\nmatrix that effectively caches retrieved information and supports continuous\nupdates, overcomes the limitations of the input length. Additionally, after\npre-loading all candidate demonstrations into the compressive memory, the model\nfurther retrieves and filters relevant information from memory based on the\ninput query, bridging the gap between the retriever and the inference model.\nExtensive experiments show that our method achieves new state-of-the-art\nperformance on three public datasets (RAMS, WikiEvents, ACE05), significantly\noutperforming existing retrieval-based EAE methods.", "published": "2024-09-14 05:51:50", "link": "http://arxiv.org/abs/2409.09322v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating Event-oriented Attribution for Movies via Two-Stage\n  Prefix-Enhanced Multimodal LLM", "abstract": "The prosperity of social media platforms has raised the urgent demand for\nsemantic-rich services, e.g., event and storyline attribution. However, most\nexisting research focuses on clip-level event understanding, primarily through\nbasic captioning tasks, without analyzing the causes of events across an entire\nmovie. This is a significant challenge, as even advanced multimodal large\nlanguage models (MLLMs) struggle with extensive multimodal information due to\nlimited context length. To address this issue, we propose a Two-Stage\nPrefix-Enhanced MLLM (TSPE) approach for event attribution, i.e., connecting\nassociated events with their causal semantics, in movie videos. In the local\nstage, we introduce an interaction-aware prefix that guides the model to focus\non the relevant multimodal information within a single clip, briefly\nsummarizing the single event. Correspondingly, in the global stage, we\nstrengthen the connections between associated events using an inferential\nknowledge graph, and design an event-aware prefix that directs the model to\nfocus on associated events rather than all preceding clips, resulting in\naccurate event attribution. Comprehensive evaluations of two real-world\ndatasets demonstrate that our framework outperforms state-of-the-art methods.", "published": "2024-09-14 08:30:59", "link": "http://arxiv.org/abs/2409.09362v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Diverse and Efficient Audio Captioning via Diffusion Models", "abstract": "We introduce Diffusion-based Audio Captioning (DAC), a non-autoregressive\ndiffusion model tailored for diverse and efficient audio captioning. Although\nexisting captioning models relying on language backbones have achieved\nremarkable success in various captioning tasks, their insufficient performance\nin terms of generation speed and diversity impede progress in audio\nunderstanding and multimedia applications. Our diffusion-based framework offers\nunique advantages stemming from its inherent stochasticity and holistic context\nmodeling in captioning. Through rigorous evaluation, we demonstrate that DAC\nnot only achieves SOTA performance levels compared to existing benchmarks in\nthe caption quality, but also significantly outperforms them in terms of\ngeneration speed and diversity. The success of DAC illustrates that text\ngeneration can also be seamlessly integrated with audio and visual generation\ntasks using a diffusion backbone, paving the way for a unified, audio-related\ngenerative model across different modalities.", "published": "2024-09-14 10:23:35", "link": "http://arxiv.org/abs/2409.09401v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Keeping Humans in the Loop: Human-Centered Automated Annotation with\n  Generative AI", "abstract": "Automated text annotation is a compelling use case for generative large\nlanguage models (LLMs) in social media research. Recent work suggests that LLMs\ncan achieve strong performance on annotation tasks; however, these studies\nevaluate LLMs on a small number of tasks and likely suffer from contamination\ndue to a reliance on public benchmark datasets. Here, we test a human-centered\nframework for responsibly evaluating artificial intelligence tools used in\nautomated annotation. We use GPT-4 to replicate 27 annotation tasks across 11\npassword-protected datasets from recently published computational social\nscience articles in high-impact journals. For each task, we compare GPT-4\nannotations against human-annotated ground-truth labels and against annotations\nfrom separate supervised classification models fine-tuned on human-generated\nlabels. Although the quality of LLM labels is generally high, we find\nsignificant variation in LLM performance across tasks, even within datasets.\nOur findings underscore the importance of a human-centered workflow and careful\nevaluation standards: Automated annotations significantly diverge from human\njudgment in numerous scenarios, despite various optimization strategies such as\nprompt tuning. Grounding automated annotation in validation labels generated by\nhumans is essential for responsible evaluation.", "published": "2024-09-14 15:27:43", "link": "http://arxiv.org/abs/2409.09467v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Uddessho: An Extensive Benchmark Dataset for Multimodal Author Intent\n  Classification in Low-Resource Bangla Language", "abstract": "With the increasing popularity of daily information sharing and acquisition\non the Internet, this paper introduces an innovative approach for intent\nclassification in Bangla language, focusing on social media posts where\nindividuals share their thoughts and opinions. The proposed method leverages\nmultimodal data with particular emphasis on authorship identification, aiming\nto understand the underlying purpose behind textual content, especially in the\ncontext of varied user-generated posts on social media. Current methods often\nface challenges in low-resource languages like Bangla, particularly when author\ntraits intricately link with intent, as observed in social media posts. To\naddress this, we present the Multimodal-based Author Bangla Intent\nClassification (MABIC) framework, utilizing text and images to gain deeper\ninsights into the conveyed intentions. We have created a dataset named\n\"Uddessho,\" comprising 3,048 instances sourced from social media. Our\nmethodology comprises two approaches for classifying textual intent and\nmultimodal author intent, incorporating early fusion and late fusion\ntechniques. In our experiments, the unimodal approach achieved an accuracy of\n64.53% in interpreting Bangla textual intent. In contrast, our multimodal\napproach significantly outperformed traditional unimodal methods, achieving an\naccuracy of 76.19%. This represents an improvement of 11.66%. To our best\nknowledge, this is the first research work on multimodal-based author intent\nclassification for low-resource Bangla language social media posts.", "published": "2024-09-14 18:37:27", "link": "http://arxiv.org/abs/2409.09504v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparing Retrieval-Augmentation and Parameter-Efficient Fine-Tuning for\n  Privacy-Preserving Personalization of Large Language Models", "abstract": "Privacy-preserving methods for personalizing large language models (LLMs) are\nrelatively under-explored. There are two schools of thought on this topic: (1)\ngenerating personalized outputs by personalizing the input prompt through\nretrieval augmentation from the user's personal information (RAG-based\nmethods), and (2) parameter-efficient fine-tuning of LLMs per user that\nconsiders efficiency and space limitations (PEFT-based methods). This paper\npresents the first systematic comparison between two approaches on a wide range\nof personalization tasks using seven diverse datasets. Our results indicate\nthat RAG-based and PEFT-based personalization methods on average yield 14.92%\nand 1.07% improvements over the non-personalized LLM, respectively. We find\nthat combining RAG with PEFT elevates these improvements to 15.98%.\nAdditionally, we identify a positive correlation between the amount of user\ndata and PEFT's effectiveness, indicating that RAG is a better choice for\ncold-start users (i.e., user's with limited personal data).", "published": "2024-09-14 19:18:26", "link": "http://arxiv.org/abs/2409.09510v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Autoregressive + Chain of Thought = Recurrent: Recurrence's Role in\n  Language Models' Computability and a Revisit of Recurrent Transformer", "abstract": "The Transformer architecture excels in a variety of language modeling tasks,\noutperforming traditional neural architectures such as RNN and LSTM. This is\npartially due to its elimination of recurrent connections, which allows for\nparallel training and a smoother flow of gradients. However, this move away\nfrom recurrent structures places the Transformer model at the lower end of\nChomsky's computational hierarchy, imposing limitations on its computational\nabilities. Consequently, even advanced Transformer-based models face\nconsiderable difficulties in tasks like counting, string reversal, and\nmultiplication. These tasks, though seemingly elementary, require a level of\ncomputational complexity that exceeds the capabilities of the Transformer\narchitecture. Concurrently, the emergence of ``Chain of Thought\" (CoT)\nprompting has enabled Transformer-based language models to tackle tasks that\nwere previously impossible or poorly executed. In this work, we thoroughly\ninvestigate the influence of recurrent structures in neural models on their\nreasoning abilities and computability, contrasting the role autoregression\nplays in the neural models' computational power. We then shed light on how the\nCoT approach can mimic recurrent computation and act as a bridge between\nautoregression and recurrence in the context of language models. It is this\napproximated recurrence that notably improves the model's performance and\ncomputational capacity. Moreover, we revisit recent recurrent-based Transformer\nmodel designs, focusing on their computational abilities through our proposed\nconcept of ``recurrence-completeness\" and identify key theoretical limitations\nin models like Linear Transformer and RWKV. Through this, we aim to provide\ninsight into the neural model architectures and prompt better model design.", "published": "2024-09-14 00:30:57", "link": "http://arxiv.org/abs/2409.09239v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An empirical evaluation of using ChatGPT to summarize disputes for\n  recommending similar labor and employment cases in Chinese", "abstract": "We present a hybrid mechanism for recommending similar cases of labor and\nemployment litigations. The classifier determines the similarity based on the\nitemized disputes of the two cases, that the courts prepared. We cluster the\ndisputes, compute the cosine similarity between the disputes, and use the\nresults as the features for the classification tasks. Experimental results\nindicate that this hybrid approach outperformed our previous system, which\nconsidered only the information about the clusters of the disputes. We replaced\nthe disputes that were prepared by the courts with the itemized disputes that\nwere generated by GPT-3.5 and GPT-4, and repeated the same experiments. Using\nthe disputes generated by GPT-4 led to better results. Although our classifier\ndid not perform as well when using the disputes that the ChatGPT generated, the\nresults were satisfactory. Hence, we hope that the future large-language models\nwill become practically useful.", "published": "2024-09-14 03:08:10", "link": "http://arxiv.org/abs/2409.09280v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ODE: Open-Set Evaluation of Hallucinations in Multimodal Large Language\n  Models", "abstract": "Hallucination poses a persistent challenge for multimodal large language\nmodels (MLLMs). However, existing benchmarks for evaluating hallucinations are\ngenerally static, which may overlook the potential risk of data contamination.\nTo address this issue, we propose ODE, an open-set, dynamic protocol designed\nto evaluate object hallucinations in MLLMs at both the existence and attribute\nlevels. ODE employs a graph-based structure to represent real-world object\nconcepts, their attributes, and the distributional associations between them.\nThis structure facilitates the extraction of concept combinations based on\ndiverse distributional criteria, generating varied samples for structured\nqueries that evaluate hallucinations in both generative and discriminative\ntasks. Through the generation of new samples, dynamic concept combinations, and\nvaried distribution frequencies, ODE mitigates the risk of data contamination\nand broadens the scope of evaluation. This protocol is applicable to both\ngeneral and specialized scenarios, including those with limited data.\nExperimental results demonstrate the effectiveness of our protocol, revealing\nthat MLLMs exhibit higher hallucination rates when evaluated with ODE-generated\nsamples, which indicates potential data contamination. Furthermore, these\ngenerated samples aid in analyzing hallucination patterns and fine-tuning\nmodels, offering an effective approach to mitigating hallucinations in MLLMs.", "published": "2024-09-14 05:31:29", "link": "http://arxiv.org/abs/2409.09318v3", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Efficient Fine-Tuning of Large Language Models for Automated Medical\n  Documentation", "abstract": "Scientific research indicates that for every hour spent in direct patient\ncare, physicians spend nearly two additional hours on administrative tasks,\nparticularly on electronic health records (EHRs) and desk work. This excessive\nadministrative burden not only reduces the time available for patient care but\nalso contributes to physician burnout and inefficiencies in healthcare\ndelivery. To address these challenges, this study introduces MediGen, a\nfine-tuned large language model (LLM) designed to automate the generation of\nmedical reports from medical dialogues. By leveraging state-of-the-art\nmethodologies for fine-tuning open-source pretrained models, including\nLLaMA3-8B, MediGen achieves high accuracy in transcribing and summarizing\nclinical interactions. The fine-tuned LLaMA3-8B model demonstrated promising\nresults, achieving a ROUGE score of 58% and a BERTScore-F1 of 72%, indicating\nits effectiveness in generating accurate and clinically relevant medical\nreports. These findings suggest that MediGen has the potential to significantly\nreduce the administrative workload on physicians, improving both healthcare\nefficiency and physician well-being.", "published": "2024-09-14 06:02:17", "link": "http://arxiv.org/abs/2409.09324v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Constructive Approach to Bidirectional Causation between Qualia\n  Structure and Language Emergence", "abstract": "This paper presents a novel perspective on the bidirectional causation\nbetween language emergence and relational structure of subjective experiences,\ntermed qualia structure, and lays out the constructive approach to the\nintricate dependency between the two. We hypothesize that languages with\ndistributional semantics, e.g., syntactic-semantic structures, may have emerged\nthrough the process of aligning internal representations among individuals, and\nsuch alignment of internal representations facilitates more structured\nlanguage. This mutual dependency is suggested by the recent advancements in AI\nand symbol emergence robotics, and collective predictive coding (CPC)\nhypothesis, in particular. Computational studies show that neural network-based\nlanguage models form systematically structured internal representations, and\nmultimodal language models can share representations between language and\nperceptual information. This perspective suggests that language emergence\nserves not only as a mechanism creating a communication tool but also as a\nmechanism for allowing people to realize shared understanding of qualitative\nexperiences. The paper discusses the implications of this bidirectional\ncausation in the context of consciousness studies, linguistics, and cognitive\nscience, and outlines future constructive research directions to further\nexplore this dynamic relationship between language emergence and qualia\nstructure.", "published": "2024-09-14 11:03:12", "link": "http://arxiv.org/abs/2409.09413v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Measuring the Influence of Incorrect Code on Test Generation", "abstract": "It is natural to suppose that a Large Language Model is more likely to\ngenerate correct test cases when prompted with correct code under test,\ncompared to incorrect code under test. However, the size of this effect has\nnever been previously measured, despite its obvious importance for both\npracticing software engineers and researchers. To answer the question, we\nconducted a comprehensive empirical study on 5 open source and 6 closed source\nlanguage models, with 3 widely-used benchmark data sets together with 41\nrepo-level real-world examples from two different real-world data sets. Our\nresults reveal that, when compared to incorrect code under test, LLMs prompted\nwith correct code achieve improvements in test accuracy, code coverage, and bug\ndetection of 57\\%, 12\\%, and 24\\% respectively. We further show that these\nscientific conclusions carry over from the three benchmark data sets to the\nreal-world code, where tests generated for incorrect code experience a 47\\%\nworse bug detection rate. Finally, we report that improvements of +18\\% in\naccuracy, +4\\% coverage, and +34\\% in bug detection can be achieved by\nproviding natural language code descriptions. These findings have actionable\nconclusions. For example, the 47\\% reduction in real-world bug detection is a\nclear concern. Fortunately, it is a concern for which our findings about the\nadded value of descriptions offer an immediately actionable remedy.", "published": "2024-09-14 15:17:34", "link": "http://arxiv.org/abs/2409.09464v3", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Synthetic4Health: Generating Annotated Synthetic Clinical Letters", "abstract": "Since clinical letters contain sensitive information, clinical-related\ndatasets can not be widely applied in model training, medical research, and\nteaching. This work aims to generate reliable, various, and de-identified\nsynthetic clinical letters. To achieve this goal, we explored different\npre-trained language models (PLMs) for masking and generating text. After that,\nwe worked on Bio\\_ClinicalBERT, a high-performing model, and experimented with\ndifferent masking strategies. Both qualitative and quantitative methods were\nused for evaluation. Additionally, a downstream task, Named Entity Recognition\n(NER), was also implemented to assess the usability of these synthetic letters.\n  The results indicate that 1) encoder-only models outperform encoder-decoder\nmodels. 2) Among encoder-only models, those trained on general corpora perform\ncomparably to those trained on clinical data when clinical information is\npreserved. 3) Additionally, preserving clinical entities and document structure\nbetter aligns with our objectives than simply fine-tuning the model. 4)\nFurthermore, different masking strategies can impact the quality of synthetic\nclinical letters. Masking stopwords has a positive impact, while masking nouns\nor verbs has a negative effect. 5) For evaluation, BERTScore should be the\nprimary quantitative evaluation metric, with other metrics serving as\nsupplementary references. 6) Contextual information does not significantly\nimpact the models' understanding, so the synthetic clinical letters have the\npotential to replace the original ones in downstream tasks.", "published": "2024-09-14 18:15:07", "link": "http://arxiv.org/abs/2409.09501v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Thinking Before Speaking: A Role-playing Model with Mindset", "abstract": "Role-playing is an easy task for Large Language Models (LLMs), as they are\nskilled at simulating human behaviors. Many current studies have enabled LLMs\nto generate responses in the tone of a specific role by fine-tuning the models\nor using specialized prompts. However, it is typically easy to recognize when a\nrole is being played by LLMs. These models tend to perform poorly when\nconfronted with knowledge that the assumed role does not possess, or a question\nthat requires the specific experience or logic of the role to answer. To\naddress this problem and make LLMs act more like real roles, we propose a\nThinking Before Speaking (TBS) model in this paper. Unlike other studies, we\nfirst extend the data based on the character's real-life scenarios and the\nhistorical dialogue, supplementing each pair of dialogue with the character's\nmindset. Then we add few data points that include elements beyond the role's\nknowledge, and fine-tune the LLMs. This approach can help LLMs adopt the role's\nthought process and logic, avoiding responses that fall outside the role's\nknowledge base. We have also prepared a dataset and evaluation metrics to test\nthese capabilities. Experimental results show that our TBS model can better\nemulate a role in terms of tone, knowledge, and mindset.", "published": "2024-09-14 02:41:48", "link": "http://arxiv.org/abs/2409.13752v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unleash LLMs Potential for Recommendation by Coordinating Twin-Tower\n  Dynamic Semantic Token Generator", "abstract": "Owing to the unprecedented capability in semantic understanding and logical\nreasoning, the pre-trained large language models (LLMs) have shown fantastic\npotential in developing the next-generation recommender systems (RSs). However,\nthe static index paradigm adopted by current methods greatly restricts the\nutilization of LLMs capacity for recommendation, leading to not only the\ninsufficient alignment between semantic and collaborative knowledge, but also\nthe neglect of high-order user-item interaction patterns. In this paper, we\npropose Twin-Tower Dynamic Semantic Recommender (TTDS), the first generative RS\nwhich adopts dynamic semantic index paradigm, targeting at resolving the above\nproblems simultaneously. To be more specific, we for the first time contrive a\ndynamic knowledge fusion framework which integrates a twin-tower semantic token\ngenerator into the LLM-based recommender, hierarchically allocating meaningful\nsemantic index for items and users, and accordingly predicting the semantic\nindex of target item. Furthermore, a dual-modality variational auto-encoder is\nproposed to facilitate multi-grained alignment between semantic and\ncollaborative knowledge. Eventually, a series of novel tuning tasks specially\ncustomized for capturing high-order user-item interaction patterns are proposed\nto take advantages of user historical behavior. Extensive experiments across\nthree public datasets demonstrate the superiority of the proposed methodology\nin developing LLM-based generative RSs. The proposed TTDS recommender achieves\nan average improvement of 19.41% in Hit-Rate and 20.84% in NDCG metric,\ncompared with the leading baseline methods.", "published": "2024-09-14 01:45:04", "link": "http://arxiv.org/abs/2409.09253v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "What Is Wrong with My Model? Identifying Systematic Problems with\n  Semantic Data Slicing", "abstract": "Machine learning models make mistakes, yet sometimes it is difficult to\nidentify the systematic problems behind the mistakes. Practitioners engage in\nvarious activities, including error analysis, testing, auditing, and\nred-teaming, to form hypotheses of what can go (or has gone) wrong with their\nmodels. To validate these hypotheses, practitioners employ data slicing to\nidentify relevant examples. However, traditional data slicing is limited by\navailable features and programmatic slicing functions. In this work, we propose\nSemSlicer, a framework that supports semantic data slicing, which identifies a\nsemantically coherent slice, without the need for existing features. SemSlicer\nuses Large Language Models to annotate datasets and generate slices from any\nuser-defined slicing criteria. We show that SemSlicer generates accurate slices\nwith low cost, allows flexible trade-offs between different design dimensions,\nreliably identifies under-performing data slices, and helps practitioners\nidentify useful data slices that reflect systematic problems.", "published": "2024-09-14 02:15:50", "link": "http://arxiv.org/abs/2409.09261v1", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "Guiding Vision-Language Model Selection for Visual Question-Answering\n  Across Tasks, Domains, and Knowledge Types", "abstract": "Visual Question-Answering (VQA) has become key to user experience,\nparticularly after improved generalization capabilities of Vision-Language\nModels (VLMs). But evaluating VLMs for an application requirement using a\nstandardized framework in practical settings is still challenging. This paper\naims to solve that using an end-to-end framework. We present VQA360 - a novel\ndataset derived from established VQA benchmarks, annotated with task types,\napplication domains, and knowledge types, for a comprehensive evaluation. We\nalso introduce GoEval, a multimodal evaluation metric developed using GPT-4o,\nachieving a correlation factor of 56.71% with human judgments. Our experiments\nwith state-of-the-art VLMs reveal that no single model excels universally,\nthus, making a right choice a key design decision. Proprietary models such as\nGemini-1.5-Pro and GPT-4o-mini generally outperform others, but open-source\nmodels like InternVL-2-8B and CogVLM-2-Llama-3-19B also demonstrate competitive\nstrengths, while providing additional advantages. Our framework can also be\nextended to other tasks.", "published": "2024-09-14 02:29:36", "link": "http://arxiv.org/abs/2409.09269v3", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Language Models \"Grok\" to Copy", "abstract": "We examine the pre-training dynamics of language models, focusing on their\nability to copy text from preceding context--a fundamental skill for various\nLLM applications, including in-context learning (ICL) and retrieval-augmented\ngeneration (RAG). We propose a novel perspective that Transformer-based\nlanguage models develop copying abilities similarly to grokking, which refers\nto sudden generalization on test set long after the model fit to the training\nset. Our experiments yield three arguments: (1) The pre-training loss decreases\nrapidly, while the context copying ability of models initially lags and then\nabruptly saturates. (2) The speed of developing copying ability is independent\nof the number of tokens trained, similarly to how grokking speed is unaffected\nby dataset size as long as the data distribution is preserved. (3) Induction\nheads, the attention heads responsible for copying, form from shallow to deep\nlayers during training, mirroring the development of circuits in deeper layers\nduring grokking. We contend that the connection between grokking and context\ncopying can provide valuable insights for more effective language model\ntraining, ultimately improving in-context performance. For example, we\ndemonstrated that techniques that enhance grokking, such as regularization,\neither accelerate or enhance the development of context copying.", "published": "2024-09-14 03:11:00", "link": "http://arxiv.org/abs/2409.09281v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Overcoming linguistic barriers in code assistants: creating a QLoRA\n  adapter to improve support for Russian-language code writing instructions", "abstract": "In this paper, an approach to training and evaluating an adapter model for\nthe popular language model \"zephyr-7b-beta\" is described. The adapter was\ndeveloped to improve the performance of the base model in tasks related to\nprogramming and understanding the Russian language. Considering the high\nquality of the original model in tasks in the English language, the goal of the\nresearch was to expand its linguistic and technical spectrum. The proposed\nadapter was trained using a large and diverse dataset, including\nquestion-answer pairs related to programming, as well code-related texts in\nRussian language. The applied training methodology ensures an improvement in\nthe model's quality of answers in understanding and generating Python code\nbased on Russian instructions. We evaluated the performance of the base model\nwith the installed adapter using various metrics, comparing it to the base\nmodel as well as other state-of-the-art models in this field. The obtained\nresults showed significant improvement, both in tasks related to writing Python\ncode and in processing the Russian language, confirming the effectiveness of\nthe proposed adapter.", "published": "2024-09-14 07:49:29", "link": "http://arxiv.org/abs/2409.09353v1", "categories": ["cs.SE", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "LLM-Powered Ensemble Learning for Paper Source Tracing: A GPU-Free\n  Approach", "abstract": "We participated in the KDD CUP 2024 paper source tracing competition and\nachieved the 3rd place. This competition tasked participants with identifying\nthe reference sources (i.e., ref-sources, as referred to by the organizers of\nthe competition) of given academic papers. Unlike most teams that addressed\nthis challenge by fine-tuning pre-trained neural language models such as BERT\nor ChatGLM, our primary approach utilized closed-source large language models\n(LLMs). With recent advancements in LLM technology, closed-source LLMs have\ndemonstrated the capability to tackle complex reasoning tasks in zero-shot or\nfew-shot scenarios. Consequently, in the absence of GPUs, we employed\nclosed-source LLMs to directly generate predicted reference sources from the\nprovided papers. We further refined these predictions through ensemble\nlearning. Notably, our method was the only one among the award-winning\napproaches that did not require the use of GPUs for model training. Code\navailable at https://github.com/Cklwanfifa/KDDCUP2024-PST.", "published": "2024-09-14 09:21:46", "link": "http://arxiv.org/abs/2409.09383v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Enhancing LLM Problem Solving with REAP: Reflection, Explicit Problem\n  Deconstruction, and Advanced Prompting", "abstract": "Large Language Models (LLMs) have transformed natural language processing,\nyet improving their problem-solving capabilities, particularly for complex,\nreasoning-intensive tasks, remains a persistent challenge. This paper\nintroduces the REAP (Reflection, Explicit Problem Deconstruction, and Advanced\nPrompting) method, an innovative approach within the dynamic context generation\nframework. REAP guides LLMs through reflection on the query, deconstructing it\ninto manageable components, and generating relevant context to enhance the\nsolution process. We evaluated REAP using a dataset designed to expose LLM\nlimitations, comparing zero-shot prompting with REAP-enhanced prompts across\nsix state-of-the-art models: OpenAI's o1-preview, o1-mini, GPT-4o, GPT-4o-mini,\nGoogle's Gemini 1.5 Pro, and Claude 3.5 Sonnet. The results demonstrate notable\nperformance gains, with o1-mini improving by 40.97%, GPT-4o by 66.26%, and\nGPT-4o-mini by 112.93%. Despite the already strong baseline performance of\nOpenAI's o1-preview, modest gains were observed. Beyond performance\nimprovements, REAP offers a cost-effective solution; for example, GPT-4o-mini,\nwhich is approximately 100 times cheaper than o1-preview, delivered competitive\nresults. REAP also improves the clarity of model outputs, making it easier for\nhumans to understand the reasoning behind the results and simplifying the\nprocess of identifying and addressing any issues. These findings demonstrate\nREAP's potential to greatly improve the capabilities of LLMs, providing both\nbetter performance and increased cost-efficiency across a wide range of\napplications.", "published": "2024-09-14 11:12:07", "link": "http://arxiv.org/abs/2409.09415v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Planning Transformer: Long-Horizon Offline Reinforcement Learning with\n  Planning Tokens", "abstract": "Supervised learning approaches to offline reinforcement learning,\nparticularly those utilizing the Decision Transformer, have shown effectiveness\nin continuous environments and for sparse rewards. However, they often struggle\nwith long-horizon tasks due to the high compounding error of auto-regressive\nmodels. To overcome this limitation, we go beyond next-token prediction and\nintroduce Planning Tokens, which contain high-level, long time-scale\ninformation about the agent's future. Predicting dual time-scale tokens at\nregular intervals enables our model to use these long-horizon Planning Tokens\nas a form of implicit planning to guide its low-level policy and reduce\ncompounding error. This architectural modification significantly enhances\nperformance on long-horizon tasks, establishing a new state-of-the-art in\ncomplex D4RL environments. Additionally, we demonstrate that Planning Tokens\nimprove the interpretability of the model's policy through the interpretable\nplan visualisations and attention map.", "published": "2024-09-14 19:30:53", "link": "http://arxiv.org/abs/2409.09513v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "ASR Error Correction using Large Language Models", "abstract": "Error correction (EC) models play a crucial role in refining Automatic Speech\nRecognition (ASR) transcriptions, enhancing the readability and quality of\ntranscriptions. Without requiring access to the underlying code or model\nweights, EC can improve performance and provide domain adaptation for black-box\nASR systems. This work investigates the use of large language models (LLMs) for\nerror correction across diverse scenarios. 1-best ASR hypotheses are commonly\nused as the input to EC models. We propose building high-performance EC models\nusing ASR N-best lists which should provide more contextual information for the\ncorrection process. Additionally, the generation process of a standard EC model\nis unrestricted in the sense that any output sequence can be generated. For\nsome scenarios, such as unseen domains, this flexibility may impact\nperformance. To address this, we introduce a constrained decoding approach\nbased on the N-best list or an ASR lattice. Finally, most EC models are trained\nfor a specific ASR system requiring retraining whenever the underlying ASR\nsystem is changed. This paper explores the ability of EC models to operate on\nthe output of different ASR systems. This concept is further extended to\nzero-shot error correction using LLMs, such as ChatGPT. Experiments on three\nstandard datasets demonstrate the efficacy of our proposed methods for both\nTransducer and attention-based encoder-decoder ASR systems. In addition, the\nproposed method can serve as an effective method for model ensembling.", "published": "2024-09-14 23:33:38", "link": "http://arxiv.org/abs/2409.09554v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Synergistic Simulations: Multi-Agent Problem Solving with Large Language\n  Models", "abstract": "Large Language Models (LLMs) have increasingly demonstrated the ability to\nfacilitate the development of multi-agent systems that allow the interpretation\nof thoughts and actions generated by each individual. Promising advancements\nhave also been made in LLM-based interaction with existing worlds, particularly\nin interacting with simulated environments. This paper aims to integrate both\naforementioned topics (agents & world interaction) into a single simulation\nwhere multiple agents can work together to solve a problem, modeling how groups\nof humans can often solve problems better than individuals. By showing whether\nLLMs demonstrate the synergy of human collaboration, it could lead to\nadvancements in the applications of LLMs. We implemented two simulations: a\nphysical studio apartment with two roommates, and another where agents\ncollaborate to complete a programming task. We provide a multi-agent framework,\ndiscuss the performance of the agents in each simulation, and discuss potential\nfuture additions.", "published": "2024-09-14 21:53:35", "link": "http://arxiv.org/abs/2409.13753v1", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.ET"], "primary_category": "cs.MA"}
{"title": "Block-Attention for Efficient RAG", "abstract": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context. Instead,\nBlock-Attention divides retrieved documents into discrete blocks, with each\nblock independently calculating key-value (KV) states except for the final\nblock. In RAG scenarios, by defining each passage as a block, Block-Attention\nenables us to reuse the KV states of passages that have been seen before,\nthereby significantly reducing the latency and the computation overhead during\ninference. The implementation of Block-Attention involves block segmentation,\nposition re-encoding, and fine-tuning the LLM to adapt to the Block-Attention\nmechanism. Experiments on four RAG benchmarks demonstrate that after block\nfine-tuning, the Block-Attention model achieves performance comparable to\nself-attention models (68.4\\% vs 67.9\\% on Llama3) or even superior performance\n(62.8\\% vs 59.6\\% on Mistral). Notably, Block-Attention significantly reduces\nthe time to first token (TTFT) and floating point operations (FLOPs) to a very\nlow level. It only takes 45 ms to output the first token for an input sequence\nwith a total length of 32K. Compared to the self-attention models, the time\nconsumption and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%,\nrespectively.", "published": "2024-09-14 02:34:26", "link": "http://arxiv.org/abs/2409.15355v4", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "StressPrompt: Does Stress Impact Large Language Models and Human\n  Performance Similarly?", "abstract": "Human beings often experience stress, which can significantly influence their\nperformance. This study explores whether Large Language Models (LLMs) exhibit\nstress responses similar to those of humans and whether their performance\nfluctuates under different stress-inducing prompts. To investigate this, we\ndeveloped a novel set of prompts, termed StressPrompt, designed to induce\nvarying levels of stress. These prompts were derived from established\npsychological frameworks and carefully calibrated based on ratings from human\nparticipants. We then applied these prompts to several LLMs to assess their\nresponses across a range of tasks, including instruction-following, complex\nreasoning, and emotional intelligence. The findings suggest that LLMs, like\nhumans, perform optimally under moderate stress, consistent with the\nYerkes-Dodson law. Notably, their performance declines under both low and\nhigh-stress conditions. Our analysis further revealed that these StressPrompts\nsignificantly alter the internal states of LLMs, leading to changes in their\nneural representations that mirror human responses to stress. This research\nprovides critical insights into the operational robustness and flexibility of\nLLMs, demonstrating the importance of designing AI systems capable of\nmaintaining high performance in real-world scenarios where stress is prevalent,\nsuch as in customer service, healthcare, and emergency response contexts.\nMoreover, this study contributes to the broader AI research community by\noffering a new perspective on how LLMs handle different scenarios and their\nsimilarities to human cognition.", "published": "2024-09-14 08:32:31", "link": "http://arxiv.org/abs/2409.17167v2", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "IW-Bench: Evaluating Large Multimodal Models for Converting Image-to-Web", "abstract": "Recently advancements in large multimodal models have led to significant\nstrides in image comprehension capabilities. Despite these advancements, there\nis a lack of the robust benchmark specifically for assessing the Image-to-Web\nconversion proficiency of these large models. Primarily, it is essential to\nensure the integrity of the web elements generated. These elements comprise\nvisible and invisible categories. Previous evaluation methods (e.g., BLEU) are\nnotably susceptible to significant alterations due to the presence of invisible\nelements in Web. Furthermore, it is crucial to measure the layout information\nof web pages, referring to the positional relationships between elements, which\nis overlooked by previous work. To address challenges, we have curated and\naligned a benchmark of images and corresponding web codes (IW-Bench).\nSpecifically, we propose the Element Accuracy, which tests the completeness of\nthe elements by parsing the Document Object Model (DOM) tree. Layout Accuracy\nis also proposed to analyze the positional relationships of elements by\nconverting DOM tree into a common subsequence. Besides, we design a five-hop\nmultimodal Chain-of-Thought Prompting for better performance, which contains\nfive hop: 1) SoM prompt injection. 2) Inferring Elements. 3) Inferring Layout.\n4) Inferring Web code. 5) Reflection. Our benchmark comprises 1200 pairs of\nimages and web codes with varying levels of difficulty. We have conducted\nextensive experiments on existing large multimodal models, offering insights\ninto their performance and areas for improvement in image-to-web domain.", "published": "2024-09-14 05:38:26", "link": "http://arxiv.org/abs/2409.18980v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Evaluating Cultural Awareness of LLMs for Yoruba, Malayalam, and English", "abstract": "Although LLMs have been extremely effective in a large number of complex\ntasks, their understanding and functionality for regional languages and\ncultures are not well studied. In this paper, we explore the ability of various\nLLMs to comprehend the cultural aspects of two regional languages: Malayalam\n(state of Kerala, India) and Yoruba (West Africa). Using Hofstede's six\ncultural dimensions: Power Distance (PDI), Individualism (IDV), Motivation\ntowards Achievement and Success (MAS), Uncertainty Avoidance (UAV), Long Term\nOrientation (LTO), and Indulgence (IVR), we quantify the cultural awareness of\nLLM-based responses. We demonstrate that although LLMs show a high cultural\nsimilarity for English, they fail to capture the cultural nuances across these\n6 metrics for Malayalam and Yoruba. We also highlight the need for large-scale\nregional language LLM training with culturally enriched datasets. This will\nhave huge implications for enhancing the user experience of chat-based LLMs and\nalso improving the validity of large-scale LLM agent-based market research.", "published": "2024-09-14 02:21:17", "link": "http://arxiv.org/abs/2410.01811v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "From Text to Multimodality: Exploring the Evolution and Impact of Large\n  Language Models in Medical Practice", "abstract": "Large Language Models (LLMs) have rapidly evolved from text-based systems to\nmultimodal platforms, significantly impacting various sectors including\nhealthcare. This comprehensive review explores the progression of LLMs to\nMultimodal Large Language Models (MLLMs) and their growing influence in medical\npractice. We examine the current landscape of MLLMs in healthcare, analyzing\ntheir applications across clinical decision support, medical imaging, patient\nengagement, and research. The review highlights the unique capabilities of\nMLLMs in integrating diverse data types, such as text, images, and audio, to\nprovide more comprehensive insights into patient health. We also address the\nchallenges facing MLLM implementation, including data limitations, technical\nhurdles, and ethical considerations. By identifying key research gaps, this\npaper aims to guide future investigations in areas such as dataset development,\nmodality alignment methods, and the establishment of ethical guidelines. As\nMLLMs continue to shape the future of healthcare, understanding their potential\nand limitations is crucial for their responsible and effective integration into\nmedical practice.", "published": "2024-09-14 02:35:29", "link": "http://arxiv.org/abs/2410.01812v5", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Robust Training of Neural Networks at Arbitrary Precision and Sparsity", "abstract": "The discontinuous operations inherent in quantization and sparsification\nintroduce obstacles to backpropagation. This is particularly challenging when\ntraining deep neural networks in ultra-low precision and sparse regimes. We\npropose a novel, robust, and universal solution: a denoising affine transform\nthat stabilizes training under these challenging conditions. By formulating\nquantization and sparsification as perturbations during training, we derive a\nperturbation-resilient approach based on ridge regression. Our solution employs\na piecewise constant backbone model to ensure a performance lower bound and\nfeatures an inherent noise reduction mechanism to mitigate perturbation-induced\ncorruption. This formulation allows existing models to be trained at\narbitrarily low precision and sparsity levels with off-the-shelf recipes.\nFurthermore, our method provides a novel perspective on training temporal\nbinary neural networks, contributing to ongoing efforts to narrow the gap\nbetween artificial and biological neural networks.", "published": "2024-09-14 00:57:32", "link": "http://arxiv.org/abs/2409.09245v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.NA", "math.NA"], "primary_category": "cs.LG"}
{"title": "Audio-text Retrieval with Transformer-based Hierarchical Alignment and\n  Disentangled Cross-modal Representation", "abstract": "Most existing audio-text retrieval (ATR) approaches typically rely on a\nsingle-level interaction to associate audio and text, limiting their ability to\nalign different modalities and leading to suboptimal matches. In this work, we\npresent a novel ATR framework that leverages two-stream Transformers in\nconjunction with a Hierarchical Alignment (THA) module to identify multi-level\ncorrespondences of different Transformer blocks between audio and text.\nMoreover, current ATR methods mainly focus on learning a global-level\nrepresentation, missing out on intricate details to capture audio occurrences\nthat correspond to textual semantics. To bridge this gap, we introduce a\nDisentangled Cross-modal Representation (DCR) approach that disentangles\nhigh-dimensional features into compact latent factors to grasp fine-grained\naudio-text semantic correlations. Additionally, we develop a confidence-aware\n(CA) module to estimate the confidence of each latent factor pair and\nadaptively aggregate cross-modal latent factors to achieve local semantic\nalignment. Experiments show that our THA effectively boosts ATR performance,\nwith the DCR approach further contributing to consistent performance gains.", "published": "2024-09-14 01:54:49", "link": "http://arxiv.org/abs/2409.09256v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Subband Splitting: Simple, Efficient and Effective Technique for Solving\n  Block Permutation Problem in Determined Blind Source Separation", "abstract": "Solving the permutation problem is essential for determined blind source\nseparation (BSS). Existing methods, such as independent vector analysis (IVA)\nand independent low-rank matrix analysis (ILRMA), tackle the permutation\nproblem by modeling the co-occurrence of the frequency components of source\nsignals. One of the remaining challenges in these methods is the block\npermutation problem, which may cause severe performance degradation. In this\npaper, we propose a simple and effective technique for solving the block\npermutation problem. The proposed technique splits the entire frequency bands\ninto several overlapping subbands and sequentially applies BSS methods (e.g.,\nIVA, ILRMA, or any other method) to each subband. Since the splitting reduces\nthe size of the problem, the BSS methods can effectively work in each subband.\nThen, the permutations among the subbands are aligned by using the separation\nresult in one subband as the initial values for the other subbands.\nAdditionally, we propose SS-IVA and SS-ILRMA by combining subband splitting\n(SS) with IVA and ILRMA. Experimental results demonstrated that our technique\nremarkably improves the separation performance without increasing computational\ncost. In particular, our SS-ILRMA achieved the separation performance\ncomparable to the oracle method (frequency-domain independent component\nanalysis with the ideal permutation solver). Moreover, SS-ILRMA converged\nfaster than conventional IVA and ILRMA.", "published": "2024-09-14 04:03:57", "link": "http://arxiv.org/abs/2409.09294v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improving Robustness of Diffusion-Based Zero-Shot Speech Synthesis via\n  Stable Formant Generation", "abstract": "Diffusion models have achieved remarkable success in text-to-speech (TTS),\neven in zero-shot scenarios. Recent efforts aim to address the trade-off\nbetween inference speed and sound quality, often considered the primary\ndrawback of diffusion models. However, we find a critical mispronunciation\nissue is being overlooked. Our preliminary study reveals the unstable\npronunciation resulting from the diffusion process. Based on this observation,\nwe introduce StableForm-TTS, a novel zero-shot speech synthesis framework\ndesigned to produce robust pronunciation while maintaining the advantages of\ndiffusion modeling. By pioneering the adoption of source-filter theory in\ndiffusion TTS, we propose an elaborate architecture for stable formant\ngeneration. Experimental results on unseen speakers show that our model\noutperforms the state-of-the-art method in terms of pronunciation accuracy and\nnaturalness, with comparable speaker similarity. Moreover, our model\ndemonstrates effective scalability as both data and model sizes increase. Audio\nsamples are available online:\nhttps://deepbrainai-research.github.io/stableformtts/.", "published": "2024-09-14 05:13:48", "link": "http://arxiv.org/abs/2409.09311v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improvements of Discriminative Feature Space Training for Anomalous\n  Sound Detection in Unlabeled Conditions", "abstract": "In anomalous sound detection, the discriminative method has demonstrated\nsuperior performance. This approach constructs a discriminative feature space\nthrough the classification of the meta-information labels for normal sounds.\nThis feature space reflects the differences in machine sounds and effectively\ncaptures anomalous sounds. However, its performance significantly degrades when\nthe meta-information labels are missing. In this paper, we improve the\nperformance of a discriminative method under unlabeled conditions by two\napproaches. First, we enhance the feature extractor to perform better under\nunlabeled conditions. Our enhanced feature extractor utilizes multi-resolution\nspectrograms with a new training strategy. Second, we propose various\npseudo-labeling methods to effectively train the feature extractor. The\nexperimental evaluations show that the proposed feature extractor and\npseudo-labeling methods significantly improve performance under unlabeled\nconditions.", "published": "2024-09-14 06:22:32", "link": "http://arxiv.org/abs/2409.09332v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "E1 TTS: Simple and Fast Non-Autoregressive TTS", "abstract": "This paper introduces Easy One-Step Text-to-Speech (E1 TTS), an efficient\nnon-autoregressive zero-shot text-to-speech system based on denoising diffusion\npretraining and distribution matching distillation. The training of E1 TTS is\nstraightforward; it does not require explicit monotonic alignment between the\ntext and audio pairs. The inference of E1 TTS is efficient, requiring only one\nneural network evaluation for each utterance. Despite its sampling efficiency,\nE1 TTS achieves naturalness and speaker similarity comparable to various strong\nbaseline models. Audio samples are available at http://e1tts.github.io/ .", "published": "2024-09-14 07:44:35", "link": "http://arxiv.org/abs/2409.09351v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MacST: Multi-Accent Speech Synthesis via Text Transliteration for Accent\n  Conversion", "abstract": "In accented voice conversion or accent conversion, we seek to convert the\naccent in speech from one another while preserving speaker identity and\nsemantic content. In this study, we formulate a novel method for creating\nmulti-accented speech samples, thus pairs of accented speech samples by the\nsame speaker, through text transliteration for training accent conversion\nsystems. We begin by generating transliterated text with Large Language Models\n(LLMs), which is then fed into multilingual TTS models to synthesize accented\nEnglish speech. As a reference system, we built a sequence-to-sequence model on\nthe synthetic parallel corpus for accent conversion. We validated the proposed\nmethod for both native and non-native English speakers. Subjective and\nobjective evaluations further validate our dataset's effectiveness in accent\nconversion studies.", "published": "2024-09-14 07:46:28", "link": "http://arxiv.org/abs/2409.09352v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Integrated Multi-Level Knowledge Distillation for Enhanced Speaker\n  Verification", "abstract": "Knowledge distillation (KD) is widely used in audio tasks, such as speaker\nverification (SV), by transferring knowledge from a well-trained large model\n(the teacher) to a smaller, more compact model (the student) for efficiency and\nportability. Existing KD methods for SV often mirror those used in image\nprocessing, focusing on approximating predicted probabilities and hidden\nrepresentations. However, these methods fail to account for the multi-level\ntemporal properties of speech audio. In this paper, we propose a novel KD\nmethod, i.e., Integrated Multi-level Knowledge Distillation (IML-KD), to\ntransfer knowledge of various temporal-scale features of speech from a teacher\nmodel to a student model. In the IML-KD, temporal context information from the\nteacher model is integrated into novel Integrated Gradient-based\ninput-sensitive representations from speech segments with various durations,\nand the student model is trained to infer these representations with\nmulti-level alignment for the output. We conduct SV experiments on the\nVoxCeleb1 dataset to evaluate the proposed method. Experimental results\ndemonstrate that IML-KD significantly enhances KD performance, reducing the\nEqual Error Rate (EER) by 5%.", "published": "2024-09-14 09:39:36", "link": "http://arxiv.org/abs/2409.09389v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Channel Adaptation for Speaker Verification Using Optimal Transport with\n  Pseudo Label", "abstract": "Domain gap often degrades the performance of speaker verification (SV)\nsystems when the statistical distributions of training data and real-world test\nspeech are mismatched. Channel variation, a primary factor causing this gap, is\nless addressed than other issues (e.g., noise). Although various domain\nadaptation algorithms could be applied to handle this domain gap problem, most\nalgorithms could not take the complex distribution structure in domain\nalignment with discriminative learning. In this paper, we propose a novel\nunsupervised domain adaptation method, i.e., Joint Partial Optimal Transport\nwith Pseudo Label (JPOT-PL), to alleviate the channel mismatch problem.\nLeveraging the geometric-aware distance metric of optimal transport in\ndistribution alignment, we further design a pseudo label-based discriminative\nlearning where the pseudo label can be regarded as a new type of soft speaker\nlabel derived from the optimal coupling. With the JPOT-PL, we carry out\nexperiments on the SV channel adaptation task with VoxCeleb as the basis\ncorpus. Experiments show our method reduces EER by over 10% compared with\nseveral state-of-the-art channel adaptation algorithms.", "published": "2024-09-14 10:07:55", "link": "http://arxiv.org/abs/2409.09396v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Language-Queried Target Sound Extraction Without Parallel Training Data", "abstract": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a parallel-data-free training scheme,\nrequiring only unlabelled audio clips for TSE model training by utilizing the\ncontrastive language-audio pre-trained model (CLAP). In a vanilla\nparallel-data-free training stage, target audio is encoded using the\npre-trained CLAP audio encoder to form a condition embedding, while during\ntesting, user language queries are encoded by CLAP text encoder as the\ncondition embedding. This vanilla approach assumes perfect alignment between\ntext and audio embeddings, which is unrealistic. Two major challenges arise\nfrom training-testing mismatch: the persistent modality gap between text and\naudio and the risk of overfitting due to the exposure of rich acoustic details\nin target audio embedding during training. To address this, we propose a\nretrieval-augmented strategy. Specifically, we create an embedding cache using\naudio captions generated by a large language model (LLM). During training,\ntarget audio embeddings retrieve text embeddings from this cache to use as\ncondition embeddings, ensuring consistent modalities between training and\ntesting and eliminating information leakage. Extensive experiment results show\nthat our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability.", "published": "2024-09-14 10:15:37", "link": "http://arxiv.org/abs/2409.09398v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Leveraging Self-Supervised Learning for Speaker Diarization", "abstract": "End-to-end neural diarization has evolved considerably over the past few\nyears, but data scarcity is still a major obstacle for further improvements.\nSelf-supervised learning methods such as WavLM have shown promising performance\non several downstream tasks, but their application on speaker diarization is\nsomehow limited. In this work, we explore using WavLM to alleviate the problem\nof data scarcity for neural diarization training. We use the same pipeline as\nPyannote and improve the local end-to-end neural diarization with WavLM and\nConformer. Experiments on far-field AMI, AISHELL-4, and AliMeeting datasets\nshow that our method substantially outperforms the Pyannote baseline and\nachieves new state-of-the-art results on AMI and AISHELL-4, respectively. In\naddition, by analyzing the system performance under different data quantity\nscenarios, we show that WavLM representations are much more robust against data\nscarcity than filterbank features, enabling less data hungry training\nstrategies. Furthermore, we found that simulated data, usually used to train\nendto-end diarization models, does not help when using WavLM in our\nexperiments. Additionally, we also evaluate our model on the recent CHiME8\nNOTSOFAR-1 task where it achieves better performance than the Pyannote\nbaseline. Our source code is publicly available at\nhttps://github.com/BUTSpeechFIT/DiariZen.", "published": "2024-09-14 10:49:06", "link": "http://arxiv.org/abs/2409.09408v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Target Speaker ASR with Whisper", "abstract": "We propose a novel approach to enable the use of large, single-speaker ASR\nmodels, such as Whisper, for target speaker ASR. The key claim of this method\nis that it is much easier to model relative differences among speakers by\nlearning to condition on frame-level diarization outputs than to learn the\nspace of all speaker embeddings. We find that adding even a single bias term\nper diarization output type before the first transformer block can transform\nsingle-speaker ASR models into target-speaker ASR models. Our approach also\nsupports speaker-attributed ASR by sequentially generating transcripts for each\nspeaker in a diarization output. This simplified method outperforms baseline\nspeech separation and diarization cascade by 12.9 % absolute ORC-WER on the\nNOTSOFAR-1 dataset.", "published": "2024-09-14 21:46:00", "link": "http://arxiv.org/abs/2409.09543v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Effective Pre-Training of Audio Transformers for Sound Event Detection", "abstract": "We propose a pre-training pipeline for audio spectrogram transformers for\nframe-level sound event detection tasks. On top of common pre-training steps,\nwe add a meticulously designed training routine on AudioSet frame-level\nannotations. This includes a balanced sampler, aggressive data augmentation,\nand ensemble knowledge distillation. For five transformers, we obtain a\nsubstantial performance improvement over previously available checkpoints both\non AudioSet frame-level predictions and on frame-level sound event detection\ndownstream tasks, confirming our pipeline's effectiveness. We publish the\nresulting checkpoints that researchers can directly fine-tune to build\nhigh-performance models for sound event detection tasks.", "published": "2024-09-14 22:00:47", "link": "http://arxiv.org/abs/2409.09546v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "M$^{3}$V: A multi-modal multi-view approach for Device-Directed Speech\n  Detection", "abstract": "With the goal of more natural and human-like interaction with virtual voice\nassistants, recent research in the field has focused on full duplex interaction\nmode without relying on repeated wake-up words. This requires that in scenes\nwith complex sound sources, the voice assistant must classify utterances as\ndevice-oriented or non-device-oriented. The dual-encoder structure, which is\njointly modeled by text and speech, has become the paradigm of device-directed\nspeech detection. However, in practice, these models often produce incorrect\npredictions for unaligned input pairs due to the unavoidable errors of\nautomatic speech recognition (ASR).To address this challenge, we propose\nM$^{3}$V, a multi-modal multi-view approach for device-directed speech\ndetection, which frames we frame the problem as a multi-view learning task that\nintroduces unimodal views and a text-audio alignment view in the network\nbesides the multi-modal. Experimental results show that M$^{3}$V significantly\noutperforms models trained using only single or multi-modality and surpasses\nhuman judgment performance on ASR error data for the first time.", "published": "2024-09-14 03:24:23", "link": "http://arxiv.org/abs/2409.09284v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DSCLAP: Domain-Specific Contrastive Language-Audio Pre-Training", "abstract": "Analyzing real-world multimodal signals is an essential and challenging task\nfor intelligent voice assistants (IVAs). Mainstream approaches have achieved\nremarkable performance on various downstream tasks of IVAs with pre-trained\naudio models and text models. However, these models are pre-trained\nindependently and usually on tasks different from target domains, resulting in\nsub-optimal modality representations for downstream tasks. Moreover, in many\ndomains, collecting enough language-audio pairs is extremely hard, and\ntranscribing raw audio also requires high professional skills, making it\ndifficult or even infeasible to joint pre-training. To address these\npainpoints, we propose DSCLAP, a simple and effective framework that enables\nlanguage-audio pre-training with only raw audio signal input. Specifically,\nDSCLAP converts raw audio signals into text via an ASR system and combines a\ncontrastive learning objective and a language-audio matching objective to align\nthe audio and ASR transcriptions. We pre-train DSCLAP on 12,107 hours of\nin-vehicle domain audio. Empirical results on two downstream tasks show that\nwhile conceptually simple, DSCLAP significantly outperforms the baseline models\nin all metrics, showing great promise for domain-specific IVAs applications.", "published": "2024-09-14 03:40:48", "link": "http://arxiv.org/abs/2409.09289v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The T05 System for The VoiceMOS Challenge 2024: Transfer Learning from\n  Deep Image Classifier to Naturalness MOS Prediction of High-Quality Synthetic\n  Speech", "abstract": "We present our system (denoted as T05) for the VoiceMOS Challenge (VMC) 2024.\nOur system was designed for the VMC 2024 Track 1, which focused on the accurate\nprediction of naturalness mean opinion score (MOS) for high-quality synthetic\nspeech. In addition to a pretrained self-supervised learning (SSL)-based speech\nfeature extractor, our system incorporates a pretrained image feature extractor\nto capture the difference of synthetic speech observed in speech spectrograms.\nWe first separately train two MOS predictors that use either of an SSL-based or\nspectrogram-based feature. Then, we fine-tune the two predictors for better MOS\nprediction using the fusion of two extracted features. In the VMC 2024 Track 1,\nour T05 system achieved first place in 7 out of 16 evaluation metrics and\nsecond place in the remaining 9 metrics, with a significant difference compared\nto those ranked third and below. We also report the results of our ablation\nstudy to investigate essential factors of our system.", "published": "2024-09-14 05:03:18", "link": "http://arxiv.org/abs/2409.09305v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Wave-U-Mamba: An End-To-End Framework For High-Quality And Efficient\n  Speech Super Resolution", "abstract": "Speech Super-Resolution (SSR) is a task of enhancing low-resolution speech\nsignals by restoring missing high-frequency components. Conventional approaches\ntypically reconstruct log-mel features, followed by a vocoder that generates\nhigh-resolution speech in the waveform domain. However, as mel features lack\nphase information, this can result in performance degradation during the\nreconstruction phase. Motivated by recent advances with Selective State Spaces\nModels (SSMs), we propose a method, referred to as Wave-U-Mamba that directly\nperforms SSR in time domain. In our comparative study, including models such as\nWSRGlow, NU-Wave 2, and AudioSR, Wave-U-Mamba demonstrates superior\nperformance, achieving the lowest Log-Spectral Distance (LSD) across various\nlow-resolution sampling rates, ranging from 8 to 24 kHz. Additionally,\nsubjective human evaluations, scored using Mean Opinion Score (MOS) reveal that\nour method produces SSR with natural and human-like quality. Furthermore,\nWave-U-Mamba achieves these results while generating high-resolution speech\nover nine times faster than baseline models on a single A100 GPU, with\nparameter sizes less than 2\\% of those in the baseline models.", "published": "2024-09-14 06:52:00", "link": "http://arxiv.org/abs/2409.09337v3", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Egocentric Speaker Classification in Child-Adult Dyadic Interactions:\n  From Sensing to Computational Modeling", "abstract": "Autism spectrum disorder (ASD) is a neurodevelopmental condition\ncharacterized by challenges in social communication, repetitive behavior, and\nsensory processing. One important research area in ASD is evaluating children's\nbehavioral changes over time during treatment. The standard protocol with this\nobjective is BOSCC, which involves dyadic interactions between a child and\nclinicians performing a pre-defined set of activities. A fundamental aspect of\nunderstanding children's behavior in these interactions is automatic speech\nunderstanding, particularly identifying who speaks and when. Conventional\napproaches in this area heavily rely on speech samples recorded from a\nspectator perspective, and there is limited research on egocentric speech\nmodeling. In this study, we design an experiment to perform speech sampling in\nBOSCC interviews from an egocentric perspective using wearable sensors and\nexplore pre-training Ego4D speech samples to enhance child-adult speaker\nclassification in dyadic interactions. Our findings highlight the potential of\negocentric speech collection and pre-training to improve speaker classification\naccuracy.", "published": "2024-09-14 07:03:08", "link": "http://arxiv.org/abs/2409.09340v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Joint Semantic Knowledge Distillation and Masked Acoustic Modeling for\n  Full-band Speech Restoration with Improved Intelligibility", "abstract": "Speech restoration aims at restoring full-band speech with high quality and\nintelligibility, considering a diverse set of distortions. MaskSR is a recently\nproposed generative model for this task. As other models of its kind, MaskSR\nattains high quality but, as we show, intelligibility can be substantially\nimproved. We do so by boosting the speech encoder component of MaskSR with\npredictions of semantic representations of the target speech, using a\npre-trained self-supervised teacher model. Then, a masked language model is\nconditioned on the learned semantic features to predict acoustic tokens that\nencode low level spectral details of the target speech. We show that, with the\nsame MaskSR model capacity and inference time, the proposed model, MaskSR2,\nsignificantly reduces the word error rate, a typical metric for\nintelligibility. MaskSR2 also achieves competitive word error rate among other\nmodels, while providing superior quality. An ablation study shows the\neffectiveness of various semantic representations.", "published": "2024-09-14 08:09:55", "link": "http://arxiv.org/abs/2409.09357v1", "categories": ["cs.SD", "cs.AI", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "Prevailing Research Areas for Music AI in the Era of Foundation Models", "abstract": "In tandem with the recent advancements in foundation model research, there\nhas been a surge of generative music AI applications within the past few years.\nAs the idea of AI-generated or AI-augmented music becomes more mainstream, many\nresearchers in the music AI community may be wondering what avenues of research\nare left. With regards to music generative models, we outline the current areas\nof research with significant room for exploration. Firstly, we pose the\nquestion of foundational representation of these generative models and\ninvestigate approaches towards explainability. Next, we discuss the current\nstate of music datasets and their limitations. We then overview different\ngenerative models, forms of evaluating these models, and their computational\nconstraints/limitations. Subsequently, we highlight applications of these\ngenerative models towards extensions to multiple modalities and integration\nwith artists' workflow as well as music education systems. Finally, we survey\nthe potential copyright implications of generative music and discuss strategies\nfor protecting the rights of musicians. While it is not meant to be exhaustive,\nour survey calls to attention a variety of research directions enabled by music\nfoundation models.", "published": "2024-09-14 09:06:43", "link": "http://arxiv.org/abs/2409.09378v1", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS", "68T05, 68T20", "I.2; I.5.4; I.2.6; I.2.7; H.5.5"], "primary_category": "cs.SD"}
{"title": "Text Prompt is Not Enough: Sound Event Enhanced Prompt Adapter for\n  Target Style Audio Generation", "abstract": "Current mainstream audio generation methods primarily rely on simple text\nprompts, often failing to capture the nuanced details necessary for multi-style\naudio generation. To address this limitation, the Sound Event Enhanced Prompt\nAdapter is proposed. Unlike traditional static global style transfer, this\nmethod extracts style embedding through cross-attention between text and\nreference audio for adaptive style control. Adaptive layer normalization is\nthen utilized to enhance the model's capacity to express multiple styles.\nAdditionally, the Sound Event Reference Style Transfer Dataset (SERST) is\nintroduced for the proposed target style audio generation task, enabling\ndual-prompt audio generation using both text and audio references. Experimental\nresults demonstrate the robustness of the model, achieving state-of-the-art\nFr\\'echet Distance of 26.94 and KL Divergence of 1.82, surpassing Tango,\nAudioLDM, and AudioGen. Furthermore, the generated audio shows high similarity\nto its corresponding audio reference. The demo, code, and dataset are publicly\navailable.", "published": "2024-09-14 09:16:38", "link": "http://arxiv.org/abs/2409.09381v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "ESPnet-EZ: Python-only ESPnet for Easy Fine-tuning and Integration", "abstract": "We introduce ESPnet-EZ, an extension of the open-source speech processing\ntoolkit ESPnet, aimed at quick and easy development of speech models. ESPnet-EZ\nfocuses on two major aspects: (i) easy fine-tuning and inference of existing\nESPnet models on various tasks and (ii) easy integration with popular deep\nneural network frameworks such as PyTorch-Lightning, Hugging Face transformers\nand datasets, and Lhotse. By replacing ESPnet design choices inherited from\nKaldi with a Python-only, Bash-free interface, we dramatically reduce the\neffort required to build, debug, and use a new model. For example, to fine-tune\na speech foundation model, ESPnet-EZ, compared to ESPnet, reduces the number of\nnewly written code by 2.7x and the amount of dependent code by 6.7x while\ndramatically reducing the Bash script dependencies. The codebase of ESPnet-EZ\nis publicly available.", "published": "2024-09-14 19:03:53", "link": "http://arxiv.org/abs/2409.09506v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Explaining Deep Learning Embeddings for Speech Emotion Recognition by\n  Predicting Interpretable Acoustic Features", "abstract": "Pre-trained deep learning embeddings have consistently shown superior\nperformance over handcrafted acoustic features in speech emotion recognition\n(SER). However, unlike acoustic features with clear physical meaning, these\nembeddings lack clear interpretability. Explaining these embeddings is crucial\nfor building trust in healthcare and security applications and advancing the\nscientific understanding of the acoustic information that is encoded in them.\nThis paper proposes a modified probing approach to explain deep learning\nembeddings in the SER space. We predict interpretable acoustic features (e.g.,\nf0, loudness) from (i) the complete set of embeddings and (ii) a subset of the\nembedding dimensions identified as most important for predicting each emotion.\nIf the subset of the most important dimensions better predicts a given emotion\nthan all dimensions and also predicts specific acoustic features more\naccurately, we infer those acoustic features are important for the embedding\nmodel for the given task. We conducted experiments using the WavLM embeddings\nand eGeMAPS acoustic features as audio representations, applying our method to\nthe RAVDESS and SAVEE emotional speech datasets. Based on this evaluation, we\ndemonstrate that Energy, Frequency, Spectral, and Temporal categories of\nacoustic features provide diminishing information to SER in that order,\ndemonstrating the utility of the probing classifier method to relate embeddings\nto interpretable acoustic features.", "published": "2024-09-14 19:18:56", "link": "http://arxiv.org/abs/2409.09511v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multi-Microphone and Multi-Modal Emotion Recognition in Reverberant\n  Environment", "abstract": "This paper presents a Multi-modal Emotion Recognition (MER) system designed\nto enhance emotion recognition accuracy in challenging acoustic conditions. Our\napproach combines a modified and extended Hierarchical Token-semantic Audio\nTransformer (HTS-AT) for multi-channel audio processing with an R(2+1)D\nConvolutional Neural Networks (CNN) model for video analysis. We evaluate our\nproposed method on a reverberated version of the Ryerson audio-visual database\nof emotional speech and song (RAVDESS) dataset using synthetic and real-world\nRoom Impulse Responsess (RIRs). Our results demonstrate that integrating audio\nand video modalities yields superior performance compared to uni-modal\napproaches, especially in challenging acoustic conditions. Moreover, we show\nthat the multimodal (audiovisual) approach that utilizes multiple microphones\noutperforms its single-microphone counterpart.", "published": "2024-09-14 21:58:39", "link": "http://arxiv.org/abs/2409.09545v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SafeEar: Content Privacy-Preserving Audio Deepfake Detection", "abstract": "Text-to-Speech (TTS) and Voice Conversion (VC) models have exhibited\nremarkable performance in generating realistic and natural audio. However,\ntheir dark side, audio deepfake poses a significant threat to both society and\nindividuals. Existing countermeasures largely focus on determining the\ngenuineness of speech based on complete original audio recordings, which\nhowever often contain private content. This oversight may refrain deepfake\ndetection from many applications, particularly in scenarios involving sensitive\ninformation like business secrets. In this paper, we propose SafeEar, a novel\nframework that aims to detect deepfake audios without relying on accessing the\nspeech content within. Our key idea is to devise a neural audio codec into a\nnovel decoupling model that well separates the semantic and acoustic\ninformation from audio samples, and only use the acoustic information (e.g.,\nprosody and timbre) for deepfake detection. In this way, no semantic content\nwill be exposed to the detector. To overcome the challenge of identifying\ndiverse deepfake audio without semantic clues, we enhance our deepfake detector\nwith real-world codec augmentation. Extensive experiments conducted on four\nbenchmark datasets demonstrate SafeEar's effectiveness in detecting various\ndeepfake techniques with an equal error rate (EER) down to 2.02%.\nSimultaneously, it shields five-language speech content from being deciphered\nby both machine and human auditory analysis, demonstrated by word error rates\n(WERs) all above 93.93% and our user study. Furthermore, our benchmark\nconstructed for anti-deepfake and anti-content recovery evaluation helps\nprovide a basis for future research in the realms of audio privacy preservation\nand deepfake detection.", "published": "2024-09-14 02:45:09", "link": "http://arxiv.org/abs/2409.09272v1", "categories": ["cs.CR", "cs.AI", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
