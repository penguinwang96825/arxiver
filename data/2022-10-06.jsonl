{"title": "U3E: Unsupervised and Erasure-based Evidence Extraction for Machine\n  Reading Comprehension", "abstract": "More tasks in Machine Reading Comprehension(MRC) require, in addition to\nanswer prediction, the extraction of evidence sentences that support the\nanswer. However, the annotation of supporting evidence sentences is usually\ntime-consuming and labor-intensive. In this paper, to address this issue and\nconsidering that most of the existing extraction methods are semi-supervised,\nwe propose an unsupervised evidence extraction method (U3E). U3E takes the\nchanges after sentence-level feature erasure in the document as input,\nsimulating the decline in problem-solving ability caused by human memory\ndecline. In order to make selections on the basis of fully understanding the\nsemantics of the original text, we also propose metrics to quickly select the\noptimal memory model for this input changes. To compare U3E with typical\nevidence extraction methods and investigate its effectiveness in evidence\nextraction, we conduct experiments on different datasets. Experimental results\nshow that U3E is simple but effective, not only extracting evidence more\naccurately, but also significantly improving model performance.", "published": "2022-10-06 00:53:24", "link": "http://arxiv.org/abs/2210.02621v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning functional sections in medical conversations: iterative\n  pseudo-labeling and human-in-the-loop approach", "abstract": "Medical conversations between patients and medical professionals have\nimplicit functional sections, such as \"history taking\", \"summarization\",\n\"education\", and \"care plan.\" In this work, we are interested in learning to\nautomatically extract these sections. A direct approach would require\ncollecting large amounts of expert annotations for this task, which is\ninherently costly due to the contextual inter-and-intra variability between\nthese sections. This paper presents an approach that tackles the problem of\nlearning to classify medical dialogue into functional sections without\nrequiring a large number of annotations. Our approach combines pseudo-labeling\nand human-in-the-loop. First, we bootstrap using weak supervision with\npseudo-labeling to generate dialogue turn-level pseudo-labels and train a\ntransformer-based model, which is then applied to individual sentences to\ncreate noisy sentence-level labels. Second, we iteratively refine\nsentence-level labels using a cluster-based human-in-the-loop approach. Each\niteration requires only a few dozen annotator decisions. We evaluate the\nresults on an expert-annotated dataset of 100 dialogues and find that while our\nmodels start with 69.5% accuracy, we can iteratively improve it to 82.5%. The\ncode used to perform all experiments described in this paper can be found here:\nhttps://github.com/curai/curai-research/tree/main/functional-sections.", "published": "2022-10-06 03:33:00", "link": "http://arxiv.org/abs/2210.02658v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Explainable Abuse Detection as Intent Classification and Slot Filling", "abstract": "To proactively offer social media users a safe online experience, there is a\nneed for systems that can detect harmful posts and promptly alert platform\nmoderators. In order to guarantee the enforcement of a consistent policy,\nmoderators are provided with detailed guidelines. In contrast, most\nstate-of-the-art models learn what abuse is from labelled examples and as a\nresult base their predictions on spurious cues, such as the presence of group\nidentifiers, which can be unreliable. In this work we introduce the concept of\npolicy-aware abuse detection, abandoning the unrealistic expectation that\nsystems can reliably learn which phenomena constitute abuse from inspecting the\ndata alone. We propose a machine-friendly representation of the policy that\nmoderators wish to enforce, by breaking it down into a collection of intents\nand slots. We collect and annotate a dataset of 3,535 English posts with such\nslots, and show how architectures for intent classification and slot filling\ncan be used for abuse detection, while providing a rationale for model\ndecisions.", "published": "2022-10-06 03:33:30", "link": "http://arxiv.org/abs/2210.02659v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Look Ma, Only 400 Samples! Revisiting the Effectiveness of Automatic\n  N-Gram Rule Generation for Spelling Normalization in Filipino", "abstract": "With 84.75 million Filipinos online, the ability for models to process online\ntext is crucial for developing Filipino NLP applications. To this end, spelling\ncorrection is a crucial preprocessing step for downstream processing. However,\nthe lack of data prevents the use of language models for this task. In this\npaper, we propose an N-Gram + Damerau Levenshtein distance model with automatic\nrule extraction. We train the model on 300 samples, and show that despite\nlimited training data, it achieves good performance and outperforms other deep\nlearning approaches in terms of accuracy and edit distance. Moreover, the model\n(1) requires little compute power, (2) trains in little time, thus allowing for\nretraining, and (3) is easily interpretable, allowing for direct\ntroubleshooting, highlighting the success of traditional approaches over more\ncomplex deep learning models in settings where data is unavailable.", "published": "2022-10-06 04:41:26", "link": "http://arxiv.org/abs/2210.02675v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Teaching Neural Module Networks to Do Arithmetic", "abstract": "Answering complex questions that require multi-step multi-type reasoning over\nraw text is challenging, especially when conducting numerical reasoning. Neural\nModule Networks(NMNs), follow the programmer-interpreter framework and design\ntrainable modules to learn different reasoning skills. However, NMNs only have\nlimited reasoning abilities, and lack numerical reasoning capability. We\nup-grade NMNs by: (a) bridging the gap between its interpreter and the complex\nquestions; (b) introducing addition and subtraction modules that perform\nnumerical reasoning over numbers. On a subset of DROP, experimental results\nshow that our proposed methods enhance NMNs' numerical reasoning skills by\n17.7% improvement of F1 score and significantly outperform previous\nstate-of-the-art models.", "published": "2022-10-06 06:38:04", "link": "http://arxiv.org/abs/2210.02703v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Distilling Task-specific Logical Rules from Large Pre-trained Models", "abstract": "Logical rules, both transferable and explainable, are widely used as weakly\nsupervised signals for many downstream tasks such as named entity tagging. To\nreduce the human effort of writing rules, previous researchers adopt an\niterative approach to automatically learn logical rules from several seed\nrules. However, obtaining more seed rules can only be accomplished by extra\nhuman annotation with heavy costs. Limited by the size and quality of the seed\nrules, the model performance of previous systems is bounded. In this paper, we\ndevelop a novel framework STREAM to distill task-specific logical rules from\nlarge pre-trained models. Specifically, we borrow recent prompt-based language\nmodels as the knowledge expert to yield initial seed rules, and based on the\nformed high-quality instance pool that acts as an intermediary role, we keep\nteaching the expert to fit our task and learning task-specific logical rules.\nExperiments on three public named entity tagging benchmarks demonstrate the\neffectiveness of our proposed framework. With several predefined prompt\ntemplates, our system has gained significant improvements over previous\nstate-of-the-art methods.", "published": "2022-10-06 09:12:18", "link": "http://arxiv.org/abs/2210.02768v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are Synonym Substitution Attacks Really Synonym Substitution Attacks?", "abstract": "In this paper, we explore the following question: Are synonym substitution\nattacks really synonym substitution attacks (SSAs)? We approach this question\nby examining how SSAs replace words in the original sentence and show that\nthere are still unresolved obstacles that make current SSAs generate invalid\nadversarial samples. We reveal that four widely used word substitution methods\ngenerate a large fraction of invalid substitution words that are ungrammatical\nor do not preserve the original sentence's semantics. Next, we show that the\nsemantic and grammatical constraints used in SSAs for detecting invalid word\nreplacements are highly insufficient in detecting invalid adversarial samples.", "published": "2022-10-06 12:01:50", "link": "http://arxiv.org/abs/2210.02844v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "XDoc: Unified Pre-training for Cross-Format Document Understanding", "abstract": "The surge of pre-training has witnessed the rapid development of document\nunderstanding recently. Pre-training and fine-tuning framework has been\neffectively used to tackle texts in various formats, including plain texts,\ndocument texts, and web texts. Despite achieving promising performance,\nexisting pre-trained models usually target one specific document format at one\ntime, making it difficult to combine knowledge from multiple document formats.\nTo address this, we propose XDoc, a unified pre-trained model which deals with\ndifferent document formats in a single model. For parameter efficiency, we\nshare backbone parameters for different formats such as the word embedding\nlayer and the Transformer layers. Meanwhile, we introduce adaptive layers with\nlightweight parameters to enhance the distinction across different formats.\nExperimental results have demonstrated that with only 36.7% parameters, XDoc\nachieves comparable or even better performance on a variety of downstream tasks\ncompared with the individual pre-trained models, which is cost effective for\nreal-world deployment. The code and pre-trained models will be publicly\navailable at \\url{https://aka.ms/xdoc}.", "published": "2022-10-06 12:07:18", "link": "http://arxiv.org/abs/2210.02849v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Time Will Change Things: An Empirical Study on Dynamic Language\n  Understanding in Social Media Classification", "abstract": "Language features are ever-evolving in the real-world social media\nenvironment. Many trained models in natural language understanding (NLU),\nineffective in semantic inference for unseen features, might consequently\nstruggle with the deteriorating performance in dynamicity. To address this\nchallenge, we empirically study social media NLU in a dynamic setup, where\nmodels are trained on the past data and test on the future. It better reflects\nthe realistic practice compared to the commonly-adopted static setup of random\ndata split. To further analyze model adaption to the dynamicity, we explore the\nusefulness of leveraging some unlabeled data created after a model is trained.\nThe performance of unsupervised domain adaption baselines based on\nauto-encoding and pseudo-labeling and a joint framework coupling them both are\nexamined in the experiments. Substantial results on four social media tasks\nimply the universally negative effects of evolving environments over\nclassification accuracy, while auto-encoding and pseudo-labeling\ncollaboratively show the best robustness in dynamicity.", "published": "2022-10-06 12:18:28", "link": "http://arxiv.org/abs/2210.02857v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Binding Language Models in Symbolic Languages", "abstract": "Though end-to-end neural approaches have recently been dominating NLP tasks\nin both performance and ease-of-use, they lack interpretability and robustness.\nWe propose Binder, a training-free neural-symbolic framework that maps the task\ninput to a program, which (1) allows binding a unified API of language model\n(LM) functionalities to a programming language (e.g., SQL, Python) to extend\nits grammar coverage and thus tackle more diverse questions, (2) adopts an LM\nas both the program parser and the underlying model called by the API during\nexecution, and (3) requires only a few in-context exemplar annotations.\nSpecifically, we employ GPT-3 Codex as the LM. In the parsing stage, with only\na few in-context exemplars, Codex is able to identify the part of the task\ninput that cannot be answerable by the original programming language, correctly\ngenerate API calls to prompt Codex to solve the unanswerable part, and identify\nwhere to place the API calls while being compatible with the original grammar.\nIn the execution stage, Codex can perform versatile functionalities (e.g.,\ncommonsense QA, information extraction) given proper prompts in the API calls.\nBinder achieves state-of-the-art results on WikiTableQuestions and TabFact\ndatasets, with explicit output programs that benefit human debugging. Note that\nprevious best systems are all finetuned on tens of thousands of task-specific\nsamples, while Binder only uses dozens of annotations as in-context exemplars\nwithout any training. Our code is available at https://github.com/HKUNLP/Binder .", "published": "2022-10-06 12:55:17", "link": "http://arxiv.org/abs/2210.02875v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Distributional Lens for Multi-Aspect Controllable Text Generation", "abstract": "Multi-aspect controllable text generation is a more challenging and practical\ntask than single-aspect control. Existing methods achieve complex multi-aspect\ncontrol by fusing multiple controllers learned from single-aspect, but suffer\nfrom attribute degeneration caused by the mutual interference of these\ncontrollers. To address this, we provide observations on attribute fusion from\na distributional perspective and propose to directly search for the\nintersection areas of multiple attribute distributions as their combination for\ngeneration. Our method first estimates the attribute space with an autoencoder\nstructure. Afterward, we iteratively approach the intersections by jointly\nminimizing distances to points representing different attributes. Finally, we\nmap them to attribute-relevant sentences with a prefix-tuning-based decoder.\nExperiments on the three-aspect control task, including sentiment, topic, and\ndetoxification aspects, reveal that our method outperforms several strong\nbaselines on attribute relevance and text quality and achieves the SOTA.\nFurther analysis also supplies some explanatory support for the effectiveness\nof our approach.", "published": "2022-10-06 13:08:04", "link": "http://arxiv.org/abs/2210.02889v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multiview Contextual Commonsense Inference: A New Dataset and Task", "abstract": "Contextual commonsense inference is the task of generating various types of\nexplanations around the events in a dyadic dialogue, including cause,\nmotivation, emotional reaction, and others. Producing a coherent and\nnon-trivial explanation requires awareness of the dialogue's structure and of\nhow an event is grounded in the context. In this work, we create CICEROv2, a\ndataset consisting of 8,351 instances from 2,379 dialogues, containing multiple\nhuman-written answers for each contextual commonsense inference question,\nrepresenting a type of explanation on cause, subsequent event, motivation, and\nemotional reaction. We show that the inferences in CICEROv2 are more\nsemantically diverse than other contextual commonsense inference datasets. To\nsolve the inference task, we propose a collection of pre-training objectives,\nincluding concept denoising and utterance sorting to prepare a pre-trained\nmodel for the downstream contextual commonsense inference task. Our results\nshow that the proposed pre-training objectives are effective at adapting the\npre-trained T5-Large model for the contextual commonsense inference task.", "published": "2022-10-06 13:08:41", "link": "http://arxiv.org/abs/2210.02890v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Debiasing isn't enough! -- On the Effectiveness of Debiasing MLMs and\n  their Social Biases in Downstream Tasks", "abstract": "We study the relationship between task-agnostic intrinsic and task-specific\nextrinsic social bias evaluation measures for Masked Language Models (MLMs),\nand find that there exists only a weak correlation between these two types of\nevaluation measures. Moreover, we find that MLMs debiased using different\nmethods still re-learn social biases during fine-tuning on downstream tasks. We\nidentify the social biases in both training instances as well as their assigned\nlabels as reasons for the discrepancy between intrinsic and extrinsic bias\nevaluation measurements. Overall, our findings highlight the limitations of\nexisting MLM bias evaluation measures and raise concerns on the deployment of\nMLMs in downstream applications using those measures.", "published": "2022-10-06 14:08:57", "link": "http://arxiv.org/abs/2210.02938v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BootAug: Boosting Text Augmentation via Hybrid Instance Filtering\n  Framework", "abstract": "Text augmentation is an effective technique for addressing the problem of\ninsufficient data in natural language processing. However, existing text\naugmentation methods tend to focus on few-shot scenarios and usually perform\npoorly on large public datasets. Our research indicates that existing\naugmentation methods often generate instances with shifted feature spaces,\nwhich leads to a drop in performance on the augmented data (for example, EDA\ngenerally loses $\\approx 2\\%$ in aspect-based sentiment classification). To\naddress this problem, we propose a hybrid instance-filtering framework\n(BootAug) based on pre-trained language models that can maintain a similar\nfeature space with natural datasets. BootAug is transferable to existing text\naugmentation methods (such as synonym substitution and back translation) and\nsignificantly improves the augmentation performance by $\\approx 2-3\\%$ in\nclassification accuracy. Our experimental results on three classification tasks\nand nine public datasets show that BootAug addresses the performance drop\nproblem and outperforms state-of-the-art text augmentation methods.\nAdditionally, we release the code to help improve existing augmentation methods\non large datasets.", "published": "2022-10-06 14:15:11", "link": "http://arxiv.org/abs/2210.02941v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving the Sample Efficiency of Prompt Tuning with Domain Adaptation", "abstract": "Prompt tuning, or the conditioning of a frozen pretrained language model\n(PLM) with soft prompts learned from data, has demonstrated impressive\nperformance on a wide range of NLP tasks. However, prompt tuning requires a\nlarge training dataset to be effective and is outperformed by finetuning the\nentire PLM in data-scarce regimes. Previous work (Gu et al., 2022, Vu et al.,\n2022) proposed to transfer soft prompts pretrained on the source domain to the\ntarget domain. In this paper, we explore domain adaptation for prompt tuning, a\nproblem setting where unlabeled data from the target domain are available\nduring pretraining. We propose bOosting Prompt TunIng with doMain Adaptation\n(OPTIMA), which regularizes the decision boundary to be smooth around regions\nwhere source and target data distributions are similar. Extensive experiments\ndemonstrate that OPTIMA significantly enhances the transferability and\nsample-efficiency of prompt tuning compared to strong baselines. Moreover, in\nfew-shot settings, OPTIMA exceeds full-model tuning by a large margin.", "published": "2022-10-06 14:44:21", "link": "http://arxiv.org/abs/2210.02952v2", "categories": ["cs.CL", "I.2.7; I.2.6; I.2.0"], "primary_category": "cs.CL"}
{"title": "Are word boundaries useful for unsupervised language learning?", "abstract": "Word or word-fragment based Language Models (LM) are typically preferred over\ncharacter-based ones in many downstream applications. This may not be\nsurprising as words seem more linguistically relevant units than characters.\nWords provide at least two kinds of relevant information: boundary information\nand meaningful units. However, word boundary information may be absent or\nunreliable in the case of speech input (word boundaries are not marked\nexplicitly in the speech stream). Here, we systematically compare LSTMs as a\nfunction of the input unit (character, phoneme, word, word part), with or\nwithout gold boundary information. We probe linguistic knowledge in the\nnetworks at the lexical, syntactic and semantic levels using three\nspeech-adapted black box NLP psycholinguistically-inpired benchmarks (pWUGGY,\npBLIMP, pSIMI). We find that the absence of boundaries costs between 2\\% and\n28\\% in relative performance depending on the task. We show that gold\nboundaries can be replaced by automatically found ones obtained with an\nunsupervised segmentation algorithm, and that even modest segmentation\nperformance gives a gain in performance on two of the three tasks compared to\nbasic character/phone based models without boundary information.", "published": "2022-10-06 14:49:42", "link": "http://arxiv.org/abs/2210.02956v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Guess the Instruction! Flipped Learning Makes Language Models Stronger\n  Zero-Shot Learners", "abstract": "Meta-training, which fine-tunes the language model (LM) on various downstream\ntasks by maximizing the likelihood of the target label given the task\ninstruction and input instance, has improved the zero-shot task generalization\nperformance. However, meta-trained LMs still struggle to generalize to\nchallenging tasks containing novel labels unseen during meta-training. In this\npaper, we propose Flipped Learning, an alternative method of meta-training\nwhich trains the LM to generate the task instruction given the input instance\nand label. During inference, the LM trained with Flipped Learning, referred to\nas Flipped, selects the label option that is most likely to generate the task\ninstruction. On 14 tasks of the BIG-bench benchmark, the 11B-sized Flipped\noutperforms zero-shot T0-11B and even a 16 times larger 3-shot GPT-3 (175B) on\naverage by 8.4% and 9.7% points, respectively. Flipped gives particularly large\nimprovements on tasks with unseen labels, outperforming T0-11B by up to +20%\naverage F1 score. This indicates that the strong task generalization of Flipped\ncomes from improved generalization to novel labels. We release our code at\nhttps://github.com/seonghyeonye/Flipped-Learning.", "published": "2022-10-06 15:00:47", "link": "http://arxiv.org/abs/2210.02969v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Measuring Fine-Grained Semantic Equivalence with Abstract Meaning\n  Representation", "abstract": "Identifying semantically equivalent sentences is important for many\ncross-lingual and mono-lingual NLP tasks. Current approaches to semantic\nequivalence take a loose, sentence-level approach to \"equivalence,\" despite\nprevious evidence that fine-grained differences and implicit content have an\neffect on human understanding (Roth and Anthonio, 2021) and system performance\n(Briakou and Carpuat, 2021). In this work, we introduce a novel, more sensitive\nmethod of characterizing semantic equivalence that leverages Abstract Meaning\nRepresentation graph structures. We develop an approach, which can be used with\neither gold or automatic AMR annotations, and demonstrate that our solution is\nin fact finer-grained than existing corpus filtering methods and more accurate\nat predicting strictly equivalent sentences than existing semantic similarity\nmetrics. We suggest that our finer-grained measure of semantic equivalence\ncould limit the workload in the task of human post-edited machine translation\nand in human evaluation of sentence similarity.", "published": "2022-10-06 16:08:27", "link": "http://arxiv.org/abs/2210.03018v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Narrative Elements in Informational Text", "abstract": "Automatic extraction of narrative elements from text, combining narrative\ntheories with computational models, has been receiving increasing attention\nover the last few years. Previous works have utilized the oral narrative theory\nby Labov and Waletzky to identify various narrative elements in personal\nstories texts. Instead, we direct our focus to informational texts,\nspecifically news stories. We introduce NEAT (Narrative Elements AnnoTation) -\na novel NLP task for detecting narrative elements in raw text. For this\npurpose, we designed a new multi-label narrative annotation scheme, better\nsuited for informational text (e.g. news media), by adapting elements from the\nnarrative theory of Labov and Waletzky (Complication and Resolution) and adding\na new narrative element of our own (Success). We then used this scheme to\nannotate a new dataset of 2,209 sentences, compiled from 46 news articles from\nvarious category domains. We trained a number of supervised models in several\ndifferent setups over the annotated dataset to identify the different narrative\nelements, achieving an average F1 score of up to 0.77. The results demonstrate\nthe holistic nature of our annotation scheme as well as its robustness to\ndomain category.", "published": "2022-10-06 16:23:33", "link": "http://arxiv.org/abs/2210.03028v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Toxicity in Multilingual Machine Translation at Scale", "abstract": "Machine Translation systems can produce different types of errors, some of\nwhich are characterized as critical or catastrophic due to the specific\nnegative impact that they can have on users. In this paper we focus on one type\nof critical error: added toxicity. We evaluate and analyze added toxicity when\ntranslating a large evaluation dataset (HOLISTICBIAS, over 472k sentences,\ncovering 13 demographic axes) from English into 164 languages. An automatic\ntoxicity evaluation shows that added toxicity across languages varies from 0%\nto 5%. The output languages with the most added toxicity tend to be\nlow-resource ones, and the demographic axes with the most added toxicity\ninclude sexual orientation, gender and sex, and ability. We also perform human\nevaluation on a subset of 8 translation directions, confirming the prevalence\nof true added toxicity. We use a measurement of the amount of source\ncontribution to the translation, where a low source contribution implies\nhallucination, to interpret what causes toxicity. Making use of the input\nattributions allows us to explain toxicity, because the source contributions\nsignificantly correlate with toxicity for 84% of languages studied. Given our\nfindings, our recommendations to reduce added toxicity are to curate training\ndata to avoid mistranslations, mitigate hallucination and check unstable\ntranslations.", "published": "2022-10-06 17:26:27", "link": "http://arxiv.org/abs/2210.03070v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Explainable Verbal Deception Detection using Transformers", "abstract": "People are regularly confronted with potentially deceptive statements (e.g.,\nfake news, misleading product reviews, or lies about activities). Only few\nworks on automated text-based deception detection have exploited the potential\nof deep learning approaches. A critique of deep-learning methods is their lack\nof interpretability, preventing us from understanding the underlying\n(linguistic) mechanisms involved in deception. However, recent advancements\nhave made it possible to explain some aspects of such models. This paper\nproposes and evaluates six deep-learning models, including combinations of BERT\n(and RoBERTa), MultiHead Attention, co-attentions, and transformers. To\nunderstand how the models reach their decisions, we then examine the model's\npredictions with LIME. We then zoom in on vocabulary uniqueness and the\ncorrelation of LIWC categories with the outcome class (truthful vs deceptive).\nThe findings suggest that our transformer-based models can enhance automated\ndeception detection performances (+2.11% in accuracy) and show significant\ndifferences pertinent to the usage of LIWC features in truthful and deceptive\nstatements.", "published": "2022-10-06 17:36:00", "link": "http://arxiv.org/abs/2210.03080v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adaptive Ranking-based Sample Selection for Weakly Supervised\n  Class-imbalanced Text Classification", "abstract": "To obtain a large amount of training labels inexpensively, researchers have\nrecently adopted the weak supervision (WS) paradigm, which leverages labeling\nrules to synthesize training labels rather than using individual annotations to\nachieve competitive results for natural language processing (NLP) tasks.\nHowever, data imbalance is often overlooked in applying the WS paradigm,\ndespite being a common issue in a variety of NLP tasks. To address this\nchallenge, we propose Adaptive Ranking-based Sample Selection (ARS2), a\nmodel-agnostic framework to alleviate the data imbalance issue in the WS\nparadigm. Specifically, it calculates a probabilistic margin score based on the\noutput of the current model to measure and rank the cleanliness of each data\npoint. Then, the ranked data are sampled based on both class-wise and\nrule-aware ranking. In particular, the two sample strategies corresponds to our\nmotivations: (1) to train the model with balanced data batches to reduce the\ndata imbalance issue and (2) to exploit the expertise of each labeling rule for\ncollecting clean samples. Experiments on four text classification datasets with\nfour different imbalance ratios show that ARS2 outperformed the\nstate-of-the-art imbalanced learning and WS methods, leading to a 2%-57.8%\nimprovement on their F1-score.", "published": "2022-10-06 17:49:22", "link": "http://arxiv.org/abs/2210.03092v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FAST: Improving Controllability for Text Generation with Feedback Aware\n  Self-Training", "abstract": "Controllable text generation systems often leverage control codes to direct\nvarious properties of the output like style and length. Inspired by recent work\non causal inference for NLP, this paper reveals a previously overlooked flaw in\nthese control code-based conditional text generation algorithms. Spurious\ncorrelations in the training data can lead models to incorrectly rely on parts\nof the input other than the control code for attribute selection, significantly\nundermining downstream generation quality and controllability. We demonstrate\nthe severity of this issue with a series of case studies and then propose two\nsimple techniques to reduce these correlations in training sets. The first\ntechnique is based on resampling the data according to an example's propensity\ntowards each linguistic attribute (IPS). The second produces multiple\ncounterfactual versions of each example and then uses an additional feedback\nmechanism to remove noisy examples (feedback aware self-training, FAST). We\nevaluate on 3 tasks -- news headline, meta review, and search ads generation --\nand demonstrate that FAST can significantly improve the controllability and\nlanguage quality of generated outputs when compared to state-of-the-art\ncontrollable text generation approaches.", "published": "2022-10-06 19:00:51", "link": "http://arxiv.org/abs/2210.03167v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Compositional Generalisation with Structured Reordering and Fertility\n  Layers", "abstract": "Seq2seq models have been shown to struggle with compositional generalisation,\ni.e. generalising to new and potentially more complex structures than seen\nduring training. Taking inspiration from grammar-based models that excel at\ncompositional generalisation, we present a flexible end-to-end differentiable\nneural model that composes two structural operations: a fertility step, which\nwe introduce in this work, and a reordering step based on previous work (Wang\net al., 2021). To ensure differentiability, we use the expected value of each\nstep. Our model outperforms seq2seq models by a wide margin on challenging\ncompositional splits of realistic semantic parsing tasks that require\ngeneralisation to longer examples. It also compares favourably to other models\ntargeting compositional generalisation.", "published": "2022-10-06 19:51:31", "link": "http://arxiv.org/abs/2210.03183v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Improving Large-scale Paraphrase Acquisition and Generation", "abstract": "This paper addresses the quality issues in existing Twitter-based paraphrase\ndatasets, and discusses the necessity of using two separate definitions of\nparaphrase for identification and generation tasks. We present a new\nMulti-Topic Paraphrase in Twitter (MultiPIT) corpus that consists of a total of\n130k sentence pairs with crowdsoursing (MultiPIT_crowd) and expert\n(MultiPIT_expert) annotations using two different paraphrase definitions for\nparaphrase identification, in addition to a multi-reference test set\n(MultiPIT_NMR) and a large automatically constructed training set\n(MultiPIT_Auto) for paraphrase generation. With improved data annotation\nquality and task-specific paraphrase definition, the best pre-trained language\nmodel fine-tuned on our dataset achieves the state-of-the-art performance of\n84.2 F1 for automatic paraphrase identification. Furthermore, our empirical\nresults also demonstrate that the paraphrase generation models trained on\nMultiPIT_Auto generate more diverse and high-quality paraphrases compared to\ntheir counterparts fine-tuned on other corpora such as Quora, MSCOCO, and\nParaNMT.", "published": "2022-10-06 22:00:56", "link": "http://arxiv.org/abs/2210.03235v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HealthE: Classifying Entities in Online Textual Health Advice", "abstract": "The processing of entities in natural language is essential to many medical\nNLP systems. Unfortunately, existing datasets vastly under-represent the\nentities required to model public health relevant texts such as health advice\noften found on sites like WebMD. People rely on such information for personal\nhealth management and clinically relevant decision making. In this work, we\nrelease a new annotated dataset, HealthE, consisting of 6,756 health advice.\nHealthE has a more granular label space compared to existing medical NER\ncorpora and contains annotation for diverse health phrases. Additionally, we\nintroduce a new health entity classification model, EP S-BERT, which leverages\ntextual context patterns in the classification of entity classes. EP S-BERT\nprovides a 4-point increase in F1 score over the nearest baseline and a\n34-point increase in F1 when compared to off-the-shelf medical NER tools\ntrained to extract disease and medication mentions from clinical texts. All\ncode and data are publicly available on Github.", "published": "2022-10-06 23:18:24", "link": "http://arxiv.org/abs/2210.03246v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Small Character Models Match Large Word Models for Autocomplete Under\n  Memory Constraints", "abstract": "Autocomplete is a task where the user inputs a piece of text, termed prompt,\nwhich is conditioned by the model to generate semantically coherent\ncontinuation. Existing works for this task have primarily focused on datasets\n(e.g., email, chat) with high frequency user prompt patterns (or focused\nprompts) where word-based language models have been quite effective. In this\nwork, we study the more challenging open-domain setting consisting of low\nfrequency user prompt patterns (or broad prompts, e.g., prompt about 93rd\nacademy awards) and demonstrate the effectiveness of character-based language\nmodels. We study this problem under memory-constrained settings (e.g., edge\ndevices and smartphones), where character-based representation is effective in\nreducing the overall model size (in terms of parameters). We use WikiText-103\nbenchmark to simulate broad prompts and demonstrate that character models rival\nword models in exact match accuracy for the autocomplete task, when controlled\nfor the model size. For instance, we show that a 20M parameter character model\nperforms similar to an 80M parameter word model in the vanilla setting. We\nfurther propose novel methods to improve character models by incorporating\ninductive bias in the form of compositional information and representation\ntransfer from large word models. Datasets and code used in this work are\navailable at https://github.com/UBC-NLP/char_autocomplete.", "published": "2022-10-06 23:29:59", "link": "http://arxiv.org/abs/2210.03251v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Not another Negation Benchmark: The NaN-NLI Test Suite for Sub-clausal\n  Negation", "abstract": "Negation is poorly captured by current language models, although the extent\nof this problem is not widely understood. We introduce a natural language\ninference (NLI) test suite to enable probing the capabilities of NLP methods,\nwith the aim of understanding sub-clausal negation. The test suite contains\npremise--hypothesis pairs where the premise contains sub-clausal negation and\nthe hypothesis is constructed by making minimal modifications to the premise in\norder to reflect different possible interpretations. Aside from adopting\nstandard NLI labels, our test suite is systematically constructed under a\nrigorous linguistic framework. It includes annotation of negation types and\nconstructions grounded in linguistic theory, as well as the operations used to\nconstruct hypotheses. This facilitates fine-grained analysis of model\nperformance. We conduct experiments using pre-trained language models to\ndemonstrate that our test suite is more challenging than existing benchmarks\nfocused on negation, and show how our annotation supports a deeper\nunderstanding of the current NLI capabilities in terms of negation and\nquantification.", "published": "2022-10-06 23:39:01", "link": "http://arxiv.org/abs/2210.03256v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Domain-Specific Word Embeddings with Structure Prediction", "abstract": "Complementary to finding good general word embeddings, an important question\nfor representation learning is to find dynamic word embeddings, e.g., across\ntime or domain. Current methods do not offer a way to use or predict\ninformation on structure between sub-corpora, time or domain and dynamic\nembeddings can only be compared after post-alignment. We propose novel word\nembedding methods that provide general word representations for the whole\ncorpus, domain-specific representations for each sub-corpus, sub-corpus\nstructure, and embedding alignment simultaneously. We present an empirical\nevaluation on New York Times articles and two English Wikipedia datasets with\narticles on science and philosophy. Our method, called Word2Vec with Structure\nPrediction (W2VPred), provides better performance than baselines in terms of\nthe general analogy tests, domain-specific analogy tests, and multiple specific\nword embedding evaluations as well as structure prediction performance when no\nstructure is given a priori. As a use case in the field of Digital Humanities\nwe demonstrate how to raise novel research questions for high literature from\nthe German Text Archive.", "published": "2022-10-06 12:45:48", "link": "http://arxiv.org/abs/2210.04962v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Scene-based Topic Channel Construction System for E-Commerce", "abstract": "Scene marketing that well demonstrates user interests within a certain\nscenario has proved effective for offline shopping. To conduct scene marketing\nfor e-commerce platforms, this work presents a novel product form, scene-based\ntopic channel which typically consists of a list of diverse products belonging\nto the same usage scenario and a topic title that describes the scenario with\nmarketing words. As manual construction of channels is time-consuming due to\nbillions of products as well as dynamic and diverse customers' interests, it is\nnecessary to leverage AI techniques to automatically construct channels for\ncertain usage scenarios and even discover novel topics. To be specific, we\nfirst frame the channel construction task as a two-step problem, i.e.,\nscene-based topic generation and product clustering, and propose an E-commerce\nScene-based Topic Channel construction system (i.e., ESTC) to achieve automated\nproduction, consisting of scene-based topic generation model for the e-commerce\ndomain, product clustering on the basis of topic similarity, as well as quality\ncontrol based on automatic model filtering and human screening. Extensive\noffline experiments and online A/B test validates the effectiveness of such a\nnovel product form as well as the proposed system. In addition, we also\nintroduce the experience of deploying the proposed system on a real-world\ne-commerce recommendation platform.", "published": "2022-10-06 02:29:10", "link": "http://arxiv.org/abs/2210.02643v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Join-Chain Network: A Logical Reasoning View of the Multi-head Attention\n  in Transformer", "abstract": "Developing neural architectures that are capable of logical reasoning has\nbecome increasingly important for a wide range of applications (e.g., natural\nlanguage processing). Towards this grand objective, we propose a symbolic\nreasoning architecture that chains many join operators together to model output\nlogical expressions. In particular, we demonstrate that such an ensemble of\njoin-chains can express a broad subset of ''tree-structured'' first-order\nlogical expressions, named FOET, which is particularly useful for modeling\nnatural languages. To endow it with differentiable learning capability, we\nclosely examine various neural operators for approximating the symbolic\njoin-chains. Interestingly, we find that the widely used multi-head\nself-attention module in transformer can be understood as a special neural\noperator that implements the union bound of the join operator in probabilistic\npredicate space. Our analysis not only provides a new perspective on the\nmechanism of the pretrained models such as BERT for natural language\nunderstanding but also suggests several important future improvement\ndirections.", "published": "2022-10-06 07:39:58", "link": "http://arxiv.org/abs/2210.02729v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Just ClozE! A Novel Framework for Evaluating the Factual Consistency\n  Faster in Abstractive Summarization", "abstract": "The issue of factual consistency in abstractive summarization has received\nextensive attention in recent years, and the evaluation of factual consistency\nbetween summary and document has become an important and urgent task. Most of\nthe current evaluation metrics are adopted from the question answering (QA) or\nnatural language inference (NLI) task. However, the application of QA-based\nmetrics is extremely time-consuming in practice while NLI-based metrics are\nlack of interpretability. In this paper, we propose a cloze-based evaluation\nframework called ClozE and show the great potential of the cloze-based metric.\nIt inherits strong interpretability from QA, while maintaining the speed of\nNLI- level reasoning. We demonstrate that ClozE can reduce the evaluation time\nby nearly 96% relative to QA-based metrics while retaining their\ninterpretability and performance through experiments on six human-annotated\ndatasets and a meta-evaluation benchmark GO FIGURE (Gabriel et al., 2021).\nFinally, we discuss three important facets of ClozE in practice, which further\nshows better overall performance of ClozE compared to other metrics.", "published": "2022-10-06 10:30:53", "link": "http://arxiv.org/abs/2210.02804v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Generative Entity Typing with Curriculum Learning", "abstract": "Entity typing aims to assign types to the entity mentions in given texts. The\ntraditional classification-based entity typing paradigm has two unignorable\ndrawbacks: 1) it fails to assign an entity to the types beyond the predefined\ntype set, and 2) it can hardly handle few-shot and zero-shot situations where\nmany long-tail types only have few or even no training instances. To overcome\nthese drawbacks, we propose a novel generative entity typing (GET) paradigm:\ngiven a text with an entity mention, the multiple types for the role that the\nentity plays in the text are generated with a pre-trained language model (PLM).\nHowever, PLMs tend to generate coarse-grained types after fine-tuning upon the\nentity typing dataset. Besides, we only have heterogeneous training data\nconsisting of a small portion of human-annotated data and a large portion of\nauto-generated but low-quality data. To tackle these problems, we employ\ncurriculum learning (CL) to train our GET model upon the heterogeneous data,\nwhere the curriculum could be self-adjusted with the self-paced learning\naccording to its comprehension of the type granularity and data heterogeneity.\nOur extensive experiments upon the datasets of different languages and\ndownstream tasks justify the superiority of our GET model over the\nstate-of-the-art entity typing models. The code has been released on\nhttps://github.com/siyuyuan/GET.", "published": "2022-10-06 13:32:50", "link": "http://arxiv.org/abs/2210.02914v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Grape: Knowledge Graph Enhanced Passage Reader for Open-domain Question\n  Answering", "abstract": "A common thread of open-domain question answering (QA) models employs a\nretriever-reader pipeline that first retrieves a handful of relevant passages\nfrom Wikipedia and then peruses the passages to produce an answer. However,\neven state-of-the-art readers fail to capture the complex relationships between\nentities appearing in questions and retrieved passages, leading to answers that\ncontradict the facts. In light of this, we propose a novel knowledge Graph\nenhanced passage reader, namely Grape, to improve the reader performance for\nopen-domain QA. Specifically, for each pair of question and retrieved passage,\nwe first construct a localized bipartite graph, attributed to entity embeddings\nextracted from the intermediate layer of the reader model. Then, a graph neural\nnetwork learns relational knowledge while fusing graph and contextual\nrepresentations into the hidden states of the reader model. Experiments on\nthree open-domain QA benchmarks show Grape can improve the state-of-the-art\nperformance by up to 2.2 exact match score with a negligible overhead increase,\nwith the same retriever and retrieved passages. Our code is publicly available\nat https://github.com/jumxglhf/GRAPE.", "published": "2022-10-06 14:06:04", "link": "http://arxiv.org/abs/2210.02933v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Efficiently Enhancing Zero-Shot Performance of Instruction Following\n  Model via Retrieval of Soft Prompt", "abstract": "Enhancing the zero-shot performance of instruction-following models requires\nheavy computation, either by scaling the total number of training datasets or\nthe model size. In this work, we explore how retrieval of soft prompts obtained\nthrough prompt tuning can efficiently assist hard prompts in zero-shot task\ngeneralization. Specifically, we train soft prompt embeddings for each prompt\nthrough prompt tuning, store the samples of the training instances mapped with\nthe prompt embeddings, and retrieve the corresponding prompt embedding of the\ntraining instance closest to the query instance during inference. While only\nadding 0.007% additional parameters, retrieval of soft prompt enhances the\nperformance of T0 on unseen tasks by outperforming it on 10 out of 11 datasets\nas well as improving the mean accuracy of T0 on BIG-bench benchmark by 2.39%\npoints. Also, we report an interesting finding that retrieving source\nembeddings trained on similar answer choice formats is more important than\nthose on similar task types.", "published": "2022-10-06 16:26:03", "link": "http://arxiv.org/abs/2210.03029v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Conversational Semantic Role Labeling with Predicate-Oriented Latent\n  Graph", "abstract": "Conversational semantic role labeling (CSRL) is a newly proposed task that\nuncovers the shallow semantic structures in a dialogue text. Unfortunately\nseveral important characteristics of the CSRL task have been overlooked by the\nexisting works, such as the structural information integration, near-neighbor\ninfluence. In this work, we investigate the integration of a latent graph for\nCSRL. We propose to automatically induce a predicate-oriented latent graph\n(POLar) with a predicate-centered Gaussian mechanism, by which the nearer and\ninformative words to the predicate will be allocated with more attention. The\nPOLar structure is then dynamically pruned and refined so as to best fit the\ntask need. We additionally introduce an effective dialogue-level pre-trained\nlanguage model, CoDiaBERT, for better supporting multiple utterance sentences\nand handling the speaker coreference issue in CSRL. Our system outperforms\nbest-performing baselines on three benchmark CSRL datasets with big margins,\nespecially achieving over 4% F1 score improvements on the cross-utterance\nargument detection. Further analyses are presented to better understand the\neffectiveness of our proposed methods.", "published": "2022-10-06 16:42:00", "link": "http://arxiv.org/abs/2210.03037v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "State-of-the-art generalisation research in NLP: A taxonomy and review", "abstract": "The ability to generalise well is one of the primary desiderata of natural\nlanguage processing (NLP). Yet, what 'good generalisation' entails and how it\nshould be evaluated is not well understood, nor are there any evaluation\nstandards for generalisation. In this paper, we lay the groundwork to address\nboth of these issues. We present a taxonomy for characterising and\nunderstanding generalisation research in NLP. Our taxonomy is based on an\nextensive literature review of generalisation research, and contains five axes\nalong which studies can differ: their main motivation, the type of\ngeneralisation they investigate, the type of data shift they consider, the\nsource of this data shift, and the locus of the shift within the modelling\npipeline. We use our taxonomy to classify over 400 papers that test\ngeneralisation, for a total of more than 600 individual experiments.\nConsidering the results of this review, we present an in-depth analysis that\nmaps out the current state of generalisation research in NLP, and we make\nrecommendations for which areas might deserve attention in the future. Along\nwith this paper, we release a webpage where the results of our review can be\ndynamically explored, and which we intend to update as new NLP generalisation\nstudies are published. With this work, we aim to take steps towards making\nstate-of-the-art generalisation testing the new status quo in NLP.", "published": "2022-10-06 16:53:33", "link": "http://arxiv.org/abs/2210.03050v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Rainier: Reinforced Knowledge Introspector for Commonsense Question\n  Answering", "abstract": "Knowledge underpins reasoning. Recent research demonstrates that when\nrelevant knowledge is provided as additional context to commonsense question\nanswering (QA), it can substantially enhance the performance even on top of\nstate-of-the-art. The fundamental challenge is where and how to find such\nknowledge that is high quality and on point with respect to the question;\nknowledge retrieved from knowledge bases are incomplete and knowledge generated\nfrom language models are inconsistent. We present Rainier, or Reinforced\nKnowledge Introspector, that learns to generate contextually relevant knowledge\nin response to given questions. Our approach starts by imitating knowledge\ngenerated by GPT-3, then learns to generate its own knowledge via reinforcement\nlearning where rewards are shaped based on the increased performance on the\nresulting question answering. Rainier demonstrates substantial and consistent\nperformance gains when tested over 9 different commonsense benchmarks:\nincluding 5 datasets that are seen during model training, as well as 4 datasets\nthat are kept unseen. Our work is the first to report that knowledge generated\nby models that are orders of magnitude smaller than GPT-3, even without direct\nsupervision on the knowledge itself, can exceed the quality of commonsense\nknowledge elicited from GPT-3.", "published": "2022-10-06 17:34:06", "link": "http://arxiv.org/abs/2210.03078v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unsupervised Domain Adaptation for COVID-19 Information Service with\n  Contrastive Adversarial Domain Mixup", "abstract": "In the real-world application of COVID-19 misinformation detection, a\nfundamental challenge is the lack of the labeled COVID data to enable\nsupervised end-to-end training of the models, especially at the early stage of\nthe pandemic. To address this challenge, we propose an unsupervised domain\nadaptation framework using contrastive learning and adversarial domain mixup to\ntransfer the knowledge from an existing source data domain to the target\nCOVID-19 data domain. In particular, to bridge the gap between the source\ndomain and the target domain, our method reduces a radial basis function (RBF)\nbased discrepancy between these two domains. Moreover, we leverage the power of\ndomain adversarial examples to establish an intermediate domain mixup, where\nthe latent representations of the input text from both domains could be mixed\nduring the training process. Extensive experiments on multiple real-world\ndatasets suggest that our method can effectively adapt misinformation detection\nsystems to the unseen COVID-19 target domain with significant improvements\ncompared to the state-of-the-art baselines.", "published": "2022-10-06 23:29:10", "link": "http://arxiv.org/abs/2210.03250v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning to Reason With Relational Abstractions", "abstract": "Large language models have recently shown promising progress in mathematical\nreasoning when fine-tuned with human-generated sequences walking through a\nsequence of solution steps. However, the solution sequences are not formally\nstructured and the resulting model-generated sequences may not reflect the kind\nof systematic reasoning we might expect an expert human to produce. In this\npaper, we study how to build stronger reasoning capability in language models\nusing the idea of relational abstractions. We introduce new types of sequences\nthat more explicitly provide an abstract characterization of the transitions\nthrough intermediate solution steps to the goal state. We find that models that\nare supplied with such sequences as prompts can solve tasks with a\nsignificantly higher accuracy, and models that are trained to produce such\nsequences solve problems better than those that are trained with previously\nused human-generated sequences and other baselines. Our work thus takes several\nsteps toward elucidating and improving how language models perform on tasks\nrequiring multi-step mathematical reasoning.", "published": "2022-10-06 00:27:50", "link": "http://arxiv.org/abs/2210.02615v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Improving the Domain Adaptation of Retrieval Augmented Generation (RAG)\n  Models for Open Domain Question Answering", "abstract": "Retrieval Augment Generation (RAG) is a recent advancement in Open-Domain\nQuestion Answering (ODQA). RAG has only been trained and explored with a\nWikipedia-based external knowledge base and is not optimized for use in other\nspecialized domains such as healthcare and news. In this paper, we evaluate the\nimpact of joint training of the retriever and generator components of RAG for\nthe task of domain adaptation in ODQA. We propose \\textit{RAG-end2end}, an\nextension to RAG, that can adapt to a domain-specific knowledge base by\nupdating all components of the external knowledge base during training. In\naddition, we introduce an auxiliary training signal to inject more\ndomain-specific knowledge. This auxiliary signal forces \\textit{RAG-end2end} to\nreconstruct a given sentence by accessing the relevant information from the\nexternal knowledge base. Our novel contribution is unlike RAG, RAG-end2end does\njoint training of the retriever and generator for the end QA task and domain\nadaptation. We evaluate our approach with datasets from three domains:\nCOVID-19, News, and Conversations, and achieve significant performance\nimprovements compared to the original RAG model. Our work has been open-sourced\nthrough the Huggingface Transformers library, attesting to our work's\ncredibility and technical consistency.", "published": "2022-10-06 01:21:25", "link": "http://arxiv.org/abs/2210.02627v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Towards Better Semantic Understanding of Mobile Interfaces", "abstract": "Improving the accessibility and automation capabilities of mobile devices can\nhave a significant positive impact on the daily lives of countless users. To\nstimulate research in this direction, we release a human-annotated dataset with\napproximately 500k unique annotations aimed at increasing the understanding of\nthe functionality of UI elements. This dataset augments images and view\nhierarchies from RICO, a large dataset of mobile UIs, with annotations for\nicons based on their shapes and semantics, and associations between different\nelements and their corresponding text labels, resulting in a significant\nincrease in the number of UI elements and the categories assigned to them. We\nalso release models using image-only and multimodal inputs; we experiment with\nvarious architectures and study the benefits of using multimodal inputs on the\nnew dataset. Our models demonstrate strong performance on an evaluation set of\nunseen apps, indicating their generalizability to newer screens. These models,\ncombined with the new dataset, can enable innovative functionalities like\nreferring to UI elements by their labels, improved coverage and better\nsemantics for icons etc., which would go a long way in making UIs more usable\nfor everyone.", "published": "2022-10-06 03:48:54", "link": "http://arxiv.org/abs/2210.02663v1", "categories": ["cs.HC", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.HC"}
{"title": "Vision Transformer Based Model for Describing a Set of Images as a Story", "abstract": "Visual Story-Telling is the process of forming a multi-sentence story from a\nset of images. Appropriately including visual variation and contextual\ninformation captured inside the input images is one of the most challenging\naspects of visual storytelling. Consequently, stories developed from a set of\nimages often lack cohesiveness, relevance, and semantic relationship. In this\npaper, we propose a novel Vision Transformer Based Model for describing a set\nof images as a story. The proposed method extracts the distinct features of the\ninput images using a Vision Transformer (ViT). Firstly, input images are\ndivided into 16X16 patches and bundled into a linear projection of flattened\npatches. The transformation from a single image to multiple image patches\ncaptures the visual variety of the input visual patterns. These features are\nused as input to a Bidirectional-LSTM which is part of the sequence encoder.\nThis captures the past and future image context of all image patches. Then, an\nattention mechanism is implemented and used to increase the discriminatory\ncapacity of the data fed into the language model, i.e. a Mogrifier-LSTM. The\nperformance of our proposed model is evaluated using the Visual Story-Telling\ndataset (VIST), and the results show that our model outperforms the current\nstate of the art models.", "published": "2022-10-06 09:01:50", "link": "http://arxiv.org/abs/2210.02762v3", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Modelling Commonsense Properties using Pre-Trained Bi-Encoders", "abstract": "Grasping the commonsense properties of everyday concepts is an important\nprerequisite to language understanding. While contextualised language models\nare reportedly capable of predicting such commonsense properties with\nhuman-level accuracy, we argue that such results have been inflated because of\nthe high similarity between training and test concepts. This means that models\nwhich capture concept similarity can perform well, even if they do not capture\nany knowledge of the commonsense properties themselves. In settings where there\nis no overlap between the properties that are considered during training and\ntesting, we find that the empirical performance of standard language models\ndrops dramatically. To address this, we study the possibility of fine-tuning\nlanguage models to explicitly model concepts and their properties. In\nparticular, we train separate concept and property encoders on two types of\nreadily available data: extracted hyponym-hypernym pairs and generic sentences.\nOur experimental results show that the resulting encoders allow us to predict\ncommonsense properties with much higher accuracy than is possible by directly\nfine-tuning language models. We also present experimental results for the\nrelated task of unsupervised hypernym discovery.", "published": "2022-10-06 09:17:34", "link": "http://arxiv.org/abs/2210.02771v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Causal Inference for Chatting Handoff", "abstract": "Aiming to ensure chatbot quality by predicting chatbot failure and enabling\nhuman-agent collaboration, Machine-Human Chatting Handoff (MHCH) has attracted\nlots of attention from both industry and academia in recent years. However,\nmost existing methods mainly focus on the dialogue context or assist with\nglobal satisfaction prediction based on multi-task learning, which ignore the\ngrounded relationships among the causal variables, like the user state and\nlabor cost. These variables are significantly associated with handoff\ndecisions, resulting in prediction bias and cost increasement. Therefore, we\npropose Causal-Enhance Module (CEM) by establishing the causal graph of MHCH\nbased on these two variables, which is a simple yet effective module and can be\neasy to plug into the existing MHCH methods. For the impact of users, we use\nthe user state to correct the prediction bias according to the causal\nrelationship of multi-task. For the labor cost, we train an auxiliary cost\nsimulator to calculate unbiased labor cost through counterfactual learning so\nthat a model becomes cost-aware. Extensive experiments conducted on four\nreal-world benchmarks demonstrate the effectiveness of CEM in generally\nimproving the performance of existing MHCH methods without any elaborated model\ncrafting.", "published": "2022-10-06 12:24:58", "link": "http://arxiv.org/abs/2210.02862v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "MuRAG: Multimodal Retrieval-Augmented Generator for Open Question\n  Answering over Images and Text", "abstract": "While language Models store a massive amount of world knowledge implicitly in\ntheir parameters, even very large models often fail to encode information about\nrare entities and events, while incurring huge computational costs. Recently,\nretrieval-augmented models, such as REALM, RAG, and RETRO, have incorporated\nworld knowledge into language generation by leveraging an external\nnon-parametric index and have demonstrated impressive performance with\nconstrained model sizes. However, these methods are restricted to retrieving\nonly textual knowledge, neglecting the ubiquitous amount of knowledge in other\nmodalities like images -- much of which contains information not covered by any\ntext. To address this limitation, we propose the first Multimodal\nRetrieval-Augmented Transformer (MuRAG), which accesses an external\nnon-parametric multimodal memory to augment language generation. MuRAG is\npre-trained with a mixture of large-scale image-text and text-only corpora\nusing a joint contrastive and generative loss. We perform experiments on two\ndifferent datasets that require retrieving and reasoning over both images and\ntext to answer a given query: WebQA, and MultimodalQA. Our results show that\nMuRAG achieves state-of-the-art accuracy, outperforming existing models by\n10-20\\% absolute on both datasets and under both distractor and full-wiki\nsettings.", "published": "2022-10-06 13:58:03", "link": "http://arxiv.org/abs/2210.02928v2", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "To Softmax, or not to Softmax: that is the question when applying Active\n  Learning for Transformer Models", "abstract": "Despite achieving state-of-the-art results in nearly all Natural Language\nProcessing applications, fine-tuning Transformer-based language models still\nrequires a significant amount of labeled data to work. A well known technique\nto reduce the amount of human effort in acquiring a labeled dataset is\n\\textit{Active Learning} (AL): an iterative process in which only the minimal\namount of samples is labeled. AL strategies require access to a quantified\nconfidence measure of the model predictions. A common choice is the softmax\nactivation function for the final layer. As the softmax function provides\nmisleading probabilities, this paper compares eight alternatives on seven\ndatasets. Our almost paradoxical finding is that most of the methods are too\ngood at identifying the true most uncertain samples (outliers), and that\nlabeling therefore exclusively outliers results in worse performance. As a\nheuristic we propose to systematically ignore samples, which results in\nimprovements of various methods compared to the softmax function.", "published": "2022-10-06 15:51:39", "link": "http://arxiv.org/abs/2210.03005v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DB"], "primary_category": "cs.LG"}
{"title": "Reinforcement Learning with Large Action Spaces for Neural Machine\n  Translation", "abstract": "Applying Reinforcement learning (RL) following maximum likelihood estimation\n(MLE) pre-training is a versatile method for enhancing neural machine\ntranslation (NMT) performance. However, recent work has argued that the gains\nproduced by RL for NMT are mostly due to promoting tokens that have already\nreceived a fairly high probability in pre-training. We hypothesize that the\nlarge action space is a main obstacle to RL's effectiveness in MT, and conduct\ntwo sets of experiments that lend support to our hypothesis. First, we find\nthat reducing the size of the vocabulary improves RL's effectiveness. Second,\nwe find that effectively reducing the dimension of the action space without\nchanging the vocabulary also yields notable improvement as evaluated by BLEU,\nsemantic similarity, and human evaluation. Indeed, by initializing the\nnetwork's final fully connected layer (that maps the network's internal\ndimension to the vocabulary dimension), with a layer that generalizes over\nsimilar actions, we obtain a substantial improvement in RL performance: 1.5\nBLEU points on average.", "published": "2022-10-06 16:58:27", "link": "http://arxiv.org/abs/2210.03053v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Language Models are Multilingual Chain-of-Thought Reasoners", "abstract": "We evaluate the reasoning abilities of large language models in multilingual\nsettings. We introduce the Multilingual Grade School Math (MGSM) benchmark, by\nmanually translating 250 grade-school math problems from the GSM8K dataset\n(Cobbe et al., 2021) into ten typologically diverse languages. We find that the\nability to solve MGSM problems via chain-of-thought prompting emerges with\nincreasing model scale, and that models have strikingly strong multilingual\nreasoning abilities, even in underrepresented languages such as Bengali and\nSwahili. Finally, we show that the multilingual reasoning abilities of language\nmodels extend to other tasks such as commonsense reasoning and word-in-context\nsemantic judgment. The MGSM benchmark is publicly available at\nhttps://github.com/google-research/url-nlp.", "published": "2022-10-06 17:03:34", "link": "http://arxiv.org/abs/2210.03057v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "InferES : A Natural Language Inference Corpus for Spanish Featuring\n  Negation-Based Contrastive and Adversarial Examples", "abstract": "In this paper, we present InferES - an original corpus for Natural Language\nInference (NLI) in European Spanish. We propose, implement, and analyze a\nvariety of corpus-creating strategies utilizing expert linguists and crowd\nworkers. The objectives behind InferES are to provide high-quality data, and,\nat the same time to facilitate the systematic evaluation of automated systems.\nSpecifically, we focus on measuring and improving the performance of machine\nlearning systems on negation-based adversarial examples and their ability to\ngeneralize across out-of-distribution topics.\n  We train two transformer models on InferES (8,055 gold examples) in a variety\nof scenarios. Our best model obtains 72.8% accuracy, leaving a lot of room for\nimprovement. The \"hypothesis-only\" baseline performs only 2%-5% higher than\nmajority, indicating much fewer annotation artifacts than prior work. We find\nthat models trained on InferES generalize very well across topics (both in- and\nout-of-distribution) and perform moderately well on negation-based adversarial\nexamples.", "published": "2022-10-06 17:22:25", "link": "http://arxiv.org/abs/2210.03068v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Iterative Vision-and-Language Navigation", "abstract": "We present Iterative Vision-and-Language Navigation (IVLN), a paradigm for\nevaluating language-guided agents navigating in a persistent environment over\ntime. Existing Vision-and-Language Navigation (VLN) benchmarks erase the\nagent's memory at the beginning of every episode, testing the ability to\nperform cold-start navigation with no prior information. However, deployed\nrobots occupy the same environment for long periods of time. The IVLN paradigm\naddresses this disparity by training and evaluating VLN agents that maintain\nmemory across tours of scenes that consist of up to 100 ordered\ninstruction-following Room-to-Room (R2R) episodes, each defined by an\nindividual language instruction and a target path. We present discrete and\ncontinuous Iterative Room-to-Room (IR2R) benchmarks comprising about 400 tours\neach in 80 indoor scenes. We find that extending the implicit memory of\nhigh-performing transformer VLN agents is not sufficient for IVLN, but agents\nthat build maps can benefit from environment persistence, motivating a renewed\nfocus on map-building agents in VLN.", "published": "2022-10-06 17:46:00", "link": "http://arxiv.org/abs/2210.03087v3", "categories": ["cs.CV", "cs.CL", "cs.RO"], "primary_category": "cs.CV"}
{"title": "A New Path: Scaling Vision-and-Language Navigation with Synthetic\n  Instructions and Imitation Learning", "abstract": "Recent studies in Vision-and-Language Navigation (VLN) train RL agents to\nexecute natural-language navigation instructions in photorealistic\nenvironments, as a step towards robots that can follow human instructions.\nHowever, given the scarcity of human instruction data and limited diversity in\nthe training environments, these agents still struggle with complex language\ngrounding and spatial language understanding. Pretraining on large text and\nimage-text datasets from the web has been extensively explored but the\nimprovements are limited. We investigate large-scale augmentation with\nsynthetic instructions. We take 500+ indoor environments captured in\ndensely-sampled 360 degree panoramas, construct navigation trajectories through\nthese panoramas, and generate a visually-grounded instruction for each\ntrajectory using Marky, a high-quality multilingual navigation instruction\ngenerator. We also synthesize image observations from novel viewpoints using an\nimage-to-image GAN. The resulting dataset of 4.2M instruction-trajectory pairs\nis two orders of magnitude larger than existing human-annotated datasets, and\ncontains a wider variety of environments and viewpoints. To efficiently\nleverage data at this scale, we train a simple transformer agent with imitation\nlearning. On the challenging RxR dataset, our approach outperforms all existing\nRL agents, improving the state-of-the-art NDTW from 71.1 to 79.1 in seen\nenvironments, and from 64.6 to 66.8 in unseen test environments. Our work\npoints to a new path to improving instruction-following agents, emphasizing\nlarge-scale imitation learning and the development of synthetic instruction\ngeneration capabilities.", "published": "2022-10-06 17:59:08", "link": "http://arxiv.org/abs/2210.03112v3", "categories": ["cs.LG", "cs.CL", "cs.CV", "cs.RO"], "primary_category": "cs.LG"}
{"title": "Prompt Compression and Contrastive Conditioning for Controllability and\n  Toxicity Reduction in Language Models", "abstract": "We explore the idea of compressing the prompts used to condition language\nmodels, and show that compressed prompts can retain a substantive amount of\ninformation about the original prompt. For severely compressed prompts, while\nfine-grained information is lost, abstract information and general sentiments\ncan be retained with surprisingly few parameters, which can be useful in the\ncontext of decode-time algorithms for controllability and toxicity reduction.\nWe explore contrastive conditioning to steer language model generation towards\ndesirable text and away from undesirable text, and find that some complex\nprompts can be effectively compressed into a single token to guide generation.\nWe also show that compressed prompts are largely compositional, and can be\nconstructed such that they can be used to control independent aspects of\ngenerated text.", "published": "2022-10-06 18:52:24", "link": "http://arxiv.org/abs/2210.03162v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PQLM -- Multilingual Decentralized Portable Quantum Language Model for\n  Privacy Protection", "abstract": "With careful manipulation, malicious agents can reverse engineer private\ninformation encoded in pre-trained language models. Security concerns motivate\nthe development of quantum pre-training. In this work, we propose a highly\nPortable Quantum Language Model (PQLM) that can easily transmit information to\ndownstream tasks on classical machines. The framework consists of a cloud PQLM\nbuilt with random Variational Quantum Classifiers (VQC) and local models for\ndownstream applications. We demonstrate the ad hoc portability of the quantum\nmodel by extracting only the word embeddings and effectively applying them to\ndownstream tasks on classical machines. Our PQLM exhibits comparable\nperformance to its classical counterpart on both intrinsic evaluation (loss,\nperplexity) and extrinsic evaluation (multilingual sentiment analysis accuracy)\nmetrics. We also perform ablation studies on the factors affecting PQLM\nperformance to analyze model stability. Our work establishes a theoretical\nfoundation for a portable quantum pre-trained language model that could be\ntrained on private data and made available for public use with privacy\nprotection guarantees.", "published": "2022-10-06 21:29:17", "link": "http://arxiv.org/abs/2210.03221v5", "categories": ["cs.LG", "cs.CL", "quant-ph"], "primary_category": "cs.LG"}
{"title": "Damage Control During Domain Adaptation for Transducer Based Automatic\n  Speech Recognition", "abstract": "Automatic speech recognition models are often adapted to improve their\naccuracy in a new domain. A potential drawback of model adaptation to new\ndomains is catastrophic forgetting, where the Word Error Rate on the original\ndomain is significantly degraded. This paper addresses the situation when we\nwant to simultaneously adapt automatic speech recognition models to a new\ndomain and limit the degradation of accuracy on the original domain without\naccess to the original training dataset. We propose several techniques such as\na limited training strategy and regularized adapter modules for the Transducer\nencoder, prediction, and joiner network. We apply these methods to the Google\nSpeech Commands and to the UK and Ireland English Dialect speech data set and\nobtain strong results on the new target domain while limiting the degradation\non the original domain.", "published": "2022-10-06 23:38:50", "link": "http://arxiv.org/abs/2210.03255v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ReAct: Synergizing Reasoning and Acting in Language Models", "abstract": "While large language models (LLMs) have demonstrated impressive capabilities\nacross tasks in language understanding and interactive decision making, their\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.\naction plan generation) have primarily been studied as separate topics. In this\npaper, we explore the use of LLMs to generate both reasoning traces and\ntask-specific actions in an interleaved manner, allowing for greater synergy\nbetween the two: reasoning traces help the model induce, track, and update\naction plans as well as handle exceptions, while actions allow it to interface\nwith external sources, such as knowledge bases or environments, to gather\nadditional information. We apply our approach, named ReAct, to a diverse set of\nlanguage and decision making tasks and demonstrate its effectiveness over\nstate-of-the-art baselines, as well as improved human interpretability and\ntrustworthiness over methods without reasoning or acting components.\nConcretely, on question answering (HotpotQA) and fact verification (Fever),\nReAct overcomes issues of hallucination and error propagation prevalent in\nchain-of-thought reasoning by interacting with a simple Wikipedia API, and\ngenerates human-like task-solving trajectories that are more interpretable than\nbaselines without reasoning traces. On two interactive decision making\nbenchmarks (ALFWorld and WebShop), ReAct outperforms imitation and\nreinforcement learning methods by an absolute success rate of 34% and 10%\nrespectively, while being prompted with only one or two in-context examples.\nProject site with code: https://react-lm.github.io", "published": "2022-10-06 01:00:32", "link": "http://arxiv.org/abs/2210.03629v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Detecting Emerging Technologies in Artificial Intelligence Scientific\n  Ecosystem Using an Indicator-based Model", "abstract": "Early identification of emergent topics is of eminent importance due to their\npotential impacts on society. There are many methods for detecting emerging\nterms and topics, all with advantages and drawbacks. However, there is no\nconsensus about the attributes and indicators of emergence. In this study, we\nevaluate emerging topic detection in the field of artificial intelligence using\na new method to evaluate emergence. We also introduce two new attributes of\ncollaboration and technological impact which can help us use both paper and\npatent information simultaneously. Our results confirm that the proposed new\nmethod can successfully identify the emerging topics in the period of the\nstudy. Moreover, this new method can provide us with the score of each\nattribute and a final emergence score, which enable us to rank the emerging\ntopics with their emergence scores and each attribute score.", "published": "2022-10-06 15:01:53", "link": "http://arxiv.org/abs/2211.01348v1", "categories": ["cs.DL", "cs.AI", "cs.CL", "cs.SI"], "primary_category": "cs.DL"}
{"title": "Matching Text and Audio Embeddings: Exploring Transfer-learning\n  Strategies for Language-based Audio Retrieval", "abstract": "We present an analysis of large-scale pretrained deep learning models used\nfor cross-modal (text-to-audio) retrieval. We use embeddings extracted by these\nmodels in a metric learning framework to connect matching pairs of audio and\ntext. Shallow neural networks map the embeddings to a common dimensionality.\nOur system, which is an extension of our submission to the Language-based Audio\nRetrieval Task of the DCASE Challenge 2022, employs the RoBERTa foundation\nmodel as the text embedding extractor. A pretrained PANNs model extracts the\naudio embeddings. To improve the generalisation of our model, we investigate\nhow pretraining with audio and associated noisy text collected from the online\nplatform Freesound improves the performance of our method. Furthermore, our\nablation study reveals that the proper choice of the loss function and\nfine-tuning the pretrained models are essential in training a competitive\nretrieval system.", "published": "2022-10-06 11:45:14", "link": "http://arxiv.org/abs/2210.02833v1", "categories": ["cs.IR", "cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.IR"}
{"title": "Fully Unsupervised Training of Few-shot Keyword Spotting", "abstract": "For training a few-shot keyword spotting (FS-KWS) model, a large labeled\ndataset containing massive target keywords has known to be essential to\ngeneralize to arbitrary target keywords with only a few enrollment samples. To\nalleviate the expensive data collection with labeling, in this paper, we\npropose a novel FS-KWS system trained only on synthetic data. The proposed\nsystem is based on metric learning enabling target keywords to be detected\nusing distance metrics. Exploiting the speech synthesis model that generates\nspeech with pseudo phonemes instead of texts, we easily obtain a large\ncollection of multi-view samples with the same semantics. These samples are\nsufficient for training, considering metric learning does not intrinsically\nnecessitate labeled data. All of the components in our framework do not require\nany supervision, making our method unsupervised. Experimental results on real\ndatasets show our proposed method is competitive even without any labeled and\nreal datasets.", "published": "2022-10-06 07:47:10", "link": "http://arxiv.org/abs/2210.02732v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "WakeUpNet: A Mobile-Transformer based Framework for End-to-End Streaming\n  Voice Trigger", "abstract": "End-to-end models have gradually become the main technical stream for voice\ntrigger, aiming to achieve an utmost prediction accuracy but with a small\nfootprint. In present paper, we propose an end-to-end voice trigger framework,\nnamely WakeupNet, which is basically structured on a Transformer encoder. The\npurpose of this framework is to explore the context-capturing capability of\nTransformer, as sequential information is vital for wakeup-word detection.\nHowever, the conventional Transformer encoder is too large to fit our task. To\naddress this issue, we introduce different model compression approaches to\nshrink the vanilla one into a tiny one, called mobile-Transformer. To evaluate\nthe performance of mobile-Transformer, we conduct extensive experiments on a\nlarge public-available dataset HiMia. The obtained results indicate that\nintroduced mobile-Transformer significantly outperforms other frequently used\nmodels for voice trigger in both clean and noisy scenarios.", "published": "2022-10-06 13:18:48", "link": "http://arxiv.org/abs/2210.02904v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "AnimeTAB: A new guitar tablature dataset of anime and game music", "abstract": "While guitar tablature has become a popular topic in MIR research, there\nexists no such a guitar tablature dataset that focuses on the soundtracks of\nanime and video games, which have a surprisingly broad and growing audience\namong the youths. In this paper, we present AnimeTAB, a fingerstyle guitar\ntablature dataset in MusicXML format, which provides more high-quality guitar\ntablature for both researchers and guitar players. AnimeTAB contains 412 full\ntracks and 547 clips, the latter are annotated with musical structures (intro,\nverse, chorus, and bridge). An accompanying analysis toolkit, TABprocessor, is\nincluded to further facilitate its use. This includes functions for melody and\nbassline extraction, key detection, and chord labeling, which are implemented\nusing rule-based algorithms. We evaluated each of these functions against a\nmanually annotated ground truth. Finally, as an example, we performed a music\nand technique analysis of AnimeTAB using TABprocessor. Our data and code have\nbeen made publicly available for composers, performers, and music information\nretrieval (MIR) researchers alike.", "published": "2022-10-06 16:21:26", "link": "http://arxiv.org/abs/2210.03027v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Feasibility on Detecting Door Slamming towards Monitoring Early Signs of\n  Domestic Violence", "abstract": "By using low-cost microcontrollers and TinyML, we investigate the feasibility\nof detecting potential early warning signs of domestic violence and other\nanti-social behaviors within the home. We created a machine learning model to\ndetermine if a door was closed aggressively by analyzing audio data and feeding\nthis into a convolutional neural network to classify the sample. Under test\nconditions, with no background noise, accuracy of 88.89\\% was achieved,\ndeclining to 87.50\\% when assorted background noises were mixed in at a\nrelative volume of 0.5 times that of the sample. The model is then deployed on\nan Arduino Nano BLE 33 Sense attached to the door, and only begins sampling\nonce an acceleration greater than a predefined threshold acceleration is\ndetected. The predictions made by the model can then be sent via BLE to another\ndevice, such as a smartphone of Raspberry Pi.", "published": "2022-10-06 02:25:03", "link": "http://arxiv.org/abs/2210.02642v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "PSVRF: Learning to restore Pitch-Shifted Voice without reference", "abstract": "Pitch scaling algorithms have a significant impact on the security of\nAutomatic Speaker Verification (ASV) systems. Although numerous anti-spoofing\nalgorithms have been proposed to identify the pitch-shifted voice and even\nrestore it to the original version, they either have poor performance or\nrequire the original voice as a reference, limiting the prospects of\napplications. In this paper, we propose a no-reference approach termed\nPSVRF$^1$ for high-quality restoration of pitch-shifted voice. Experiments on\nAISHELL-1 and AISHELL-3 demonstrate that PSVRF can restore the voice disguised\nby various pitch-scaling techniques, which obviously enhances the robustness of\nASV systems to pitch-scaling attacks. Furthermore, the performance of PSVRF\neven surpasses that of the state-of-the-art reference-based approach.", "published": "2022-10-06 07:44:51", "link": "http://arxiv.org/abs/2210.02731v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The Sound of Silence: Efficiency of First Digit Features in Synthetic\n  Audio Detection", "abstract": "The recent integration of generative neural strategies and audio processing\ntechniques have fostered the widespread of synthetic speech synthesis or\ntransformation algorithms. This capability proves to be harmful in many legal\nand informative processes (news, biometric authentication, audio evidence in\ncourts, etc.). Thus, the development of efficient detection algorithms is both\ncrucial and challenging due to the heterogeneity of forgery techniques.\n  This work investigates the discriminative role of silenced parts in synthetic\nspeech detection and shows how first digit statistics extracted from MFCC\ncoefficients can efficiently enable a robust detection. The proposed procedure\nis computationally-lightweight and effective on many different algorithms since\nit does not rely on large neural detection architecture and obtains an accuracy\nabove 90\\% in most of the classes of the ASVSpoof dataset.", "published": "2022-10-06 08:31:21", "link": "http://arxiv.org/abs/2210.02746v1", "categories": ["cs.SD", "cs.CR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Melody Infilling with User-Provided Structural Context", "abstract": "This paper proposes a novel Transformer-based model for music score\ninfilling, to generate a music passage that fills in the gap between given past\nand future contexts. While existing infilling approaches can generate a passage\nthat connects smoothly locally with the given contexts, they do not take into\naccount the musical form or structure of the music and may therefore generate\noverly smooth results. To address this issue, we propose a structure-aware\nconditioning approach that employs a novel attention-selecting module to supply\nuser-provided structure-related information to the Transformer for infilling.\nWith both objective and subjective evaluations, we show that the proposed model\ncan harness the structural information effectively and generate melodies in the\nstyle of pop of higher quality than the two existing structure-agnostic\ninfilling models.", "published": "2022-10-06 11:37:04", "link": "http://arxiv.org/abs/2210.02829v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
