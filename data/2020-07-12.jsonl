{"title": "Neural disambiguation of lemma and part of speech in morphologically\n  rich languages", "abstract": "We consider the problem of disambiguating the lemma and part of speech of\nambiguous words in morphologically rich languages. We propose a method for\ndisambiguating ambiguous words in context, using a large un-annotated corpus of\ntext, and a morphological analyser -- with no manual disambiguation or data\nannotation. We assume that the morphological analyser produces multiple\nanalyses for ambiguous words. The idea is to train recurrent neural networks on\nthe output that the morphological analyser produces for unambiguous words. We\npresent performance on POS and lemma disambiguation that reaches or surpasses\nthe state of the art -- including supervised models -- using no manually\nannotated data. We evaluate the method on several morphologically rich\nlanguages.", "published": "2020-07-12 21:48:52", "link": "http://arxiv.org/abs/2007.06104v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Stance Detection in Web and Social Media: A Comparative Study", "abstract": "Online forums and social media platforms are increasingly being used to\ndiscuss topics of varying polarities where different people take different\nstances. Several methodologies for automatic stance detection from text have\nbeen proposed in literature. To our knowledge, there has not been any\nsystematic investigation towards their reproducibility, and their comparative\nperformances. In this work, we explore the reproducibility of several existing\nstance detection models, including both neural models and classical\nclassifier-based models. Through experiments on two datasets -- (i)~the popular\nSemEval microblog dataset, and (ii)~a set of health-related online news\narticles -- we also perform a detailed comparative analysis of various methods\nand explore their shortcomings. Implementations of all algorithms discussed in\nthis paper are available at\nhttps://github.com/prajwal1210/Stance-Detection-in-Web-and-Social-Media.", "published": "2020-07-12 12:39:35", "link": "http://arxiv.org/abs/2007.05976v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "HyperGrid: Efficient Multi-Task Transformers with Grid-wise Decomposable\n  Hyper Projections", "abstract": "Achieving state-of-the-art performance on natural language understanding\ntasks typically relies on fine-tuning a fresh model for every task.\nConsequently, this approach leads to a higher overall parameter cost, along\nwith higher technical maintenance for serving multiple models. Learning a\nsingle multi-task model that is able to do well for all the tasks has been a\nchallenging and yet attractive proposition. In this paper, we propose\n\\textsc{HyperGrid}, a new approach for highly effective multi-task learning.\nThe proposed approach is based on a decomposable hypernetwork that learns\ngrid-wise projections that help to specialize regions in weight matrices for\ndifferent tasks. In order to construct the proposed hypernetwork, our method\nlearns the interactions and composition between a global (task-agnostic) state\nand a local task-specific state. We apply our proposed \\textsc{HyperGrid} on\nthe current state-of-the-art T5 model, demonstrating strong performance across\nthe GLUE and SuperGLUE benchmarks when using only a single multi-task model.\nOur method helps bridge the gap between fine-tuning and multi-task learning\napproaches.", "published": "2020-07-12 02:49:16", "link": "http://arxiv.org/abs/2007.05891v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The ASRU 2019 Mandarin-English Code-Switching Speech Recognition\n  Challenge: Open Datasets, Tracks, Methods and Results", "abstract": "Code-switching (CS) is a common phenomenon and recognizing CS speech is\nchallenging. But CS speech data is scarce and there' s no common testbed in\nrelevant research. This paper describes the design and main outcomes of the\nASRU 2019 Mandarin-English code-switching speech recognition challenge, which\naims to improve the ASR performance in Mandarin-English code-switching\nsituation. 500 hours Mandarin speech data and 240 hours Mandarin-English\nintra-sentencial CS data are released to the participants. Three tracks were\nset for advancing the AM and LM part in traditional DNN-HMM ASR system, as well\nas exploring the E2E models' performance. The paper then presents an overview\nof the results and system performance in the three tracks. It turns out that\ntraditional ASR system benefits from pronunciation lexicon, CS text generating\nand data augmentation. In E2E track, however, the results highlight the\nimportance of using language identification, building-up a rational set of\nmodeling units and spec-augment. The other details in model training and method\ncomparsion are discussed.", "published": "2020-07-12 05:38:57", "link": "http://arxiv.org/abs/2007.05916v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improving Maximum Likelihood Training for Text Generation with Density\n  Ratio Estimation", "abstract": "Auto-regressive sequence generative models trained by Maximum Likelihood\nEstimation suffer the exposure bias problem in practical finite sample\nscenarios. The crux is that the number of training samples for Maximum\nLikelihood Estimation is usually limited and the input data distributions are\ndifferent at training and inference stages. Many method shave been proposed to\nsolve the above problem (Yu et al., 2017; Lu et al., 2018), which relies on\nsampling from the non-stationary model distribution and suffers from high\nvariance or biased estimations. In this paper, we propose{\\psi}-MLE, a new\ntraining scheme for auto-regressive sequence generative models, which is\neffective and stable when operating at large sample space encountered in text\ngeneration. We derive our algorithm from a new perspective of self-augmentation\nand introduce bias correction with density ratio estimation. Extensive\nexperimental results on synthetic data and real-world text generation tasks\ndemonstrate that our method stably outperforms Maximum Likelihood Estimation\nand other state-of-the-art sequence generative models in terms of both quality\nand diversity.", "published": "2020-07-12 15:31:24", "link": "http://arxiv.org/abs/2007.06018v1", "categories": ["stat.ML", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
{"title": "TERA: Self-Supervised Learning of Transformer Encoder Representation for\n  Speech", "abstract": "We introduce a self-supervised speech pre-training method called TERA, which\nstands for Transformer Encoder Representations from Alteration. Recent\napproaches often learn by using a single auxiliary task like contrastive\nprediction, autoregressive prediction, or masked reconstruction. Unlike\nprevious methods, we use alteration along three orthogonal axes to pre-train\nTransformer Encoders on a large amount of unlabeled speech. The model learns\nthrough the reconstruction of acoustic frames from their altered counterpart,\nwhere we use a stochastic policy to alter along various dimensions: time,\nfrequency, and magnitude. TERA can be used for speech representations\nextraction or fine-tuning with downstream models. We evaluate TERA on several\ndownstream tasks, including phoneme classification, keyword spotting, speaker\nrecognition, and speech recognition. We present a large-scale comparison of\nvarious self-supervised models. TERA achieves strong performance in the\ncomparison by improving upon surface features and outperforming previous\nmodels. In our experiments, we study the effect of applying different\nalteration techniques, pre-training on more data, and pre-training on various\nfeatures. We analyze different model sizes and find that smaller models are\nstrong representation learners than larger models, while larger models are more\neffective for downstream fine-tuning than smaller models. Furthermore, we show\nthe proposed method is transferable to downstream datasets not used in\npre-training.", "published": "2020-07-12 16:19:00", "link": "http://arxiv.org/abs/2007.06028v3", "categories": ["eess.AS", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Sparse Graph to Sequence Learning for Vision Conditioned Long Textual\n  Sequence Generation", "abstract": "Generating longer textual sequences when conditioned on the visual\ninformation is an interesting problem to explore. The challenge here\nproliferate over the standard vision conditioned sentence-level generation\n(e.g., image or video captioning) as it requires to produce a brief and\ncoherent story describing the visual content. In this paper, we mask this\nVision-to-Sequence as Graph-to-Sequence learning problem and approach it with\nthe Transformer architecture. To be specific, we introduce Sparse\nGraph-to-Sequence Transformer (SGST) for encoding the graph and decoding a\nsequence. The encoder aims to directly encode graph-level semantics, while the\ndecoder is used to generate longer sequences. Experiments conducted with the\nbenchmark image paragraph dataset show that our proposed achieve 13.3%\nimprovement on the CIDEr evaluation measure when comparing to the previous\nstate-of-the-art approach.", "published": "2020-07-12 19:54:32", "link": "http://arxiv.org/abs/2007.06077v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Fine-grained Language Identification with Multilingual CapsNet Model", "abstract": "Due to a drastic improvement in the quality of internet services worldwide,\nthere is an explosion of multilingual content generation and consumption. This\nis especially prevalent in countries with large multilingual audience, who are\nincreasingly consuming media outside their linguistic familiarity/preference.\nHence, there is an increasing need for real-time and fine-grained content\nanalysis services, including language identification, content transcription,\nand analysis. Accurate and fine-grained spoken language detection is an\nessential first step for all the subsequent content analysis algorithms.\nCurrent techniques in spoken language detection may lack on one of these\nfronts: accuracy, fine-grained detection, data requirements, manual effort in\ndata collection \\& pre-processing. Hence in this work, a real-time language\ndetection approach to detect spoken language from 5 seconds' audio clips with\nan accuracy of 91.8\\% is presented with exiguous data requirements and minimal\npre-processing. Novel architectures for Capsule Networks is proposed which\noperates on spectrogram images of the provided audio snippets. We use previous\napproaches based on Recurrent Neural Networks and iVectors to present the\nresults. Finally we show a ``Non-Class'' analysis to further stress on why\nCapsNet architecture works for LID task.", "published": "2020-07-12 20:01:22", "link": "http://arxiv.org/abs/2007.06078v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Xiaomingbot: A Multilingual Robot News Reporter", "abstract": "This paper proposes the building of Xiaomingbot, an intelligent, multilingual\nand multimodal software robot equipped with four integral capabilities: news\ngeneration, news translation, news reading and avatar animation. Its system\nsummarizes Chinese news that it automatically generates from data tables. Next,\nit translates the summary or the full article into multiple languages, and\nreads the multilingual rendition through synthesized speech. Notably,\nXiaomingbot utilizes a voice cloning technology to synthesize the speech\ntrained from a real person's voice data in one input language. The proposed\nsystem enjoys several merits: it has an animated avatar, and is able to\ngenerate and read multilingual news. Since it was put into practice,\nXiaomingbot has written over 600,000 articles, and gained over 150,000\nfollowers on social media platforms.", "published": "2020-07-12 14:49:41", "link": "http://arxiv.org/abs/2007.08005v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "NISP: A Multi-lingual Multi-accent Dataset for Speaker Profiling", "abstract": "Many commercial and forensic applications of speech demand the extraction of\ninformation about the speaker characteristics, which falls into the broad\ncategory of speaker profiling. The speaker characteristics needed for profiling\ninclude physical traits of the speaker like height, age, and gender of the\nspeaker along with the native language of the speaker. Many of the datasets\navailable have only partial information for speaker profiling. In this paper,\nwe attempt to overcome this limitation by developing a new dataset which has\nspeech data from five different Indian languages along with English. The\nmetadata information for speaker profiling applications like linguistic\ninformation, regional information, and physical characteristics of a speaker\nare also collected. We call this dataset as NITK-IISc Multilingual Multi-accent\nSpeaker Profiling (NISP) dataset. The description of the dataset, potential\napplications, and baseline results for speaker profiling on this dataset are\nprovided in this paper.", "published": "2020-07-12 15:46:57", "link": "http://arxiv.org/abs/2007.06021v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Learning Frame Level Attention for Environmental Sound Classification", "abstract": "Environmental sound classification (ESC) is a challenging problem due to the\ncomplexity of sounds. The classification performance is heavily dependent on\nthe effectiveness of representative features extracted from the environmental\nsounds. However, ESC often suffers from the semantically irrelevant frames and\nsilent frames. In order to deal with this, we employ a frame-level attention\nmodel to focus on the semantically relevant frames and salient frames.\nSpecifically, we first propose a convolutional recurrent neural network to\nlearn spectro-temporal features and temporal correlations. Then, we extend our\nconvolutional RNN model with a frame-level attention mechanism to learn\ndiscriminative feature representations for ESC. We investigated the\nclassification performance when using different attention scaling function and\napplying different layers. Experiments were conducted on ESC-50 and ESC-10\ndatasets. Experimental results demonstrated the effectiveness of the proposed\nmethod and our method achieved the state-of-the-art or competitive\nclassification accuracy with lower computational complexity. We also visualized\nour attention results and observed that the proposed attention mechanism was\nable to lead the network tofocus on the semantically relevant parts of\nenvironmental sounds.", "published": "2020-07-12 10:33:27", "link": "http://arxiv.org/abs/2007.07241v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Data augmentation enhanced speaker enrollment for text-dependent speaker\n  verification", "abstract": "Data augmentation is commonly used for generating additional data from the\navailable training data to achieve a robust estimation of the parameters of\ncomplex models like the one for speaker verification (SV), especially for\nunder-resourced applications. SV involves training speaker-independent (SI)\nmodels and speaker-dependent models where speakers are represented by models\nderived from an SI model using the training data for the particular speaker\nduring the enrollment phase. While data augmentation for training SI models is\nwell studied, data augmentation for speaker enrollment is rarely explored. In\nthis paper, we propose the use of data augmentation methods for generating\nextra data to empower speaker enrollment. Each data augmentation method\ngenerates a new data set. Two strategies of using the data sets are explored:\nthe first one is to training separate systems and fuses them at the score level\nand the other is to conduct multi-conditional training. Furthermore, we study\nthe effect of data augmentation under noisy conditions. Experiments are\nperformed on RedDots challenge 2016 database, and the results validate the\neffectiveness of the proposed methods.", "published": "2020-07-12 09:04:03", "link": "http://arxiv.org/abs/2007.08004v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Tandem Assessment of Spoofing Countermeasures and Automatic Speaker\n  Verification: Fundamentals", "abstract": "Recent years have seen growing efforts to develop spoofing countermeasures\n(CMs) to protect automatic speaker verification (ASV) systems from being\ndeceived by manipulated or artificial inputs. The reliability of spoofing CMs\nis typically gauged using the equal error rate (EER) metric. The primitive EER\nfails to reflect application requirements and the impact of spoofing and CMs\nupon ASV and its use as a primary metric in traditional ASV research has long\nbeen abandoned in favour of risk-based approaches to assessment. This paper\npresents several new extensions to the tandem detection cost function (t-DCF),\na recent risk-based approach to assess the reliability of spoofing CMs deployed\nin tandem with an ASV system. Extensions include a simplified version of the\nt-DCF with fewer parameters, an analysis of a special case for a fixed ASV\nsystem, simulations which give original insights into its interpretation and\nnew analyses using the ASVspoof 2019 database. It is hoped that adoption of the\nt-DCF for the CM assessment will help to foster closer collaboration between\nthe anti-spoofing and ASV research communities.", "published": "2020-07-12 12:44:08", "link": "http://arxiv.org/abs/2007.05979v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "OtoWorld: Towards Learning to Separate by Learning to Move", "abstract": "We present OtoWorld, an interactive environment in which agents must learn to\nlisten in order to solve navigational tasks. The purpose of OtoWorld is to\nfacilitate reinforcement learning research in computer audition, where agents\nmust learn to listen to the world around them to navigate. OtoWorld is built on\nthree open source libraries: OpenAI Gym for environment and agent interaction,\nPyRoomAcoustics for ray-tracing and acoustics simulation, and nussl for\ntraining deep computer audition models. OtoWorld is the audio analogue of\nGridWorld, a simple navigation game. OtoWorld can be easily extended to more\ncomplex environments and games. To solve one episode of OtoWorld, an agent must\nmove towards each sounding source in the auditory scene and \"turn it off\". The\nagent receives no other input than the current sound of the room. The sources\nare placed randomly within the room and can vary in number. The agent receives\na reward for turning off a source. We present preliminary results on the\nability of agents to win at OtoWorld. OtoWorld is open-source and available.", "published": "2020-07-12 22:53:24", "link": "http://arxiv.org/abs/2007.06123v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
