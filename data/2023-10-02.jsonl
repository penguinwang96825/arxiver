{"title": "TRAM: Benchmarking Temporal Reasoning for Large Language Models", "abstract": "Reasoning about time is essential for understanding the nuances of events\ndescribed in natural language. Previous research on this topic has been limited\nin scope, characterized by a lack of standardized benchmarks that would allow\nfor consistent evaluations across different studies. In this paper, we\nintroduce TRAM, a temporal reasoning benchmark composed of ten datasets,\nencompassing various temporal aspects of events such as order, arithmetic,\nfrequency, and duration, designed to facilitate a comprehensive evaluation of\nthe TeR capabilities of large language models (LLMs). We evaluate popular LLMs\nlike GPT-4 and Llama2 in zero-shot and few-shot scenarios, and establish\nbaselines with BERT-based and domain-specific models. Our findings indicate\nthat the best-performing model lags significantly behind human performance. It\nis our aspiration that TRAM will spur further progress in enhancing the TeR\ncapabilities of LLMs.", "published": "2023-10-02 00:59:07", "link": "http://arxiv.org/abs/2310.00835v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Error Norm Truncation: Robust Training in the Presence of Data Noise for\n  Text Generation Models", "abstract": "Text generation models are notoriously vulnerable to errors in the training\ndata. With the wide-spread availability of massive amounts of web-crawled data\nbecoming more commonplace, how can we enhance the robustness of models trained\non a massive amount of noisy web-crawled text? In our work, we propose Error\nNorm Truncation (ENT), a robust enhancement method to the standard training\nobjective that truncates noisy data. Compared to methods that only uses the\nnegative log-likelihood loss to estimate data quality, our method provides a\nmore accurate estimation by considering the distribution of non-target tokens,\nwhich is often overlooked by previous work. Through comprehensive experiments\nacross language modeling, machine translation, and text summarization, we show\nthat equipping text generation models with ENT improves generation quality over\nstandard training and previous soft and hard truncation methods. Furthermore,\nwe show that our method improves the robustness of models against two of the\nmost detrimental types of noise in machine translation, resulting in an\nincrease of more than 2 BLEU points over the MLE baseline when up to 50% of\nnoise is added to the data.", "published": "2023-10-02 01:30:27", "link": "http://arxiv.org/abs/2310.00840v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enabling Language Models to Implicitly Learn Self-Improvement", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nopen-ended text generation tasks. However, the inherent open-ended nature of\nthese tasks implies that there is always room for improvement in the quality of\nmodel responses. To address this challenge, various approaches have been\nproposed to enhance the performance of LLMs. There has been a growing focus on\nenabling LLMs to self-improve their response quality, thereby reducing the\nreliance on extensive human annotation efforts for collecting diverse and\nhigh-quality training data. Recently, prompting-based methods have been widely\nexplored among self-improvement methods owing to their effectiveness,\nefficiency, and convenience. However, those methods usually require explicitly\nand thoroughly written rubrics as inputs to LLMs. It is expensive and\nchallenging to manually derive and provide all necessary rubrics with a\nreal-world complex goal for improvement (e.g., being more helpful and less\nharmful). To this end, we propose an ImPlicit Self-ImprovemenT (PIT) framework\nthat implicitly learns the improvement goal from human preference data. PIT\nonly requires preference data that are used to train reward models without\nextra human efforts. Specifically, we reformulate the training objective of\nreinforcement learning from human feedback (RLHF) -- instead of maximizing\nresponse quality for a given input, we maximize the quality gap of the response\nconditioned on a reference response. In this way, PIT is implicitly trained\nwith the improvement goal of better aligning with human preferences.\nExperiments on two real-world datasets and one synthetic dataset show that our\nmethod significantly outperforms prompting-based methods.", "published": "2023-10-02 04:29:40", "link": "http://arxiv.org/abs/2310.00898v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PACIT: Unlocking the Power of Examples for Better In-Context Instruction\n  Tuning", "abstract": "Instruction tuning enhances the instruction following ability of large\nlanguage models by finetuning with supervised instruction data. Previous work\nproposes in-context instruction tuning (ICIT) where specific positive or\nnegative examples are incorporated into the prompt for better performance. In\nthis work, we propose PACIT, a simple and effective in-context instruction\ntuning method, inspired by the pedagogical concept of desirable difficulty. The\nPACIT method unlocks the power of examples by encouraging the model to actively\nlearn to grasp the distinctions between the positive and negative examples\ninstead of merely reading. The model is expected to first verify the\ncorrectness of the provided example according to the task description, which is\nthen set as the condition for generating a better response to the task\ninstance. Our extensive experiments prove the effectiveness of PACIT,\noutperforming ICIT baseline on both in-domain and out-domain tasks up to 9.16\nand 3.14 average ROUGE-L scores, respectively. Moreover, PACIT can notably\nenhance the performance of instruction tuning even when all positive and\nnegative examples are generated with a self-instruct method.", "published": "2023-10-02 04:42:53", "link": "http://arxiv.org/abs/2310.00901v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Resolving Knowledge Conflicts in Large Language Models", "abstract": "Large language models (LLMs) often encounter knowledge conflicts, scenarios\nwhere discrepancy arises between the internal parametric knowledge of LLMs and\nnon-parametric information provided in the prompt context. In this work we ask\nwhat are the desiderata for LLMs when a knowledge conflict arises and whether\nexisting LLMs fulfill them. We posit that LLMs should 1) identify knowledge\nconflicts, 2) pinpoint conflicting information segments, and 3) provide\ndistinct answers or viewpoints in conflicting scenarios. To this end, we\nintroduce an evaluation framework for simulating contextual knowledge conflicts\nand quantitatively evaluating to what extent LLMs achieve these goals. It\nincludes diverse and complex situations of knowledge conflict, knowledge from\ndiverse entities and domains, two synthetic conflict creation methods, and\nsettings with progressively increasing difficulty to reflect realistic\nknowledge conflicts. Extensive experiments with the framework reveal that while\nLLMs perform well in identifying the existence of knowledge conflicts, they\nstruggle to determine the specific conflicting knowledge and produce a response\nwith distinct answers amidst conflicting information. To address these\nchallenges, we propose new instruction-based approaches that augment LLMs to\nbetter achieve the three goals. Further analysis shows that abilities to tackle\nknowledge conflicts are greatly impacted by factors such as knowledge domain,\nwhile generating robust responses to knowledge conflict scenarios remains an\nopen research question.", "published": "2023-10-02 06:57:45", "link": "http://arxiv.org/abs/2310.00935v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EALM: Introducing Multidimensional Ethical Alignment in Conversational\n  Information Retrieval", "abstract": "Artificial intelligence (AI) technologies should adhere to human norms to\nbetter serve our society and avoid disseminating harmful or misleading\ninformation, particularly in Conversational Information Retrieval (CIR).\nPrevious work, including approaches and datasets, has not always been\nsuccessful or sufficiently robust in taking human norms into consideration. To\nthis end, we introduce a workflow that integrates ethical alignment, with an\ninitial ethical judgment stage for efficient data screening. To address the\nneed for ethical judgment in CIR, we present the QA-ETHICS dataset, adapted\nfrom the ETHICS benchmark, which serves as an evaluation tool by unifying\nscenarios and label meanings. However, each scenario only considers one ethical\nconcept. Therefore, we introduce the MP-ETHICS dataset to evaluate a scenario\nunder multiple ethical concepts, such as justice and Deontology. In addition,\nwe suggest a new approach that achieves top performance in both binary and\nmulti-label ethical judgment tasks. Our research provides a practical method\nfor introducing ethical alignment into the CIR workflow. The data and code are\navailable at https://github.com/wanng-ide/ealm .", "published": "2023-10-02 08:22:34", "link": "http://arxiv.org/abs/2310.00970v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ARN: Analogical Reasoning on Narratives", "abstract": "As a core cognitive skill that enables the transferability of information\nacross domains, analogical reasoning has been extensively studied for both\nhumans and computational models. However, while cognitive theories of analogy\noften focus on narratives and study the distinction between surface,\nrelational, and system similarities, existing work in natural language\nprocessing has a narrower focus as far as relational analogies between word\npairs. This gap brings a natural question: can state-of-the-art large language\nmodels (LLMs) detect system analogies between narratives? To gain insight into\nthis question and extend word-based relational analogies to relational system\nanalogies, we devise a comprehensive computational framework that\noperationalizes dominant theories of analogy, using narrative elements to\ncreate surface and system mappings. Leveraging the interplay between these\nmappings, we create a binary task and benchmark for Analogical Reasoning on\nNarratives (ARN), covering four categories of far (cross-domain)/near\n(within-domain) analogies and disanalogies. We show that while all LLMs can\nlargely recognize near analogies, even the largest ones struggle with far\nanalogies in a zero-shot setting, with GPT4.0 scoring below random. Guiding the\nmodels through solved examples and chain-of-thought reasoning enhances their\nanalogical reasoning ability. Yet, since even in the few-shot setting, the best\nmodel only performs halfway between random and humans, ARN opens exciting\ndirections for computational analogical reasoners.", "published": "2023-10-02 08:58:29", "link": "http://arxiv.org/abs/2310.00996v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Model Decoding as Direct Metrics Optimization", "abstract": "Despite the remarkable advances in language modeling, current mainstream\ndecoding methods still struggle to generate texts that align with human texts\nacross different aspects. In particular, sampling-based methods produce\nless-repetitive texts which are often disjunctive in discourse, while\nsearch-based methods maintain topic coherence at the cost of increased\nrepetition. Overall, these methods fall short in achieving holistic alignment\nacross a broad range of aspects. In this work, we frame decoding from a\nlanguage model as an optimization problem with the goal of strictly matching\nthe expected performance with human texts measured by multiple metrics of\ndesired aspects simultaneously. The resulting decoding distribution enjoys an\nanalytical solution that scales the input language model distribution via a\nsequence-level energy function defined by these metrics. And most importantly,\nwe prove that this induced distribution is guaranteed to improve the perplexity\non human texts, which suggests a better approximation to the underlying\ndistribution of human texts. To facilitate tractable sampling from this\nglobally normalized distribution, we adopt the Sampling-Importance-Resampling\ntechnique. Experiments on various domains and model scales demonstrate the\nsuperiority of our method in metrics alignment with human texts and human\nevaluation over strong baselines.", "published": "2023-10-02 09:35:27", "link": "http://arxiv.org/abs/2310.01041v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tool-Augmented Reward Modeling", "abstract": "Reward modeling (a.k.a., preference modeling) is instrumental for aligning\nlarge language models with human preferences, particularly within the context\nof reinforcement learning from human feedback (RLHF). While conventional reward\nmodels (RMs) have exhibited remarkable scalability, they oft struggle with\nfundamental functionality such as arithmetic computation, code execution, and\nfactual lookup. In this paper, we propose a tool-augmented preference modeling\napproach, named Themis, to address these limitations by empowering RMs with\naccess to external environments, including calculators and search engines. This\napproach not only fosters synergy between tool utilization and reward grading\nbut also enhances interpretive capacity and scoring reliability. Our study\ndelves into the integration of external tools into RMs, enabling them to\ninteract with diverse external sources and construct task-specific tool\nengagement and reasoning traces in an autoregressive manner. We validate our\napproach across a wide range of domains, incorporating seven distinct external\ntools. Our experimental results demonstrate a noteworthy overall improvement of\n17.7% across eight tasks in preference ranking. Furthermore, our approach\noutperforms Gopher 280B by 7.3% on TruthfulQA task in zero-shot evaluation. In\nhuman evaluations, RLHF trained with Themis attains an average win rate of 32%\nwhen compared to baselines across four distinct tasks. Additionally, we provide\na comprehensive collection of tool-related RM datasets, incorporating data from\nseven distinct tool APIs, totaling 15,000 instances. We have made the code,\ndata, and model checkpoints publicly available to facilitate and inspire\nfurther research\nadvancements\\footnote{\\url{https://github.com/ernie-research/Tool-Augmented-Reward-Model}}.", "published": "2023-10-02 09:47:40", "link": "http://arxiv.org/abs/2310.01045v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Target-Aware Contextual Political Bias Detection in News", "abstract": "Media bias detection requires comprehensive integration of information\nderived from multiple news sources. Sentence-level political bias detection in\nnews is no exception, and has proven to be a challenging task that requires an\nunderstanding of bias in consideration of the context. Inspired by the fact\nthat humans exhibit varying degrees of writing styles, resulting in a diverse\nrange of statements with different local and global contexts, previous work in\nmedia bias detection has proposed augmentation techniques to exploit this fact.\nDespite their success, we observe that these techniques introduce noise by\nover-generalizing bias context boundaries, which hinders performance. To\nalleviate this issue, we propose techniques to more carefully search for\ncontext using a bias-sensitive, target-aware approach for data augmentation.\nComprehensive experiments on the well-known BASIL dataset show that when\ncombined with pre-trained models such as BERT, our augmentation techniques lead\nto state-of-the-art results. Our approach outperforms previous methods\nsignificantly, obtaining an F1-score of 58.15 over state-of-the-art bias\ndetection task.", "published": "2023-10-02 12:25:05", "link": "http://arxiv.org/abs/2310.01138v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Label Supervised LLaMA Finetuning", "abstract": "The recent success of Large Language Models (LLMs) has gained significant\nattention in both academia and industry. Substantial efforts have been made to\nenhance the zero- and few-shot generalization capabilities of open-source LLMs\nthrough finetuning. Currently, the prevailing approach is instruction-tuning,\nwhich trains LLMs to complete real-world tasks by generating responses guided\nby natural language instructions. It is worth noticing that such an approach\nmay underperform in sequence and token classification tasks. Unlike text\ngeneration tasks, classification tasks have a limited label space, where\nprecise label prediction is more appreciated than generating diverse and\nhuman-like responses. Prior research has unveiled that instruction-tuned LLMs\ncannot outperform BERT, prompting us to explore the potential of leveraging\nlatent representations from LLMs for supervised label prediction. In this\npaper, we introduce a label-supervised adaptation for LLMs, which aims to\nfinetuning the model with discriminant labels. We evaluate this approach with\nLabel Supervised LLaMA (LS-LLaMA), based on LLaMA-2-7B, a relatively\nsmall-scale LLM, and can be finetuned on a single GeForce RTX4090 GPU. We\nextract latent representations from the final LLaMA layer and project them into\nthe label space to compute the cross-entropy loss. The model is finetuned by\nLow-Rank Adaptation (LoRA) to minimize this loss. Remarkably, without intricate\nprompt engineering or external knowledge, LS-LLaMA substantially outperforms\nLLMs ten times its size in scale and demonstrates consistent improvements\ncompared to robust baselines like BERT-Large and RoBERTa-Large in text\nclassification. Moreover, by removing the causal mask from decoders, LS-unLLaMA\nachieves the state-of-the-art performance in named entity recognition (NER).\nOur work will shed light on a novel approach to adapting LLMs for various\ndownstream tasks.", "published": "2023-10-02 13:53:03", "link": "http://arxiv.org/abs/2310.01208v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Emotional Expression and Cohesion in Image-Based Playlist\n  Description and Music Topics: A Continuous Parameterization Approach", "abstract": "Text generation in image-based platforms, particularly for music-related\ncontent, requires precise control over text styles and the incorporation of\nemotional expression. However, existing approaches often need help to control\nthe proportion of external factors in generated text and rely on discrete\ninputs, lacking continuous control conditions for desired text generation. This\nstudy proposes Continuous Parameterization for Controlled Text Generation\n(CPCTG) to overcome these limitations. Our approach leverages a Language Model\n(LM) as a style learner, integrating Semantic Cohesion (SC) and Emotional\nExpression Proportion (EEP) considerations. By enhancing the reward method and\nmanipulating the CPCTG level, our experiments on playlist description and music\ntopic generation tasks demonstrate significant improvements in ROUGE scores,\nindicating enhanced relevance and coherence in the generated text.", "published": "2023-10-02 14:32:07", "link": "http://arxiv.org/abs/2310.01248v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Who is ChatGPT? Benchmarking LLMs' Psychological Portrayal Using\n  PsychoBench", "abstract": "Large Language Models (LLMs) have recently showcased their remarkable\ncapacities, not only in natural language processing tasks but also across\ndiverse domains such as clinical medicine, legal consultation, and education.\nLLMs become more than mere applications, evolving into assistants capable of\naddressing diverse user requests. This narrows the distinction between human\nbeings and artificial intelligence agents, raising intriguing questions\nregarding the potential manifestation of personalities, temperaments, and\nemotions within LLMs. In this paper, we propose a framework, PsychoBench, for\nevaluating diverse psychological aspects of LLMs. Comprising thirteen scales\ncommonly used in clinical psychology, PsychoBench further classifies these\nscales into four distinct categories: personality traits, interpersonal\nrelationships, motivational tests, and emotional abilities. Our study examines\nfive popular models, namely text-davinci-003, gpt-3.5-turbo, gpt-4, LLaMA-2-7b,\nand LLaMA-2-13b. Additionally, we employ a jailbreak approach to bypass the\nsafety alignment protocols and test the intrinsic natures of LLMs. We have made\nPsychoBench openly accessible via https://github.com/CUHK-ARISE/PsychoBench.", "published": "2023-10-02 17:46:09", "link": "http://arxiv.org/abs/2310.01386v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "It's MBR All the Way Down: Modern Generation Techniques Through the Lens\n  of Minimum Bayes Risk", "abstract": "Minimum Bayes Risk (MBR) decoding is a method for choosing the outputs of a\nmachine learning system based not on the output with the highest probability,\nbut the output with the lowest risk (expected error) among multiple candidates.\nIt is a simple but powerful method: for an additional cost at inference time,\nMBR provides reliable several-point improvements across metrics for a wide\nvariety of tasks without any additional data or training. Despite this, MBR is\nnot frequently applied in NLP works, and knowledge of the method itself is\nlimited. We first provide an introduction to the method and the recent\nliterature. We show that several recent methods that do not reference MBR can\nbe written as special cases of MBR; this reformulation provides additional\ntheoretical justification for the performance of these methods, explaining some\nresults that were previously only empirical. We provide theoretical and\nempirical results about the effectiveness of various MBR variants and make\nconcrete recommendations for the application of MBR in NLP models, including\nfuture directions in this area.", "published": "2023-10-02 17:47:10", "link": "http://arxiv.org/abs/2310.01387v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Closing the Curious Case of Neural Text Degeneration", "abstract": "Despite their ubiquity in language generation, it remains unknown why\ntruncation sampling heuristics like nucleus sampling are so effective. We\nprovide a theoretical explanation for the effectiveness of the truncation\nsampling by proving that truncation methods that discard tokens below some\nprobability threshold (the most common type of truncation) can guarantee that\nall sampled tokens have nonzero true probability. However, thresholds are a\ncoarse heuristic, and necessarily discard some tokens with nonzero true\nprobability as well. In pursuit of a more precise sampling strategy, we show\nthat we can leverage a known source of model errors, the softmax bottleneck, to\nprove that certain tokens have nonzero true probability, without relying on a\nthreshold. Based on our findings, we develop an experimental truncation\nstrategy and the present pilot studies demonstrating the promise of this type\nof algorithm. Our evaluations show that our method outperforms its\nthreshold-based counterparts under automatic and human evaluation metrics for\nlow-entropy (i.e., close to greedy) open-ended text generation. Our theoretical\nfindings and pilot experiments provide both insight into why truncation\nsampling works, and make progress toward more expressive sampling algorithms\nthat better surface the generative capabilities of large language models.", "published": "2023-10-02 23:16:25", "link": "http://arxiv.org/abs/2310.01693v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Natural Language Models for Data Visualization Utilizing nvBench Dataset", "abstract": "Translation of natural language into syntactically correct commands for data\nvisualization is an important application of natural language models and could\nbe leveraged to many different tasks. A closely related effort is the task of\ntranslating natural languages into SQL queries, which in turn could be\ntranslated into visualization with additional information from the natural\nlanguage query supplied\\cite{Zhong:2017qr}. Contributing to the progress in\nthis area of research, we built natural language translation models to\nconstruct simplified versions of data and visualization queries in a language\ncalled Vega Zero. In this paper, we explore the design and performance of these\nsequence to sequence transformer based machine learning model architectures\nusing large language models such as BERT as encoders to predict visualization\ncommands from natural language queries, as well as apply available T5 sequence\nto sequence models to the problem for comparison.", "published": "2023-10-02 00:48:01", "link": "http://arxiv.org/abs/2310.00832v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Necessary and Sufficient Watermark for Large Language Models", "abstract": "In recent years, large language models (LLMs) have achieved remarkable\nperformances in various NLP tasks. They can generate texts that are\nindistinguishable from those written by humans. Such remarkable performance of\nLLMs increases their risk of being used for malicious purposes, such as\ngenerating fake news articles. Therefore, it is necessary to develop methods\nfor distinguishing texts written by LLMs from those written by humans.\nWatermarking is one of the most powerful methods for achieving this. Although\nexisting watermarking methods have successfully detected texts generated by\nLLMs, they significantly degrade the quality of the generated texts. In this\nstudy, we propose the Necessary and Sufficient Watermark (NS-Watermark) for\ninserting watermarks into generated texts without degrading the text quality.\nMore specifically, we derive minimum constraints required to be imposed on the\ngenerated texts to distinguish whether LLMs or humans write the texts. Then, we\nformulate the NS-Watermark as a constrained optimization problem and propose an\nefficient algorithm to solve it. Through the experiments, we demonstrate that\nthe NS-Watermark can generate more natural texts than existing watermarking\nmethods and distinguish more accurately between texts written by LLMs and those\nwritten by humans. Especially in machine translation tasks, the NS-Watermark\ncan outperform the existing watermarking method by up to 30 BLEU scores.", "published": "2023-10-02 00:48:51", "link": "http://arxiv.org/abs/2310.00833v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards LogiGLUE: A Brief Survey and A Benchmark for Analyzing Logical\n  Reasoning Capabilities of Language Models", "abstract": "Logical reasoning is fundamental for humans yet presents a substantial\nchallenge in the domain of Artificial Intelligence. Initially, researchers used\nKnowledge Representation and Reasoning (KR) systems that did not scale and\nrequired non-trivial manual effort. Recently, the emergence of large language\nmodels (LLMs) has demonstrated the ability to overcome various limitations of\nformal Knowledge Representation (KR) systems. Consequently, there's a growing\ninterest in using LLMs for logical reasoning via natural language. This work\nstrives to understand the proficiency of LLMs in logical reasoning by offering\na brief review of the latest progress in this area; with a focus on the logical\nreasoning datasets, tasks, and the methods adopted to utilize LLMs for\nreasoning. To offer a thorough analysis, we have compiled a benchmark titled\nLogiGLUE. This includes 24 varied datasets encompassing deductive, abductive,\nand inductive reasoning. Utilizing LogiGLUE as a foundation, we have trained an\ninstruction fine-tuned language model, resulting in LogiT5. We study\nsingle-task training, multi-task training, and \"chain-of-thought\" knowledge\ndistillation fine-tuning technique to assess the performance of model across\nthe different logical reasoning categories. We also assess various LLMs using\nLogiGLUE, and the findings indicate that LLMs excel most in abductive\nreasoning, followed by deductive reasoning, while they are least effective at\ninductive reasoning. We aim to shed light on the capabilities and potential\npathways for enhancing logical reasoning proficiency in LLMs, paving the way\nfor more advanced and nuanced developments in this critical field.", "published": "2023-10-02 01:00:50", "link": "http://arxiv.org/abs/2310.00836v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Application of frozen large-scale models to multimodal task-oriented\n  dialogue", "abstract": "In this study, we use the existing Large Language Models ENnhanced to See\nFramework (LENS Framework) to test the feasibility of multimodal task-oriented\ndialogues. The LENS Framework has been proposed as a method to solve computer\nvision tasks without additional training and with fixed parameters of\npre-trained models. We used the Multimodal Dialogs (MMD) dataset, a multimodal\ntask-oriented dialogue benchmark dataset from the fashion field, and for the\nevaluation, we used the ChatGPT-based G-EVAL, which only accepts textual\nmodalities, with arrangements to handle multimodal data. Compared to\nTransformer-based models in previous studies, our method demonstrated an\nabsolute lift of 10.8% in fluency, 8.8% in usefulness, and 5.2% in relevance\nand coherence. The results show that using large-scale models with fixed\nparameters rather than using models trained on a dataset from scratch improves\nperformance in multimodal task-oriented dialogues. At the same time, we show\nthat Large Language Models (LLMs) are effective for multimodal task-oriented\ndialogues. This is expected to lead to efficient applications to existing\nsystems.", "published": "2023-10-02 01:42:28", "link": "http://arxiv.org/abs/2310.00845v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Syllable-level lyrics generation from melody exploiting character-level\n  language model", "abstract": "The generation of lyrics tightly connected to accompanying melodies involves\nestablishing a mapping between musical notes and syllables of lyrics. This\nprocess requires a deep understanding of music constraints and semantic\npatterns at syllable-level, word-level, and sentence-level semantic meanings.\nHowever, pre-trained language models specifically designed at the syllable\nlevel are publicly unavailable. To solve these challenging issues, we propose\nto exploit fine-tuning character-level language models for syllable-level\nlyrics generation from symbolic melody. In particular, our method endeavors to\nincorporate linguistic knowledge of the language model into the beam search\nprocess of a syllable-level Transformer generator network. Additionally, by\nexploring ChatGPT-based evaluation for generated lyrics, along with human\nsubjective evaluation, we demonstrate that our approach enhances the coherence\nand correctness of the generated lyrics, eliminating the need to train\nexpensive new language models.", "published": "2023-10-02 02:53:29", "link": "http://arxiv.org/abs/2310.00863v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Do Compressed LLMs Forget Knowledge? An Experimental Study with\n  Practical Implications", "abstract": "Compressing Large Language Models (LLMs) often leads to reduced performance,\nespecially for knowledge-intensive tasks. In this work, we dive into how\ncompression damages LLMs' inherent knowledge and the possible remedies. We\nstart by proposing two conjectures on the nature of the damage: one is certain\nknowledge being forgotten (or erased) after LLM compression, hence\nnecessitating the compressed model to (re)learn from data with additional\nparameters; the other presumes that knowledge is internally displaced and hence\none requires merely \"inference re-direction\" with input-side augmentation such\nas prompting, to recover the knowledge-related performance. Extensive\nexperiments are then designed to (in)validate the two conjectures. We observe\nthe promise of prompting in comparison to model tuning; we further unlock\nprompting's potential by introducing a variant called Inference-time Dynamic\nPrompting (IDP), that can effectively increase prompt diversity without\nincurring any inference overhead. Our experiments consistently suggest that\ncompared to the classical re-training alternatives such as LoRA, prompting with\nIDP leads to better or comparable post-compression performance recovery, while\nsaving the extra parameter size by 21x and reducing inference latency by 60%.\nOur experiments hence strongly endorse the conjecture of \"knowledge displaced\"\nover \"knowledge forgotten\", and shed light on a new efficient mechanism to\nrestore compressed LLM performance. We additionally visualize and analyze the\ndifferent attention and activation patterns between prompted and re-trained\nmodels, demonstrating they achieve performance recovery in two different\nregimes.", "published": "2023-10-02 03:12:06", "link": "http://arxiv.org/abs/2310.00867v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "No Offense Taken: Eliciting Offensiveness from Language Models", "abstract": "This work was completed in May 2022.\n  For safe and reliable deployment of language models in the real world,\ntesting needs to be robust. This robustness can be characterized by the\ndifficulty and diversity of the test cases we evaluate these models on.\nLimitations in human-in-the-loop test case generation has prompted an advent of\nautomated test case generation approaches. In particular, we focus on Red\nTeaming Language Models with Language Models by Perez et al.(2022). Our\ncontributions include developing a pipeline for automated test case generation\nvia red teaming that leverages publicly available smaller language models\n(LMs), experimenting with different target LMs and red classifiers, and\ngenerating a corpus of test cases that can help in eliciting offensive\nresponses from widely deployed LMs and identifying their failure modes.", "published": "2023-10-02 04:17:35", "link": "http://arxiv.org/abs/2310.00892v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "All Languages Matter: On the Multilingual Safety of Large Language\n  Models", "abstract": "Safety lies at the core of developing and deploying large language models\n(LLMs). However, previous safety benchmarks only concern the safety in one\nlanguage, e.g. the majority language in the pretraining data such as English.\nIn this work, we build the first multilingual safety benchmark for LLMs,\nXSafety, in response to the global deployment of LLMs in practice. XSafety\ncovers 14 kinds of commonly used safety issues across 10 languages that span\nseveral language families. We utilize XSafety to empirically study the\nmultilingual safety for 4 widely-used LLMs, including both close-API and\nopen-source models. Experimental results show that all LLMs produce\nsignificantly more unsafe responses for non-English queries than English ones,\nindicating the necessity of developing safety alignment for non-English\nlanguages. In addition, we propose several simple and effective prompting\nmethods to improve the multilingual safety of ChatGPT by evoking safety\nknowledge and improving cross-lingual generalization of safety alignment. Our\nprompting method can significantly reduce the ratio of unsafe responses from\n19.1% to 9.7% for non-English queries. We release our data at\nhttps://github.com/Jarviswang94/Multilingual_safety_benchmark.", "published": "2023-10-02 05:23:34", "link": "http://arxiv.org/abs/2310.00905v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Reasoning on Graphs: Faithful and Interpretable Large Language Model\n  Reasoning", "abstract": "Large language models (LLMs) have demonstrated impressive reasoning abilities\nin complex tasks. However, they lack up-to-date knowledge and experience\nhallucinations during reasoning, which can lead to incorrect reasoning\nprocesses and diminish their performance and trustworthiness. Knowledge graphs\n(KGs), which capture vast amounts of facts in a structured format, offer a\nreliable source of knowledge for reasoning. Nevertheless, existing KG-based LLM\nreasoning methods only treat KGs as factual knowledge bases and overlook the\nimportance of their structural information for reasoning. In this paper, we\npropose a novel method called reasoning on graphs (RoG) that synergizes LLMs\nwith KGs to enable faithful and interpretable reasoning. Specifically, we\npresent a planning-retrieval-reasoning framework, where RoG first generates\nrelation paths grounded by KGs as faithful plans. These plans are then used to\nretrieve valid reasoning paths from the KGs for LLMs to conduct faithful\nreasoning. Furthermore, RoG not only distills knowledge from KGs to improve the\nreasoning ability of LLMs through training but also allows seamless integration\nwith any arbitrary LLMs during inference. Extensive experiments on two\nbenchmark KGQA datasets demonstrate that RoG achieves state-of-the-art\nperformance on KG reasoning tasks and generates faithful and interpretable\nreasoning results.", "published": "2023-10-02 10:14:43", "link": "http://arxiv.org/abs/2310.01061v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Back to the Future: Towards Explainable Temporal Reasoning with Large\n  Language Models", "abstract": "Temporal reasoning is a crucial NLP task, providing a nuanced understanding\nof time-sensitive contexts within textual data. Although recent advancements in\nLLMs have demonstrated their potential in temporal reasoning, the predominant\nfocus has been on tasks such as temporal expression and temporal relation\nextraction. These tasks are primarily designed for the extraction of direct and\npast temporal cues and to engage in simple reasoning processes. A significant\ngap remains when considering complex reasoning tasks such as event forecasting,\nwhich requires multi-step temporal reasoning on events and prediction on the\nfuture timestamp. Another notable limitation of existing methods is their\nincapability to provide an illustration of their reasoning process, hindering\nexplainability. In this paper, we introduce the first task of explainable\ntemporal reasoning, to predict an event's occurrence at a future timestamp\nbased on context which requires multiple reasoning over multiple events, and\nsubsequently provide a clear explanation for their prediction. Our task offers\na comprehensive evaluation of both the LLMs' complex temporal reasoning\nability, the future event prediction ability, and explainability-a critical\nattribute for AI applications. To support this task, we present the first\nmulti-source instruction-tuning dataset of explainable temporal reasoning\n(ExpTime) with 26k derived from the temporal knowledge graph datasets and their\ntemporal reasoning paths, using a novel knowledge-graph-instructed-generation\nstrategy. Based on the dataset, we propose the first open-source LLM series\nTimeLlaMA based on the foundation LlaMA2, with the ability of instruction\nfollowing for explainable temporal reasoning. We compare the performance of our\nmethod and a variety of LLMs, where our method achieves the state-of-the-art\nperformance of temporal prediction and explanation.", "published": "2023-10-02 10:35:23", "link": "http://arxiv.org/abs/2310.01074v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GraphText: Graph Reasoning in Text Space", "abstract": "Large Language Models (LLMs) have gained the ability to assimilate human\nknowledge and facilitate natural language interactions with both humans and\nother LLMs. However, despite their impressive achievements, LLMs have not made\nsignificant advancements in the realm of graph machine learning. This\nlimitation arises because graphs encapsulate distinct relational data, making\nit challenging to transform them into natural language that LLMs understand. In\nthis paper, we bridge this gap with a novel framework, GraphText, that\ntranslates graphs into natural language. GraphText derives a graph-syntax tree\nfor each graph that encapsulates both the node attributes and inter-node\nrelationships. Traversal of the tree yields a graph text sequence, which is\nthen processed by an LLM to treat graph tasks as text generation tasks.\nNotably, GraphText offers multiple advantages. It introduces training-free\ngraph reasoning: even without training on graph data, GraphText with ChatGPT\ncan achieve on par with, or even surpassing, the performance of\nsupervised-trained graph neural networks through in-context learning (ICL).\nFurthermore, GraphText paves the way for interactive graph reasoning, allowing\nboth humans and LLMs to communicate with the model seamlessly using natural\nlanguage. These capabilities underscore the vast, yet-to-be-explored potential\nof LLMs in the domain of graph machine learning.", "published": "2023-10-02 11:03:57", "link": "http://arxiv.org/abs/2310.01089v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Synthetic Data Generation in Low-Resource Settings via Fine-Tuning of\n  Large Language Models", "abstract": "The in-context learning ability of large language models (LLMs) enables them\nto generalize to novel downstream tasks with relatively few labeled examples.\nHowever, they require enormous computational resources to be deployed.\nAlternatively, smaller models can solve specific tasks if fine-tuned with\nenough labeled examples. These examples, however, are expensive to obtain. In\npursuit of the best of both worlds, we study synthetic data generation of\nfine-tuning training data via fine-tuned teacher LLMs to improve the downstream\nperformance of much smaller models. In four text classification and two text\ngeneration tasks, we find that both data generation and annotation dramatically\nimprove the respective downstream model's performance, occasionally\nnecessitating only a minor fraction of the original training dataset.", "published": "2023-10-02 11:49:05", "link": "http://arxiv.org/abs/2310.01119v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Automated Evaluation of Classroom Instructional Support with LLMs and\n  BoWs: Connecting Global Predictions to Specific Feedback", "abstract": "With the aim to provide teachers with more specific, frequent, and actionable\nfeedback about their teaching, we explore how Large Language Models (LLMs) can\nbe used to estimate ``Instructional Support'' domain scores of the CLassroom\nAssessment Scoring System (CLASS), a widely used observation protocol. We\ndesign a machine learning architecture that uses either zero-shot prompting of\nMeta's Llama2, and/or a classic Bag of Words (BoW) model, to classify\nindividual utterances of teachers' speech (transcribed automatically using\nOpenAI's Whisper) for the presence of Instructional Support. Then, these\nutterance-level judgments are aggregated over a 15-min observation session to\nestimate a global CLASS score. Experiments on two CLASS-coded datasets of\ntoddler and pre-kindergarten classrooms indicate that (1) automatic CLASS\nInstructional Support estimation accuracy using the proposed method (Pearson\n$R$ up to $0.48$) approaches human inter-rater reliability (up to $R=0.55$);\n(2) LLMs generally yield slightly greater accuracy than BoW for this task,\nthough the best models often combined features extracted from both LLM and BoW;\nand (3) for classifying individual utterances, there is still room for\nimprovement of automated methods compared to human-level judgments. Finally,\n(4) we illustrate how the model's outputs can be visualized at the utterance\nlevel to provide teachers with explainable feedback on which utterances were\nmost positively or negatively correlated with specific CLASS dimensions.", "published": "2023-10-02 12:11:17", "link": "http://arxiv.org/abs/2310.01132v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "appjsonify: An Academic Paper PDF-to-JSON Conversion Toolkit", "abstract": "We present appjsonify, a Python-based PDF-to-JSON conversion toolkit for\nacademic papers. It parses a PDF file using several visual-based document\nlayout analysis models and rule-based text processing approaches. appjsonify is\na flexible tool that allows users to easily configure the processing pipeline\nto handle a specific format of a paper they wish to process. We are publicly\nreleasing appjsonify as an easy-to-install toolkit available via PyPI and\nGitHub.", "published": "2023-10-02 13:48:16", "link": "http://arxiv.org/abs/2310.01206v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SPELL: Semantic Prompt Evolution based on a LLM", "abstract": "Prompt engineering is a new paradigm for enhancing the performance of trained\nneural network models. For optimizing text-style prompts, existing methods\nusually individually operate small portions of a text step by step, which\neither breaks the fluency or could not globally adjust a prompt. Since large\nlanguage models (LLMs) have powerful ability of generating coherent texts token\nby token, can we utilize LLMs for improving prompts? Based on this motivation,\nin this paper, considering a trained LLM as a text generator, we attempt to\ndesign a black-box evolution algorithm for automatically optimizing texts,\nnamely SPELL (Semantic Prompt Evolution based on a LLM). The proposed method is\nevaluated with different LLMs and evolution parameters in different text tasks.\nExperimental results show that SPELL could rapidly improve the prompts indeed.\nWe further explore the evolution process and discuss on the limitations,\npotential possibilities and future work.", "published": "2023-10-02 14:51:16", "link": "http://arxiv.org/abs/2310.01260v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LEEC: A Legal Element Extraction Dataset with an Extensive\n  Domain-Specific Label System", "abstract": "As a pivotal task in natural language processing, element extraction has\ngained significance in the legal domain. Extracting legal elements from\njudicial documents helps enhance interpretative and analytical capacities of\nlegal cases, and thereby facilitating a wide array of downstream applications\nin various domains of law. Yet existing element extraction datasets are limited\nby their restricted access to legal knowledge and insufficient coverage of\nlabels. To address this shortfall, we introduce a more comprehensive,\nlarge-scale criminal element extraction dataset, comprising 15,831 judicial\ndocuments and 159 labels. This dataset was constructed through two main steps:\nfirst, designing the label system by our team of legal experts based on prior\nlegal research which identified critical factors driving and processes\ngenerating sentencing outcomes in criminal cases; second, employing the legal\nknowledge to annotate judicial documents according to the label system and\nannotation guideline. The Legal Element ExtraCtion dataset (LEEC) represents\nthe most extensive and domain-specific legal element extraction dataset for the\nChinese legal system. Leveraging the annotated data, we employed various SOTA\nmodels that validates the applicability of LEEC for Document Event Extraction\n(DEE) task. The LEEC dataset is available on https://github.com/THUlawtech/LEEC .", "published": "2023-10-02 15:16:31", "link": "http://arxiv.org/abs/2310.01271v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Knowledge Crosswords: Geometric Knowledge Reasoning with Large Language\n  Models", "abstract": "We propose Knowledge Crosswords, a geometric knowledge reasoning benchmark\nconsisting of incomplete knowledge networks bounded by structured factual\nconstraints, where LLMs are tasked with inferring the missing facts to meet all\nconstraints. The novel setting of geometric knowledge reasoning necessitates\nnew LM abilities beyond existing atomic/linear multi-hop QA, such as\nbacktracking, verifying facts and constraints, reasoning with uncertainty, and\nmore. Knowledge Crosswords contains 2,101 individual problems, covering diverse\nknowledge domains, and is further divided into three difficulty levels. We\nconduct extensive experiments to evaluate existing LLMs and approaches on\nKnowledge Crosswords. Results demonstrate that baseline approaches struggle\nwith larger knowledge networks and semantically-equivalent entity distractors.\nIn light of their limitations, we propose two new approaches, Staged Prompting\nand Verify-All, to augment LLMs' abilities for error-aware backtracking and\nconstraint verification. Our Verify-All significantly outperforms prior methods\nand is more robust towards problems in the hard subset. Further analysis shows\nthat geometric knowledge reasoning poses new challenges to LLMs' knowledge\nabilities, particularly in robustness towards varying option orders, complex\nstructural constraints in knowledge networks, \"none of the above\" scenarios,\nand more.", "published": "2023-10-02 15:43:53", "link": "http://arxiv.org/abs/2310.01290v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Generating Explanations in Medical Question-Answering by Expectation\n  Maximization Inference over Evidence", "abstract": "Medical Question Answering~(medical QA) systems play an essential role in\nassisting healthcare workers in finding answers to their questions. However, it\nis not sufficient to merely provide answers by medical QA systems because users\nmight want explanations, that is, more analytic statements in natural language\nthat describe the elements and context that support the answer. To do so, we\npropose a novel approach for generating natural language explanations for\nanswers predicted by medical QA systems. As high-quality medical explanations\nrequire additional medical knowledge, so that our system extract knowledge from\nmedical textbooks to enhance the quality of explanations during the explanation\ngeneration process. Concretely, we designed an expectation-maximization\napproach that makes inferences about the evidence found in these texts,\noffering an efficient way to focus attention on lengthy evidence passages.\nExperimental results, conducted on two datasets MQAE-diag and MQAE, demonstrate\nthe effectiveness of our framework for reasoning with textual evidence. Our\napproach outperforms state-of-the-art models, achieving a significant\nimprovement of \\textbf{6.86} and \\textbf{9.43} percentage points on the Rouge-1\nscore; \\textbf{8.23} and \\textbf{7.82} percentage points on the Bleu-4 score on\nthe respective datasets.", "published": "2023-10-02 16:00:37", "link": "http://arxiv.org/abs/2310.01299v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "BTR: Binary Token Representations for Efficient Retrieval Augmented\n  Language Models", "abstract": "Retrieval augmentation addresses many critical problems in large language\nmodels such as hallucination, staleness, and privacy leaks. However, running\nretrieval-augmented language models (LMs) is slow and difficult to scale due to\nprocessing large amounts of retrieved text. We introduce binary token\nrepresentations (BTR), which use 1-bit vectors to precompute every token in\npassages, significantly reducing computation during inference. Despite the\npotential loss of accuracy, our new calibration techniques and training\nobjectives restore performance. Combined with offline and runtime compression,\nthis only requires 127GB of disk space for encoding 3 billion tokens in\nWikipedia. Our experiments show that on five knowledge-intensive NLP tasks, BTR\naccelerates state-of-the-art inference by up to 4x and reduces storage by over\n100x while maintaining over 95% task performance.", "published": "2023-10-02 16:48:47", "link": "http://arxiv.org/abs/2310.01329v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Dialogue Management: Quality Datasets vs Models", "abstract": "Task-oriented dialogue systems (TODS) have become crucial for users to\ninteract with machines and computers using natural language. One of its key\ncomponents is the dialogue manager, which guides the conversation towards a\ngood goal for the user by providing the best possible response. Previous works\nhave proposed rule-based systems (RBS), reinforcement learning (RL), and\nsupervised learning (SL) as solutions for the correct dialogue management; in\nother words, select the best response given input by the user. However, this\nwork argues that the leading cause of DMs not achieving maximum performance\nresides in the quality of the datasets rather than the models employed thus\nfar; this means that dataset errors, like mislabeling, originate a large\npercentage of failures in dialogue management. We studied the main errors in\nthe most widely used datasets, Multiwoz 2.1 and SGD, to demonstrate this\nhypothesis. To do this, we have designed a synthetic dialogue generator to\nfully control the amount and type of errors introduced in the dataset. Using\nthis generator, we demonstrated that errors in the datasets contribute\nproportionally to the performance of the models", "published": "2023-10-02 17:02:57", "link": "http://arxiv.org/abs/2310.01339v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RA-DIT: Retrieval-Augmented Dual Instruction Tuning", "abstract": "Retrieval-augmented language models (RALMs) improve performance by accessing\nlong-tail and up-to-date knowledge from external data stores, but are\nchallenging to build. Existing approaches require either expensive\nretrieval-specific modifications to LM pre-training or use post-hoc integration\nof the data store that leads to suboptimal performance. We introduce\nRetrieval-Augmented Dual Instruction Tuning (RA-DIT), a lightweight fine-tuning\nmethodology that provides a third option by retrofitting any LLM with retrieval\ncapabilities. Our approach operates in two distinct fine-tuning steps: (1) one\nupdates a pre-trained LM to better use retrieved information, while (2) the\nother updates the retriever to return more relevant results, as preferred by\nthe LM. By fine-tuning over tasks that require both knowledge utilization and\ncontextual awareness, we demonstrate that each stage yields significant\nperformance improvements, and using both leads to additional gains. Our best\nmodel, RA-DIT 65B, achieves state-of-the-art performance across a range of\nknowledge-intensive zero- and few-shot learning benchmarks, significantly\noutperforming existing in-context RALM approaches by up to +8.9% in 0-shot\nsetting and +1.4% in 5-shot setting on average.", "published": "2023-10-02 17:16:26", "link": "http://arxiv.org/abs/2310.01352v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Compressing LLMs: The Truth is Rarely Pure and Never Simple", "abstract": "Despite their remarkable achievements, modern Large Language Models (LLMs)\nface exorbitant computational and memory footprints. Recently, several works\nhave shown significant success in training-free and data-free compression\n(pruning and quantization) of LLMs that achieve 50 - 60% sparsity and reduce\nthe bit width to 3 or 4 bits per weight, with negligible degradation of\nperplexity over the uncompressed baseline. As recent research efforts are\nfocused on developing increasingly sophisticated compression methods, our work\ntakes a step back and re-evaluates the effectiveness of existing SoTA\ncompression methods, which rely on a fairly simple and widely questioned\nmetric, perplexity (even for dense LLMs). We introduce Knowledge-Intensive\nCompressed LLM BenchmarK (LLM-KICK), a collection of carefully curated tasks to\nredefine the evaluation protocol for compressed LLMs, which have significant\nalignment with their dense counterparts and perplexity fail to capture subtle\nchange in their true capabilities. LLM-KICK unveils many favorable merits and\nunfortunate plights of current SoTA compression methods: all pruning methods\nsuffer significant performance degradation, sometimes at trivial sparsity\nratios (e.g., 25-30%), and fail for N:M sparsity in knowledge-intensive tasks;\ncurrent quantization methods are more successful than pruning; yet, pruned LLMs\neven at $\\geq 50$% sparsity are robust in-context retrieval and summarization\nsystems; among others. LLM-KICK is designed to holistically access compressed\nLLMs' ability for language understanding, reasoning, generation, in-context\nretrieval, in-context summarization, etc. We hope our study can foster the\ndevelopment of better LLM compression methods. The reproduced codes are\navailable at https://github.com/VITA-Group/llm-kick.", "published": "2023-10-02 17:42:37", "link": "http://arxiv.org/abs/2310.01382v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Fooling the Textual Fooler via Randomizing Latent Representations", "abstract": "Despite outstanding performance in a variety of NLP tasks, recent studies\nhave revealed that NLP models are vulnerable to adversarial attacks that\nslightly perturb the input to cause the models to misbehave. Among these\nattacks, adversarial word-level perturbations are well-studied and effective\nattack strategies. Since these attacks work in black-box settings, they do not\nrequire access to the model architecture or model parameters and thus can be\ndetrimental to existing NLP applications. To perform an attack, the adversary\nqueries the victim model many times to determine the most important words in an\ninput text and to replace these words with their corresponding synonyms. In\nthis work, we propose a lightweight and attack-agnostic defense whose main goal\nis to perplex the process of generating an adversarial example in these\nquery-based black-box attacks; that is to fool the textual fooler. This\ndefense, named AdvFooler, works by randomizing the latent representation of the\ninput at inference time. Different from existing defenses, AdvFooler does not\nnecessitate additional computational overhead during training nor relies on\nassumptions about the potential adversarial perturbation set while having a\nnegligible impact on the model's accuracy. Our theoretical and empirical\nanalyses highlight the significance of robustness resulting from confusing the\nadversary via randomizing the latent space, as well as the impact of\nrandomization on clean accuracy. Finally, we empirically demonstrate near\nstate-of-the-art robustness of AdvFooler against representative adversarial\nword-level attacks on two benchmark datasets.", "published": "2023-10-02 06:57:25", "link": "http://arxiv.org/abs/2310.01452v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FedBPT: Efficient Federated Black-box Prompt Tuning for Large Language\n  Models", "abstract": "Pre-trained language models (PLM) have revolutionized the NLP landscape,\nachieving stellar performances across diverse tasks. These models, while\nbenefiting from vast training data, often require fine-tuning on specific data\nto cater to distinct downstream tasks. However, this data adaptation process\nhas inherent security and privacy concerns, primarily when leveraging\nuser-generated, device-residing data. Federated learning (FL) provides a\nsolution, allowing collaborative model fine-tuning without centralized data\ncollection. However, applying FL to finetune PLMs is hampered by challenges,\nincluding restricted model parameter access, high computational requirements,\nand communication overheads. This paper introduces Federated Black-box Prompt\nTuning (FedBPT), a framework designed to address these challenges. FedBPT does\nnot require the clients to access the model parameters. By focusing on training\noptimal prompts and utilizing gradient-free optimization methods, FedBPT\nreduces the number of exchanged variables, boosts communication efficiency, and\nminimizes computational and storage costs. Experiments highlight the\nframework's ability to drastically cut communication and memory costs while\nmaintaining competitive performance. Ultimately, FedBPT presents a promising\nsolution for efficient, privacy-preserving fine-tuning of PLM in the age of\nlarge language models.", "published": "2023-10-02 16:43:14", "link": "http://arxiv.org/abs/2310.01467v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLM Lies: Hallucinations are not Bugs, but Features as Adversarial\n  Examples", "abstract": "Large Language Models (LLMs), including GPT-3.5, LLaMA, and PaLM, seem to be\nknowledgeable and able to adapt to many tasks. However, we still cannot\ncompletely trust their answers, since LLMs suffer from\n\\textbf{hallucination}\\textemdash fabricating non-existent facts, deceiving\nusers with or without their awareness. However, the reasons for their existence\nand pervasiveness remain unclear. In this paper, we demonstrate that\nnonsensical prompts composed of random tokens can also elicit the LLMs to\nrespond with hallucinations. Moreover, we provide both theoretical and\nexperimental evidence that transformers can be manipulated to produce specific\npre-define tokens by perturbing its input sequence. This phenomenon forces us\nto revisit that \\emph{hallucination may be another view of adversarial\nexamples}, and it shares similar characteristics with conventional adversarial\nexamples as a basic property of LLMs. Therefore, we formalize an automatic\nhallucination triggering method as the \\textit{hallucination attack} in an\nadversarial way. Finally, we explore the basic properties of attacked\nadversarial prompts and propose a simple yet effective defense strategy. Our\ncode is released on\nGitHub\\footnote{https://github.com/PKU-YuanGroup/Hallucination-Attack}.", "published": "2023-10-02 17:01:56", "link": "http://arxiv.org/abs/2310.01469v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Making Retrieval-Augmented Language Models Robust to Irrelevant Context", "abstract": "Retrieval-augmented language models (RALMs) hold promise to produce language\nunderstanding systems that are are factual, efficient, and up-to-date. An\nimportant desideratum of RALMs, is that retrieved information helps model\nperformance when it is relevant, and does not harm performance when it is not.\nThis is particularly important in multi-hop reasoning scenarios, where misuse\nof irrelevant evidence can lead to cascading errors. However, recent work has\nshown that retrieval augmentation can sometimes have a negative effect on\nperformance. In this work, we present a thorough analysis on five open-domain\nquestion answering benchmarks, characterizing cases when retrieval reduces\naccuracy. We then propose two methods to mitigate this issue. First, a simple\nbaseline that filters out retrieved passages that do not entail question-answer\npairs according to a natural language inference (NLI) model. This is effective\nin preventing performance reduction, but at a cost of also discarding relevant\npassages. Thus, we propose a method for automatically generating data to\nfine-tune the language model to properly leverage retrieved passages, using a\nmix of relevant and irrelevant contexts at training time. We empirically show\nthat even 1,000 examples suffice to train the model to be robust to irrelevant\ncontexts while maintaining high performance on examples with relevant ones.", "published": "2023-10-02 18:52:35", "link": "http://arxiv.org/abs/2310.01558v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Defending Against Authorship Identification Attacks", "abstract": "Authorship identification has proven unsettlingly effective in inferring the\nidentity of the author of an unsigned document, even when sensitive personal\ninformation has been carefully omitted. In the digital era, individuals leave a\nlasting digital footprint through their written content, whether it is posted\non social media, stored on their employer's computers, or located elsewhere.\nWhen individuals need to communicate publicly yet wish to remain anonymous,\nthere is little available to protect them from unwanted authorship\nidentification. This unprecedented threat to privacy is evident in scenarios\nsuch as whistle-blowing. Proposed defenses against authorship identification\nattacks primarily aim to obfuscate one's writing style, thereby making it\nunlinkable to their pre-existing writing, while concurrently preserving the\noriginal meaning and grammatical integrity. The presented work offers a\ncomprehensive review of the advancements in this research area spanning over\nthe past two decades and beyond. It emphasizes the methodological frameworks of\nmodification and generation-based strategies devised to evade authorship\nidentification attacks, highlighting joint efforts from the differential\nprivacy community. Limitations of current research are discussed, with a\nspotlight on open challenges and potential research avenues.", "published": "2023-10-02 19:03:11", "link": "http://arxiv.org/abs/2310.01568v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Continuous Prompt Transfer: Generalizing Task Semantics Across\n  Language Models", "abstract": "Prompt tuning in natural language processing (NLP) has become an increasingly\npopular method for adapting large language models to specific tasks. However,\nthe transferability of these prompts, especially continuous prompts, between\ndifferent models remains a challenge. In this work, we propose a zero-shot\ncontinuous prompt transfer method, where source prompts are encoded into\nrelative space and the corresponding target prompts are searched for\ntransferring to target models. Experimental results confirm the effectiveness\nof our method, showing that 'task semantics' in continuous prompts can be\ngeneralized across various language models. Moreover, we find that combining\n'task semantics' from multiple source models can further enhance the\ngeneralizability of transfer.", "published": "2023-10-02 23:12:21", "link": "http://arxiv.org/abs/2310.01691v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Human Mobility Question Answering (Vision Paper)", "abstract": "Question answering (QA) systems have attracted much attention from the\nartificial intelligence community as they can learn to answer questions based\non the given knowledge source (e.g., images in visual question answering).\nHowever, the research into question answering systems with human mobility data\nremains unexplored. Mining human mobility data is crucial for various\napplications such as smart city planning, pandemic management, and personalised\nrecommendation system. In this paper, we aim to tackle this gap and introduce a\nnovel task, that is, human mobility question answering (MobQA). The aim of the\ntask is to let the intelligent system learn from mobility data and answer\nrelated questions. This task presents a new paradigm change in mobility\nprediction research and further facilitates the research of human mobility\nrecommendation systems. To better support this novel research topic, this\nvision paper also proposes an initial design of the dataset and a potential\ndeep learning model framework for the introduced MobQA task. We hope that this\npaper will provide novel insights and open new directions in human mobility\nresearch and question answering research.", "published": "2023-10-02 21:24:26", "link": "http://arxiv.org/abs/2310.04443v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "uSee: Unified Speech Enhancement and Editing with Conditional Diffusion\n  Models", "abstract": "Speech enhancement aims to improve the quality of speech signals in terms of\nquality and intelligibility, and speech editing refers to the process of\nediting the speech according to specific user needs. In this paper, we propose\na Unified Speech Enhancement and Editing (uSee) model with conditional\ndiffusion models to handle various tasks at the same time in a generative\nmanner. Specifically, by providing multiple types of conditions including\nself-supervised learning embeddings and proper text prompts to the score-based\ndiffusion model, we can enable controllable generation of the unified speech\nenhancement and editing model to perform corresponding actions on the source\nspeech. Our experiments show that our proposed uSee model can achieve superior\nperformance in both speech denoising and dereverberation compared to other\nrelated generative speech enhancement models, and can perform speech editing\ngiven desired environmental sound text description, signal-to-noise ratios\n(SNR), and room impulse responses (RIR). Demos of the generated speech are\navailable at https://muqiaoy.github.io/usee.", "published": "2023-10-02 04:36:39", "link": "http://arxiv.org/abs/2310.00900v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Towards human-like spoken dialogue generation between AI agents from\n  written dialogue", "abstract": "The advent of large language models (LLMs) has made it possible to generate\nnatural written dialogues between two agents. However, generating human-like\nspoken dialogues from these written dialogues remains challenging. Spoken\ndialogues have several unique characteristics: they frequently include\nbackchannels and laughter, and the smoothness of turn-taking significantly\ninfluences the fluidity of conversation. This study proposes CHATS - CHatty\nAgents Text-to-Speech - a discrete token-based system designed to generate\nspoken dialogues based on written dialogues. Our system can generate speech for\nboth the speaker side and the listener side simultaneously, using only the\ntranscription from the speaker side, which eliminates the need for\ntranscriptions of backchannels or laughter. Moreover, CHATS facilitates natural\nturn-taking; it determines the appropriate duration of silence after each\nutterance in the absence of overlap, and it initiates the generation of\noverlapping speech based on the phoneme sequence of the next utterance in case\nof overlap. Experimental evaluations indicate that CHATS outperforms the\ntext-to-speech baseline, producing spoken dialogues that are more interactive\nand fluid while retaining clarity and intelligibility.", "published": "2023-10-02 11:03:20", "link": "http://arxiv.org/abs/2310.01088v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Quantifying the Plausibility of Context Reliance in Neural Machine\n  Translation", "abstract": "Establishing whether language models can use contextual information in a\nhuman-plausible way is important to ensure their trustworthiness in real-world\nsettings. However, the questions of when and which parts of the context affect\nmodel generations are typically tackled separately, with current plausibility\nevaluations being practically limited to a handful of artificial benchmarks. To\naddress this, we introduce Plausibility Evaluation of Context Reliance\n(PECoRe), an end-to-end interpretability framework designed to quantify context\nusage in language models' generations. Our approach leverages model internals\nto (i) contrastively identify context-sensitive target tokens in generated\ntexts and (ii) link them to contextual cues justifying their prediction. We use\n\\pecore to quantify the plausibility of context-aware machine translation\nmodels, comparing model rationales with human annotations across several\ndiscourse-level phenomena. Finally, we apply our method to unannotated model\ntranslations to identify context-mediated predictions and highlight instances\nof (im)plausible context usage throughout generation.", "published": "2023-10-02 13:26:43", "link": "http://arxiv.org/abs/2310.01188v2", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "ScaLearn: Simple and Highly Parameter-Efficient Task Transfer by\n  Learning to Scale", "abstract": "Multi-task learning (MTL) has shown considerable practical benefits,\nparticularly when using language models (LMs). While this is commonly achieved\nby learning $n$ tasks under a joint optimization procedure, some methods, such\nas AdapterFusion, divide the problem into two stages: (i) task learning, where\nknowledge specific to a task is encapsulated within sets of parameters (e.g.,\nadapters), and (ii) transfer, where this already learned knowledge is leveraged\nfor a target task. This separation of concerns provides numerous benefits\n(e.g., promoting reusability). However, current two-stage MTL introduces a\nsubstantial number of additional parameters. We address this issue by\nleveraging the usefulness of linearly scaling the output representations of\nsource adapters for transfer learning. We introduce ScaLearn, a simple and\nhighly parameter-efficient two-stage MTL method that capitalizes on the\nknowledge of the source tasks by learning a minimal set of scaling parameters\nthat enable effective transfer to a target task. Our experiments on three\nbenchmarks (GLUE, SuperGLUE, and HumSet) and two encoder LMs show that ScaLearn\nconsistently outperforms strong baselines with a small number of transfer\nparameters (~ $0.35$% of those of AdapterFusion). Remarkably, we observe that\nScaLearn maintains its strong abilities even when further reducing parameters,\nachieving competitive results with only $8$ transfer parameters per target\ntask. Our proposed approach thus demonstrates the power of simple scaling as a\npromise for more efficient task transfer.", "published": "2023-10-02 14:01:36", "link": "http://arxiv.org/abs/2310.01217v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Co-audit: tools to help humans double-check AI-generated content", "abstract": "Users are increasingly being warned to check AI-generated content for\ncorrectness. Still, as LLMs (and other generative models) generate more complex\noutput, such as summaries, tables, or code, it becomes harder for the user to\naudit or evaluate the output for quality or correctness. Hence, we are seeing\nthe emergence of tool-assisted experiences to help the user double-check a\npiece of AI-generated content. We refer to these as co-audit tools. Co-audit\ntools complement prompt engineering techniques: one helps the user construct\nthe input prompt, while the other helps them check the output response. As a\nspecific example, this paper describes recent research on co-audit tools for\nspreadsheet computations powered by generative models. We explain why co-audit\nexperiences are essential for any application of generative AI where quality is\nimportant and errors are consequential (as is common in spreadsheet\ncomputations). We propose a preliminary list of principles for co-audit, and\noutline research challenges.", "published": "2023-10-02 15:59:10", "link": "http://arxiv.org/abs/2310.01297v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.PL"], "primary_category": "cs.HC"}
{"title": "On the Generalization of Training-based ChatGPT Detection Methods", "abstract": "ChatGPT is one of the most popular language models which achieve amazing\nperformance on various natural language tasks. Consequently, there is also an\nurgent need to detect the texts generated ChatGPT from human written. One of\nthe extensively studied methods trains classification models to distinguish\nboth. However, existing studies also demonstrate that the trained models may\nsuffer from distribution shifts (during test), i.e., they are ineffective to\npredict the generated texts from unseen language tasks or topics. In this work,\nwe aim to have a comprehensive investigation on these methods' generalization\nbehaviors under distribution shift caused by a wide range of factors, including\nprompts, text lengths, topics, and language tasks. To achieve this goal, we\nfirst collect a new dataset with human and ChatGPT texts, and then we conduct\nextensive studies on the collected dataset. Our studies unveil insightful\nfindings which provide guidance for developing future methodologies or data\ncollection strategies for ChatGPT detection.", "published": "2023-10-02 16:13:08", "link": "http://arxiv.org/abs/2310.01307v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Merge, Then Compress: Demystify Efficient SMoE with Hints from Its\n  Routing Policy", "abstract": "Sparsely activated Mixture-of-Experts (SMoE) has shown promise to scale up\nthe learning capacity of neural networks, however, they have issues like (a)\nHigh Memory Usage, due to duplication of the network layers into multiple\ncopies as experts; and (b) Redundancy in Experts, as common learning-based\nrouting policies suffer from representational collapse. Therefore, vanilla SMoE\nmodels are memory inefficient and non-scalable, especially for\nresource-constrained downstream scenarios. In this paper, we ask: Can we craft\na compact SMoE model by consolidating expert information? What is the best\nrecipe to merge multiple experts into fewer but more knowledgeable experts? Our\npilot investigation reveals that conventional model merging methods fail to be\neffective in such expert merging for SMoE. The potential reasons are: (1)\nredundant information overshadows critical experts; (2) appropriate neuron\npermutation for each expert is missing to bring all of them in alignment. To\naddress this, we propose M-SMoE, which leverages routing statistics to guide\nexpert merging. Specifically, it starts with neuron permutation alignment for\nexperts; then, dominant experts and their \"group members\" are formed; lastly,\nevery expert group is merged into a single expert by utilizing each expert's\nactivation frequency as their weight for merging, thus diminishing the impact\nof insignificant experts. Moreover, we observed that our proposed merging\npromotes a low dimensionality in the merged expert's weight space, naturally\npaving the way for additional compression. Hence, our final method, MC-SMoE\n(i.e., Merge, then Compress SMoE), further decomposes the merged experts into\nlow-rank and structural sparse alternatives. Extensive experiments across 8\nbenchmarks validate the effectiveness of MC-SMoE. For instance, our MC-SMoE\nachieves up to 80% memory and a 20% FLOPs reduction, with virtually no loss in\nperformance.", "published": "2023-10-02 16:51:32", "link": "http://arxiv.org/abs/2310.01334v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "GenSim: Generating Robotic Simulation Tasks via Large Language Models", "abstract": "Collecting large amounts of real-world interaction data to train general\nrobotic policies is often prohibitively expensive, thus motivating the use of\nsimulation data. However, existing methods for data generation have generally\nfocused on scene-level diversity (e.g., object instances and poses) rather than\ntask-level diversity, due to the human effort required to come up with and\nverify novel tasks. This has made it challenging for policies trained on\nsimulation data to demonstrate significant task-level generalization. In this\npaper, we propose to automatically generate rich simulation environments and\nexpert demonstrations by exploiting a large language models' (LLM) grounding\nand coding ability. Our approach, dubbed GenSim, has two modes: goal-directed\ngeneration, wherein a target task is given to the LLM and the LLM proposes a\ntask curriculum to solve the target task, and exploratory generation, wherein\nthe LLM bootstraps from previous tasks and iteratively proposes novel tasks\nthat would be helpful in solving more complex tasks. We use GPT4 to expand the\nexisting benchmark by ten times to over 100 tasks, on which we conduct\nsupervised finetuning and evaluate several LLMs including finetuned GPTs and\nCode Llama on code generation for robotic simulation tasks. Furthermore, we\nobserve that LLMs-generated simulation programs can enhance task-level\ngeneralization significantly when used for multitask policy training. We\nfurther find that with minimal sim-to-real adaptation, the multitask policies\npretrained on GPT4-generated simulation tasks exhibit stronger transfer to\nunseen long-horizon tasks in the real world and outperform baselines by 25%.\nSee the project website (https://liruiw.github.io/gensim) for code, demos, and\nvideos.", "published": "2023-10-02 17:23:48", "link": "http://arxiv.org/abs/2310.01361v2", "categories": ["cs.LG", "cs.CL", "cs.CV", "cs.RO"], "primary_category": "cs.LG"}
{"title": "UltraFeedback: Boosting Language Models with Scaled AI Feedback", "abstract": "Learning from human feedback has become a pivot technique in aligning large\nlanguage models (LLMs) with human preferences. However, acquiring vast and\npremium human feedback is bottlenecked by time, labor, and human capability,\nresulting in small sizes or limited topics of current datasets. This further\nhinders feedback learning as well as alignment research within the open-source\ncommunity. To address this issue, we explore how to go beyond human feedback\nand collect high-quality \\textit{AI feedback} automatically for a scalable\nalternative. Specifically, we identify \\textbf{scale and diversity} as the key\nfactors for feedback data to take effect. Accordingly, we first broaden\ninstructions and responses in both amount and breadth to encompass a wider\nrange of user-assistant interactions. Then, we meticulously apply a series of\ntechniques to mitigate annotation biases for more reliable AI feedback. We\nfinally present \\textsc{UltraFeedback}, a large-scale, high-quality, and\ndiversified AI feedback dataset, which contains over 1 million GPT-4 feedback\nfor 250k user-assistant conversations from various aspects. Built upon\n\\textsc{UltraFeedback}, we align a LLaMA-based model by best-of-$n$ sampling\nand reinforcement learning, demonstrating its exceptional performance on chat\nbenchmarks. Our work validates the effectiveness of scaled AI feedback data in\nconstructing strong open-source chat language models, serving as a solid\nfoundation for future feedback learning research. Our data and models are\navailable at https://github.com/thunlp/UltraFeedback.", "published": "2023-10-02 17:40:01", "link": "http://arxiv.org/abs/2310.01377v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DiffAR: Denoising Diffusion Autoregressive Model for Raw Speech Waveform\n  Generation", "abstract": "Diffusion models have recently been shown to be relevant for high-quality\nspeech generation. Most work has been focused on generating spectrograms, and\nas such, they further require a subsequent model to convert the spectrogram to\na waveform (i.e., a vocoder). This work proposes a diffusion probabilistic\nend-to-end model for generating a raw speech waveform. The proposed model is\nautoregressive, generating overlapping frames sequentially, where each frame is\nconditioned on a portion of the previously generated one. Hence, our model can\neffectively synthesize an unlimited speech duration while preserving\nhigh-fidelity synthesis and temporal coherence. We implemented the proposed\nmodel for unconditional and conditional speech generation, where the latter can\nbe driven by an input sequence of phonemes, amplitudes, and pitch values.\nWorking on the waveform directly has some empirical advantages. Specifically,\nit allows the creation of local acoustic behaviors, like vocal fry, which makes\nthe overall waveform sounds more natural. Furthermore, the proposed diffusion\nmodel is stochastic and not deterministic; therefore, each inference generates\na slightly different waveform variation, enabling abundance of valid\nrealizations. Experiments show that the proposed model generates speech with\nsuperior quality compared with other state-of-the-art neural speech generation\nsystems.", "published": "2023-10-02 17:42:22", "link": "http://arxiv.org/abs/2310.01381v3", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "GPT-Driver: Learning to Drive with GPT", "abstract": "We present a simple yet effective approach that can transform the OpenAI\nGPT-3.5 model into a reliable motion planner for autonomous vehicles. Motion\nplanning is a core challenge in autonomous driving, aiming to plan a driving\ntrajectory that is safe and comfortable. Existing motion planners predominantly\nleverage heuristic methods to forecast driving trajectories, yet these\napproaches demonstrate insufficient generalization capabilities in the face of\nnovel and unseen driving scenarios. In this paper, we propose a novel approach\nto motion planning that capitalizes on the strong reasoning capabilities and\ngeneralization potential inherent to Large Language Models (LLMs). The\nfundamental insight of our approach is the reformulation of motion planning as\na language modeling problem, a perspective not previously explored.\nSpecifically, we represent the planner inputs and outputs as language tokens,\nand leverage the LLM to generate driving trajectories through a language\ndescription of coordinate positions. Furthermore, we propose a novel\nprompting-reasoning-finetuning strategy to stimulate the numerical reasoning\npotential of the LLM. With this strategy, the LLM can describe highly precise\ntrajectory coordinates and also its internal decision-making process in natural\nlanguage. We evaluate our approach on the large-scale nuScenes dataset, and\nextensive experiments substantiate the effectiveness, generalization ability,\nand interpretability of our GPT-based motion planner. Code is now available at\nhttps://github.com/PointsCoder/GPT-Driver.", "published": "2023-10-02 17:59:57", "link": "http://arxiv.org/abs/2310.01415v3", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "primary_category": "cs.CV"}
{"title": "NarrativePlay: Interactive Narrative Understanding", "abstract": "In this paper, we introduce NarrativePlay, a novel system that allows users\nto role-play a fictional character and interact with other characters in\nnarratives such as novels in an immersive environment. We leverage Large\nLanguage Models (LLMs) to generate human-like responses, guided by personality\ntraits extracted from narratives. The system incorporates auto-generated visual\ndisplay of narrative settings, character portraits, and character speech,\ngreatly enhancing user experience. Our approach eschews predefined sandboxes,\nfocusing instead on main storyline events extracted from narratives from the\nperspective of a user-selected character. NarrativePlay has been evaluated on\ntwo types of narratives, detective and adventure stories, where users can\neither explore the world or improve their favorability with the narrative\ncharacters through conversations.", "published": "2023-10-02 13:24:00", "link": "http://arxiv.org/abs/2310.01459v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Probing the Multi-turn Planning Capabilities of LLMs via 20 Question\n  Games", "abstract": "Large language models (LLMs) are effective at answering questions that are\nclearly asked. However, when faced with ambiguous queries they can act\nunpredictably and produce incorrect outputs. This underscores the need for the\ndevelopment of intelligent agents capable of asking clarification questions to\nresolve ambiguities effectively. This capability requires complex\nunderstanding, state tracking, reasoning and planning over multiple\nconversational turns. However, directly measuring this can be challenging. In\nthis paper, we offer a surrogate problem which assesses an LLMs's capability to\ndeduce an entity unknown to itself, but revealed to a judge, by asking the\njudge a series of queries. This \\textit{entity-deducing game} can serve as an\nevaluation framework to probe the conversational reasoning and planning\ncapabilities of language models. We systematically evaluate various LLMs and\ndiscover significant differences in their performance on this task. We find\nthat strong LLMs like GPT-4 outperform human players by a large margin. We\nfurther employ Behavior Cloning (BC) to examine whether a weaker model is\ncapable of imitating a stronger model and generalizing to data or domains,\nusing only the demonstrations from a stronger model. We finally propose to use\nReinforcement Learning to enhance reasoning and planning capacity of Vicuna\nmodels through episodes of game playing, which lead to significant performance\nimprovement. We hope that this problem offers insights into how autonomous\nagents could be trained to behave more intelligently in ambiguous\ncircumstances.", "published": "2023-10-02 16:55:37", "link": "http://arxiv.org/abs/2310.01468v3", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Review of Digital Learning Environments for Teaching Natural Language\n  Processing in K-12 Education", "abstract": "Natural Language Processing (NLP) plays a significant role in our daily lives\nand has become an essential part of Artificial Intelligence (AI) education in\nK-12. As children grow up with NLP-powered applications, it is crucial to\nintroduce NLP concepts to them, fostering their understanding of language\nprocessing, language generation, and ethical implications of AI and NLP. This\npaper presents a comprehensive review of digital learning environments for\nteaching NLP in K-12. Specifically, it explores existing digital learning\ntools, discusses how they support specific NLP tasks and procedures, and\ninvestigates their explainability and evaluation results in educational\ncontexts. By examining the strengths and limitations of these tools, this\nliterature review sheds light on the current state of NLP learning tools in\nK-12 education. It aims to guide future research efforts to refine existing\ntools, develop new ones, and explore more effective and inclusive strategies\nfor integrating NLP into K-12 educational contexts.", "published": "2023-10-02 19:54:30", "link": "http://arxiv.org/abs/2310.01603v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "VAL: Interactive Task Learning with GPT Dialog Parsing", "abstract": "Machine learning often requires millions of examples to produce static,\nblack-box models. In contrast, interactive task learning (ITL) emphasizes\nincremental knowledge acquisition from limited instruction provided by humans\nin modalities such as natural language. However, ITL systems often suffer from\nbrittle, error-prone language parsing, which limits their usability. Large\nlanguage models (LLMs) are resistant to brittleness but are not interpretable\nand cannot learn incrementally. We present VAL, an ITL system with a new\nphilosophy for LLM/symbolic integration. By using LLMs only for specific\ntasks--such as predicate and argument selection--within an algorithmic\nframework, VAL reaps the benefits of LLMs to support interactive learning of\nhierarchical task knowledge from natural language. Acquired knowledge is human\ninterpretable and generalizes to support execution of novel tasks without\nadditional training. We studied users' interactions with VAL in a video game\nsetting, finding that most users could successfully teach VAL using language\nthey felt was natural.", "published": "2023-10-02 20:45:41", "link": "http://arxiv.org/abs/2310.01627v2", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "One model to rule them all ? Towards End-to-End Joint Speaker\n  Diarization and Speech Recognition", "abstract": "This paper presents a novel framework for joint speaker diarization (SD) and\nautomatic speech recognition (ASR), named SLIDAR (sliding-window\ndiarization-augmented recognition). SLIDAR can process arbitrary length inputs\nand can handle any number of speakers, effectively solving ``who spoke what,\nwhen'' concurrently. SLIDAR leverages a sliding window approach and consists of\nan end-to-end diarization-augmented speech transcription (E2E DAST) model which\nprovides, locally, for each window: transcripts, diarization and speaker\nembeddings. The E2E DAST model is based on an encoder-decoder architecture and\nleverages recent techniques such as serialized output training and\n``Whisper-style\" prompting. The local outputs are then combined to get the\nfinal SD+ASR result by clustering the speaker embeddings to get global speaker\nidentities. Experiments performed on monaural recordings from the AMI corpus\nconfirm the effectiveness of the method in both close-talk and far-field speech\nscenarios.", "published": "2023-10-02 23:03:30", "link": "http://arxiv.org/abs/2310.01688v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Use Your INSTINCT: INSTruction optimization for LLMs usIng Neural\n  bandits Coupled with Transformers", "abstract": "Large language models (LLMs) have shown remarkable instruction-following\ncapabilities and achieved impressive performances in various applications.\nHowever, the performances of LLMs depend heavily on the instructions given to\nthem, which are typically manually tuned with substantial human efforts. Recent\nwork has used the query-efficient Bayesian optimization (BO) algorithm to\nautomatically optimize the instructions given to black-box LLMs. However, BO\nusually falls short when optimizing highly sophisticated (e.g.,\nhigh-dimensional) objective functions, such as the functions mapping an\ninstruction to the performance of an LLM. This is mainly due to the limited\nexpressive power of the Gaussian process (GP) which is used by BO as a\nsurrogate to model the objective function. Meanwhile, it has been repeatedly\nshown that neural networks (NNs), especially pre-trained transformers, possess\nstrong expressive power and can model highly complex functions. So, we adopt a\nneural bandit algorithm which replaces the GP in BO by an NN surrogate to\noptimize instructions for black-box LLMs. More importantly, the neural bandit\nalgorithm allows us to naturally couple the NN surrogate with the hidden\nrepresentation learned by a pre-trained transformer (i.e., an open-source LLM),\nwhich significantly boosts its performance. These motivate us to propose our\nINSTruction optimization usIng Neural bandits Coupled with Transformers\n(INSTINCT) algorithm. We perform instruction optimization for ChatGPT and use\nextensive experiments to show that INSTINCT consistently outperforms baselines\nin different tasks, e.g., various instruction induction tasks and the task of\nimproving zero-shot chain-of-thought instructions. Our code is available at\nhttps://github.com/xqlin98/INSTINCT.", "published": "2023-10-02 02:01:16", "link": "http://arxiv.org/abs/2310.02905v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "What's the Magic Word? A Control Theory of LLM Prompting", "abstract": "Prompt engineering is crucial for deploying LLMs but is poorly understood\nmathematically. We formalize LLM systems as a class of discrete stochastic\ndynamical systems to explore prompt engineering through the lens of control\ntheory. We offer a mathematical analysis of the limitations on the\ncontrollability of self-attention as a function of the singular values of the\nparameter matrices. We present complementary empirical results on the\ncontrollability of a panel of LLMs, including Falcon-7b, Llama-7b, and\nFalcon-40b. Given initial state $\\mathbf x_0$ from Wikitext and prompts of\nlength $k \\leq 10$ tokens, we find that the \"correct\" next token is reachable\nat least 97% of the time, and that the top 75 most likely next tokens are\nreachable at least 85% of the time. Intriguingly, short prompt sequences can\ndramatically alter the likelihood of specific outputs, even making the least\nlikely tokens become the most likely ones. This control-theoretic analysis of\nLLMs demonstrates the significant and poorly understood role of input sequences\nin steering output probabilities, offering a foundational perspective for\nenhancing language model system capabilities.", "published": "2023-10-02 22:35:40", "link": "http://arxiv.org/abs/2310.04444v4", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "LoFT: Local Proxy Fine-tuning For Improving Transferability Of\n  Adversarial Attacks Against Large Language Model", "abstract": "It has been shown that Large Language Model (LLM) alignments can be\ncircumvented by appending specially crafted attack suffixes with harmful\nqueries to elicit harmful responses. To conduct attacks against private target\nmodels whose characterization is unknown, public models can be used as proxies\nto fashion the attack, with successful attacks being transferred from public\nproxies to private target models. The success rate of attack depends on how\nclosely the proxy model approximates the private model. We hypothesize that for\nattacks to be transferrable, it is sufficient if the proxy can approximate the\ntarget model in the neighborhood of the harmful query. Therefore, in this\npaper, we propose \\emph{Local Fine-Tuning (LoFT)}, \\textit{i.e.}, fine-tuning\nproxy models on similar queries that lie in the lexico-semantic neighborhood of\nharmful queries to decrease the divergence between the proxy and target models.\nFirst, we demonstrate three approaches to prompt private target models to\nobtain similar queries given harmful queries. Next, we obtain data for local\nfine-tuning by eliciting responses from target models for the generated similar\nqueries. Then, we optimize attack suffixes to generate attack prompts and\nevaluate the impact of our local fine-tuning on the attack's success rate.\nExperiments show that local fine-tuning of proxy models improves attack\ntransferability and increases attack success rate by $39\\%$, $7\\%$, and $0.5\\%$\n(absolute) on target models ChatGPT, GPT-4, and Claude respectively.", "published": "2023-10-02 23:29:23", "link": "http://arxiv.org/abs/2310.04445v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Avalon's Game of Thoughts: Battle Against Deception through Recursive\n  Contemplation", "abstract": "Recent breakthroughs in large language models (LLMs) have brought remarkable\nsuccess in the field of LLM-as-Agent. Nevertheless, a prevalent assumption is\nthat the information processed by LLMs is consistently honest, neglecting the\npervasive deceptive or misleading information in human society and AI-generated\ncontent. This oversight makes LLMs susceptible to malicious manipulations,\npotentially resulting in detrimental outcomes. This study utilizes the\nintricate Avalon game as a testbed to explore LLMs' potential in deceptive\nenvironments. Avalon, full of misinformation and requiring sophisticated logic,\nmanifests as a \"Game-of-Thoughts\". Inspired by the efficacy of humans'\nrecursive thinking and perspective-taking in the Avalon game, we introduce a\nnovel framework, Recursive Contemplation (ReCon), to enhance LLMs' ability to\nidentify and counteract deceptive information. ReCon combines formulation and\nrefinement contemplation processes; formulation contemplation produces initial\nthoughts and speech, while refinement contemplation further polishes them.\nAdditionally, we incorporate first-order and second-order perspective\ntransitions into these processes respectively. Specifically, the first-order\nallows an LLM agent to infer others' mental states, and the second-order\ninvolves understanding how others perceive the agent's mental state. After\nintegrating ReCon with different LLMs, extensive experiment results from the\nAvalon game indicate its efficacy in aiding LLMs to discern and maneuver around\ndeceptive information without extra fine-tuning and data. Finally, we offer a\npossible explanation for the efficacy of ReCon and explore the current\nlimitations of LLMs in terms of safety, reasoning, speaking style, and format,\npotentially furnishing insights for subsequent research.", "published": "2023-10-02 16:27:36", "link": "http://arxiv.org/abs/2310.01320v3", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG", "cs.MA"], "primary_category": "cs.AI"}
{"title": "Representation Engineering: A Top-Down Approach to AI Transparency", "abstract": "In this paper, we identify and characterize the emerging area of\nrepresentation engineering (RepE), an approach to enhancing the transparency of\nAI systems that draws on insights from cognitive neuroscience. RepE places\npopulation-level representations, rather than neurons or circuits, at the\ncenter of analysis, equipping us with novel methods for monitoring and\nmanipulating high-level cognitive phenomena in deep neural networks (DNNs). We\nprovide baselines and an initial analysis of RepE techniques, showing that they\noffer simple yet effective solutions for improving our understanding and\ncontrol of large language models. We showcase how these methods can provide\ntraction on a wide range of safety-relevant problems, including honesty,\nharmlessness, power-seeking, and more, demonstrating the promise of top-down\ntransparency research. We hope that this work catalyzes further exploration of\nRepE and fosters advancements in the transparency and safety of AI systems.", "published": "2023-10-02 17:59:07", "link": "http://arxiv.org/abs/2310.01405v4", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.CY"], "primary_category": "cs.LG"}
{"title": "End-to-End Continuous Speech Emotion Recognition in Real-life Customer\n  Service Call Center Conversations", "abstract": "Speech Emotion recognition (SER) in call center conversations has emerged as\na valuable tool for assessing the quality of interactions between clients and\nagents. In contrast to controlled laboratory environments, real-life\nconversations take place under uncontrolled conditions and are subject to\ncontextual factors that influence the expression of emotions. In this paper, we\npresent our approach to constructing a large-scale reallife dataset (CusEmo)\nfor continuous SER in customer service call center conversations. We adopted\nthe dimensional emotion annotation approach to capture the subtlety,\ncomplexity, and continuity of emotions in real-life call center conversations,\nwhile annotating contextual information. The study also addresses the\nchallenges encountered during the application of the End-to-End (E2E) SER\nsystem to the dataset, including determining the appropriate label sampling\nrate and input segment length, as well as integrating contextual information\n(interlocutor's gender and empathy level) with different weights using\nmultitask learning. The result shows that incorporating the empathy level\ninformation improved the model's performance.", "published": "2023-10-02 11:53:48", "link": "http://arxiv.org/abs/2310.02281v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Fused Deep Denoising Sound Coding Strategy for Bilateral Cochlear\n  Implants", "abstract": "Cochlear implants (CIs) provide a solution for individuals with severe\nsensorineural hearing loss to regain their hearing abilities. When someone\nexperiences this form of hearing impairment in both ears, they may be equipped\nwith two separate CI devices, which will typically further improve the CI\nbenefits. This spatial hearing is particularly crucial when tackling the\nchallenge of understanding speech in noisy environments, a common issue CI\nusers face. Currently, extensive research is dedicated to developing algorithms\nthat can autonomously filter out undesired background noises from desired\nspeech signals. At present, some research focuses on achieving end-to-end\ndenoising, either as an integral component of the initial CI signal processing\nor by fully integrating the denoising process into the CI sound coding\nstrategy. This work is presented in the context of bilateral CI (BiCI) systems,\nwhere we propose a deep-learning-based bilateral speech enhancement model that\nshares information between both hearing sides. Specifically, we connect two\nmonaural end-to-end deep denoising sound coding techniques through intermediary\nlatent fusion layers. These layers amalgamate the latent representations\ngenerated by these techniques by multiplying them together, resulting in an\nenhanced ability to reduce noise and improve learning generalization. The\nobjective instrumental results demonstrate that the proposed fused BiCI sound\ncoding strategy achieves higher interaural coherence, superior noise reduction,\nand enhanced predicted speech intelligibility scores compared to the baseline\nmethods. Furthermore, our speech-in-noise intelligibility results in BiCI users\nreveal that the deep denoising sound coding strategy can attain scores similar\nto those achieved in quiet conditions.", "published": "2023-10-02 11:54:49", "link": "http://arxiv.org/abs/2310.01122v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Disentangling Voice and Content with Self-Supervision for Speaker\n  Recognition", "abstract": "For speaker recognition, it is difficult to extract an accurate speaker\nrepresentation from speech because of its mixture of speaker traits and\ncontent. This paper proposes a disentanglement framework that simultaneously\nmodels speaker traits and content variability in speech. It is realized with\nthe use of three Gaussian inference layers, each consisting of a learnable\ntransition model that extracts distinct speech components. Notably, a\nstrengthened transition model is specifically designed to model complex speech\ndynamics. We also propose a self-supervision method to dynamically disentangle\ncontent without the use of labels other than speaker identities. The efficacy\nof the proposed framework is validated via experiments conducted on the\nVoxCeleb and SITW datasets with 9.56% and 8.24% average reductions in EER and\nminDCF, respectively. Since neither additional model training nor data is\nspecifically needed, it is easily applicable in practical use.", "published": "2023-10-02 12:02:07", "link": "http://arxiv.org/abs/2310.01128v3", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "Scaling Up Music Information Retrieval Training with Semi-Supervised\n  Learning", "abstract": "In the era of data-driven Music Information Retrieval (MIR), the scarcity of\nlabeled data has been one of the major concerns to the success of an MIR task.\nIn this work, we leverage the semi-supervised teacher-student training approach\nto improve MIR tasks. For training, we scale up the unlabeled music data to\n240k hours, which is much larger than any public MIR datasets. We iteratively\ncreate and refine the pseudo-labels in the noisy teacher-student training\nprocess. Knowledge expansion is also explored to iteratively scale up the model\nsizes from as small as less than 3M to almost 100M parameters. We study the\nperformance correlation between data size and model size in the experiments. By\nscaling up both model size and training data, our models achieve\nstate-of-the-art results on several MIR tasks compared to models that are\neither trained in a supervised manner or based on a self-supervised pretrained\nmodel. To our knowledge, this is the first attempt to study the effects of\nscaling up both model and training data for a variety of MIR tasks.", "published": "2023-10-02 17:16:47", "link": "http://arxiv.org/abs/2310.01353v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "F0 analysis of Ghanaian pop singing reveals progressive alignment with\n  equal temperament over the past three decades: a case study", "abstract": "Contemporary Ghanaian popular singing combines European and traditional\nGhanaian influences. We hypothesize that access to technology embedded with\nequal temperament catalyzed a progressive alignment of Ghanaian singing with\nequal-tempered scales over time. To test this, we study the Ghanaian singer\nDaddy Lumba, whose work spans from the earliest Ghanaian electronic style in\nthe late 1980s to the present. Studying a singular musician as a case study\nallows us to refine our analysis without over-interpreting the findings. We\ncurated a collection of his songs, distributed between 1989 and 2016, to\nextract F0 values from isolated vocals. We used Gaussian mixture modeling (GMM)\nto approximate each song's scale and found that the pitch variance has been\ndecreasing over time. We also determined whether the GMM components follow the\narithmetic relationships observed in equal-tempered scales, and observed that\nDaddy Lumba's singing better aligns with equal temperament in recent years.\nTogether, results reveal the impact of exposure to equal-tempered scales,\nresulting in lessened microtonal content in Daddy Lumba's singing. Our study\nhighlights a potential vulnerability of Ghanaian musical scales and implies a\nneed for research that maps and archives singing styles.", "published": "2023-10-02 03:19:45", "link": "http://arxiv.org/abs/2310.00870v1", "categories": ["cs.SD", "cs.IR", "eess.AS"], "primary_category": "cs.SD"}
