{"title": "Few-Shot Data-to-Text Generation via Unified Representation and\n  Multi-Source Learning", "abstract": "We present a novel approach for structured data-to-text generation that\naddresses the limitations of existing methods that primarily focus on specific\ntypes of structured data. Our proposed method aims to improve performance in\nmulti-task training, zero-shot and few-shot scenarios by providing a unified\nrepresentation that can handle various forms of structured data such as tables,\nknowledge graph triples, and meaning representations. We demonstrate that our\nproposed approach can effectively adapt to new structured forms, and can\nimprove performance in comparison to current methods. For example, our method\nresulted in a 66% improvement in zero-shot BLEU scores when transferring models\ntrained on table inputs to a knowledge graph dataset. Our proposed method is an\nimportant step towards a more general data-to-text generation framework.", "published": "2023-08-10 03:09:12", "link": "http://arxiv.org/abs/2308.05317v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Developing an Informal-Formal Persian Corpus", "abstract": "Informal language is a style of spoken or written language frequently used in\ncasual conversations, social media, weblogs, emails and text messages. In\ninformal writing, the language faces some lexical and/or syntactic changes\nvarying among different languages. Persian is one of the languages with many\ndifferences between its formal and informal styles of writing, thus developing\ninformal language processing tools for this language seems necessary. Such a\nconverter needs a large aligned parallel corpus of colloquial-formal sentences\nwhich can be useful for linguists to extract a regulated grammar and\northography for colloquial Persian as is done for the formal language. In this\npaper we explain our methodology in building a parallel corpus of 50,000\nsentence pairs with alignments in the word/phrase level. The sentences were\nattempted to cover almost all kinds of lexical and syntactic changes between\ninformal and formal Persian, therefore both methods of exploring and collecting\nfrom the different resources of informal scripts and following the phonological\nand morphological patterns of changes were applied to find as much instances as\npossible. The resulting corpus has about 530,000 alignments and a dictionary\ncontaining 49,397 word and phrase pairs.", "published": "2023-08-10 04:57:34", "link": "http://arxiv.org/abs/2308.05336v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WeaverBird: Empowering Financial Decision-Making with Large Language\n  Model, Knowledge Base, and Search Engine", "abstract": "We present WeaverBird, an intelligent dialogue system designed specifically\nfor the finance domain. Our system harnesses a large language model of GPT\narchitecture that has been tuned using extensive corpora of finance-related\ntext. As a result, our system possesses the capability to understand complex\nfinancial queries, such as \"How should I manage my investments during\ninflation?\", and provide informed responses. Furthermore, our system\nincorporates a local knowledge base and a search engine to retrieve relevant\ninformation. The final responses are conditioned on the search results and\ninclude proper citations to the sources, thus enjoying an enhanced credibility.\nThrough a range of finance-related questions, we have demonstrated the superior\nperformance of our system compared to other models. To experience our system\nfirsthand, users can interact with our live demo at\nhttps://weaverbird.ttic.edu, as well as watch our 2-min video illustration at\nhttps://www.youtube.com/watch?v=yofgeqnlrMc.", "published": "2023-08-10 06:08:20", "link": "http://arxiv.org/abs/2308.05361v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do Language Models' Words Refer?", "abstract": "What do language models (LMs) do with language? Everyone agrees that they can\nproduce sequences of (mostly) coherent strings of English. But do those\nsentences mean something, or are LMs simply babbling in a convincing simulacrum\nof language use? Here we will address one aspect of this broad question:\nwhether LMs' words can refer, that is, achieve \"word-to-world\" connections.\nThere is prima facie reason to think they do not since LMs do not interact with\nthe world in the way that ordinary language users do. Drawing on insights from\nthe externalist tradition in philosophy of language, we argue that those\nappearances are misleading: even if the inputs to an LM are simply strings of\ntext, they are strings of text with natural histories, and that may suffice to\nput LMs' words into referential contact with the external world.", "published": "2023-08-10 13:39:40", "link": "http://arxiv.org/abs/2308.05576v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Preliminary Study of the Intrinsic Relationship between Complexity and\n  Alignment", "abstract": "Training large language models (LLMs) with open-domain instruction data has\nyielded remarkable success in aligning to end tasks and human preferences.\nExtensive research has highlighted the importance of the quality and diversity\nof instruction data. However, the impact of data complexity, as a crucial\nmetric, remains relatively unexplored from three aspects: (1)where the\nsustainability of performance improvements with increasing complexity is\nuncertain; (2)whether the improvement brought by complexity merely comes from\nintroducing more training tokens; and (3)where the potential benefits of\nincorporating instructions from easy to difficult are not yet fully understood.\nIn this paper, we propose Tree-Instruct to systematically enhance the\ninstruction complexity in a controllable manner. By adding a specified number\nof nodes to instructions' semantic trees, this approach not only yields new\ninstruction data from the modified tree but also allows us to control the\ndifficulty level of modified instructions. Our preliminary experiments reveal\nthe following insights: (1)Increasing complexity consistently leads to\nsustained performance improvements of LLMs. (2)Under the same token budget, a\nfew complex instructions outperform diverse yet simple instructions.\n(3)Curriculum instruction tuning might not yield the anticipated results;\nfocusing on increasing complexity appears to be the key.", "published": "2023-08-10 16:58:51", "link": "http://arxiv.org/abs/2308.05696v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Classification of Human- and AI-Generated Texts: Investigating Features\n  for ChatGPT", "abstract": "Recently, generative AIs like ChatGPT have become available to the wide\npublic. These tools can for instance be used by students to generate essays or\nwhole theses. But how does a teacher know whether a text is written by a\nstudent or an AI? In our work, we explore traditional and new features to (1)\ndetect text generated by AI from scratch and (2) text rephrased by AI. Since we\nfound that classification is more difficult when the AI has been instructed to\ncreate the text in a way that a human would not recognize that it was generated\nby an AI, we also investigate this more advanced case. For our experiments, we\nproduced a new text corpus covering 10 school topics. Our best systems to\nclassify basic and advanced human-generated/AI-generated texts have F1-scores\nof over 96%. Our best systems for classifying basic and advanced\nhuman-generated/AI-rephrased texts have F1-scores of more than 78%. The systems\nuse a combination of perplexity, semantic, list lookup, error-based,\nreadability, AI feedback, and text vector features. Our results show that the\nnew features substantially help to improve the performance of many classifiers.\nOur best basic text rephrasing detection system even outperforms GPTZero by\n183.8% relative in F1-score.", "published": "2023-08-10 05:09:42", "link": "http://arxiv.org/abs/2308.05341v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Metacognitive Prompting Improves Understanding in Large Language Models", "abstract": "In Large Language Models (LLMs), there have been consistent advancements in\ntask-specific performance, largely influenced by effective prompt design.\nRecent advancements in prompting have enhanced reasoning in logic-intensive\ntasks for LLMs, yet the nuanced understanding abilities of these models,\ncrucial for processing and interpreting complex information, remain\nunderexplored. In this study, we introduce Metacognitive Prompting (MP), a\nstrategy inspired by human introspective reasoning processes. Using MP, LLMs\nundergo a systematic series of structured, self-aware evaluations, drawing on\nboth their vast inherent knowledge and new insights. We conduct extensive\nexperiments on four prevalent LLMs: Llama2, PaLM2, GPT-3.5, and GPT-4, across\nten natural language understanding (NLU) datasets from GLUE, SuperGLUE, BLUE,\nand LexGLUE benchmarks. Additionally, we compare our method with\nchain-of-thought prompting and its advanced versions. The results show that\nGPT-4 consistently excels across all tasks, while other models have shown\nsignificant progress in some tasks when used in conjunction with MP.\nFurthermore, MP consistently outperforms existing prompting methods in both\ngeneral and domain-specific NLU tasks. This study underscores the potential to\namplify the understanding abilities of LLMs and highlights the benefits of\nmirroring human introspective reasoning in NLU tasks.", "published": "2023-08-10 05:10:17", "link": "http://arxiv.org/abs/2308.05342v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring Machine Learning and Transformer-based Approaches for\n  Deceptive Text Classification: A Comparative Analysis", "abstract": "Deceptive text classification is a critical task in natural language\nprocessing that aims to identify deceptive o fraudulent content. This study\npresents a comparative analysis of machine learning and transformer-based\napproaches for deceptive text classification. We investigate the effectiveness\nof traditional machine learning algorithms and state-of-the-art transformer\nmodels, such as BERT, XLNET, DistilBERT, and RoBERTa, in detecting deceptive\ntext. A labeled dataset consisting of deceptive and non-deceptive texts is used\nfor training and evaluation purposes. Through extensive experimentation, we\ncompare the performance metrics, including accuracy, precision, recall, and F1\nscore, of the different approaches. The results of this study shed light on the\nstrengths and limitations of machine learning and transformer-based methods for\ndeceptive text classification, enabling researchers and practitioners to make\ninformed decisions when dealing with deceptive content.", "published": "2023-08-10 10:07:00", "link": "http://arxiv.org/abs/2308.05476v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exploring Linguistic Similarity and Zero-Shot Learning for Multilingual\n  Translation of Dravidian Languages", "abstract": "Current research in zero-shot translation is plagued by several issues such\nas high compute requirements, increased training time and off target\ntranslations. Proposed remedies often come at the cost of additional data or\ncompute requirements. Pivot based neural machine translation is preferred over\na single-encoder model for most settings despite the increased training and\nevaluation time. In this work, we overcome the shortcomings of zero-shot\ntranslation by taking advantage of transliteration and linguistic similarity.\nWe build a single encoder-decoder neural machine translation system for\nDravidian-Dravidian multilingual translation and perform zero-shot translation.\nWe compare the data vs zero-shot accuracy tradeoff and evaluate the performance\nof our vanilla method against the current state of the art pivot based method.\nWe also test the theory that morphologically rich languages require large\nvocabularies by restricting the vocabulary using an optimal transport based\ntechnique. Our model manages to achieves scores within 3 BLEU of large-scale\npivot-based models when it is trained on 50\\% of the language directions.", "published": "2023-08-10 13:38:09", "link": "http://arxiv.org/abs/2308.05574v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "You Only Prompt Once: On the Capabilities of Prompt Learning on Large\n  Language Models to Tackle Toxic Content", "abstract": "The spread of toxic content online is an important problem that has adverse\neffects on user experience online and in our society at large. Motivated by the\nimportance and impact of the problem, research focuses on developing solutions\nto detect toxic content, usually leveraging machine learning (ML) models\ntrained on human-annotated datasets. While these efforts are important, these\nmodels usually do not generalize well and they can not cope with new trends\n(e.g., the emergence of new toxic terms). Currently, we are witnessing a shift\nin the approach to tackling societal issues online, particularly leveraging\nlarge language models (LLMs) like GPT-3 or T5 that are trained on vast corpora\nand have strong generalizability. In this work, we investigate how we can use\nLLMs and prompt learning to tackle the problem of toxic content, particularly\nfocusing on three tasks; 1) Toxicity Classification, 2) Toxic Span Detection,\nand 3) Detoxification. We perform an extensive evaluation over five model\narchitectures and eight datasets demonstrating that LLMs with prompt learning\ncan achieve similar or even better performance compared to models trained on\nthese specific tasks. We find that prompt learning achieves around 10\\%\nimprovement in the toxicity classification task compared to the baselines,\nwhile for the toxic span detection task we find better performance to the best\nbaseline (0.643 vs. 0.640 in terms of $F_1$-score). Finally, for the\ndetoxification task, we find that prompt learning can successfully reduce the\naverage toxicity score (from 0.775 to 0.213) while preserving semantic meaning.", "published": "2023-08-10 14:14:13", "link": "http://arxiv.org/abs/2308.05596v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "A Novel Self-training Approach for Low-resource Speech Recognition", "abstract": "In this paper, we propose a self-training approach for automatic speech\nrecognition (ASR) for low-resource settings. While self-training approaches\nhave been extensively developed and evaluated for high-resource languages such\nas English, their applications to low-resource languages like Punjabi have been\nlimited, despite the language being spoken by millions globally. The scarcity\nof annotated data has hindered the development of accurate ASR systems,\nespecially for low-resource languages (e.g., Punjabi and M\\=aori languages). To\naddress this issue, we propose an effective self-training approach that\ngenerates highly accurate pseudo-labels for unlabeled low-resource speech. Our\nexperimental analysis demonstrates that our approach significantly improves\nword error rate, achieving a relative improvement of 14.94% compared to a\nbaseline model across four real speech datasets. Further, our proposed approach\nreports the best results on the Common Voice Punjabi dataset.", "published": "2023-08-10 01:02:45", "link": "http://arxiv.org/abs/2308.05269v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Investigating disaster response through social media data and the\n  Susceptible-Infected-Recovered (SIR) model: A case study of 2020 Western U.S.\n  wildfire season", "abstract": "Effective disaster response is critical for affected communities. Responders\nand decision-makers would benefit from reliable, timely measures of the issues\nimpacting their communities during a disaster, and social media offers a\npotentially rich data source. Social media can reflect public concerns and\ndemands during a disaster, offering valuable insights for decision-makers to\nunderstand evolving situations and optimize resource allocation. We used\nBidirectional Encoder Representations from Transformers (BERT) topic modeling\nto cluster topics from Twitter data. Then, we conducted a temporal-spatial\nanalysis to examine the distribution of these topics across different regions\nduring the 2020 western U.S. wildfire season. Our results show that Twitter\nusers mainly focused on three topics:\"health impact,\" \"damage,\" and\n\"evacuation.\" We used the Susceptible-Infected-Recovered (SIR) theory to\nexplore the magnitude and velocity of topic diffusion on Twitter. The results\ndisplayed a clear relationship between topic trends and wildfire propagation\npatterns. The estimated parameters obtained from the SIR model in selected\ncities revealed that residents exhibited a high level of several concerns\nduring the wildfire. Our study details how the SIR model and topic modeling\nusing social media data can provide decision-makers with a quantitative\napproach to measure disaster response and support their decision-making\nprocesses.", "published": "2023-08-10 01:51:33", "link": "http://arxiv.org/abs/2308.05281v2", "categories": ["cs.SI", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.SI"}
{"title": "LLM As DBA", "abstract": "Database administrators (DBAs) play a crucial role in managing, maintaining\nand optimizing a database system to ensure data availability, performance, and\nreliability. However, it is hard and tedious for DBAs to manage a large number\nof database instances (e.g., millions of instances on the cloud databases).\nRecently large language models (LLMs) have shown great potential to understand\nvaluable documents and accordingly generate reasonable answers. Thus, we\npropose D-Bot, a LLM-based database administrator that can continuously acquire\ndatabase maintenance experience from textual sources, and provide reasonable,\nwell-founded, in-time diagnosis and optimization advice for target databases.\nThis paper presents a revolutionary LLM-centric framework for database\nmaintenance, including (i) database maintenance knowledge detection from\ndocuments and tools, (ii) tree of thought reasoning for root cause analysis,\nand (iii) collaborative diagnosis among multiple LLMs. Our preliminary\nexperimental results that D-Bot can efficiently and effectively diagnose the\nroot causes and our code is available at\ngithub.com/TsinghuaDatabaseGroup/DB-GPT.", "published": "2023-08-10 10:12:43", "link": "http://arxiv.org/abs/2308.05481v2", "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.DB"}
{"title": "LASIGE and UNICAGE solution to the NASA LitCoin NLP Competition", "abstract": "Biomedical Natural Language Processing (NLP) tends to become cumbersome for\nmost researchers, frequently due to the amount and heterogeneity of text to be\nprocessed. To address this challenge, the industry is continuously developing\nhighly efficient tools and creating more flexible engineering solutions. This\nwork presents the integration between industry data engineering solutions for\nefficient data processing and academic systems developed for Named Entity\nRecognition (LasigeUnicage\\_NER) and Relation Extraction (BiOnt). Our design\nreflects an integration of those components with external knowledge in the form\nof additional training data from other datasets and biomedical ontologies. We\nused this pipeline in the 2022 LitCoin NLP Challenge, where our team\nLasigeUnicage was awarded the 7th Prize out of approximately 200 participating\nteams, reflecting a successful collaboration between the academia (LASIGE) and\nthe industry (Unicage). The software supporting this work is available at\n\\url{https://github.com/lasigeBioTM/Litcoin-Lasige_Unicage}.", "published": "2023-08-10 14:41:17", "link": "http://arxiv.org/abs/2308.05609v1", "categories": ["cs.CL", "cs.IR", "cs.PF"], "primary_category": "cs.CL"}
{"title": "IIHT: Medical Report Generation with Image-to-Indicator Hierarchical\n  Transformer", "abstract": "Automated medical report generation has become increasingly important in\nmedical analysis. It can produce computer-aided diagnosis descriptions and thus\nsignificantly alleviate the doctors' work. Inspired by the huge success of\nneural machine translation and image captioning, various deep learning methods\nhave been proposed for medical report generation. However, due to the inherent\nproperties of medical data, including data imbalance and the length and\ncorrelation between report sequences, the generated reports by existing methods\nmay exhibit linguistic fluency but lack adequate clinical accuracy. In this\nwork, we propose an image-to-indicator hierarchical transformer (IIHT)\nframework for medical report generation. It consists of three modules, i.e., a\nclassifier module, an indicator expansion module and a generator module. The\nclassifier module first extracts image features from the input medical images\nand produces disease-related indicators with their corresponding states. The\ndisease-related indicators are subsequently utilised as input for the indicator\nexpansion module, incorporating the \"data-text-data\" strategy. The\ntransformer-based generator then leverages these extracted features along with\nimage features as auxiliary information to generate final reports. Furthermore,\nthe proposed IIHT method is feasible for radiologists to modify disease\nindicators in real-world scenarios and integrate the operations into the\nindicator expansion module for fluent and accurate medical report generation.\nExtensive experiments and comparisons with state-of-the-art methods under\nvarious evaluation metrics demonstrate the great performance of the proposed\nmethod.", "published": "2023-08-10 15:22:11", "link": "http://arxiv.org/abs/2308.05633v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "AST-MHSA : Code Summarization using Multi-Head Self-Attention", "abstract": "Code summarization aims to generate concise natural language descriptions for\nsource code. The prevailing approaches adopt transformer-based encoder-decoder\narchitectures, where the Abstract Syntax Tree (AST) of the source code is\nutilized for encoding structural information. However, ASTs are much longer\nthan the corresponding source code, and existing methods ignore this size\nconstraint by directly feeding the entire linearized AST into the encoders.\nThis simplistic approach makes it challenging to extract truly valuable\ndependency relations from the overlong input sequence and leads to significant\ncomputational overhead due to self-attention applied to all nodes in the AST.\n  To address this issue effectively and efficiently, we present a model,\nAST-MHSA that uses multi-head attention to extract the important semantic\ninformation from the AST. The model consists of two main components: an encoder\nand a decoder. The encoder takes as input the abstract syntax tree (AST) of the\ncode and generates a sequence of hidden states. The decoder then takes these\nhidden states as input and generates a natural language summary of the code.\n  The multi-head attention mechanism allows the model to learn different\nrepresentations of the input code, which can be combined to generate a more\ncomprehensive summary. The model is trained on a dataset of code and summaries,\nand the parameters of the model are optimized to minimize the loss between the\ngenerated summaries and the ground-truth summaries.", "published": "2023-08-10 15:43:46", "link": "http://arxiv.org/abs/2308.05646v1", "categories": ["cs.CL", "cs.LG", "cs.PL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "EXPRESSO: A Benchmark and Analysis of Discrete Expressive Speech\n  Resynthesis", "abstract": "Recent work has shown that it is possible to resynthesize high-quality speech\nbased, not on text, but on low bitrate discrete units that have been learned in\na self-supervised fashion and can therefore capture expressive aspects of\nspeech that are hard to transcribe (prosody, voice styles, non-verbal\nvocalization). The adoption of these methods is still limited by the fact that\nmost speech synthesis datasets are read, severely limiting spontaneity and\nexpressivity. Here, we introduce Expresso, a high-quality expressive speech\ndataset for textless speech synthesis that includes both read speech and\nimprovised dialogues rendered in 26 spontaneous expressive styles. We\nillustrate the challenges and potentials of this dataset with an expressive\nresynthesis benchmark where the task is to encode the input in low-bitrate\nunits and resynthesize it in a target voice while preserving content and style.\nWe evaluate resynthesis quality with automatic metrics for different\nself-supervised discrete encoders, and explore tradeoffs between quality,\nbitrate and invariance to speaker and style. All the dataset, evaluation\nmetrics and baseline models are open source", "published": "2023-08-10 17:41:19", "link": "http://arxiv.org/abs/2308.05725v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Bringing order into the realm of Transformer-based language models for\n  artificial intelligence and law", "abstract": "Transformer-based language models (TLMs) have widely been recognized to be a\ncutting-edge technology for the successful development of deep-learning-based\nsolutions to problems and applications that require natural language processing\nand understanding. Like for other textual domains, TLMs have indeed pushed the\nstate-of-the-art of AI approaches for many tasks of interest in the legal\ndomain. Despite the first Transformer model being proposed about six years ago,\nthere has been a rapid progress of this technology at an unprecedented rate,\nwhereby BERT and related models represent a major reference, also in the legal\ndomain. This article provides the first systematic overview of TLM-based\nmethods for AI-driven problems and tasks in the legal sphere. A major goal is\nto highlight research advances in this field so as to understand, on the one\nhand, how the Transformers have contributed to the success of AI in supporting\nlegal processes, and on the other hand, what are the current limitations and\nopportunities for further research development.", "published": "2023-08-10 11:14:22", "link": "http://arxiv.org/abs/2308.05502v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.NE", "physics.soc-ph"], "primary_category": "cs.CL"}
{"title": "Breaking Language Barriers with MMTweets: Advancing Cross-Lingual\n  Debunked Narrative Retrieval for Fact-Checking", "abstract": "Finding previously debunked narratives involves identifying claims that have\nalready undergone fact-checking. The issue intensifies when similar false\nclaims persist in multiple languages, despite the availability of debunks for\nseveral months in another language. Hence, automatically finding debunks (or\nfact-checks) in multiple languages is crucial to make the best use of scarce\nfact-checkers' resources. Mainly due to the lack of readily available data,\nthis is an understudied problem, particularly when considering the\ncross-lingual scenario, i.e. the retrieval of debunks in a language different\nfrom the language of the online post being checked. This study introduces\ncross-lingual debunked narrative retrieval and addresses this research gap by:\n(i) creating Multilingual Misinformation Tweets (MMTweets): a dataset that\nstands out, featuring cross-lingual pairs, images, human annotations, and\nfine-grained labels, making it a comprehensive resource compared to its\ncounterparts; (ii) conducting an extensive experiment to benchmark\nstate-of-the-art cross-lingual retrieval models and introducing multistage\nretrieval methods tailored for the task; and (iii) comprehensively evaluating\nretrieval models for their cross-lingual and cross-dataset transfer\ncapabilities within MMTweets, and conducting a retrieval latency analysis. We\nfind that MMTweets presents challenges for cross-lingual debunked narrative\nretrieval, highlighting areas for improvement in retrieval models. Nonetheless,\nthe study provides valuable insights for creating MMTweets datasets and\noptimising debunked narrative retrieval models to empower fact-checking\nendeavours. The dataset and annotation codebook are publicly available at\nhttps://doi.org/10.5281/zenodo.10637161.", "published": "2023-08-10 16:33:17", "link": "http://arxiv.org/abs/2308.05680v2", "categories": ["cs.CL", "cs.CY", "cs.IR", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Stabilizing Training with Soft Dynamic Time Warping: A Case Study for\n  Pitch Class Estimation with Weakly Aligned Targets", "abstract": "Soft dynamic time warping (SDTW) is a differentiable loss function that\nallows for training neural networks from weakly aligned data. Typically, SDTW\nis used to iteratively compute and refine soft alignments that compensate for\ntemporal deviations between the training data and its weakly annotated targets.\nOne major problem is that a mismatch between the estimated soft alignments and\nthe reference alignments in the early training stage leads to incorrect\nparameter updates, making the overall training procedure unstable. In this\npaper, we investigate such stability issues by considering the task of pitch\nclass estimation from music recordings as an illustrative case study. In\nparticular, we introduce and discuss three conceptually different strategies (a\nhyperparameter scheduling, a diagonal prior, and a sequence unfolding strategy)\nwith the objective of stabilizing intermediate soft alignment results. Finally,\nwe report on experiments that demonstrate the effectiveness of the strategies\nand discuss efficiency and implementation issues.", "published": "2023-08-10 08:42:57", "link": "http://arxiv.org/abs/2308.05429v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "AudioLDM 2: Learning Holistic Audio Generation with Self-supervised\n  Pretraining", "abstract": "Although audio generation shares commonalities across different types of\naudio, such as speech, music, and sound effects, designing models for each type\nrequires careful consideration of specific objectives and biases that can\nsignificantly differ from those of other types. To bring us closer to a unified\nperspective of audio generation, this paper proposes a framework that utilizes\nthe same learning method for speech, music, and sound effect generation. Our\nframework introduces a general representation of audio, called \"language of\naudio\" (LOA). Any audio can be translated into LOA based on AudioMAE, a\nself-supervised pre-trained representation learning model. In the generation\nprocess, we translate any modalities into LOA by using a GPT-2 model, and we\nperform self-supervised audio generation learning with a latent diffusion model\nconditioned on LOA. The proposed framework naturally brings advantages such as\nin-context learning abilities and reusable self-supervised pretrained AudioMAE\nand latent diffusion models. Experiments on the major benchmarks of\ntext-to-audio, text-to-music, and text-to-speech demonstrate state-of-the-art\nor competitive performance against previous approaches. Our code, pretrained\nmodel, and demo are available at https://audioldm.github.io/audioldm2.", "published": "2023-08-10 17:55:13", "link": "http://arxiv.org/abs/2308.05734v3", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
