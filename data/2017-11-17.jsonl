{"title": "Phonological (un)certainty weights lexical activation", "abstract": "Spoken word recognition involves at least two basic computations. First is\nmatching acoustic input to phonological categories (e.g. /b/, /p/, /d/). Second\nis activating words consistent with those phonological categories. Here we test\nthe hypothesis that the listener's probability distribution over lexical items\nis weighted by the outcome of both computations: uncertainty about phonological\ndiscretisation and the frequency of the selected word(s). To test this, we\nrecord neural responses in auditory cortex using magnetoencephalography, and\nmodel this activity as a function of the size and relative activation of\nlexical candidates. Our findings indicate that towards the beginning of a word,\nthe processing system indeed weights lexical candidates by both phonological\ncertainty and lexical frequency; however, later into the word, activation is\nweighted by frequency alone.", "published": "2017-11-17 21:17:20", "link": "http://arxiv.org/abs/1711.06729v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Identifying Patterns of Associated-Conditions through Topic Models of\n  Electronic Medical Records", "abstract": "Multiple adverse health conditions co-occurring in a patient are typically\nassociated with poor prognosis and increased office or hospital visits.\nDeveloping methods to identify patterns of co-occurring conditions can assist\nin diagnosis. Thus identifying patterns of associations among co-occurring\nconditions is of growing interest. In this paper, we report preliminary results\nfrom a data-driven study, in which we apply a machine learning method, namely,\ntopic modeling, to electronic medical records, aiming to identify patterns of\nassociated conditions. Specifically, we use the well established latent\ndirichlet allocation, a method based on the idea that documents can be modeled\nas a mixture of latent topics, where each topic is a distribution over words.\nIn our study, we adapt the LDA model to identify latent topics in patients'\nEMRs. We evaluate the performance of our method both qualitatively, and show\nthat the obtained topics indeed align well with distinct medical phenomena\ncharacterized by co-occurring conditions.", "published": "2017-11-17 21:39:19", "link": "http://arxiv.org/abs/1711.10960v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Organize Knowledge and Answer Questions with N-Gram Machines", "abstract": "Though deep neural networks have great success in natural language\nprocessing, they are limited at more knowledge intensive AI tasks, such as\nopen-domain Question Answering (QA). Existing end-to-end deep QA models need to\nprocess the entire text after observing the question, and therefore their\ncomplexity in responding a question is linear in the text size. This is\nprohibitive for practical tasks such as QA from Wikipedia, a novel, or the Web.\nWe propose to solve this scalability issue by using symbolic meaning\nrepresentations, which can be indexed and retrieved efficiently with complexity\nthat is independent of the text size. We apply our approach, called the N-Gram\nMachine (NGM), to three representative tasks. First as proof-of-concept, we\ndemonstrate that NGM successfully solves the bAbI tasks of synthetic text.\nSecond, we show that NGM scales to large corpus by experimenting on \"life-long\nbAbI\", a special version of bAbI that contains millions of sentences. Lastly on\nthe WikiMovies dataset, we use NGM to induce latent structure (i.e. schema) and\nanswer questions from natural language Wikipedia text, with only QA pairs as\nweak supervision.", "published": "2017-11-17 22:02:53", "link": "http://arxiv.org/abs/1711.06744v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Double Joint Bayesian Approach for J-Vector Based Text-dependent\n  Speaker Verification", "abstract": "J-vector has been proved to be very effective in text-dependent speaker\nverification with short-duration speech. However, the current state-of-the-art\nback-end classifiers, e.g. joint Bayesian model, cannot make full use of such\ndeep features. In this paper, we generalize the standard joint Bayesian\napproach to model the multi-faceted information in the j-vector explicitly and\njointly. In our generalization, the j-vector was modeled as a result derived by\na generative Double Joint Bayesian (DoJoBa) model, which contains several kinds\nof latent variables. With DoJoBa, we are able to explicitly build a model that\ncan combine multiple heterogeneous information from the j-vectors. In\nverification step, we calculated the likelihood to describe whether the two\nj-vectors having consistent labels or not. On the public RSR2015 data corpus,\nthe experimental results showed that our approach can achieve 0.02\\% EER and\n0.02\\% EER for impostor wrong and impostor correct cases respectively.", "published": "2017-11-17 07:19:03", "link": "http://arxiv.org/abs/1711.06434v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
