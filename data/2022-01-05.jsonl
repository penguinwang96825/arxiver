{"title": "Strategies of Effective Digitization of Commentaries and\n  Sub-commentaries: Towards the Construction of Textual History", "abstract": "This paper describes additional aspects of a digital tool called the 'Textual\nHistory Tool'. We describe its various salient features with special reference\nto those of its features that may help the philologist digitize commentaries\nand sub-commentaries on a text. This tool captures the historical evolution of\na text through various temporal stages, and interrelated data culled from\nvarious types of related texts. We use the text of the K\\=a\\'sik\\=avrtti (KV)\nas a sample text, and with the help of philologists, we digitize the\ncommentaries available to us. We digitize the Ny\\=asa (Ny), the Padama\\~njar\\=i\n(Pm) and sub commentaries on the KV text known as the Tantraprad\\=ipa (Tp), and\nthe Makaranda (Mk). We divide each commentary and sub-commentary into\nfunctional units and describe the methodology and motivation behind the\nfunctional unit division. Our functional unit division helps generate more\naccurate phylogenetic trees for the text, based on distance methods using the\ndata entered in the tool.", "published": "2022-01-05 16:43:43", "link": "http://arxiv.org/abs/2201.01693v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Some Strategies to Capture Karaka-Yogyata with Special Reference to\n  apadana", "abstract": "In today's digital world language technology has gained importance. Several\nsoftwares, have been developed and are available in the field of computational\nlinguistics. Such tools play a crucial role in making classical language texts\neasily accessible. Some Indian philosophical schools have contributed towards\nvarious techniques of verbal cognition to analyze sentences correctly. These\ntheories can be used to build computational tools for word sense disambiguation\n(WSD). In the absence of WSD, one cannot have proper verbal cognition. These\ntheories considered the concept of 'Yogyat\\=a' (congruity or compatibility) as\nthe indispensable cause of verbal cognition. In this work, we come up with some\ninsights on the basis of these theories to create a tool that will capture\nYogyat\\=a of words. We describe the problem of ambiguity in a text and present\na method to resolve it computationally with the help of Yogyat\\=a. Here, only\ntwo major schools i.e. Ny\\=aya and Vy\\=akarana are considered. Our paper\nattempts to show the implication of the creation of our tool in this area.\nAlso, our tool involves the creation of an 'ontological tag-set' as well as\nstrategies to mark up the lexicon. The introductory description of ablation is\nalso covered in this paper. Such strategies and some case studies shall form\nthe core of our paper.", "published": "2022-01-05 16:50:13", "link": "http://arxiv.org/abs/2201.01700v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semi-automatic WordNet Linking using Word Embeddings", "abstract": "Wordnets are rich lexico-semantic resources. Linked wordnets are extensions\nof wordnets, which link similar concepts in wordnets of different languages.\nSuch resources are extremely useful in many Natural Language Processing (NLP)\napplications, primarily those based on knowledge-based approaches. In such\napproaches, these resources are considered as gold standard/oracle. Thus, it is\ncrucial that these resources hold correct information. Thereby, they are\ncreated by human experts. However, manual maintenance of such resources is a\ntedious and costly affair. Thus techniques that can aid the experts are\ndesirable. In this paper, we propose an approach to link wordnets. Given a\nsynset of the source language, the approach returns a ranked list of potential\ncandidate synsets in the target language from which the human expert can choose\nthe correct one(s). Our technique is able to retrieve a winner synset in the\ntop 10 ranked list for 60% of all synsets and 70% of noun synsets.", "published": "2022-01-05 18:15:55", "link": "http://arxiv.org/abs/2201.01747v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KUDO Interpreter Assist: Automated Real-time Support for Remote\n  Interpretation", "abstract": "High-quality human interpretation requires linguistic and factual preparation\nas well as the ability to retrieve information in real-time. This situation\nbecomes particularly relevant in the context of remote simultaneous\ninterpreting (RSI) where time-to-event may be short, posing new challenges to\nprofessional interpreters and their commitment to delivering high-quality\nservices. In order to mitigate these challenges, we present Interpreter Assist,\na computer-assisted interpreting tool specifically designed for the integration\nin RSI scenarios. Interpreter Assist comprises two main feature sets: an\nautomatic glossary creation tool and a real-time suggestion system. In this\npaper, we describe the overall design of our tool, its integration into the\ntypical RSI workflow, and the results achieved on benchmark tests both in terms\nof quality and relevance of glossary creation as well as in precision and\nrecall of the real-time suggestion feature.", "published": "2022-01-05 19:38:06", "link": "http://arxiv.org/abs/2201.01800v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data-driven Model Generalizability in Crosslinguistic Low-resource\n  Morphological Segmentation", "abstract": "Common designs of model evaluation typically focus on monolingual settings,\nwhere different models are compared according to their performance on a single\ndata set that is assumed to be representative of all possible data for the task\nat hand. While this may be reasonable for a large data set, this assumption is\ndifficult to maintain in low-resource scenarios, where artifacts of the data\ncollection can yield data sets that are outliers, potentially making\nconclusions about model performance coincidental. To address these concerns, we\ninvestigate model generalizability in crosslinguistic low-resource scenarios.\nUsing morphological segmentation as the test case, we compare three broad\nclasses of models with different parameterizations, taking data from 11\nlanguages across 6 language families. In each experimental setting, we evaluate\nall models on a first data set, then examine their performance consistency when\nintroducing new randomly sampled data sets with the same size and when applying\nthe trained models to unseen test sets of varying sizes. The results\ndemonstrate that the extent of model generalization depends on the\ncharacteristics of the data set, and does not necessarily rely heavily on the\ndata set size. Among the characteristics that we studied, the ratio of morpheme\noverlap and that of the average number of morphemes per word between the\ntraining and test sets are the two most prominent factors. Our findings suggest\nthat future work should adopt random sampling to construct data sets with\ndifferent sizes in order to make more responsible claims about model\nevaluation.", "published": "2022-01-05 22:19:10", "link": "http://arxiv.org/abs/2201.01845v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hyperparameter-free Continuous Learning for Domain Classification in\n  Natural Language Understanding", "abstract": "Domain classification is the fundamental task in natural language\nunderstanding (NLU), which often requires fast accommodation to new emerging\ndomains. This constraint makes it impossible to retrain all previous domains,\neven if they are accessible to the new model. Most existing continual learning\napproaches suffer from low accuracy and performance fluctuation, especially\nwhen the distributions of old and new data are significantly different. In\nfact, the key real-world problem is not the absence of old data, but the\ninefficiency to retrain the model with the whole old dataset. Is it potential\nto utilize some old data to yield high accuracy and maintain stable\nperformance, while at the same time, without introducing extra hyperparameters?\nIn this paper, we proposed a hyperparameter-free continual learning model for\ntext data that can stably produce high performance under various environments.\nSpecifically, we utilize Fisher information to select exemplars that can\n\"record\" key information of the original model. Also, a novel scheme called\ndynamical weight consolidation is proposed to enable hyperparameter-free\nlearning during the retrain process. Extensive experiments demonstrate that\nbaselines suffer from fluctuated performance and therefore useless in practice.\nOn the contrary, our proposed model CCFI significantly and consistently\noutperforms the best state-of-the-art method by up to 20% in average accuracy,\nand each component of CCFI contributes effectively to overall performance.", "published": "2022-01-05 02:46:16", "link": "http://arxiv.org/abs/2201.01420v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Monitoring Energy Trends through Automatic Information Extraction", "abstract": "Energy research is of crucial public importance but the use of computer\nscience technologies like automatic text processing and data management for the\nenergy domain is still rare. Employing these technologies in the energy domain\nwill be a significant contribution to the interdisciplinary topic of ``energy\ninformatics\", just like the related progress within the interdisciplinary area\nof ``bioinformatics\". In this paper, we present the architecture of a Web-based\nsemantic system called EneMonIE (Energy Monitoring through Information\nExtraction) for monitoring up-to-date energy trends through the use of\nautomatic, continuous, and guided information extraction from diverse types of\nmedia available on the Web. The types of media handled by the system will\ninclude online news articles, social media texts, online news videos, and\nopen-access scholarly papers and technical reports as well as various numeric\nenergy data made publicly available by energy organizations. The system will\nutilize and contribute to the energy-related ontologies and its ultimate form\nwill comprise components for (i) text categorization, (ii) named entity\nrecognition, (iii) temporal expression extraction, (iv) event extraction, (v)\nsocial network construction, (vi) sentiment analysis, (vii) information fusion\nand summarization, (viii) media interlinking, and (ix) Web-based information\nretrieval and visualization. Wits its diverse data sources, automatic text\nprocessing capabilities, and presentation facilities open for public use;\nEneMonIE will be an important source of distilled and concise information for\ndecision-makers including energy generation, transmission, and distribution\nsystem operators, energy research centres, related investors and entrepreneurs\nas well as for academicians, students, other individuals interested in the pace\nof energy events and technologies.", "published": "2022-01-05 12:07:32", "link": "http://arxiv.org/abs/2201.01559v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "All You Need In Sign Language Production", "abstract": "Sign Language is the dominant form of communication language used in the deaf\nand hearing-impaired community. To make an easy and mutual communication\nbetween the hearing-impaired and the hearing communities, building a robust\nsystem capable of translating the spoken language into sign language and vice\nversa is fundamental. To this end, sign language recognition and production are\ntwo necessary parts for making such a two-way system. Sign language recognition\nand production need to cope with some critical challenges. In this survey, we\nreview recent advances in Sign Language Production (SLP) and related areas\nusing deep learning. To have more realistic perspectives to sign language, we\npresent an introduction to the Deaf culture, Deaf centers, psychological\nperspective of sign language, the main differences between spoken language and\nsign language. Furthermore, we present the fundamental components of a\nbi-directional sign language translation system, discussing the main challenges\nin this area. Also, the backbone architectures and methods in SLP are briefly\nintroduced and the proposed taxonomy on SLP is presented. Finally, a general\nframework for SLP and performance evaluation, and also a discussion on the\nrecent developments, advantages, and limitations in SLP, commenting on possible\nlines for future research are presented.", "published": "2022-01-05 13:45:09", "link": "http://arxiv.org/abs/2201.01609v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "SMDT: Selective Memory-Augmented Neural Document Translation", "abstract": "Existing document-level neural machine translation (NMT) models have\nsufficiently explored different context settings to provide guidance for target\ngeneration. However, little attention is paid to inaugurate more diverse\ncontext for abundant context information. In this paper, we propose a Selective\nMemory-augmented Neural Document Translation model to deal with documents\ncontaining large hypothesis space of the context. Specifically, we retrieve\nsimilar bilingual sentence pairs from the training corpus to augment global\ncontext and then extend the two-stream attention model with selective mechanism\nto capture local context and diverse global contexts. This unified approach\nallows our model to be trained elegantly on three publicly document-level\nmachine translation datasets and significantly outperforms previous\ndocument-level NMT models.", "published": "2022-01-05 14:23:30", "link": "http://arxiv.org/abs/2201.01631v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi Document Reading Comprehension", "abstract": "Reading Comprehension (RC) is a task of answering a question from a given\npassage or a set of passages. In the case of multiple passages, the task is to\nfind the best possible answer to the question. Recent trials and experiments in\nthe field of Natural Language Processing (NLP) have proved that machines can be\nprovided with the ability to not only process the text in the passage and\nunderstand its meaning to answer the question from the passage, but also can\nsurpass the Human Performance on many datasets such as Standford's Question\nAnswering Dataset (SQuAD). This paper presents a study on Reading Comprehension\nand its evolution in Natural Language Processing over the past few decades. We\nshall also study how the task of Single Document Reading Comprehension acts as\na building block for our Multi-Document Reading Comprehension System. In the\nlatter half of the paper, we'll be studying about a recently proposed model for\nMulti-Document Reading Comprehension - RE3QA that is comprised of a Reader,\nRetriever, and a Re-ranker based network to fetch the best possible answer from\na given set of passages.", "published": "2022-01-05 16:54:48", "link": "http://arxiv.org/abs/2201.01706v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Atomized Search Length: Beyond User Models", "abstract": "We argue that current IR metrics, modeled on optimizing user experience,\nmeasure too narrow a portion of the IR space. If IR systems are weak, these\nmetrics undersample or completely filter out the deeper documents that need\nimprovement. If IR systems are relatively strong, these metrics undersample\ndeeper relevant documents that could underpin even stronger IR systems, ones\nthat could present content from tens or hundreds of relevant documents in a\nuser-digestible hierarchy or text summary. We reanalyze over 70 TREC tracks\nfrom the past 28 years, showing that roughly half undersample top ranked\ndocuments and nearly all undersample tail documents. We show that in the 2020\nDeep Learning tracks, neural systems were actually near-optimal at top-ranked\ndocuments, compared to only modest gains over BM25 on tail documents. Our\nanalysis is based on a simple new systems-oriented metric, 'atomized search\nlength', which is capable of accurately and evenly measuring all relevant\ndocuments at any depth.", "published": "2022-01-05 18:10:30", "link": "http://arxiv.org/abs/2201.01745v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "RabindraNet, Creating Literary Works in the Style of Rabindranath Tagore", "abstract": "Bengali literature has a rich history of hundreds of years with luminary\nfigures such as Rabindranath Tagore and Kazi Nazrul Islam. However, analytical\nworks involving the most recent advancements in NLP have barely scratched the\nsurface utilizing the enormous volume of the collected works from the writers\nof the language. In order to bring attention to the analytical study involving\nthe works of Bengali writers and spearhead the text generation endeavours in\nthe style of existing literature, we are introducing RabindraNet, a character\nlevel RNN model with stacked-LSTM layers trained on the works of Rabindranath\nTagore to produce literary works in his style for multiple genres. We created\nan extensive dataset as well by compiling the digitized works of Rabindranath\nTagore from authentic online sources and published as open source dataset on\ndata science platform Kaggle.", "published": "2022-01-05 16:23:37", "link": "http://arxiv.org/abs/2202.00481v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Auto-ABSA: Cross-Domain Aspect Detection and Sentiment Analysis Using\n  Auxiliary Sentences", "abstract": "After transformer is proposed, lots of pre-trained language models have been\ncome up with and sentiment analysis (SA) task has been improved. In this paper,\nwe proposed a method that uses an auxiliary sentence about aspects that the\nsentence contains to help sentiment prediction. The first is aspect detection,\nwhich uses a multi-aspects detection model to predict all aspects that the\nsentence has. Combining the predicted aspects and the original sentence as\nSentiment Analysis (SA) model's input. The second is to do out-of-domain\naspect-based sentiment analysis(ABSA), train sentiment classification model\nwith one kind of dataset and validate it with another kind of dataset. Finally,\nwe created two baselines, they use no aspect and all aspects as sentiment\nclassification model's input, respectively. Compare two baselines performance\nto our method, found that our method really makes sense.", "published": "2022-01-05 04:23:29", "link": "http://arxiv.org/abs/2202.00484v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mining Adverse Drug Reactions from Unstructured Mediums at Scale", "abstract": "Adverse drug reactions / events (ADR/ADE) have a major impact on patient\nhealth and health care costs. Detecting ADR's as early as possible and sharing\nthem with regulators, pharma companies, and healthcare providers can prevent\nmorbidity and save many lives. While most ADR's are not reported via formal\nchannels, they are often documented in a variety of unstructured conversations\nsuch as social media posts by patients, customer support call transcripts, or\nCRM notes of meetings between healthcare providers and pharma sales reps. In\nthis paper, we propose a natural language processing (NLP) solution that\ndetects ADR's in such unstructured free-text conversations, which improves on\nprevious work in three ways. First, a new Named Entity Recognition (NER) model\nobtains new state-of-the-art accuracy for ADR and Drug entity extraction on the\nADE, CADEC, and SMM4H benchmark datasets (91.75%, 78.76%, and 83.41% F1 scores\nrespectively). Second, two new Relation Extraction (RE) models are introduced -\none based on BioBERT while the other utilizing crafted features over a Fully\nConnected Neural Network (FCNN) - are shown to perform on par with existing\nstate-of-the-art models, and outperform them when trained with a supplementary\nclinician-annotated RE dataset. Third, a new text classification model, for\ndeciding if a conversation includes an ADR, obtains new state-of-the-art\naccuracy on the CADEC dataset (86.69% F1 score). The complete solution is\nimplemented as a unified NLP pipeline in a production-grade library built on\ntop of Apache Spark, making it natively scalable and able to process millions\nof batch or streaming records on commodity clusters.", "published": "2022-01-05 01:52:42", "link": "http://arxiv.org/abs/2201.01405v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; I.2.4; I.2.11"], "primary_category": "cs.CL"}
{"title": "Debiased Learning from Naturally Imbalanced Pseudo-Labels", "abstract": "Pseudo-labels are confident predictions made on unlabeled target data by a\nclassifier trained on labeled source data. They are widely used for adapting a\nmodel to unlabeled data, e.g., in a semi-supervised learning setting.\n  Our key insight is that pseudo-labels are naturally imbalanced due to\nintrinsic data similarity, even when a model is trained on balanced source data\nand evaluated on balanced target data. If we address this previously unknown\nimbalanced classification problem arising from pseudo-labels instead of\nground-truth training labels, we could remove model biases towards false\nmajorities created by pseudo-labels.\n  We propose a novel and effective debiased learning method with pseudo-labels,\nbased on counterfactual reasoning and adaptive margins: The former removes the\nclassifier response bias, whereas the latter adjusts the margin of each class\naccording to the imbalance of pseudo-labels. Validated by extensive\nexperimentation, our simple debiased learning delivers significant accuracy\ngains over the state-of-the-art on ImageNet-1K: 26% for semi-supervised\nlearning with 0.2% annotations and 9% for zero-shot learning. Our code is\navailable at: https://github.com/frank-xwang/debiased-pseudo-labeling.", "published": "2022-01-05 07:40:24", "link": "http://arxiv.org/abs/2201.01490v2", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Comparison of biomedical relationship extraction methods and models for\n  knowledge graph creation", "abstract": "Biomedical research is growing at such an exponential pace that scientists,\nresearchers, and practitioners are no more able to cope with the amount of\npublished literature in the domain. The knowledge presented in the literature\nneeds to be systematized in such a way that claims and hypotheses can be easily\nfound, accessed, and validated. Knowledge graphs can provide such a framework\nfor semantic knowledge representation from literature. However, in order to\nbuild a knowledge graph, it is necessary to extract knowledge as relationships\nbetween biomedical entities and normalize both entities and relationship types.\nIn this paper, we present and compare few rule-based and machine learning-based\n(Naive Bayes, Random Forests as examples of traditional machine learning\nmethods and DistilBERT, PubMedBERT, T5 and SciFive-based models as examples of\nmodern deep learning transformers) methods for scalable relationship extraction\nfrom biomedical literature, and for the integration into the knowledge graphs.\nWe examine how resilient are these various methods to unbalanced and fairly\nsmall datasets. Our experiments show that transformer-based models handle well\nboth small (due to pre-training on a large dataset) and unbalanced datasets.\nThe best performing model was the PubMedBERT-based model fine-tuned on balanced\ndata, with a reported F1-score of 0.92. DistilBERT-based model followed with\nF1-score of 0.89, performing faster and with lower resource requirements.\nBERT-based models performed better then T5-based generative models.", "published": "2022-01-05 15:09:33", "link": "http://arxiv.org/abs/2201.01647v4", "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.LG", "E.2; I.7"], "primary_category": "cs.AI"}
{"title": "Does Entity Abstraction Help Generative Transformers Reason?", "abstract": "We study the utility of incorporating entity type abstractions into\npre-trained Transformers and test these methods on four NLP tasks requiring\ndifferent forms of logical reasoning: (1) compositional language understanding\nwith text-based relational reasoning (CLUTRR), (2) abductive reasoning\n(ProofWriter), (3) multi-hop question answering (HotpotQA), and (4)\nconversational question answering (CoQA). We propose and empirically explore\nthree ways to add such abstraction: (i) as additional input embeddings, (ii) as\na separate sequence to encode, and (iii) as an auxiliary prediction task for\nthe model. Overall, our analysis demonstrates that models with abstract entity\nknowledge performs better than without it. The best abstraction aware models\nachieved an overall accuracy of 88.8% and 91.8% compared to the baseline model\nachieving 62.9% and 89.8% on CLUTRR and ProofWriter respectively. However, for\nHotpotQA and CoQA, we find that F1 scores improve by only 0.5% on average. Our\nresults suggest that the benefit of explicit abstraction is significant in\nformally defined logical reasoning settings requiring many reasoning hops, but\npoint to the notion that it is less beneficial for NLP tasks having less formal\nlogical structure.", "published": "2022-01-05 19:00:53", "link": "http://arxiv.org/abs/2201.01787v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Formal Analysis of Art: Proxy Learning of Visual Concepts from Style\n  Through Language Models", "abstract": "We present a machine learning system that can quantify fine art paintings\nwith a set of visual elements and principles of art. This formal analysis is\nfundamental for understanding art, but developing such a system is challenging.\nPaintings have high visual complexities, but it is also difficult to collect\nenough training data with direct labels. To resolve these practical\nlimitations, we introduce a novel mechanism, called proxy learning, which\nlearns visual concepts in paintings though their general relation to styles.\nThis framework does not require any visual annotation, but only uses style\nlabels and a general relationship between visual concepts and style. In this\npaper, we propose a novel proxy model and reformulate four pre-existing methods\nin the context of proxy learning. Through quantitative and qualitative\ncomparison, we evaluate these methods and compare their effectiveness in\nquantifying the artistic visual concepts, where the general relationship is\nestimated by language models; GloVe or BERT. The language modeling is a\npractical and scalable solution requiring no labeling, but it is inevitably\nimperfect. We demonstrate how the new proxy model is robust to the\nimperfection, while the other models are sensitively affected by it.", "published": "2022-01-05 21:03:29", "link": "http://arxiv.org/abs/2201.01819v1", "categories": ["cs.LG", "cs.CL", "cs.CV", "I.2.6; I.2.7; I.2.10; J.5"], "primary_category": "cs.LG"}
{"title": "Frame Shift Prediction", "abstract": "Frame shift is a cross-linguistic phenomenon in translation which results in\ncorresponding pairs of linguistic material evoking different frames. The\nability to predict frame shifts enables automatic creation of multilingual\nFrameNets through annotation projection. Here, we propose the Frame Shift\nPrediction task and demonstrate that graph attention networks, combined with\nauxiliary training, can learn cross-linguistic frame-to-frame correspondence\nand predict frame shifts.", "published": "2022-01-05 22:03:06", "link": "http://arxiv.org/abs/2201.01837v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Maximizing a Perceptual Sweet Spot", "abstract": "The sweet spot can be interpreted as the region where acoustic sources create\na spatial auditory illusion. We study the problem of maximizing this sweet spot\nwhen reproducing a desired sound wave using an array of loudspeakers. To\nachieve this, we introduce a theoretical framework for spatial sound perception\nthat can be used to define a sweet spot, and we develop a method that aims to\ngenerate a sound wave that directly maximizes the sweet spot defined by a model\nwithin this framework. Our method aims to incorporate perceptual principles\nfrom the onset and is flexible: while it imposes little to no constraints on\nthe regions of interest, the arrangement of loudspeakers or their radiation\npattern, it allows for audio perception models that include state-of-the-art\nmonaural perceptual models. Proof-of-concept experiments show that our method\noutperforms state-of-the-art methods when comparing them in terms of their\nlocalization and coloration properties.", "published": "2022-01-05 05:26:53", "link": "http://arxiv.org/abs/2201.01461v2", "categories": ["eess.AS", "cs.SD", "q-bio.NC"], "primary_category": "eess.AS"}
{"title": "Formant Tracking Using Quasi-Closed Phase Forward-Backward Linear\n  Prediction Analysis and Deep Neural Networks", "abstract": "Formant tracking is investigated in this study by using trackers based on\ndynamic programming (DP) and deep neural nets (DNNs). Using the DP approach,\nsix formant estimation methods were first compared. The six methods include\nlinear prediction (LP) algorithms, weighted LP algorithms and the recently\ndeveloped quasi-closed phase forward-backward (QCP-FB) method. QCP-FB gave the\nbest performance in the comparison. Therefore, a novel formant tracking\napproach, which combines benefits of deep learning and signal processing based\non QCP-FB, was proposed. In this approach, the formants predicted by a\nDNN-based tracker from a speech frame are refined using the peaks of the\nall-pole spectrum computed by QCP-FB from the same frame. Results show that the\nproposed DNN-based tracker performed better both in detection rate and\nestimation error for the lowest three formants compared to reference formant\ntrackers. Compared to the popular Wavesurfer, for example, the proposed tracker\ngave a reduction of 29%, 48% and 35% in the estimation error for the lowest\nthree formants, respectively.", "published": "2022-01-05 10:27:07", "link": "http://arxiv.org/abs/2201.01525v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Using Deep Learning with Large Aggregated Datasets for COVID-19\n  Classification from Cough", "abstract": "The Covid-19 pandemic has been one of the most devastating events in recent\nhistory, claiming the lives of more than 5 million people worldwide. Even with\nthe worldwide distribution of vaccines, there is an apparent need for\naffordable, reliable, and accessible screening techniques to serve parts of the\nWorld that do not have access to Western medicine. Artificial Intelligence can\nprovide a solution utilizing cough sounds as a primary screening mode for\nCOVID-19 diagnosis. This paper presents multiple models that have achieved\nrelatively respectable performance on the largest evaluation dataset currently\npresented in academic literature. Through investigation of a self-supervised\nlearning model (Area under the ROC curve, AUC = 0.807) and a convolutional\nnerual network (CNN) model (AUC = 0.802), we observe the possibility of model\nbias with limited datasets. Moreover, we observe that performance increases\nwith training data size, showing the need for the worldwide collection of data\nto help combat the Covid-19 pandemic with non-traditional means.", "published": "2022-01-05 15:49:58", "link": "http://arxiv.org/abs/2201.01669v3", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Robust Self-Supervised Audio-Visual Speech Recognition", "abstract": "Audio-based automatic speech recognition (ASR) degrades significantly in\nnoisy environments and is particularly vulnerable to interfering speech, as the\nmodel cannot determine which speaker to transcribe. Audio-visual speech\nrecognition (AVSR) systems improve robustness by complementing the audio stream\nwith the visual information that is invariant to noise and helps the model\nfocus on the desired speaker. However, previous AVSR work focused solely on the\nsupervised learning setup; hence the progress was hindered by the amount of\nlabeled data available. In this work, we present a self-supervised AVSR\nframework built upon Audio-Visual HuBERT (AV-HuBERT), a state-of-the-art\naudio-visual speech representation learning model. On the largest available\nAVSR benchmark dataset LRS3, our approach outperforms prior state-of-the-art by\n~50% (28.0% vs. 14.1%) using less than 10% of labeled data (433hr vs. 30hr) in\nthe presence of babble noise, while reducing the WER of an audio-based model by\nover 75% (25.8% vs. 5.8%) on average.", "published": "2022-01-05 18:50:50", "link": "http://arxiv.org/abs/2201.01763v3", "categories": ["cs.SD", "cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Self-Supervised Beat Tracking in Musical Signals with Polyphonic\n  Contrastive Learning", "abstract": "Annotating musical beats is a very long and tedious process. In order to\ncombat this problem, we present a new self-supervised learning pretext task for\nbeat tracking and downbeat estimation. This task makes use of Spleeter, an\naudio source separation model, to separate a song's drums from the rest of its\nsignal. The first set of signals are used as positives, and by extension\nnegatives, for contrastive learning pre-training. The drum-less signals, on the\nother hand, are used as anchors. When pre-training a fully-convolutional and\nrecurrent model using this pretext task, an onset function is learned. In some\ncases, this function is found to be mapped to periodic elements in a song. We\nfind that pre-trained models outperform randomly initialized models when a beat\ntracking training set is extremely small (less than 10 examples). When this is\nnot the case, pre-training leads to a learning speed-up that causes the model\nto overfit to the training set. More generally, this work defines new\nperspectives in the realm of musical self-supervised learning. It is notably\none of the first works to use audio source separation as a fundamental\ncomponent of self-supervision.", "published": "2022-01-05 13:24:39", "link": "http://arxiv.org/abs/2201.01771v2", "categories": ["cs.SD", "cs.LG", "eess.AS", "68T07 (Primary) 00A65, 68T10 (Secondary)", "I.5.4"], "primary_category": "cs.SD"}
{"title": "Learning Audio-Visual Speech Representation by Masked Multimodal Cluster\n  Prediction", "abstract": "Video recordings of speech contain correlated audio and visual information,\nproviding a strong signal for speech representation learning from the speaker's\nlip movements and the produced sound. We introduce Audio-Visual Hidden Unit\nBERT (AV-HuBERT), a self-supervised representation learning framework for\naudio-visual speech, which masks multi-stream video input and predicts\nautomatically discovered and iteratively refined multimodal hidden units.\nAV-HuBERT learns powerful audio-visual speech representation benefiting both\nlip-reading and automatic speech recognition. On the largest public lip-reading\nbenchmark LRS3 (433 hours), AV-HuBERT achieves 32.5% WER with only 30 hours of\nlabeled data, outperforming the former state-of-the-art approach (33.6%)\ntrained with a thousand times more transcribed video data (31K hours). The\nlip-reading WER is further reduced to 26.9% when using all 433 hours of labeled\ndata from LRS3 and combined with self-training. Using our audio-visual\nrepresentation on the same benchmark for audio-only speech recognition leads to\na 40% relative WER reduction over the state-of-the-art performance (1.3% vs\n2.3%). Our code and models are available at\nhttps://github.com/facebookresearch/av_hubert", "published": "2022-01-05 17:40:45", "link": "http://arxiv.org/abs/2201.02184v2", "categories": ["eess.AS", "cs.CV", "cs.SD"], "primary_category": "eess.AS"}
