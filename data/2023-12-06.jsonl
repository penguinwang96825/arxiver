{"title": "Detecting Rumor Veracity with Only Textual Information by Double-Channel\n  Structure", "abstract": "Kyle (1985) proposes two types of rumors: informed rumors which are based on\nsome private information and uninformed rumors which are not based on any\ninformation (i.e. bluffing). Also, prior studies find that when people have\ncredible source of information, they are likely to use a more confident textual\ntone in their spreading of rumors. Motivated by these theoretical findings, we\npropose a double-channel structure to determine the ex-ante veracity of rumors\non social media. Our ultimate goal is to classify each rumor into true, false,\nor unverifiable category. We first assign each text into either certain\n(informed rumor) or uncertain (uninformed rumor) category. Then, we apply lie\ndetection algorithm to informed rumors and thread-reply agreement detection\nalgorithm to uninformed rumors. Using the dataset of SemEval 2019 Task 7, which\nrequires ex-ante threefold classification (true, false, or unverifiable) of\nsocial media rumors, our model yields a macro-F1 score of 0.4027, outperforming\nall the baseline models and the second-place winner (Gorrell et al., 2019).\nFurthermore, we empirically validate that the double-channel structure\noutperforms single-channel structures which use either lie detection or\nagreement detection algorithm to all posts.", "published": "2023-12-06 00:08:44", "link": "http://arxiv.org/abs/2312.03195v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Topic and genre in dialogue", "abstract": "In this paper we argue that topic plays a fundamental role in conversations,\nand that the concept is needed in addition to that of genre to define\ninteractions. In particular, the concepts of genre and topic need to be\nseparated and orthogonally defined. This would enable modular, reliable and\ncontrollable flexible-domain dialogue systems.", "published": "2023-12-06 08:33:51", "link": "http://arxiv.org/abs/2312.03342v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KhabarChin: Automatic Detection of Important News in the Persian\n  Language", "abstract": "Being aware of important news is crucial for staying informed and making\nwell-informed decisions efficiently. Natural Language Processing (NLP)\napproaches can significantly automate this process. This paper introduces the\ndetection of important news, in a previously unexplored area, and presents a\nnew benchmarking dataset (Khabarchin) for detecting important news in the\nPersian language. We define important news articles as those deemed significant\nfor a considerable portion of society, capable of influencing their mindset or\ndecision-making. The news articles are obtained from seven different prominent\nPersian news agencies, resulting in the annotation of 7,869 samples and the\ncreation of the dataset. Two challenges of high disagreement and imbalance\nbetween classes were faced, and solutions were provided for them. We also\npropose several learning-based models, ranging from conventional machine\nlearning to state-of-the-art transformer models, to tackle this task.\nFurthermore, we introduce the second task of important sentence detection in\nnews articles, as they often come with a significant contextual length that\nmakes it challenging for readers to identify important information. We identify\nthese sentences in a weakly supervised manner.", "published": "2023-12-06 09:01:21", "link": "http://arxiv.org/abs/2312.03361v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lazy-k: Decoding for Constrained Token Classification", "abstract": "We explore the possibility of improving probabilistic models in structured\nprediction. Specifically, we combine the models with constrained decoding\napproaches in the context of token classification for information extraction.\nThe decoding methods search for constraint-satisfying label-assignments while\nmaximizing the total probability. To do this, we evaluate several existing\napproaches, as well as propose a novel decoding method called Lazy-$k$. Our\nfindings demonstrate that constrained decoding approaches can significantly\nimprove the models' performances, especially when using smaller models. The\nLazy-$k$ approach allows for more flexibility between decoding time and\naccuracy. The code for using Lazy-$k$ decoding can be found here:\nhttps://github.com/ArthurDevNL/lazyk.", "published": "2023-12-06 09:08:32", "link": "http://arxiv.org/abs/2312.03367v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Text-to-Text Model for Multilingual Offensive Language Identification", "abstract": "The ubiquity of offensive content on social media is a growing cause for\nconcern among companies and government organizations. Recently,\ntransformer-based models such as BERT, XLNET, and XLM-R have achieved\nstate-of-the-art performance in detecting various forms of offensive content\n(e.g. hate speech, cyberbullying, and cyberaggression). However, the majority\nof these models are limited in their capabilities due to their encoder-only\narchitecture, which restricts the number and types of labels in downstream\ntasks. Addressing these limitations, this study presents the first pre-trained\nmodel with encoder-decoder architecture for offensive language identification\nwith text-to-text transformers (T5) trained on two large offensive language\nidentification datasets; SOLID and CCTK. We investigate the effectiveness of\ncombining two datasets and selecting an optimal threshold in semi-supervised\ninstances in SOLID in the T5 retraining step. Our pre-trained T5 model\noutperforms other transformer-based models fine-tuned for offensive language\ndetection, such as fBERT and HateBERT, in multiple English benchmarks.\nFollowing a similar approach, we also train the first multilingual pre-trained\nmodel for offensive language identification using mT5 and evaluate its\nperformance on a set of six different languages (German, Hindi, Korean,\nMarathi, Sinhala, and Spanish). The results demonstrate that this multilingual\nmodel achieves a new state-of-the-art on all the above datasets, showing its\nusefulness in multilingual scenarios. Our proposed T5-based models will be made\nfreely available to the community.", "published": "2023-12-06 09:37:27", "link": "http://arxiv.org/abs/2312.03379v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Think from Words(TFW): Initiating Human-Like Cognition in Large Language\n  Models Through Think from Words for Japanese Text-level Classification", "abstract": "The proliferation of Large Language Models (LLMs) has spurred extensive\nresearch into LLM-related Prompt investigations, such as Instruction Learning\n(IL), In-context Learning (ICL), and Chain-of-Thought (CoT). These approaches\naim to improve LLMs' responses by enabling them to provide concise statements\nor examples for deeper contemplation when addressing questions. However,\nindependent thinking by LLMs can introduce variability in their thought\nprocesses, leading to potential inaccuracies. In response, our study seeks to\nbridge the gap between LLM and human-like thinking processes, recognizing that\ntext comprehension begins with understanding individual words. To tackle this\nchallenge, we have expanded the CoT method to cater to a specific domain. Our\napproach, known as \"Think from Words\" (TFW), initiates the comprehension\nprocess at the word level and then extends it to encompass the entire text. We\nalso propose \"TFW with Extra word-level information\" (TFW Extra), augmenting\ncomprehension with additional word-level data. To assess our methods, we employ\ntext classification on six Japanese datasets comprising text-level and\nword-level elements. Our findings not only validate the effectiveness of TFW\nbut also shed light on the impact of various word-level information types on\nLLMs' text comprehension, offering insights into their potential to cause\nmisinterpretations and errors in the overall comprehension of the final text.", "published": "2023-12-06 12:34:46", "link": "http://arxiv.org/abs/2312.03458v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AMR Parsing is Far from Solved: GrAPES, the Granular AMR Parsing\n  Evaluation Suite", "abstract": "We present the Granular AMR Parsing Evaluation Suite (GrAPES), a challenge\nset for Abstract Meaning Representation (AMR) parsing with accompanying\nevaluation metrics. AMR parsers now obtain high scores on the standard AMR\nevaluation metric Smatch, close to or even above reported inter-annotator\nagreement. But that does not mean that AMR parsing is solved; in fact, human\nevaluation in previous work indicates that current parsers still quite\nfrequently make errors on node labels or graph structure that substantially\ndistort sentence meaning. Here, we provide an evaluation suite that tests AMR\nparsers on a range of phenomena of practical, technical, and linguistic\ninterest. Our 36 categories range from seen and unseen labels, to structural\ngeneralization, to coreference. GrAPES reveals in depth the abilities and\nshortcomings of current AMR parsers.", "published": "2023-12-06 13:19:56", "link": "http://arxiv.org/abs/2312.03480v1", "categories": ["cs.CL", "J.5"], "primary_category": "cs.CL"}
{"title": "Sig-Networks Toolkit: Signature Networks for Longitudinal Language\n  Modelling", "abstract": "We present an open-source, pip installable toolkit, Sig-Networks, the first\nof its kind for longitudinal language modelling. A central focus is the\nincorporation of Signature-based Neural Network models, which have recently\nshown success in temporal tasks. We apply and extend published research\nproviding a full suite of signature-based models. Their components can be used\nas PyTorch building blocks in future architectures. Sig-Networks enables\ntask-agnostic dataset plug-in, seamless pre-processing for sequential data,\nparameter flexibility, automated tuning across a range of models. We examine\nsignature networks under three different NLP tasks of varying temporal\ngranularity: counselling conversations, rumour stance switch and mood changes\nin social media threads, showing SOTA performance in all three, and provide\nguidance for future tasks. We release the Toolkit as a PyTorch package with an\nintroductory video, Git repositories for preprocessing and modelling including\nsample notebooks on the modeled NLP tasks.", "published": "2023-12-06 14:34:30", "link": "http://arxiv.org/abs/2312.03523v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "XAIQA: Explainer-Based Data Augmentation for Extractive Question\n  Answering", "abstract": "Extractive question answering (QA) systems can enable physicians and\nresearchers to query medical records, a foundational capability for designing\nclinical studies and understanding patient medical history. However, building\nthese systems typically requires expert-annotated QA pairs. Large language\nmodels (LLMs), which can perform extractive QA, depend on high quality data in\ntheir prompts, specialized for the application domain. We introduce a novel\napproach, XAIQA, for generating synthetic QA pairs at scale from data naturally\navailable in electronic health records. Our method uses the idea of a\nclassification model explainer to generate questions and answers about medical\nconcepts corresponding to medical codes. In an expert evaluation with two\nphysicians, our method identifies $2.2\\times$ more semantic matches and\n$3.8\\times$ more clinical abbreviations than two popular approaches that use\nsentence transformers to create QA pairs. In an ML evaluation, adding our QA\npairs improves performance of GPT-4 as an extractive QA model, including on\ndifficult questions. In both the expert and ML evaluations, we examine\ntrade-offs between our method and sentence transformers for QA pair generation\ndepending on question difficulty.", "published": "2023-12-06 15:59:06", "link": "http://arxiv.org/abs/2312.03567v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Improving Bias Mitigation through Bias Experts in Natural Language\n  Understanding", "abstract": "Biases in the dataset often enable the model to achieve high performance on\nin-distribution data, while poorly performing on out-of-distribution data. To\nmitigate the detrimental effect of the bias on the networks, previous works\nhave proposed debiasing methods that down-weight the biased examples identified\nby an auxiliary model, which is trained with explicit bias labels. However,\nfinding a type of bias in datasets is a costly process. Therefore, recent\nstudies have attempted to make the auxiliary model biased without the guidance\n(or annotation) of bias labels, by constraining the model's training\nenvironment or the capability of the model itself. Despite the promising\ndebiasing results of recent works, the multi-class learning objective, which\nhas been naively used to train the auxiliary model, may harm the bias\nmitigation effect due to its regularization effect and competitive nature\nacross classes. As an alternative, we propose a new debiasing framework that\nintroduces binary classifiers between the auxiliary model and the main model,\ncoined bias experts. Specifically, each bias expert is trained on a binary\nclassification task derived from the multi-class classification task via the\nOne-vs-Rest approach. Experimental results demonstrate that our proposed\nstrategy improves the bias identification ability of the auxiliary model.\nConsequently, our debiased model consistently outperforms the state-of-the-art\non various challenge datasets.", "published": "2023-12-06 16:15:00", "link": "http://arxiv.org/abs/2312.03577v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating and Mitigating Discrimination in Language Model Decisions", "abstract": "As language models (LMs) advance, interest is growing in applying them to\nhigh-stakes societal decisions, such as determining financing or housing\neligibility. However, their potential for discrimination in such contexts\nraises ethical concerns, motivating the need for better methods to evaluate\nthese risks. We present a method for proactively evaluating the potential\ndiscriminatory impact of LMs in a wide range of use cases, including\nhypothetical use cases where they have not yet been deployed. Specifically, we\nuse an LM to generate a wide array of potential prompts that decision-makers\nmay input into an LM, spanning 70 diverse decision scenarios across society,\nand systematically vary the demographic information in each prompt. Applying\nthis methodology reveals patterns of both positive and negative discrimination\nin the Claude 2.0 model in select settings when no interventions are applied.\nWhile we do not endorse or permit the use of language models to make automated\ndecisions for the high-risk use cases we study, we demonstrate techniques to\nsignificantly decrease both positive and negative discrimination through\ncareful prompt engineering, providing pathways toward safer deployment in use\ncases where they may be appropriate. Our work enables developers and\npolicymakers to anticipate, measure, and address discrimination as language\nmodel capabilities and applications continue to expand. We release our dataset\nand prompts at https://huggingface.co/datasets/Anthropic/discrim-eval", "published": "2023-12-06 18:53:01", "link": "http://arxiv.org/abs/2312.03689v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PROMISE: A Framework for Developing Complex Conversational Interactions\n  (Technical Report)", "abstract": "The advent of increasingly powerful language models has raised expectations\nfor language-based interactions. However, controlling these models is a\nchallenge, emphasizing the need to be able to investigate the feasibility and\nvalue of their application. We present PROMISE, a framework that facilitates\nthe development of complex language-based interactions with information\nsystems. Its use of state machine modeling concepts enables model-driven,\ndynamic prompt orchestration across hierarchically nested states and\ntransitions. This improves the control of the behavior of language models and\nthus enables their effective and efficient use. In this technical report we\nshow the benefits of PROMISE in the context of application scenarios within\nhealth information systems and demonstrate its ability to handle complex\ninteractions. We also include code examples and present default user interfaces\navailable as part of PROMISE.", "published": "2023-12-06 18:59:11", "link": "http://arxiv.org/abs/2312.03699v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparative Analysis of Multilingual Text Classification &\n  Identification through Deep Learning and Embedding Visualization", "abstract": "This research conducts a comparative study on multilingual text\nclassification methods, utilizing deep learning and embedding visualization.\nThe study employs LangDetect, LangId, FastText, and Sentence Transformer on a\ndataset encompassing 17 languages. It explores dimensionality's impact on\nclustering, revealing FastText's clearer clustering in 2D visualization due to\nits extensive multilingual corpus training. Notably, the FastText multi-layer\nperceptron model achieved remarkable accuracy, precision, recall, and F1 score,\noutperforming the Sentence Transformer model. The study underscores the\neffectiveness of these techniques in multilingual text classification,\nemphasizing the importance of large multilingual corpora for training\nembeddings. It lays the groundwork for future research and assists\npractitioners in developing language detection and classification systems.\nAdditionally, it includes the comparison of multi-layer perceptron, LSTM, and\nConvolution models for classification.", "published": "2023-12-06 12:03:27", "link": "http://arxiv.org/abs/2312.03789v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting the Optimality of Word Lengths", "abstract": "Zipf (1935) posited that wordforms are optimized to minimize utterances'\ncommunicative costs. Under the assumption that cost is given by an utterance's\nlength, he supported this claim by showing that words' lengths are inversely\ncorrelated with their frequencies. Communicative cost, however, can be\noperationalized in different ways. Piantadosi et al. (2011) claim that cost\nshould be measured as the distance between an utterance's information rate and\nchannel capacity, which we dub the channel capacity hypothesis (CCH) here.\nFollowing this logic, they then proposed that a word's length should be\nproportional to the expected value of its surprisal (negative log-probability\nin context). In this work, we show that Piantadosi et al.'s derivation does not\nminimize CCH's cost, but rather a lower bound, which we term CCH-lower. We\npropose a novel derivation, suggesting an improved way to minimize CCH's cost.\nUnder this method, we find that a language's word lengths should instead be\nproportional to the surprisal's expectation plus its variance-to-mean ratio.\nExperimentally, we compare these three communicative cost functions: Zipf's,\nCCH-lower , and CCH. Across 13 languages and several experimental settings, we\nfind that length is better predicted by frequency than either of the other\nhypotheses. In fact, when surprisal's expectation, or expectation plus\nvariance-to-mean ratio, is estimated using better language models, it leads to\nworse word length predictions. We take these results as evidence that Zipf's\nlongstanding hypothesis holds.", "published": "2023-12-06 20:41:47", "link": "http://arxiv.org/abs/2312.03897v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Collaboration or Corporate Capture? Quantifying NLP's Reliance on\n  Industry Artifacts and Contributions", "abstract": "Impressive performance of pre-trained models has garnered public attention\nand made news headlines in recent years. Almost always, these models are\nproduced by or in collaboration with industry. Using them is critical for\ncompeting on natural language processing (NLP) benchmarks and correspondingly\nto stay relevant in NLP research. We surveyed 100 papers published at EMNLP\n2022 to determine the degree to which researchers rely on industry models,\nother artifacts, and contributions to publish in prestigious NLP venues and\nfound that the ratio of their citation is at least three times greater than\nwhat would be expected. Our work serves as a scaffold to enable future\nresearchers to more accurately address whether: 1) Collaboration with industry\nis still collaboration in the absence of an alternative or 2) if NLP inquiry\nhas been captured by the motivations and research direction of private\ncorporations.", "published": "2023-12-06 21:12:22", "link": "http://arxiv.org/abs/2312.03912v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Model Alignment with Elastic Reset", "abstract": "Finetuning language models with reinforcement learning (RL), e.g. from human\nfeedback (HF), is a prominent method for alignment. But optimizing against a\nreward model can improve on reward while degrading performance in other areas,\na phenomenon known as reward hacking, alignment tax, or language drift. First,\nwe argue that commonly-used test metrics are insufficient and instead measure\nhow different algorithms tradeoff between reward and drift. The standard method\nmodified the reward with a Kullback-Lieber (KL) penalty between the online and\ninitial model. We propose Elastic Reset, a new algorithm that achieves higher\nreward with less drift without explicitly modifying the training objective. We\nperiodically reset the online model to an exponentially moving average (EMA) of\nitself, then reset the EMA model to the initial model. Through the use of an\nEMA, our model recovers quickly after resets and achieves higher reward with\nless drift in the same number of steps. We demonstrate that fine-tuning\nlanguage models with Elastic Reset leads to state-of-the-art performance on a\nsmall scale pivot-translation benchmark, outperforms all baselines in a\nmedium-scale RLHF-like IMDB mock sentiment task and leads to a more performant\nand more aligned technical QA chatbot with LLaMA-7B. Code available at\ngithub.com/mnoukhov/elastic-reset.", "published": "2023-12-06 22:53:34", "link": "http://arxiv.org/abs/2312.07551v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rethinking E-Commerce Search", "abstract": "E-commerce search and recommendation usually operate on structured data such\nas product catalogs and taxonomies. However, creating better search and\nrecommendation systems often requires a large variety of unstructured data\nincluding customer reviews and articles on the web. Traditionally, the solution\nhas always been converting unstructured data into structured data through\ninformation extraction, and conducting search over the structured data.\nHowever, this is a costly approach that often has low quality. In this paper,\nwe envision a solution that does entirely the opposite. Instead of converting\nunstructured data (web pages, customer reviews, etc) to structured data, we\ninstead convert structured data (product inventory, catalogs, taxonomies, etc)\ninto textual data, which can be easily integrated into the text corpus that\ntrains LLMs. Then, search and recommendation can be performed through a Q/A\nmechanism through an LLM instead of using traditional information retrieval\nmethods over structured data.", "published": "2023-12-06 01:15:40", "link": "http://arxiv.org/abs/2312.03217v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Can language agents be alternatives to PPO? A Preliminary Empirical\n  Study On OpenAI Gym", "abstract": "The formidable capacity for zero- or few-shot decision-making in language\nagents encourages us to pose a compelling question: Can language agents be\nalternatives to PPO agents in traditional sequential decision-making tasks? To\ninvestigate this, we first take environments collected in OpenAI Gym as our\ntestbeds and ground them to textual environments that construct the TextGym\nsimulator. This allows for straightforward and efficient comparisons between\nPPO agents and language agents, given the widespread adoption of OpenAI Gym. To\nensure a fair and effective benchmarking, we introduce $5$ levels of scenario\nfor accurate domain-knowledge controlling and a unified RL-inspired framework\nfor language agents. Additionally, we propose an innovative\nexplore-exploit-guided language (EXE) agent to solve tasks within TextGym.\nThrough numerical experiments and ablation studies, we extract valuable\ninsights into the decision-making capabilities of language agents and make a\npreliminary evaluation of their potential to be alternatives to PPO in\nclassical sequential decision-making problems. This paper sheds light on the\nperformance of language agents and paves the way for future research in this\nexciting domain. Our code is publicly available\nat~\\url{https://github.com/mail-ecnu/Text-Gym-Agents}.", "published": "2023-12-06 04:48:26", "link": "http://arxiv.org/abs/2312.03290v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Compressed Context Memory For Online Language Model Interaction", "abstract": "This paper presents a context key/value compression method for Transformer\nlanguage models in online scenarios, where the context continually expands. As\nthe context lengthens, the attention process demands increasing memory and\ncomputations, which in turn reduces the throughput of the language model. To\naddress this challenge, we propose a compressed context memory system that\ncontinually compresses the accumulating attention key/value pairs into a\ncompact memory space, facilitating language model inference in a limited memory\nspace of computing environments. Our compression process involves integrating a\nlightweight conditional LoRA into the language model's forward pass during\ninference, without the need for fine-tuning the model's entire set of weights.\nWe achieve efficient training by modeling the recursive compression process as\na single parallelized forward computation. Through evaluations on conversation,\npersonalization, and multi-task learning, we demonstrate that our approach\nachieves the performance level of a full context model with $5\\times$ smaller\ncontext memory size. We further demonstrate the applicability of our approach\nin a streaming setting with an unlimited context length, outperforming the\nsliding window approach. Codes are available at\nhttps://github.com/snu-mllab/context-memory.", "published": "2023-12-06 10:50:43", "link": "http://arxiv.org/abs/2312.03414v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Exploring Answer Information Methods for Question Generation with\n  Transformers", "abstract": "There has been a lot of work in question generation where different methods\nto provide target answers as input, have been employed. This experimentation\nhas been mostly carried out for RNN based models. We use three different\nmethods and their combinations for incorporating answer information and explore\ntheir effect on several automatic evaluation metrics. The methods that are used\nare answer prompting, using a custom product method using answer embeddings and\nencoder outputs, choosing sentences from the input paragraph that have answer\nrelated information, and using a separate cross-attention attention block in\nthe decoder which attends to the answer. We observe that answer prompting\nwithout any additional modes obtains the best scores across rouge, meteor\nscores. Additionally, we use a custom metric to calculate how many of the\ngenerated questions have the same answer, as the answer which is used to\ngenerate them.", "published": "2023-12-06 13:26:16", "link": "http://arxiv.org/abs/2312.03483v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Holmes: Towards Distributed Training Across Clusters with Heterogeneous\n  NIC Environment", "abstract": "Large language models (LLMs) such as GPT-3, OPT, and LLaMA have demonstrated\nremarkable accuracy in a wide range of tasks. However, training these models\ncan incur significant expenses, often requiring tens of thousands of GPUs for\nmonths of continuous operation. Typically, this training is carried out in\nspecialized GPU clusters equipped with homogeneous high-speed Remote Direct\nMemory Access (RDMA) network interface cards (NICs). The acquisition and\nmaintenance of such dedicated clusters is challenging. Current LLM training\nframeworks, like Megatron-LM and Megatron-DeepSpeed, focus primarily on\noptimizing training within homogeneous cluster settings. In this paper, we\nintroduce Holmes, a training framework for LLMs that employs thoughtfully\ncrafted data and model parallelism strategies over the heterogeneous NIC\nenvironment. Our primary technical contribution lies in a novel scheduling\nmethod that intelligently allocates distinct computational tasklets in LLM\ntraining to specific groups of GPU devices based on the characteristics of\ntheir connected NICs. Furthermore, our proposed framework, utilizing pipeline\nparallel techniques, demonstrates scalability to multiple GPU clusters, even in\nscenarios without high-speed interconnects between nodes in distinct clusters.\nWe conducted comprehensive experiments that involved various scenarios in the\nheterogeneous NIC environment. In most cases, our framework achieves\nperformance levels close to those achievable with homogeneous RDMA-capable\nnetworks (InfiniBand or RoCE), significantly exceeding training efficiency\nwithin the pure Ethernet environment. Additionally, we verified that our\nframework outperforms other mainstream LLM frameworks under heterogeneous NIC\nenvironment in terms of training efficiency and can be seamlessly integrated\nwith them.", "published": "2023-12-06 15:27:26", "link": "http://arxiv.org/abs/2312.03549v4", "categories": ["cs.CL", "cs.DC"], "primary_category": "cs.CL"}
{"title": "Interpretability Illusions in the Generalization of Simplified Models", "abstract": "A common method to study deep learning systems is to use simplified model\nrepresentations--for example, using singular value decomposition to visualize\nthe model's hidden states in a lower dimensional space. This approach assumes\nthat the results of these simplifications are faithful to the original model.\nHere, we illustrate an important caveat to this assumption: even if the\nsimplified representations can accurately approximate the full model on the\ntraining set, they may fail to accurately capture the model's behavior out of\ndistribution. We illustrate this by training Transformer models on controlled\ndatasets with systematic generalization splits, including the Dyck\nbalanced-parenthesis languages and a code completion task. We simplify these\nmodels using tools like dimensionality reduction and clustering, and then\nexplicitly test how these simplified proxies match the behavior of the original\nmodel. We find consistent generalization gaps: cases in which the simplified\nproxies are more faithful to the original model on the in-distribution\nevaluations and less faithful on various tests of systematic generalization.\nThis includes cases where the original model generalizes systematically but the\nsimplified proxies fail, and cases where the simplified proxies generalize\nbetter. Together, our results raise questions about the extent to which\nmechanistic interpretations derived using tools like SVD can reliably predict\nwhat a model will do in novel situations.", "published": "2023-12-06 18:25:53", "link": "http://arxiv.org/abs/2312.03656v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Generative agent-based modeling with actions grounded in physical,\n  social, or digital space using Concordia", "abstract": "Agent-based modeling has been around for decades, and applied widely across\nthe social and natural sciences. The scope of this research method is now\npoised to grow dramatically as it absorbs the new affordances provided by Large\nLanguage Models (LLM)s. Generative Agent-Based Models (GABM) are not just\nclassic Agent-Based Models (ABM)s where the agents talk to one another. Rather,\nGABMs are constructed using an LLM to apply common sense to situations, act\n\"reasonably\", recall common semantic knowledge, produce API calls to control\ndigital technologies like apps, and communicate both within the simulation and\nto researchers viewing it from the outside. Here we present Concordia, a\nlibrary to facilitate constructing and working with GABMs. Concordia makes it\neasy to construct language-mediated simulations of physically- or\ndigitally-grounded environments. Concordia agents produce their behavior using\na flexible component system which mediates between two fundamental operations:\nLLM calls and associative memory retrieval. A special agent called the Game\nMaster (GM), which was inspired by tabletop role-playing games, is responsible\nfor simulating the environment where the agents interact. Agents take actions\nby describing what they want to do in natural language. The GM then translates\ntheir actions into appropriate implementations. In a simulated physical world,\nthe GM checks the physical plausibility of agent actions and describes their\neffects. In digital environments simulating technologies such as apps and\nservices, the GM may handle API calls to integrate with external tools such as\ngeneral AI assistants (e.g., Bard, ChatGPT), and digital apps (e.g., Calendar,\nEmail, Search, etc.). Concordia was designed to support a wide array of\napplications both in scientific research and for evaluating performance of real\ndigital services by simulating users and/or generating synthetic data.", "published": "2023-12-06 18:33:50", "link": "http://arxiv.org/abs/2312.03664v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "SmoothQuant+: Accurate and Efficient 4-bit Post-Training\n  WeightQuantization for LLM", "abstract": "Large language models (LLMs) have shown remarkable capabilities in various\ntasks. However their huge model size and the consequent demand for\ncomputational and memory resources also pose challenges to model deployment.\nCurrently, 4-bit post-training quantization (PTQ) has achieved some success in\nLLMs, reducing the memory footprint by approximately 75% compared to FP16\nmodels, albeit with some accuracy loss. In this paper, we propose SmoothQuant+,\nan accurate and efficient 4-bit weight-only PTQ that requires no additional\ntraining, which enables lossless in accuracy for LLMs for the first time. Based\non the fact that the loss of weight quantization is amplified by the activation\noutliers, SmoothQuant+ smoothes the activation outliers by channel before\nquantization, while adjusting the corresponding weights for mathematical\nequivalence, and then performs group-wise 4-bit weight quantization for linear\nlayers. We have integrated SmoothQuant+ into the vLLM framework, an advanced\nhigh-throughput inference engine specially developed for LLMs, and equipped it\nwith an efficient W4A16 CUDA kernels, so that vLLM can seamlessly support\nSmoothQuant+ 4-bit weight quantization. Our results show that, with\nSmoothQuant+, the Code Llama-34B model can be quantized and deployed on a A100\n40GB GPU, achieving lossless accuracy and a throughput increase of 1.9 to 4.0\ntimes compared to the FP16 model deployed on two A100 40GB GPUs. Moreover, the\nlatency per token is only 68% of the FP16 model deployed on two A100 40GB GPUs.\nThis is the state-of-the-art 4-bit weight quantization for LLMs as we know.", "published": "2023-12-06 11:10:55", "link": "http://arxiv.org/abs/2312.03788v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Efficient Large Language Models: A Survey", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nimportant tasks such as natural language understanding and language generation,\nand thus have the potential to make a substantial impact on our society. Such\ncapabilities, however, come with the considerable resources they demand,\nhighlighting the strong need to develop effective techniques for addressing\ntheir efficiency challenges. In this survey, we provide a systematic and\ncomprehensive review of efficient LLMs research. We organize the literature in\na taxonomy consisting of three main categories, covering distinct yet\ninterconnected efficient LLMs topics from model-centric, data-centric, and\nframework-centric perspective, respectively. We have also created a GitHub\nrepository where we organize the papers featured in this survey at\nhttps://github.com/AIoT-MLSys-Lab/Efficient-LLMs-Survey. We will actively\nmaintain the repository and incorporate new research as it emerges. We hope our\nsurvey can serve as a valuable resource to help researchers and practitioners\ngain a systematic understanding of efficient LLMs research and inspire them to\ncontribute to this important and exciting field.", "published": "2023-12-06 19:18:42", "link": "http://arxiv.org/abs/2312.03863v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Corporate Bankruptcy Prediction with Domain-Adapted BERT", "abstract": "This study performs BERT-based analysis, which is a representative\ncontextualized language model, on corporate disclosure data to predict\nimpending bankruptcies. Prior literature on bankruptcy prediction mainly\nfocuses on developing more sophisticated prediction methodologies with\nfinancial variables. However, in our study, we focus on improving the quality\nof input dataset. Specifically, we employ BERT model to perform sentiment\nanalysis on MD&A disclosures. We show that BERT outperforms dictionary-based\npredictions and Word2Vec-based predictions in terms of adjusted R-square in\nlogistic regression, k-nearest neighbor (kNN-5), and linear kernel support\nvector machine (SVM). Further, instead of pre-training the BERT model from\nscratch, we apply self-learning with confidence-based filtering to corporate\ndisclosure data (10-K). We achieve the accuracy rate of 91.56% and demonstrate\nthat the domain adaptation procedure brings a significant improvement in\nprediction accuracy.", "published": "2023-12-06 00:05:25", "link": "http://arxiv.org/abs/2312.03194v1", "categories": ["cs.CL", "cs.LG", "econ.GN", "q-fin.EC"], "primary_category": "cs.CL"}
{"title": "Dyport: Dynamic Importance-based Hypothesis Generation Benchmarking\n  Technique", "abstract": "This paper presents a novel benchmarking framework Dyport for evaluating\nbiomedical hypothesis generation systems. Utilizing curated datasets, our\napproach tests these systems under realistic conditions, enhancing the\nrelevance of our evaluations. We integrate knowledge from the curated databases\ninto a dynamic graph, accompanied by a method to quantify discovery importance.\nThis not only assesses hypothesis accuracy but also their potential impact in\nbiomedical research which significantly extends traditional link prediction\nbenchmarks. Applicability of our benchmarking process is demonstrated on\nseveral link prediction systems applied on biomedical semantic knowledge\ngraphs. Being flexible, our benchmarking system is designed for broad\napplication in hypothesis generation quality verification, aiming to expand the\nscope of scientific discovery within the biomedical research community.\nAvailability and implementation: Dyport framework is fully open-source. All\ncode and datasets are available at: https://github.com/IlyaTyagin/Dyport", "published": "2023-12-06 06:07:50", "link": "http://arxiv.org/abs/2312.03303v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Optimizing Two-Pass Cross-Lingual Transfer Learning: Phoneme Recognition\n  and Phoneme to Grapheme Translation", "abstract": "This research optimizes two-pass cross-lingual transfer learning in\nlow-resource languages by enhancing phoneme recognition and phoneme-to-grapheme\ntranslation models. Our approach optimizes these two stages to improve speech\nrecognition across languages. We optimize phoneme vocabulary coverage by\nmerging phonemes based on shared articulatory characteristics, thus improving\nrecognition accuracy. Additionally, we introduce a global phoneme noise\ngenerator for realistic ASR noise during phoneme-to-grapheme training to reduce\nerror propagation. Experiments on the CommonVoice 12.0 dataset show significant\nreductions in Word Error Rate (WER) for low-resource languages, highlighting\nthe effectiveness of our approach. This research contributes to the\nadvancements of two-pass ASR systems in low-resource languages, offering the\npotential for improved cross-lingual transfer learning.", "published": "2023-12-06 06:37:24", "link": "http://arxiv.org/abs/2312.03312v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Measuring Misogyny in Natural Language Generation: Preliminary Results\n  from a Case Study on two Reddit Communities", "abstract": "Generic `toxicity' classifiers continue to be used for evaluating the\npotential for harm in natural language generation, despite mounting evidence of\ntheir shortcomings. We consider the challenge of measuring misogyny in natural\nlanguage generation, and argue that generic `toxicity' classifiers are\ninadequate for this task. We use data from two well-characterised `Incel'\ncommunities on Reddit that differ primarily in their degrees of misogyny to\nconstruct a pair of training corpora which we use to fine-tune two language\nmodels. We show that an open source `toxicity' classifier is unable to\ndistinguish meaningfully between generations from these models. We contrast\nthis with a misogyny-specific lexicon recently proposed by feminist\nsubject-matter experts, demonstrating that, despite the limitations of simple\nlexicon-based approaches, this shows promise as a benchmark to evaluate\nlanguage models for misogyny, and that it is sensitive enough to reveal the\nknown differences in these Reddit communities. Our preliminary findings\nhighlight the limitations of a generic approach to evaluating harms, and\nfurther emphasise the need for careful benchmark design and selection in\nnatural language evaluation.", "published": "2023-12-06 07:38:46", "link": "http://arxiv.org/abs/2312.03330v1", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Teaching Specific Scientific Knowledge into Large Language Models\n  through Additional Training", "abstract": "Through additional training, we explore embedding specialized scientific\nknowledge into the Llama 2 Large Language Model (LLM). Key findings reveal that\neffective knowledge integration requires reading texts from multiple\nperspectives, especially in instructional formats. We utilize text augmentation\nto tackle the scarcity of specialized texts, including style conversions and\ntranslations. Hyperparameter optimization proves crucial, with different size\nmodels (7b, 13b, and 70b) reasonably undergoing additional training. Validating\nour methods, we construct a dataset of 65,000 scientific papers. Although we\nhave succeeded in partially embedding knowledge, the study highlights the\ncomplexities and limitations of incorporating specialized information into\nLLMs, suggesting areas for further improvement.", "published": "2023-12-06 08:55:55", "link": "http://arxiv.org/abs/2312.03360v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DBCopilot: Natural Language Querying over Massive Databases via Schema\n  Routing", "abstract": "The development of Natural Language Interfaces to Databases (NLIDBs) has been\ngreatly advanced by the advent of large language models (LLMs), which provide\nan intuitive way to translate natural language (NL) questions into Structured\nQuery Language (SQL) queries. While significant progress has been made in\nLLM-based NL2SQL, existing approaches face several challenges in real-world\nscenarios of natural language querying over massive databases. In this paper,\nwe present DBCopilot, a framework that addresses these challenges by employing\na compact and flexible copilot model for routing over massive databases.\nSpecifically, DBCopilot decouples schema-agnostic NL2SQL into schema routing\nand SQL generation. This framework utilizes a single lightweight differentiable\nsearch index to construct semantic mappings for massive database schemata, and\nnavigates natural language questions to their target databases and tables in a\nrelation-aware joint retrieval manner. The routed schemata and questions are\nthen fed into LLMs for effective SQL generation. Furthermore, DBCopilot\nintroduces a reverse schema-to-question generation paradigm that can\nautomatically learn and adapt the router over massive databases without manual\nintervention. Experimental results verify that DBCopilot is a scalable and\neffective solution for schema-agnostic NL2SQL, providing a significant advance\nin handling natural language querying over massive databases for NLIDBs.", "published": "2023-12-06 12:37:28", "link": "http://arxiv.org/abs/2312.03463v3", "categories": ["cs.CL", "cs.DB", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Exploring the Reversal Curse and Other Deductive Logical Reasoning in\n  BERT and GPT-Based Large Language Models", "abstract": "The term \"Reversal Curse\" refers to the scenario where auto-regressive\ndecoder large language models (LLMs), such as ChatGPT, trained on \"A is B\" fail\nto learn \"B is A,\" assuming that B and A are distinct and can be uniquely\nidentified from each other, demonstrating a basic failure of logical deduction.\nThis raises a red flag in the use of GPT models for certain general tasks such\nas constructing knowledge graphs, considering their adherence to this symmetric\nprinciple. In our study, we examined a bidirectional LLM, BERT, and found that\nit is immune to the reversal curse. Driven by ongoing efforts to construct\nbiomedical knowledge graphs with LLMs, we also embarked on evaluating more\ncomplex but essential deductive reasoning capabilities. This process included\nfirst training encoder and decoder language models to master the intersection\nand union operations on two sets and then moving on to assess their capability\nto infer different combinations of union and intersection operations on three\nnewly created sets. The findings showed that while both encoder and decoder\nlanguage models, trained for tasks involving two sets (union/intersection),\nwere proficient in such scenarios, they encountered difficulties when dealing\nwith operations that included three sets (various combinations of union and\nintersection). Our research highlights the distinct characteristics of encoder\nand decoder models in simple and complex logical reasoning. In practice, the\nchoice between BERT and GPT should be guided by the specific requirements and\nnature of the task at hand, leveraging their respective strengths in\nbidirectional context comprehension and sequence prediction.", "published": "2023-12-06 17:29:45", "link": "http://arxiv.org/abs/2312.03633v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Integrating Pre-Trained Speech and Language Models for End-to-End Speech\n  Recognition", "abstract": "Advances in machine learning have made it possible to perform various text\nand speech processing tasks, such as automatic speech recognition (ASR), in an\nend-to-end (E2E) manner. E2E approaches utilizing pre-trained models are\ngaining attention for conserving training data and resources. However, most of\ntheir applications in ASR involve only one of either a pre-trained speech or a\nlanguage model. This paper proposes integrating a pre-trained speech\nrepresentation model and a large language model (LLM) for E2E ASR. The proposed\nmodel enables the optimization of the entire ASR process, including acoustic\nfeature extraction and acoustic and language modeling, by combining pre-trained\nmodels with a bridge network and also enables the application of remarkable\ndevelopments in LLM utilization, such as parameter-efficient domain adaptation\nand inference optimization. Experimental results demonstrate that the proposed\nmodel achieves a performance comparable to that of modern E2E ASR models by\nutilizing powerful pre-training models with the proposed integrated approach.", "published": "2023-12-06 18:34:42", "link": "http://arxiv.org/abs/2312.03668v2", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Improving Activation Steering in Language Models with Mean-Centring", "abstract": "Recent work in activation steering has demonstrated the potential to better\ncontrol the outputs of Large Language Models (LLMs), but it involves finding\nsteering vectors. This is difficult because engineers do not typically know how\nfeatures are represented in these models. We seek to address this issue by\napplying the idea of mean-centring to steering vectors. We find that taking the\naverage of activations associated with a target dataset, and then subtracting\nthe mean of all training activations, results in effective steering vectors. We\ntest this method on a variety of models on natural language tasks by steering\naway from generating toxic text, and steering the completion of a story towards\na target genre. We also apply mean-centring to extract function vectors, more\neffectively triggering the execution of a range of natural language tasks by a\nsignificant margin (compared to previous baselines). This suggests that\nmean-centring can be used to easily improve the effectiveness of activation\nsteering in a wide range of contexts.", "published": "2023-12-06 18:27:07", "link": "http://arxiv.org/abs/2312.03813v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LLM as OS, Agents as Apps: Envisioning AIOS, Agents and the AIOS-Agent\n  Ecosystem", "abstract": "This paper envisions a revolutionary AIOS-Agent ecosystem, where Large\nLanguage Model (LLM) serves as the (Artificial) Intelligent Operating System\n(IOS, or AIOS)--an operating system \"with soul\". Upon this foundation, a\ndiverse range of LLM-based AI Agent Applications (Agents, or AAPs) are\ndeveloped, enriching the AIOS-Agent ecosystem and signaling a paradigm shift\nfrom the traditional OS-APP ecosystem. We envision that LLM's impact will not\nbe limited to the AI application level, instead, it will in turn revolutionize\nthe design and implementation of computer system, architecture, software, and\nprogramming language, featured by several main concepts: LLM as OS\n(system-level), Agents as Applications (application-level), Natural Language as\nProgramming Interface (user-level), and Tools as Devices/Libraries\n(hardware/middleware-level). We begin by introducing the architecture of\ntraditional OS. Then we formalize a conceptual framework for AIOS through \"LLM\nas OS (LLMOS)\", drawing analogies between AIOS and traditional OS: LLM is\nlikened to OS kernel, context window to memory, external storage to file\nsystem, hardware tools to peripheral devices, software tools to programming\nlibraries, and user prompts to user commands. Subsequently, we introduce the\nnew AIOS-Agent Ecosystem, where users can easily program Agent Applications\n(AAPs) using natural language, democratizing the development of software, which\nis different from the traditional OS-APP ecosystem. Following this, we explore\nthe diverse scope of Agent Applications. We delve into both single-agent and\nmulti-agent systems, as well as human-agent interaction. Lastly, drawing on the\ninsights from traditional OS-APP ecosystem, we propose a roadmap for the\nevolution of the AIOS-Agent ecosystem. This roadmap is designed to guide the\nfuture research and development, suggesting systematic progresses of AIOS and\nits Agent applications.", "published": "2023-12-06 18:50:26", "link": "http://arxiv.org/abs/2312.03815v2", "categories": ["cs.OS", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.OS"}
{"title": "Alpha-CLIP: A CLIP Model Focusing on Wherever You Want", "abstract": "Contrastive Language-Image Pre-training (CLIP) plays an essential role in\nextracting valuable content information from images across diverse tasks. It\naligns textual and visual modalities to comprehend the entire image, including\nall the details, even those irrelevant to specific tasks. However, for a finer\nunderstanding and controlled editing of images, it becomes crucial to focus on\nspecific regions of interest, which can be indicated as points, masks, or boxes\nby humans or perception models. To fulfill the requirements, we introduce\nAlpha-CLIP, an enhanced version of CLIP with an auxiliary alpha channel to\nsuggest attentive regions and fine-tuned with constructed millions of RGBA\nregion-text pairs. Alpha-CLIP not only preserves the visual recognition ability\nof CLIP but also enables precise control over the emphasis of image contents.\nIt demonstrates effectiveness in various tasks, including but not limited to\nopen-world recognition, multimodal large language models, and conditional 2D /\n3D generation. It has a strong potential to serve as a versatile tool for\nimage-related tasks.", "published": "2023-12-06 18:59:30", "link": "http://arxiv.org/abs/2312.03818v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "A Pseudo-Semantic Loss for Autoregressive Models with Logical\n  Constraints", "abstract": "Neuro-symbolic AI bridges the gap between purely symbolic and neural\napproaches to learning. This often requires maximizing the likelihood of a\nsymbolic constraint w.r.t the neural network's output distribution. Such output\ndistributions are typically assumed to be fully-factorized. This limits the\napplicability of neuro-symbolic learning to the more expressive autoregressive\ndistributions, e.g., transformers. Under such distributions, computing the\nlikelihood of even simple constraints is #P-hard. Instead of attempting to\nenforce the constraint on the entire output distribution, we propose to do so\non a random, local approximation thereof. More precisely, we optimize the\nlikelihood of the constraint under a pseudolikelihood-based approximation\ncentered around a model sample. Our approximation is factorized, allowing the\nreuse of solutions to sub-problems, a main tenet for efficiently computing\nneuro-symbolic losses. Moreover, it is a local, high-fidelity approximation of\nthe likelihood, exhibiting low entropy and KL-divergence around the model\nsample. We evaluate our approach on Sudoku and shortest-path prediction cast as\nautoregressive generation, and observe that we greatly improve upon the base\nmodel's ability to predict logically-consistent outputs. We also evaluate on\nthe task of detoxifying large language models. Using a simple constraint\ndisallowing a list of toxic words, we are able to steer the model's outputs\naway from toxic generations, achieving SoTA detoxification compared to previous\napproaches.", "published": "2023-12-06 20:58:07", "link": "http://arxiv.org/abs/2312.03905v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Evaluating Self-supervised Speech Models on a Taiwanese Hokkien Corpus", "abstract": "Taiwanese Hokkien is declining in use and status due to a language shift\ntowards Mandarin in Taiwan. This is partly why it is a low resource language in\nNLP and speech research today. To ensure that the state of the art in speech\nprocessing does not leave Taiwanese Hokkien behind, we contribute a 1.5-hour\ndataset of Taiwanese Hokkien to ML-SUPERB's hidden set. Evaluating ML-SUPERB's\nsuite of self-supervised learning (SSL) speech representations on our dataset,\nwe find that model size does not consistently determine performance. In fact,\ncertain smaller models outperform larger ones. Furthermore, linguistic\nalignment between pretraining data and the target language plays a crucial\nrole.", "published": "2023-12-06 01:32:20", "link": "http://arxiv.org/abs/2312.06668v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Understanding (Un)Intended Memorization in Text-to-Image Generative\n  Models", "abstract": "Multimodal machine learning, especially text-to-image models like Stable\nDiffusion and DALL-E 3, has gained significance for transforming text into\ndetailed images.\n  Despite their growing use and remarkable generative capabilities, there is a\npressing need for a detailed examination of these models' behavior,\nparticularly with respect to memorization. Historically, memorization in\nmachine learning has been context-dependent, with diverse definitions emerging\nfrom classification tasks to complex models like Large Language Models (LLMs)\nand Diffusion models. Yet, a definitive concept of memorization that aligns\nwith the intricacies of text-to-image synthesis remains elusive. This\nunderstanding is vital as memorization poses privacy risks yet is essential for\nmeeting user expectations, especially when generating representations of\nunderrepresented entities. In this paper, we introduce a specialized definition\nof memorization tailored to text-to-image models, categorizing it into three\ndistinct types according to user expectations. We closely examine the subtle\ndistinctions between intended and unintended memorization, emphasizing the\nimportance of balancing user privacy with the generative quality of the model\noutputs. Using the Stable Diffusion model, we offer examples to validate our\nmemorization definitions and clarify their application.", "published": "2023-12-06 19:53:17", "link": "http://arxiv.org/abs/2312.07550v1", "categories": ["cs.CV", "cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Empowering ChatGPT-Like Large-Scale Language Models with Local Knowledge\n  Base for Industrial Prognostics and Health Management", "abstract": "Prognostics and health management (PHM) is essential for industrial operation\nand maintenance, focusing on predicting, diagnosing, and managing the health\nstatus of industrial systems. The emergence of the ChatGPT-Like large-scale\nlanguage model (LLM) has begun to lead a new round of innovation in the AI\nfield. It has extensively promoted the level of intelligence in various fields.\nTherefore, it is also expected further to change the application paradigm in\nindustrial PHM and promote PHM to become intelligent. Although ChatGPT-Like\nLLMs have rich knowledge reserves and powerful language understanding and\ngeneration capabilities, they lack domain-specific expertise, significantly\nlimiting their practicability in PHM applications. To this end, this study\nexplores the ChatGPT-Like LLM empowered by the local knowledge base (LKB) in\nindustrial PHM to solve the above limitations. In addition, we introduce the\nmethod and steps of combining the LKB with LLMs, including LKB preparation, LKB\nvectorization, prompt engineering, etc. Experimental analysis of real cases\nshows that combining the LKB with ChatGPT-Like LLM can significantly improve\nits performance and make ChatGPT-Like LLMs more accurate, relevant, and able to\nprovide more insightful information. This can promote the development of\nChatGPT-Like LLMs in industrial PHM and promote their efficiency and quality.", "published": "2023-12-06 15:24:01", "link": "http://arxiv.org/abs/2312.14945v3", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "OneLLM: One Framework to Align All Modalities with Language", "abstract": "Multimodal large language models (MLLMs) have gained significant attention\ndue to their strong multimodal understanding capability. However, existing\nworks rely heavily on modality-specific encoders, which usually differ in\narchitecture and are limited to common modalities. In this paper, we present\nOneLLM, an MLLM that aligns eight modalities to language using a unified\nframework. We achieve this through a unified multimodal encoder and a\nprogressive multimodal alignment pipeline. In detail, we first train an image\nprojection module to connect a vision encoder with LLM. Then, we build a\nuniversal projection module (UPM) by mixing multiple image projection modules\nand dynamic routing. Finally, we progressively align more modalities to LLM\nwith the UPM. To fully leverage the potential of OneLLM in following\ninstructions, we also curated a comprehensive multimodal instruction dataset,\nincluding 2M items from image, audio, video, point cloud, depth/normal map, IMU\nand fMRI brain activity. OneLLM is evaluated on 25 diverse benchmarks,\nencompassing tasks such as multimodal captioning, question answering and\nreasoning, where it delivers excellent performance. Code, data, model and\nonline demo are available at https://github.com/csuhan/OneLLM", "published": "2023-12-06 18:59:19", "link": "http://arxiv.org/abs/2312.03700v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "primary_category": "cs.CV"}
{"title": "The BigCode Project Governance Card", "abstract": "This document serves as an overview of the different mechanisms and areas of\ngovernance in the BigCode project. It aims to support transparency by providing\nrelevant information about choices that were made during the project to the\nbroader public, and to serve as an example of intentional governance of an open\nresearch project that future endeavors can leverage to shape their own\napproach. The first section, Project Structure, covers the project\norganization, its stated goals and values, its internal decision processes, and\nits funding and resources. The second section, Data and Model Governance,\ncovers decisions relating to the questions of data subject consent, privacy,\nand model release.", "published": "2023-12-06 19:37:08", "link": "http://arxiv.org/abs/2312.03872v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG", "cs.PL"], "primary_category": "cs.CY"}
{"title": "Lightweight Speaker Verification Using Transformation Module with\n  Feature Partition and Fusion", "abstract": "Although many efforts have been made on decreasing the model complexity for\nspeaker verification, it is still challenging to deploy speaker verification\nsystems with satisfactory result on low-resource terminals. We design a\ntransformation module that performs feature partition and fusion to implement\nlightweight speaker verification. The transformation module consists of\nmultiple simple but effective operations, such as convolution, pooling, mean,\nconcatenation, normalization, and element-wise summation. It works in a\nplug-and-play way, and can be easily implanted into a wide variety of models to\nreduce the model complexity while maintaining the model error. First, the input\nfeature is split into several low-dimensional feature subsets for decreasing\nthe model complexity. Then, each feature subset is updated by fusing it with\nthe inter-feature-subsets correlational information to enhance its\nrepresentational capability. Finally, the updated feature subsets are\nindependently fed into the block (one or several layers) of the model for\nfurther processing. The features that are output from current block of the\nmodel are processed according to the steps above before they are fed into the\nnext block of the model. Experimental data are selected from two public speech\ncorpora (namely VoxCeleb1 and VoxCeleb2). Results show that implanting the\ntransformation module into three models (namely AMCRN, ResNet34, and\nECAPA-TDNN) for speaker verification slightly increases the model error and\nsignificantly decreases the model complexity. Our proposed method outperforms\nbaseline methods on the whole in memory requirement and computational\ncomplexity with lower equal error rate. It also generalizes well across\ntruncated segments with various lengths.", "published": "2023-12-06 07:25:16", "link": "http://arxiv.org/abs/2312.03324v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Parameter-Efficient Transfer Learning of Audio Spectrogram Transformers", "abstract": "Parameter-efficient transfer learning (PETL) methods have emerged as a solid\nalternative to the standard full fine-tuning approach. They only train a few\nextra parameters for each downstream task, without sacrificing performance and\ndispensing with the issue of storing a copy of the pre-trained model for each\ntask. For audio classification tasks, the Audio Spectrogram Transformer (AST)\nmodel shows impressive results. However, surprisingly, how to efficiently adapt\nit to several downstream tasks has not been tackled before. In this paper, we\nbridge this gap and present a detailed investigation of common PETL methods for\nthe adaptation of the AST model to audio/speech tasks. Furthermore, we propose\na new adapter design that exploits the convolution module of the Conformer\nmodel, leading to superior performance over the standard PETL approaches and\nsurpassing or achieving performance parity with full fine-tuning by updating\nonly 0.29% of the parameters. Finally, we provide ablation studies revealing\nthat our proposed adapter: 1) proves to be effective in few-shot efficient\ntransfer learning, 2) attains optimal results regardless of the amount of the\nallocated parameters, and 3) can be applied to other pre-trained models.", "published": "2023-12-06 18:55:34", "link": "http://arxiv.org/abs/2312.03694v4", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Golden Gemini is All You Need: Finding the Sweet Spots for Speaker\n  Verification", "abstract": "Previous studies demonstrate the impressive performance of residual neural\nnetworks (ResNet) in speaker verification. The ResNet models treat the time and\nfrequency dimensions equally. They follow the default stride configuration\ndesigned for image recognition, where the horizontal and vertical axes exhibit\nsimilarities. This approach ignores the fact that time and frequency are\nasymmetric in speech representation. In this paper, we address this issue and\nlook for optimal stride configurations specifically tailored for speaker\nverification. We represent the stride space on a trellis diagram, and conduct a\nsystematic study on the impact of temporal and frequency resolutions on the\nperformance and further identify two optimal points, namely Golden Gemini,\nwhich serves as a guiding principle for designing 2D ResNet-based speaker\nverification models. By following the principle, a state-of-the-art ResNet\nbaseline model gains a significant performance improvement on VoxCeleb, SITW,\nand CNCeleb datasets with 7.70%/11.76% average EER/minDCF reductions,\nrespectively, across different network depths (ResNet18, 34, 50, and 101),\nwhile reducing the number of parameters by 16.5% and FLOPs by 4.1%. We refer to\nit as Gemini ResNet. Further investigation reveals the efficacy of the proposed\nGolden Gemini operating points across various training conditions and\narchitectures. Furthermore, we present a new benchmark, namely the Gemini\nDF-ResNet, using a cutting-edge model.", "published": "2023-12-06 17:08:49", "link": "http://arxiv.org/abs/2312.03620v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Detecting Voice Cloning Attacks via Timbre Watermarking", "abstract": "Nowadays, it is common to release audio content to the public. However, with\nthe rise of voice cloning technology, attackers have the potential to easily\nimpersonate a specific person by utilizing his publicly released audio without\nany permission. Therefore, it becomes significant to detect any potential\nmisuse of the released audio content and protect its timbre from being\nimpersonated. To this end, we introduce a novel concept, \"Timbre Watermarking\",\nwhich embeds watermark information into the target individual's speech,\neventually defeating the voice cloning attacks. To ensure the watermark is\nrobust to the voice cloning model's learning process, we design an end-to-end\nvoice cloning-resistant detection framework. The core idea of our solution is\nto embed and extract the watermark in the frequency domain in a temporally\ninvariant manner. To acquire generalization across different voice cloning\nattacks, we modulate their shared process and integrate it into our framework\nas a distortion layer. Experiments demonstrate that the proposed timbre\nwatermarking can defend against different voice cloning attacks, exhibit strong\nresistance against various adaptive attacks (e.g., reconstruction-based removal\nattacks, watermark overwriting attacks), and achieve practicality in real-world\nservices such as PaddleSpeech, Voice-Cloning-App, and so-vits-svc. In addition,\nablation studies are also conducted to verify the effectiveness of our design.\nSome audio samples are available at\nhttps://timbrewatermarking.github.io/samples.", "published": "2023-12-06 10:48:36", "link": "http://arxiv.org/abs/2312.03410v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Subnetwork-to-go: Elastic Neural Network with Dynamic Training and\n  Customizable Inference", "abstract": "Deploying neural networks to different devices or platforms is in general\nchallenging, especially when the model size is large or model complexity is\nhigh. Although there exist ways for model pruning or distillation, it is\ntypically required to perform a full round of model training or finetuning\nprocedure in order to obtain a smaller model that satisfies the model size or\ncomplexity constraints. Motivated by recent works on dynamic neural networks,\nwe propose a simple way to train a large network and flexibly extract a\nsubnetwork from it given a model size or complexity constraint during\ninference. We introduce a new way to allow a large model to be trained with\ndynamic depth and width during the training phase, and after the large model is\ntrained we can select a subnetwork from it with arbitrary depth and width\nduring the inference phase with a relatively better performance compared to\ntraining the subnetwork independently from scratch. Experiment results on a\nmusic source separation model show that our proposed method can effectively\nimprove the separation performance across different subnetwork sizes and\ncomplexities with a single large model, and training the large model takes\nsignificantly shorter time than training all the different subnetworks.", "published": "2023-12-06 12:40:06", "link": "http://arxiv.org/abs/2312.03464v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "JAMMIN-GPT: Text-based Improvisation using LLMs in Ableton Live", "abstract": "We introduce a system that allows users of Ableton Live to create MIDI-clips\nby naming them with musical descriptions. Users can compose by typing the\ndesired musical content directly in Ableton's clip view, which is then inserted\nby our integrated system. This allows users to stay in the flow of their\ncreative process while quickly generating musical ideas. The system works by\nprompting ChatGPT to reply using one of several text-based musical formats,\nsuch as ABC notation, chord symbols, or drum tablature. This is an important\nstep in integrating generative AI tools into pre-existing musical workflows,\nand could be valuable for content makers who prefer to express their creative\nvision through descriptive language. Code is available at\nhttps://github.com/supersational/JAMMIN-GPT.", "published": "2023-12-06 13:19:34", "link": "http://arxiv.org/abs/2312.03479v1", "categories": ["cs.SD", "cs.AI", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Schrodinger Bridges Beat Diffusion Models on Text-to-Speech Synthesis", "abstract": "In text-to-speech (TTS) synthesis, diffusion models have achieved promising\ngeneration quality. However, because of the pre-defined data-to-noise diffusion\nprocess, their prior distribution is restricted to a noisy representation,\nwhich provides little information of the generation target. In this work, we\npresent a novel TTS system, Bridge-TTS, making the first attempt to substitute\nthe noisy Gaussian prior in established diffusion-based TTS methods with a\nclean and deterministic one, which provides strong structural information of\nthe target. Specifically, we leverage the latent representation obtained from\ntext input as our prior, and build a fully tractable Schrodinger bridge between\nit and the ground-truth mel-spectrogram, leading to a data-to-data process.\nMoreover, the tractability and flexibility of our formulation allow us to\nempirically study the design spaces such as noise schedules, as well as to\ndevelop stochastic and deterministic samplers. Experimental results on the\nLJ-Speech dataset illustrate the effectiveness of our method in terms of both\nsynthesis quality and sampling efficiency, significantly outperforming our\ndiffusion counterpart Grad-TTS in 50-step/1000-step synthesis and strong fast\nTTS models in few-step scenarios. Project page: https://bridge-tts.github.io/", "published": "2023-12-06 13:31:55", "link": "http://arxiv.org/abs/2312.03491v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Multimodal Data and Resource Efficient Device-Directed Speech Detection\n  with Large Foundation Models", "abstract": "Interactions with virtual assistants typically start with a trigger phrase\nfollowed by a command. In this work, we explore the possibility of making these\ninteractions more natural by eliminating the need for a trigger phrase. Our\ngoal is to determine whether a user addressed the virtual assistant based on\nsignals obtained from the streaming audio recorded by the device microphone. We\naddress this task by combining 1-best hypotheses and decoder signals from an\nautomatic speech recognition system with acoustic representations from an audio\nencoder as input features to a large language model (LLM). In particular, we\nare interested in data and resource efficient systems that require only a small\namount of training data and can operate in scenarios with only a single frozen\nLLM available on a device. For this reason, our model is trained on 80k or less\nexamples of multimodal data using a combination of low-rank adaptation and\nprefix tuning. We compare the proposed system to unimodal baselines and show\nthat the multimodal approach achieves lower equal-error-rates (EERs), while\nusing only a fraction of the training data. We also show that low-dimensional\nspecialized audio representations lead to lower EERs than high-dimensional\ngeneral audio representations.", "published": "2023-12-06 17:29:03", "link": "http://arxiv.org/abs/2312.03632v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Towards small and accurate convolutional neural networks for acoustic\n  biodiversity monitoring", "abstract": "Automated classification of animal sounds is a prerequisite for large-scale\nmonitoring of biodiversity. Convolutional Neural Networks (CNNs) are among the\nmost promising algorithms but they are slow, often achieve poor classification\nin the field and typically require large training data sets. Our objective was\nto design CNNs that are fast at inference time and achieve good classification\nperformance while learning from moderate-sized data. Recordings from a\nrainforest ecosystem were used. Start and end-point of sounds from 20 bird\nspecies were manually annotated. Spectrograms from 10 second segments were used\nas CNN input. We designed simple CNNs with a frequency unwrapping layer\n(SIMP-FU models) such that any output unit was connected to all spectrogram\nfrequencies but only to a sub-region of time, the Receptive Field (RF). Our\nmodels allowed experimentation with different RF durations. Models either used\nthe time-indexed labels that encode start and end-point of sounds or simpler\nsegment-level labels. Models learning from time-indexed labels performed\nconsiderably better than their segment-level counterparts. Best classification\nperformances was achieved for models with intermediate RF duration of 1.5\nseconds. The best SIMP-FU models achieved AUCs over 0.95 in 18 of 20 classes on\nthe test set. On compact low-cost hardware the best SIMP-FU models evaluated up\nto seven times faster than real-time data acquisition. RF duration was a major\ndriver of classification performance. The optimum of 1.5 s was in the same\nrange as the duration of the sounds. Our models achieved good classification\nperformance while learning from moderate-sized training data. This is explained\nby the usage of time-indexed labels during training and adequately sized RF.\nResults confirm the feasibility of deploying small CNNs with good\nclassification performance on compact low-cost devices.", "published": "2023-12-06 18:34:01", "link": "http://arxiv.org/abs/2312.03666v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Deep Multimodal Fusion for Surgical Feedback Classification", "abstract": "Quantification of real-time informal feedback delivered by an experienced\nsurgeon to a trainee during surgery is important for skill improvements in\nsurgical training. Such feedback in the live operating room is inherently\nmultimodal, consisting of verbal conversations (e.g., questions and answers) as\nwell as non-verbal elements (e.g., through visual cues like pointing to\nanatomic elements). In this work, we leverage a clinically-validated\nfive-category classification of surgical feedback: \"Anatomic\", \"Technical\",\n\"Procedural\", \"Praise\" and \"Visual Aid\". We then develop a multi-label machine\nlearning model to classify these five categories of surgical feedback from\ninputs of text, audio, and video modalities. The ultimate goal of our work is\nto help automate the annotation of real-time contextual surgical feedback at\nscale. Our automated classification of surgical feedback achieves AUCs ranging\nfrom 71.5 to 77.6 with the fusion improving performance by 3.1%. We also show\nthat high-quality manual transcriptions of feedback audio from experts improve\nAUCs to between 76.5 and 96.2, which demonstrates a clear path toward future\nimprovements. Empirically, we find that the Staged training strategy, with\nfirst pre-training each modality separately and then training them jointly, is\nmore effective than training different modalities altogether. We also present\nintuitive findings on the importance of modalities for different feedback\ncategories. This work offers an important first look at the feasibility of\nautomated classification of real-world live surgical feedback based on text,\naudio, and video modalities.", "published": "2023-12-06 01:59:47", "link": "http://arxiv.org/abs/2312.03231v1", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.HC", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Multimodal Group Emotion Recognition In-the-wild Using Privacy-Compliant\n  Features", "abstract": "This paper explores privacy-compliant group-level emotion recognition\n''in-the-wild'' within the EmotiW Challenge 2023. Group-level emotion\nrecognition can be useful in many fields including social robotics,\nconversational agents, e-coaching and learning analytics. This research imposes\nitself using only global features avoiding individual ones, i.e. all features\nthat can be used to identify or track people in videos (facial landmarks, body\nposes, audio diarization, etc.). The proposed multimodal model is composed of a\nvideo and an audio branches with a cross-attention between modalities. The\nvideo branch is based on a fine-tuned ViT architecture. The audio branch\nextracts Mel-spectrograms and feed them through CNN blocks into a transformer\nencoder. Our training paradigm includes a generated synthetic dataset to\nincrease the sensitivity of our model on facial expression within the image in\na data-driven way. The extensive experiments show the significance of our\nmethodology. Our privacy-compliant proposal performs fairly on the EmotiW\nchallenge, with 79.24% and 75.13% of accuracy respectively on validation and\ntest set for the best models. Noticeably, our findings highlight that it is\npossible to reach this accuracy level with privacy-compliant features using\nonly 5 frames uniformly distributed on the video.", "published": "2023-12-06 08:58:11", "link": "http://arxiv.org/abs/2312.05265v1", "categories": ["cs.AI", "cs.CR", "cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.AI"}
{"title": "Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence\n  of Training Data", "abstract": "Perceptual metrics are traditionally used to evaluate the quality of natural\nsignals, such as images and audio. They are designed to mimic the perceptual\nbehaviour of human observers and usually reflect structures found in natural\nsignals. This motivates their use as loss functions for training generative\nmodels such that models will learn to capture the structure held in the metric.\nWe take this idea to the extreme in the audio domain by training a compressive\nautoencoder to reconstruct uniform noise, in lieu of natural data. We show that\ntraining with perceptual losses improves the reconstruction of spectrograms and\nre-synthesized audio at test time over models trained with a standard Euclidean\nloss. This demonstrates better generalisation to unseen natural signals when\nusing perceptual metrics.", "published": "2023-12-06 12:27:25", "link": "http://arxiv.org/abs/2312.03455v1", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.LG", "eess.AS", "eess.IV"], "primary_category": "cs.SD"}
