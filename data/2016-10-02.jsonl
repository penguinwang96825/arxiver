{"title": "Sentence Segmentation in Narrative Transcripts from Neuropsychological\n  Tests using Recurrent Convolutional Neural Networks", "abstract": "Automated discourse analysis tools based on Natural Language Processing (NLP)\naiming at the diagnosis of language-impairing dementias generally extract\nseveral textual metrics of narrative transcripts. However, the absence of\nsentence boundary segmentation in the transcripts prevents the direct\napplication of NLP methods which rely on these marks to function properly, such\nas taggers and parsers. We present the first steps taken towards automatic\nneuropsychological evaluation based on narrative discourse analysis, presenting\na new automatic sentence segmentation method for impaired speech. Our model\nuses recurrent convolutional neural networks with prosodic, Part of Speech\n(PoS) features, and word embeddings. It was evaluated intrinsically on\nimpaired, spontaneous speech, as well as, normal, prepared speech, and presents\nbetter results for healthy elderly (CTL) (F1 = 0.74) and Mild Cognitive\nImpairment (MCI) patients (F1 = 0.70) than the Conditional Random Fields method\n(F1 = 0.55 and 0.53, respectively) used in the same context of our study. The\nresults suggest that our model is robust for impaired speech and can be used in\nautomated discourse analysis tools to differentiate narratives produced by MCI\nand CTL.", "published": "2016-10-02 00:49:15", "link": "http://arxiv.org/abs/1610.00211v2", "categories": ["cs.CL", "68T50"], "primary_category": "cs.CL"}
{"title": "Very Deep Convolutional Neural Networks for Robust Speech Recognition", "abstract": "This paper describes the extension and optimization of our previous work on\nvery deep convolutional neural networks (CNNs) for effective recognition of\nnoisy speech in the Aurora 4 task. The appropriate number of convolutional\nlayers, the sizes of the filters, pooling operations and input feature maps are\nall modified: the filter and pooling sizes are reduced and dimensions of input\nfeature maps are extended to allow adding more convolutional layers.\nFurthermore appropriate input padding and input feature map selection\nstrategies are developed. In addition, an adaptation framework using joint\ntraining of very deep CNN with auxiliary features i-vector and fMLLR features\nis developed. These modifications give substantial word error rate reductions\nover the standard CNN used as baseline. Finally the very deep CNN is combined\nwith an LSTM-RNN acoustic model and it is shown that state-level weighted log\nlikelihood score combination in a joint acoustic model decoding scheme is very\neffective. On the Aurora 4 task, the very deep CNN achieves a WER of 8.81%,\nfurther 7.99% with auxiliary feature joint training, and 7.09% with LSTM-RNN\njoint decoding.", "published": "2016-10-02 13:29:14", "link": "http://arxiv.org/abs/1610.00277v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Syntactic Structures and Code Parameters", "abstract": "We assign binary and ternary error-correcting codes to the data of syntactic\nstructures of world languages and we study the distribution of code points in\nthe space of code parameters. We show that, while most codes populate the lower\nregion approximating a superposition of Thomae functions, there is a\nsubstantial presence of codes above the Gilbert-Varshamov bound and even above\nthe asymptotic bound and the Plotkin bound. We investigate the dynamics induced\non the space of code parameters by spin glass models of language change, and\nshow that, in the presence of entailment relations between syntactic parameters\nthe dynamics can sometimes improve the code. For large sets of languages and\nsyntactic data, one can gain information on the spin glass dynamics from the\ninduced dynamics in the space of code parameters.", "published": "2016-10-02 16:54:41", "link": "http://arxiv.org/abs/1610.00311v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text Network Exploration via Heterogeneous Web of Topics", "abstract": "A text network refers to a data type that each vertex is associated with a\ntext document and the relationship between documents is represented by edges.\nThe proliferation of text networks such as hyperlinked webpages and academic\ncitation networks has led to an increasing demand for quickly developing a\ngeneral sense of a new text network, namely text network exploration. In this\npaper, we address the problem of text network exploration through constructing\na heterogeneous web of topics, which allows people to investigate a text\nnetwork associating word level with document level. To achieve this, a\nprobabilistic generative model for text and links is proposed, where three\ndifferent relationships in the heterogeneous topic web are quantified. We also\ndevelop a prototype demo system named TopicAtlas to exhibit such heterogeneous\ntopic web, and demonstrate how this system can facilitate the task of text\nnetwork exploration. Extensive qualitative analyses are included to verify the\neffectiveness of this heterogeneous topic web. Besides, we validate our model\non real-life text networks, showing that it preserves good performance on\nobjective evaluation metrics.", "published": "2016-10-02 03:35:11", "link": "http://arxiv.org/abs/1610.00219v1", "categories": ["cs.SI", "cs.CL", "cs.IR"], "primary_category": "cs.SI"}
{"title": "Sentiment Analysis on Bangla and Romanized Bangla Text (BRBT) using Deep\n  Recurrent models", "abstract": "Sentiment Analysis (SA) is an action research area in the digital age. With\nrapid and constant growth of online social media sites and services, and the\nincreasing amount of textual data such as - statuses, comments, reviews etc.\navailable in them, application of automatic SA is on the rise. However, most of\nthe research works on SA in natural language processing (NLP) are based on\nEnglish language. Despite being the sixth most widely spoken language in the\nworld, Bangla still does not have a large and standard dataset. Because of\nthis, recent research works in Bangla have failed to produce results that can\nbe both comparable to works done by others and reusable as stepping stones for\nfuture researchers to progress in this field. Therefore, we first tried to\nprovide a textual dataset - that includes not just Bangla, but Romanized Bangla\ntexts as well, is substantial, post-processed and multiple validated, ready to\nbe used in SA experiments. We tested this dataset in Deep Recurrent model,\nspecifically, Long Short Term Memory (LSTM), using two types of loss functions\n- binary crossentropy and categorical crossentropy, and also did some\nexperimental pre-training by using data from one validation to pre-train the\nother and vice versa. Lastly, we documented the results along with some\nanalysis on them, which were promising.", "published": "2016-10-02 23:45:23", "link": "http://arxiv.org/abs/1610.00369v2", "categories": ["cs.CL", "cs.IR", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
