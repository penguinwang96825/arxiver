{"title": "Skill2vec: Machine Learning Approach for Determining the Relevant Skills\n  from Job Description", "abstract": "Unsupervise learned word embeddings have seen tremendous success in numerous\nNatural Language Processing (NLP) tasks in recent years. The main contribution\nof this paper is to develop a technique called Skill2vec, which applies machine\nlearning techniques in recruitment to enhance the search strategy to find\ncandidates possessing the appropriate skills. Skill2vec is a neural network\narchitecture inspired by Word2vec, developed by Mikolov et al. in 2013. It\ntransforms skills to new vector space, which has the characteristics of\ncalculation and presents skills relationships. We conducted an experiment\nevaluation manually by a recruitment company's domain experts to demonstrate\nthe effectiveness of our approach.", "published": "2017-07-31 08:10:49", "link": "http://arxiv.org/abs/1707.09751v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Low-Resource Neural Headline Generation", "abstract": "Recent neural headline generation models have shown great results, but are\ngenerally trained on very large datasets. We focus our efforts on improving\nheadline quality on smaller datasets by the means of pretraining. We propose\nnew methods that enable pre-training all the parameters of the model and\nutilize all available text, resulting in improvements by up to 32.4% relative\nin perplexity and 2.84 points in ROUGE.", "published": "2017-07-31 08:56:32", "link": "http://arxiv.org/abs/1707.09769v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Combining Thesaurus Knowledge and Probabilistic Topic Models", "abstract": "In this paper we present the approach of introducing thesaurus knowledge into\nprobabilistic topic models. The main idea of the approach is based on the\nassumption that the frequencies of semantically related words and phrases,\nwhich are met in the same texts, should be enhanced: this action leads to their\nlarger contribution into topics found in these texts. We have conducted\nexperiments with several thesauri and found that for improving topic models, it\nis useful to utilize domain-specific knowledge. If a general thesaurus, such as\nWordNet, is used, the thesaurus-based improvement of topic models can be\nachieved with excluding hyponymy relations in combined topic models.", "published": "2017-07-31 12:32:16", "link": "http://arxiv.org/abs/1707.09816v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Linguistically Motivated Vocabulary Reduction for Neural Machine\n  Translation from Turkish to English", "abstract": "The necessity of using a fixed-size word vocabulary in order to control the\nmodel complexity in state-of-the-art neural machine translation (NMT) systems\nis an important bottleneck on performance, especially for morphologically rich\nlanguages. Conventional methods that aim to overcome this problem by using\nsub-word or character-level representations solely rely on statistics and\ndisregard the linguistic properties of words, which leads to interruptions in\nthe word structure and causes semantic and syntactic losses. In this paper, we\npropose a new vocabulary reduction method for NMT, which can reduce the\nvocabulary of a given input corpus at any rate while also considering the\nmorphological properties of the language. Our method is based on unsupervised\nmorphology learning and can be, in principle, used for pre-processing any\nlanguage pair. We also present an alternative word segmentation method based on\nsupervised morphological analysis, which aids us in measuring the accuracy of\nour model. We evaluate our method in Turkish-to-English NMT task where the\ninput language is morphologically rich and agglutinative. We analyze different\nrepresentation methods in terms of translation accuracy as well as the semantic\nand syntactic properties of the generated output. Our method obtains a\nsignificant improvement of 2.3 BLEU points over the conventional vocabulary\nreduction technique, showing that it can provide better accuracy in open\nvocabulary translation of morphologically rich languages.", "published": "2017-07-31 14:31:01", "link": "http://arxiv.org/abs/1707.09879v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Regularization techniques for fine-tuning in neural machine translation", "abstract": "We investigate techniques for supervised domain adaptation for neural machine\ntranslation where an existing model trained on a large out-of-domain dataset is\nadapted to a small in-domain dataset. In this scenario, overfitting is a major\nchallenge. We investigate a number of techniques to reduce overfitting and\nimprove transfer learning, including regularization techniques such as dropout\nand L2-regularization towards an out-of-domain prior. In addition, we introduce\ntuneout, a novel regularization technique inspired by dropout. We apply these\ntechniques, alone and in combination, to neural machine translation, obtaining\nimprovements on IWSLT datasets for English->German and English->Russian. We\nalso investigate the amounts of in-domain training data needed for domain\nadaptation in NMT, and find a logarithmic relationship between the amount of\ntraining data and gain in BLEU score.", "published": "2017-07-31 15:31:12", "link": "http://arxiv.org/abs/1707.09920v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SemEval-2017 Task 1: Semantic Textual Similarity - Multilingual and\n  Cross-lingual Focused Evaluation", "abstract": "Semantic Textual Similarity (STS) measures the meaning similarity of\nsentences. Applications include machine translation (MT), summarization,\ngeneration, question answering (QA), short answer grading, semantic search,\ndialog and conversational systems. The STS shared task is a venue for assessing\nthe current state-of-the-art. The 2017 task focuses on multilingual and\ncross-lingual pairs with one sub-track exploring MT quality estimation (MTQE)\ndata. The task obtained strong participation from 31 teams, with 17\nparticipating in all language tracks. We summarize performance and review a\nselection of well performing methods. Analysis highlights common errors,\nproviding insight into the limitations of existing models. To support ongoing\nwork on semantic representations, the STS Benchmark is introduced as a new\nshared training and evaluation set carefully selected from the corpus of\nEnglish STS shared task data (2012-2017).", "published": "2017-07-31 20:12:06", "link": "http://arxiv.org/abs/1708.00055v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "The Code2Text Challenge: Text Generation in Source Code Libraries", "abstract": "We propose a new shared task for tactical data-to-text generation in the\ndomain of source code libraries. Specifically, we focus on text generation of\nfunction descriptions from example software projects. Data is drawn from\nexisting resources used for studying the related problem of semantic parser\ninduction (Richardson and Kuhn, 2017b; Richardson and Kuhn, 2017a), and spans a\nwide variety of both natural languages and programming languages. In this\npaper, we describe these existing resources, which will serve as training and\ndevelopment data for the task, and discuss plans for building new independent\ntest sets.", "published": "2017-07-31 23:29:41", "link": "http://arxiv.org/abs/1708.00098v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Familia: An Open-Source Toolkit for Industrial Topic Modeling", "abstract": "Familia is an open-source toolkit for pragmatic topic modeling in industry.\nFamilia abstracts the utilities of topic modeling in industry as two paradigms:\nsemantic representation and semantic matching. Efficient implementations of the\ntwo paradigms are made publicly available for the first time. Furthermore, we\nprovide off-the-shelf topic models trained on large-scale industrial corpora,\nincluding Latent Dirichlet Allocation (LDA), SentenceLDA and Topical Word\nEmbedding (TWE). We further describe typical applications which are\nsuccessfully powered by topic modeling, in order to ease the confusions and\ndifficulties of software engineers during topic model selection and\nutilization.", "published": "2017-07-31 12:48:45", "link": "http://arxiv.org/abs/1707.09823v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Reporting Score Distributions Makes a Difference: Performance Study of\n  LSTM-networks for Sequence Tagging", "abstract": "In this paper we show that reporting a single performance score is\ninsufficient to compare non-deterministic approaches. We demonstrate for common\nsequence tagging tasks that the seed value for the random number generator can\nresult in statistically significant (p < 10^-4) differences for\nstate-of-the-art systems. For two recent systems for NER, we observe an\nabsolute difference of one percentage point F1-score depending on the selected\nseed value, making these systems perceived either as state-of-the-art or\nmediocre. Instead of publishing and reporting single performance scores, we\npropose to compare score distributions based on multiple executions. Based on\nthe evaluation of 50.000 LSTM-networks for five sequence tagging tasks, we\npresent network architectures that produce both superior performance as well as\nare more stable with respect to the remaining hyperparameters.", "published": "2017-07-31 14:25:24", "link": "http://arxiv.org/abs/1707.09861v1", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Bayesian Sparsification of Recurrent Neural Networks", "abstract": "Recurrent neural networks show state-of-the-art results in many text analysis\ntasks but often require a lot of memory to store their weights. Recently\nproposed Sparse Variational Dropout eliminates the majority of the weights in a\nfeed-forward neural network without significant loss of quality. We apply this\ntechnique to sparsify recurrent neural networks. To account for recurrent\nspecifics we also rely on Binary Variational Dropout for RNN. We report 99.5%\nsparsity level on sentiment analysis task without a quality drop and up to 87%\nsparsity level on language modeling task with slight loss of accuracy.", "published": "2017-07-31 21:33:42", "link": "http://arxiv.org/abs/1708.00077v1", "categories": ["stat.ML", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
