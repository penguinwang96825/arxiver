{"title": "SECNLP: A Survey of Embeddings in Clinical Natural Language Processing", "abstract": "Traditional representations like Bag of words are high dimensional, sparse\nand ignore the order as well as syntactic and semantic information. Distributed\nvector representations or embeddings map variable length text to dense fixed\nlength vectors as well as capture the prior knowledge which can transferred to\ndownstream tasks. Even though embedding has become de facto standard for\nrepresentations in deep learning based NLP tasks in both general and clinical\ndomains, there is no survey paper which presents a detailed review of\nembeddings in Clinical Natural Language Processing. In this survey paper, we\ndiscuss various medical corpora and their characteristics, medical codes and\npresent a brief overview as well as comparison of popular embeddings models. We\nclassify clinical embeddings into nine types and discuss each embedding type in\ndetail. We discuss various evaluation methods followed by possible solutions to\nvarious challenges in clinical embeddings. Finally, we conclude with some of\nthe future directions which will advance the research in clinical embeddings.", "published": "2019-03-04 01:37:52", "link": "http://arxiv.org/abs/1903.01039v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Knowledge Map to Mind Map: Artificial Imagination", "abstract": "Imagination is one of the most important factors which makes an artistic\npainting unique and impressive. With the rapid development of Artificial\nIntelligence, more and more researchers try to create painting with AI\ntechnology automatically. However, lacking of imagination is still a main\nproblem for AI painting. In this paper, we propose a novel approach to inject\nrich imagination into a special painting art Mind Map creation. We firstly\nconsider lexical and phonological similarities of seed word, then learn and\ninherit original painting style of the author, and finally apply Dadaism and\nimpossibility of improvisation principles into painting process. We also design\nseveral metrics for imagination evaluation. Experimental results show that our\nproposed method can increase imagination of painting and also improve its\noverall quality.", "published": "2019-03-04 05:38:29", "link": "http://arxiv.org/abs/1903.01080v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Relation Extraction Datasets in the Digital Humanities Domain and their\n  Evaluation with Word Embeddings", "abstract": "In this research, we manually create high-quality datasets in the digital\nhumanities domain for the evaluation of language models, specifically word\nembedding models. The first step comprises the creation of unigram and n-gram\ndatasets for two fantasy novel book series for two task types each, analogy and\ndoesn't-match. This is followed by the training of models on the two book\nseries with various popular word embedding model types such as word2vec, GloVe,\nfastText, or LexVec. Finally, we evaluate the suitability of word embedding\nmodels for such specific relation extraction tasks in a situation of comparably\nsmall corpus sizes. In the evaluations, we also investigate and analyze\nparticular aspects such as the impact of corpus term frequencies and task\ndifficulty on accuracy. The datasets, and the underlying system and word\nembedding models are available on github and can be easily extended with new\ndatasets and tasks, be used to reproduce the presented results, or be\ntransferred to other domains.", "published": "2019-03-04 14:46:20", "link": "http://arxiv.org/abs/1903.01284v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Polylingual Wordnet", "abstract": "Princeton WordNet is one of the most important resources for natural language\nprocessing, but is only available for English. While it has been translated\nusing the expand approach to many other languages, this is an expensive manual\nprocess. Therefore it would be beneficial to have a high-quality automatic\ntranslation approach that would support NLP techniques, which rely on WordNet\nin new languages. The translation of wordnets is fundamentally complex because\nof the need to translate all senses of a word including low frequency senses,\nwhich is very challenging for current machine translation approaches. For this\nreason we leverage existing translations of WordNet in other languages to\nidentify contextual information for wordnet senses from a large set of generic\nparallel corpora. We evaluate our approach using 10 translated wordnets for\nEuropean languages. Our experiment shows a significant improvement over\ntranslation without any contextual information. Furthermore, we evaluate how\nthe choice of pivot languages affects performance of multilingual word sense\ndisambiguation.", "published": "2019-03-04 18:10:52", "link": "http://arxiv.org/abs/1903.01411v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using Word Embeddings for Visual Data Exploration with Ontodia and\n  Wikidata", "abstract": "One of the big challenges in Linked Data consumption is to create visual and\nnatural language interfaces to the data usable for non-technical users. Ontodia\nprovides support for diagrammatic data exploration, showcased in this\npublication in combination with the Wikidata dataset. We present improvements\nto the natural language interface regarding exploring and querying Linked Data\nentities. The method uses models of distributional semantics to find and rank\nentity properties related to user input in Ontodia. Various word embedding\ntypes and model settings are evaluated, and the results show that user\nexperience in visual data exploration benefits from the proposed approach.", "published": "2019-03-04 14:36:21", "link": "http://arxiv.org/abs/1903.01275v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Traditional Machine Learning for Pitch Detection", "abstract": "Pitch detection is a fundamental problem in speech processing as F0 is used\nin a large number of applications. Recent articles have proposed deep learning\nfor robust pitch tracking. In this paper, we consider voicing detection as a\nclassification problem and F0 contour estimation as a regression problem. For\nboth tasks, acoustic features from multiple domains and traditional machine\nlearning methods are used. The discrimination power of existing and proposed\nfeatures is assessed through mutual information. Multiple supervised and\nunsupervised approaches are compared. A significant relative reduction of\nvoicing errors over the best baseline is obtained: 20% with the best clustering\nmethod (K-means) and 45% with a Multi-Layer Perceptron. For F0 contour\nestimation, the benefits of regression techniques are limited though. We\ninvestigate whether those objective gains translate in a parametric synthesis\ntask. Clear perceptual preferences are observed for the proposed approach over\ntwo widely-used baselines (RAPT and DIO).", "published": "2019-03-04 14:53:26", "link": "http://arxiv.org/abs/1903.01290v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Russian Language Datasets in the Digitial Humanities Domain and Their\n  Evaluation with Word Embeddings", "abstract": "In this paper, we present Russian language datasets in the digital humanities\ndomain for the evaluation of word embedding techniques or similar language\nmodeling and feature learning algorithms. The datasets are split into two task\ntypes, word intrusion and word analogy, and contain 31362 task units in total.\nThe characteristics of the tasks and datasets are that they build upon small,\ndomain-specific corpora, and that the datasets contain a high number of named\nentities. The datasets were created manually for two fantasy novel book series\n(\"A Song of Ice and Fire\" and \"Harry Potter\"). We provide baseline evaluations\nwith popular word embedding models trained on the book corpora for the given\ntasks, both for the Russian and English language versions of the datasets.\nFinally, we compare and analyze the results and discuss specifics of Russian\nlanguage with regards to the problem setting.", "published": "2019-03-04 14:18:48", "link": "http://arxiv.org/abs/1903.08739v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Long-tail Relation Extraction via Knowledge Graph Embeddings and Graph\n  Convolution Networks", "abstract": "We propose a distance supervised relation extraction approach for\nlong-tailed, imbalanced data which is prevalent in real-world settings. Here,\nthe challenge is to learn accurate \"few-shot\" models for classes existing at\nthe tail of the class distribution, for which little data is available.\nInspired by the rich semantic correlations between classes at the long tail and\nthose at the head, we take advantage of the knowledge from data-rich classes at\nthe head of the distribution to boost the performance of the data-poor classes\nat the tail. First, we propose to leverage implicit relational knowledge among\nclass labels from knowledge graph embeddings and learn explicit relational\nknowledge using graph convolution networks. Second, we integrate that\nrelational knowledge into relation extraction model by coarse-to-fine\nknowledge-aware attention mechanism. We demonstrate our results for a\nlarge-scale benchmark dataset which show that our approach significantly\noutperforms other baselines, especially for long-tail relations.", "published": "2019-03-04 15:32:39", "link": "http://arxiv.org/abs/1903.01306v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.DB", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Irrelevant speech effect in open plan offices: A laboratory study", "abstract": "It seems now accepted that speech noise in open plan offices is the main\nsource of discomfort for employees. This work follows a series of studies\nconducted at INRS France and INSA Lyon based on Hongisto's theoretical model\n(2005) linking the Decrease in Performance (DP) and the Speech Transmission\nIndex (STI). This model predicts that for STI values between 0.7 and 1, which\nmeans a speech signal close to 100% of intelligibility, the DP remains constant\nat about 7%. The experiment that we carried out aimed to gather more\ninformation about the relation between DP and STI, varying the STI value up to\n0.9. Fifty-five subjects between 25-59 years old participated in the\nexperiment. First, some psychological parameters were observed in order to\nbetter characterize the inter-subjects variability. Then, subjects performed a\nWorking-Memory (WM) task in silence and in four different sound conditions (STI\nfrom 0.25 to 0.9). This task was customized by an initial measure of mnemonic\nspan so that two different cognitive loads (low/high) were equally defined for\neach subject around their span value. Subjects also subjectively evaluated\ntheir mental load and discomfort at the end of each WM task, for each noise\ncondition. Results show a significant effect of the STI on the DP, the mental\nload and the discomfort. Furthermore, a significant correlation was found\nbetween the age of subjects and their performance during the WM task. This\nresult was confirmed by a cluster analysis that enabled us to separate the\nsubjects on two different groups, one group of younger and more efficient\nsubjects and one group of older and less efficient subjects. General results\ndid not show any increase of DP for the highest STI values, so the \"plateau\"\nhypothesis of Hongisto's model cannot be rejected on the basis of this\nexperiment.", "published": "2019-03-04 07:58:12", "link": "http://arxiv.org/abs/1903.11386v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Analysing Deep Learning-Spectral Envelope Prediction Methods for Singing\n  Synthesis", "abstract": "We conduct an investigation on various hyper-parameters regarding neural\nnetworks used to generate spectral envelopes for singing synthesis. Two\nperceptive tests, where the first compares two models directly and the other\nranks models with a mean opinion score, are performed. With these tests we show\nthat when learning to predict spectral envelopes, 2d-convolutions are superior\nover previously proposed 1d-convolutions and that predicting multiple frames in\nan iterated fashion during training is superior over injecting noise to the\ninput data. An experimental investigation whether learning to predict a\nprobability distribution vs.\\ single samples was performed but turned out to be\ninconclusive. A network architecture is proposed that incorporates the\nimprovements which we found to be useful and we show in our experiments that\nthis network produces better results than other stat-of-the-art methods.", "published": "2019-03-04 10:23:55", "link": "http://arxiv.org/abs/1903.01161v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improving singing voice separation using Deep U-Net and Wave-U-Net with\n  data augmentation", "abstract": "State-of-the-art singing voice separation is based on deep learning making\nuse of CNN structures with skip connections (like U-net model, Wave-U-Net\nmodel, or MSDENSELSTM). A key to the success of these models is the\navailability of a large amount of training data. In the following study, we are\ninterested in singing voice separation for mono signals and will investigate\ninto comparing the U-Net and the Wave-U-Net that are structurally similar, but\nwork on different input representations. First, we report a few results on\nvariations of the U-Net model. Second, we will discuss the potential of state\nof the art speech and music transformation algorithms for augmentation of\nexisting data sets and demonstrate that the effect of these augmentations\ndepends on the signal representations used by the model. The results\ndemonstrate a considerable improvement due to the augmentation for both models.\nBut pitch transposition is the most effective augmentation strategy for the\nU-Net model, while transposition, time stretching, and formant shifting have a\nmuch more balanced effect on the Wave-U-Net model. Finally, we compare the two\nmodels on the same dataset.", "published": "2019-03-04 18:17:28", "link": "http://arxiv.org/abs/1903.01415v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Data Augmentation for Drum Transcription with Convolutional Neural\n  Networks", "abstract": "A recurrent issue in deep learning is the scarcity of data, in particular\nprecisely annotated data. Few publicly available databases are correctly\nannotated and generating correct labels is very time consuming. The present\narticle investigates into data augmentation strategies for Neural Networks\ntraining, particularly for tasks related to drum transcription. These tasks\nneed very precise annotations. This article investigates state-of-the-art sound\ntransformation algorithms for remixing noise and sinusoidal parts, remixing\nattacks, transposing with and without time compensation and compares them to\nbasic regularization methods such as using dropout and additive Gaussian noise.\nAnd it shows how a drum transcription algorithm based on CNN benefits from the\nproposed data augmentation strategy.", "published": "2019-03-04 18:17:29", "link": "http://arxiv.org/abs/1903.01416v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
