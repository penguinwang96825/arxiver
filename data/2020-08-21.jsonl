{"title": "Spatial Language Representation with Multi-Level Geocoding", "abstract": "We present a multi-level geocoding model (MLG) that learns to associate texts\nto geographic locations. The Earth's surface is represented using space-filling\ncurves that decompose the sphere into a hierarchy of similarly sized,\nnon-overlapping cells. MLG balances generalization and accuracy by combining\nlosses across multiple levels and predicting cells at each level\nsimultaneously. Without using any dataset-specific tuning, we show that MLG\nobtains state-of-the-art results for toponym resolution on three English\ndatasets. Furthermore, it obtains large gains without any knowledge base\nmetadata, demonstrating that it can effectively learn the connection between\ntext spans and coordinates - and thus can be extended to toponymns not present\nin knowledge bases.", "published": "2020-08-21 00:05:08", "link": "http://arxiv.org/abs/2008.09236v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GRIT: Generative Role-filler Transformers for Document-level Event\n  Entity Extraction", "abstract": "We revisit the classic problem of document-level role-filler entity\nextraction (REE) for template filling. We argue that sentence-level approaches\nare ill-suited to the task and introduce a generative transformer-based\nencoder-decoder framework (GRIT) that is designed to model context at the\ndocument level: it can make extraction decisions across sentence boundaries; is\nimplicitly aware of noun phrase coreference structure, and has the capacity to\nrespect cross-role dependencies in the template structure. We evaluate our\napproach on the MUC-4 dataset, and show that our model performs substantially\nbetter than prior work. We also show that our modeling choices contribute to\nmodel performance, e.g., by implicitly capturing linguistic knowledge such as\nrecognizing coreferent entity mentions.", "published": "2020-08-21 01:07:36", "link": "http://arxiv.org/abs/2008.09249v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adapting Event Extractors to Medical Data: Bridging the Covariate Shift", "abstract": "We tackle the task of adapting event extractors to new domains without\nlabeled data, by aligning the marginal distributions of source and target\ndomains. As a testbed, we create two new event extraction datasets using\nEnglish texts from two medical domains: (i) clinical notes, and (ii)\ndoctor-patient conversations. We test the efficacy of three marginal alignment\ntechniques: (i) adversarial domain adaptation (ADA), (ii) domain adaptive\nfine-tuning (DAFT), and (iii) a novel instance weighting technique based on\nlanguage model likelihood scores (LIW). LIW and DAFT improve over a no-transfer\nBERT baseline on both domains, but ADA only improves on clinical notes. Deeper\nanalysis of performance under different types of shifts (e.g., lexical shift,\nsemantic shift) reveals interesting variations among models. Our\nbest-performing models reach F1 scores of 70.0 and 72.9 on notes and\nconversations respectively, using no labeled data from target domains.", "published": "2020-08-21 02:12:32", "link": "http://arxiv.org/abs/2008.09266v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Don't Change Me! User-Controllable Selective Paraphrase Generation", "abstract": "In the paraphrase generation task, source sentences often contain phrases\nthat should not be altered. Which phrases, however, can be context dependent\nand can vary by application. Our solution to this challenge is to provide the\nuser with explicit tags that can be placed around any arbitrary segment of text\nto mean \"don't change me!\" when generating a paraphrase; the model learns to\nexplicitly copy these phrases to the output. The contribution of this work is a\nnovel data generation technique using distant supervision that allows us to\nstart with a pretrained sequence-to-sequence model and fine-tune a paraphrase\ngenerator that exhibits this behavior, allowing user-controllable paraphrase\ngeneration. Additionally, we modify the loss during fine-tuning to explicitly\nencourage diversity in model output. Our technique is language agnostic, and we\nreport experiments in English and Chinese.", "published": "2020-08-21 03:31:50", "link": "http://arxiv.org/abs/2008.09290v2", "categories": ["cs.CL", "I.2"], "primary_category": "cs.CL"}
{"title": "A Variational Approach to Unsupervised Sentiment Analysis", "abstract": "In this paper, we propose a variational approach to unsupervised sentiment\nanalysis. Instead of using ground truth provided by domain experts, we use\ntarget-opinion word pairs as a supervision signal. For example, in a document\nsnippet \"the room is big,\" (room, big) is a target-opinion word pair. These\nword pairs can be extracted by using dependency parsers and simple rules. Our\nobjective function is to predict an opinion word given a target word while our\nultimate goal is to learn a sentiment classifier. By introducing a latent\nvariable, i.e., the sentiment polarity, to the objective function, we can\ninject the sentiment classifier to the objective function via the evidence\nlower bound. We can learn a sentiment classifier by optimizing the lower bound.\nWe also impose sophisticated constraints on opinion words as regularization\nwhich encourages that if two documents have similar (dissimilar) opinion words,\nthe sentiment classifiers should produce similar (different) probability\ndistribution. We apply our method to sentiment analysis on customer reviews and\nclinical narratives. The experiment results show our method can outperform\nunsupervised baselines in sentiment analysis task on both domains, and our\nmethod obtains comparable results to the supervised method with hundreds of\nlabels per aspect in customer reviews domain, and obtains comparable results to\nsupervised methods in clinical narratives domain.", "published": "2020-08-21 09:52:35", "link": "http://arxiv.org/abs/2008.09394v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Keywords lie far from the mean of all words in local vector space", "abstract": "Keyword extraction is an important document process that aims at finding a\nsmall set of terms that concisely describe a document's topics. The most\npopular state-of-the-art unsupervised approaches belong to the family of the\ngraph-based methods that build a graph-of-words and use various centrality\nmeasures to score the nodes (candidate keywords). In this work, we follow a\ndifferent path to detect the keywords from a text document by modeling the main\ndistribution of the document's words using local word vector representations.\nThen, we rank the candidates based on their position in the text and the\ndistance between the corresponding local vectors and the main distribution's\ncenter. We confirm the high performance of our approach compared to strong\nbaselines and state-of-the-art unsupervised keyword extraction methods, through\nan extended experimental study, investigating the properties of the local\nrepresentations.", "published": "2020-08-21 14:42:33", "link": "http://arxiv.org/abs/2008.09513v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Team DoNotDistribute at SemEval-2020 Task 11: Features, Finetuning, and\n  Data Augmentation in Neural Models for Propaganda Detection in News Articles", "abstract": "This paper presents our systems for SemEval 2020 Shared Task 11: Detection of\nPropaganda Techniques in News Articles. We participate in both the span\nidentification and technique classification subtasks and report on experiments\nusing different BERT-based models along with handcrafted features. Our models\nperform well above the baselines for both tasks, and we contribute ablation\nstudies and discussion of our results to dissect the effectiveness of different\nfeatures and techniques with the goal of aiding future studies in propaganda\ndetection.", "published": "2020-08-21 22:35:57", "link": "http://arxiv.org/abs/2008.09703v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Turkish Text Classification: From Lexicon Analysis to Bidirectional\n  Transformer", "abstract": "Text classification has seen an increased use in both academic and industry\nsettings. Though rule based methods have been fairly successful, supervised\nmachine learning has been shown to be most successful for most languages, where\nmost research was done on English. In this article, the success of lexicon\nanalysis, support vector machines, and extreme gradient boosting for the task\nof text classification and sentiment analysis are evaluated in Turkish and a\npretrained transformer based classifier is proposed, outperforming previous\nmethods for Turkish text classification. In the context of text classification,\nall machine learning models proposed in the article are domain-independent and\ndo not require any task-specific modifications.", "published": "2020-08-21 13:30:44", "link": "http://arxiv.org/abs/2104.11642v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Tweet to News Conversion: An Investigation into Unsupervised\n  Controllable Text Generation", "abstract": "Text generator systems have become extremely popular with the advent of\nrecent deep learning models such as encoder-decoder. Controlling the\ninformation and style of the generated output without supervision is an\nimportant and challenging Natural Language Processing (NLP) task. In this\npaper, we define the task of constructing a coherent paragraph from a set of\ndisaster domain tweets, without any parallel data. We tackle the problem by\nbuilding two systems in pipeline. The first system focuses on unsupervised\nstyle transfer and converts the individual tweets into news sentences. The\nsecond system stitches together the outputs from the first system to form a\ncoherent news paragraph. We also propose a novel training mechanism, by\nsplitting the sentences into propositions and training the second system to\nmerge the sentences. We create a validation and test set consisting of\ntweet-sets and their equivalent news paragraphs to perform empirical\nevaluation. In a completely unsupervised setting, our model was able to achieve\na BLEU score of 19.32, while successfully transferring styles and joining\ntweets to form a meaningful news paragraph.", "published": "2020-08-21 06:56:57", "link": "http://arxiv.org/abs/2008.09333v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MTOP: A Comprehensive Multilingual Task-Oriented Semantic Parsing\n  Benchmark", "abstract": "Scaling semantic parsing models for task-oriented dialog systems to new\nlanguages is often expensive and time-consuming due to the lack of available\ndatasets. Available datasets suffer from several shortcomings: a) they contain\nfew languages b) they contain small amounts of labeled examples per language c)\nthey are based on the simple intent and slot detection paradigm for\nnon-compositional queries. In this paper, we present a new multilingual\ndataset, called MTOP, comprising of 100k annotated utterances in 6 languages\nacross 11 domains. We use this dataset and other publicly available datasets to\nconduct a comprehensive benchmarking study on using various state-of-the-art\nmultilingual pre-trained models for task-oriented semantic parsing. We achieve\nan average improvement of +6.3 points on Slot F1 for the two existing\nmultilingual datasets, over best results reported in their experiments.\nFurthermore, we demonstrate strong zero-shot performance using pre-trained\nmodels combined with automatic translation and alignment, and a proposed\ndistant supervision method to reduce the noise in slot label projection.", "published": "2020-08-21 07:02:11", "link": "http://arxiv.org/abs/2008.09335v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Improving Selective Prediction Ability of NLP Systems", "abstract": "It's better to say \"I can't answer\" than to answer incorrectly. This\nselective prediction ability is crucial for NLP systems to be reliably deployed\nin real-world applications. Prior work has shown that existing selective\nprediction techniques fail to perform well, especially in the out-of-domain\nsetting. In this work, we propose a method that improves probability estimates\nof models by calibrating them using prediction confidence and difficulty score\nof instances. Using these two signals, we first annotate held-out instances and\nthen train a calibrator to predict the likelihood of correctness of the model's\nprediction. We instantiate our method with Natural Language Inference (NLI) and\nDuplicate Detection (DD) tasks and evaluate it in both In-Domain (IID) and\nOut-of-Domain (OOD) settings. In (IID, OOD) settings, we show that the\nrepresentations learned by our calibrator result in an improvement of (15.81%,\n5.64%) and (6.19%, 13.9%) over 'MaxProb' -- a selective prediction baseline --\non NLI and DD tasks respectively.", "published": "2020-08-21 08:46:36", "link": "http://arxiv.org/abs/2008.09371v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "EmoGraph: Capturing Emotion Correlations using Graph Networks", "abstract": "Most emotion recognition methods tackle the emotion understanding task by\nconsidering individual emotion independently while ignoring their fuzziness\nnature and the interconnections among them. In this paper, we explore how\nemotion correlations can be captured and help different classification tasks.\nWe propose EmoGraph that captures the dependencies among different emotions\nthrough graph networks. These graphs are constructed by leveraging the\nco-occurrence statistics among different emotion categories. Empirical results\non two multi-label classification datasets demonstrate that EmoGraph\noutperforms strong baselines, especially for macro-F1. An additional experiment\nillustrates the captured emotion correlations can also benefit a single-label\nclassification task.", "published": "2020-08-21 08:59:29", "link": "http://arxiv.org/abs/2008.09378v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Howl: A Deployed, Open-Source Wake Word Detection System", "abstract": "We describe Howl, an open-source wake word detection toolkit with native\nsupport for open speech datasets, like Mozilla Common Voice and Google Speech\nCommands. We report benchmark results on Speech Commands and our own freely\navailable wake word detection dataset, built from MCV. We operationalize our\nsystem for Firefox Voice, a plugin enabling speech interactivity for the\nFirefox web browser. Howl represents, to the best of our knowledge, the first\nfully productionized yet open-source wake word detection toolkit with a web\nbrowser deployment target. Our codebase is at\nhttps://github.com/castorini/howl.", "published": "2020-08-21 17:59:01", "link": "http://arxiv.org/abs/2008.09606v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Abstractive Summarization of Spoken and Written Instructions with BERT", "abstract": "Summarization of speech is a difficult problem due to the spontaneity of the\nflow, disfluencies, and other issues that are not usually encountered in\nwritten texts. Our work presents the first application of the BERTSum model to\nconversational language. We generate abstractive summaries of narrated\ninstructional videos across a wide variety of topics, from gardening and\ncooking to software configuration and sports. In order to enrich the\nvocabulary, we use transfer learning and pretrain the model on a few large\ncross-domain datasets in both written and spoken English. We also do\npreprocessing of transcripts to restore sentence segmentation and punctuation\nin the output of an ASR system. The results are evaluated with ROUGE and\nContent-F1 scoring for the How2 and WikiHow datasets. We engage human judges to\nscore a set of summaries randomly selected from a dataset curated from\nHowTo100M and YouTube. Based on blind evaluation, we achieve a level of textual\nfluency and utility close to that of summaries written by human content\ncreators. The model beats current SOTA when applied to WikiHow articles that\nvary widely in style and topic, while showing no performance regression on the\ncanonical CNN/DailyMail dataset. Due to the high generalizability of the model\nacross different styles and domains, it has great potential to improve\naccessibility and discoverability of internet content. We envision this\nintegrated as a feature in intelligent virtual assistants, enabling them to\nsummarize both written and spoken instructional content upon request.", "published": "2020-08-21 20:59:34", "link": "http://arxiv.org/abs/2008.09676v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Fine-tune BERT for E-commerce Non-Default Search Ranking", "abstract": "The quality of non-default ranking on e-commerce platforms, such as based on\nascending item price or descending historical sales volume, often suffers from\nacute relevance problems, since the irrelevant items are much easier to be\nexposed at the top of the ranking results. In this work, we propose a two-stage\nranking scheme, which first recalls wide range of candidate items through\nrefined query/title keyword matching, and then classifies the recalled items\nusing BERT-Large fine-tuned on human label data. We also implemented parallel\nprediction on multiple GPU hosts and a C++ tokenization custom op of\nTensorflow. In this data challenge, our model won the 1st place in the\nsupervised phase (based on overall F1 score) and 2nd place in the final phase\n(based on average per query F1 score).", "published": "2020-08-21 21:48:00", "link": "http://arxiv.org/abs/2008.09689v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Detecting and Classifying Malevolent Dialogue Responses: Taxonomy, Data\n  and Methodology", "abstract": "Conversational interfaces are increasingly popular as a way of connecting\npeople to information. Corpus-based conversational interfaces are able to\ngenerate more diverse and natural responses than template-based or\nretrieval-based agents. With their increased generative capacity of corpusbased\nconversational agents comes the need to classify and filter out malevolent\nresponses that are inappropriate in terms of content and dialogue acts.\nPrevious studies on the topic of recognizing and classifying inappropriate\ncontent are mostly focused on a certain category of malevolence or on single\nsentences instead of an entire dialogue. In this paper, we define the task of\nMalevolent Dialogue Response Detection and Classification (MDRDC). We make\nthree contributions to advance research on this task. First, we present a\nHierarchical Malevolent Dialogue Taxonomy (HMDT). Second, we create a labelled\nmulti-turn dialogue dataset and formulate the MDRDC task as a hierarchical\nclassification task over this taxonomy. Third, we apply stateof-the-art text\nclassification methods to the MDRDC task and report on extensive experiments\naimed at assessing the performance of these approaches.", "published": "2020-08-21 22:43:27", "link": "http://arxiv.org/abs/2008.09706v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "COOKIE: A Dataset for Conversational Recommendation over Knowledge\n  Graphs in E-commerce", "abstract": "In this work, we present a new dataset for conversational recommendation over\nknowledge graphs in e-commerce platforms called COOKIE. The dataset is\nconstructed from an Amazon review corpus by integrating both user-agent\ndialogue and custom knowledge graphs for recommendation. Specifically, we first\nconstruct a unified knowledge graph and extract key entities between\nuser--product pairs, which serve as the skeleton of a conversation. Then we\nsimulate conversations mirroring the human coarse-to-fine process of choosing\npreferred items. The proposed baselines and experiments demonstrate that our\ndataset is able to provide innovative opportunities for conversational\nrecommendation.", "published": "2020-08-21 00:11:31", "link": "http://arxiv.org/abs/2008.09237v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Neural Machine Translation without Embeddings", "abstract": "Many NLP models operate over sequences of subword tokens produced by\nhand-crafted tokenization rules and heuristic subword induction algorithms. A\nsimple universal alternative is to represent every computerized text as a\nsequence of bytes via UTF-8, obviating the need for an embedding layer since\nthere are fewer token types (256) than dimensions. Surprisingly, replacing the\nubiquitous embedding layer with one-hot representations of each byte does not\nhurt performance; experiments on byte-to-byte machine translation from English\nto 10 different languages show a consistent improvement in BLEU, rivaling\ncharacter-level and even standard subword-level models. A deeper investigation\nreveals that the combination of embeddingless models with decoder-input dropout\namounts to token dropout, which benefits byte-to-byte models in particular.", "published": "2020-08-21 09:54:11", "link": "http://arxiv.org/abs/2008.09396v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Entropia: A Family of Entropy-Based Conformance Checking Measures for\n  Process Mining", "abstract": "This paper presents a command-line tool, called Entropia, that implements a\nfamily of conformance checking measures for process mining founded on the\nnotion of entropy from information theory. The measures allow quantifying\nclassical non-deterministic and stochastic precision and recall quality\ncriteria for process models automatically discovered from traces executed by\nIT-systems and recorded in their event logs. A process model has \"good\"\nprecision with respect to the log it was discovered from if it does not encode\nmany traces that are not part of the log, and has \"good\" recall if it encodes\nmost of the traces from the log. By definition, the measures possess useful\nproperties and can often be computed quickly.", "published": "2020-08-21 15:54:47", "link": "http://arxiv.org/abs/2008.09558v2", "categories": ["cs.AI", "cs.CL", "cs.FL", "cs.IT", "math.IT"], "primary_category": "cs.AI"}
{"title": "CITISEN: A Deep Learning-Based Speech Signal-Processing Mobile\n  Application", "abstract": "This study presents a deep learning-based speech signal-processing mobile\napplication known as CITISEN. The CITISEN provides three functions: speech\nenhancement (SE), model adaptation (MA), and background noise conversion (BNC),\nallowing CITISEN to be used as a platform for utilizing and evaluating SE\nmodels and flexibly extend the models to address various noise environments and\nusers. For SE, a pretrained SE model downloaded from the cloud server is used\nto effectively reduce noise components from instant or saved recordings\nprovided by users. For encountering unseen noise or speaker environments, the\nMA function is applied to promote CITISEN. A few audio samples recording on a\nnoisy environment are uploaded and used to adapt the pretrained SE model on the\nserver. Finally, for BNC, CITISEN first removes the background noises through\nan SE model and then mixes the processed speech with new background noise. The\nnovel BNC function can evaluate SE performance under specific conditions, cover\npeople's tracks, and provide entertainment. The experimental results confirmed\nthe effectiveness of SE, MA, and BNC functions. Compared with the noisy speech\nsignals, the enhanced speech signals achieved about 6\\% and 33\\% of\nimprovements, respectively, in terms of short-time objective intelligibility\n(STOI) and perceptual evaluation of speech quality (PESQ). With MA, the STOI\nand PESQ could be further improved by approximately 6\\% and 11\\%, respectively.\nFinally, the BNC experiment results indicated that the speech signals converted\nfrom noisy and silent backgrounds have a close scene identification accuracy\nand similar embeddings in an acoustic scene classification model. Therefore,\nthe proposed BNC can effectively convert the background noise of a speech\nsignal and be a data augmentation method when clean speech signals are\nunavailable.", "published": "2020-08-21 02:04:12", "link": "http://arxiv.org/abs/2008.09264v5", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "An Overview of Deep-Learning-Based Audio-Visual Speech Enhancement and\n  Separation", "abstract": "Speech enhancement and speech separation are two related tasks, whose purpose\nis to extract either one or more target speech signals, respectively, from a\nmixture of sounds generated by several sources. Traditionally, these tasks have\nbeen tackled using signal processing and machine learning techniques applied to\nthe available acoustic signals. Since the visual aspect of speech is\nessentially unaffected by the acoustic environment, visual information from the\ntarget speakers, such as lip movements and facial expressions, has also been\nused for speech enhancement and speech separation systems. In order to\nefficiently fuse acoustic and visual information, researchers have exploited\nthe flexibility of data-driven approaches, specifically deep learning,\nachieving strong performance. The ceaseless proposal of a large number of\ntechniques to extract features and fuse multimodal information has highlighted\nthe need for an overview that comprehensively describes and discusses\naudio-visual speech enhancement and separation based on deep learning. In this\npaper, we provide a systematic survey of this research topic, focusing on the\nmain elements that characterise the systems in the literature: acoustic\nfeatures; visual features; deep learning methods; fusion techniques; training\ntargets and objective functions. In addition, we review deep-learning-based\nmethods for speech reconstruction from silent videos and audio-visual sound\nsource separation for non-speech signals, since these methods can be more or\nless directly applied to audio-visual speech enhancement and separation.\nFinally, we survey commonly employed audio-visual speech datasets, given their\ncentral role in the development of data-driven approaches, and evaluation\nmethods, because they are generally used to compare different systems and\ndetermine their performance.", "published": "2020-08-21 17:24:09", "link": "http://arxiv.org/abs/2008.09586v2", "categories": ["eess.AS", "cs.LG", "eess.IV"], "primary_category": "eess.AS"}
