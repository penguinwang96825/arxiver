{"title": "A logical word embedding for learning grammar", "abstract": "We introduce the logical grammar emdebbing (LGE), a model inspired by\npregroup grammars and categorial grammars to enable unsupervised inference of\nlexical categories and syntactic rules from a corpus of text. LGE produces\ncomprehensible output summarizing its inferences, has a completely transparent\nprocess for producing novel sentences, and can learn from as few as a hundred\nsentences.", "published": "2023-04-28 01:53:54", "link": "http://arxiv.org/abs/2304.14590v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CED: Catalog Extraction from Documents", "abstract": "Sentence-by-sentence information extraction from long documents is an\nexhausting and error-prone task. As the indicator of document skeleton,\ncatalogs naturally chunk documents into segments and provide informative\ncascade semantics, which can help to reduce the search space. Despite their\nusefulness, catalogs are hard to be extracted without the assist from external\nknowledge. For documents that adhere to a specific template, regular\nexpressions are practical to extract catalogs. However, handcrafted heuristics\nare not applicable when processing documents from different sources with\ndiverse formats. To address this problem, we build a large manually annotated\ncorpus, which is the first dataset for the Catalog Extraction from Documents\n(CED) task. Based on this corpus, we propose a transition-based framework for\nparsing documents into catalog trees. The experimental results demonstrate that\nour proposed method outperforms baseline systems and shows a good ability to\ntransfer. We believe the CED task could fill the gap between raw text segments\nand information extraction tasks on extremely long documents. Data and code are\navailable at \\url{https://github.com/Spico197/CatalogExtraction}", "published": "2023-04-28 07:32:00", "link": "http://arxiv.org/abs/2304.14662v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Search-in-the-Chain: Interactively Enhancing Large Language Models with\n  Search for Knowledge-intensive Tasks", "abstract": "Making the content generated by Large Language Model (LLM), accurate,\ncredible and traceable is crucial, especially in complex knowledge-intensive\ntasks that require multi-step reasoning and each step needs knowledge to solve.\nRetrieval-augmented generation is good potential to solve this problem.\nHowever, where and how to introduce Information Retrieval (IR) to LLM is a big\nchallenge. Previous work has the problems that wrong knowledge retrieved by IR\nmisleads the LLM and interaction between IR and LLM breaks the reasoning chain\nof LLM. This paper proposes a novel framework named\n\\textbf{Search-in-the-Chain} (SearChain) for the interaction between LLM and IR\nto solve the challenges. First, LLM generates the reasoning chain named\nChain-of-Query (CoQ) where each node consists of an IR-oriented query-answer\npair. Second, IR verifies the answer of each node of CoQ. It corrects the\nanswer that is not consistent with the retrieved information when IR gives high\nconfidence, which improves the credibility. Third, LLM can indicate its missing\nknowledge in CoQ and rely on IR to provide this knowledge to LLM. These\noperations improve the accuracy in terms of reasoning and knowledge. Finally,\nSearChain generates the reasoning process and marks references to supporting\ndocuments for each reasoning step, which improves traceability. Interaction\nwith IR in SearChain forms a novel reasoning path based on a tree, which\nenables LLM to dynamically modify the direction of reasoning. Experiments show\nthat SearChain outperforms state-of-the-art baselines on complex\nknowledge-intensive tasks including multi-hop Q\\&A, slot filling, fact\nchecking, and long-form Q\\&A.", "published": "2023-04-28 10:15:25", "link": "http://arxiv.org/abs/2304.14732v7", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dissecting Recall of Factual Associations in Auto-Regressive Language\n  Models", "abstract": "Transformer-based language models (LMs) are known to capture factual\nknowledge in their parameters. While previous work looked into where factual\nassociations are stored, only little is known about how they are retrieved\ninternally during inference. We investigate this question through the lens of\ninformation flow. Given a subject-relation query, we study how the model\naggregates information about the subject and relation to predict the correct\nattribute. With interventions on attention edges, we first identify two\ncritical points where information propagates to the prediction: one from the\nrelation positions followed by another from the subject positions. Next, by\nanalyzing the information at these points, we unveil a three-step internal\nmechanism for attribute extraction. First, the representation at the\nlast-subject position goes through an enrichment process, driven by the early\nMLP sublayers, to encode many subject-related attributes. Second, information\nfrom the relation propagates to the prediction. Third, the prediction\nrepresentation \"queries\" the enriched subject to extract the attribute. Perhaps\nsurprisingly, this extraction is typically done via attention heads, which\noften encode subject-attribute mappings in their parameters. Overall, our\nfindings introduce a comprehensive view of how factual associations are stored\nand extracted internally in LMs, facilitating future research on knowledge\nlocalization and editing.", "published": "2023-04-28 11:26:17", "link": "http://arxiv.org/abs/2304.14767v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RexUIE: A Recursive Method with Explicit Schema Instructor for Universal\n  Information Extraction", "abstract": "Universal Information Extraction (UIE) is an area of interest due to the\nchallenges posed by varying targets, heterogeneous structures, and\ndemand-specific schemas. However, previous works have only achieved limited\nsuccess by unifying a few tasks, such as Named Entity Recognition (NER) and\nRelation Extraction (RE), which fall short of being authentic UIE models\nparticularly when extracting other general schemas such as quadruples and\nquintuples. Additionally, these models used an implicit structural schema\ninstructor, which could lead to incorrect links between types, hindering the\nmodel's generalization and performance in low-resource scenarios. In this\npaper, we redefine the authentic UIE with a formal formulation that encompasses\nalmost all extraction schemas. To the best of our knowledge, we are the first\nto introduce UIE for any kind of schemas. In addition, we propose RexUIE, which\nis a Recursive Method with Explicit Schema Instructor for UIE. To avoid\ninterference between different types, we reset the position ids and attention\nmask matrices. RexUIE shows strong performance under both full-shot and\nfew-shot settings and achieves State-of-the-Art results on the tasks of\nextracting complex schemas.", "published": "2023-04-28 11:28:56", "link": "http://arxiv.org/abs/2304.14770v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SemEval-2023 Task 11: Learning With Disagreements (LeWiDi)", "abstract": "NLP datasets annotated with human judgments are rife with disagreements\nbetween the judges. This is especially true for tasks depending on subjective\njudgments such as sentiment analysis or offensive language detection.\nParticularly in these latter cases, the NLP community has come to realize that\nthe approach of 'reconciling' these different subjective interpretations is\ninappropriate. Many NLP researchers have therefore concluded that rather than\neliminating disagreements from annotated corpora, we should preserve\nthem-indeed, some argue that corpora should aim to preserve all annotator\njudgments. But this approach to corpus creation for NLP has not yet been widely\naccepted. The objective of the LeWiDi series of shared tasks is to promote this\napproach to developing NLP models by providing a unified framework for training\nand evaluating with such datasets. We report on the second LeWiDi shared task,\nwhich differs from the first edition in three crucial respects: (i) it focuses\nentirely on NLP, instead of both NLP and computer vision tasks in its first\nedition; (ii) it focuses on subjective tasks, instead of covering different\ntypes of disagreements-as training with aggregated labels for subjective NLP\ntasks is a particularly obvious misrepresentation of the data; and (iii) for\nthe evaluation, we concentrate on soft approaches to evaluation. This second\nedition of LeWiDi attracted a wide array of participants resulting in 13 shared\ntask submission papers.", "published": "2023-04-28 12:20:35", "link": "http://arxiv.org/abs/2304.14803v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ChatGPT Evaluation on Sentence Level Relations: A Focus on Temporal,\n  Causal, and Discourse Relations", "abstract": "This paper aims to quantitatively evaluate the performance of ChatGPT, an\ninteractive large language model, on inter-sentential relations such as\ntemporal relations, causal relations, and discourse relations. Given ChatGPT's\npromising performance across various tasks, we proceed to carry out thorough\nevaluations on the whole test sets of 11 datasets, including temporal and\ncausal relations, PDTB2.0-based, and dialogue-based discourse relations. To\nensure the reliability of our findings, we employ three tailored prompt\ntemplates for each task, including the zero-shot prompt template, zero-shot\nprompt engineering (PE) template, and in-context learning (ICL) prompt\ntemplate, to establish the initial baseline scores for all popular\nsentence-pair relation classification tasks for the first time. Through our\nstudy, we discover that ChatGPT exhibits exceptional proficiency in detecting\nand reasoning about causal relations, albeit it may not possess the same level\nof expertise in identifying the temporal order between two events. While it is\ncapable of identifying the majority of discourse relations with existing\nexplicit discourse connectives, the implicit discourse relation remains a\nformidable challenge. Concurrently, ChatGPT demonstrates subpar performance in\nthe dialogue discourse parsing task that requires structural understanding in a\ndialogue before being aware of the discourse relation.", "published": "2023-04-28 13:14:36", "link": "http://arxiv.org/abs/2304.14827v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HQP: A Human-Annotated Dataset for Detecting Online Propaganda", "abstract": "Online propaganda poses a severe threat to the integrity of societies.\nHowever, existing datasets for detecting online propaganda have a key\nlimitation: they were annotated using weak labels that can be noisy and even\nincorrect. To address this limitation, our work makes the following\ncontributions: (1) We present HQP: a novel dataset (N = 30,000) for detecting\nonline propaganda with high-quality labels. To the best of our knowledge, HQP\nis the first large-scale dataset for detecting online propaganda that was\ncreated through human annotation. (2) We show empirically that state-of-the-art\nlanguage models fail in detecting online propaganda when trained with weak\nlabels (AUC: 64.03). In contrast, state-of-the-art language models can\naccurately detect online propaganda when trained with our high-quality labels\n(AUC: 92.25), which is an improvement of ~44%. (3) We show that prompt-based\nlearning using a small sample of high-quality labels can still achieve a\nreasonable performance (AUC: 80.27) while significantly reducing the cost of\nlabeling. (4) We extend HQP to HQP+ to test how well propaganda across\ndifferent contexts can be detected. Crucially, our work highlights the\nimportance of high-quality labels for sensitive NLP tasks such as propaganda\ndetection.", "published": "2023-04-28 15:42:55", "link": "http://arxiv.org/abs/2304.14931v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CCpdf: Building a High Quality Corpus for Visually Rich Documents from\n  Web Crawl Data", "abstract": "In recent years, the field of document understanding has progressed a lot. A\nsignificant part of this progress has been possible thanks to the use of\nlanguage models pretrained on large amounts of documents. However, pretraining\ncorpora used in the domain of document understanding are single domain,\nmonolingual, or nonpublic. Our goal in this paper is to propose an efficient\npipeline for creating a big-scale, diverse, multilingual corpus of PDF files\nfrom all over the Internet using Common Crawl, as PDF files are the most\ncanonical types of documents as considered in document understanding. We\nanalysed extensively all of the steps of the pipeline and proposed a solution\nwhich is a trade-off between data quality and processing time. We also share a\nCCpdf corpus in a form or an index of PDF files along with a script for\ndownloading them, which produces a collection useful for language model\npretraining. The dataset and tools published with this paper offer researchers\nthe opportunity to develop even better multilingual language models.", "published": "2023-04-28 16:12:18", "link": "http://arxiv.org/abs/2304.14953v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text-Blueprint: An Interactive Platform for Plan-based Conditional\n  Generation", "abstract": "While conditional generation models can now generate natural language well\nenough to create fluent text, it is still difficult to control the generation\nprocess, leading to irrelevant, repetitive, and hallucinated content. Recent\nwork shows that planning can be a useful intermediate step to render\nconditional generation less opaque and more grounded. We present a web\nbrowser-based demonstration for query-focused summarization that uses a\nsequence of question-answer pairs, as a blueprint plan for guiding text\ngeneration (i.e., what to say and in what order). We illustrate how users may\ninteract with the generated text and associated plan visualizations, e.g., by\nediting and modifying the blueprint in order to improve or control the\ngenerated output.\n  A short video demonstrating our system is available at\nhttps://goo.gle/text-blueprint-demo.", "published": "2023-04-28 18:14:48", "link": "http://arxiv.org/abs/2305.00034v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HausaNLP at SemEval-2023 Task 10: Transfer Learning, Synthetic Data and\n  Side-Information for Multi-Level Sexism Classification", "abstract": "We present the findings of our participation in the SemEval-2023 Task 10:\nExplainable Detection of Online Sexism (EDOS) task, a shared task on offensive\nlanguage (sexism) detection on English Gab and Reddit dataset. We investigated\nthe effects of transferring two language models: XLM-T (sentiment\nclassification) and HateBERT (same domain -- Reddit) for multi-level\nclassification into Sexist or not Sexist, and other subsequent\nsub-classifications of the sexist data. We also use synthetic classification of\nunlabelled dataset and intermediary class information to maximize the\nperformance of our models. We submitted a system in Task A, and it ranked 49th\nwith F1-score of 0.82. This result showed to be competitive as it only\nunder-performed the best system by 0.052% F1-score.", "published": "2023-04-28 20:03:46", "link": "http://arxiv.org/abs/2305.00076v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NLNDE at SemEval-2023 Task 12: Adaptive Pretraining and Source Language\n  Selection for Low-Resource Multilingual Sentiment Analysis", "abstract": "This paper describes our system developed for the SemEval-2023 Task 12\n\"Sentiment Analysis for Low-resource African Languages using Twitter Dataset\".\nSentiment analysis is one of the most widely studied applications in natural\nlanguage processing. However, most prior work still focuses on a small number\nof high-resource languages. Building reliable sentiment analysis systems for\nlow-resource languages remains challenging, due to the limited training data in\nthis task. In this work, we propose to leverage language-adaptive and\ntask-adaptive pretraining on African texts and study transfer learning with\nsource language selection on top of an African language-centric pretrained\nlanguage model. Our key findings are: (1) Adapting the pretrained model to the\ntarget language and task using a small yet relevant corpus improves performance\nremarkably by more than 10 F1 score points. (2) Selecting source languages with\npositive transfer gains during training can avoid harmful interference from\ndissimilar languages, leading to better results in multilingual and\ncross-lingual settings. In the shared task, our system wins 8 out of 15 tracks\nand, in particular, performs best in the multilingual evaluation.", "published": "2023-04-28 21:02:58", "link": "http://arxiv.org/abs/2305.00090v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4", "abstract": "In this work, we carry out a data archaeology to infer books that are known\nto ChatGPT and GPT-4 using a name cloze membership inference query. We find\nthat OpenAI models have memorized a wide collection of copyrighted materials,\nand that the degree of memorization is tied to the frequency with which\npassages of those books appear on the web. The ability of these models to\nmemorize an unknown set of books complicates assessments of measurement\nvalidity for cultural analytics by contaminating test data; we show that models\nperform much better on memorized books than on non-memorized books for\ndownstream tasks. We argue that this supports a case for open models whose\ntraining data is known.", "published": "2023-04-28 22:35:03", "link": "http://arxiv.org/abs/2305.00118v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Knowledge Graph Entity Alignment with Graph Augmentation", "abstract": "Entity alignment (EA) which links equivalent entities across different\nknowledge graphs (KGs) plays a crucial role in knowledge fusion. In recent\nyears, graph neural networks (GNNs) have been successfully applied in many\nembedding-based EA methods. However, existing GNN-based methods either suffer\nfrom the structural heterogeneity issue that especially appears in the real KG\ndistributions or ignore the heterogeneous representation learning for unseen\n(unlabeled) entities, which would lead the model to overfit on few alignment\nseeds (i.e., training data) and thus cause unsatisfactory alignment\nperformance. To enhance the EA ability, we propose GAEA, a novel EA approach\nbased on graph augmentation. In this model, we design a simple Entity-Relation\n(ER) Encoder to generate latent representations for entities via jointly\nmodeling comprehensive structural information and rich relation semantics.\nMoreover, we use graph augmentation to create two graph views for margin-based\nalignment learning and contrastive entity representation learning, thus\nmitigating structural heterogeneity and further improving the model's alignment\nperformance. Extensive experiments conducted on benchmark datasets demonstrate\nthe effectiveness of our method.", "published": "2023-04-28 01:22:47", "link": "http://arxiv.org/abs/2304.14585v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Antisemitic Messages? A Guide to High-Quality Annotation and a Labeled\n  Dataset of Tweets", "abstract": "One of the major challenges in automatic hate speech detection is the lack of\ndatasets that cover a wide range of biased and unbiased messages and that are\nconsistently labeled. We propose a labeling procedure that addresses some of\nthe common weaknesses of labeled datasets. We focus on antisemitic speech on\nTwitter and create a labeled dataset of 6,941 tweets that cover a wide range of\ntopics common in conversations about Jews, Israel, and antisemitism between\nJanuary 2019 and December 2021 by drawing from representative samples with\nrelevant keywords. Our annotation process aims to strictly apply a commonly\nused definition of antisemitism by forcing annotators to specify which part of\nthe definition applies, and by giving them the option to personally disagree\nwith the definition on a case-by-case basis. Labeling tweets that call out\nantisemitism, report antisemitism, or are otherwise related to antisemitism\n(such as the Holocaust) but are not actually antisemitic can help reduce false\npositives in automated detection. The dataset includes 1,250 tweets (18%) that\nare antisemitic according to the International Holocaust Remembrance Alliance\n(IHRA) definition of antisemitism. It is important to note, however, that the\ndataset is not comprehensive. Many topics are still not covered, and it only\nincludes tweets collected from Twitter between January 2019 and December 2021.\nAdditionally, the dataset only includes tweets that were written in English.\nDespite these limitations, we hope that this is a meaningful contribution to\nimproving the automated detection of antisemitic speech.", "published": "2023-04-28 02:52:38", "link": "http://arxiv.org/abs/2304.14599v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Training and Evaluation of a Multilingual Tokenizer for GPT-SW3", "abstract": "This paper provides a detailed discussion of the multilingual tokenizer used\nfor GPT-SW3. It was trained on the Nordic Pile using the SentencePiece library\nand the BPE algorithm. We outline the tokenizer's most important features and\nshare details on its learned vocabulary. In addition, we systematically analyze\nthe properties and evaluate the performance of the tokenizer with regard to the\ndifferent languages present in the data.", "published": "2023-04-28 11:40:48", "link": "http://arxiv.org/abs/2304.14780v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Are the Best Multilingual Document Embeddings simply Based on Sentence\n  Embeddings?", "abstract": "Dense vector representations for textual data are crucial in modern NLP. Word\nembeddings and sentence embeddings estimated from raw texts are key in\nachieving state-of-the-art results in various tasks requiring semantic\nunderstanding. However, obtaining embeddings at the document level is\nchallenging due to computational requirements and lack of appropriate data.\nInstead, most approaches fall back on computing document embeddings based on\nsentence representations. Although there exist architectures and models to\nencode documents fully, they are in general limited to English and few other\nhigh-resourced languages. In this work, we provide a systematic comparison of\nmethods to produce document-level representations from sentences based on\nLASER, LaBSE, and Sentence BERT pre-trained multilingual models. We compare\ninput token number truncation, sentence averaging as well as some simple\nwindowing and in some cases new augmented and learnable approaches, on 3 multi-\nand cross-lingual tasks in 8 languages belonging to 3 different language\nfamilies. Our task-based extrinsic evaluations show that, independently of the\nlanguage, a clever combination of sentence embeddings is usually better than\nencoding the full document as a single unit, even when this is possible. We\ndemonstrate that while a simple sentence average results in a strong baseline\nfor classification tasks, more complex combinations are necessary for semantic\ntasks.", "published": "2023-04-28 12:11:21", "link": "http://arxiv.org/abs/2304.14796v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Information Redundancy and Biases in Public Document Information\n  Extraction Benchmarks", "abstract": "Advances in the Visually-rich Document Understanding (VrDU) field and\nparticularly the Key-Information Extraction (KIE) task are marked with the\nemergence of efficient Transformer-based approaches such as the LayoutLM\nmodels. Despite the good performance of KIE models when fine-tuned on public\nbenchmarks, they still struggle to generalize on complex real-life use-cases\nlacking sufficient document annotations. Our research highlighted that KIE\nstandard benchmarks such as SROIE and FUNSD contain significant similarity\nbetween training and testing documents and can be adjusted to better evaluate\nthe generalization of models. In this work, we designed experiments to quantify\nthe information redundancy in public benchmarks, revealing a 75% template\nreplication in SROIE official test set and 16% in FUNSD. We also proposed\nresampling strategies to provide benchmarks more representative of the\ngeneralization ability of models. We showed that models not suited for document\nanalysis struggle on the adjusted splits dropping on average 10,5% F1 score on\nSROIE and 3.5% on FUNSD compared to multi-modal models dropping only 7,5% F1 on\nSROIE and 0.5% F1 on FUNSD.", "published": "2023-04-28 15:48:26", "link": "http://arxiv.org/abs/2304.14936v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Empirical Analysis of the Strengths and Weaknesses of PEFT Techniques\n  for LLMs", "abstract": "As foundation models continue to exponentially scale in size, efficient\nmethods of adaptation become increasingly critical. Parameter-efficient\nfine-tuning (PEFT), a recent class of techniques that require only modifying a\nsmall percentage of the model parameters, is currently the most popular method\nfor adapting large language models (LLMs). Several PEFT techniques have\nrecently been proposed with varying tradeoffs. We provide a comprehensive and\nuniform benchmark of various PEFT techniques across a representative LLM, the\nFLAN-T5 model, and evaluate model performance across different data scales of\nclassification and generation datasets. Based on this, we provide a framework\nfor choosing the optimal fine-tuning techniques given the task type and data\navailability. Contrary to popular belief, we also empirically prove that PEFT\ntechniques converge slower than full tuning in low data scenarios, and posit\nthe amount of data required for PEFT methods to both perform well and converge\nefficiently. Lastly, we further optimize these PEFT techniques by selectively\nchoosing which parts of the model to train, and find that these techniques can\nbe applied with significantly fewer parameters while maintaining and even\nimproving performance.", "published": "2023-04-28 17:39:49", "link": "http://arxiv.org/abs/2304.14999v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Explainable Verbal Reasoner Plus (EVR+): A Natural Language Reasoning\n  Framework that Supports Diverse Compositional Reasoning", "abstract": "Languages models have been successfully applied to a variety of reasoning\ntasks in NLP, yet the language models still suffer from compositional\ngeneralization. In this paper we present Explainable Verbal Reasoner Plus\n(EVR+), a reasoning framework that enhances language models' compositional\nreasoning ability by (1) allowing the model to explicitly generate and execute\nsymbolic operators, and (2) allowing the model to decompose a complex task into\nseveral simpler ones in a flexible manner. Compared with its predecessor\nExplainable Verbal Reasoner (EVR) and other previous approaches adopting\nsimilar ideas, our framework supports more diverse types of reasoning such as\nnested loops and different types of recursion. To evaluate our reasoning\nframework, we build a synthetic dataset with five tasks that require\ncompositional reasoning. Results show that our reasoning framework can enhance\nthe language model's compositional generalization performance on the five\ntasks, using a fine-tuned language model. We also discussed the possibility and\nthe challenges to combine our reasoning framework with a few-shot prompted\nlanguage model.", "published": "2023-04-28 19:27:26", "link": "http://arxiv.org/abs/2305.00061v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Cost-Sensitive Self-Training for Optimizing Non-Decomposable Metrics", "abstract": "Self-training based semi-supervised learning algorithms have enabled the\nlearning of highly accurate deep neural networks, using only a fraction of\nlabeled data. However, the majority of work on self-training has focused on the\nobjective of improving accuracy, whereas practical machine learning systems can\nhave complex goals (e.g. maximizing the minimum of recall across classes, etc.)\nthat are non-decomposable in nature. In this work, we introduce the\nCost-Sensitive Self-Training (CSST) framework which generalizes the\nself-training-based methods for optimizing non-decomposable metrics. We prove\nthat our framework can better optimize the desired non-decomposable metric\nutilizing unlabeled data, under similar data distribution assumptions made for\nthe analysis of self-training. Using the proposed CSST framework, we obtain\npractical self-training methods (for both vision and NLP tasks) for optimizing\ndifferent non-decomposable metrics using deep neural networks. Our results\ndemonstrate that CSST achieves an improvement over the state-of-the-art in\nmajority of the cases across datasets and objectives.", "published": "2023-04-28 10:31:12", "link": "http://arxiv.org/abs/2304.14738v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Made of Steel? Learning Plausible Materials for Components in the\n  Vehicle Repair Domain", "abstract": "We propose a novel approach to learn domain-specific plausible materials for\ncomponents in the vehicle repair domain by probing Pretrained Language Models\n(PLMs) in a cloze task style setting to overcome the lack of annotated\ndatasets. We devise a new method to aggregate salient predictions from a set of\ncloze query templates and show that domain-adaptation using either a small,\nhigh-quality or a customized Wikipedia corpus boosts performance. When\nexploring resource-lean alternatives, we find a distilled PLM clearly\noutperforming a classic pattern-based algorithm. Further, given that 98% of our\ndomain-specific components are multiword expressions, we successfully exploit\nthe compositionality assumption as a way to address data sparsity.", "published": "2023-04-28 10:39:44", "link": "http://arxiv.org/abs/2304.14745v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "ResiDual: Transformer with Dual Residual Connections", "abstract": "Transformer networks have become the preferred architecture for many tasks\ndue to their state-of-the-art performance. However, the optimal way to\nimplement residual connections in Transformer, which are essential for\neffective training, is still debated. Two widely used variants are the\nPost-Layer-Normalization (Post-LN) and Pre-Layer-Normalization (Pre-LN)\nTransformers, which apply layer normalization after each residual block's\noutput or before each residual block's input, respectively. While both variants\nenjoy their advantages, they also suffer from severe limitations: Post-LN\ncauses gradient vanishing issue that hinders training deep Transformers, and\nPre-LN causes representation collapse issue that limits model capacity. In this\npaper, we propose ResiDual, a novel Transformer architecture with Pre-Post-LN\n(PPLN), which fuses the connections in Post-LN and Pre-LN together and inherits\ntheir advantages while avoids their limitations. We conduct both theoretical\nanalyses and empirical experiments to verify the effectiveness of ResiDual.\nTheoretically, we prove that ResiDual has a lower bound on the gradient to\navoid the vanishing issue due to the residual connection from Pre-LN. Moreover,\nResiDual also has diverse model representations to avoid the collapse issue due\nto the residual connection from Post-LN. Empirically, ResiDual outperforms both\nPost-LN and Pre-LN on several machine translation benchmarks across different\nnetwork depths and data sizes. Thanks to the good theoretical and empirical\nperformance, ResiDual Transformer can serve as a foundation architecture for\ndifferent AI models (e.g., large language models). Our code is available at\nhttps://github.com/microsoft/ResiDual.", "published": "2023-04-28 12:19:47", "link": "http://arxiv.org/abs/2304.14802v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "An Empirical Study of Multimodal Model Merging", "abstract": "Model merging (e.g., via interpolation or task arithmetic) fuses multiple\nmodels trained on different tasks to generate a multi-task solution. The\ntechnique has been proven successful in previous studies, where the models are\ntrained on similar tasks and with the same initialization. In this paper, we\nexpand on this concept to a multimodal setup by merging transformers trained on\ndifferent modalities. Furthermore, we conduct our study for a novel goal where\nwe can merge vision, language, and cross-modal transformers of a\nmodality-specific architecture to create a parameter-efficient\nmodality-agnostic architecture. Through comprehensive experiments, we\nsystematically investigate the key factors impacting model performance after\nmerging, including initialization, merging mechanisms, and model architectures.\nWe also propose two metrics that assess the distance between weights to be\nmerged and can serve as an indicator of the merging outcomes. Our analysis\nleads to an effective training recipe for matching the performance of the\nmodality-agnostic baseline (i.e., pre-trained from scratch) via model merging.\nOur method also outperforms naive merging significantly on various tasks, with\nimprovements of 3% on VQA, 7% on COCO retrieval, 25% on NLVR2, 14% on Flickr30k\nand 3% on ADE20k. Our code is available at https://github.com/ylsung/vl-merging", "published": "2023-04-28 15:43:21", "link": "http://arxiv.org/abs/2304.14933v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Interpreting Vision and Language Generative Models with Semantic Visual\n  Priors", "abstract": "When applied to Image-to-text models, interpretability methods often provide\ntoken-by-token explanations namely, they compute a visual explanation for each\ntoken of the generated sequence. Those explanations are expensive to compute\nand unable to comprehensively explain the model's output. Therefore, these\nmodels often require some sort of approximation that eventually leads to\nmisleading explanations. We develop a framework based on SHAP, that allows for\ngenerating comprehensive, meaningful explanations leveraging the meaning\nrepresentation of the output sequence as a whole. Moreover, by exploiting\nsemantic priors in the visual backbone, we extract an arbitrary number of\nfeatures that allows the efficient computation of Shapley values on large-scale\nmodels, generating at the same time highly meaningful visual explanations. We\ndemonstrate that our method generates semantically more expressive explanations\nthan traditional methods at a lower compute cost and that it can be generalized\nover other explainability methods.", "published": "2023-04-28 17:10:08", "link": "http://arxiv.org/abs/2304.14986v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Towards autonomous system: flexible modular production system enhanced\n  with large language model agents", "abstract": "In this paper, we present a novel framework that combines large language\nmodels (LLMs), digital twins and industrial automation system to enable\nintelligent planning and control of production processes. We retrofit the\nautomation system for a modular production facility and create executable\ncontrol interfaces of fine-granular functionalities and coarse-granular skills.\nLow-level functionalities are executed by automation components, and high-level\nskills are performed by automation modules. Subsequently, a digital twin system\nis developed, registering these interfaces and containing additional\ndescriptive information about the production system. Based on the retrofitted\nautomation system and the created digital twins, LLM-agents are designed to\ninterpret descriptive information in the digital twins and control the physical\nsystem through service interfaces. These LLM-agents serve as intelligent agents\non different levels within an automation system, enabling autonomous planning\nand control of flexible production. Given a task instruction as input, the\nLLM-agents orchestrate a sequence of atomic functionalities and skills to\naccomplish the task. We demonstrate how our implemented prototype can handle\nun-predefined tasks, plan a production process, and execute the operations.\nThis research highlights the potential of integrating LLMs into industrial\nautomation systems in the context of smart factory for more agile, flexible,\nand adaptive production processes, while it also underscores the critical\ninsights and limitations for future work. Demos at:\nhttps://github.com/YuchenXia/GPT4IndustrialAutomation", "published": "2023-04-28 09:42:18", "link": "http://arxiv.org/abs/2304.14721v4", "categories": ["cs.RO", "cs.CL", "cs.SE", "cs.SY", "eess.SY"], "primary_category": "cs.RO"}
{"title": "FlowTransformer: A Transformer Framework for Flow-based Network\n  Intrusion Detection Systems", "abstract": "This paper presents the FlowTransformer framework, a novel approach for\nimplementing transformer-based Network Intrusion Detection Systems (NIDSs).\nFlowTransformer leverages the strengths of transformer models in identifying\nthe long-term behaviour and characteristics of networks, which are often\noverlooked by most existing NIDSs. By capturing these complex patterns in\nnetwork traffic, FlowTransformer offers a flexible and efficient tool for\nresearchers and practitioners in the cybersecurity community who are seeking to\nimplement NIDSs using transformer-based models. FlowTransformer allows the\ndirect substitution of various transformer components, including the input\nencoding, transformer, classification head, and the evaluation of these across\nany flow-based network dataset. To demonstrate the effectiveness and efficiency\nof the FlowTransformer framework, we utilise it to provide an extensive\nevaluation of various common transformer architectures, such as GPT 2.0 and\nBERT, on three commonly used public NIDS benchmark datasets. We provide results\nfor accuracy, model size and speed. A key finding of our evaluation is that the\nchoice of classification head has the most significant impact on the model\nperformance. Surprisingly, Global Average Pooling, which is commonly used in\ntext classification, performs very poorly in the context of NIDS. In addition,\nwe show that model size can be reduced by over 50\\%, and inference and training\ntimes improved, with no loss of accuracy, by making specific choices of input\nencoding and classification head instead of other commonly used alternatives.", "published": "2023-04-28 10:40:34", "link": "http://arxiv.org/abs/2304.14746v1", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.NE", "cs.NI"], "primary_category": "cs.CR"}
{"title": "LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model", "abstract": "How to efficiently transform large language models (LLMs) into instruction\nfollowers is recently a popular research direction, while training LLM for\nmulti-modal reasoning remains less explored. Although the recent LLaMA-Adapter\ndemonstrates the potential to handle visual inputs with LLMs, it still cannot\ngeneralize well to open-ended visual instructions and lags behind GPT-4. In\nthis paper, we present LLaMA-Adapter V2, a parameter-efficient visual\ninstruction model. Specifically, we first augment LLaMA-Adapter by unlocking\nmore learnable parameters (e.g., norm, bias and scale), which distribute the\ninstruction-following ability across the entire LLaMA model besides adapters.\nSecondly, we propose an early fusion strategy to feed visual tokens only into\nthe early LLM layers, contributing to better visual knowledge incorporation.\nThirdly, a joint training paradigm of image-text pairs and\ninstruction-following data is introduced by optimizing disjoint groups of\nlearnable parameters. This strategy effectively alleviates the interference\nbetween the two tasks of image-text alignment and instruction following and\nachieves strong multi-modal reasoning with only a small-scale image-text and\ninstruction dataset. During inference, we incorporate additional expert models\n(e.g. captioning/OCR systems) into LLaMA-Adapter to further enhance its image\nunderstanding capability without incurring training costs. Compared to the\noriginal LLaMA-Adapter, our LLaMA-Adapter V2 can perform open-ended multi-modal\ninstructions by merely introducing 14M parameters over LLaMA. The newly\ndesigned framework also exhibits stronger language-only instruction-following\ncapabilities and even excels in chat interactions. Our code and models are\navailable at https://github.com/ZrrSkywalker/LLaMA-Adapter.", "published": "2023-04-28 17:59:25", "link": "http://arxiv.org/abs/2304.15010v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Causal Reasoning and Large Language Models: Opening a New Frontier for\n  Causality", "abstract": "The causal capabilities of large language models (LLMs) are a matter of\nsignificant debate, with critical implications for the use of LLMs in\nsocietally impactful domains such as medicine, science, law, and policy. We\nconduct a \"behavorial\" study of LLMs to benchmark their capability in\ngenerating causal arguments. Across a wide range of tasks, we find that LLMs\ncan generate text corresponding to correct causal arguments with high\nprobability, surpassing the best-performing existing methods. Algorithms based\non GPT-3.5 and 4 outperform existing algorithms on a pairwise causal discovery\ntask (97%, 13 points gain), counterfactual reasoning task (92%, 20 points gain)\nand event causality (86% accuracy in determining necessary and sufficient\ncauses in vignettes). We perform robustness checks across tasks and show that\nthe capabilities cannot be explained by dataset memorization alone, especially\nsince LLMs generalize to novel datasets that were created after the training\ncutoff date.\n  That said, LLMs exhibit unpredictable failure modes, and we discuss the kinds\nof errors that may be improved and what are the fundamental limits of LLM-based\nanswers. Overall, by operating on the text metadata, LLMs bring capabilities so\nfar understood to be restricted to humans, such as using collected knowledge to\ngenerate causal graphs or identifying background causal context from natural\nlanguage. As a result, LLMs may be used by human domain experts to save effort\nin setting up a causal analysis, one of the biggest impediments to the\nwidespread adoption of causal methods. Given that LLMs ignore the actual data,\nour results also point to a fruitful research direction of developing\nalgorithms that combine LLMs with existing causal techniques. Code and datasets\nare available at https://github.com/py-why/pywhy-llm.", "published": "2023-04-28 19:00:43", "link": "http://arxiv.org/abs/2305.00050v3", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.HC", "cs.LG", "stat.ME"], "primary_category": "cs.AI"}
{"title": "Towards Better Domain Adaptation for Self-supervised Models: A Case\n  Study of Child ASR", "abstract": "Recently, self-supervised learning (SSL) from unlabelled speech data has\ngained increased attention in the automatic speech recognition (ASR) community.\nTypical SSL methods include autoregressive predictive coding (APC), Wav2vec2.0,\nand hidden unit BERT (HuBERT). However, SSL models are biased to the\npretraining data. When SSL models are finetuned with data from another domain,\ndomain shifting occurs and might cause limited knowledge transfer for\ndownstream tasks. In this paper, we propose a novel framework, domain\nresponsible adaptation and finetuning (DRAFT), to reduce domain shifting in\npretrained speech models, and evaluate it for a causal and non-causal\ntransformer. For the causal transformer, an extension of APC (E-APC) is\nproposed to learn richer information from unlabelled data by using multiple\ntemporally-shifted sequences to perform prediction. For the non-causal\ntransformer, various solutions for using the bidirectional APC (Bi-APC) are\ninvestigated. In addition, the DRAFT framework is examined for Wav2vec2.0 and\nHuBERT methods, which use non-causal transformers as the backbone. The\nexperiments are conducted on child ASR (using the OGI and MyST databases) using\nSSL models trained with unlabelled adult speech data from Librispeech. The\nrelative WER improvements of up to 19.7% on the two child tasks are observed\nwhen compared to the pretrained models without adaptation. With the proposed\nmethods (E-APC and DRAFT), the relative WER improvements are even larger (30%\nand 19% on the OGI and MyST data, respectively) when compared to the models\nwithout using pretraining methods.", "published": "2023-04-28 22:26:50", "link": "http://arxiv.org/abs/2305.00115v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "A noise-robust acoustic method for recognizing foraging activities of\n  grazing cattle", "abstract": "Farmers must continuously improve their livestock production systems to\nremain competitive in the growing dairy market. Precision livestock farming\ntechnologies provide individualized monitoring of animals on commercial farms,\noptimizing livestock production. Continuous acoustic monitoring is a widely\naccepted sensing technique used to estimate the daily rumination and grazing\ntime budget of free-ranging cattle. However, typical environmental and natural\nnoises on pastures noticeably affect the performance limiting the practical\napplication of current acoustic methods. In this study, we present the\noperating principle and generalization capability of an acoustic method called\nNoise-Robust Foraging Activity Recognizer (NRFAR). The proposed method\ndetermines foraging activity bouts by analyzing fixed-length segments of\nidentified jaw movement events produced during grazing and rumination. The\nadditive noise robustness of the NRFAR was evaluated for several\nsignal-to-noise ratios using stationary Gaussian white noise and four different\nnonstationary natural noise sources. In noiseless conditions, NRFAR reached an\naverage balanced accuracy of 86.4%, outperforming two previous acoustic methods\nby more than 7.5%. Furthermore, NRFAR performed better than previous acoustic\nmethods in 77 of 80 evaluated noisy scenarios (53 cases with p<0.05). NRFAR has\nbeen shown to be effective in harsh free-ranging environments and could be used\nas a reliable solution to improve pasture management and monitor the health and\nwelfare of dairy cows. The instrumentation and computational algorithms\npresented in this publication are protected by a pending patent application: AR\nP20220100910. Web demo available at: https://sinc.unl.edu.ar/web-demo/nrfar", "published": "2023-04-28 13:06:14", "link": "http://arxiv.org/abs/2304.14824v3", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Musical Voice Separation as Link Prediction: Modeling a Musical\n  Perception Task as a Multi-Trajectory Tracking Problem", "abstract": "This paper targets the perceptual task of separating the different\ninteracting voices, i.e., monophonic melodic streams, in a polyphonic musical\npiece. We target symbolic music, where notes are explicitly encoded, and model\nthis task as a Multi-Trajectory Tracking (MTT) problem from discrete\nobservations, i.e., notes in a pitch-time space. Our approach builds a graph\nfrom a musical piece, by creating one node for every note, and separates the\nmelodic trajectories by predicting a link between two notes if they are\nconsecutive in the same voice/stream. This kind of local, greedy prediction is\nmade possible by node embeddings created by a heterogeneous graph neural\nnetwork that can capture inter- and intra-trajectory information. Furthermore,\nwe propose a new regularization loss that encourages the output to respect the\nMTT premise of at most one incoming and one outgoing link for every node,\nfavouring monophonic (voice) trajectories; this loss function might also be\nuseful in other general MTT scenarios. Our approach does not use\ndomain-specific heuristics, is scalable to longer sequences and a higher number\nof voices, and can handle complex cases such as voice inversions and overlaps.\nWe reach new state-of-the-art results for the voice separation task in\nclassical music of different styles.", "published": "2023-04-28 13:48:00", "link": "http://arxiv.org/abs/2304.14848v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The ACM Multimedia 2023 Computational Paralinguistics Challenge: Emotion\n  Share & Requests", "abstract": "The ACM Multimedia 2023 Computational Paralinguistics Challenge addresses two\ndifferent problems for the first time in a research competition under\nwell-defined conditions: In the Emotion Share Sub-Challenge, a regression on\nspeech has to be made; and in the Requests Sub-Challenges, requests and\ncomplaints need to be detected. We describe the Sub-Challenges, baseline\nfeature extraction, and classifiers based on the usual ComPaRE features, the\nauDeep toolkit, and deep feature extraction from pre-trained CNNs using the\nDeepSpectRum toolkit; in addition, wav2vec2 models are used.", "published": "2023-04-28 14:42:55", "link": "http://arxiv.org/abs/2304.14882v2", "categories": ["cs.SD", "cs.LG", "eess.AS", "68", "I.2.7; I.5.0; J.3"], "primary_category": "cs.SD"}
{"title": "MMViT: Multiscale Multiview Vision Transformers", "abstract": "We present Multiscale Multiview Vision Transformers (MMViT), which introduces\nmultiscale feature maps and multiview encodings to transformer models. Our\nmodel encodes different views of the input signal and builds several\nchannel-resolution feature stages to process the multiple views of the input at\ndifferent resolutions in parallel. At each scale stage, we use a\ncross-attention block to fuse information across different views. This enables\nthe MMViT model to acquire complex high-dimensional representations of the\ninput at different resolutions. The proposed model can serve as a backbone\nmodel in multiple domains. We demonstrate the effectiveness of MMViT on audio\nand image classification tasks, achieving state-of-the-art results.", "published": "2023-04-28 21:51:41", "link": "http://arxiv.org/abs/2305.00104v1", "categories": ["cs.CV", "eess.AS", "eess.IV"], "primary_category": "cs.CV"}
