{"title": "COMEX Copper Futures Volatility Forecasting: Econometric Models and Deep Learning", "abstract": "This paper investigates the forecasting performance of COMEX copper futures\nrealized volatility across various high-frequency intervals using both\neconometric volatility models and deep learning recurrent neural network\nmodels. The econometric models considered are GARCH and HAR, while the deep\nlearning models include RNN (Recurrent Neural Network), LSTM (Long Short-Term\nMemory), and GRU (Gated Recurrent Unit). In forecasting daily realized\nvolatility for COMEX copper futures with a rolling window approach, the\neconometric models, particularly HAR, outperform recurrent neural networks\noverall, with HAR achieving the lowest QLIKE loss function value. However, when\nthe data is replaced with hourly high-frequency realized volatility, the deep\nlearning models outperform the GARCH model, and HAR attains a comparable QLIKE\nloss function value. Despite the black-box nature of machine learning models,\nthe deep learning models demonstrate superior forecasting performance,\nsurpassing the fixed QLIKE value of HAR in the experiment. Moreover, as the\nforecast horizon extends for daily realized volatility, deep learning models\ngradually close the performance gap with the GARCH model in certain loss\nfunction metrics. Nonetheless, HAR remains the most effective model overall for\ndaily realized volatility forecasting in copper futures.", "published": "2024-09-12 18:44:31", "link": "http://arxiv.org/abs/2409.08356v1", "categories": ["q-fin.MF", "cs.LG"], "primary_category": "q-fin.MF"}
{"title": "A market resilient data-driven approach to option pricing", "abstract": "In this paper, we present a data-driven ensemble approach for option price\nprediction whose derivation is based on the no-arbitrage theory of option\npricing. Using the theoretical treatment, we derive a common representation\nspace for achieving domain adaptation. The success of an implementation of this\nidea is shown using some real data. Then we report several experimental results\nfor critically examining the performance of the derived pricing models.", "published": "2024-09-12 16:46:07", "link": "http://arxiv.org/abs/2409.08205v1", "categories": ["q-fin.MF"], "primary_category": "q-fin.MF"}
{"title": "On the macroeconomic fundamentals of long-term volatilities and dynamic correlations in COMEX copper futures", "abstract": "This paper examines the influence of low-frequency macroeconomic variables on\nthe high-frequency returns of copper futures and the long-term correlation with\nthe S&P 500 index, employing GARCH-MIDAS and DCC-MIDAS modeling frameworks. The\nestimated results of GARCH-MIDAS show that realized volatility (RV), level of\ninterest rates (IR), industrial production (IP) and producer price index (PPI),\nvolatility of Slope, PPI, consumer sentiment index (CSI), and dollar index (DI)\nhave significant impacts on Copper futures returns, among which PPI is the most\nefficient macroeconomic variable. From comparison among DCC-GARCH and DCC-MIDAS\nmodel, the added MIDAS filter of PPI improves the model fitness and have better\nperformance than RV in effecting the long-run relationship between Copper\nfutures and S&P 500.", "published": "2024-09-12 18:41:09", "link": "http://arxiv.org/abs/2409.08355v1", "categories": ["q-fin.ST"], "primary_category": "q-fin.ST"}
{"title": "Experimenting with Legal AI Solutions: The Case of Question-Answering\n  for Access to Justice", "abstract": "Generative AI models, such as the GPT and Llama series, have significant\npotential to assist laypeople in answering legal questions. However, little\nprior work focuses on the data sourcing, inference, and evaluation of these\nmodels in the context of laypersons. To this end, we propose a human-centric\nlegal NLP pipeline, covering data sourcing, inference, and evaluation. We\nintroduce and release a dataset, LegalQA, with real and specific legal\nquestions spanning from employment law to criminal law, corresponding answers\nwritten by legal experts, and citations for each answer. We develop an\nautomatic evaluation protocol for this dataset, then show that\nretrieval-augmented generation from only 850 citations in the train set can\nmatch or outperform internet-wide retrieval, despite containing 9 orders of\nmagnitude less data. Finally, we propose future directions for open-sourced\nefforts, which fall behind closed-sourced models.", "published": "2024-09-12 02:40:28", "link": "http://arxiv.org/abs/2409.07713v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ruri: Japanese General Text Embeddings", "abstract": "We report the development of Ruri, a series of Japanese general text\nembedding models. While the development of general-purpose text embedding\nmodels in English and multilingual contexts has been active in recent years,\nmodel development in Japanese remains insufficient. The primary reasons for\nthis are the lack of datasets and the absence of necessary expertise. In this\nreport, we provide a detailed account of the development process of Ruri.\nSpecifically, we discuss the training of embedding models using synthesized\ndatasets generated by LLMs, the construction of the reranker for dataset\nfiltering and knowledge distillation, and the performance evaluation of the\nresulting general-purpose text embedding models.", "published": "2024-09-12 04:06:31", "link": "http://arxiv.org/abs/2409.07737v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Supporting Online Discussions: Integrating AI Into the adhocracy+\n  Participation Platform To Enhance Deliberation", "abstract": "Online spaces allow people to discuss important issues and make joint\ndecisions, regardless of their location or time zone. However, without proper\nsupport and thoughtful design, these discussions often lack structure and\npoliteness during the exchanges of opinions. Artificial intelligence (AI)\nrepresents an opportunity to support both participants and organizers of\nlarge-scale online participation processes. In this paper, we present an\nextension of adhocracy+, a large-scale open source participation platform, that\nprovides two additional debate modules that are supported by AI to enhance the\ndiscussion quality and participant interaction.", "published": "2024-09-12 06:27:35", "link": "http://arxiv.org/abs/2409.07780v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Stable Language Model Pre-training by Reducing Embedding Variability", "abstract": "Stable pre-training is essential for achieving better-performing language\nmodels. However, tracking pre-training stability by calculating gradient\nvariance at every step is impractical due to the significant computational\ncosts. We explore Token Embedding Variability (TEV) as a simple and efficient\nproxy for assessing pre-training stability in language models with pre-layer\nnormalization, given that shallower layers are more prone to gradient explosion\n(section 2.2). Moreover, we propose Multi-head Low-Rank Attention (MLRA) as an\narchitecture to alleviate such instability by limiting the exponential growth\nof output embedding variance, thereby preventing the gradient explosion\n(section 3.2). Empirical results on GPT-2 with MLRA demonstrate increased\nstability and lower perplexity, particularly in deeper models.", "published": "2024-09-12 06:37:46", "link": "http://arxiv.org/abs/2409.07787v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Rules from KGs Guided by Language Models", "abstract": "Advances in information extraction have enabled the automatic construction of\nlarge knowledge graphs (e.g., Yago, Wikidata or Google KG), which are widely\nused in many applications like semantic search or data analytics. However, due\nto their semi-automatic construction, KGs are often incomplete. Rule learning\nmethods, concerned with the extraction of frequent patterns from KGs and\ncasting them into rules, can be applied to predict potentially missing facts. A\ncrucial step in this process is rule ranking. Ranking of rules is especially\nchallenging over highly incomplete or biased KGs (e.g., KGs predominantly\nstoring facts about famous people), as in this case biased rules might fit the\ndata best and be ranked at the top based on standard statistical metrics like\nrule confidence. To address this issue, prior works proposed to rank rules not\nonly relying on the original KG but also facts predicted by a KG embedding\nmodel. At the same time, with the recent rise of Language Models (LMs), several\nworks have claimed that LMs can be used as alternative means for KG completion.\nIn this work, our goal is to verify to which extent the exploitation of LMs is\nhelpful for improving the quality of rule learning systems.", "published": "2024-09-12 09:27:36", "link": "http://arxiv.org/abs/2409.07869v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLM-POTUS Score: A Framework of Analyzing Presidential Debates with\n  Large Language Models", "abstract": "Large language models have demonstrated remarkable capabilities in natural\nlanguage processing, yet their application to political discourse analysis\nremains underexplored. This paper introduces a novel approach to evaluating\npresidential debate performances using LLMs, addressing the longstanding\nchallenge of objectively assessing debate outcomes. We propose a framework that\nanalyzes candidates' \"Policies, Persona, and Perspective\" (3P) and how they\nresonate with the \"Interests, Ideologies, and Identity\" (3I) of four key\naudience groups: voters, businesses, donors, and politicians. Our method\nemploys large language models to generate the LLM-POTUS Score, a quantitative\nmeasure of debate performance based on the alignment between 3P and 3I. We\napply this framework to analyze transcripts from recent U.S. presidential\ndebates, demonstrating its ability to provide nuanced, multi-dimensional\nassessments of candidate performances. Our results reveal insights into the\neffectiveness of different debating strategies and their impact on various\naudience segments. This study not only offers a new tool for political analysis\nbut also explores the potential and limitations of using LLMs as impartial\njudges in complex social contexts. In addition, this framework provides\nindividual citizens with an independent tool to evaluate presidential debate\nperformances, which enhances democratic engagement and reduces reliance on\npotentially biased media interpretations and institutional influence, thereby\nstrengthening the foundation of informed civic participation.", "published": "2024-09-12 15:40:45", "link": "http://arxiv.org/abs/2409.08147v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Unsupervised Dialogue Topic Segmentation Model Based on Utterance\n  Rewriting", "abstract": "Dialogue topic segmentation plays a crucial role in various types of dialogue\nmodeling tasks. The state-of-the-art unsupervised DTS methods learn topic-aware\ndiscourse representations from conversation data through adjacent discourse\nmatching and pseudo segmentation to further mine useful clues in unlabeled\nconversational relations. However, in multi-round dialogs, discourses often\nhave co-references or omissions, leading to the fact that direct use of these\ndiscourses for representation learning may negatively affect the semantic\nsimilarity computation in the neighboring discourse matching task. In order to\nfully utilize the useful cues in conversational relations, this study proposes\na novel unsupervised dialog topic segmentation method that combines the\nUtterance Rewriting (UR) technique with an unsupervised learning algorithm to\nefficiently utilize the useful cues in unlabeled dialogs by rewriting the\ndialogs in order to recover the co-referents and omitted words. Compared with\nexisting unsupervised models, the proposed Discourse Rewriting Topic\nSegmentation Model (UR-DTS) significantly improves the accuracy of topic\nsegmentation. The main finding is that the performance on DialSeg711 improves\nby about 6% in terms of absolute error score and WD, achieving 11.42% in terms\nof absolute error score and 12.97% in terms of WD. on Doc2Dial the absolute\nerror score and WD improves by about 3% and 2%, respectively, resulting in SOTA\nreaching 35.17% in terms of absolute error score and 38.49% in terms of WD.\nThis shows that the model is very effective in capturing the nuances of\nconversational topics, as well as the usefulness and challenges of utilizing\nunlabeled conversations.", "published": "2024-09-12 00:27:31", "link": "http://arxiv.org/abs/2409.07672v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DSBench: How Far Are Data Science Agents to Becoming Data Science\n  Experts?", "abstract": "Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) have\ndemonstrated impressive language/vision reasoning abilities, igniting the\nrecent trend of building agents for targeted applications such as shopping\nassistants or AI software engineers. Recently, many data science benchmarks\nhave been proposed to investigate their performance in the data science domain.\nHowever, existing data science benchmarks still fall short when compared to\nreal-world data science applications due to their simplified settings. To\nbridge this gap, we introduce DSBench, a comprehensive benchmark designed to\nevaluate data science agents with realistic tasks. This benchmark includes 466\ndata analysis tasks and 74 data modeling tasks, sourced from Eloquence and\nKaggle competitions. DSBench offers a realistic setting by encompassing long\ncontexts, multimodal task backgrounds, reasoning with large data files and\nmulti-table structures, and performing end-to-end data modeling tasks. Our\nevaluation of state-of-the-art LLMs, LVLMs, and agents shows that they struggle\nwith most tasks, with the best agent solving only 34.12% of data analysis tasks\nand achieving a 34.74% Relative Performance Gap (RPG). These findings\nunderscore the need for further advancements in developing more practical,\nintelligent, and autonomous data science agents.", "published": "2024-09-12 02:08:00", "link": "http://arxiv.org/abs/2409.07703v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Full-text Error Correction for Chinese Speech Recognition with Large\n  Language Model", "abstract": "Large Language Models (LLMs) have demonstrated substantial potential for\nerror correction in Automatic Speech Recognition (ASR). However, most research\nfocuses on utterances from short-duration speech recordings, which are the\npredominant form of speech data for supervised ASR training. This paper\ninvestigates the effectiveness of LLMs for error correction in full-text\ngenerated by ASR systems from longer speech recordings, such as transcripts\nfrom podcasts, news broadcasts, and meetings. First, we develop a Chinese\ndataset for full-text error correction, named ChFT, utilizing a pipeline that\ninvolves text-to-speech synthesis, ASR, and error-correction pair extractor.\nThis dataset enables us to correct errors across contexts, including both\nfull-text and segment, and to address a broader range of error types, such as\npunctuation restoration and inverse text normalization, thus making the\ncorrection process comprehensive. Second, we fine-tune a pre-trained LLM on the\nconstructed dataset using a diverse set of prompts and target formats, and\nevaluate its performance on full-text error correction. Specifically, we design\nprompts based on full-text and segment, considering various output formats,\nsuch as directly corrected text and JSON-based error-correction pairs. Through\nvarious test settings, including homogeneous, up-to-date, and hard test sets,\nwe find that the fine-tuned LLMs perform well in the full-text setting with\ndifferent prompts, each presenting its own strengths and weaknesses. This\nestablishes a promising baseline for further research. The dataset is available\non the website.", "published": "2024-09-12 06:50:45", "link": "http://arxiv.org/abs/2409.07790v2", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Controllable Synthetic Clinical Note Generation with Privacy Guarantees", "abstract": "In the field of machine learning, domain-specific annotated data is an\ninvaluable resource for training effective models. However, in the medical\ndomain, this data often includes Personal Health Information (PHI), raising\nsignificant privacy concerns. The stringent regulations surrounding PHI limit\nthe availability and sharing of medical datasets, which poses a substantial\nchallenge for researchers and practitioners aiming to develop advanced machine\nlearning models. In this paper, we introduce a novel method to \"clone\" datasets\ncontaining PHI. Our approach ensures that the cloned datasets retain the\nessential characteristics and utility of the original data without compromising\npatient privacy. By leveraging differential-privacy techniques and a novel\nfine-tuning task, our method produces datasets that are free from identifiable\ninformation while preserving the statistical properties necessary for model\ntraining. We conduct utility testing to evaluate the performance of machine\nlearning models trained on the cloned datasets. The results demonstrate that\nour cloned datasets not only uphold privacy standards but also enhance model\nperformance compared to those trained on traditional anonymized datasets. This\nwork offers a viable solution for the ethical and effective utilization of\nsensitive medical data in machine learning, facilitating progress in medical\nresearch and the development of robust predictive models.", "published": "2024-09-12 07:38:34", "link": "http://arxiv.org/abs/2409.07809v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Online vs Offline: A Comparative Study of First-Party and Third-Party\n  Evaluations of Social Chatbots", "abstract": "This paper explores the efficacy of online versus offline evaluation methods\nin assessing conversational chatbots, specifically comparing first-party direct\ninteractions with third-party observational assessments. By extending a\nbenchmarking dataset of user dialogs with empathetic chatbots with offline\nthird-party evaluations, we present a systematic comparison between the\nfeedback from online interactions and the more detached offline third-party\nevaluations. Our results reveal that offline human evaluations fail to capture\nthe subtleties of human-chatbot interactions as effectively as online\nassessments. In comparison, automated third-party evaluations using a GPT-4\nmodel offer a better approximation of first-party human judgments given\ndetailed instructions. This study highlights the limitations of third-party\nevaluations in grasping the complexities of user experiences and advocates for\nthe integration of direct interaction feedback in conversational AI evaluation\nto enhance system development and user satisfaction.", "published": "2024-09-12 08:11:08", "link": "http://arxiv.org/abs/2409.07823v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "FPMT: Enhanced Semi-Supervised Model for Traffic Incident Detection", "abstract": "For traffic incident detection, the acquisition of data and labels is notably\nresource-intensive, rendering semi-supervised traffic incident detection both a\nformidable and consequential challenge. Thus, this paper focuses on traffic\nincident detection with a semi-supervised learning way. It proposes a\nsemi-supervised learning model named FPMT within the framework of MixText. The\ndata augmentation module introduces Generative Adversarial Networks to balance\nand expand the dataset. During the mix-up process in the hidden space, it\nemploys a probabilistic pseudo-mixing mechanism to enhance regularization and\nelevate model precision. In terms of training strategy, it initiates with\nunsupervised training on all data, followed by supervised fine-tuning on a\nsubset of labeled data, and ultimately completing the goal of semi-supervised\ntraining. Through empirical validation on four authentic datasets, our FPMT\nmodel exhibits outstanding performance across various metrics. Particularly\nnoteworthy is its robust performance even in scenarios with low label rates.", "published": "2024-09-12 08:38:42", "link": "http://arxiv.org/abs/2409.07839v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Enhanced Online Grooming Detection Employing Context Determination and\n  Message-Level Analysis", "abstract": "Online Grooming (OG) is a prevalent threat facing predominately children\nonline, with groomers using deceptive methods to prey on the vulnerability of\nchildren on social media/messaging platforms. These attacks can have severe\npsychological and physical impacts, including a tendency towards\nrevictimization. Current technical measures are inadequate, especially with the\nadvent of end-to-end encryption which hampers message monitoring. Existing\nsolutions focus on the signature analysis of child abuse media, which does not\neffectively address real-time OG detection. This paper proposes that OG attacks\nare complex, requiring the identification of specific communication patterns\nbetween adults and children. It introduces a novel approach leveraging advanced\nmodels such as BERT and RoBERTa for Message-Level Analysis and a Context\nDetermination approach for classifying actor interactions, including the\nintroduction of Actor Significance Thresholds and Message Significance\nThresholds. The proposed method aims to enhance accuracy and robustness in\ndetecting OG by considering the dynamic and multi-faceted nature of these\nattacks. Cross-dataset experiments evaluate the robustness and versatility of\nour approach. This paper's contributions include improved detection\nmethodologies and the potential for application in various scenarios,\naddressing gaps in current literature and practices.", "published": "2024-09-12 11:37:34", "link": "http://arxiv.org/abs/2409.07958v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "TravelAgent: An AI Assistant for Personalized Travel Planning", "abstract": "As global tourism expands and artificial intelligence technology advances,\nintelligent travel planning services have emerged as a significant research\nfocus. Within dynamic real-world travel scenarios with multi-dimensional\nconstraints, services that support users in automatically creating practical\nand customized travel itineraries must address three key objectives:\nRationality, Comprehensiveness, and Personalization. However, existing systems\nwith rule-based combinations or LLM-based planning methods struggle to fully\nsatisfy these criteria. To overcome the challenges, we introduce TravelAgent, a\ntravel planning system powered by large language models (LLMs) designed to\nprovide reasonable, comprehensive, and personalized travel itineraries grounded\nin dynamic scenarios. TravelAgent comprises four modules: Tool-usage,\nRecommendation, Planning, and Memory Module. We evaluate TravelAgent's\nperformance with human and simulated users, demonstrating its overall\neffectiveness in three criteria and confirming the accuracy of personalized\nrecommendations.", "published": "2024-09-12 14:24:45", "link": "http://arxiv.org/abs/2409.08069v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "The CLC-UKET Dataset: Benchmarking Case Outcome Prediction for the UK\n  Employment Tribunal", "abstract": "This paper explores the intersection of technological innovation and access\nto justice by developing a benchmark for predicting case outcomes in the UK\nEmployment Tribunal (UKET). To address the challenge of extensive manual\nannotation, the study employs a large language model (LLM) for automatic\nannotation, resulting in the creation of the CLC-UKET dataset. The dataset\nconsists of approximately 19,000 UKET cases and their metadata. Comprehensive\nlegal annotations cover facts, claims, precedent references, statutory\nreferences, case outcomes, reasons and jurisdiction codes. Facilitated by the\nCLC-UKET data, we examine a multi-class case outcome prediction task in the\nUKET. Human predictions are collected to establish a performance reference for\nmodel comparison. Empirical results from baseline models indicate that\nfinetuned transformer models outperform zero-shot and few-shot LLMs on the UKET\nprediction task. The performance of zero-shot LLMs can be enhanced by\nintegrating task-related information into few-shot examples. We hope that the\nCLC-UKET dataset, along with human annotations and empirical findings, can\nserve as a valuable benchmark for employment-related dispute resolution.", "published": "2024-09-12 14:51:43", "link": "http://arxiv.org/abs/2409.08098v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "WhisperNER: Unified Open Named Entity and Speech Recognition", "abstract": "Integrating named entity recognition (NER) with automatic speech recognition\n(ASR) can significantly enhance transcription accuracy and informativeness. In\nthis paper, we introduce WhisperNER, a novel model that allows joint speech\ntranscription and entity recognition. WhisperNER supports open-type NER,\nenabling recognition of diverse and evolving entities at inference. Building on\nrecent advancements in open NER research, we augment a large synthetic dataset\nwith synthetic speech samples. This allows us to train WhisperNER on a large\nnumber of examples with diverse NER tags. During training, the model is\nprompted with NER labels and optimized to output the transcribed utterance\nalong with the corresponding tagged entities. To evaluate WhisperNER, we\ngenerate synthetic speech for commonly used NER benchmarks and annotate\nexisting ASR datasets with open NER tags. Our experiments demonstrate that\nWhisperNER outperforms natural baselines on both out-of-domain open type NER\nand supervised finetuning.", "published": "2024-09-12 15:00:56", "link": "http://arxiv.org/abs/2409.08107v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On the Role of Context in Reading Time Prediction", "abstract": "We present a new perspective on how readers integrate context during\nreal-time language comprehension. Our proposals build on surprisal theory,\nwhich posits that the processing effort of a linguistic unit (e.g., a word) is\nan affine function of its in-context information content. We first observe that\nsurprisal is only one out of many potential ways that a contextual predictor\ncan be derived from a language model. Another one is the pointwise mutual\ninformation (PMI) between a unit and its context, which turns out to yield the\nsame predictive power as surprisal when controlling for unigram frequency.\nMoreover, both PMI and surprisal are correlated with frequency. This means that\nneither PMI nor surprisal contains information about context alone. In response\nto this, we propose a technique where we project surprisal onto the orthogonal\ncomplement of frequency, yielding a new contextual predictor that is\nuncorrelated with frequency. Our experiments show that the proportion of\nvariance in reading times explained by context is a lot smaller when context is\nrepresented by the orthogonalized predictor. From an interpretability\nstandpoint, this indicates that previous studies may have overstated the role\nthat context has in predicting reading times.", "published": "2024-09-12 15:52:22", "link": "http://arxiv.org/abs/2409.08160v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Source2Synth: Synthetic Data Generation and Curation Grounded in Real\n  Data Sources", "abstract": "Large Language Models still struggle in challenging scenarios that leverage\nstructured data, complex reasoning, or tool usage. In this paper, we propose\nSource2Synth: a new method that can be used for teaching LLMs new skills\nwithout relying on costly human annotations. Source2Synth takes as input a\ncustom data source and produces synthetic data points with intermediate\nreasoning steps grounded in real-world sources. Source2Synth improves the\ndataset quality by discarding low-quality generations based on their\nanswerability. We demonstrate the generality of this approach by applying it to\ntwo challenging domains: we test reasoning abilities in multi-hop question\nanswering (MHQA), and tool usage in tabular question answering (TQA). Our\nmethod improves performance by 25.51% for TQA on WikiSQL and 22.57% for MHQA on\nHotPotQA compared to the fine-tuned baselines.", "published": "2024-09-12 17:39:08", "link": "http://arxiv.org/abs/2409.08239v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Knowledge Tagging with Large Language Model based Multi-Agent System", "abstract": "Knowledge tagging for questions is vital in modern intelligent educational\napplications, including learning progress diagnosis, practice question\nrecommendations, and course content organization. Traditionally, these\nannotations have been performed by pedagogical experts, as the task demands not\nonly a deep semantic understanding of question stems and knowledge definitions\nbut also a strong ability to link problem-solving logic with relevant knowledge\nconcepts. With the advent of advanced natural language processing (NLP)\nalgorithms, such as pre-trained language models and large language models\n(LLMs), pioneering studies have explored automating the knowledge tagging\nprocess using various machine learning models. In this paper, we investigate\nthe use of a multi-agent system to address the limitations of previous\nalgorithms, particularly in handling complex cases involving intricate\nknowledge definitions and strict numerical constraints. By demonstrating its\nsuperior performance on the publicly available math question knowledge tagging\ndataset, MathKnowCT, we highlight the significant potential of an LLM-based\nmulti-agent system in overcoming the challenges that previous methods have\nencountered. Finally, through an in-depth discussion of the implications of\nautomating knowledge tagging, we underscore the promising results of deploying\nLLM-based algorithms in educational contexts.", "published": "2024-09-12 21:39:01", "link": "http://arxiv.org/abs/2409.08406v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing Q&A Text Retrieval with Ranking Models: Benchmarking,\n  fine-tuning and deploying Rerankers for RAG", "abstract": "Ranking models play a crucial role in enhancing overall accuracy of text\nretrieval systems. These multi-stage systems typically utilize either dense\nembedding models or sparse lexical indices to retrieve relevant passages based\non a given query, followed by ranking models that refine the ordering of the\ncandidate passages by its relevance to the query.\n  This paper benchmarks various publicly available ranking models and examines\ntheir impact on ranking accuracy. We focus on text retrieval for\nquestion-answering tasks, a common use case for Retrieval-Augmented Generation\nsystems. Our evaluation benchmarks include models some of which are\ncommercially viable for industrial applications.\n  We introduce a state-of-the-art ranking model, NV-RerankQA-Mistral-4B-v3,\nwhich achieves a significant accuracy increase of ~14% compared to pipelines\nwith other rerankers. We also provide an ablation study comparing the\nfine-tuning of ranking models with different sizes, losses and self-attention\nmechanisms.\n  Finally, we discuss challenges of text retrieval pipelines with ranking\nmodels in real-world industry applications, in particular the trade-offs among\nmodel size, ranking accuracy and system requirements like indexing and serving\nlatency / throughput.", "published": "2024-09-12 01:51:06", "link": "http://arxiv.org/abs/2409.07691v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Large Language Models are Pattern Matchers: Editing Semi-Structured and\n  Structured Documents with ChatGPT", "abstract": "Large Language Models (LLMs) offer numerous applications, the full extent of\nwhich is not yet understood. This paper investigates if LLMs can be applied for\nediting structured and semi-structured documents with minimal effort. Using a\nqualitative research approach, we conduct two case studies with ChatGPT and\nthoroughly analyze the results. Our experiments indicate that LLMs can\neffectively edit structured and semi-structured documents when provided with\nbasic, straightforward prompts. ChatGPT demonstrates a strong ability to\nrecognize and process the structure of annotated documents. This suggests that\nexplicitly structuring tasks and data in prompts might enhance an LLM's ability\nto understand and solve tasks. Furthermore, the experiments also reveal\nimpressive pattern matching skills in ChatGPT. This observation deserves\nfurther investigation, as it may contribute to understanding the processes\nleading to hallucinations in LLMs.", "published": "2024-09-12 03:41:39", "link": "http://arxiv.org/abs/2409.07732v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2"], "primary_category": "cs.LG"}
{"title": "Multi-object event graph representation learning for Video Question\n  Answering", "abstract": "Video question answering (VideoQA) is a task to predict the correct answer to\nquestions posed about a given video. The system must comprehend spatial and\ntemporal relationships among objects extracted from videos to perform causal\nand temporal reasoning. While prior works have focused on modeling individual\nobject movements using transformer-based methods, they falter when capturing\ncomplex scenarios involving multiple objects (e.g., \"a boy is throwing a ball\nin a hoop\"). We propose a contrastive language event graph representation\nlearning method called CLanG to address this limitation. Aiming to capture\nevent representations associated with multiple objects, our method employs a\nmulti-layer GNN-cluster module for adversarial graph representation learning,\nenabling contrastive learning between the question text and its relevant\nmulti-object event graph. Our method outperforms a strong baseline, achieving\nup to 2.2% higher accuracy on two challenging VideoQA datasets, NExT-QA and\nTGIF-QA-R. In particular, it is 2.8% better than baselines in handling causal\nand temporal questions, highlighting its strength in reasoning multiple\nobject-based events.", "published": "2024-09-12 04:42:51", "link": "http://arxiv.org/abs/2409.07747v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Top-down Activity Representation Learning for Video Question Answering", "abstract": "Capturing complex hierarchical human activities, from atomic actions (e.g.,\npicking up one present, moving to the sofa, unwrapping the present) to\ncontextual events (e.g., celebrating Christmas) is crucial for achieving\nhigh-performance video question answering (VideoQA). Recent works have expanded\nmultimodal models (e.g., CLIP, LLaVA) to process continuous video sequences,\nenhancing the model's temporal reasoning capabilities. However, these\napproaches often fail to capture contextual events that can be decomposed into\nmultiple atomic actions non-continuously distributed over relatively long-term\nsequences. In this paper, to leverage the spatial visual context representation\ncapability of the CLIP model for obtaining non-continuous visual\nrepresentations in terms of contextual events in videos, we convert long-term\nvideo sequences into a spatial image domain and finetune the multimodal model\nLLaVA for the VideoQA task. Our approach achieves competitive performance on\nthe STAR task, in particular, with a 78.4% accuracy score, exceeding the\ncurrent state-of-the-art score by 2.8 points on the NExTQA task.", "published": "2024-09-12 04:43:27", "link": "http://arxiv.org/abs/2409.07748v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "A corpus-based investigation of pitch contours of monosyllabic words in\n  conversational Taiwan Mandarin", "abstract": "In Mandarin, the tonal contours of monosyllabic words produced in isolation\nor in careful speech are characterized by four lexical tones: a high-level tone\n(T1), a rising tone (T2), a dipping tone (T3) and a falling tone (T4). However,\nin spontaneous speech, the actual tonal realization of monosyllabic words can\ndeviate significantly from these canonical tones due to intra-syllabic\nco-articulation and inter-syllabic co-articulation with adjacent tones. In\naddition, Chuang et al. (2024) recently reported that the tonal contours of\ndisyllabic Mandarin words with T2-T4 tone pattern are co-determined by their\nmeanings. Following up on their research, we present a corpus-based\ninvestigation of how the pitch contours of monosyllabic words are realized in\nspontaneous conversational Mandarin, focusing on the effects of contextual\npredictors on the one hand, and the way in words' meanings co-determine pitch\ncontours on the other hand. We analyze the F0 contours of 3824 tokens of 63\ndifferent word types in a spontaneous Taiwan Mandarin corpus, using the\ngeneralized additive (mixed) model to decompose a given observed pitch contour\ninto a set of component pitch contours. We show that the tonal context\nsubstantially modify a word's canonical tone. Once the effect of tonal context\nis controlled for, T2 and T3 emerge as low flat tones, contrasting with T1 as a\nhigh tone, and with T4 as a high-to-mid falling tone. The neutral tone (T0),\nwhich in standard descriptions, is realized based on the preceding tone,\nemerges as a low tone in its own right, modified by the other predictors in the\nsame way as the standard tones T1, T2, T3, and T4. We also show that word, and\neven more so, word sense, co-determine words' F0 contours. Analyses of variable\nimportance using random forests further supported the substantial effect of\ntonal context and an effect of word sense.", "published": "2024-09-12 09:51:56", "link": "http://arxiv.org/abs/2409.07891v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "The Faetar Benchmark: Speech Recognition in a Very Under-Resourced\n  Language", "abstract": "We introduce the Faetar Automatic Speech Recognition Benchmark, a benchmark\ncorpus designed to push the limits of current approaches to low-resource speech\nrecognition. Faetar, a Franco-Proven\\c{c}al variety spoken primarily in Italy,\nhas no standard orthography, has virtually no existing textual or speech\nresources other than what is included in the benchmark, and is quite different\nfrom other forms of Franco-Proven\\c{c}al. The corpus comes from field\nrecordings, most of which are noisy, for which only 5 hrs have matching\ntranscriptions, and for which forced alignment is of variable quality. The\ncorpus contains an additional 20 hrs of unlabelled speech. We report baseline\nresults from state-of-the-art multilingual speech foundation models with a best\nphone error rate of 30.4%, using a pipeline that continues pre-training on the\nfoundation model using the unlabelled set.", "published": "2024-09-12 14:55:33", "link": "http://arxiv.org/abs/2409.08103v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Fine-tuning Large Language Models for Entity Matching", "abstract": "Generative large language models (LLMs) are a promising alternative to\npre-trained language models for entity matching due to their high zero-shot\nperformance and their ability to generalize to unseen entities. Existing\nresearch on using LLMs for entity matching has focused on prompt engineering\nand in-context learning. This paper explores the potential of fine-tuning LLMs\nfor entity matching. We analyze fine-tuning along two dimensions: 1) The\nrepresentation of training examples, where we experiment with adding different\ntypes of LLM-generated explanations to the training set, and 2) the selection\nand generation of training examples using LLMs. In addition to the matching\nperformance on the source dataset, we investigate how fine-tuning affects the\nmodel's ability to generalize to other in-domain datasets as well as across\ntopical domains. Our experiments show that fine-tuning significantly improves\nthe performance of the smaller models while the results for the larger models\nare mixed. Fine-tuning also improves the generalization to in-domain datasets\nwhile hurting cross-domain transfer. We show that adding structured\nexplanations to the training set has a positive impact on the performance of\nthree out of four LLMs, while the proposed example selection and generation\nmethods only improve the performance of Llama 3.1 8B while decreasing the\nperformance of GPT-4o Mini.", "published": "2024-09-12 16:20:57", "link": "http://arxiv.org/abs/2409.08185v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "AudioBERT: Audio Knowledge Augmented Language Model", "abstract": "Recent studies have identified that language models, pretrained on text-only\ndatasets, often lack elementary visual knowledge, \\textit{e.g.,} colors of\neveryday objects. Motivated by this observation, we ask whether a similar\nshortcoming exists in terms of the \\textit{auditory} knowledge. To answer this\nquestion, we construct a new dataset called AuditoryBench, which consists of\ntwo novel tasks for evaluating auditory knowledge. Based on our analysis using\nthe benchmark, we find that language models also suffer from a severe lack of\nauditory knowledge. To address this limitation, we propose AudioBERT, a novel\nmethod to augment the auditory knowledge of BERT through a retrieval-based\napproach. First, we detect auditory knowledge spans in prompts to query our\nretrieval model efficiently. Then, we inject audio knowledge into BERT and\nswitch on low-rank adaptation for effective adaptation when audio knowledge is\nrequired. Our experiments demonstrate that AudioBERT is quite effective,\nachieving superior performance on the AuditoryBench. The dataset and code are\navailable at \\bulurl{https://github.com/HJ-Ok/AudioBERT}.", "published": "2024-09-12 16:36:39", "link": "http://arxiv.org/abs/2409.08199v2", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "What Makes a Maze Look Like a Maze?", "abstract": "A unique aspect of human visual understanding is the ability to flexibly\ninterpret abstract concepts: acquiring lifted rules explaining what they\nsymbolize, grounding them across familiar and unfamiliar contexts, and making\npredictions or reasoning about them. While off-the-shelf vision-language models\nexcel at making literal interpretations of images (e.g., recognizing object\ncategories such as tree branches), they still struggle to make sense of such\nvisual abstractions (e.g., how an arrangement of tree branches may form the\nwalls of a maze). To address this challenge, we introduce Deep Schema Grounding\n(DSG), a framework that leverages explicit structured representations of visual\nabstractions for grounding and reasoning. At the core of DSG are\nschemas--dependency graph descriptions of abstract concepts that decompose them\ninto more primitive-level symbols. DSG uses large language models to extract\nschemas, then hierarchically grounds concrete to abstract components of the\nschema onto images with vision-language models. The grounded schema is used to\naugment visual abstraction understanding. We systematically evaluate DSG and\ndifferent methods in reasoning on our new Visual Abstractions Dataset, which\nconsists of diverse, real-world images of abstract concepts and corresponding\nquestion-answer pairs labeled by humans. We show that DSG significantly\nimproves the abstract visual reasoning performance of vision-language models,\nand is a step toward human-aligned understanding of visual abstractions.", "published": "2024-09-12 16:41:47", "link": "http://arxiv.org/abs/2409.08202v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "The Design of Informative Take-Over Requests for Semi-Autonomous\n  Cyber-Physical Systems: Combining Spoken Language and Visual Icons in a\n  Drone-Controller Setting", "abstract": "The question of how cyber-physical systems should interact with human\npartners that can take over control or exert oversight is becoming more\npressing, as these systems are deployed for an ever larger range of tasks.\nDrawing on the literatures on handing over control during semi-autonomous\ndriving and human-robot interaction, we propose a design of a take-over request\nthat combines an abstract pre-alert with an informative TOR: Relevant sensor\ninformation is highlighted on the controller's display, while a spoken message\nverbalizes the reason for the TOR. We conduct our study in the context of a\nsemi-autonomous drone control scenario as our testbed. The goal of our online\nstudy is to assess in more detail what form a language-based TOR should take.\nSpecifically, we compare a full sentence condition to shorter fragments, and\ntest whether the visual highlighting should be done synchronously or\nasynchronously with the speech. Participants showed a higher accuracy in\nchoosing the correct solution with our bi-modal TOR and felt that they were\nbetter able to recognize the critical situation. Using only fragments in the\nspoken message rather than full sentences did not lead to improved accuracy or\nfaster reactions. Also, synchronizing the visual highlighting with the spoken\nmessage did not result in better accuracy and response times were even\nincreased in this condition.", "published": "2024-09-12 17:50:05", "link": "http://arxiv.org/abs/2409.08253v2", "categories": ["cs.HC", "cs.CL", "cs.RO"], "primary_category": "cs.HC"}
{"title": "Real or Robotic? Assessing Whether LLMs Accurately Simulate Qualities of\n  Human Responses in Dialogue", "abstract": "Studying and building datasets for dialogue tasks is both expensive and\ntime-consuming due to the need to recruit, train, and collect data from study\nparticipants. In response, much recent work has sought to use large language\nmodels (LLMs) to simulate both human-human and human-LLM interactions, as they\nhave been shown to generate convincingly human-like text in many settings.\nHowever, to what extent do LLM-based simulations \\textit{actually} reflect\nhuman dialogues? In this work, we answer this question by generating a\nlarge-scale dataset of 100,000 paired LLM-LLM and human-LLM dialogues from the\nWildChat dataset and quantifying how well the LLM simulations align with their\nhuman counterparts. Overall, we find relatively low alignment between\nsimulations and human interactions, demonstrating a systematic divergence along\nthe multiple textual properties, including style and content. Further, in\ncomparisons of English, Chinese, and Russian dialogues, we find that models\nperform similarly. Our results suggest that LLMs generally perform better when\nthe human themself writes in a way that is more similar to the LLM's own style.", "published": "2024-09-12 18:00:18", "link": "http://arxiv.org/abs/2409.08330v2", "categories": ["cs.CL", "cs.CY", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Towards Quantifying and Reducing Language Mismatch Effects in\n  Cross-Lingual Speech Anti-Spoofing", "abstract": "The effects of language mismatch impact speech anti-spoofing systems, while\ninvestigations and quantification of these effects remain limited. Existing\nanti-spoofing datasets are mainly in English, and the high cost of acquiring\nmultilingual datasets hinders training language-independent models. We initiate\nthis work by evaluating top-performing speech anti-spoofing systems that are\ntrained on English data but tested on other languages, observing notable\nperformance declines. We propose an innovative approach - Accent-based data\nexpansion via TTS (ACCENT), which introduces diverse linguistic knowledge to\nmonolingual-trained models, improving their cross-lingual capabilities. We\nconduct experiments on a large-scale dataset consisting of over 3 million\nsamples, including 1.8 million training samples and nearly 1.2 million testing\nsamples across 12 languages. The language mismatch effects are preliminarily\nquantified and remarkably reduced over 15% by applying the proposed ACCENT.\nThis easily implementable method shows promise for multilingual and\nlow-resource language scenarios.", "published": "2024-09-12 18:18:22", "link": "http://arxiv.org/abs/2409.08346v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Machine Translation with Large Language Models: Decoder Only vs.\n  Encoder-Decoder", "abstract": "This project, titled \"Machine Translation with Large Language Models:\nDecoder-only vs. Encoder-Decoder,\" aims to develop a multilingual machine\ntranslation (MT) model. Focused on Indian regional languages, especially\nTelugu, Tamil, and Malayalam, the model seeks to enable accurate and\ncontextually appropriate translations across diverse language pairs. By\ncomparing Decoder-only and Encoder-Decoder architectures, the project aims to\noptimize translation quality and efficiency, advancing cross-linguistic\ncommunication tools.The primary objective is to develop a model capable of\ndelivering high-quality translations that are accurate and contextually\nappropriate. By leveraging large language models, specifically comparing the\neffectiveness of Decoder-only and Encoder-Decoder architectures, the project\nseeks to optimize translation performance and efficiency across multilingual\ncontexts. Through rigorous experimentation and analysis, this project aims to\nadvance the field of machine translation, contributing valuable insights into\nthe effectiveness of different model architectures and paving the way for\nenhanced cross-linguistic communication tools.", "published": "2024-09-12 00:21:05", "link": "http://arxiv.org/abs/2409.13747v1", "categories": ["cs.CL", "cs.ET", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TheraGen: Therapy for Every Generation", "abstract": "We present TheraGen, an advanced AI-powered mental health chatbot utilizing\nthe LLaMA 2 7B model. This approach builds upon recent advancements in language\nmodels and transformer architectures. TheraGen provides all-day personalized,\ncompassionate mental health care by leveraging a large dataset of 1 million\nconversational entries, combining anonymized therapy transcripts, online mental\nhealth discussions, and psychological literature, including APA resources. Our\nimplementation employs transfer learning, fine-tuning, and advanced training\ntechniques to optimize performance. TheraGen offers a user-friendly interface\nfor seamless interaction, providing empathetic responses and evidence-based\ncoping strategies. Evaluation results demonstrate high user satisfaction rates,\nwith 94% of users reporting improved mental well-being. The system achieved a\nBLEU score of 0.67 and a ROUGE score of 0.62, indicating strong response\naccuracy. With an average response time of 1395 milliseconds, TheraGen ensures\nreal-time, efficient support. While not a replacement for professional therapy,\nTheraGen serves as a valuable complementary tool, significantly improving user\nwell-being and addressing the accessibility gap in mental health treatments.\nThis paper details TheraGen's architecture, training methodology, ethical\nconsiderations, and future directions, contributing to the growing field of\nAI-assisted mental healthcare and offering a scalable solution to the pressing\nneed for mental health support.", "published": "2024-09-12 17:15:44", "link": "http://arxiv.org/abs/2409.13748v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Retro-li: Small-Scale Retrieval Augmented Generation Supporting Noisy\n  Similarity Searches and Domain Shift Generalization", "abstract": "The retrieval augmented generation (RAG) system such as Retro has been shown\nto improve language modeling capabilities and reduce toxicity and\nhallucinations by retrieving from a database of non-parametric memory\ncontaining trillions of entries. We introduce Retro-li that shows retrieval can\nalso help using a small-scale database, but it demands more accurate and better\nneighbors when searching in a smaller hence sparser non-parametric memory. This\ncan be met by using a proper semantic similarity search. We further propose\nadding a regularization to the non-parametric memory for the first time: it\nsignificantly reduces perplexity when the neighbor search operations are noisy\nduring inference, and it improves generalization when a domain shift occurs. We\nalso show that Retro-li's non-parametric memory can potentially be implemented\non analog in-memory computing hardware, exhibiting O(1) search time while\ncausing noise in retrieving neighbors, with minimal (<1%) performance loss. Our\ncode is available at:\nhttps://github.com/IBM/Retrieval-Enhanced-Transformer-Little.", "published": "2024-09-12 23:29:33", "link": "http://arxiv.org/abs/2410.00004v2", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "LLM Honeypot: Leveraging Large Language Models as Advanced Interactive\n  Honeypot Systems", "abstract": "The rapid evolution of cyber threats necessitates innovative solutions for\ndetecting and analyzing malicious activity. Honeypots, which are decoy systems\ndesigned to lure and interact with attackers, have emerged as a critical\ncomponent in cybersecurity. In this paper, we present a novel approach to\ncreating realistic and interactive honeypot systems using Large Language Models\n(LLMs). By fine-tuning a pre-trained open-source language model on a diverse\ndataset of attacker-generated commands and responses, we developed a honeypot\ncapable of sophisticated engagement with attackers. Our methodology involved\nseveral key steps: data collection and processing, prompt engineering, model\nselection, and supervised fine-tuning to optimize the model's performance.\nEvaluation through similarity metrics and live deployment demonstrated that our\napproach effectively generates accurate and informative responses. The results\nhighlight the potential of LLMs to revolutionize honeypot technology, providing\ncybersecurity professionals with a powerful tool to detect and analyze\nmalicious activity, thereby enhancing overall security infrastructure.", "published": "2024-09-12 17:33:06", "link": "http://arxiv.org/abs/2409.08234v2", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG", "cs.NI", "68T50, 68M10", "I.2.7; D.4.6; K.6.5"], "primary_category": "cs.CR"}
{"title": "Rethinking Prompting Strategies for Multi-Label Recognition with Partial\n  Annotations", "abstract": "Vision-language models (VLMs) like CLIP have been adapted for Multi-Label\nRecognition (MLR) with partial annotations by leveraging prompt-learning, where\npositive and negative prompts are learned for each class to associate their\nembeddings with class presence or absence in the shared vision-text feature\nspace. While this approach improves MLR performance by relying on VLM priors,\nwe hypothesize that learning negative prompts may be suboptimal, as the\ndatasets used to train VLMs lack image-caption pairs explicitly focusing on\nclass absence. To analyze the impact of positive and negative prompt learning\non MLR, we introduce PositiveCoOp and NegativeCoOp, where only one prompt is\nlearned with VLM guidance while the other is replaced by an embedding vector\nlearned directly in the shared feature space without relying on the text\nencoder. Through empirical analysis, we observe that negative prompts degrade\nMLR performance, and learning only positive prompts, combined with learned\nnegative embeddings (PositiveCoOp), outperforms dual prompt learning\napproaches. Moreover, we quantify the performance benefits that prompt-learning\noffers over a simple vision-features-only baseline, observing that the baseline\ndisplays strong performance comparable to dual prompt learning approach\n(DualCoOp), when the proportion of missing labels is low, while requiring half\nthe training compute and 16 times fewer parameters", "published": "2024-09-12 20:02:51", "link": "http://arxiv.org/abs/2409.08381v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Self-Supervised Inference of Agents in Trustless Environments", "abstract": "In this paper, we propose a novel approach where agents can form swarms to\nproduce high-quality responses effectively. This is accomplished by utilizing\nagents capable of data inference and ranking, which can be effectively\nimplemented using LLMs as response classifiers. We assess existing approaches\nfor trustless agent inference, define our methodology, estimate practical\nparameters, and model various types of malicious agent attacks. Our method\nleverages the collective intelligence of swarms, ensuring robust and efficient\ndecentralized AI inference with better accuracy, security, and reliability. We\nshow that our approach is an order of magnitude faster than other trustless\ninference strategies reaching less than 125 ms validation latency.", "published": "2024-09-12 20:32:07", "link": "http://arxiv.org/abs/2409.08386v1", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.CR", "cs.DC"], "primary_category": "cs.MA"}
{"title": "On the Vulnerability of Applying Retrieval-Augmented Generation within\n  Knowledge-Intensive Application Domains", "abstract": "Retrieval-Augmented Generation (RAG) has been empirically shown to enhance\nthe performance of large language models (LLMs) in knowledge-intensive domains\nsuch as healthcare, finance, and legal contexts. Given a query, RAG retrieves\nrelevant documents from a corpus and integrates them into the LLMs' generation\nprocess. In this study, we investigate the adversarial robustness of RAG,\nfocusing specifically on examining the retrieval system. First, across 225\ndifferent setup combinations of corpus, retriever, query, and targeted\ninformation, we show that retrieval systems are vulnerable to universal\npoisoning attacks in medical Q\\&A. In such attacks, adversaries generate\npoisoned documents containing a broad spectrum of targeted information, such as\npersonally identifiable information. When these poisoned documents are inserted\ninto a corpus, they can be accurately retrieved by any users, as long as\nattacker-specified queries are used. To understand this vulnerability, we\ndiscovered that the deviation from the query's embedding to that of the\npoisoned document tends to follow a pattern in which the high similarity\nbetween the poisoned document and the query is retained, thereby enabling\nprecise retrieval. Based on these findings, we develop a new detection-based\ndefense to ensure the safe use of RAG. Through extensive experiments spanning\nvarious Q\\&A domains, we observed that our proposed method consistently\nachieves excellent detection rates in nearly all cases.", "published": "2024-09-12 02:43:40", "link": "http://arxiv.org/abs/2409.17275v1", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.DB", "cs.ET", "cs.IR", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Detecting and Defending Against Adversarial Attacks on Automatic Speech\n  Recognition via Diffusion Models", "abstract": "Automatic speech recognition (ASR) systems are known to be vulnerable to\nadversarial attacks. This paper addresses detection and defence against\ntargeted white-box attacks on speech signals for ASR systems. While existing\nwork has utilised diffusion models (DMs) to purify adversarial examples,\nachieving state-of-the-art results in keyword spotting tasks, their\neffectiveness for more complex tasks such as sentence-level ASR remains\nunexplored. Additionally, the impact of the number of forward diffusion steps\non performance is not well understood. In this paper, we systematically\ninvestigate the use of DMs for defending against adversarial attacks on\nsentences and examine the effect of varying forward diffusion steps. Through\ncomprehensive experiments on the Mozilla Common Voice dataset, we demonstrate\nthat two forward diffusion steps can completely defend against adversarial\nattacks on sentences. Moreover, we introduce a novel, training-free approach\nfor detecting adversarial attacks by leveraging a pre-trained DM. Our\nexperimental results show that this method can detect adversarial attacks with\nhigh accuracy.", "published": "2024-09-12 11:03:50", "link": "http://arxiv.org/abs/2409.07936v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Auto-Landmark: Acoustic Landmark Dataset and Open-Source Toolkit for\n  Landmark Extraction", "abstract": "In the speech signal, acoustic landmarks identify times when the acoustic\nmanifestations of the linguistically motivated distinctive features are most\nsalient. Acoustic landmarks have been widely applied in various domains,\nincluding speech recognition, speech depression detection, clinical analysis of\nspeech abnormalities, and the detection of disordered speech. However, there is\ncurrently no dataset available that provides precise timing information for\nlandmarks, which has been proven to be crucial for downstream applications\ninvolving landmarks. In this paper, we selected the most useful acoustic\nlandmarks based on previous research and annotated the TIMIT dataset with them,\nbased on a combination of phoneme boundary information and manual inspection.\nMoreover, previous landmark extraction tools were not open source or\nbenchmarked, so to address this, we developed an open source Python-based\nlandmark extraction tool and established a series of landmark detection\nbaselines. The first of their kinds, the dataset with landmark precise timing\ninformation, landmark extraction tool and baselines are designed to support a\nwide variety of future research.", "published": "2024-09-12 12:03:26", "link": "http://arxiv.org/abs/2409.07969v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Dark Experience for Incremental Keyword Spotting", "abstract": "Spoken keyword spotting (KWS) is crucial for identifying keywords within\naudio inputs and is widely used in applications like Apple Siri and Google\nHome, particularly on edge devices. Current deep learning-based KWS systems,\nwhich are typically trained on a limited set of keywords, can suffer from\nperformance degradation when encountering new domains, a challenge often\naddressed through few-shot fine-tuning. However, this adaptation frequently\nleads to catastrophic forgetting, where the model's performance on original\ndata deteriorates. Progressive continual learning (CL) strategies have been\nproposed to overcome this, but they face limitations such as the need for\ntask-ID information and increased storage, making them less practical for\nlightweight devices. To address these challenges, we introduce Dark Experience\nfor Keyword Spotting (DE-KWS), a novel CL approach that leverages dark\nknowledge to distill past experiences throughout the training process. DE-KWS\ncombines rehearsal and distillation, using both ground truth labels and logits\nstored in a memory buffer to maintain model performance across tasks.\nEvaluations on the Google Speech Command dataset show that DE-KWS outperforms\nexisting CL baselines in average accuracy without increasing model size,\noffering an effective solution for resource-constrained edge devices. The\nscripts are available on GitHub for the future research.", "published": "2024-09-12 15:48:45", "link": "http://arxiv.org/abs/2409.08153v3", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Hierarchical Symbolic Pop Music Generation with Graph Neural Networks", "abstract": "Music is inherently made up of complex structures, and representing them as\ngraphs helps to capture multiple levels of relationships. While music\ngeneration has been explored using various deep generation techniques, research\non graph-related music generation is sparse. Earlier graph-based music\ngeneration worked only on generating melodies, and recent works to generate\npolyphonic music do not account for longer-term structure. In this paper, we\nexplore a multi-graph approach to represent both the rhythmic patterns and\nphrase structure of Chinese pop music. Consequently, we propose a two-step\napproach that aims to generate polyphonic music with coherent rhythm and\nlong-term structure. We train two Variational Auto-Encoder networks - one on a\nMIDI dataset to generate 4-bar phrases, and another on song structure labels to\ngenerate full song structure. Our work shows that the models are able to learn\nmost of the structural nuances in the training dataset, including chord and\npitch frequency distributions, and phrase attributes.", "published": "2024-09-12 15:51:09", "link": "http://arxiv.org/abs/2409.08155v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Super Monotonic Alignment Search", "abstract": "Monotonic alignment search (MAS), introduced by Glow-TTS, is one of the most\npopular algorithm in TTS to estimate unknown alignments between text and\nspeech. Since this algorithm needs to search for the most probable alignment\nwith dynamic programming by caching all paths, the time complexity of the\nalgorithm is $O(T \\times S)$. The authors of Glow-TTS run this algorithm on\nCPU, and while they mentioned it is difficult to parallelize, we found that MAS\ncan be parallelized in text-length dimension and CPU execution consumes an\ninordinate amount of time for inter-device copy. Therefore, we implemented a\nTriton kernel and PyTorch JIT script to accelerate MAS on GPU without\ninter-device copy. As a result, Super-MAS Triton kernel is up to 72 times\nfaster in the extreme-length case. The code is available at\n\\url{https://github.com/supertone-inc/super-monotonic-align}.", "published": "2024-09-12 02:13:57", "link": "http://arxiv.org/abs/2409.07704v1", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "Universal Pooling Method of Multi-layer Features from Pretrained Models\n  for Speaker Verification", "abstract": "Recent advancements in automatic speaker verification (ASV) studies have been\nachieved by leveraging large-scale pretrained networks. In this study, we\nanalyze the approaches toward such a paradigm and underline the significance of\ninterlayer information processing as a result. Accordingly, we present a novel\napproach for exploiting the multilayered nature of pretrained models for ASV,\nwhich comprises a layer/frame-level network and two steps of pooling\narchitectures for each layer and frame axis. Specifically, we let convolutional\narchitecture directly processes a stack of layer outputs.Then, we present a\nchannel attention-based scheme of gauging layer significance and squeeze the\nlayer level with the most representative value. Finally, attentive statistics\nover frame-level representations yield a single vector speaker embedding.\nComparative experiments are designed using versatile data environments and\ndiverse pretraining models to validate the proposed approach. The experimental\nresults demonstrate the stability of the approach using multi-layer outputs in\nleveraging pretrained architectures. Then, we verify the superiority of the\nproposed ASV backend structure, which involves layer-wise operations, in terms\nof performance improvement along with cost efficiency compared to the\nconventional method. The ablation study shows how the proposed interlayer\nprocessing aids in maximizing the advantage of utilizing pretrained models.", "published": "2024-09-12 05:55:32", "link": "http://arxiv.org/abs/2409.07770v1", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "Graph Neural Networks for Parkinsons Disease Detection", "abstract": "Despite the promising performance of state of the art approaches for\nParkinsons Disease (PD) detection, these approaches often analyze individual\nspeech segments in isolation, which can lead to suboptimal results. Dysarthric\ncues that characterize speech impairments from PD patients are expected to be\nrelated across segments from different speakers. Isolated segment analysis\nfails to exploit these inter segment relationships. Additionally, not all\nspeech segments from PD patients exhibit clear dysarthric symptoms, introducing\nlabel noise that can negatively affect the performance and generalizability of\ncurrent approaches. To address these challenges, we propose a novel PD\ndetection framework utilizing Graph Convolutional Networks (GCNs). By\nrepresenting speech segments as nodes and capturing the similarity between\nsegments through edges, our GCN model facilitates the aggregation of dysarthric\ncues across the graph, effectively exploiting segment relationships and\nmitigating the impact of label noise. Experimental results demonstrate\ntheadvantages of the proposed GCN model for PD detection and provide insights\ninto its underlying mechanisms", "published": "2024-09-12 09:44:13", "link": "http://arxiv.org/abs/2409.07884v3", "categories": ["cs.LG", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Zero-Shot Sing Voice Conversion: built upon clustering-based phoneme\n  representations", "abstract": "This study presents an innovative Zero-Shot any-to-any Singing Voice\nConversion (SVC) method, leveraging a novel clustering-based phoneme\nrepresentation to effectively separate content, timbre, and singing style. This\napproach enables precise voice characteristic manipulation. We discovered that\ndatasets with fewer recordings per artist are more susceptible to timbre\nleakage. Extensive testing on over 10,000 hours of singing and user feedback\nrevealed our model significantly improves sound quality and timbre accuracy,\naligning with our objectives and advancing voice conversion technology.\nFurthermore, this research advances zero-shot SVC and sets the stage for future\nwork on discrete speech representation, emphasizing the preservation of rhyme.", "published": "2024-09-12 13:42:04", "link": "http://arxiv.org/abs/2409.08039v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Faster Speech-LLaMA Inference with Multi-token Prediction", "abstract": "Large language models (LLMs) have become proficient at solving a wide variety\nof tasks, including those involving multi-modal inputs. In particular,\ninstantiating an LLM (such as LLaMA) with a speech encoder and training it on\npaired data imparts speech recognition (ASR) abilities to the decoder-only\nmodel, hence called Speech-LLaMA. Nevertheless, due to the sequential nature of\nauto-regressive inference and the relatively large decoder, Speech-LLaMA models\nrequire relatively high inference time. In this work, we propose to speed up\nSpeech-LLaMA inference by predicting multiple tokens in the same decoding step.\nWe explore several model architectures that enable this, and investigate their\nperformance using threshold-based and verification-based inference strategies.\nWe also propose a prefix-based beam search decoding method that allows\nefficient minimum word error rate (MWER) training for such models. We evaluate\nour models on a variety of public benchmarks, where they reduce the number of\ndecoder calls by ~3.2x while maintaining or improving WER performance.", "published": "2024-09-12 15:43:10", "link": "http://arxiv.org/abs/2409.08148v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Efficient Sparse Coding with the Adaptive Locally Competitive Algorithm\n  for Speech Classification", "abstract": "Researchers are exploring novel computational paradigms such as sparse coding\nand neuromorphic computing to bridge the efficiency gap between the human brain\nand conventional computers in complex tasks. A key area of focus is\nneuromorphic audio processing. While the Locally Competitive Algorithm has\nemerged as a promising solution for sparse coding, offering potential for\nreal-time and low-power processing on neuromorphic hardware, its applications\nin neuromorphic speech classification have not been thoroughly studied. The\nAdaptive Locally Competitive Algorithm builds upon the Locally Competitive\nAlgorithm by dynamically adjusting the modulation parameters of the filter bank\nto fine-tune the filters' sensitivity. This adaptability enhances lateral\ninhibition, improving reconstruction quality, sparsity, and convergence time,\nwhich is crucial for real-time applications. This paper demonstrates the\npotential of the Locally Competitive Algorithm and its adaptive variant as\nrobust feature extractors for neuromorphic speech classification. Results show\nthat the Locally Competitive Algorithm achieves better speech classification\naccuracy at the expense of higher power consumption compared to the LAUSCHER\ncochlea model used for benchmarking. On the other hand, the Adaptive Locally\nCompetitive Algorithm mitigates this power consumption issue without\ncompromising the accuracy. The dynamic power consumption is reduced to a range\nof 4 to 13 milliwatts on neuromorphic hardware, three orders of magnitude less\nthan setups using Graphics Processing Units. These findings position the\nAdaptive Locally Competitive Algorithm as a compelling solution for efficient\nspeech classification systems, promising substantial advancements in balancing\nspeech classification accuracy and power efficiency.", "published": "2024-09-12 16:26:29", "link": "http://arxiv.org/abs/2409.08188v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "OpenACE: An Open Benchmark for Evaluating Audio Coding Performance", "abstract": "Audio and speech coding lack unified evaluation and open-source testing. Many\ncandidate systems were evaluated on proprietary, non-reproducible, or small\ndata, and machine learning-based codecs are often tested on datasets with\nsimilar distributions as trained on, which is unfairly compared to digital\nsignal processing-based codecs that usually work well with unseen data. This\npaper presents a full-band audio and speech coding quality benchmark with more\nvariable content types, including traditional open test vectors. An example use\ncase of audio coding quality assessment is presented with open-source Opus,\n3GPP's EVS, and recent ETSI's LC3 with LC3+ used in Bluetooth LE Audio\nprofiles. Besides, quality variations of emotional speech encoding at 16 kbps\nare shown. The proposed open-source benchmark contributes to audio and speech\ncoding democratization and is available at\nhttps://github.com/JozefColdenhoff/OpenACE.", "published": "2024-09-12 19:49:01", "link": "http://arxiv.org/abs/2409.08374v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SoloAudio: Target Sound Extraction with Language-oriented Audio\n  Diffusion Transformer", "abstract": "In this paper, we introduce SoloAudio, a novel diffusion-based generative\nmodel for target sound extraction (TSE). Our approach trains latent diffusion\nmodels on audio, replacing the previous U-Net backbone with a skip-connected\nTransformer that operates on latent features. SoloAudio supports both\naudio-oriented and language-oriented TSE by utilizing a CLAP model as the\nfeature extractor for target sounds. Furthermore, SoloAudio leverages synthetic\naudio generated by state-of-the-art text-to-audio models for training,\ndemonstrating strong generalization to out-of-domain data and unseen sound\nevents. We evaluate this approach on the FSD Kaggle 2018 mixture dataset and\nreal data from AudioSet, where SoloAudio achieves the state-of-the-art results\non both in-domain and out-of-domain data, and exhibits impressive zero-shot and\nfew-shot capabilities. Source code and demos are released.", "published": "2024-09-12 23:12:25", "link": "http://arxiv.org/abs/2409.08425v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Music auto-tagging in the long tail: A few-shot approach", "abstract": "In the realm of digital music, using tags to efficiently organize and\nretrieve music from extensive databases is crucial for music catalog owners.\nHuman tagging by experts is labor-intensive but mostly accurate, whereas\nautomatic tagging through supervised learning has approached satisfying\naccuracy but is restricted to a predefined set of training tags. Few-shot\nlearning offers a viable solution to expand beyond this small set of predefined\ntags by enabling models to learn from only a few human-provided examples to\nunderstand tag meanings and subsequently apply these tags autonomously. We\npropose to integrate few-shot learning methodology into multi-label music\nauto-tagging by using features from pre-trained models as inputs to a\nlightweight linear classifier, also known as a linear probe. We investigate\ndifferent popular pre-trained features, as well as different few-shot\nparametrizations with varying numbers of classes and samples per class. Our\nexperiments demonstrate that a simple model with pre-trained features can\nachieve performance close to state-of-the-art models while using significantly\nless training data, such as 20 samples per tag. Additionally, our linear probe\nperforms competitively with leading models when trained on the entire training\ndataset. The results show that this transfer learning-based few-shot approach\ncould effectively address the issue of automatically assigning long-tail tags\nwith only limited labeled data.", "published": "2024-09-12 03:33:19", "link": "http://arxiv.org/abs/2409.07730v2", "categories": ["eess.AS", "cs.IR", "cs.LG", "cs.SD", "H.3.3"], "primary_category": "eess.AS"}
{"title": "Bridging Paintings and Music -- Exploring Emotion based Music Generation\n  through Paintings", "abstract": "Rapid advancements in artificial intelligence have significantly enhanced\ngenerative tasks involving music and images, employing both unimodal and\nmultimodal approaches. This research develops a model capable of generating\nmusic that resonates with the emotions depicted in visual arts, integrating\nemotion labeling, image captioning, and language models to transform visual\ninputs into musical compositions. Addressing the scarcity of aligned art and\nmusic data, we curated the Emotion Painting Music Dataset, pairing paintings\nwith corresponding music for effective training and evaluation. Our dual-stage\nframework converts images to text descriptions of emotional content and then\ntransforms these descriptions into music, facilitating efficient learning with\nminimal data. Performance is evaluated using metrics such as Fr\\'echet Audio\nDistance (FAD), Total Harmonic Distortion (THD), Inception Score (IS), and KL\ndivergence, with audio-emotion text similarity confirmed by the pre-trained\nCLAP model to demonstrate high alignment between generated music and text. This\nsynthesis tool bridges visual art and music, enhancing accessibility for the\nvisually impaired and opening avenues in educational and therapeutic\napplications by providing enriched multi-sensory experiences.", "published": "2024-09-12 08:19:25", "link": "http://arxiv.org/abs/2409.07827v1", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "TSELM: Target Speaker Extraction using Discrete Tokens and Language\n  Models", "abstract": "We propose TSELM, a novel target speaker extraction network that leverages\ndiscrete tokens and language models. TSELM utilizes multiple discretized layers\nfrom WavLM as input tokens and incorporates cross-attention mechanisms to\nintegrate target speaker information. Language models are employed to capture\nthe sequence dependencies, while a scalable HiFi-GAN is used to reconstruct the\naudio from the tokens. By applying a cross-entropy loss, TSELM models the\nprobability distribution of output tokens, thus converting the complex\nregression problem of audio generation into a classification task. Experimental\nresults show that TSELM achieves excellent results in speech quality and\ncomparable results in speech intelligibility.", "published": "2024-09-12 08:41:07", "link": "http://arxiv.org/abs/2409.07841v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio Decoding by Inverse Problem Solving", "abstract": "We consider audio decoding as an inverse problem and solve it through\ndiffusion posterior sampling. Explicit conditioning functions are developed for\ninput signal measurements provided by an example of a transform domain\nperceptual audio codec. Viability is demonstrated by evaluating arbitrary\npairings of a set of bitrates and task-agnostic prior models. For instance, we\nobserve significant improvements on piano while maintaining speech performance\nwhen a speech model is replaced by a joint model trained on both speech and\npiano. With a more general music model, improved decoding compared to legacy\nmethods is obtained for a broad range of content types and bitrates. The noisy\nmean model, underlying the proposed derivation of conditioning, enables a\nsignificant reduction of gradient evaluations for diffusion posterior sampling,\ncompared to methods based on Tweedie's mean. Combining Tweedie's mean with our\nconditioning functions improves the objective performance. An audio demo is\navailable at https://dpscodec-demo.github.io/.", "published": "2024-09-12 09:05:18", "link": "http://arxiv.org/abs/2409.07858v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Tidal MerzA: Combining affective modelling and autonomous code\n  generation through Reinforcement Learning", "abstract": "This paper presents Tidal-MerzA, a novel system designed for collaborative\nperformances between humans and a machine agent in the context of live coding,\nspecifically focusing on the generation of musical patterns. Tidal-MerzA fuses\ntwo foundational models: ALCAA (Affective Live Coding Autonomous Agent) and\nTidal Fuzz, a computational framework. By integrating affective modelling with\ncomputational generation, this system leverages reinforcement learning\ntechniques to dynamically adapt music composition parameters within the\nTidalCycles framework, ensuring both affective qualities to the patterns and\nsyntactical correctness. The development of Tidal-MerzA introduces two distinct\nagents: one focusing on the generation of mini-notation strings for musical\nexpression, and another on the alignment of music with targeted affective\nstates through reinforcement learning. This approach enhances the adaptability\nand creative potential of live coding practices and allows exploration of\nhuman-machine creative interactions. Tidal-MerzA advances the field of\ncomputational music generation, presenting a novel methodology for\nincorporating artificial intelligence into artistic practices.", "published": "2024-09-12 10:38:55", "link": "http://arxiv.org/abs/2409.07918v1", "categories": ["cs.HC", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
{"title": "Detection of Electric Motor Damage Through Analysis of Sound Signals\n  Using Bayesian Neural Networks", "abstract": "Fault monitoring and diagnostics are important to ensure reliability of\nelectric motors. Efficient algorithms for fault detection improve reliability,\nyet development of cost-effective and reliable classifiers for diagnostics of\nequipment is challenging, in particular due to unavailability of well-balanced\ndatasets, with signals from properly functioning equipment and those from\nfaulty equipment. Thus, we propose to use a Bayesian neural network to detect\nand classify faults in electric motors, given its efficacy with imbalanced\ntraining data. The performance of the proposed network is demonstrated on real\nlife signals, and a robustness analysis of the proposed solution is provided.", "published": "2024-09-12 07:15:59", "link": "http://arxiv.org/abs/2409.08309v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Early Joint Learning of Emotion Information Makes MultiModal Model\n  Understand You Better", "abstract": "In this paper, we present our solutions for emotion recognition in the\nsub-challenges of Multimodal Emotion Recognition Challenge (MER2024). To\nmitigate the modal competition issue between audio and text, we adopt an early\nfusion strategy based on a large language model, where joint training of audio\nand text is conducted initially. And the joint Audio-Text modal feature will be\nlate-fused with other unimodal features. In order to solve the problems of data\ninsufficiency and class imbalance, We use multiple turns of multi-model voting\nfor data mining. Moreover, to enhance the quality of audio features, we employ\nspeech source separation to preprocess audios. Our model ranks \\textbf{2nd} in\nboth MER2024-SEMI and MER2024-NOISE, validating our method's effectiveness.", "published": "2024-09-12 05:05:34", "link": "http://arxiv.org/abs/2409.18971v1", "categories": ["cs.MM", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
