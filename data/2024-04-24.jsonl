{"title": "Retrieval Head Mechanistically Explains Long-Context Factuality", "abstract": "Despite the recent progress in long-context language models, it remains\nelusive how transformer-based models exhibit the capability to retrieve\nrelevant information from arbitrary locations within the long context. This\npaper aims to address this question. Our systematic investigation across a wide\nspectrum of models reveals that a special type of attention heads are largely\nresponsible for retrieving information, which we dub retrieval heads. We\nidentify intriguing properties of retrieval heads:(1) universal: all the\nexplored models with long-context capability have a set of retrieval heads; (2)\nsparse: only a small portion (less than 5\\%) of the attention heads are\nretrieval. (3) intrinsic: retrieval heads already exist in models pretrained\nwith short context. When extending the context length by continual pretraining,\nit is still the same set of heads that perform information retrieval. (4)\ndynamically activated: take Llama-2 7B for example, 12 retrieval heads always\nattend to the required information no matter how the context is changed. The\nrest of the retrieval heads are activated in different contexts. (5) causal:\ncompletely pruning retrieval heads leads to failure in retrieving relevant\ninformation and results in hallucination, while pruning random non-retrieval\nheads does not affect the model's retrieval ability. We further show that\nretrieval heads strongly influence chain-of-thought (CoT) reasoning, where the\nmodel needs to frequently refer back the question and previously-generated\ncontext. Conversely, tasks where the model directly generates the answer using\nits intrinsic knowledge are less impacted by masking out retrieval heads. These\nobservations collectively explain which internal part of the model seeks\ninformation from the input tokens. We believe our insights will foster future\nresearch on reducing hallucination, improving reasoning, and compressing the KV\ncache.", "published": "2024-04-24 00:24:03", "link": "http://arxiv.org/abs/2404.15574v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Foundational Large Language Models Assist with Conducting\n  Pharmaceuticals Manufacturing Investigations?", "abstract": "General purpose Large Language Models (LLM) such as the Generative Pretrained\nTransformer (GPT) and Large Language Model Meta AI (LLaMA) have attracted much\nattention in recent years. There is strong evidence that these models can\nperform remarkably well in various natural language processing tasks. However,\nhow to leverage them to approach domain-specific use cases and drive value\nremains an open question. In this work, we focus on a specific use case,\npharmaceutical manufacturing investigations, and propose that leveraging\nhistorical records of manufacturing incidents and deviations in an organization\ncan be beneficial for addressing and closing new cases, or de-risking new\nmanufacturing campaigns. Using a small but diverse dataset of real\nmanufacturing deviations selected from different product lines, we evaluate and\nquantify the power of three general purpose LLMs (GPT-3.5, GPT-4, and Claude-2)\nin performing tasks related to the above goal. In particular, (1) the ability\nof LLMs in automating the process of extracting specific information such as\nroot cause of a case from unstructured data, as well as (2) the possibility of\nidentifying similar or related deviations by performing semantic search on the\ndatabase of historical records are examined. While our results point to the\nhigh accuracy of GPT-4 and Claude-2 in the information extraction task, we\ndiscuss cases of complex interplay between the apparent reasoning and\nhallucination behavior of LLMs as a risk factor. Furthermore, we show that\nsemantic search on vector embedding of deviation descriptions can be used to\nidentify similar records, such as those with a similar type of defect, with a\nhigh level of accuracy. We discuss further improvements to enhance the accuracy\nof similar record identification.", "published": "2024-04-24 00:56:22", "link": "http://arxiv.org/abs/2404.15578v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Minimal Evidence Group Identification for Claim Verification", "abstract": "Claim verification in real-world settings (e.g. against a large collection of\ncandidate evidences retrieved from the web) typically requires identifying and\naggregating a complete set of evidence pieces that collectively provide full\nsupport to the claim. The problem becomes particularly challenging when there\nexists distinct sets of evidence that could be used to verify the claim from\ndifferent perspectives. In this paper, we formally define and study the problem\nof identifying such minimal evidence groups (MEGs) for claim verification. We\nshow that MEG identification can be reduced from Set Cover problem, based on\nentailment inference of whether a given evidence group provides full/partial\nsupport to a claim. Our proposed approach achieves 18.4% and 34.8% absolute\nimprovements on the WiCE and SciFact datasets over LLM prompting. Finally, we\ndemonstrate the benefits of MEGs in downstream applications such as claim\ngeneration.", "published": "2024-04-24 01:44:09", "link": "http://arxiv.org/abs/2404.15588v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CodeIP: A Grammar-Guided Multi-Bit Watermark for Large Language Models\n  of Code", "abstract": "Large Language Models (LLMs) have achieved remarkable progress in code\ngeneration. It now becomes crucial to identify whether the code is AI-generated\nand to determine the specific model used, particularly for purposes such as\nprotecting Intellectual Property (IP) in industry and preventing cheating in\nprogramming exercises. To this end, several attempts have been made to insert\nwatermarks into machine-generated code. However, existing approaches are\nlimited to inserting only a single bit of information. In this paper, we\nintroduce CodeIP, a novel multi-bit watermarking technique that inserts\nadditional information to preserve crucial provenance details, such as the\nvendor ID of an LLM, thereby safeguarding the IPs of LLMs in code generation.\nFurthermore, to ensure the syntactical correctness of the generated code, we\npropose constraining the sampling process for predicting the next token by\ntraining a type predictor. Experiments conducted on a real-world dataset across\nfive programming languages demonstrate the effectiveness of CodeIP in\nwatermarking LLMs for code generation while maintaining the syntactical\ncorrectness of code.", "published": "2024-04-24 04:25:04", "link": "http://arxiv.org/abs/2404.15639v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Return of EM: Entity-driven Answer Set Expansion for QA Evaluation", "abstract": "Recently, directly using large language models (LLMs) has been shown to be\nthe most reliable method to evaluate QA models. However, it suffers from\nlimited interpretability, high cost, and environmental harm. To address these,\nwe propose to use soft EM with entity-driven answer set expansion. Our approach\nexpands the gold answer set to include diverse surface forms, based on the\nobservation that the surface forms often follow particular patterns depending\non the entity type. The experimental results show that our method outperforms\ntraditional evaluation methods by a large margin. Moreover, the reliability of\nour evaluation method is comparable to that of LLM-based ones, while offering\nthe benefits of high interpretability and reduced environmental harm.", "published": "2024-04-24 05:08:55", "link": "http://arxiv.org/abs/2404.15650v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KS-LLM: Knowledge Selection of Large Language Models with Evidence\n  Document for Question Answering", "abstract": "Large language models (LLMs) suffer from the hallucination problem and face\nsignificant challenges when applied to knowledge-intensive tasks. A promising\napproach is to leverage evidence documents as extra supporting knowledge, which\ncan be obtained through retrieval or generation. However, existing methods\ndirectly leverage the entire contents of the evidence document, which may\nintroduce noise information and impair the performance of large language\nmodels. To tackle this problem, we propose a novel Knowledge Selection of Large\nLanguage Models (KS-LLM) method, aiming to identify valuable information from\nevidence documents. The KS-LLM approach utilizes triples to effectively select\nknowledge snippets from evidence documents that are beneficial to answering\nquestions. Specifically, we first generate triples based on the input question,\nthen select the evidence sentences most similar to triples from the evidence\ndocument, and finally combine the evidence sentences and triples to assist\nlarge language models in generating answers. Experimental comparisons on\nseveral question answering datasets, such as TriviaQA, WebQ, and NQ,\ndemonstrate that the proposed method surpasses the baselines and achieves the\nbest results.", "published": "2024-04-24 05:32:41", "link": "http://arxiv.org/abs/2404.15660v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Nyonic Technical Report", "abstract": "This report details the development and key achievements of our latest\nlanguage model designed for custom large language models. The advancements\nintroduced include a novel Online Data Scheduler that supports flexible\ntraining data adjustments and curriculum learning. The model's architecture is\nfortified with state-of-the-art techniques such as Rotary Positional\nEmbeddings, QK-LayerNorm, and a specially crafted multilingual tokenizer to\nenhance stability and performance. Moreover, our robust training framework\nincorporates advanced monitoring and rapid recovery features to ensure optimal\nefficiency. Our Wonton 7B model has demonstrated competitive performance on a\nrange of multilingual and English benchmarks. Future developments will\nprioritize narrowing the performance gap with more extensively trained models,\nthereby enhancing the model's real-world efficacy and adaptability.GitHub:\n\\url{https://github.com/nyonicai/nyonic-public}", "published": "2024-04-24 07:38:44", "link": "http://arxiv.org/abs/2404.15702v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Annotator-Centric Active Learning for Subjective NLP Tasks", "abstract": "Active Learning (AL) addresses the high costs of collecting human annotations\nby strategically annotating the most informative samples. However, for\nsubjective NLP tasks, incorporating a wide range of perspectives in the\nannotation process is crucial to capture the variability in human judgments. We\nintroduce Annotator-Centric Active Learning (ACAL), which incorporates an\nannotator selection strategy following data sampling. Our objective is\ntwo-fold: 1) to efficiently approximate the full diversity of human judgments,\nand 2) to assess model performance using annotator-centric metrics, which value\nminority and majority perspectives equally. We experiment with multiple\nannotator selection strategies across seven subjective NLP tasks, employing\nboth traditional and novel, human-centered evaluation metrics. Our findings\nindicate that ACAL improves data efficiency and excels in annotator-centric\nperformance evaluations. However, its success depends on the availability of a\nsufficiently large and diverse pool of annotators to sample from.", "published": "2024-04-24 08:13:02", "link": "http://arxiv.org/abs/2404.15720v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "No Train but Gain: Language Arithmetic for training-free Language\n  Adapters enhancement", "abstract": "Modular deep learning is the state-of-the-art solution for lifting the curse\nof multilinguality, preventing the impact of negative interference and enabling\ncross-lingual performance in Multilingual Pre-trained Language Models. However,\na trade-off of this approach is the reduction in positive transfer learning\nfrom closely related languages. In response, we introduce a novel method called\nlanguage arithmetic, which enables training-free post-processing to address\nthis limitation. Extending the task arithmetic framework, we apply learning via\naddition to the language adapters, transitioning the framework from a\nmulti-task to a multilingual setup. The effectiveness of the proposed solution\nis demonstrated on three downstream tasks in a MAD-X-based set of cross-lingual\nschemes, acting as a post-processing procedure. Language arithmetic\nconsistently improves the baselines with significant gains, especially in the\nmost challenging case of zero-shot application. Our code and models are\navailable at https://github.com/mklimasz/language-arithmetic .", "published": "2024-04-24 08:52:40", "link": "http://arxiv.org/abs/2404.15737v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Comprehensive Survey on Evaluating Large Language Model Applications\n  in the Medical Industry", "abstract": "Since the inception of the Transformer architecture in 2017, Large Language\nModels (LLMs) such as GPT and BERT have evolved significantly, impacting\nvarious industries with their advanced capabilities in language understanding\nand generation. These models have shown potential to transform the medical\nfield, highlighting the necessity for specialized evaluation frameworks to\nensure their effective and ethical deployment. This comprehensive survey\ndelineates the extensive application and requisite evaluation of LLMs within\nhealthcare, emphasizing the critical need for empirical validation to fully\nexploit their capabilities in enhancing healthcare outcomes. Our survey is\nstructured to provide an in-depth analysis of LLM applications across clinical\nsettings, medical text data processing, research, education, and public health\nawareness. We begin by exploring the roles of LLMs in various medical\napplications, detailing their evaluation based on performance in tasks such as\nclinical diagnosis, medical text data processing, information retrieval, data\nanalysis, and educational content generation. The subsequent sections offer a\ncomprehensive discussion on the evaluation methods and metrics employed,\nincluding models, evaluators, and comparative experiments. We further examine\nthe benchmarks and datasets utilized in these evaluations, providing a\ncategorized description of benchmarks for tasks like question answering,\nsummarization, information extraction, bioinformatics, information retrieval\nand general comprehensive benchmarks. This structure ensures a thorough\nunderstanding of how LLMs are assessed for their effectiveness, accuracy,\nusability, and ethical alignment in the medical domain. ...", "published": "2024-04-24 09:55:24", "link": "http://arxiv.org/abs/2404.15777v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "One Subgraph for All: Efficient Reasoning on Opening Subgraphs for\n  Inductive Knowledge Graph Completion", "abstract": "Knowledge Graph Completion (KGC) has garnered massive research interest\nrecently, and most existing methods are designed following a transductive\nsetting where all entities are observed during training. Despite the great\nprogress on the transductive KGC, these methods struggle to conduct reasoning\non emerging KGs involving unseen entities. Thus, inductive KGC, which aims to\ndeduce missing links among unseen entities, has become a new trend. Many\nexisting studies transform inductive KGC as a graph classification problem by\nextracting enclosing subgraphs surrounding each candidate triple.\nUnfortunately, they still face certain challenges, such as the expensive time\nconsumption caused by the repeat extraction of enclosing subgraphs, and the\ndeficiency of entity-independent feature learning. To address these issues, we\npropose a global-local anchor representation (GLAR) learning method for\ninductive KGC. Unlike previous methods that utilize enclosing subgraphs, we\nextract a shared opening subgraph for all candidates and perform reasoning on\nit, enabling the model to perform reasoning more efficiently. Moreover, we\ndesign some transferable global and local anchors to learn rich\nentity-independent features for emerging entities. Finally, a global-local\ngraph reasoning model is applied on the opening subgraph to rank all\ncandidates. Extensive experiments show that our GLAR outperforms most existing\nstate-of-the-art methods.", "published": "2024-04-24 11:12:08", "link": "http://arxiv.org/abs/2404.15807v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring LLM Prompting Strategies for Joint Essay Scoring and Feedback\n  Generation", "abstract": "Individual feedback can help students improve their essay writing skills.\nHowever, the manual effort required to provide such feedback limits\nindividualization in practice. Automatically-generated essay feedback may serve\nas an alternative to guide students at their own pace, convenience, and desired\nfrequency. Large language models (LLMs) have demonstrated strong performance in\ngenerating coherent and contextually relevant text. Yet, their ability to\nprovide helpful essay feedback is unclear. This work explores several prompting\nstrategies for LLM-based zero-shot and few-shot generation of essay feedback.\nInspired by Chain-of-Thought prompting, we study how and to what extent\nautomated essay scoring (AES) can benefit the quality of generated feedback. We\nevaluate both the AES performance that LLMs can achieve with prompting only and\nthe helpfulness of the generated essay feedback. Our results suggest that\ntackling AES and feedback generation jointly improves AES performance. However,\nwhile our manual evaluation emphasizes the quality of the generated essay\nfeedback, the impact of essay scoring on the generated feedback remains low\nultimately.", "published": "2024-04-24 12:48:06", "link": "http://arxiv.org/abs/2404.15845v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Complex to Simple: Enhancing Multi-Constraint Complex Instruction\n  Following Ability of Large Language Models", "abstract": "It is imperative for Large language models (LLMs) to follow instructions with\nelaborate requirements (i.e. Complex Instructions Following). Yet, it remains\nunder-explored how to enhance the ability of LLMs to follow complex\ninstructions with multiple constraints. To bridge the gap, we initially study\nwhat training data is effective in enhancing complex constraints following\nabilities. We found that training LLMs with instructions containing multiple\nconstraints enhances their understanding of complex instructions, especially\nthose with lower complexity levels. The improvement can even generalize to\ncompositions of out-of-domain constraints. Additionally, we further propose\nmethods addressing how to obtain and utilize the effective training data.\nFinally, we conduct extensive experiments to prove the effectiveness of our\nmethods in terms of overall performance and training efficiency. We also\ndemonstrate that our methods improve models' ability to follow instructions\ngenerally and generalize effectively across out-of-domain, in-domain, and\nadversarial settings, while maintaining general capabilities.", "published": "2024-04-24 12:51:14", "link": "http://arxiv.org/abs/2404.15846v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Effective Unsupervised Constrained Text Generation based on Perturbed\n  Masking", "abstract": "Unsupervised constrained text generation aims to generate text under a given\nset of constraints without any supervised data. Current state-of-the-art\nmethods stochastically sample edit positions and actions, which may cause\nunnecessary search steps. In this paper, we propose PMCTG to improve\neffectiveness by searching for the best edit position and action in each step.\nSpecifically, PMCTG extends perturbed masking technique to effectively search\nfor the most incongruent token to edit. Then it introduces four multi-aspect\nscoring functions to select edit action to further reduce search difficulty.\nSince PMCTG does not require supervised data, it could be applied to different\ngeneration tasks. We show that under the unsupervised setting, PMCTG achieves\nnew state-of-the-art results in two representative tasks, namely\nkeywords-to-sentence generation and paraphrasing.", "published": "2024-04-24 13:42:26", "link": "http://arxiv.org/abs/2404.15877v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generalization Measures for Zero-Shot Cross-Lingual Transfer", "abstract": "A model's capacity to generalize its knowledge to interpret unseen inputs\nwith different characteristics is crucial to build robust and reliable machine\nlearning systems. Language model evaluation tasks lack information metrics\nabout model generalization and their applicability in a new setting is measured\nusing task and language-specific downstream performance, which is often lacking\nin many languages and tasks. In this paper, we explore a set of efficient and\nreliable measures that could aid in computing more information related to the\ngeneralization capability of language models in cross-lingual zero-shot\nsettings. In addition to traditional measures such as variance in parameters\nafter training and distance from initialization, we also measure the\neffectiveness of sharpness in loss landscape in capturing the success in\ncross-lingual transfer and propose a novel and stable algorithm to reliably\ncompute the sharpness of a model optimum that correlates to generalization.", "published": "2024-04-24 15:38:22", "link": "http://arxiv.org/abs/2404.15928v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The PRISM Alignment Dataset: What Participatory, Representative and\n  Individualised Human Feedback Reveals About the Subjective and Multicultural\n  Alignment of Large Language Models", "abstract": "Human feedback is central to the alignment of Large Language Models (LLMs).\nHowever, open questions remain about methods (how), domains (where), people\n(who) and objectives (to what end) of feedback processes. To navigate these\nquestions, we introduce PRISM, a dataset that maps the sociodemographics and\nstated preferences of 1,500 diverse participants from 75 countries, to their\ncontextual preferences and fine-grained feedback in 8,011 live conversations\nwith 21 LLMs. With PRISM, we contribute (i) wider geographic and demographic\nparticipation in feedback; (ii) census-representative samples for two countries\n(UK, US); and (iii) individualised ratings that link to detailed participant\nprofiles, permitting personalisation and attribution of sample artefacts. We\ntarget subjective and multicultural perspectives on value-laden and\ncontroversial issues, where we expect interpersonal and cross-cultural\ndisagreement. We use PRISM in three case studies to demonstrate the need for\ncareful consideration of which humans provide what alignment data.", "published": "2024-04-24 17:51:36", "link": "http://arxiv.org/abs/2404.16019v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating Adversarial Trigger Transfer in Large Language Models", "abstract": "Recent work has developed optimization procedures to find token sequences,\ncalled adversarial triggers, which can elicit unsafe responses from aligned\nlanguage models. These triggers are believed to be highly transferable, i.e., a\ntrigger optimized on one model can jailbreak other models. In this paper, we\nconcretely show that such adversarial triggers are not consistently\ntransferable. We extensively investigate trigger transfer amongst 13 open\nmodels and observe poor and inconsistent transfer. Our experiments further\nreveal a significant difference in robustness to adversarial triggers between\nmodels Aligned by Preference Optimization (APO) and models Aligned by\nFine-Tuning (AFT). We find that APO models are extremely hard to jailbreak even\nwhen the trigger is optimized directly on the model. On the other hand, while\nAFT models may appear safe on the surface, exhibiting refusals to a range of\nunsafe instructions, we show that they are highly susceptible to adversarial\ntriggers. Lastly, we observe that most triggers optimized on AFT models also\ngeneralize to new unsafe instructions from five diverse domains, further\nemphasizing their vulnerability. Overall, our work highlights the need for more\ncomprehensive safety evaluations for aligned language models.", "published": "2024-04-24 17:53:14", "link": "http://arxiv.org/abs/2404.16020v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Efficient Patient Recruitment for Clinical Trials: Application\n  of a Prompt-Based Learning Model", "abstract": "Objective: Clinical trials are essential for advancing pharmaceutical\ninterventions, but they face a bottleneck in selecting eligible participants.\nAlthough leveraging electronic health records (EHR) for recruitment has gained\npopularity, the complex nature of unstructured medical texts presents\nchallenges in efficiently identifying participants. Natural Language Processing\n(NLP) techniques have emerged as a solution with a recent focus on transformer\nmodels. In this study, we aimed to evaluate the performance of a prompt-based\nlarge language model for the cohort selection task from unstructured medical\nnotes collected in the EHR. Methods: To process the medical records, we\nselected the most related sentences of the records to the eligibility criteria\nneeded for the trial. The SNOMED CT concepts related to each eligibility\ncriterion were collected. Medical records were also annotated with MedCAT based\non the SNOMED CT ontology. Annotated sentences including concepts matched with\nthe criteria-relevant terms were extracted. A prompt-based large language model\n(Generative Pre-trained Transformer (GPT) in this study) was then used with the\nextracted sentences as the training set. To assess its effectiveness, we\nevaluated the model's performance using the dataset from the 2018 n2c2\nchallenge, which aimed to classify medical records of 311 patients based on 13\neligibility criteria through NLP techniques. Results: Our proposed model showed\nthe overall micro and macro F measures of 0.9061 and 0.8060 which were among\nthe highest scores achieved by the experiments performed with this dataset.\nConclusion: The application of a prompt-based large language model in this\nstudy to classify patients based on eligibility criteria received promising\nscores. Besides, we proposed a method of extractive summarization with the aid\nof SNOMED CT ontology that can be also applied to other medical texts.", "published": "2024-04-24 20:42:28", "link": "http://arxiv.org/abs/2404.16198v1", "categories": ["cs.CL", "I.7"], "primary_category": "cs.CL"}
{"title": "Computational analysis of the language of pain: a systematic review", "abstract": "Objectives: This study aims to systematically review the literature on the\ncomputational processing of the language of pain, or pain narratives, whether\ngenerated by patients or physicians, identifying current trends and challenges.\nMethods: Following the PRISMA guidelines, a comprehensive literature search was\nconducted to select relevant studies on the computational processing of the\nlanguage of pain and answer pre-defined research questions. Data extraction and\nsynthesis were performed to categorize selected studies according to their\nprimary purpose and outcome, patient and pain population, textual data,\ncomputational methodology, and outcome targets. Results: Physician-generated\nlanguage of pain, specifically from clinical notes, was the most used data.\nTasks included patient diagnosis and triaging, identification of pain mentions,\ntreatment response prediction, biomedical entity extraction, correlation of\nlinguistic features with clinical states, and lexico-semantic analysis of pain\nnarratives. Only one study included previous linguistic knowledge on pain\nutterances in their experimental setup. Most studies targeted their outcomes\nfor physicians, either directly as clinical tools or as indirect knowledge. The\nleast targeted stage of clinical pain care was self-management, in which\npatients are most involved. Affective and sociocultural dimensions were the\nleast studied domains. Only one study measured how physician performance on\nclinical tasks improved with the inclusion of the proposed algorithm.\nDiscussion: This review found that future research should focus on analyzing\npatient-generated language of pain, developing patient-centered resources for\nself-management and patient-empowerment, exploring affective and sociocultural\naspects of pain, and measuring improvements in physician performance when aided\nby the proposed tools.", "published": "2024-04-24 21:59:40", "link": "http://arxiv.org/abs/2404.16226v2", "categories": ["cs.CL", "I.2.7; J.3; A.1"], "primary_category": "cs.CL"}
{"title": "Semgrex and Ssurgeon, Searching and Manipulating Dependency Graphs", "abstract": "Searching dependency graphs and manipulating them can be a time consuming and\nchallenging task to get right. We document Semgrex, a system for searching\ndependency graphs, and introduce Ssurgeon, a system for manipulating the output\nof Semgrex. The compact language used by these systems allows for easy command\nline or API processing of dependencies. Additionally, integration with publicly\nreleased toolkits in Java and Python allows for searching text relations and\nattributes over natural text.", "published": "2024-04-24 23:39:56", "link": "http://arxiv.org/abs/2404.16250v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hybrid LLM/Rule-based Approaches to Business Insights Generation from\n  Structured Data", "abstract": "In the field of business data analysis, the ability to extract actionable\ninsights from vast and varied datasets is essential for informed\ndecision-making and maintaining a competitive edge. Traditional rule-based\nsystems, while reliable, often fall short when faced with the complexity and\ndynamism of modern business data. Conversely, Artificial Intelligence (AI)\nmodels, particularly Large Language Models (LLMs), offer significant potential\nin pattern recognition and predictive analytics but can lack the precision\nnecessary for specific business applications. This paper explores the efficacy\nof hybrid approaches that integrate the robustness of rule-based systems with\nthe adaptive power of LLMs in generating actionable business insights.", "published": "2024-04-24 02:42:24", "link": "http://arxiv.org/abs/2404.15604v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Promise and Challenges of Using LLMs to Accelerate the Screening\n  Process of Systematic Reviews", "abstract": "Systematic review (SR) is a popular research method in software engineering\n(SE). However, conducting an SR takes an average of 67 weeks. Thus, automating\nany step of the SR process could reduce the effort associated with SRs. Our\nobjective is to investigate if Large Language Models (LLMs) can accelerate\ntitle-abstract screening by simplifying abstracts for human screeners, and\nautomating title-abstract screening. We performed an experiment where humans\nscreened titles and abstracts for 20 papers with both original and simplified\nabstracts from a prior SR. The experiment with human screeners was reproduced\nwith GPT-3.5 and GPT-4 LLMs to perform the same screening tasks. We also\nstudied if different prompting techniques (Zero-shot (ZS), One-shot (OS),\nFew-shot (FS), and Few-shot with Chain-of-Thought (FS-CoT)) improve the\nscreening performance of LLMs. Lastly, we studied if redesigning the prompt\nused in the LLM reproduction of screening leads to improved performance. Text\nsimplification did not increase the screeners' screening performance, but\nreduced the time used in screening. Screeners' scientific literacy skills and\nresearcher status predict screening performance. Some LLM and prompt\ncombinations perform as well as human screeners in the screening tasks. Our\nresults indicate that the GPT-4 LLM is better than its predecessor, GPT-3.5.\nAdditionally, Few-shot and One-shot prompting outperforms Zero-shot prompting.\nUsing LLMs for text simplification in the screening process does not\nsignificantly improve human performance. Using LLMs to automate title-abstract\nscreening seems promising, but current LLMs are not significantly more accurate\nthan human screeners. To recommend the use of LLMs in the screening process of\nSRs, more research is needed. We recommend future SR studies publish\nreplication packages with screening data to enable more conclusive\nexperimenting with LLM screening.", "published": "2024-04-24 05:53:20", "link": "http://arxiv.org/abs/2404.15667v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Beyond Chain-of-Thought: A Survey of Chain-of-X Paradigms for LLMs", "abstract": "Chain-of-Thought (CoT) has been a widely adopted prompting method, eliciting\nimpressive reasoning abilities of Large Language Models (LLMs). Inspired by the\nsequential thought structure of CoT, a number of Chain-of-X (CoX) methods have\nbeen developed to address various challenges across diverse domains and tasks\ninvolving LLMs. In this paper, we provide a comprehensive survey of Chain-of-X\nmethods for LLMs in different contexts. Specifically, we categorize them by\ntaxonomies of nodes, i.e., the X in CoX, and application tasks. We also discuss\nthe findings and implications of existing CoX methods, as well as potential\nfuture directions. Our survey aims to serve as a detailed and up-to-date\nresource for researchers seeking to apply the idea of CoT to broader scenarios.", "published": "2024-04-24 06:12:00", "link": "http://arxiv.org/abs/2404.15676v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Neural Proto-Language Reconstruction", "abstract": "Proto-form reconstruction has been a painstaking process for linguists.\nRecently, computational models such as RNN and Transformers have been proposed\nto automate this process. We take three different approaches to improve upon\nprevious methods, including data augmentation to recover missing reflexes,\nadding a VAE structure to the Transformer model for proto-to-language\nprediction, and using a neural machine translation model for the reconstruction\ntask. We find that with the additional VAE structure, the Transformer model has\na better performance on the WikiHan dataset, and the data augmentation step\nstabilizes the training.", "published": "2024-04-24 06:56:46", "link": "http://arxiv.org/abs/2404.15690v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Let's Think Dot by Dot: Hidden Computation in Transformer Language\n  Models", "abstract": "Chain-of-thought responses from language models improve performance across\nmost benchmarks. However, it remains unclear to what extent these performance\ngains can be attributed to human-like task decomposition or simply the greater\ncomputation that additional tokens allow. We show that transformers can use\nmeaningless filler tokens (e.g., '......') in place of a chain of thought to\nsolve two hard algorithmic tasks they could not solve when responding without\nintermediate tokens. However, we find empirically that learning to use filler\ntokens is difficult and requires specific, dense supervision to converge. We\nalso provide a theoretical characterization of the class of problems where\nfiller tokens are useful in terms of the quantifier depth of a first-order\nformula. For problems satisfying this characterization, chain-of-thought tokens\nneed not provide information about the intermediate computational steps\ninvolved in multi-token computations. In summary, our results show that\nadditional tokens can provide computational benefits independent of token\nchoice. The fact that intermediate tokens can act as filler tokens raises\nconcerns about large language models engaging in unauditable, hidden\ncomputations that are increasingly detached from the observed chain-of-thought\ntokens.", "published": "2024-04-24 09:30:00", "link": "http://arxiv.org/abs/2404.15758v1", "categories": ["cs.CL", "cs.AI", "I.2.6"], "primary_category": "cs.CL"}
{"title": "BASS: Batched Attention-optimized Speculative Sampling", "abstract": "Speculative decoding has emerged as a powerful method to improve latency and\nthroughput in hosting large language models. However, most existing\nimplementations focus on generating a single sequence. Real-world generative AI\napplications often require multiple responses and how to perform speculative\ndecoding in a batched setting while preserving its latency benefits poses\nnon-trivial challenges. This paper describes a system of batched speculative\ndecoding that sets a new state of the art in multi-sequence generation latency\nand that demonstrates superior GPU utilization as well as quality of\ngenerations within a time budget. For example, for a 7.8B-size model on a\nsingle A100 GPU and with a batch size of 8, each sequence is generated at an\naverage speed of 5.8ms per token, the overall throughput being 1.1K tokens per\nsecond. These results represent state-of-the-art latency and a 2.15X speed-up\nover optimized regular decoding. Within a time budget that regular decoding\ndoes not finish, our system is able to generate sequences with HumanEval\nPass@First of 43% and Pass@All of 61%, far exceeding what's feasible with\nsingle-sequence speculative decoding. Our peak GPU utilization during decoding\nreaches as high as 15.8%, more than 3X the highest of that of regular decoding\nand around 10X of single-sequence speculative decoding.", "published": "2024-04-24 09:57:11", "link": "http://arxiv.org/abs/2404.15778v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Detecting Conceptual Abstraction in LLMs", "abstract": "We present a novel approach to detecting noun abstraction within a large\nlanguage model (LLM). Starting from a psychologically motivated set of noun\npairs in taxonomic relationships, we instantiate surface patterns indicating\nhypernymy and analyze the attention matrices produced by BERT. We compare the\nresults to two sets of counterfactuals and show that we can detect hypernymy in\nthe abstraction mechanism, which cannot solely be related to the distributional\nsimilarity of noun pairs. Our findings are a first step towards the\nexplainability of conceptual abstraction in LLMs.", "published": "2024-04-24 12:52:45", "link": "http://arxiv.org/abs/2404.15848v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Assessing The Potential Of Mid-Sized Language Models For Clinical QA", "abstract": "Large language models, such as GPT-4 and Med-PaLM, have shown impressive\nperformance on clinical tasks; however, they require access to compute, are\nclosed-source, and cannot be deployed on device. Mid-size models such as\nBioGPT-large, BioMedLM, LLaMA 2, and Mistral 7B avoid these drawbacks, but\ntheir capacity for clinical tasks has been understudied. To help assess their\npotential for clinical use and help researchers decide which model they should\nuse, we compare their performance on two clinical question-answering (QA)\ntasks: MedQA and consumer query answering. We find that Mistral 7B is the best\nperforming model, winning on all benchmarks and outperforming models trained\nspecifically for the biomedical domain. While Mistral 7B's MedQA score of 63.0%\napproaches the original Med-PaLM, and it often can produce plausible responses\nto consumer health queries, room for improvement still exists. This study\nprovides the first head-to-head assessment of open source mid-sized models on\nclinical tasks.", "published": "2024-04-24 14:32:34", "link": "http://arxiv.org/abs/2404.15894v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "KGValidator: A Framework for Automatic Validation of Knowledge Graph\n  Construction", "abstract": "This study explores the use of Large Language Models (LLMs) for automatic\nevaluation of knowledge graph (KG) completion models. Historically, validating\ninformation in KGs has been a challenging task, requiring large-scale human\nannotation at prohibitive cost. With the emergence of general-purpose\ngenerative AI and LLMs, it is now plausible that human-in-the-loop validation\ncould be replaced by a generative agent. We introduce a framework for\nconsistency and validation when using generative models to validate knowledge\ngraphs. Our framework is based upon recent open-source developments for\nstructural and semantic validation of LLM outputs, and upon flexible approaches\nto fact checking and verification, supported by the capacity to reference\nexternal knowledge sources of any kind. The design is easy to adapt and extend,\nand can be used to verify any kind of graph-structured data through a\ncombination of model-intrinsic knowledge, user-supplied context, and agents\ncapable of external knowledge retrieval.", "published": "2024-04-24 15:27:25", "link": "http://arxiv.org/abs/2404.15923v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Inside the echo chamber: Linguistic underpinnings of misinformation on\n  Twitter", "abstract": "Social media users drive the spread of misinformation online by sharing posts\nthat include erroneous information or commenting on controversial topics with\nunsubstantiated arguments often in earnest. Work on echo chambers has suggested\nthat users' perspectives are reinforced through repeated interactions with\nlike-minded peers, promoted by homophily and bias in information diffusion.\nBuilding on long-standing interest in the social bases of language and\nlinguistic underpinnings of social behavior, this work explores how\nconversations around misinformation are mediated through language use. We\ncompare a number of linguistic measures, e.g., in-/out-group cues, readability,\nand discourse connectives, within and across topics of conversation and user\ncommunities. Our findings reveal increased presence of group identity signals\nand processing fluency within echo chambers during discussions of\nmisinformation. We discuss the specific character of these broader trends\nacross topics and examine contextual influences.", "published": "2024-04-24 15:37:12", "link": "http://arxiv.org/abs/2404.15925v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Uncertainty Estimation and Quantification for LLMs: A Simple Supervised\n  Approach", "abstract": "In this paper, we study the problem of uncertainty estimation and calibration\nfor LLMs. We begin by formulating the uncertainty estimation problem, a\nrelevant yet underexplored area in existing literature. We then propose a\nsupervised approach that leverages labeled datasets to estimate the uncertainty\nin LLMs' responses. Based on the formulation, we illustrate the difference\nbetween the uncertainty estimation for LLMs and that for standard ML models and\nexplain why the hidden neurons of the LLMs may contain uncertainty information.\nOur designed approach demonstrates the benefits of utilizing hidden activations\nto enhance uncertainty estimation across various tasks and shows robust\ntransferability in out-of-distribution settings. We distinguish the uncertainty\nestimation task from the uncertainty calibration task and show that better\nuncertainty estimation leads to better calibration performance. Furthermore,\nour method is easy to implement and adaptable to different levels of model\naccessibility including black box, grey box, and white box.", "published": "2024-04-24 17:10:35", "link": "http://arxiv.org/abs/2404.15993v4", "categories": ["cs.LG", "cs.CL", "68T07, 68T50"], "primary_category": "cs.LG"}
{"title": "Cantor: Inspiring Multimodal Chain-of-Thought of MLLM", "abstract": "With the advent of large language models(LLMs) enhanced by the\nchain-of-thought(CoT) methodology, visual reasoning problem is usually\ndecomposed into manageable sub-tasks and tackled sequentially with various\nexternal tools. However, such a paradigm faces the challenge of the potential\n\"determining hallucinations\" in decision-making due to insufficient visual\ninformation and the limitation of low-level perception tools that fail to\nprovide abstract summaries necessary for comprehensive reasoning. We argue that\nconverging visual context acquisition and logical reasoning is pivotal for\ntackling visual reasoning tasks. This paper delves into the realm of multimodal\nCoT to solve intricate visual reasoning tasks with multimodal large language\nmodels(MLLMs) and their cognitive capability. To this end, we propose an\ninnovative multimodal CoT framework, termed Cantor, characterized by a\nperception-decision architecture. Cantor first acts as a decision generator and\nintegrates visual inputs to analyze the image and problem, ensuring a closer\nalignment with the actual context. Furthermore, Cantor leverages the advanced\ncognitive functions of MLLMs to perform as multifaceted experts for deriving\nhigher-level information, enhancing the CoT generation process. Our extensive\nexperiments demonstrate the efficacy of the proposed framework, showing\nsignificant improvements in multimodal CoT performance across two complex\nvisual reasoning datasets, without necessitating fine-tuning or ground-truth\nrationales. Project Page: https://ggg0919.github.io/cantor/ .", "published": "2024-04-24 17:59:48", "link": "http://arxiv.org/abs/2404.16033v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Classifying Human-Generated and AI-Generated Election Claims in Social\n  Media", "abstract": "Politics is one of the most prevalent topics discussed on social media\nplatforms, particularly during major election cycles, where users engage in\nconversations about candidates and electoral processes. Malicious actors may\nuse this opportunity to disseminate misinformation to undermine trust in the\nelectoral process. The emergence of Large Language Models (LLMs) exacerbates\nthis issue by enabling malicious actors to generate misinformation at an\nunprecedented scale. Artificial intelligence (AI)-generated content is often\nindistinguishable from authentic user content, raising concerns about the\nintegrity of information on social networks. In this paper, we present a novel\ntaxonomy for characterizing election-related claims. This taxonomy provides an\ninstrument for analyzing election-related claims, with granular categories\nrelated to jurisdiction, equipment, processes, and the nature of claims. We\nintroduce ElectAI, a novel benchmark dataset that consists of 9,900 tweets,\neach labeled as human- or AI-generated. For AI-generated tweets, the specific\nLLM variant that produced them is specified. We annotated a subset of 1,550\ntweets using the proposed taxonomy to capture the characteristics of\nelection-related claims. We explored the capabilities of LLMs in extracting the\ntaxonomy attributes and trained various machine learning models using ElectAI\nto distinguish between human- and AI-generated posts and identify the specific\nLLM variant.", "published": "2024-04-24 18:13:29", "link": "http://arxiv.org/abs/2404.16116v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Domain-Specific Improvement on Psychotherapy Chatbot Using Assistant", "abstract": "Large language models (LLMs) have demonstrated impressive generalization\ncapabilities on specific tasks with human-written instruction data. However,\nthe limited quantity, diversity, and professional expertise of such instruction\ndata raise concerns about the performance of LLMs in psychotherapy tasks when\nprovided with domain-specific instructions. To address this, we firstly propose\nDomain-Specific Assistant Instructions based on AlexanderStreet therapy, and\nsecondly, we use an adaption fine-tuning method and retrieval augmented\ngeneration method to improve pre-trained LLMs. Through quantitative evaluation\nof linguistic quality using automatic and human evaluation, we observe that\npre-trained LLMs on Psychotherapy Assistant Instructions outperform\nstate-of-the-art LLMs response baselines. Our Assistant-Instruction approach\noffers a half-annotation method to align pre-trained LLMs with instructions and\nprovide pre-trained LLMs with more psychotherapy knowledge.", "published": "2024-04-24 19:30:18", "link": "http://arxiv.org/abs/2404.16160v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Fusion of Domain-Adapted Vision and Language Models for Medical Visual\n  Question Answering", "abstract": "Vision-language models, while effective in general domains and showing strong\nperformance in diverse multi-modal applications like visual question-answering\n(VQA), struggle to maintain the same level of effectiveness in more specialized\ndomains, e.g., medical. We propose a medical vision-language model that\nintegrates large vision and language models adapted for the medical domain.\nThis model goes through three stages of parameter-efficient training using\nthree separate biomedical and radiology multi-modal visual and text datasets.\nThe proposed model achieves state-of-the-art performance on the SLAKE 1.0\nmedical VQA (MedVQA) dataset with an overall accuracy of 87.5% and demonstrates\nstrong performance on another MedVQA dataset, VQA-RAD, achieving an overall\naccuracy of 73.2%.", "published": "2024-04-24 20:31:15", "link": "http://arxiv.org/abs/2404.16192v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Knowledge Graph Completion using Structural and Textual Embeddings", "abstract": "Knowledge Graphs (KGs) are widely employed in artificial intelligence\napplications, such as question-answering and recommendation systems. However,\nKGs are frequently found to be incomplete. While much of the existing\nliterature focuses on predicting missing nodes for given incomplete KG triples,\nthere remains an opportunity to complete KGs by exploring relations between\nexisting nodes, a task known as relation prediction. In this study, we propose\na relations prediction model that harnesses both textual and structural\ninformation within KGs. Our approach integrates walks-based embeddings with\nlanguage model embeddings to effectively represent nodes. We demonstrate that\nour model achieves competitive results in the relation prediction task when\nevaluated on a widely used dataset.", "published": "2024-04-24 21:04:14", "link": "http://arxiv.org/abs/2404.16206v1", "categories": ["cs.AI", "cs.CL", "I.2.4"], "primary_category": "cs.AI"}
{"title": "URL: Universal Referential Knowledge Linking via Task-instructed\n  Representation Compression", "abstract": "Linking a claim to grounded references is a critical ability to fulfill human\ndemands for authentic and reliable information. Current studies are limited to\nspecific tasks like information retrieval or semantic matching, where the\nclaim-reference relationships are unique and fixed, while the referential\nknowledge linking (RKL) in real-world can be much more diverse and complex. In\nthis paper, we propose universal referential knowledge linking (URL), which\naims to resolve diversified referential knowledge linking tasks by one unified\nmodel. To this end, we propose a LLM-driven task-instructed representation\ncompression, as well as a multi-view learning approach, in order to effectively\nadapt the instruction following and semantic understanding abilities of LLMs to\nreferential knowledge linking. Furthermore, we also construct a new benchmark\nto evaluate ability of models on referential knowledge linking tasks across\ndifferent scenarios. Experiments demonstrate that universal RKL is challenging\nfor existing approaches, while the proposed framework can effectively resolve\nthe task across various scenarios, and therefore outperforms previous\napproaches by a large margin.", "published": "2024-04-24 23:37:15", "link": "http://arxiv.org/abs/2404.16248v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Homonym Sense Disambiguation in the Georgian Language", "abstract": "This research proposes a novel approach to the Word Sense Disambiguation\n(WSD) task in the Georgian language, based on supervised fine-tuning of a\npre-trained Large Language Model (LLM) on a dataset formed by filtering the\nGeorgian Common Crawls corpus. The dataset is used to train a classifier for\nwords with multiple senses. Additionally, we present experimental results of\nusing LSTM for WSD. Accurately disambiguating homonyms is crucial in natural\nlanguage processing. Georgian, an agglutinative language belonging to the\nKartvelian language family, presents unique challenges in this context. The aim\nof this paper is to highlight the specific problems concerning homonym\ndisambiguation in the Georgian language and to present our approach to solving\nthem. The techniques discussed in the article achieve 95% accuracy for\npredicting lexical meanings of homonyms using a hand-classified dataset of over\n7500 sentences.", "published": "2024-04-24 21:48:43", "link": "http://arxiv.org/abs/2405.00710v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CatLIP: CLIP-level Visual Recognition Accuracy with 2.7x Faster\n  Pre-training on Web-scale Image-Text Data", "abstract": "Contrastive learning has emerged as a transformative method for learning\neffective visual representations through the alignment of image and text\nembeddings. However, pairwise similarity computation in contrastive loss\nbetween image and text pairs poses computational challenges. This paper\npresents a novel weakly supervised pre-training of vision models on web-scale\nimage-text data. The proposed method reframes pre-training on image-text data\nas a classification task. Consequently, it eliminates the need for pairwise\nsimilarity computations in contrastive loss, achieving a remarkable $2.7\\times$\nacceleration in training speed compared to contrastive learning on web-scale\ndata. Through extensive experiments spanning diverse vision tasks, including\ndetection and segmentation, we demonstrate that the proposed method maintains\nhigh representation quality. Our source code along with pre-trained model\nweights and training recipes is available at\n\\url{https://github.com/apple/corenet}.", "published": "2024-04-24 05:13:28", "link": "http://arxiv.org/abs/2404.15653v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "ChEX: Interactive Localization and Region Description in Chest X-rays", "abstract": "Report generation models offer fine-grained textual interpretations of\nmedical images like chest X-rays, yet they often lack interactivity (i.e. the\nability to steer the generation process through user queries) and localized\ninterpretability (i.e. visually grounding their predictions), which we deem\nessential for future adoption in clinical practice. While there have been\nefforts to tackle these issues, they are either limited in their interactivity\nby not supporting textual queries or fail to also offer localized\ninterpretability. Therefore, we propose a novel multitask architecture and\ntraining paradigm integrating textual prompts and bounding boxes for diverse\naspects like anatomical regions and pathologies. We call this approach the\nChest X-Ray Explainer (ChEX). Evaluations across a heterogeneous set of 9 chest\nX-ray tasks, including localized image interpretation and report generation,\nshowcase its competitiveness with SOTA models while additional analysis\ndemonstrates ChEX's interactive capabilities. Code:\nhttps://github.com/philip-mueller/chex", "published": "2024-04-24 09:44:44", "link": "http://arxiv.org/abs/2404.15770v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "CORM: Cache Optimization with Recent Message for Large Language Model\n  Inference", "abstract": "Large Language Models (LLMs), despite their remarkable performance across a\nwide range of tasks, necessitate substantial GPU memory and consume significant\ncomputational resources. Beyond the memory taken up by model weights, the\nmemory used by the KV cache rises linearly with sequence length, becoming a\nprimary bottleneck for inference. In this paper, we introduce an innovative\nmethod for optimizing the KV cache, which considerably minimizes its memory\nfootprint. Upon thorough investigation, we discover that in most Transformer\nmodels, (i) there is a striking similarity between adjacent tokens' query\nvectors, and (ii) the attention calculation of the current query can rely\nexclusively on the attention information of a small fraction of preceding\nqueries. Based on these observations, we present CORM, a KV cache eviction\npolicy that dynamically retains essential key-value pairs for inference without\nthe need for model fine-tuning. Our validation shows that CORM reduces the\ninference memory usage of KV cache by up to 70\\% with negligible performance\ndegradation across six tasks in LongBench. Furthermore, we demonstrate that\nCORM is compatible with GQA for further compression rate.", "published": "2024-04-24 16:11:54", "link": "http://arxiv.org/abs/2404.15949v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MoDE: CLIP Data Experts via Clustering", "abstract": "The success of contrastive language-image pretraining (CLIP) relies on the\nsupervision from the pairing between images and captions, which tends to be\nnoisy in web-crawled data. We present Mixture of Data Experts (MoDE) and learn\na system of CLIP data experts via clustering. Each data expert is trained on\none data cluster, being less sensitive to false negative noises in other\nclusters. At inference time, we ensemble their outputs by applying weights\ndetermined through the correlation between task metadata and cluster\nconditions. To estimate the correlation precisely, the samples in one cluster\nshould be semantically similar, but the number of data experts should still be\nreasonable for training and inference. As such, we consider the ontology in\nhuman language and propose to use fine-grained cluster centers to represent\neach data expert at a coarse-grained level. Experimental studies show that four\nCLIP data experts on ViT-B/16 outperform the ViT-L/14 by OpenAI CLIP and\nOpenCLIP on zero-shot image classification but with less ($<$35\\%) training\ncost. Meanwhile, MoDE can train all data expert asynchronously and can flexibly\ninclude new data experts. The code is available at\nhttps://github.com/facebookresearch/MetaCLIP/tree/main/mode.", "published": "2024-04-24 17:59:24", "link": "http://arxiv.org/abs/2404.16030v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Semantic Evolvement Enhanced Graph Autoencoder for Rumor Detection", "abstract": "Due to the rapid spread of rumors on social media, rumor detection has become\nan extremely important challenge. Recently, numerous rumor detection models\nwhich utilize textual information and the propagation structure of events have\nbeen proposed. However, these methods overlook the importance of semantic\nevolvement information of event in propagation process, which is often\nchallenging to be truly learned in supervised training paradigms and\ntraditional rumor detection methods. To address this issue, we propose a novel\nsemantic evolvement enhanced Graph Autoencoder for Rumor Detection (GARD) model\nin this paper. The model learns semantic evolvement information of events by\ncapturing local semantic changes and global semantic evolvement information\nthrough specific graph autoencoder and reconstruction strategies. By combining\nsemantic evolvement information and propagation structure information, the\nmodel achieves a comprehensive understanding of event propagation and perform\naccurate and robust detection, while also detecting rumors earlier by capturing\nsemantic evolvement information in the early stages. Moreover, in order to\nenhance the model's ability to learn the distinct patterns of rumors and\nnon-rumors, we introduce a uniformity regularizer to further improve the\nmodel's performance. Experimental results on three public benchmark datasets\nconfirm the superiority of our GARD method over the state-of-the-art approaches\nin both overall performance and early rumor detection.", "published": "2024-04-24 05:05:58", "link": "http://arxiv.org/abs/2404.16076v1", "categories": ["cs.SI", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.SI"}
{"title": "Evolution of Voices in French Audiovisual Media Across Genders and Age\n  in a Diachronic Perspective", "abstract": "We present a diachronic acoustic analysis of the voice of 1023 speakers from\nFrench media archives. The speakers are spread across 32 categories based on\nfour periods (years 1955/56, 1975/76, 1995/96, 2015/16), four age groups\n(20-35; 36-50; 51-65, >65), and two genders. The fundamental frequency ($F_0$)\nand the first four formants (F1-4) were estimated. Procedures used to ensure\nthe quality of these estimations on heterogeneous data are described. From each\nspeaker's $F_0$ distribution, the base-$F_0$ value was calculated to estimate\nthe register. Average vocal tract length was estimated from formant\nfrequencies. Base-$F_0$ and vocal tract length were fit by linear mixed models\nto evaluate how they may have changed across time periods and genders,\ncorrected for age effects. Results show an effect of the period with a tendency\nto lower voices, independently of gender. A lowering of pitch is observed with\nage for female but not male speakers.", "published": "2024-04-24 18:00:06", "link": "http://arxiv.org/abs/2404.16104v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Online Personalizing White-box LLMs Generation with Neural Bandits", "abstract": "The advent of personalized content generation by LLMs presents a novel\nchallenge: how to efficiently adapt text to meet individual preferences without\nthe unsustainable demand of creating a unique model for each user. This study\nintroduces an innovative online method that employs neural bandit algorithms to\ndynamically optimize soft instruction embeddings based on user feedback,\nenhancing the personalization of open-ended text generation by white-box LLMs.\nThrough rigorous experimentation on various tasks, we demonstrate significant\nperformance improvements over baseline strategies. NeuralTS, in particular,\nleads to substantial enhancements in personalized news headline generation,\nachieving up to a 62.9% improvement in terms of best ROUGE scores and up to\n2.76% increase in LLM-agent evaluation against the baseline.", "published": "2024-04-24 18:13:12", "link": "http://arxiv.org/abs/2404.16115v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FairDeDup: Detecting and Mitigating Vision-Language Fairness Disparities\n  in Semantic Dataset Deduplication", "abstract": "Recent dataset deduplication techniques have demonstrated that content-aware\ndataset pruning can dramatically reduce the cost of training Vision-Language\nPretrained (VLP) models without significant performance losses compared to\ntraining on the original dataset. These results have been based on pruning\ncommonly used image-caption datasets collected from the web -- datasets that\nare known to harbor harmful social biases that may then be codified in trained\nmodels. In this work, we evaluate how deduplication affects the prevalence of\nthese biases in the resulting trained models and introduce an easy-to-implement\nmodification to the recent SemDeDup algorithm that can reduce the negative\neffects that we observe. When examining CLIP-style models trained on\ndeduplicated variants of LAION-400M, we find our proposed FairDeDup algorithm\nconsistently leads to improved fairness metrics over SemDeDup on the FairFace\nand FACET datasets while maintaining zero-shot performance on CLIP benchmarks.", "published": "2024-04-24 18:28:17", "link": "http://arxiv.org/abs/2404.16123v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "I.4.10; I.2.7; E.0"], "primary_category": "cs.CV"}
{"title": "From Local to Global: A Graph RAG Approach to Query-Focused\n  Summarization", "abstract": "The use of retrieval-augmented generation (RAG) to retrieve relevant\ninformation from an external knowledge source enables large language models\n(LLMs) to answer questions over private and/or previously unseen document\ncollections. However, RAG fails on global questions directed at an entire text\ncorpus, such as \"What are the main themes in the dataset?\", since this is\ninherently a query-focused summarization (QFS) task, rather than an explicit\nretrieval task. Prior QFS methods, meanwhile, do not scale to the quantities of\ntext indexed by typical RAG systems. To combine the strengths of these\ncontrasting methods, we propose GraphRAG, a graph-based approach to question\nanswering over private text corpora that scales with both the generality of\nuser questions and the quantity of source text. Our approach uses an LLM to\nbuild a graph index in two stages: first, to derive an entity knowledge graph\nfrom the source documents, then to pregenerate community summaries for all\ngroups of closely related entities. Given a question, each community summary is\nused to generate a partial response, before all partial responses are again\nsummarized in a final response to the user. For a class of global sensemaking\nquestions over datasets in the 1 million token range, we show that GraphRAG\nleads to substantial improvements over a conventional RAG baseline for both the\ncomprehensiveness and diversity of generated answers.", "published": "2024-04-24 18:38:11", "link": "http://arxiv.org/abs/2404.16130v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "H.3.3; I.2.7"], "primary_category": "cs.CL"}
{"title": "Towards a Holistic Evaluation of LLMs on Factual Knowledge Recall", "abstract": "Large language models (LLMs) have shown remarkable performance on a variety\nof NLP tasks, and are being rapidly adopted in a wide range of use cases. It is\ntherefore of vital importance to holistically evaluate the factuality of their\ngenerated outputs, as hallucinations remain a challenging issue.\n  In this work, we focus on assessing LLMs' ability to recall factual knowledge\nlearned from pretraining, and the factors that affect this ability. To that\nend, we construct FACT-BENCH, a representative benchmark covering 20 domains,\n134 property types, 3 answer types, and different knowledge popularity levels.\nWe benchmark 31 models from 10 model families and provide a holistic assessment\nof their strengths and weaknesses. We observe that instruction-tuning hurts\nknowledge recall, as pretraining-only models consistently outperform their\ninstruction-tuned counterparts, and positive effects of model scaling, as\nlarger models outperform smaller ones for all model families. However, the best\nperformance from GPT-4 still represents a large gap with the upper-bound. We\nadditionally study the role of in-context exemplars using counterfactual\ndemonstrations, which lead to significant degradation of factual knowledge\nrecall for large models. By further decoupling model known and unknown\nknowledge, we find the degradation is attributed to exemplars that contradict a\nmodel's known knowledge, as well as the number of such exemplars. Lastly, we\nfine-tune LLaMA-7B in different settings of known and unknown knowledge. In\nparticular, fine-tuning on a model's known knowledge is beneficial, and\nconsistently outperforms fine-tuning on unknown and mixed knowledge. We will\nmake our benchmark publicly available.", "published": "2024-04-24 19:40:01", "link": "http://arxiv.org/abs/2404.16164v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Prompt Leakage effect and defense strategies for multi-turn LLM\n  interactions", "abstract": "Prompt leakage poses a compelling security and privacy threat in LLM\napplications. Leakage of system prompts may compromise intellectual property,\nand act as adversarial reconnaissance for an attacker. A systematic evaluation\nof prompt leakage threats and mitigation strategies is lacking, especially for\nmulti-turn LLM interactions. In this paper, we systematically investigate LLM\nvulnerabilities against prompt leakage for 10 closed- and open-source LLMs,\nacross four domains. We design a unique threat model which leverages the LLM\nsycophancy effect and elevates the average attack success rate (ASR) from 17.7%\nto 86.2% in a multi-turn setting. Our standardized setup further allows\ndissecting leakage of specific prompt contents such as task instructions and\nknowledge documents. We measure the mitigation effect of 7 black-box defense\nstrategies, along with finetuning an open-source model to defend against\nleakage attempts. We present different combination of defenses against our\nthreat model, including a cost analysis. Our study highlights key takeaways for\nbuilding secure LLM applications and provides directions for research in\nmulti-turn LLM interactions", "published": "2024-04-24 23:39:58", "link": "http://arxiv.org/abs/2404.16251v3", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Attacks on Third-Party APIs of Large Language Models", "abstract": "Large language model (LLM) services have recently begun offering a plugin\necosystem to interact with third-party API services. This innovation enhances\nthe capabilities of LLMs, but it also introduces risks, as these plugins\ndeveloped by various third parties cannot be easily trusted. This paper\nproposes a new attacking framework to examine security and safety\nvulnerabilities within LLM platforms that incorporate third-party services.\nApplying our framework specifically to widely used LLMs, we identify real-world\nmalicious attacks across various domains on third-party APIs that can\nimperceptibly modify LLM outputs. The paper discusses the unique challenges\nposed by third-party API integration and offers strategic possibilities to\nimprove the security and safety of LLM ecosystems moving forward. Our code is\nreleased at https://github.com/vk0812/Third-Party-Attacks-on-LLMs.", "published": "2024-04-24 19:27:02", "link": "http://arxiv.org/abs/2404.16891v1", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.CR"}
{"title": "Lessons from the Use of Natural Language Inference (NLI) in Requirements\n  Engineering Tasks", "abstract": "We investigate the use of Natural Language Inference (NLI) in automating\nrequirements engineering tasks. In particular, we focus on three tasks:\nrequirements classification, identification of requirements specification\ndefects, and detection of conflicts in stakeholders' requirements. While\nprevious research has demonstrated significant benefit in using NLI as a\nuniversal method for a broad spectrum of natural language processing tasks,\nthese advantages have not been investigated within the context of software\nrequirements engineering. Therefore, we design experiments to evaluate the use\nof NLI in requirements analysis. We compare the performance of NLI with a\nspectrum of approaches, including prompt-based models, conventional transfer\nlearning, Large Language Models (LLMs)-powered chatbot models, and\nprobabilistic models. Through experiments conducted under various learning\nsettings including conventional learning and zero-shot, we demonstrate\nconclusively that our NLI method surpasses classical NLP methods as well as\nother LLMs-based and chatbot models in the analysis of requirements\nspecifications. Additionally, we share lessons learned characterizing the\nlearning settings that make NLI a suitable approach for automating requirements\nengineering tasks.", "published": "2024-04-24 20:26:48", "link": "http://arxiv.org/abs/2405.05135v1", "categories": ["cs.SE", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "Integrating LSTM and BERT for Long-Sequence Data Analysis in Intelligent\n  Tutoring Systems", "abstract": "The field of Knowledge Tracing aims to understand how students learn and\nmaster knowledge over time by analyzing their historical behaviour data. To\nachieve this goal, many researchers have proposed Knowledge Tracing models that\nuse data from Intelligent Tutoring Systems to predict students' subsequent\nactions. However, with the development of Intelligent Tutoring Systems,\nlarge-scale datasets containing long-sequence data began to emerge. Recent deep\nlearning based Knowledge Tracing models face obstacles such as low efficiency,\nlow accuracy, and low interpretability when dealing with large-scale datasets\ncontaining long-sequence data. To address these issues and promote the\nsustainable development of Intelligent Tutoring Systems, we propose a LSTM\nBERT-based Knowledge Tracing model for long sequence data processing, namely\nLBKT, which uses a BERT-based architecture with a Rasch model-based embeddings\nblock to deal with different difficulty levels information and an LSTM block to\nprocess the sequential characteristic in students' actions. LBKT achieves the\nbest performance on most benchmark datasets on the metrics of ACC and AUC.\nAdditionally, an ablation study is conducted to analyse the impact of each\ncomponent of LBKT's overall performance. Moreover, we used t-SNE as the\nvisualisation tool to demonstrate the model's embedding strategy. The results\nindicate that LBKT is faster, more interpretable, and has a lower memory cost\nthan the traditional deep learning based Knowledge Tracing methods.", "published": "2024-04-24 18:19:44", "link": "http://arxiv.org/abs/2405.05136v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "BERT vs GPT for financial engineering", "abstract": "The paper benchmarks several Transformer models [4], to show how these models\ncan judge sentiment from a news event. This signal can then be used for\ndownstream modelling and signal identification for commodity trading. We find\nthat fine-tuned BERT models outperform fine-tuned or vanilla GPT models on this\ntask. Transformer models have revolutionized the field of natural language\nprocessing (NLP) in recent years, achieving state-of-the-art results on various\ntasks such as machine translation, text summarization, question answering, and\nnatural language generation. Among the most prominent transformer models are\nBidirectional Encoder Representations from Transformers (BERT) and Generative\nPre-trained Transformer (GPT), which differ in their architectures and\nobjectives.\n  A CopBERT model training data and process overview is provided. The CopBERT\nmodel outperforms similar domain specific BERT trained models such as FinBERT.\nThe below confusion matrices show the performance on CopBERT & CopGPT\nrespectively. We see a ~10 percent increase in f1_score when compare CopBERT vs\nGPT4 and 16 percent increase vs CopGPT. Whilst GPT4 is dominant It highlights\nthe importance of considering alternatives to GPT models for financial\nengineering tasks, given risks of hallucinations, and challenges with\ninterpretability. We unsurprisingly see the larger LLMs outperform the BERT\nmodels, with predictive power. In summary BERT is partially the new XGboost,\nwhat it lacks in predictive power it provides with higher levels of\ninterpretability. Concluding that BERT models might not be the next XGboost\n[2], but represent an interesting alternative for financial engineering tasks,\nthat require a blend of interpretability and accuracy.", "published": "2024-04-24 11:30:04", "link": "http://arxiv.org/abs/2405.12990v1", "categories": ["q-fin.ST", "cs.AI", "cs.CL"], "primary_category": "q-fin.ST"}
{"title": "Gated Low-rank Adaptation for personalized Code-Switching Automatic\n  Speech Recognition on the low-spec devices", "abstract": "In recent times, there has been a growing interest in utilizing personalized\nlarge models on low-spec devices, such as mobile and CPU-only devices. However,\nutilizing a personalized large model in the on-device is inefficient, and\nsometimes limited due to computational cost. To tackle the problem, this paper\npresents the weights separation method to minimize on-device model weights\nusing parameter-efficient fine-tuning methods. Moreover, some people speak\nmultiple languages in an utterance, as known as code-switching, the\npersonalized ASR model is necessary to address such cases. However, current\nmultilingual speech recognition models are limited to recognizing a single\nlanguage within each utterance. To tackle this problem, we propose\ncode-switching speech recognition models that incorporate fine-tuned\nmonolingual and multilingual speech recognition models. Additionally, we\nintroduce a gated low-rank adaptation(GLoRA) for parameter-efficient\nfine-tuning with minimal performance degradation. Our experiments, conducted on\nKorean-English code-switching datasets, demonstrate that fine-tuning speech\nrecognition models for code-switching surpasses the performance of traditional\ncode-switching speech recognition models trained from scratch. Furthermore,\nGLoRA enhances parameter-efficient fine-tuning performance compared to\nconventional LoRA.", "published": "2024-04-24 01:31:39", "link": "http://arxiv.org/abs/2406.02562v1", "categories": ["eess.AS", "cs.AI", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Using Artificial Intelligence to Unlock Crowdfunding Success for Small\n  Businesses", "abstract": "While small businesses are increasingly turning to online crowdfunding\nplatforms for essential funding, over 40% of these campaigns may fail to raise\nany money, especially those from low socio-economic areas. We utilize the\nlatest advancements in AI technology to identify crucial factors that influence\nthe success of crowdfunding campaigns and to improve their fundraising outcomes\nby strategically optimizing these factors. Our best-performing machine learning\nmodel accurately predicts the fundraising outcomes of 81.0% of campaigns,\nprimarily based on their textual descriptions. Interpreting the machine\nlearning model allows us to provide actionable suggestions on improving the\ntextual description before launching a campaign. We demonstrate that by\naugmenting just three aspects of the narrative using a large language model, a\ncampaign becomes more preferable to 83% human evaluators, and its likelihood of\nsecuring financial support increases by 11.9%. Our research uncovers the\neffective strategies for crafting descriptions for small business fundraising\ncampaigns and opens up a new realm in integrating large language models into\ncrowdfunding methodologies.", "published": "2024-04-24 20:53:10", "link": "http://arxiv.org/abs/2407.09480v1", "categories": ["econ.GN", "cs.AI", "cs.CL", "q-fin.EC"], "primary_category": "econ.GN"}
{"title": "ImplicitAVE: An Open-Source Dataset and Multimodal LLMs Benchmark for\n  Implicit Attribute Value Extraction", "abstract": "Existing datasets for attribute value extraction (AVE) predominantly focus on\nexplicit attribute values while neglecting the implicit ones, lack product\nimages, are often not publicly available, and lack an in-depth human inspection\nacross diverse domains. To address these limitations, we present ImplicitAVE,\nthe first, publicly available multimodal dataset for implicit attribute value\nextraction. ImplicitAVE, sourced from the MAVE dataset, is carefully curated\nand expanded to include implicit AVE and multimodality, resulting in a refined\ndataset of 68k training and 1.6k testing data across five domains. We also\nexplore the application of multimodal large language models (MLLMs) to implicit\nAVE, establishing a comprehensive benchmark for MLLMs on the ImplicitAVE\ndataset. Six recent MLLMs with eleven variants are evaluated across diverse\nsettings, revealing that implicit value extraction remains a challenging task\nfor MLLMs. The contributions of this work include the development and release\nof ImplicitAVE, and the exploration and benchmarking of various MLLMs for\nimplicit AVE, providing valuable insights and potential future research\ndirections. Dataset and code are available at\nhttps://github.com/HenryPengZou/ImplicitAVE", "published": "2024-04-24 01:54:40", "link": "http://arxiv.org/abs/2404.15592v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CV"}
{"title": "HybridVC: Efficient Voice Style Conversion with Text and Audio Prompts", "abstract": "We introduce HybridVC, a voice conversion (VC) framework built upon a\npre-trained conditional variational autoencoder (CVAE) that combines the\nstrengths of a latent model with contrastive learning. HybridVC supports text\nand audio prompts, enabling more flexible voice style conversion. HybridVC\nmodels a latent distribution conditioned on speaker embeddings acquired by a\npretrained speaker encoder and optimises style text embeddings to align with\nthe speaker style information through contrastive learning in parallel.\nTherefore, HybridVC can be efficiently trained under limited computational\nresources. Our experiments demonstrate HybridVC's superior training efficiency\nand its capability for advanced multi-modal voice style conversion. This\nunderscores its potential for widespread applications such as user-defined\npersonalised voice in various social media platforms. A comprehensive ablation\nstudy further validates the effectiveness of our method.", "published": "2024-04-24 04:18:31", "link": "http://arxiv.org/abs/2404.15637v2", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Efficient Multi-Model Fusion with Adversarial Complementary\n  Representation Learning", "abstract": "Single-model systems often suffer from deficiencies in tasks such as speaker\nverification (SV) and image classification, relying heavily on partial prior\nknowledge during decision-making, resulting in suboptimal performance. Although\nmulti-model fusion (MMF) can mitigate some of these issues, redundancy in\nlearned representations may limits improvements. To this end, we propose an\nadversarial complementary representation learning (ACoRL) framework that\nenables newly trained models to avoid previously acquired knowledge, allowing\neach individual component model to learn maximally distinct, complementary\nrepresentations. We make three detailed explanations of why this works and\nexperimental results demonstrate that our method more efficiently improves\nperformance compared to traditional MMF. Furthermore, attribution analysis\nvalidates the model trained under ACoRL acquires more complementary knowledge,\nhighlighting the efficacy of our approach in enhancing efficiency and\nrobustness across tasks.", "published": "2024-04-24 07:47:55", "link": "http://arxiv.org/abs/2404.15704v1", "categories": ["cs.LG", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "CLAD: Robust Audio Deepfake Detection Against Manipulation Attacks with\n  Contrastive Learning", "abstract": "The increasing prevalence of audio deepfakes poses significant security\nthreats, necessitating robust detection methods. While existing detection\nsystems exhibit promise, their robustness against malicious audio manipulations\nremains underexplored. To bridge the gap, we undertake the first comprehensive\nstudy of the susceptibility of the most widely adopted audio deepfake detectors\nto manipulation attacks. Surprisingly, even manipulations like volume control\ncan significantly bypass detection without affecting human perception. To\naddress this, we propose CLAD (Contrastive Learning-based Audio deepfake\nDetector) to enhance the robustness against manipulation attacks. The key idea\nis to incorporate contrastive learning to minimize the variations introduced by\nmanipulations, therefore enhancing detection robustness. Additionally, we\nincorporate a length loss, aiming to improve the detection accuracy by\nclustering real audios more closely in the feature space. We comprehensively\nevaluated the most widely adopted audio deepfake detection models and our\nproposed CLAD against various manipulation attacks. The detection models\nexhibited vulnerabilities, with FAR rising to 36.69%, 31.23%, and 51.28% under\nvolume control, fading, and noise injection, respectively. CLAD enhanced\nrobustness, reducing the FAR to 0.81% under noise injection and consistently\nmaintaining an FAR below 1.63% across all tests. Our source code and\ndocumentation are available in the artifact repository\n(https://github.com/CLAD23/CLAD).", "published": "2024-04-24 13:10:35", "link": "http://arxiv.org/abs/2404.15854v1", "categories": ["cs.CR", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
{"title": "ActiveRIR: Active Audio-Visual Exploration for Acoustic Environment\n  Modeling", "abstract": "An environment acoustic model represents how sound is transformed by the\nphysical characteristics of an indoor environment, for any given\nsource/receiver location. Traditional methods for constructing acoustic models\ninvolve expensive and time-consuming collection of large quantities of acoustic\ndata at dense spatial locations in the space, or rely on privileged knowledge\nof scene geometry to intelligently select acoustic data sampling locations. We\npropose active acoustic sampling, a new task for efficiently building an\nenvironment acoustic model of an unmapped environment in which a mobile agent\nequipped with visual and acoustic sensors jointly constructs the environment\nacoustic model and the occupancy map on-the-fly. We introduce ActiveRIR, a\nreinforcement learning (RL) policy that leverages information from audio-visual\nsensor streams to guide agent navigation and determine optimal acoustic data\nsampling positions, yielding a high quality acoustic model of the environment\nfrom a minimal set of acoustic samples. We train our policy with a novel RL\nreward based on information gain in the environment acoustic model. Evaluating\non diverse unseen indoor environments from a state-of-the-art acoustic\nsimulation platform, ActiveRIR outperforms an array of methods--both\ntraditional navigation agents based on spatial novelty and visual exploration\nas well as existing state-of-the-art methods.", "published": "2024-04-24 21:30:01", "link": "http://arxiv.org/abs/2404.16216v1", "categories": ["cs.CV", "cs.RO", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
