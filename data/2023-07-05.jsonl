{"title": "Open-Domain Hierarchical Event Schema Induction by Incremental Prompting\n  and Verification", "abstract": "Event schemas are a form of world knowledge about the typical progression of\nevents. Recent methods for event schema induction use information extraction\nsystems to construct a large number of event graph instances from documents,\nand then learn to generalize the schema from such instances. In contrast, we\npropose to treat event schemas as a form of commonsense knowledge that can be\nderived from large language models (LLMs). This new paradigm greatly simplifies\nthe schema induction process and allows us to handle both hierarchical\nrelations and temporal relations between events in a straightforward way. Since\nevent schemas have complex graph structures, we design an incremental prompting\nand verification method to break down the construction of a complex event graph\ninto three stages: event skeleton construction, event expansion, and\nevent-event relation verification. Compared to directly using LLMs to generate\na linearized graph, our method can generate large and complex schemas with 7.2%\nF1 improvement in temporal relations and 31.0% F1 improvement in hierarchical\nrelations. In addition, compared to the previous state-of-the-art closed-domain\nschema induction model, human assessors were able to cover $\\sim$10% more\nevents when translating the schemas into coherent stories and rated our schemas\n1.3 points higher (on a 5-point scale) in terms of readability.", "published": "2023-07-05 01:00:44", "link": "http://arxiv.org/abs/2307.01972v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PULSAR at MEDIQA-Sum 2023: Large Language Models Augmented by Synthetic\n  Dialogue Convert Patient Dialogues to Medical Records", "abstract": "This paper describes PULSAR, our system submission at the ImageClef 2023\nMediQA-Sum task on summarising patient-doctor dialogues into clinical records.\nThe proposed framework relies on domain-specific pre-training, to produce a\nspecialised language model which is trained on task-specific natural data\naugmented by synthetic data generated by a black-box LLM. We find limited\nevidence towards the efficacy of domain-specific pre-training and data\naugmentation, while scaling up the language model yields the best performance\ngains. Our approach was ranked second and third among 13 submissions on task B\nof the challenge. Our code is available at https://github.com/yuping-wu/PULSAR.", "published": "2023-07-05 03:31:12", "link": "http://arxiv.org/abs/2307.02006v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using Data Augmentations and VTLN to Reduce Bias in Dutch End-to-End\n  Speech Recognition Systems", "abstract": "Speech technology has improved greatly for norm speakers, i.e., adult native\nspeakers of a language without speech impediments or strong accents. However,\nnon-norm or diverse speaker groups show a distinct performance gap with norm\nspeakers, which we refer to as bias. In this work, we aim to reduce bias\nagainst different age groups and non-native speakers of Dutch. For an\nend-to-end (E2E) ASR system, we use state-of-the-art speed perturbation and\nspectral augmentation as data augmentation techniques and explore Vocal Tract\nLength Normalization (VTLN) to normalise for spectral differences due to\ndifferences in anatomy. The combination of data augmentation and VTLN reduced\nthe average WER and bias across various diverse speaker groups by 6.9% and\n3.9%, respectively. The VTLN model trained on Dutch was also effective in\nimproving performance of Mandarin Chinese child speech, thus, showing\ngeneralisability across languages", "published": "2023-07-05 03:39:40", "link": "http://arxiv.org/abs/2307.02009v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CAME: Confidence-guided Adaptive Memory Efficient Optimization", "abstract": "Adaptive gradient methods, such as Adam and LAMB, have demonstrated excellent\nperformance in the training of large language models. Nevertheless, the need\nfor adaptivity requires maintaining second-moment estimates of the\nper-parameter gradients, which entails a high cost of extra memory overheads.\nTo solve this problem, several memory-efficient optimizers (e.g., Adafactor)\nhave been proposed to obtain a drastic reduction in auxiliary memory usage, but\nwith a performance penalty. In this paper, we first study a confidence-guided\nstrategy to reduce the instability of existing memory efficient optimizers.\nBased on this strategy, we propose CAME to simultaneously achieve two goals:\nfast convergence as in traditional adaptive methods, and low memory usage as in\nmemory-efficient methods. Extensive experiments demonstrate the training\nstability and superior performance of CAME across various NLP tasks such as\nBERT and GPT-2 training. Notably, for BERT pre-training on the large batch size\nof 32,768, our proposed optimizer attains faster convergence and higher\naccuracy compared with the Adam optimizer. The implementation of CAME is\npublicly available.", "published": "2023-07-05 06:05:36", "link": "http://arxiv.org/abs/2307.02047v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Flacuna: Unleashing the Problem Solving Power of Vicuna using FLAN\n  Fine-Tuning", "abstract": "Recently, the release of INSTRUCTEVAL has provided valuable insights into the\nperformance of large language models (LLMs) that utilize encoder-decoder or\ndecoder-only architecture. Interestingly, despite being introduced four years\nago, T5-based LLMs, such as FLAN-T5, continue to outperform the latest\ndecoder-based LLMs, such as LLAMA and VICUNA, on tasks that require general\nproblem-solving skills. This performance discrepancy can be attributed to three\nkey factors: (1) Pre-training data, (2) Backbone architecture, and (3)\nInstruction dataset. In this technical report, our main focus is on\ninvestigating the impact of the third factor by leveraging VICUNA, a large\nlanguage model based on LLAMA, which has undergone fine-tuning on ChatGPT\nconversations. To achieve this objective, we fine-tuned VICUNA using a\ncustomized instruction dataset collection called FLANMINI. This collection\nincludes a subset of the large-scale instruction dataset known as FLAN, as well\nas various code-related datasets and conversational datasets derived from\nChatGPT/GPT-4. This dataset comprises a large number of tasks that demand\nproblem-solving skills. Our experimental findings strongly indicate that the\nenhanced problem-solving abilities of our model, FLACUNA, are obtained through\nfine-tuning VICUNA on the FLAN dataset, leading to significant improvements\nacross numerous benchmark datasets in INSTRUCTEVAL. FLACUNA is publicly\navailable at https://huggingface.co/declare-lab/flacuna-13b-v1.0.", "published": "2023-07-05 06:36:54", "link": "http://arxiv.org/abs/2307.02053v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Graph Contrastive Topic Model", "abstract": "Existing NTMs with contrastive learning suffer from the sample bias problem\nowing to the word frequency-based sampling strategy, which may result in false\nnegative samples with similar semantics to the prototypes. In this paper, we\naim to explore the efficient sampling strategy and contrastive learning in NTMs\nto address the aforementioned issue. We propose a new sampling assumption that\nnegative samples should contain words that are semantically irrelevant to the\nprototype. Based on it, we propose the graph contrastive topic model (GCTM),\nwhich conducts graph contrastive learning (GCL) using informative positive and\nnegative samples that are generated by the graph-based sampling strategy\nleveraging in-depth correlation and irrelevance among documents and words. In\nGCTM, we first model the input document as the document word bipartite graph\n(DWBG), and construct positive and negative word co-occurrence graphs (WCGs),\nencoded by graph neural networks, to express in-depth semantic correlation and\nirrelevance among words. Based on the DWBG and WCGs, we design the\ndocument-word information propagation (DWIP) process to perform the edge\nperturbation of DWBG, based on multi-hop correlations/irrelevance among\ndocuments and words. This yields the desired negative and positive samples,\nwhich will be utilized for GCL together with the prototypes to improve learning\ndocument topic representations and latent topics. We further show that GCL can\nbe interpreted as the structured variational graph auto-encoder which maximizes\nthe mutual information of latent topic representations of different\nperspectives on DWBG. Experiments on several benchmark datasets demonstrate the\neffectiveness of our method for topic coherence and document representation\nlearning compared with existing SOTA methods.", "published": "2023-07-05 07:39:47", "link": "http://arxiv.org/abs/2307.02078v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Different Games in Dialogue: Combining character and conversational\n  types in strategic choice", "abstract": "In this paper, we show that investigating the interaction of conversational\ntype (often known as language game or speech genre) with the character types of\nthe interlocutors is worthwhile. We present a method of calculating the\ndecision making process for selecting dialogue moves that combines character\ntype and conversational type. We also present a mathematical model that\nillustrate these factors' interactions in a quantitative way.", "published": "2023-07-05 07:51:47", "link": "http://arxiv.org/abs/2307.02087v1", "categories": ["cs.CL", "68T50", "I.2.4; I.2.7"], "primary_category": "cs.CL"}
{"title": "Do predictability factors towards signing avatars hold across cultures?", "abstract": "Avatar technology can offer accessibility possibilities and improve the\nDeaf-and-Hard of Hearing sign language users access to communication, education\nand services, such as the healthcare system. However, sign language users\nacceptance of signing avatars as well as their attitudes towards them vary and\ndepend on many factors. Furthermore, research on avatar technology is mostly\ndone by researchers who are not Deaf. The study examines the extent to which\nintrinsic or extrinsic factors contribute to predict the attitude towards\navatars across cultures. Intrinsic factors include the characteristics of the\navatar, such as appearance, movements and facial expressions. Extrinsic factors\ninclude users technology experience, their hearing status, age and their sign\nlanguage fluency. This work attempts to answer questions such as, if lower\nattitude ratings are related to poor technology experience with ASL users, for\nexample, is that also true for Moroccan Sign Language (MSL) users? For the\npurposes of the study, we designed a questionnaire to understand MSL users\nattitude towards avatars. Three groups of participants were surveyed: Deaf\n(57), Hearing (20) and Hard-of-Hearing (3). The results of our study were then\ncompared with those reported in other relevant studies.", "published": "2023-07-05 08:22:46", "link": "http://arxiv.org/abs/2307.02103v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Controllable Transformer-Based Lexical Simplification", "abstract": "Text is by far the most ubiquitous source of knowledge and information and\nshould be made easily accessible to as many people as possible; however, texts\noften contain complex words that hinder reading comprehension and\naccessibility. Therefore, suggesting simpler alternatives for complex words\nwithout compromising meaning would help convey the information to a broader\naudience. This paper proposes mTLS, a multilingual controllable\nTransformer-based Lexical Simplification (LS) system fined-tuned with the T5\nmodel. The novelty of this work lies in the use of language-specific prefixes,\ncontrol tokens, and candidates extracted from pre-trained masked language\nmodels to learn simpler alternatives for complex words. The evaluation results\non three well-known LS datasets -- LexMTurk, BenchLS, and NNSEval -- show that\nour model outperforms the previous state-of-the-art models like LSBert and\nConLS. Moreover, further evaluation of our approach on the part of the recent\nTSAR-2022 multilingual LS shared-task dataset shows that our model performs\ncompetitively when compared with the participating systems for English LS and\neven outperforms the GPT-3 model on several metrics. Moreover, our model\nobtains performance gains also for Spanish and Portuguese.", "published": "2023-07-05 08:48:19", "link": "http://arxiv.org/abs/2307.02120v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Denoised Abstract Meaning Representation for Grammatical\n  Error Correction", "abstract": "Grammatical Error Correction (GEC) is the task of correcting errorful\nsentences into grammatically correct, semantically consistent, and coherent\nsentences. Popular GEC models either use large-scale synthetic corpora or use a\nlarge number of human-designed rules. The former is costly to train, while the\nlatter requires quite a lot of human expertise. In recent years, AMR, a\nsemantic representation framework, has been widely used by many natural\nlanguage tasks due to its completeness and flexibility. A non-negligible\nconcern is that AMRs of grammatically incorrect sentences may not be exactly\nreliable. In this paper, we propose the AMR-GEC, a seq-to-seq model that\nincorporates denoised AMR as additional knowledge. Specifically, We design a\nsemantic aggregated GEC model and explore denoising methods to get AMRs more\nreliable. Experiments on the BEA-2019 shared task and the CoNLL-2014 shared\ntask have shown that AMR-GEC performs comparably to a set of strong baselines\nwith a large number of synthetic data. Compared with the T5 model with\nsynthetic data, AMR-GEC can reduce the training time by 32\\% while inference\ntime is comparable. To the best of our knowledge, we are the first to\nincorporate AMR for grammatical error correction.", "published": "2023-07-05 09:06:56", "link": "http://arxiv.org/abs/2307.02127v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Open-Source LLMs for Text Annotation: A Practical Guide for Model\n  Setting and Fine-Tuning", "abstract": "This paper studies the performance of open-source Large Language Models\n(LLMs) in text classification tasks typical for political science research. By\nexamining tasks like stance, topic, and relevance classification, we aim to\nguide scholars in making informed decisions about their use of LLMs for text\nanalysis. Specifically, we conduct an assessment of both zero-shot and\nfine-tuned LLMs across a range of text annotation tasks using news articles and\ntweets datasets. Our analysis shows that fine-tuning improves the performance\nof open-source LLMs, allowing them to match or even surpass zero-shot GPT-3.5\nand GPT-4, though still lagging behind fine-tuned GPT-3.5. We further establish\nthat fine-tuning is preferable to few-shot training with a relatively modest\nquantity of annotated text. Our findings show that fine-tuned open-source LLMs\ncan be effectively deployed in a broad spectrum of text annotation\napplications. We provide a Python notebook facilitating the application of LLMs\nin text annotation for other researchers.", "published": "2023-07-05 10:15:07", "link": "http://arxiv.org/abs/2307.02179v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SpaceNLI: Evaluating the Consistency of Predicting Inferences in Space", "abstract": "While many natural language inference (NLI) datasets target certain semantic\nphenomena, e.g., negation, tense & aspect, monotonicity, and presupposition, to\nthe best of our knowledge, there is no NLI dataset that involves diverse types\nof spatial expressions and reasoning. We fill this gap by semi-automatically\ncreating an NLI dataset for spatial reasoning, called SpaceNLI. The data\nsamples are automatically generated from a curated set of reasoning patterns,\nwhere the patterns are annotated with inference labels by experts. We test\nseveral SOTA NLI systems on SpaceNLI to gauge the complexity of the dataset and\nthe system's capacity for spatial reasoning. Moreover, we introduce a Pattern\nAccuracy and argue that it is a more reliable and stricter measure than the\naccuracy for evaluating a system's performance on pattern-based generated data\nsamples. Based on the evaluation results we find that the systems obtain\nmoderate results on the spatial NLI problems but lack consistency per inference\npattern. The results also reveal that non-projective spatial inferences\n(especially due to the \"between\" preposition) are the most challenging ones.", "published": "2023-07-05 13:08:18", "link": "http://arxiv.org/abs/2307.02269v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Utilizing ChatGPT Generated Data to Retrieve Depression Symptoms from\n  Social Media", "abstract": "In this work, we present the contribution of the BLUE team in the eRisk Lab\ntask on searching for symptoms of depression. The task consists of retrieving\nand ranking Reddit social media sentences that convey symptoms of depression\nfrom the BDI-II questionnaire. Given that synthetic data provided by LLMs have\nbeen proven to be a reliable method for augmenting data and fine-tuning\ndownstream models, we chose to generate synthetic data using ChatGPT for each\nof the symptoms of the BDI-II questionnaire. We designed a prompt such that the\ngenerated data contains more richness and semantic diversity than the BDI-II\nresponses for each question and, at the same time, contains emotional and\nanecdotal experiences that are specific to the more intimate way of sharing\nexperiences on Reddit. We perform semantic search and rank the sentences'\nrelevance to the BDI-II symptoms by cosine similarity. We used two\nstate-of-the-art transformer-based models (MentalRoBERTa and a variant of\nMPNet) for embedding the social media posts, the original and generated\nresponses of the BDI-II. Our results show that using sentence embeddings from a\nmodel designed for semantic search outperforms the approach using embeddings\nfrom a model pre-trained on mental health data. Furthermore, the generated\nsynthetic data were proved too specific for this task, the approach simply\nrelying on the BDI-II responses had the best performance.", "published": "2023-07-05 14:15:15", "link": "http://arxiv.org/abs/2307.02313v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MuLMS-AZ: An Argumentative Zoning Dataset for the Materials Science\n  Domain", "abstract": "Scientific publications follow conventionalized rhetorical structures.\nClassifying the Argumentative Zone (AZ), e.g., identifying whether a sentence\nstates a Motivation, a Result or Background information, has been proposed to\nimprove processing of scholarly documents. In this work, we adapt and extend\nthis idea to the domain of materials science research. We present and release a\nnew dataset of 50 manually annotated research articles. The dataset spans seven\nsub-topics and is annotated with a materials-science focused multi-label\nannotation scheme for AZ. We detail corpus statistics and demonstrate high\ninter-annotator agreement. Our computational experiments show that using\ndomain-specific pre-trained transformer-based text encoders is key to high\nclassification performance. We also find that AZ categories from existing\ndatasets in other domains are transferable to varying degrees.", "published": "2023-07-05 14:55:18", "link": "http://arxiv.org/abs/2307.02340v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Won't Get Fooled Again: Answering Questions with False Premises", "abstract": "Pre-trained language models (PLMs) have shown unprecedented potential in\nvarious fields, especially as the backbones for question-answering (QA)\nsystems. However, they tend to be easily deceived by tricky questions such as\n\"How many eyes does the sun have?\". Such frailties of PLMs often allude to the\nlack of knowledge within them. In this paper, we find that the PLMs already\npossess the knowledge required to rebut such questions, and the key is how to\nactivate the knowledge. To systematize this observation, we investigate the\nPLMs' responses to one kind of tricky questions, i.e., the false premises\nquestions (FPQs). We annotate a FalseQA dataset containing 2365 human-written\nFPQs, with the corresponding explanations for the false premises and the\nrevised true premise questions. Using FalseQA, we discover that PLMs are\ncapable of discriminating FPQs by fine-tuning on moderate numbers (e.g., 256)\nof examples. PLMs also generate reasonable explanations for the false premise,\nwhich serve as rebuttals. Further replaying a few general questions during\ntraining allows PLMs to excel on FPQs and general questions simultaneously. Our\nwork suggests that once the rebuttal ability is stimulated, knowledge inside\nthe PLMs can be effectively utilized to handle FPQs, which incentivizes the\nresearch on PLM-based QA systems.", "published": "2023-07-05 16:09:21", "link": "http://arxiv.org/abs/2307.02394v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SkipDecode: Autoregressive Skip Decoding with Batching and Caching for\n  Efficient LLM Inference", "abstract": "Autoregressive large language models (LLMs) have made remarkable progress in\nvarious natural language generation tasks. However, they incur high computation\ncost and latency resulting from the autoregressive token-by-token generation.\nTo address this issue, several approaches have been proposed to reduce\ncomputational cost using early-exit strategies. These strategies enable faster\ntext generation using reduced computation without applying the full computation\ngraph to each token. While existing token-level early exit methods show\npromising results for online inference, they cannot be readily applied for\nbatch inferencing and Key-Value caching. This is because they have to wait\nuntil the last token in a batch exits before they can stop computing. This\nseverely limits the practical application of such techniques. In this paper, we\npropose a simple and effective token-level early exit method, SkipDecode,\ndesigned to work seamlessly with batch inferencing and KV caching. It overcomes\nprior constraints by setting up a singular exit point for every token in a\nbatch at each sequence position. It also guarantees a monotonic decrease in\nexit points, thereby eliminating the need to recompute KV Caches for preceding\ntokens. Rather than terminating computation prematurely as in prior works, our\napproach bypasses lower to middle layers, devoting most of the computational\nresources to upper layers, allowing later tokens to benefit from the compute\nexpenditure by earlier tokens. Our experimental results show that SkipDecode\ncan obtain 2x to 5x inference speedups with negligible regression across a\nvariety of tasks. This is achieved using OPT models of 1.3 billion and 6.7\nbillion parameters, all the while being directly compatible with batching and\nKV caching optimization techniques.", "published": "2023-07-05 19:59:09", "link": "http://arxiv.org/abs/2307.02628v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Sentiment Analysis of Plastic Surgery Social Media Posts", "abstract": "The massive collection of user posts across social media platforms is\nprimarily untapped for artificial intelligence (AI) use cases based on the\nsheer volume and velocity of textual data. Natural language processing (NLP) is\na subfield of AI that leverages bodies of documents, known as corpora, to train\ncomputers in human-like language understanding. Using a word ranking method,\nterm frequency-inverse document frequency (TF-IDF), to create features across\ndocuments, it is possible to perform unsupervised analytics, machine learning\n(ML) that can group the documents without a human manually labeling the data.\nFor large datasets with thousands of features, t-distributed stochastic\nneighbor embedding (t-SNE), k-means clustering and Latent Dirichlet allocation\n(LDA) are employed to learn top words and generate topics for a Reddit and\nTwitter combined corpus. Using extremely simple deep learning models, this\nstudy demonstrates that the applied results of unsupervised analysis allow a\ncomputer to predict either negative, positive, or neutral user sentiment\ntowards plastic surgery based on a tweet or subreddit post with almost 90%\naccuracy. Furthermore, the model is capable of achieving higher accuracy on the\nunsupervised sentiment task than on a rudimentary supervised document\nclassification task. Therefore, unsupervised learning may be considered a\nviable option in labeling social media documents for NLP tasks.", "published": "2023-07-05 20:16:20", "link": "http://arxiv.org/abs/2307.02640v1", "categories": ["cs.CL", "68T50"], "primary_category": "cs.CL"}
{"title": "Learning Symbolic Rules over Abstract Meaning Representations for\n  Textual Reinforcement Learning", "abstract": "Text-based reinforcement learning agents have predominantly been neural\nnetwork-based models with embeddings-based representation, learning\nuninterpretable policies that often do not generalize well to unseen games. On\nthe other hand, neuro-symbolic methods, specifically those that leverage an\nintermediate formal representation, are gaining significant attention in\nlanguage understanding tasks. This is because of their advantages ranging from\ninherent interpretability, the lesser requirement of training data, and being\ngeneralizable in scenarios with unseen data. Therefore, in this paper, we\npropose a modular, NEuro-Symbolic Textual Agent (NESTA) that combines a generic\nsemantic parser with a rule induction system to learn abstract interpretable\nrules as policies. Our experiments on established text-based game benchmarks\nshow that the proposed NESTA method outperforms deep reinforcement\nlearning-based techniques by achieving better generalization to unseen test\ngames and learning from fewer training interactions.", "published": "2023-07-05 23:21:05", "link": "http://arxiv.org/abs/2307.02689v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Emoji Prediction in Tweets using BERT", "abstract": "In recent years, the use of emojis in social media has increased\ndramatically, making them an important element in understanding online\ncommunication. However, predicting the meaning of emojis in a given text is a\nchallenging task due to their ambiguous nature. In this study, we propose a\ntransformer-based approach for emoji prediction using BERT, a widely-used\npre-trained language model. We fine-tuned BERT on a large corpus of text\n(tweets) containing both text and emojis to predict the most appropriate emoji\nfor a given text. Our experimental results demonstrate that our approach\noutperforms several state-of-the-art models in predicting emojis with an\naccuracy of over 75 percent. This work has potential applications in natural\nlanguage processing, sentiment analysis, and social media marketing.", "published": "2023-07-05 06:38:52", "link": "http://arxiv.org/abs/2307.02054v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Leveraging multilingual transfer for unsupervised semantic acoustic word\n  embeddings", "abstract": "Acoustic word embeddings (AWEs) are fixed-dimensional vector representations\nof speech segments that encode phonetic content so that different realisations\nof the same word have similar embeddings. In this paper we explore semantic AWE\nmodelling. These AWEs should not only capture phonetics but also the meaning of\na word (similar to textual word embeddings). We consider the scenario where we\nonly have untranscribed speech in a target language. We introduce a number of\nstrategies leveraging a pre-trained multilingual AWE model -- a phonetic AWE\nmodel trained on labelled data from multiple languages excluding the target.\nOur best semantic AWE approach involves clustering word segments using the\nmultilingual AWE model, deriving soft pseudo-word labels from the cluster\ncentroids, and then training a Skipgram-like model on the soft vectors. In an\nintrinsic word similarity task measuring semantics, this multilingual transfer\napproach outperforms all previous semantic AWE methods. We also show -- for the\nfirst time -- that AWEs can be used for downstream semantic query-by-example\nsearch.", "published": "2023-07-05 07:46:54", "link": "http://arxiv.org/abs/2307.02083v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Generative Job Recommendations with Large Language Model", "abstract": "The rapid development of online recruitment services has encouraged the\nutilization of recommender systems to streamline the job seeking process.\nPredominantly, current job recommendations deploy either collaborative\nfiltering or person-job matching strategies. However, these models tend to\noperate as \"black-box\" systems and lack the capacity to offer explainable\nguidance to job seekers. Moreover, conventional matching-based recommendation\nmethods are limited to retrieving and ranking existing jobs in the database,\nrestricting their potential as comprehensive career AI advisors. To this end,\nhere we present GIRL (GeneratIve job Recommendation based on Large language\nmodels), a novel approach inspired by recent advancements in the field of Large\nLanguage Models (LLMs). We initially employ a Supervised Fine-Tuning (SFT)\nstrategy to instruct the LLM-based generator in crafting suitable Job\nDescriptions (JDs) based on the Curriculum Vitae (CV) of a job seeker.\nMoreover, we propose to train a model which can evaluate the matching degree\nbetween CVs and JDs as a reward model, and we use Proximal Policy Optimization\n(PPO)-based Reinforcement Learning (RL) method to further fine-tine the\ngenerator. This aligns the generator with recruiter feedback, tailoring the\noutput to better meet employer preferences. In particular, GIRL serves as a job\nseeker-centric generative model, providing job suggestions without the need of\na candidate set. This capability also enhances the performance of existing job\nrecommendation models by supplementing job seeking features with generated\ncontent. With extensive experiments on a large-scale real-world dataset, we\ndemonstrate the substantial effectiveness of our approach. We believe that GIRL\nintroduces a paradigm-shifting approach to job recommendation systems,\nfostering a more personalized and comprehensive job-seeking experience.", "published": "2023-07-05 09:58:08", "link": "http://arxiv.org/abs/2307.02157v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Performance Comparison of Large Language Models on VNHSGE English\n  Dataset: OpenAI ChatGPT, Microsoft Bing Chat, and Google Bard", "abstract": "This paper presents a performance comparison of three large language models\n(LLMs), namely OpenAI ChatGPT, Microsoft Bing Chat (BingChat), and Google Bard,\non the VNHSGE English dataset. The performance of BingChat, Bard, and ChatGPT\n(GPT-3.5) is 92.4\\%, 86\\%, and 79.2\\%, respectively. The results show that\nBingChat is better than ChatGPT and Bard. Therefore, BingChat and Bard can\nreplace ChatGPT while ChatGPT is not yet officially available in Vietnam. The\nresults also indicate that BingChat, Bard and ChatGPT outperform Vietnamese\nstudents in English language proficiency. The findings of this study contribute\nto the understanding of the potential of LLMs in English language education.\nThe remarkable performance of ChatGPT, BingChat, and Bard demonstrates their\npotential as effective tools for teaching and learning English at the high\nschool level.", "published": "2023-07-05 13:40:57", "link": "http://arxiv.org/abs/2307.02288v3", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "To be or not to be: a translation reception study of a literary text\n  translated into Dutch and Catalan using machine translation", "abstract": "This article presents the results of a study involving the reception of a\nfictional story by Kurt Vonnegut translated from English into Catalan and Dutch\nin three conditions: machine-translated (MT), post-edited (PE) and translated\nfrom scratch (HT). 223 participants were recruited who rated the reading\nconditions using three scales: Narrative Engagement, Enjoyment and Translation\nReception. The results show that HT presented a higher engagement, enjoyment\nand translation reception in Catalan if compared to PE and MT. However, the\nDutch readers show higher scores in PE than in both HT and MT, and the highest\nengagement and enjoyments scores are reported when reading the original English\nversion. We hypothesize that when reading a fictional story in translation, not\nonly the condition and the quality of the translations is key to understand its\nreception, but also the participants reading patterns, reading language, and,\nperhaps language status in their own societies.", "published": "2023-07-05 15:18:52", "link": "http://arxiv.org/abs/2307.02358v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "What Matters in Training a GPT4-Style Language Model with Multimodal\n  Inputs?", "abstract": "Recent advancements in Large Language Models (LLMs) such as GPT4 have\ndisplayed exceptional multi-modal capabilities in following open-ended\ninstructions given images. However, the performance of these models heavily\nrelies on design choices such as network structures, training data, and\ntraining strategies, and these choices have not been extensively discussed in\nthe literature, making it difficult to quantify progress in this field. To\naddress this issue, this paper presents a systematic and comprehensive study,\nquantitatively and qualitatively, on training such models. We implement over 20\nvariants with controlled settings. Concretely, for network structures, we\ncompare different LLM backbones and model designs. For training data, we\ninvestigate the impact of data and sampling strategies. For instructions, we\nexplore the influence of diversified prompts on the instruction-following\nability of the trained models. For benchmarks, we contribute the first, to our\nbest knowledge, comprehensive evaluation set including both image and video\ntasks through crowd-sourcing. Based on our findings, we present Lynx, which\nperforms the most accurate multi-modal understanding while keeping the best\nmulti-modal generation ability compared to existing open-sourced GPT4-style\nmodels.", "published": "2023-07-05 17:44:28", "link": "http://arxiv.org/abs/2307.02469v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Deductive Additivity for Planning of Natural Language Proofs", "abstract": "Current natural language systems designed for multi-step claim validation\ntypically operate in two phases: retrieve a set of relevant premise statements\nusing heuristics (planning), then generate novel conclusions from those\nstatements using a large language model (deduction). The planning step often\nrequires expensive Transformer operations and does not scale to arbitrary\nnumbers of premise statements. In this paper, we investigate whether an\nefficient planning heuristic is possible via embedding spaces compatible with\ndeductive reasoning. Specifically, we evaluate whether embedding spaces exhibit\na property we call deductive additivity: the sum of premise statement\nembeddings should be close to embeddings of conclusions based on those\npremises. We explore multiple sources of off-the-shelf dense embeddings in\naddition to fine-tuned embeddings from GPT3 and sparse embeddings from BM25. We\nstudy embedding models both intrinsically, evaluating whether the property of\ndeductive additivity holds, and extrinsically, using them to assist planning in\nnatural language proof generation. Lastly, we create a dataset, Single-Step\nReasoning Contrast (SSRC), to further probe performance on various reasoning\ntypes. Our findings suggest that while standard embedding methods frequently\nembed conclusions near the sums of their premises, they fall short of being\neffective heuristics and lack the ability to model certain categories of\nreasoning.", "published": "2023-07-05 17:45:48", "link": "http://arxiv.org/abs/2307.02472v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Reasoning or Reciting? Exploring the Capabilities and Limitations of\n  Language Models Through Counterfactual Tasks", "abstract": "The impressive performance of recent language models across a wide range of\ntasks suggests that they possess a degree of abstract reasoning skills. Are\nthese skills general and transferable, or specialized to specific tasks seen\nduring pretraining? To disentangle these effects, we propose an evaluation\nframework based on \"counterfactual\" task variants that deviate from the default\nassumptions underlying standard tasks. Across a suite of 11 tasks, we observe\nnontrivial performance on the counterfactual variants, but nevertheless find\nthat performance substantially and consistently degrades compared to the\ndefault conditions. This suggests that while current LMs may possess abstract\ntask-solving skills to an extent, they often also rely on narrow,\nnon-transferable procedures for task-solving. These results motivate a more\ncareful interpretation of language model performance that teases apart these\naspects of behavior.", "published": "2023-07-05 17:50:42", "link": "http://arxiv.org/abs/2307.02477v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LongNet: Scaling Transformers to 1,000,000,000 Tokens", "abstract": "Scaling sequence length has become a critical demand in the era of large\nlanguage models. However, existing methods struggle with either computational\ncomplexity or model expressivity, rendering the maximum sequence length\nrestricted. To address this issue, we introduce LongNet, a Transformer variant\nthat can scale sequence length to more than 1 billion tokens, without\nsacrificing the performance on shorter sequences. Specifically, we propose\ndilated attention, which expands the attentive field exponentially as the\ndistance grows. LongNet has significant advantages: 1) it has a linear\ncomputation complexity and a logarithm dependency between any two tokens in a\nsequence; 2) it can be served as a distributed trainer for extremely long\nsequences; 3) its dilated attention is a drop-in replacement for standard\nattention, which can be seamlessly integrated with the existing\nTransformer-based optimization. Experiments results demonstrate that LongNet\nyields strong performance on both long-sequence modeling and general language\ntasks. Our work opens up new possibilities for modeling very long sequences,\ne.g., treating a whole corpus or even the entire Internet as a sequence.", "published": "2023-07-05 17:59:38", "link": "http://arxiv.org/abs/2307.02486v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ODD: A Benchmark Dataset for the Natural Language Processing based\n  Opioid Related Aberrant Behavior Detection", "abstract": "Opioid related aberrant behaviors (ORABs) present novel risk factors for\nopioid overdose. This paper introduces a novel biomedical natural language\nprocessing benchmark dataset named ODD, for ORAB Detection Dataset. ODD is an\nexpert-annotated dataset designed to identify ORABs from patients' EHR notes\nand classify them into nine categories; 1) Confirmed Aberrant Behavior, 2)\nSuggested Aberrant Behavior, 3) Opioids, 4) Indication, 5) Diagnosed opioid\ndependency, 6) Benzodiazepines, 7) Medication Changes, 8) Central Nervous\nSystem-related, and 9) Social Determinants of Health. We explored two\nstate-of-the-art natural language processing models (fine-tuning and\nprompt-tuning approaches) to identify ORAB. Experimental results show that the\nprompt-tuning models outperformed the fine-tuning models in most categories and\nthe gains were especially higher among uncommon categories (Suggested Aberrant\nBehavior, Confirmed Aberrant Behaviors, Diagnosed Opioid Dependence, and\nMedication Change). Although the best model achieved the highest 88.17% on\nmacro average area under precision recall curve, uncommon classes still have a\nlarge room for performance improvement. ODD is publicly available.", "published": "2023-07-05 18:41:29", "link": "http://arxiv.org/abs/2307.02591v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evade ChatGPT Detectors via A Single Space", "abstract": "ChatGPT brings revolutionary social value but also raises concerns about the\nmisuse of AI-generated text. Consequently, an important question is how to\ndetect whether texts are generated by ChatGPT or by human. Existing detectors\nare built upon the assumption that there are distributional gaps between\nhuman-generated and AI-generated text. These gaps are typically identified\nusing statistical information or classifiers. Our research challenges the\ndistributional gap assumption in detectors. We find that detectors do not\neffectively discriminate the semantic and stylistic gaps between\nhuman-generated and AI-generated text. Instead, the \"subtle differences\", such\nas an extra space, become crucial for detection. Based on this discovery, we\npropose the SpaceInfi strategy to evade detection. Experiments demonstrate the\neffectiveness of this strategy across multiple benchmarks and detectors. We\nalso provide a theoretical explanation for why SpaceInfi is successful in\nevading perplexity-based detection. And we empirically show that a phenomenon\ncalled token mutation causes the evasion for language model-based detectors.\nOur findings offer new insights and challenges for understanding and\nconstructing more applicable ChatGPT detectors.", "published": "2023-07-05 18:48:28", "link": "http://arxiv.org/abs/2307.02599v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Dense Video Captioning by Jointly Optimizing Text and Moment", "abstract": "Dense video captioning, a task of localizing meaningful moments and\ngenerating relevant captions for videos, often requires a large, expensive\ncorpus of annotated video segments paired with text. In an effort to minimize\nthe annotation cost, we propose ZeroTA, a novel method for dense video\ncaptioning in a zero-shot manner. Our method does not require any videos or\nannotations for training; instead, it localizes and describes events within\neach input video at test time by optimizing solely on the input. This is\naccomplished by introducing a soft moment mask that represents a temporal\nsegment in the video and jointly optimizing it with the prefix parameters of a\nlanguage model. This joint optimization aligns a frozen language generation\nmodel (i.e., GPT-2) with a frozen vision-language contrastive model (i.e.,\nCLIP) by maximizing the matching score between the generated text and a moment\nwithin the video. We also introduce a pairwise temporal IoU loss to let a set\nof soft moment masks capture multiple distinct events within the video. Our\nmethod effectively discovers diverse significant events within the video, with\nthe resulting captions appropriately describing these events. The empirical\nresults demonstrate that ZeroTA surpasses zero-shot baselines and even\noutperforms the state-of-the-art few-shot method on the widely-used benchmark\nActivityNet Captions. Moreover, our method shows greater robustness compared to\nsupervised methods when evaluated in out-of-domain scenarios. This research\nprovides insight into the potential of aligning widely-used models, such as\nlanguage generation models and vision-language models, to unlock a new\ncapability: understanding temporal aspects of videos.", "published": "2023-07-05 23:01:26", "link": "http://arxiv.org/abs/2307.02682v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Becoming self-instruct: introducing early stopping criteria for minimal\n  instruct tuning", "abstract": "In this paper, we introduce the Instruction Following Score (IFS), a metric\nthat detects language models' ability to follow instructions. The metric has a\ndual purpose. First, IFS can be used to distinguish between base and instruct\nmodels. We benchmark publicly available base and instruct models, and show that\nthe ratio of well formatted responses to partial and full sentences can be an\neffective measure between those two model classes. Secondly, the metric can be\nused as an early stopping criteria for instruct tuning. We compute IFS for\nSupervised Fine-Tuning (SFT) of 7B and 13B LLaMA models, showing that models\nlearn to follow instructions relatively early in the training process, and the\nfurther finetuning can result in changes in the underlying base model\nsemantics. As an example of semantics change we show the objectivity of model\npredictions, as defined by an auxiliary metric ObjecQA. We show that in this\nparticular case, semantic changes are the steepest when the IFS tends to\nplateau. We hope that decomposing instruct tuning into IFS and semantic factors\nstarts a new trend in better controllable instruct tuning and opens\npossibilities for designing minimal instruct interfaces querying foundation\nmodels.", "published": "2023-07-05 09:42:25", "link": "http://arxiv.org/abs/2307.03692v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Several categories of Large Language Models (LLMs): A Short Survey", "abstract": "Large Language Models(LLMs)have become effective tools for natural language\nprocessing and have been used in many different fields. This essay offers a\nsuccinct summary of various LLM subcategories. The survey emphasizes recent\ndevelopments and efforts made for various LLM kinds, including task-based\nfinancial LLMs, multilingual language LLMs, biomedical and clinical LLMs,\nvision language LLMs, and code language models. The survey gives a general\nsummary of the methods, attributes, datasets, transformer models, and\ncomparison metrics applied in each category of LLMs. Furthermore, it highlights\nunresolved problems in the field of developing chatbots and virtual assistants,\nsuch as boosting natural language processing, enhancing chatbot intelligence,\nand resolving moral and legal dilemmas. The purpose of this study is to provide\nreaders, developers, academics, and users interested in LLM-based chatbots and\nvirtual intelligent assistant technologies with useful information and future\ndirections.", "published": "2023-07-05 18:18:23", "link": "http://arxiv.org/abs/2307.10188v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Comparative Analysis of GPT-4 and Human Graders in Evaluating Praise\n  Given to Students in Synthetic Dialogues", "abstract": "Research suggests that providing specific and timely feedback to human tutors\nenhances their performance. However, it presents challenges due to the\ntime-consuming nature of assessing tutor performance by human evaluators. Large\nlanguage models, such as the AI-chatbot ChatGPT, hold potential for offering\nconstructive feedback to tutors in practical settings. Nevertheless, the\naccuracy of AI-generated feedback remains uncertain, with scant research\ninvestigating the ability of models like ChatGPT to deliver effective feedback.\nIn this work-in-progress, we evaluate 30 dialogues generated by GPT-4 in a\ntutor-student setting. We use two different prompting approaches, the zero-shot\nchain of thought and the few-shot chain of thought, to identify specific\ncomponents of effective praise based on five criteria. These approaches are\nthen compared to the results of human graders for accuracy. Our goal is to\nassess the extent to which GPT-4 can accurately identify each praise criterion.\nWe found that both zero-shot and few-shot chain of thought approaches yield\ncomparable results. GPT-4 performs moderately well in identifying instances\nwhen the tutor offers specific and immediate praise. However, GPT-4\nunderperforms in identifying the tutor's ability to deliver sincere praise,\nparticularly in the zero-shot prompting scenario where examples of sincere\ntutor praise statements were not provided. Future work will focus on enhancing\nprompt engineering, developing a more general tutoring rubric, and evaluating\nour method using real-life tutoring dialogues.", "published": "2023-07-05 04:14:01", "link": "http://arxiv.org/abs/2307.02018v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "EHRSHOT: An EHR Benchmark for Few-Shot Evaluation of Foundation Models", "abstract": "While the general machine learning (ML) community has benefited from public\ndatasets, tasks, and models, the progress of ML in healthcare has been hampered\nby a lack of such shared assets. The success of foundation models creates new\nchallenges for healthcare ML by requiring access to shared pretrained models to\nvalidate performance benefits. We help address these challenges through three\ncontributions. First, we publish a new dataset, EHRSHOT, which contains\ndeidentified structured data from the electronic health records (EHRs) of 6,739\npatients from Stanford Medicine. Unlike MIMIC-III/IV and other popular EHR\ndatasets, EHRSHOT is longitudinal and not restricted to ICU/ED patients.\nSecond, we publish the weights of CLMBR-T-base, a 141M parameter clinical\nfoundation model pretrained on the structured EHR data of 2.57M patients. We\nare one of the first to fully release such a model for coded EHR data; in\ncontrast, most prior models released for clinical data (e.g. GatorTron,\nClinicalBERT) only work with unstructured text and cannot process the rich,\nstructured data within an EHR. We provide an end-to-end pipeline for the\ncommunity to validate and build upon its performance. Third, we define 15\nfew-shot clinical prediction tasks, enabling evaluation of foundation models on\nbenefits such as sample efficiency and task adaptation. Our model and dataset\nare available via a research data use agreement from our website:\nhttps://ehrshot.stanford.edu. Code to reproduce our results are available at\nour Github repo: https://github.com/som-shahlab/ehrshot-benchmark", "published": "2023-07-05 05:24:59", "link": "http://arxiv.org/abs/2307.02028v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Recommender Systems in the Era of Large Language Models (LLMs)", "abstract": "With the prosperity of e-commerce and web applications, Recommender Systems\n(RecSys) have become an important component of our daily life, providing\npersonalized suggestions that cater to user preferences. While Deep Neural\nNetworks (DNNs) have made significant advancements in enhancing recommender\nsystems by modeling user-item interactions and incorporating textual side\ninformation, DNN-based methods still face limitations, such as difficulties in\nunderstanding users' interests and capturing textual side information,\ninabilities in generalizing to various recommendation scenarios and reasoning\non their predictions, etc. Meanwhile, the emergence of Large Language Models\n(LLMs), such as ChatGPT and GPT4, has revolutionized the fields of Natural\nLanguage Processing (NLP) and Artificial Intelligence (AI), due to their\nremarkable abilities in fundamental responsibilities of language understanding\nand generation, as well as impressive generalization and reasoning\ncapabilities. As a result, recent studies have attempted to harness the power\nof LLMs to enhance recommender systems. Given the rapid evolution of this\nresearch direction in recommender systems, there is a pressing need for a\nsystematic overview that summarizes existing LLM-empowered recommender systems,\nto provide researchers in relevant fields with an in-depth understanding.\nTherefore, in this paper, we conduct a comprehensive review of LLM-empowered\nrecommender systems from various aspects including Pre-training, Fine-tuning,\nand Prompting. More specifically, we first introduce representative methods to\nharness the power of LLMs (as a feature encoder) for learning representations\nof users and items. Then, we review recent techniques of LLMs for enhancing\nrecommender systems from three paradigms, namely pre-training, fine-tuning, and\nprompting. Finally, we comprehensively discuss future directions in this\nemerging field.", "published": "2023-07-05 06:03:40", "link": "http://arxiv.org/abs/2307.02046v6", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Combating Confirmation Bias: A Unified Pseudo-Labeling Framework for\n  Entity Alignment", "abstract": "Entity alignment (EA) aims at identifying equivalent entity pairs across\ndifferent knowledge graphs (KGs) that refer to the same real-world identity. To\nsystematically combat confirmation bias for pseudo-labeling-based entity\nalignment, we propose a Unified Pseudo-Labeling framework for Entity Alignment\n(UPL-EA) that explicitly eliminates pseudo-labeling errors to boost the\naccuracy of entity alignment. UPL-EA consists of two complementary components:\n(1) The Optimal Transport (OT)-based pseudo-labeling uses discrete OT modeling\nas an effective means to enable more accurate determination of entity\ncorrespondences across two KGs and to mitigate the adverse impact of erroneous\nmatches. A simple but highly effective criterion is further devised to derive\npseudo-labeled entity pairs that satisfy one-to-one correspondences at each\niteration. (2) The cross-iteration pseudo-label calibration operates across\nmultiple consecutive iterations to further improve the pseudo-labeling\nprecision rate by reducing the local pseudo-label selection variability with a\ntheoretical guarantee. The two components are respectively designed to\neliminate Type I and Type II pseudo-labeling errors identified through our\nanalyse. The calibrated pseudo-labels are thereafter used to augment prior\nalignment seeds to reinforce subsequent model training for alignment inference.\nThe effectiveness of UPL-EA in eliminating pseudo-labeling errors is both\ntheoretically supported and experimentally validated. The experimental results\nshow that our approach achieves competitive performance with limited prior\nalignment seeds.", "published": "2023-07-05 07:32:34", "link": "http://arxiv.org/abs/2307.02075v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "LOAF-M2L: Joint Learning of Wording and Formatting for Singable\n  Melody-to-Lyric Generation", "abstract": "Despite previous efforts in melody-to-lyric generation research, there is\nstill a significant compatibility gap between generated lyrics and melodies,\nnegatively impacting the singability of the outputs. This paper bridges the\nsingability gap with a novel approach to generating singable lyrics by jointly\nLearning wOrding And Formatting during Melody-to-Lyric training. After\ngeneral-domain pretraining, our proposed model acquires length awareness first\nfrom a large text-only lyric corpus. Then, we introduce a new objective\ninformed by musicological research on the relationship between melody and\nlyrics during melody-to-lyric training, which enables the model to learn the\nfine-grained format requirements of the melody. Our model achieves 3.75% and\n21.44% absolute accuracy gains in the outputs' number-of-line and\nsyllable-per-line requirements compared to naive fine-tuning, without\nsacrificing text fluency. Furthermore, our model demonstrates a 63.92% and\n74.18% relative improvement of music-lyric compatibility and overall quality in\nthe subjective evaluation, compared to the state-of-the-art melody-to-lyric\ngeneration model, highlighting the significance of formatting learning.", "published": "2023-07-05 09:42:47", "link": "http://arxiv.org/abs/2307.02146v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Citation: A Key to Building Responsible and Accountable Large Language\n  Models", "abstract": "Large Language Models (LLMs) bring transformative benefits alongside unique\nchallenges, including intellectual property (IP) and ethical concerns. This\nposition paper explores a novel angle to mitigate these risks, drawing\nparallels between LLMs and established web systems. We identify \"citation\" -\nthe acknowledgement or reference to a source or evidence - as a crucial yet\nmissing component in LLMs. Incorporating citation could enhance content\ntransparency and verifiability, thereby confronting the IP and ethical issues\nin the deployment of LLMs. We further propose that a comprehensive citation\nmechanism for LLMs should account for both non-parametric and parametric\ncontent. Despite the complexity of implementing such a citation mechanism,\nalong with the potential pitfalls, we advocate for its development. Building on\nthis foundation, we outline several research problems in this area, aiming to\nguide future explorations towards building more responsible and accountable\nLLMs.", "published": "2023-07-05 10:25:45", "link": "http://arxiv.org/abs/2307.02185v3", "categories": ["cs.CL", "cs.AI", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Sumformer: Universal Approximation for Efficient Transformers", "abstract": "Natural language processing (NLP) made an impressive jump with the\nintroduction of Transformers. ChatGPT is one of the most famous examples,\nchanging the perception of the possibilities of AI even outside the research\ncommunity. However, besides the impressive performance, the quadratic time and\nspace complexity of Transformers with respect to sequence length pose\nsignificant limitations for handling long sequences. While efficient\nTransformer architectures like Linformer and Performer with linear complexity\nhave emerged as promising solutions, their theoretical understanding remains\nlimited. In this paper, we introduce Sumformer, a novel and simple architecture\ncapable of universally approximating equivariant sequence-to-sequence\nfunctions. We use Sumformer to give the first universal approximation results\nfor Linformer and Performer. Moreover, we derive a new proof for Transformers,\nshowing that just one attention layer is sufficient for universal\napproximation.", "published": "2023-07-05 13:59:35", "link": "http://arxiv.org/abs/2307.02301v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Causal Discovery with Language Models as Imperfect Experts", "abstract": "Understanding the causal relationships that underlie a system is a\nfundamental prerequisite to accurate decision-making. In this work, we explore\nhow expert knowledge can be used to improve the data-driven identification of\ncausal graphs, beyond Markov equivalence classes. In doing so, we consider a\nsetting where we can query an expert about the orientation of causal\nrelationships between variables, but where the expert may provide erroneous\ninformation. We propose strategies for amending such expert knowledge based on\nconsistency properties, e.g., acyclicity and conditional independencies in the\nequivalence class. We then report a case study, on real data, where a large\nlanguage model is used as an imperfect expert.", "published": "2023-07-05 16:01:38", "link": "http://arxiv.org/abs/2307.02390v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Exploring Continual Learning for Code Generation Models", "abstract": "Large-scale code generation models such as Codex and CodeT5 have achieved\nimpressive performance. However, libraries are upgraded or deprecated very\nfrequently and re-training large-scale language models is computationally\nexpensive. Therefore, Continual Learning (CL) is an important aspect that\nremains underexplored in the code domain. In this paper, we introduce a\nbenchmark called CodeTask-CL that covers a wide range of tasks, including code\ngeneration, translation, summarization, and refinement, with different input\nand output programming languages. Next, on our CodeTask-CL benchmark, we\ncompare popular CL techniques from NLP and Vision domains. We find that\neffective methods like Prompt Pooling (PP) suffer from catastrophic forgetting\ndue to the unstable training of the prompt selection mechanism caused by stark\ndistribution shifts in coding tasks. We address this issue with our proposed\nmethod, Prompt Pooling with Teacher Forcing (PP-TF), that stabilizes training\nby enforcing constraints on the prompt selection mechanism and leads to a\n21.54% improvement over Prompt Pooling. Along with the benchmark, we establish\na training pipeline that can be used for CL on code models, which we believe\ncan motivate further development of CL methods for code models. Our code is\navailable at https://github.com/amazon-science/codetaskcl-pptf", "published": "2023-07-05 16:58:39", "link": "http://arxiv.org/abs/2307.02435v1", "categories": ["cs.LG", "cs.CL", "cs.SE"], "primary_category": "cs.LG"}
{"title": "Building Cooperative Embodied Agents Modularly with Large Language\n  Models", "abstract": "In this work, we address challenging multi-agent cooperation problems with\ndecentralized control, raw sensory observations, costly communication, and\nmulti-objective tasks instantiated in various embodied environments. While\nprevious research either presupposes a cost-free communication channel or\nrelies on a centralized controller with shared observations, we harness the\ncommonsense knowledge, reasoning ability, language comprehension, and text\ngeneration prowess of LLMs and seamlessly incorporate them into a\ncognitive-inspired modular framework that integrates with perception, memory,\nand execution. Thus building a Cooperative Embodied Language Agent CoELA, who\ncan plan, communicate, and cooperate with others to accomplish long-horizon\ntasks efficiently. Our experiments on C-WAH and TDW-MAT demonstrate that CoELA\ndriven by GPT-4 can surpass strong planning-based methods and exhibit emergent\neffective communication. Though current Open LMs like LLAMA-2 still\nunderperform, we fine-tune a CoELA with data collected with our agents and show\nhow they can achieve promising performance. We also conducted a user study for\nhuman-agent interaction and discovered that CoELA communicating in natural\nlanguage can earn more trust and cooperate more effectively with humans. Our\nresearch underscores the potential of LLMs for future research in multi-agent\ncooperation. Videos can be found on the project website\nhttps://vis-www.cs.umass.edu/Co-LLM-Agents/.", "published": "2023-07-05 17:59:27", "link": "http://arxiv.org/abs/2307.02485v2", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.RO"], "primary_category": "cs.AI"}
{"title": "Named Entity Inclusion in Abstractive Text Summarization", "abstract": "We address the named entity omission - the drawback of many current\nabstractive text summarizers. We suggest a custom pretraining objective to\nenhance the model's attention on the named entities in a text. At first, the\nnamed entity recognition model RoBERTa is trained to determine named entities\nin the text. After that, this model is used to mask named entities in the text\nand the BART model is trained to reconstruct them. Next, the BART model is\nfine-tuned on the summarization task. Our experiments showed that this\npretraining approach improves named entity inclusion precision and recall\nmetrics.", "published": "2023-07-05 18:12:31", "link": "http://arxiv.org/abs/2307.02570v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Human Inspired Progressive Alignment and Comparative Learning for\n  Grounded Word Acquisition", "abstract": "Human language acquisition is an efficient, supervised, and continual\nprocess. In this work, we took inspiration from how human babies acquire their\nfirst language, and developed a computational process for word acquisition\nthrough comparative learning. Motivated by cognitive findings, we generated a\nsmall dataset that enables the computation models to compare the similarities\nand differences of various attributes, learn to filter out and extract the\ncommon information for each shared linguistic label. We frame the acquisition\nof words as not only the information filtration process, but also as\nrepresentation-symbol mapping. This procedure does not involve a fixed\nvocabulary size, nor a discriminative objective, and allows the models to\ncontinually learn more concepts efficiently. Our results in controlled\nexperiments have shown the potential of this approach for efficient continual\nlearning of grounded words.", "published": "2023-07-05 19:38:04", "link": "http://arxiv.org/abs/2307.02615v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Scaling In-Context Demonstrations with Structured Attention", "abstract": "The recent surge of large language models (LLMs) highlights their ability to\nperform in-context learning, i.e., \"learning\" to perform a task from a few\ndemonstrations in the context without any parameter updates. However, their\ncapabilities of in-context learning are limited by the model architecture: 1)\nthe use of demonstrations is constrained by a maximum sentence length due to\npositional embeddings; 2) the quadratic complexity of attention hinders users\nfrom using more demonstrations efficiently; 3) LLMs are shown to be sensitive\nto the order of the demonstrations. In this work, we tackle these challenges by\nproposing a better architectural design for in-context learning. We propose\nSAICL (Structured Attention for In-Context Learning), which replaces the\nfull-attention by a structured attention mechanism designed for in-context\nlearning, and removes unnecessary dependencies between individual\ndemonstrations, while making the model invariant to the permutation of\ndemonstrations. We evaluate SAICL in a meta-training framework and show that\nSAICL achieves comparable or better performance than full attention while\nobtaining up to 3.4x inference speed-up. SAICL also consistently outperforms a\nstrong Fusion-in-Decoder (FiD) baseline which processes each demonstration\nindependently. Finally, thanks to its linear nature, we demonstrate that SAICL\ncan easily scale to hundreds of demonstrations with continuous performance\ngains with scaling.", "published": "2023-07-05 23:26:01", "link": "http://arxiv.org/abs/2307.02690v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "OpenDelta: A Plug-and-play Library for Parameter-efficient Adaptation of\n  Pre-trained Models", "abstract": "The scale of large pre-trained models (PTMs) poses significant challenges in\nadapting to downstream tasks due to the high optimization overhead and storage\ncosts associated with full-parameter fine-tuning. To address this, many studies\nexplore parameter-efficient tuning methods, also framed as \"delta tuning\",\nwhich updates only a small subset of parameters, known as \"delta modules\",\nwhile keeping the backbone model's parameters fixed. However, the practicality\nand flexibility of delta tuning have been limited due to existing\nimplementations that directly modify the code of the backbone PTMs and\nhard-code specific delta tuning methods for each PTM. In this paper, we present\nOpenDelta, an open-source library that overcomes these limitations by providing\na plug-and-play implementation of various delta tuning methods. Our novel\ntechniques eliminate the need to modify the backbone PTMs' code, making\nOpenDelta compatible with different, even novel PTMs. OpenDelta is designed to\nbe simple, modular, and extensible, providing a comprehensive platform for\nresearchers and practitioners to adapt large PTMs efficiently.", "published": "2023-07-05 16:30:14", "link": "http://arxiv.org/abs/2307.03084v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Evaluating the Effectiveness of Large Language Models in Representing\n  Textual Descriptions of Geometry and Spatial Relations", "abstract": "This research focuses on assessing the ability of large language models\n(LLMs) in representing geometries and their spatial relations. We utilize LLMs\nincluding GPT-2 and BERT to encode the well-known text (WKT) format of\ngeometries and then feed their embeddings into classifiers and regressors to\nevaluate the effectiveness of the LLMs-generated embeddings for geometric\nattributes. The experiments demonstrate that while the LLMs-generated\nembeddings can preserve geometry types and capture some spatial relations (up\nto 73% accuracy), challenges remain in estimating numeric values and retrieving\nspatially related objects. This research highlights the need for improvement in\nterms of capturing the nuances and complexities of the underlying geospatial\ndata and integrating domain knowledge to support various GeoAI applications\nusing foundation models.", "published": "2023-07-05 03:50:08", "link": "http://arxiv.org/abs/2307.03678v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2"], "primary_category": "cs.CL"}
{"title": "Comparing Apples to Apples: Generating Aspect-Aware Comparative\n  Sentences from User Reviews", "abstract": "It is time-consuming to find the best product among many similar\nalternatives. Comparative sentences can help to contrast one item from others\nin a way that highlights important features of an item that stand out. Given\nreviews of one or multiple items and relevant item features, we generate\ncomparative review sentences to aid users to find the best fit. Specifically,\nour model consists of three successive components in a transformer: (i) an item\nencoding module to encode an item for comparison, (ii) a comparison generation\nmodule that generates comparative sentences in an autoregressive manner, (iii)\na novel decoding method for user personalization. We show that our pipeline\ngenerates fluent and diverse comparative sentences. We run experiments on the\nrelevance and fidelity of our generated sentences in a human evaluation study\nand find that our algorithm creates comparative review sentences that are\nrelevant and truthful.", "published": "2023-07-05 23:19:18", "link": "http://arxiv.org/abs/2307.03691v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "External Reasoning: Towards Multi-Large-Language-Models Interchangeable\n  Assistance with Human Feedback", "abstract": "Memory is identified as a crucial human faculty that allows for the retention\nof visual and linguistic information within the hippocampus and neurons in the\nbrain, which can subsequently be retrieved to address real-world challenges\nthat arise through a lifetime of learning. The resolution of complex AI tasks\nthrough the application of acquired knowledge represents a stride toward the\nrealization of artificial general intelligence. However, despite the prevalence\nof Large Language Models (LLMs) like GPT-3.5 and GPT-4 \\cite{brown2020language,\nleiter2023chatgpt, zaitsu2023distinguishing, OpenAI2023GPT4TR} , which have\ndisplayed remarkable capabilities in language comprehension, generation,\ninteraction, and reasoning, they are inhibited by constraints on context length\nthat preclude the processing of extensive, continually evolving knowledge\nbases. This paper proposes that LLMs could be augmented through the selective\nintegration of knowledge from external repositories, and in doing so,\nintroduces a novel methodology for External Reasoning, exemplified by ChatPDF.\nCentral to this approach is the establishment of a tiered policy for\n\\textbf{External Reasoning based on Multiple LLM Interchange Assistance} in\n\\cref{fig:overall}, where the level of support rendered is modulated across\nentry, intermediate, and advanced tiers based on the complexity of the query,\nwith adjustments made in response to human feedback. A comprehensive evaluation\nof this methodology is conducted using multiple LLMs and the results indicate\nstate-of-the-art performance in \\cref{comparison} , surpassing existing\nsolutions including ChatPDF.com. Moreover, the paper emphasizes that this\napproach is more efficient compared to the direct processing of full text by\nLLMs. The source code is publicly available at:\n\\url{https://github.com/AkideLiu/ANLP}.", "published": "2023-07-05 17:05:32", "link": "http://arxiv.org/abs/2307.12057v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Hoodwinked: Deception and Cooperation in a Text-Based Game for Language\n  Models", "abstract": "Are current language models capable of deception and lie detection? We study\nthis question by introducing a text-based game called $\\textit{Hoodwinked}$,\ninspired by Mafia and Among Us. Players are locked in a house and must find a\nkey to escape, but one player is tasked with killing the others. Each time a\nmurder is committed, the surviving players have a natural language discussion\nthen vote to banish one player from the game. We conduct experiments with\nagents controlled by GPT-3, GPT-3.5, and GPT-4 and find evidence of deception\nand lie detection capabilities. The killer often denies their crime and accuses\nothers, leading to measurable effects on voting outcomes. More advanced models\nare more effective killers, outperforming smaller models in 18 of 24 pairwise\ncomparisons. Secondary metrics provide evidence that this improvement is not\nmediated by different actions, but rather by stronger persuasive skills during\ndiscussions. To evaluate the ability of AI agents to deceive humans, we make\nthis game publicly available at h https://hoodwinked.ai/ .", "published": "2023-07-05 17:22:09", "link": "http://arxiv.org/abs/2308.01404v2", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Artificial Intelligence in archival and historical scholarship workflow:\n  HTS and ChatGPT", "abstract": "This article examines the impact of Artificial Intelligence on the archival\nheritage digitization processes, specifically regarding the manuscripts'\nautomatic transcription, their correction, and normalization. It highlights how\ndigitality has compelled scholars to redefine Archive and History field and has\nfacilitated the accessibility of analogue sources through digitization and\nintegration into big data. The study focuses on two AI systems, namely\nTranskribus and ChatGPT, which enable efficient analysis and transcription of\ndigitized sources. The article presents a test of ChatGPT, which was utilized\nto normalize the text of 366 letters stored in the Correspondence section of\nthe Biscari Archive (Catania). Although the AI exhibited some limitations that\nresulted in inaccuracies, the corrected texts met expectations. Overall, the\narticle concludes that digitization and AI can significantly enhance archival\nand historical research by allowing the analysis of vast amounts of data and\nthe application of computational linguistic tools.", "published": "2023-07-05 18:32:28", "link": "http://arxiv.org/abs/2308.02044v1", "categories": ["cs.DL", "cs.AI", "cs.CL"], "primary_category": "cs.DL"}
{"title": "Flowchase: a Mobile Application for Pronunciation Training", "abstract": "In this paper, we present a solution for providing personalized and instant\nfeedback to English learners through a mobile application, called Flowchase,\nthat is connected to a speech technology able to segment and analyze speech\nsegmental and supra-segmental features. The speech processing pipeline receives\nlinguistic information corresponding to an utterance to analyze along with a\nspeech sample. After validation of the speech sample, a joint forced-alignment\nand phonetic recognition is performed thanks to a combination of machine\nlearning models based on speech representation learning that provides necessary\ninformation for designing a feedback on a series of segmental and\nsupra-segmental pronunciation aspects.", "published": "2023-07-05 06:32:42", "link": "http://arxiv.org/abs/2307.02051v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.HC", "cs.SD"], "primary_category": "eess.AS"}
{"title": "An Exploratory Literature Study on Sharing and Energy Use of Language\n  Models for Source Code", "abstract": "Large language models trained on source code can support a variety of\nsoftware development tasks, such as code recommendation and program repair.\nLarge amounts of data for training such models benefit the models' performance.\nHowever, the size of the data and models results in long training times and\nhigh energy consumption. While publishing source code allows for replicability,\nusers need to repeat the expensive training process if models are not shared.\nThe main goal of the study is to investigate if publications that trained\nlanguage models for software engineering (SE) tasks share source code and\ntrained artifacts. The second goal is to analyze the transparency on training\nenergy usage. We perform a snowballing-based literature search to find\npublications on language models for source code, and analyze their reusability\nfrom a sustainability standpoint.\n  From 494 unique publications, we identified 293 relevant publications that\nuse language models to address code-related tasks. Among them, 27% (79 out of\n293) make artifacts available for reuse. This can be in the form of tools or\nIDE plugins designed for specific tasks or task-agnostic models that can be\nfine-tuned for a variety of downstream tasks. Moreover, we collect insights on\nthe hardware used for model training, as well as training time, which together\ndetermine the energy consumption of the development process. We find that there\nare deficiencies in the sharing of information and artifacts for current\nstudies on source code models for software engineering tasks, with 40% of the\nsurveyed papers not sharing source code or trained artifacts. We recommend the\nsharing of source code as well as trained artifacts, to enable sustainable\nreproducibility. Moreover, comprehensive information on training times and\nhardware configurations should be shared for transparency on a model's carbon\nfootprint.", "published": "2023-07-05 17:13:00", "link": "http://arxiv.org/abs/2307.02443v1", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.SE"}
{"title": "Differentially Private Adversarial Auto-Encoder to Protect Gender in\n  Voice Biometrics", "abstract": "Over the last decade, the use of Automatic Speaker Verification (ASV) systems\nhas become increasingly widespread in response to the growing need for secure\nand efficient identity verification methods. The voice data encompasses a\nwealth of personal information, which includes but is not limited to gender,\nage, health condition, stress levels, and geographical and socio-cultural\norigins. These attributes, known as soft biometrics, are private and the user\nmay wish to keep them confidential. However, with the advancement of machine\nlearning algorithms, soft biometrics can be inferred automatically, creating\nthe potential for unauthorized use. As such, it is crucial to ensure the\nprotection of these personal data that are inherent within the voice while\nretaining the utility of identity recognition. In this paper, we present an\nadversarial Auto-Encoder--based approach to hide gender-related information in\nspeaker embeddings, while preserving their effectiveness for speaker\nverification. We use an adversarial procedure against a gender classifier and\nincorporate a layer based on the Laplace mechanism into the Auto-Encoder\narchitecture. This layer adds Laplace noise for more robust gender concealment\nand ensures differential privacy guarantees during inference for the output\nspeaker embeddings. Experiments conducted on the VoxCeleb dataset demonstrate\nthat speaker verification tasks can be effectively carried out while concealing\nspeaker gender and ensuring differential privacy guarantees; moreover, the\nintensity of the Laplace noise can be tuned to select the desired trade-off\nbetween privacy and utility.", "published": "2023-07-05 09:24:48", "link": "http://arxiv.org/abs/2307.02135v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Online Hybrid CTC/Attention End-to-End Automatic Speech Recognition\n  Architecture", "abstract": "Recently, there has been increasing progress in end-to-end automatic speech\nrecognition (ASR) architecture, which transcribes speech to text without any\npre-trained alignments. One popular end-to-end approach is the hybrid\nConnectionist Temporal Classification (CTC) and attention (CTC/attention) based\nASR architecture. However, how to deploy hybrid CTC/attention systems for\nonline speech recognition is still a non-trivial problem. This article\ndescribes our proposed online hybrid CTC/attention end-to-end ASR architecture,\nwhich replaces all the offline components of conventional CTC/attention ASR\narchitecture with their corresponding streaming components. Firstly, we propose\nstable monotonic chunk-wise attention (sMoChA) to stream the conventional\nglobal attention, and further propose monotonic truncated attention (MTA) to\nsimplify sMoChA and solve the training-and-decoding mismatch problem of sMoChA.\nSecondly, we propose truncated CTC (T-CTC) prefix score to stream CTC prefix\nscore calculation. Thirdly, we design dynamic waiting joint decoding (DWJD)\nalgorithm to dynamically collect the predictions of CTC and attention in an\nonline manner. Finally, we use latency-controlled bidirectional long short-term\nmemory (LC-BLSTM) to stream the widely-used offline bidirectional encoder\nnetwork. Experiments with LibriSpeech English and HKUST Mandarin tasks\ndemonstrate that, compared with the offline CTC/attention model, our proposed\nonline CTC/attention model improves the real time factor in human-computer\ninteraction services and maintains its performance with moderate degradation.\nTo the best of our knowledge, this is the first work to provide the full-stack\nonline solution for CTC/attention end-to-end ASR architecture.", "published": "2023-07-05 15:10:49", "link": "http://arxiv.org/abs/2307.02351v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Deep Speech Synthesis from MRI-Based Articulatory Representations", "abstract": "In this paper, we study articulatory synthesis, a speech synthesis method\nusing human vocal tract information that offers a way to develop efficient,\ngeneralizable and interpretable synthesizers. While recent advances have\nenabled intelligible articulatory synthesis using electromagnetic\narticulography (EMA), these methods lack critical articulatory information like\nexcitation and nasality, limiting generalization capabilities. To bridge this\ngap, we propose an alternative MRI-based feature set that covers a much more\nextensive articulatory space than EMA. We also introduce normalization and\ndenoising procedures to enhance the generalizability of deep learning methods\ntrained on MRI data. Moreover, we propose an MRI-to-speech model that improves\nboth computational efficiency and speech fidelity. Finally, through a series of\nablations, we show that the proposed MRI representation is more comprehensive\nthan EMA and identify the most suitable MRI feature subset for articulatory\nsynthesis.", "published": "2023-07-05 17:45:36", "link": "http://arxiv.org/abs/2307.02471v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "A Database with Directivities of Musical Instruments", "abstract": "We present a database of recordings and radiation patterns of individual\nnotes for 41 modern and historical musical instruments, measured with a\n32-channel spherical microphone array in anechoic conditions. In addition,\ndirectivities averaged in one-third octave bands have been calculated for each\ninstrument, which are suitable for use in acoustic simulation and auralisation.\nThe data are provided in SOFA format. Spatial upsampling of the directivities\nwas performed based on spherical spline interpolation and converted to OpenDAFF\nand GLL format for use in room acoustic and electro-acoustic simulation\nsoftware. For this purpose, a method is presented how these directivities can\nbe referenced to a specific microphone position in order to achieve a\nphysically correct auralisation without colouration. The data is available\nunder the CC BY-SA 4.0 licence.", "published": "2023-07-05 08:37:23", "link": "http://arxiv.org/abs/2307.02110v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Going Retro: Astonishingly Simple Yet Effective Rule-based Prosody\n  Modelling for Speech Synthesis Simulating Emotion Dimensions", "abstract": "We introduce two rule-based models to modify the prosody of speech synthesis\nin order to modulate the emotion to be expressed. The prosody modulation is\nbased on speech synthesis markup language (SSML) and can be used with any\ncommercial speech synthesizer. The models as well as the optimization result\nare evaluated against human emotion annotations. Results indicate that with a\nvery simple method both dimensions arousal (.76 UAR) and valence (.43 UAR) can\nbe simulated.", "published": "2023-07-05 09:20:46", "link": "http://arxiv.org/abs/2307.02132v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Self-supervised learning with diffusion-based multichannel speech\n  enhancement for speaker verification under noisy conditions", "abstract": "The paper introduces Diff-Filter, a multichannel speech enhancement approach\nbased on the diffusion probabilistic model, for improving speaker verification\nperformance under noisy and reverberant conditions. It also presents a new\ntwo-step training procedure that takes the benefit of self-supervised learning.\nIn the first stage, the Diff-Filter is trained by conducting timedomain speech\nfiltering using a scoring-based diffusion model. In the second stage, the\nDiff-Filter is jointly optimized with a pre-trained ECAPA-TDNN speaker\nverification model under a self-supervised learning framework. We present a\nnovel loss based on equal error rate. This loss is used to conduct\nselfsupervised learning on a dataset that is not labelled in terms of speakers.\nThe proposed approach is evaluated on MultiSV, a multichannel speaker\nverification dataset, and shows significant improvements in performance under\nnoisy multichannel conditions.", "published": "2023-07-05 12:36:39", "link": "http://arxiv.org/abs/2307.02244v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Why can big.bi be changed to bi.gbi? A mathematical model of\n  syllabification and articulatory synthesis", "abstract": "A simplified model of articulatory synthesis involving four stages is\npresented. The planning of articulatory gestures is based on syllable graphs\nwith arcs and nodes that are implemented in a complex representation. This was\nfirst motivated by a reduction in the many-to-one relationship between\narticulatory parameters and formant space. This allows for consistent\ntrajectory planning and computation of articulation dynamics with coordination\nand selection operators. The flow of articulatory parameters is derived from\nthese graphs with four equations. Many assertions of Articulatory Phonology\nhave been abandoned. This framework is adapted to synthesis using VLAM (a\nMaeda's model) and simulations are performed with syllables including main\nvowels and the plosives /b,d,g/ only. The model is able to describe\nconsonant-vowel coarticulation, articulation of consonant clusters, and verbal\ntransformations are seen as transitions of the syllable graph structure.", "published": "2023-07-05 13:58:11", "link": "http://arxiv.org/abs/2307.02299v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Exploring Multimodal Approaches for Alzheimer's Disease Detection Using\n  Patient Speech Transcript and Audio Data", "abstract": "Alzheimer's disease (AD) is a common form of dementia that severely impacts\npatient health. As AD impairs the patient's language understanding and\nexpression ability, the speech of AD patients can serve as an indicator of this\ndisease. This study investigates various methods for detecting AD using\npatients' speech and transcripts data from the DementiaBank Pitt database. The\nproposed approach involves pre-trained language models and Graph Neural Network\n(GNN) that constructs a graph from the speech transcript, and extracts features\nusing GNN for AD detection. Data augmentation techniques, including synonym\nreplacement, GPT-based augmenter, and so on, were used to address the small\ndataset size. Audio data was also introduced, and WavLM model was used to\nextract audio features. These features were then fused with text features using\nvarious methods. Finally, a contrastive learning approach was attempted by\nconverting speech transcripts back to audio and using it for contrastive\nlearning with the original audio. We conducted intensive experiments and\nanalysis on the above methods. Our findings shed light on the challenges and\npotential solutions in AD detection using speech and audio data.", "published": "2023-07-05 12:40:11", "link": "http://arxiv.org/abs/2307.02514v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
