{"title": "Multimodal Search on a Line", "abstract": "Inspired by the diverse set of technologies used in underground object\ndetection and imaging, we introduce a novel multimodal linear search problem\nwhereby a single searcher starts at the origin and must find a target that can\nonly be detected when the searcher moves through its location using the correct\nof $p$ possible search modes.\n  The target's location, its distance $d$ from the origin, and the correct\nsearch mode are all initially unknown to the searcher. We prove tight upper and\nlower bounds on the competitive ratio for this problem. Specifically, we show\nthat when $p$ is odd, the optimal competitive ratio is given by\n$2p+3+\\sqrt{8(p+1)}$, whereas when $p$ is even, the optimal competitive ratio\nis given by $c$: the unique solution to $(c-1)^4-4p(c+1)^2(c-p-1)=0$ in the\ninterval $\\left[2p+1+\\sqrt{8p},\\infty\\right)$. This solution $c$ has the\nexplicit bounds $2p+3+\\sqrt{8(p-1)}\\leq c\\leq 2p+3+\\sqrt{8p}$. The optimal\nalgorithms we propose require the searcher to move infinitesimal distances and\nchange directions infinitely many times within finite intervals. To better suit\npractical applications, we also propose an approximation algorithm with a\ncompetitive ratio of $c+\\varepsilon$ (where $c$ is the optimal competitive\nratio and $\\varepsilon > 0$ is an arbitrarily small constant). This algorithm\ninvolves the searcher moving finite distances and changing directions a finite\nnumber of times within any finite interval.", "published": "2025-02-10 19:50:54", "link": "http://arxiv.org/abs/2502.07000v2", "categories": ["cs.DM"], "primary_category": "cs.DM"}
{"title": "Minimal non-comparability graphs and semi-transitivity", "abstract": "The concept of word-representable graphs has been widely explored in the\nliterature. The class of word-representable graphs is characterized by the\nexistence of a semi-transitive orientation. Specifically, a graph is\nword-representable if and only if it admits such an orientation. Comparability\ngraphs form a subclass of word-representable graphs. Both word-representable\nand comparability graphs belong to hereditary graph classes. Every hereditary\nclass can be characterized in terms of their forbidden induced subgraphs. The\nminimal forbidden induced subgraphs of comparability graphs and\nword-representable graphs are referred to as minimal non-comparability graphs\nand minimal non-word-representable graphs, respectively.\n  While the complete set of minimal non-comparability graphs is known,\nidentifying the set of all minimal non-word-representable graphs remains an\nopen problem. In this paper, we precisely determine the set of all minimal\nnon-comparability graphs that are minimal non-word-representable graphs as\nwell. To achieve this, we categorize all minimal non-comparability graphs into\nthose that are semi-transitive and those that are not.\n  Furthermore, as a byproduct of our classification, we establish a\ncharacterization and a complete list of minimal non-word-representable graphs\nthat contain an all-adjacent vertex. This is accomplished by introducing an\nall-adjacent vertex to each minimal non-comparability graph that is\nsemi-transitive. As a result of our study, we identify several infinite\nfamilies of minimal non-word-representable graphs, expanding the understanding\nof their structural properties.", "published": "2025-02-10 19:18:42", "link": "http://arxiv.org/abs/2502.06979v1", "categories": ["cs.DM"], "primary_category": "cs.DM"}
{"title": "The Hajnal--Rothschild problem", "abstract": "For a family $\\mathcal F$ define $\\nu(\\mathcal F,t)$ as the largest $s$ for\nwhich there exist $A_1,\\ldots, A_{s}\\in \\mathcal F$ such that for $i\\ne j$ we\nhave $|A_i\\cap A_j|< t$. What is the largest family $\\mathcal\nF\\subset{[n]\\choose k}$ with $\\nu(\\mathcal F,t)\\le s$? This question goes back\nto a paper Hajnal and Rothschild from 1973. We show that, for some absolute $C$\nand $n>2k+Ct^{4/5}s^{1/5}(k-t)\\log_2^4n$, $n>2k+Cs(k-t)\\log_2^4 n$ the largest\nfamily with $\\nu(\\mathcal F,t)\\le s$ has the following structure: there are\nsets $X_1,\\ldots, X_s$ of sizes $t+2x_1,\\ldots, t+2x_s$, such that for any\n$A\\in \\mathcal F$ there is $i\\in [s]$ such that $|A\\cap X_i|\\ge t+x_i$. That\nis, the extremal constructions are unions of the extremal constructions in the\nComplete $t$-Intersection Theorem. For the proof, we enhance the spread\napproximation technique of Zakharov and the second author. In particular, we\nintroduce the idea of iterative spread approximation.", "published": "2025-02-10 17:24:56", "link": "http://arxiv.org/abs/2502.06699v1", "categories": ["math.CO", "cs.DM"], "primary_category": "math.CO"}
{"title": "Deciding Local Unitary Equivalence of Graph States in Quasi-Polynomial Time", "abstract": "We describe an algorithm with quasi-polynomial runtime $n^{\\log_2(n)+O(1)}$\nfor deciding local unitary (LU) equivalence of graph states. The algorithm\nbuilds on a recent graphical characterisation of LU-equivalence via generalised\nlocal complementation. By first transforming the corresponding graphs into a\nstandard form using usual local complementations, LU-equivalence reduces to the\nexistence of a single generalised local complementation that maps one graph to\nthe other. We crucially demonstrate that this reduces to solving a system of\nquasi-polynomially many linear equations, avoiding an exponential blow-up. As a\nbyproduct, we generalise Bouchet's algorithm for deciding local Clifford (LC)\nequivalence of graph states by allowing the addition of arbitrary linear\nconstraints. We also improve existing bounds on the size of graph states that\nare LU- but not LC-equivalent. While the smallest known examples involve 27\nqubits, and it is established that no such examples exist for up to 8 qubits,\nwe refine this bound by proving that LU- and LC-equivalence coincide for graph\nstates involving up to 19 qubits.", "published": "2025-02-10 15:34:41", "link": "http://arxiv.org/abs/2502.06566v1", "categories": ["quant-ph", "cs.DM"], "primary_category": "quant-ph"}
{"title": "Approximation Algorithms for Optimal Hopsets", "abstract": "For a given graph $G$, a \"hopset\" $H$ with hopbound $\\beta$ and stretch\n$\\alpha$ is a set of edges such that between every pair of vertices $u$ and\n$v$, there is a path with at most $\\beta$ hops in $G \\cup H$ that approximates\nthe distance between $u$ and $v$ up to a multiplicative stretch of $\\alpha$.\nHopsets have found a wide range of applications for distance-based problems in\nvarious computational models since the 90s. More recently, there has been\nsignificant interest in understanding these fundamental objects from an\nexistential and structural perspective. But all of this work takes a worst-case\n(or existential) point of view: How many edges do we need to add to satisfy a\ngiven hopbound and stretch requirement for any input graph?\n  We initiate the study of the natural optimization variant of this problem:\ngiven a specific graph instance, what is the minimum number of edges that\nsatisfy the hopbound and stretch requirements? We give approximation algorithms\nfor a generalized hopset problem which, when combined with known existential\nbounds, lead to different approximation guarantees for various regimes\ndepending on hopbound, stretch, and directed vs. undirected inputs. We\ncomplement our upper bounds with a lower bound that implies Label Cover\nhardness for directed hopsets and shortcut sets with hopbound at least $3$.", "published": "2025-02-10 14:47:41", "link": "http://arxiv.org/abs/2502.06522v1", "categories": ["cs.DS", "cs.DM"], "primary_category": "cs.DS"}
{"title": "Complete Compositional Syntax for Finite Transducers on Finite and Bi-Infinite Words", "abstract": "Minimizing finite automata, proving trace equivalence of labelled transition\nsystems or representing sofic subshifts involve very similar arguments, which\nsuggests the possibility of a unified formalism. We propose finite states\nnon-deterministic transducer as a lingua franca for automata theory, transition\nsystems, and sofic subshifts. We introduce a compositional diagrammatical\nsyntax for transducers in form of string diagrams interpreted as relations.\nThis syntax comes with sound rewriting rules allowing diagrammatical reasoning.\nOur main result is the completeness of our equational theory, ensuring that\nlanguage-equivalence, trace-equivalence, or subshift equivalence can always be\nproved using our rewriting rules.", "published": "2025-02-10 13:27:03", "link": "http://arxiv.org/abs/2502.06450v1", "categories": ["cs.LO", "cs.DM", "cs.FL"], "primary_category": "cs.LO"}
{"title": "On the Expressiveness of Rational ReLU Neural Networks With Bounded Depth", "abstract": "To confirm that the expressive power of ReLU neural networks grows with their\ndepth, the function $F_n = \\max \\{0,x_1,\\ldots,x_n\\}$ has been considered in\nthe literature. A conjecture by Hertrich, Basu, Di Summa, and Skutella [NeurIPS\n2021] states that any ReLU network that exactly represents $F_n$ has at least\n$\\lceil\\log_2 (n+1)\\rceil$ hidden layers. The conjecture has recently been\nconfirmed for networks with integer weights by Haase, Hertrich, and Loho [ICLR\n2023].\n  We follow up on this line of research and show that, within ReLU networks\nwhose weights are decimal fractions, $F_n$ can only be represented by networks\nwith at least $\\lceil\\log_3 (n+1)\\rceil$ hidden layers. Moreover, if all\nweights are $N$-ary fractions, then $F_n$ can only be represented by networks\nwith at least $\\Omega( \\frac{\\ln n}{\\ln \\ln N})$ layers. These results are a\npartial confirmation of the above conjecture for rational ReLU networks, and\nprovide the first non-constant lower bound on the depth of practically relevant\nReLU networks.", "published": "2025-02-10 09:26:35", "link": "http://arxiv.org/abs/2502.06283v2", "categories": ["cs.LG", "cs.DM"], "primary_category": "cs.LG"}
{"title": "TWICE: What Advantages Can Low-Resource Domain-Specific Embedding Model Bring? -- A Case Study on Korea Financial Texts", "abstract": "Domain specificity of embedding models is critical for effective performance.\nHowever, existing benchmarks, such as FinMTEB, are primarily designed for\nhigh-resource languages, leaving low-resource settings, such as Korean,\nunder-explored. Directly translating established English benchmarks often fails\nto capture the linguistic and cultural nuances present in low-resource domains.\nIn this paper, titled TWICE: What Advantages Can Low-Resource Domain-Specific\nEmbedding Models Bring? A Case Study on Korea Financial Texts, we introduce\nKorFinMTEB, a novel benchmark for the Korean financial domain, specifically\ntailored to reflect its unique cultural characteristics in low-resource\nlanguages. Our experimental results reveal that while the models perform\nrobustly on a translated version of FinMTEB, their performance on KorFinMTEB\nuncovers subtle yet critical discrepancies, especially in tasks requiring\ndeeper semantic understanding, that underscore the limitations of direct\ntranslation. This discrepancy highlights the necessity of benchmarks that\nincorporate language-specific idiosyncrasies and cultural nuances. The insights\nfrom our study advocate for the development of domain-specific evaluation\nframeworks that can more accurately assess and drive the progress of embedding\nmodels in low-resource settings.", "published": "2025-02-10 23:49:39", "link": "http://arxiv.org/abs/2502.07131v3", "categories": ["cs.CL", "q-fin.CP"], "primary_category": "cs.CL"}
{"title": "A nested MLMC framework for efficient simulations on FPGAs", "abstract": "Multilevel Monte Carlo (MLMC) reduces the total computational cost of\nfinancial option pricing by combining SDE approximations with multiple\nresolutions. This paper explores a further avenue for reducing cost and\nimproving power efficiency through the use of low precision calculations on\nconfigurable hardware devices such as Field-Programmable Gate Arrays (FPGAs).\nWe propose a new framework that exploits approximate random variables and\nfixed-point operations with optimised precision to generate most SDE paths with\na lower cost and reduce the overall cost of the MLMC framework. We first\ndiscuss several methods for the cheap generation of approximate random Normal\nincrements. To set the bit-width of variables in the path generation we then\npropose a rounding error model and optimise the precision of all variables on\neach MLMC level. With these key improvements, our proposed framework offers\nhigher computational savings than the existing mixed-precision MLMC frameworks.", "published": "2025-02-10 23:36:41", "link": "http://arxiv.org/abs/2502.07123v1", "categories": ["q-fin.CP", "cs.NA", "math.NA", "65C05, 91G60, 65G50, 65C10"], "primary_category": "q-fin.CP"}
{"title": "Is a Peeled Apple Still Red? Evaluating LLMs' Ability for Conceptual\n  Combination with Property Type", "abstract": "Conceptual combination is a cognitive process that merges basic concepts,\nenabling the creation of complex expressions. During this process, the\nproperties of combination (e.g., the whiteness of a peeled apple) can be\ninherited from basic concepts, newly emerge, or be canceled. However, previous\nstudies have evaluated a limited set of properties and have not examined the\ngenerative process. To address this gap, we introduce the Conceptual\nCombination with Property Type dataset (CCPT), which consists of 12.3K\nannotated triplets of noun phrases, properties, and property types. Using CCPT,\nwe establish three types of tasks to evaluate LLMs for conceptual combination\nthoroughly. Our key findings are threefold: (1) Our automatic metric grading\nproperty emergence and cancellation closely corresponds with human judgments.\n(2) LLMs, including OpenAI's o1, struggle to generate noun phrases which\npossess given emergent properties. (3) Our proposed method, inspired by\ncognitive psychology model that explains how relationships between concepts are\nformed, improves performances in all generative tasks. The dataset and\nexperimental code are available at https://github.com/seokwon99/CCPT.git.", "published": "2025-02-10 00:52:17", "link": "http://arxiv.org/abs/2502.06086v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ConMeC: A Dataset for Metonymy Resolution with Common Nouns", "abstract": "Metonymy plays an important role in our daily communication. People naturally\nthink about things using their most salient properties or commonly related\nconcepts. For example, by saying \"The bus decided to skip our stop today,\" we\nactually mean that the bus driver made the decision, not the bus. Prior work on\nmetonymy resolution has mainly focused on named entities. However, metonymy\ninvolving common nouns (such as desk, baby, and school) is also a frequent and\nchallenging phenomenon. We argue that NLP systems should be capable of\nidentifying the metonymic use of common nouns in context. We create a new\nmetonymy dataset ConMeC, which consists of 6,000 sentences, where each sentence\nis paired with a target common noun and annotated by humans to indicate whether\nthat common noun is used metonymically or not in that context. We also\nintroduce a chain-of-thought based prompting method for detecting metonymy\nusing large language models (LLMs). We evaluate our LLM-based pipeline, as well\nas a supervised BERT model on our dataset and three other metonymy datasets.\nOur experimental results demonstrate that LLMs could achieve performance\ncomparable to the supervised BERT model on well-defined metonymy categories,\nwhile still struggling with instances requiring nuanced semantic understanding.\nOur dataset is publicly available at: https://github.com/SaptGhosh/ConMeC.", "published": "2025-02-10 01:04:36", "link": "http://arxiv.org/abs/2502.06087v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LCIRC: A Recurrent Compression Approach for Efficient Long-form Context\n  and Query Dependent Modeling in LLMs", "abstract": "While large language models (LLMs) excel in generating coherent and\ncontextually rich outputs, their capacity to efficiently handle long-form\ncontexts is limited by fixed-length position embeddings. Additionally, the\ncomputational cost of processing long sequences increases quadratically, making\nit challenging to extend context length. To address these challenges, we\npropose Long-form Context Injection with Recurrent Compression (LCIRC), a\nmethod that enables the efficient processing long-form sequences beyond the\nmodel's length limit through recurrent compression without retraining the\nentire model. We further introduce query dependent context modeling, which\nselectively compresses query-relevant information, ensuring that the model\nretains the most pertinent content. Our empirical results demonstrate that\nQuery Dependent LCIRC (QD-LCIRC) significantly improves LLM's ability to manage\nextended contexts, making it well-suited for tasks that require both\ncomprehensive context understanding and query relevance.", "published": "2025-02-10 04:02:18", "link": "http://arxiv.org/abs/2502.06139v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LegalViz: Legal Text Visualization by Text To Diagram Generation", "abstract": "Legal documents including judgments and court orders require highly\nsophisticated legal knowledge for understanding. To disclose expert knowledge\nfor non-experts, we explore the problem of visualizing legal texts with\neasy-to-understand diagrams and propose a novel dataset of LegalViz with 23\nlanguages and 7,010 cases of legal document and visualization pairs, using the\nDOT graph description language of Graphviz. LegalViz provides a simple diagram\nfrom a complicated legal corpus identifying legal entities, transactions, legal\nsources, and statements at a glance, that are essential in each judgment. In\naddition, we provide new evaluation metrics for the legal diagram visualization\nby considering graph structures, textual similarities, and legal contents. We\nconducted empirical studies on few-shot and finetuning large language models\nfor generating legal diagrams and evaluated them with these metrics, including\nlegal content-based evaluation within 23 languages. Models trained with\nLegalViz outperform existing models including GPTs, confirming the\neffectiveness of our dataset.", "published": "2025-02-10 04:25:05", "link": "http://arxiv.org/abs/2502.06147v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Scaling Public Health Text Annotation: Zero-Shot Learning vs.\n  Crowdsourcing for Improved Efficiency and Labeling Accuracy", "abstract": "Public health researchers are increasingly interested in using social media\ndata to study health-related behaviors, but manually labeling this data can be\nlabor-intensive and costly. This study explores whether zero-shot labeling\nusing large language models (LLMs) can match or surpass conventional\ncrowd-sourced annotation for Twitter posts related to sleep disorders, physical\nactivity, and sedentary behavior. Multiple annotation pipelines were designed\nto compare labels produced by domain experts, crowd workers, and LLM-driven\napproaches under varied prompt-engineering strategies. Our findings indicate\nthat LLMs can rival human performance in straightforward classification tasks\nand significantly reduce labeling time, yet their accuracy diminishes for tasks\nrequiring more nuanced domain knowledge. These results clarify the trade-offs\nbetween automated scalability and human expertise, demonstrating conditions\nunder which LLM-based labeling can be efficiently integrated into public health\nresearch without undermining label quality.", "published": "2025-02-10 04:33:27", "link": "http://arxiv.org/abs/2502.06150v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Non-literal Understanding of Number Words by Language Models", "abstract": "Humans naturally interpret numbers non-literally, effortlessly combining\ncontext, world knowledge, and speaker intent. We investigate whether large\nlanguage models (LLMs) interpret numbers similarly, focusing on hyperbole and\npragmatic halo effects. Through systematic comparison with human data and\ncomputational models of pragmatic reasoning, we find that LLMs diverge from\nhuman interpretation in striking ways. By decomposing pragmatic reasoning into\ntestable components, grounded in the Rational Speech Act framework, we pinpoint\nwhere LLM processing diverges from human cognition -- not in prior knowledge,\nbut in reasoning with it. This insight leads us to develop a targeted solution\n-- chain-of-thought prompting inspired by an RSA model makes LLMs'\ninterpretations more human-like. Our work demonstrates how computational\ncognitive models can both diagnose AI-human differences and guide development\nof more human-like language understanding capabilities.", "published": "2025-02-10 07:03:00", "link": "http://arxiv.org/abs/2502.06204v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Latent Convergence Modulation in Large Language Models: A Novel Approach\n  to Iterative Contextual Realignment", "abstract": "Token prediction stability remains a challenge in autoregressive generative\nmodels, where minor variations in early inference steps often lead to\nsignificant semantic drift over extended sequences. A structured modulation\nmechanism was introduced to regulate hidden state transitions, ensuring that\nlatent representation trajectories remain aligned with prior contextual\ndependencies while preserving generative flexibility. The modulation framework\nwas designed to function within transformer-based architectures, dynamically\nconstraining representation evolution without imposing external memory\ndependencies or extensive architectural modifications. Empirical evaluations\ndemonstrated that structured latent adjustments contributed to reductions in\nperplexity fluctuations, entropy variance, and lexical instability, improving\ncoherence in long-form text generation. Gradient propagation stability was\nfurther analyzed, revealing that the modulation process led to smoother\noptimization pathways, mitigating erratic fluctuations in weight updates across\nsuccessive inference steps. The computational efficiency of the modulation\nprocess was assessed, showing that its integration within transformer-based\narchitectures introduced only marginal overhead while maintaining compatibility\nwith existing optimization frameworks. The structured modulation constraints\nalso influenced syntactic variation, preventing excessive repetition while\nmaintaining balanced sentence length distributions. Comparative evaluations\nagainst baseline models reinforced the role of controlled latent state\nevolution in improving pronoun resolution, logical consistency, and contextual\nalignment across autoregressive text generation tasks.", "published": "2025-02-10 09:46:33", "link": "http://arxiv.org/abs/2502.06302v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can AI Examine Novelty of Patents?: Novelty Evaluation Based on the\n  Correspondence between Patent Claim and Prior Art", "abstract": "Assessing the novelty of patent claims is a critical yet challenging task\ntraditionally performed by patent examiners. While advancements in NLP have\nenabled progress in various patent-related tasks, novelty assessment remains\nunexplored. This paper introduces a novel challenge by evaluating the ability\nof large language models (LLMs) to assess patent novelty by comparing claims\nwith cited prior art documents, following the process similar to that of patent\nexaminers done. We present the first dataset specifically designed for novelty\nevaluation, derived from real patent examination cases, and analyze the\ncapabilities of LLMs to address this task. Our study reveals that while\nclassification models struggle to effectively assess novelty, generative models\nmake predictions with a reasonable level of accuracy, and their explanations\nare accurate enough to understand the relationship between the target patent\nand prior art. These findings demonstrate the potential of LLMs to assist in\npatent evaluation, reducing the workload for both examiners and applicants. Our\ncontributions highlight the limitations of current models and provide a\nfoundation for improving AI-driven patent analysis through advanced models and\nrefined datasets.", "published": "2025-02-10 10:09:29", "link": "http://arxiv.org/abs/2502.06316v1", "categories": ["cs.CL", "68T50"], "primary_category": "cs.CL"}
{"title": "Expect the Unexpected: FailSafe Long Context QA for Finance", "abstract": "We propose a new long-context financial benchmark, FailSafeQA, designed to\ntest the robustness and context-awareness of LLMs against six variations in\nhuman-interface interactions in LLM-based query-answer systems within finance.\nWe concentrate on two case studies: Query Failure and Context Failure. In the\nQuery Failure scenario, we perturb the original query to vary in domain\nexpertise, completeness, and linguistic accuracy. In the Context Failure case,\nwe simulate the uploads of degraded, irrelevant, and empty documents. We employ\nthe LLM-as-a-Judge methodology with Qwen2.5-72B-Instruct and use fine-grained\nrating criteria to define and calculate Robustness, Context Grounding, and\nCompliance scores for 24 off-the-shelf models. The results suggest that\nalthough some models excel at mitigating input perturbations, they must balance\nrobust answering with the ability to refrain from hallucinating. Notably,\nPalmyra-Fin-128k-Instruct, recognized as the most compliant model, maintained\nstrong baseline performance but encountered challenges in sustaining robust\npredictions in 17% of test cases. On the other hand, the most robust model,\nOpenAI o3-mini, fabricated information in 41% of tested cases. The results\ndemonstrate that even high-performing models have significant room for\nimprovement and highlight the role of FailSafeQA as a tool for developing LLMs\noptimized for dependability in financial applications. The dataset is available\nat: https://huggingface.co/datasets/Writer/FailSafeQA", "published": "2025-02-10 10:29:28", "link": "http://arxiv.org/abs/2502.06329v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SynthDetoxM: Modern LLMs are Few-Shot Parallel Detoxification Data\n  Annotators", "abstract": "Existing approaches to multilingual text detoxification are hampered by the\nscarcity of parallel multilingual datasets. In this work, we introduce a\npipeline for the generation of multilingual parallel detoxification data. We\nalso introduce SynthDetoxM, a manually collected and synthetically generated\nmultilingual parallel text detoxification dataset comprising 16,000\nhigh-quality detoxification sentence pairs across German, French, Spanish and\nRussian. The data was sourced from different toxicity evaluation datasets and\nthen rewritten with nine modern open-source LLMs in few-shot setting. Our\nexperiments demonstrate that models trained on the produced synthetic datasets\nhave superior performance to those trained on the human-annotated\nMultiParaDetox dataset even in data limited setting. Models trained on\nSynthDetoxM outperform all evaluated LLMs in few-shot setting. We release our\ndataset and code to help further research in multilingual text detoxification.", "published": "2025-02-10 12:30:25", "link": "http://arxiv.org/abs/2502.06394v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Literal Token Overlap: Token Alignability for Multilinguality", "abstract": "Previous work has considered token overlap, or even similarity of token\ndistributions, as predictors for multilinguality and cross-lingual knowledge\ntransfer in language models. However, these very literal metrics assign large\ndistances to language pairs with different scripts, which can nevertheless show\ngood cross-linguality. This limits the explanatory strength of token overlap\nfor knowledge transfer between language pairs that use distinct scripts or\nfollow different orthographic conventions. In this paper, we propose subword\ntoken alignability as a new way to understand the impact and quality of\nmultilingual tokenisation. In particular, this metric predicts multilinguality\nmuch better when scripts are disparate and the overlap of literal tokens is\nlow. We analyse this metric in the context of both encoder and decoder models,\nlook at data size as a potential distractor, and discuss how this insight may\nbe applied to multilingual tokenisation in future work. We recommend our\nsubword token alignability metric for identifying optimal language pairs for\ncross-lingual transfer, as well as to guide the construction of better\nmultilingual tokenisers in the future. We publish our code and reproducibility\ndetails.", "published": "2025-02-10 13:50:12", "link": "http://arxiv.org/abs/2502.06468v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adaptive Prompting: Ad-hoc Prompt Composition for Social Bias Detection", "abstract": "Recent advances on instruction fine-tuning have led to the development of\nvarious prompting techniques for large language models, such as explicit\nreasoning steps. However, the success of techniques depends on various\nparameters, such as the task, language model, and context provided. Finding an\neffective prompt is, therefore, often a trial-and-error process. Most existing\napproaches to automatic prompting aim to optimize individual techniques instead\nof compositions of techniques and their dependence on the input. To fill this\ngap, we propose an adaptive prompting approach that predicts the optimal prompt\ncomposition ad-hoc for a given input. We apply our approach to social bias\ndetection, a highly context-dependent task that requires semantic\nunderstanding. We evaluate it with three large language models on three\ndatasets, comparing compositions to individual techniques and other baselines.\nThe results underline the importance of finding an effective prompt\ncomposition. Our approach robustly ensures high detection performance, and is\nbest in several settings. Moreover, first experiments on other tasks support\nits generalizability.", "published": "2025-02-10 14:06:19", "link": "http://arxiv.org/abs/2502.06487v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Scientific Full Text Classification: The Case of EICAT Impact\n  Assessments", "abstract": "This study explores strategies for efficiently classifying scientific full\ntexts using both small, BERT-based models and local large language models like\nLlama-3.1 8B. We focus on developing methods for selecting subsets of input\nsentences to reduce input size while simultaneously enhancing classification\nperformance. To this end, we compile a novel dataset consisting of full-text\nscientific papers from the field of invasion biology, specifically addressing\nthe impacts of invasive species. These papers are aligned with publicly\navailable impact assessments created by researchers for the International Union\nfor Conservation of Nature (IUCN). Through extensive experimentation, we\ndemonstrate that various sources like human evidence annotations, LLM-generated\nannotations or explainability scores can be used to train sentence selection\nmodels that improve the performance of both encoder- and decoder-based language\nmodels while optimizing efficiency through the reduction in input length,\nleading to improved results even if compared to models like ModernBERT that are\nable to handle the complete text as input. Additionally, we find that repeated\nsampling of shorter inputs proves to be a very effective strategy that, at a\nslightly increased cost, can further improve classification performance.", "published": "2025-02-10 15:19:22", "link": "http://arxiv.org/abs/2502.06551v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models Meet Symbolic Provers for Logical Reasoning\n  Evaluation", "abstract": "First-order logic (FOL) reasoning, which involves sequential deduction, is\npivotal for intelligent systems and serves as a valuable task for evaluating\nreasoning capabilities, particularly in chain-of-thought (CoT) contexts.\nExisting benchmarks often rely on extensive human annotation or handcrafted\ntemplates, making it difficult to achieve the necessary complexity,\nscalability, and diversity for robust evaluation. To address these limitations,\nwe propose a novel framework called ProverGen that synergizes the generative\nstrengths of Large Language Models (LLMs) with the rigor and precision of\nsymbolic provers, enabling the creation of a scalable, diverse, and\nhigh-quality FOL reasoning dataset, ProverQA. ProverQA is also distinguished by\nits inclusion of accessible and logically coherent intermediate reasoning steps\nfor each problem. Our evaluation shows that state-of-the-art LLMs struggle to\nsolve ProverQA problems, even with CoT prompting, highlighting the dataset's\nchallenging nature. We also finetune Llama3.1-8B-Instruct on a separate\ntraining set generated by our framework. The finetuned model demonstrates\nconsistent improvements on both in-distribution and out-of-distribution test\nsets, suggesting the value of our proposed data generation framework. Code\navailable at: https://github.com/opendatalab/ProverGen", "published": "2025-02-10 15:31:54", "link": "http://arxiv.org/abs/2502.06563v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do we really have to filter out random noise in pre-training data for\n  language models?", "abstract": "Web-scale pre-training datasets are the cornerstone of LLMs' success.\nHowever, text data curated from the internet inevitably contains random noise\ncaused by decoding errors or unregulated web content. In contrast to previous\nworks that focus on low quality or synthetic data, our study \\textbf{provides\nthe first systematic investigation into such random noise through a cohesive\n``What-Why-How'' framework.} Surprisingly, we observed that the resulting\nincrease in next-token prediction (NTP) loss was significantly lower than the\nproportion of random noise. We provide a theoretical justification for this\nphenomenon, which also elucidates the success of multilingual models. On the\nother hand, experiments show that the model's performance in downstream tasks\nis not based solely on the NTP loss, which means that random noise may result\nin degraded downstream performance. To address the potential adverse effects,\nwe introduce a novel plug-and-play Local Gradient Matching loss, which\nexplicitly enhances the denoising capability of the downstream task head by\naligning the gradient of normal and perturbed features without requiring\nknowledge of the model's parameters. Additional experiments on 8 language and\n14 vision benchmarks further validate its effectiveness.", "published": "2025-02-10 16:01:55", "link": "http://arxiv.org/abs/2502.06604v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Scaling Multi-Document Event Summarization: Evaluating Compression vs.\n  Full-Text Approaches", "abstract": "Automatically summarizing large text collections is a valuable tool for\ndocument research, with applications in journalism, academic research, legal\nwork, and many other fields. In this work, we contrast two classes of systems\nfor large-scale multi-document summarization (MDS): compression and full-text.\nCompression-based methods use a multi-stage pipeline and often lead to lossy\nsummaries. Full-text methods promise a lossless summary by relying on recent\nadvances in long-context reasoning. To understand their utility on large-scale\nMDS, we evaluated them on three datasets, each containing approximately one\nhundred documents per summary. Our experiments cover a diverse set of\nlong-context transformers (Llama-3.1, Command-R, Jamba-1.5-Mini) and\ncompression methods (retrieval-augmented, hierarchical, incremental). Overall,\nwe find that full-text and retrieval methods perform the best in most settings.\nWith further analysis into the salient information retention patterns, we show\nthat compression-based methods show strong promise at intermediate stages, even\noutperforming full-context. However, they suffer information loss due to their\nmulti-stage pipeline and lack of global context. Our results highlight the need\nto develop hybrid approaches that combine compression and full-text approaches\nfor optimal performance on large-scale multi-document summarization.", "published": "2025-02-10 16:15:08", "link": "http://arxiv.org/abs/2502.06617v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transparent NLP: Using RAG and LLM Alignment for Privacy Q&A", "abstract": "The transparency principle of the General Data Protection Regulation (GDPR)\nrequires data processing information to be clear, precise, and accessible.\nWhile language models show promise in this context, their probabilistic nature\ncomplicates truthfulness and comprehensibility.\n  This paper examines state-of-the-art Retrieval Augmented Generation (RAG)\nsystems enhanced with alignment techniques to fulfill GDPR obligations. We\nevaluate RAG systems incorporating an alignment module like Rewindable\nAuto-regressive Inference (RAIN) and our proposed multidimensional extension,\nMultiRAIN, using a Privacy Q&A dataset. Responses are optimized for preciseness\nand comprehensibility and are assessed through 21 metrics, including\ndeterministic and large language model-based evaluations.\n  Our results show that RAG systems with an alignment module outperform\nbaseline RAG systems on most metrics, though none fully match human answers.\nPrincipal component analysis of the results reveals complex interactions\nbetween metrics, highlighting the need to refine metrics. This study provides a\nfoundation for integrating advanced natural language processing systems into\nlegal compliance frameworks.", "published": "2025-02-10 16:42:00", "link": "http://arxiv.org/abs/2502.06652v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "In-Context Learning (and Unlearning) of Length Biases", "abstract": "Large language models have demonstrated strong capabilities to learn\nin-context, where exemplar input-output pairings are appended to the prompt for\ndemonstration. However, existing work has demonstrated the ability of models to\nlearn lexical and label biases in-context, which negatively impacts both\nperformance and robustness of models. The impact of other statistical data\nbiases remains under-explored, which this work aims to address. We specifically\ninvestigate the impact of length biases on in-context learning. We demonstrate\nthat models do learn length biases in the context window for their predictions,\nand further empirically analyze the factors that modulate the level of bias\nexhibited by the model. In addition, we show that learning length information\nin-context can be used to counter the length bias that has been encoded in\nmodels (e.g., via fine-tuning). This reveals the power of in-context learning\nin debiasing model prediction behaviors without the need for costly parameter\nupdates.", "published": "2025-02-10 16:43:32", "link": "http://arxiv.org/abs/2502.06653v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Who Taught You That? Tracing Teachers in Model Distillation", "abstract": "Model distillation -- using outputs from a large teacher model to teach a\nsmall student model -- is a practical means of creating efficient models for a\nparticular task. We ask: Can we identify a students' teacher based on its\noutputs? Such \"footprints\" left by teacher LLMs would be interesting artifacts.\nBeyond this, reliable teacher inference may have practical implications as\nactors seek to distill specific capabilities of massive proprietary LLMs into\ndeployed smaller LMs, potentially violating terms of service. We consider\npractical task distillation targets including summarization, question\nanswering, and instruction-following. We assume a finite set of candidate\nteacher models, which we treat as blackboxes. We design discriminative models\nthat operate over lexical features. We find that $n$-gram similarity alone is\nunreliable for identifying teachers, but part-of-speech (PoS) templates\npreferred by student models mimic those of their teachers.", "published": "2025-02-10 16:48:56", "link": "http://arxiv.org/abs/2502.06659v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time\n  Scaling", "abstract": "Test-Time Scaling (TTS) is an important method for improving the performance\nof Large Language Models (LLMs) by using additional computation during the\ninference phase. However, current studies do not systematically analyze how\npolicy models, Process Reward Models (PRMs), and problem difficulty influence\nTTS. This lack of analysis limits the understanding and practical use of TTS\nmethods. In this paper, we focus on two core questions: (1) What is the optimal\napproach to scale test-time computation across different policy models, PRMs,\nand problem difficulty levels? (2) To what extent can extended computation\nimprove the performance of LLMs on complex tasks, and can smaller language\nmodels outperform larger ones through this approach? Through comprehensive\nexperiments on MATH-500 and challenging AIME24 tasks, we have the following\nobservations: (1) The compute-optimal TTS strategy is highly dependent on the\nchoice of policy model, PRM, and problem difficulty. (2) With our\ncompute-optimal TTS strategy, extremely small policy models can outperform\nlarger models. For example, a 1B LLM can exceed a 405B LLM on MATH-500.\nMoreover, on both MATH-500 and AIME24, a 0.5B LLM outperforms GPT-4o, a 3B LLM\nsurpasses a 405B LLM, and a 7B LLM beats o1 and DeepSeek-R1, while with higher\ninference efficiency. These findings show the significance of adapting TTS\nstrategies to the specific characteristics of each task and model and indicate\nthat TTS is a promising approach for enhancing the reasoning abilities of LLMs.", "published": "2025-02-10 17:30:23", "link": "http://arxiv.org/abs/2502.06703v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploiting Sparsity for Long Context Inference: Million Token Contexts\n  on Commodity GPUs", "abstract": "There is growing demand for performing inference with hundreds of thousands\nof input tokens on trained transformer models. Inference at this extreme scale\ndemands significant computational resources, hindering the application of\ntransformers at long contexts on commodity (i.e not data center scale)\nhardware. To address the inference time costs associated with running\nself-attention based transformer language models on long contexts and enable\ntheir adoption on widely available hardware, we propose a tunable mechanism\nthat reduces the cost of the forward pass by attending to only the most\nrelevant tokens at every generation step using a top-k selection mechanism. We\nshowcase the efficiency gains afforded by our method by performing inference on\ncontext windows up to 1M tokens using approximately 16GB of GPU RAM. Our\nexperiments reveal that models are capable of handling the sparsity induced by\nthe reduced number of keys and values. By attending to less than 2% of input\ntokens, we achieve over 95% of model performance on common benchmarks (RULER,\nAlpacaEval, and Open LLM Leaderboard).", "published": "2025-02-10 18:47:04", "link": "http://arxiv.org/abs/2502.06766v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating the Zone of Proximal Development of Language Models for\n  In-Context Learning", "abstract": "In this paper, we introduce a learning analytics framework to analyze the\nin-context learning (ICL) behavior of large language models (LLMs) through the\nlens of the Zone of Proximal Development (ZPD), an established theory in\neducational psychology. ZPD delineates the space between what a learner is\ncapable of doing unsupported and what the learner cannot do even with support.\nWe adapt this concept to ICL, measuring the ZPD of LLMs based on model\nperformance on individual examples with and without ICL. Furthermore, we\npropose an item response theory (IRT) model to predict the distribution of\nzones for LLMs. Our findings reveal a series of intricate and multifaceted\nbehaviors of ICL, providing new insights into understanding and leveraging this\ntechnique. Finally, we demonstrate how our framework can enhance LLM in both\ninference and fine-tuning scenarios: (1) By predicting a model's zone of\nproximal development, we selectively apply ICL to queries that are most likely\nto benefit from demonstrations, achieving a better balance between inference\ncost and performance; (2) We propose a human-like curriculum for fine-tuning,\nwhich prioritizes examples within the model's ZPD. The curriculum results in\nimproved performance, and we explain its effectiveness through an analysis of\nthe training dynamics of LLMs.", "published": "2025-02-10 19:36:21", "link": "http://arxiv.org/abs/2502.06990v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Demystifying Singular Defects in Large Language Models", "abstract": "Large transformer models are known to produce high-norm tokens. In vision\ntransformers (ViTs), such tokens have been mathematically modeled through the\nsingular vectors of the linear approximations of layers. However, in large\nlanguage models (LLMs), the underlying causes of high-norm tokens remain\nlargely unexplored, and their different properties from those of ViTs require a\nnew analysis framework. In this paper, we provide both theoretical insights and\nempirical validation across a range of recent models, leading to the following\nobservations: i) The layer-wise singular direction predicts the abrupt\nexplosion of token norms in LLMs. ii) The negative eigenvalues of a layer\nexplain its sudden decay. iii) The computational pathways leading to high-norm\ntokens differ between initial and noninitial tokens. iv) High-norm tokens are\ntriggered by the right leading singular vector of the matrix approximating the\ncorresponding modules. We showcase two practical applications of these\nfindings: the improvement of quantization schemes and the design of LLM\nsignatures. Our findings not only advance the understanding of singular defects\nin LLMs but also open new avenues for their application. We expect that this\nwork will stimulate further research into the internal mechanisms of LLMs and\nwill therefore publicly release our code.", "published": "2025-02-10 20:09:16", "link": "http://arxiv.org/abs/2502.07004v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tokenization Standards for Linguistic Integrity: Turkish as a Benchmark", "abstract": "Tokenization is a fundamental preprocessing step in NLP, directly impacting\nlarge language models' (LLMs) ability to capture syntactic, morphosyntactic,\nand semantic structures. This paper introduces a novel framework for\nsystematically evaluating tokenization strategies, addressing challenges in\nmorphologically rich and low-resource languages. Using a Turkish dataset of\n6,200 multiple-choice questions from the Massive Multitask Language\nUnderstanding (MMLU) benchmark, the framework assesses tokenizers across five\nkey metrics: vocabulary size, token count, processing time, language-specific\ntoken percentages (\\%TR), and token purity. These metrics provide a structured\napproach to evaluating how well tokenizers preserve linguistic structures.\nWhile \\%TR measures the proportion of valid words in the target language,\n\\%Pure assesses the alignment of tokens with meaningful linguistic units, such\nas roots and valid morphemes, minimizing semantic fragmentation. The findings\nreveal that \\%TR, introduced as a critical metric, exhibits a stronger\ncorrelation with downstream performance (e.g., MMLU scores) than token purity,\nemphasizing its role in improving model accuracy. Additionally, larger model\nparameters do not necessarily yield better tokenization quality or enhanced\nresults, highlighting the importance of tailored tokenization strategies that\nprioritize linguistic alignment. This framework sets a new standard for\ndeveloping robust tokenization methods optimized for morphologically complex\nand low-resource languages. Future work will refine morphological analysis,\nexplore domain-specific customizations, and conduct cross-linguistic\nevaluations to further enhance tokenization practices.", "published": "2025-02-10 21:47:49", "link": "http://arxiv.org/abs/2502.07057v1", "categories": ["cs.CL", "68T50, 68T10", "I.2.7; I.2.6; H.3.1"], "primary_category": "cs.CL"}
{"title": "Specializing Large Language Models to Simulate Survey Response\n  Distributions for Global Populations", "abstract": "Large-scale surveys are essential tools for informing social science research\nand policy, but running surveys is costly and time-intensive. If we could\naccurately simulate group-level survey results, this would therefore be very\nvaluable to social science research. Prior work has explored the use of large\nlanguage models (LLMs) for simulating human behaviors, mostly through\nprompting. In this paper, we are the first to specialize LLMs for the task of\nsimulating survey response distributions. As a testbed, we use country-level\nresults from two global cultural surveys. We devise a fine-tuning method based\non first-token probabilities to minimize divergence between predicted and\nactual response distributions for a given question. Then, we show that this\nmethod substantially outperforms other methods and zero-shot classifiers, even\non unseen questions, countries, and a completely unseen survey. While even our\nbest models struggle with the task, especially on unseen questions, our results\ndemonstrate the benefits of specialization for simulation, which may accelerate\nprogress towards sufficiently accurate simulation in the future.", "published": "2025-02-10 21:59:27", "link": "http://arxiv.org/abs/2502.07068v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"Once Upon a Time...\" Literary Narrative Connectedness Progresses with\n  Grade Level: Potential Impact on Reading Fluency and Literacy Skills", "abstract": "Selecting an appropriate book is crucial for fostering reading habits in\nchildren. While children exhibit varying levels of complexity when generating\noral narratives, the question arises: do children's books also differ in\nnarrative complexity? This study explores the narrative dynamics of literary\ntexts used in schools, focusing on how their complexity evolves across\ndifferent grade levels. Using Word-Recurrence Graph Analysis, we examined a\ndataset of 1,627 literary texts spanning 13 years of education. The findings\nreveal significant exponential growth in connectedness, particularly during the\nfirst three years of schooling, mirroring patterns observed in children's oral\nnarratives. These results highlight the potential of literary texts as a tool\nto support the development of literacy skills.", "published": "2025-02-10 22:21:29", "link": "http://arxiv.org/abs/2502.07082v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SMAB: MAB based word Sensitivity Estimation Framework and its\n  Applications in Adversarial Text Generation", "abstract": "To understand the complexity of sequence classification tasks, Hahn et al.\n(2021) proposed sensitivity as the number of disjoint subsets of the input\nsequence that can each be individually changed to change the output. Though\neffective, calculating sensitivity at scale using this framework is costly\nbecause of exponential time complexity. Therefore, we introduce a\nSensitivity-based Multi-Armed Bandit framework (SMAB), which provides a\nscalable approach for calculating word-level local (sentence-level) and global\n(aggregated) sensitivities concerning an underlying text classifier for any\ndataset. We establish the effectiveness of our approach through various\napplications. We perform a case study on CHECKLIST generated sentiment analysis\ndataset where we show that our algorithm indeed captures intuitively high and\nlow-sensitive words. Through experiments on multiple tasks and languages, we\nshow that sensitivity can serve as a proxy for accuracy in the absence of gold\ndata. Lastly, we show that guiding perturbation prompts using sensitivity\nvalues in adversarial example generation improves attack success rate by\n15.58%, whereas using sensitivity as an additional reward in adversarial\nparaphrase generation gives a 12.00% improvement over SOTA approaches. Warning:\nContains potentially offensive content.", "published": "2025-02-10 22:46:57", "link": "http://arxiv.org/abs/2502.07101v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Structural Reformation of Large Language Model Neuron Encapsulation for\n  Divergent Information Aggregation", "abstract": "Structured neuron encapsulation introduces a modular framework that enables\nmore effective aggregation and specialization of information within deep\nlearning architectures. A model modified through this framework demonstrated\nimproved perplexity scores, greater lexical variability, and enhanced\nconsistency in logical reasoning, suggesting that structured parameter\ndistribution contributes to more efficient language representation. Statistical\nanalyses of generated text highlighted a wider range of sentence structures and\nreduced redundancy in token selection, indicating that encapsulation fosters\nmore adaptable language generation. A detailed evaluation of attention weight\ndistributions revealed that the experimental model exhibited greater divergence\nin cross-layer activations, supporting the hypothesis that encapsulated neurons\nassume specialized processing roles. Logical consistency assessments further\ndemonstrated that modular architectures mitigate contradictory outputs,\nreducing internal conflicts in inferred relationships between linguistic\nconstructs. Computational trade-offs were analyzed, with results showing a\nminor increase in processing overhead, though improvements in parameter\nefficiency and structured decision-making compensated for the additional\ncomplexity. The mathematical formulation of the encapsulation mechanism\nconfirmed that modular aggregation maintains stable convergence properties\nwhile promoting distinct functional roles for different neuron clusters.", "published": "2025-02-10 23:37:39", "link": "http://arxiv.org/abs/2502.07124v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RALLRec: Improving Retrieval Augmented Large Language Model\n  Recommendation with Representation Learning", "abstract": "Large Language Models (LLMs) have been integrated into recommendation systems\nto enhance user behavior comprehension. The Retrieval Augmented Generation\n(RAG) technique is further incorporated into these systems to retrieve more\nrelevant items and improve system performance. However, existing RAG methods\nrely primarily on textual semantics and often fail to incorporate the most\nrelevant items, limiting the effectiveness of the systems.\n  In this paper, we propose Representation learning for retrieval-Augmented\nLarge Language model Recommendation (RALLRec). Specifically, we enhance textual\nsemantics by prompting LLMs to generate more detailed item descriptions,\nfollowed by joint representation learning of textual and collaborative\nsemantics, which are extracted by the LLM and recommendation models,\nrespectively. Considering the potential time-varying characteristics of user\ninterest, a simple yet effective reranking method is further introduced to\ncapture the dynamics of user preference. We conducted extensive experiments on\nthree real-world datasets, and the evaluation results validated the\neffectiveness of our method. Code is made public at\nhttps://github.com/JianXu95/RALLRec.", "published": "2025-02-10 02:15:12", "link": "http://arxiv.org/abs/2502.06101v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Task-driven Layerwise Additive Activation Intervention", "abstract": "Modern language models (LMs) have significantly advanced generative modeling\nin natural language processing (NLP). Despite their success, LMs often struggle\nwith adaptation to new contexts in real-time applications. A promising approach\nto task adaptation is activation intervention, which steers the LMs' generation\nprocess by identifying and manipulating the activations. However, existing\ninterventions are highly dependent on heuristic rules or require many prompt\ninputs to determine effective interventions. This paper proposes a layer-wise\nadditive activation intervention framework that optimizes the intervention\nprocess, thus enhancing the sample efficiency. We benchmark our framework on\nvarious datasets, demonstrating improvements in the accuracy of pre-trained LMs\nand competing intervention baselines.", "published": "2025-02-10 02:49:46", "link": "http://arxiv.org/abs/2502.06115v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Self-Correcting Decoding with Generative Feedback for Mitigating\n  Hallucinations in Large Vision-Language Models", "abstract": "While recent Large Vision-Language Models (LVLMs) have shown remarkable\nperformance in multi-modal tasks, they are prone to generating hallucinatory\ntext responses that do not align with the given visual input, which restricts\ntheir practical applicability in real-world scenarios. In this work, inspired\nby the observation that the text-to-image generation process is the inverse of\nimage-conditioned response generation in LVLMs, we explore the potential of\nleveraging text-to-image generative models to assist in mitigating\nhallucinations in LVLMs. We discover that generative models can offer valuable\nself-feedback for mitigating hallucinations at both the response and token\nlevels. Building on this insight, we introduce self-correcting Decoding with\nGenerative Feedback (DeGF), a novel training-free algorithm that incorporates\nfeedback from text-to-image generative models into the decoding process to\neffectively mitigate hallucinations in LVLMs. Specifically, DeGF generates an\nimage from the initial response produced by LVLMs, which acts as an auxiliary\nvisual reference and provides self-feedback to verify and correct the initial\nresponse through complementary or contrastive decoding. Extensive experimental\nresults validate the effectiveness of our approach in mitigating diverse types\nof hallucinations, consistently surpassing state-of-the-art methods across six\nbenchmarks. Code is available at https://github.com/zhangce01/DeGF.", "published": "2025-02-10 03:43:55", "link": "http://arxiv.org/abs/2502.06130v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Enhancing Document Key Information Localization Through Data\n  Augmentation", "abstract": "The Visually Rich Form Document Intelligence and Understanding (VRDIU) Track\nB focuses on the localization of key information in document images. The goal\nis to develop a method capable of localizing objects in both digital and\nhandwritten documents, using only digital documents for training. This paper\npresents a simple yet effective approach that includes a document augmentation\nphase and an object detection phase. Specifically, we augment the training set\nof digital documents by mimicking the appearance of handwritten documents. Our\nexperiments demonstrate that this pipeline enhances the models' generalization\nability and achieves high performance in the competition.", "published": "2025-02-10 03:46:39", "link": "http://arxiv.org/abs/2502.06132v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Optimizing Knowledge Integration in Retrieval-Augmented Generation with\n  Self-Selection", "abstract": "Retrieval-Augmented Generation (RAG), which integrates external knowledge\ninto Large Language Models (LLMs), has proven effective in enabling LLMs to\nproduce more accurate and reliable responses. However, it remains a significant\nchallenge how to effectively integrate external retrieved knowledge with\ninternal parametric knowledge in LLMs. In this work, we propose a novel\nSelf-Selection RAG framework, where the LLM is made to select from pairwise\nresponses generated with internal parametric knowledge solely and with external\nretrieved knowledge together to achieve enhanced accuracy. To this end, we\ndevise a Self-Selection-RGP method to enhance the capabilities of the LLM in\nboth generating and selecting the correct answer, by training the LLM with\nDirect Preference Optimization (DPO) over a curated Retrieval Generation\nPreference (RGP) dataset. Experimental results with two open-source LLMs (i.e.,\nLlama2-13B-Chat and Mistral-7B) well demonstrate the superiority of our\napproach over other baseline methods on Natural Questions (NQ) and TrivialQA\ndatasets.", "published": "2025-02-10 04:29:36", "link": "http://arxiv.org/abs/2502.06148v1", "categories": ["cs.CL", "cs.IR", "68"], "primary_category": "cs.CL"}
{"title": "RideKE: Leveraging Low-Resource, User-Generated Twitter Content for\n  Sentiment and Emotion Detection in Kenyan Code-Switched Dataset", "abstract": "Social media has become a crucial open-access platform for individuals to\nexpress opinions and share experiences. However, leveraging low-resource\nlanguage data from Twitter is challenging due to scarce, poor-quality content\nand the major variations in language use, such as slang and code-switching.\nIdentifying tweets in these languages can be difficult as Twitter primarily\nsupports high-resource languages. We analyze Kenyan code-switched data and\nevaluate four state-of-the-art (SOTA) transformer-based pretrained models for\nsentiment and emotion classification, using supervised and semi-supervised\nmethods. We detail the methodology behind data collection and annotation, and\nthe challenges encountered during the data curation phase. Our results show\nthat XLM-R outperforms other models; for sentiment analysis, XLM-R supervised\nmodel achieves the highest accuracy (69.2\\%) and F1 score (66.1\\%), XLM-R\nsemi-supervised (67.2\\% accuracy, 64.1\\% F1 score). In emotion analysis,\nDistilBERT supervised leads in accuracy (59.8\\%) and F1 score (31\\%), mBERT\nsemi-supervised (accuracy (59\\% and F1 score 26.5\\%). AfriBERTa models show the\nlowest accuracy and F1 scores. All models tend to predict neutral sentiment,\nwith Afri-BERT showing the highest bias and unique sensitivity to empathy\nemotion. https://github.com/NEtori21/Ride_hailing", "published": "2025-02-10 06:18:07", "link": "http://arxiv.org/abs/2502.06180v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Discourse-Driven Evaluation: Unveiling Factual Inconsistency in Long\n  Document Summarization", "abstract": "Detecting factual inconsistency for long document summarization remains\nchallenging, given the complex structure of the source article and long summary\nlength. In this work, we study factual inconsistency errors and connect them\nwith a line of discourse analysis. We find that errors are more common in\ncomplex sentences and are associated with several discourse features. We\npropose a framework that decomposes long texts into discourse-inspired chunks\nand utilizes discourse information to better aggregate sentence-level scores\npredicted by natural language inference models. Our approach shows improved\nperformance on top of different model baselines over several evaluation\nbenchmarks, covering rich domains of texts, focusing on long document\nsummarization. This underscores the significance of incorporating discourse\nfeatures in developing models for scoring summaries for long document factual\ninconsistency.", "published": "2025-02-10 06:30:15", "link": "http://arxiv.org/abs/2502.06185v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unveiling the Capabilities of Large Language Models in Detecting\n  Offensive Language with Annotation Disagreement", "abstract": "Large Language Models (LLMs) have become essential for offensive language\ndetection, yet their ability to handle annotation disagreement remains\nunderexplored. Disagreement samples, which arise from subjective\ninterpretations, pose a unique challenge due to their ambiguous nature.\nUnderstanding how LLMs process these cases, particularly their confidence\nlevels, can offer insight into their alignment with human annotators. This\nstudy systematically evaluates the performance of multiple LLMs in detecting\noffensive language at varying levels of annotation agreement. We analyze binary\nclassification accuracy, examine the relationship between model confidence and\nhuman disagreement, and explore how disagreement samples influence model\ndecision-making during few-shot learning and instruction fine-tuning. Our\nfindings reveal that LLMs struggle with low-agreement samples, often exhibiting\noverconfidence in these ambiguous cases. However, utilizing disagreement\nsamples in training improves both detection accuracy and model alignment with\nhuman judgment. These insights provide a foundation for enhancing LLM-based\noffensive language detection in real-world moderation tasks.", "published": "2025-02-10 07:14:26", "link": "http://arxiv.org/abs/2502.06207v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Examining False Positives under Inference Scaling for Mathematical\n  Reasoning", "abstract": "Recent advancements in language models have led to significant improvements\nin mathematical reasoning across various benchmarks. However, most of these\nbenchmarks rely on automatic evaluation methods that only compare final answers\nusing heuristics, without verifying the underlying reasoning steps. This\nlimitation results in false positive solutions, where models may produce\ncorrect final answers but with flawed deduction paths. In this paper, we\nsystematically examine the prevalence of false positive solutions in\nmathematical problem solving for language models. We analyze the\ncharacteristics and extent of this issue across different open-source models,\ndatasets of varying difficulty levels, and decoding strategies. Specifically,\nwe explore how false positives influence the inference time scaling behavior of\nlanguage models. Our experimental results reveal that: (1) false positive\nsolutions persist across different models, datasets, and decoding methods, (2)\nsampling-based inference time scaling methods do not alleviate the problem, and\n(3) the pass@N evaluation metric is more susceptible to false positives,\nsuggesting a significantly lower scaling ceiling than what automatic\nevaluations indicate. Additionally, we analyze specific instances of false\npositives and discuss potential limitations in self-improvement techniques and\nsynthetic data generation under such conditions.", "published": "2025-02-10 07:49:35", "link": "http://arxiv.org/abs/2502.06217v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Confidence Improves Self-Consistency in LLMs", "abstract": "Self-consistency decoding enhances LLMs' performance on reasoning tasks by\nsampling diverse reasoning paths and selecting the most frequent answer.\nHowever, it is computationally expensive, as sampling many of these (lengthy)\npaths is required to increase the chances that the correct answer emerges as\nthe most frequent one. To address this, we introduce Confidence-Informed\nSelf-Consistency (CISC). CISC performs a weighted majority vote based on\nconfidence scores obtained directly from the model. By prioritizing\nhigh-confidence paths, it can identify the correct answer with a significantly\nsmaller sample size. When tested on nine models and four datasets, CISC\noutperforms self-consistency in nearly all configurations, reducing the\nrequired number of reasoning paths by over 40% on average. In addition, we\nintroduce the notion of within-question confidence evaluation, after showing\nthat standard evaluation methods are poor predictors of success in\ndistinguishing correct and incorrect answers to the same question. In fact, the\nmost calibrated confidence method proved to be the least effective for CISC.\nLastly, beyond these practical implications, our results and analyses show that\nLLMs can effectively judge the correctness of their own outputs, contributing\nto the ongoing debate on this topic.", "published": "2025-02-10 08:10:29", "link": "http://arxiv.org/abs/2502.06233v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CliniQ: A Multi-faceted Benchmark for Electronic Health Record Retrieval\n  with Semantic Match Assessment", "abstract": "Electronic Health Record (EHR) retrieval plays a pivotal role in various\nclinical tasks, but its development has been severely impeded by the lack of\npublicly available benchmarks. In this paper, we introduce a novel public EHR\nretrieval benchmark, CliniQ, to address this gap. We consider two retrieval\nsettings: Single-Patient Retrieval and Multi-Patient Retrieval, reflecting\nvarious real-world scenarios. Single-Patient Retrieval focuses on finding\nrelevant parts within a patient note, while Multi-Patient Retrieval involves\nretrieving EHRs from multiple patients. We build our benchmark upon 1,000\ndischarge summary notes along with the ICD codes and prescription labels from\nMIMIC-III, and collect 1,246 unique queries with 77,206 relevance judgments by\nfurther leveraging powerful LLMs as annotators. Additionally, we include a\nnovel assessment of the semantic gap issue in EHR retrieval by categorizing\nmatching types into string match and four types of semantic matches. On our\nproposed benchmark, we conduct a comprehensive evaluation of various retrieval\nmethods, ranging from conventional exact match to popular dense retrievers. Our\nexperiments find that BM25 sets a strong baseline and performs competitively to\nthe dense retrievers, and general domain dense retrievers surprisingly\noutperform those designed for the medical domain. In-depth analyses on various\nmatching types reveal the strengths and drawbacks of different methods,\nenlightening the potential for targeted improvement. We believe that our\nbenchmark will stimulate the research communities to advance EHR retrieval\nsystems.", "published": "2025-02-10 08:33:47", "link": "http://arxiv.org/abs/2502.06252v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "K-ON: Stacking Knowledge On the Head Layer of Large Language Model", "abstract": "Recent advancements in large language models (LLMs) have significantly\nimproved various natural language processing (NLP) tasks. Typically, LLMs are\ntrained to predict the next token, aligning well with many NLP tasks. However,\nin knowledge graph (KG) scenarios, entities are the fundamental units and\nidentifying an entity requires at least several tokens. This leads to a\ngranularity mismatch between KGs and natural languages. To address this issue,\nwe propose K-ON, which integrates KG knowledge into the LLM by employing\nmultiple head layers for next k-step prediction. K-ON can not only generate\nentity-level results in one step, but also enables contrastive loss against\nentities, which is the most powerful tool in KG representation learning.\nExperimental results show that K-ON outperforms state-of-the-art methods that\nincorporate text and even the other modalities.", "published": "2025-02-10 08:45:56", "link": "http://arxiv.org/abs/2502.06257v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Emergent Response Planning in LLM", "abstract": "In this work, we argue that large language models (LLMs), though trained to\npredict only the next token, exhibit emergent planning behaviors:\n$\\textbf{their hidden representations encode future outputs beyond the next\ntoken}$. Through simple probing, we demonstrate that LLM prompt representations\nencode global attributes of their entire responses, including\n$\\textit{structural attributes}$ (response length, reasoning steps),\n$\\textit{content attributes}$ (character choices in storywriting,\nmultiple-choice answers at the end of response), and $\\textit{behavioral\nattributes}$ (answer confidence, factual consistency). In addition to\nidentifying response planning, we explore how it scales with model size across\ntasks and how it evolves during generation. The findings that LLMs plan ahead\nfor the future in their hidden representations suggests potential applications\nfor improving transparency and generation control.", "published": "2025-02-10 08:48:10", "link": "http://arxiv.org/abs/2502.06258v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DebateBench: A Challenging Long Context Reasoning Benchmark For Large\n  Language Models", "abstract": "We introduce DebateBench, a novel dataset consisting of an extensive\ncollection of transcripts and metadata from some of the world's most\nprestigious competitive debates. The dataset consists of British Parliamentary\ndebates from prestigious debating tournaments on diverse topics, annotated with\ndetailed speech-level scores and house rankings sourced from official\nadjudication data. We curate 256 speeches across 32 debates with each debate\nbeing over 1 hour long with each input being an average of 32,000 tokens.\nDesigned to capture long-context, large-scale reasoning tasks, DebateBench\nprovides a benchmark for evaluating modern large language models (LLMs) on\ntheir ability to engage in argumentation, deliberation, and alignment with\nhuman experts. To do well on DebateBench, the LLMs must perform in-context\nlearning to understand the rules and evaluation criteria of the debates, then\nanalyze 8 seven minute long speeches and reason about the arguments presented\nby all speakers to give the final results. Our preliminary evaluation using GPT\no1, GPT-4o, and Claude Haiku, shows that LLMs struggle to perform well on\nDebateBench, highlighting the need to develop more sophisticated techniques for\nimproving their performance.", "published": "2025-02-10 09:23:03", "link": "http://arxiv.org/abs/2502.06279v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SeaExam and SeaBench: Benchmarking LLMs with Local Multilingual\n  Questions in Southeast Asia", "abstract": "This study introduces two novel benchmarks, SeaExam and SeaBench, designed to\nevaluate the capabilities of Large Language Models (LLMs) in Southeast Asian\n(SEA) application scenarios. Unlike existing multilingual datasets primarily\nderived from English translations, these benchmarks are constructed based on\nreal-world scenarios from SEA regions. SeaExam draws from regional educational\nexams to form a comprehensive dataset that encompasses subjects such as local\nhistory and literature. In contrast, SeaBench is crafted around multi-turn,\nopen-ended tasks that reflect daily interactions within SEA communities. Our\nevaluations demonstrate that SeaExam and SeaBench more effectively discern LLM\nperformance on SEA language tasks compared to their translated benchmarks. This\nhighlights the importance of using real-world queries to assess the\nmultilingual capabilities of LLMs.", "published": "2025-02-10 09:40:25", "link": "http://arxiv.org/abs/2502.06298v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The exponential distribution of the orders of demonstrative, numeral,\n  adjective and noun", "abstract": "The frequency of the preferred order for a noun phrase formed by\ndemonstrative, numeral, adjective and noun has received significant attention\nover the last two decades. We investigate the actual distribution of the\npreferred 24 possible orders. There is no consensus on whether it can be\nwell-fitted by an exponential or a power law distribution. We find that an\nexponential distribution is a much better model. This finding and other\ncircumstances where an exponential-like distribution is found challenge the\nview that power-law distributions, e.g., Zipf's law for word frequencies, are\ninevitable. We also investigate which of two exponential distributions gives a\nbetter fit: an exponential model where the 24 orders have non-zero probability\nor an exponential model where the number of orders that can have non-zero\nprobability is variable. When parsimony and generalizability are prioritized,\nwe find strong support for the exponential model where all 24 orders have\nnon-zero probability. This finding suggests that there is no hard constraint on\nword order variation and then unattested orders merely result from\nundersampling, consistently with Cysouw's view.", "published": "2025-02-10 10:45:00", "link": "http://arxiv.org/abs/2502.06342v1", "categories": ["cs.CL", "physics.soc-ph"], "primary_category": "cs.CL"}
{"title": "Content-Driven Local Response: Supporting Sentence-Level and\n  Message-Level Mobile Email Replies With and Without AI", "abstract": "Mobile emailing demands efficiency in diverse situations, which motivates the\nuse of AI. However, generated text does not always reflect how people want to\nrespond. This challenges users with AI involvement tradeoffs not yet considered\nin email UIs. We address this with a new UI concept called Content-Driven Local\nResponse (CDLR), inspired by microtasking. This allows users to insert\nresponses into the email by selecting sentences, which additionally serves to\nguide AI suggestions. The concept supports combining AI for local suggestions\nand message-level improvements. Our user study (N=126) compared CDLR with\nmanual typing and full reply generation. We found that CDLR supports flexible\nworkflows with varying degrees of AI involvement, while retaining the benefits\nof reduced typing and errors. This work contributes a new approach to\nintegrating AI capabilities: By redesigning the UI for workflows with and\nwithout AI, we can empower users to dynamically adjust AI involvement.", "published": "2025-02-10 13:06:25", "link": "http://arxiv.org/abs/2502.06430v1", "categories": ["cs.HC", "cs.CL", "H.5.2; I.2.7"], "primary_category": "cs.HC"}
{"title": "A Survey of Theory of Mind in Large Language Models: Evaluations,\n  Representations, and Safety Risks", "abstract": "Theory of Mind (ToM), the ability to attribute mental states to others and\npredict their behaviour, is fundamental to social intelligence. In this paper,\nwe survey studies evaluating behavioural and representational ToM in Large\nLanguage Models (LLMs), identify important safety risks from advanced LLM ToM\ncapabilities, and suggest several research directions for effective evaluation\nand mitigation of these risks.", "published": "2025-02-10 13:50:25", "link": "http://arxiv.org/abs/2502.06470v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GuideLLM: Exploring LLM-Guided Conversation with Applications in\n  Autobiography Interviewing", "abstract": "Although Large Language Models (LLMs) succeed in human-guided conversations\nsuch as instruction following and question answering, the potential of\nLLM-guided conversations-where LLMs direct the discourse and steer the\nconversation's objectives-remains under-explored. In this study, we first\ncharacterize LLM-guided conversation into three fundamental components: (i)\nGoal Navigation; (ii) Context Management; (iii) Empathetic Engagement, and\npropose GuideLLM as an installation. We then implement an interviewing\nenvironment for the evaluation of LLM-guided conversation. Specifically,\nvarious topics are involved in this environment for comprehensive interviewing\nevaluation, resulting in around 1.4k turns of utterances, 184k tokens, and over\n200 events mentioned during the interviewing for each chatbot evaluation. We\ncompare GuideLLM with 6 state-of-the-art LLMs such as GPT-4o and\nLlama-3-70b-Instruct, from the perspective of interviewing quality, and\nautobiography generation quality. For automatic evaluation, we derive user\nproxies from multiple autobiographies and employ LLM-as-a-judge to score LLM\nbehaviors. We further conduct a human-involved experiment by employing 45 human\nparticipants to chat with GuideLLM and baselines. We then collect human\nfeedback, preferences, and ratings regarding the qualities of conversation and\nautobiography. Experimental results indicate that GuideLLM significantly\noutperforms baseline LLMs in automatic evaluation and achieves consistent\nleading performances in human ratings.", "published": "2025-02-10 14:11:32", "link": "http://arxiv.org/abs/2502.06494v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Ignore the KL Penalty! Boosting Exploration on Critical Tokens to\n  Enhance RL Fine-Tuning", "abstract": "The ability to achieve long-term goals is a key challenge in the current\ndevelopment of large language models (LLMs). To address this, pre-trained LLMs\ncan be fine-tuned with reinforcement learning (RL) to explore solutions that\noptimize a given goal. However, exploration with LLMs is difficult, as a\nbalance has to be struck between discovering new solutions and staying close\nenough to the pre-trained model, so as not to degrade basic capabilities. This\nis typically controlled with a Kullback-Leibler (KL) penalty. In this paper, we\ninvestigate the exploration dynamics of a small language model on a simple\narithmetic task. We show how varying degrees of pre-training influence\nexploration and demonstrate the importance of \"critical tokens\" which have a\ndramatic impact on the final outcome. Consequently, we introduce a simple\nmodification to the KL penalty that favors exploration on critical tokens,\nincreasing the efficiency of the RL fine-tuning stage.", "published": "2025-02-10 14:56:25", "link": "http://arxiv.org/abs/2502.06533v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ProjectTest: A Project-level LLM Unit Test Generation Benchmark and\n  Impact of Error Fixing Mechanisms", "abstract": "Unit test generation has become a promising and important use case of LLMs.\nHowever, existing evaluation benchmarks for assessing LLM unit test generation\ncapabilities focus on function- or class-level code rather than more practical\nand challenging project-level codebases. To address such limitation, we propose\nProjectTest, a project-level benchmark for unit test generation covering\nPython, Java, and JavaScript. ProjectTest features 20 moderate-sized and\nhigh-quality projects per language. We evaluate nine frontier LLMs on\nProjectTest and the results show that all frontier LLMs tested exhibit moderate\nperformance on ProjectTest on Python and Java, highlighting the difficulty of\nProjectTest. We also conduct a thorough error analysis, which shows that even\nfrontier LLMs, such as Claude-3.5-Sonnet, have significant basic yet critical\nerrors, including compilation and cascade errors. Motivated by this\nobservation, we further evaluate all frontier LLMs under manual error-fixing\nand self-error-fixing scenarios to assess their potential when equipped with\nerror-fixing mechanisms. Our code and dataset is available at\n\\href{https://github.com/YiboWANG214/ProjectTest}{ProjectTest}.", "published": "2025-02-10 15:24:30", "link": "http://arxiv.org/abs/2502.06556v4", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Position: It's Time to Act on the Risk of Efficient Personalized Text\n  Generation", "abstract": "The recent surge in high-quality open-sourced Generative AI text models\n(colloquially: LLMs), as well as efficient finetuning techniques, has opened\nthe possibility of creating high-quality personalized models, i.e., models\ngenerating text attuned to a specific individual's needs and capable of\ncredibly imitating their writing style by leveraging that person's own data to\nrefine an open-source model. The technology to create such models is accessible\nto private individuals, and training and running such models can be done\ncheaply on consumer-grade hardware. These advancements are a huge gain for\nusability and privacy. This position paper argues, however, that these\nadvancements also introduce new safety risks by making it practically feasible\nfor malicious actors to impersonate specific individuals at scale, for instance\nfor the purpose of phishing emails, based on small amounts of publicly\navailable text. We further argue that these risks are complementary to - and\ndistinct from - the much-discussed risks of other impersonation attacks such as\nimage, voice, or video deepfakes, and are not adequately addressed by the\nlarger research community, or the current generation of open - and\nclosed-source models.", "published": "2025-02-10 15:25:11", "link": "http://arxiv.org/abs/2502.06560v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "LawGPT: Knowledge-Guided Data Generation and Its Application to Legal\n  LLM", "abstract": "Large language models (LLMs), both proprietary and open-source, have\ndemonstrated remarkable capabilities across various natural language processing\ntasks. However, they face significant limitations in legal reasoning tasks.\nProprietary models introduce data privacy risks and high inference costs, while\nopen-source models underperform due to insufficient legal domain training data.\nTo address these limitations, we study data generation for legal reasoning to\nimprove the legal reasoning performance of open-source LLMs with the help of\nproprietary LLMs. This is challenging due to the lack of legal knowledge in\nproprietary LLMs and the difficulty in verifying the generated data. We propose\nKgDG, a knowledge-guided data generation framework for legal reasoning. Our\nframework enables leveraging legal knowledge to enhance generation diversity\nand introduces a refinement and verification process to ensure the quality of\ngenerated data. Moreover, we expand the generated dataset to further enhance\nthe LLM reasoning capabilities. Using KgDG, we create a synthetic legal\nreasoning dataset containing 50K high-quality examples. Our trained model\nLawGPT outperforms existing legal-specific LLMs and achieves performance\ncomparable to proprietary LLMs, demonstrating the effectiveness of KgDG and\nLawGPT. Our code and resources is publicly available at\nhttps://github.com/LAMDASZ-ML/Knowledge-Guide-Data-Generation .", "published": "2025-02-10 15:40:35", "link": "http://arxiv.org/abs/2502.06572v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evaluation of Multilingual Image Captioning: How far can we get with\n  CLIP models?", "abstract": "The evaluation of image captions, looking at both linguistic fluency and\nsemantic correspondence to visual contents, has witnessed a significant effort.\nStill, despite advancements such as the CLIPScore metric, multilingual\ncaptioning evaluation has remained relatively unexplored. This work presents\nseveral strategies, and extensive experiments, related to evaluating CLIPScore\nvariants in multilingual settings. To address the lack of multilingual test\ndata, we consider two different strategies: (1) using quality aware\nmachine-translated datasets with human judgements, and (2) re-purposing\nmultilingual datasets that target semantic inference and reasoning. Our results\nhighlight the potential of finetuned multilingual models to generalize across\nlanguages and to handle complex linguistic challenges. Tests with\nmachine-translated data show that multilingual CLIPScore models can maintain a\nhigh correlation with human judgements across different languages, and\nadditional tests with natively multilingual and multicultural data further\nattest to the high-quality assessments.", "published": "2025-02-10 16:00:00", "link": "http://arxiv.org/abs/2502.06600v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Steel-LLM:From Scratch to Open Source -- A Personal Journey in Building\n  a Chinese-Centric LLM", "abstract": "Steel-LLM is a Chinese-centric language model developed from scratch with the\ngoal of creating a high-quality, open-source model despite limited\ncomputational resources. Launched in March 2024, the project aimed to train a\n1-billion-parameter model on a large-scale dataset, prioritizing transparency\nand the sharing of practical insights to assist others in the community. The\ntraining process primarily focused on Chinese data, with a small proportion of\nEnglish data included, addressing gaps in existing open-source LLMs by\nproviding a more detailed and practical account of the model-building journey.\nSteel-LLM has demonstrated competitive performance on benchmarks such as CEVAL\nand CMMLU, outperforming early models from larger institutions. This paper\nprovides a comprehensive summary of the project's key contributions, including\ndata collection, model design, training methodologies, and the challenges\nencountered along the way, offering a valuable resource for researchers and\npractitioners looking to develop their own LLMs. The model checkpoints and\ntraining script are available at https://github.com/zhanshijinwat/Steel-LLM.", "published": "2025-02-10 16:31:37", "link": "http://arxiv.org/abs/2502.06635v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Automatic Evaluation of Healthcare LLMs Beyond Question-Answering", "abstract": "Current Large Language Models (LLMs) benchmarks are often based on open-ended\nor close-ended QA evaluations, avoiding the requirement of human labor.\nClose-ended measurements evaluate the factuality of responses but lack\nexpressiveness. Open-ended capture the model's capacity to produce discourse\nresponses but are harder to assess for correctness. These two approaches are\ncommonly used, either independently or together, though their relationship\nremains poorly understood. This work is focused on the healthcare domain, where\nboth factuality and discourse matter greatly. It introduces a comprehensive,\nmulti-axis suite for healthcare LLM evaluation, exploring correlations between\nopen and close benchmarks and metrics. Findings include blind spots and\noverlaps in current methodologies. As an updated sanity check, we release a new\nmedical benchmark --CareQA-- with both open and closed variants. Finally, we\npropose a novel metric for open-ended evaluations -- Relaxed Perplexity -- to\nmitigate the identified limitations.", "published": "2025-02-10 16:52:39", "link": "http://arxiv.org/abs/2502.06666v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Boosting Self-Efficacy and Performance of Large Language Models via\n  Verbal Efficacy Stimulations", "abstract": "Significant improvements have been observed in the zero-shot capabilities of\nthe Large Language Models (LLMs). Due to their high sensitivity to input,\nresearch has increasingly focused on enhancing LLMs' performance via direct and\nsimple prompt engineering rather than intricate domain adaptation. Studies\nsuggest that LLMs exhibit emotional intelligence, and both positive and\nnegative emotions can potentially enhance task performances. However, prior\ninteraction prompts have predominantly concentrated on a single stimulus type,\nneglecting to compare different stimulus effects, examine the influence of\nvarying task difficulties, or explore underlying mechanisms. This paper,\ninspired by the positive correlation between self-efficacy and task performance\nwithin the social cognitive theory, introduces Verbal Efficacy Stimulations\n(VES). Our VES comprises three types of verbal prompts: encouraging,\nprovocative, and critical, addressing six aspects such as helpfulness and\ncompetence. And we further categorize task difficulty, aiming to extensively\ninvestigate how distinct VES influence the self-efficacy and task achievements\nof language models at varied levels of difficulty. The experimental results\nshow that the three types of VES improve the performance of LLMs on most tasks,\nand the most effective VES varies for different models. In extensive\nexperiments, we have obtained some findings consistent with psychological\ntheories, providing novel insights for future research.", "published": "2025-02-10 16:54:03", "link": "http://arxiv.org/abs/2502.06669v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi-label Scandinavian Language Identification (SLIDE)", "abstract": "Identifying closely related languages at sentence level is difficult, in\nparticular because it is often impossible to assign a sentence to a single\nlanguage. In this paper, we focus on multi-label sentence-level Scandinavian\nlanguage identification (LID) for Danish, Norwegian Bokm\\r{a}l, Norwegian\nNynorsk, and Swedish. We present the Scandinavian Language Identification and\nEvaluation, SLIDE, a manually curated multi-label evaluation dataset and a\nsuite of LID models with varying speed-accuracy tradeoffs. We demonstrate that\nthe ability to identify multiple languages simultaneously is necessary for any\naccurate LID method, and present a novel approach to training such multi-label\nLID models.", "published": "2025-02-10 17:16:55", "link": "http://arxiv.org/abs/2502.06692v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring the Limit of Outcome Reward for Learning Mathematical\n  Reasoning", "abstract": "Reasoning abilities, especially those for solving complex math problems, are\ncrucial components of general intelligence. Recent advances by proprietary\ncompanies, such as o-series models of OpenAI, have made remarkable progress on\nreasoning tasks. However, the complete technical details remain unrevealed, and\nthe techniques that are believed certainly to be adopted are only reinforcement\nlearning (RL) and the long chain of thoughts. This paper proposes a new RL\nframework, termed OREAL, to pursue the performance limit that can be achieved\nthrough \\textbf{O}utcome \\textbf{RE}w\\textbf{A}rd-based reinforcement\n\\textbf{L}earning for mathematical reasoning tasks, where only binary outcome\nrewards are easily accessible. We theoretically prove that behavior cloning on\npositive trajectories from best-of-N (BoN) sampling is sufficient to learn the\nKL-regularized optimal policy in binary feedback environments. This formulation\nfurther implies that the rewards of negative samples should be reshaped to\nensure the gradient consistency between positive and negative samples. To\nalleviate the long-existing difficulties brought by sparse rewards in RL, which\nare even exacerbated by the partial correctness of the long chain of thought\nfor reasoning tasks, we further apply a token-level reward model to sample\nimportant tokens in reasoning trajectories for learning. With OREAL, for the\nfirst time, a 7B model can obtain 94.0 pass@1 accuracy on MATH-500 through RL,\nbeing on par with 32B models. OREAL-32B also surpasses previous 32B models\ntrained by distillation with 95.0 pass@1 accuracy on MATH-500. Our\ninvestigation also indicates the importance of initial policy models and\ntraining queries for RL. Code, models, and data will be released to benefit\nfuture research\\footnote{https://github.com/InternLM/OREAL}.", "published": "2025-02-10 18:57:29", "link": "http://arxiv.org/abs/2502.06781v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Finding Words Associated with DIF: Predicting Differential Item\n  Functioning using LLMs and Explainable AI", "abstract": "We fine-tuned and compared several encoder-based Transformer large language\nmodels (LLM) to predict differential item functioning (DIF) from the item text.\nWe then applied explainable artificial intelligence (XAI) methods to these\nmodels to identify specific words associated with DIF. The data included 42,180\nitems designed for English language arts and mathematics summative state\nassessments among students in grades 3 to 11. Prediction $R^2$ ranged from .04\nto .32 among eight focal and reference group pairs. Our findings suggest that\nmany words associated with DIF reflect minor sub-domains included in the test\nblueprint by design, rather than construct-irrelevant item content that should\nbe removed from assessments. This may explain why qualitative reviews of DIF\nitems often yield confusing or inconclusive results. Our approach can be used\nto screen words associated with DIF during the item-writing process for\nimmediate revision, or help review traditional DIF analysis results by\nhighlighting key words in the text. Extensions of this research can enhance the\nfairness of assessment programs, especially those that lack resources to build\nhigh-quality items, and among smaller subpopulations where we do not have\nsufficient sample sizes for traditional DIF analyses.", "published": "2025-02-10 20:22:32", "link": "http://arxiv.org/abs/2502.07017v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Using Contextually Aligned Online Reviews to Measure LLMs' Performance\n  Disparities Across Language Varieties", "abstract": "A language can have different varieties. These varieties can affect the\nperformance of natural language processing (NLP) models, including large\nlanguage models (LLMs), which are often trained on data from widely spoken\nvarieties. This paper introduces a novel and cost-effective approach to\nbenchmark model performance across language varieties. We argue that\ninternational online review platforms, such as Booking.com, can serve as\neffective data sources for constructing datasets that capture comments in\ndifferent language varieties from similar real-world scenarios, like reviews\nfor the same hotel with the same rating using the same language (e.g., Mandarin\nChinese) but different language varieties (e.g., Taiwan Mandarin, Mainland\nMandarin). To prove this concept, we constructed a contextually aligned dataset\ncomprising reviews in Taiwan Mandarin and Mainland Mandarin and tested six LLMs\nin a sentiment analysis task. Our results show that LLMs consistently\nunderperform in Taiwan Mandarin.", "published": "2025-02-10 21:49:35", "link": "http://arxiv.org/abs/2502.07058v3", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "RoToR: Towards More Reliable Responses for Order-Invariant Inputs", "abstract": "Mitigating positional bias of language models (LMs) for listwise inputs is a\nwell-known and important problem (e.g., lost-in-the-middle). While zero-shot\norder-invariant LMs have been proposed to solve this issue, their success on\npractical listwise problems has been limited. In this work, as a first\ncontribution, we identify and overcome two limitations to make zero-shot\ninvariant LMs more practical: (1) training and inference distribution mismatch\narising from modifying positional ID assignments to enforce invariance, and (2)\nfailure to adapt to a mixture of order-invariant and sensitive inputs in\npractical listwise problems. Then, to overcome these issues we propose (1)\nRoToR, a zero-shot invariant LM for genuinely order-invariant inputs with\nminimal modifications of positional IDs, and (2) Selective Routing, an adaptive\nframework that handles both order-invariant and order-sensitive inputs in\nlistwise tasks. On the Lost in the middle (LitM), Knowledge Graph QA (KGQA),\nand MMLU benchmarks, we show that RoToR with Selective Routing can effectively\nhandle practical listwise input tasks in a zero-shot manner.", "published": "2025-02-10 09:34:15", "link": "http://arxiv.org/abs/2502.08662v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Online Social Support Detection in Spanish Social Media Texts", "abstract": "The advent of social media has transformed communication, enabling\nindividuals to share their experiences, seek support, and participate in\ndiverse discussions. While extensive research has focused on identifying\nharmful content like hate speech, the recognition and promotion of positive and\nsupportive interactions remain largely unexplored. This study proposes an\ninnovative approach to detecting online social support in Spanish-language\nsocial media texts. We introduce the first annotated dataset specifically\ncreated for this task, comprising 3,189 YouTube comments classified as\nsupportive or non-supportive. To address data imbalance, we employed GPT-4o to\ngenerate paraphrased comments and create a balanced dataset. We then evaluated\nsocial support classification using traditional machine learning models, deep\nlearning architectures, and transformer-based models, including GPT-4o, but\nonly on the unbalanced dataset. Subsequently, we utilized a transformer model\nto compare the performance between the balanced and unbalanced datasets. Our\nfindings indicate that the balanced dataset yielded improved results for Task 2\n(Individual and Group) and Task 3 (Nation, Other, LGBTQ, Black Community,\nWomen, Religion), whereas GPT-4o performed best for Task 1 (Social Support and\nNon-Support). This study highlights the significance of fostering a supportive\nonline environment and lays the groundwork for future research in automated\nsocial support detection.", "published": "2025-02-10 04:04:23", "link": "http://arxiv.org/abs/2502.09640v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Krutrim LLM: Multilingual Foundational Model for over a Billion People", "abstract": "India is a diverse society with unique challenges in developing AI systems,\nincluding linguistic diversity, oral traditions, data accessibility, and\nscalability. Existing foundation models are primarily trained on English,\nlimiting their effectiveness for India's population. Indic languages comprise\nonly 1 percent of Common Crawl corpora despite India representing 18 percent of\nthe global population, leading to linguistic biases. Thousands of regional\nlanguages, dialects, and code mixing create additional representation\nchallenges due to sparse training data.\n  We introduce Krutrim LLM, a 2 trillion token multilingual model designed for\nIndia's linguistic landscape. It incorporates the largest known Indic dataset,\nmitigating data scarcity and ensuring balanced performance across dialects.\nKrutrim outperforms or matches state-of-the-art models on Indic benchmarks\nwhile maintaining competitive English performance. Despite being significantly\nsmaller in training flops, Krutrim LLM matches or exceeds models like LLAMA-2\non 10 out of 16 tasks, with an average score of 0.57 versus 0.55. This\nevidences Krutrim's flexible multilingual fluency across diverse linguistic\ncontexts.\n  Krutrim is integrated with real-time search to improve factual accuracy in\nconversational AI applications. This enhances accessibility for over 1 billion\nusers worldwide. Through intentional design choices addressing data imbalances,\nKrutrim LLM signifies meaningful progress in building ethical, globally\nrepresentative AI models.", "published": "2025-02-10 09:32:08", "link": "http://arxiv.org/abs/2502.09642v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "From No to Know: Taxonomy, Challenges, and Opportunities for Negation\n  Understanding in Multimodal Foundation Models", "abstract": "Negation, a linguistic construct conveying absence, denial, or contradiction,\nposes significant challenges for multilingual multimodal foundation models.\nThese models excel in tasks like machine translation, text-guided generation,\nimage captioning, audio interactions, and video processing but often struggle\nto accurately interpret negation across diverse languages and cultural\ncontexts. In this perspective paper, we propose a comprehensive taxonomy of\nnegation constructs, illustrating how structural, semantic, and cultural\nfactors influence multimodal foundation models. We present open research\nquestions and highlight key challenges, emphasizing the importance of\naddressing these issues to achieve robust negation handling. Finally, we\nadvocate for specialized benchmarks, language-specific tokenization,\nfine-grained attention mechanisms, and advanced multimodal architectures. These\nstrategies can foster more adaptable and semantically precise multimodal\nfoundation models, better equipped to navigate and accurately interpret the\ncomplexities of negation in multilingual, multimodal environments.", "published": "2025-02-10 16:55:13", "link": "http://arxiv.org/abs/2502.09645v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Language Shift or Maintenance? An Intergenerational Study of the Tibetan\n  Community in Saudi Arabia", "abstract": "The present study provides the first-ever report on the language shift from\nTibetan to Arabic among descendants of Tibetan families who migrated from the\nTibet region to Saudi Arabia around 70 years ago. The aim of this study was to\ndetermine whether three age groups had adopted different practices in terms of\nmaintaining Tibetan or shifting to Hijazi Arabic. To this end, 96 male and\nfemale members of the Tibetan community responded to a questionnaire in which\nthey were asked about their code choice in different domains (home,\nneighbourhood, friends and relatives, expressing emotion, and performing\nreligious rituals). The data revealed significant intergenerational differences\nbetween members of the community in terms of the extent of the shift to Arabic,\nwith Tibetan rarely used by younger members and older members making only\nslightly more use of it. The difference between the three age groups was\nsignificant, at a p-value of .001.", "published": "2025-02-10 22:24:21", "link": "http://arxiv.org/abs/2502.09646v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Circuit-tuning: A Mechanistic Approach for Identifying Parameter\n  Redundancy and Fine-tuning Neural Networks", "abstract": "The study of mechanistic interpretability aims to reverse-engineer a model to\nexplain its behaviors. While recent studies have focused on the static\nmechanism of a certain behavior, the training dynamics inside a model remain to\nbe explored. In this work, we develop an interpretable method for fine-tuning\nand reveal the mechanism behind learning. We first propose the concept of node\nredundancy as an extension of intrinsic dimension and explain the idea behind\ncircuit discovery from a fresh view. Based on the theory, we propose\ncircuit-tuning, a two-stage algorithm that iteratively performs circuit\ndiscovery to mask out irrelevant edges and updates the remaining parameters\nresponsible for a specific task. Experiments show that our method not only\nimproves performance on a wide range of tasks but is also scalable while\npreserving general capabilities. We visualize and analyze the circuits before,\nduring, and after fine-tuning, providing new insights into the\nself-organization mechanism of a neural network in the learning process.", "published": "2025-02-10 02:35:53", "link": "http://arxiv.org/abs/2502.06106v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Universal Approximation of Visual Autoregressive Transformers", "abstract": "We investigate the fundamental limits of transformer-based foundation models,\nextending our analysis to include Visual Autoregressive (VAR) transformers. VAR\nrepresents a big step toward generating images using a novel, scalable,\ncoarse-to-fine ``next-scale prediction'' framework. These models set a new\nquality bar, outperforming all previous methods, including Diffusion\nTransformers, while having state-of-the-art performance for image synthesis\ntasks. Our primary contributions establish that, for single-head VAR\ntransformers with a single self-attention layer and single interpolation layer,\nthe VAR Transformer is universal. From the statistical perspective, we prove\nthat such simple VAR transformers are universal approximators for any\nimage-to-image Lipschitz functions. Furthermore, we demonstrate that flow-based\nautoregressive transformers inherit similar approximation capabilities. Our\nresults provide important design principles for effective and computationally\nefficient VAR Transformer strategies that can be used to extend their utility\nto more sophisticated VAR models in image generation and other related areas.", "published": "2025-02-10 05:36:30", "link": "http://arxiv.org/abs/2502.06167v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "C-3PO: Compact Plug-and-Play Proxy Optimization to Achieve Human-like\n  Retrieval-Augmented Generation", "abstract": "Retrieval-augmented generation (RAG) systems face a fundamental challenge in\naligning independently developed retrievers and large language models (LLMs).\nExisting approaches typically involve modifying either component or introducing\nsimple intermediate modules, resulting in practical limitations and sub-optimal\nperformance. Inspired by human search behavior -- typically involving a\nback-and-forth process of proposing search queries and reviewing documents, we\npropose C-3PO, a proxy-centric framework that facilitates communication between\nretrievers and LLMs through a lightweight multi-agent system. Our framework\nimplements three specialized agents that collaboratively optimize the entire\nRAG pipeline without altering the retriever and LLMs. These agents work\ntogether to assess the need for retrieval, generate effective queries, and\nselect information suitable for the LLMs. To enable effective multi-agent\ncoordination, we develop a tree-structured rollout approach for reward credit\nassignment in reinforcement learning. Extensive experiments in both in-domain\nand out-of-distribution scenarios demonstrate that C-3PO significantly enhances\nRAG performance while maintaining plug-and-play flexibility and superior\ngeneralization capabilities.", "published": "2025-02-10 07:04:32", "link": "http://arxiv.org/abs/2502.06205v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LessLeak-Bench: A First Investigation of Data Leakage in LLMs Across 83\n  Software Engineering Benchmarks", "abstract": "Large Language Models (LLMs) are widely utilized in software engineering (SE)\ntasks, such as code generation and automated program repair. However, their\nreliance on extensive and often undisclosed pre-training datasets raises\nsignificant concerns about data leakage, where the evaluation benchmark data is\nunintentionally ``seen'' by LLMs during the model's construction phase. The\ndata leakage issue could largely undermine the validity of LLM-based research\nand evaluations. Despite the increasing use of LLMs in the SE community, there\nis no comprehensive study that assesses the extent of data leakage in SE\nbenchmarks for LLMs yet. To address this gap, this paper presents the first\nlarge-scale analysis of data leakage in 83 SE benchmarks concerning LLMs. Our\nresults show that in general, data leakage in SE benchmarks is minimal, with\naverage leakage ratios of only 4.8\\%, 2.8\\%, and 0.7\\% for Python, Java, and\nC/C++ benchmarks, respectively. However, some benchmarks exhibit relatively\nhigher leakage ratios, which raises concerns about their bias in evaluation.\nFor instance, QuixBugs and BigCloneBench have leakage ratios of 100.0\\% and\n55.7\\%, respectively. Furthermore, we observe that data leakage has a\nsubstantial impact on LLM evaluation. We also identify key causes of high data\nleakage, such as the direct inclusion of benchmark data in pre-training\ndatasets and the use of coding platforms like LeetCode for benchmark\nconstruction. To address the data leakage, we introduce\n\\textbf{LessLeak-Bench}, a new benchmark that removes leaked samples from the\n83 SE benchmarks, enabling more reliable LLM evaluations in future research.\nOur study enhances the understanding of data leakage in SE benchmarks and\nprovides valuable insights for future research involving LLMs in SE.", "published": "2025-02-10 07:33:49", "link": "http://arxiv.org/abs/2502.06215v1", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Jakiro: Boosting Speculative Decoding with Decoupled Multi-Head via MoE", "abstract": "Speculative decoding (SD) accelerates large language model inference by using\na smaller draft model to predict multiple tokens, which are then verified in\nparallel by the larger target model. However, the limited capacity of the draft\nmodel often necessitates tree-based sampling to improve prediction accuracy,\nwhere multiple candidates are generated at each step. We identify a key\nlimitation in this approach: the candidates at the same step are derived from\nthe same representation, limiting diversity and reducing overall effectiveness.\nTo address this, we propose Jakiro, leveraging Mixture of Experts (MoE), where\nindependent experts generate diverse predictions, effectively decoupling\ncorrelations among candidates. Furthermore, we introduce a hybrid inference\nstrategy, combining autoregressive decoding for initial tokens with parallel\ndecoding for subsequent stages, and enhance the latter with contrastive\nmechanism in features to improve accuracy. Our method significantly boosts\nprediction accuracy and achieves higher inference speedups. Extensive\nexperiments across diverse models validate the effectiveness and robustness of\nour approach, establishing a new SOTA in speculative decoding. Our codes are\navailable at https://github.com/haiduo/Jakiro.", "published": "2025-02-10 09:24:06", "link": "http://arxiv.org/abs/2502.06282v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Systematic Outliers in Large Language Models", "abstract": "Outliers have been widely observed in Large Language Models (LLMs),\nsignificantly impacting model performance and posing challenges for model\ncompression. Understanding the functionality and formation mechanisms of these\noutliers is critically important. Existing works, however, largely focus on\nreducing the impact of outliers from an algorithmic perspective, lacking an\nin-depth investigation into their causes and roles. In this work, we provide a\ndetailed analysis of the formation process, underlying causes, and functions of\noutliers in LLMs. We define and categorize three types of outliers-activation\noutliers, weight outliers, and attention outliers-and analyze their\ndistributions across different dimensions, uncovering inherent connections\nbetween their occurrences and their ultimate influence on the attention\nmechanism. Based on these observations, we hypothesize and explore the\nmechanisms by which these outliers arise and function, demonstrating through\ntheoretical derivations and experiments that they emerge due to the\nself-attention mechanism's softmax operation. These outliers act as implicit\ncontext-aware scaling factors within the attention mechanism. As these outliers\nstem from systematic influences, we term them systematic outliers. Our study\nnot only enhances the understanding of Transformer-based LLMs but also shows\nthat structurally eliminating outliers can accelerate convergence and improve\nmodel compression. The code is avilable at\nhttps://github.com/an-yongqi/systematic-outliers.", "published": "2025-02-10 12:54:17", "link": "http://arxiv.org/abs/2502.06415v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MATH-Perturb: Benchmarking LLMs' Math Reasoning Abilities against Hard\n  Perturbations", "abstract": "Large language models have demonstrated impressive performance on challenging\nmathematical reasoning tasks, which has triggered the discussion of whether the\nperformance is achieved by true reasoning capability or memorization. To\ninvestigate this question, prior work has constructed mathematical benchmarks\nwhen questions undergo simple perturbations -- modifications that still\npreserve the underlying reasoning patterns of the solutions. However, no work\nhas explored hard perturbations, which fundamentally change the nature of the\nproblem so that the original solution steps do not apply. To bridge the gap, we\nconstruct MATH-P-Simple and MATH-P-Hard via simple perturbation and hard\nperturbation, respectively. Each consists of 279 perturbed math problems\nderived from level-5 (hardest) problems in the MATH dataset (Hendrycksmath et.\nal., 2021). We observe significant performance drops on MATH-P-Hard across\nvarious models, including o1-mini (-16.49%) and gemini-2.0-flash-thinking\n(-12.9%). We also raise concerns about a novel form of memorization where\nmodels blindly apply learned problem-solving skills without assessing their\napplicability to modified contexts. This issue is amplified when using original\nproblems for in-context learning. We call for research efforts to address this\nchallenge, which is critical for developing more robust and reliable reasoning\nmodels.", "published": "2025-02-10 13:31:46", "link": "http://arxiv.org/abs/2502.06453v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "KARMA: Leveraging Multi-Agent LLMs for Automated Knowledge Graph\n  Enrichment", "abstract": "Maintaining comprehensive and up-to-date knowledge graphs (KGs) is critical\nfor modern AI systems, but manual curation struggles to scale with the rapid\ngrowth of scientific literature. This paper presents KARMA, a novel framework\nemploying multi-agent large language models (LLMs) to automate KG enrichment\nthrough structured analysis of unstructured text. Our approach employs nine\ncollaborative agents, spanning entity discovery, relation extraction, schema\nalignment, and conflict resolution that iteratively parse documents, verify\nextracted knowledge, and integrate it into existing graph structures while\nadhering to domain-specific schema. Experiments on 1,200 PubMed articles from\nthree different domains demonstrate the effectiveness of KARMA in knowledge\ngraph enrichment, with the identification of up to 38,230 new entities while\nachieving 83.1\\% LLM-verified correctness and reducing conflict edges by 18.6\\%\nthrough multi-layer assessments.", "published": "2025-02-10 13:51:36", "link": "http://arxiv.org/abs/2502.06472v1", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.DL"], "primary_category": "cs.CL"}
{"title": "Hephaestus: Improving Fundamental Agent Capabilities of Large Language\n  Models through Continual Pre-Training", "abstract": "Due to the scarcity of agent-oriented pre-training data, LLM-based autonomous\nagents typically rely on complex prompting or extensive fine-tuning, which\noften fails to introduce new capabilities while preserving strong\ngeneralizability. We introduce Hephaestus-Forge, the first large-scale\npre-training corpus designed to enhance the fundamental capabilities of LLM\nagents in API function calling, intrinsic reasoning and planning, and adapting\nto environmental feedback. Hephaestus-Forge comprises 103B agent-specific data\nencompassing 76,537 APIs, including both tool documentation to introduce\nknowledge of API functions and function calling trajectories to strengthen\nintrinsic reasoning. To explore effective training protocols, we investigate\nscaling laws to identify the optimal recipe in data mixing ratios. By continual\npre-training on Hephaestus-Forge, Hephaestus outperforms small- to medium-scale\nopen-source LLMs and rivals commercial LLMs on three agent benchmarks,\ndemonstrating the effectiveness of our pre-training corpus in enhancing\nfundamental agentic capabilities and generalization of LLMs to new tasks or\nenvironments.", "published": "2025-02-10 15:54:34", "link": "http://arxiv.org/abs/2502.06589v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The 2021 Tokyo Olympics Multilingual News Article Dataset", "abstract": "In this paper, we introduce a dataset of multilingual news articles covering\nthe 2021 Tokyo Olympics. A total of 10,940 news articles were gathered from\n1,918 different publishers, covering 1,350 sub-events of the 2021 Olympics, and\npublished between July 1, 2021, and August 14, 2021. These articles are written\nin nine languages from different language families and in different scripts. To\ncreate the dataset, the raw news articles were first retrieved via a service\nthat collects and analyzes news articles. Then, the articles were grouped using\nan online clustering algorithm, with each group containing articles reporting\non the same sub-event. Finally, the groups were manually annotated and\nevaluated. The development of this dataset aims to provide a resource for\nevaluating the performance of multilingual news clustering algorithms, for\nwhich limited datasets are available. It can also be used to analyze the\ndynamics and events of the 2021 Tokyo Olympics from different perspectives. The\ndataset is available in CSV format and can be accessed from the CLARIN.SI\nrepository.", "published": "2025-02-10 16:38:03", "link": "http://arxiv.org/abs/2502.06648v2", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Rationalization Models for Text-to-SQL", "abstract": "We introduce a framework for generating Chain-of-Thought (CoT) rationales to\nenhance text-to-SQL model fine-tuning. These rationales consist of intermediate\nSQL statements and explanations, serving as incremental steps toward\nconstructing the final SQL query. The process begins with manually annotating a\nsmall set of examples, which are then used to prompt a large language model in\nan iterative, dynamic few-shot knowledge distillation procedure from a teacher\nmodel. A rationalization model is subsequently trained on the validated\ndecomposed queries, enabling extensive synthetic CoT annotations for\ntext-to-SQL datasets. To evaluate the approach, we fine-tune small language\nmodels with and without these rationales on the BIRD dataset. Results indicate\nthat step-by-step query generation improves execution accuracy, especially for\nmoderately and highly complex queries, while also enhancing explainability.", "published": "2025-02-10 18:38:57", "link": "http://arxiv.org/abs/2502.06759v4", "categories": ["cs.CL", "cs.AI", "cs.DB"], "primary_category": "cs.CL"}
{"title": "ReasonFlux: Hierarchical LLM Reasoning via Scaling Thought Templates", "abstract": "We present that hierarchical LLM reasoning via scaling thought templates can\neffectively optimize the reasoning search space and outperform the mathematical\nreasoning capabilities of powerful LLMs like OpenAI o1-preview and DeepSeek V3.\nWe train our ReasonFlux-32B model with only 8 GPUs and introduces three\ninnovations: (i) a structured and generic thought template library, containing\naround 500 high-level thought templates capable of generalizing to similar or\nrelevant reasoning problems; (ii) performing hierarchical reinforcement\nlearning on a sequence of thought templates instead of long CoTs, optimizing a\nbase LLM to plan out an optimal template trajectory for gradually handling\ncomplex problems; (iii) a brand new inference scaling system that enables\nhierarchical LLM reasoning by adaptively scaling thought templates at inference\ntime. With a template trajectory containing more explainable reasoning\nstructures than DeepSeek-R1 and o3-mini, our ReasonFlux-32B significantly\nadvances math reasoning capabilities to state-of-the-art levels. Notably, on\nthe MATH benchmark, it achieves an accuracy of 91.2% and surpasses o1-preview\nby 6.7%. On the USA Math Olympiad (AIME) benchmark, ReasonFlux-32B solves an\naverage of 56.7% of problems, surpassing o1-preview and DeepSeek-V3 by 27% and\n45%, respectively. Code: https://github.com/Gen-Verse/ReasonFlux", "published": "2025-02-10 18:51:47", "link": "http://arxiv.org/abs/2502.06772v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On the Emergence of Thinking in LLMs I: Searching for the Right\n  Intuition", "abstract": "Recent AI advancements, such as OpenAI's new models, are transforming LLMs\ninto LRMs (Large Reasoning Models) that perform reasoning during inference,\ntaking extra time and compute for higher-quality outputs. We aim to uncover the\nalgorithmic framework for training LRMs. Methods like self-consistency, PRM,\nand AlphaZero suggest reasoning as guided search. We ask: what is the simplest,\nmost scalable way to enable search in LLMs?\n  We propose a post-training framework called Reinforcement Learning via\nSelf-Play (RLSP). RLSP involves three steps: (1) supervised fine-tuning with\nhuman or synthetic demonstrations of the reasoning process, (2) using an\nexploration reward signal to encourage diverse and efficient reasoning\nbehaviors, and (3) RL training with an outcome verifier to ensure correctness\nwhile preventing reward hacking. Our key innovation is to decouple exploration\nand correctness signals during PPO training, carefully balancing them to\nimprove performance and efficiency.\n  Empirical studies in the math domain show that RLSP improves reasoning. On\nthe Llama-3.1-8B-Instruct model, RLSP can boost performance by 23% in MATH-500\ntest set; On AIME 2024 math problems, Qwen2.5-32B-Instruct improved by 10% due\nto RLSP. However, a more important finding of this work is that the models\ntrained using RLSP, even with the simplest exploration reward that encourages\nthe model to take more intermediate steps, showed several emergent behaviors\nsuch as backtracking, exploration of ideas, and verification. These findings\ndemonstrate that RLSP framework might be enough to enable emergence of complex\nreasoning abilities in LLMs when scaled. Lastly, we propose a theory as to why\nRLSP search strategy is more suitable for LLMs inspired by a remarkable result\nthat says CoT provably increases computational power of LLMs, which grows as\nthe number of steps in CoT \\cite{li2024chain,merrill2023expresssive}.", "published": "2025-02-10 18:52:04", "link": "http://arxiv.org/abs/2502.06773v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Synthetic Audio Helps for Cognitive State Tasks", "abstract": "The NLP community has broadly focused on text-only approaches of cognitive\nstate tasks, but audio can provide vital missing cues through prosody. We posit\nthat text-to-speech models learn to track aspects of cognitive state in order\nto produce naturalistic audio, and that the signal audio models implicitly\nidentify is orthogonal to the information that language models exploit. We\npresent Synthetic Audio Data fine-tuning (SAD), a framework where we show that\n7 tasks related to cognitive state modeling benefit from multimodal training on\nboth text and zero-shot synthetic audio data from an off-the-shelf TTS system.\nWe show an improvement over the text-only modality when adding synthetic audio\ndata to text-only corpora. Furthermore, on tasks and corpora that do contain\ngold audio, we show our SAD framework achieves competitive performance with\ntext and synthetic audio compared to text and gold audio.", "published": "2025-02-10 17:16:24", "link": "http://arxiv.org/abs/2502.06922v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.SD"}
{"title": "Neighborhood-Order Learning Graph Attention Network for Fake News\n  Detection", "abstract": "Fake news detection is a significant challenge in the digital age, which has\nbecome increasingly important with the proliferation of social media and online\ncommunication networks. Graph Neural Networks (GNN)-based methods have shown\nhigh potential in analyzing graph-structured data for this problem. However, a\nmajor limitation in conventional GNN architectures is their inability to\neffectively utilize information from neighbors beyond the network's layer\ndepth, which can reduce the model's accuracy and effectiveness. In this paper,\nwe propose a novel model called Neighborhood-Order Learning Graph Attention\nNetwork (NOL-GAT) for fake news detection. This model allows each node in each\nlayer to independently learn its optimal neighborhood order. By doing so, the\nmodel can purposefully and efficiently extract critical information from\ndistant neighbors. The NOL-GAT architecture consists of two main components: a\nHop Network that determines the optimal neighborhood order and an Embedding\nNetwork that updates node embeddings using these optimal neighborhoods. To\nevaluate the model's performance, experiments are conducted on various fake\nnews datasets. Results demonstrate that NOL-GAT significantly outperforms\nbaseline models in metrics such as accuracy and F1-score, particularly in\nscenarios with limited labeled data. Features such as mitigating the\nover-squashing problem, improving information flow, and reducing computational\ncomplexity further highlight the advantages of the proposed model.", "published": "2025-02-10 18:51:57", "link": "http://arxiv.org/abs/2502.06927v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "SyncMind: Measuring Agent Out-of-Sync Recovery in Collaborative Software\n  Engineering", "abstract": "Software engineering (SE) is increasingly collaborative, with developers\nworking together on shared complex codebases. Effective collaboration in shared\nenvironments requires participants -- whether humans or AI agents -- to stay on\nthe same page as their environment evolves. When a collaborator's understanding\ndiverges from the current state -- what we term the out-of-sync challenge --\nthe collaborator's actions may fail, leading to integration issues. In this\nwork, we introduce SyncMind, a framework that systematically defines the\nout-of-sync problem faced by large language model (LLM) agents in collaborative\nsoftware engineering (CSE). Based on SyncMind, we create SyncBench, a benchmark\nfeaturing 24,332 instances of agent out-of-sync scenarios in real-world CSE\nderived from 21 popular GitHub repositories with executable verification tests.\nExperiments on SyncBench uncover critical insights into existing LLM agents'\ncapabilities and limitations. Besides substantial performance gaps among agents\n(from Llama-3.1 agent <= 3.33% to Claude-3.5-Sonnet >= 28.18%), their\nconsistently low collaboration willingness (<= 4.86%) suggests fundamental\nlimitations of existing LLM in CSE. However, when collaboration occurs, it\npositively correlates with out-of-sync recovery success. Minimal performance\ndifferences in agents' resource-aware out-of-sync recoveries further reveal\ntheir significant lack of resource awareness and adaptability, shedding light\non future resource-efficient collaborative systems. Code and data are openly\navailable on our project website: https://xhguo7.github.io/SyncMind/.", "published": "2025-02-10 19:38:36", "link": "http://arxiv.org/abs/2502.06994v1", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "AIMS.au: A Dataset for the Analysis of Modern Slavery Countermeasures in\n  Corporate Statements", "abstract": "Despite over a decade of legislative efforts to address modern slavery in the\nsupply chains of large corporations, the effectiveness of government oversight\nremains hampered by the challenge of scrutinizing thousands of statements\nannually. While Large Language Models (LLMs) can be considered a well\nestablished solution for the automatic analysis and summarization of documents,\nrecognizing concrete modern slavery countermeasures taken by companies and\ndifferentiating those from vague claims remains a challenging task. To help\nevaluate and fine-tune LLMs for the assessment of corporate statements, we\nintroduce a dataset composed of 5,731 modern slavery statements taken from the\nAustralian Modern Slavery Register and annotated at the sentence level. This\npaper details the construction steps for the dataset that include the careful\ndesign of annotation specifications, the selection and preprocessing of\nstatements, and the creation of high-quality annotation subsets for effective\nmodel evaluations. To demonstrate our dataset's utility, we propose a machine\nlearning methodology for the detection of sentences relevant to mandatory\nreporting requirements set by the Australian Modern Slavery Act. We then follow\nthis methodology to benchmark modern language models under zero-shot and\nsupervised learning settings.", "published": "2025-02-10 20:30:32", "link": "http://arxiv.org/abs/2502.07022v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Leveraging Allophony in Self-Supervised Speech Models for Atypical\n  Pronunciation Assessment", "abstract": "Allophony refers to the variation in the phonetic realization of a phoneme\nbased on its phonetic environment. Modeling allophones is crucial for atypical\npronunciation assessment, which involves distinguishing atypical from typical\npronunciations. However, recent phoneme classifier-based approaches often\nsimplify this by treating various realizations as a single phoneme, bypassing\nthe complexity of modeling allophonic variation. Motivated by the acoustic\nmodeling capabilities of frozen self-supervised speech model (S3M) features, we\npropose MixGoP, a novel approach that leverages Gaussian mixture models to\nmodel phoneme distributions with multiple subclusters. Our experiments show\nthat MixGoP achieves state-of-the-art performance across four out of five\ndatasets, including dysarthric and non-native speech. Our analysis further\nsuggests that S3M features capture allophonic variation more effectively than\nMFCCs and Mel spectrograms, highlighting the benefits of integrating MixGoP\nwith S3M features.", "published": "2025-02-10 20:46:42", "link": "http://arxiv.org/abs/2502.07029v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Scalable and Ethical Insider Threat Detection through Data Synthesis and\n  Analysis by LLMs", "abstract": "Insider threats wield an outsized influence on organizations,\ndisproportionate to their small numbers. This is due to the internal access\ninsiders have to systems, information, and infrastructure. %One example of this\ninfluence is where anonymous respondents submit web-based job search site\nreviews, an insider threat risk to organizations. Signals for such risks may be\nfound in anonymous submissions to public web-based job search site reviews.\nThis research studies the potential for large language models (LLMs) to analyze\nand detect insider threat sentiment within job site reviews. Addressing ethical\ndata collection concerns, this research utilizes synthetic data generation\nusing LLMs alongside existing job review datasets. A comparative analysis of\nsentiment scores generated by LLMs is benchmarked against expert human scoring.\nFindings reveal that LLMs demonstrate alignment with human evaluations in most\ncases, thus effectively identifying nuanced indicators of threat sentiment. The\nperformance is lower on human-generated data than synthetic data, suggesting\nareas for improvement in evaluating real-world data. Text diversity analysis\nfound differences between human-generated and LLM-generated datasets, with\nsynthetic data exhibiting somewhat lower diversity. Overall, the results\ndemonstrate the applicability of LLMs to insider threat detection, and a\nscalable solution for insider sentiment testing by overcoming ethical and\nlogistical barriers tied to data acquisition.", "published": "2025-02-10 21:27:06", "link": "http://arxiv.org/abs/2502.07045v2", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.CY", "C.2.0; I.2.7; K.4.1; H.3.3"], "primary_category": "cs.CR"}
{"title": "IRepair: An Intent-Aware Approach to Repair Data-Driven Errors in Large\n  Language Models", "abstract": "Not a day goes by without hearing about the impressive feats of large\nlanguage models (LLMs), and equally, not a day passes without hearing about\ntheir challenges. LLMs are notoriously vulnerable to biases in their dataset,\nleading to issues such as toxicity. While domain-adaptive training has been\nemployed to mitigate these issues, these techniques often address all model\nparameters indiscriminately during the repair process, resulting in poor repair\nquality and reduced model versatility. In this paper, we introduce a novel\ndynamic slicing-based intent-aware LLM repair strategy, IRepair. This approach\nselectively targets the most error-prone sections of the model for repair.\nSpecifically, we propose dynamically slicing the model's most sensitive layers\nthat require immediate attention, concentrating repair efforts on those areas.\nThis method enables more effective repairs with potentially less impact on the\nmodel's overall performance by altering a smaller portion of the model. We\nevaluated our technique on three models from the GPT2 and GPT-Neo families,\nwith parameters ranging from 800M to 1.6B, in a toxicity mitigation setup. Our\nresults show that IRepair repairs errors 43.6% more effectively while causing\n46% less disruption to general performance compared to the closest baseline,\ndirect preference optimization. Our empirical analysis also reveals that errors\nare more concentrated in a smaller section of the model, with the top 20% of\nlayers exhibiting 773% more error density than the remaining 80\\%. This\nhighlights the need for selective repair. Additionally, we demonstrate that a\ndynamic selection approach is essential for addressing errors dispersed\nthroughout the model, ensuring a robust and efficient repair.", "published": "2025-02-10 22:07:02", "link": "http://arxiv.org/abs/2502.07072v3", "categories": ["cs.CL", "cs.AI", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Multi-turn Evaluation of Anthropomorphic Behaviours in Large Language\n  Models", "abstract": "The tendency of users to anthropomorphise large language models (LLMs) is of\ngrowing interest to AI developers, researchers, and policy-makers. Here, we\npresent a novel method for empirically evaluating anthropomorphic LLM\nbehaviours in realistic and varied settings. Going beyond single-turn static\nbenchmarks, we contribute three methodological advances in state-of-the-art\n(SOTA) LLM evaluation. First, we develop a multi-turn evaluation of 14\nanthropomorphic behaviours. Second, we present a scalable, automated approach\nby employing simulations of user interactions. Third, we conduct an\ninteractive, large-scale human subject study (N=1101) to validate that the\nmodel behaviours we measure predict real users' anthropomorphic perceptions. We\nfind that all SOTA LLMs evaluated exhibit similar behaviours, characterised by\nrelationship-building (e.g., empathy and validation) and first-person pronoun\nuse, and that the majority of behaviours only first occur after multiple turns.\nOur work lays an empirical foundation for investigating how design choices\ninfluence anthropomorphic model behaviours and for progressing the ethical\ndebate on the desirability of these behaviours. It also showcases the necessity\nof multi-turn evaluations for complex social phenomena in human-AI interaction.", "published": "2025-02-10 22:09:57", "link": "http://arxiv.org/abs/2502.07077v1", "categories": ["cs.CL", "cs.CY", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Cardiverse: Harnessing LLMs for Novel Card Game Prototyping", "abstract": "The prototyping of computer games, particularly card games, requires\nextensive human effort in creative ideation and gameplay evaluation. Recent\nadvances in Large Language Models (LLMs) offer opportunities to automate and\nstreamline these processes. However, it remains challenging for LLMs to design\nnovel game mechanics beyond existing databases, generate consistent gameplay\nenvironments, and develop scalable gameplay AI for large-scale evaluations.\nThis paper addresses these challenges by introducing a comprehensive automated\ncard game prototyping framework. The approach highlights a graph-based indexing\nmethod for generating novel game designs, an LLM-driven system for consistent\ngame code generation validated by gameplay records, and a gameplay AI\nconstructing method that uses an ensemble of LLM-generated action-value\nfunctions optimized through self-play. These contributions aim to accelerate\ncard game prototyping, reduce human labor, and lower barriers to entry for game\ndevelopers.", "published": "2025-02-10 23:47:35", "link": "http://arxiv.org/abs/2502.07128v1", "categories": ["cs.CL", "cs.AI", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Hallucination Detection: A Probabilistic Framework Using Embeddings\n  Distance Analysis", "abstract": "Hallucinations are one of the major issues affecting LLMs, hindering their\nwide adoption in production systems. While current research solutions for\ndetecting hallucinations are mainly based on heuristics, in this paper we\nintroduce a mathematically sound methodology to reason about hallucination, and\nleverage it to build a tool to detect hallucinations. To the best of our\nknowledge, we are the first to show that hallucinated content has structural\ndifferences with respect to correct content. To prove this result, we resort to\nthe Minkowski distances in the embedding space. Our findings demonstrate\nstatistically significant differences in the embedding distance distributions,\nthat are also scale free -- they qualitatively hold regardless of the distance\nnorm used and the number of keywords, questions, or responses. We leverage\nthese structural differences to develop a tool to detect hallucinated\nresponses, achieving an accuracy of 66\\% for a specific configuration of system\nparameters -- comparable with the best results in the field. In conclusion, the\nsuggested methodology is promising and novel, possibly paving the way for\nfurther research in the domain, also along the directions highlighted in our\nfuture work.", "published": "2025-02-10 09:44:13", "link": "http://arxiv.org/abs/2502.08663v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "From Argumentation to Deliberation: Perspectivized Stance Vectors for\n  Fine-grained (Dis)agreement Analysis", "abstract": "Debating over conflicting issues is a necessary first step towards resolving\nconflicts. However, intrinsic perspectives of an arguer are difficult to\novercome by persuasive argumentation skills. Proceeding from a debate to a\ndeliberative process, where we can identify actionable options for resolving a\nconflict requires a deeper analysis of arguments and the perspectives they are\ngrounded in - as it is only from there that one can derive mutually agreeable\nresolution steps. In this work we develop a framework for a deliberative\nanalysis of arguments in a computational argumentation setup. We conduct a\nfine-grained analysis of perspectivized stances expressed in the arguments of\ndifferent arguers or stakeholders on a given issue, aiming not only to identify\ntheir opposing views, but also shared perspectives arising from their\nattitudes, values or needs. We formalize this analysis in Perspectivized Stance\nVectors that characterize the individual perspectivized stances of all arguers\non a given issue. We construct these vectors by determining issue- and\nargument-specific concepts, and predict an arguer's stance relative to each of\nthem. The vectors allow us to measure a modulated (dis)agreement between\narguers, structured by perspectives, which allows us to identify actionable\npoints for conflict resolution, as a first step towards deliberation.", "published": "2025-02-10 13:08:46", "link": "http://arxiv.org/abs/2502.09644v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Uncertainty-Aware Adaptation of Large Language Models for\n  Protein-Protein Interaction Analysis", "abstract": "Identification of protein-protein interactions (PPIs) helps derive cellular\nmechanistic understanding, particularly in the context of complex conditions\nsuch as neurodegenerative disorders, metabolic syndromes, and cancer. Large\nLanguage Models (LLMs) have demonstrated remarkable potential in predicting\nprotein structures and interactions via automated mining of vast biomedical\nliterature; yet their inherent uncertainty remains a key challenge for deriving\nreproducible findings, critical for biomedical applications. In this study, we\npresent an uncertainty-aware adaptation of LLMs for PPI analysis, leveraging\nfine-tuned LLaMA-3 and BioMedGPT models. To enhance prediction reliability, we\nintegrate LoRA ensembles and Bayesian LoRA models for uncertainty\nquantification (UQ), ensuring confidence-calibrated insights into protein\nbehavior. Our approach achieves competitive performance in PPI identification\nacross diverse disease contexts while addressing model uncertainty, thereby\nenhancing trustworthiness and reproducibility in computational biology. These\nfindings underscore the potential of uncertainty-aware LLM adaptation for\nadvancing precision medicine and biomedical research.", "published": "2025-02-10 05:54:36", "link": "http://arxiv.org/abs/2502.06173v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.AP", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Towards Copyright Protection for Knowledge Bases of Retrieval-augmented\n  Language Models via Ownership Verification with Reasoning", "abstract": "Large language models (LLMs) are increasingly integrated into real-world\napplications through retrieval-augmented generation (RAG) mechanisms to\nsupplement their responses with up-to-date and domain-specific knowledge.\nHowever, the valuable and often proprietary nature of the knowledge bases used\nin RAG introduces the risk of unauthorized usage by adversaries. Existing\nmethods that can be generalized as watermarking techniques to protect these\nknowledge bases typically involve poisoning attacks. However, these methods\nrequire to alter the results of verification samples (\\eg, generating incorrect\noutputs), inevitably making them susceptible to anomaly detection and even\nintroduce new security risks. To address these challenges, we propose \\name{}\nfor `harmless' copyright protection of knowledge bases. Instead of manipulating\nLLM's final output, \\name{} implants distinct verification behaviors in the\nspace of chain-of-thought (CoT) reasoning, maintaining the correctness of the\nfinal answer. Our method has three main stages: (1) \\textbf{Generating CoTs}:\nFor each verification question, we generate two CoTs, including a target CoT\nfor building watermark behaviors; (2) \\textbf{Optimizing Watermark Phrases and\nTarget CoTs}: We optimize them to minimize retrieval errors under the black-box\nsetting of suspicious LLM, ensuring that the watermarked verification queries\nactivate the target CoTs without being activated in non-watermarked ones; (3)\n\\textbf{Ownership Verification}: We exploit a pairwise Wilcoxon test to\nstatistically verify whether a suspicious LLM is augmented with the protected\nknowledge base by comparing its responses to watermarked and benign\nverification queries. Our experiments on diverse benchmarks demonstrate that\n\\name{} effectively protects knowledge bases against unauthorized usage while\npreserving the integrity and performance of the RAG.", "published": "2025-02-10 09:15:56", "link": "http://arxiv.org/abs/2502.10440v1", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CR"}
{"title": "An adaptive filter bank based neural network approach for time delay\n  estimation and speech enhancement", "abstract": "Time delay estimation (TDE) plays a key role in acoustic echo cancellation\n(AEC) using adaptive filter method. Considerable residual echo will be left if\nestimation error arises. Here, in this paper, we proposed an adaptive filter\nbank based neural network approach where the delay is estimated by a bank of\nadaptive filters with overlapped time scope, and all the energy of filter\nweights are concatenated and feed to a classification network. The index with\nmaximal probability is chosen as the estimated delay. Based on this TDE, an AEC\nscheme is designed using a neural network for residual echo and noise\nsuppression, and the optimally-modified log-spectral amplitude (OMLSA)\nalgorithm is adopted to make it robust. Also, a robust automatic gain control\n(AGC) scheme with spectrum smoothing method is designed to amplify speech\nsegments. Performance evaluations reveal that higher performance can be\nachieved for our scheme.", "published": "2025-02-10 02:09:28", "link": "http://arxiv.org/abs/2502.06098v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Adaptive Central Frequencies Locally Competitive Algorithm for Speech", "abstract": "Neuromorphic computing, inspired by nervous systems, revolutionizes\ninformation processing with its focus on efficiency and low power consumption.\nUsing sparse coding, this paradigm enhances processing efficiency, which is\ncrucial for edge devices with power constraints. The Locally Competitive\nAlgorithm (LCA), adapted for audio with Gammatone and Gammachirp filter banks,\nprovides an efficient sparse coding method for neuromorphic speech processing.\nAdaptive LCA (ALCA) further refines this method by dynamically adjusting\nmodulation parameters, thereby improving reconstruction quality and sparsity.\nThis paper introduces an enhanced ALCA version, the ALCA Central Frequency\n(ALCA-CF), which dynamically adapts both modulation parameters and central\nfrequencies, optimizing the speech representation. Evaluations show that this\napproach improves reconstruction quality and sparsity while significantly\nreducing the power consumption of speech classification, without compromising\nclassification accuracy, particularly on Intel's Loihi 2 neuromorphic chip.", "published": "2025-02-10 19:34:16", "link": "http://arxiv.org/abs/2502.06989v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Automatic Identification of Samples in Hip-Hop Music via Multi-Loss\n  Training and an Artificial Dataset", "abstract": "Sampling, the practice of reusing recorded music or sounds from another\nsource in a new work, is common in popular music genres like hip-hop and rap.\nNumerous services have emerged that allow users to identify connections between\nsamples and the songs that incorporate them, with the goal of enhancing music\ndiscovery. Designing a system that can perform the same task automatically is\nchallenging, as samples are commonly altered with audio effects like pitch- and\ntime-stretching and may only be seconds long. Progress on this task has been\nminimal and is further blocked by the limited availability of training data.\nHere, we show that a convolutional neural network trained on an artificial\ndataset can identify real-world samples in commercial hip-hop music. We extract\nvocal, harmonic, and percussive elements from several databases of\nnon-commercial music recordings using audio source separation, and train the\nmodel to fingerprint a subset of these elements in transformed versions of the\noriginal audio. We optimize the model using a joint classification and metric\nlearning loss and show that it achieves 13% greater precision on real-world\ninstances of sampling than a fingerprinting system using acoustic landmarks,\nand that it can recognize samples that have been both pitch shifted and time\nstretched. We also show that, for half of the commercial music recordings we\ntested, our model is capable of locating the position of a sample to within\nfive seconds.", "published": "2025-02-10 11:30:35", "link": "http://arxiv.org/abs/2502.06364v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Learning Musical Representations for Music Performance Question\n  Answering", "abstract": "Music performances are representative scenarios for audio-visual modeling.\nUnlike common scenarios with sparse audio, music performances continuously\ninvolve dense audio signals throughout. While existing multimodal learning\nmethods on the audio-video QA demonstrate impressive capabilities in general\nscenarios, they are incapable of dealing with fundamental problems within the\nmusic performances: they underexplore the interaction between the multimodal\nsignals in performance and fail to consider the distinctive characteristics of\ninstruments and music. Therefore, existing methods tend to answer questions\nregarding musical performances inaccurately. To bridge the above research gaps,\n(i) given the intricate multimodal interconnectivity inherent to music data,\nour primary backbone is designed to incorporate multimodal interactions within\nthe context of music; (ii) to enable the model to learn music characteristics,\nwe annotate and release rhythmic and music sources in the current music\ndatasets; (iii) for time-aware audio-visual modeling, we align the model's\nmusic predictions with the temporal dimension. Our experiments show\nstate-of-the-art effects on the Music AVQA datasets. Our code is available at\nhttps://github.com/xid32/Amuse.", "published": "2025-02-10 17:41:57", "link": "http://arxiv.org/abs/2502.06710v1", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Recent Advances in Discrete Speech Tokens: A Review", "abstract": "The rapid advancement of speech generation technologies in the era of large\nlanguage models (LLMs) has established discrete speech tokens as a foundational\nparadigm for speech representation. These tokens, characterized by their\ndiscrete, compact, and concise nature, are not only advantageous for efficient\ntransmission and storage, but also inherently compatible with the language\nmodeling framework, enabling seamless integration of speech into text-dominated\nLLM architectures. Current research categorizes discrete speech tokens into two\nprincipal classes: acoustic tokens and semantic tokens, each of which has\nevolved into a rich research domain characterized by unique design philosophies\nand methodological approaches. This survey systematically synthesizes the\nexisting taxonomy and recent innovations in discrete speech tokenization,\nconducts a critical examination of the strengths and limitations of each\nparadigm, and presents systematic experimental comparisons across token types.\nFurthermore, we identify persistent challenges in the field and propose\npotential research directions, aiming to offer actionable insights to inspire\nfuture advancements in the development and application of discrete speech\ntokens.", "published": "2025-02-10 14:08:25", "link": "http://arxiv.org/abs/2502.06490v2", "categories": ["eess.AS", "cs.AI", "cs.MM", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
