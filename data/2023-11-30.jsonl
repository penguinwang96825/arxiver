{"title": "INarIG: Iterative Non-autoregressive Instruct Generation Model For\n  Word-Level Auto Completion", "abstract": "Computer-aided translation (CAT) aims to enhance human translation efficiency\nand is still important in scenarios where machine translation cannot meet\nquality requirements. One fundamental task within this field is Word-Level Auto\nCompletion (WLAC). WLAC predicts a target word given a source sentence,\ntranslation context, and a human typed character sequence. Previous works\neither employ word classification models to exploit contextual information from\nboth sides of the target word or directly disregarded the dependencies from the\nright-side context. Furthermore, the key information, i.e. human typed\nsequences, is only used as prefix constraints in the decoding module. In this\npaper, we propose the INarIG (Iterative Non-autoregressive Instruct Generation)\nmodel, which constructs the human typed sequence into Instruction Unit and\nemploys iterative decoding with subwords to fully utilize input information\ngiven in the task. Our model is more competent in dealing with low-frequency\nwords (core scenario of this task), and achieves state-of-the-art results on\nthe WMT22 and benchmark datasets, with a maximum increase of over 10%\nprediction accuracy.", "published": "2023-11-30 02:39:38", "link": "http://arxiv.org/abs/2311.18200v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Construction of a Korean Toxic Instruction Dataset for Ethical\n  Tuning of Large Language Models", "abstract": "Caution: this paper may include material that could be offensive or\ndistressing.\n  The advent of Large Language Models (LLMs) necessitates the development of\ntraining approaches that mitigate the generation of unethical language and\naptly manage toxic user queries. Given the challenges related to human labor\nand the scarcity of data, we present KoTox, comprising 39K unethical\ninstruction-output pairs. This collection of automatically generated toxic\ninstructions refines the training of LLMs and establishes a foundational\nframework for improving LLMs' ethical awareness and response to various toxic\ninputs, promoting more secure and responsible interactions in Natural Language\nProcessing (NLP) applications.", "published": "2023-11-30 03:19:45", "link": "http://arxiv.org/abs/2311.18215v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating the Rationale Understanding of Critical Reasoning in Logical\n  Reading Comprehension", "abstract": "To precisely evaluate a language model's capability for logical reading\ncomprehension, we present a dataset for testing the understanding of the\nrationale behind critical reasoning. For questions taken from an existing\nmultiplechoice logical reading comprehension dataset, we crowdsource rationale\ntexts that explain why we should select or eliminate answer options, resulting\nin 3,003 multiple-choice subquestions that are associated with 943 main\nquestions. Experiments on our dataset show that recent large language models\n(e.g., InstructGPT) struggle to answer the subquestions even if they are able\nto answer the main questions correctly. We find that the models perform\nparticularly poorly in answering subquestions written for the incorrect options\nof the main questions, implying that the models have a limited capability for\nexplaining why incorrect alternatives should be eliminated. These results\nsuggest that our dataset encourages further investigation into the critical\nreasoning ability of language models while focusing on the elimination process\nof relevant alternatives.", "published": "2023-11-30 08:44:55", "link": "http://arxiv.org/abs/2311.18353v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IAG: Induction-Augmented Generation Framework for Answering Reasoning\n  Questions", "abstract": "Retrieval-Augmented Generation (RAG), by incorporating external knowledge\nwith parametric memory of language models, has become the state-of-the-art\narchitecture for open-domain QA tasks. However, common knowledge bases are\ninherently constrained by limited coverage and noisy information, making\nretrieval-based approaches inadequate to answer implicit reasoning questions.\nIn this paper, we propose an Induction-Augmented Generation (IAG) framework\nthat utilizes inductive knowledge along with the retrieved documents for\nimplicit reasoning. We leverage large language models (LLMs) for deriving such\nknowledge via a novel prompting method based on inductive reasoning patterns.\nOn top of this, we implement two versions of IAG named IAG-GPT and IAG-Student,\nrespectively. IAG-GPT directly utilizes the knowledge generated by GPT-3 for\nanswer prediction, while IAG-Student gets rid of dependencies on GPT service at\ninference time by incorporating a student inductor model. The inductor is\nfirstly trained via knowledge distillation and further optimized by\nback-propagating the generator feedback via differentiable beam scores.\nExperimental results show that IAG outperforms RAG baselines as well as ChatGPT\non two Open-Domain QA tasks. Notably, our best models have won the first place\nin the official leaderboards of CSQA2.0 (since Nov 1, 2022) and StrategyQA\n(since Jan 8, 2023).", "published": "2023-11-30 09:48:51", "link": "http://arxiv.org/abs/2311.18397v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Causal Influence of Grammatical Gender on Distributional Semantics", "abstract": "How much meaning influences gender assignment across languages is an active\narea of research in linguistics and cognitive science. We can view current\napproaches as aiming to determine where gender assignment falls on a spectrum,\nfrom being fully arbitrarily determined to being largely semantically\ndetermined. For the latter case, there is a formulation of the neo-Whorfian\nhypothesis, which claims that even inanimate noun gender influences how people\nconceive of and talk about objects (using the choice of adjective used to\nmodify inanimate nouns as a proxy for meaning). We offer a novel, causal\ngraphical model that jointly represents the interactions between a noun's\ngrammatical gender, its meaning, and adjective choice. In accordance with past\nresults, we find a significant relationship between the gender of nouns and the\nadjectives that modify them. However, when we control for the meaning of the\nnoun, the relationship between grammatical gender and adjective choice is near\nzero and insignificant.", "published": "2023-11-30 13:58:13", "link": "http://arxiv.org/abs/2311.18567v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ArthModel: Enhance Arithmetic Skills to Large Language Model", "abstract": "With the great success of ChatGPT, the research of large language models has\nbecome increasingly popular. However, the models have several limitations, such\nas toxicity and pool performance of arithmetic solving. Meanwhile, LLM may have\nsome potential abilities that have yet to be exploited. In this paper, we\nchoose a different way to enhance the arithmetic ability of LLM. We propose to\ntrain LLM to generate a postfix expression related to the arithmetic problem\nand incorporate it with small pretrained models. Moreover, this small model\ntransfers the token embeddings into real dense numbers and invokes native\nfunctions of a deep learning platform to get the correct answer. To generate\nthe final result, we propose prompt injection for adding the result outputs by\nthe small model to LLM. This work provides different ways of thinking, training\nand using a language model. The codes and models will be released at\n\\url{https://github.com/eteced/arithmetic_finetuning_v1}.", "published": "2023-11-30 15:06:50", "link": "http://arxiv.org/abs/2311.18609v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ArcMMLU: A Library and Information Science Benchmark for Large Language\n  Models", "abstract": "In light of the rapidly evolving capabilities of large language models\n(LLMs), it becomes imperative to develop rigorous domain-specific evaluation\nbenchmarks to accurately assess their capabilities. In response to this need,\nthis paper introduces ArcMMLU, a specialized benchmark tailored for the Library\n& Information Science (LIS) domain in Chinese. This benchmark aims to measure\nthe knowledge and reasoning capability of LLMs within four key sub-domains:\nArchival Science, Data Science, Library Science, and Information Science.\nFollowing the format of MMLU/CMMLU, we collected over 6,000 high-quality\nquestions for the compilation of ArcMMLU. This extensive compilation can\nreflect the diverse nature of the LIS domain and offer a robust foundation for\nLLM evaluation. Our comprehensive evaluation reveals that while most mainstream\nLLMs achieve an average accuracy rate above 50% on ArcMMLU, there remains a\nnotable performance gap, suggesting substantial headroom for refinement in LLM\ncapabilities within the LIS domain. Further analysis explores the effectiveness\nof few-shot examples on model performance and highlights challenging questions\nwhere models consistently underperform, providing valuable insights for\ntargeted improvements. ArcMMLU fills a critical gap in LLM evaluations within\nthe Chinese LIS domain and paves the way for future development of LLMs\ntailored to this specialized area.", "published": "2023-11-30 16:08:04", "link": "http://arxiv.org/abs/2311.18658v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Women Are Beautiful, Men Are Leaders: Gender Stereotypes in Machine\n  Translation and Language Modeling", "abstract": "We present GEST -- a new manually created dataset designed to measure\ngender-stereotypical reasoning in language models and machine translation\nsystems. GEST contains samples for 16 gender stereotypes about men and women\n(e.g., Women are beautiful, Men are leaders) that are compatible with the\nEnglish language and 9 Slavic languages. The definition of said stereotypes was\ninformed by gender experts. We used GEST to evaluate English and Slavic masked\nLMs, English generative LMs, and machine translation systems. We discovered\nsignificant and consistent amounts of gender-stereotypical reasoning in almost\nall the evaluated models and languages. Our experiments confirm the previously\npostulated hypothesis that the larger the model, the more stereotypical it\nusually is.", "published": "2023-11-30 17:06:00", "link": "http://arxiv.org/abs/2311.18711v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CoRec: An Easy Approach for Coordination Recognition", "abstract": "In this paper, we observe and address the challenges of the coordination\nrecognition task. Most existing methods rely on syntactic parsers to identify\nthe coordinators in a sentence and detect the coordination boundaries. However,\nstate-of-the-art syntactic parsers are slow and suffer from errors, especially\nfor long and complicated sentences. To better solve the problems, we propose a\npipeline model COordination RECognizer (CoRec). It consists of two components:\ncoordinator identifier and conjunct boundary detector. The experimental results\non datasets from various domains demonstrate the effectiveness and efficiency\nof the proposed method. Further experiments show that CoRec positively impacts\ndownstream tasks, improving the yield of state-of-the-art Open IE models.", "published": "2023-11-30 17:11:27", "link": "http://arxiv.org/abs/2311.18712v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mavericks at ArAIEval Shared Task: Towards a Safer Digital Space --\n  Transformer Ensemble Models Tackling Deception and Persuasion", "abstract": "In this paper, we highlight our approach for the \"Arabic AI Tasks Evaluation\n(ArAiEval) Shared Task 2023\". We present our approaches for task 1-A and task\n2-A of the shared task which focus on persuasion technique detection and\ndisinformation detection respectively. Detection of persuasion techniques and\ndisinformation has become imperative to avoid distortion of authentic\ninformation. The tasks use multigenre snippets of tweets and news articles for\nthe given binary classification problem. We experiment with several\ntransformer-based models that were pre-trained on the Arabic language. We\nfine-tune these state-of-the-art models on the provided dataset. Ensembling is\nemployed to enhance the performance of the systems. We achieved a micro\nF1-score of 0.742 on task 1-A (8th rank on the leaderboard) and 0.901 on task\n2-A (7th rank on the leaderboard) respectively.", "published": "2023-11-30 17:26:57", "link": "http://arxiv.org/abs/2311.18730v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mavericks at NADI 2023 Shared Task: Unravelling Regional Nuances through\n  Dialect Identification using Transformer-based Approach", "abstract": "In this paper, we present our approach for the \"Nuanced Arabic Dialect\nIdentification (NADI) Shared Task 2023\". We highlight our methodology for\nsubtask 1 which deals with country-level dialect identification. Recognizing\ndialects plays an instrumental role in enhancing the performance of various\ndownstream NLP tasks such as speech recognition and translation. The task uses\nthe Twitter dataset (TWT-2023) that encompasses 18 dialects for the multi-class\nclassification problem. Numerous transformer-based models, pre-trained on\nArabic language, are employed for identifying country-level dialects. We\nfine-tune these state-of-the-art models on the provided dataset. The ensembling\nmethod is leveraged to yield improved performance of the system. We achieved an\nF1-score of 76.65 (11th rank on the leaderboard) on the test dataset.", "published": "2023-11-30 17:37:56", "link": "http://arxiv.org/abs/2311.18739v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can training neural language models on a curriculum with developmentally\n  plausible data improve alignment with human reading behavior?", "abstract": "The use of neural language models to model human behavior has met with mixed\nsuccess. While some work has found that the surprisal estimates from these\nmodels can be used to predict a wide range of human neural and behavioral\nresponses, other work studying more complex syntactic phenomena has found that\nthese surprisal estimates generate incorrect behavioral predictions. This paper\nexplores the extent to which the misalignment between empirical and\nmodel-predicted behavior can be minimized by training models on more\ndevelopmentally plausible data, such as in the BabyLM Challenge. We trained\nteacher language models on the BabyLM \"strict-small\" dataset and used sentence\nlevel surprisal estimates from these teacher models to create a curriculum. We\nfound tentative evidence that our curriculum made it easier for models to\nacquire linguistic knowledge from the training data: on the subset of tasks in\nthe BabyLM challenge suite evaluating models' grammatical knowledge of English,\nmodels first trained on the BabyLM data curriculum and then on a few randomly\nordered training epochs performed slightly better than models trained on\nrandomly ordered epochs alone. This improved linguistic knowledge acquisition\ndid not result in better alignment with human reading behavior, however: models\ntrained on the BabyLM dataset (with or without a curriculum) generated\npredictions that were as misaligned with human behavior as models trained on\nlarger less curated datasets. This suggests that training on developmentally\nplausible datasets alone is likely insufficient to generate language models\ncapable of accurately predicting human language processing.", "published": "2023-11-30 18:03:58", "link": "http://arxiv.org/abs/2311.18761v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mavericks at BLP-2023 Task 1: Ensemble-based Approach Using Language\n  Models for Violence Inciting Text Detection", "abstract": "This paper presents our work for the Violence Inciting Text Detection shared\ntask in the First Workshop on Bangla Language Processing. Social media has\naccelerated the propagation of hate and violence-inciting speech in society. It\nis essential to develop efficient mechanisms to detect and curb the propagation\nof such texts. The problem of detecting violence-inciting texts is further\nexacerbated in low-resource settings due to sparse research and less data. The\ndata provided in the shared task consists of texts in the Bangla language,\nwhere each example is classified into one of the three categories defined based\non the types of violence-inciting texts. We try and evaluate several BERT-based\nmodels, and then use an ensemble of the models as our final submission. Our\nsubmission is ranked 10th in the final leaderboard of the shared task with a\nmacro F1 score of 0.737.", "published": "2023-11-30 18:23:38", "link": "http://arxiv.org/abs/2311.18778v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What Do Llamas Really Think? Revealing Preference Biases in Language\n  Model Representations", "abstract": "Do large language models (LLMs) exhibit sociodemographic biases, even when\nthey decline to respond? To bypass their refusal to \"speak,\" we study this\nresearch question by probing contextualized embeddings and exploring whether\nthis bias is encoded in its latent representations. We propose a logistic\nBradley-Terry probe which predicts word pair preferences of LLMs from the\nwords' hidden vectors. We first validate our probe on three pair preference\ntasks and thirteen LLMs, where we outperform the word embedding association\ntest (WEAT), a standard approach in testing for implicit association, by a\nrelative 27% in error rate. We also find that word pair preferences are best\nrepresented in the middle layers. Next, we transfer probes trained on harmless\ntasks (e.g., pick the larger number) to controversial ones (compare\nethnicities) to examine biases in nationality, politics, religion, and gender.\nWe observe substantial bias for all target classes: for instance, the Mistral\nmodel implicitly prefers Europe to Africa, Christianity to Judaism, and\nleft-wing to right-wing politics, despite declining to answer. This suggests\nthat instruction fine-tuning does not necessarily debias contextualized\nembeddings. Our codebase is at https://github.com/castorini/biasprobe.", "published": "2023-11-30 18:53:13", "link": "http://arxiv.org/abs/2311.18812v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Introducing Rhetorical Parallelism Detection: A New Task with Datasets,\n  Metrics, and Baselines", "abstract": "Rhetoric, both spoken and written, involves not only content but also style.\nOne common stylistic tool is $\\textit{parallelism}$: the juxtaposition of\nphrases which have the same sequence of linguistic ($\\textit{e.g.}$,\nphonological, syntactic, semantic) features. Despite the ubiquity of\nparallelism, the field of natural language processing has seldom investigated\nit, missing a chance to better understand the nature of the structure, meaning,\nand intent that humans convey. To address this, we introduce the task of\n$\\textit{rhetorical parallelism detection}$. We construct a formal definition\nof it; we provide one new Latin dataset and one adapted Chinese dataset for it;\nwe establish a family of metrics to evaluate performance on it; and, lastly, we\ncreate baseline systems and novel sequence labeling schemes to capture it. On\nour strictest metric, we attain $F_{1}$ scores of $0.40$ and $0.43$ on our\nLatin and Chinese datasets, respectively.", "published": "2023-11-30 15:24:57", "link": "http://arxiv.org/abs/2312.00100v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Navigating News Narratives: A Media Bias Analysis Dataset", "abstract": "The proliferation of biased news narratives across various media platforms\nhas become a prominent challenge, influencing public opinion on critical topics\nlike politics, health, and climate change. This paper introduces the\n\"Navigating News Narratives: A Media Bias Analysis Dataset\", a comprehensive\ndataset to address the urgent need for tools to detect and analyze media bias.\nThis dataset encompasses a broad spectrum of biases, making it a unique and\nvaluable asset in the field of media studies and artificial intelligence. The\ndataset is available at\nhttps://huggingface.co/datasets/newsmediabias/news-bias-full-data.", "published": "2023-11-30 19:59:19", "link": "http://arxiv.org/abs/2312.00168v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Relevance-guided Neural Machine Translation", "abstract": "With the advent of the Transformer architecture, Neural Machine Translation\n(NMT) results have shown great improvement lately. However, results in\nlow-resource conditions still lag behind in both bilingual and multilingual\nsetups, due to the limited amount of available monolingual and/or parallel\ndata; hence, the need for methods addressing data scarcity in an efficient, and\nexplainable way, is eminent. We propose an explainability-based training\napproach for NMT, applied in Unsupervised and Supervised model training, for\ntranslation of three languages of varying resources, French, Gujarati, Kazakh,\nto and from English. Our results show our method can be promising, particularly\nwhen training in low-resource conditions, outperforming simple training\nbaselines; though the improvement is marginal, it sets the ground for further\nexploration of the approach and the parameters, and its extension to other\nlanguages.", "published": "2023-11-30 21:52:02", "link": "http://arxiv.org/abs/2312.00214v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Positional Information Matters for Invariant In-Context Learning: A Case\n  Study of Simple Function Classes", "abstract": "In-context learning (ICL) refers to the ability of a model to condition on a\nfew in-context demonstrations (input-output examples of the underlying task) to\ngenerate the answer for a new query input, without updating parameters. Despite\nthe impressive ICL ability of LLMs, it has also been found that ICL in LLMs is\nsensitive to input demonstrations and limited to short context lengths. To\nunderstand the limitations and principles for successful ICL, we conduct an\ninvestigation with ICL linear regression of transformers. We characterize\nseveral Out-of-Distribution (OOD) cases for ICL inspired by realistic LLM ICL\nfailures and compare transformers with DeepSet, a simple yet powerful\narchitecture for ICL. Surprisingly, DeepSet outperforms transformers across a\nvariety of distribution shifts, implying that preserving permutation invariance\nsymmetry to input demonstrations is crucial for OOD ICL. The phenomenon\nspecifies a fundamental requirement by ICL, which we termed as ICL invariance.\nNevertheless, the positional encodings in LLMs will break ICL invariance. To\nthis end, we further evaluate transformers with identical positional encodings\nand find preserving ICL invariance in transformers achieves state-of-the-art\nperformance across various ICL distribution shifts", "published": "2023-11-30 02:26:55", "link": "http://arxiv.org/abs/2311.18194v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "COVID-19 Vaccine Misinformation in Middle Income Countries", "abstract": "This paper introduces a multilingual dataset of COVID-19 vaccine\nmisinformation, consisting of annotated tweets from three middle-income\ncountries: Brazil, Indonesia, and Nigeria. The expertly curated dataset\nincludes annotations for 5,952 tweets, assessing their relevance to COVID-19\nvaccines, presence of misinformation, and the themes of the misinformation. To\naddress challenges posed by domain specificity, the low-resource setting, and\ndata imbalance, we adopt two approaches for developing COVID-19 vaccine\nmisinformation detection models: domain-specific pre-training and text\naugmentation using a large language model. Our best misinformation detection\nmodels demonstrate improvements ranging from 2.7 to 15.9 percentage points in\nmacro F1-score compared to the baseline models. Additionally, we apply our\nmisinformation detection models in a large-scale study of 19 million unlabeled\ntweets from the three countries between 2020 and 2022, showcasing the practical\napplication of our dataset and models for detecting and analyzing vaccine\nmisinformation in multiple countries and languages. Our analysis indicates that\npercentage changes in the number of new COVID-19 cases are positively\nassociated with COVID-19 vaccine misinformation rates in a staggered manner for\nBrazil and Indonesia, and there are significant positive associations between\nthe misinformation rates across the three countries.", "published": "2023-11-30 02:27:34", "link": "http://arxiv.org/abs/2311.18195v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "mPLUG-PaperOwl: Scientific Diagram Analysis with the Multimodal Large\n  Language Model", "abstract": "Recently, the strong text creation ability of Large Language Models(LLMs) has\ngiven rise to many tools for assisting paper reading or even writing. However,\nthe weak diagram analysis abilities of LLMs or Multimodal LLMs greatly limit\ntheir application scenarios, especially for scientific academic paper writing.\nIn this work, towards a more versatile copilot for academic paper writing, we\nmainly focus on strengthening the multi-modal diagram analysis ability of\nMultimodal LLMs. By parsing Latex source files of high-quality papers, we\ncarefully build a multi-modal diagram understanding dataset M-Paper. By\naligning diagrams in the paper with related paragraphs, we construct\nprofessional diagram analysis samples for training and evaluation. M-Paper is\nthe first dataset to support joint comprehension of multiple scientific\ndiagrams, including figures and tables in the format of images or Latex codes.\nBesides, to better align the copilot with the user's intention, we introduce\nthe `outline' as the control signal, which could be directly given by the user\nor revised based on auto-generated ones. Comprehensive experiments with a\nstate-of-the-art Mumtimodal LLM demonstrate that training on our dataset shows\nstronger scientific diagram understanding performance, including diagram\ncaptioning, diagram analysis, and outline recommendation. The dataset, code,\nand model are available at\nhttps://github.com/X-PLUG/mPLUG-DocOwl/tree/main/PaperOwl.", "published": "2023-11-30 04:43:26", "link": "http://arxiv.org/abs/2311.18248v2", "categories": ["cs.MM", "cs.CL"], "primary_category": "cs.MM"}
{"title": "FFT: Towards Harmlessness Evaluation and Analysis for LLMs with\n  Factuality, Fairness, Toxicity", "abstract": "The widespread of generative artificial intelligence has heightened concerns\nabout the potential harms posed by AI-generated texts, primarily stemming from\nfactoid, unfair, and toxic content. Previous researchers have invested much\neffort in assessing the harmlessness of generative language models. However,\nexisting benchmarks are struggling in the era of large language models (LLMs),\ndue to the stronger language generation and instruction following capabilities,\nas well as wider applications. In this paper, we propose FFT, a new benchmark\nwith 2116 elaborated-designed instances, for LLM harmlessness evaluation with\nfactuality, fairness, and toxicity. To investigate the potential harms of LLMs,\nwe evaluate 9 representative LLMs covering various parameter scales, training\nstages, and creators. Experiments show that the harmlessness of LLMs is still\nunder-satisfactory, and extensive analysis derives some insightful findings\nthat could inspire future research for harmless LLM research.", "published": "2023-11-30 14:18:47", "link": "http://arxiv.org/abs/2311.18580v2", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "RaDialog: A Large Vision-Language Model for Radiology Report Generation\n  and Conversational Assistance", "abstract": "Conversational AI tools that can generate and discuss clinically correct\nradiology reports for a given medical image have the potential to transform\nradiology. Such a human-in-the-loop radiology assistant could facilitate a\ncollaborative diagnostic process, thus saving time and improving the quality of\nreports. Towards this goal, we introduce RaDialog, the first thoroughly\nevaluated and publicly available large vision-language model for radiology\nreport generation and interactive dialog. RaDialog effectively integrates\nvisual image features and structured pathology findings with a large language\nmodel (LLM) while simultaneously adapting it to a specialized domain using\nparameter-efficient fine-tuning. To keep the conversational abilities of the\nunderlying LLM, we propose a comprehensive, semi-automatically labeled,\nimage-grounded instruct dataset for chest X-ray radiology tasks. By training\nwith this dataset, our method achieves state-of-the-art clinical correctness in\nreport generation and shows impressive abilities in interactive tasks such as\ncorrecting reports and answering questions, serving as a foundational step\ntoward clinical dialog systems. Our code is available on github:\nhttps://github.com/ChantalMP/RaDialog.", "published": "2023-11-30 16:28:40", "link": "http://arxiv.org/abs/2311.18681v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "CritiqueLLM: Towards an Informative Critique Generation Model for\n  Evaluation of Large Language Model Generation", "abstract": "Since the natural language processing (NLP) community started to make large\nlanguage models (LLMs) act as a critic to evaluate the quality of generated\ntexts, most of the existing works train a critique generation model on the\nevaluation data labeled by GPT-4's direct prompting. We observe that these\nmodels lack the ability to generate informative critiques in both pointwise\ngrading and pairwise comparison especially without references. As a result,\ntheir generated critiques cannot provide fine-grained distinguishability on\ngenerated texts, causing unsatisfactory evaluation performance. In this paper,\nwe propose a simple yet effective method called Eval-Instruct, which can first\nacquire pointwise grading critiques with pseudo references and then revise\nthese critiques via multi-path prompting to obtain informative evaluation data\nin different tasks and settings, including pointwise grading and pairwise\ncomparison with / without references. After fine-tuning on these data, the\nresulting model CritiqueLLM is empirically shown to outperform ChatGPT and all\nthe open-source baselines and even achieve comparable evaluation performance to\nGPT-4 in system-level correlations of pointwise grading. We also demonstrate\nthat our generated critiques can act as scalable feedback to further improve\nthe generation quality of strong LLMs like ChatGPT.", "published": "2023-11-30 16:52:42", "link": "http://arxiv.org/abs/2311.18702v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TaskBench: Benchmarking Large Language Models for Task Automation", "abstract": "In recent years, the remarkable progress of large language models (LLMs) has\nsparked interest in task automation, which involves decomposing complex tasks\ndescribed by user instructions into sub-tasks and invoking external tools to\nexecute them, playing a central role in autonomous agents. However, there is a\nlack of systematic and standardized benchmarks to promote the development of\nLLMs in task automation. To address this, we introduce TaskBench, a\ncomprehensive framework to evaluate the capability of LLMs in task automation.\nSpecifically, task automation can be divided into three critical stages: task\ndecomposition, tool selection, and parameter prediction. To tackle the\ncomplexities inherent in these stages, we introduce the concept of Tool Graph\nto represent decomposed tasks and adopt a back-instruct method to generate\nhigh-quality user instructions. We propose TaskEval, a multi-faceted evaluation\nmethodology that assesses LLM performance across these three stages. Our\napproach combines automated construction with rigorous human verification,\nensuring high consistency with human evaluation. Experimental results\ndemonstrate that TaskBench effectively reflects the capabilities of various\nLLMs in task automation. It provides insights into model performance across\ndifferent task complexities and domains, pushing the boundaries of what current\nmodels can achieve. TaskBench offers a scalable, adaptable, and reliable\nbenchmark for advancing LLM-based autonomous agents.", "published": "2023-11-30 18:02:44", "link": "http://arxiv.org/abs/2311.18760v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "X-InstructBLIP: A Framework for aligning X-Modal instruction-aware\n  representations to LLMs and Emergent Cross-modal Reasoning", "abstract": "Recent research has achieved significant advancements in visual reasoning\ntasks through learning image-to-language projections and leveraging the\nimpressive reasoning abilities of Large Language Models (LLMs). This paper\nintroduces an efficient and effective framework that integrates multiple\nmodalities (images, 3D, audio and video) to a frozen LLM and demonstrates an\nemergent ability for cross-modal reasoning (2+ modality inputs). Our approach\nexplores two distinct projection mechanisms: Q-Formers and Linear Projections\n(LPs). Through extensive experimentation across all four modalities on 16\nbenchmarks, we explore both methods and assess their adaptability in integrated\nand separate cross-modal reasoning. The Q-Former projection demonstrates\nsuperior performance in single modality scenarios and adaptability in joint\nversus discriminative reasoning involving two or more modalities. However, it\nexhibits lower generalization capabilities than linear projection in contexts\nwhere task-modality data are limited. To enable this framework, we devise a\nscalable pipeline that automatically generates high-quality, instruction-tuning\ndatasets from readily available captioning data across different modalities,\nand contribute 24K QA data for audio and 250K QA data for 3D. To facilitate\nfurther research in cross-modal reasoning, we introduce the DisCRn\n(Discriminative Cross-modal Reasoning) benchmark comprising 9K audio-video QA\nsamples and 28K image-3D QA samples that require the model to reason\ndiscriminatively across disparate input modalities.", "published": "2023-11-30 18:43:51", "link": "http://arxiv.org/abs/2311.18799v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Unnatural Error Correction: GPT-4 Can Almost Perfectly Handle Unnatural\n  Scrambled Text", "abstract": "While Large Language Models (LLMs) have achieved remarkable performance in\nmany tasks, much about their inner workings remains unclear. In this study, we\npresent novel experimental insights into the resilience of LLMs, particularly\nGPT-4, when subjected to extensive character-level permutations. To investigate\nthis, we first propose the Scrambled Bench, a suite designed to measure the\ncapacity of LLMs to handle scrambled input, in terms of both recovering\nscrambled sentences and answering questions given scrambled context. The\nexperimental results indicate that most powerful LLMs demonstrate the\ncapability akin to typoglycemia, a phenomenon where humans can understand the\nmeaning of words even when the letters within those words are scrambled, as\nlong as the first and last letters remain in place. More surprisingly, we found\nthat only GPT-4 nearly flawlessly processes inputs with unnatural errors, even\nunder the extreme condition, a task that poses significant challenges for other\nLLMs and often even for humans. Specifically, GPT-4 can almost perfectly\nreconstruct the original sentences from scrambled ones, decreasing the edit\ndistance by 95%, even when all letters within each word are entirely scrambled.\nIt is counter-intuitive that LLMs can exhibit such resilience despite severe\ndisruption to input tokenization caused by scrambled text.", "published": "2023-11-30 18:51:38", "link": "http://arxiv.org/abs/2311.18805v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Video is Worth 10,000 Words: Training and Benchmarking with Diverse\n  Captions for Better Long Video Retrieval", "abstract": "Existing long video retrieval systems are trained and tested in the\nparagraph-to-video retrieval regime, where every long video is described by a\nsingle long paragraph. This neglects the richness and variety of possible valid\ndescriptions of a video, which could range anywhere from moment-by-moment\ndetail to a single phrase summary. To provide a more thorough evaluation of the\ncapabilities of long video retrieval systems, we propose a pipeline that\nleverages state-of-the-art large language models to carefully generate a\ndiverse set of synthetic captions for long videos. We validate this pipeline's\nfidelity via rigorous human inspection. We use synthetic captions from this\npipeline to perform a benchmark of a representative set of video language\nmodels using long video datasets, and show that the models struggle on shorter\ncaptions. We show that finetuning on this data can both mitigate these issues\n(+2.8% R@1 over SOTA on ActivityNet with diverse captions), and even improve\nperformance on standard paragraph-to-video retrieval (+1.0% R@1 on\nActivityNet). We also use synthetic data from our pipeline as query expansion\nin the zero-shot setting (+3.4% R@1 on ActivityNet). We derive insights by\nanalyzing failure cases for retrieval with short captions. For data access and\nother details, please refer to our project website at\nhttps://mgwillia.github.io/10k-words.", "published": "2023-11-30 18:59:45", "link": "http://arxiv.org/abs/2312.00115v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Robust Concept Erasure via Kernelized Rate-Distortion Maximization", "abstract": "Distributed representations provide a vector space that captures meaningful\nrelationships between data instances. The distributed nature of these\nrepresentations, however, entangles together multiple attributes or concepts of\ndata instances (e.g., the topic or sentiment of a text, characteristics of the\nauthor (age, gender, etc), etc). Recent work has proposed the task of concept\nerasure, in which rather than making a concept predictable, the goal is to\nremove an attribute from distributed representations while retaining other\ninformation from the original representation space as much as possible. In this\npaper, we propose a new distance metric learning-based objective, the\nKernelized Rate-Distortion Maximizer (KRaM), for performing concept erasure.\nKRaM fits a transformation of representations to match a specified distance\nmeasure (defined by a labeled concept to erase) using a modified\nrate-distortion function. Specifically, KRaM's objective function aims to make\ninstances with similar concept labels dissimilar in the learned representation\nspace while retaining other information. We find that optimizing KRaM\neffectively erases various types of concepts: categorical, continuous, and\nvector-valued variables from data representations across diverse domains. We\nalso provide a theoretical analysis of several properties of KRaM's objective.\nTo assess the quality of the learned representations, we propose an alignment\nscore to evaluate their similarity with the original representation space.\nAdditionally, we conduct experiments to showcase KRaM's efficacy in various\nsettings, from erasing binary gender variables in word embeddings to\nvector-valued variables in GPT-3 representations.", "published": "2023-11-30 21:10:44", "link": "http://arxiv.org/abs/2312.00194v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Applying Large Language Models and Chain-of-Thought for Automatic\n  Scoring", "abstract": "This study investigates the application of large language models (LLMs),\nspecifically GPT-3.5 and GPT-4, with Chain-of-Though (CoT) in the automatic\nscoring of student-written responses to science assessments. We focused on\novercoming the challenges of accessibility, technical complexity, and lack of\nexplainability that have previously limited the use of artificial\nintelligence-based automatic scoring tools among researchers and educators.\nWith a testing dataset comprising six assessment tasks (three binomial and\nthree trinomial) with 1,650 student responses, we employed six prompt\nengineering strategies to automatically score student responses. The six\nstrategies combined zero-shot or few-shot learning with CoT, either alone or\nalongside item stem and scoring rubrics. Results indicated that few-shot (acc =\n.67) outperformed zero-shot learning (acc = .60), with 12.6% increase. CoT,\nwhen used without item stem and scoring rubrics, did not significantly affect\nscoring accuracy (acc = .60). However, CoT prompting paired with contextual\nitem stems and rubrics proved to be a significant contributor to scoring\naccuracy (13.44% increase for zero-shot; 3.7% increase for few-shot). We found\na more balanced accuracy across different proficiency categories when CoT was\nused with a scoring rubric, highlighting the importance of domain-specific\nreasoning in enhancing the effectiveness of LLMs in scoring tasks. We also\nfound that GPT-4 demonstrated superior performance over GPT -3.5 in various\nscoring tasks when combined with the single-call greedy sampling or ensemble\nvoting nucleus sampling strategy, showing 8.64% difference. Particularly, the\nsingle-call greedy sampling strategy with GPT-4 outperformed other approaches.", "published": "2023-11-30 21:22:43", "link": "http://arxiv.org/abs/2312.03748v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LMRL Gym: Benchmarks for Multi-Turn Reinforcement Learning with Language\n  Models", "abstract": "Large language models (LLMs) provide excellent text-generation capabilities,\nbut standard prompting and generation methods generally do not lead to\nintentional or goal-directed agents and might necessitate considerable prompt\ntuning. This becomes particularly apparent in multi-turn conversations: even\nthe best current LLMs rarely ask clarifying questions, engage in explicit\ninformation gathering, or take actions now that lead to better decisions after\nmultiple turns. Reinforcement learning has the potential to leverage the\npowerful modeling capabilities of LLMs, as well as their internal\nrepresentation of textual interactions, to create capable goal-directed\nlanguage agents. This can enable intentional and temporally extended\ninteractions, such as with humans, through coordinated persuasion and carefully\ncrafted questions, or in goal-directed play through text games to bring about\ndesired final outcomes. However, enabling this requires the community to\ndevelop stable and reliable reinforcement learning algorithms that can\neffectively train LLMs. Developing such algorithms requires tasks that can\ngauge progress on algorithm design, provide accessible and reproducible\nevaluations for multi-turn interactions, and cover a range of task properties\nand challenges in improving reinforcement learning algorithms. Our paper\nintroduces the LMRL-Gym benchmark for evaluating multi-turn RL for LLMs,\ntogether with an open-source research framework containing a basic toolkit for\ngetting started on multi-turn RL with offline value-based and policy-based RL\nmethods. Our benchmark consists of 8 different language tasks, which require\nmultiple rounds of language interaction and cover a range of tasks in\nopen-ended dialogue and text games.", "published": "2023-11-30 03:59:31", "link": "http://arxiv.org/abs/2311.18232v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Consensus, dissensus and synergy between clinicians and specialist\n  foundation models in radiology report generation", "abstract": "Radiology reports are an instrumental part of modern medicine, informing key\nclinical decisions such as diagnosis and treatment. The worldwide shortage of\nradiologists, however, restricts access to expert care and imposes heavy\nworkloads, contributing to avoidable errors and delays in report delivery.\nWhile recent progress in automated report generation with vision-language\nmodels offer clear potential in ameliorating the situation, the path to\nreal-world adoption has been stymied by the challenge of evaluating the\nclinical quality of AI-generated reports. In this study, we build a\nstate-of-the-art report generation system for chest radiographs,\n$\\textit{Flamingo-CXR}$, by fine-tuning a well-known vision-language foundation\nmodel on radiology data. To evaluate the quality of the AI-generated reports, a\ngroup of 16 certified radiologists provide detailed evaluations of AI-generated\nand human written reports for chest X-rays from an intensive care setting in\nthe United States and an inpatient setting in India. At least one radiologist\n(out of two per case) preferred the AI report to the ground truth report in\nover 60$\\%$ of cases for both datasets. Amongst the subset of AI-generated\nreports that contain errors, the most frequently cited reasons were related to\nthe location and finding, whereas for human written reports, most mistakes were\nrelated to severity and finding. This disparity suggested potential\ncomplementarity between our AI system and human experts, prompting us to\ndevelop an assistive scenario in which Flamingo-CXR generates a first-draft\nreport, which is subsequently revised by a clinician. This is the first\ndemonstration of clinician-AI collaboration for report writing, and the\nresultant reports are assessed to be equivalent or preferred by at least one\nradiologist to reports written by experts alone in 80$\\%$ of in-patient cases\nand 60$\\%$ of intensive care cases.", "published": "2023-11-30 05:38:34", "link": "http://arxiv.org/abs/2311.18260v3", "categories": ["eess.IV", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "eess.IV"}
{"title": "Hubness Reduction Improves Sentence-BERT Semantic Spaces", "abstract": "Semantic representations of text, i.e. representations of natural language\nwhich capture meaning by geometry, are essential for areas such as information\nretrieval and document grouping. High-dimensional trained dense vectors have\nreceived much attention in recent years as such representations. We investigate\nthe structure of semantic spaces that arise from embeddings made with\nSentence-BERT and find that the representations suffer from a well-known\nproblem in high dimensions called hubness. Hubness results in asymmetric\nneighborhood relations, such that some texts (the hubs) are neighbours of many\nother texts while most texts (so-called anti-hubs), are neighbours of few or no\nother texts. We quantify the semantic quality of the embeddings using hubness\nscores and error rate of a neighbourhood based classifier. We find that when\nhubness is high, we can reduce error rate and hubness using hubness reduction\nmethods. We identify a combination of two methods as resulting in the best\nreduction. For example, on one of the tested pretrained models, this combined\nmethod can reduce hubness by about 75% and error rate by about 9%. Thus, we\nargue that mitigating hubness in the embedding space provides better semantic\nrepresentations of text.", "published": "2023-11-30 09:03:49", "link": "http://arxiv.org/abs/2311.18364v1", "categories": ["cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Use of explicit replies as coordination mechanisms in online student\n  debate", "abstract": "People in conversation entrain their linguistic behaviours through\nspontaneous alignment mechanisms [7] - both in face-to-face and\ncomputer-mediated communication (CMC) [8]. In CMC, one of the mechanisms\nthrough which linguistic entrainment happens is through explicit replies.\nIndeed, the use of explicit replies influences the structure of conversations,\nfavouring the formation of reply-trees typically delineated by topic shifts\n[5]. The interpersonal coordination mechanisms realized by how actors address\neach other have been studied using a probabilistic framework proposed by David\nGibson [2,3]. Other recent approaches use computational methods and information\ntheory to quantify changes in text. We explore coordination mechanisms\nconcerned with some of the roles utterances play in dialogues - specifically in\nexplicit replies. We identify these roles by finding community structure in the\nconversation's vocabulary using a non-parametric, hierarchical topic model.\nSome conversations may always stay on the ground, remaining at the level of\ngeneral introductory chatter. Some others may develop a specific sub-topic in\nsignificant depth and detail. Even others may jump between general chatter,\nout-of-topic remarks and people agreeing or disagreeing without further\nelaboration.", "published": "2023-11-30 11:18:45", "link": "http://arxiv.org/abs/2311.18466v1", "categories": ["cs.CL", "cs.CY", "cs.SI"], "primary_category": "cs.CL"}
{"title": "ESG Accountability Made Easy: DocQA at Your Service", "abstract": "We present Deep Search DocQA. This application enables information extraction\nfrom documents via a question-answering conversational assistant. The system\nintegrates several technologies from different AI disciplines consisting of\ndocument conversion to machine-readable format (via computer vision), finding\nrelevant data (via natural language processing), and formulating an eloquent\nresponse (via large language models). Users can explore over 10,000\nEnvironmental, Social, and Governance (ESG) disclosure reports from over 2000\ncorporations. The Deep Search platform can be accessed at:\nhttps://ds4sd.github.io.", "published": "2023-11-30 11:47:50", "link": "http://arxiv.org/abs/2311.18481v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Automatic Functional Differentiation in JAX", "abstract": "We extend JAX with the capability to automatically differentiate higher-order\nfunctions (functionals and operators). By representing functions as a\ngeneralization of arrays, we seamlessly use JAX's existing primitive system to\nimplement higher-order functions. We present a set of primitive operators that\nserve as foundational building blocks for constructing several key types of\nfunctionals. For every introduced primitive operator, we derive and implement\nboth linearization and transposition rules, aligning with JAX's internal\nprotocols for forward and reverse mode automatic differentiation. This\nenhancement allows for functional differentiation in the same syntax\ntraditionally use for functions. The resulting functional gradients are\nthemselves functions ready to be invoked in python. We showcase this tool's\nefficacy and simplicity through applications where functional derivatives are\nindispensable. The source code of this work is released at\nhttps://github.com/sail-sg/autofd .", "published": "2023-11-30 17:23:40", "link": "http://arxiv.org/abs/2311.18727v2", "categories": ["cs.PL", "cs.CL", "cs.LG"], "primary_category": "cs.PL"}
{"title": "AlignBench: Benchmarking Chinese Alignment of Large Language Models", "abstract": "Alignment has become a critical step for instruction-tuned Large Language\nModels (LLMs) to become helpful assistants. However, the effective evaluation\nof alignment for emerging Chinese LLMs is still largely unexplored. To fill in\nthis gap, we introduce AlignBench, a comprehensive multi-dimensional benchmark\nfor evaluating LLMs' alignment in Chinese. We design a human-in-the-loop data\ncuration pipeline, containing eight main categories, 683 real-scenario rooted\nqueries and corresponding human verified references. To ensure the correctness\nof references, each knowledge-intensive query is accompanied with evidences\ncollected from reliable web sources (including URLs and quotations) by our\nannotators. For automatic evaluation, our benchmark employs a rule-calibrated\nmulti-dimensional LLM-as-Judge~\\cite{zheng2023judging} approach with\nChain-of-Thought to generate explanations and final ratings, ensuring high\nreliability and interpretability. All evaluation code, data, and LLM\ngenerations are available at \\url{https://github.com/THUDM/AlignBench}. Since\nits release, AlignBench has been adopted by top (Chinese) LLMs for evaluating\ntheir alignment capabilities in Chinese, including ChatGLM, Qwen, DeepSeek, Yi,\nBaichuan, and Abab.", "published": "2023-11-30 17:41:30", "link": "http://arxiv.org/abs/2311.18743v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exposing Limitations of Language Model Agents in Sequential-Task\n  Compositions on the Web", "abstract": "Language model agents (LMA) recently emerged as a promising paradigm on\nmuti-step decision making tasks, often outperforming humans and other\nreinforcement learning agents. Despite the promise, their performance on\nreal-world applications that often involve combinations of tasks is still\nunderexplored. In this work, we introduce a new benchmark, called CompWoB -- 50\nnew compositional web automation tasks reflecting more realistic assumptions.\nWe show that while existing prompted LMAs (gpt-3.5-turbo or gpt-4) achieve\n94.0% average success rate on base tasks, their performance degrades to 24.9%\nsuccess rate on compositional tasks. On the other hand, transferred LMAs\n(finetuned only on base tasks) show less generalization gap, dropping from\n85.4% to 54.8%. By balancing data distribution across tasks, we train a new\nmodel, HTML-T5++, that surpasses human-level performance (95.2%) on MiniWoB,\nand achieves the best zero-shot performance on CompWoB (61.5%). While these\nhighlight the promise of small-scale finetuned and transferred models for task\ncompositionality, their performance further degrades under different\ninstruction compositions changing combinational order. In contrast to the\nrecent remarkable success of LMA, our benchmark and detailed analysis emphasize\nthe necessity of building LMAs that are robust and generalizable to task\ncompositionality for real-world deployment.", "published": "2023-11-30 17:50:47", "link": "http://arxiv.org/abs/2311.18751v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "MLLMs-Augmented Visual-Language Representation Learning", "abstract": "Visual-language pre-training has achieved remarkable success in many\nmulti-modal tasks, largely attributed to the availability of large-scale\nimage-text datasets. In this work, we demonstrate that Multi-modal Large\nLanguage Models (MLLMs) can enhance visual-language representation learning by\nestablishing richer image-text associations for image-text datasets. Our\napproach is simple, utilizing MLLMs to extend multiple diverse captions for\neach image. To prevent the bias introduced by MLLMs' hallucinations and\nmonotonous language styles, we propose \"text shearing\" to maintain the quality\nand availability of extended captions. In image-text retrieval, without\nintroducing additional training cost, our method consistently obtains 5.6 ~\n35.0 and 16.8 ~ 46.1 improvement on Recall@1 under the fine-tuning and\nzero-shot settings, respectively. Notably, we obtain zero-shot results that are\ncomparable to fine-tuning on target datasets, which encourages more exploration\nof the versatile use of MLLMs.", "published": "2023-11-30 18:05:52", "link": "http://arxiv.org/abs/2311.18765v3", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "BioCLIP: A Vision Foundation Model for the Tree of Life", "abstract": "Images of the natural world, collected by a variety of cameras, from drones\nto individual phones, are increasingly abundant sources of biological\ninformation. There is an explosion of computational methods and tools,\nparticularly computer vision, for extracting biologically relevant information\nfrom images for science and conservation. Yet most of these are bespoke\napproaches designed for a specific task and are not easily adaptable or\nextendable to new questions, contexts, and datasets. A vision model for general\norganismal biology questions on images is of timely need. To approach this, we\ncurate and release TreeOfLife-10M, the largest and most diverse ML-ready\ndataset of biology images. We then develop BioCLIP, a foundation model for the\ntree of life, leveraging the unique properties of biology captured by\nTreeOfLife-10M, namely the abundance and variety of images of plants, animals,\nand fungi, together with the availability of rich structured biological\nknowledge. We rigorously benchmark our approach on diverse fine-grained biology\nclassification tasks and find that BioCLIP consistently and substantially\noutperforms existing baselines (by 16% to 17% absolute). Intrinsic evaluation\nreveals that BioCLIP has learned a hierarchical representation conforming to\nthe tree of life, shedding light on its strong generalizability.\nhttps://imageomics.github.io/bioclip has models, data and code.", "published": "2023-11-30 18:49:43", "link": "http://arxiv.org/abs/2311.18803v3", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "HiFi Tuner: High-Fidelity Subject-Driven Fine-Tuning for Diffusion\n  Models", "abstract": "This paper explores advancements in high-fidelity personalized image\ngeneration through the utilization of pre-trained text-to-image diffusion\nmodels. While previous approaches have made significant strides in generating\nversatile scenes based on text descriptions and a few input images, challenges\npersist in maintaining the subject fidelity within the generated images. In\nthis work, we introduce an innovative algorithm named HiFi Tuner to enhance the\nappearance preservation of objects during personalized image generation. Our\nproposed method employs a parameter-efficient fine-tuning framework, comprising\na denoising process and a pivotal inversion process. Key enhancements include\nthe utilization of mask guidance, a novel parameter regularization technique,\nand the incorporation of step-wise subject representations to elevate the\nsample fidelity. Additionally, we propose a reference-guided generation\napproach that leverages the pivotal inversion of a reference image to mitigate\nunwanted subject variations and artifacts. We further extend our method to a\nnovel image editing task: substituting the subject in an image through textual\nmanipulations. Experimental evaluations conducted on the DreamBooth dataset\nusing the Stable Diffusion model showcase promising results. Fine-tuning solely\non textual embeddings improves CLIP-T score by 3.6 points and improves DINO\nscore by 9.6 points over Textual Inversion. When fine-tuning all parameters,\nHiFi Tuner improves CLIP-T score by 1.2 points and improves DINO score by 1.2\npoints over DreamBooth, establishing a new state of the art.", "published": "2023-11-30 02:33:29", "link": "http://arxiv.org/abs/2312.00079v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Multi-Modal Video Topic Segmentation with Dual-Contrastive Domain\n  Adaptation", "abstract": "Video topic segmentation unveils the coarse-grained semantic structure\nunderlying videos and is essential for other video understanding tasks. Given\nthe recent surge in multi-modal, relying solely on a single modality is\narguably insufficient. On the other hand, prior solutions for similar tasks\nlike video scene/shot segmentation cater to short videos with clear visual\nshifts but falter for long videos with subtle changes, such as livestreams. In\nthis paper, we introduce a multi-modal video topic segmenter that utilizes both\nvideo transcripts and frames, bolstered by a cross-modal attention mechanism.\nFurthermore, we propose a dual-contrastive learning framework adhering to the\nunsupervised domain adaptation paradigm, enhancing our model's adaptability to\nlonger, more semantically complex videos. Experiments on short and long video\ncorpora demonstrate that our proposed solution, significantly surpasses\nbaseline methods in terms of both accuracy and transferability, in both intra-\nand cross-domain settings.", "published": "2023-11-30 21:59:05", "link": "http://arxiv.org/abs/2312.00220v1", "categories": ["cs.MM", "cs.CL", "cs.CV"], "primary_category": "cs.MM"}
{"title": "Large Language Models for Travel Behavior Prediction", "abstract": "Travel behavior prediction is a fundamental task in transportation demand\nmanagement. The conventional methods for travel behavior prediction rely on\nnumerical data to construct mathematical models and calibrate model parameters\nto represent human preferences. Recent advancement in large language models\n(LLMs) has shown great reasoning abilities to solve complex problems. In this\nstudy, we propose to use LLMs to predict travel behavior with prompt\nengineering without data-based parameter learning. Specifically, we carefully\ndesign our prompts that include 1) task description, 2) travel characteristics,\n3) individual attributes, and 4) guides of thinking with domain knowledge, and\nask the LLMs to predict an individual's travel behavior and explain the\nresults. We select the travel mode choice task as a case study. Results show\nthat, though no training samples are provided, LLM-based predictions have\ncompetitive accuracy and F1-score as canonical supervised learning methods such\nas multinomial logit, random forest, and neural networks. LLMs can also output\nreasons that support their prediction. However, though in most of the cases,\nthe output explanations are reasonable, we still observe cases that violate\nlogic or with hallucinations.", "published": "2023-11-30 04:35:55", "link": "http://arxiv.org/abs/2312.00819v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Evaluating Large Language Model Creativity from a Literary Perspective", "abstract": "This paper assesses the potential for large language models (LLMs) to serve\nas assistive tools in the creative writing process, by means of a single,\nin-depth case study. In the course of the study, we develop interactive and\nmulti-voice prompting strategies that interleave background descriptions (scene\nsetting, plot elements), instructions that guide composition, samples of text\nin the target style, and critical discussion of the given samples. We\nqualitatively evaluate the results from a literary critical perspective, as\nwell as from the standpoint of computational creativity (a sub-field of\nartificial intelligence). Our findings lend support to the view that the\nsophistication of the results that can be achieved with an LLM mirrors the\nsophistication of the prompting.", "published": "2023-11-30 16:46:25", "link": "http://arxiv.org/abs/2312.03746v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Classifying patient voice in social media data using neural networks: A\n  comparison of AI models on different data sources and therapeutic domains", "abstract": "It is essential that healthcare professionals and members of the healthcare\ncommunity can access and easily understand patient experiences in the real\nworld, so that care standards can be improved and driven towards personalised\ndrug treatment. Social media platforms and message boards are deemed suitable\nsources of patient experience information, as patients have been observed to\ndiscuss and exchange knowledge, look for and provide support online. This paper\ntests the hypothesis that not all online patient experience information can be\ntreated and collected in the same way, as a result of the inherent differences\nin the way individuals talk about their journeys, in different therapeutic\ndomains and or data sources.\n  We used linguistic analysis to understand and identify similarities between\ndatasets, across patient language, between data sources (Reddit, SocialGist)\nand therapeutic domains (cardiovascular, oncology, immunology, neurology). We\ndetected common vocabulary used by patients in the same therapeutic domain\nacross data sources, except for immunology patients, who use unique vocabulary\nbetween the two data sources, and compared to all other datasets. We combined\nlinguistically similar datasets to train classifiers (CNN, transformer) to\naccurately identify patient experience posts from social media, a task we refer\nto as patient voice classification. The cardiovascular and neurology\ntransformer classifiers perform the best in their respective comparisons for\nthe Reddit data source, achieving F1-scores of 0.865 and 1.0 respectively. The\noverall best performing classifier is the transformer classifier trained on all\ndata collected for this experiment, achieving F1-scores ranging between 0.863\nand 0.995 across all therapeutic domain and data source specific test datasets.", "published": "2023-11-30 18:35:24", "link": "http://arxiv.org/abs/2312.03747v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Compression of end-to-end non-autoregressive image-to-speech system for\n  low-resourced devices", "abstract": "People with visual impairments have difficulty accessing touchscreen-enabled\npersonal computing devices like mobile phones and laptops. The image-to-speech\n(ITS) systems can assist them in mitigating this problem, but their huge model\nsize makes it extremely hard to be deployed on low-resourced embedded devices.\nIn this paper, we aim to overcome this challenge by developing an efficient\nendto-end neural architecture for generating audio from tiny segments of\ndisplay content on low-resource devices. We introduced a vision\ntransformers-based image encoder and utilized knowledge distillation to\ncompress the model from 6.1 million to 2.46 million parameters. Human and\nautomatic evaluation results show that our approach leads to a very minimal\ndrop in performance and can speed up the inference time by 22%.", "published": "2023-11-30 20:13:10", "link": "http://arxiv.org/abs/2312.00174v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.CV", "eess.IV"], "primary_category": "eess.AS"}
{"title": "CoDi-2: In-Context, Interleaved, and Interactive Any-to-Any Generation", "abstract": "We present CoDi-2, a versatile and interactive Multimodal Large Language\nModel (MLLM) that can follow complex multimodal interleaved instructions,\nconduct in-context learning (ICL), reason, chat, edit, etc., in an any-to-any\ninput-output modality paradigm. By aligning modalities with language for both\nencoding and generation, CoDi-2 empowers Large Language Models (LLMs) to not\nonly understand complex modality-interleaved instructions and in-context\nexamples, but also autoregressively generate grounded and coherent multimodal\noutputs in the continuous feature space. To train CoDi-2, we build a\nlarge-scale generation dataset encompassing in-context multimodal instructions\nacross text, vision, and audio. CoDi-2 demonstrates a wide range of zero-shot\ncapabilities for multimodal generation, such as in-context learning, reasoning,\nand compositionality of any-to-any modality generation through multi-round\ninteractive conversation. CoDi-2 surpasses previous domain-specific models on\ntasks such as subject-driven image generation, vision transformation, and audio\nediting. CoDi-2 signifies a substantial breakthrough in developing a\ncomprehensive multimodal foundation model adept at interpreting in-context\nlanguage-vision-audio interleaved instructions and producing multimodal\noutputs.", "published": "2023-11-30 18:21:25", "link": "http://arxiv.org/abs/2311.18775v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Learning domain-invariant classifiers for infant cry sounds", "abstract": "The issue of domain shift remains a problematic phenomenon in most real-world\ndatasets and clinical audio is no exception. In this work, we study the nature\nof domain shift in a clinical database of infant cry sounds acquired across\ndifferent geographies. We find that though the pitches of infant cries are\nsimilarly distributed regardless of the place of birth, other characteristics\nintroduce peculiar biases into the data. We explore methodologies for\nmitigating the impact of domain shift in a model for identifying neurological\ninjury from cry sounds. We adapt unsupervised domain adaptation methods from\ncomputer vision which learn an audio representation that is domain-invariant to\nhospitals and is task discriminative. We also propose a new approach, target\nnoise injection (TNI), for unsupervised domain adaptation which requires\nneither labels nor training data from the target domain. Our best-performing\nmodel significantly improves target accuracy by 7.2%, without negatively\naffecting the source domain.", "published": "2023-11-30 22:27:57", "link": "http://arxiv.org/abs/2312.00231v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Acoustic Prompt Tuning: Empowering Large Language Models with Audition\n  Capabilities", "abstract": "The auditory system plays a substantial role in shaping the overall human\nperceptual experience. While prevailing large language models (LLMs) and visual\nlanguage models (VLMs) have shown their promise in solving a wide variety of\nlanguage and vision understanding tasks, only a few of them can be generalised\nto the audio domain without compromising their domain-specific capability. In\nthis work, we introduce Acoustic Prompt Tuning (APT), a new adapter extending\nLLMs and VLMs to the audio domain by injecting audio embeddings to the input of\nLLMs, namely soft prompting. Specifically, APT applies an instruction-aware\naudio aligner to generate soft prompts, conditioned on both input text and\nsounds, as the inputs to the language model. To mitigate data scarcity in the\naudio domain, a curriculum learning strategy is proposed by formulating diverse\naudio tasks in a sequential manner. Moreover, we improve the audio language\nmodel by using interleaved audio-text embeddings as the input sequence. In this\nimproved model, zero constraints are imposed on the input format, thus it is\ncapable of tackling diverse modelling tasks, such as few-shot audio\nclassification and audio comparison. To further evaluate the advanced ability\nof the audio networks, we introduce natural language audio reasoning (NLAR), a\nnew task that analyses two audio clips by comparison and summarisation.\nExperiments show that APT-enhanced LLMs (namely APT-LLMs) achieve competitive\nresults compared to the expert models (i.e., the networks trained on the target\ndatasets) across various tasks. We finally demonstrate APT's ability in\nextending frozen VLMs to the audio domain without fine-tuning, achieving\npromising results in audio-visual question and answering. Our code and model\nweights will be released at https://github.com/JinhuaLiang/APT", "published": "2023-11-30 23:43:59", "link": "http://arxiv.org/abs/2312.00249v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Speech Understanding on Tiny Devices with A Learning Cache", "abstract": "This paper addresses spoken language understanding (SLU) on\nmicrocontroller-like embedded devices, integrating on-device execution with\ncloud offloading in a novel fashion. We leverage temporal locality in the\nspeech inputs to a device and reuse recent SLU inferences accordingly. Our idea\nis simple: let the device match incoming inputs against cached results, and\nonly offload inputs not matched to any cached ones to the cloud for full\ninference. Realization of this idea, however, is non-trivial: the device needs\nto compare acoustic features in a robust yet low-cost way. To this end, we\npresent SpeechCache (or SC), a speech cache for tiny devices. It matches speech\ninputs at two levels of representations: first by sequences of clustered raw\nsound units, then as sequences of phonemes. Working in tandem, the two\nrepresentations offer complementary tradeoffs between cost and efficiency. To\nboost accuracy even further, our cache learns to personalize: with the\nmismatched and then offloaded inputs, it continuously finetunes the device's\nfeature extractors with the assistance of the cloud. We implement SC on an\noff-the-shelf STM32 microcontroller. The complete implementation has a small\nmemory footprint of 2MB. Evaluated on challenging speech benchmarks, our system\nresolves 45%-90% of inputs on device, reducing the average latency by up to 80%\ncompared to offloading to popular cloud speech recognition services. The\nbenefit brought by our proposed SC is notable even in adversarial settings -\nnoisy environments, cold cache, or one device shared by a number of users.", "published": "2023-11-30 02:15:07", "link": "http://arxiv.org/abs/2311.18188v4", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Audio Prompt Tuning for Universal Sound Separation", "abstract": "Universal sound separation (USS) is a task to separate arbitrary sounds from\nan audio mixture. Existing USS systems are capable of separating arbitrary\nsources, given a few examples of the target sources as queries. However,\nseparating arbitrary sounds with a single system is challenging, and the\nrobustness is not always guaranteed. In this work, we propose audio prompt\ntuning (APT), a simple yet effective approach to enhance existing USS systems.\nSpecifically, APT improves the separation performance of specific sources\nthrough training a small number of prompt parameters with limited audio\nsamples, while maintaining the generalization of the USS model by keeping its\nparameters frozen. We evaluate the proposed method on MUSDB18 and ESC-50\ndatasets. Compared with the baseline model, APT can improve the\nsignal-to-distortion ratio performance by 0.67 dB and 2.06 dB using the full\ntraining set of two datasets. Moreover, APT with only 5 audio samples even\noutperforms the baseline systems utilizing full training data on the ESC-50\ndataset, indicating the great potential of few-shot APT.", "published": "2023-11-30 09:49:22", "link": "http://arxiv.org/abs/2311.18399v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Sound Terminology Describing Production and Perception of Sonification", "abstract": "Sonification research is intrinsically interdisciplinary. Consequently, a\nproper documentation of, and interdisciplinary discourse about a sonification\nis often hindered by terminology discrepancies between involved disciplines,\ni.e., the lack of a common sound terminology in sonification research. Without\na common ground, a researcher from one discipline may have troubles\nunderstanding the implementation and imagining the resulting sound perception\nof a sonification, if the sonification is described by a researcher from\nanother discipline. To find a common ground, I consulted literature on\ninterdisciplinary research and discourse, identified problems that occur in\nsonification, and applied the recommended solutions. As a result, I recommend\nconsidering three aspects of sonification individually, namely 1.) Sound Design\nConcept, 2.) Objective and 3.) Method, clarifying which discipline is involved\nin which aspect, and sticking to this discipline's terminology. As two\nrequirements of sonifications are that they are a) reproducible and b)\ninterpretable, I recommend documenting and discussing every sonification design\nonce using audio engineering terminology, and once using psychoacoustic\nterminology. The appendix provides comprehensive lists of sound terms from both\ndisciplines, together with relevant literature and a clarification of often\nmisunderstood and misused terms.", "published": "2023-11-30 10:49:28", "link": "http://arxiv.org/abs/2312.00091v1", "categories": ["cs.SD", "eess.AS", "J.5"], "primary_category": "cs.SD"}
{"title": "Probabilistic Speech-Driven 3D Facial Motion Synthesis: New Benchmarks,\n  Methods, and Applications", "abstract": "We consider the task of animating 3D facial geometry from speech signal.\nExisting works are primarily deterministic, focusing on learning a one-to-one\nmapping from speech signal to 3D face meshes on small datasets with limited\nspeakers. While these models can achieve high-quality lip articulation for\nspeakers in the training set, they are unable to capture the full and diverse\ndistribution of 3D facial motions that accompany speech in the real world.\nImportantly, the relationship between speech and facial motion is one-to-many,\ncontaining both inter-speaker and intra-speaker variations and necessitating a\nprobabilistic approach. In this paper, we identify and address key challenges\nthat have so far limited the development of probabilistic models: lack of\ndatasets and metrics that are suitable for training and evaluating them, as\nwell as the difficulty of designing a model that generates diverse results\nwhile remaining faithful to a strong conditioning signal as speech. We first\npropose large-scale benchmark datasets and metrics suitable for probabilistic\nmodeling. Then, we demonstrate a probabilistic model that achieves both\ndiversity and fidelity to speech, outperforming other methods across the\nproposed benchmarks. Finally, we showcase useful applications of probabilistic\nmodels trained on these large-scale datasets: we can generate diverse\nspeech-driven 3D facial motion that matches unseen speaker styles extracted\nfrom reference clips; and our synthetic meshes can be used to improve the\nperformance of downstream audio-visual models.", "published": "2023-11-30 01:14:43", "link": "http://arxiv.org/abs/2311.18168v1", "categories": ["cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.CV"}
{"title": "String Sound Synthesizer on GPU-accelerated Finite Difference Scheme", "abstract": "This paper introduces a nonlinear string sound synthesizer, based on a finite\ndifference simulation of the dynamic behavior of strings under various\nexcitations. The presented synthesizer features a versatile string simulation\nengine capable of stochastic parameterization, encompassing fundamental\nfrequency modulation, stiffness, tension, frequency-dependent loss, and\nexcitation control. This open-source physical model simulator not only benefits\nthe audio signal processing community but also contributes to the burgeoning\nfield of neural network-based audio synthesis by serving as a novel dataset\nconstruction tool. Implemented in PyTorch, this synthesizer offers flexibility,\nfacilitating both CPU and GPU utilization, thereby enhancing its applicability\nas a simulator. GPU utilization expedites computation by parallelizing\noperations across spatial and batch dimensions, further enhancing its utility\nas a data generator.", "published": "2023-11-30 12:30:36", "link": "http://arxiv.org/abs/2311.18505v2", "categories": ["cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "Barwise Music Structure Analysis with the Correlation Block-Matching\n  Segmentation Algorithm", "abstract": "Music Structure Analysis (MSA) is a Music Information Retrieval task\nconsisting of representing a song in a simplified, organized manner by breaking\nit down into sections typically corresponding to ``chorus'', ``verse'',\n``solo'', etc. In this work, we extend an MSA algorithm called the Correlation\nBlock-Matching (CBM) algorithm introduced by (Marmoret et al., 2020, 2022b).\nThe CBM algorithm is a dynamic programming algorithm that segments\nself-similarity matrices, which are a standard description used in MSA and in\nnumerous other applications. In this work, self-similarity matrices are\ncomputed from the feature representation of an audio signal and time is sampled\nat the bar-scale. This study examines three different standard similarity\nfunctions for the computation of self-similarity matrices. Results show that,\nin optimal conditions, the proposed algorithm achieves a level of performance\nwhich is competitive with supervised state-of-the-art methods while only\nrequiring knowledge of bar positions. In addition, the algorithm is made\nopen-source and is highly customizable.", "published": "2023-11-30 15:00:25", "link": "http://arxiv.org/abs/2311.18604v1", "categories": ["cs.SD", "cs.IR", "eess.AS", "H.5.5"], "primary_category": "cs.SD"}
{"title": "Subspace Hybrid MVDR Beamforming for Augmented Hearing", "abstract": "Signal-dependent beamformers are advantageous over signal-independent\nbeamformers when the acoustic scenario - be it real-world or simulated - is\nstraightforward in terms of the number of sound sources, the ambient sound\nfield and their dynamics. However, in the context of augmented reality audio\nusing head-worn microphone arrays, the acoustic scenarios encountered are often\nfar from straightforward. The design of robust, high-performance, adaptive\nbeamformers for such scenarios is an on-going challenge. This is due to the\nviolation of the typically required assumptions on the noise field caused by,\nfor example, rapid variations resulting from complex acoustic environments,\nand/or rotations of the listener's head. This work proposes a multi-channel\nspeech enhancement algorithm which utilises the adaptability of\nsignal-dependent beamformers while still benefiting from the computational\nefficiency and robust performance of signal-independent super-directive\nbeamformers. The algorithm has two stages. (i) The first stage is a hybrid\nbeamformer based on a dictionary of weights corresponding to a set of noise\nfield models. (ii) The second stage is a wide-band subspace post-filter to\nremove any artifacts resulting from (i). The algorithm is evaluated using both\nreal-world recordings and simulations of a cocktail-party scenario. Noise\nsuppression, intelligibility and speech quality results show a significant\nperformance improvement by the proposed algorithm compared to the baseline\nsuper-directive beamformer. A data-driven implementation of the noise field\ndictionary is shown to provide more noise suppression, and similar speech\nintelligibility and quality, compared to a parametric dictionary.", "published": "2023-11-30 16:34:50", "link": "http://arxiv.org/abs/2311.18689v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "An Aliasing-Free Hybrid Digital-Analog Polyphonic Synthesizer", "abstract": "Analog subtractive synthesizers are generally considered to provide superior\nsound quality compared to digital emulations. However, analog circuitry\nrequires calibration and suffers from aging, temperature instability, and\nlimited flexibility in generating a wide variety of waveforms. Digital\nsynthesis can mitigate many of these drawbacks, but generating arbitrary\naliasing-free waveforms remains challenging. In this paper, we present the\n+-synth, a hybrid digital-analog eight-voice polyphonic synthesizer prototype\nthat combines the best of both worlds. At the heart of the synthesizer is the\nbig Fourier oscillator (BFO), a novel digital very-large scale integration\n(VLSI) design that utilizes additive synthesis to generate a wide variety of\naliasing-free waveforms. Each BFO produces two voices, using four oscillators\nper voice. A single oscillator can generate up to 1024 freely configurable\npartials (harmonic or inharmonic), which are calculated using coordinate\nrotation digital computers (CORDICs). The BFOs were fabricated as 65nm CMOS\ncustom application-specific integrated circuits (ASICs), which are integrated\nin the +-synth to simultaneously generate up to 32768 partials. Four 24-bit\n96kHz stereo DACs then convert the eight voices into the analog domain,\nfollowed by digitally controlled analog low-pass filtering and amplification.\nMeasurement results of the +-synth prototype demonstrate high fidelity and low\nlatency.", "published": "2023-11-30 18:20:48", "link": "http://arxiv.org/abs/2311.18774v1", "categories": ["eess.AS", "cs.SY", "eess.SY"], "primary_category": "eess.AS"}
{"title": "DanceMeld: Unraveling Dance Phrases with Hierarchical Latent Codes for\n  Music-to-Dance Synthesis", "abstract": "In the realm of 3D digital human applications, music-to-dance presents a\nchallenging task. Given the one-to-many relationship between music and dance,\nprevious methods have been limited in their approach, relying solely on\nmatching and generating corresponding dance movements based on music rhythm. In\nthe professional field of choreography, a dance phrase consists of several\ndance poses and dance movements. Dance poses composed of a series of basic\nmeaningful body postures, while dance movements can reflect dynamic changes\nsuch as the rhythm, melody, and style of dance. Taking inspiration from these\nconcepts, we introduce an innovative dance generation pipeline called\nDanceMeld, which comprising two stages, i.e., the dance decouple stage and the\ndance generation stage. In the decouple stage, a hierarchical VQ-VAE is used to\ndisentangle dance poses and dance movements in different feature space levels,\nwhere the bottom code represents dance poses, and the top code represents dance\nmovements. In the generation stage, we utilize a diffusion model as a prior to\nmodel the distribution and generate latent codes conditioned on music features.\nWe have experimentally demonstrated the representational capabilities of top\ncode and bottom code, enabling the explicit decoupling expression of dance\nposes and dance movements. This disentanglement not only provides control over\nmotion details, styles, and rhythm but also facilitates applications such as\ndance style transfer and dance unit editing. Our approach has undergone\nqualitative and quantitative experiments on the AIST++ dataset, demonstrating\nits superiority over other methods.", "published": "2023-11-30 12:36:21", "link": "http://arxiv.org/abs/2401.10242v1", "categories": ["cs.OH", "cs.GR", "cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.OH"}
