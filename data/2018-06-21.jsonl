{"title": "An empirical study on the names of points of interest and their changes\n  with geographic distance", "abstract": "While Points Of Interest (POIs), such as restaurants, hotels, and barber\nshops, are part of urban areas irrespective of their specific locations, the\nnames of these POIs often reveal valuable information related to local culture,\nlandmarks, influential families, figures, events, and so on. Place names have\nlong been studied by geographers, e.g., to understand their origins and\nrelations to family names. However, there is a lack of large-scale empirical\nstudies that examine the localness of place names and their changes with\ngeographic distance. In addition to enhancing our understanding of the\ncoherence of geographic regions, such empirical studies are also significant\nfor geographic information retrieval where they can inform computational models\nand improve the accuracy of place name disambiguation. In this work, we conduct\nan empirical study based on 112,071 POIs in seven US metropolitan areas\nextracted from an open Yelp dataset. We propose to adopt term frequency and\ninverse document frequency in geographic contexts to identify local terms used\nin POI names and to analyze their usages across different POI types. Our\nresults show an uneven usage of local terms across POI types, which is highly\nconsistent among different geographic regions. We also examine the decaying\neffect of POI name similarity with the increase of distance among POIs. While\nour analysis focuses on urban POI names, the presented methods can be\ngeneralized to other place types as well, such as mountain peaks and streets.", "published": "2018-06-21 01:33:30", "link": "http://arxiv.org/abs/1806.08040v1", "categories": ["cs.CL", "H.2.8; H.3.1"], "primary_category": "cs.CL"}
{"title": "Coherence Models for Dialogue", "abstract": "Coherence across multiple turns is a major challenge for state-of-the-art\ndialogue models. Arguably the most successful approach to automatically\nlearning text coherence is the entity grid, which relies on modelling patterns\nof distribution of entities across multiple sentences of a text. Originally\napplied to the evaluation of automatic summaries and the news genre, among its\nmany extensions, this model has also been successfully used to assess dialogue\ncoherence. Nevertheless, both the original grid and its extensions do not model\nintents, a crucial aspect that has been studied widely in the literature in\nconnection to dialogue structure. We propose to augment the original grid\ndocument representation for dialogue with the intentional structure of the\nconversation. Our models outperform the original grid representation on both\ntext discrimination and insertion, the two main standard tasks for coherence\nassessment across three different dialogue datasets, confirming that intents\nplay a key role in modelling dialogue coherence.", "published": "2018-06-21 02:14:24", "link": "http://arxiv.org/abs/1806.08044v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dictionary-Guided Editing Networks for Paraphrase Generation", "abstract": "An intuitive way for a human to write paraphrase sentences is to replace\nwords or phrases in the original sentence with their corresponding synonyms and\nmake necessary changes to ensure the new sentences are fluent and grammatically\ncorrect. We propose a novel approach to modeling the process with\ndictionary-guided editing networks which effectively conduct rewriting on the\nsource sentence to generate paraphrase sentences. It jointly learns the\nselection of the appropriate word level and phrase level paraphrase pairs in\nthe context of the original sentence from an off-the-shelf dictionary as well\nas the generation of fluent natural language sentences. Specifically, the\nsystem retrieves a set of word level and phrase level araphrased pairs derived\nfrom the Paraphrase Database (PPDB) for the original sentence, which is used to\nguide the decision of which the words might be deleted or inserted with the\nsoft attention mechanism under the sequence-to-sequence framework. We conduct\nexperiments on two benchmark datasets for paraphrase generation, namely the\nMSCOCO and Quora dataset. The evaluation results demonstrate that our\ndictionary-guided editing networks outperforms the baseline methods.", "published": "2018-06-21 06:21:14", "link": "http://arxiv.org/abs/1806.08077v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BFGAN: Backward and Forward Generative Adversarial Networks for\n  Lexically Constrained Sentence Generation", "abstract": "Incorporating prior knowledge like lexical constraints into the model's\noutput to generate meaningful and coherent sentences has many applications in\ndialogue system, machine translation, image captioning, etc. However, existing\nRNN-based models incrementally generate sentences from left to right via beam\nsearch, which makes it difficult to directly introduce lexical constraints into\nthe generated sentences. In this paper, we propose a new algorithmic framework,\ndubbed BFGAN, to address this challenge. Specifically, we employ a backward\ngenerator and a forward generator to generate lexically constrained sentences\ntogether, and use a discriminator to guide the joint training of two generators\nby assigning them reward signals. Due to the difficulty of BFGAN training, we\npropose several training techniques to make the training process more stable\nand efficient. Our extensive experiments on two large-scale datasets with human\nevaluation demonstrate that BFGAN has significant improvements over previous\nmethods.", "published": "2018-06-21 07:44:32", "link": "http://arxiv.org/abs/1806.08097v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling Word Emotion in Historical Language: Quantity Beats Supposed\n  Stability in Seed Word Selection", "abstract": "To understand historical texts, we must be aware that language -- including\nthe emotional connotation attached to words -- changes over time. In this\npaper, we aim at estimating the emotion which is associated with a given word\nin former language stages of English and German. Emotion is represented\nfollowing the popular Valence-Arousal-Dominance (VAD) annotation scheme. While\nbeing more expressive than polarity alone, existing word emotion induction\nmethods are typically not suited for addressing it. To overcome this\nlimitation, we present adaptations of two popular algorithms to VAD. To measure\ntheir effectiveness in diachronic settings, we present the first gold standard\nfor historical word emotions, which was created by scholars with proficiency in\nthe respective language stages and covers both English and German. In contrast\nto claims in previous work, our findings indicate that hand-selecting small\nsets of seed words with supposedly stable emotional meaning is actually harmful\nrather than helpful.", "published": "2018-06-21 08:53:06", "link": "http://arxiv.org/abs/1806.08115v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Par4Sim -- Adaptive Paraphrasing for Text Simplification", "abstract": "Learning from a real-world data stream and continuously updating the model\nwithout explicit supervision is a new challenge for NLP applications with\nmachine learning components. In this work, we have developed an adaptive\nlearning system for text simplification, which improves the underlying\nlearning-to-rank model from usage data, i.e. how users have employed the system\nfor the task of simplification. Our experimental result shows that, over a\nperiod of time, the performance of the embedded paraphrase ranking model\nincreases steadily improving from a score of 62.88% up to 75.70% based on the\nNDCG@10 evaluation metrics. To our knowledge, this is the first study where an\nNLP component is adaptively improved through usage.", "published": "2018-06-21 16:24:31", "link": "http://arxiv.org/abs/1806.08309v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Metadata Enrichment of Multi-Disciplinary Digital Library: A\n  Semantic-based Approach", "abstract": "In the scientific digital libraries, some papers from different research\ncommunities can be described by community-dependent keywords even if they share\na semantically similar topic. Articles that are not tagged with enough keyword\nvariations are poorly indexed in any information retrieval system which limits\npotentially fruitful exchanges between scientific disciplines. In this paper,\nwe introduce a novel experimentally designed pipeline for multi-label\nsemantic-based tagging developed for open-access metadata digital libraries.\nThe approach starts by learning from a standard scientific categorization and a\nsample of topic tagged articles to find semantically relevant articles and\nenrich its metadata accordingly. Our proposed pipeline aims to enable\nresearchers reaching articles from various disciplines that tend to use\ndifferent terminologies. It allows retrieving semantically relevant articles\ngiven a limited known variation of search terms. In addition to achieving an\naccuracy that is higher than an expanded query based method using a topic\nsynonym set extracted from a semantic network, our experiments also show a\nhigher computational scalability versus other comparable techniques. We created\na new benchmark extracted from the open-access metadata of a scientific digital\nlibrary and published it along with the experiment code to allow further\nresearch in the topic.", "published": "2018-06-21 12:35:23", "link": "http://arxiv.org/abs/1806.08202v1", "categories": ["cs.DL", "cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.DL"}
{"title": "End-to-End Audio Visual Scene-Aware Dialog using Multimodal\n  Attention-Based Video Features", "abstract": "Dialog systems need to understand dynamic visual scenes in order to have\nconversations with users about the objects and events around them. Scene-aware\ndialog systems for real-world applications could be developed by integrating\nstate-of-the-art technologies from multiple research areas, including:\nend-to-end dialog technologies, which generate system responses using models\ntrained from dialog data; visual question answering (VQA) technologies, which\nanswer questions about images using learned image features; and video\ndescription technologies, in which descriptions/captions are generated from\nvideos using multimodal information. We introduce a new dataset of dialogs\nabout videos of human behaviors. Each dialog is a typed conversation that\nconsists of a sequence of 10 question-and-answer(QA) pairs between two Amazon\nMechanical Turk (AMT) workers. In total, we collected dialogs on roughly 9,000\nvideos. Using this new dataset for Audio Visual Scene-aware dialog (AVSD), we\ntrained an end-to-end conversation model that generates responses in a dialog\nabout a video. Our experiments demonstrate that using multimodal features that\nwere developed for multimodal attention-based video description enhances the\nquality of generated dialog about dynamic scenes (videos). Our dataset, model\ncode and pretrained models will be publicly available for a new Video\nScene-Aware Dialog challenge.", "published": "2018-06-21 19:43:13", "link": "http://arxiv.org/abs/1806.08409v2", "categories": ["cs.CL", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Towards Automated Single Channel Source Separation using Neural Networks", "abstract": "Many applications of single channel source separation (SCSS) including\nautomatic speech recognition (ASR), hearing aids etc. require an estimation of\nonly one source from a mixture of many sources. Treating this special case as a\nregular SCSS problem where in all constituent sources are given equal priority\nin terms of reconstruction may result in a suboptimal separation performance.\nIn this paper, we tackle the one source separation problem by suitably\nmodifying the orthodox SCSS framework and focus only on one source at a time.\nThe proposed approach is a generic framework that can be applied to any\nexisting SCSS algorithm, improves performance, and scales well when there are\nmore than two sources in the mixture unlike most existing SCSS methods.\nAdditionally, existing SCSS algorithms rely on fine hyper-parameter tuning\nhence making them difficult to use in practice. Our framework takes a step\ntowards automatic tuning of the hyper-parameters thereby making our method\nbetter suited for the mixture to be separated and thus practically more useful.\nWe test our framework on a neural network based algorithm and the results show\nan improved performance in terms of SDR and SAR.", "published": "2018-06-21 07:03:51", "link": "http://arxiv.org/abs/1806.08086v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "On the Relationship Between Short-Time Objective Intelligibility and\n  Short-Time Spectral-Amplitude Mean-Square Error for Speech Enhancement", "abstract": "The majority of deep neural network (DNN) based speech enhancement algorithms\nrely on the mean-square error (MSE) criterion of short-time spectral amplitudes\n(STSA), which has no apparent link to human perception, e.g. speech\nintelligibility. Short-Time Objective Intelligibility (STOI), a popular\nstate-of-the-art speech intelligibility estimator, on the other hand, relies on\nlinear correlation of speech temporal envelopes. This raises the question if a\nDNN training criterion based on envelope linear correlation (ELC) can lead to\nimproved speech intelligibility performance of DNN based speech enhancement\nalgorithms compared to algorithms based on the STSA-MSE criterion. In this\npaper we derive that, under certain general conditions, the STSA-MSE and ELC\ncriteria are practically equivalent, and we provide empirical data to support\nour theoretical results. Furthermore, our experimental findings suggest that\nthe standard STSA minimum-MSE estimator is near optimal, if the objective is to\nenhance noisy speech in a manner which is optimal with respect to the STOI\nspeech intelligibility estimator.", "published": "2018-06-21 19:07:12", "link": "http://arxiv.org/abs/1806.08404v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Learning Transposition-Invariant Interval Features from Symbolic Music\n  and Audio", "abstract": "Many music theoretical constructs (such as scale types, modes, cadences, and\nchord types) are defined in terms of pitch intervals---relative distances\nbetween pitches. Therefore, when computer models are employed in music tasks,\nit can be useful to operate on interval representations rather than on the raw\nmusical surface. Moreover, interval representations are\ntransposition-invariant, valuable for tasks like audio alignment, cover song\ndetection and music structure analysis. We employ a gated autoencoder to learn\nfixed-length, invertible and transposition-invariant interval representations\nfrom polyphonic music in the symbolic domain and in audio. An unsupervised\ntraining method is proposed yielding an organization of intervals in the\nrepresentation space which is musically plausible. Based on the\nrepresentations, a transposition-invariant self-similarity matrix is\nconstructed and used to determine repeated sections in symbolic music and in\naudio, yielding competitive results in the MIREX task \"Discovery of Repeated\nThemes and Sections\".", "published": "2018-06-21 13:35:44", "link": "http://arxiv.org/abs/1806.08236v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
