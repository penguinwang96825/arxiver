{"title": "Character-level White-Box Adversarial Attacks against Transformers via\n  Attachable Subwords Substitution", "abstract": "We propose the first character-level white-box adversarial attack method\nagainst transformer models. The intuition of our method comes from the\nobservation that words are split into subtokens before being fed into the\ntransformer models and the substitution between two close subtokens has a\nsimilar effect to the character modification. Our method mainly contains three\nsteps. First, a gradient-based method is adopted to find the most vulnerable\nwords in the sentence. Then we split the selected words into subtokens to\nreplace the origin tokenization result from the transformer tokenizer. Finally,\nwe utilize an adversarial loss to guide the substitution of attachable\nsubtokens in which the Gumbel-softmax trick is introduced to ensure gradient\npropagation. Meanwhile, we introduce the visual and length constraint in the\noptimization process to achieve minimum character modifications. Extensive\nexperiments on both sentence-level and token-level tasks demonstrate that our\nmethod could outperform the previous attack methods in terms of success rate\nand edit distance. Furthermore, human evaluation verifies our adversarial\nexamples could preserve their origin labels.", "published": "2022-10-31 01:46:29", "link": "http://arxiv.org/abs/2210.17004v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "GPS: Genetic Prompt Search for Efficient Few-shot Learning", "abstract": "Prompt-based techniques have demostrated great potential for improving the\nfew-shot generalization of pretrained language models. However, their\nperformance heavily relies on the manual design of prompts and thus requires a\nlot of human efforts. In this paper, we introduce Genetic Prompt Search (GPS)\nto improve few-shot learning with prompts, which utilizes a genetic algorithm\nto automatically search for high-performing prompts. GPS is gradient-free and\nrequires no update of model parameters but only a small validation set.\nExperiments on diverse datasets proved the effectiveness of GPS, which\noutperforms manual prompts by a large margin of 2.6 points. Our method is also\nbetter than other parameter-efficient tuning methods such as prompt tuning.", "published": "2022-10-31 03:36:21", "link": "http://arxiv.org/abs/2210.17041v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RLET: A Reinforcement Learning Based Approach for Explainable QA with\n  Entailment Trees", "abstract": "Interpreting the reasoning process from questions to answers poses a\nchallenge in approaching explainable QA. A recently proposed structured\nreasoning format, entailment tree, manages to offer explicit logical deductions\nwith entailment steps in a tree structure. To generate entailment trees, prior\nsingle pass sequence-to-sequence models lack visible internal decision\nprobability, while stepwise approaches are supervised with extracted single\nstep data and cannot model the tree as a whole. In this work, we propose RLET,\na Reinforcement Learning based Entailment Tree generation framework, which is\ntrained utilising the cumulative signals across the whole tree. RLET\niteratively performs single step reasoning with sentence selection and\ndeduction generation modules, from which the training signal is accumulated\nacross the tree with elaborately designed aligned reward function that is\nconsistent with the evaluation. To the best of our knowledge, we are the first\nto introduce RL into the entailment tree generation task. Experiments on three\nsettings of the EntailmentBank dataset demonstrate the strength of using RL\nframework.", "published": "2022-10-31 06:45:05", "link": "http://arxiv.org/abs/2210.17095v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do Charge Prediction Models Learn Legal Theory?", "abstract": "The charge prediction task aims to predict the charge for a case given its\nfact description. Recent models have already achieved impressive accuracy in\nthis task, however, little is understood about the mechanisms they use to\nperform the judgment.For practical applications, a charge prediction model\nshould conform to the certain legal theory in civil law countries, as under the\nframework of civil law, all cases are judged according to certain local legal\ntheories. In China, for example, nearly all criminal judges make decisions\nbased on the Four Elements Theory (FET).In this paper, we argue that\ntrustworthy charge prediction models should take legal theories into\nconsideration, and standing on prior studies in model interpretation, we\npropose three principles for trustworthy models should follow in this task,\nwhich are sensitive, selective, and presumption of innocence.We further design\na new framework to evaluate whether existing charge prediction models learn\nlegal theories. Our findings indicate that, while existing charge prediction\nmodels meet the selective principle on a benchmark dataset, most of them are\nstill not sensitive enough and do not satisfy the presumption of innocence. Our\ncode and dataset are released at https://github.com/ZhenweiAn/EXP_LJP.", "published": "2022-10-31 07:32:12", "link": "http://arxiv.org/abs/2210.17108v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "QuaLA-MiniLM: a Quantized Length Adaptive MiniLM", "abstract": "Limited computational budgets often prevent transformers from being used in\nproduction and from having their high accuracy utilized. A knowledge\ndistillation approach addresses the computational efficiency by self-distilling\nBERT into a smaller transformer representation having fewer layers and smaller\ninternal embedding. However, the performance of these models drops as we reduce\nthe number of layers, notably in advanced NLP tasks such as span question\nanswering. In addition, a separate model must be trained for each inference\nscenario with its distinct computational budget. Dynamic-TinyBERT tackles both\nlimitations by partially implementing the Length Adaptive Transformer (LAT)\ntechnique onto TinyBERT, achieving x3 speedup over BERT-base with minimal\naccuracy loss. In this work, we expand the Dynamic-TinyBERT approach to\ngenerate a much more highly efficient model. We use MiniLM distillation jointly\nwith the LAT method, and we further enhance the efficiency by applying low-bit\nquantization. Our quantized length-adaptive MiniLM model (QuaLA-MiniLM) is\ntrained only once, dynamically fits any inference scenario, and achieves an\naccuracy-efficiency trade-off superior to any other efficient approaches per\nany computational budget on the SQuAD1.1 dataset (up to x8.8 speedup with <1%\naccuracy loss). The code to reproduce this work is publicly available on\nGithub.", "published": "2022-10-31 07:42:52", "link": "http://arxiv.org/abs/2210.17114v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mining Word Boundaries in Speech as Naturally Annotated Word\n  Segmentation Data", "abstract": "Inspired by early research on exploring naturally annotated data for Chinese\nword segmentation (CWS), and also by recent research on integration of speech\nand text processing, this work for the first time proposes to mine word\nboundaries from parallel speech/text data. First we collect parallel\nspeech/text data from two Internet sources that are related with CWS data used\nin our experiments. Then, we obtain character-level alignments and design\nsimple heuristic rules for determining word boundaries according to pause\nduration between adjacent characters. Finally, we present an effective\ncomplete-then-train strategy that can better utilize extra naturally annotated\ndata for model training. Experiments demonstrate our approach can significantly\nboost CWS performance in both cross-domain and low-resource scenarios.", "published": "2022-10-31 08:02:21", "link": "http://arxiv.org/abs/2210.17122v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Temporal Generalization of Pre-trained Language Models with\n  Lexical Semantic Change", "abstract": "Recent research has revealed that neural language models at scale suffer from\npoor temporal generalization capability, i.e., the language model pre-trained\non static data from past years performs worse over time on emerging data.\nExisting methods mainly perform continual training to mitigate such a\nmisalignment. While effective to some extent but is far from being addressed on\nboth the language modeling and downstream tasks. In this paper, we empirically\nobserve that temporal generalization is closely affiliated with lexical\nsemantic change, which is one of the essential phenomena of natural languages.\nBased on this observation, we propose a simple yet effective lexical-level\nmasking strategy to post-train a converged language model. Experiments on two\npre-trained language models, two different classification tasks, and four\nbenchmark datasets demonstrate the effectiveness of our proposed method over\nexisting temporal adaptation methods, i.e., continual training with new data.\nOur code is available at \\url{https://github.com/zhaochen0110/LMLM}.", "published": "2022-10-31 08:12:41", "link": "http://arxiv.org/abs/2210.17127v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "1Cademy @ Causal News Corpus 2022: Enhance Causal Span Detection via\n  Beam-Search-based Position Selector", "abstract": "In this paper, we present our approach and empirical observations for\nCause-Effect Signal Span Detection -- Subtask 2 of Shared task\n3~\\cite{tan-etal-2022-event} at CASE 2022. The shared task aims to extract the\ncause, effect, and signal spans from a given causal sentence. We model the task\nas a reading comprehension (RC) problem and apply a token-level RC-based span\nprediction paradigm to the task as the baseline. We explore different training\nobjectives to fine-tune the model, as well as data augmentation (DA) tricks\nbased on the language model (LM) for performance improvement. Additionally, we\npropose an efficient beam-search post-processing strategy to due with the\ndrawbacks of span detection to obtain a further performance gain. Our approach\nachieves an average $F_1$ score of 54.15 and ranks \\textbf{$1^{st}$} in the\nCASE competition. Our code is available at\n\\url{https://github.com/Gzhang-umich/1CademyTeamOfCASE}.", "published": "2022-10-31 09:09:59", "link": "http://arxiv.org/abs/2210.17157v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reduce Catastrophic Forgetting of Dense Retrieval Training with\n  Teleportation Negatives", "abstract": "In this paper, we investigate the instability in the standard dense retrieval\ntraining, which iterates between model training and hard negative selection\nusing the being-trained model. We show the catastrophic forgetting phenomena\nbehind the training instability, where models learn and forget different\nnegative groups during training iterations. We then propose ANCE-Tele, which\naccumulates momentum negatives from past iterations and approximates future\niterations using lookahead negatives, as \"teleportations\" along the time axis\nto smooth the learning process. On web search and OpenQA, ANCE-Tele outperforms\nprevious state-of-the-art systems of similar size, eliminates the dependency on\nsparse retrieval negatives, and is competitive among systems using\nsignificantly more (50x) parameters. Our analysis demonstrates that\nteleportation negatives reduce catastrophic forgetting and improve convergence\nspeed for dense retrieval training. Our code is available at\nhttps://github.com/OpenMatch/ANCE-Tele.", "published": "2022-10-31 09:25:42", "link": "http://arxiv.org/abs/2210.17167v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pneg: Prompt-based Negative Response Generation for Dialogue Response\n  Selection Task", "abstract": "In retrieval-based dialogue systems, a response selection model acts as a\nranker to select the most appropriate response among several candidates.\nHowever, such selection models tend to rely on context-response content\nsimilarity, which makes models vulnerable to adversarial responses that are\nsemantically similar but not relevant to the dialogue context. Recent studies\nhave shown that leveraging these adversarial responses as negative training\nsamples is useful for improving the discriminating power of the selection\nmodel. Nevertheless, collecting human-written adversarial responses is\nexpensive, and existing synthesizing methods often have limited scalability. To\novercome these limitations, this paper proposes a simple but efficient method\nfor generating adversarial negative responses leveraging a large-scale language\nmodel. Experimental results on dialogue selection tasks show that our method\noutperforms other methods of synthesizing adversarial negative responses. These\nresults suggest that our method can be an effective alternative to human\nannotators in generating adversarial responses. Our dataset and generation code\nis available at https://github.com/leenw23/generating-negatives-by-gpt3.", "published": "2022-10-31 11:49:49", "link": "http://arxiv.org/abs/2210.17238v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Effective Cross-Task Transfer Learning for Explainable Natural Language\n  Inference with T5", "abstract": "We compare sequential fine-tuning with a model for multi-task learning in the\ncontext where we are interested in boosting performance on two tasks, one of\nwhich depends on the other. We test these models on the FigLang2022 shared task\nwhich requires participants to predict language inference labels on figurative\nlanguage along with corresponding textual explanations of the inference\npredictions. Our results show that while sequential multi-task learning can be\ntuned to be good at the first of two target tasks, it performs less well on the\nsecond and additionally struggles with overfitting. Our findings show that\nsimple sequential fine-tuning of text-to-text models is an extraordinarily\npowerful method for cross-task knowledge transfer while simultaneously\npredicting multiple interdependent targets. So much so, that our best model\nachieved the (tied) highest score on the task.", "published": "2022-10-31 13:26:08", "link": "http://arxiv.org/abs/2210.17301v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Questioning the Validity of Summarization Datasets and Improving Their\n  Factual Consistency", "abstract": "The topic of summarization evaluation has recently attracted a surge of\nattention due to the rapid development of abstractive summarization systems.\nHowever, the formulation of the task is rather ambiguous, neither the\nlinguistic nor the natural language processing community has succeeded in\ngiving a mutually agreed-upon definition. Due to this lack of well-defined\nformulation, a large number of popular abstractive summarization datasets are\nconstructed in a manner that neither guarantees validity nor meets one of the\nmost essential criteria of summarization: factual consistency. In this paper,\nwe address this issue by combining state-of-the-art factual consistency models\nto identify the problematic instances present in popular summarization\ndatasets. We release SummFC, a filtered summarization dataset with improved\nfactual consistency, and demonstrate that models trained on this dataset\nachieve improved performance in nearly all quality aspects. We argue that our\ndataset should become a valid benchmark for developing and evaluating\nsummarization systems.", "published": "2022-10-31 15:04:20", "link": "http://arxiv.org/abs/2210.17378v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic Novelty Detection and Characterization in Factual Text\n  Involving Named Entities", "abstract": "Much of the existing work on text novelty detection has been studied at the\ntopic level, i.e., identifying whether the topic of a document or a sentence is\nnovel or not. Little work has been done at the fine-grained semantic level (or\ncontextual level). For example, given that we know Elon Musk is the CEO of a\ntechnology company, the sentence \"Elon Musk acted in the sitcom The Big Bang\nTheory\" is novel and surprising because normally a CEO would not be an actor.\nExisting topic-based novelty detection methods work poorly on this problem\nbecause they do not perform semantic reasoning involving relations between\nnamed entities in the text and their background knowledge. This paper proposes\nan effective model (called PAT-SND) to solve the problem, which can also\ncharacterize the novelty. An annotated dataset is also created. Evaluation\nshows that PAT-SND outperforms 10 baselines by large margins.", "published": "2022-10-31 16:11:01", "link": "http://arxiv.org/abs/2210.17440v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Domain Curricula for Code-Switched MT at MixMT 2022", "abstract": "In multilingual colloquial settings, it is a habitual occurrence to compose\nexpressions of text or speech containing tokens or phrases of different\nlanguages, a phenomenon popularly known as code-switching or code-mixing (CMX).\nWe present our approach and results for the Code-mixed Machine Translation\n(MixMT) shared task at WMT 2022: the task consists of two subtasks, monolingual\nto code-mixed machine translation (Subtask-1) and code-mixed to monolingual\nmachine translation (Subtask-2). Most non-synthetic code-mixed data are from\nsocial media but gathering a significant amount of this kind of data would be\nlaborious and this form of data has more writing variation than other domains,\nso for both subtasks, we experimented with data schedules for out-of-domain\ndata. We jointly learn multiple domains of text by pretraining and fine-tuning,\ncombined with a sentence alignment objective. We found that switching between\ndomains caused improved performance in the domains seen earliest during\ntraining, but depleted the performance on the remaining domains. A continuous\ntraining run with strategically dispensed data of different domains showed a\nsignificantly improved performance over fine-tuning.", "published": "2022-10-31 16:41:57", "link": "http://arxiv.org/abs/2210.17463v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Chronic pain patient narratives allow for the estimation of current pain\n  intensity", "abstract": "Chronic pain is a multi-dimensional experience, and pain intensity plays an\nimportant part, impacting the patients emotional balance, psychology, and\nbehaviour. Standard self-reporting tools, such as the Visual Analogue Scale for\npain, fail to capture this burden. Moreover, this type of tools is susceptible\nto a degree of subjectivity, dependent on the patients clear understanding of\nhow to use it, social biases, and their ability to translate a complex\nexperience to a scale. To overcome these and other self-reporting challenges,\npain intensity estimation has been previously studied based on facial\nexpressions, electroencephalograms, brain imaging, and autonomic features.\nHowever, to the best of our knowledge, it has never been attempted to base this\nestimation on the patient narratives of the personal experience of chronic\npain, which is what we propose in this work. Indeed, in the clinical assessment\nand management of chronic pain, verbal communication is essential to convey\ninformation to physicians that would otherwise not be easily accessible through\nstandard reporting tools, since language, sociocultural, and psychosocial\nvariables are intertwined. We show that language features from patient\nnarratives indeed convey information relevant for pain intensity estimation,\nand that our computational models can take advantage of that. Specifically, our\nresults show that patients with mild pain focus more on the use of verbs,\nwhilst moderate and severe pain patients focus on adverbs, and nouns and\nadjectives, respectively, and that these differences allow for the distinction\nbetween these three pain classes.", "published": "2022-10-31 16:59:21", "link": "http://arxiv.org/abs/2210.17473v3", "categories": ["cs.CL", "I.2.7; I.5.4; J.3; J.4"], "primary_category": "cs.CL"}
{"title": "Query Refinement Prompts for Closed-Book Long-Form Question Answering", "abstract": "Large language models (LLMs) have been shown to perform well in answering\nquestions and in producing long-form texts, both in few-shot closed-book\nsettings. While the former can be validated using well-known evaluation\nmetrics, the latter is difficult to evaluate. We resolve the difficulties to\nevaluate long-form output by doing both tasks at once -- to do question\nanswering that requires long-form answers. Such questions tend to be\nmultifaceted, i.e., they may have ambiguities and/or require information from\nmultiple sources. To this end, we define query refinement prompts that\nencourage LLMs to explicitly express the multifacetedness in questions and\ngenerate long-form answers covering multiple facets of the question. Our\nexperiments on two long-form question answering datasets, ASQA and AQuAMuSe,\nshow that using our prompts allows us to outperform fully finetuned models in\nthe closed book setting, as well as achieve results comparable to\nretrieve-then-generate open-book models.", "published": "2022-10-31 17:44:42", "link": "http://arxiv.org/abs/2210.17525v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Very Low Resource Sentence Alignment: Luhya and Swahili", "abstract": "Language-agnostic sentence embeddings generated by pre-trained models such as\nLASER and LaBSE are attractive options for mining large datasets to produce\nparallel corpora for low-resource machine translation. We test LASER and LaBSE\nin extracting bitext for two related low-resource African languages: Luhya and\nSwahili. For this work, we created a new parallel set of nearly 8000\nLuhya-English sentences which allows a new zero-shot test of LASER and LaBSE.\nWe find that LaBSE significantly outperforms LASER on both languages. Both\nLASER and LaBSE however perform poorly at zero-shot alignment on Luhya,\nachieving just 1.5% and 22.0% successful alignments respectively (P@1 score).\nWe fine-tune the embeddings on a small set of parallel Luhya sentences and show\nsignificant gains, improving the LaBSE alignment accuracy to 53.3%. Further,\nrestricting the dataset to sentence embedding pairs with cosine similarity\nabove 0.7 yielded alignments with over 85% accuracy.", "published": "2022-10-31 18:01:13", "link": "http://arxiv.org/abs/2211.00046v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating Sequences by Learning to Self-Correct", "abstract": "Sequence generation applications require satisfying semantic constraints,\nsuch as ensuring that programs are correct, using certain keywords, or avoiding\nundesirable content. Language models, whether fine-tuned or prompted with\nfew-shot demonstrations, frequently violate these constraints, and lack a\nmechanism to iteratively revise their outputs. Moreover, some powerful language\nmodels are of extreme scale or inaccessible, making it inefficient, if not\ninfeasible, to update their parameters for task-specific adaptation. We present\nSelf-Correction, an approach that decouples an imperfect base generator (an\noff-the-shelf language model or supervised sequence-to-sequence model) from a\nseparate corrector that learns to iteratively correct imperfect generations. To\ntrain the corrector, we propose an online training procedure that can use\neither scalar or natural language feedback on intermediate imperfect\ngenerations. We show that Self-Correction improves upon the base generator in\nthree diverse generation tasks - mathematical program synthesis,\nlexically-constrained generation, and toxicity control - even when the\ncorrector is much smaller than the base generator.", "published": "2022-10-31 18:09:51", "link": "http://arxiv.org/abs/2211.00053v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data-Efficient Cross-Lingual Transfer with Language-Specific Subnetworks", "abstract": "Large multilingual language models typically share their parameters across\nall languages, which enables cross-lingual task transfer, but learning can also\nbe hindered when training updates from different languages are in conflict. In\nthis paper, we propose novel methods for using language-specific subnetworks,\nwhich control cross-lingual parameter sharing, to reduce conflicts and increase\npositive transfer during fine-tuning. We introduce dynamic subnetworks, which\nare jointly updated with the model, and we combine our methods with\nmeta-learning, an established, but complementary, technique for improving\ncross-lingual transfer. Finally, we provide extensive analyses of how each of\nour methods affects the models.", "published": "2022-10-31 19:23:33", "link": "http://arxiv.org/abs/2211.00106v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluation of large-scale synthetic data for Grammar Error Correction", "abstract": "Grammar Error Correction(GEC) mainly relies on the availability of high\nquality of large amount of synthetic parallel data of grammatically correct and\nerroneous sentence pairs. The quality of the synthetic data is evaluated on how\nwell the GEC system performs when pre-trained using it. But this does not\nprovide much insight into what are the necessary factors which define the\nquality of these data. So this work aims to introduce 3 metrics - reliability,\ndiversity and distribution match to provide more insight into the quality of\nlarge-scale synthetic data generated for the GEC task, as well as automatically\nevaluate them. Evaluating these three metrics automatically can also help in\nproviding feedback to the data generation systems and thereby improve the\nquality of the synthetic data generated dynamically", "published": "2022-10-31 03:19:09", "link": "http://arxiv.org/abs/2210.17035v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Cause-of-Death Classification from Verbal Autopsy Reports", "abstract": "In many lower-and-middle income countries including South Africa, data access\nin health facilities is restricted due to patient privacy and confidentiality\npolicies. Further, since clinical data is unique to individual institutions and\nlaboratories, there are insufficient data annotation standards and conventions.\nAs a result of the scarcity of textual data, natural language processing (NLP)\ntechniques have fared poorly in the health sector. A cause of death (COD) is\noften determined by a verbal autopsy (VA) report in places without reliable\ndeath registration systems. A non-clinician field worker does a VA report using\na set of standardized questions as a guide to uncover symptoms of a COD. This\nanalysis focuses on the textual part of the VA report as a case study to\naddress the challenge of adapting NLP techniques in the health domain. We\npresent a system that relies on two transfer learning paradigms of monolingual\nlearning and multi-source domain adaptation to improve VA narratives for the\ntarget task of the COD classification. We use the Bidirectional Encoder\nRepresentations from Transformers (BERT) and Embeddings from Language Models\n(ELMo) models pre-trained on the general English and health domains to extract\nfeatures from the VA narratives. Our findings suggest that this transfer\nlearning system improves the COD classification tasks and that the narrative\ntext contains valuable information for figuring out a COD. Our results further\nshow that combining binary VA features and narrative text features learned via\nthis framework boosts the classification task of COD.", "published": "2022-10-31 09:14:08", "link": "http://arxiv.org/abs/2210.17161v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SDCL: Self-Distillation Contrastive Learning for Chinese Spell Checking", "abstract": "Due to the ambiguity of homophones, Chinese Spell Checking (CSC) has\nwidespread applications. Existing systems typically utilize BERT for text\nencoding. However, CSC requires the model to account for both phonetic and\ngraphemic information. To adapt BERT to the CSC task, we propose a token-level\nself-distillation contrastive learning method. We employ BERT to encode both\nthe corrupted and corresponding correct sentence. Then, we use contrastive\nlearning loss to regularize corrupted tokens' hidden states to be closer to\ncounterparts in the correct sentence. On three CSC datasets, we confirmed our\nmethod provides a significant improvement above baselines.", "published": "2022-10-31 09:29:21", "link": "http://arxiv.org/abs/2210.17168v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "IITD at the WANLP 2022 Shared Task: Multilingual Multi-Granularity\n  Network for Propaganda Detection", "abstract": "We present our system for the two subtasks of the shared task on propaganda\ndetection in Arabic, part of WANLP'2022. Subtask 1 is a multi-label\nclassification problem to find the propaganda techniques used in a given tweet.\nOur system for this task uses XLM-R to predict probabilities for the target\ntweet to use each of the techniques. In addition to finding the techniques,\nSubtask 2 further asks to identify the textual span for each instance of each\ntechnique that is present in the tweet; the task can be modeled as a sequence\ntagging problem. We use a multi-granularity network with mBERT encoder for\nSubtask 2. Overall, our system ranks second for both subtasks (out of 14 and 3\nparticipants, respectively). Our empirical analysis show that it does not help\nto use a much larger English corpus annotated with propaganda techniques,\nregardless of whether used in English or after translation to Arabic.", "published": "2022-10-31 10:14:43", "link": "http://arxiv.org/abs/2210.17190v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Effect of Multiple Replies for Natural Language Generation Chatbots", "abstract": "In this research, by responding to users' utterances with multiple replies to\ncreate a group chat atmosphere, we alleviate the problem that Natural Language\nGeneration chatbots might reply with inappropriate content, thus causing a bad\nuser experience. Because according to our findings, users tend to pay attention\nto appropriate replies and ignore inappropriate replies. We conducted a 2\n(single reply vs. five replies) x 2 (anonymous avatar vs. anime avatar)\nrepeated measures experiment to compare the chatting experience in different\nconditions. The result shows that users will have a better chatting experience\nwhen receiving multiple replies at once from the NLG model compared to the\nsingle reply. Furthermore, according to the effect size of our result, to\nimprove the chatting experience for NLG chatbots which is single reply and\nanonymous avatar, providing five replies will have more benefits than setting\nan anime avatar.", "published": "2022-10-31 10:45:18", "link": "http://arxiv.org/abs/2210.17209v2", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "50 Ways to Bake a Cookie: Mapping the Landscape of Procedural Texts", "abstract": "The web is full of guidance on a wide variety of tasks, from changing the oil\nin your car to baking an apple pie. However, as content is created\nindependently, a single task could have thousands of corresponding procedural\ntexts. This makes it difficult for users to view the bigger picture and\nunderstand the multiple ways the task could be accomplished. In this work we\npropose an unsupervised learning approach for summarizing multiple procedural\ntexts into an intuitive graph representation, allowing users to easily explore\ncommonalities and differences. We demonstrate our approach on recipes, a\nprominent example of procedural texts. User studies show that our\nrepresentation is intuitive and coherent and that it has the potential to help\nusers with several sensemaking tasks, including adapting recipes for a novice\ncook and finding creative ways to spice up a dish.", "published": "2022-10-31 11:41:54", "link": "http://arxiv.org/abs/2210.17235v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Emergent Linguistic Structures in Neural Networks are Fragile", "abstract": "Large Language Models (LLMs) have been reported to have strong performance on\nnatural language processing tasks. However, performance metrics such as\naccuracy do not measure the quality of the model in terms of its ability to\nrobustly represent complex linguistic structures. In this paper, focusing on\nthe ability of language models to represent syntax, we propose a framework to\nassess the consistency and robustness of linguistic representations. To this\nend, we introduce measures of robustness of neural network models that leverage\nrecent advances in extracting linguistic constructs from LLMs via probing\ntasks, i.e., simple tasks used to extract meaningful information about a single\nfacet of a language model, such as syntax reconstruction and root\nidentification. Empirically, we study the performance of four LLMs across six\ndifferent corpora on the proposed robustness measures by analysing their\nperformance and robustness with respect to syntax-preserving perturbations. We\nprovide evidence that context-free representation (e.g., GloVe) are in some\ncases competitive with context-dependent representations from modern LLMs\n(e.g., BERT), yet equally brittle to syntax-preserving perturbations. Our key\nobservation is that emergent syntactic representations in neural networks are\nbrittle. We make the code, trained models and logs available to the community\nas a contribution to the debate about the capabilities of LLMs.", "published": "2022-10-31 15:43:57", "link": "http://arxiv.org/abs/2210.17406v8", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "SSD-LM: Semi-autoregressive Simplex-based Diffusion Language Model for\n  Text Generation and Modular Control", "abstract": "Despite the growing success of diffusion models in continuous-valued domains\n(e.g., images), similar efforts for discrete domains such as text have yet to\nmatch the performance of autoregressive language models. In this work, we\npresent SSD-LM -- a diffusion-based language model with two key design choices.\nFirst, SSD-LM is semi-autoregressive, iteratively generating blocks of text,\nallowing for flexible output length at decoding time while enabling local\nbidirectional context updates. Second, it is simplex-based, performing\ndiffusion on the natural vocabulary space rather than a learned latent space,\nallowing us to incorporate classifier guidance and modular control using\noff-the-shelf classifiers without any adaptation. We evaluate SSD-LM on\nunconstrained text generation benchmarks, and show that it matches or\noutperforms strong autoregressive GPT-2 models across standard quality and\ndiversity metrics, while vastly outperforming diffusion-based baselines. On\ncontrolled text generation, SSD-LM also outperforms competitive baselines, with\nan extra advantage in modularity.", "published": "2022-10-31 16:02:00", "link": "http://arxiv.org/abs/2210.17432v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning New Tasks from a Few Examples with Soft-Label Prototypes", "abstract": "Existing approaches to few-shot learning in NLP rely on large language models\n(LLMs) and/or fine-tuning of these to generalise on out-of-distribution data.\nIn this work, we propose a novel few-shot learning approach based on soft-label\nprototypes (SLPs) designed to collectively capture the distribution of\ndifferent classes across the input domain space. We focus on learning\npreviously unseen NLP tasks from very few examples (4, 8, 16) per class and\nexperimentally demonstrate that our approach achieves superior performance on\nthe majority of tested tasks in this data-lean setting while being highly\nparameter efficient. We also show that our few-shot adaptation method can be\nintegrated into more generalised learning settings, primarily meta-learning, to\nyield superior performance against strong baselines.", "published": "2022-10-31 16:06:48", "link": "http://arxiv.org/abs/2210.17437v4", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A Case Study of Chinese Sentiment Analysis on Social Media Reviews Based\n  on LSTM", "abstract": "Network public opinion analysis is obtained by a combination of natural\nlanguage processing (NLP) and public opinion supervision, and is crucial for\nmonitoring public mood and trends. Therefore, network public opinion analysis\ncan identify and solve potential and budding social problems. This study aims\nto realize an analysis of Chinese sentiment in social media reviews using a\nlong short-term memory network (LSTM) model. The dataset was obtained from Sina\nWeibo using a web crawler and was cleaned with Pandas. First, Chinese comments\nregarding the legal sentencing in of Tangshan attack and Jiang Ge Case were\nsegmented and vectorized. Then, a binary LSTM model was trained and tested.\nFinally, sentiment analysis results were obtained by analyzing the comments\nwith the LSTM model. The accuracy of the proposed model has reached\napproximately 92%.", "published": "2022-10-31 16:23:37", "link": "http://arxiv.org/abs/2210.17452v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Lila: A Unified Benchmark for Mathematical Reasoning", "abstract": "Mathematical reasoning skills are essential for general-purpose intelligent\nsystems to perform tasks from grocery shopping to climate modeling. Towards\nevaluating and improving AI systems in this domain, we propose LILA, a unified\nmathematical reasoning benchmark consisting of 23 diverse tasks along four\ndimensions: (i) mathematical abilities e.g., arithmetic, calculus (ii) language\nformat e.g., question-answering, fill-in-the-blanks (iii) language diversity\ne.g., no language, simple language (iv) external knowledge e.g., commonsense,\nphysics. We construct our benchmark by extending 20 datasets benchmark by\ncollecting task instructions and solutions in the form of Python programs,\nthereby obtaining explainable solutions in addition to the correct answer. We\nadditionally introduce two evaluation datasets to measure out-of-distribution\nperformance and robustness to language perturbation. Finally, we introduce\nBHASKARA, a general-purpose mathematical reasoning model trained on LILA.\nImportantly, we find that multi-tasking leads to significant improvements\n(average relative improvement of 21.83% F1 score vs. single-task models), while\nthe best performing model only obtains 60.40%, indicating the room for\nimprovement in general mathematical reasoning and understanding.", "published": "2022-10-31 17:41:26", "link": "http://arxiv.org/abs/2210.17517v2", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Text Classification with Self-Training", "abstract": "Recent advances in large pretrained language models have increased attention\nto zero-shot text classification. In particular, models finetuned on natural\nlanguage inference datasets have been widely adopted as zero-shot classifiers\ndue to their promising results and off-the-shelf availability. However, the\nfact that such models are unfamiliar with the target task can lead to\ninstability and performance issues. We propose a plug-and-play method to bridge\nthis gap using a simple self-training approach, requiring only the class names\nalong with an unlabeled dataset, and without the need for domain expertise or\ntrial and error. We show that fine-tuning the zero-shot classifier on its most\nconfident predictions leads to significant performance gains across a wide\nrange of text classification tasks, presumably since self-training adapts the\nzero-shot model to the task at hand.", "published": "2022-10-31 17:55:00", "link": "http://arxiv.org/abs/2210.17541v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Preventing Verbatim Memorization in Language Models Gives a False Sense\n  of Privacy", "abstract": "Studying data memorization in neural language models helps us understand the\nrisks (e.g., to privacy or copyright) associated with models regurgitating\ntraining data and aids in the development of countermeasures. Many prior works\n-- and some recently deployed defenses -- focus on \"verbatim memorization\",\ndefined as a model generation that exactly matches a substring from the\ntraining set. We argue that verbatim memorization definitions are too\nrestrictive and fail to capture more subtle forms of memorization.\nSpecifically, we design and implement an efficient defense that perfectly\nprevents all verbatim memorization. And yet, we demonstrate that this \"perfect\"\nfilter does not prevent the leakage of training data. Indeed, it is easily\ncircumvented by plausible and minimally modified \"style-transfer\" prompts --\nand in some cases even the non-modified original prompts -- to extract\nmemorized information. We conclude by discussing potential alternative\ndefinitions and why defining memorization is a difficult yet crucial open\nquestion for neural language models.", "published": "2022-10-31 17:57:55", "link": "http://arxiv.org/abs/2210.17546v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "TaTa: A Multilingual Table-to-Text Dataset for African Languages", "abstract": "Existing data-to-text generation datasets are mostly limited to English. To\naddress this lack of data, we create Table-to-Text in African languages (TaTa),\nthe first large multilingual table-to-text dataset with a focus on African\nlanguages. We created TaTa by transcribing figures and accompanying text in\nbilingual reports by the Demographic and Health Surveys Program, followed by\nprofessional translation to make the dataset fully parallel. TaTa includes\n8,700 examples in nine languages including four African languages (Hausa, Igbo,\nSwahili, and Yor\\`ub\\'a) and a zero-shot test language (Russian). We\nadditionally release screenshots of the original figures for future research on\nmultilingual multi-modal approaches. Through an in-depth human evaluation, we\nshow that TaTa is challenging for current models and that less than half the\noutputs from an mT5-XXL-based model are understandable and attributable to the\nsource data. We further demonstrate that existing metrics perform poorly for\nTaTa and introduce learned metrics that achieve a high correlation with human\njudgments. We release all data and annotations at\nhttps://github.com/google-research/url-nlp.", "published": "2022-10-31 21:05:42", "link": "http://arxiv.org/abs/2211.00142v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Close Look into the Calibration of Pre-trained Language Models", "abstract": "Pre-trained language models (PLMs) may fail in giving reliable estimates of\ntheir predictive uncertainty. We take a close look into this problem, aiming to\nanswer two questions: (1) Do PLMs learn to become calibrated in the training\nprocess? (2) How effective are existing calibration methods? For the first\nquestion, we conduct fine-grained control experiments to study the dynamic\nchange in PLMs' calibration performance in training. We consider six factors as\ncontrol variables, including dataset difficulty, available training samples,\ntraining steps, the number of tunable parameters, model scale, and pretraining.\nWe observe a consistent change in calibration performance across six factors.\nWe find that PLMs don't learn to become calibrated in training, evidenced by\nthe continual increase in confidence, no matter whether the predictions are\ncorrect or not. We highlight that our finding somewhat contradicts two\nestablished conclusions: (a) Larger PLMs are more calibrated; (b) Pretraining\nimproves model calibration. Next, we study the effectiveness of existing\ncalibration methods in mitigating the overconfidence issue. Besides unlearnable\ncalibration methods (e.g., label smoothing), we adapt and extend two recently\nproposed learnable methods that directly collect data to train models to have\nreasonable confidence estimations. Experimental results show that learnable\nmethods significantly reduce PLMs' confidence in wrong predictions. The code is\navailable at \\url{https://github.com/lifan-yuan/PLMCalibration}.", "published": "2022-10-31 21:31:07", "link": "http://arxiv.org/abs/2211.00151v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Blank Collapse: Compressing CTC emission for the faster decoding", "abstract": "Connectionist Temporal Classification (CTC) model is a very efficient method\nfor modeling sequences, especially for speech data. In order to use CTC model\nas an Automatic Speech Recognition (ASR) task, the beam search decoding with an\nexternal language model like n-gram LM is necessary to obtain reasonable\nresults. In this paper we analyze the blank label in CTC beam search deeply and\npropose a very simple method to reduce the amount of calculation resulting in\nfaster beam search decoding speed. With this method, we can get up to 78%\nfaster decoding speed than ordinary beam search decoding with a very small loss\nof accuracy in LibriSpeech datasets. We prove this method is effective not only\npractically by experiments but also theoretically by mathematical reasoning. We\nalso observe that this reduction is more obvious if the accuracy of the model\nis higher.", "published": "2022-10-31 02:12:51", "link": "http://arxiv.org/abs/2210.17017v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Joint Pre-Training with Speech and Bilingual Text for Direct Speech to\n  Speech Translation", "abstract": "Direct speech-to-speech translation (S2ST) is an attractive research topic\nwith many advantages compared to cascaded S2ST. However, direct S2ST suffers\nfrom the data scarcity problem because the corpora from speech of the source\nlanguage to speech of the target language are very rare. To address this issue,\nwe propose in this paper a Speech2S model, which is jointly pre-trained with\nunpaired speech and bilingual text data for direct speech-to-speech translation\ntasks. By effectively leveraging the paired text data, Speech2S is capable of\nmodeling the cross-lingual speech conversion from source to target language. We\nverify the performance of the proposed Speech2S on Europarl-ST and VoxPopuli\ndatasets. Experimental results demonstrate that Speech2S gets an improvement of\nabout 5 BLEU scores compared to encoder-only pre-training models, and achieves\na competitive or even better performance than existing state-of-the-art\nmodels1.", "published": "2022-10-31 02:55:51", "link": "http://arxiv.org/abs/2210.17027v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "FusionFormer: Fusing Operations in Transformer for Efficient Streaming\n  Speech Recognition", "abstract": "The recently proposed Conformer architecture which combines convolution with\nattention to capture both local and global dependencies has become the\n\\textit{de facto} backbone model for Automatic Speech Recognition~(ASR).\nInherited from the Natural Language Processing (NLP) tasks, the architecture\ntakes Layer Normalization~(LN) as a default normalization technique. However,\nthrough a series of systematic studies, we find that LN might take 10\\% of the\ninference time despite that it only contributes to 0.1\\% of the FLOPs. This\nmotivates us to replace LN with other normalization techniques, e.g., Batch\nNormalization~(BN), to speed up inference with the help of operator fusion\nmethods and the avoidance of calculating the mean and variance statistics\nduring inference. After examining several plain attempts which directly remove\nall LN layers or replace them with BN in the same place, we find that the\ndivergence issue is mainly caused by the unstable layer output. We therefore\npropose to append a BN layer to each linear or convolution layer where\nstabilized training results are observed. We also propose to simplify the\nactivations in Conformer, such as Swish and GLU, by replacing them with ReLU.\nAll these exchanged modules can be fused into the weights of the adjacent\nlinear/convolution layers and hence have zero inference cost. Therefore, we\nname it FusionFormer. Our experiments indicate that FusionFormer is as\neffective as the LN-based Conformer and is about 10\\% faster.", "published": "2022-10-31 06:01:02", "link": "http://arxiv.org/abs/2210.17079v1", "categories": ["cs.SD", "cs.CL", "eess.AS", "I.2.7"], "primary_category": "cs.SD"}
{"title": "Exploring Train and Test-Time Augmentations for Audio-Language Learning", "abstract": "In this paper, we aim to unveil the impact of data augmentation in\naudio-language multi-modal learning, which has not been explored despite its\nimportance. We explore various augmentation methods at not only train-time but\nalso test-time and find out that proper data augmentation can lead to\nsubstantial improvements. Specifically, applying our proposed audio-language\npaired augmentation PairMix, which is the first multi-modal audio-language\naugmentation method, outperforms the baselines for both automated audio\ncaptioning and audio-text retrieval tasks. To fully take advantage of data\naugmentation, we also present multi-level test-time augmentation (Multi-TTA)\nfor the test-time. We successfully incorporate the two proposed methods and\nuni-modal augmentations and achieve 47.5 SPIDEr on audio captioning, which is\nan 18.2% relative increase over the baseline. In audio-text retrieval, the\nproposed methods also show an improvement in performance as well.", "published": "2022-10-31 08:50:52", "link": "http://arxiv.org/abs/2210.17143v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "When Language Model Meets Private Library", "abstract": "With the rapid development of pre-training techniques, a number of language\nmodels have been pre-trained on large-scale code corpora and perform well in\ncode generation. In this paper, we investigate how to equip pre-trained\nlanguage models with the ability of code generation for private libraries. In\npractice, it is common for programmers to write code using private libraries.\nHowever, this is a challenge for language models since they have never seen\nprivate APIs during training. Motivated by the fact that private libraries\nusually come with elaborate API documentation, we propose a novel framework\nwith two modules: the APIRetriever finds useful APIs, and then the APICoder\ngenerates code using these APIs. For APIRetriever, we present a dense retrieval\nsystem and also design a friendly interaction to involve uses. For APICoder, we\ncan directly use off-the-shelf language models, or continually pre-train the\nbase model on a code corpus containing API information. Both modules are\ntrained with data from public libraries and can be generalized to private ones.\nFurthermore, we craft three benchmarks for private libraries, named\nTorchDataEval, MonkeyEval, and BeatNumEval. Experimental results demonstrate\nthe impressive performance of our framework.", "published": "2022-10-31 11:42:06", "link": "http://arxiv.org/abs/2210.17236v1", "categories": ["cs.PL", "cs.CL", "cs.SE"], "primary_category": "cs.PL"}
{"title": "Cross-lingual Text-To-Speech with Flow-based Voice Conversion for\n  Improved Pronunciation", "abstract": "This paper presents a method for end-to-end cross-lingual text-to-speech\n(TTS) which aims to preserve the target language's pronunciation regardless of\nthe original speaker's language. The model used is based on a non-attentive\nTacotron architecture, where the decoder has been replaced with a normalizing\nflow network conditioned on the speaker identity, allowing both TTS and voice\nconversion (VC) to be performed by the same model due to the inherent\nlinguistic content and speaker identity disentanglement. When used in a\ncross-lingual setting, acoustic features are initially produced with a native\nspeaker of the target language and then voice conversion is applied by the same\nmodel in order to convert these features to the target speaker's voice. We\nverify through objective and subjective evaluations that our method can have\nbenefits compared to baseline cross-lingual synthesis. By including speakers\naveraging 7.5 minutes of speech, we also present positive results on\nlow-resource scenarios.", "published": "2022-10-31 12:44:53", "link": "http://arxiv.org/abs/2210.17264v2", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Controllable Factuality in Document-Grounded Dialog Systems Using a\n  Noisy Channel Model", "abstract": "In this work, we present a model for document-grounded response generation in\ndialog that is decomposed into two components according to Bayes theorem. One\ncomponent is a traditional ungrounded response generation model and the other\ncomponent models the reconstruction of the grounding document based on the\ndialog context and generated response. We propose different approximate\ndecoding schemes and evaluate our approach on multiple open-domain and\ntask-oriented document-grounded dialog datasets. Our experiments show that the\nmodel is more factual in terms of automatic factuality metrics than the\nbaseline model. Furthermore, we outline how introducing scaling factors between\nthe components allows for controlling the tradeoff between factuality and\nfluency in the model output. Finally, we compare our approach to a recently\nproposed method to control factuality in grounded dialog, CTRL\n(arXiv:2107.06963), and show that both approaches can be combined to achieve\nadditional improvements.", "published": "2022-10-31 15:48:01", "link": "http://arxiv.org/abs/2210.17418v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AdaMix: Mixture-of-Adaptations for Parameter-efficient Model Tuning", "abstract": "Standard fine-tuning of large pre-trained language models (PLMs) for\ndownstream tasks requires updating hundreds of millions to billions of\nparameters, and storing a large copy of the PLM weights for every task\nresulting in increased cost for storing, sharing and serving the models. To\naddress this, parameter-efficient fine-tuning (PEFT) techniques were introduced\nwhere small trainable components are injected in the PLM and updated during\nfine-tuning. We propose AdaMix as a general PEFT method that tunes a mixture of\nadaptation modules -- given the underlying PEFT method of choice -- introduced\nin each Transformer layer while keeping most of the PLM weights frozen. For\ninstance, AdaMix can leverage a mixture of adapters like Houlsby or a mixture\nof low rank decomposition matrices like LoRA to improve downstream task\nperformance over the corresponding PEFT methods for fully supervised and\nfew-shot NLU and NLG tasks. Further, we design AdaMix such that it matches the\nsame computational cost and the number of tunable parameters as the underlying\nPEFT method. By only tuning 0.1-0.2% of PLM parameters, we show that AdaMix\noutperforms SOTA parameter-efficient fine-tuning and full model fine-tuning for\nboth NLU and NLG tasks.", "published": "2022-10-31 16:23:36", "link": "http://arxiv.org/abs/2210.17451v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Leveraging Pre-trained Models for Failure Analysis Triplets Generation", "abstract": "Pre-trained Language Models recently gained traction in the Natural Language\nProcessing (NLP) domain for text summarization, generation and\nquestion-answering tasks. This stems from the innovation introduced in\nTransformer models and their overwhelming performance compared with Recurrent\nNeural Network Models (Long Short Term Memory (LSTM)). In this paper, we\nleverage the attention mechanism of pre-trained causal language models such as\nTransformer model for the downstream task of generating Failure Analysis\nTriplets (FATs) - a sequence of steps for analyzing defected components in the\nsemiconductor industry. We compare different transformer models for this\ngenerative task and observe that Generative Pre-trained Transformer 2 (GPT2)\noutperformed other transformer model for the failure analysis triplet\ngeneration (FATG) task. In particular, we observe that GPT2 (trained on 1.5B\nparameters) outperforms pre-trained BERT, BART and GPT3 by a large margin on\nROUGE. Furthermore, we introduce Levenshstein Sequential Evaluation metric\n(LESE) for better evaluation of the structured FAT data and show that it\ncompares exactly with human judgment than existing metrics.", "published": "2022-10-31 17:21:15", "link": "http://arxiv.org/abs/2210.17497v1", "categories": ["cs.CL", "cs.LG", "stat.AP", "68Txx, 68Uxx", "G.3; I.2; I.7"], "primary_category": "cs.CL"}
{"title": "WHEN FLUE MEETS FLANG: Benchmarks and Large Pre-trained Language Model\n  for Financial Domain", "abstract": "Pre-trained language models have shown impressive performance on a variety of\ntasks and domains. Previous research on financial language models usually\nemploys a generic training scheme to train standard model architectures,\nwithout completely leveraging the richness of the financial data. We propose a\nnovel domain specific Financial LANGuage model (FLANG) which uses financial\nkeywords and phrases for better masking, together with span boundary objective\nand in-filing objective. Additionally, the evaluation benchmarks in the field\nhave been limited. To this end, we contribute the Financial Language\nUnderstanding Evaluation (FLUE), an open-source comprehensive suite of\nbenchmarks for the financial domain. These include new benchmarks across 5 NLP\ntasks in financial domain as well as common benchmarks used in the previous\nresearch. Experiments on these benchmarks suggest that our model outperforms\nthose in prior literature on a variety of NLP tasks. Our models, code and\nbenchmark data are publicly available on Github and Huggingface.", "published": "2022-10-31 18:35:18", "link": "http://arxiv.org/abs/2211.00083v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An analysis of degenerating speech due to progressive dysarthria on ASR\n  performance", "abstract": "Although personalized automatic speech recognition (ASR) models have recently\nbeen designed to recognize even severely impaired speech, model performance may\ndegrade over time for persons with degenerating speech. The aims of this study\nwere to (1) analyze the change of performance of ASR over time in individuals\nwith degrading speech, and (2) explore mitigation strategies to optimize\nrecognition throughout disease progression. Speech was recorded by four\nindividuals with degrading speech due to amyotrophic lateral sclerosis (ALS).\nWord error rates (WER) across recording sessions were computed for three ASR\nmodels: Unadapted Speaker Independent (U-SI), Adapted Speaker Independent\n(A-SI), and Adapted Speaker Dependent (A-SD or personalized). The performance\nof all three models degraded significantly over time as speech became more\nimpaired, but the performance of the A-SD model improved markedly when it was\nupdated with recordings from the severe stages of speech progression. Recording\nadditional utterances early in the disease before speech degraded significantly\ndid not improve the performance of A-SD models. Overall, our findings emphasize\nthe importance of continuous recording (and model retraining) when providing\npersonalized models for individuals with progressive speech impairments.", "published": "2022-10-31 18:42:41", "link": "http://arxiv.org/abs/2211.00089v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Where to start? Analyzing the potential value of intermediate models", "abstract": "Previous studies observed that finetuned models may be better base models\nthan the vanilla pretrained model. Such a model, finetuned on some source\ndataset, may provide a better starting point for a new finetuning process on a\ndesired target dataset. Here, we perform a systematic analysis of this\nintertraining scheme, over a wide range of English classification tasks.\nSurprisingly, our analysis suggests that the potential intertraining gain can\nbe analyzed independently for the target dataset under consideration, and for a\nbase model being considered as a starting point. This is in contrast to current\nperception that the alignment between the target dataset and the source dataset\nused to generate the base model is a major factor in determining intertraining\nsuccess. We analyze different aspects that contribute to each. Furthermore, we\nleverage our analysis to propose a practical and efficient approach to\ndetermine if and how to select a base model in real-world settings. Last, we\nrelease an updating ranking of best models in the HuggingFace hub per\narchitecture https://ibm.github.io/model-recycling/.", "published": "2022-10-31 19:24:02", "link": "http://arxiv.org/abs/2211.00107v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Textless Direct Speech-to-Speech Translation with Discrete Speech\n  Representation", "abstract": "Research on speech-to-speech translation (S2ST) has progressed rapidly in\nrecent years. Many end-to-end systems have been proposed and show advantages\nover conventional cascade systems, which are often composed of recognition,\ntranslation and synthesis sub-systems. However, most of the end-to-end systems\nstill rely on intermediate textual supervision during training, which makes it\ninfeasible to work for languages without written forms. In this work, we\npropose a novel model, Textless Translatotron, which is based on Translatotron\n2, for training an end-to-end direct S2ST model without any textual\nsupervision. Instead of jointly training with an auxiliary task predicting\ntarget phonemes as in Translatotron 2, the proposed model uses an auxiliary\ntask predicting discrete speech representations which are obtained from learned\nor random speech quantizers. When a speech encoder pre-trained with\nunsupervised speech data is used for both models, the proposed model obtains\ntranslation quality nearly on-par with Translatotron 2 on the multilingual\nCVSS-C corpus as well as the bilingual Fisher Spanish-English corpus. On the\nlatter, it outperforms the prior state-of-the-art textless model by +18.5 BLEU.", "published": "2022-10-31 19:48:38", "link": "http://arxiv.org/abs/2211.00115v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Do LSTMs See Gender? Probing the Ability of LSTMs to Learn Abstract\n  Syntactic Rules", "abstract": "LSTMs trained on next-word prediction can accurately perform linguistic tasks\nthat require tracking long-distance syntactic dependencies. Notably, model\naccuracy approaches human performance on number agreement tasks (Gulordava et\nal., 2018). However, we do not have a mechanistic understanding of how LSTMs\nperform such linguistic tasks. Do LSTMs learn abstract grammatical rules, or do\nthey rely on simple heuristics? Here, we test gender agreement in French which\nrequires tracking both hierarchical syntactic structures and the inherent\ngender of lexical units. Our model is able to reliably predict long-distance\ngender agreement in two subject-predicate contexts: noun-adjective and\nnoun-passive-verb agreement. The model showed more inaccuracies on plural noun\nphrases with gender attractors compared to singular cases, suggesting a\nreliance on clues from gendered articles for agreement. Overall, our study\nhighlights key ways in which LSTMs deviate from human behaviour and questions\nwhether LSTMs genuinely learn abstract syntactic rules and categories. We\npropose using gender agreement as a useful probe to investigate the underlying\nmechanisms, internal representations, and linguistic capabilities of LSTM\nlanguage models.", "published": "2022-10-31 21:37:12", "link": "http://arxiv.org/abs/2211.00153v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Using Emotion Embeddings to Transfer Knowledge Between Emotions,\n  Languages, and Annotation Formats", "abstract": "The need for emotional inference from text continues to diversify as more and\nmore disciplines integrate emotions into their theories and applications. These\nneeds include inferring different emotion types, handling multiple languages,\nand different annotation formats. A shared model between different\nconfigurations would enable the sharing of knowledge and a decrease in training\ncosts, and would simplify the process of deploying emotion recognition models\nin novel environments. In this work, we study how we can build a single model\nthat can transition between these different configurations by leveraging\nmultilingual models and Demux, a transformer-based model whose input includes\nthe emotions of interest, enabling us to dynamically change the emotions\npredicted by the model. Demux also produces emotion embeddings, and performing\noperations on them allows us to transition to clusters of emotions by pooling\nthe embeddings of each cluster. We show that Demux can simultaneously transfer\nknowledge in a zero-shot manner to a new language, to a novel annotation format\nand to unseen emotions. Code is available at\nhttps://github.com/gchochla/Demux-MEmo .", "published": "2022-10-31 22:32:36", "link": "http://arxiv.org/abs/2211.00171v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Joint Audio/Text Training for Transformer Rescorer of Streaming Speech\n  Recognition", "abstract": "Recently, there has been an increasing interest in two-pass streaming\nend-to-end speech recognition (ASR) that incorporates a 2nd-pass rescoring\nmodel on top of the conventional 1st-pass streaming ASR model to improve\nrecognition accuracy while keeping latency low. One of the latest 2nd-pass\nrescoring model, Transformer Rescorer, takes the n-best initial outputs and\naudio embeddings from the 1st-pass model, and then choose the best output by\nre-scoring the n-best initial outputs. However, training this Transformer\nRescorer requires expensive paired audio-text training data because the model\nuses audio embeddings as input. In this work, we present our Joint Audio/Text\ntraining method for Transformer Rescorer, to leverage unpaired text-only data\nwhich is relatively cheaper than paired audio-text data. We evaluate\nTransformer Rescorer with our Joint Audio/Text training on Librispeech dataset\nas well as our large-scale in-house dataset and show that our training method\ncan improve word error rate (WER) significantly compared to standard\nTransformer Rescorer without requiring any extra model parameters or latency.", "published": "2022-10-31 22:38:28", "link": "http://arxiv.org/abs/2211.00174v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Fast and parallel decoding for transducer", "abstract": "The transducer architecture is becoming increasingly popular in the field of\nspeech recognition, because it is naturally streaming as well as high in\naccuracy. One of the drawbacks of transducer is that it is difficult to decode\nin a fast and parallel way due to an unconstrained number of symbols that can\nbe emitted per time step. In this work, we introduce a constrained version of\ntransducer loss to learn strictly monotonic alignments between the sequences;\nwe also improve the standard greedy search and beam search algorithms by\nlimiting the number of symbols that can be emitted per time step in transducer\ndecoding, making it more efficient to decode in parallel with batches.\nFurthermore, we propose an finite state automaton-based (FSA) parallel beam\nsearch algorithm that can run with graphs on GPU efficiently. The experiment\nresults show that we achieve slight word error rate (WER) improvement as well\nas significant speedup in decoding. Our work is open-sourced and publicly\navailable\\footnote{https://github.com/k2-fsa/icefall}.", "published": "2022-10-31 07:46:10", "link": "http://arxiv.org/abs/2211.00484v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Delay-penalized transducer for low-latency streaming ASR", "abstract": "In streaming automatic speech recognition (ASR), it is desirable to reduce\nlatency as much as possible while having minimum impact on recognition\naccuracy. Although a few existing methods are able to achieve this goal, they\nare difficult to implement due to their dependency on external alignments. In\nthis paper, we propose a simple way to penalize symbol delay in transducer\nmodel, so that we can balance the trade-off between symbol delay and accuracy\nfor streaming models without external alignments. Specifically, our method adds\na small constant times (T/2 - t), where T is the number of frames and t is the\ncurrent frame, to all the non-blank log-probabilities (after normalization)\nthat are fed into the two dimensional transducer recursion. For both streaming\nConformer models and unidirectional long short-term memory (LSTM) models,\nexperimental results show that it can significantly reduce the symbol delay\nwith an acceptable performance degradation. Our method achieves similar\ndelay-accuracy trade-off to the previously published FastEmit, but we believe\nour method is preferable because it has a better justification: it is\nequivalent to penalizing the average symbol delay. Our work is open-sourced and\npublicly available (https://github.com/k2-fsa/k2).", "published": "2022-10-31 07:03:50", "link": "http://arxiv.org/abs/2211.00490v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Predicting Multi-Codebook Vector Quantization Indexes for Knowledge\n  Distillation", "abstract": "Knowledge distillation(KD) is a common approach to improve model performance\nin automatic speech recognition (ASR), where a student model is trained to\nimitate the output behaviour of a teacher model. However, traditional KD\nmethods suffer from teacher label storage issue, especially when the training\ncorpora are large. Although on-the-fly teacher label generation tackles this\nissue, the training speed is significantly slower as the teacher model has to\nbe evaluated every batch. In this paper, we reformulate the generation of\nteacher label as a codec problem. We propose a novel Multi-codebook Vector\nQuantization (MVQ) approach that compresses teacher embeddings to codebook\nindexes (CI). Based on this, a KD training framework (MVQ-KD) is proposed where\na student model predicts the CI generated from the embeddings of a\nself-supervised pre-trained teacher model. Experiments on the LibriSpeech\nclean-100 hour show that MVQ-KD framework achieves comparable performance as\ntraditional KD methods (l1, l2), while requiring 256 times less storage. When\nthe full LibriSpeech dataset is used, MVQ-KD framework results in 13.8% and\n8.2% relative word error rate reductions (WERRs) for non -streaming transducer\non test-clean and test-other and 4.0% and 4.9% for streaming transducer. The\nimplementation of this work is already released as a part of the open-source\nproject icefall.", "published": "2022-10-31 07:03:17", "link": "http://arxiv.org/abs/2211.00508v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Revisiting Attention Weights as Explanations from an Information\n  Theoretic Perspective", "abstract": "Attention mechanisms have recently demonstrated impressive performance on a\nrange of NLP tasks, and attention scores are often used as a proxy for model\nexplainability. However, there is a debate on whether attention weights can, in\nfact, be used to identify the most important inputs to a model. We approach\nthis question from an information theoretic perspective by measuring the mutual\ninformation between the model output and the hidden states. From extensive\nexperiments, we draw the following conclusions: (i) Additive and Deep attention\nmechanisms are likely to be better at preserving the information between the\nhidden states and the model output (compared to Scaled Dot-product); (ii)\nablation studies indicate that Additive attention can actively learn to explain\nthe importance of its input hidden representations; (iii) when attention values\nare nearly the same, the rank order of attention values is not consistent with\nthe rank order of the mutual information(iv) Using Gumbel-Softmax with a\ntemperature lower than one, tends to produce a more skewed attention score\ndistribution compared to softmax and hence is a better choice for explainable\ndesign; (v) some building blocks are better at preserving the correlation\nbetween the ordered list of mutual information and attention weights order (for\ne.g., the combination of BiLSTM encoder and Additive attention). Our findings\nindicate that attention mechanisms do have the potential to function as a\nshortcut to model explanations when they are carefully combined with other\nmodel elements.", "published": "2022-10-31 12:53:20", "link": "http://arxiv.org/abs/2211.07714v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multilingual Speech Emotion Recognition With Multi-Gating Mechanism and\n  Neural Architecture Search", "abstract": "Speech emotion recognition (SER) classifies audio into emotion categories\nsuch as Happy, Angry, Fear, Disgust and Neutral. While Speech Emotion\nRecognition (SER) is a common application for popular languages, it continues\nto be a problem for low-resourced languages, i.e., languages with no pretrained\nspeech-to-text recognition models. This paper firstly proposes a\nlanguage-specific model that extract emotional information from multiple\npre-trained speech models, and then designs a multi-domain model that\nsimultaneously performs SER for various languages. Our multidomain model\nemploys a multi-gating mechanism to generate unique weighted feature\ncombination for each language, and also searches for specific neural network\nstructure for each language through a neural architecture search module. In\naddition, we introduce a contrastive auxiliary loss to build more separable\nrepresentations for audio data. Our experiments show that our model raises the\nstate-of-the-art accuracy by 3% for German and 14.3% for French.", "published": "2022-10-31 19:55:33", "link": "http://arxiv.org/abs/2211.08237v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Design Considerations For Hypothesis Rejection Modules In Spoken\n  Language Understanding Systems", "abstract": "Spoken Language Understanding (SLU) systems typically consist of a set of\nmachine learning models that operate in conjunction to produce an SLU\nhypothesis. The generated hypothesis is then sent to downstream components for\nfurther action. However, it is desirable to discard an incorrect hypothesis\nbefore sending it downstream. In this work, we present two designs for SLU\nhypothesis rejection modules: (i) scheme R1 that performs rejection on domain\nspecific SLU hypothesis and, (ii) scheme R2 that performs rejection on\nhypothesis generated from the overall SLU system. Hypothesis rejection modules\nin both schemes reject/accept a hypothesis based on features drawn from the\nutterance directed to the SLU system, the associated SLU hypothesis and SLU\nconfidence score. Our experiments suggest that both the schemes yield similar\nresults (scheme R1: 2.5% FRR @ 4.5% FAR, scheme R2: 2.5% FRR @ 4.6% FAR), with\nthe best performing systems using all the available features. We argue that\nwhile either of the rejection schemes can be chosen over the other, they carry\nsome inherent differences which need to be considered while making this choice.\nAdditionally, we incorporate ASR features in the rejection module (obtaining an\n1.9% FRR @ 3.8% FAR) and analyze the improvements.", "published": "2022-10-31 21:16:23", "link": "http://arxiv.org/abs/2211.09711v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Modular Hybrid Autoregressive Transducer", "abstract": "Text-only adaptation of a transducer model remains challenging for end-to-end\nspeech recognition since the transducer has no clearly separated acoustic model\n(AM), language model (LM) or blank model. In this work, we propose a modular\nhybrid autoregressive transducer (MHAT) that has structurally separated label\nand blank decoders to predict label and blank distributions, respectively,\nalong with a shared acoustic encoder. The encoder and label decoder outputs are\ndirectly projected to AM and internal LM scores and then added to compute label\nposteriors. We train MHAT with an internal LM loss and a HAT loss to ensure\nthat its internal LM becomes a standalone neural LM that can be effectively\nadapted to text. Moreover, text adaptation of MHAT fosters a much better LM\nfusion than internal LM subtraction-based methods. On Google's large-scale\nproduction data, a multi-domain MHAT adapted with 100B sentences achieves\nrelative WER reductions of up to 12.4% without LM fusion and 21.5% with LM\nfusion from 400K-hour trained HAT.", "published": "2022-10-31 03:56:37", "link": "http://arxiv.org/abs/2210.17049v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Wespeaker: A Research and Production oriented Speaker Embedding Learning\n  Toolkit", "abstract": "Speaker modeling is essential for many related tasks, such as speaker\nrecognition and speaker diarization. The dominant modeling approach is\nfixed-dimensional vector representation, i.e., speaker embedding. This paper\nintroduces a research and production oriented speaker embedding learning\ntoolkit, Wespeaker. Wespeaker contains the implementation of scalable data\nmanagement, state-of-the-art speaker embedding models, loss functions, and\nscoring back-ends, with highly competitive results achieved by structured\nrecipes which were adopted in the winning systems in several speaker\nverification challenges. The application to other downstream tasks such as\nspeaker diarization is also exhibited in the related recipe. Moreover, CPU- and\nGPU-compatible deployment codes are integrated for production-oriented\ndevelopment. The toolkit is publicly available at\nhttps://github.com/wenet-e2e/wespeaker.", "published": "2022-10-31 02:11:58", "link": "http://arxiv.org/abs/2210.17016v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio Time-Scale Modification with Temporal Compressing Networks", "abstract": "We propose a novel approach for time-scale modification of audio signals.\nUnlike traditional methods that rely on the framing technique or the short-time\nFourier transform to preserve the frequency during temporal stretching, our\nneural network model encodes the raw audio into a high-level latent\nrepresentation, dubbed Neuralgram, where each vector represents 1024 audio\nsample points. Due to a sufficient compression ratio, we are able to apply\narbitrary spatial interpolation of the Neuralgram to perform temporal\nstretching. Finally, a learned neural decoder synthesizes the time-scaled audio\nsamples based on the stretched Neuralgram representation. Both the encoder and\ndecoder are trained with latent regression losses and adversarial losses in\norder to obtain high-fidelity audio samples. Despite its simplicity, our method\nhas comparable performance compared to the existing baselines and opens a new\npossibility in research into modern time-scale modification. Audio samples can\nbe found at https://tsmnet-mmasia23.github.io", "published": "2022-10-31 09:04:33", "link": "http://arxiv.org/abs/2210.17152v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Towards Developing State-of-the-Art TTS Synthesisers for 13 Indian\n  Languages with Signal Processing aided Alignments", "abstract": "End-to-end (E2E) systems synthesise high-quality speech, but this typically\nrequires a large amount of data. As E2E synthesis progressed from Tacotron to\nFastSpeech2, it became evident that features representing prosody, particularly\nsub-word durations, are important for error-free synthesis. Variants of\nFastSpeech use a teacher model or forced alignments for training. This paper\nuses signal processing cues in tandem with forced alignment to produce accurate\nphone boundaries for the training data. As a result of better duration\nmodelling, good-quality synthesisers are developed. Evaluations indicate that\nsystems developed using the proposed signal processing-aided approach are\nbetter than systems developed using other alignment approaches, especially in\nlow-resource scenarios. Our systems also outperform the existing best TTS\nsystems available for 13 Indian languages.", "published": "2022-10-31 09:05:51", "link": "http://arxiv.org/abs/2210.17153v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DiaCorrect: End-to-end error correction for speaker diarization", "abstract": "In recent years, speaker diarization has attracted widespread attention. To\nachieve better performance, some studies propose to diarize speech in multiple\nstages. Although these methods might bring additional benefits, most of them\nare quite complex. Motivated by spelling correction in automatic speech\nrecognition (ASR), in this paper, we propose an end-to-end error correction\nframework, termed DiaCorrect, to refine the initial diarization results in a\nsimple but efficient way. By exploiting the acoustic interactions between input\nmixture and its corresponding speaker activity, DiaCorrect could automatically\nadapt the initial speaker activity to minimize the diarization errors. Without\nbells and whistles, experiments on LibriSpeech based 2-speaker meeting-like\ndata show that, the self-attentitive end-to-end neural diarization (SA-EEND)\nbaseline with DiaCorrect could reduce its diarization error rate (DER) by over\n62.4% from 12.31% to 4.63%. Our source code is available online at\nhttps://github.com/jyhan03/diacorrect.", "published": "2022-10-31 10:12:32", "link": "http://arxiv.org/abs/2210.17189v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "AccentSpeech: Learning Accent from Crowd-sourced Data for Target Speaker\n  TTS with Accents", "abstract": "Learning accent from crowd-sourced data is a feasible way to achieve a target\nspeaker TTS system that can synthesize accent speech. To this end, there are\ntwo challenging problems to be solved. First, direct use of the poor acoustic\nquality crowd-sourced data and the target speaker data in accent transfer will\napparently lead to synthetic speech with degraded quality. To mitigate this\nproblem, we take a bottleneck feature (BN) based TTS approach, in which TTS is\ndecomposed into a Text-to-BN (T2BN) module to learn accent and a BN-to-Mel\n(BN2Mel) module to learn speaker timbre, where neural network based BN feature\nserves as the intermediate representation that are robust to noise\ninterference. Second, direct training T2BN using the crowd-sourced data in the\ntwo-stage system will produce accent speech of target speaker with poor\nprosody. This is because the the crowd-sourced recordings are contributed from\nthe ordinary unprofessional speakers. To tackle this problem, we update the\ntwo-stage approach to a novel three-stage approach, where T2BN and BN2Mel are\ntrained using the high-quality target speaker data and a new BN-to-BN module is\nplugged in between the two modules to perform accent transfer. To train the\nBN2BN module, the parallel unaccented and accented BN features are obtained by\na proposed data augmentation procedure. Finally the proposed three-stage\napproach manages to produce accent speech for the target speaker with good\nprosody, as the prosody pattern is inherited from the professional target\nspeaker and accent transfer is achieved by the BN2BN module at the same time.\nThe proposed approach, named as AccentSpeech, is validated in a Mandarin TTS\naccent transfer task.", "published": "2022-10-31 13:31:17", "link": "http://arxiv.org/abs/2210.17305v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Convolution-Based Channel-Frequency Attention for Text-Independent\n  Speaker Verification", "abstract": "Deep convolutional neural networks (CNNs) have been applied to extracting\nspeaker embeddings with significant success in speaker verification.\nIncorporating the attention mechanism has shown to be effective in improving\nthe model performance. This paper presents an efficient two-dimensional\nconvolution-based attention module, namely C2D-Att. The interaction between the\nconvolution channel and frequency is involved in the attention calculation by\nlightweight convolution layers. This requires only a small number of\nparameters. Fine-grained attention weights are produced to represent channel\nand frequency-specific information. The weights are imposed on the input\nfeatures to improve the representation ability for speaker modeling. The\nC2D-Att is integrated into a modified version of ResNet for speaker embedding\nextraction. Experiments are conducted on VoxCeleb datasets. The results show\nthat C2DAtt is effective in generating discriminative attention maps and\noutperforms other attention methods. The proposed model shows robust\nperformance with different scales of model size and achieves state-of-the-art\nresults.", "published": "2022-10-31 13:34:22", "link": "http://arxiv.org/abs/2210.17310v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Model Compression for DNN-based Speaker Verification Using Weight\n  Quantization", "abstract": "DNN-based speaker verification (SV) models demonstrate significant\nperformance at relatively high computation costs. Model compression can be\napplied to reduce the model size for lower resource consumption. The present\nstudy exploits weight quantization to compress two widely-used SV models,\nnamely ECAPA-TDNN and ResNet. Experimental results on VoxCeleb show that weight\nquantization is effective for compressing SV models. The model size can be\nreduced multiple times without noticeable degradation in performance.\nCompression of ResNet shows more robust results than ECAPA-TDNN with\nlower-bitwidth quantization. Analysis of the layer weights suggests that the\nsmooth weight distribution of ResNet may be related to its better robustness.\nThe generalization ability of the quantized model is validated via a\nlanguage-mismatched SV task. Furthermore, analysis by information probing\nreveals that the quantized models can retain most of the speaker-relevant\nknowledge learned by the original models.", "published": "2022-10-31 13:46:47", "link": "http://arxiv.org/abs/2210.17326v5", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Robust MelGAN: A robust universal neural vocoder for high-fidelity TTS", "abstract": "In current two-stage neural text-to-speech (TTS) paradigm, it is ideal to\nhave a universal neural vocoder, once trained, which is robust to imperfect\nmel-spectrogram predicted from the acoustic model. To this end, we propose\nRobust MelGAN vocoder by solving the original multi-band MelGAN's metallic\nsound problem and increasing its generalization ability. Specifically, we\nintroduce a fine-grained network dropout strategy to the generator. With a\nspecifically designed over-smooth handler which separates speech signal intro\nperiodic and aperiodic components, we only perform network dropout to the\naperodic components, which alleviates metallic sounding and maintains good\nspeaker similarity. To further improve generalization ability, we introduce\nseveral data augmentation methods to augment fake data in the discriminator,\nincluding harmonic shift, harmonic noise and phase noise. Experiments show that\nRobust MelGAN can be used as a universal vocoder, significantly improving sound\nquality in TTS systems built on various types of data.", "published": "2022-10-31 14:24:10", "link": "http://arxiv.org/abs/2210.17349v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio-Visual Speech Enhancement and Separation by Utilizing Multi-Modal\n  Self-Supervised Embeddings", "abstract": "AV-HuBERT, a multi-modal self-supervised learning model, has been shown to be\neffective for categorical problems such as automatic speech recognition and\nlip-reading. This suggests that useful audio-visual speech representations can\nbe obtained via utilizing multi-modal self-supervised embeddings. Nevertheless,\nit is unclear if such representations can be generalized to solve real-world\nmulti-modal AV regression tasks, such as audio-visual speech enhancement (AVSE)\nand audio-visual speech separation (AVSS). In this study, we leveraged the\npre-trained AV-HuBERT model followed by an SE module for AVSE and AVSS.\nComparative experimental results demonstrate that our proposed model performs\nbetter than the state-of-the-art AVSE and traditional audio-only SE models. In\nsummary, our results confirm the effectiveness of our proposed model for the\nAVSS task with proper fine-tuning strategies, demonstrating that multi-modal\nself-supervised embeddings obtained from AV-HuBERT can be generalized to\naudio-visual regression tasks.", "published": "2022-10-31 16:30:10", "link": "http://arxiv.org/abs/2210.17456v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "ImagineNET: Target Speaker Extraction with Intermittent Visual Cue\n  through Embedding Inpainting", "abstract": "The speaker extraction technique seeks to single out the voice of a target\nspeaker from the interfering voices in a speech mixture. Typically an auxiliary\nreference of the target speaker is used to form voluntary attention. Either a\npre-recorded utterance or a synchronized lip movement in a video clip can serve\nas the auxiliary reference. The use of visual cue is not only feasible, but\nalso effective due to its noise robustness, and becoming popular. However, it\nis difficult to guarantee that such parallel visual cue is always available in\nreal-world applications where visual occlusion or intermittent communication\ncan occur. In this paper, we study the audio-visual speaker extraction\nalgorithms with intermittent visual cue. We propose a joint speaker extraction\nand visual embedding inpainting framework to explore the mutual benefits. To\nencourage the interaction between the two tasks, they are performed alternately\nwith an interlacing structure and optimized jointly. We also propose two types\nof visual inpainting losses and study our proposed method with two types of\npopularly used visual embeddings. The experimental results show that we\noutperform the baseline in terms of signal quality, perceptual quality, and\nintelligibility.", "published": "2022-10-31 19:29:29", "link": "http://arxiv.org/abs/2211.00109v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Active Learning of Non-semantic Speech Tasks with Pretrained Models", "abstract": "Pretraining neural networks with massive unlabeled datasets has become\npopular as it equips the deep models with a better prior to solve downstream\ntasks. However, this approach generally assumes that the downstream tasks have\naccess to annotated data of sufficient size. In this work, we propose ALOE, a\nnovel system for improving the data- and label-efficiency of non-semantic\nspeech tasks with active learning. ALOE uses pretrained models in conjunction\nwith active learning to label data incrementally and learn classifiers for\ndownstream tasks, thereby mitigating the need to acquire labeled data\nbeforehand. We demonstrate the effectiveness of ALOE on a wide range of tasks,\nuncertainty-based acquisition functions, and model architectures. Training a\nlinear classifier on top of a frozen encoder with ALOE is shown to achieve\nperformance similar to several baselines that utilize the entire labeled data.", "published": "2022-10-31 20:08:55", "link": "http://arxiv.org/abs/2211.00119v4", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Magnitude or Phase? A Two Stage Algorithm for Dereverberation", "abstract": "In this work we present a new single-microphone speech dereverberation\nalgorithm. First, a performance analysis is presented to interpret that\nalgorithms focused on improving solely magnitude or phase are not good enough.\nFurthermore, we demonstrate that few objective measurements have high\ncorrelation with the clean magnitude while others with the clean phase.\nConsequently ,we propose a new architecture which consists of two sub-models,\neach of which is responsible for a different task. The first model estimates\nthe clean magnitude given the noisy input. The enhanced magnitude together with\nthe noisy-input phase are then used as inputs to the second model to estimate\nthe real and imaginary portions of the dereverberated signal. A training scheme\nincluding pre-training and fine-tuning is presented in the paper. We evaluate\nour proposed approach using data from the REVERB challenge and compare our\nresults to other methods. We demonstrate consistent improvements in all\nmeasures, which can be attributed to the improved estimates of both the\nmagnitude and the phase.", "published": "2022-10-31 07:56:48", "link": "http://arxiv.org/abs/2211.00607v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Structured State Space Decoder for Speech Recognition and Synthesis", "abstract": "Automatic speech recognition (ASR) systems developed in recent years have\nshown promising results with self-attention models (e.g., Transformer and\nConformer), which are replacing conventional recurrent neural networks.\nMeanwhile, a structured state space model (S4) has been recently proposed,\nproducing promising results for various long-sequence modeling tasks, including\nraw speech classification. The S4 model can be trained in parallel, same as the\nTransformer model. In this study, we applied S4 as a decoder for ASR and\ntext-to-speech (TTS) tasks by comparing it with the Transformer decoder. For\nthe ASR task, our experimental results demonstrate that the proposed model\nachieves a competitive word error rate (WER) of 1.88%/4.25% on LibriSpeech\ntest-clean/test-other set and a character error rate (CER) of 3.80%/2.63%/2.98%\non the CSJ eval1/eval2/eval3 set. Furthermore, the proposed model is more\nrobust than the standard Transformer model, particularly for long-form speech\non both the datasets. For the TTS task, the proposed method outperforms the\nTransformer baseline.", "published": "2022-10-31 06:54:23", "link": "http://arxiv.org/abs/2210.17098v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Minimum Processing Near-end Listening Enhancement", "abstract": "The intelligibility and quality of speech from a mobile phone or public\nannouncement system are often affected by background noise in the listening\nenvironment. By pre-processing the speech signal it is possible to improve the\nspeech intelligibility and quality -- this is known as near-end listening\nenhancement (NLE). Although, existing NLE techniques are able to greatly\nincrease intelligibility in harsh noise environments, in favorable noise\nconditions the intelligibility of speech reaches a ceiling where it cannot be\nfurther enhanced. Actually, the focus of existing methods solely on improving\nthe intelligibility causes unnecessary processing of the speech signal and\nleads to speech distortions and quality degradations. In this paper, we provide\na new rationale for NLE, where the target speech is minimally processed in\nterms of a processing penalty, provided that a certain performance constraint,\ne.g., intelligibility, is satisfied. We present a closed-form solution for the\ncase where the performance criterion is an intelligibility estimator based on\nthe approximated speech intelligibility index and the processing penalty is the\nmean-square error between the processed and the clean speech. This produces an\nNLE method that adapts to changing noise conditions via a simple gain rule by\nlimiting the processing to the minimum necessary to achieve a desired\nintelligibility, while at the same time focusing on quality in favorable noise\nsituations by minimizing the amount of speech distortions. Through simulation\nstudies, we show the proposed method attains speech quality on par or better\nthan existing methods in both objective measurements and subjective listening\ntests, whilst still sustaining objective speech intelligibility performance on\npar with existing methods.", "published": "2022-10-31 09:08:00", "link": "http://arxiv.org/abs/2210.17154v2", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Self-Supervised Hierarchical Metrical Structure Modeling", "abstract": "We propose a novel method to model hierarchical metrical structures for both\nsymbolic music and audio signals in a self-supervised manner with minimal\ndomain knowledge. The model trains and inferences on beat-aligned music signals\nand predicts an 8-layer hierarchical metrical tree from beat, measure to the\nsection level. The training procedure does not require any hierarchical\nmetrical labeling except for beats, purely relying on the nature of metrical\nregularity and inter-voice consistency as inductive biases. We show in\nexperiments that the method achieves comparable performance with supervised\nbaselines on multiple metrical structure analysis tasks on both symbolic music\nand audio signals. All demos, source code and pre-trained models are publicly\navailable on GitHub.", "published": "2022-10-31 10:05:19", "link": "http://arxiv.org/abs/2210.17183v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Combining Automatic Speaker Verification and Prosody Analysis for\n  Synthetic Speech Detection", "abstract": "The rapid spread of media content synthesis technology and the potentially\ndamaging impact of audio and video deepfakes on people's lives have raised the\nneed to implement systems able to detect these forgeries automatically. In this\nwork we present a novel approach for synthetic speech detection, exploiting the\ncombination of two high-level semantic properties of the human voice. On one\nside, we focus on speaker identity cues and represent them as speaker\nembeddings extracted using a state-of-the-art method for the automatic speaker\nverification task. On the other side, voice prosody, intended as variations in\nrhythm, pitch or accent in speech, is extracted through a specialized encoder.\nWe show that the combination of these two embeddings fed to a supervised binary\nclassifier allows the detection of deepfake speech generated with both\nText-to-Speech and Voice Conversion techniques. Our results show improvements\nover the considered baselines, good generalization properties over multiple\ndatasets and robustness to audio compression.", "published": "2022-10-31 11:03:03", "link": "http://arxiv.org/abs/2210.17222v1", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Diffusion-based Generative Speech Source Separation", "abstract": "We propose DiffSep, a new single channel source separation method based on\nscore-matching of a stochastic differential equation (SDE). We craft a tailored\ncontinuous time diffusion-mixing process starting from the separated sources\nand converging to a Gaussian distribution centered on their mixture. This\nformulation lets us apply the machinery of score-based generative modelling.\nFirst, we train a neural network to approximate the score function of the\nmarginal probabilities or the diffusion-mixing process. Then, we use it to\nsolve the reverse time SDE that progressively separates the sources starting\nfrom their mixture. We propose a modified training strategy to handle model\nmismatch and source permutation ambiguity. Experiments on the WSJ0 2mix dataset\ndemonstrate the potential of the method. Furthermore, the method is also\nsuitable for speech enhancement and shows performance competitive with prior\nwork on the VoiceBank-DEMAND dataset.", "published": "2022-10-31 13:46:55", "link": "http://arxiv.org/abs/2210.17327v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "VoicePrivacy 2022 System Description: Speaker Anonymization with\n  Feature-matched F0 Trajectories", "abstract": "We introduce a novel method to improve the performance of the VoicePrivacy\nChallenge 2022 baseline B1 variants. Among the known deficiencies of\nx-vector-based anonymization systems is the insufficient disentangling of the\ninput features. In particular, the fundamental frequency (F0) trajectories,\nwhich are used for voice synthesis without any modifications. Especially in\ncross-gender conversion, this situation causes unnatural sounding voices,\nincreases word error rates (WERs), and personal information leakage. Our\nsubmission overcomes this problem by synthesizing an F0 trajectory, which\nbetter harmonizes with the anonymized x-vector. We utilized a low-complexity\ndeep neural network to estimate an appropriate F0 value per frame, using the\nlinguistic content from the bottleneck features (BN) and the anonymized\nx-vector. Our approach results in a significantly improved anonymization system\nand increased naturalness of the synthesized voice. Consequently, our results\nsuggest that F0 extraction is not required for voice anonymization.", "published": "2022-10-31 13:59:27", "link": "http://arxiv.org/abs/2210.17338v1", "categories": ["eess.AS", "cs.CR", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Analysis and Detection of Singing Techniques in Repertoires of J-POP\n  Solo Singers", "abstract": "In this paper, we focus on singing techniques within the scope of music\ninformation retrieval research. We investigate how singers use singing\ntechniques using real-world recordings of famous solo singers in Japanese\npopular music songs (J-POP). First, we built a new dataset of singing\ntechniques. The dataset consists of 168 commercial J-POP songs, and each song\nis annotated using various singing techniques with timestamps and vocal pitch\ncontours. We also present descriptive statistics of singing techniques on the\ndataset to clarify what and how often singing techniques appear. We further\nexplored the difficulty of the automatic detection of singing techniques using\npreviously proposed machine learning techniques. In the detection, we also\ninvestigate the effectiveness of auxiliary information (i.e., pitch and\ndistribution of label duration), not only providing the baseline. The best\nresult achieves 40.4% at macro-average F-measure on nine-way multi-class\ndetection. We provide the annotation of the dataset and its detail on the\nappendix website 0 .", "published": "2022-10-31 14:45:01", "link": "http://arxiv.org/abs/2210.17367v2", "categories": ["cs.SD", "cs.DL", "cs.IR", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
