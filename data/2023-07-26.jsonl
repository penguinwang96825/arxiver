{"title": "GrammarGPT: Exploring Open-Source LLMs for Native Chinese Grammatical\n  Error Correction with Supervised Fine-Tuning", "abstract": "Grammatical error correction aims to correct ungrammatical sentences\nautomatically. Recently, some work has demonstrated the excellent capabilities\nof closed-source Large Language Models (LLMs, e.g., ChatGPT) in grammatical\nerror correction. However, the potential of open-source LLMs remains\nunexplored. In this paper, we introduced GrammarGPT, an open-source LLM, to\npreliminary explore its potential for native Chinese grammatical error\ncorrection. The core recipe of GrammarGPT is to leverage the hybrid dataset of\nChatGPT-generated and human-annotated. For grammatical errors with clues, we\nproposed a heuristic method to guide ChatGPT to generate ungrammatical\nsentences by providing those clues. For grammatical errors without clues, we\ncollected ungrammatical sentences from publicly available websites and manually\ncorrected them. In addition, we employed an error-invariant augmentation method\nto enhance the ability of the model to correct native Chinese grammatical\nerrors. We ultimately constructed about 1k parallel data and utilized these\ndata to fine-tune open-source LLMs (e.g., Phoenix, released by The Chinese\nUniversity of Hong Kong, Shenzhen) with instruction tuning. The experimental\nresults show that GrammarGPT outperforms the existing SOTA system\nsignificantly. Although model parameters are 20x larger than the SOTA baseline,\nthe required amount of data for instruction tuning is 1200x smaller,\nillustrating the potential of open-source LLMs on native CGEC. Our GrammarGPT\nranks $3^{rd}$ on NLPCC2023 SharedTask1, demonstrating our approach's\neffectiveness. The code and data are available at\n\\url{https://github.com/FreedomIntelligence/GrammarGPT}.", "published": "2023-07-26 02:45:38", "link": "http://arxiv.org/abs/2307.13923v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Affective Natural Language Generation of Event Descriptions through\n  Fine-grained Appraisal Conditions", "abstract": "Models for affective text generation have shown a remarkable progress, but\nthey commonly rely only on basic emotion theories or valance/arousal values as\nconditions. This is appropriate when the goal is to create explicit emotion\nstatements (\"The kid is happy.\"). Emotions are, however, commonly communicated\nimplicitly. For instance, the emotional interpretation of an event (\"Their dog\ndied.\") does often not require an explicit emotion statement. In psychology,\nappraisal theories explain the link between a cognitive evaluation of an event\nand the potentially developed emotion. They put the assessment of the situation\non the spot, for instance regarding the own control or the responsibility for\nwhat happens. We hypothesize and subsequently show that including appraisal\nvariables as conditions in a generation framework comes with two advantages.\n(1) The generation model is informed in greater detail about what makes a\nspecific emotion and what properties it has. This leads to text generation that\nbetter fulfills the condition. (2) The variables of appraisal allow a user to\nperform a more fine-grained control of the generated text, by stating\nproperties of a situation instead of only providing the emotion category. Our\nBart and T5-based experiments with 7 emotions (Anger, Disgust, Fear, Guilt,\nJoy, Sadness, Shame), and 7 appraisals (Attention, Responsibility, Control,\nCircumstance, Pleasantness, Effort, Certainty) show that (1) adding appraisals\nduring training improves the accurateness of the generated texts by 10 pp in\nF1. Further, (2) the texts with appraisal variables are longer and contain more\ndetails. This exemplifies the greater control for users.", "published": "2023-07-26 07:34:19", "link": "http://arxiv.org/abs/2307.14004v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi3WOZ: A Multilingual, Multi-Domain, Multi-Parallel Dataset for\n  Training and Evaluating Culturally Adapted Task-Oriented Dialog Systems", "abstract": "Creating high-quality annotated data for task-oriented dialog (ToD) is known\nto be notoriously difficult, and the challenges are amplified when the goal is\nto create equitable, culturally adapted, and large-scale ToD datasets for\nmultiple languages. Therefore, the current datasets are still very scarce and\nsuffer from limitations such as translation-based non-native dialogs with\ntranslation artefacts, small scale, or lack of cultural adaptation, among\nothers. In this work, we first take stock of the current landscape of\nmultilingual ToD datasets, offering a systematic overview of their properties\nand limitations. Aiming to reduce all the detected limitations, we then\nintroduce Multi3WOZ, a novel multilingual, multi-domain, multi-parallel ToD\ndataset. It is large-scale and offers culturally adapted dialogs in 4 languages\nto enable training and evaluation of multilingual and cross-lingual ToD\nsystems. We describe a complex bottom-up data collection process that yielded\nthe final dataset, and offer the first sets of baseline scores across different\nToD-related tasks for future reference, also highlighting its challenging\nnature.", "published": "2023-07-26 08:29:42", "link": "http://arxiv.org/abs/2307.14031v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Implicit Feedback from Deployment Data in Dialogue", "abstract": "We study improving social conversational agents by learning from natural\ndialogue between users and a deployed model, without extra annotations. To\nimplicitly measure the quality of a machine-generated utterance, we leverage\nsignals like user response length, sentiment and reaction of the future human\nutterances in the collected dialogue episodes. Our experiments use the publicly\nreleased deployment data from BlenderBot (Xu et al., 2023). Human evaluation\nindicates improvements in our new models over baseline responses; however, we\nfind that some proxy signals can lead to more generations with undesirable\nproperties as well. For example, optimizing for conversation length can lead to\nmore controversial or unfriendly generations compared to the baseline, whereas\noptimizing for positive sentiment or reaction can decrease these behaviors.", "published": "2023-07-26 11:34:53", "link": "http://arxiv.org/abs/2307.14117v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Founding a mathematical diffusion model in linguistics. The case study\n  of German syntactic features in the North-Eastern Italian dialects", "abstract": "The initial motivation for this work was the linguistic case of the spread of\nGermanic syntactic features into Romance dialects of North-Eastern Italy, which\noccurred after the immigration of German people to Tyrol during the High Middle\nAges. To obtain a representation of the data over the territory suitable for a\nmathematical formulation, an interactive map is produced as a first step, using\ntools of what is called Geographic Data Science. A smooth two-dimensional\nsurface G is introduced, expressing locally which fraction of territory uses a\ngiven German language feature: it is obtained by a piecewise cubic curvature\nminimizing interpolant of the discrete function that says if at any surveyed\nlocality that feature is used or not. This surface G is thought of as the value\nat the present time of a function describing a diffusion-convection phenomenon\nin two dimensions (here said tidal mode), which is subjected in a very natural\nway to the same equation used in physics, introducing a contextual diffusivity\nconcept: it is shown that with two different assumptions about diffusivity,\nsolutions of this equation, evaluated at the present time, fit well with the\ndata interpolated by G, thus providing two convincing different pictures of\ndiffusion-convection in the case under study, albeit simplifications and\napproximations. Very importantly, it is shown that the linguistic diffusion\nmodel known to linguists as Schmidt waves can be counted among the solutions of\nthe diffusion equation", "published": "2023-07-26 16:49:11", "link": "http://arxiv.org/abs/2307.14291v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatically Evaluating Opinion Prevalence in Opinion Summarization", "abstract": "When faced with a large number of product reviews, it is not clear that a\nhuman can remember all of them and weight opinions representatively to write a\ngood reference summary. We propose an automatic metric to test the prevalence\nof the opinions that a summary expresses, based on counting the number of\nreviews that are consistent with each statement in the summary, while\ndiscrediting trivial or redundant statements. To formulate this opinion\nprevalence metric, we consider several existing methods to score the factual\nconsistency of a summary statement with respect to each individual source\nreview. On a corpus of Amazon product reviews, we gather multiple human\njudgments of the opinion consistency, to determine which automatic metric best\nexpresses consistency in product reviews. Using the resulting opinion\nprevalence metric, we show that a human authored summary has only slightly\nbetter opinion prevalence than randomly selected extracts from the source\nreviews, and previous extractive and abstractive unsupervised opinion\nsummarization methods perform worse than humans. We demonstrate room for\nimprovement with a greedy construction of extractive summaries with twice the\nopinion prevalence achieved by humans. Finally, we show that preprocessing\nsource reviews by simplification can raise the opinion prevalence achieved by\nexisting abstractive opinion summarization systems to the level of human\nperformance.", "published": "2023-07-26 17:13:00", "link": "http://arxiv.org/abs/2307.14305v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparative Analysis of Libraries for the Sentimental Analysis", "abstract": "This study is main goal is to provide a comparative comparison of libraries\nusing machine learning methods. Experts in natural language processing (NLP)\nare becoming more and more interested in sentiment analysis (SA) of text\nchanges. The objective of employing NLP text analysis techniques is to\nrecognize and categorize feelings related to twitter users utterances. In this\nexamination, issues with SA and the libraries utilized are also looked at.\nprovides a number of cooperative methods to classify emotional polarity. The\nNaive Bayes Classifier, Decision Tree Classifier, Maxent Classifier, Sklearn\nClassifier, Sklearn Classifier MultinomialNB, and other conjoint learning\nalgorithms, according to recent research, are very effective. In the project\nwill use Five Python and R libraries NLTK, TextBlob, Vader, Transformers (GPT\nand BERT pretrained), and Tidytext will be used in the study to apply sentiment\nanalysis techniques. Four machine learning models Tree of Decisions (DT),\nSupport Vector Machine (SVM), Naive Bayes (NB), and K-Nearest Neighbor (KNN)\nwill also be used. To evaluate how well libraries for SA operate in the social\nnetwork environment, comparative study was also carried out. The measures to\nassess the best algorithms in this experiment, which used a single data set for\neach method, were precision, recall, and F1 score. We conclude that the BERT\ntransformer method with an Accuracy: 0.973 is recommended for sentiment\nanalysis.", "published": "2023-07-26 17:21:53", "link": "http://arxiv.org/abs/2307.14311v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mental-LLM: Leveraging Large Language Models for Mental Health\n  Prediction via Online Text Data", "abstract": "Advances in large language models (LLMs) have empowered a variety of\napplications. However, there is still a significant gap in research when it\ncomes to understanding and enhancing the capabilities of LLMs in the field of\nmental health. In this work, we present a comprehensive evaluation of multiple\nLLMs on various mental health prediction tasks via online text data, including\nAlpaca, Alpaca-LoRA, FLAN-T5, GPT-3.5, and GPT-4. We conduct a broad range of\nexperiments, covering zero-shot prompting, few-shot prompting, and instruction\nfine-tuning. The results indicate a promising yet limited performance of LLMs\nwith zero-shot and few-shot prompt designs for mental health tasks. More\nimportantly, our experiments show that instruction finetuning can significantly\nboost the performance of LLMs for all tasks simultaneously. Our best-finetuned\nmodels, Mental-Alpaca and Mental-FLAN-T5, outperform the best prompt design of\nGPT-3.5 (25 and 15 times bigger) by 10.9% on balanced accuracy and the best of\nGPT-4 (250 and 150 times bigger) by 4.8%. They further perform on par with the\nstate-of-the-art task-specific language model. We also conduct an exploratory\ncase study on LLMs' capability on mental health reasoning tasks, illustrating\nthe promising capability of certain models such as GPT-4. We summarize our\nfindings into a set of action guidelines for potential methods to enhance LLMs'\ncapability for mental health tasks. Meanwhile, we also emphasize the important\nlimitations before achieving deployability in real-world mental health\nsettings, such as known racial and gender bias. We highlight the important\nethical risks accompanying this line of research.", "published": "2023-07-26 06:00:50", "link": "http://arxiv.org/abs/2307.14385v4", "categories": ["cs.CL", "68U35", "H.5.2; I.2.m"], "primary_category": "cs.CL"}
{"title": "Controllable Generation of Dialogue Acts for Dialogue Systems via\n  Few-Shot Response Generation and Ranking", "abstract": "Dialogue systems need to produce responses that realize multiple types of\ndialogue acts (DAs) with high semantic fidelity. In the past, natural language\ngenerators (NLGs) for dialogue were trained on large parallel corpora that map\nfrom a domain-specific DA and its semantic attributes to an output utterance.\nRecent work shows that pretrained language models (LLMs) offer new\npossibilities for controllable NLG using prompt-based learning. Here we develop\na novel few-shot overgenerate-and-rank approach that achieves the controlled\ngeneration of DAs. We compare eight few-shot prompt styles that include a novel\nmethod of generating from textual pseudo-references using a textual style\ntransfer approach. We develop six automatic ranking functions that identify\noutputs with both the correct DA and high semantic accuracy at generation time.\nWe test our approach on three domains and four LLMs. To our knowledge, this is\nthe first work on NLG for dialogue that automatically ranks outputs using both\nDA and attribute accuracy. For completeness, we compare our results to\nfine-tuned few-shot models trained with 5 to 100 instances per DA. Our results\nshow that several prompt settings achieve perfect DA accuracy, and near perfect\nsemantic accuracy (99.81%) and perform better than few-shot fine-tuning.", "published": "2023-07-26 18:16:45", "link": "http://arxiv.org/abs/2307.14440v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CliniDigest: A Case Study in Large Language Model Based Large-Scale\n  Summarization of Clinical Trial Descriptions", "abstract": "A clinical trial is a study that evaluates new biomedical interventions. To\ndesign new trials, researchers draw inspiration from those current and\ncompleted. In 2022, there were on average more than 100 clinical trials\nsubmitted to ClinicalTrials.gov every day, with each trial having a mean of\napproximately 1500 words [1]. This makes it nearly impossible to keep up to\ndate. To mitigate this issue, we have created a batch clinical trial summarizer\ncalled CliniDigest using GPT-3.5. CliniDigest is, to our knowledge, the first\ntool able to provide real-time, truthful, and comprehensive summaries of\nclinical trials. CliniDigest can reduce up to 85 clinical trial descriptions\n(approximately 10,500 words) into a concise 200-word summary with references\nand limited hallucinations. We have tested CliniDigest on its ability to\nsummarize 457 trials divided across 27 medical subdomains. For each field,\nCliniDigest generates summaries of $\\mu=153,\\ \\sigma=69 $ words, each of which\nutilizes $\\mu=54\\%,\\ \\sigma=30\\% $ of the sources. A more comprehensive\nevaluation is planned and outlined in this paper.", "published": "2023-07-26 21:49:14", "link": "http://arxiv.org/abs/2307.14522v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Sentence is Worth a Thousand Pictures: Can Large Language Models\n  Understand Hum4n L4ngu4ge and the W0rld behind W0rds?", "abstract": "Modern Artificial Intelligence applications show great potential for\nlanguage-related tasks that rely on next-word prediction. The current\ngeneration of Large Language Models (LLMs) have been linked to claims about\nhuman-like linguistic performance and their applications are hailed both as a\nstep towards artificial general intelligence and as a major advance in\nunderstanding the cognitive, and even neural basis of human language. To assess\nthese claims, first we analyze the contribution of LLMs as theoretically\ninformative representations of a target cognitive system vs. atheoretical\nmechanistic tools. Second, we evaluate the models' ability to see the bigger\npicture, through top-down feedback from higher levels of processing, which\nrequires grounding in previous expectations and past world experience. We\nhypothesize that since models lack grounded cognition, they cannot take\nadvantage of these features and instead solely rely on fixed associations\nbetween represented words and word vectors. To assess this, we designed and ran\na novel 'leet task' (l33t t4sk), which requires decoding sentences in which\nletters are systematically replaced by numbers. The results suggest that humans\nexcel in this task whereas models struggle, confirming our hypothesis. We\ninterpret the results by identifying the key abilities that are still missing\nfrom the current state of development of these models, which require solutions\nthat go beyond increased system scaling.", "published": "2023-07-26 18:58:53", "link": "http://arxiv.org/abs/2308.00109v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FinTree: Financial Dataset Pretrain Transformer Encoder for Relation\n  Extraction", "abstract": "We present FinTree, Financial Dataset Pretrain Transformer Encoder for\nRelation Extraction. Utilizing an encoder language model, we further pretrain\nFinTree on the financial dataset, adapting the model in financial domain tasks.\nFinTree stands out with its novel structure that predicts a masked token\ninstead of the conventional [CLS] token, inspired by the Pattern Exploiting\nTraining methodology. This structure allows for more accurate relation\npredictions between two given entities. The model is trained with a unique\ninput pattern to provide contextual and positional information about the\nentities of interest, and a post-processing step ensures accurate predictions\nin line with the entity types. Our experiments demonstrate that FinTree\noutperforms on the REFinD, a large-scale financial relation extraction dataset.\nThe code and pretrained models are available at\nhttps://github.com/HJ-Ok/FinTree.", "published": "2023-07-26 01:48:52", "link": "http://arxiv.org/abs/2307.13900v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "How Does Diffusion Influence Pretrained Language Models on\n  Out-of-Distribution Data?", "abstract": "Transformer-based pretrained language models (PLMs) have achieved great\nsuccess in modern NLP. An important advantage of PLMs is good\nout-of-distribution (OOD) robustness. Recently, diffusion models have attracted\na lot of work to apply diffusion to PLMs. It remains under-explored how\ndiffusion influences PLMs on OOD data. The core of diffusion models is a\nforward diffusion process which gradually applies Gaussian noise to inputs, and\na reverse denoising process which removes noise. The noised input\nreconstruction is a fundamental ability of diffusion models. We directly\nanalyze OOD robustness by measuring the reconstruction loss, including testing\nthe abilities to reconstruct OOD data, and to detect OOD samples. Experiments\nare conducted by analyzing different training parameters and data statistical\nfeatures on eight datasets. It shows that finetuning PLMs with diffusion\ndegrades the reconstruction ability on OOD data. The comparison also shows that\ndiffusion models can effectively detect OOD samples, achieving state-of-the-art\nperformance in most of the datasets with an absolute accuracy improvement up to\n18%. These results indicate that diffusion reduces OOD robustness of PLMs.", "published": "2023-07-26 04:03:25", "link": "http://arxiv.org/abs/2307.13949v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "This is not correct! Negation-aware Evaluation of Language Generation\n  Systems", "abstract": "Large language models underestimate the impact of negations on how much they\nchange the meaning of a sentence. Therefore, learned evaluation metrics based\non these models are insensitive to negations. In this paper, we propose\nNegBLEURT, a negation-aware version of the BLEURT evaluation metric. For that,\nwe designed a rule-based sentence negation tool and used it to create the\nCANNOT negation evaluation dataset. Based on this dataset, we fine-tuned a\nsentence transformer and an evaluation metric to improve their negation\nsensitivity. Evaluating these models on existing benchmarks shows that our\nfine-tuned models outperform existing metrics on the negated sentences by far\nwhile preserving their base models' performances on other perturbations.", "published": "2023-07-26 06:54:31", "link": "http://arxiv.org/abs/2307.13989v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unsupervised extraction of local and global keywords from a single text", "abstract": "We propose an unsupervised, corpus-independent method to extract keywords\nfrom a single text. It is based on the spatial distribution of words and the\nresponse of this distribution to a random permutation of words. As compared to\nexisting methods (such as e.g. YAKE) our method has three advantages. First, it\nis significantly more effective at extracting keywords from long texts. Second,\nit allows inference of two types of keywords: local and global. Third, it\nuncovers basic themes in texts. Additionally, our method is\nlanguage-independent and applies to short texts. The results are obtained via\nhuman annotators with previous knowledge of texts from our database of\nclassical literary works (the agreement between annotators is from moderate to\nsubstantial). Our results are supported via human-independent arguments based\non the average length of extracted content words and on the average number of\nnouns in extracted words. We discuss relations of keywords with higher-order\ntextual features and reveal a connection between keywords and chapter\ndivisions.", "published": "2023-07-26 07:36:25", "link": "http://arxiv.org/abs/2307.14005v2", "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.CL"}
{"title": "Decoding ChatGPT: A Taxonomy of Existing Research, Current Challenges,\n  and Possible Future Directions", "abstract": "Chat Generative Pre-trained Transformer (ChatGPT) has gained significant\ninterest and attention since its launch in November 2022. It has shown\nimpressive performance in various domains, including passing exams and creative\nwriting. However, challenges and concerns related to biases and trust persist.\nIn this work, we present a comprehensive review of over 100 Scopus-indexed\npublications on ChatGPT, aiming to provide a taxonomy of ChatGPT research and\nexplore its applications. We critically analyze the existing literature,\nidentifying common approaches employed in the studies. Additionally, we\ninvestigate diverse application areas where ChatGPT has found utility, such as\nhealthcare, marketing and financial services, software engineering, academic\nand scientific writing, research and education, environmental science, and\nnatural language processing. Through examining these applications, we gain\nvaluable insights into the potential of ChatGPT in addressing real-world\nchallenges. We also discuss crucial issues related to ChatGPT, including biases\nand trustworthiness, emphasizing the need for further research and development\nin these areas. Furthermore, we identify potential future directions for\nChatGPT research, proposing solutions to current challenges and speculating on\nexpected advancements. By fully leveraging the capabilities of ChatGPT, we can\nunlock its potential across various domains, leading to advancements in\nconversational AI and transformative impacts in society.", "published": "2023-07-26 11:10:04", "link": "http://arxiv.org/abs/2307.14107v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Towards Generalist Biomedical AI", "abstract": "Medicine is inherently multimodal, with rich data modalities spanning text,\nimaging, genomics, and more. Generalist biomedical artificial intelligence (AI)\nsystems that flexibly encode, integrate, and interpret this data at scale can\npotentially enable impactful applications ranging from scientific discovery to\ncare delivery. To enable the development of these models, we first curate\nMultiMedBench, a new multimodal biomedical benchmark. MultiMedBench encompasses\n14 diverse tasks such as medical question answering, mammography and\ndermatology image interpretation, radiology report generation and\nsummarization, and genomic variant calling. We then introduce Med-PaLM\nMultimodal (Med-PaLM M), our proof of concept for a generalist biomedical AI\nsystem. Med-PaLM M is a large multimodal generative model that flexibly encodes\nand interprets biomedical data including clinical language, imaging, and\ngenomics with the same set of model weights. Med-PaLM M reaches performance\ncompetitive with or exceeding the state of the art on all MultiMedBench tasks,\noften surpassing specialist models by a wide margin. We also report examples of\nzero-shot generalization to novel medical concepts and tasks, positive transfer\nlearning across tasks, and emergent zero-shot medical reasoning. To further\nprobe the capabilities and limitations of Med-PaLM M, we conduct a radiologist\nevaluation of model-generated (and human) chest X-ray reports and observe\nencouraging performance across model scales. In a side-by-side ranking on 246\nretrospective chest X-rays, clinicians express a pairwise preference for\nMed-PaLM M reports over those produced by radiologists in up to 40.50% of\ncases, suggesting potential clinical utility. While considerable work is needed\nto validate these models in real-world use cases, our results represent a\nmilestone towards the development of generalist biomedical AI systems.", "published": "2023-07-26 17:52:22", "link": "http://arxiv.org/abs/2307.14334v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Skill-it! A Data-Driven Skills Framework for Understanding and Training\n  Language Models", "abstract": "The quality of training data impacts the performance of pre-trained large\nlanguage models (LMs). Given a fixed budget of tokens, we study how to best\nselect data that leads to good downstream model performance across tasks. We\ndevelop a new framework based on a simple hypothesis: just as humans acquire\ninterdependent skills in a deliberate order, language models also follow a\nnatural order when learning a set of skills from their training data. If such\nan order exists, it can be utilized for improved understanding of LMs and for\ndata-efficient training. Using this intuition, our framework formalizes the\nnotion of a skill and of an ordered set of skills in terms of the associated\ndata. First, using both synthetic and real data, we demonstrate that these\nordered skill sets exist, and that their existence enables more advanced skills\nto be learned with less data when we train on their prerequisite skills.\nSecond, using our proposed framework, we introduce an online data sampling\nalgorithm, Skill-It, over mixtures of skills for both continual pre-training\nand fine-tuning regimes, where the objective is to efficiently learn multiple\nskills in the former and an individual skill in the latter. On the LEGO\nsynthetic in the continual pre-training setting, Skill-It obtains 36.5 points\nhigher accuracy than random sampling. On the Natural Instructions dataset in\nthe fine-tuning setting, Skill-It reduces the validation loss on the target\nskill by 13.6% versus training on data associated with the target skill itself.\nWe apply our skills framework on the recent RedPajama dataset to continually\npre-train a 3B-parameter LM, achieving higher accuracy on the LM Evaluation\nHarness with 1B tokens than the baseline approach of sampling uniformly over\ndata sources with 3B tokens.", "published": "2023-07-26 18:01:49", "link": "http://arxiv.org/abs/2307.14430v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal\n  Language Models", "abstract": "We introduce new jailbreak attacks on vision language models (VLMs), which\nuse aligned LLMs and are resilient to text-only jailbreak attacks.\nSpecifically, we develop cross-modality attacks on alignment where we pair\nadversarial images going through the vision encoder with textual prompts to\nbreak the alignment of the language model. Our attacks employ a novel\ncompositional strategy that combines an image, adversarially targeted towards\ntoxic embeddings, with generic prompts to accomplish the jailbreak. Thus, the\nLLM draws the context to answer the generic prompt from the adversarial image.\nThe generation of benign-appearing adversarial images leverages a novel\nembedding-space-based methodology, operating with no access to the LLM model.\nInstead, the attacks require access only to the vision encoder and utilize one\nof our four embedding space targeting strategies. By not requiring access to\nthe LLM, the attacks lower the entry barrier for attackers, particularly when\nvision encoders such as CLIP are embedded in closed-source LLMs. The attacks\nachieve a high success rate across different VLMs, highlighting the risk of\ncross-modality alignment vulnerabilities, and the need for new alignment\napproaches for multi-modal models.", "published": "2023-07-26 23:11:15", "link": "http://arxiv.org/abs/2307.14539v2", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Speed Reading Tool Powered by Artificial Intelligence for Students with\n  ADHD, Dyslexia, or Short Attention Span", "abstract": "This paper presents a novel approach to assist students with dyslexia, ADHD,\nand short attention span in digesting any text-based information more\nefficiently. The proposed solution utilizes the Multilayer Perceptron (MLP)\nalgorithm for complex text processing and summarization tasks. The tool\nleverages the T5 (Text-to-Text Transfer Transformer) model from Hugging Face,\nwhich treats every NLP task as a text generation task. The model is fine-tuned\non specific tasks using a smaller dataset. The NLTK's Punkt Sentence Tokenizer\nis used to divide a text into a list of sentences. The application is served\nusing Flask, a lightweight web server and framework. The tool also applies\nprinciples from Bionic Reading to enhance readability, which includes a bolding\nfunction and adjustments to line, word, and character spacing. The paper\ndiscusses the methodology, implementation, and results of the AI-based speed\nreading tool.", "published": "2023-07-26 23:47:14", "link": "http://arxiv.org/abs/2307.14544v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The flow of ideas in word embeddings", "abstract": "The flow of ideas has been extensively studied by physicists, psychologists,\nand machine learning engineers. This paper adopts specific tools from\nmicrorheology to investigate the similarity-based flow of ideas. We introduce a\nrandom walker in word embeddings and study its behavior. Such\nsimilarity-mediated random walks through the embedding space show signatures of\nanomalous diffusion commonly observed in complex structured systems such as\nbiological cells and complex fluids. The paper concludes by proposing the\napplication of popular tools employed in the study of random walks and\ndiffusion of particles under Brownian motion to assess quantitatively the\nincorporation of diverse ideas in a document. Overall, this paper presents a\nself-referenced method combining microrheology and machine learning concepts to\nexplore the meandering tendencies of language models and their potential\nassociation with creativity.", "published": "2023-07-26 15:51:31", "link": "http://arxiv.org/abs/2307.16819v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Data Augmentation for Neural Machine Translation using Generative\n  Language Model", "abstract": "Despite the rapid growth in model architecture, the scarcity of large\nparallel corpora remains the main bottleneck in Neural Machine Translation.\nData augmentation is a technique that enhances the performance of data-hungry\nmodels by generating synthetic data instead of collecting new ones. We explore\nprompt-based data augmentation approaches that leverage large-scale language\nmodels such as ChatGPT. To create a synthetic parallel corpus, we compare 3\nmethods using different prompts. We employ two assessment metrics to measure\nthe diversity of the generated synthetic data. This approach requires no\nfurther model training cost, which is mandatory in other augmentation methods\nlike back-translation. The proposed method improves the unaugmented baseline by\n0.68 BLEU score.", "published": "2023-07-26 02:12:58", "link": "http://arxiv.org/abs/2307.16833v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DPBERT: Efficient Inference for BERT based on Dynamic Planning", "abstract": "Large-scale pre-trained language models such as BERT have contributed\nsignificantly to the development of NLP. However, those models require large\ncomputational resources, making it difficult to be applied to mobile devices\nwhere computing power is limited. In this paper we aim to address the weakness\nof existing input-adaptive inference methods which fail to take full advantage\nof the structure of BERT. We propose Dynamic Planning in BERT, a novel\nfine-tuning strategy that can accelerate the inference process of BERT through\nselecting a subsequence of transformer layers list of backbone as a\ncomputational path for an input sample. To do this, our approach adds a\nplanning module to the original BERT model to determine whether a layer is\nincluded or bypassed during inference. Experimental results on the GLUE\nbenchmark exhibit that our method reduces latency to 75\\% while maintaining\n98\\% accuracy, yielding a better accuracy-speed trade-off compared to\nstate-of-the-art input-adaptive methods.", "published": "2023-07-26 07:18:50", "link": "http://arxiv.org/abs/2308.00108v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Three Bricks to Consolidate Watermarks for Large Language Models", "abstract": "The task of discerning between generated and natural texts is increasingly\nchallenging. In this context, watermarking emerges as a promising technique for\nascribing generated text to a specific model. It alters the sampling generation\nprocess so as to leave an invisible trace in the generated output, facilitating\nlater detection. This research consolidates watermarks for large language\nmodels based on three theoretical and empirical considerations. First, we\nintroduce new statistical tests that offer robust theoretical guarantees which\nremain valid even at low false-positive rates (less than 10$^{\\text{-6}}$).\nSecond, we compare the effectiveness of watermarks using classical benchmarks\nin the field of natural language processing, gaining insights into their\nreal-world applicability. Third, we develop advanced detection schemes for\nscenarios where access to the LLM is available, as well as multi-bit\nwatermarking.", "published": "2023-07-26 17:56:36", "link": "http://arxiv.org/abs/2308.00113v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CIF-T: A Novel CIF-based Transducer Architecture for Automatic Speech\n  Recognition", "abstract": "RNN-T models are widely used in ASR, which rely on the RNN-T loss to achieve\nlength alignment between input audio and target sequence. However, the\nimplementation complexity and the alignment-based optimization target of RNN-T\nloss lead to computational redundancy and a reduced role for predictor network,\nrespectively. In this paper, we propose a novel model named CIF-Transducer\n(CIF-T) which incorporates the Continuous Integrate-and-Fire (CIF) mechanism\nwith the RNN-T model to achieve efficient alignment. In this way, the RNN-T\nloss is abandoned, thus bringing a computational reduction and allowing the\npredictor network a more significant role. We also introduce Funnel-CIF,\nContext Blocks, Unified Gating and Bilinear Pooling joint network, and\nauxiliary training strategy to further improve performance. Experiments on the\n178-hour AISHELL-1 and 10000-hour WenetSpeech datasets show that CIF-T achieves\nstate-of-the-art results with lower computational overhead compared to RNN-T\nmodels.", "published": "2023-07-26 11:59:14", "link": "http://arxiv.org/abs/2307.14132v4", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Developing and Evaluating Tiny to Medium-Sized Turkish BERT Models", "abstract": "This study introduces and evaluates tiny, mini, small, and medium-sized\nuncased Turkish BERT models, aiming to bridge the research gap in\nless-resourced languages. We trained these models on a diverse dataset\nencompassing over 75GB of text from multiple sources and tested them on several\ntasks, including mask prediction, sentiment analysis, news classification, and,\nzero-shot classification. Despite their smaller size, our models exhibited\nrobust performance, including zero-shot task, while ensuring computational\nefficiency and faster execution times. Our findings provide valuable insights\ninto the development and application of smaller language models, especially in\nthe context of the Turkish language.", "published": "2023-07-26 12:02:30", "link": "http://arxiv.org/abs/2307.14134v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LOIS: Looking Out of Instance Semantics for Visual Question Answering", "abstract": "Visual question answering (VQA) has been intensively studied as a multimodal\ntask that requires effort in bridging vision and language to infer answers\ncorrectly. Recent attempts have developed various attention-based modules for\nsolving VQA tasks. However, the performance of model inference is largely\nbottlenecked by visual processing for semantics understanding. Most existing\ndetection methods rely on bounding boxes, remaining a serious challenge for VQA\nmodels to understand the causal nexus of object semantics in images and\ncorrectly infer contextual information. To this end, we propose a finer model\nframework without bounding boxes in this work, termed Looking Out of Instance\nSemantics (LOIS) to tackle this important issue. LOIS enables more fine-grained\nfeature descriptions to produce visual facts. Furthermore, to overcome the\nlabel ambiguity caused by instance masks, two types of relation attention\nmodules: 1) intra-modality and 2) inter-modality, are devised to infer the\ncorrect answers from the different multi-view features. Specifically, we\nimplement a mutual relation attention module to model sophisticated and deeper\nvisual semantic relations between instance objects and background information.\nIn addition, our proposed attention model can further analyze salient image\nregions by focusing on important word-related questions. Experimental results\non four benchmark VQA datasets prove that our proposed method has favorable\nperformance in improving visual reasoning capability.", "published": "2023-07-26 12:13:00", "link": "http://arxiv.org/abs/2307.14142v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "UnScientify: Detecting Scientific Uncertainty in Scholarly Full Text", "abstract": "This demo paper presents UnScientify, an interactive system designed to\ndetect scientific uncertainty in scholarly full text. The system utilizes a\nweakly supervised technique that employs a fine-grained annotation scheme to\nidentify verbally formulated uncertainty at the sentence level in scientific\ntexts. The pipeline for the system includes a combination of pattern matching,\ncomplex sentence checking, and authorial reference checking. Our approach\nautomates labeling and annotation tasks for scientific uncertainty\nidentification, taking into account different types of scientific uncertainty,\nthat can serve various applications such as information retrieval, text mining,\nand scholarly document processing. Additionally, UnScientify provides\ninterpretable results, aiding in the comprehension of identified instances of\nscientific uncertainty in text.", "published": "2023-07-26 15:04:24", "link": "http://arxiv.org/abs/2307.14236v1", "categories": ["cs.CL", "cs.AI", "cs.DL"], "primary_category": "cs.CL"}
{"title": "ChatGPT and Persuasive Technologies for the Management and Delivery of\n  Personalized Recommendations in Hotel Hospitality", "abstract": "Recommender systems have become indispensable tools in the hotel hospitality\nindustry, enabling personalized and tailored experiences for guests. Recent\nadvancements in large language models (LLMs), such as ChatGPT, and persuasive\ntechnologies, have opened new avenues for enhancing the effectiveness of those\nsystems. This paper explores the potential of integrating ChatGPT and\npersuasive technologies for automating and improving hotel hospitality\nrecommender systems. First, we delve into the capabilities of ChatGPT, which\ncan understand and generate human-like text, enabling more accurate and\ncontext-aware recommendations. We discuss the integration of ChatGPT into\nrecommender systems, highlighting the ability to analyze user preferences,\nextract valuable insights from online reviews, and generate personalized\nrecommendations based on guest profiles. Second, we investigate the role of\npersuasive technology in influencing user behavior and enhancing the persuasive\nimpact of hotel recommendations. By incorporating persuasive techniques, such\nas social proof, scarcity and personalization, recommender systems can\neffectively influence user decision-making and encourage desired actions, such\nas booking a specific hotel or upgrading their room. To investigate the\nefficacy of ChatGPT and persuasive technologies, we present a pilot experi-ment\nwith a case study involving a hotel recommender system. We aim to study the\nimpact of integrating ChatGPT and persua-sive techniques on user engagement,\nsatisfaction, and conversion rates. The preliminary results demonstrate the\npotential of these technologies in enhancing the overall guest experience and\nbusiness performance. Overall, this paper contributes to the field of hotel\nhospitality by exploring the synergistic relationship between LLMs and\npersuasive technology in recommender systems, ultimately influencing guest\nsatisfaction and hotel revenue.", "published": "2023-07-26 16:58:10", "link": "http://arxiv.org/abs/2307.14298v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG", "68T01", "I.2.1"], "primary_category": "cs.IR"}
{"title": "Evaluating the Moral Beliefs Encoded in LLMs", "abstract": "This paper presents a case study on the design, administration,\npost-processing, and evaluation of surveys on large language models (LLMs). It\ncomprises two components: (1) A statistical method for eliciting beliefs\nencoded in LLMs. We introduce statistical measures and evaluation metrics that\nquantify the probability of an LLM \"making a choice\", the associated\nuncertainty, and the consistency of that choice. (2) We apply this method to\nstudy what moral beliefs are encoded in different LLMs, especially in ambiguous\ncases where the right choice is not obvious. We design a large-scale survey\ncomprising 680 high-ambiguity moral scenarios (e.g., \"Should I tell a white\nlie?\") and 687 low-ambiguity moral scenarios (e.g., \"Should I stop for a\npedestrian on the road?\"). Each scenario includes a description, two possible\nactions, and auxiliary labels indicating violated rules (e.g., \"do not kill\").\nWe administer the survey to 28 open- and closed-source LLMs. We find that (a)\nin unambiguous scenarios, most models \"choose\" actions that align with\ncommonsense. In ambiguous cases, most models express uncertainty. (b) Some\nmodels are uncertain about choosing the commonsense action because their\nresponses are sensitive to the question-wording. (c) Some models reflect clear\npreferences in ambiguous scenarios. Specifically, closed-source models tend to\nagree with each other.", "published": "2023-07-26 17:42:43", "link": "http://arxiv.org/abs/2307.14324v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Diff-E: Diffusion-based Learning for Decoding Imagined Speech EEG", "abstract": "Decoding EEG signals for imagined speech is a challenging task due to the\nhigh-dimensional nature of the data and low signal-to-noise ratio. In recent\nyears, denoising diffusion probabilistic models (DDPMs) have emerged as\npromising approaches for representation learning in various domains. Our study\nproposes a novel method for decoding EEG signals for imagined speech using\nDDPMs and a conditional autoencoder named Diff-E. Results indicate that Diff-E\nsignificantly improves the accuracy of decoding EEG signals for imagined speech\ncompared to traditional machine learning techniques and baseline models. Our\nfindings suggest that DDPMs can be an effective tool for EEG signal decoding,\nwith potential implications for the development of brain-computer interfaces\nthat enable communication through imagined speech.", "published": "2023-07-26 07:12:39", "link": "http://arxiv.org/abs/2307.14389v1", "categories": ["eess.AS", "cs.CL", "cs.HC", "cs.LG", "68T10"], "primary_category": "eess.AS"}
{"title": "A Predictive Model of Digital Information Engagement: Forecasting User\n  Engagement With English Words by Incorporating Cognitive Biases,\n  Computational Linguistics and Natural Language Processing", "abstract": "This study introduces and empirically tests a novel predictive model for\ndigital information engagement (IE) - the READ model, an acronym for the four\npivotal attributes of engaging information: Representativeness, Ease-of-use,\nAffect, and Distribution. Conceptualized within the theoretical framework of\nCumulative Prospect Theory, the model integrates key cognitive biases with\ncomputational linguistics and natural language processing to develop a\nmultidimensional perspective on information engagement. A rigorous testing\nprotocol was implemented, involving 50 randomly selected pairs of synonymous\nwords (100 words in total) from the WordNet database. These words' engagement\nlevels were evaluated through a large-scale online survey (n = 80,500) to\nderive empirical IE metrics. The READ attributes for each word were then\ncomputed and their predictive efficacy examined. The findings affirm the READ\nmodel's robustness, accurately predicting a word's IE level and distinguishing\nthe more engaging word from a pair of synonyms with an 84% accuracy rate. The\nREAD model's potential extends across various domains, including business,\neducation, government, and healthcare, where it could enhance content\nengagement and inform AI language model development and generative text work.\nFuture research should address the model's scalability and adaptability across\ndifferent domains and languages, thereby broadening its applicability and\nefficacy.", "published": "2023-07-26 20:58:47", "link": "http://arxiv.org/abs/2307.14500v1", "categories": ["cs.HC", "cs.CL", "cs.LG", "68U15", "H.5; H.5.1; H.5.2; D.3.2"], "primary_category": "cs.HC"}
{"title": "Words That Stick: Predicting Decision Making and Synonym Engagement\n  Using Cognitive Biases and Computational Linguistics", "abstract": "This research draws upon cognitive psychology and information systems studies\nto anticipate user engagement and decision-making on digital platforms. By\nemploying natural language processing (NLP) techniques and insights from\ncognitive bias research, we delve into user interactions with synonyms within\ndigital content. Our methodology synthesizes four cognitive\nbiasesRepresentativeness, Ease-of-use, Affect, and Distributioninto the READ\nmodel. Through a comprehensive user survey, we assess the model's ability to\npredict user engagement, discovering that synonyms that accurately represent\ncore ideas, are easy to understand, elicit emotional responses, and are\ncommonly encountered, promote greater user engagement. Crucially, our work\noffers a fresh lens on human-computer interaction, digital behaviors, and\ndecision-making processes. Our results highlight the promise of cognitive\nbiases as potent indicators of user engagement, underscoring their significance\nin designing effective digital content across fields like education and\nmarketing.", "published": "2023-07-26 21:20:03", "link": "http://arxiv.org/abs/2307.14511v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "03B65", "H.5; I.7"], "primary_category": "cs.HC"}
{"title": "Utilizing Large Language Models for Natural Interface to Pharmacology\n  Databases", "abstract": "The drug development process necessitates that pharmacologists undertake\nvarious tasks, such as reviewing literature, formulating hypotheses, designing\nexperiments, and interpreting results. Each stage requires accessing and\nquerying vast amounts of information. In this abstract, we introduce a Large\nLanguage Model (LLM)-based Natural Language Interface designed to interact with\nstructured information stored in databases. Our experiments demonstrate the\nfeasibility and effectiveness of the proposed framework. This framework can\ngeneralize to query a wide range of pharmaceutical data and knowledge bases.", "published": "2023-07-26 17:50:11", "link": "http://arxiv.org/abs/2307.15717v1", "categories": ["cs.CL", "cs.AI", "cs.DB"], "primary_category": "cs.CL"}
{"title": "How User Language Affects Conflict Fatality Estimates in ChatGPT", "abstract": "OpenAI's ChatGPT language model has gained popularity as a powerful tool for\ncomplex problem-solving and information retrieval. However, concerns arise\nabout the reproduction of biases present in the language-specific training\ndata. In this study, we address this issue in the context of the\nIsraeli-Palestinian and Turkish-Kurdish conflicts. Using GPT-3.5, we employed\nan automated query procedure to inquire about casualties in specific\nairstrikes, in both Hebrew and Arabic for the former conflict and Turkish and\nKurdish for the latter. Our analysis reveals that GPT-3.5 provides 27$\\pm$11\npercent lower fatality estimates when queried in the language of the attacker\nthan in the language of the targeted group. Evasive answers denying the\nexistence of such attacks further increase the discrepancy, creating a novel\nbias mechanism not present in regular search engines. This language bias has\nthe potential to amplify existing media biases and contribute to information\nbubbles, ultimately reinforcing conflicts.", "published": "2023-07-26 07:07:44", "link": "http://arxiv.org/abs/2308.00072v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Sound Field Estimation around a Rigid Sphere with Physics-informed\n  Neural Network", "abstract": "Accurate estimation of the sound field around a rigid sphere necessitates\nadequate sampling on the sphere, which may not always be possible. To overcome\nthis challenge, this paper proposes a method for sound field estimation based\non a physics-informed neural network. This approach integrates physical\nknowledge into the architecture and training process of the network. In\ncontrast to other learning-based methods, the proposed method incorporates\nadditional constraints derived from the Helmholtz equation and the zero radial\nvelocity condition on the rigid sphere. Consequently, it can generate\nphysically feasible estimations without requiring a large dataset. In contrast\nto the spherical harmonic-based method, the proposed approach has better\nfitting abilities and circumvents the ill condition caused by truncation.\nSimulation results demonstrate the effectiveness of the proposed method in\nachieving accurate sound field estimations from limited measurements,\noutperforming the spherical harmonic method and plane-wave decomposition\nmethod.", "published": "2023-07-26 07:52:29", "link": "http://arxiv.org/abs/2307.14013v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "NeuroHeed: Neuro-Steered Speaker Extraction using EEG Signals", "abstract": "Humans possess the remarkable ability to selectively attend to a single\nspeaker amidst competing voices and background noise, known as selective\nauditory attention. Recent studies in auditory neuroscience indicate a strong\ncorrelation between the attended speech signal and the corresponding brain's\nelicited neuronal activities, which the latter can be measured using affordable\nand non-intrusive electroencephalography (EEG) devices. In this study, we\npresent NeuroHeed, a speaker extraction model that leverages EEG signals to\nestablish a neuronal attractor which is temporally associated with the speech\nstimulus, facilitating the extraction of the attended speech signal in a\ncocktail party scenario. We propose both an offline and an online NeuroHeed,\nwith the latter designed for real-time inference. In the online NeuroHeed, we\nadditionally propose an autoregressive speaker encoder, which accumulates past\nextracted speech signals for self-enrollment of the attended speaker\ninformation into an auditory attractor, that retains the attentional momentum\nover time. Online NeuroHeed extracts the current window of the speech signals\nwith guidance from both attractors. Experimental results demonstrate that\nNeuroHeed effectively extracts brain-attended speech signals, achieving high\nsignal quality, excellent perceptual quality, and intelligibility in a\ntwo-speaker scenario.", "published": "2023-07-26 17:07:27", "link": "http://arxiv.org/abs/2307.14303v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Exploring the Interactions between Target Positive and Negative\n  Information for Acoustic Echo Cancellation", "abstract": "Acoustic echo cancellation (AEC) aims to remove interference signals while\nleaving near-end speech least distorted. As the indistinguishable patterns\nbetween near-end speech and interference signals, near-end speech can't be\nseparated completely, causing speech distortion and interference signals\nresidual. We observe that besides target positive information, e.g.,\nground-truth speech and features, the target negative information, such as\ninterference signals and features, helps make pattern of target speech and\ninterference signals more discriminative. Therefore, we present a novel AEC\nmodel encoder-decoder architecture with the guidance of negative information\ntermed as CMNet. A collaboration module (CM) is designed to establish the\ncorrelation between the target positive and negative information in a learnable\nmanner via three blocks: target positive, target negative, and interactive\nblock. Experimental results demonstrate our CMNet achieves superior performance\nthan recent methods.", "published": "2023-07-26 01:28:25", "link": "http://arxiv.org/abs/2307.13888v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Perceptual Quality Enhancement of Sound Field Synthesis Based on\n  Combination of Pressure and Amplitude Matching", "abstract": "A sound field synthesis method enhancing perceptual quality is proposed.\nSound field synthesis using multiple loudspeakers enables spatial audio\nreproduction with a broad listening area; however, synthesis errors at high\nfrequencies called spatial aliasing artifacts are unavoidable. To minimize\nthese artifacts, we propose a method based on the combination of pressure and\namplitude matching. On the basis of the human's auditory properties,\nsynthesizing the amplitude distribution will be sufficient for horizontal sound\nlocalization. Furthermore, a flat amplitude response should be synthesized as\nmuch as possible to avoid coloration. Therefore, we apply amplitude matching,\nwhich is a method to synthesize the desired amplitude distribution with\narbitrary phase distribution, for high frequencies and conventional pressure\nmatching for low frequencies. Experimental results of numerical simulations and\nlistening tests using a practical system indicated that the perceptual quality\nof the sound field synthesized by the proposed method was improved from that\nsynthesized by pressure matching.", "published": "2023-07-26 03:39:16", "link": "http://arxiv.org/abs/2307.13941v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Rethinking Voice-Face Correlation: A Geometry View", "abstract": "Previous works on voice-face matching and voice-guided face synthesis\ndemonstrate strong correlations between voice and face, but mainly rely on\ncoarse semantic cues such as gender, age, and emotion. In this paper, we aim to\ninvestigate the capability of reconstructing the 3D facial shape from voice\nfrom a geometry perspective without any semantic information. We propose a\nvoice-anthropometric measurement (AM)-face paradigm, which identifies\npredictable facial AMs from the voice and uses them to guide 3D face\nreconstruction. By leveraging AMs as a proxy to link the voice and face\ngeometry, we can eliminate the influence of unpredictable AMs and make the face\ngeometry tractable. Our approach is evaluated on our proposed dataset with\nground-truth 3D face scans and corresponding voice recordings, and we find\nsignificant correlations between voice and specific parts of the face geometry,\nsuch as the nasal cavity and cranium. Our work offers a new perspective on\nvoice-face correlation and can serve as a good empirical study for\nanthropometry science.", "published": "2023-07-26 04:03:10", "link": "http://arxiv.org/abs/2307.13948v1", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Link\n  between Phonemes and Facial Features", "abstract": "This work unveils the enigmatic link between phonemes and facial features.\nTraditional studies on voice-face correlations typically involve using a long\nperiod of voice input, including generating face images from voices and\nreconstructing 3D face meshes from voices. However, in situations like\nvoice-based crimes, the available voice evidence may be short and limited.\nAdditionally, from a physiological perspective, each segment of speech --\nphoneme -- corresponds to different types of airflow and movements in the face.\nTherefore, it is advantageous to discover the hidden link between phonemes and\nface attributes. In this paper, we propose an analysis pipeline to help us\nexplore the voice-face relationship in a fine-grained manner, i.e., phonemes\nv.s. facial anthropometric measurements (AM). We build an estimator for each\nphoneme-AM pair and evaluate the correlation through hypothesis testing. Our\nresults indicate that AMs are more predictable from vowels compared to\nconsonants, particularly with plosives. Additionally, we observe that if a\nspecific AM exhibits more movement during phoneme pronunciation, it is more\npredictable. Our findings support those in physiology regarding correlation and\nlay the groundwork for future research on speech-face multimodal learning.", "published": "2023-07-26 04:08:12", "link": "http://arxiv.org/abs/2307.13953v1", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "BovineTalk: Machine Learning for Vocalization Analysis of Dairy Cattle\n  under Negative Affective States", "abstract": "There is a critical need to develop and validate non-invasive animal-based\nindicators of affective states in livestock species, in order to integrate them\ninto on-farm assessment protocols, potentially via the use of precision\nlivestock farming (PLF) tools. One such promising approach is the use of vocal\nindicators. The acoustic structure of vocalizations and their functions were\nextensively studied in important livestock species, such as pigs, horses,\npoultry and goats, yet cattle remain understudied in this context to date. Cows\nwere shown to produce two types vocalizations: low-frequency calls (LF),\nproduced with the mouth closed, or partially closed, for close distance\ncontacts and open mouth emitted high-frequency calls (HF), produced for long\ndistance communication, with the latter considered to be largely associated\nwith negative affective states. Moreover, cattle vocalizations were shown to\ncontain information on individuality across a wide range of contexts, both\nnegative and positive. Nowadays, dairy cows are facing a series of negative\nchallenges and stressors in a typical production cycle, making vocalizations\nduring negative affective states of special interest for research. One\ncontribution of this study is providing the largest to date pre-processed\n(clean from noises) dataset of lactating adult multiparous dairy cows during\nnegative affective states induced by visual isolation challenges. Here we\npresent two computational frameworks - deep learning based and explainable\nmachine learning based, to classify high and low-frequency cattle calls, and\nindividual cow voice recognition. Our models in these two frameworks reached\n87.2% and 89.4% accuracy for LF and HF classification, with 68.9% and 72.5%\naccuracy rates for the cow individual identification, respectively.", "published": "2023-07-26 07:07:03", "link": "http://arxiv.org/abs/2307.13994v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "WavJourney: Compositional Audio Creation with Large Language Models", "abstract": "Despite breakthroughs in audio generation models, their capabilities are\noften confined to domain-specific conditions such as speech transcriptions and\naudio captions. However, real-world audio creation aims to generate harmonious\naudio containing various elements such as speech, music, and sound effects with\ncontrollable conditions, which is challenging to address using existing audio\ngeneration systems. We present WavJourney, a novel framework that leverages\nLarge Language Models (LLMs) to connect various audio models for audio\ncreation. WavJourney allows users to create storytelling audio content with\ndiverse audio elements simply from textual descriptions. Specifically, given a\ntext instruction, WavJourney first prompts LLMs to generate an audio script\nthat serves as a structured semantic representation of audio elements. The\naudio script is then converted into a computer program, where each line of the\nprogram calls a task-specific audio generation model or computational operation\nfunction. The computer program is then executed to obtain a compositional and\ninterpretable solution for audio creation. Experimental results suggest that\nWavJourney is capable of synthesizing realistic audio aligned with\ntextually-described semantic, spatial and temporal conditions, achieving\nstate-of-the-art results on text-to-audio generation benchmarks. Additionally,\nwe introduce a new multi-genre story benchmark. Subjective evaluations\ndemonstrate the potential of WavJourney in crafting engaging storytelling audio\ncontent from text. We further demonstrate that WavJourney can facilitate\nhuman-machine co-creation in multi-round dialogues. To foster future research,\nthe code and synthesized audio are available at:\nhttps://audio-agi.github.io/WavJourney_demopage/.", "published": "2023-07-26 17:54:04", "link": "http://arxiv.org/abs/2307.14335v2", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Single Channel Speech Enhancement Using U-Net Spiking Neural Networks", "abstract": "Speech enhancement (SE) is crucial for reliable communication devices or\nrobust speech recognition systems. Although conventional artificial neural\nnetworks (ANN) have demonstrated remarkable performance in SE, they require\nsignificant computational power, along with high energy costs. In this paper,\nwe propose a novel approach to SE using a spiking neural network (SNN) based on\na U-Net architecture. SNNs are suitable for processing data with a temporal\ndimension, such as speech, and are known for their energy-efficient\nimplementation on neuromorphic hardware. As such, SNNs are thus interesting\ncandidates for real-time applications on devices with limited resources. The\nprimary objective of the current work is to develop an SNN-based model with\ncomparable performance to a state-of-the-art ANN model for SE. We train a deep\nSNN using surrogate-gradient-based optimization and evaluate its performance\nusing perceptual objective tests under different signal-to-noise ratios and\nreal-world noise conditions. Our results demonstrate that the proposed\nenergy-efficient SNN model outperforms the Intel Neuromorphic Deep Noise\nSuppression Challenge (Intel N-DNS Challenge) baseline solution and achieves\nacceptable performance compared to an equivalent ANN model.", "published": "2023-07-26 19:10:29", "link": "http://arxiv.org/abs/2307.14464v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Unified Framework for Modality-Agnostic Deepfakes Detection", "abstract": "As AI-generated content (AIGC) thrives, deepfakes have expanded from\nsingle-modality falsification to cross-modal fake content creation, where\neither audio or visual components can be manipulated. While using two unimodal\ndetectors can detect audio-visual deepfakes, cross-modal forgery clues could be\noverlooked. Existing multimodal deepfake detection methods typically establish\ncorrespondence between the audio and visual modalities for binary real/fake\nclassification, and require the co-occurrence of both modalities. However, in\nreal-world multi-modal applications, missing modality scenarios may occur where\neither modality is unavailable. In such cases, audio-visual detection methods\nare less practical than two independent unimodal methods. Consequently, the\ndetector can not always obtain the number or type of manipulated modalities\nbeforehand, necessitating a fake-modality-agnostic audio-visual detector. In\nthis work, we introduce a comprehensive framework that is agnostic to fake\nmodalities, which facilitates the identification of multimodal deepfakes and\nhandles situations with missing modalities, regardless of the manipulations\nembedded in audio, video, or even cross-modal forms. To enhance the modeling of\ncross-modal forgery clues, we employ audio-visual speech recognition (AVSR) as\na preliminary task. This efficiently extracts speech correlations across\nmodalities, a feature challenging for deepfakes to replicate. Additionally, we\npropose a dual-label detection approach that follows the structure of AVSR to\nsupport the independent detection of each modality. Extensive experiments on\nthree audio-visual datasets show that our scheme outperforms state-of-the-art\ndetection methods with promising performance on modality-agnostic audio/video\ndeepfakes.", "published": "2023-07-26 20:30:34", "link": "http://arxiv.org/abs/2307.14491v2", "categories": ["cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "Differentiable adaptive short-time Fourier transform with respect to the\n  window length", "abstract": "This paper presents a gradient-based method for on-the-fly optimization for\nboth per-frame and per-frequency window length of the short-time Fourier\ntransform (STFT), related to previous work in which we developed a\ndifferentiable version of STFT by making the window length a continuous\nparameter. The resulting differentiable adaptive STFT possesses commendable\nproperties, such as the ability to adapt in the same time-frequency\nrepresentation to both transient and stationary components, while being easily\noptimized by gradient descent. We validate the performance of our method in\nvibration analysis.", "published": "2023-07-26 06:55:42", "link": "http://arxiv.org/abs/2308.02418v1", "categories": ["eess.SP", "cs.LG", "eess.AS", "math.ST", "stat.TH"], "primary_category": "eess.SP"}
