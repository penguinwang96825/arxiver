{"title": "Improving Abstractive Text Summarization with History Aggregation", "abstract": "Recent neural sequence to sequence models have provided feasible solutions\nfor abstractive summarization. However, such models are still hard to tackle\nlong text dependency in the summarization task. A high-quality summarization\nsystem usually depends on strong encoder which can refine important information\nfrom long input texts so that the decoder can generate salient summaries from\nthe encoder's memory. In this paper, we propose an aggregation mechanism based\non the Transformer model to address the challenge of long text representation.\nOur model can review history information to make encoder hold more memory\ncapacity. Empirically, we apply our aggregation mechanism to the Transformer\nmodel and experiment on CNN/DailyMail dataset to achieve higher quality\nsummaries compared to several strong baseline models on the ROUGE metrics.", "published": "2019-12-24 03:34:58", "link": "http://arxiv.org/abs/1912.11046v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Falcon 2.0: An Entity and Relation Linking Tool over Wikidata", "abstract": "The Natural Language Processing (NLP) community has significantly contributed\nto the solutions for entity and relation recognition from the text, and\npossibly linking them to proper matches in Knowledge Graphs (KGs). Considering\nWikidata as the background KG, still, there are limited tools to link knowledge\nwithin the text to Wikidata. In this paper, we present Falcon 2.0, first joint\nentity, and relation linking tool over Wikidata. It receives a short natural\nlanguage text in the English language and outputs a ranked list of entities and\nrelations annotated with the proper candidates in Wikidata. The candidates are\nrepresented by their Internationalized Resource Identifier (IRI) in Wikidata.\nFalcon 2.0 resorts to the English language model for the recognition task\n(e.g., N-Gram tiling and N-Gram splitting), and then an optimization approach\nfor linking task. We have empirically studied the performance of Falcon 2.0 on\nWikidata and concluded that it outperforms all the existing baselines. Falcon\n2.0 is public and can be reused by the community; all the required instructions\nof Falcon 2.0 are well-documented at our GitHub repository. We also demonstrate\nan online API, which can be run without any technical expertise. Falcon 2.0 and\nits background knowledge bases are available as resources at\nhttps://labs.tib.eu/falcon/falcon2/.", "published": "2019-12-24 09:53:27", "link": "http://arxiv.org/abs/1912.11270v7", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Simultaneous Identification of Tweet Purpose and Position", "abstract": "Tweet classification has attracted considerable attention recently. Most of\nthe existing work on tweet classification focuses on topic classification,\nwhich classifies tweets into several predefined categories, and sentiment\nclassification, which classifies tweets into positive, negative and neutral.\nSince tweets are different from conventional text in that they generally are of\nlimited length and contain informal, irregular or new words, so it is difficult\nto determine user intention to publish a tweet and user attitude towards\ncertain topic. In this paper, we aim to simultaneously classify tweet purpose,\ni.e., the intention for user to publish a tweet, and position, i.e.,\nsupporting, opposing or being neutral to a given topic. By transforming this\nproblem to a multi-label classification problem, a multi-label classification\nmethod with post-processing is proposed. Experiments on real-world data sets\ndemonstrate the effectiveness of this method and the results outperform the\nindividual classification methods.", "published": "2019-12-24 17:09:54", "link": "http://arxiv.org/abs/2001.00051v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "cs.SI", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Audio-based automatic mating success prediction of giant pandas", "abstract": "Giant pandas, stereotyped as silent animals, make significantly more vocal\nsounds during breeding season, suggesting that sounds are essential for\ncoordinating their reproduction and expression of mating preference. Previous\nbiological studies have also proven that giant panda sounds are correlated with\nmating results and reproduction. This paper makes the first attempt to devise\nan automatic method for predicting mating success of giant pandas based on\ntheir vocal sounds. Given an audio sequence of mating giant pandas recorded\nduring breeding encounters, we first crop out the segments with vocal sound of\ngiant pandas, and normalize its magnitude, and length. We then extract acoustic\nfeatures from the audio segment and feed the features into a deep neural\nnetwork, which classifies the mating into success or failure. The proposed deep\nneural network employs convolution layers followed by bidirection gated\nrecurrent units to extract vocal features, and applies attention mechanism to\nforce the network to focus on most relevant features. Evaluation experiments on\na data set collected during the past nine years obtain promising results,\nproving the potential of audio-based automatic mating success prediction\nmethods in assisting giant panda reproduction.", "published": "2019-12-24 16:08:48", "link": "http://arxiv.org/abs/1912.11333v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SoundSpaces: Audio-Visual Navigation in 3D Environments", "abstract": "Moving around in the world is naturally a multisensory experience, but\ntoday's embodied agents are deaf---restricted to solely their visual perception\nof the environment. We introduce audio-visual navigation for complex,\nacoustically and visually realistic 3D environments. By both seeing and\nhearing, the agent must learn to navigate to a sounding object. We propose a\nmulti-modal deep reinforcement learning approach to train navigation policies\nend-to-end from a stream of egocentric audio-visual observations, allowing the\nagent to (1) discover elements of the geometry of the physical space indicated\nby the reverberating audio and (2) detect and follow sound-emitting targets. We\nfurther introduce SoundSpaces: a first-of-its-kind dataset of audio renderings\nbased on geometrical acoustic simulations for two sets of publicly available 3D\nenvironments (Matterport3D and Replica), and we instrument Habitat to support\nthe new sensor, making it possible to insert arbitrary sound sources in an\narray of real-world scanned environments. Our results show that audio greatly\nbenefits embodied visual navigation in 3D spaces, and our work lays groundwork\nfor new research in embodied AI with audio-visual perception.", "published": "2019-12-24 18:59:50", "link": "http://arxiv.org/abs/1912.11474v3", "categories": ["cs.CV", "cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
