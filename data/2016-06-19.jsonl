{"title": "Can Machine Generate Traditional Chinese Poetry? A Feigenbaum Test", "abstract": "Recent progress in neural learning demonstrated that machines can do well in\nregularized tasks, e.g., the game of Go. However, artistic activities such as\npoem generation are still widely regarded as human's special capability. In\nthis paper, we demonstrate that a simple neural model can imitate human in some\ntasks of art generation. We particularly focus on traditional Chinese poetry,\nand show that machines can do as well as many contemporary poets and weakly\npass the Feigenbaum Test, a variant of Turing test in professional domains. Our\nmethod is based on an attention-based recurrent neural network, which accepts a\nset of keywords as the theme and generates poems by looking at each keyword\nduring the generation. A number of techniques are proposed to improve the\nmodel, including character vector initialization, attention to input and\nhybrid-style training. Compared to existing poetry generation methods, our\nmodel can generate much more theme-consistent and semantic-rich poems.", "published": "2016-06-19 03:17:29", "link": "http://arxiv.org/abs/1606.05829v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Full-Time Supervision based Bidirectional RNN for Factoid Question\n  Answering", "abstract": "Recently, bidirectional recurrent neural network (BRNN) has been widely used\nfor question answering (QA) tasks with promising performance. However, most\nexisting BRNN models extract the information of questions and answers by\ndirectly using a pooling operation to generate the representation for loss or\nsimilarity calculation. Hence, these existing models don't put supervision\n(loss or similarity calculation) at every time step, which will lose some\nuseful information. In this paper, we propose a novel BRNN model called\nfull-time supervision based BRNN (FTS-BRNN), which can put supervision at every\ntime step. Experiments on the factoid QA task show that our FTS-BRNN can\noutperform other baselines to achieve the state-of-the-art accuracy.", "published": "2016-06-19 11:03:47", "link": "http://arxiv.org/abs/1606.05854v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Graph based manifold regularized deep neural networks for automatic\n  speech recognition", "abstract": "Deep neural networks (DNNs) have been successfully applied to a wide variety\nof acoustic modeling tasks in recent years. These include the applications of\nDNNs either in a discriminative feature extraction or in a hybrid acoustic\nmodeling scenario. Despite the rapid progress in this area, a number of\nchallenges remain in training DNNs. This paper presents an effective way of\ntraining DNNs using a manifold learning based regularization framework. In this\nframework, the parameters of the network are optimized to preserve underlying\nmanifold based relationships between speech feature vectors while minimizing a\nmeasure of loss between network outputs and targets. This is achieved by\nincorporating manifold based locality constraints in the objective criterion of\nDNNs. Empirical evidence is provided to demonstrate that training a network\nwith manifold constraints preserves structural compactness in the hidden layers\nof the network. Manifold regularization is applied to train bottleneck DNNs for\nfeature extraction in hidden Markov model (HMM) based speech recognition. The\nexperiments in this work are conducted on the Aurora-2 spoken digits and the\nAurora-4 read news large vocabulary continuous speech recognition tasks. The\nperformance is measured in terms of word error rate (WER) on these tasks. It is\nshown that the manifold regularized DNNs result in up to 37% reduction in WER\nrelative to standard DNNs.", "published": "2016-06-19 23:40:51", "link": "http://arxiv.org/abs/1606.05925v1", "categories": ["stat.ML", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
