{"title": "A Spanish dataset for Targeted Sentiment Analysis of political headlines", "abstract": "Subjective texts have been studied by several works as they can induce\ncertain behaviours in their users. Most work focuses on user-generated texts in\nsocial networks, but some other texts also comprise opinions on certain topics\nand could influence judgement criteria during political decisions. In this\nwork, we address the task of Targeted Sentiment Analysis for the domain of news\nheadlines, published by the main outlets during the 2019 Argentinean\nPresidential Elections. For this purpose, we present a polarity dataset of\n1,976 headlines mentioning candidates in the 2019 elections at the target\nlevel. Preliminary experiments with state-of-the-art classification algorithms\nbased on pre-trained linguistic models suggest that target information is\nhelpful for this task. We make our data and pre-trained models publicly\navailable.", "published": "2022-08-30 01:30:30", "link": "http://arxiv.org/abs/2208.13947v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NEAR: Named Entity and Attribute Recognition of clinical concepts", "abstract": "Named Entity Recognition (NER) or the extraction of concepts from clinical\ntext is the task of identifying entities in text and slotting them into\ncategories such as problems, treatments, tests, clinical departments,\noccurrences (such as admission and discharge) and others. NER forms a critical\ncomponent of processing and leveraging unstructured data from Electronic Health\nRecords (EHR). While identifying the spans and categories of concepts is itself\na challenging task, these entities could also have attributes such as negation\nthat pivot their meanings implied to the consumers of the named entities. There\nhas been little research dedicated to identifying the entities and their\nqualifying attributes together. This research hopes to contribute to the area\nof detecting entities and their corresponding attributes by modelling the NER\ntask as a supervised, multi-label tagging problem with each of the attributes\nassigned tagging sequence labels. In this paper, we propose 3 architectures to\nachieve this multi-label entity tagging: BiLSTM n-CRF, BiLSTM-CRF-Smax-TF and\nBiLSTM n-CRF-TF. We evaluate these methods on the 2010 i2b2/VA and the i2b2\n2012 shared task datasets. Our different models obtain best NER F1 scores of 0.\n894 and 0.808 on the i2b2 2010/VA and i2b2 2012 respectively. The highest span\nbased micro-averaged F1 polarity scores obtained were 0.832 and 0.836 on the\ni2b2 2010/VA and i2b2 2012 datasets respectively, and the highest\nmacro-averaged F1 polarity scores obtained were 0.924 and 0.888 respectively.\nThe modality studies conducted on i2b2 2012 dataset revealed high scores of\n0.818 and 0.501 for span based micro-averaged F1 and macro-averaged F1\nrespectively.", "published": "2022-08-30 01:46:11", "link": "http://arxiv.org/abs/2208.13949v1", "categories": ["cs.CL", "68U15 (primary)"], "primary_category": "cs.CL"}
{"title": "IMCI: Integrate Multi-view Contextual Information for Fact Extraction\n  and Verification", "abstract": "With the rapid development of automatic fake news detection technology, fact\nextraction and verification (FEVER) has been attracting more attention. The\ntask aims to extract the most related fact evidences from millions of\nopen-domain Wikipedia documents and then verify the credibility of\ncorresponding claims. Although several strong models have been proposed for the\ntask and they have made great progress, we argue that they fail to utilize\nmulti-view contextual information and thus cannot obtain better performance. In\nthis paper, we propose to integrate multi-view contextual information (IMCI)\nfor fact extraction and verification. For each evidence sentence, we define two\nkinds of context, i.e. intra-document context and inter-document context}.\nIntra-document context consists of the document title and all the other\nsentences from the same document. Inter-document context consists of all other\nevidences which may come from different documents. Then we integrate the\nmulti-view contextual information to encode the evidence sentences to handle\nthe task. Our experimental results on FEVER 1.0 shared task show that our IMCI\nframework makes great progress on both fact extraction and verification, and\nachieves state-of-the-art performance with a winning FEVER score of 72.97% and\nlabel accuracy of 75.84% on the online blind test set. We also conduct ablation\nstudy to detect the impact of multi-view contextual information. Our codes will\nbe released at https://github.com/phoenixsecularbird/IMCI.", "published": "2022-08-30 05:57:34", "link": "http://arxiv.org/abs/2208.14001v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transformers with Learnable Activation Functions", "abstract": "Activation functions can have a significant impact on reducing the\ntopological complexity of input data and therefore improve the performance of\nthe model. Selecting a suitable activation function is an essential step in\nneural model design. However, the choice of activation function is seldom\ndiscussed or explored in Transformer-based language models. Their activation\nfunctions are chosen beforehand and then remain fixed from pre-training to\nfine-tuning. As a result, the inductive biases they imposed on models cannot be\nadjusted during this long life cycle. Moreover, subsequently developed models\n(e.g., RoBERTa, BART, and GPT-3) often follow up prior work (e.g., BERT) to use\nthe same activation function without justification. In this paper, we\ninvestigate the effectiveness of using Rational Activation Function (RAF), a\nlearnable activation function, in the Transformer architecture. In contrast to\nconventional, predefined activation functions, RAFs can adaptively learn\noptimal activation functions during training according to input data. Our\nexperiments show the RAF-based Transformer (RAFT) achieves a lower validation\nperplexity than a vanilla BERT with the GELU function. We further evaluate RAFT\non downstream tasks in low- and full-data settings. Our results show that RAFT\noutperforms the counterpart model across the majority of tasks and settings.\nFor instance, RAFT outperforms vanilla BERT on the GLUE benchmark by 5.71\npoints on average in low-data scenario (where 100 training examples are\navailable) and by 2.05 points on SQuAD in full-data setting. Analysis of the\nshapes of learned RAFs further unveils that they substantially vary between\ndifferent layers of the pre-trained model and mostly look very different from\nconventional activation functions. RAFT opens a new research direction for\nanalyzing and interpreting pre-trained models according to the learned\nactivation functions.", "published": "2022-08-30 09:47:31", "link": "http://arxiv.org/abs/2208.14111v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Boosting the Open-Domain Chatbot with Human Feedback", "abstract": "Many open-domain dialogue models pre-trained with social media comments can\ngenerate coherent replies but have difficulties producing engaging responses\nwhen interacting with real users. This phenomenon might mainly result from the\ndeficiency of annotated human-human conversations and the misalignment with\nhuman preference. In this paper, we propose a novel and efficient approach\nDiamante to boost the open-domain chatbot, where two kinds of human feedback\n(including explicit demonstration and implicit preference) are collected and\nleveraged. By asking annotators to select or amend the model-generated\ncandidate responses, Diamante efficiently collects the human demonstrated\nresponses and constructs a Chinese chit-chat dataset. To enhance the alignment\nwith human preference, Diamante leverages the implicit preference in the data\ncollection process and introduces the generation-evaluation joint training.\nComprehensive experiments indicate that the Diamante dataset and joint training\nparadigm can significantly boost the performance of Chinese pre-trained\ndialogue models.", "published": "2022-08-30 11:32:35", "link": "http://arxiv.org/abs/2208.14165v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient and Interpretable Neural Models for Entity Tracking", "abstract": "What would it take for a natural language model to understand a novel, such\nas The Lord of the Rings? Among other things, such a model must be able to: (a)\nidentify and record new characters (entities) and their attributes as they are\nintroduced in the text, and (b) identify subsequent references to the\ncharacters previously introduced and update their attributes. This problem of\nentity tracking is essential for language understanding, and thus, useful for a\nwide array of downstream applications in NLP such as question-answering,\nsummarization.\n  In this thesis, we focus on two key problems in relation to facilitating the\nuse of entity tracking models: (i) scaling entity tracking models to long\ndocuments, such as a novel, and (ii) integrating entity tracking into language\nmodels. Applying language technologies to long documents has garnered interest\nrecently, but computational constraints are a significant bottleneck in scaling\nup current methods. In this thesis, we argue that computationally efficient\nentity tracking models can be developed by representing entities with rich,\nfixed-dimensional vector representations derived from pretrained language\nmodels, and by exploiting the ephemeral nature of entities. We also argue for\nthe integration of entity tracking into language models as it will allow for:\n(i) wider application given the current ubiquitous use of pretrained language\nmodels in NLP applications, and (ii) easier adoption since it is much easier to\nswap in a new pretrained language model than to integrate a separate standalone\nentity tracking model.", "published": "2022-08-30 13:25:27", "link": "http://arxiv.org/abs/2208.14252v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MultiCoNER: A Large-scale Multilingual dataset for Complex Named Entity\n  Recognition", "abstract": "We present MultiCoNER, a large multilingual dataset for Named Entity\nRecognition that covers 3 domains (Wiki sentences, questions, and search\nqueries) across 11 languages, as well as multilingual and code-mixing subsets.\nThis dataset is designed to represent contemporary challenges in NER, including\nlow-context scenarios (short and uncased text), syntactically complex entities\nlike movie titles, and long-tail entity distributions. The 26M token dataset is\ncompiled from public resources using techniques such as heuristic-based\nsentence sampling, template extraction and slotting, and machine translation.\nWe applied two NER models on our dataset: a baseline XLM-RoBERTa model, and a\nstate-of-the-art GEMNET model that leverages gazetteers. The baseline achieves\nmoderate performance (macro-F1=54%), highlighting the difficulty of our data.\nGEMNET, which uses gazetteers, improvement significantly (average improvement\nof macro-F1=+30%). MultiCoNER poses challenges even for large pre-trained\nlanguage models, and we believe that it can help further research in building\nrobust NER systems. MultiCoNER is publicly available at\nhttps://registry.opendata.aws/multiconer/ and we hope that this resource will\nhelp advance research in various aspects of NER.", "published": "2022-08-30 20:45:54", "link": "http://arxiv.org/abs/2208.14536v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "To Adapt or to Fine-tune: A Case Study on Abstractive Summarization", "abstract": "Recent advances in the field of abstractive summarization leverage\npre-trained language models rather than train a model from scratch. However,\nsuch models are sluggish to train and accompanied by a massive overhead.\nResearchers have proposed a few lightweight alternatives such as smaller\nadapters to mitigate the drawbacks. Nonetheless, it remains uncertain whether\nusing adapters benefits the task of summarization, in terms of improved\nefficiency without an unpleasant sacrifice in performance. In this work, we\ncarry out multifaceted investigations on fine-tuning and adapters for\nsummarization tasks with varying complexity: language, domain, and task\ntransfer. In our experiments, fine-tuning a pre-trained language model\ngenerally attains a better performance than using adapters; the performance gap\npositively correlates with the amount of training data used. Notably, adapters\nexceed fine-tuning under extremely low-resource conditions. We further provide\ninsights on multilinguality, model convergence, and robustness, hoping to shed\nlight on the pragmatic choice of fine-tuning or adapters in abstractive\nsummarization.", "published": "2022-08-30 22:48:28", "link": "http://arxiv.org/abs/2208.14559v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Combining keyphrase extraction and lexical diversity to characterize\n  ideas in publication titles", "abstract": "Beyond bibliometrics, there is interest in characterizing the evolution of\nthe number of ideas in scientific papers. A common approach for investigating\nthis involves analyzing the titles of publications to detect vocabulary changes\nover time. With the notion that phrases, or more specifically keyphrases,\nrepresent concepts, lexical diversity metrics are applied to phrased versions\nof the titles. Thus changes in lexical diversity are treated as indicators of\nshifts, and possibly expansion, of research. Therefore, optimizing detection of\nkeyphrases is an important aspect of this process. Rather than just one, we\npropose to use multiple phrase detection models with the goal to produce a more\ncomprehensive set of keyphrases from the source corpora. Another potential\nadvantage to this approach is that the union and difference of these sets may\nprovide automated techniques for identifying and omitting non-specific phrases.\nWe compare the performance of several phrase detection models, analyze the\nkeyphrase sets output of each, and calculate lexical diversity of corpora\nvariants incorporating keyphrases from each model, using four common lexical\ndiversity metrics.", "published": "2022-08-30 04:08:35", "link": "http://arxiv.org/abs/2208.13978v1", "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.CL"}
{"title": "Faithful Reasoning Using Large Language Models", "abstract": "Although contemporary large language models (LMs) demonstrate impressive\nquestion-answering capabilities, their answers are typically the product of a\nsingle call to the model. This entails an unwelcome degree of opacity and\ncompromises performance, especially on problems that are inherently multi-step.\nTo address these limitations, we show how LMs can be made to perform faithful\nmulti-step reasoning via a process whose causal structure mirrors the\nunderlying logical structure of the problem. Our approach works by chaining\ntogether reasoning steps, where each step results from calls to two fine-tuned\nLMs, one for selection and one for inference, to produce a valid reasoning\ntrace. Our method carries out a beam search through the space of reasoning\ntraces to improve reasoning quality. We demonstrate the effectiveness of our\nmodel on multi-step logical deduction and scientific question-answering,\nshowing that it outperforms baselines on final answer accuracy, and generates\nhumanly interpretable reasoning traces whose validity can be checked by the\nuser.", "published": "2022-08-30 13:44:41", "link": "http://arxiv.org/abs/2208.14271v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "WikiLink: an encyclopedia-based semantic network for design innovation", "abstract": "Data-driven design and innovation is a process to reuse and provide valuable\nand useful information. However, existing semantic networks for design\ninnovation is built on data source restricted to technological and scientific\ninformation. Besides, existing studies build the edges of a semantic network\nonly on either statistical or semantic relationships, which is less likely to\nmake full use of the benefits from both types of relationships and discover\nimplicit knowledge for design innovation. Therefore, we constructed WikiLink, a\nsemantic network based on Wikipedia. Combined weight which fuses both the\nstatistic and semantic weights between concepts is introduced in WikiLink, and\nfour algorithms are developed for inspiring new ideas. Evaluation experiments\nare undertaken and results show that the network is characterised by high\ncoverage of terms, relationships and disciplines, which proves the network's\neffectiveness and usefulness. Then a demonstration and case study results\nindicate that WikiLink can serve as an idea generation tool for innovation in\nconceptual design. The source code of WikiLink and the backend data are\nprovided open-source for more users to explore and build on.", "published": "2022-08-30 15:48:36", "link": "http://arxiv.org/abs/2208.14349v1", "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.CL"}
{"title": "Optimizing Bi-Encoder for Named Entity Recognition via Contrastive\n  Learning", "abstract": "We present a bi-encoder framework for named entity recognition (NER), which\napplies contrastive learning to map candidate text spans and entity types into\nthe same vector representation space. Prior work predominantly approaches NER\nas sequence labeling or span classification. We instead frame NER as a\nrepresentation learning problem that maximizes the similarity between the\nvector representations of an entity mention and its type. This makes it easy to\nhandle nested and flat NER alike, and can better leverage noisy\nself-supervision signals. A major challenge to this bi-encoder formulation for\nNER lies in separating non-entity spans from entity mentions. Instead of\nexplicitly labeling all non-entity spans as the same class $\\texttt{Outside}$\n($\\texttt{O}$) as in most prior methods, we introduce a novel dynamic\nthresholding loss. Experiments show that our method performs well in both\nsupervised and distantly supervised settings, for nested and flat NER alike,\nestablishing new state of the art across standard datasets in the general\ndomain (e.g., ACE2004, ACE2005) and high-value verticals such as biomedicine\n(e.g., GENIA, NCBI, BC5CDR, JNLPBA). We release the code at\ngithub.com/microsoft/binder.", "published": "2022-08-30 23:19:04", "link": "http://arxiv.org/abs/2208.14565v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Expressions Causing Differences in Emotion Recognition in Social\n  Networking Service Documents", "abstract": "It is often difficult to correctly infer a writer's emotion from text\nexchanged online, and differences in recognition between writers and readers\ncan be problematic. In this paper, we propose a new framework for detecting\nsentences that create differences in emotion recognition between the writer and\nthe reader and for detecting the kinds of expressions that cause such\ndifferences. The proposed framework consists of a bidirectional encoder\nrepresentations from transformers (BERT)-based detector that detects sentences\ncausing differences in emotion recognition and an analysis that acquires\nexpressions that characteristically appear in such sentences. The detector,\nbased on a Japanese SNS-document dataset with emotion labels annotated by both\nthe writer and three readers of the social networking service (SNS) documents,\ndetected \"hidden-anger sentences\" with AUC = 0.772; these sentences gave rise\nto differences in the recognition of anger. Because SNS documents contain many\nsentences whose meaning is extremely difficult to interpret, by analyzing the\nsentences detected by this detector, we obtained several expressions that\nappear characteristically in hidden-anger sentences. The detected sentences and\nexpressions do not convey anger explicitly, and it is difficult to infer the\nwriter's anger, but if the implicit anger is pointed out, it becomes possible\nto guess why the writer is angry. Put into practical use, this framework would\nlikely have the ability to mitigate problems based on misunderstandings.", "published": "2022-08-30 13:17:32", "link": "http://arxiv.org/abs/2208.14244v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Annotated Dataset Creation through General Purpose Language Models for\n  non-English Medical NLP", "abstract": "Obtaining text datasets with semantic annotations is an effortful process,\nyet crucial for supervised training in natural language processsing (NLP). In\ngeneral, developing and applying new NLP pipelines in domain-specific contexts\nfor tasks often requires custom designed datasets to address NLP tasks in\nsupervised machine learning fashion. When operating in non-English languages\nfor medical data processing, this exposes several minor and major,\ninterconnected problems such as lack of task-matching datasets as well as\ntask-specific pre-trained models. In our work we suggest to leverage pretrained\nlanguage models for training data acquisition in order to retrieve sufficiently\nlarge datasets for training smaller and more efficient models for use-case\nspecific tasks. To demonstrate the effectiveness of your approach, we create a\ncustom dataset which we use to train a medical NER model for German texts,\nGPTNERMED, yet our method remains language-independent in principle. Our\nobtained dataset as well as our pre-trained models are publicly available at:\nhttps://github.com/frankkramer-lab/GPTNERMED", "published": "2022-08-30 18:42:55", "link": "http://arxiv.org/abs/2208.14493v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SwiftPruner: Reinforced Evolutionary Pruning for Efficient Ad Relevance", "abstract": "Ad relevance modeling plays a critical role in online advertising systems\nincluding Microsoft Bing. To leverage powerful transformers like BERT in this\nlow-latency setting, many existing approaches perform ad-side computations\noffline. While efficient, these approaches are unable to serve cold start ads,\nresulting in poor relevance predictions for such ads. This work aims to design\na new, low-latency BERT via structured pruning to empower real-time online\ninference for cold start ads relevance on a CPU platform. Our challenge is that\nprevious methods typically prune all layers of the transformer to a high,\nuniform sparsity, thereby producing models which cannot achieve satisfactory\ninference speed with an acceptable accuracy.\n  In this paper, we propose SwiftPruner - an efficient framework that leverages\nevolution-based search to automatically find the best-performing layer-wise\nsparse BERT model under the desired latency constraint. Different from existing\nevolution algorithms that conduct random mutations, we propose a reinforced\nmutator with a latency-aware multi-objective reward to conduct better mutations\nfor efficiently searching the large space of layer-wise sparse models.\nExtensive experiments demonstrate that our method consistently achieves higher\nROC AUC and lower latency than the uniform sparse baseline and state-of-the-art\nsearch methods. Remarkably, under our latency requirement of 1900us on CPU,\nSwiftPruner achieves a 0.86% higher AUC than the state-of-the-art uniform\nsparse baseline for BERT-Mini on a large scale real-world dataset. Online A/B\ntesting shows that our model also achieves a significant 11.7% cut in the ratio\nof defective cold start ads with satisfactory real-time serving latency.", "published": "2022-08-30 03:05:56", "link": "http://arxiv.org/abs/2209.00625v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Flexible Job Classification with Zero-Shot Learning", "abstract": "Using a taxonomy to organize information requires classifying objects\n(documents, images, etc) with appropriate taxonomic classes. The flexible\nnature of zero-shot learning is appealing for this task because it allows\nclassifiers to naturally adapt to taxonomy modifications. This work studies\nzero-shot multi-label document classification with fine-tuned language models\nunder realistic taxonomy expansion scenarios in the human resource domain.\nExperiments show that zero-shot learning can be highly effective in this\nsetting. When controlling for training data budget, zero-shot classifiers\nachieve a 12% relative increase in macro-AP when compared to a traditional\nmulti-label classifier trained on all classes. Counterintuitively, these\nresults suggest in some settings it would be preferable to adopt zero-shot\ntechniques and spend resources annotating more documents with an incomplete set\nof classes, rather than spreading the labeling budget uniformly over all\nclasses and using traditional classification techniques. Additional experiments\ndemonstrate that adopting the well-known filter/re-rank decomposition from the\nrecommender systems literature can significantly reduce the computational\nburden of high-performance zero-shot classifiers, empirically resulting in a\n98% reduction in computational overhead for only a 2% relative decrease in\nperformance. The evidence presented here demonstrates that zero-shot learning\nhas the potential to significantly increase the flexibility of taxonomies and\nhighlights directions for future research.", "published": "2022-08-30 18:18:21", "link": "http://arxiv.org/abs/2209.12678v1", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "MeloForm: Generating Melody with Musical Form based on Expert Systems\n  and Neural Networks", "abstract": "Human usually composes music by organizing elements according to the musical\nform to express music ideas. However, for neural network-based music\ngeneration, it is difficult to do so due to the lack of labelled data on\nmusical form. In this paper, we develop MeloForm, a system that generates\nmelody with musical form using expert systems and neural networks.\nSpecifically, 1) we design an expert system to generate a melody by developing\nmusical elements from motifs to phrases then to sections with repetitions and\nvariations according to pre-given musical form; 2) considering the generated\nmelody is lack of musical richness, we design a Transformer based refinement\nmodel to improve the melody without changing its musical form. MeloForm enjoys\nthe advantages of precise musical form control by expert systems and musical\nrichness learning via neural models. Both subjective and objective experimental\nevaluations demonstrate that MeloForm generates melodies with precise musical\nform control with 97.79% accuracy, and outperforms baseline systems in terms of\nsubjective evaluation score by 0.75, 0.50, 0.86 and 0.89 in structure,\nthematic, richness and overall quality, without any labelled musical form data.\nBesides, MeloForm can support various kinds of forms, such as verse and chorus\nform, rondo form, variational form, sonata form, etc.", "published": "2022-08-30 15:44:15", "link": "http://arxiv.org/abs/2208.14345v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Do language models make human-like predictions about the coreferents of\n  Italian anaphoric zero pronouns?", "abstract": "Some languages allow arguments to be omitted in certain contexts. Yet human\nlanguage comprehenders reliably infer the intended referents of these zero\npronouns, in part because they construct expectations about which referents are\nmore likely. We ask whether Neural Language Models also extract the same\nexpectations. We test whether 12 contemporary language models display\nexpectations that reflect human behavior when exposed to sentences with zero\npronouns from five behavioral experiments conducted in Italian by Carminati\n(2005). We find that three models - XGLM 2.9B, 4.5B, and 7.5B - capture the\nhuman behavior from all the experiments, with others successfully modeling some\nof the results. This result suggests that human expectations about coreference\ncan be derived from exposure to language, and also indicates features of\nlanguage models that allow them to better reflect human behavior.", "published": "2022-08-30 22:06:07", "link": "http://arxiv.org/abs/2208.14554v2", "categories": ["cs.CL", "cs.AI", "cs.IT", "cs.LG", "math.IT"], "primary_category": "cs.CL"}
{"title": "Towards robust music source separation on loud commercial music", "abstract": "Nowadays, commercial music has extreme loudness and heavily compressed\ndynamic range compared to the past. Yet, in music source separation, these\ncharacteristics have not been thoroughly considered, resulting in the domain\nmismatch between the laboratory and the real world. In this paper, we confirmed\nthat this domain mismatch negatively affect the performance of the music source\nseparation networks. To this end, we first created the out-of-domain evaluation\ndatasets, musdb-L and XL, by mimicking the music mastering process. Then, we\nquantitatively verify that the performance of the state-of-the-art algorithms\nsignificantly deteriorated in our datasets. Lastly, we proposed LimitAug data\naugmentation method to reduce the domain mismatch, which utilizes an online\nlimiter during the training data sampling process. We confirmed that it not\nonly alleviates the performance degradation on our out-of-domain datasets, but\nalso results in higher performance on in-domain data.", "published": "2022-08-30 16:00:51", "link": "http://arxiv.org/abs/2208.14355v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Classify Respiratory Abnormality in Lung Sounds Using STFT and a\n  Fine-Tuned ResNet18 Network", "abstract": "Recognizing patterns in lung sounds is crucial to detecting and monitoring\nrespiratory diseases. Current techniques for analyzing respiratory sounds\ndemand domain experts and are subject to interpretation. Hence an accurate and\nautomatic respiratory sound classification system is desired. In this work, we\ntook a data-driven approach to classify abnormal lung sounds. We compared the\nperformance using three different feature extraction techniques, which are\nshort-time Fourier transformation (STFT), Mel spectrograms, and Wav2vec, as\nwell as three different classifiers, including pre-trained ResNet18, LightCNN,\nand Audio Spectrogram Transformer. Our key contributions include the\nbench-marking of different audio feature extractors and neural network based\nclassifiers, and the implementation of a complete pipeline using STFT and a\nfine-tuned ResNet18 network. The proposed method achieved Harmonic Scores of\n0.89, 0.80, 0.71, 0.36 for tasks 1-1, 1-2, 2-1 and 2-2, respectively on the\ntesting sets in the IEEE BioCAS 2022 Grand Challenge on Respiratory Sound\nClassification.", "published": "2022-08-30 01:09:34", "link": "http://arxiv.org/abs/2208.13943v1", "categories": ["eess.AS", "cs.SD", "eess.SP", "q-bio.QM"], "primary_category": "eess.AS"}
{"title": "Video-based Cross-modal Auxiliary Network for Multimodal Sentiment\n  Analysis", "abstract": "Multimodal sentiment analysis has a wide range of applications due to its\ninformation complementarity in multimodal interactions. Previous works focus\nmore on investigating efficient joint representations, but they rarely consider\nthe insufficient unimodal features extraction and data redundancy of multimodal\nfusion. In this paper, a Video-based Cross-modal Auxiliary Network (VCAN) is\nproposed, which is comprised of an audio features map module and a cross-modal\nselection module. The first module is designed to substantially increase\nfeature diversity in audio feature extraction, aiming to improve classification\naccuracy by providing more comprehensive acoustic representations. To empower\nthe model to handle redundant visual features, the second module is addressed\nto efficiently filter the redundant visual frames during integrating\naudiovisual data. Moreover, a classifier group consisting of several image\nclassification networks is introduced to predict sentiment polarities and\nemotion categories. Extensive experimental results on RAVDESS, CMU-MOSI, and\nCMU-MOSEI benchmarks indicate that VCAN is significantly superior to the\nstate-of-the-art methods for improving the classification accuracy of\nmultimodal sentiment analysis.", "published": "2022-08-30 02:08:06", "link": "http://arxiv.org/abs/2208.13954v1", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Gridless 3D Recovery of Image Sources from Room Impulse Responses", "abstract": "Given a sound field generated by a sparse distribution of impulse image\nsources, can the continuous 3D positions and amplitudes of these sources be\nrecovered from discrete, bandlimited measurements of the field at a finite set\nof locations, e.g., a multichannel room impulse response? Borrowing from recent\nadvances in super-resolution imaging, it is shown that this nonlinear,\nnon-convex inverse problem can be efficiently relaxed into a convex linear\ninverse problem over the space of Radon measures in R3. The linear operator\nintroduced here stems from the fundamental solution of the free-field\ninhomogenous wave equation combined with the receivers' responses. An\nadaptation of the Sliding Frank-Wolfe algorithm is proposed to numerically\nsolve the problem off-the-grid, i.e., in continuous 3D space. Simulated\nexperiments show that the approach achieves near-exact recovery of hundreds of\nimage sources using an arbitrarily placed compact 32-channel spherical\nmicrophone array in random rectangular rooms. The impact of noise, sampling\nrate and array diameter on these results is also examined.", "published": "2022-08-30 06:46:10", "link": "http://arxiv.org/abs/2208.14017v2", "categories": ["cs.SD", "eess.AS", "eess.SP", "physics.class-ph"], "primary_category": "cs.SD"}
{"title": "A Study on the relationship between the geometrical shapes and the\n  biometrical acoustic characteristics of human ear canal", "abstract": "Ear acoustic authentication is a new biometrics method and it utilizes the\ndifferences in acoustic characteristics of the ear canal between users.\nHowever, there have been few reports on the factors that cause differences in\nthe acoustic characteristics. We investigate the relationship between ear canal\nshapes and acoustic characteristics in terms of user-to-user similarity. We\nused magnetic resonance imaging (MRI) to measure ear canal geometry. As a\nresult, the correlation coefficient between shape similarity and acoustic\ncharacteristic similarity is higher than 0.7 and the coefficient of\ndetermination is higher than 0.5. This suggests that the difference in the\nshape of the ear canal is one of the important factors.", "published": "2022-08-30 11:58:57", "link": "http://arxiv.org/abs/2208.14182v2", "categories": ["eess.SP", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
{"title": "HPPNet: Modeling the Harmonic Structure and Pitch Invariance in Piano\n  Transcription", "abstract": "While neural network models are making significant progress in piano\ntranscription, they are becoming more resource-consuming due to requiring\nlarger model size and more computing power. In this paper, we attempt to apply\nmore prior about piano to reduce model size and improve the transcription\nperformance. The sound of a piano note contains various overtones, and the\npitch of a key does not change over time. To make full use of such latent\ninformation, we propose HPPNet that using the Harmonic Dilated Convolution to\ncapture the harmonic structures and the Frequency Grouped Recurrent Neural\nNetwork to model the pitch-invariance over time. Experimental results on the\nMAESTRO dataset show that our piano transcription system achieves\nstate-of-the-art performance both in frame and note scores (frame F1 93.15%,\nnote F1 97.18%). Moreover, the model size is much smaller than the previous\nstate-of-the-art deep learning models.", "published": "2022-08-30 15:32:52", "link": "http://arxiv.org/abs/2208.14339v2", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
