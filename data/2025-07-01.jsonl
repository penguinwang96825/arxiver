{"title": "The Medium Is Not the Message: Deconfounding Text Embeddings via Linear Concept Erasure", "abstract": "Embedding-based similarity metrics between text sequences can be influenced\nnot just by the content dimensions we most care about, but can also be biased\nby spurious attributes like the text's source or language. These document\nconfounders cause problems for many applications, but especially those that\nneed to pool texts from different corpora. This paper shows that a debiasing\nalgorithm that removes information about observed confounders from the encoder\nrepresentations substantially reduces these biases at a minimal computational\ncost. Document similarity and clustering metrics improve across every embedding\nvariant and task we evaluate -- often dramatically. Interestingly, performance\non out-of-distribution benchmarks is not impacted, indicating that the\nembeddings are not otherwise degraded.", "published": "2025-07-01 23:17:12", "link": "http://arxiv.org/abs/2507.01234v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MEGA: xLSTM with Multihead Exponential Gated Fusion for Precise Aspect-based Sentiment Analysis", "abstract": "Aspect-based Sentiment Analysis (ABSA) is a critical Natural Language\nProcessing (NLP) task that extracts aspects from text and determines their\nassociated sentiments, enabling fine-grained analysis of user opinions.\nExisting ABSA methods struggle to balance computational efficiency with high\nperformance: deep learning models often lack global context, transformers\ndemand significant computational resources, and Mamba-based approaches face\nCUDA dependency and diminished local correlations. Recent advancements in\nExtended Long Short-Term Memory (xLSTM) models, particularly their efficient\nmodeling of long-range dependencies, have significantly advanced the NLP\ncommunity. However, their potential in ABSA remains untapped. To this end, we\npropose xLSTM with Multihead Exponential Gated Fusion (MEGA), a novel framework\nintegrating a bi-directional mLSTM architecture with forward and partially\nflipped backward (PF-mLSTM) streams. The PF-mLSTM enhances localized context\nmodeling by processing the initial sequence segment in reverse with dedicated\nparameters, preserving critical short-range patterns. We further introduce an\nmLSTM-based multihead cross exponential gated fusion mechanism (MECGAF) that\ndynamically combines forward mLSTM outputs as query and key with PF-mLSTM\noutputs as value, optimizing short-range dependency capture while maintaining\nglobal context and efficiency. Experimental results on three benchmark datasets\ndemonstrate that MEGA outperforms state-of-the-art baselines, achieving\nsuperior accuracy and efficiency in ABSA tasks.", "published": "2025-07-01 22:21:33", "link": "http://arxiv.org/abs/2507.01213v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Matching and Linking Entries in Historical Swedish Encyclopedias", "abstract": "The \\textit{Nordisk familjebok} is a Swedish encyclopedia from the 19th and\n20th centuries. It was written by a team of experts and aimed to be an\nintellectual reference, stressing precision and accuracy. This encyclopedia had\nfour main editions remarkable by their size, ranging from 20 to 38 volumes. As\na consequence, the \\textit{Nordisk familjebok} had a considerable influence in\nuniversities, schools, the media, and society overall. As new editions were\nreleased, the selection of entries and their content evolved, reflecting\nintellectual changes in Sweden.\n  In this paper, we used digitized versions from \\textit{Project Runeberg}. We\nfirst resegmented the raw text into entries and matched pairs of entries\nbetween the first and second editions using semantic sentence embeddings. We\nthen extracted the geographical entries from both editions using a\ntransformer-based classifier and linked them to Wikidata. This enabled us to\nidentify geographic trends and possible shifts between the first and second\neditions, written between 1876-1899 and 1904-1926, respectively.\n  Interpreting the results, we observe a small but significant shift in\ngeographic focus away from Europe and towards North America, Africa, Asia,\nAustralia, and northern Scandinavia from the first to the second edition,\nconfirming the influence of the First World War and the rise of new powers. The\ncode and data are available on GitHub at\nhttps://github.com/sibbo/nordisk-familjebok.", "published": "2025-07-01 20:17:19", "link": "http://arxiv.org/abs/2507.01170v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Event-based evaluation of abstractive news summarization", "abstract": "An abstractive summary of a news article contains its most important\ninformation in a condensed version. The evaluation of automatically generated\nsummaries by generative language models relies heavily on human-authored\nsummaries as gold references, by calculating overlapping units or similarity\nscores. News articles report events, and ideally so should the summaries. In\nthis work, we propose to evaluate the quality of abstractive summaries by\ncalculating overlapping events between generated summaries, reference\nsummaries, and the original news articles. We experiment on a richly annotated\nNorwegian dataset comprising both events annotations and summaries authored by\nexpert human annotators. Our approach provides more insight into the event\ninformation contained in the summaries.", "published": "2025-07-01 19:49:23", "link": "http://arxiv.org/abs/2507.01160v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SciArena: An Open Evaluation Platform for Foundation Models in Scientific Literature Tasks", "abstract": "We present SciArena, an open and collaborative platform for evaluating\nfoundation models on scientific literature tasks. Unlike traditional benchmarks\nfor scientific literature understanding and synthesis, SciArena engages the\nresearch community directly, following the Chatbot Arena evaluation approach of\ncommunity voting on model comparisons. By leveraging collective intelligence,\nSciArena offers a community-driven evaluation of model performance on\nopen-ended scientific tasks that demand literature-grounded, long-form\nresponses. The platform currently supports 23 open-source and proprietary\nfoundation models and has collected over 13,000 votes from trusted researchers\nacross diverse scientific domains. We analyze the data collected so far and\nconfirm that the submitted questions are diverse, aligned with real-world\nliterature needs, and that participating researchers demonstrate strong\nself-consistency and inter-annotator agreement in their evaluations. We discuss\nthe results and insights based on the model ranking leaderboard. To further\npromote research in building model-based automated evaluation systems for\nliterature tasks, we release SciArena-Eval, a meta-evaluation benchmark based\non our collected preference data. The benchmark measures the accuracy of models\nin judging answer quality by comparing their pairwise assessments with human\nvotes. Our experiments highlight the benchmark's challenges and emphasize the\nneed for more reliable automated evaluation methods.", "published": "2025-07-01 17:51:59", "link": "http://arxiv.org/abs/2507.01001v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "La Leaderboard: A Large Language Model Leaderboard for Spanish Varieties and Languages of Spain and Latin America", "abstract": "Leaderboards showcase the current capabilities and limitations of Large\nLanguage Models (LLMs). To motivate the development of LLMs that represent the\nlinguistic and cultural diversity of the Spanish-speaking community, we present\nLa Leaderboard, the first open-source leaderboard to evaluate generative LLMs\nin languages and language varieties of Spain and Latin America. La Leaderboard\nis a community-driven project that aims to establish an evaluation standard for\neveryone interested in developing LLMs for the Spanish-speaking community. This\ninitial version combines 66 datasets in Basque, Catalan, Galician, and\ndifferent Spanish varieties, showcasing the evaluation results of 50 models. To\nencourage community-driven development of leaderboards in other languages, we\nexplain our methodology, including guidance on selecting the most suitable\nevaluation setup for each downstream task. In particular, we provide a\nrationale for using fewer few-shot examples than typically found in the\nliterature, aiming to reduce environmental impact and facilitate access to\nreproducible results for a broader research community.", "published": "2025-07-01 17:50:48", "link": "http://arxiv.org/abs/2507.00999v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Should We Still Pretrain Encoders with Masked Language Modeling?", "abstract": "Learning high-quality text representations is fundamental to a wide range of\nNLP tasks. While encoder pretraining has traditionally relied on Masked\nLanguage Modeling (MLM), recent evidence suggests that decoder models\npretrained with Causal Language Modeling (CLM) can be effectively repurposed as\nencoders, often surpassing traditional encoders on text representation\nbenchmarks. However, it remains unclear whether these gains reflect an inherent\nadvantage of the CLM objective or arise from confounding factors such as model\nand data scale. In this paper, we address this question through a series of\nlarge-scale, carefully controlled pretraining ablations, training a total of 30\nmodels ranging from 210 million to 1 billion parameters, and conducting over\n15,000 fine-tuning and evaluation runs. We find that while training with MLM\ngenerally yields better performance across text representation tasks,\nCLM-trained models are more data-efficient and demonstrate improved fine-tuning\nstability. Building on these findings, we experimentally show that a biphasic\ntraining strategy that sequentially applies CLM and then MLM, achieves optimal\nperformance under a fixed computational training budget. Moreover, we\ndemonstrate that this strategy becomes more appealing when initializing from\nreadily available pretrained CLM models (from the existing LLM ecosystem),\nreducing the computational burden needed to train best-in-class encoder models.\nWe release all project artifacts at https://hf.co/MLMvsCLM to foster further\nresearch.", "published": "2025-07-01 17:45:48", "link": "http://arxiv.org/abs/2507.00994v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Discourse Heuristics For Paradoxically Moral Self-Correction", "abstract": "Moral self-correction has emerged as a promising approach for aligning the\noutput of Large Language Models (LLMs) with human moral values. However, moral\nself-correction techniques are subject to two primary paradoxes. First, despite\nempirical and theoretical evidence to support the effectiveness of\nself-correction, this LLM capability only operates at a superficial level.\nSecond, while LLMs possess the capability of self-diagnosing immoral aspects of\ntheir output, they struggle to identify the cause of this moral inconsistency\nduring their self-correction process. To better understand and address these\nparadoxes, we analyze the discourse constructions in fine-tuning corpora\ndesigned to enhance moral self-correction, uncovering the existence of the\nheuristics underlying effective constructions. We demonstrate that moral\nself-correction relies on discourse constructions that reflect heuristic\nshortcuts, and that the presence of these heuristic shortcuts during\nself-correction leads to inconsistency when attempting to enhance both\nself-correction and self-diagnosis capabilities jointly. Based on our findings,\nwe propose a solution to improve moral self-correction by leveraging the\nheuristics of curated datasets. We also highlight the generalization challenges\nof this capability, particularly in terms of learning from situated context and\nmodel scales.", "published": "2025-07-01 17:36:41", "link": "http://arxiv.org/abs/2507.00985v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing LLM Agent Safety via Causal Influence Prompting", "abstract": "As autonomous agents powered by large language models (LLMs) continue to\ndemonstrate potential across various assistive tasks, ensuring their safe and\nreliable behavior is crucial for preventing unintended consequences. In this\nwork, we introduce CIP, a novel technique that leverages causal influence\ndiagrams (CIDs) to identify and mitigate risks arising from agent\ndecision-making. CIDs provide a structured representation of cause-and-effect\nrelationships, enabling agents to anticipate harmful outcomes and make safer\ndecisions. Our approach consists of three key steps: (1) initializing a CID\nbased on task specifications to outline the decision-making process, (2)\nguiding agent interactions with the environment using the CID, and (3)\niteratively refining the CID based on observed behaviors and outcomes.\nExperimental results demonstrate that our method effectively enhances safety in\nboth code execution and mobile device control tasks.", "published": "2025-07-01 17:31:51", "link": "http://arxiv.org/abs/2507.00979v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "The Cognate Data Bottleneck in Language Phylogenetics", "abstract": "To fully exploit the potential of computational phylogenetic methods for\ncognate data one needs to leverage specific (complex) models an machine\nlearning-based techniques. However, both approaches require datasets that are\nsubstantially larger than the manually collected cognate data currently\navailable. To the best of our knowledge, there exists no feasible approach to\nautomatically generate larger cognate datasets. We substantiate this claim by\nautomatically extracting datasets from BabelNet, a large multilingual\nencyclopedic dictionary. We demonstrate that phylogenetic inferences on the\nrespective character matrices yield trees that are largely inconsistent with\nthe established gold standard ground truth trees. We also discuss why we\nconsider it as being unlikely to be able to extract more suitable character\nmatrices from other multilingual resources. Phylogenetic data analysis\napproaches that require larger datasets can therefore not be applied to cognate\ndata. Thus, it remains an open question how, and if these computational\napproaches can be applied in historical linguistics.", "published": "2025-07-01 16:14:20", "link": "http://arxiv.org/abs/2507.00911v1", "categories": ["cs.CL", "q-bio.PE"], "primary_category": "cs.CL"}
{"title": "ONLY: One-Layer Intervention Sufficiently Mitigates Hallucinations in Large Vision-Language Models", "abstract": "Recent Large Vision-Language Models (LVLMs) have introduced a new paradigm\nfor understanding and reasoning about image input through textual responses.\nAlthough they have achieved remarkable performance across a range of\nmulti-modal tasks, they face the persistent challenge of hallucination, which\nintroduces practical weaknesses and raises concerns about their reliable\ndeployment in real-world applications. Existing work has explored contrastive\ndecoding approaches to mitigate this issue, where the output of the original\nLVLM is compared and contrasted with that of a perturbed version. However,\nthese methods require two or more queries that slow down LVLM response\ngeneration, making them less suitable for real-time applications. To overcome\nthis limitation, we propose ONLY, a training-free decoding approach that\nrequires only a single query and a one-layer intervention during decoding,\nenabling efficient real-time deployment. Specifically, we enhance textual\noutputs by selectively amplifying crucial textual information using a\ntext-to-visual entropy ratio for each token. Extensive experimental results\ndemonstrate that our proposed ONLY consistently outperforms state-of-the-art\nmethods across various benchmarks while requiring minimal implementation effort\nand computational cost. Code is available at https://github.com/zifuwan/ONLY.", "published": "2025-07-01 16:01:08", "link": "http://arxiv.org/abs/2507.00898v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "MemeCMD: An Automatically Generated Chinese Multi-turn Dialogue Dataset with Contextually Retrieved Memes", "abstract": "Memes are widely used in online social interactions, providing vivid,\nintuitive, and often humorous means to express intentions and emotions.\nExisting dialogue datasets are predominantly limited to either manually\nannotated or pure-text conversations, lacking the expressiveness and contextual\nnuance that multimodal interactions provide.To address these challenges, we\nintroduce MemeCMD, an automatically generated Chinese Multi-turn Dialogue\ndataset with contextually retrieved memes. Our dataset combines a large-scale,\nMLLM-annotated meme library with dialogues auto-generated by dual agents across\ndiverse scenarios. We introduce a retrieval framework and adaptive threshold to\nensure contextually relevant, naturally spaced meme usage. Experiments\ndemonstrate the effectiveness of our approach in generating contextually\nappropriate and diverse meme-incorporated dialogues, offering a scalable and\nprivacy-preserving resource for advancing multimodal conversational AI.", "published": "2025-07-01 15:57:14", "link": "http://arxiv.org/abs/2507.00891v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Scaling Laws Are Unreliable for Downstream Tasks: A Reality Check", "abstract": "Downstream scaling laws aim to predict task performance at larger scales from\npretraining losses at smaller scales. Whether this prediction should be\npossible is unclear: some works demonstrate that task performance follows clear\nlinear scaling trends under transformation, whereas others point out\nfundamental challenges to downstream scaling laws, such as emergence and\ninverse scaling. In this work, we conduct a meta-analysis of existing data on\ndownstream scaling laws, finding that close fit to linear scaling laws only\noccurs in a minority of cases: 39% of the time. Furthermore, seemingly benign\nchanges to the experimental setting can completely change the scaling trend.\nOur analysis underscores the need to understand the conditions under which\nscaling laws succeed. To fully model the relationship between pretraining loss\nand downstream task performance, we must embrace the cases in which scaling\nbehavior deviates from linear trends.", "published": "2025-07-01 15:52:55", "link": "http://arxiv.org/abs/2507.00885v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mathematics Isn't Culture-Free: Probing Cultural Gaps via Entity and Scenario Perturbations", "abstract": "Although mathematics is often considered culturally neutral, the way\nmathematical problems are presented can carry implicit cultural context.\nExisting benchmarks like GSM8K are predominantly rooted in Western norms,\nincluding names, currencies, and everyday scenarios. In this work, we create\nculturally adapted variants of the GSM8K test set for five regions Africa,\nIndia, China, Korea, and Japan using prompt-based transformations followed by\nmanual verification. We evaluate six large language models (LLMs), ranging from\n8B to 72B parameters, across five prompting strategies to assess their\nrobustness to cultural variation in math problem presentation. Our findings\nreveal a consistent performance gap: models perform best on the original\nUS-centric dataset and comparatively worse on culturally adapted versions.\nHowever, models with reasoning capabilities are more resilient to these shifts,\nsuggesting that deeper reasoning helps bridge cultural presentation gaps in\nmathematical tasks", "published": "2025-07-01 15:51:46", "link": "http://arxiv.org/abs/2507.00883v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Verifiable Natural Language to Linear Temporal Logic Translation: A Benchmark Dataset and Evaluation Suite", "abstract": "Empirical evaluation of state-of-the-art natural-language (NL) to\ntemporal-logic (TL) translation systems reveals near-perfect performance on\nexisting benchmarks. However, current studies measure only the accuracy of the\ntranslation of NL logic into formal TL, ignoring a system's capacity to ground\natomic propositions into new scenarios or environments. This is a critical\nfeature, necessary for the verification of resulting formulas in a concrete\nstate space. Consequently, most NL-to-TL translation frameworks propose their\nown bespoke dataset in which the correct grounding is known a-priori, inflating\nperformance metrics and neglecting the need for extensible, domain-general\nsystems. In this paper, we introduce the Verifiable Linear Temporal Logic\nBenchmark ( VLTL-Bench), a unifying benchmark that measures verification and\nverifiability of automated NL-to-LTL translation. The dataset consists of three\nunique state spaces and thousands of diverse natural language specifications\nand corresponding formal specifications in temporal logic. Moreover, the\nbenchmark contains sample traces to validate the temporal logic expressions.\nWhile the benchmark directly supports end-to-end evaluation, we observe that\nmany frameworks decompose the process into i) lifting, ii) grounding, iii)\ntranslation, and iv) verification. The benchmark provides ground truths after\neach of these steps to enable researches to improve and evaluate different\nsubsteps of the overall problem. To encourage methodologically sound advances\nin verifiable NL-to-LTL translation approaches, we release VLTL-Bench here:\nhttps://www.kaggle.com/datasets/dubascudes/vltl bench.", "published": "2025-07-01 15:41:57", "link": "http://arxiv.org/abs/2507.00877v1", "categories": ["eess.SY", "cs.CL", "cs.SY"], "primary_category": "eess.SY"}
{"title": "TransLaw: Benchmarking Large Language Models in Multi-Agent Simulation of the Collaborative Translation", "abstract": "Multi-agent systems empowered by large language models (LLMs) have\ndemonstrated remarkable capabilities in a wide range of downstream\napplications, including machine translation. However, the potential of LLMs in\ntranslating Hong Kong legal judgments remains uncertain due to challenges such\nas intricate legal terminology, culturally embedded nuances, and strict\nlinguistic structures. In this work, we introduce TransLaw, a novel multi-agent\nframework implemented for real-world Hong Kong case law translation. It employs\nthree specialized agents, namely, Translator, Annotator, and Proofreader, to\ncollaboratively produce translations for high accuracy in legal meaning,\nappropriateness in style, and adequate coherence and cohesion in structure.\nThis framework supports customizable LLM configurations and achieves tremendous\ncost reduction compared to professional human translation services. We\nevaluated its performance using 13 open-source and commercial LLMs as agents\nand obtained interesting findings, including that it surpasses GPT-4o in legal\nsemantic accuracy, structural coherence, and stylistic fidelity, yet trails\nhuman experts in contextualizing complex terminology and stylistic naturalness.\nOur platform website is available at CityUHK, and our bilingual judgment corpus\nused for the evaluation is available at Hugging Face.", "published": "2025-07-01 15:39:26", "link": "http://arxiv.org/abs/2507.00875v1", "categories": ["cs.CL", "cs.HC", "cs.MA"], "primary_category": "cs.CL"}
{"title": "Stylometry recognizes human and LLM-generated texts in short samples", "abstract": "The paper explores stylometry as a method to distinguish between texts\ncreated by Large Language Models (LLMs) and humans, addressing issues of model\nattribution, intellectual property, and ethical AI use. Stylometry has been\nused extensively to characterise the style and attribute authorship of texts.\nBy applying it to LLM-generated texts, we identify their emergent writing\npatterns. The paper involves creating a benchmark dataset based on Wikipedia,\nwith (a) human-written term summaries, (b) texts generated purely by LLMs\n(GPT-3.5/4, LLaMa 2/3, Orca, and Falcon), (c) processed through multiple text\nsummarisation methods (T5, BART, Gensim, and Sumy), and (d) rephrasing methods\n(Dipper, T5). The 10-sentence long texts were classified by tree-based models\n(decision trees and LightGBM) using human-designed (StyloMetrix) and\nn-gram-based (our own pipeline) stylometric features that encode lexical,\ngrammatical, syntactic, and punctuation patterns. The cross-validated results\nreached a performance of up to .87 Matthews correlation coefficient in the\nmulticlass scenario with 7 classes, and accuracy between .79 and 1. in binary\nclassification, with the particular example of Wikipedia and GPT-4 reaching up\nto .98 accuracy on a balanced dataset. Shapley Additive Explanations pinpointed\nfeatures characteristic of the encyclopaedic text type, individual overused\nwords, as well as a greater grammatical standardisation of LLMs with respect to\nhuman-written texts. These results show -- crucially, in the context of the\nincreasingly sophisticated LLMs -- that it is possible to distinguish machine-\nfrom human-generated texts at least for a well-defined text type.", "published": "2025-07-01 15:08:53", "link": "http://arxiv.org/abs/2507.00838v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ProxAnn: Use-Oriented Evaluations of Topic Models and Document Clustering", "abstract": "Topic model and document-clustering evaluations either use automated metrics\nthat align poorly with human preferences or require expert labels that are\nintractable to scale. We design a scalable human evaluation protocol and a\ncorresponding automated approximation that reflect practitioners' real-world\nusage of models. Annotators -- or an LLM-based proxy -- review text items\nassigned to a topic or cluster, infer a category for the group, then apply that\ncategory to other documents. Using this protocol, we collect extensive\ncrowdworker annotations of outputs from a diverse set of topic models on two\ndatasets. We then use these annotations to validate automated proxies, finding\nthat the best LLM proxies are statistically indistinguishable from a human\nannotator and can therefore serve as a reasonable substitute in automated\nevaluations. Package, web interface, and data are at\nhttps://github.com/ahoho/proxann", "published": "2025-07-01 15:00:55", "link": "http://arxiv.org/abs/2507.00828v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Many LLMs Are More Utilitarian Than One", "abstract": "Moral judgment is integral to large language model (LLM) alignment and social\nreasoning. As multi-agent systems gain prominence, it becomes crucial to\nunderstand how LLMs function collectively during collaboration, compared to\nindividual agents. In human moral judgment, group deliberation leads to a\nutilitarian boost: a tendency to endorse norm violations that maximize benefits\nfor the greatest number of people despite harms. We study whether a similar\ndynamic emerges in multi-agent LLM systems. We tested six models on\nwell-established sets of moral dilemmas across two conditions: (1) Solo, where\nmodels reasoned independently, and (2) Group, where they engaged in multi-turn\ndiscussions in pairs or triads. In personal moral dilemmas, where agents must\ndecide to directly harm one individual to maximize the utility for others, all\nmodels found moral violations to be more acceptable when part of a group than\nindividually, similar to human experiments. Some models endorsed actions that\nmaximized overall well-being, even if they benefited strangers over familiar\nindividuals. Others became more willing to violate moral norms in groups.\nHowever, while human groups show a similar action bias, the mechanism for their\nutilitarian boost differs from LLMs. Whereas the human shift comes from\nheightened sensitivity to decision outcomes, LLM groups show either reduced\nnorm sensitivity or enhanced impartiality. This suggests that while the surface\nbehavior of LLM collectives mimics human group reasoning, the underlying\ndrivers differ. We discuss the implications for AI alignment, multi-agent\ndesign, and artificial moral reasoning.", "published": "2025-07-01 14:46:16", "link": "http://arxiv.org/abs/2507.00814v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.7; I.2.11"], "primary_category": "cs.CL"}
{"title": "Multi-interaction TTS toward professional recording reproduction", "abstract": "Voice directors often iteratively refine voice actors' performances by\nproviding feedback to achieve the desired outcome. While this iterative\nfeedback-based refinement process is important in actual recordings, it has\nbeen overlooked in text-to-speech synthesis (TTS). As a result, fine-grained\nstyle refinement after the initial synthesis is not possible, even though the\nsynthesized speech often deviates from the user's intended style. To address\nthis issue, we propose a TTS method with multi-step interaction that allows\nusers to intuitively and rapidly refine synthesized speech. Our approach models\nthe interaction between the TTS model and its user to emulate the relationship\nbetween voice actors and voice directors. Experiments show that the proposed\nmodel with its corresponding dataset enables iterative style refinements in\naccordance with users' directions, thus demonstrating its multi-interaction\ncapability. Sample audios are available:\nhttps://ntt-hilab-gensp.github.io/ssw13multiinteractiontts/", "published": "2025-07-01 14:40:51", "link": "http://arxiv.org/abs/2507.00808v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Generative AI and the future of scientometrics: current topics and future questions", "abstract": "The aim of this paper is to review the use of GenAI in scientometrics, and to\nbegin a debate on the broader implications for the field. First, we provide an\nintroduction on GenAI's generative and probabilistic nature as rooted in\ndistributional linguistics. And we relate this to the debate on the extent to\nwhich GenAI might be able to mimic human 'reasoning'. Second, we leverage this\ndistinction for a critical engagement with recent experiments using GenAI in\nscientometrics, including topic labelling, the analysis of citation contexts,\npredictive applications, scholars' profiling, and research assessment. GenAI\nshows promise in tasks where language generation dominates, such as labelling,\nbut faces limitations in tasks that require stable semantics, pragmatic\nreasoning, or structured domain knowledge. However, these results might become\nquickly outdated. Our recommendation is, therefore, to always strive to\nsystematically compare the performance of different GenAI models for specific\ntasks. Third, we inquire whether, by generating large amounts of scientific\nlanguage, GenAI might have a fundamental impact on our field by affecting\ntextual characteristics used to measure science, such as authors, words, and\nreferences. We argue that careful empirical work and theoretical reflection\nwill be essential to remain capable of interpreting the evolving patterns of\nknowledge production.", "published": "2025-07-01 14:22:16", "link": "http://arxiv.org/abs/2507.00783v1", "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.CL"}
{"title": "A Diagrammatic Calculus for a Functional Model of Natural Language Semantics", "abstract": "In this paper, we study a functional programming approach to natural language\nsemantics, allowing us to increase the expressivity of a more traditional\ndenotation style. We will formalize a category based type and effect system,\nand construct a diagrammatic calculus to model parsing and handling of effects,\nand use it to efficiently compute the denotations for sentences.", "published": "2025-07-01 14:21:20", "link": "http://arxiv.org/abs/2507.00782v1", "categories": ["cs.CL", "cs.PL", "J.5; D.3.1; D.3.3"], "primary_category": "cs.CL"}
{"title": "LitBench: A Benchmark and Dataset for Reliable Evaluation of Creative Writing", "abstract": "Evaluating creative writing generated by large language models (LLMs) remains\nchallenging because open-ended narratives lack ground truths. Without\nperformant automated evaluation methods, off-the-shelf (OTS) language models\nare employed as zero-shot judges, yet their reliability is unclear in this\ncontext. In pursuit of robust evaluation for creative writing, we introduce\nLitBench, the first standardized benchmark and paired dataset for creative\nwriting verification, comprising a held-out test set of 2,480 debiased,\nhuman-labeled story comparisons drawn from Reddit and a 43,827-pair training\ncorpus of human preference labels. Using LitBench, we (i) benchmark zero-shot\nLLM judges, (ii) train Bradley Terry and generative reward models, and (iii)\nconduct an online human study to validate reward model rankings on newly\nLLM-generated stories. Our benchmark identifies Claude-3.7-Sonnet as the\nstrongest off-the-shelf judge, reaching 73% agreement with human preferences;\namong trained reward models, Bradley-Terry and Generative reward models both\nattain an accuracy of 78%, outperforming all off-the-shelf judges. An online\nhuman study further confirms that our trained reward models consistently align\nwith human preferences in novel LLM-generated stories. We release LitBench and\nreward models at\nhttps://huggingface.co/collections/SAA-Lab/litbench-68267b5da3aafe58f9e43461,\nproviding a vetted resource for reliable, automated evaluation and optimization\nof creative writing systems.", "published": "2025-07-01 14:10:36", "link": "http://arxiv.org/abs/2507.00769v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Safe Low Bandwidth SPV: A Formal Treatment of Simplified Payment Verification Protocols and Security Bounds", "abstract": "This paper presents a complete formal specification, protocol description,\nand mathematical proof structure for Simplified Payment Verification (SPV) as\noriginally defined in the Bitcoin whitepaper \\cite{nakamoto2008}. In stark\ncontrast to the misrepresentations proliferated by popular implementations, we\nshow that SPV is not only secure under bounded adversarial assumptions but\nstrictly optimal for digital cash systems requiring scalable and verifiable\ntransaction inclusion. We reconstruct the SPV protocol from first principles,\ngrounding its verification model in symbolic automata, Merkle membership\nrelations, and chain-of-proof dominance predicates. Through rigorous\nprobabilistic and game-theoretic analysis, we derive the economic bounds within\nwhich the protocol operates securely and verify its liveness and safety\nproperties under partial connectivity, hostile relay networks, and adversarial\npropagation delay. Our specification further introduces low-bandwidth\noptimisations such as adaptive polling and compressed header synchronisation\nwhile preserving correctness. This document serves both as a blueprint for\nsecure SPV implementation and a rebuttal of common misconceptions surrounding\nnon-validating clients.", "published": "2025-07-01 13:44:48", "link": "http://arxiv.org/abs/2507.00740v1", "categories": ["cs.CR", "cs.CL", "cs.DC", "68Q85, 68M10, 94A60, 91A80, 68Q17, 68W10, 68R10", "C.2.2; F.2.2; D.4.6; K.6.5"], "primary_category": "cs.CR"}
{"title": "AI Analyst: Framework and Comprehensive Evaluation of Large Language Models for Financial Time Series Report Generation", "abstract": "This paper explores the potential of large language models (LLMs) to generate\nfinancial reports from time series data. We propose a framework encompassing\nprompt engineering, model selection, and evaluation. We introduce an automated\nhighlighting system to categorize information within the generated reports,\ndifferentiating between insights derived directly from time series data,\nstemming from financial reasoning, and those reliant on external knowledge.\nThis approach aids in evaluating the factual grounding and reasoning\ncapabilities of the models. Our experiments, utilizing both data from the real\nstock market indices and synthetic time series, demonstrate the capability of\nLLMs to produce coherent and informative financial reports.", "published": "2025-07-01 12:57:18", "link": "http://arxiv.org/abs/2507.00718v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contrasting Cognitive Styles in Vision-Language Models: Holistic Attention in Japanese Versus Analytical Focus in English", "abstract": "Cross-cultural research in perception and cognition has shown that\nindividuals from different cultural backgrounds process visual information in\ndistinct ways. East Asians, for example, tend to adopt a holistic perspective,\nattending to contextual relationships, whereas Westerners often employ an\nanalytical approach, focusing on individual objects and their attributes. In\nthis study, we investigate whether Vision-Language Models (VLMs) trained\npredominantly on different languages, specifically Japanese and English,\nexhibit similar culturally grounded attentional patterns. Using comparative\nanalysis of image descriptions, we examine whether these models reflect\ndifferences in holistic versus analytic tendencies. Our findings suggest that\nVLMs not only internalize the structural properties of language but also\nreproduce cultural behaviors embedded in the training data, indicating that\ncultural cognition may implicitly shape model outputs.", "published": "2025-07-01 11:56:45", "link": "http://arxiv.org/abs/2507.00700v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Large Language Models for Spontaneous Speech-Based Suicide Risk Detection", "abstract": "Early identification of suicide risk is crucial for preventing suicidal\nbehaviors. As a result, the identification and study of patterns and markers\nrelated to suicide risk have become a key focus of current research. In this\npaper, we present the results of our work in the 1st SpeechWellness Challenge\n(SW1), which aims to explore speech as a non-invasive and easily accessible\nmental health indicator for identifying adolescents at risk of suicide.Our\napproach leverages large language model (LLM) as the primary tool for feature\nextraction, alongside conventional acoustic and semantic features. The proposed\nmethod achieves an accuracy of 74\\% on the test set, ranking first in the SW1\nchallenge. These findings demonstrate the potential of LLM-based methods for\nanalyzing speech in the context of suicide risk assessment.", "published": "2025-07-01 11:45:23", "link": "http://arxiv.org/abs/2507.00693v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SAFER: Probing Safety in Reward Models with Sparse Autoencoder", "abstract": "Reinforcement learning from human feedback (RLHF) is a key paradigm for\naligning large language models (LLMs) with human values, yet the reward models\nat its core remain largely opaque. In this work, we present sparse Autoencoder\nFor Enhanced Reward model (\\textbf{SAFER}), a novel framework for interpreting\nand improving reward models through mechanistic analysis. Leveraging Sparse\nAutoencoders (SAEs), we uncover human-interpretable features in reward model\nactivations, enabling insight into safety-relevant decision-making. We apply\nSAFER to safety-oriented preference datasets and quantify the salience of\nindividual features by activation differences between chosen and rejected\nresponses. Using these feature-level signals, we design targeted data poisoning\nand denoising strategies. Experiments show that SAFER can precisely degrade or\nenhance safety alignment with minimal data modification, without sacrificing\ngeneral chat performance. Our approach contributes to interpreting, auditing\nand refining reward models in high-stakes LLM alignment tasks. Our codes are\navailable at https://github.com/xzy-101/SAFER-code. \\textit{This paper\ndiscusses topics related to large language model safety and may include\ndiscussions or examples that highlight potential risks or unsafe outcomes.}", "published": "2025-07-01 11:04:03", "link": "http://arxiv.org/abs/2507.00665v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mixture of Reasonings: Teach Large Language Models to Reason with Adaptive Strategies", "abstract": "Large language models (LLMs) excel in complex tasks through advanced\nprompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT), but\ntheir reliance on manually crafted, task-specific prompts limits adaptability\nand efficiency. We introduce Mixture of Reasoning (MoR), a training framework\nthat embeds diverse reasoning strategies into LLMs for autonomous,\ntask-adaptive reasoning without external prompt engineering. MoR has two\nphases: Thought Generation, creating reasoning chain templates with models like\nGPT-4o, and SFT Dataset Construction, pairing templates with benchmark datasets\nfor supervised fine-tuning.Our experiments show that MoR significantly enhances\nperformance, with MoR150 achieving 0.730 (2.2% improvement) using CoT prompting\nand 0.734 (13.5% improvement) compared to baselines. MoR eliminates the need\nfor task-specific prompts, offering a generalizable solution for robust\nreasoning across diverse tasks.", "published": "2025-07-01 09:39:04", "link": "http://arxiv.org/abs/2507.00606v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Transferable Modeling Strategies for Low-Resource LLM Tasks: A Prompt and Alignment-Based Approach", "abstract": "This paper addresses the limited transfer and adaptation capabilities of\nlarge language models in low-resource language scenarios. It proposes a unified\nframework that combines a knowledge transfer module with parameter-efficient\nfine-tuning strategies. The method introduces knowledge alignment loss and soft\nprompt tuning to guide the model in effectively absorbing the structural\nfeatures of target languages or tasks under minimal annotation. This enhances\nboth generalization performance and training stability. The framework includes\nlightweight adaptation modules to reduce computational costs. During training,\nit integrates freezing strategies and prompt injection to preserve the model's\noriginal knowledge while enabling quick adaptation to new tasks. The study also\nconducts stability analysis experiments and synthetic pseudo-data transfer\nexperiments to systematically evaluate the method's applicability and\nrobustness across different low-resource tasks. Experimental results show that\ncompared with existing multilingual pre-trained models and mainstream transfer\nmethods, the proposed approach achieves higher performance and stability on\ncross-lingual tasks such as MLQA, XQuAD, and PAWS-X. It demonstrates\nparticularly strong advantages under extremely data-scarce conditions. The\nproposed method offers strong generality and scalability. It enhances\ntask-specific adaptability while preserving the general capabilities of large\nlanguage models. This makes it well-suited for complex semantic modeling and\nmultilingual processing tasks.", "published": "2025-07-01 09:34:49", "link": "http://arxiv.org/abs/2507.00601v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TUM-MiKaNi at SemEval-2025 Task 3: Towards Multilingual and Knowledge-Aware Non-factual Hallucination Identification", "abstract": "Hallucinations are one of the major problems of LLMs, hindering their\ntrustworthiness and deployment to wider use cases. However, most of the\nresearch on hallucinations focuses on English data, neglecting the multilingual\nnature of LLMs. This paper describes our submission to the SemEval-2025 Task-3\n- Mu-SHROOM, the Multilingual Shared-task on Hallucinations and Related\nObservable Overgeneration Mistakes. We propose a two-part pipeline that\ncombines retrieval-based fact verification against Wikipedia with a BERT-based\nsystem fine-tuned to identify common hallucination patterns. Our system\nachieves competitive results across all languages, reaching top-10 results in\neight languages, including English. Moreover, it supports multiple languages\nbeyond the fourteen covered by the shared task. This multilingual hallucination\nidentifier can help to improve LLM outputs and their usefulness in the future.", "published": "2025-07-01 09:00:50", "link": "http://arxiv.org/abs/2507.00579v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Methodological Rigour in Algorithm Application: An Illustration of Topic Modelling Algorithm", "abstract": "The rise of advanced computational algorithms has opened new avenues for\ncomputationally intensive research approaches to theory development. However,\nthe opacity of these algorithms and lack of transparency and rigour in their\napplication pose methodological challenges, potentially undermining trust in\nresearch. The discourse on methodological rigour in this new genre of research\nis still emerging. Against this backdrop, I attempt to offer guidance on\nmethodological rigour, particularly in the context of topic modelling\nalgorithms. By illustrating the application of the structural topic modelling\nalgorithm and presenting a set of guidelines, I discuss how to ensure rigour in\ntopic modelling studies. Although the guidelines are for the application of\ntopic modelling algorithms, they can be applied to other algorithms with\ncontext-specific adjustments. The guidelines are helpful, especially for novice\nresearchers applying topic modelling, and editors and reviewers handling topic\nmodelling manuscripts. I contribute to the literature on topic modelling and\njoin the emerging dialogue on methodological rigour in computationally\nintensive theory construction research.", "published": "2025-07-01 08:11:07", "link": "http://arxiv.org/abs/2507.00547v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Capsule Network-Based Semantic Intent Modeling for Human-Computer Interaction", "abstract": "This paper proposes a user semantic intent modeling algorithm based on\nCapsule Networks to address the problem of insufficient accuracy in intent\nrecognition for human-computer interaction. The method represents semantic\nfeatures in input text through a vectorized capsule structure. It uses a\ndynamic routing mechanism to transfer information across multiple capsule\nlayers. This helps capture hierarchical relationships and part-whole structures\nbetween semantic entities more effectively. The model uses a convolutional\nfeature extraction module as the low-level encoder. After generating initial\nsemantic capsules, it forms high-level abstract intent representations through\nan iterative routing process. To further enhance performance, a margin-based\nmechanism is introduced into the loss function. This improves the model's\nability to distinguish between intent classes. Experiments are conducted using\na public natural language understanding dataset. Multiple mainstream models are\nused for comparison. Results show that the proposed model outperforms\ntraditional methods and other deep learning structures in terms of accuracy,\nF1-score, and intent detection rate. The study also analyzes the effect of the\nnumber of dynamic routing iterations on model performance. A convergence curve\nof the loss function during training is provided. These results verify the\nstability and effectiveness of the proposed method in semantic modeling.\nOverall, this study presents a new structured modeling approach to improve\nintent recognition under complex semantic conditions.", "published": "2025-07-01 08:00:12", "link": "http://arxiv.org/abs/2507.00540v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NIRANTAR: Continual Learning with New Languages and Domains on Real-world Speech Data", "abstract": "We introduce Nirantar, a comprehensive framework for evaluating continual\nlearning (CL) in multilingual and multi-domain ASR. Designed to reflect\nreal-world CL challenges, Nirantar leverages data collected incrementally\nacross 22 languages and 208 districts in India through natural episodes. This\nenables evaluation across Language-Incremental (LIL), Domain-Incremental (DIL),\nand the novel Language-Incremental Domain-Incremental Learning (LIDIL)\nscenarios. Unlike prior work that relies on simulated episodes, Nirantar\npresents dynamic, non-uniform language and domain shifts, making it an ideal\ntestbed for CL research. With 3250 hours of human-transcribed speech, including\n1720 hours newly introduced in this work, our framework enables systematic\nbenchmarking of CL methods. We evaluate existing approaches and demonstrate\nthat no single method performs consistently well, underscoring the need for\nmore robust CL strategies.", "published": "2025-07-01 07:53:39", "link": "http://arxiv.org/abs/2507.00534v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TeamCMU at Touch\u00e9: Adversarial Co-Evolution for Advertisement Integration and Detection in Conversational Search", "abstract": "As conversational search engines increasingly adopt generation-based\nparadigms powered by Large Language Models (LLMs) and Retrieval-Augmented\nGeneration (RAG), the integration of advertisements into generated responses\npresents both commercial opportunities and challenges for user experience.\nUnlike traditional search, where advertisements are clearly delineated,\ngenerative systems blur the boundary between informational content and\npromotional material, raising concerns around transparency and trust. In this\nwork, we propose a modular pipeline for advertisement management in RAG-based\nconversational systems, consisting of an ad-rewriter for seamless ad\nintegration and a robust ad-classifier for detection. We leverage synthetic\ndata to train high-performing classifiers, which are then used to guide two\ncomplementary ad-integration strategies: supervised fine-tuning of the\nad-rewriter and a best-of-N sampling approach that selects the least detectable\nad-integrated response among multiple candidates. Our evaluation focuses on two\ncore questions: the effectiveness of ad classifiers in detecting diverse ad\nintegration strategies, and the training methods that best support coherent,\nminimally intrusive ad insertion. Experimental results show that our\nad-classifier, trained on synthetic advertisement data inspired by marketing\nstrategies and enhanced through curriculum learning, achieves robust detection\nperformance. Additionally, we demonstrate that classifier-guided optimization,\nthrough both fine-tuning and best-of-N sampling, significantly improves ad\nstealth, enabling more seamless integration. These findings contribute an\nadversarial co-evolution framework for developing more sophisticated ad-aware\ngenerative search systems and robust ad classifiers.", "published": "2025-07-01 07:24:29", "link": "http://arxiv.org/abs/2507.00509v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MassTool: A Multi-Task Search-Based Tool Retrieval Framework for Large Language Models", "abstract": "Tool retrieval is a critical component in enabling large language models\n(LLMs) to interact effectively with external tools. It aims to precisely filter\nthe massive tools into a small set of candidates for the downstream\ntool-augmented LLMs. However, most existing approaches primarily focus on\noptimizing tool representations, often neglecting the importance of precise\nquery comprehension. To address this gap, we introduce MassTool, a multi-task\nsearch-based framework designed to enhance both query representation and tool\nretrieval accuracy. MassTool employs a two-tower architecture: a tool usage\ndetection tower that predicts the need for function calls, and a tool retrieval\ntower that leverages a query-centric graph convolution network (QC-GCN) for\neffective query-tool matching. It also incorporates search-based user intent\nmodeling (SUIM) to handle diverse and out-of-distribution queries, alongside an\nadaptive knowledge transfer (AdaKT) module for efficient multi-task learning.\nBy jointly optimizing tool usage detection loss, list-wise retrieval loss, and\ncontrastive regularization loss, MassTool establishes a robust dual-step\nsequential decision-making pipeline for precise query understanding. Extensive\nexperiments demonstrate its effectiveness in improving retrieval accuracy. Our\ncode is available at https://github.com/wxydada/MassTool.", "published": "2025-07-01 07:02:26", "link": "http://arxiv.org/abs/2507.00487v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Beat and Downbeat Tracking in Performance MIDI Using an End-to-End Transformer Architecture", "abstract": "Beat tracking in musical performance MIDI is a challenging and important task\nfor notation-level music transcription and rhythmical analysis, yet existing\nmethods primarily focus on audio-based approaches. This paper proposes an\nend-to-end transformer-based model for beat and downbeat tracking in\nperformance MIDI, leveraging an encoder-decoder architecture for\nsequence-to-sequence translation of MIDI input to beat annotations. Our\napproach introduces novel data preprocessing techniques, including dynamic\naugmentation and optimized tokenization strategies, to improve accuracy and\ngeneralizability across different datasets. We conduct extensive experiments\nusing the A-MAPS, ASAP, GuitarSet, and Leduc datasets, comparing our model\nagainst state-of-the-art hidden Markov models (HMMs) and deep learning-based\nbeat tracking methods. The results demonstrate that our model outperforms\nexisting symbolic music beat tracking approaches, achieving competitive\nF1-scores across various musical styles and instruments. Our findings highlight\nthe potential of transformer architectures for symbolic beat tracking and\nsuggest future integration with automatic music transcription systems for\nenhanced music analysis and score generation.", "published": "2025-07-01 06:27:42", "link": "http://arxiv.org/abs/2507.00466v1", "categories": ["cs.SD", "cs.CL", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Pitfalls of Evaluating Language Models with Open Benchmarks", "abstract": "Open Large Language Model (LLM) benchmarks, such as HELM and BIG-bench, offer\nstandardized, transparent protocols that facilitate the fair comparison,\nreproducibility, and iterative advancement of Language Models (LMs). However,\ntheir openness also introduces critical and underexplored pitfalls. This study\nexposes these weaknesses by systematically constructing ``cheating'' models --\nsmaller variants of BART, T5, and GPT-2 fine-tuned directly on public test sets\n-- which achieve top rankings on a prominent open, holistic benchmark (HELM)\ndespite poor generalization and limited practical utility. Our findings\nunderscore three key insights: \\ca high leaderboard performance on open\nbenchmarks may not always reflect real-world effectiveness; \\cb private or\ndynamic benchmarks must complement open evaluations to safeguard integrity; and\n\\cc a fundamental reevaluation of current benchmarking practices is essential\nto ensure robust and trustworthy LM assessments.", "published": "2025-07-01 06:17:48", "link": "http://arxiv.org/abs/2507.00460v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Overcoming Long-Context Limitations of State-Space Models via Context-Dependent Sparse Attention", "abstract": "Efficient long-context modeling remains a critical challenge for natural\nlanguage processing (NLP), as the time complexity of the predominant\nTransformer architecture scales quadratically with the sequence length. While\nstate-space models (SSMs) offer alternative sub-quadratic solutions, they\nstruggle to capture long-range dependencies effectively. In this work, we focus\non analyzing and improving the long-context modeling capabilities of SSMs. We\nshow that the widely used synthetic task, associative recall, which requires a\nmodel to recall a value associated with a single key without context,\ninsufficiently represents the complexities of real-world long-context modeling.\nTo address this limitation, we extend the associative recall to a novel\nsynthetic task, \\emph{joint recall}, which requires a model to recall the value\nassociated with a key given in a specified context. Theoretically, we prove\nthat SSMs do not have the expressiveness to solve multi-query joint recall in\nsub-quadratic time complexity. To resolve this issue, we propose a solution\nbased on integrating SSMs with Context-Dependent Sparse Attention (CDSA), which\nhas the expressiveness to solve multi-query joint recall with sub-quadratic\ncomputation. To bridge the gap between theoretical analysis and real-world\napplications, we propose locality-sensitive Hashing Attention with sparse Key\nSelection (HAX), which instantiates the theoretical solution and is further\ntailored to natural language domains. Extensive experiments on both synthetic\nand real-world long-context benchmarks show that HAX consistently outperforms\nSSM baselines and SSMs integrated with context-independent sparse attention\n(CISA).", "published": "2025-07-01 06:03:50", "link": "http://arxiv.org/abs/2507.00449v1", "categories": ["cs.LG", "cs.CL", "I.2.7"], "primary_category": "cs.LG"}
{"title": "Beyond Sociodemographic Prompting: Using Supervision to Align LLMs with Human Response Distributions", "abstract": "The ability to accurately predict how different population groups would\nanswer subjective questions would have great value. In this work, we show that\nuse of relatively simple supervision can greatly improve language model\nalignment with diverse population groups, as measured over three datasets\nspanning various topics. Beyond evaluating average performance, we also report\nhow alignment varies across specific groups. The simplicity and generality of\nour approach promotes easy adoption, while our broad findings provide useful\nguidance for when to use or not use our approach in practice. By conducting\nevaluation over many LLMs and prompting strategies, along with open-sourcing\nour work, we provide a useful benchmark to stimulate future research.", "published": "2025-07-01 05:46:22", "link": "http://arxiv.org/abs/2507.00439v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Does Math Reasoning Improve General LLM Capabilities? Understanding Transferability of LLM Reasoning", "abstract": "Math reasoning has become the poster child of progress in large language\nmodels (LLMs), with new models rapidly surpassing human-level performance on\nbenchmarks like MATH and AIME. But as math leaderboards improve week by week,\nit is worth asking: do these gains reflect broader problem-solving ability or\njust narrow overfitting? To answer this question, we evaluate over 20\nopen-weight reasoning-tuned models across a broad suite of tasks, including\nmath, scientific QA, agent planning, coding, and standard\ninstruction-following. We surprisingly find that most models that succeed in\nmath fail to transfer their gains to other domains. To rigorously study this\nphenomenon, we conduct controlled experiments on Qwen3-14B models using\nmath-only data but different tuning methods. We find that reinforcement\nlearning (RL)-tuned models generalize well across domains, while supervised\nfine-tuning (SFT)-tuned models often forget general capabilities. Latent-space\nrepresentation and token-space distribution shift analyses reveal that SFT\ninduces substantial representation and output drift, while RL preserves\ngeneral-domain structure. Our results suggest a need to rethink standard\npost-training recipes, particularly the reliance on SFT-distilled data for\nadvancing reasoning models.", "published": "2025-07-01 05:23:05", "link": "http://arxiv.org/abs/2507.00432v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Flexible Language Modeling in Continuous Space with Transformer-based Autoregressive Flows", "abstract": "Autoregressive models have driven remarkable progress in language modeling.\nTheir foundational reliance on discrete tokens, unidirectional context, and\nsingle-pass decoding, while central to their success, also inspires the\nexploration of a design space that could offer new axes of modeling\nflexibility. In this work, we explore an alternative paradigm, shifting\nlanguage modeling from a discrete token space to a continuous latent space. We\npropose a novel framework TarFlowLM, that employs transformer-based\nautoregressive normalizing flows to model these continuous representations.\nThis approach unlocks substantial flexibility, enabling the construction of\nmodels that can capture global bi-directional context through stacked,\nalternating-direction autoregressive transformations, support block-wise\ngeneration with flexible token patch sizes, and facilitate a hierarchical\nmulti-pass generation process. We further propose new mixture-based coupling\ntransformations designed to capture complex dependencies within the latent\nspace shaped by discrete data, and demonstrate theoretical connections to\nconventional discrete autoregressive models. Extensive experiments on language\nmodeling benchmarks demonstrate strong likelihood performance and highlight the\nflexible modeling capabilities inherent in our framework.", "published": "2025-07-01 04:51:25", "link": "http://arxiv.org/abs/2507.00425v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "ASTRO: Teaching Language Models to Reason by Reflecting and Backtracking In-Context", "abstract": "We introduce ASTRO, the \"Autoregressive Search-Taught Reasoner\", a framework\nfor training language models to reason like search algorithms, explicitly\nleveraging self-reflection, backtracking, and exploration in their outputs.\nRecently, training large language models (LLMs) via reinforcement learning (RL)\nhas led to the advent of reasoning models with greatly enhanced reasoning\ncapabilities. Open-source replications of reasoning models, while successful,\nbuild upon models that already exhibit strong reasoning capabilities along with\nsearch behavior observed even before RL. As a result, it is yet unclear how to\nboost the reasoning capabilities of other non-reasoner models including Llama\n3. ASTRO teaches such models to internalize structured search behavior through\na synthetic dataset derived from Monte Carlo Tree Search (MCTS) over\nmathematical problem-solving trajectories. By converting search traces into\nnatural language chain-of-thoughts that capture both successes and recoveries\nfrom failure, ASTRO bootstraps models with a rich prior for exploration during\nRL. We finetune our models on these search-derived traces and further improve\nperformance via RL with verifiable rewards. We apply ASTRO to the Llama 3\nfamily of models and achieve absolute performance gains of 16.0% on MATH-500,\n26.9% on AMC 2023, and 20.0% on AIME 2024, especially improving upon\nchallenging problems that require iterative correction. Our results demonstrate\nthat search-inspired training offers a principled way to instill robust\nreasoning capabilities into open LLMs.", "published": "2025-07-01 04:10:15", "link": "http://arxiv.org/abs/2507.00417v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Causal Prompting for Implicit Sentiment Analysis with Large Language Models", "abstract": "Implicit Sentiment Analysis (ISA) aims to infer sentiment that is implied\nrather than explicitly stated, requiring models to perform deeper reasoning\nover subtle contextual cues. While recent prompting-based methods using Large\nLanguage Models (LLMs) have shown promise in ISA, they often rely on majority\nvoting over chain-of-thought (CoT) reasoning paths without evaluating their\ncausal validity, making them susceptible to internal biases and spurious\ncorrelations. To address this challenge, we propose CAPITAL, a causal prompting\nframework that incorporates front-door adjustment into CoT reasoning. CAPITAL\ndecomposes the overall causal effect into two components: the influence of the\ninput prompt on the reasoning chains, and the impact of those chains on the\nfinal output. These components are estimated using encoder-based clustering and\nthe NWGM approximation, with a contrastive learning objective used to better\nalign the encoder's representation with the LLM's reasoning space. Experiments\non benchmark ISA datasets with three LLMs demonstrate that CAPITAL consistently\noutperforms strong prompting baselines in both accuracy and robustness,\nparticularly under adversarial conditions. This work offers a principled\napproach to integrating causal inference into LLM prompting and highlights its\nbenefits for bias-aware sentiment reasoning. The source code and case study are\navailable at: https://github.com/whZ62/CAPITAL.", "published": "2025-07-01 03:01:09", "link": "http://arxiv.org/abs/2507.00389v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Gregorian melody, modality, and memory: Segmenting chant with Bayesian nonparametrics", "abstract": "The idea that Gregorian melodies are constructed from some vocabulary of\nsegments has long been a part of chant scholarship. This so-called\n\"centonisation\" theory has received much musicological criticism, but frequent\nre-use of certain melodic segments has been observed in chant melodies, and the\nintractable number of possible segmentations allowed the option that some\nundiscovered segmentation exists that will yet prove the value of\ncentonisation, and recent empirical results have shown that segmentations can\noutperform music-theoretical features in mode classification. Inspired by the\nfact that Gregorian chant was memorised, we search for an optimal unsupervised\nsegmentation of chant melody using nested hierarchical Pitman-Yor language\nmodels. The segmentation we find achieves state-of-the-art performance in mode\nclassification. Modeling a monk memorising the melodies from one liturgical\nmanuscript, we then find empirical evidence for the link between mode\nclassification and memory efficiency, and observe more formulaic areas at the\nbeginnings and ends of melodies corresponding to the practical role of modality\nin performance. However, the resulting segmentations themselves indicate that\neven such a memory-optimal segmentation is not what is understood as\ncentonisation.", "published": "2025-07-01 02:28:09", "link": "http://arxiv.org/abs/2507.00380v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Question Decomposition for Retrieval-Augmented Generation", "abstract": "Grounding large language models (LLMs) in verifiable external sources is a\nwell-established strategy for generating reliable answers. Retrieval-augmented\ngeneration (RAG) is one such approach, particularly effective for tasks like\nquestion answering: it retrieves passages that are semantically related to the\nquestion and then conditions the model on this evidence. However, multi-hop\nquestions, such as \"Which company among NVIDIA, Apple, and Google made the\nbiggest profit in 2023?,\" challenge RAG because relevant facts are often\ndistributed across multiple documents rather than co-occurring in one source,\nmaking it difficult for standard RAG to retrieve sufficient information. To\naddress this, we propose a RAG pipeline that incorporates question\ndecomposition: (i) an LLM decomposes the original query into sub-questions,\n(ii) passages are retrieved for each sub-question, and (iii) the merged\ncandidate pool is reranked to improve the coverage and precision of the\nretrieved evidence. We show that question decomposition effectively assembles\ncomplementary documents, while reranking reduces noise and promotes the most\nrelevant passages before answer generation. Although reranking itself is\nstandard, we show that pairing an off-the-shelf cross-encoder reranker with\nLLM-driven question decomposition bridges the retrieval gap on multi-hop\nquestions and provides a practical, drop-in enhancement, without any extra\ntraining or specialized indexing. We evaluate our approach on the MultiHop-RAG\nand HotpotQA, showing gains in retrieval (MRR@10: +36.7%) and answer accuracy\n(F1: +11.6%) over standard RAG baselines.", "published": "2025-07-01 01:01:54", "link": "http://arxiv.org/abs/2507.00355v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling Data Diversity for Joint Instance and Verbalizer Selection in Cold-Start Scenarios", "abstract": "Prompt-based methods leverage the knowledge of pre-trained language models\n(PLMs) trained with a masked language modeling (MLM) objective; however, these\nmethods are sensitive to template, verbalizer, and few-shot instance selection,\nparticularly in cold-start settings with no labeled data. Existing studies\noverlook the dependency between instances and verbalizers, where instance-label\nprobabilities depend on verbalizer token proximity in the embedding space. To\naddress this, we propose COLDSELECT, a joint verbalizer and instance selection\napproach that models data diversity. COLDSELECT maps PLM vocabulary and\n$h_{[MASK]}$ embeddings into a shared space, applying dimensionality reduction\nand clustering to ensure efficient and diverse selection. By optimizing for\nminimal uncertainty and maximal diversity, COLDSELECT captures data\nrelationships effectively. Experiments on eight benchmarks demonstrate\nCOLDSELECT's superiority in reducing uncertainty and enhancing generalization,\noutperforming baselines in verbalizer and few-shot instance selection for\ncold-start scenarios.", "published": "2025-07-01 00:01:50", "link": "http://arxiv.org/abs/2507.00330v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Beyond First-Order: Training LLMs with Stochastic Conjugate Subgradients and AdamW", "abstract": "Stochastic gradient-based descent (SGD), have long been central to training\nlarge language models (LLMs). However, their effectiveness is increasingly\nbeing questioned, particularly in large-scale applications where empirical\nevidence suggests potential performance limitations. In response, this paper\nproposes a stochastic conjugate subgradient method together with adaptive\nsampling tailored specifically for training LLMs. The method not only achieves\nfaster convergence per iteration but also demonstrates improved scalability\ncompared to traditional SGD techniques. It leverages sample complexity analysis\nto adaptively choose the sample size, employs a stochastic conjugate\nsubgradient approach to determine search directions and utilizing an AdamW-like\nalgorithm to adaptively adjust step sizes. This approach preserves the key\nadvantages of first-order methods while effectively addressing the nonconvexity\nand non-smoothness inherent in LLMs training. Additionally, we provide a\ndetailed analysis of the advantage of the algorithm. Experimental results show\nthat the proposed method not only maintains, but in many cases surpasses, the\nscalability of traditional SGD techniques, significantly enhancing both the\nspeed and accuracy of the optimization process.", "published": "2025-07-01 23:30:15", "link": "http://arxiv.org/abs/2507.01241v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Rethinking the Illusion of Thinking", "abstract": "Earlier this year, Apple ignited controversy by publishing \"The Illusion of\nThinking,\" prompting heated debate within the AI community. Critics seized upon\nthe findings as conclusive evidence that Large Reasoning Models (LRMs) lack\ngenuine reasoning capabilities, branding them as mere stochastic parrots.\nMeanwhile, defenders-spearheaded by Lawsen et al. (2025)-fired back, condemning\nthe experimental setup as flawed and the conclusions overstated. We clarify\nthis debate by replicating and refining two of the original study's most\ncontentious benchmarks: Towers of Hanoi and River Crossing. By introducing\nincremental stepwise prompting and agentic collaborative dialogue, we show that\npreviously reported failures solving the Towers of Hanoi were not purely result\nof output constraints, but also partly a result of cognition limitations: LRMs\nstill stumble when complexity rises moderately (around 8 disks). Moreover, the\nRiver Crossing results initially heralded as catastrophic failures turn out to\nhinge upon testing unsolvable configurations. Once we limit tests strictly to\nsolvable problems-LRMs effortlessly solve large instances involving over 100\nagent pairs. Our findings ultimately defy simplistic narratives: today's LRMs\nare stochastic, RL-tuned searchers in a discrete state space we barely\nunderstand. Real progress in symbolic, long-horizon reasoning demands mapping\nthat terrain through fine-grained ablations like those introduced here.", "published": "2025-07-01 23:10:02", "link": "http://arxiv.org/abs/2507.01231v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Capacity Planning and Scheduling for Jobs with Uncertainty in Resource Usage and Duration", "abstract": "Organizations around the world schedule jobs (programs) regularly to perform\nvarious tasks dictated by their end users. With the major movement towards\nusing a cloud computing infrastructure, our organization follows a hybrid\napproach with both cloud and on-prem servers. The objective of this work is to\nperform capacity planning, i.e., estimate resource requirements, and job\nscheduling for on-prem grid computing environments. A key contribution of our\napproach is handling uncertainty in both resource usage and duration of the\njobs, a critical aspect in the finance industry where stochastic market\nconditions significantly influence job characteristics. For capacity planning\nand scheduling, we simultaneously balance two conflicting objectives: (a)\nminimize resource usage, and (b) provide high quality-of-service to the end\nusers by completing jobs by their requested deadlines. We propose approximate\napproaches using deterministic estimators and pair sampling-based constraint\nprogramming. Our best approach (pair sampling-based) achieves much lower peak\nresource usage compared to manual scheduling without compromising on the\nquality-of-service.", "published": "2025-07-01 22:56:08", "link": "http://arxiv.org/abs/2507.01225v1", "categories": ["cs.DC", "cs.AI"], "primary_category": "cs.DC"}
{"title": "Search-Based Robot Motion Planning With Distance-Based Adaptive Motion Primitives", "abstract": "This work proposes a motion planning algorithm for robotic manipulators that\ncombines sampling-based and search-based planning methods. The core\ncontribution of the proposed approach is the usage of burs of free\nconfiguration space (C-space) as adaptive motion primitives within the graph\nsearch algorithm. Due to their feature to adaptively expand in free C-space,\nburs enable more efficient exploration of the configuration space compared to\nfixed-sized motion primitives, significantly reducing the time to find a valid\npath and the number of required expansions. The algorithm is implemented within\nthe existing SMPL (Search-Based Motion Planning Library) library and evaluated\nthrough a series of different scenarios involving manipulators with varying\nnumber of degrees-of-freedom (DoF) and environment complexity. Results\ndemonstrate that the bur-based approach outperforms fixed-primitive planning in\ncomplex scenarios, particularly for high DoF manipulators, while achieving\ncomparable performance in simpler scenarios.", "published": "2025-07-01 21:33:33", "link": "http://arxiv.org/abs/2507.01198v1", "categories": ["cs.RO", "cs.AI", "cs.CG"], "primary_category": "cs.RO"}
{"title": "Are Large Brainwave Foundation Models Capable Yet? Insights from Fine-tuning", "abstract": "Foundation Models have demonstrated significant success across various\ndomains in Artificial Intelligence (AI), yet their capabilities for brainwave\nmodeling remain unclear. In this paper, we comprehensively evaluate current\nLarge Brainwave Foundation Models (LBMs) through systematic fine-tuning\nexperiments across multiple Brain-Computer Interface (BCI) benchmark tasks,\nincluding memory tasks and sleep stage classification. Our extensive analysis\nshows that state-of-the-art LBMs achieve only marginal improvements (0.9%-1.2%)\nover traditional deep architectures while requiring significantly more\nparameters (millions vs thousands), raising important questions about their\nefficiency and applicability in BCI contexts. Moreover, through detailed\nablation studies and Low-Rank Adaptation (LoRA), we significantly reduce\ntrainable parameters without performance degradation, while demonstrating that\narchitectural and training inefficiencies limit LBMs' current capabilities. Our\nexperiments span both full model fine-tuning and parameter-efficient adaptation\ntechniques, providing insights into optimal training strategies for BCI\napplications. We pioneer the application of LoRA to LBMs, revealing that\nperformance benefits generally emerge when adapting multiple neural network\ncomponents simultaneously. These findings highlight the critical need for\ndomain-specific development strategies to advance LBMs, suggesting that current\narchitectures may require redesign to fully leverage the potential of\nfoundation models in brainwave analysis.", "published": "2025-07-01 21:21:42", "link": "http://arxiv.org/abs/2507.01196v1", "categories": ["cs.LG", "cs.AI", "cs.HC"], "primary_category": "cs.LG"}
{"title": "Geometry-aware 4D Video Generation for Robot Manipulation", "abstract": "Understanding and predicting the dynamics of the physical world can enhance a\nrobot's ability to plan and interact effectively in complex environments. While\nrecent video generation models have shown strong potential in modeling dynamic\nscenes, generating videos that are both temporally coherent and geometrically\nconsistent across camera views remains a significant challenge. To address\nthis, we propose a 4D video generation model that enforces multi-view 3D\nconsistency of videos by supervising the model with cross-view pointmap\nalignment during training. This geometric supervision enables the model to\nlearn a shared 3D representation of the scene, allowing it to predict future\nvideo sequences from novel viewpoints based solely on the given RGB-D\nobservations, without requiring camera poses as inputs. Compared to existing\nbaselines, our method produces more visually stable and spatially aligned\npredictions across multiple simulated and real-world robotic datasets. We\nfurther show that the predicted 4D videos can be used to recover robot\nend-effector trajectories using an off-the-shelf 6DoF pose tracker, supporting\nrobust robot manipulation and generalization to novel camera viewpoints.", "published": "2025-07-01 18:01:41", "link": "http://arxiv.org/abs/2507.01099v1", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "primary_category": "cs.CV"}
{"title": "AI-guided digital intervention with physiological monitoring reduces intrusive memories after experimental trauma", "abstract": "Trauma prevalence is vast globally. Evidence-based digital treatments can\nhelp, but most require human guidance. Human guides provide tailored\ninstructions and responsiveness to internal cognitive states, but limit\nscalability. Can generative AI and neurotechnology provide a scalable\nalternative? Here we test ANTIDOTE, combining AI guidance and pupillometry to\nautomatically deliver and monitor an evidence-based digital treatment,\nspecifically the Imagery Competing Task Intervention (ICTI), to reduce\nintrusive memories after psychological trauma. One hundred healthy volunteers\nwere exposed to videos of traumatic events and randomly assigned to an\nintervention or active control condition. As predicted, intervention\nparticipants reported significantly fewer intrusive memories over the following\nweek. Post-hoc assessment against clinical rubrics confirmed the AI guide\ndelivered the intervention successfully. Additionally, pupil size tracked\nintervention engagement and predicted symptom reduction, providing a candidate\nbiomarker of intervention effectiveness. These findings open a path toward\nrigorous AI-guided digital interventions that can scale to trauma prevalence.", "published": "2025-07-01 17:59:01", "link": "http://arxiv.org/abs/2507.01081v1", "categories": ["cs.HC", "cs.AI"], "primary_category": "cs.HC"}
{"title": "GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning", "abstract": "We present GLM-4.1V-Thinking, a vision-language model (VLM) designed to\nadvance general-purpose multimodal understanding and reasoning. In this report,\nwe share our key findings in the development of the reasoning-centric training\nframework. We first develop a capable vision foundation model with significant\npotential through large-scale pre-training, which arguably sets the upper bound\nfor the final performance. We then propose Reinforcement Learning with\nCurriculum Sampling (RLCS) to unlock the full potential of the model, leading\nto comprehensive capability enhancement across a diverse range of tasks,\nincluding STEM problem solving, video understanding, content recognition,\ncoding, grounding, GUI-based agents, and long document understanding. We\nopen-source GLM-4.1V-9B-Thinking, which achieves state-of-the-art performance\namong models of comparable size. In a comprehensive evaluation across 28 public\nbenchmarks, our model outperforms Qwen2.5-VL-7B on nearly all tasks and\nachieves comparable or even superior performance on 18 benchmarks relative to\nthe significantly larger Qwen2.5-VL-72B. Notably, GLM-4.1V-9B-Thinking also\ndemonstrates competitive or superior performance compared to closed-source\nmodels such as GPT-4o on challenging tasks including long document\nunderstanding and STEM reasoning, further underscoring its strong capabilities.\nCode, models and more information are released at\nhttps://github.com/THUDM/GLM-4.1V-Thinking.", "published": "2025-07-01 17:55:04", "link": "http://arxiv.org/abs/2507.01006v2", "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Description of the Training Process of Neural Networks via Ergodic Theorem : Ghost nodes", "abstract": "Recent studies have proposed interpreting the training process from an\nergodic perspective. Building on this foundation we present a unified framework\nfor understanding and accelerating the training of deep neural networks via\nstochastic gradient descent. By analyzing the geometric landscape of the\nobjective function we introduce a practical diagnostic, the running estimate of\nthe largest Lyapunov exponent, which provably distinguishes genuine convergence\ntoward stable minimizers from mere statistical stabilization near saddle\npoints. We then propose a ghost category extension for standard classifiers\nthat adds auxiliary ghost output nodes so the model gains extra descent\ndirections that open a lateral corridor around narrow loss barriers and enable\nthe optimizer to bypass poor basins during the early training phase. We show\nthat this extension strictly reduces approximation error and that after\nsufficient convergence the ghost dimensions collapse and the extended model's\ninvariant law coincides with that of the original and there exists a path in\nthe enlarged parameter space along which the total loss does not increase while\nthe original loss decreases by an arbitrary margin. Taken together these\nresults provide a principled architecture level intervention that accelerates\nearly stage trainability while preserving asymptotic behavior.", "published": "2025-07-01 17:54:35", "link": "http://arxiv.org/abs/2507.01003v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Seeing is not believing in limited visibility cops and robbers", "abstract": "We consider the model of limited visibility Cops and Robbers, where the cops\ncan only see within their $l$-neighbourhood. We prove that the number of cops\nneeded to see the robber can be arbitrarily smaller than the number needed to\ncapture the robber, answering an open question from the literature. We then\nconsider how close we can get to seeing the robber when we do not have enough\ncops, along with a probabilistic interpretation.", "published": "2025-07-01 16:47:24", "link": "http://arxiv.org/abs/2507.00941v1", "categories": ["math.CO", "cs.DM", "49N75"], "primary_category": "math.CO"}
{"title": "Inverse matroid optimization under subset constraints", "abstract": "In the Inverse Matroid problem, we are given a matroid, a fixed basis $B$,\nand an initial weight function, and the goal is to minimally modify the weights\n-- measured by some function -- so that $B$ becomes a maximum-weight basis. The\nproblem arises naturally in settings where one wishes to explain or enforce a\ngiven solution by minimally perturbing the input.\n  We extend this classical problem by replacing the fixed basis with a subset\n$S_0$ of the ground set and imposing various structural constraints on the set\nof maximum-weight bases relative to $S_0$. Specifically, we study six variants:\n(A) Inverse Matroid Exists, where $S_0$ must contain at least one\nmaximum-weight basis; (B) Inverse Matroid All, where all bases contained in\n$S_0$ are maximum-weight; and (C) Inverse Matroid Only, where $S_0$ contains\nexactly the maximum-weight bases, along with their natural negated\ncounterparts.\n  For all variants, we develop combinatorial polynomial-time algorithms under\nthe $\\ell_\\infty$-norm. A key ingredient is a refined min-max theorem for\nInverse Matroid under the $\\ell_\\infty$-norm, which enables simpler and faster\nalgorithms than previous approaches and may be of independent combinatorial\ninterest. Our work significantly broadens the range of inverse optimization\nproblems on matroids that can be solved efficiently, especially those that\nconstrain the structure of optimal solutions through subset inclusion or\nexclusion.", "published": "2025-07-01 16:36:38", "link": "http://arxiv.org/abs/2507.00930v2", "categories": ["cs.DS", "cs.DM"], "primary_category": "cs.DS"}
{"title": "Temporal Orienteering with Changing Fuel Costs", "abstract": "The problem Orienteering asks whether there exists a walk which visits a\nnumber of sites without exceeding some fuel budget. In the variant of the\nproblem we consider, the cost of each edge in the walk is dependent on the time\nwe depart one endpoint and the time we arrive at the other endpoint. This\nmirrors applications such as travel between orbiting objects where fuel costs\nare dependent on both the departure time and the length of time spent\ntravelling. In defining this problem, we introduce a natural generalisation of\nthe standard notion of temporal graphs: the pair consisting of the graph of the\nsites and a cost function, in which costs as well as shortest travel times\nbetween pairs of objects change over time. We believe this model is likely to\nbe of independent interest. The problem of deciding whether a stated goal is\nfeasible is easily seen to be NP-complete; we investigate three different ways\nto restrict the input which lead to efficient algorithms. These include the\nnumber of times an edge can be used, an analogue of vertex-interval-membership\nwidth, and the number of sites to be visited.", "published": "2025-07-01 13:20:14", "link": "http://arxiv.org/abs/2507.00728v1", "categories": ["cs.DM"], "primary_category": "cs.DM"}
{"title": "Computational complexity of covering regular trees", "abstract": "A graph covering projection, also referred to as a locally bijective\nhomomorphism, is a mapping between the vertices and edges of two graphs that\npreserves incidences and is a local bijection. This concept originates in\ntopological graph theory but has also found applications in combinatorics and\ntheoretical computer science. In this paper we consider undirected graphs in\nthe most general setting -- graphs may contain multiple edges, loops, and\nsemi-edges. This is in line with recent trends in topological graph theory and\nmathematical physics.\n  We advance the study of the computational complexity of the {\\sc $H$-Cover}\nproblem, which asks whether an input graph allows a covering projection onto a\nparameter graph $H$. The quest for a complete characterization started in\n1990's. Several results for simple graphs or graphs without semi-edges have\nbeen known, the role of semi-edges in the complexity setting has started to be\ninvestigated only recently. One of the most general known NP-hardness results\nstates that {\\sc $H$}-Cover is NP-complete for every simple connected regular\ngraph of valency greater than two. We complement this result by considering\nregular graphs $H$ arising from connected acyclic graphs by adding semi-edges.\nNamely, we prove that any graph obtained by adding semi-edges to the vertices\nof a tree making it a $d$-regular graph with $d \\geq 3$, defines an NP-complete\ngraph covering problem. In line with the so called Strong Dichotomy Conjecture,\nwe prove that the NP-hardness holds even for simple graphs on input.", "published": "2025-07-01 08:32:33", "link": "http://arxiv.org/abs/2507.00564v1", "categories": ["cs.DM", "math.CO"], "primary_category": "cs.DM"}
{"title": "Towards a Signal Detection Based Measure for Assessing Information Quality of Explainable Recommender Systems", "abstract": "There is growing interest in explainable recommender systems that provide\nrecommendations along with explanations for the reasoning behind them. When\nevaluating recommender systems, most studies focus on overall recommendation\nperformance. Only a few assess the quality of the explanations. Explanation\nquality is often evaluated through user studies that subjectively gather users'\nopinions on representative explanatory factors that shape end-users'\nperspective towards the results, not about the explanation contents itself. We\naim to fill this gap by developing an objective metric to evaluate Veracity:\nthe information quality of explanations. Specifically, we decompose Veracity\ninto two dimensions: Fidelity and Attunement. Fidelity refers to whether the\nexplanation includes accurate information about the recommended item.\nAttunement evaluates whether the explanation reflects the target user's\npreferences. By applying signal detection theory, we first determine decision\noutcomes for each dimension and then combine them to calculate a sensitivity,\nwhich serves as the final Veracity value. To assess the effectiveness of the\nproposed metric, we set up four cases with varying levels of information\nquality to validate whether our metric can accurately capture differences in\nquality. The results provided meaningful insights into the effectiveness of our\nproposed metric.", "published": "2025-07-01 20:11:17", "link": "http://arxiv.org/abs/2507.01168v1", "categories": ["cs.IR", "cs.HC"], "primary_category": "cs.IR"}
{"title": "Digital Collections Explorer: An Open-Source, Multimodal Viewer for Searching Digital Collections", "abstract": "We present Digital Collections Explorer, a web-based, open-source exploratory\nsearch platform that leverages CLIP (Contrastive Language-Image Pre-training)\nfor enhanced visual discovery of digital collections. Our Digital Collections\nExplorer can be installed locally and configured to run on a visual collection\nof interest on disk in just a few steps. Building upon recent advances in\nmultimodal search techniques, our interface enables natural language queries\nand reverse image searches over digital collections with visual features. This\npaper describes the system's architecture, implementation, and application to\nvarious cultural heritage collections, demonstrating its potential for\ndemocratizing access to digital archives, especially those with impoverished\nmetadata. We present case studies with maps, photographs, and PDFs extracted\nfrom web archives in order to demonstrate the flexibility of the Digital\nCollections Explorer, as well as its ease of use. We demonstrate that the\nDigital Collections Explorer scales to hundreds of thousands of images on a\nMacBook Pro with an M4 chip. Lastly, we host a public demo of Digital\nCollections Explorer.", "published": "2025-07-01 17:10:34", "link": "http://arxiv.org/abs/2507.00961v1", "categories": ["cs.DL", "cs.IR"], "primary_category": "cs.DL"}
{"title": "WebArXiv: Evaluating Multimodal Agents on Time-Invariant arXiv Tasks", "abstract": "Recent progress in large language models (LLMs) has enabled the development\nof autonomous web agents capable of navigating and interacting with real\nwebsites. However, evaluating such agents remains challenging due to the\ninstability and inconsistency of existing benchmarks, which often rely on\ndynamic content or oversimplified simulations. In this work, we introduce\nWebArXiv, a static and time-invariant benchmark comprising 275 web-based tasks\ngrounded in the arXiv platform. WebArXiv ensures reproducible and reliable\nevaluation by anchoring tasks in fixed web snapshots with deterministic ground\ntruths and standardized action trajectories. Through behavioral analysis, we\nidentify a common failure mode, Rigid History Reflection, where agents\nover-rely on fixed interaction histories. To address this, we propose a\nlightweight dynamic reflection mechanism that allows agents to selectively\nretrieve relevant past steps during decision-making. We evaluate ten\nstate-of-the-art web agents on WebArXiv. Results demonstrate clear performance\ndifferences across agents and validate the effectiveness of our proposed\nreflection strategy.", "published": "2025-07-01 16:43:57", "link": "http://arxiv.org/abs/2507.00938v1", "categories": ["cs.IR", "cs.AI", "cs.DB", "F.2.2; I.2.7"], "primary_category": "cs.IR"}
{"title": "EARN: Efficient Inference Acceleration for LLM-based Generative Recommendation by Register Tokens", "abstract": "Large Language Model-based generative recommendation (LLMRec) has achieved\nnotable success, but it suffers from high inference latency due to massive\ncomputational overhead and memory pressure of KV Cache. Existing KV Cache\nreduction methods face critical limitations: cache compression offers marginal\nacceleration given recommendation tasks' short decoding steps, while prompt\ncompression risks discarding vital interaction history. Through systematic\nanalysis of attention patterns in LLMRec, we uncover two pivotal insights: 1)\nlayer-wise attention sparsity inversion where early layers retain dense\ninformative patterns while later layers exhibit high redundancy, and 2) dual\nattention sinks phenomenon where attention scores concentrate on both head and\ntail tokens of input sequences. Motivated by these insights, we propose EARN,\nan efficient inference framework that leverages the early layers to compress\ninformation into register tokens placed at the input sequence boundaries, then\nfocuses solely on these tokens in the subsequent layers. Extensive experiments\non three datasets, two LLMRec methods and two LLM architectures demonstrate\nEARN's superiority, achieving up to 3.79x speedup and 80.8% KV Cache reduction\nwith better accuracy than the general finetuning approach. Our work bridges the\nefficiency-effectiveness gap in LLMRec, offering practical deployment\nadvantages for industrial scenarios.", "published": "2025-07-01 12:42:06", "link": "http://arxiv.org/abs/2507.00715v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Reliable Annotations with Less Effort: Evaluating LLM-Human Collaboration in Search Clarifications", "abstract": "Despite growing interest in using large language models (LLMs) to automate\nannotation, their effectiveness in complex, nuanced, and multi-dimensional\nlabelling tasks remains relatively underexplored. This study focuses on\nannotation for the search clarification task, leveraging a high-quality,\nmulti-dimensional dataset that includes five distinct fine-grained annotation\nsubtasks. Although LLMs have shown impressive capabilities in general settings,\nour study reveals that even state-of-the-art models struggle to replicate\nhuman-level performance in subjective or fine-grained evaluation tasks. Through\na systematic assessment, we demonstrate that LLM predictions are often\ninconsistent, poorly calibrated, and highly sensitive to prompt variations. To\naddress these limitations, we propose a simple yet effective human-in-the-loop\n(HITL) workflow that uses confidence thresholds and inter-model disagreement to\nselectively involve human review. Our findings show that this lightweight\nintervention significantly improves annotation reliability while reducing human\neffort by up to 45%, offering a relatively scalable and cost-effective yet\naccurate path forward for deploying LLMs in real-world evaluation settings.", "published": "2025-07-01 08:04:58", "link": "http://arxiv.org/abs/2507.00543v1", "categories": ["cs.IR", "cs.HC"], "primary_category": "cs.IR"}
{"title": "Rethinking Group Recommender Systems in the Era of Generative AI: From One-Shot Recommendations to Agentic Group Decision Support", "abstract": "More than twenty-five years ago, first ideas were developed on how to design\na system that can provide recommendations to groups of users instead of\nindividual users. Since then, a rich variety of algorithmic proposals were\npublished, e.g., on how to acquire individual preferences, how to aggregate\nthem, and how to generate recommendations for groups of users. However, despite\nthe rich literature on the topic, barely any examples of real-world group\nrecommender systems can be found. This lets us question common assumptions in\nacademic research, in particular regarding communication processes in a group\nand how recommendation-supported decisions are made. In this essay, we argue\nthat these common assumptions and corresponding system designs often may not\nmatch the needs or expectations of users. We thus call for a reorientation in\nthis research area, leveraging the capabilities of modern Generative AI\nassistants like ChatGPT. Specifically, as one promising future direction, we\nenvision group recommender systems to be systems where human group members\ninteract in a chat and an AI-based group recommendation agent assists the\ndecision-making process in an agentic way. Ultimately, this shall lead to a\nmore natural group decision-making environment and finally to wider adoption of\ngroup recommendation systems in practice.", "published": "2025-07-01 07:56:37", "link": "http://arxiv.org/abs/2507.00535v1", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR"}
{"title": "WebANNS: Fast and Efficient Approximate Nearest Neighbor Search in Web Browsers", "abstract": "Approximate nearest neighbor search (ANNS) has become vital to modern AI\ninfrastructure, particularly in retrieval-augmented generation (RAG)\napplications. Numerous in-browser ANNS engines have emerged to seamlessly\nintegrate with popular LLM-based web applications, while addressing privacy\nprotection and challenges of heterogeneous device deployments. However, web\nbrowsers present unique challenges for ANNS, including computational\nlimitations, external storage access issues, and memory utilization\nconstraints, which state-of-the-art (SOTA) solutions fail to address\ncomprehensively. We propose WebANNS, a novel ANNS engine specifically designed\nfor web browsers. WebANNS leverages WebAssembly to overcome computational\nbottlenecks, designs a lazy loading strategy to optimize data retrieval from\nexternal storage, and applies a heuristic approach to reduce memory usage.\nExperiments show that WebANNS is fast and memory efficient, achieving up to\n$743.8\\times$ improvement in 99th percentile query latency over the SOTA\nengine, while reducing memory usage by up to 39\\%. Note that WebANNS decreases\nquery time from 10 seconds to the 10-millisecond range in browsers, making\nin-browser ANNS practical with user-acceptable latency.", "published": "2025-07-01 07:37:18", "link": "http://arxiv.org/abs/2507.00521v2", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Exploring Large Action Sets with Hyperspherical Embeddings using von Mises-Fisher Sampling", "abstract": "This paper introduces von Mises-Fisher exploration (vMF-exp), a scalable\nmethod for exploring large action sets in reinforcement learning problems where\nhyperspherical embedding vectors represent these actions. vMF-exp involves\ninitially sampling a state embedding representation using a von Mises-Fisher\ndistribution, then exploring this representation's nearest neighbors, which\nscales to virtually unlimited numbers of candidate actions. We show that, under\ntheoretical assumptions, vMF-exp asymptotically maintains the same probability\nof exploring each action as Boltzmann Exploration (B-exp), a popular\nalternative that, nonetheless, suffers from scalability issues as it requires\ncomputing softmax values for each action. Consequently, vMF-exp serves as a\nscalable alternative to B-exp for exploring large action sets with\nhyperspherical embeddings. Experiments on simulated data, real-world public\ndata, and the successful large-scale deployment of vMF-exp on the recommender\nsystem of a global music streaming service empirically validate the key\nproperties of the proposed method.", "published": "2025-07-01 07:32:54", "link": "http://arxiv.org/abs/2507.00518v1", "categories": ["cs.LG", "cs.IR"], "primary_category": "cs.LG"}
{"title": "On Mitigating Data Sparsity in Conversational Recommender Systems", "abstract": "Conversational recommender systems (CRSs) capture user preference through\ntextual information in dialogues. However, they suffer from data sparsity on\ntwo fronts: the dialogue space is vast and linguistically diverse, while the\nitem space exhibits long-tail and sparse distributions. Existing methods\nstruggle with (1) generalizing to varied dialogue expressions due to\nunderutilization of rich textual cues, and (2) learning informative item\nrepresentations under severe sparsity. To address these problems, we propose a\nCRS model named DACRS. It consists of three modules, namely Dialogue\nAugmentation, Knowledge-Guided Entity Modeling, and Dialogue-Entity Matching.\nIn the Dialogue Augmentation module, we apply a two-stage augmentation pipeline\nto augment the dialogue context to enrich the data and improve\ngeneralizability. In the Knowledge-Guided Entity Modeling, we propose a\nknowledge graph (KG) based entity substitution and an entity similarity\nconstraint to enhance the expressiveness of entity embeddings. In the\nDialogue-Entity Matching module, we fuse the dialogue embedding with the\nmentioned entity embeddings through a dialogue-guided attention aggregation to\nacquire user embeddings that contain both the explicit and implicit user\npreferences. Extensive experiments on two public datasets demonstrate the\nstate-of-the-art performance of DACRS.", "published": "2025-07-01 06:54:51", "link": "http://arxiv.org/abs/2507.00479v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Read the Docs Before Rewriting: Equip Rewriter with Domain Knowledge via Continual Pre-training", "abstract": "A Retrieval-Augmented Generation (RAG)-based question-answering (QA) system\nenhances a large language model's knowledge by retrieving relevant documents\nbased on user queries. Discrepancies between user queries and document\nphrasings often necessitate query rewriting. However, in specialized domains,\nthe rewriter model may struggle due to limited domain-specific knowledge. To\nresolve this, we propose the R\\&R (Read the doc before Rewriting) rewriter,\nwhich involves continual pre-training on professional documents, akin to how\nstudents prepare for open-book exams by reviewing textbooks. Additionally, it\ncan be combined with supervised fine-tuning for improved results. Experiments\non multiple datasets demonstrate that R\\&R excels in professional QA across\nmultiple domains, effectively bridging the query-document gap, while\nmaintaining good performance in general scenarios, thus advancing the\napplication of RAG-based QA systems in specialized fields.", "published": "2025-07-01 06:51:00", "link": "http://arxiv.org/abs/2507.00477v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Numerical Techniques for the Maximum Likelihood Toeplitz Covariance Matrix Estimation: Part I. Symmetric Toeplitz Matrices", "abstract": "In several applications, one must estimate a real-valued (symmetric) Toeplitz\ncovariance matrix, typically shifted by the conjugated diagonal matrices of\nphase progression and phase \"calibration\" errors. Unlike the Hermitian Toeplitz\ncovariance matrices, these symmetric matrices have a unique potential\ncapability of being estimated regardless of these beam-steering phase\nprogression and/or phase \"calibration\" errors. This unique capability is the\nprimary motivation of this paper.", "published": "2025-07-01 23:07:22", "link": "http://arxiv.org/abs/2507.01230v1", "categories": ["eess.SP", "cs.IT", "math.IT"], "primary_category": "eess.SP"}
{"title": "Degrees of Freedom of Spatial Multiplexing in Distance Domain of Arbitrary Continuous-Aperture Array in Near-Field Region", "abstract": "Extremely large aperture array operating in the near-field regime unlocks\nadditional spatial resources that can be exploited to simultaneously serve\nmultiple users even when they share the same angular direction, a capability\nnot achievable in conventional far-field systems. A fundamental question,\nhowever, remains: What is the maximum spatial degree of freedom (DoF) of\nspatial multiplexing in the distance domain?\n  In this paper, we address this open problem by investigating the spatial DoF\nof a line-of-sight (LoS) channel between a large two-dimensional transmit\naperture and a linear receive array with collinearly-aligned elements (i.e., at\nthe same angular direction) but located at different distances from the\ntransmit aperture. We assume that both the aperture and linear array are\ncontinuous-aperture (CAP) arrays with an infinite number of elements and\ninfinitesimal spacing, which establishes an upper bound for the spatial degrees\nof freedom (DoF) in the case of finite elements. First, we assume an ideal case\nwhere the transmit array is a single piece and the linear array is on the broad\nside of the transmit array. By reformulating the channel as an integral\noperator with a Hermitian convolution kernel, we derive a closed-form\nexpression for the spatial DoF via the Fourier transform. Our analysis shows\nthat the spatial DoF in the distance domain is predominantly determined by the\nextreme boundaries of the array rather than its detailed interior structure. We\nfurther extend the framework to non-broadside configurations by employing a\nprojection method, which effectively converts the spatial DoF to an equivalent\nbroadside case. Finally, we extend our analytical framework to the modular\narray, which shows the spatial DoF gain over the single-piece array given the\nconstraint of the physical length of the array.", "published": "2025-07-01 22:59:26", "link": "http://arxiv.org/abs/2507.01227v1", "categories": ["eess.SP", "cs.IT", "math.IT"], "primary_category": "eess.SP"}
{"title": "LotteryCodec: Searching the Implicit Representation in a Random Network for Low-Complexity Image Compression", "abstract": "We introduce and validate the lottery codec hypothesis, which states that\nuntrained subnetworks within randomly initialized networks can serve as\nsynthesis networks for overfitted image compression, achieving rate-distortion\n(RD) performance comparable to trained networks. This hypothesis leads to a new\nparadigm for image compression by encoding image statistics into the network\nsubstructure. Building on this hypothesis, we propose LotteryCodec, which\noverfits a binary mask to an individual image, leveraging an over-parameterized\nand randomly initialized network shared by the encoder and the decoder. To\naddress over-parameterization challenges and streamline subnetwork search, we\ndevelop a rewind modulation mechanism that improves the RD performance.\nLotteryCodec outperforms VTM and sets a new state-of-the-art in single-image\ncompression. LotteryCodec also enables adaptive decoding complexity through\nadjustable mask ratios, offering flexible compression solutions for diverse\ndevice constraints and application requirements.", "published": "2025-07-01 21:48:16", "link": "http://arxiv.org/abs/2507.01204v1", "categories": ["eess.IV", "cs.IT", "math.IT", "68P30, 94A08", "I.4.2; E.4"], "primary_category": "eess.IV"}
{"title": "Quasi-twisted codes: decoding and applications in code-based cryptography", "abstract": "Quasi-twisted (QT) codes generalize several important families of linear\ncodes, including cyclic, constacyclic, and quasi-cyclic codes. Despite their\npotential, to the best of our knowledge, there exists no efficient decoding\nalgorithm for QT codes. In this work, we propose a syndrome-based decoding\nmethod capable of efficiently correcting up to (d* - 1)/2 errors, where d*\ndenotes an HT-like lower bound on the minimum distance of QT codes, which we\nformalize here. Additionally, we introduce a Niederreiter-like cryptosystem\nconstructed from QT codes. This cryptosystem is resistant to some classical\nattacks as well as some quantum attacks based on Quantum Fourier Sampling.", "published": "2025-07-01 18:26:27", "link": "http://arxiv.org/abs/2507.01118v1", "categories": ["cs.CR", "cs.IT", "math.IT"], "primary_category": "cs.CR"}
{"title": "Optimal Feedback Schemes for Dirty Paper Channels With State Estimation at the Receiver", "abstract": "In the literature, it has been shown that feedback does not increase the\noptimal rate-distortion region of the dirty paper channel with state estimation\nat the receiver (SE-R). On the other hand, it is well-known that feedback helps\nto construct low-complexity coding schemes in Gaussian channels, such as the\nelegant Schalkwijk-Kailath (SK) feedback scheme. This motivates us to explore\ncapacity-achieving SK-type schemes in dirty paper channels with SE-R and\nfeedback. In this paper, we first propose a capacity-achieving feedback scheme\nfor the dirty paper channel with SE-R (DPC-SE-R), which combines the\nsuperposition coding and the classical SK-type scheme. Then, we extend this\nscheme to the dirty paper multiple-access channel with SE-R and feedback, and\nalso show the extended scheme is capacity-achieving. Finally, we discuss how to\nextend our scheme to a noisy state observation case of the DPC-SE-R. However,\nthe capacity-achieving SK-type scheme for such a case remains unknown.", "published": "2025-07-01 16:48:16", "link": "http://arxiv.org/abs/2507.00942v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "MichelangeRoll: Sculpting Rational Distributions Exactly and Efficiently", "abstract": "Simulating an arbitrary discrete distribution $D \\in [0, 1]^n$ using fair\ncoin tosses incurs trade-offs between entropy complexity and space and time\ncomplexity. Shannon's theory suggests that $H(D)$ tosses are necessary and\nsufficient, but does not guarantee exact distribution. Knuth and Yao showed\nthat a decision tree consumes fewer than $H(D) + 2$ tosses for one exact\nsample. Drapper and Saad's recent work addresses the space and time aspect,\nshowing that $H(D) + 2$ tosses, $O(n \\log(n) \\log(m))$ memory, and $O(H(D))$\noperations are all it costs, where $m$ is the common denominator of the\nprobability masses in $D$ and $n$ is the number of possible outcomes.\n  In this paper, MichelangeRoll recycles leftover entropy to break the \"$+2$\"\nbarrier. With $O((n + 1/\\varepsilon) \\log(m/\\varepsilon))$ memory, the entropy\ncost of generating a ongoing sequence of $D$ is reduced to $H(D) + \\varepsilon$\nper sample.", "published": "2025-07-01 16:20:13", "link": "http://arxiv.org/abs/2507.00915v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "On Hierarchical Coded Caching with Offline Users", "abstract": "This paper studies a two-layer hierarchical network in which some users are\noffline during the content delivery phase. A two-layer hierarchical network\nconsists of a single server connected to multiple cache-aided mirror sites, and\neach mirror site is connected to a distinct set of cache-aided users. A scheme\nfor such a hierarchical system with offline users has been proposed recently\nbut considered a special case where all mirror caches have zero memory, which\nis a significant limitation. We propose an array known as a hierarchical\nhotplug placement delivery array (HHPDA), which describes the placement and\ndelivery phases of a coded caching scheme for a general two-layer hierarchical\nnetwork with offline users. Further, we construct a class of HHPDAs using\ncombinatorial t-designs.", "published": "2025-07-01 13:17:46", "link": "http://arxiv.org/abs/2507.00727v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "The Rate-Distortion Function for Sampled Cyclostationary Gaussian Processes with Memory and with Bounded Processing Delay: Extended Version with Proofs", "abstract": "We study the rate-distortion function (RDF) for the lossy compression of\ndiscrete-time (DT) wide-sense almost cyclostationary (WSACS) Gaussian processes\nwith memory, arising from sampling continuous-time (CT) wide-sense\ncyclostationary (WSCS) Gaussian source processes. The importance of this\nproblem arises as such CT processes represent communications signals, and\nsampling must be applied to facilitate the DT processing associated with their\ncompression. Moreover, the physical characteristics of oscillators imply that\nthe sampling interval is incommensurate with the period of the autocorrelation\nfunction (AF) of the physical process, giving rise to the DT WSACS model\nconsidered. In addition, to reduce the loss, the sampling interval is generally\nshorter than the correlation length, and thus, the DT process is correlated as\nwell. The difficulty in the RDF characterization follows from the\ninformation-instability of WSACS processes, which renders the traditional\ninformation-theoretic tools inapplicable. In this work we utilize the\ninformation-spectrum framework to characterize the RDF when a finite and\nbounded delay is allowed between processing of subsequent source sequences.\nThis scenario extends our previous works which studied settings without\nprocessing delays or without memory. Numerical evaluations reveal the impact of\nscenario parameters on the RDF with asynchronous sampling.", "published": "2025-07-01 10:54:40", "link": "http://arxiv.org/abs/2507.00656v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Decentralized Pliable Index Coding For Federated Learning In Intelligent Transportation Systems", "abstract": "Federated Learning is a promising option for data privacy and security in\nITS, because it allows edge devices, Road Side Units (RSUs), and Central Server\n(CS) to jointly train the machine learning model. Since RSU collects data from\nthe vehicles passing through its range, the local data of each RSU will have a\nnon-IID distribution, which adversely affects the convergence speed and\naccuracy of FL training. Generating synthetic data locally at individual nodes,\nfollowed by data shuffling among the nodes, is a promising approach to address\nthe Non-IID data problem. In this work, we propose pliable index coding (PIC)\nsolutions for efficient data shuffling among the nodes in an FL system. In\nPIC($S$) problems, a client is satisfied if it can retrieve any $S$ new\nmessages not originally present in its side-information. We particularly\nconsider decentralized pliable index coding problems (DPIC) where the clients\ncommunicate among themselves without a central server to model the data\nshuffling in FL. A class of DPIC, known as Consecutive Decentralized Pliable\nIndex Coding (CDPIC($S$,$K$)), where each client has $K$ consecutive messages\nas side-information, is considered. For CDPIC($S$,$K$) problems, pliable index\ncode designs are provided for any value of $K$ and $S$, and optimality proofs\nfor some of the cases are established. Further, these CDPIC solutions are\napplied for data shuffling in FL, to transform the local data distribution\ntowards IID progressively with each transmission, thereby enhancing the\nperformance of FL. The improvement in the accuracy and convergence of the most\npopular FL technique, FedAvg, and a promising federated submodel technique,\nCELL (Communication Efficient Lottery Learning), are analysed by providing\ndifferent degrees of data shuffling using the proposed CDPIC schemes.", "published": "2025-07-01 10:36:36", "link": "http://arxiv.org/abs/2507.00643v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "On the rank weight hierarchy of $M$-codes", "abstract": "We study the rank weight hierarchy of linear codes which are stable under a\nlinear endomorphism defined over the base field, in particular when the\nendomorphism is cyclic. In this last case, we give a necessary and sufficient\ncondition for such a code to have first rank weight equal to $1$ in terms of\nits generator polynomial, as well as an explicit formula for its last rank\nweight.", "published": "2025-07-01 09:41:29", "link": "http://arxiv.org/abs/2507.00609v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Construction of LDPC convolutional codes with large girth from Latin squares", "abstract": "Due to their capacity approaching performance low-density parity-check (LDPC)\ncodes gained a lot of attention in the last years. The parity-check matrix of\nthe codes can be associated with a bipartite graph, called Tanner graph. To\ndecrease the probability of decoding failure it is desirable to have LDPC codes\nwith large girth of the associated Tanner graph. Moreover, to store such codes\nefficiently, it is desirable to have compact constructions for them. In this\npaper, we present constructions of LDPC convolutional codes with girth up to\n$12$ using a special class of Latin squares and several lifting steps, which\nenables a compact representation of these codes. With these techniques, we can\nprovide constructions for well-performing and efficiently storable time-varying\nand time-invariant LDPC convolutional codes as well as for LDPC block codes.", "published": "2025-07-01 09:18:16", "link": "http://arxiv.org/abs/2507.00591v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Linear rank-metric intersecting codes", "abstract": "In this paper we introduce and investigate rank-metric intersecting codes, a\nnew class of linear codes in the rank-metric context, inspired by the\nwell-studied notion of intersecting codes in the Hamming metric. A rank-metric\ncode is said to be intersecting if any two nonzero codewords have supports\nintersecting non trivially. We explore this class from both a coding-theoretic\nand geometric perspective, highlighting its relationship with minimal codes,\nMRD codes, and Hamming-metric intersecting codes. We derive structural\nproperties, sufficient conditions based on minimum distance, and geometric\ncharacterizations in terms of 2-spannable $q$-systems. We establish upper and\nlower bounds on code parameters and show some constructions, which leave a\nrange of unexplored parameters. Finally, we connect rank-intersecting codes to\nother combinatorial structures such as $(2,1)$-separating systems and\nframeproof codes.", "published": "2025-07-01 08:39:17", "link": "http://arxiv.org/abs/2507.00569v1", "categories": ["math.CO", "cs.IT", "math.IT"], "primary_category": "math.CO"}
{"title": "Best Agent Identification for General Game Playing", "abstract": "We present an efficient and generalised procedure to accurately identify the\nbest performing algorithm for each sub-task in a multi-problem domain. Our\napproach treats this as a set of best arm identification problems for\nmulti-armed bandits, where each bandit corresponds to a specific task and each\narm corresponds to a specific algorithm or agent. We propose an optimistic\nselection process based on the Wilson score interval (Optimistic-WS) that ranks\neach arm across all bandits in terms of their potential regret reduction. We\nevaluate the performance of Optimistic-WS on two of the most popular general\ngame domains, the General Video Game AI (GVGAI) framework and the Ludii general\ngame playing system, with the goal of identifying the highest performing agent\nfor each game within a limited number of trials. Compared to previous best arm\nidentification algorithms for multi-armed bandits, our results demonstrate a\nsubstantial performance improvement in terms of average simple regret. This\nnovel approach can be used to significantly improve the quality and accuracy of\nagent evaluation procedures for general game frameworks, as well as other\nmulti-task domains with high algorithm runtimes.", "published": "2025-07-01 06:07:56", "link": "http://arxiv.org/abs/2507.00451v1", "categories": ["cs.LG", "cs.AI", "cs.DS", "cs.IT", "math.IT", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Accuracy and Security-Guaranteed Participant Selection and Beamforming Design for RIS-Assisted Federated Learning", "abstract": "Federated learning (FL) has emerged as an effective approach for training\nneural network models without requiring the sharing of participants' raw data,\nthereby addressing data privacy concerns. In this paper, we propose a\nreconfigurable intelligent surface (RIS)-assisted FL framework in the presence\nof eavesdropping, where partial edge devices are selected to participate in the\nFL training process. In contrast, the remaining devices serve as cooperative\njammers by transmitting jamming signals to disrupt eavesdropping. We aim to\nminimize the training latency in each FL round by jointly optimizing\nparticipant selection, bandwidth allocation, and RIS beamforming design,\nsubject to the convergence accuracy of FL and the secure uploading\nrequirements. To solve the resulting mixed-integer nonlinear programming\nproblem, we propose a twin delayed deep deterministic policy gradient (TD3)\nalgorithm. Simulation results demonstrate that the proposed scheme reduces the\nFL training latency by approximately 27$\\%$ compared to baselines.", "published": "2025-07-01 02:53:30", "link": "http://arxiv.org/abs/2507.00388v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Wireless AI Evolution: From Statistical Learners to Electromagnetic-Guided Foundation Models", "abstract": "While initial applications of artificial intelligence (AI) in wireless\ncommunications over the past decade have demonstrated considerable potential\nusing specialized models for targeted communication tasks, the revolutionary\ndemands of sixth-generation (6G) networks for holographic communications,\nubiquitous sensing, and native intelligence are propelling a necessary\nevolution towards AI-native wireless networks. The arrival of large AI models\npaves the way for the next phase of Wireless AI, driven by wireless foundation\nmodels (WFMs). In particular, pre-training on universal electromagnetic (EM)\nprinciples equips WFMs with the essential adaptability for a multitude of\ndemanding 6G applications. However, existing large AI models face critical\nlimitations, including pre-training strategies disconnected from EM-compliant\nconstraints leading to physically inconsistent predictions, a lack of embedded\nunderstanding of wave propagation physics, and the inaccessibility of massive\nlabeled datasets for comprehensive EM-aware training. To address these\nchallenges, this article presents an electromagnetic information theory-guided\nself-supervised pre-training (EIT-SPT) framework designed to systematically\ninject EM physics into WFMs. The EIT-SPT framework aims to infuse WFMs with\nintrinsic EM knowledge, thereby enhancing their physical consistency,\ngeneralization capabilities across varied EM landscapes, and overall data\nefficiency. Building upon the proposed EIT-SPT framework, this article first\nelaborates on diverse potential applications in 6G scenarios of WFMs, then\nvalidates the efficacy of the proposed framework through illustrative case\nstudies, and finally summarizes critical open research challenges and future\ndirections for WFMs.", "published": "2025-07-01 01:40:21", "link": "http://arxiv.org/abs/2507.00366v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Jump-Start Reinforcement Learning with Self-Evolving Priors for Extreme Monopedal Locomotion", "abstract": "Reinforcement learning (RL) has shown great potential in enabling quadruped\nrobots to perform agile locomotion. However, directly training policies to\nsimultaneously handle dual extreme challenges, i.e., extreme underactuation and\nextreme terrains, as in monopedal hopping tasks, remains highly challenging due\nto unstable early-stage interactions and unreliable reward feedback. To address\nthis, we propose JumpER (jump-start reinforcement learning via self-evolving\npriors), an RL training framework that structures policy learning into multiple\nstages of increasing complexity. By dynamically generating self-evolving priors\nthrough iterative bootstrapping of previously learned policies, JumpER\nprogressively refines and enhances guidance, thereby stabilizing exploration\nand policy optimization without relying on external expert priors or\nhandcrafted reward shaping. Specifically, when integrated with a structured\nthree-stage curriculum that incrementally evolves action modality, observation\nspace, and task objective, JumpER enables quadruped robots to achieve robust\nmonopedal hopping on unpredictable terrains for the first time. Remarkably, the\nresulting policy effectively handles challenging scenarios that traditional\nmethods struggle to conquer, including wide gaps up to 60 cm, irregularly\nspaced stairs, and stepping stones with distances varying from 15 cm to 35 cm.\nJumpER thus provides a principled and scalable approach for addressing\nlocomotion tasks under the dual challenges of extreme underactuation and\nextreme terrains.", "published": "2025-07-01 23:31:36", "link": "http://arxiv.org/abs/2507.01243v1", "categories": ["cs.RO", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Quantum Machine Learning in Transportation: A Case Study of Pedestrian Stress Modelling", "abstract": "Quantum computing has opened new opportunities to tackle complex machine\nlearning tasks, for instance, high-dimensional data representations commonly\nrequired in intelligent transportation systems. We explore quantum machine\nlearning to model complex skin conductance response (SCR) events that reflect\npedestrian stress in a virtual reality road crossing experiment. For this\npurpose, Quantum Support Vector Machine (QSVM) with an eight-qubit ZZ feature\nmap and a Quantum Neural Network (QNN) using a Tree Tensor Network ansatz and\nan eight-qubit ZZ feature map, were developed on Pennylane. The dataset\nconsists of SCR measurements along with features such as the response amplitude\nand elapsed time, which have been categorized into amplitude-based classes. The\nQSVM achieved good training accuracy, but had an overfitting problem, showing a\nlow test accuracy of 45% and therefore impacting the reliability of the\nclassification model. The QNN model reached a higher test accuracy of 55%,\nmaking it a better classification model than the QSVM and the classic versions.", "published": "2025-07-01 23:18:50", "link": "http://arxiv.org/abs/2507.01235v1", "categories": ["cs.LG", "quant-ph"], "primary_category": "cs.LG"}
{"title": "PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile Device via Additive Side-Tuning", "abstract": "There is a huge gap between numerous intriguing applications fostered by\non-device large language model (LLM) fine-tuning (FT) from fresh mobile data\nand the limited resources of a mobile device. While existing server-assisted\nmethods (e.g., split learning or side-tuning) may enable LLM FT on the local\nmobile device, they suffer from heavy communication burdens of activation\ntransmissions, and may disclose data, labels or fine-tuned models to the\nserver. To address those issues, we develop PAE MobiLLM, a privacy-aware and\nefficient LLM FT method which can be deployed on the mobile device via\nserver-assisted additive side-tuning. To further accelerate FT convergence and\nimprove computing efficiency, PAE MobiLLM integrates activation caching on the\nserver side, which allows the server to reuse historical activations and saves\nthe mobile device from repeatedly computing forward passes for the recurring\ndata samples. Besides, to reduce communication cost, PAE MobiLLM develops a\none-token (i.e., ``pivot'' token) activation shortcut that transmits only a\nsingle activation dimension instead of full activation matrices to guide the\nside network tuning. Last but not least, PAE MobiLLM introduces the additive\nadapter side-network design which makes the server train the adapter modules\nbased on device-defined prediction differences rather than raw ground-truth\nlabels. In this way, the server can only assist device-defined side-network\ncomputing, and learn nothing about data, labels or fine-tuned models.", "published": "2025-07-01 22:27:21", "link": "http://arxiv.org/abs/2507.01216v1", "categories": ["cs.LG", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Deep Learning-Based Intrusion Detection for Automotive Ethernet: Evaluating & Optimizing Fast Inference Techniques for Deployment on Low-Cost Platform", "abstract": "Modern vehicles are increasingly connected, and in this context, automotive\nEthernet is one of the technologies that promise to provide the necessary\ninfrastructure for intra-vehicle communication. However, these systems are\nsubject to attacks that can compromise safety, including flow injection\nattacks. Deep Learning-based Intrusion Detection Systems (IDS) are often\ndesigned to combat this problem, but they require expensive hardware to run in\nreal time. In this work, we propose to evaluate and apply fast neural network\ninference techniques like Distilling and Prunning for deploying IDS models on\nlow-cost platforms in real time. The results show that these techniques can\nachieve intrusion detection times of up to 727 {\\mu}s using a Raspberry Pi 4,\nwith AUCROC values of 0.9890.", "published": "2025-07-01 22:05:02", "link": "http://arxiv.org/abs/2507.01208v1", "categories": ["cs.LG", "cs.CR", "C.2.0; I.2.0"], "primary_category": "cs.LG"}
{"title": "Escaping Platos Cave: JAM for Aligning Independently Trained Vision and Language Models", "abstract": "Independently trained vision and language models inhabit disjoint\nrepresentational spaces, shaped by their respective modalities, objectives, and\narchitectures. Yet an emerging hypothesis - the Platonic Representation\nHypothesis - suggests that such models may nonetheless converge toward a shared\nstatistical model of reality. This compatibility, if it exists, raises a\nfundamental question: can we move beyond post-hoc statistical detection of\nalignment and explicitly optimize for it between such disjoint representations?\nWe cast this Platonic alignment problem as a multi-objective optimization task\n- preserve each modality's native structure while aligning for mutual\ncoherence. We introduce the Joint Autoencoder Modulator (JAM) framework that\njointly trains modality-specific autoencoders on the latent representations of\npre-trained single modality models, encouraging alignment through both\nreconstruction and cross-modal objectives. By analogy, this framework serves as\na method to escape Plato's Cave, enabling the emergence of shared structure\nfrom disjoint inputs. We evaluate this framework across three critical design\naxes: (i) the alignment objective - comparing contrastive loss (Con), its\nhard-negative variant (NegCon), and our Spread loss, (ii) the layer depth at\nwhich alignment is most effective, and (iii) the impact of foundation model\nscale on representational convergence. Our findings show that our lightweight\nPareto-efficient framework reliably induces alignment, even across frozen,\nindependently trained representations, offering both theoretical insight and\npractical pathways for transforming generalist unimodal foundations into\nspecialist multimodal models.", "published": "2025-07-01 21:43:50", "link": "http://arxiv.org/abs/2507.01201v1", "categories": ["cs.LG", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Diffusion Explorer: Interactive Exploration of Diffusion Models", "abstract": "Diffusion models have been central to the development of recent image, video,\nand even text generation systems. They posses striking geometric properties\nthat can be faithfully portrayed in low-dimensional settings. However, existing\nresources for explaining diffusion either require an advanced theoretical\nfoundation or focus on their neural network architectures rather than their\nrich geometric properties. We introduce Diffusion Explorer, an interactive tool\nto explain the geometric properties of diffusion models. Users can train 2D\ndiffusion models in the browser and observe the temporal dynamics of their\nsampling process. Diffusion Explorer leverages interactive animation, which has\nbeen shown to be a powerful tool for making engaging visualizations of dynamic\nsystems, making it well suited to explaining diffusion models which represent\nstochastic processes that evolve over time. Diffusion Explorer is open source\nand a live demo is available at alechelbling.com/Diffusion-Explorer.", "published": "2025-07-01 20:28:02", "link": "http://arxiv.org/abs/2507.01178v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Large Language Model Powered Intelligent Urban Agents: Concepts, Capabilities, and Applications", "abstract": "The long-standing vision of intelligent cities is to create efficient,\nlivable, and sustainable urban environments using big data and artificial\nintelligence technologies. Recently, the advent of Large Language Models (LLMs)\nhas opened new ways toward realizing this vision. With powerful semantic\nunderstanding and reasoning capabilities, LLMs can be deployed as intelligent\nagents capable of autonomously solving complex problems across domains. In this\narticle, we focus on Urban LLM Agents, which are LLM-powered agents that are\nsemi-embodied within the hybrid cyber-physical-social space of cities and used\nfor system-level urban decision-making. First, we introduce the concept of\nurban LLM agents, discussing their unique capabilities and features. Second, we\nsurvey the current research landscape from the perspective of agent workflows,\nencompassing urban sensing, memory management, reasoning, execution, and\nlearning. Third, we categorize the application domains of urban LLM agents into\nfive groups: urban planning, transportation, environment, public safety, and\nurban society, presenting representative works in each group. Finally, we\ndiscuss trustworthiness and evaluation issues that are critical for real-world\ndeployment, and identify several open problems for future research. This survey\naims to establish a foundation for the emerging field of urban LLM agents and\nto provide a roadmap for advancing the intersection of LLMs and urban\nintelligence. A curated list of relevant papers and open-source resources is\nmaintained and continuously updated at\nhttps://github.com/usail-hkust/Awesome-Urban-LLM-Agents.", "published": "2025-07-01 16:18:29", "link": "http://arxiv.org/abs/2507.00914v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA"}
{"title": "Horus: A Protocol for Trustless Delegation Under Uncertainty", "abstract": "Correctness is an emergent property of systems where exposing error is\ncheaper than committing it. In dynamic, low-trust environments, autonomous AI\nagents benefit from delegating work to sub-agents, yet correctness cannot be\nassured through upfront specification or centralized oversight. We propose a\nprotocol that enforces correctness through collateralized claims in a recursive\nverification game. Tasks are published as intents, and solvers compete to\nfulfill them. Selected solvers carry out tasks under risk, with correctness\nchecked post hoc by verifiers. Any challenger can challenge a result by staking\nagainst it to trigger the verification process. Incorrect agents are slashed\nand correct opposition is rewarded, with an escalation path that penalizes\nerroneous verifiers themselves. When incentives are aligned across solvers,\nchallengers, and verifiers, falsification conditions make correctness the Nash\nequilibrium.", "published": "2025-07-01 10:22:35", "link": "http://arxiv.org/abs/2507.00631v2", "categories": ["cs.GT", "cs.AI", "cs.MA", "I.2.11; F.2.2"], "primary_category": "cs.GT"}
{"title": "Twill: Scheduling Compound AI Systems on Heterogeneous Mobile Edge Platforms", "abstract": "Compound AI (cAI) systems chain multiple AI models to solve complex problems.\ncAI systems are typically composed of deep neural networks (DNNs),\ntransformers, and large language models (LLMs), exhibiting a high degree of\ncomputational diversity and dynamic workload variation. Deploying cAI services\non mobile edge platforms poses a significant challenge in scheduling concurrent\nDNN-transformer inference tasks, which arrive dynamically in an unknown\nsequence. Existing mobile edge AI inference strategies manage multi-DNN or\ntransformer-only workloads, relying on design-time profiling, and cannot handle\nconcurrent inference of DNNs and transformers required by cAI systems. In this\nwork, we address the challenge of scheduling cAI systems on heterogeneous\nmobile edge platforms. We present Twill, a run-time framework to handle\nconcurrent inference requests of cAI workloads through task affinity-aware\ncluster mapping and migration, priority-aware task freezing/unfreezing, and\nDVFS, while minimizing inference latency within power budgets. We implement and\ndeploy our Twill framework on the Nvidia Jetson Orin NX platform. We evaluate\nTwill against state-of-the-art edge AI inference techniques over contemporary\nDNNs and LLMs, reducing inference latency by 54% on average, while honoring\npower budgets.", "published": "2025-07-01 07:06:45", "link": "http://arxiv.org/abs/2507.00491v1", "categories": ["cs.MA", "cs.AI", "cs.CV", "cs.PF"], "primary_category": "cs.MA"}
{"title": "Novel Pigeon-inspired 3D Obstacle Detection and Avoidance Maneuver for Multi-UAV Systems", "abstract": "Recent advances in multi-agent systems manipulation have demonstrated a\nrising demand for the implementation of multi-UAV systems in urban areas, which\nare always subjected to the presence of static and dynamic obstacles. Inspired\nby the collective behavior of tilapia fish and pigeons, the focus of the\npresented research is on the introduction of a nature-inspired collision-free\nformation control for a multi-UAV system, considering the obstacle avoidance\nmaneuvers. The developed framework in this study utilizes a semi-distributed\ncontrol approach, in which, based on a probabilistic Lloyd's algorithm, a\ncentralized guidance algorithm works for optimal positioning of the UAVs, while\na distributed control approach has been used for the intervehicle collision and\nobstacle avoidance. Further, the presented framework has been extended to the\n3D space with a novel definition of 3D maneuvers. Finally, the presented\nframework has been applied to multi-UAV systems in 2D and 3D scenarios, and the\nobtained results demonstrated the validity of the presented method in dynamic\nenvironments with stationary and moving obstacles.", "published": "2025-07-01 05:52:21", "link": "http://arxiv.org/abs/2507.00443v1", "categories": ["cs.RO", "cs.AI", "cs.MA"], "primary_category": "cs.RO"}
{"title": "A fourth-order exponential time differencing scheme with real and distinct poles rational approximation for solving non-linear reaction-diffusion systems", "abstract": "A fourth-order, L-stable, exponential time differencing Runge-Kutta type\nscheme is developed to solve nonlinear systems of reaction diffusion equations\nwith nonsmooth data. The new scheme, ETDRK4RDP, is constructed by approximating\nthe matrix exponentials in the ETDRK4 scheme with a fourth order, L-acceptable,\nnon-Pad\\'e rational function having real and distinct poles (RDP). Using RDP\nrational functions to construct the scheme ensures efficient damping of\nspurious oscillations arising from non-smooth initial and boundary conditions\nand a straightforward parallelization. We verify empirically that the new\nETDRK4RDP scheme is fourth-order accurate for several reaction diffusion\nsystems with Dirichlet and Neumann boundary conditions and show it to be more\nefficient than competing exponential time differencing schemes, especially when\nimplemented in parallel, with up to six times speedup in CPU time.", "published": "2025-07-01 23:36:06", "link": "http://arxiv.org/abs/2507.01245v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "On the Intensity-based Inversion Method for Quantitative Quasi-Static Elastography", "abstract": "In this paper, we consider the intensity-based inversion method (IIM) for\nquantitative material parameter estimation in quasi-static elastography. In\nparticular, we consider the problem of estimating the material parameters of a\ngiven sample from two internal measurements, one obtained before and one after\napplying some form of deformation. These internal measurements can be obtained\nvia any imaging modality of choice, for example ultrasound, optical coherence\nor photo-acoustic tomography. Compared to two-step approaches to elastography,\nwhich first estimate internal displacement fields or strains and then\nreconstruct the material parameters from them, the IIM is a one-step approach\nwhich computes the material parameters directly from the internal measurements.\nTo do so, the IIM combines image registration together with a model-based,\nregularized parameter reconstruction approach. This combination has the\nadvantage of avoiding some approximations and derivative computations typically\nfound in two-step approaches, and results in the IIM being generally more\nstable to measurement noise. In the paper, we provide a full convergence\nanalysis of the IIM within the framework of inverse problems, and detail its\napplication to linear elastography. Furthermore, we discuss the numerical\nimplementation of the IIM and provide numerical examples simulating an optical\ncoherence elastography (OCE) experiment.", "published": "2025-07-01 21:55:44", "link": "http://arxiv.org/abs/2507.01207v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Computational Insights into Orthotropic Fracture: Crack-Tip Fields in Strain-Limiting Materials under Non-Uniform Loads", "abstract": "A finite element framework is presented for analyzing crack-tip phenomena in\ntransversely isotropic, strain-limiting elastic materials. Mechanical response\nis characterized by an algebraically nonlinear constitutive model, relating\nstress to linearized strain. Non-physical strain singularities at the crack\napex are mitigated, ensuring bounded strain magnitudes. This methodology\nsignificantly advances boundary value problem (BVP) formulation, especially for\nfirst-order approximate theories. For a transversely isotropic elastic solid\nwith a crack, the governing equilibrium equation, derived from linear momentum\nbalance and the nonlinear constitutive model, is reduced to a second-order,\nvector-valued, quasilinear elliptic BVP. This BVP is solved using a robust\nnumerical scheme combining Picard-type linearization with a continuous Galerkin\nfinite element method for spatial discretization. Numerical results are\npresented for various loading conditions, including uniform tension,\nnon-uniform slope, and parabolic loading, with two distinct material fiber\norientations. It is demonstrated that crack-tip strain growth is substantially\nlower than stress growth. Nevertheless, strain-energy density is found to be\nconcentrated at the crack tip, consistent with linear elastic fracture\nmechanics principles. The proposed framework provides a robust basis for\nformulating physically meaningful, rigorous BVPs, critical for investigating\nfundamental processes like crack propagation, damage, and nucleation in\nanisotropic, strain-limiting elastic solids under diverse loading conditions.", "published": "2025-07-01 19:18:36", "link": "http://arxiv.org/abs/2507.01150v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Stable skeleton integral equations for general coefficient Helmholtz transmission problems", "abstract": "A novel variational formulation of layer potentials and boundary integral\noperators generalizes their classical construction by Green's functions, which\nare not explicitly available for Helmholtz problems with variable coefficients.\nWavenumber explicit estimates and properties like jump conditions follow\ndirectly from their variational definition and enable a non-local\n(``integral'') formulation of acoustic transmission problems (TP) with\npiecewise Lipschitz coefficients. We obtain the well-posedness of the integral\nequations directly from the stability of the underlying TP. The simultaneous\nanalysis for general dimensions and complex wavenumbers (in this paper) imposes\nan artificial boundary on the external Helmholtz problem and employs recent\ninsights into the associated Dirichlet-to-Neumann map.", "published": "2025-07-01 17:40:43", "link": "http://arxiv.org/abs/2507.00991v1", "categories": ["math.AP", "cs.NA", "math.NA", "31B10, 35C15, 45A05, 65R20"], "primary_category": "math.AP"}
{"title": "Anatomy of High-Performance Column-Pivoted QR Decomposition", "abstract": "We introduce an algorithmic framework for performing QR factorization with\ncolumn pivoting (QRCP) on general matrices. The framework enables the design of\npractical QRCP algorithms through user-controlled choices for the core\nsubroutines. We provide a comprehensive overview of how to navigate these\nchoices on modern hardware platforms, offering detailed descriptions of\nalternative methods for both CPUs and GPUs. The practical QRCP algorithms\ndeveloped within this framework are implemented as part of the open-source\nRandLAPACK library. Our empirical evaluation demonstrates that, on a dual AMD\nEPYC 9734 system, the proposed method achieves performance improvements of up\nto two orders of magnitude over LAPACK's standard QRCP routine and greatly\nsurpasses the performance of the current state-of-the-art randomized QRCP\nalgorithm. Additionally, on an NVIDIA H100 GPU, our method attains\napproximately 65 percent of the performance of cuSOLVER's unpivoted QR\nfactorization.", "published": "2025-07-01 17:25:36", "link": "http://arxiv.org/abs/2507.00976v1", "categories": ["cs.MS", "cs.NA", "math.NA"], "primary_category": "cs.MS"}
{"title": "Swarm-based optimization with jumps: a kinetic BGK framework and convergence analysis", "abstract": "Metaheuristic algorithms are powerful tools for global optimization,\nparticularly for non-convex and non-differentiable problems where exact methods\nare often impractical. Particle-based optimization methods, inspired by swarm\nintelligence principles, have shown effectiveness due to their ability to\nbalance exploration and exploitation within the search space. In this work, we\nintroduce a novel particle-based optimization algorithm where velocities are\nupdated via random jumps, a strategy commonly used to enhance stochastic\nexploration. We formalize this approach by describing the dynamics through a\nkinetic modelling of BGK type, offering a unified framework that accommodates\ngeneral noise distributions, including heavy-tailed ones like Cauchy. Under\nsuitable parameter scaling, the model reduces to the Consensus-Based\nOptimization (CBO) dynamics. For non-degenerate Gaussian noise in bounded\ndomains, we prove propagation of chaos and convergence towards minimizers.\nNumerical results on benchmark problems validate the approach and highlight its\nconnection to CBO.", "published": "2025-07-01 15:34:53", "link": "http://arxiv.org/abs/2507.00871v1", "categories": ["math.OC", "cs.NA", "math.NA", "65K10, 90C26, 65C35, 82C40, 35Q90"], "primary_category": "math.OC"}
{"title": "A posteriori and a priori error estimates for linearized thin sheet folding", "abstract": "We describe a posteriori error analysis for a discontinuous Galerkin method\nfor a fourth order elliptic interface problem that arises from a linearized\nmodel of thin sheet folding. The primary contribution is a local efficiency\nbound for an estimator that measures the extent to which the interface\nconditions along the fold are satisfied, which is accomplished by constructing\na novel edge bubble function. We subsequently conduct a medius analysis to\nobtain improved a priori error estimates under the minimal regularity\nassumption on the exact solution. The performance of the method is illustrated\nby numerical experiments.", "published": "2025-07-01 14:40:30", "link": "http://arxiv.org/abs/2507.00807v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Multi-goal-oriented anisotropic error control and mesh adaptivity for time-dependent convection-dominated problems", "abstract": "In this work, we present an anisotropic multi-goal error control based on the\nDual Weighted Residual (DWR) method for time-dependent\nconvection-diffusion-reaction (CDR) equations. This multi-goal oriented\napproach allows for an accurate and efficient error control with regard to\nseveral quantities of interest simultaneously. Using anisotropic interpolation\nand restriction operators, we obtain elementwise error indicators in space and\ntime, where the spatial indicators are additionally separated with respect to\nthe single directions. The directional error indicators quantify anisotropy of\nthe solution with respect to the goals, and produce adaptive, anisotropic\nmeshes that efficiently capture layers. To prevent spurious oscillations the\nstreamline upwind Petrov-Galerkin (SUPG) method is applied to stabilize the\nunderlying system in the case of high P\\'{e}clet numbers. Numerical examples\nshow efficiency and robustness of the proposed approach for several goal\nquantities using established benchmarks for convection-dominated transport.", "published": "2025-07-01 13:03:49", "link": "http://arxiv.org/abs/2507.00723v1", "categories": ["math.NA", "cs.NA", "65M60, 65M50"], "primary_category": "math.NA"}
{"title": "A simplified unified wave-particle method for diatomic gases with rotational and vibrational non-equilibrium", "abstract": "The hypersonic flow around near-space vehicles constitutes a multi-scale flow\nproblem. Due to insufficient molecular collisions to achieve equilibrium,\nrarefied gas effects are present in the flow field. Thus, numerical methods\ncapable of accurately resolving multi-scale flows are required. Furthermore,\nhigh-temperature gas effects in hypersonic flows mean vibrational excitation of\npolyatomic molecules. Consequently, numerical methods accounting for\nnon-equilibrium in rotational and vibrational internal energy modes are\nrequired. This study derives a quantified model-competition (QMC) mechanism for\ndiatomic gases with rotational and vibrational non-equilibrium, starting from\nintegral solutions of kinetic model equations with rotational and vibrational\nenergy. The QMC mechanism categorize collisional and free-transport particles\nin cell, applying computational weighting based on their local scale regimes.\nWe developed a simplified unified wave-particle (SUWP) method for diatomic\ngases based on QMC mechanism. For the macroscopic of the method, a\nthree-temperature model accounting for rotational and vibrational energy is\nincorporated into both the kinetic inviscid flux scheme and {Navier-Stokes}\nsolvers. For the microscopic of the method, a collisionless DSMC solver is\nemployed to resolve non-equilibrium flow physics. This work validates the\nproposed SUWP method with rotational and vibrational non-equilibrium through\nbenchmark cases, including shock tube, shock structures, flow past a cylinder,\nApollo 6 command module and space station Mir. Compared to the DSMC and\ndeterministic methods, the SUWP method exhibits favorable computational\nefficiency while maintaining accuracy.", "published": "2025-07-01 12:59:56", "link": "http://arxiv.org/abs/2507.00720v1", "categories": ["physics.flu-dyn", "cs.NA", "math.NA"], "primary_category": "physics.flu-dyn"}
{"title": "General Perturbation Resilient Dynamic String-Averaging for Inconsistent Problems with Superiorization", "abstract": "In this paper we introduce a General Dynamic String-Averaging (GDSA)\niterative scheme and investigate its convergence properties in the inconsistent\ncase, that is, when the input operators don't have a common fixed point. The\nDynamic String-Averaging Projection (DSAP) algorithm itself was introduced in\nan 2013 paper, where its strong convergence and bounded perturbation resilience\nwere studied in the consistent case (that is, when the sets under consideration\nhad a nonempty intersection). Results involving combination of the DSAP method\nwith superiorization, were presented in 2015. The proof of the weak convergence\nof our GDSA method is based on the notion of \"strong coherence\" of sequences of\noperators that was introduced in 2019. This is an improvement of the property\nof \"coherence\" of sequences of operators introduced in 2001 by Bauschke and\nCombettes. Strong coherence provides a more convenient sufficient convergence\ncondition for methods that employ infinite sequences of operators and it turns\nout to be a useful general tool when applied to proving the convergence of many\niterative methods. In this paper we combine the ideas of both dynamic\nstring-averaging and strong coherence, in order to analyze our GDSA method for\na general class of operators and its bounded perturbation resilience in the\ninconsistent case with weak and strong convergence. We then discuss an\napplication of the GDSA method to the Superiorization Methodology, developing\nresults on the behavior of its superiorized version.", "published": "2025-07-01 12:51:24", "link": "http://arxiv.org/abs/2507.00717v1", "categories": ["math.OC", "cs.NA", "math.FA", "math.NA", "46N10, 46N40, 47H09, 47H10, 47J25, 47N10, 65F10, 65J99"], "primary_category": "math.OC"}
{"title": "Analysis of A Mixed Finite Element Method for Poisson's Equation with Rough Boundary Data", "abstract": "This paper is concerned with finite element methods for Poisson's equation\nwith rough boundary data. Conventional methods require that the boundary data\n$g$ of the problem belongs to $H^{1/2} (\\partial \\Omega)$. However, in many\napplications one has to consider the case when $g$ is in $L^2(\\partial \\Omega)$\nonly. To this end, very weak solutions are considered to establish the\nwell-posedness of the problem. Most previously proposed numerical methods use\nregularizations of the boundary data. The main purpose of this paper is to use\nthe Raviart--Thomas mixed finite element method to solve the Poisson equation\nwith rough boundary data directly. We prove that the solution to the proposed\nmixed method converges to the very weak solution. In particular, we prove that\nthe convergence rate of the numerical solution is $O(h^{1/2})$ in convex\ndomains and $O(h^{s-1/2})$ in nonconvex domains, where $s > 1/2$ depends on the\ngeometry of the domain. The analysis is based on a regularized approach and a\nrigorous estimate for the corresponding dual problem. Numerical experiments\nconfirm the theoretically predicted convergence rates for the proposed mixed\nmethod for Poisson's equation with rough boundary data.", "published": "2025-07-01 11:48:43", "link": "http://arxiv.org/abs/2507.00697v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Sectional Kolmogorov N-widths for parameter-dependent function spaces: A general framework with application to parametrized Friedrichs' systems", "abstract": "We investigate parametrized variational problems where for each parameter the\nsolution may originate from a different parameter-dependent function space. Our\nmain motivation is the theory of Friedrichs' systems, a large abstract class of\nlinear PDE-problems whose solutions are sought in operator- (and thus\nparameter-)dependent graph spaces. Other applications include function spaces\non parametrized domains or discretizations involving data-dependent\nstabilizers. Concerning the set of all parameter-dependent solutions, we argue\nthat in these cases the interpretation as a \"solution manifold\" widely adopted\nin the model order reduction community is no longer applicable. Instead, we\npropose a novel framework based on the theory of fiber bundles and explain how\nestablished concepts such as approximability generalize by introducing a\nSectional Kolmogorov N-width. Further, we prove exponential approximation rates\nof this N-width if a norm equivalence criterion is fulfilled. Applying this\nresult to problems with Friedrichs' structure then gives a sufficient criterion\nthat can be easily verified.", "published": "2025-07-01 11:20:33", "link": "http://arxiv.org/abs/2507.00678v1", "categories": ["math.NA", "cs.NA", "65M22, 41A46, 65N30, 65J05"], "primary_category": "math.NA"}
{"title": "A hyperboloidal method for numerical simulations of multidimensional nonlinear wave equations", "abstract": "We consider the scalar wave equation with power nonlinearity in n+1\ndimensions. Unlike previous numerical studies, we go beyond the radial case and\ndo not assume any symmetries for n=3, and we only impose an SO(n-1) symmetry in\nhigher dimensions. Our method is based on a hyperboloidal foliation of\nMinkowski spacetime and conformal compactification. We focus on the late-time\npower-law decay (tails) of the solutions and compute decay exponents for\ndifferent spherical harmonic modes, for subcritical, critical and\nsupercritical, focusing and defocusing nonlinear wave equations.", "published": "2025-07-01 11:15:20", "link": "http://arxiv.org/abs/2507.00674v1", "categories": ["math.NA", "cs.NA", "math-ph", "math.AP", "math.MP"], "primary_category": "math.NA"}
{"title": "Special measures of smoothness for approximation by sampling operators in $L_p(\\Bbb{R}^d)$", "abstract": "Traditional measures of smoothness often fail to provide accurate $L_p$-error\nestimates for approximation by sampling or interpolation operators, especially\nfor functions with low smoothness. To address this issue, we introduce a\nmodified measure of smoothness that incorporates the local behavior of a\nfunction at the sampling points through the use of averaged operators. With\nthis new tool, we obtain matching direct and inverse error estimates for a wide\nclass of sampling operators and functions in $L_p$ spaces. Additionally, we\nderive a criterion for the convergence of sampling operators in $L_p$, identify\nconditions that ensure the exact rate of approximation, construct realizations\nof $K$-functionals based on these operators, and study the smoothness\nproperties of sampling operators. We also demonstrate how our results apply to\nseveral well-known operators, including the classical Whittaker-Shannon\nsampling operator, sampling operators generated by $B$-splines, and those based\non the Gaussian.", "published": "2025-07-01 11:05:38", "link": "http://arxiv.org/abs/2507.00667v1", "categories": ["math.NA", "cs.NA", "math.CA", "41A05, 41A15, 41A17, 41A25, 41A27"], "primary_category": "math.NA"}
{"title": "A convex lifting approach for the Calder\u00f3n problem", "abstract": "The Calder\\'on problem consists in recovering an unknown coefficient of a\npartial differential equation from boundary measurements of its solution. These\nmeasurements give rise to a highly nonlinear forward operator. As a\nconsequence, the development of reconstruction methods for this inverse problem\nis challenging, as they usually suffer from the problem of local convergence.\nTo circumvent this issue, we propose an alternative approach based on lifting\nand convex relaxation techniques, that have been successfully developed for\nsolving finite-dimensional quadratic inverse problems. This leads to a convex\noptimization problem whose solution coincides with the sought-after\ncoefficient, provided that a non-degenerate source condition holds. We\ndemonstrate the validity of our approach on a toy model where the solution of\nthe partial differential equation is known everywhere in the domain. In this\nsimplified setting, we verify that the non-degenerate source condition holds\nunder certain assumptions on the unknown coefficient. We leave the\ninvestigation of its validity in the Calder\\'on setting for future works.", "published": "2025-07-01 10:37:52", "link": "http://arxiv.org/abs/2507.00645v1", "categories": ["math.AP", "cs.NA", "math.FA", "math.NA", "math.OC"], "primary_category": "math.AP"}
{"title": "Forward Reverse Kernel Regression for the Schr\u00f6dinger bridge problem", "abstract": "In this paper, we study the Schr\\\"odinger Bridge Problem (SBP), which is\ncentral to entropic optimal transport. For general reference processes and\nbegin--endpoint distributions, we propose a forward-reverse iterative Monte\nCarlo procedure to approximate the Schr\\\"odinger potentials in a nonparametric\nway. In particular, we use kernel based Monte Carlo regression in the context\nof Picard iteration of a corresponding fixed point problem. By preserving in\nthe iteration positivity and contractivity in a Hilbert metric sense, we\ndevelop a provably convergent algorithm. Furthermore, we provide convergence\nrates for the potential estimates and prove their optimality. Finally, as an\napplication, we propose a non-nested Monte Carlo procedure for the final\ndimensional distributions of the Schr\\\"odinger Bridge process, based on the\nconstructed potentials and the forward-reverse simulation method for\nconditional diffusions.", "published": "2025-07-01 10:32:36", "link": "http://arxiv.org/abs/2507.00640v1", "categories": ["stat.ML", "cs.LG", "cs.NA", "math.NA", "90C40, 65C05, 62G08"], "primary_category": "stat.ML"}
{"title": "Accelerating MPGP-type Methods Through Preconditioning", "abstract": "This work investigates the acceleration of MPGP-type algorithms using\npreconditioning for the solution of quadratic programming problems. The\npreconditioning needs to be done only on the free set so as not to change the\nconstraints. A variant of preconditioning restricted to the free set is the\npreconditioning in face. The inner preconditioner in preconditioning in face\nneeds to be recomputed or updated every time the free set changes. Here, we\ninvestigate an approximate variant of preconditioning in face that computes the\ninner preconditioner only once. We analyze the error of the approximate variant\nand provide numerical experiments demonstrating that very large speedups can be\nachieved by the approximate variant.", "published": "2025-07-01 09:56:40", "link": "http://arxiv.org/abs/2507.00617v1", "categories": ["math.NA", "cs.NA", "math.OC", "47N10, 65K10", "G.1.6"], "primary_category": "math.NA"}
{"title": "High order global flux schemes for general steady state preservation of shallow water moment equations with non-conservative products", "abstract": "Shallow water moment equations are reduced-order models for free-surface\nflows that allow to represent vertical variations of the velocity profile at\nthe expense of additional evolution equations for a number of additional\nvariables, so called moments. This introduces non-linear non-conservative\nproducts in the system, which make the analytical characterization of steady\nstates much harder if not impossible. The lack of analytical steady states\nposes a challenge for the design of well-balanced schemes, which aim at\npreserving such steady states as crucial in many applications. In this work, we\npresent a family of fully well-balanced, high-order WENO finite volume methods\nfor general hyperbolic balance laws with non-conservative products like the\nshallow water moment equations, for which no analytical steady states are\navailable. The schemes are based on the flux globalization approach, in which\nboth source terms and non-conservative products are integrated with a tailored\nhigh order quadrature in the divergence term. The resulting global flux is then\nreconstructed instead of the conservative variables to preserve all steady\nstates. Numerical tests show the optimal convergence of the method and a\nsignificant error reduction for steady state solutions. Furthermore, we provide\na numerical comparison of perturbed steady states for different families of\nshallow water moment equations, which illustrates the flexibility of our method\nthat is valid for general equations without prior knowledge of steady states.", "published": "2025-07-01 08:51:46", "link": "http://arxiv.org/abs/2507.00573v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Isogeometric contact analysis in subsea umbilical and power cables", "abstract": "Subsea umbilical and power cables contain a large number of contact\ninterfaces between different geometries and materials. These complex\ninteractions rise significant challenges for accurately considering contact\nsurface properties by using traditional analytical solutions or finite element\nmethods. These properties have been identified as the most sensitive parameters\nwhen performing the numerical simulation for stress analysis. Therefore, it is\nessential to apply a novel approach for contact analysis which improves the\naccuracy and efficiency for predicting contact properties. This paper presents\nan isogeometric analysis (IGA) approach addressing contact problems in dynamic\numbilicals and power cables. Firstly, this isogeometric contact algorithm is\nformulated in MATLAB as a tool including the geometry description, contact\ndetection and penalty function. Secondly, the contact interface between a steel\ntube and an outer sheath in an dynamic umbilical is established by this IGA\ncontact algorithm and validated against that in ABAQUS for proving the accuracy\nand efficiency of IGA. Finally, the effects of element refinement, geometrical\ndescription, penalty factor on the accuracy, efficiency and stability of IGA\nare discussed.", "published": "2025-07-01 08:30:56", "link": "http://arxiv.org/abs/2507.00563v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "An inverse-free fixed-time stable dynamical system and its forward-Euler discretization for solving generalized absolute value equations", "abstract": "An inverse-free dynamical system is proposed to solve the generalized\nabsolute value equation (GAVE) within a fixed time, where the time of\nconvergence is finite and is uniformly bounded for all initial points.\nMoreover, an iterative method obtained by using the forward-Euler\ndiscretization of the proposed dynamic model are developed and sufficient\nconditions which guarantee that the discrete iteration globally converge to an\narbitrarily small neighborhood of the unique solution of GAVE within a finite\nnumber of iterative steps are given.", "published": "2025-07-01 07:49:39", "link": "http://arxiv.org/abs/2507.00531v1", "categories": ["math.NA", "cs.NA", "math.OC"], "primary_category": "math.NA"}
{"title": "The Fourier spectral approach to the spatial discretization of quasilinear hyperbolic systems", "abstract": "We discuss the rigorous justification of the spatial discretization by means\nof Fourier spectral methods of quasilinear first-order hyperbolic systems. We\nprovide uniform stability estimates that grant spectral convergence of the\n(spatially) semi-discretized solutions towards the corresponding continuous\nsolution provided that the underlying system satisfies some suitable structural\nassumptions. We consider a setting with sharp low-pass filters and a setting\nwith smooth low-pass filters and argue that - at least theoretically - smooth\nlow-pass filters are operable on a larger class of systems. While our\ntheoretical results are supported with numerical evidence, we also pinpoint\nsome behavior of the numerical method that currently has no theoretical\nexplanation.", "published": "2025-07-01 07:31:49", "link": "http://arxiv.org/abs/2507.00516v1", "categories": ["math.NA", "cs.NA", "math.AP", "65M12, 65M70, 76M22"], "primary_category": "math.NA"}
{"title": "Learning collective variables that respect permutational symmetry", "abstract": "In addition to translational and rotational symmetries, clusters of identical\ninteracting particles possess permutational symmetry. Coarse-grained models for\nsuch systems are instrumental in identifying metastable states, providing an\neffective description of their dynamics, and estimating transition rates. We\npropose a numerical framework for learning collective variables that respect\ntranslational, rotational, and permutational symmetries, and for estimating\ntransition rates and residence times. It combines a sort-based featurization,\nresidence manifold learning in the feature space, and learning collective\nvariables with autoencoders whose loss function utilizes the orthogonality\nrelationship (Legoll and Lelievre, 2010). The committor of the resulting\nreduced model is used as the reaction coordinate in the forward flux sampling\nand to design a control for sampling the transition path process. We offer two\ncase studies, the Lennard-Jones-7 in 2D and the Lennard-Jones-8 in 3D. The\ntransition rates and residence times computed with the aid of the reduced\nmodels agree with those obtained via brute-force methods.", "published": "2025-07-01 03:50:43", "link": "http://arxiv.org/abs/2507.00408v1", "categories": ["physics.chem-ph", "cs.NA", "math.NA", "82B31, 60G99, 70F99"], "primary_category": "physics.chem-ph"}
{"title": "Adaptive finite element convergence analysis of AT1 phase-field model for quasi-static fracture in strain-limiting solids", "abstract": "This research rigorously investigates the convergence of adaptive finite\nelement methods for regularized variational models of quasi-static brittle\nfracture in elastic solids. We specifically examine a novel Ambrosio-Tortorelli\n(AT1) phase-field model within the framework of elasticity theories,\nparticularly for material models characterized by an algebraically nonlinear\nstress-strain relationship. Two distinct and novel adaptive mesh refinement\nalgorithms, underpinned by robust local error indicators, were introduced to\nefficiently solve the underlying nonlinear energy minimization problem. A\ndetailed convergence analysis was conducted on the sequences of minimizers\nproduced by these strategies. Our findings rigorously demonstrate that the\nminimizer sequences from the first adaptive algorithm achieve convergence to a\npredefined tolerance. Crucially, the second algorithm is proven to generate\ninherently convergent sequences, thereby eliminating the need for an explicit\nstopping criterion. The practical effectiveness of this proposed adaptive\nframework is thoroughly validated through extensive numerical simulations. A\ncase study involving an edge crack in an elastic body, governed by an\nalgebraically nonlinear strain-limiting relationship and subjected to\nanti-plane shear-type loading, is presented. Critical comparisons of the energy\ncomponents-bulk, surface, and total-showcase the superior performance of both\nadaptive algorithms.", "published": "2025-07-01 02:17:08", "link": "http://arxiv.org/abs/2507.00376v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Affine-Invariant Global Non-Asymptotic Convergence Analysis of BFGS under Self-Concordance", "abstract": "In this paper, we establish global non-asymptotic convergence guarantees for\nthe BFGS quasi-Newton method without requiring strong convexity or the\nLipschitz continuity of the gradient or Hessian. Instead, we consider the\nsetting where the objective function is strictly convex and strongly\nself-concordant. For an arbitrary initial point and any arbitrary\npositive-definite initial Hessian approximation, we prove global linear and\nsuperlinear convergence guarantees for BFGS when the step size is determined\nusing a line search scheme satisfying the weak Wolfe conditions. Moreover, all\nour global guarantees are affine-invariant, with the convergence rates\ndepending solely on the initial error and the strongly self-concordant\nconstant. Our results extend the global non-asymptotic convergence theory of\nBFGS beyond traditional assumptions and, for the first time, establish\naffine-invariant convergence guarantees aligning with the inherent affine\ninvariance of the BFGS method.", "published": "2025-07-01 01:21:58", "link": "http://arxiv.org/abs/2507.00361v1", "categories": ["math.OC", "cs.NA", "math.NA"], "primary_category": "math.OC"}
{"title": "Optimization Method of Multi-factor Investment Model Driven by Deep Learning for Risk Control", "abstract": "Propose a deep learning driven multi factor investment model optimization\nmethod for risk control. By constructing a deep learning model based on Long\nShort Term Memory (LSTM) and combining it with a multi factor investment model,\nwe optimize factor selection and weight determination to enhance the model's\nadaptability and robustness to market changes. Empirical analysis shows that\nthe LSTM model is significantly superior to the benchmark model in risk control\nindicators such as maximum retracement, Sharp ratio and value at risk (VaR),\nand shows strong adaptability and robustness in different market environments.\nFurthermore, the model is applied to the actual portfolio to optimize the asset\nallocation, which significantly improves the performance of the portfolio,\nprovides investors with more scientific and accurate investment decision-making\nbasis, and effectively balances the benefits and risks.", "published": "2025-07-01 00:14:09", "link": "http://arxiv.org/abs/2507.00332v1", "categories": ["q-fin.CP"], "primary_category": "q-fin.CP"}
{"title": "Ranking Quantilized Mean-Field Games with an Application to Early-Stage Venture Investments", "abstract": "Quantilized mean-field game models involve quantiles of the population's\ndistribution. We study a class of such games with a capacity for ranking games,\nwhere the performance of each agent is evaluated based on its terminal state\nrelative to the population's $\\alpha$-quantile value, $\\alpha \\in (0,1)$. This\nevaluation criterion is designed to select the top $(1-\\alpha)\\%$ performing\nagents. We provide two formulations for this competition: a target-based\nformulation and a threshold-based formulation. In the former and latter\nformulations, to satisfy the selection condition, each agent aims for its\nterminal state to be \\textit{exactly} equal and \\textit{at least} equal to the\npopulation's $\\alpha$-quantile value, respectively.\n  For the target-based formulation, we obtain an analytic solution and\ndemonstrate the $\\epsilon$-Nash property for the asymptotic best-response\nstrategies in the $N$-player game. Specifically, the quantilized mean-field\nconsistency condition is expressed as a set of forward-backward ordinary\ndifferential equations, characterizing the $\\alpha$-quantile value at\nequilibrium. For the threshold-based formulation, we obtain a semi-explicit\nsolution and numerically solve the resulting quantilized mean-field consistency\ncondition.\n  Subsequently, we propose a new application in the context of early-stage\nventure investments, where a venture capital firm financially supports a group\nof start-up companies engaged in a competition over a finite time horizon, with\nthe goal of selecting a percentage of top-ranking ones to receive the next\nround of funding at the end of the time horizon. We present the results and\ninterpretations of numerical experiments for both formulations discussed in\nthis context and show that the target-based formulation provides a very good\napproximation for the threshold-based formulation.", "published": "2025-07-01 15:24:14", "link": "http://arxiv.org/abs/2507.00853v1", "categories": ["math.OC", "cs.SY", "eess.SY", "q-fin.MF"], "primary_category": "math.OC"}
{"title": "Scale-Dependent Multifractality in Bitcoin Realised Volatility: Implications for Rough Volatility Modelling", "abstract": "We assess the applicability of rough volatility models to Bitcoin realised\nvolatility using the normalised p-variation framework of Cont and Das (2024).\nApplying this model free estimator to high-frequency Bitcoin data from 2017 to\n2024 across multiple sampling resolutions, we find that the normalised\nstatistic remains strictly negative throughout, precluding the estimation of a\nvalid roughness index. Stationarity tests and robustness checks reveal no\nsignificant evidence of non-stationarity or structural breaks as explanatory\nfactors. Instead, convergent evidence from three complementary diagnostics,\nnamely multifractal detrended fluctuation analysis, log-log moment scaling, and\nwavelet leaders, reveals a multifractal structure in Bitcoin volatility. This\nscale-dependent behaviour violates the homogeneity assumptions underlying rough\nvolatility estimation and accounts for the estimator's systematic failure.\nThese findings suggest that while rough volatility models perform well in\ntraditional markets, they are structurally misaligned with the empirical\nfeatures of Bitcoin volatility.", "published": "2025-07-01 08:54:20", "link": "http://arxiv.org/abs/2507.00575v1", "categories": ["q-fin.ST", "q-fin.MF", "60G22, 62M10"], "primary_category": "q-fin.ST"}
{"title": "Shrinkage-Based Regressions with Many Related Treatments", "abstract": "When using observational causal models, practitioners often want to\ndisentangle the effects of many related, partially-overlapping treatments.\nExamples include estimating treatment effects of different marketing\ntouchpoints, ordering different types of products, or signing up for different\nservices. Common approaches that estimate separate treatment coefficients are\ntoo noisy for practical decision-making. We propose a computationally light\nmodel that uses a customized ridge regression to move between a heterogeneous\nand a homogenous model: it substantially reduces MSE for the effects of each\nindividual sub-treatment while allowing us to easily reconstruct the effects of\nan aggregated treatment. We demonstrate the properties of this estimator in\ntheory and simulation, and illustrate how it has unlocked targeted\ndecision-making at Wayfair.", "published": "2025-07-01 21:44:30", "link": "http://arxiv.org/abs/2507.01202v1", "categories": ["econ.EM", "stat.ME", "stat.ML"], "primary_category": "econ.EM"}
{"title": "Proof of a perfect platonic representation hypothesis", "abstract": "In this note, we elaborate on and explain in detail the proof given by Ziyin\net al. (2025) of the \"perfect\" Platonic Representation Hypothesis (PRH) for the\nembedded deep linear network model (EDLN). We show that if trained with SGD,\ntwo EDLNs with different widths and depths and trained on different data will\nbecome Perfectly Platonic, meaning that every possible pair of layers will\nlearn the same representation up to a rotation. Because most of the global\nminima of the loss function are not Platonic, that SGD only finds the perfectly\nPlatonic solution is rather extraordinary. The proof also suggests at least six\nways the PRH can be broken. We also show that in the EDLN model, the emergence\nof the Platonic representations is due to the same reason as the emergence of\nprogressive sharpening. This implies that these two seemingly unrelated\nphenomena in deep learning can, surprisingly, have a common cause. Overall, the\ntheory and proof highlight the importance of understanding emergent \"entropic\nforces\" due to the irreversibility of SGD training and their role in\nrepresentation learning. The goal of this note is to be instructive and avoid\nlengthy technical details.", "published": "2025-07-01 18:01:32", "link": "http://arxiv.org/abs/2507.01098v1", "categories": ["cs.LG", "cond-mat.dis-nn", "q-bio.NC", "stat.ML"], "primary_category": "cs.LG"}
{"title": "An in depth look at the Procrustes-Wasserstein distance: properties and barycenters", "abstract": "Due to its invariance to rigid transformations such as rotations and\nreflections, Procrustes-Wasserstein (PW) was introduced in the literature as an\noptimal transport (OT) distance, alternative to Wasserstein and more suited to\ntasks such as the alignment and comparison of point clouds. Having that\napplication in mind, we carefully build a space of discrete probability\nmeasures and show that over that space PW actually is a distance. Algorithms to\nsolve the PW problems already exist, however we extend the PW framework by\ndiscussing and testing several initialization strategies. We then introduce the\nnotion of PW barycenter and detail an algorithm to estimate it from the data.\nThe result is a new method to compute representative shapes from a collection\nof point clouds. We benchmark our method against existing OT approaches,\ndemonstrating superior performance in scenarios requiring precise alignment and\nshape preservation. We finally show the usefulness of the PW barycenters in an\narchaeological context. Our results highlight the potential of PW in boosting\n2D and 3D point cloud analysis for machine learning and computational geometry\napplications.", "published": "2025-07-01 15:59:27", "link": "http://arxiv.org/abs/2507.00894v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Ordinality in Discrete-level Question Difficulty Estimation: Introducing Balanced DRPS and OrderedLogitNN", "abstract": "Recent years have seen growing interest in Question Difficulty Estimation\n(QDE) using natural language processing techniques. Question difficulty is\noften represented using discrete levels, framing the task as ordinal regression\ndue to the inherent ordering from easiest to hardest. However, the literature\nhas neglected the ordinal nature of the task, relying on classification or\ndiscretized regression models, with specialized ordinal regression methods\nremaining unexplored. Furthermore, evaluation metrics are tightly coupled to\nthe modeling paradigm, hindering cross-study comparability. While some metrics\nfail to account for the ordinal structure of difficulty levels, none adequately\naddress class imbalance, resulting in biased performance assessments. This\nstudy addresses these limitations by benchmarking three types of model outputs\n-- discretized regression, classification, and ordinal regression -- using the\nbalanced Discrete Ranked Probability Score (DRPS), a novel metric that jointly\ncaptures ordinality and class imbalance. In addition to using popular ordinal\nregression methods, we propose OrderedLogitNN, extending the ordered logit\nmodel from econometrics to neural networks. We fine-tune BERT on the RACE++ and\nARC datasets and find that OrderedLogitNN performs considerably better on\ncomplex tasks. The balanced DRPS offers a robust and fair evaluation metric for\ndiscrete-level QDE, providing a principled foundation for future research.", "published": "2025-07-01 13:38:33", "link": "http://arxiv.org/abs/2507.00736v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Harnessing the Power of Reinforcement Learning for Adaptive MCMC", "abstract": "Sampling algorithms drive probabilistic machine learning, and recent years\nhave seen an explosion in the diversity of tools for this task. However, the\nincreasing sophistication of sampling algorithms is correlated with an increase\nin the tuning burden. There is now a greater need than ever to treat the tuning\nof samplers as a learning task in its own right. In a conceptual breakthrough,\nWang et al (2025) formulated Metropolis-Hastings as a Markov decision process,\nopening up the possibility for adaptive tuning using Reinforcement Learning\n(RL). Their emphasis was on theoretical foundations; realising the practical\nbenefit of Reinforcement Learning Metropolis-Hastings (RLMH) was left for\nsubsequent work. The purpose of this paper is twofold: First, we observe the\nsurprising result that natural choices of reward, such as the acceptance rate,\nor the expected squared jump distance, provide insufficient signal for training\nRLMH. Instead, we propose a novel reward based on the contrastive divergence,\nwhose superior performance in the context of RLMH is demonstrated. Second, we\nexplore the potential of RLMH and present adaptive gradient-based samplers that\nbalance flexibility of the Markov transition kernel with learnability of the\nassociated RL task. A comprehensive simulation study using the posteriordb\nbenchmark supports the practical effectiveness of RLMH.", "published": "2025-07-01 11:12:34", "link": "http://arxiv.org/abs/2507.00671v1", "categories": ["stat.CO", "cs.LG", "stat.ML"], "primary_category": "stat.CO"}
{"title": "GANs Secretly Perform Approximate Bayesian Model Selection", "abstract": "Generative Adversarial Networks (GANs) are popular and successful generative\nmodels. Despite their success, optimization is notoriously challenging and they\nrequire regularization against overfitting. In this work, we explain the\nsuccess and limitations of GANs by interpreting them as probabilistic\ngenerative models. This interpretation enables us to view GANs as Bayesian\nneural networks with partial stochasticity, allowing us to establish conditions\nof universal approximation. We can then cast the adversarial-style optimization\nof several variants of GANs as the optimization of a proxy for the marginal\nlikelihood. Taking advantage of the connection between marginal likelihood\noptimization and Occam's razor, we can define regularization and optimization\nstrategies to smooth the loss landscape and search for solutions with minimum\ndescription length, which are associated with flat minima and good\ngeneralization. The results on a wide range of experiments indicate that these\nstrategies lead to performance improvements and pave the way to a deeper\nunderstanding of regularization strategies for GANs.", "published": "2025-07-01 10:49:06", "link": "http://arxiv.org/abs/2507.00651v1", "categories": ["cs.LG", "cs.CV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Posterior Inference in Latent Space for Scalable Constrained Black-box Optimization", "abstract": "Optimizing high-dimensional black-box functions under black-box constraints\nis a pervasive task in a wide range of scientific and engineering problems.\nThese problems are typically harder than unconstrained problems due to\nhard-to-find feasible regions. While Bayesian optimization (BO) methods have\nbeen developed to solve such problems, they often struggle with the curse of\ndimensionality. Recently, generative model-based approaches have emerged as a\npromising alternative for constrained optimization. However, they suffer from\npoor scalability and are vulnerable to mode collapse, particularly when the\ntarget distribution is highly multi-modal. In this paper, we propose a new\nframework to overcome these challenges. Our method iterates through two stages.\nFirst, we train flow-based models to capture the data distribution and\nsurrogate models that predict both function values and constraint violations\nwith uncertainty quantification. Second, we cast the candidate selection\nproblem as a posterior inference problem to effectively search for promising\ncandidates that have high objective values while not violating the constraints.\nDuring posterior inference, we find that the posterior distribution is highly\nmulti-modal and has a large plateau due to constraints, especially when\nconstraint feedback is given as binary indicators of feasibility. To mitigate\nthis issue, we amortize the sampling from the posterior distribution in the\nlatent space of flow-based models, which is much smoother than that in the data\nspace. We empirically demonstrate that our method achieves superior performance\non various synthetic and real-world constrained black-box optimization tasks.\nOur code is publicly available \\href{https://github.com/umkiyoung/CiBO}{here}.", "published": "2025-07-01 06:55:36", "link": "http://arxiv.org/abs/2507.00480v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "GRAND: Graph Release with Assured Node Differential Privacy", "abstract": "Differential privacy is a well-established framework for safeguarding\nsensitive information in data. While extensively applied across various\ndomains, its application to network data -- particularly at the node level --\nremains underexplored. Existing methods for node-level privacy either focus\nexclusively on query-based approaches, which restrict output to pre-specified\nnetwork statistics, or fail to preserve key structural properties of the\nnetwork. In this work, we propose GRAND (Graph Release with Assured Node\nDifferential privacy), which is, to the best of our knowledge, the first\nnetwork release mechanism that releases entire networks while ensuring\nnode-level differential privacy and preserving structural properties. Under a\nbroad class of latent space models, we show that the released network\nasymptotically follows the same distribution as the original network. The\neffectiveness of the approach is evaluated through extensive experiments on\nboth synthetic and real-world datasets.", "published": "2025-07-01 03:39:08", "link": "http://arxiv.org/abs/2507.00402v1", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME", "stat.TH"], "primary_category": "stat.ML"}
{"title": "Classical Guitar Duet Separation using GuitarDuets -- a Dataset of Real and Synthesized Guitar Recordings", "abstract": "Recent advancements in music source separation (MSS) have focused in the\nmulti-timbral case, with existing architectures tailored for the separation of\ndistinct instruments, overlooking thus the challenge of separating instruments\nwith similar timbral characteristics. Addressing this gap, our work focuses on\nmonotimbral MSS, specifically within the context of classical guitar duets. To\nthis end, we introduce the GuitarDuets dataset, featuring a combined total of\napproximately three hours of real and synthesized classical guitar duet\nrecordings, as well as note-level annotations of the synthesized duets. We\nperform an extensive cross-dataset evaluation by adapting Demucs, a\nstate-of-the-art MSS architecture, to monotimbral source separation.\nFurthermore, we develop a joint permutation-invariant transcription and\nseparation framework, to exploit note event predictions as auxiliary\ninformation. Our results indicate that utilizing both the real and synthesized\nsubsets of GuitarDuets leads to improved separation performance in an\nindependently recorded test set compared to utilizing solely one subset. We\nalso find that while the availability of ground-truth note labels greatly helps\nthe performance of the separation network, the predicted note estimates result\nonly in marginal improvement. Finally, we discuss the behavior of commonly\nutilized metrics, such as SDR and SI-SDR, in the context of monotimbral MSS.", "published": "2025-07-01 20:22:56", "link": "http://arxiv.org/abs/2507.01172v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "A Review on Sound Source Localization in Robotics: Focusing on Deep Learning Methods", "abstract": "Sound source localization (SSL) adds a spatial dimension to auditory\nperception, allowing a system to pinpoint the origin of speech, machinery\nnoise, warning tones, or other acoustic events, capabilities that facilitate\nrobot navigation, human-machine dialogue, and condition monitoring. While\nexisting surveys provide valuable historical context, they typically address\ngeneral audio applications and do not fully account for robotic constraints or\nthe latest advancements in deep learning. This review addresses these gaps by\noffering a robotics-focused synthesis, emphasizing recent progress in deep\nlearning methodologies. We start by reviewing classical methods such as Time\nDifference of Arrival (TDOA), beamforming, Steered-Response Power (SRP), and\nsubspace analysis. Subsequently, we delve into modern machine learning (ML) and\ndeep learning (DL) approaches, discussing traditional ML and neural networks\n(NNs), convolutional neural networks (CNNs), convolutional recurrent neural\nnetworks (CRNNs), and emerging attention-based architectures. The data and\ntraining strategy that are the two cornerstones of DL-based SSL are explored.\nStudies are further categorized by robot types and application domains to\nfacilitate researchers in identifying relevant work for their specific\ncontexts. Finally, we highlight the current challenges in SSL works in general,\nregarding environmental robustness, sound source multiplicity, and specific\nimplementation constraints in robotics, as well as data and learning strategies\nin DL-based SSL. Also, we sketch promising directions to offer an actionable\nroadmap toward robust, adaptable, efficient, and explainable DL-based SSL for\nnext-generation robots.", "published": "2025-07-01 19:00:50", "link": "http://arxiv.org/abs/2507.01143v1", "categories": ["cs.RO", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.RO"}
{"title": "MambAttention: Mamba with Multi-Head Attention for Generalizable Single-Channel Speech Enhancement", "abstract": "With the advent of new sequence models like Mamba and xLSTM, several studies\nhave shown that these models match or outperform state-of-the-art models in\nsingle-channel speech enhancement, automatic speech recognition, and\nself-supervised audio representation learning. However, prior research has\ndemonstrated that sequence models like LSTM and Mamba tend to overfit to the\ntraining set. To address this issue, previous works have shown that adding\nself-attention to LSTMs substantially improves generalization performance for\nsingle-channel speech enhancement. Nevertheless, neither the concept of hybrid\nMamba and time-frequency attention models nor their generalization performance\nhave been explored for speech enhancement. In this paper, we propose a novel\nhybrid architecture, MambAttention, which combines Mamba and shared time- and\nfrequency-multi-head attention modules for generalizable single-channel speech\nenhancement. To train our model, we introduce VoiceBank+Demand Extended\n(VB-DemandEx), a dataset inspired by VoiceBank+Demand but with more challenging\nnoise types and lower signal-to-noise ratios. Trained on VB-DemandEx, our\nproposed MambAttention model significantly outperforms existing\nstate-of-the-art LSTM-, xLSTM-, Mamba-, and Conformer-based systems of similar\ncomplexity across all reported metrics on two out-of-domain datasets: DNS 2020\nand EARS-WHAM_v2, while matching their performance on the in-domain dataset\nVB-DemandEx. Ablation studies highlight the role of weight sharing between the\ntime- and frequency-multi-head attention modules for generalization\nperformance. Finally, we explore integrating the shared time- and\nfrequency-multi-head attention modules with LSTM and xLSTM, which yields a\nnotable performance improvement on the out-of-domain datasets. However, our\nMambAttention model remains superior on both out-of-domain datasets across all\nreported evaluation metrics.", "published": "2025-07-01 17:16:05", "link": "http://arxiv.org/abs/2507.00966v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improving Stereo 3D Sound Event Localization and Detection: Perceptual Features, Stereo-specific Data Augmentation, and Distance Normalization", "abstract": "This technical report presents our submission to Task 3 of the DCASE 2025\nChallenge: Stereo Sound Event Localization and Detection (SELD) in Regular\nVideo Content. We address the audio-only task in this report and introduce\nseveral key contributions. First, we design perceptually-motivated input\nfeatures that improve event detection, sound source localization, and distance\nestimation. Second, we adapt augmentation strategies specifically for the\nintricacies of stereo audio, including channel swapping and time-frequency\nmasking. We also incorporate the recently proposed FilterAugment technique that\nhas yet to be explored for SELD work. Lastly, we apply a distance normalization\napproach during training to stabilize regression targets. Experiments on the\nstereo STARSS23 dataset demonstrate consistent performance gains across all\nSELD metrics. Code to replicate our work is available in this repository:\nhttps://github.com/itsjunwei/NTU_SNTL_Task3", "published": "2025-07-01 15:37:46", "link": "http://arxiv.org/abs/2507.00874v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "LearnAFE: Circuit-Algorithm Co-design Framework for Learnable Audio Analog Front-End", "abstract": "This paper presents a circuit-algorithm co-design framework for learnable\nanalog front-end (AFE) in audio signal classification. Designing AFE and\nbackend classifiers separately is a common practice but non-ideal, as shown in\nthis paper. Instead, this paper proposes a joint optimization of the backend\nclassifier with the AFE's transfer function to achieve system-level optimum.\nMore specifically, the transfer function parameters of an analog bandpass\nfilter (BPF) bank are tuned in a signal-to-noise ratio (SNR)-aware training\nloop for the classifier. Using a co-design loss function LBPF, this work shows\nsuperior optimization of both the filter bank and the classifier. Implemented\nin open-source SKY130 130nm CMOS process, the optimized design achieved\n90.5%-94.2% accuracy for 10-keyword classification task across a wide range of\ninput signal SNR from 5 dB to 20 dB, with only 22k classifier parameters.\nCompared to conventional approach, the proposed audio AFE achieves 8.7% and\n12.9% reduction in power and capacitor area respectively.", "published": "2025-07-01 13:59:24", "link": "http://arxiv.org/abs/2507.00755v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MuteSwap: Silent Face-based Voice Conversion", "abstract": "Conventional voice conversion modifies voice characteristics from a source\nspeaker to a target speaker, relying on audio input from both sides. However,\nthis process becomes infeasible when clean audio is unavailable, such as in\nsilent videos or noisy environments. In this work, we focus on the task of\nSilent Face-based Voice Conversion (SFVC), which does voice conversion entirely\nfrom visual inputs. i.e., given images of a target speaker and a silent video\nof a source speaker containing lip motion, SFVC generates speech aligning the\nidentity of the target speaker while preserving the speech content in the\nsource silent video. As this task requires generating intelligible speech and\nconverting identity using only visual cues, it is particularly challenging. To\naddress this, we introduce MuteSwap, a novel framework that employs contrastive\nlearning to align cross-modality identities and minimize mutual information to\nseparate shared visual features. Experimental results show that MuteSwap\nachieves impressive performance in both speech synthesis and identity\nconversion, especially under noisy conditions where methods dependent on audio\ninput fail to produce intelligible results, demonstrating both the\neffectiveness of our training approach and the feasibility of SFVC.", "published": "2025-07-01 07:13:34", "link": "http://arxiv.org/abs/2507.00498v1", "categories": ["cs.SD", "cs.CV", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "AudioBERTScore: Objective Evaluation of Environmental Sound Synthesis Based on Similarity of Audio embedding Sequences", "abstract": "We propose a novel objective evaluation metric for synthesized audio in\ntext-to-audio (TTA), aiming to improve the performance of TTA models. In TTA,\nsubjective evaluation of the synthesized sound is an important, but its\nimplementation requires monetary costs. Therefore, objective evaluation such as\nmel-cepstral distortion are used, but the correlation between these objective\nmetrics and subjective evaluation values is weak. Our proposed objective\nevaluation metric, AudioBERTScore, calculates the similarity between embedding\nof the synthesized and reference sounds. The method is based not only on the\nmax-norm used in conventional BERTScore but also on the $p$-norm to reflect the\nnon-local nature of environmental sounds. Experimental results show that scores\nobtained by the proposed method have a higher correlation with subjective\nevaluation values than conventional metrics.", "published": "2025-07-01 06:46:28", "link": "http://arxiv.org/abs/2507.00475v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Mitigating Language Mismatch in SSL-Based Speaker Anonymization", "abstract": "Speaker anonymization aims to protect speaker identity while preserving\ncontent information and the intelligibility of speech. However, most speaker\nanonymization systems (SASs) are developed and evaluated using only English,\nresulting in degraded utility for other languages. This paper investigates\nlanguage mismatch in SASs for Japanese and Mandarin speech. First, we fine-tune\na self-supervised learning (SSL)-based content encoder with Japanese speech to\nverify effective language adaptation. Then, we propose fine-tuning a\nmultilingual SSL model with Japanese speech and evaluating the SAS in Japanese\nand Mandarin. Downstream experiments show that fine-tuning an English-only SSL\nmodel with the target language enhances intelligibility while maintaining\nprivacy and that multilingual SSL further extends SASs' utility across\ndifferent languages. These findings highlight the importance of language\nadaptation and multilingual pre-training of SSLs for robust multilingual\nspeaker anonymization.", "published": "2025-07-01 06:15:47", "link": "http://arxiv.org/abs/2507.00458v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Enhancing Open RAN Digital Twin Through Power Consumption Measurement", "abstract": "The increasing demand for high-speed, ultra-reliable and low-latency\ncommunications in 5G and beyond networks has led to a significant increase in\npower consumption, particularly within the Radio Access Network (RAN). This\ngrowing energy demand raises operational and sustainability challenges for\nmobile network operators, requiring novel solutions to enhance energy\nefficiency while maintaining Quality of Service (QoS). 5G networks are evolving\ntowards disaggregated, programmable, and intelligent architectures, with Open\nRadio Access Network (O-RAN) spearheaded by the O-RAN Alliance, enabling\ngreater flexibility, interoperability, and cost-effectiveness. However, this\ndisaggregated approach introduces new complexities, especially in terms of\npower consumption across different network components, including Open Radio\nUnits (RUs), Open Distributed Units (DUs) and Open Central Units (CUs).\nUnderstanding the power efficiency of different O-RAN functional splits is\ncrucial for optimising energy consumption and network sustainability. In this\npaper, we present a comprehensive measurement study of power consumption in\nRUs, DUs and CUs under varying network loads, specifically analysing the impact\nof Physical resource block (PRB) utilisation in Split 8 and Split 7.2b. The\nmeasurements were conducted on both software-defined radio (SDR)-based RUs and\ncommercial indoor and outdoor RU, as well as their corresponding DU and CU. By\nevaluating real-world hardware deployments under different operational\nconditions, this study provides empirical insights into the power efficiency of\nvarious O-RAN configurations. The results highlight that power consumption does\nnot scale significantly with network load, suggesting that a large portion of\nenergy consumption remains constant regardless of traffic demand.", "published": "2025-07-01 16:35:28", "link": "http://arxiv.org/abs/2507.00928v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Privacy-Preserving Quantized Federated Learning with Diverse Precision", "abstract": "Federated learning (FL) has emerged as a promising paradigm for distributed\nmachine learning, enabling collaborative training of a global model across\nmultiple local devices without requiring them to share raw data. Despite its\nadvancements, FL is limited by factors such as: (i) privacy risks arising from\nthe unprotected transmission of local model updates to the fusion center (FC)\nand (ii) decreased learning utility caused by heterogeneity in model\nquantization resolution across participating devices. Prior work typically\naddresses only one of these challenges because maintaining learning utility\nunder both privacy risks and quantization heterogeneity is a non-trivial task.\nIn this paper, our aim is therefore to improve the learning utility of a\nprivacy-preserving FL that allows clusters of devices with different\nquantization resolutions to participate in each FL round. Specifically, we\nintroduce a novel stochastic quantizer (SQ) that is designed to simultaneously\nachieve differential privacy (DP) and minimum quantization error. Notably, the\nproposed SQ guarantees bounded distortion, unlike other DP approaches. To\naddress quantization heterogeneity, we introduce a cluster size optimization\ntechnique combined with a linear fusion approach to enhance model aggregation\naccuracy. Numerical simulations validate the benefits of our approach in terms\nof privacy protection and learning utility compared to the conventional\nLaplaceSQ-FL algorithm.", "published": "2025-07-01 16:26:20", "link": "http://arxiv.org/abs/2507.00920v1", "categories": ["cs.LG", "eess.SP"], "primary_category": "cs.LG"}
{"title": "Constellation as a Service: Tailored Connectivity Management in Direct-Satellite-to-Device Networks", "abstract": "Direct-satellite-to-device (DS2D) communication is emerging as a promising\nsolution for global mobile service extension, leveraging the deployment of\nsatellite constellations. However, the challenge of managing DS2D connectivity\nfor multi-constellations becomes outstanding, including high interference and\nfrequent handovers caused by multi-coverage overlap and rapid satellite\nmovement. Moreover, existing approaches primarily operate within\nsingle-constellation shell, which inherently limits the ability to exploit the\nvast potential of multi-constellation connectivity provision, resulting in\nsuboptimal DS2D service performances. To address these challenges, this article\nproposes a Constellation as a Service (CaaS) framework, which treats the entire\nmulti-constellation infrastructure as a shared resource pool and dynamically\nforms optimal sub-constellations (SCs) for each DS2D service region. The\nformation of each SC integrates satellites from various orbits to provide\ntailored connectivity based on user demands, guided by two innovative\nstrategies: predictive satellite beamforming using generative artificial\nintelligence (GenAI) and pre-configured handover path for efficient satellite\naccess and mobility management. Simulation results demonstrate that CaaS\nsignificantly improves satellite service rates while reducing handover\noverhead, making it an efficient and continuable solution for managing DS2D\nconnectivity in multi-constellation environments.", "published": "2025-07-01 16:06:29", "link": "http://arxiv.org/abs/2507.00902v1", "categories": ["eess.SY", "cs.AI", "cs.SY", "eess.SP"], "primary_category": "eess.SY"}
{"title": "SComCP: Task-Oriented Semantic Communication for Collaborative Perception", "abstract": "Reliable detection of surrounding objects is critical for the safe operation\nof connected automated vehicles (CAVs). However, inherent limitations such as\nthe restricted perception range and occlusion effects compromise the\nreliability of single-vehicle perception systems in complex traffic\nenvironments. Collaborative perception has emerged as a promising approach by\nfusing sensor data from surrounding CAVs with diverse viewpoints, thereby\nimproving environmental awareness. Although collaborative perception holds\ngreat promise, its performance is bottlenecked by wireless communication\nconstraints, as unreliable and bandwidth-limited channels hinder the\ntransmission of sensor data necessary for real-time perception. To address\nthese challenges, this paper proposes SComCP, a novel task-oriented semantic\ncommunication framework for collaborative perception. Specifically, SComCP\nintegrates an importance-aware feature selection network that selects and\ntransmits semantic features most relevant to the perception task, significantly\nreducing communication overhead without sacrificing accuracy. Furthermore, we\ndesign a semantic codec network based on a joint source and channel coding\n(JSCC) architecture, which enables bidirectional transformation between\nsemantic features and noise-tolerant channel symbols, thereby ensuring stable\nperception under adverse wireless conditions. Extensive experiments demonstrate\nthe effectiveness of the proposed framework. In particular, compared to\nexisting approaches, SComCP can maintain superior perception performance across\nvarious channel conditions, especially in low signal-to-noise ratio (SNR)\nscenarios. In addition, SComCP exhibits strong generalization capability,\nenabling the framework to maintain high performance across diverse channel\nconditions, even when trained with a specific channel model.", "published": "2025-07-01 15:59:53", "link": "http://arxiv.org/abs/2507.00895v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Enhancing Vehicular Platooning with Wireless Federated Learning: A Resource-Aware Control Framework", "abstract": "This paper aims to enhance the performance of Vehicular Platooning (VP)\nsystems integrated with Wireless Federated Learning (WFL). In highly dynamic\nenvironments, vehicular platoons experience frequent communication changes and\nresource constraints, which significantly affect information exchange and\nlearning model synchronization. To address these challenges, we first formulate\nWFL in VP as a joint optimization problem that simultaneously considers Age of\nInformation (AoI) and Federated Learning Model Drift (FLMD) to ensure timely\nand accurate control. Through theoretical analysis, we examine the impact of\nFLMD on convergence performance and develop a two-stage Resource-Aware Control\nframework (RACE). The first stage employs a Lagrangian dual decomposition\nmethod for resource configuration, while the second stage implements a\nmulti-agent deep reinforcement learning approach for vehicle selection. The\napproach integrates Multi-Head Self-Attention and Long Short-Term Memory\nnetworks to capture spatiotemporal correlations in communication states.\nExperimental results demonstrate that, compared to baseline methods, the\nproposed framework improves AoI optimization by up to 45%, accelerates learning\nconvergence, and adapts more effectively to dynamic VP environments on the\nAI4MARS dataset.", "published": "2025-07-01 15:25:12", "link": "http://arxiv.org/abs/2507.00856v1", "categories": ["cs.NI", "eess.SP"], "primary_category": "cs.NI"}
{"title": "Tunable Wavelet Unit based Convolutional Neural Network in Optical Coherence Tomography Analysis Enhancement for Classifying Type of Epiretinal Membrane Surgery", "abstract": "In this study, we developed deep learning-based method to classify the type\nof surgery performed for epiretinal membrane (ERM) removal, either internal\nlimiting membrane (ILM) removal or ERM-alone removal. Our model, based on the\nResNet18 convolutional neural network (CNN) architecture, utilizes\npostoperative optical coherence tomography (OCT) center scans as inputs. We\nevaluated the model using both original scans and scans preprocessed with\nenergy crop and wavelet denoising, achieving 72% accuracy on preprocessed\ninputs, outperforming the 66% accuracy achieved on original scans. To further\nimprove accuracy, we integrated tunable wavelet units with two key adaptations:\nOrthogonal Lattice-based Wavelet Units (OrthLatt-UwU) and Perfect\nReconstruction Relaxation-based Wavelet Units (PR-Relax-UwU). These units\nallowed the model to automatically adjust filter coefficients during training\nand were incorporated into downsampling, stride-two convolution, and pooling\nlayers, enhancing its ability to distinguish between ERM-ILM removal and\nERM-alone removal, with OrthLattUwU boosting accuracy to 76% and PR-Relax-UwU\nincreasing performance to 78%. Performance comparisons showed that our AI model\noutperformed a trained human grader, who achieved only 50% accuracy in\nclassifying the removal surgery types from postoperative OCT scans. These\nfindings highlight the potential of CNN based models to improve clinical\ndecision-making by providing more accurate and reliable classifications. To the\nbest of our knowledge, this is the first work to employ tunable wavelets for\nclassifying different types of ERM removal surgery.", "published": "2025-07-01 13:46:06", "link": "http://arxiv.org/abs/2507.00743v1", "categories": ["eess.IV", "cs.CV", "eess.SP"], "primary_category": "eess.IV"}
{"title": "Biorthogonal Tunable Wavelet Unit with Lifting Scheme in Convolutional Neural Network", "abstract": "This work introduces a novel biorthogonal tunable wavelet unit constructed\nusing a lifting scheme that relaxes both the orthogonality and equal filter\nlength constraints, providing greater flexibility in filter design. The\nproposed unit enhances convolution, pooling, and downsampling operations,\nleading to improved image classification and anomaly detection in convolutional\nneural networks (CNN). When integrated into an 18-layer residual neural network\n(ResNet-18), the approach improved classification accuracy on CIFAR-10 by 2.12%\nand on the Describable Textures Dataset (DTD) by 9.73%, demonstrating its\neffectiveness in capturing fine-grained details. Similar improvements were\nobserved in ResNet-34. For anomaly detection in the hazelnut category of the\nMVTec Anomaly Detection dataset, the proposed method achieved competitive and\nwellbalanced performance in both segmentation and detection tasks,\noutperforming existing approaches in terms of accuracy and robustness.", "published": "2025-07-01 13:42:46", "link": "http://arxiv.org/abs/2507.00739v1", "categories": ["cs.CV", "eess.IV", "eess.SP"], "primary_category": "cs.CV"}
{"title": "Physical Layer Group Key Generation With the Aid of Reconfigurable Intelligent Surfaces", "abstract": "Reconfigurable intelligent surfaces (RIS) have the ability to alter the\nwireless environment by making changes in the impinging signal. Motivated by\nthis ability, in this study, we exploit the RIS to make the aggregate\nreflecting channels of different user terminals (UTs) as similar as possible to\nbe able to extract common group secret keys from their channels. Specifically,\nthe RIS will adjust its parameters to pave the way for group key generation\n(GKG) based on the physical channels of the UTs. Our method exploits the\nalready gathered channel state information (CSI) in the RIS to beneficially\ndesign the phase shifts and does not impose additional probing burden on the\nnetwork. Additionally, this scheme is broadcast-based and does not entail the\noverheads of the pairwise-based key generation. We consider both passive RIS\n(PRIS) and active RIS (ARIS) to generate the group keys. The PRIS is widely\nadopted in physical layer key generation (PLKG) studies due to its use of\npassive elements, whereas the ARIS demonstrates superior capability in aligning\nthe aggregate reflected channels among nodes in the GKG scenario, as\ndemonstrated in this study. We will exploit various optimization methods like\nsuccessive convex approximation (SCA) and semidefinite relaxation with Gaussian\nrandomization (SDR-GR) to address the raised optimization problems. Unlike most\nof the studies in the literature, our scheme can achieve a high GKG rate in\nstatic environments as well. Finally, we will examine the performance of the\nproposed method by normalized mean squared error (NMSE), key error rate (KER),\nkey generation rate (KGR) and key randomness metrics. Our numerical results\nverify that for the equal available power budget, the ARIS significantly\noutperforms PRIS in NMSE and KER, achieving more than four times higher KGR.", "published": "2025-07-01 12:31:59", "link": "http://arxiv.org/abs/2507.00714v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Neural Augmented Kalman Filters for Road Network assisted GNSS positioning", "abstract": "The Global Navigation Satellite System (GNSS) provides critical positioning\ninformation globally, but its accuracy in dense urban environments is often\ncompromised by multipath and non-line-of-sight errors. Road network data can be\nused to reduce the impact of these errors and enhance the accuracy of a\npositioning system. Previous works employing road network data are either\nlimited to offline applications, or rely on Kalman Filter (KF) heuristics with\nlittle flexibility and robustness. We instead propose training a Temporal Graph\nNeural Network (TGNN) to integrate road network information into a KF. The TGNN\nis designed to predict the correct road segment and its associated uncertainty\nto be used in the measurement update step of the KF. We validate our approach\nwith real-world GNSS data and open-source road networks, observing a 29%\ndecrease in positioning error for challenging scenarios compared to a GNSS-only\nKF. To the best of our knowledge, ours is the first deep learning-based\napproach jointly employing road network data and GNSS measurements to determine\nthe user position on Earth.", "published": "2025-07-01 10:52:42", "link": "http://arxiv.org/abs/2507.00654v1", "categories": ["cs.LG", "cs.SY", "eess.SP", "eess.SY"], "primary_category": "cs.LG"}
{"title": "Quantize-Sample-and-Verify: LLM Acceleration via Adaptive Edge-Cloud Speculative Decoding", "abstract": "In edge-cloud speculative decoding (SD), edge devices equipped with small\nlanguage models (SLMs) generate draft tokens that are verified by large\nlanguage models (LLMs) in the cloud. A key bottleneck in such systems is the\nlimited communication bandwidth between edge and cloud, which necessitates\nquantization of the information transmitted about generated tokens. In this\nwork, we introduce a novel quantize-sample (Q-S) strategy that provably\npreserves the output distribution of the cloud-based model, ensuring that the\nverified tokens match the distribution of those that would have been generated\ndirectly by the LLM. We develop a throughput model for edge-cloud SD that\nexplicitly accounts for communication latency. Leveraging this model, we\npropose an adaptive mechanism that optimizes token throughput by dynamically\nadjusting the draft length and quantization precision in response to both\nsemantic uncertainty and channel conditions. Simulations demonstrate that the\nproposed Q-S approach significantly improves decoding efficiency in realistic\nedge-cloud deployment scenarios.", "published": "2025-07-01 09:38:15", "link": "http://arxiv.org/abs/2507.00605v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Delay Bound Relaxation with Deep Learning-based Haptic Estimation for Tactile Internet", "abstract": "Haptic teleoperation typically demands sub-millisecond latency and ultra-high\nreliability (99.999%) in Tactile Internet. At a 1 kHz haptic signal sampling\nrate, this translates into an extremely high packet transmission rate, posing\nsignificant challenges for timely delivery and introducing substantial\ncomplexity and overhead in radio resource allocation. To address this critical\nchallenge, we introduce a novel DL modelthat estimates force feedback using\nmulti-modal input, i.e. both force measurements from the remote side and local\noperator motion signals. The DL model can capture complex temporal features of\nhaptic time-series with the use of CNN and LSTM layers, followed by a\ntransformer encoder, and autoregressively produce a highly accurate estimation\nof the next force values for different teleoperation activities. By ensuring\nthat the estimation error is within a predefined threshold, the teleoperation\nsystem can safely relax its strict delay requirements. This enables the\nbatching and transmission of multiple haptic packets within a single resource\nblock, improving resource efficiency and facilitating scheduling in resource\nallocation. Through extensive simulations, we evaluated network performance in\nterms of reliability and capacity. Results show that, for both dynamic and\nrigid object interactions, the proposed method increases the number of reliably\nserved users by up to 66%.", "published": "2025-07-01 08:43:36", "link": "http://arxiv.org/abs/2507.00571v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Fair Rate Maximization for Fluid Antenna Relay (FAR)-assisted Multi-user MISO Communications", "abstract": "In this paper, we investigate the problem of max-min rate maximization in\nfluid antenna relay (FAR)-assisted multi-user uplink multiple-input\nsingle-output (MISO) wireless systems, where each user is equipped with a\nsingle fluid antenna (FA) and the base station (BS) is equipped with multiple\nFAs. Unlike most existing relevant work focusing on maximizing sum rate of the\nfluid antenna system (FAS), which may cause unbearable rate loss to weak users,\nwe propose to maximize the minimal rate of the system to ensure fairness. The\nmax-min optimization problem is formulated by jointly optimizing the positions\nof FAs with meeting the minimum distance requirements of FAs, maximum\ntransmitting power limit, and feasible antenna region constraints. To solve\nthis problem, we propose an alternating algorithm with utilizing the successive\nconvex approximation (SCA) method. Simulation results demonstrate that the\nproposed method significantly outperforms conventional methods in terms of\nmaximizing the minimal achievable rate across different signal-to-noise ratios\n(SNRs) and normalized region sizes.", "published": "2025-07-01 07:48:57", "link": "http://arxiv.org/abs/2507.00529v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Quadrature Over-the-Air-Computing for Multimodal Dual-Stream Signal Processing", "abstract": "We propose a novel quadrature over-the-air computing (Q-OTAC) framework that\nenables the simultaneously computation of two independent functions and/or data\nstream within a single transmission. In contrast to conventional OTAC schemes,\nwhere a single function is computed by treating each complex signal as a single\ncomponent, the proposed Q-OTAC exploits both in-phase and quadrature (IQ)\ncomponents of a complex signal, encoding two distinct functions and/or data\nstreams at the edge devices (EDs) and employing a novel low-complexity\nIQ-decoupled combiner at the access point (AP) to independently recover each\nstream, which effectively doubles the computation rate. A key strength of this\nframework lies in its simplicity and broad compatibility: the extension into\nthe quadrature domain is conceptually straightforward, yet remakably powerful,\nallowing seamless integration into existing OTAC techniques. Simulation results\nvalidate the effectiveness of this approach, including the first demonstration\nof dual-function aggregation (e.g., parallel summation and product),\nhighlighting the potential of Q-OTAC for enabling multi-modal and\nhigh-efficiency beyond fifth generation (B5G) applications.", "published": "2025-07-01 07:24:12", "link": "http://arxiv.org/abs/2507.00508v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Mixture of Reasonings: Teach Large Language Models to Reason with Adaptive Strategies", "abstract": "Large language models (LLMs) excel in complex tasks through advanced\nprompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT), but\ntheir reliance on manually crafted, task-specific prompts limits adaptability\nand efficiency. We introduce Mixture of Reasoning (MoR), a training framework\nthat embeds diverse reasoning strategies into LLMs for autonomous,\ntask-adaptive reasoning without external prompt engineering. MoR has two\nphases: Thought Generation, creating reasoning chain templates with models like\nGPT-4o, and SFT Dataset Construction, pairing templates with benchmark datasets\nfor supervised fine-tuning. Our experiments show that MoR significantly\nenhances performance, with MoR150 achieving 0.730 (2.2% improvement) using CoT\nprompting and 0.734 (13.5% improvement) compared to baselines. MoR eliminates\nthe need for task-specific prompts, offering a generalizable solution for\nrobust reasoning across diverse tasks.", "published": "2025-07-01 09:39:04", "link": "http://arxiv.org/abs/2507.00606v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On the rank weight hierarchy of $M$-codes", "abstract": "We study the rank weight hierarchy of linear codes which are stable under a\nlinear endomorphism defined over the base field, in particular when the\nendomorphism is cyclic. In this last case, we give a necessary and sufficient\ncondition for such a code to have first rank weight equal to $1$ in terms of\nits generator polynomial, as well as an explicit formula for its last rank\nweight.", "published": "2025-07-01 09:41:29", "link": "http://arxiv.org/abs/2507.00609v2", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Escaping Platos Cave: JAM for Aligning Independently Trained Vision and Language Models", "abstract": "Independently trained vision and language models inhabit disjoint\nrepresentational spaces, shaped by their respective modalities, objectives, and\narchitectures. Yet an emerging hypothesis - the Platonic Representation\nHypothesis - suggests that such models may nonetheless converge toward a shared\nstatistical model of reality. This compatibility, if it exists, raises a\nfundamental question: can we move beyond post-hoc statistical detection of\nalignment and explicitly optimize for it between such disjoint representations?\nWe cast this Platonic alignment problem as a multi-objective optimization task\n- preserve each modality's native structure while aligning for mutual\ncoherence. We introduce the Joint Autoencoder Modulator (JAM) framework that\njointly trains modality-specific autoencoders on the latent representations of\npre-trained single modality models, encouraging alignment through both\nreconstruction and cross-modal objectives. By analogy, this framework serves as\na method to escape Plato's Cave, enabling the emergence of shared structure\nfrom disjoint inputs. We evaluate this framework across three critical design\naxes: (i) the alignment objective - comparing contrastive loss (Con), its\nhard-negative variant (NegCon), and our Spread loss, (ii) the layer depth at\nwhich alignment is most effective, and (iii) the impact of foundation model\nscale on representational convergence. Our findings show that our lightweight\nPareto-efficient framework reliably induces alignment, even across frozen,\nindependently trained representations, offering both theoretical insight and\npractical pathways for transforming generalist unimodal foundations into\nspecialist multimodal models.", "published": "2025-07-01 21:43:50", "link": "http://arxiv.org/abs/2507.01201v2", "categories": ["cs.LG", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Horus: A Protocol for Trustless Delegation Under Uncertainty", "abstract": "Correctness is an emergent property of systems where exposing error is\ncheaper than committing it. In dynamic, low-trust environments, autonomous AI\nagents benefit from delegating work to sub-agents, yet correctness cannot be\nassured through upfront specification or centralized oversight. We propose a\nprotocol that enforces correctness through collateralized claims in a recursive\nverification game. Tasks are published as intents, and solvers compete to\nfulfill them. Selected solvers carry out tasks under risk, with correctness\nchecked post hoc by verifiers. Any challenger can challenge a result by staking\nagainst it to trigger the verification process. Incorrect agents are slashed\nand correct opposition is rewarded, with an escalation path that penalizes\nerroneous verifiers themselves. When incentives are aligned across solvers,\nchallengers, and verifiers, falsification conditions make correctness the Nash\nequilibrium.", "published": "2025-07-01 10:22:35", "link": "http://arxiv.org/abs/2507.00631v3", "categories": ["cs.GT", "cs.AI", "cs.MA", "I.2.11; F.2.2"], "primary_category": "cs.GT"}
{"title": "Ordinality in Discrete-level Question Difficulty Estimation: Introducing Balanced DRPS and OrderedLogitNN", "abstract": "Recent years have seen growing interest in Question Difficulty Estimation\n(QDE) using natural language processing techniques. Question difficulty is\noften represented using discrete levels, framing the task as ordinal regression\ndue to the inherent ordering from easiest to hardest. However, the literature\nhas neglected the ordinal nature of the task, relying on classification or\ndiscretized regression models, with specialized ordinal regression methods\nremaining unexplored. Furthermore, evaluation metrics are tightly coupled to\nthe modeling paradigm, hindering cross-study comparability. While some metrics\nfail to account for the ordinal structure of difficulty levels, none adequately\naddress class imbalance, resulting in biased performance assessments. This\nstudy addresses these limitations by benchmarking three types of model outputs\n-- discretized regression, classification, and ordinal regression -- using the\nbalanced Discrete Ranked Probability Score (DRPS), a novel metric that jointly\ncaptures ordinality and class imbalance. In addition to using popular ordinal\nregression methods, we propose OrderedLogitNN, extending the ordered logit\nmodel from econometrics to neural networks. We fine-tune BERT on the RACE++ and\nARC datasets and find that OrderedLogitNN performs considerably better on\ncomplex tasks. The balanced DRPS offers a robust and fair evaluation metric for\ndiscrete-level QDE, providing a principled foundation for future research.", "published": "2025-07-01 13:38:33", "link": "http://arxiv.org/abs/2507.00736v2", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Privacy-Preserving Quantized Federated Learning with Diverse Precision", "abstract": "Federated learning (FL) has emerged as a promising paradigm for distributed\nmachine learning, enabling collaborative training of a global model across\nmultiple local devices without requiring them to share raw data. Despite its\nadvancements, FL is limited by factors such as: (i) privacy risks arising from\nthe unprotected transmission of local model updates to the fusion center (FC)\nand (ii) decreased learning utility caused by heterogeneity in model\nquantization resolution across participating devices. Prior work typically\naddresses only one of these challenges because maintaining learning utility\nunder both privacy risks and quantization heterogeneity is a non-trivial task.\nIn this paper, our aim is therefore to improve the learning utility of a\nprivacy-preserving FL that allows clusters of devices with different\nquantization resolutions to participate in each FL round. Specifically, we\nintroduce a novel stochastic quantizer (SQ) that is designed to simultaneously\nachieve differential privacy (DP) and minimum quantization error. Notably, the\nproposed SQ guarantees bounded distortion, unlike other DP approaches. To\naddress quantization heterogeneity, we introduce a cluster size optimization\ntechnique combined with a linear fusion approach to enhance model aggregation\naccuracy. Numerical simulations validate the benefits of our approach in terms\nof privacy protection and learning utility compared to the conventional\nLaplaceSQ-FL algorithm.", "published": "2025-07-01 16:26:20", "link": "http://arxiv.org/abs/2507.00920v2", "categories": ["cs.LG", "eess.SP"], "primary_category": "cs.LG"}
{"title": "STELLA: Self-Evolving LLM Agent for Biomedical Research", "abstract": "The rapid growth of biomedical data, tools, and literature has created a\nfragmented research landscape that outpaces human expertise. While AI agents\noffer a solution, they typically rely on static, manually curated toolsets,\nlimiting their ability to adapt and scale. Here, we introduce STELLA, a\nself-evolving AI agent designed to overcome these limitations. STELLA employs a\nmulti-agent architecture that autonomously improves its own capabilities\nthrough two core mechanisms: an evolving Template Library for reasoning\nstrategies and a dynamic Tool Ocean that expands as a Tool Creation Agent\nautomatically discovers and integrates new bioinformatics tools. This allows\nSTELLA to learn from experience. We demonstrate that STELLA achieves\nstate-of-the-art accuracy on a suite of biomedical benchmarks, scoring\napproximately 26\\% on Humanity's Last Exam: Biomedicine, 54\\% on LAB-Bench:\nDBQA, and 63\\% on LAB-Bench: LitQA, outperforming leading models by up to 6\npercentage points. More importantly, we show that its performance\nsystematically improves with experience; for instance, its accuracy on the\nHumanity's Last Exam benchmark almost doubles with increased trials. STELLA\nrepresents a significant advance towards AI Agent systems that can learn and\ngrow, dynamically scaling their expertise to accelerate the pace of biomedical\ndiscovery.", "published": "2025-07-01 20:52:01", "link": "http://arxiv.org/abs/2507.02004v1", "categories": ["cs.AI", "cs.CL", "q-bio.BM"], "primary_category": "cs.AI"}
{"title": "Why Multi-Interest Fairness Matters: Hypergraph Contrastive Multi-Interest Learning for Fair Conversational Recommender System", "abstract": "Unfairness is a well-known challenge in Recommender Systems (RSs), often\nresulting in biased outcomes that disadvantage users or items based on\nattributes such as gender, race, age, or popularity. Although some approaches\nhave started to improve fairness recommendation in offline or static contexts,\nthe issue of unfairness often exacerbates over time, leading to significant\nproblems like the Matthew effect, filter bubbles, and echo chambers. To address\nthese challenges, we proposed a novel framework, Hypergraph Contrastive\nMulti-Interest Learning for Fair Conversational Recommender System (HyFairCRS),\naiming to promote multi-interest diversity fairness in dynamic and interactive\nConversational Recommender Systems (CRSs). HyFairCRS first captures a wide\nrange of user interests by establishing diverse hypergraphs through contrastive\nlearning. These interests are then utilized in conversations to generate\ninformative responses and ensure fair item predictions within the dynamic\nuser-system feedback loop. Experiments on two CRS-based datasets show that\nHyFairCRS achieves a new state-of-the-art performance while effectively\nalleviating unfairness. Our code is available at\nhttps://github.com/zysensmile/HyFairCRS.", "published": "2025-07-01 11:39:42", "link": "http://arxiv.org/abs/2507.02000v1", "categories": ["cs.IR", "cs.CL", "cs.MM"], "primary_category": "cs.IR"}
{"title": "Dynamic Strategy Adaptation in Multi-Agent Environments with Large Language Models", "abstract": "Large language models (LLMs) demonstrate strong reasoning abilities across\nmathematical, strategic, and linguistic tasks, yet little is known about how\nwell they reason in dynamic, real-time, multi-agent scenarios, such as\ncollaborative environments in which agents continuously adapt to each other's\nbehavior, as in cooperative gameplay settings. In this paper, we bridge this\ngap by combining LLM-driven agents with strategic reasoning and real-time\nadaptation in cooperative, multi-agent environments grounded in game-theoretic\nprinciples such as belief consistency and Nash equilibrium. The proposed\nframework applies broadly to dynamic scenarios in which agents coordinate,\ncommunicate, and make decisions in response to continuously changing\nconditions. We provide real-time strategy refinement and adaptive feedback\nmechanisms that enable agents to dynamically adjust policies based on immediate\ncontextual interactions, in contrast to previous efforts that evaluate LLM\ncapabilities in static or turn-based settings. Empirical results show that our\nmethod achieves up to a 26\\% improvement in return over PPO baselines in\nhigh-noise environments, while maintaining real-time latency under 1.05\nmilliseconds. Our approach improves collaboration efficiency, task completion\nrates, and flexibility, illustrating that game-theoretic guidance integrated\nwith real-time feedback enhances LLM performance, ultimately fostering more\nresilient and flexible strategic multi-agent systems.", "published": "2025-07-01 20:09:50", "link": "http://arxiv.org/abs/2507.02002v1", "categories": ["cs.MA"], "primary_category": "cs.MA"}
{"title": "Towards a Playground to Democratize Experimentation and Benchmarking of AI Agents for Network Troubleshooting", "abstract": "Recent research has demonstrated the effectiveness of Artificial Intelligence\n(AI), and more specifically, Large Language Models (LLMs), in supporting\nnetwork configuration synthesis and automating network diagnosis tasks, among\nothers. In this preliminary work, we restrict our focus to the application of\nAI agents to network troubleshooting and elaborate on the need for a\nstandardized, reproducible, and open benchmarking platform, where to build and\nevaluate AI agents with low operational effort.", "published": "2025-07-01 08:46:37", "link": "http://arxiv.org/abs/2507.01997v1", "categories": ["cs.NI", "cs.AI", "cs.MA"], "primary_category": "cs.NI"}
{"title": "Horus: A Protocol for Trustless Delegation Under Uncertainty", "abstract": "Correctness is an emergent property of systems where exposing error is\ncheaper than committing it. In dynamic, low-trust environments, autonomous AI\nagents benefit from delegating work to sub-agents, yet correctness cannot be\nassured through upfront specification or centralized oversight. We propose a\nprotocol that enforces correctness through collateralized claims in a recursive\nverification game. Tasks are published as intents, and solvers compete to\nfulfill them. Selected solvers carry out tasks under risk, with correctness\nchecked post hoc by verifiers. Any challenger can challenge a result by staking\nagainst it to trigger the verification process. Incorrect agents are slashed\nand correct opposition is rewarded, with an escalation path that penalizes\nerroneous verifiers themselves. When incentives are aligned across solvers,\nchallengers, and verifiers, falsification conditions make correctness the Nash\nequilibrium.", "published": "2025-07-01 10:22:35", "link": "http://arxiv.org/abs/2507.00631v4", "categories": ["cs.GT", "cs.AI", "cs.MA", "I.2.11; F.2.2"], "primary_category": "cs.GT"}
{"title": "Towards a Playground to Democratize Experimentation and Benchmarking of AI Agents for Network Troubleshooting", "abstract": "Recent research has demonstrated the effectiveness of Artificial Intelligence\n(AI), and more specifically, Large Language Models (LLMs), in supporting\nnetwork configuration synthesis and automating network diagnosis tasks, among\nothers. In this preliminary work, we restrict our focus to the application of\nAI agents to network troubleshooting and elaborate on the need for a\nstandardized, reproducible, and open benchmarking platform, where to build and\nevaluate AI agents with low operational effort.", "published": "2025-07-01 08:46:37", "link": "http://arxiv.org/abs/2507.01997v2", "categories": ["cs.NI", "cs.AI", "cs.MA"], "primary_category": "cs.NI"}
{"title": "Learning Beyond Euclid: Curvature-Adaptive Generalization for Neural Networks on Manifolds", "abstract": "In this work, we develop new generalization bounds for neural networks\ntrained on data supported on Riemannian manifolds. Existing generalization\ntheories often rely on complexity measures derived from Euclidean geometry,\nwhich fail to account for the intrinsic structure of non-Euclidean spaces. Our\nanalysis introduces a geometric refinement: we derive covering number bounds\nthat explicitly incorporate manifold-specific properties such as sectional\ncurvature, volume growth, and injectivity radius. These geometric corrections\nlead to sharper Rademacher complexity bounds for classes of Lipschitz neural\nnetworks defined on compact manifolds. The resulting generalization guarantees\nrecover standard Euclidean results when curvature is zero but improve\nsubstantially in settings where the data lies on curved, low-dimensional\nmanifolds embedded in high-dimensional ambient spaces. We illustrate the\ntightness of our bounds in negatively curved spaces, where the exponential\nvolume growth leads to provably higher complexity, and in positively curved\nspaces, where the curvature acts as a regularizing factor. This framework\nprovides a principled understanding of how intrinsic geometry affects learning\ncapacity, offering both theoretical insight and practical implications for deep\nlearning on structured data domains.", "published": "2025-07-01 23:16:49", "link": "http://arxiv.org/abs/2507.02999v1", "categories": ["cs.LG", "math.DG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "A Weakly Supervised Transformer to Support Rare Disease Diagnosis from Electronic Health Records: Methods and Applications in Rare Pulmonary Disease", "abstract": "Rare diseases affect an estimated 300-400 million people worldwide, yet\nindividual conditions often remain poorly characterized and difficult to\ndiagnose due to their low prevalence and limited clinician familiarity. While\ncomputational phenotyping algorithms show promise for automating rare disease\ndetection, their development is hindered by the scarcity of labeled data and\nbiases in existing label sources. Gold-standard labels from registries and\nexpert chart reviews are highly accurate but constrained by selection bias and\nthe cost of manual review. In contrast, labels derived from electronic health\nrecords (EHRs) cover a broader range of patients but can introduce substantial\nnoise. To address these challenges, we propose a weakly supervised,\ntransformer-based framework that combines a small set of gold-standard labels\nwith a large volume of iteratively updated silver-standard labels derived from\nEHR data. This hybrid approach enables the training of a highly accurate and\ngeneralizable phenotyping model that scales rare disease detection beyond the\nscope of individual clinical expertise. Our method is initialized by learning\nembeddings of medical concepts based on their semantic meaning or co-occurrence\npatterns in EHRs, which are then refined and aggregated into patient-level\nrepresentations via a multi-layer transformer architecture. Using two rare\npulmonary diseases as a case study, we validate our model on EHR data from\nBoston Children's Hospital. Our framework demonstrates notable improvements in\nphenotype classification, identification of clinically meaningful subphenotypes\nthrough patient clustering, and prediction of disease progression compared to\nbaseline methods. These results highlight the potential of our approach to\nenable scalable identification and stratification of rare disease patients for\nclinical care and research applications.", "published": "2025-07-01 23:11:20", "link": "http://arxiv.org/abs/2507.02998v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Horus: A Protocol for Trustless Delegation Under Uncertainty", "abstract": "Correctness is an emergent property of systems where exposing error is\ncheaper than committing it. In dynamic, low-trust environments, autonomous AI\nagents benefit from delegating work to sub-agents, yet correctness cannot be\nassured through upfront specification or centralized oversight. We propose a\nprotocol that enforces correctness through collateralized claims in a recursive\nverification game. Tasks are published as intents, and solvers compete to\nfulfill them. Selected solvers carry out tasks under risk, with correctness\nchecked post hoc by verifiers. Any challenger can challenge a result by staking\nagainst it to trigger the verification process. Incorrect agents are slashed\nand correct opposition is rewarded, with an escalation path that penalizes\nerroneous verifiers themselves. When incentives are aligned across solvers,\nchallengers, and verifiers, falsification conditions make correctness the Nash\nequilibrium.", "published": "2025-07-01 10:22:35", "link": "http://arxiv.org/abs/2507.00631v5", "categories": ["cs.GT", "cs.AI", "cs.MA", "I.2.11; F.2.2"], "primary_category": "cs.GT"}
{"title": "Horus: A Protocol for Trustless Delegation Under Uncertainty", "abstract": "Correctness is an emergent property of systems where exposing error is\ncheaper than committing it. In dynamic, low-trust environments, autonomous AI\nagents benefit from delegating work to sub-agents, yet correctness cannot be\nassured through upfront specification or centralized oversight. We propose a\nprotocol that enforces correctness through collateralized claims in a recursive\nverification game. Tasks are published as intents, and solvers compete to\nfulfill them. Selected solvers carry out tasks under risk, with correctness\nchecked post hoc by verifiers. Any challenger can challenge a result by staking\nagainst it to trigger the verification process. Incorrect agents are slashed\nand correct opposition is rewarded, with an escalation path that penalizes\nerroneous verifiers themselves. When incentives are aligned across solvers,\nchallengers, and verifiers, falsification conditions make correctness the Nash\nequilibrium.", "published": "2025-07-01 10:22:35", "link": "http://arxiv.org/abs/2507.00631v6", "categories": ["cs.GT", "cs.AI", "cs.MA", "I.2.11; F.2.2"], "primary_category": "cs.GT"}
{"title": "Scale-Dependent Multifractality in Bitcoin Realised Volatility: Implications for Rough Volatility Modelling", "abstract": "We assess the applicability of rough volatility models to Bitcoin realised\nvolatility using the normalised p-variation framework of Cont and Das (2024).\nApplying this model free estimator to high-frequency Bitcoin data from 2017 to\n2024 across multiple sampling resolutions, we find that the normalised\nstatistic remains strictly negative throughout, precluding the estimation of a\nvalid roughness index. Stationarity tests and robustness checks reveal no\nsignificant evidence of non-stationarity or structural breaks as explanatory\nfactors. Instead, convergent evidence from three complementary diagnostics,\nnamely multifractal detrended fluctuation analysis, log-log moment scaling, and\nwavelet leaders, reveals a multifractal structure in Bitcoin volatility. This\nscale-dependent behaviour violates the homogeneity assumptions underlying rough\nvolatility estimation and accounts for the estimator's systematic failure.\nThese findings suggest that while rough volatility models perform well in\ntraditional markets, they are structurally misaligned with the empirical\nfeatures of Bitcoin volatility.", "published": "2025-07-01 08:54:20", "link": "http://arxiv.org/abs/2507.00575v2", "categories": ["q-fin.ST", "q-fin.MF", "60G22, 62M10"], "primary_category": "q-fin.ST"}
