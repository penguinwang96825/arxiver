{"title": "RADE: Reference-Assisted Dialogue Evaluation for Open-Domain Dialogue", "abstract": "Evaluating open-domain dialogue systems is challenging for reasons such as\nthe one-to-many problem, i.e., many appropriate responses other than just the\ngolden response. As of now, automatic evaluation methods need better\nconsistency with humans, while reliable human evaluation can be time- and\ncost-intensive. To this end, we propose the Reference-Assisted Dialogue\nEvaluation (RADE) approach under the multi-task learning framework, which\nleverages the pre-created utterance as reference other than the gold response\nto relief the one-to-many problem. Specifically, RADE explicitly compares\nreference and the candidate response to predict their overall scores. Moreover,\nan auxiliary response generation task enhances prediction via a shared encoder.\nTo support RADE, we extend three datasets with additional rated responses other\nthan just a golden response by human annotation. Experiments on our three\ndatasets and two existing benchmarks demonstrate the effectiveness of our\nmethod, where Pearson, Spearman, and Kendall correlations with human evaluation\noutperform state-of-the-art baselines.", "published": "2023-09-15 04:47:19", "link": "http://arxiv.org/abs/2309.08156v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Draft & Verify: Lossless Large Language Model Acceleration via\n  Self-Speculative Decoding", "abstract": "We present a novel inference scheme, self-speculative decoding, for\naccelerating Large Language Models (LLMs) without the need for an auxiliary\nmodel. This approach is characterized by a two-stage process: drafting and\nverification. The drafting stage generates draft tokens at a slightly lower\nquality but more quickly, which is achieved by selectively skipping certain\nintermediate layers during drafting. Subsequently, the verification stage\nemploys the original LLM to validate those draft output tokens in one forward\npass. This process ensures the final output remains identical to that produced\nby the unaltered LLM. Moreover, the proposed method requires no additional\nneural network training and no extra memory footprint, making it a\nplug-and-play and cost-effective solution for inference acceleration.\nBenchmarks with LLaMA-2 and its variants demonstrated a speedup up to\n1.99$\\times$.", "published": "2023-09-15 05:34:32", "link": "http://arxiv.org/abs/2309.08168v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LASER: LLM Agent with State-Space Exploration for Web Navigation", "abstract": "Large language models (LLMs) have been successfully adapted for interactive\ndecision-making tasks like web navigation. While achieving decent performance,\nprevious methods implicitly assume a forward-only execution mode for the model,\nwhere they only provide oracle trajectories as in-context examples to guide the\nmodel on how to reason in the environment. Consequently, the model could not\nhandle more challenging scenarios not covered in the in-context examples, e.g.,\nmistakes, leading to sub-optimal performance. To address this issue, we propose\nto model the interactive task as state space exploration, where the LLM agent\ntransitions among a pre-defined set of states by performing actions to complete\nthe task. This formulation enables flexible backtracking, allowing the model to\nrecover from errors easily. We evaluate our proposed LLM Agent with State-Space\nExploRation (LASER) on both the WebShop task and amazon.com. Experimental\nresults show that LASER significantly outperforms previous methods and closes\nthe gap with human performance on the web navigation task.", "published": "2023-09-15 05:44:08", "link": "http://arxiv.org/abs/2309.08172v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FedJudge: Federated Legal Large Language Model", "abstract": "Large Language Models (LLMs) have gained prominence in the field of Legal\nIntelligence, offering potential applications in assisting legal professionals\nand laymen. However, the centralized training of these Legal LLMs raises data\nprivacy concerns, as legal data is distributed among various institutions\ncontaining sensitive individual information. This paper addresses this\nchallenge by exploring the integration of Legal LLMs with Federated Learning\n(FL) methodologies. By employing FL, Legal LLMs can be fine-tuned locally on\ndevices or clients, and their parameters are aggregated and distributed on a\ncentral server, ensuring data privacy without directly sharing raw data.\nHowever, computation and communication overheads hinder the full fine-tuning of\nLLMs under the FL setting. Moreover, the distribution shift of legal data\nreduces the effectiveness of FL methods. To this end, in this paper, we propose\nthe first Federated Legal Large Language Model (FedJudge) framework, which\nfine-tunes Legal LLMs efficiently and effectively. Specifically, FedJudge\nutilizes parameter-efficient fine-tuning methods to update only a few\nadditional parameters during the FL training. Besides, we explore the continual\nlearning methods to preserve the global model's important parameters when\ntraining local clients to mitigate the problem of data shifts. Extensive\nexperimental results on three real-world datasets clearly validate the\neffectiveness of FedJudge. Code is released at\nhttps://github.com/yuelinan/FedJudge.", "published": "2023-09-15 05:45:44", "link": "http://arxiv.org/abs/2309.08173v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models for Failure Mode Classification: An Investigation", "abstract": "In this paper we present the first investigation into the effectiveness of\nLarge Language Models (LLMs) for Failure Mode Classification (FMC). FMC, the\ntask of automatically labelling an observation with a corresponding failure\nmode code, is a critical task in the maintenance domain as it reduces the need\nfor reliability engineers to spend their time manually analysing work orders.\nWe detail our approach to prompt engineering to enable an LLM to predict the\nfailure mode of a given observation using a restricted code list. We\ndemonstrate that the performance of a GPT-3.5 model (F1=0.80) fine-tuned on\nannotated data is a significant improvement over a currently available text\nclassification model (F1=0.60) trained on the same annotated data set. The\nfine-tuned model also outperforms the out-of-the box GPT-3.5 (F1=0.46). This\ninvestigation reinforces the need for high quality fine-tuning data sets for\ndomain-specific tasks using LLMs.", "published": "2023-09-15 06:13:01", "link": "http://arxiv.org/abs/2309.08181v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Sentence-Level Semantic Search using Meta-Distillation\n  Learning", "abstract": "Multilingual semantic search is the task of retrieving relevant contents to a\nquery expressed in different language combinations. This requires a better\nsemantic understanding of the user's intent and its contextual meaning.\nMultilingual semantic search is less explored and more challenging than its\nmonolingual or bilingual counterparts, due to the lack of multilingual parallel\nresources for this task and the need to circumvent \"language bias\". In this\nwork, we propose an alignment approach: MAML-Align, specifically for\nlow-resource scenarios. Our approach leverages meta-distillation learning based\non MAML, an optimization-based Model-Agnostic Meta-Learner. MAML-Align distills\nknowledge from a Teacher meta-transfer model T-MAML, specialized in\ntransferring from monolingual to bilingual semantic search, to a Student model\nS-MAML, which meta-transfers from bilingual to multilingual semantic search. To\nthe best of our knowledge, we are the first to extend meta-distillation to a\nmultilingual search application. Our empirical results show that on top of a\nstrong baseline based on sentence transformers, our meta-distillation approach\nboosts the gains provided by MAML and significantly outperforms naive\nfine-tuning methods. Furthermore, multilingual meta-distillation learning\nimproves generalization even to unseen languages.", "published": "2023-09-15 06:22:37", "link": "http://arxiv.org/abs/2309.08185v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Encoded Summarization: Summarizing Documents into Continuous Vector\n  Space for Legal Case Retrieval", "abstract": "We present our method for tackling a legal case retrieval task by introducing\nour method of encoding documents by summarizing them into continuous vector\nspace via our phrase scoring framework utilizing deep neural networks. On the\nother hand, we explore the benefits from combining lexical features and latent\nfeatures generated with neural networks. Our experiments show that lexical\nfeatures and latent features generated with neural networks complement each\nother to improve the retrieval system performance. Furthermore, our\nexperimental results suggest the importance of case summarization in different\naspects: using provided summaries and performing encoded summarization. Our\napproach achieved F1 of 65.6% and 57.6% on the experimental datasets of legal\ncase retrieval tasks.", "published": "2023-09-15 06:33:49", "link": "http://arxiv.org/abs/2309.08187v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating Answerability of LLMs for Long-Form Question Answering", "abstract": "As we embark on a new era of LLMs, it becomes increasingly crucial to\nunderstand their capabilities, limitations, and differences. Toward making\nfurther progress in this direction, we strive to build a deeper understanding\nof the gaps between massive LLMs (e.g., ChatGPT) and smaller yet effective\nopen-source LLMs and their distilled counterparts. To this end, we specifically\nfocus on long-form question answering (LFQA) because it has several practical\nand impactful applications (e.g., troubleshooting, customer service, etc.) yet\nis still understudied and challenging for LLMs. We propose a\nquestion-generation method from abstractive summaries and show that generating\nfollow-up questions from summaries of long documents can create a challenging\nsetting for LLMs to reason and infer from long contexts. Our experimental\nresults confirm that: (1) our proposed method of generating questions from\nabstractive summaries pose a challenging setup for LLMs and shows performance\ngaps between LLMs like ChatGPT and open-source LLMs (Alpaca, Llama) (2)\nopen-source LLMs exhibit decreased reliance on context for generated questions\nfrom the original document, but their generation capabilities drop\nsignificantly on generated questions from summaries -- especially for longer\ncontexts (>1024 tokens)", "published": "2023-09-15 07:22:56", "link": "http://arxiv.org/abs/2309.08210v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Consistent Narrative Prompts on Abductive Natural Language\n  Inference", "abstract": "Abduction has long been seen as crucial for narrative comprehension and\nreasoning about everyday situations. The abductive natural language inference\n($\\alpha$NLI) task has been proposed, and this narrative text-based task aims\nto infer the most plausible hypothesis from the candidates given two\nobservations. However, the inter-sentential coherence and the model consistency\nhave not been well exploited in the previous works on this task. In this work,\nwe propose a prompt tuning model $\\alpha$-PACE, which takes self-consistency\nand inter-sentential coherence into consideration. Besides, we propose a\ngeneral self-consistent framework that considers various narrative sequences\n(e.g., linear narrative and reverse chronology) for guiding the pre-trained\nlanguage model in understanding the narrative context of input. We conduct\nextensive experiments and thorough ablation studies to illustrate the necessity\nand effectiveness of $\\alpha$-PACE. The performance of our method shows\nsignificant improvement against extensive competitive baselines.", "published": "2023-09-15 10:48:10", "link": "http://arxiv.org/abs/2309.08303v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How to Handle Different Types of Out-of-Distribution Scenarios in\n  Computational Argumentation? A Comprehensive and Fine-Grained Field Study", "abstract": "The advent of pre-trained Language Models (LMs) has markedly advanced natural\nlanguage processing, but their efficacy in out-of-distribution (OOD) scenarios\nremains a significant challenge. Computational argumentation (CA), modeling\nhuman argumentation processes, is a field notably impacted by these challenges\nbecause complex annotation schemes and high annotation costs naturally lead to\nresources barely covering the multiplicity of available text sources and\ntopics. Due to this data scarcity, generalization to data from uncovered\ncovariant distributions is a common challenge for CA tasks like stance\ndetection or argument classification. This work systematically assesses LMs'\ncapabilities for such OOD scenarios. While previous work targets specific OOD\ntypes like topic shifts or OOD uniformly, we address three prevalent OOD\nscenarios in CA: topic shift, domain shift, and language shift. Our findings\nchallenge the previously asserted general superiority of in-context learning\n(ICL) for OOD. We find that the efficacy of such learning paradigms varies with\nthe type of OOD. Specifically, while ICL excels for domain shifts, prompt-based\nfine-tuning surpasses for topic shifts. To sum up, we navigate the\nheterogeneity of OOD scenarios in CA and empirically underscore the potential\nof base-sized LMs in overcoming these challenges.", "published": "2023-09-15 11:15:47", "link": "http://arxiv.org/abs/2309.08316v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Distributional Inclusion Hypothesis and Quantifications: Probing for\n  Hypernymy in Functional Distributional Semantics", "abstract": "Functional Distributional Semantics (FDS) models the meaning of words by\ntruth-conditional functions. This provides a natural representation for\nhypernymy but no guarantee that it can be learnt when FDS models are trained on\na corpus. In this paper, we probe into FDS models and study the representations\nlearnt, drawing connections between quantifications, the Distributional\nInclusion Hypothesis (DIH), and the variational-autoencoding objective of FDS\nmodel training. Using synthetic data sets, we reveal that FDS models learn\nhypernymy on a restricted class of corpus that strictly follows the DIH. We\nfurther introduce a training objective that both enables hypernymy learning\nunder the reverse of the DIH and improves hypernymy detection from real\ncorpora.", "published": "2023-09-15 11:28:52", "link": "http://arxiv.org/abs/2309.08325v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reward Engineering for Generating Semi-structured Explanation", "abstract": "Semi-structured explanation depicts the implicit process of a reasoner with\nan explicit representation. This explanation highlights how available\ninformation in a specific query is utilised and supplemented with information a\nreasoner produces from its internal weights towards generating an answer.\nDespite the recent improvements in generative capabilities of language models,\nproducing structured explanations to verify a model's true reasoning\ncapabilities remains a challenge. This issue is particularly pronounced for\nnot-so-large LMs (e.g., FLAN-T5-XXL). In this work, we first underscore the\nlimitations of supervised fine-tuning (SFT) in tackling this challenge, and\nthen introduce a carefully crafted reward engineering method in reinforcement\nlearning (RL) to better address this problem. We investigate multiple reward\naggregation methods and provide a detailed discussion which sheds light on the\npromising potential of RL for future research. Our proposed method on two\nsemi-structured explanation generation benchmarks (ExplaGraph and COPA-SSE)\nachieves new state-of-the-art results.", "published": "2023-09-15 12:10:03", "link": "http://arxiv.org/abs/2309.08347v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Headless Language Models: Learning without Predicting with Contrastive\n  Weight Tying", "abstract": "Self-supervised pre-training of language models usually consists in\npredicting probability distributions over extensive token vocabularies. In this\nstudy, we propose an innovative method that shifts away from probability\nprediction and instead focuses on reconstructing input embeddings in a\ncontrastive fashion via Constrastive Weight Tying (CWT). We apply this approach\nto pretrain Headless Language Models in both monolingual and multilingual\ncontexts. Our method offers practical advantages, substantially reducing\ntraining computational requirements by up to 20 times, while simultaneously\nenhancing downstream performance and data efficiency. We observe a significant\n+1.6 GLUE score increase and a notable +2.7 LAMBADA accuracy improvement\ncompared to classical LMs within similar compute budgets.", "published": "2023-09-15 12:20:00", "link": "http://arxiv.org/abs/2309.08351v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unleashing Potential of Evidence in Knowledge-Intensive Dialogue\n  Generation", "abstract": "Incorporating external knowledge into dialogue generation (KIDG) is crucial\nfor improving the correctness of response, where evidence fragments serve as\nknowledgeable snippets supporting the factual dialogue replies. However,\nintroducing irrelevant content often adversely impacts reply quality and easily\nleads to hallucinated responses. Prior work on evidence retrieval and\nintegration in dialogue systems falls short of fully leveraging existing\nevidence since the model fails to locate useful fragments accurately and\noverlooks hidden evidence labels within the KIDG dataset. To fully Unleash the\npotential of evidence, we propose a framework to effectively incorporate\nEvidence in knowledge-Intensive Dialogue Generation (u-EIDG). Specifically, we\nintroduce an automatic evidence generation framework that harnesses the power\nof Large Language Models (LLMs) to mine reliable evidence veracity labels from\nunlabeled data. By utilizing these evidence labels, we train a reliable\nevidence indicator to effectively identify relevant evidence from retrieved\npassages. Furthermore, we propose an evidence-augmented generator with an\nevidence-focused attention mechanism, which allows the model to concentrate on\nevidenced segments. Experimental results on MultiDoc2Dial demonstrate the\nefficacy of evidential label augmentation and refined attention mechanisms in\nimproving model performance. Further analysis confirms that the proposed method\noutperforms other baselines (+3~+5 points) regarding coherence and factual\nconsistency.", "published": "2023-09-15 13:13:30", "link": "http://arxiv.org/abs/2309.08380v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Advancing the Evaluation of Traditional Chinese Language Models: Towards\n  a Comprehensive Benchmark Suite", "abstract": "The evaluation of large language models is an essential task in the field of\nlanguage understanding and generation. As language models continue to advance,\nthe need for effective benchmarks to assess their performance has become\nimperative. In the context of Traditional Chinese, there is a scarcity of\ncomprehensive and diverse benchmarks to evaluate the capabilities of language\nmodels, despite the existence of certain benchmarks such as DRCD, TTQA, CMDQA,\nand FGC dataset. To address this gap, we propose a novel set of benchmarks that\nleverage existing English datasets and are tailored to evaluate language models\nin Traditional Chinese. These benchmarks encompass a wide range of tasks,\nincluding contextual question-answering, summarization, classification, and\ntable understanding. The proposed benchmarks offer a comprehensive evaluation\nframework, enabling the assessment of language models' capabilities across\ndifferent tasks. In this paper, we evaluate the performance of GPT-3.5,\nTaiwan-LLaMa-v1.0, and Model 7-C, our proprietary model, on these benchmarks.\nThe evaluation results highlight that our model, Model 7-C, achieves\nperformance comparable to GPT-3.5 with respect to a part of the evaluated\ncapabilities. In an effort to advance the evaluation of language models in\nTraditional Chinese and stimulate further research in this field, we have\nopen-sourced our benchmark and opened the model for trial.", "published": "2023-09-15 14:52:23", "link": "http://arxiv.org/abs/2309.08448v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ICLEF: In-Context Learning with Expert Feedback for Explainable Style\n  Transfer", "abstract": "While state-of-the-art large language models (LLMs) can excel at adapting\ntext from one style to another, current work does not address the\nexplainability of style transfer models. Recent work has explored generating\ntextual explanations from larger teacher models and distilling them into\nsmaller student models. One challenge with such approach is that LLM outputs\nmay contain errors that require expertise to correct, but gathering and\nincorporating expert feedback is difficult due to cost and availability. To\naddress this challenge, we propose ICLEF, a novel human-AI collaboration\napproach to model distillation that incorporates scarce expert human feedback\nby combining in-context learning and model self-critique. We show that our\nmethod leads to generation of high-quality synthetic explainable style transfer\ndatasets for formality (e-GYAFC) and subjective bias (e-WNC). Via automatic and\nhuman evaluation, we show that specialized student models fine-tuned on our\ndatasets outperform generalist teacher models on the explainable style transfer\ntask in one-shot settings, and perform competitively compared to few-shot\nteacher models, highlighting the quality of the data and the role of expert\nfeedback. In an extrinsic task of authorship attribution, we show that\nexplanations generated by smaller models fine-tuned on e-GYAFC are more\npredictive of authorship than explanations generated by few-shot teacher\nmodels.", "published": "2023-09-15 17:41:14", "link": "http://arxiv.org/abs/2309.08583v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Machine Translation Models Can Learn to be Few-shot Learners", "abstract": "The emergent ability of Large Language Models to use a small number of\nexamples to learn to perform in novel domains and tasks, also called in-context\nlearning (ICL). In this work, we show that a much smaller model can be trained\nto perform ICL by fine-tuning towards a specialized training objective,\nexemplified on the task of domain adaptation for neural machine translation.\nWith this capacity for ICL, the model can take advantage of relevant few-shot\nexamples to adapt its output towards the domain. We compare the quality of this\ndomain adaptation to traditional supervised techniques and ICL with a\n40B-parameter Large Language Model. Our approach allows efficient batch\ninference on a mix of domains and outperforms state-of-the-art baselines in\nterms of both translation quality and immediate adaptation rate, i.e. the\nability to reproduce a specific term after being shown a single example.", "published": "2023-09-15 17:44:21", "link": "http://arxiv.org/abs/2309.08590v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are Multilingual LLMs Culturally-Diverse Reasoners? An Investigation\n  into Multicultural Proverbs and Sayings", "abstract": "Large language models (LLMs) are highly adept at question answering and\nreasoning tasks, but when reasoning in a situational context, human\nexpectations vary depending on the relevant cultural common ground. As\nlanguages are associated with diverse cultures, LLMs should also be\nculturally-diverse reasoners. In this paper, we study the ability of a wide\nrange of state-of-the-art multilingual LLMs (mLLMs) to reason with proverbs and\nsayings in a conversational context. Our experiments reveal that: (1) mLLMs\n\"know\" limited proverbs and memorizing proverbs does not mean understanding\nthem within a conversational context; (2) mLLMs struggle to reason with\nfigurative proverbs and sayings, and when asked to select the wrong answer\n(instead of asking it to select the correct answer); and (3) there is a\n\"culture gap\" in mLLMs when reasoning about proverbs and sayings translated\nfrom other languages. We construct and release our evaluation dataset MAPS\n(MulticultrAl Proverbs and Sayings) for proverb understanding with\nconversational context for six different languages.", "published": "2023-09-15 17:45:28", "link": "http://arxiv.org/abs/2309.08591v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"Merge Conflicts!\" Exploring the Impacts of External Distractors to\n  Parametric Knowledge Graphs", "abstract": "Large language models (LLMs) acquire extensive knowledge during pre-training,\nknown as their parametric knowledge. However, in order to remain up-to-date and\nalign with human instructions, LLMs inevitably require external knowledge\nduring their interactions with users. This raises a crucial question: How will\nLLMs respond when external knowledge interferes with their parametric\nknowledge? To investigate this question, we propose a framework that\nsystematically elicits LLM parametric knowledge and introduces external\nknowledge. Specifically, we uncover the impacts by constructing a parametric\nknowledge graph to reveal the different knowledge structures of LLMs, and\nintroduce external knowledge through distractors of varying degrees, methods,\npositions, and formats. Our experiments on both black-box and open-source\nmodels demonstrate that LLMs tend to produce responses that deviate from their\nparametric knowledge, particularly when they encounter direct conflicts or\nconfounding changes of information within detailed contexts. We also find that\nwhile LLMs are sensitive to the veracity of external knowledge, they can still\nbe distracted by unrelated information. These findings highlight the risk of\nhallucination when integrating external knowledge, even indirectly, during\ninteractions with current LLMs. All the data and results are publicly\navailable.", "published": "2023-09-15 17:47:59", "link": "http://arxiv.org/abs/2309.08594v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Vocabulary-level Memory Efficiency for Language Model Fine-tuning", "abstract": "The extensive memory footprint of language model (LM) fine-tuning poses a\nchallenge for both researchers and practitioners. LMs use an embedding matrix\nto represent extensive vocabularies, forming a substantial proportion of the\nmodel parameters. While previous work towards memory-efficient fine-tuning has\nfocused on minimizing the number of trainable parameters, reducing the memory\nfootprint of the embedding matrix has yet to be explored. We first demonstrate\nthat a significant proportion of the vocabulary remains unused during\nfine-tuning. We then propose a simple yet effective approach that leverages\nthis finding to minimize memory usage. We show that our approach provides\nsubstantial reductions in memory usage across a wide range of models and tasks.\nNotably, our approach does not impact downstream task performance, while\nallowing more efficient use of computational resources.", "published": "2023-09-15 19:00:00", "link": "http://arxiv.org/abs/2309.08708v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-training Strategies for Sentiment Analysis: An Empirical Study", "abstract": "Sentiment analysis is a crucial task in natural language processing that\ninvolves identifying and extracting subjective sentiment from text.\nSelf-training has recently emerged as an economical and efficient technique for\ndeveloping sentiment analysis models by leveraging a small amount of labeled\ndata and a large amount of unlabeled data. However, given a set of training\ndata, how to utilize them to conduct self-training makes a significant\ndifference in the final performance of the model. We refer to this methodology\nas the self-training strategy. In this paper, we present an empirical study of\nvarious self-training strategies for sentiment analysis. First, we investigate\nthe influence of the self-training strategy and hyper-parameters on the\nperformance of traditional small language models (SLMs) in various few-shot\nsettings. Second, we also explore the feasibility of leveraging large language\nmodels (LLMs) to help self-training. We propose and empirically compare several\nself-training strategies with the intervention of LLMs. Extensive experiments\nare conducted on three real-world sentiment analysis datasets.", "published": "2023-09-15 21:42:46", "link": "http://arxiv.org/abs/2309.08777v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Embeddings for Measuring Text Relatedness: Unveiling\n  Sentiments and Relationships in Online Comments", "abstract": "After the COVID-19 pandemic caused internet usage to grow by 70%, there has\nbeen an increased number of people all across the world using social media.\nApplications like Twitter, Meta Threads, YouTube, and Reddit have become\nincreasingly pervasive, leaving almost no digital space where public opinion is\nnot expressed. This paper investigates sentiment and semantic relationships\namong comments across various social media platforms, as well as discusses the\nimportance of shared opinions across these different media platforms, using\nword embeddings to analyze components in sentences and documents. It allows\nresearchers, politicians, and business representatives to trace a path of\nshared sentiment among users across the world. This research paper presents\nmultiple approaches that measure the relatedness of text extracted from user\ncomments on these popular online platforms. By leveraging embeddings, which\ncapture semantic relationships between words and help analyze sentiments across\nthe web, we can uncover connections regarding public opinion as a whole. The\nstudy utilizes pre-existing datasets from YouTube, Reddit, Twitter, and more.\nWe made use of popular natural language processing models like Bidirectional\nEncoder Representations from Transformers (BERT) to analyze sentiments and\nexplore relationships between comment embeddings. Additionally, we aim to\nutilize clustering and Kl-divergence to find semantic relationships within\nthese comment embeddings across various social media platforms. Our analysis\nwill enable a deeper understanding of the interconnectedness of online comments\nand will investigate the notion of the internet functioning as a large\ninterconnected brain.", "published": "2023-09-15 04:57:23", "link": "http://arxiv.org/abs/2310.05964v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Research on Joint Representation Learning Methods for Entity\n  Neighborhood Information and Description Information", "abstract": "To address the issue of poor embedding performance in the knowledge graph of\na programming design course, a joint represen-tation learning model that\ncombines entity neighborhood infor-mation and description information is\nproposed. Firstly, a graph at-tention network is employed to obtain the\nfeatures of entity neigh-boring nodes, incorporating relationship features to\nenrich the structural information. Next, the BERT-WWM model is utilized in\nconjunction with attention mechanisms to obtain the representation of entity\ndescription information. Finally, the final entity vector representation is\nobtained by combining the vector representations of entity neighborhood\ninformation and description information. Experimental results demonstrate that\nthe proposed model achieves favorable performance on the knowledge graph\ndataset of the pro-gramming design course, outperforming other baseline models.", "published": "2023-09-15 01:38:07", "link": "http://arxiv.org/abs/2309.08100v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Self-Assessment Tests are Unreliable Measures of LLM Personality", "abstract": "As large language models (LLM) evolve in their capabilities, various recent\nstudies have tried to quantify their behavior using psychological tools created\nto study human behavior. One such example is the measurement of \"personality\"\nof LLMs using self-assessment personality tests developed to measure human\npersonality. Yet almost none of these works verify the applicability of these\ntests on LLMs. In this paper, we analyze the reliability of LLM personality\nscores obtained from self-assessment personality tests using two simple\nexperiments. We first introduce the property of prompt sensitivity, where three\nsemantically equivalent prompts representing three intuitive ways of\nadministering self-assessment tests on LLMs are used to measure the personality\nof the same LLM. We find that all three prompts lead to very different\npersonality scores, a difference that is statistically significant for all\ntraits in a large majority of scenarios. We then introduce the property of\noption-order symmetry for personality measurement of LLMs. Since most of the\nself-assessment tests exist in the form of multiple choice question (MCQ)\nquestions, we argue that the scores should also be robust to not just the\nprompt template but also the order in which the options are presented. This\ntest unsurprisingly reveals that the self-assessment test scores are not robust\nto the order of the options. These simple tests, done on ChatGPT and three\nLlama2 models of different sizes, show that self-assessment personality tests\ncreated for humans are unreliable measures of personality in LLMs.", "published": "2023-09-15 05:19:39", "link": "http://arxiv.org/abs/2309.08163v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Using Large Language Model to Solve and Explain Physics Word Problems\n  Approaching Human Level", "abstract": "Our work demonstrates that large language model (LLM) pre-trained on texts\ncan not only solve pure math word problems, but also physics word problems,\nwhose solution requires calculation and inference based on prior physical\nknowledge. We collect and annotate the first physics word problem\ndataset-PhysQA, which contains over 1000 junior high school physics word\nproblems (covering Kinematics, Mass&Density, Mechanics, Heat, Electricity).\nThen we use OpenAI' s GPT3.5 to generate the answer of these problems and found\nthat GPT3.5 could automatically solve 49.3% of the problems through zero-shot\nlearning and 73.2% through few-shot learning. This result demonstrates that by\nusing similar problems and their answers as prompt, LLM could solve elementary\nphysics word problems approaching human level performance. In addition to\nsolving problems, GPT3.5 can also summarize the knowledge or topics covered by\nthe problems, provide relevant explanations, and generate new physics word\nproblems based on the input. Our work is the first research to focus on the\nautomatic solving, explanation, and generation of physics word problems across\nvarious types and scenarios, and we achieve an acceptable and state-of-the-art\naccuracy. This underscores the potential of LLMs for further applications in\nsecondary education.", "published": "2023-09-15 06:13:06", "link": "http://arxiv.org/abs/2309.08182v2", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Structural Self-Supervised Objectives for Transformers", "abstract": "This thesis focuses on improving the pre-training of natural language models\nusing unsupervised raw data to make them more efficient and aligned with\ndownstream applications.\n  In the first part, we introduce three alternative pre-training objectives to\nBERT's Masked Language Modeling (MLM), namely Random Token Substitution (RTS),\nCluster-based Random Token Substitution (C-RTS), and Swapped Language Modeling\n(SLM). These objectives involve token swapping instead of masking, with RTS and\nC-RTS aiming to predict token originality and SLM predicting the original token\nvalues. Results show that RTS and C-RTS require less pre-training time while\nmaintaining performance comparable to MLM. Surprisingly, SLM outperforms MLM on\ncertain tasks despite using the same computational budget.\n  In the second part, we proposes self-supervised pre-training tasks that align\nstructurally with downstream applications, reducing the need for labeled data.\nWe use large corpora like Wikipedia and CC-News to train models to recognize if\ntext spans originate from the same paragraph or document in several ways. By\ndoing continuous pre-training, starting from existing models like RoBERTa,\nELECTRA, DeBERTa, BART, and T5, we demonstrate significant performance\nimprovements in tasks like Fact Verification, Answer Sentence Selection, and\nSummarization. These improvements are especially pronounced when limited\nannotation data is available. The proposed objectives also achieve\nstate-of-the-art results on various benchmark datasets, including FEVER (dev\nset), ASNQ, WikiQA, and TREC-QA, as well as enhancing the quality of summaries.\nImportantly, these techniques can be easily integrated with other methods\nwithout altering the internal structure of Transformer models, making them\nversatile for various NLP applications.", "published": "2023-09-15 09:30:45", "link": "http://arxiv.org/abs/2309.08272v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Data Distribution Bottlenecks in Grounding Language Models to Knowledge\n  Bases", "abstract": "Language models (LMs) have already demonstrated remarkable abilities in\nunderstanding and generating both natural and formal language. Despite these\nadvances, their integration with real-world environments such as large-scale\nknowledge bases (KBs) remains an underdeveloped area, affecting applications\nsuch as semantic parsing and indulging in \"hallucinated\" information. This\npaper is an experimental investigation aimed at uncovering the robustness\nchallenges that LMs encounter when tasked with knowledge base question\nanswering (KBQA). The investigation covers scenarios with inconsistent data\ndistribution between training and inference, such as generalization to unseen\ndomains, adaptation to various language variations, and transferability across\ndifferent datasets. Our comprehensive experiments reveal that even when\nemployed with our proposed data augmentation techniques, advanced small and\nlarge language models exhibit poor performance in various dimensions. While the\nLM is a promising technology, the robustness of the current form in dealing\nwith complex environments is fragile and of limited practicality because of the\ndata distribution issue. This calls for future research on data collection and\nLM learning paradims.", "published": "2023-09-15 12:06:45", "link": "http://arxiv.org/abs/2309.08345v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PatFig: Generating Short and Long Captions for Patent Figures", "abstract": "This paper introduces Qatent PatFig, a novel large-scale patent figure\ndataset comprising 30,000+ patent figures from over 11,000 European patent\napplications. For each figure, this dataset provides short and long captions,\nreference numerals, their corresponding terms, and the minimal claim set that\ndescribes the interactions between the components of the image. To assess the\nusability of the dataset, we finetune an LVLM model on Qatent PatFig to\ngenerate short and long descriptions, and we investigate the effects of\nincorporating various text-based cues at the prediction stage of the patent\nfigure captioning process.", "published": "2023-09-15 13:10:36", "link": "http://arxiv.org/abs/2309.08379v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Silver Retriever: Advancing Neural Passage Retrieval for Polish Question\n  Answering", "abstract": "Modern open-domain question answering systems often rely on accurate and\nefficient retrieval components to find passages containing the facts necessary\nto answer the question. Recently, neural retrievers have gained popularity over\nlexical alternatives due to their superior performance. However, most of the\nwork concerns popular languages such as English or Chinese. For others, such as\nPolish, few models are available. In this work, we present Silver Retriever, a\nneural retriever for Polish trained on a diverse collection of manually or\nweakly labeled datasets. Silver Retriever achieves much better results than\nother Polish models and is competitive with larger multilingual models.\nTogether with the model, we open-source five new passage retrieval datasets.", "published": "2023-09-15 15:19:53", "link": "http://arxiv.org/abs/2309.08469v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Using Large Language Models for Knowledge Engineering (LLMKE): A Case\n  Study on Wikidata", "abstract": "In this work, we explore the use of Large Language Models (LLMs) for\nknowledge engineering tasks in the context of the ISWC 2023 LM-KBC Challenge.\nFor this task, given subject and relation pairs sourced from Wikidata, we\nutilize pre-trained LLMs to produce the relevant objects in string format and\nlink them to their respective Wikidata QIDs. We developed a pipeline using LLMs\nfor Knowledge Engineering (LLMKE), combining knowledge probing and Wikidata\nentity mapping. The method achieved a macro-averaged F1-score of 0.701 across\nthe properties, with the scores varying from 1.00 to 0.328. These results\ndemonstrate that the knowledge of LLMs varies significantly depending on the\ndomain and that further experimentation is required to determine the\ncircumstances under which LLMs can be used for automatic Knowledge Base (e.g.,\nWikidata) completion and correction. The investigation of the results also\nsuggests the promising contribution of LLMs in collaborative knowledge\nengineering. LLMKE won Track 2 of the challenge. The implementation is\navailable at https://github.com/bohuizhang/LLMKE.", "published": "2023-09-15 15:51:14", "link": "http://arxiv.org/abs/2309.08491v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "HealthFC: Verifying Health Claims with Evidence-Based Medical\n  Fact-Checking", "abstract": "In the digital age, seeking health advice on the Internet has become a common\npractice. At the same time, determining the trustworthiness of online medical\ncontent is increasingly challenging. Fact-checking has emerged as an approach\nto assess the veracity of factual claims using evidence from credible knowledge\nsources. To help advance automated Natural Language Processing (NLP) solutions\nfor this task, in this paper we introduce a novel dataset HealthFC. It consists\nof 750 health-related claims in German and English, labeled for veracity by\nmedical experts and backed with evidence from systematic reviews and clinical\ntrials. We provide an analysis of the dataset, highlighting its characteristics\nand challenges. The dataset can be used for NLP tasks related to automated\nfact-checking, such as evidence retrieval, claim verification, or explanation\ngeneration. For testing purposes, we provide baseline systems based on\ndifferent approaches, examine their performance, and discuss the findings. We\nshow that the dataset is a challenging test bed with a high potential for\nfuture use.", "published": "2023-09-15 16:05:48", "link": "http://arxiv.org/abs/2309.08503v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Connecting Large Language Models with Evolutionary Algorithms Yields\n  Powerful Prompt Optimizers", "abstract": "Large Language Models (LLMs) excel in various tasks, but they rely on\ncarefully crafted prompts that often demand substantial human effort. To\nautomate this process, in this paper, we propose a novel framework for discrete\nprompt optimization, called EvoPrompt, which borrows the idea of evolutionary\nalgorithms (EAs) as they exhibit good performance and fast convergence. To\nenable EAs to work on discrete prompts, which are natural language expressions\nthat need to be coherent and human-readable, we connect LLMs with EAs. This\napproach allows us to simultaneously leverage the powerful language processing\ncapabilities of LLMs and the efficient optimization performance of EAs.\nSpecifically, abstaining from any gradients or parameters, EvoPrompt starts\nfrom a population of prompts and iteratively generates new prompts with LLMs\nbased on the evolutionary operators, improving the population based on the\ndevelopment set. We optimize prompts for both closed- and open-source LLMs\nincluding GPT-3.5 and Alpaca, on 31 datasets covering language understanding,\ngeneration tasks, as well as BIG-Bench Hard (BBH) tasks. EvoPrompt\nsignificantly outperforms human-engineered prompts and existing methods for\nautomatic prompt generation (e.g., up to 25% on BBH). Furthermore, EvoPrompt\ndemonstrates that connecting LLMs with EAs creates synergies, which could\ninspire further research on the combination of LLMs and conventional\nalgorithms.", "published": "2023-09-15 16:50:09", "link": "http://arxiv.org/abs/2309.08532v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "How Transferable are Attribute Controllers on Pretrained Multilingual\n  Translation Models?", "abstract": "Customizing machine translation models to comply with desired attributes\n(e.g., formality or grammatical gender) is a well-studied topic. However, most\ncurrent approaches rely on (semi-)supervised data with attribute annotations.\nThis data scarcity bottlenecks democratizing such customization possibilities\nto a wider range of languages, particularly lower-resource ones. This gap is\nout of sync with recent progress in pretrained massively multilingual\ntranslation models. In response, we transfer the attribute controlling\ncapabilities to languages without attribute-annotated data with an NLLB-200\nmodel as a foundation. Inspired by techniques from controllable generation, we\nemploy a gradient-based inference-time controller to steer the pretrained\nmodel. The controller transfers well to zero-shot conditions, as it operates on\npretrained multilingual representations and is attribute -- rather than\nlanguage-specific. With a comprehensive comparison to finetuning-based control,\nwe demonstrate that, despite finetuning's clear dominance in supervised\nsettings, the gap to inference-time control closes when moving to zero-shot\nconditions, especially with new and distant target languages. The latter also\nshows stronger domain robustness. We further show that our inference-time\ncontrol complements finetuning. A human evaluation on a real low-resource\nlanguage, Bengali, confirms our findings. Our code is\nhttps://github.com/dannigt/attribute-controller-transfer", "published": "2023-09-15 17:33:24", "link": "http://arxiv.org/abs/2309.08565v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Indian-BhED: A Dataset for Measuring India-Centric Biases in Large\n  Language Models", "abstract": "Large Language Models (LLMs), now used daily by millions, can encode societal\nbiases, exposing their users to representational harms. A large body of\nscholarship on LLM bias exists but it predominantly adopts a Western-centric\nframe and attends comparatively less to bias levels and potential harms in the\nGlobal South. In this paper, we quantify stereotypical bias in popular LLMs\naccording to an Indian-centric frame through Indian-BhED, a first of its kind\ndataset, containing stereotypical and anti-stereotypical examples in the\ncontext of caste and religious stereotypes in India. We find that the majority\nof LLMs tested have a strong propensity to output stereotypes in the Indian\ncontext, especially when compared to axes of bias traditionally studied in the\nWestern context, such as gender and race. Notably, we find that GPT-2, GPT-2\nLarge, and GPT 3.5 have a particularly high propensity for preferring\nstereotypical outputs as a percent of all sentences for the axes of caste\n(63-79%) and religion (69-72%). We finally investigate potential causes for\nsuch harmful behaviour in LLMs, and posit intervention techniques to reduce\nboth stereotypical and anti-stereotypical biases. The findings of this work\nhighlight the need for including more diverse voices when researching fairness\nin AI and evaluating LLMs.", "published": "2023-09-15 17:38:41", "link": "http://arxiv.org/abs/2309.08573v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Sparse Autoencoders Find Highly Interpretable Features in Language\n  Models", "abstract": "One of the roadblocks to a better understanding of neural networks' internals\nis \\textit{polysemanticity}, where neurons appear to activate in multiple,\nsemantically distinct contexts. Polysemanticity prevents us from identifying\nconcise, human-understandable explanations for what neural networks are doing\ninternally. One hypothesised cause of polysemanticity is\n\\textit{superposition}, where neural networks represent more features than they\nhave neurons by assigning features to an overcomplete set of directions in\nactivation space, rather than to individual neurons. Here, we attempt to\nidentify those directions, using sparse autoencoders to reconstruct the\ninternal activations of a language model. These autoencoders learn sets of\nsparsely activating features that are more interpretable and monosemantic than\ndirections identified by alternative approaches, where interpretability is\nmeasured by automated methods. Moreover, we show that with our learned set of\nfeatures, we can pinpoint the features that are causally responsible for\ncounterfactual behaviour on the indirect object identification task\n\\citep{wang2022interpretability} to a finer degree than previous\ndecompositions. This work indicates that it is possible to resolve\nsuperposition in language models using a scalable, unsupervised method. Our\nmethod may serve as a foundation for future mechanistic interpretability work,\nwhich we hope will enable greater model transparency and steerability.", "published": "2023-09-15 17:56:55", "link": "http://arxiv.org/abs/2309.08600v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Intent Detection at Scale: Tuning a Generic Model using Relevant Intents", "abstract": "Accurately predicting the intent of customer support requests is vital for\nefficient support systems, enabling agents to quickly understand messages and\nprioritize responses accordingly. While different approaches exist for intent\ndetection, maintaining separate client-specific or industry-specific models can\nbe costly and impractical as the client base expands.\n  This work proposes a system to scale intent predictions to various clients\neffectively, by combining a single generic model with a per-client list of\nrelevant intents. Our approach minimizes training and maintenance costs while\nproviding a personalized experience for clients, allowing for seamless\nadaptation to changes in their relevant intents. Furthermore, we propose a\nstrategy for using the clients relevant intents as model features that proves\nto be resilient to changes in the relevant intents of clients -- a common\noccurrence in production environments.\n  The final system exhibits significantly superior performance compared to\nindustry-specific models, showcasing its flexibility and ability to cater to\ndiverse client needs.", "published": "2023-09-15 13:15:20", "link": "http://arxiv.org/abs/2309.08647v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MAPLE: Mobile App Prediction Leveraging Large Language Model Embeddings", "abstract": "In recent years, predicting mobile app usage has become increasingly\nimportant for areas like app recommendation, user behaviour analysis, and\nmobile resource management. Existing models, however, struggle with the\nheterogeneous nature of contextual data and the user cold start problem. This\nstudy introduces a novel prediction model, Mobile App Prediction Leveraging\nLarge Language Model Embeddings (MAPLE), which employs Large Language Models\n(LLMs) and installed app similarity to overcome these challenges. MAPLE\nutilises the power of LLMs to process contextual data and discern intricate\nrelationships within it effectively. Additionally, we explore the use of\ninstalled app similarity to address the cold start problem, facilitating the\nmodelling of user preferences and habits, even for new users with limited\nhistorical data. In essence, our research presents MAPLE as a novel, potent,\nand practical approach to app usage prediction, making significant strides in\nresolving issues faced by existing models. MAPLE stands out as a comprehensive\nand effective solution, setting a new benchmark for more precise and\npersonalised app usage predictions. In tests on two real-world datasets, MAPLE\nsurpasses contemporary models in both standard and cold start scenarios. These\noutcomes validate MAPLE's capacity for precise app usage predictions and its\nresilience against the cold start problem. This enhanced performance stems from\nthe model's proficiency in capturing complex temporal patterns and leveraging\ncontextual information. As a result, MAPLE can potentially improve personalised\nmobile app usage predictions and user experiences markedly.", "published": "2023-09-15 13:15:54", "link": "http://arxiv.org/abs/2309.08648v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Fake News Detectors are Biased against Texts Generated by Large Language\n  Models", "abstract": "The spread of fake news has emerged as a critical challenge, undermining\ntrust and posing threats to society. In the era of Large Language Models\n(LLMs), the capability to generate believable fake content has intensified\nthese concerns. In this study, we present a novel paradigm to evaluate fake\nnews detectors in scenarios involving both human-written and LLM-generated\nmisinformation. Intriguingly, our findings reveal a significant bias in many\nexisting detectors: they are more prone to flagging LLM-generated content as\nfake news while often misclassifying human-written fake news as genuine. This\nunexpected bias appears to arise from distinct linguistic patterns inherent to\nLLM outputs. To address this, we introduce a mitigation strategy that leverages\nadversarial training with LLM-paraphrased genuine news. The resulting model\nyielded marked improvements in detection accuracy for both human and\nLLM-generated news. To further catalyze research in this domain, we release two\ncomprehensive datasets, \\texttt{GossipCop++} and \\texttt{PolitiFact++}, thus\namalgamating human-validated articles with LLM-generated fake and real news.", "published": "2023-09-15 18:04:40", "link": "http://arxiv.org/abs/2309.08674v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Generating Semantic Graph Corpora with Graph Expansion Grammar", "abstract": "We introduce Lovelace, a tool for creating corpora of semantic graphs. The\nsystem uses graph expansion grammar as a representational language, thus\nallowing users to craft a grammar that describes a corpus with desired\nproperties. When given such grammar as input, the system generates a set of\noutput graphs that are well-formed according to the grammar, i.e., a graph\nbank. The generation process can be controlled via a number of configurable\nparameters that allow the user to, for example, specify a range of desired\noutput graph sizes. Central use cases are the creation of synthetic data to\naugment existing corpora, and as a pedagogical tool for teaching formal\nlanguage theory.", "published": "2023-09-15 19:10:19", "link": "http://arxiv.org/abs/2309.08714v1", "categories": ["cs.FL", "cs.CL", "F.4.3; I.2.7"], "primary_category": "cs.FL"}
{"title": "OpenAI Cribbed Our Tax Example, But Can GPT-4 Really Do Tax?", "abstract": "The authors explain where OpenAI got the tax law example in its livestream\ndemonstration of GPT-4, why GPT-4 got the wrong answer, and how it fails to\nreliably calculate taxes.", "published": "2023-09-15 20:00:27", "link": "http://arxiv.org/abs/2309.09992v2", "categories": ["cs.AI", "cs.CL", "I.2.7; I.2.0"], "primary_category": "cs.AI"}
{"title": "Personality Profiling: How informative are social media profiles in\n  predicting personal information?", "abstract": "Personality profiling has been utilised by companies for targeted\nadvertising, political campaigns and public health campaigns. However, the\naccuracy and versatility of such models remains relatively unknown. Here we\nexplore the extent to which peoples' online digital footprints can be used to\nprofile their Myers-Briggs personality type. We analyse and compare four\nmodels: logistic regression, naive Bayes, support vector machines (SVMs) and\nrandom forests. We discover that a SVM model achieves the best accuracy of\n20.95% for predicting a complete personality type. However, logistic regression\nmodels perform only marginally worse and are significantly faster to train and\nperform predictions. Moreover, we develop a statistical framework for assessing\nthe importance of different sets of features in our models. We discover some\nfeatures to be more informative than others in the Intuitive/Sensory (p =\n0.032) and Thinking/Feeling (p = 0.019) models. Many labelled datasets present\nsubstantial class imbalances of personal characteristics on social media,\nincluding our own. We therefore highlight the need for attentive consideration\nwhen reporting model performance on such datasets and compare a number of\nmethods to fix class-imbalance problems.", "published": "2023-09-15 03:09:43", "link": "http://arxiv.org/abs/2309.13065v2", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Characterizing the temporal dynamics of universal speech representations\n  for generalizable deepfake detection", "abstract": "Existing deepfake speech detection systems lack generalizability to unseen\nattacks (i.e., samples generated by generative algorithms not seen during\ntraining). Recent studies have explored the use of universal speech\nrepresentations to tackle this issue and have obtained inspiring results. These\nworks, however, have focused on innovating downstream classifiers while leaving\nthe representation itself untouched. In this study, we argue that\ncharacterizing the long-term temporal dynamics of these representations is\ncrucial for generalizability and propose a new method to assess representation\ndynamics. Indeed, we show that different generative models generate similar\nrepresentation dynamics patterns with our proposed method. Experiments on the\nASVspoof 2019 and 2021 datasets validate the benefits of the proposed method to\ndetect deepfakes from methods unseen during training, significantly improving\non several benchmark methods.", "published": "2023-09-15 01:37:45", "link": "http://arxiv.org/abs/2309.08099v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Empowering Private Tutoring by Chaining Large Language Models", "abstract": "Artificial intelligence has been applied in various aspects of online\neducation to facilitate teaching and learning. However, few approaches has been\nmade toward a complete AI-powered tutoring system. In this work, we explore the\ndevelopment of a full-fledged intelligent tutoring system powered by\nstate-of-the-art large language models (LLMs), covering automatic course\nplanning and adjusting, tailored instruction, and flexible quiz evaluation. To\nmake the system robust to prolonged interaction and cater to individualized\neducation, the system is decomposed into three inter-connected core\nprocesses-interaction, reflection, and reaction. Each process is implemented by\nchaining LLM-powered tools along with dynamically updated memory modules. Tools\nare LLMs prompted to execute one specific task at a time, while memories are\ndata storage that gets updated during education process. Statistical results\nfrom learning logs demonstrate the effectiveness and mechanism of each tool\nusage. Subjective feedback from human users reveal the usability of each\nfunction, and comparison with ablation systems further testify the benefits of\nthe designed processes in long-term interaction.", "published": "2023-09-15 02:42:03", "link": "http://arxiv.org/abs/2309.08112v2", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "PromptTTS++: Controlling Speaker Identity in Prompt-Based Text-to-Speech\n  Using Natural Language Descriptions", "abstract": "We propose PromptTTS++, a prompt-based text-to-speech (TTS) synthesis system\nthat allows control over speaker identity using natural language descriptions.\nTo control speaker identity within the prompt-based TTS framework, we introduce\nthe concept of speaker prompt, which describes voice characteristics (e.g.,\ngender-neutral, young, old, and muffled) designed to be approximately\nindependent of speaking style. Since there is no large-scale dataset containing\nspeaker prompts, we first construct a dataset based on the LibriTTS-R corpus\nwith manually annotated speaker prompts. We then employ a diffusion-based\nacoustic model with mixture density networks to model diverse speaker factors\nin the training data. Unlike previous studies that rely on style prompts\ndescribing only a limited aspect of speaker individuality, such as pitch,\nspeaking speed, and energy, our method utilizes an additional speaker prompt to\neffectively learn the mapping from natural language descriptions to the\nacoustic features of diverse speakers. Our subjective evaluation results show\nthat the proposed method can better control speaker characteristics than the\nmethods without the speaker prompt. Audio samples are available at\nhttps://reppy4620.github.io/demo.promptttspp/.", "published": "2023-09-15 04:11:37", "link": "http://arxiv.org/abs/2309.08140v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Unimodal Aggregation for CTC-based Speech Recognition", "abstract": "This paper works on non-autoregressive automatic speech recognition. A\nunimodal aggregation (UMA) is proposed to segment and integrate the feature\nframes that belong to the same text token, and thus to learn better feature\nrepresentations for text tokens. The frame-wise features and weights are both\nderived from an encoder. Then, the feature frames with unimodal weights are\nintegrated and further processed by a decoder. Connectionist temporal\nclassification (CTC) loss is applied for training. Compared to the regular CTC,\nthe proposed method learns better feature representations and shortens the\nsequence length, resulting in lower recognition error and computational\ncomplexity. Experiments on three Mandarin datasets show that UMA demonstrates\nsuperior or comparable performance to other advanced non-autoregressive\nmethods, such as self-conditioned CTC. Moreover, by integrating\nself-conditioned CTC into the proposed framework, the performance can be\nfurther noticeably improved.", "published": "2023-09-15 04:34:40", "link": "http://arxiv.org/abs/2309.08150v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Cross-lingual Knowledge Distillation via Flow-based Voice Conversion for\n  Robust Polyglot Text-To-Speech", "abstract": "In this work, we introduce a framework for cross-lingual speech synthesis,\nwhich involves an upstream Voice Conversion (VC) model and a downstream\nText-To-Speech (TTS) model. The proposed framework consists of 4 stages. In the\nfirst two stages, we use a VC model to convert utterances in the target locale\nto the voice of the target speaker. In the third stage, the converted data is\ncombined with the linguistic features and durations from recordings in the\ntarget language, which are then used to train a single-speaker acoustic model.\nFinally, the last stage entails the training of a locale-independent vocoder.\nOur evaluations show that the proposed paradigm outperforms state-of-the-art\napproaches which are based on training a large multilingual TTS model. In\naddition, our experiments demonstrate the robustness of our approach with\ndifferent model architectures, languages, speakers and amounts of data.\nMoreover, our solution is especially beneficial in low-resource settings.", "published": "2023-09-15 09:03:14", "link": "http://arxiv.org/abs/2309.08255v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DiaCorrect: Error Correction Back-end For Speaker Diarization", "abstract": "In this work, we propose an error correction framework, named DiaCorrect, to\nrefine the output of a diarization system in a simple yet effective way. This\nmethod is inspired by error correction techniques in automatic speech\nrecognition. Our model consists of two parallel convolutional encoders and a\ntransform-based decoder. By exploiting the interactions between the input\nrecording and the initial system's outputs, DiaCorrect can automatically\ncorrect the initial speaker activities to minimize the diarization errors.\nExperiments on 2-speaker telephony data show that the proposed DiaCorrect can\neffectively improve the initial model's results. Our source code is publicly\navailable at https://github.com/BUTSpeechFIT/diacorrect.", "published": "2023-09-15 13:08:12", "link": "http://arxiv.org/abs/2309.08377v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Combining TF-GridNet and Mixture Encoder for Continuous Speech\n  Separation for Meeting Transcription", "abstract": "Many real-life applications of automatic speech recognition (ASR) require\nprocessing of overlapped speech. A common method involves first separating the\nspeech into overlap-free streams on which ASR is performed. Recently,\nTF-GridNet has shown impressive performance in speech separation in real\nreverberant conditions. Furthermore, a mixture encoder was proposed that\nleverages the mixed speech to mitigate the effect of separation artifacts. In\nthis work, we extended the mixture encoder from a static two-speaker scenario\nto a natural meeting context featuring an arbitrary number of speakers and\nvarying degrees of overlap. We further demonstrate its limits by the\nintegration with separators of varying strength including TF-GridNet. Our\nexperiments result in a new state-of-the-art performance on LibriCSS using a\nsingle microphone. They show that TF-GridNet largely closes the gap between\nprevious methods and oracle separation independent of mixture encoding. We\nfurther investigate the remaining potential for improvement.", "published": "2023-09-15 14:57:28", "link": "http://arxiv.org/abs/2309.08454v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Towards Practical and Efficient Image-to-Speech Captioning with\n  Vision-Language Pre-training and Multi-modal Tokens", "abstract": "In this paper, we propose methods to build a powerful and efficient\nImage-to-Speech captioning (Im2Sp) model. To this end, we start with importing\nthe rich knowledge related to image comprehension and language modeling from a\nlarge-scale pre-trained vision-language model into Im2Sp. We set the output of\nthe proposed Im2Sp as discretized speech units, i.e., the quantized speech\nfeatures of a self-supervised speech model. The speech units mainly contain\nlinguistic information while suppressing other characteristics of speech. This\nallows us to incorporate the language modeling capability of the pre-trained\nvision-language model into the spoken language modeling of Im2Sp. With the\nvision-language pre-training strategy, we set new state-of-the-art Im2Sp\nperformances on two widely used benchmark databases, COCO and Flickr8k. Then,\nwe further improve the efficiency of the Im2Sp model. Similar to the speech\nunit case, we convert the original image into image units, which are derived\nthrough vector quantization of the raw image. With these image units, we can\ndrastically reduce the required data storage for saving image data to just 0.8%\nwhen compared to the original image data in terms of bits. Demo page:\nhttps://ms-dot-k.github.io/Image-to-Speech-Captioning.", "published": "2023-09-15 16:48:34", "link": "http://arxiv.org/abs/2309.08531v1", "categories": ["cs.CV", "cs.CL", "eess.AS", "eess.IV"], "primary_category": "cs.CV"}
{"title": "When do Generative Query and Document Expansions Fail? A Comprehensive\n  Study Across Methods, Retrievers, and Datasets", "abstract": "Using large language models (LMs) for query or document expansion can improve\ngeneralization in information retrieval. However, it is unknown whether these\ntechniques are universally beneficial or only effective in specific settings,\nsuch as for particular retrieval models, dataset domains, or query types. To\nanswer this, we conduct the first comprehensive analysis of LM-based expansion.\nWe find that there exists a strong negative correlation between retriever\nperformance and gains from expansion: expansion improves scores for weaker\nmodels, but generally harms stronger models. We show this trend holds across a\nset of eleven expansion techniques, twelve datasets with diverse distribution\nshifts, and twenty-four retrieval models. Through qualitative error analysis,\nwe hypothesize that although expansions provide extra information (potentially\nimproving recall), they add additional noise that makes it difficult to discern\nbetween the top relevant documents (thus introducing false positives). Our\nresults suggest the following recipe: use expansions for weaker models or when\nthe target dataset significantly differs from training corpus in format;\notherwise, avoid expansions to keep the relevance signal clear.", "published": "2023-09-15 17:05:43", "link": "http://arxiv.org/abs/2309.08541v2", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Augmenting conformers with structured state-space sequence models for\n  online speech recognition", "abstract": "Online speech recognition, where the model only accesses context to the left,\nis an important and challenging use case for ASR systems. In this work, we\ninvestigate augmenting neural encoders for online ASR by incorporating\nstructured state-space sequence models (S4), a family of models that provide a\nparameter-efficient way of accessing arbitrarily long left context. We\nperformed systematic ablation studies to compare variants of S4 models and\npropose two novel approaches that combine them with convolutions. We found that\nthe most effective design is to stack a small S4 using real-valued recurrent\nweights with a local convolution, allowing them to work complementarily. Our\nbest model achieves WERs of 4.01%/8.53% on test sets from Librispeech,\noutperforming Conformers with extensively tuned convolution.", "published": "2023-09-15 17:14:17", "link": "http://arxiv.org/abs/2309.08551v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Chain-of-Thought Reasoning is a Policy Improvement Operator", "abstract": "Large language models have astounded the world with fascinating new\ncapabilities. However, they currently lack the ability to teach themselves new\nskills, relying instead on large amounts of human-generated training data. We\nintroduce SECToR (Self-Education via Chain-of-Thought Reasoning), a\nproof-of-concept demonstration that language models can teach themselves new\nskills using chain-of-thought reasoning. During the self-learning loop, SECToR\nasks models to solve addition problems using chain-of-thought reasoning before\ntraining the next version of the model to solve those same problems directly\nwithout using such reasoning. This process often results in an improved model\nwhich can, when again augmented with chain-of-thought reasoning, solve even\nharder problems than the original model, allowing the self-learning loop to\ncontinue. Language models trained via SECToR autonomously learn to add up to\nthe longest-length-digit numbers without access to any ground truth examples\nbeyond an initial supervised fine-tuning phase consisting only of numbers with\n6 or fewer digits. Our central hypothesis is that chain-of-thought reasoning\ncan act as a policy improvement operator, similarly to how Monte-Carlo Tree\nSearch is used in AlphaZero (Silver et al., 2017). We hope that this research\ncan lead to new directions in which language models can learn to teach\nthemselves without the need for human demonstrations.", "published": "2023-09-15 17:44:17", "link": "http://arxiv.org/abs/2309.08589v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "CoCA: Fusing Position Embedding with Collinear Constrained Attention in\n  Transformers for Long Context Window Extending", "abstract": "Self-attention and position embedding are two key modules in\ntransformer-based Large Language Models (LLMs). However, the potential\nrelationship between them is far from well studied, especially for long context\nwindow extending. In fact, anomalous behaviors harming long context\nextrapolation exist between Rotary Position Embedding (RoPE) and vanilla\nself-attention unveiled by our work. To address this issue, we propose a novel\nattention mechanism, CoCA (Collinear Constrained Attention). Specifically, we\nenforce a collinear constraint between $Q$ and $K$ to seamlessly integrate RoPE\nand self-attention. While only adding minimal computational and spatial\ncomplexity, this integration significantly enhances long context window\nextrapolation ability. We provide an optimized implementation, making it a\ndrop-in replacement for any existing transformer-based models. Extensive\nexperiments show that CoCA performs extraordinarily well in extending context\nwindows. A CoCA-based GPT model, trained with a context length of 512, can\nseamlessly extend the context window up to 32K (60$\\times$), without any\nfine-tuning. Additionally, by dropping CoCA in LLaMA-7B, we achieve\nextrapolation up to 32K within only 2K training length. Our code is publicly\navailable at: https://github.com/codefuse-ai/Collinear-Constrained-Attention", "published": "2023-09-15 09:36:51", "link": "http://arxiv.org/abs/2309.08646v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Adversarial Attacks on Tables with Entity Swap", "abstract": "The capabilities of large language models (LLMs) have been successfully\napplied in the context of table representation learning. The recently proposed\ntabular language models have reported state-of-the-art results across various\ntasks for table interpretation. However, a closer look into the datasets\ncommonly used for evaluation reveals an entity leakage from the train set into\nthe test set. Motivated by this observation, we explore adversarial attacks\nthat represent a more realistic inference setup. Adversarial attacks on text\nhave been shown to greatly affect the performance of LLMs, but currently, there\nare no attacks targeting tabular language models. In this paper, we propose an\nevasive entity-swap attack for the column type annotation (CTA) task. Our CTA\nattack is the first black-box attack on tables, where we employ a\nsimilarity-based sampling strategy to generate adversarial examples. The\nexperimental results show that the proposed attack generates up to a 70% drop\nin performance.", "published": "2023-09-15 15:03:33", "link": "http://arxiv.org/abs/2309.08650v1", "categories": ["cs.CL", "cs.AI", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Resolving Legalese: A Multilingual Exploration of Negation Scope\n  Resolution in Legal Documents", "abstract": "Resolving the scope of a negation within a sentence is a challenging NLP\ntask. The complexity of legal texts and the lack of annotated in-domain\nnegation corpora pose challenges for state-of-the-art (SotA) models when\nperforming negation scope resolution on multilingual legal data. Our\nexperiments demonstrate that models pre-trained without legal data underperform\nin the task of negation scope resolution. Our experiments, using language\nmodels exclusively fine-tuned on domains like literary texts and medical data,\nyield inferior results compared to the outcomes documented in prior\ncross-domain experiments. We release a new set of annotated court decisions in\nGerman, French, and Italian and use it to improve negation scope resolution in\nboth zero-shot and multilingual settings. We achieve token-level F1-scores of\nup to 86.7% in our zero-shot cross-lingual experiments, where the models are\ntrained on two languages of our legal datasets and evaluated on the third. Our\nmultilingual experiments, where the models were trained on all available\nnegation data and evaluated on our legal datasets, resulted in F1-scores of up\nto 91.1%.", "published": "2023-09-15 18:38:06", "link": "http://arxiv.org/abs/2309.08695v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2"], "primary_category": "cs.CL"}
{"title": "AlbNER: A Corpus for Named Entity Recognition in Albanian", "abstract": "Scarcity of resources such as annotated text corpora for under-resourced\nlanguages like Albanian is a serious impediment in computational linguistics\nand natural language processing research. This paper presents AlbNER, a corpus\nof 900 sentences with labeled named entities, collected from Albanian Wikipedia\narticles. Preliminary results with BERT and RoBERTa variants fine-tuned and\ntested with AlbNER data indicate that model size has slight impact on NER\nperformance, whereas language transfer has a significant one. AlbNER corpus and\nthese obtained results should serve as baselines for future experiments.", "published": "2023-09-15 20:03:19", "link": "http://arxiv.org/abs/2309.08741v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "InvestLM: A Large Language Model for Investment using Financial Domain\n  Instruction Tuning", "abstract": "We present a new financial domain large language model, InvestLM, tuned on\nLLaMA-65B (Touvron et al., 2023), using a carefully curated instruction dataset\nrelated to financial investment. Inspired by less-is-more-for-alignment (Zhou\net al., 2023), we manually curate a small yet diverse instruction dataset,\ncovering a wide range of financial related topics, from Chartered Financial\nAnalyst (CFA) exam questions to SEC filings to Stackexchange quantitative\nfinance discussions. InvestLM shows strong capabilities in understanding\nfinancial text and provides helpful responses to investment related questions.\nFinancial experts, including hedge fund managers and research analysts, rate\nInvestLM's response as comparable to those of state-of-the-art commercial\nmodels (GPT-3.5, GPT-4 and Claude-2). Zero-shot evaluation on a set of\nfinancial NLP benchmarks demonstrates strong generalizability. From a research\nperspective, this work suggests that a high-quality domain specific LLM can be\ntuned using a small set of carefully curated instructions on a well-trained\nfoundation model, which is consistent with the Superficial Alignment Hypothesis\n(Zhou et al., 2023). From a practical perspective, this work develops a\nstate-of-the-art financial domain LLM with superior capability in understanding\nfinancial texts and providing helpful investment advice, potentially enhancing\nthe work efficiency of financial professionals. We release the model parameters\nto the research community.", "published": "2023-09-15 02:59:31", "link": "http://arxiv.org/abs/2309.13064v1", "categories": ["q-fin.GN", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "q-fin.GN"}
{"title": "Detecting Relevant Information in High-Volume Chat Logs: Keyphrase\n  Extraction for Grooming and Drug Dealing Forensic Analysis", "abstract": "The growing use of digital communication platforms has given rise to various\ncriminal activities, such as grooming and drug dealing, which pose significant\nchallenges to law enforcement and forensic experts. This paper presents a\nsupervised keyphrase extraction approach to detect relevant information in\nhigh-volume chat logs involving grooming and drug dealing for forensic\nanalysis. The proposed method, JointKPE++, builds upon the JointKPE keyphrase\nextractor by employing improvements to handle longer texts effectively. We\nevaluate JointKPE++ using BERT-based pre-trained models on grooming and drug\ndealing datasets, including BERT, RoBERTa, SpanBERT, and BERTimbau. The results\nshow significant improvements over traditional approaches and demonstrate the\npotential for JointKPE++ to aid forensic experts in efficiently detecting\nkeyphrases related to criminal activities.", "published": "2023-09-15 03:18:31", "link": "http://arxiv.org/abs/2311.04905v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Audio Difference Learning for Audio Captioning", "abstract": "This study introduces a novel training paradigm, audio difference learning,\nfor improving audio captioning. The fundamental concept of the proposed\nlearning method is to create a feature representation space that preserves the\nrelationship between audio, enabling the generation of captions that detail\nintricate audio information. This method employs a reference audio along with\nthe input audio, both of which are transformed into feature representations via\na shared encoder. Captions are then generated from these differential features\nto describe their differences. Furthermore, a unique technique is proposed that\ninvolves mixing the input audio with additional audio, and using the additional\naudio as a reference. This results in the difference between the mixed audio\nand the reference audio reverting back to the original input audio. This allows\nthe original input's caption to be used as the caption for their difference,\neliminating the need for additional annotations for the differences. In the\nexperiments using the Clotho and ESC50 datasets, the proposed method\ndemonstrated an improvement in the SPIDEr score by 7% compared to conventional\nmethods.", "published": "2023-09-15 04:11:37", "link": "http://arxiv.org/abs/2309.08141v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "MusiLingo: Bridging Music and Text with Pre-trained Language Models for\n  Music Captioning and Query Response", "abstract": "Large Language Models (LLMs) have shown immense potential in multimodal\napplications, yet the convergence of textual and musical domains remains not\nwell-explored. To address this gap, we present MusiLingo, a novel system for\nmusic caption generation and music-related query responses. MusiLingo employs a\nsingle projection layer to align music representations from the pre-trained\nfrozen music audio model MERT with a frozen LLM, bridging the gap between music\naudio and textual contexts. We train it on an extensive music caption dataset\nand fine-tune it with instructional data. Due to the scarcity of high-quality\nmusic Q&A datasets, we created the MusicInstruct (MI) dataset from captions in\nthe MusicCaps datasets, tailored for open-ended music inquiries. Empirical\nevaluations demonstrate its competitive performance in generating music\ncaptions and composing music-related Q&A pairs. Our introduced dataset enables\nnotable advancements beyond previous ones.", "published": "2023-09-15 19:31:40", "link": "http://arxiv.org/abs/2309.08730v3", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SSL-Net: A Synergistic Spectral and Learning-based Network for Efficient\n  Bird Sound Classification", "abstract": "Efficient and accurate bird sound classification is of important for ecology,\nhabitat protection and scientific research, as it plays a central role in\nmonitoring the distribution and abundance of species. However, prevailing\nmethods typically demand extensively labeled audio datasets and have highly\ncustomized frameworks, imposing substantial computational and annotation loads.\nIn this study, we present an efficient and general framework called SSL-Net,\nwhich combines spectral and learned features to identify different bird sounds.\nEncouraging empirical results gleaned from a standard field-collected bird\naudio dataset validate the efficacy of our method in extracting features\nefficiently and achieving heightened performance in bird sound classification,\neven when working with limited sample sizes. Furthermore, we present three\nfeature fusion strategies, aiding engineers and researchers in their selection\nthrough quantitative analysis.", "published": "2023-09-15 00:02:44", "link": "http://arxiv.org/abs/2309.08072v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Libriheavy: a 50,000 hours ASR corpus with punctuation casing and\n  context", "abstract": "In this paper, we introduce Libriheavy, a large-scale ASR corpus consisting\nof 50,000 hours of read English speech derived from LibriVox. To the best of\nour knowledge, Libriheavy is the largest freely-available corpus of speech with\nsupervisions. Different from other open-sourced datasets that only provide\nnormalized transcriptions, Libriheavy contains richer information such as\npunctuation, casing and text context, which brings more flexibility for system\nbuilding. Specifically, we propose a general and efficient pipeline to locate,\nalign and segment the audios in previously published Librilight to its\ncorresponding texts. The same as Librilight, Libriheavy also has three training\nsubsets small, medium, large of the sizes 500h, 5000h, 50000h respectively. We\nalso extract the dev and test evaluation sets from the aligned audios and\nguarantee there is no overlapping speakers and books in training sets. Baseline\nsystems are built on the popular CTC-Attention and transducer models.\nAdditionally, we open-source our dataset creatation pipeline which can also be\nused to other audio alignment tasks.", "published": "2023-09-15 01:59:21", "link": "http://arxiv.org/abs/2309.08105v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Foundation Model Assisted Automatic Speech Emotion Recognition:\n  Transcribing, Annotating, and Augmenting", "abstract": "Significant advances are being made in speech emotion recognition (SER) using\ndeep learning models. Nonetheless, training SER systems remains challenging,\nrequiring both time and costly resources. Like many other machine learning\ntasks, acquiring datasets for SER requires substantial data annotation efforts,\nincluding transcription and labeling. These annotation processes present\nchallenges when attempting to scale up conventional SER systems. Recent\ndevelopments in foundational models have had a tremendous impact, giving rise\nto applications such as ChatGPT. These models have enhanced human-computer\ninteractions including bringing unique possibilities for streamlining data\ncollection in fields like SER. In this research, we explore the use of\nfoundational models to assist in automating SER from transcription and\nannotation to augmentation. Our study demonstrates that these models can\ngenerate transcriptions to enhance the performance of SER systems that rely\nsolely on speech data. Furthermore, we note that annotating emotions from\ntranscribed speech remains a challenging task. However, combining outputs from\nmultiple LLMs enhances the quality of annotations. Lastly, our findings suggest\nthe feasibility of augmenting existing speech emotion datasets by annotating\nunlabeled speech samples.", "published": "2023-09-15 02:19:03", "link": "http://arxiv.org/abs/2309.08108v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Diversity-based core-set selection for text-to-speech with linguistic\n  and acoustic features", "abstract": "This paper proposes a method for extracting a lightweight subset from a\ntext-to-speech (TTS) corpus ensuring synthetic speech quality. In recent years,\nmethods have been proposed for constructing large-scale TTS corpora by\ncollecting diverse data from massive sources such as audiobooks and YouTube.\nAlthough these methods have gained significant attention for enhancing the\nexpressive capabilities of TTS systems, they often prioritize collecting vast\namounts of data without considering practical constraints like storage capacity\nand computation time in training, which limits the available data quantity.\nConsequently, the need arises to efficiently collect data within these volume\nconstraints. To address this, we propose a method for selecting the core\nsubset~(known as \\textit{core-set}) from a TTS corpus on the basis of a\n\\textit{diversity metric}, which measures the degree to which a subset\nencompasses a wide range. Experimental results demonstrate that our proposed\nmethod performs significantly better than the baseline phoneme-balanced data\nselection across language and corpus size.", "published": "2023-09-15 03:36:08", "link": "http://arxiv.org/abs/2309.08127v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "t-SOT FNT: Streaming Multi-talker ASR with Text-only Domain Adaptation\n  Capability", "abstract": "Token-level serialized output training (t-SOT) was recently proposed to\naddress the challenge of streaming multi-talker automatic speech recognition\n(ASR). T-SOT effectively handles overlapped speech by representing multi-talker\ntranscriptions as a single token stream with $\\langle \\text{cc}\\rangle$ symbols\ninterspersed. However, the use of a naive neural transducer architecture\nsignificantly constrained its applicability for text-only adaptation. To\novercome this limitation, we propose a novel t-SOT model structure that\nincorporates the idea of factorized neural transducers (FNT). The proposed\nmethod separates a language model (LM) from the transducer's predictor and\nhandles the unnatural token order resulting from the use of $\\langle\n\\text{cc}\\rangle$ symbols in t-SOT. We achieve this by maintaining multiple\nhidden states and introducing special handling of the $\\langle\n\\text{cc}\\rangle$ tokens within the LM. The proposed t-SOT FNT model achieves\ncomparable performance to the original t-SOT model while retaining the ability\nto reduce word error rate (WER) on both single and multi-talker datasets\nthrough text-only adaptation.", "published": "2023-09-15 03:55:14", "link": "http://arxiv.org/abs/2309.08131v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Fine-tune the pretrained ATST model for sound event detection", "abstract": "Sound event detection (SED) often suffers from the data deficiency problem.\nThe recent baseline system in the DCASE2023 challenge task 4 leverages the\nlarge pretrained self-supervised learning (SelfSL) models to mitigate such\nrestriction, where the pretrained models help to produce more discriminative\nfeatures for SED. However, the pretrained models are regarded as a frozen\nfeature extractor in the challenge baseline system and most of the challenge\nsubmissions, and fine-tuning of the pretrained models has been rarely studied.\nIn this work, we study the fine-tuning method of the pretrained models for SED.\nWe first introduce ATST-Frame, our newly proposed SelfSL model, to the SED\nsystem. ATST-Frame was especially designed for learning frame-level\nrepresentations of audio signals and obtained state-of-the-art (SOTA)\nperformances on a series of downstream tasks. We then propose a fine-tuning\nmethod for ATST-Frame using both (in-domain) unlabelled and labelled SED data.\nOur experiments show that, the proposed method overcomes the overfitting\nproblem when fine-tuning the large pretrained network, and our SED system\nobtains new SOTA results of 0.587/0.812 PSDS1/PSDS2 scores on the DCASE\nchallenge task 4 dataset.", "published": "2023-09-15 04:38:08", "link": "http://arxiv.org/abs/2309.08153v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "RVAE-EM: Generative speech dereverberation based on recurrent\n  variational auto-encoder and convolutive transfer function", "abstract": "In indoor scenes, reverberation is a crucial factor in degrading the\nperceived quality and intelligibility of speech. In this work, we propose a\ngenerative dereverberation method. Our approach is based on a probabilistic\nmodel utilizing a recurrent variational auto-encoder (RVAE) network and the\nconvolutive transfer function (CTF) approximation. Different from most previous\napproaches, the output of our RVAE serves as the prior of the clean speech. And\nour target is the maximum a posteriori (MAP) estimation of clean speech, which\nis achieved iteratively through the expectation maximization (EM) algorithm.\nThe proposed method integrates the capabilities of network-based speech prior\nmodelling and CTF-based observation modelling. Experiments on single-channel\nspeech dereverberation show that the proposed generative method noticeably\noutperforms the advanced discriminative networks.", "published": "2023-09-15 04:48:10", "link": "http://arxiv.org/abs/2309.08157v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Residual Speaker Representation for One-Shot Voice Conversion", "abstract": "Recently, there have been significant advancements in voice conversion,\nresulting in high-quality performance. However, there are still two critical\nchallenges in this field. Firstly, current voice conversion methods have\nlimited robustness when encountering unseen speakers. Secondly, they also have\nlimited ability to control timbre representation. To address these challenges,\nthis paper presents a novel approach that leverages tokens of multi-layer\nresidual approximations to enhance robustness when dealing with unseen\nspeakers, called the residual speaker module. Introducing multi-layer\napproximations facilitates the separation of information from the timbre,\nenabling effective control over timbre in voice conversion. The proposed method\noutperforms baselines in subjective and objective evaluations, demonstrating\nsuperior performance and increased robustness. Our demo page is publicly\navailable.", "published": "2023-09-15 05:27:21", "link": "http://arxiv.org/abs/2309.08166v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "TF-SepNet: An Efficient 1D Kernel Design in CNNs for Low-Complexity\n  Acoustic Scene Classification", "abstract": "Recent studies focus on developing efficient systems for acoustic scene\nclassification (ASC) using convolutional neural networks (CNNs), which\ntypically consist of consecutive kernels. This paper highlights the benefits of\nusing separate kernels as a more powerful and efficient design approach in ASC\ntasks. Inspired by the time-frequency nature of audio signals, we propose\nTF-SepNet, a CNN architecture that separates the feature processing along the\ntime and frequency dimensions. Features resulted from the separate paths are\nthen merged by channels and directly forwarded to the classifier. Instead of\nthe conventional two dimensional (2D) kernel, TF-SepNet incorporates one\ndimensional (1D) kernels to reduce the computational costs. Experiments have\nbeen conducted using the TAU Urban Acoustic Scene 2022 Mobile development\ndataset. The results show that TF-SepNet outperforms similar state-of-the-arts\nthat use consecutive kernels. A further investigation reveals that the separate\nkernels lead to a larger effective receptive field (ERF), which enables\nTF-SepNet to capture more time-frequency features.", "published": "2023-09-15 07:05:29", "link": "http://arxiv.org/abs/2309.08200v4", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improving Voice Conversion for Dissimilar Speakers Using Perceptual\n  Losses", "abstract": "The rising trend of using voice as a means of interacting with smart devices\nhas sparked worries over the protection of users' privacy and data security.\nThese concerns have become more pressing, especially after the European Union's\nadoption of the General Data Protection Regulation (GDPR). The information\ncontained in an utterance encompasses critical personal details about the\nspeaker, such as their age, gender, socio-cultural origins and more. If there\nis a security breach and the data is compromised, attackers may utilise the\nspeech data to circumvent the speaker verification systems or imitate\nauthorised users. Therefore, it is pertinent to anonymise the speech data\nbefore being shared across devices, such that the source speaker of the\nutterance cannot be traced. Voice conversion (VC) can be used to achieve speech\nanonymisation, which involves altering the speaker's characteristics while\npreserving the linguistic content.", "published": "2023-09-15 09:18:38", "link": "http://arxiv.org/abs/2309.08263v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improving Short Utterance Anti-Spoofing with AASIST2", "abstract": "The wav2vec 2.0 and integrated spectro-temporal graph attention network\n(AASIST) based countermeasure achieves great performance in speech\nanti-spoofing. However, current spoof speech detection systems have fixed\ntraining and evaluation durations, while the performance degrades significantly\nduring short utterance evaluation. To solve this problem, AASIST can be\nimproved to AASIST2 by modifying the residual blocks to Res2Net blocks. The\nmodified Res2Net blocks can extract multi-scale features and improve the\ndetection performance for speech of different durations, thus improving the\nshort utterance evaluation performance. On the other hand, adaptive large\nmargin fine-tuning (ALMFT) has achieved performance improvement in short\nutterance speaker verification. Therefore, we apply Dynamic Chunk Size (DCS)\nand ALMFT training strategies in speech anti-spoofing to further improve the\nperformance of short utterance evaluation. Experiments demonstrate that the\nproposed AASIST2 improves the performance of short utterance evaluation while\nmaintaining the performance of regular evaluation on different datasets.", "published": "2023-09-15 09:45:00", "link": "http://arxiv.org/abs/2309.08279v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "One-Class Knowledge Distillation for Spoofing Speech Detection", "abstract": "The detection of spoofing speech generated by unseen algorithms remains an\nunresolved challenge. One reason for the lack of generalization ability is\ntraditional detecting systems follow the binary classification paradigm, which\ninherently assumes the possession of prior knowledge of spoofing speech.\nOne-class methods attempt to learn the distribution of bonafide speech and are\ninherently suited to the task where spoofing speech exhibits significant\ndifferences. However, training a one-class system using only bonafide speech is\nchallenging. In this paper, we introduce a teacher-student framework to provide\nguidance for the training of a one-class model. The proposed one-class\nknowledge distillation method outperforms other state-of-the-art methods on the\nASVspoof 21DF dataset and InTheWild dataset, which demonstrates its superior\ngeneralization ability.", "published": "2023-09-15 09:59:06", "link": "http://arxiv.org/abs/2309.08285v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Head-Related Transfer Function Interpolation with a Spherical CNN", "abstract": "Head-related transfer functions (HRTFs) are crucial for spatial soundfield\nreproduction in virtual reality applications. However, obtaining personalized,\nhigh-resolution HRTFs is a time-consuming and costly task. Recently, deep\nlearning-based methods showed promise in interpolating high-resolution HRTFs\nfrom sparse measurements. Some of these methods treat HRTF interpolation as an\nimage super-resolution task, which neglects spatial acoustic features. This\npaper proposes a spherical convolutional neural network method for HRTF\ninterpolation. The proposed method realizes the convolution process by\ndecomposing and reconstructing HRTF through the Spherical Harmonics (SHs). The\nSHs, an orthogonal function set defined on a sphere, allow the convolution\nlayers to effectively capture the spatial features of HRTFs, which are sampled\non a sphere. Simulation results demonstrate the effectiveness of the proposed\nmethod in achieving accurate interpolation from sparse measurements,\noutperforming the SH method and learning-based methods.", "published": "2023-09-15 10:11:37", "link": "http://arxiv.org/abs/2309.08290v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speech-dependent Modeling of Own Voice Transfer Characteristics for\n  In-ear Microphones in Hearables", "abstract": "Many hearables contain an in-ear microphone, which may be used to capture the\nown voice of its user in noisy environments. Since the in-ear microphone mostly\nrecords body-conducted speech due to ear canal occlusion, it suffers from\nband-limitation effects while only capturing a limited amount of external\nnoise. To enhance the quality of the in-ear microphone signal using algorithms\naiming at joint bandwidth extension, equalization, and noise reduction, it is\ndesirable to have an accurate model of the own voice transfer characteristics\nbetween the entrance of the ear canal and the in-ear microphone. Such a model\ncan be used, e.g., to simulate a large amount of in-ear recordings to train\nsupervised learning-based algorithms. Since previous research on ear canal\nocclusion suggests that own voice transfer characteristics depend on speech\ncontent, in this contribution we propose a speech-dependent system\nidentification model based on phoneme recognition. We assess the accuracy of\nsimulating own voice speech by speech-dependent and speech-independent modeling\nand investigate how well modeling approaches are able to generalize to\ndifferent talkers. Simulation results show that using the proposed\nspeech-dependent model is preferable for simulating in-ear recordings compared\nto using a speech-independent model.", "published": "2023-09-15 10:19:06", "link": "http://arxiv.org/abs/2309.08294v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The Multimodal Information Based Speech Processing (MISP) 2023\n  Challenge: Audio-Visual Target Speaker Extraction", "abstract": "Previous Multimodal Information based Speech Processing (MISP) challenges\nmainly focused on audio-visual speech recognition (AVSR) with commendable\nsuccess. However, the most advanced back-end recognition systems often hit\nperformance limits due to the complex acoustic environments. This has prompted\na shift in focus towards the Audio-Visual Target Speaker Extraction (AVTSE)\ntask for the MISP 2023 challenge in ICASSP 2024 Signal Processing Grand\nChallenges. Unlike existing audio-visual speech enhance-ment challenges\nprimarily focused on simulation data, the MISP 2023 challenge uniquely explores\nhow front-end speech processing, combined with visual clues, impacts back-end\ntasks in real-world scenarios. This pioneering effort aims to set the first\nbenchmark for the AVTSE task, offering fresh insights into enhancing the\nac-curacy of back-end speech recognition systems through AVTSE in challenging\nand real acoustic environments. This paper delivers a thorough overview of the\ntask setting, dataset, and baseline system of the MISP 2023 challenge. It also\nincludes an in-depth analysis of the challenges participants may encounter. The\nexperimental results highlight the demanding nature of this task, and we look\nforward to the innovative solutions participants will bring forward.", "published": "2023-09-15 12:15:45", "link": "http://arxiv.org/abs/2309.08348v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Semi-supervised Sound Event Detection with Local and Global Consistency\n  Regularization", "abstract": "Learning meaningful frame-wise features on a partially labeled dataset is\ncrucial to semi-supervised sound event detection. Prior works either maintain\nconsistency on frame-level predictions or seek feature-level similarity among\nneighboring frames, which cannot exploit the potential of unlabeled data. In\nthis work, we design a Local and Global Consistency (LGC) regularization scheme\nto enhance the model on both label- and feature-level. The audio CutMix is\nintroduced to change the contextual information of clips. Then, the local\nconsistency is adopted to encourage the model to leverage local features for\nframe-level predictions, and the global consistency is applied to force\nfeatures to align with global prototypes through a specially designed\ncontrastive loss. Experiments on the DESED dataset indicate the superiority of\nLGC, surpassing its respective competitors largely with the same settings as\nthe baseline system. Besides, combining LGC with existing methods can obtain\nfurther improvements. The code will be released soon.", "published": "2023-09-15 12:29:48", "link": "http://arxiv.org/abs/2309.08355v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Audio-free Prompt Tuning for Language-Audio Models", "abstract": "Contrastive Language-Audio Pretraining (CLAP) is pre-trained to associate\naudio features with human language, making it a natural zero-shot classifier to\nrecognize unseen sound categories. To adapt CLAP to downstream tasks, prior\nworks inevitably require labeled domain audios, which limits their scalability\nunder data scarcity and deprives them of the capability to detect novel classes\nas the original CLAP. In this work, by leveraging the modality alignment in\nCLAP, we propose an efficient audio-free prompt tuning scheme aimed at\noptimizing a few prompt tokens from texts instead of audios, which regularizes\nthe model space to avoid overfitting the seen classes as well. Based on this, a\nmulti-grained prompt design is further explored to fuse global and local\ninformation. Experiments on several tasks demonstrate that our approach can\nboost the CLAP and outperform other training methods on model performance and\ntraining efficiency. While conducting zero-shot inference on unseen categories,\nit still shows better transferability than the vanilla CLAP. Moreover, our\nmethod is flexible enough even if only knowing the downstream class names. The\ncode will be released soon.", "published": "2023-09-15 12:31:36", "link": "http://arxiv.org/abs/2309.08357v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Audio-Visual Active Speaker Extraction for Sparsely Overlapped\n  Multi-talker Speech", "abstract": "Target speaker extraction aims to extract the speech of a specific speaker\nfrom a multi-talker mixture as specified by an auxiliary reference. Most\nstudies focus on the scenario where the target speech is highly overlapped with\nthe interfering speech. However, this scenario only accounts for a small\npercentage of real-world conversations. In this paper, we aim at the sparsely\noverlapped scenarios in which the auxiliary reference needs to perform two\ntasks simultaneously: detect the activity of the target speaker and disentangle\nthe active speech from any interfering speech. We propose an audio-visual\nspeaker extraction model named ActiveExtract, which leverages speaking activity\nfrom audio-visual active speaker detection (ASD). The ASD directly provides the\nframe-level activity of the target speaker, while its intermediate feature\nrepresentation is trained to discriminate speech-lip synchronization that could\nbe used for speaker disentanglement. Experimental results show our model\noutperforms baselines across various overlapping ratios, achieving an average\nimprovement of more than 4 dB in terms of SI-SNR.", "published": "2023-09-15 14:10:46", "link": "http://arxiv.org/abs/2309.08408v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Music Source Separation Based on a Lightweight Deep Learning Framework\n  (DTTNET: DUAL-PATH TFC-TDF UNET)", "abstract": "Music source separation (MSS) aims to extract 'vocals', 'drums', 'bass' and\n'other' tracks from a piece of mixed music. While deep learning methods have\nshown impressive results, there is a trend toward larger models. In our paper,\nwe introduce a novel and lightweight architecture called DTTNet, which is based\non Dual-Path Module and Time-Frequency Convolutions Time-Distributed\nFully-connected UNet (TFC-TDF UNet). DTTNet achieves 10.12 dB cSDR on 'vocals'\ncompared to 10.01 dB reported for Bandsplit RNN (BSRNN) but with 86.7% fewer\nparameters. We also assess pattern-specific performance and model\ngeneralization for intricate audio patterns.", "published": "2023-09-15 18:20:55", "link": "http://arxiv.org/abs/2309.08684v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Stack-and-Delay: a new codebook pattern for music generation", "abstract": "In language modeling based music generation, a generated waveform is\nrepresented by a sequence of hierarchical token stacks that can be decoded\neither in an auto-regressive manner or in parallel, depending on the codebook\npatterns. In particular, flattening the codebooks represents the highest\nquality decoding strategy, while being notoriously slow. To this end, we\npropose a novel stack-and-delay style of decoding strategy to improve upon the\nflat pattern decoding where generation speed is four times faster as opposed to\nvanilla flat decoding. This brings the inference time close to that of the\ndelay decoding strategy, and allows for faster inference on GPU for small batch\nsizes. For the same inference efficiency budget as the delay pattern, we show\nthat the proposed approach performs better in objective evaluations, almost\nclosing the gap with the flat pattern in terms of quality. The results are\ncorroborated by subjective evaluations which show that samples generated by the\nnew model are slightly more often preferred to samples generated by the\ncompeting model given the same text prompts.", "published": "2023-09-15 22:57:25", "link": "http://arxiv.org/abs/2309.08804v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Fewer-token Neural Speech Codec with Time-invariant Codes", "abstract": "Language model based text-to-speech (TTS) models, like VALL-E, have gained\nattention for their outstanding in-context learning capability in zero-shot\nscenarios. Neural speech codec is a critical component of these models, which\ncan convert speech into discrete token representations. However, excessive\ntoken sequences from the codec may negatively affect prediction accuracy and\nrestrict the progression of Language model based TTS models. To address this\nissue, this paper proposes a novel neural speech codec with time-invariant\ncodes named TiCodec. By encoding and quantizing time-invariant information into\na separate code, TiCodec can reduce the amount of frame-level information that\nneeds encoding, effectively decreasing the number of tokens as codes of speech.\nFurthermore, this paper introduces a time-invariant encoding consistency loss\nto enhance the consistency of time-invariant code within an utterance and force\nit to capture more global information, which can benefit the zero-shot TTS\ntask. Experimental results demonstrate that TiCodec can not only enhance the\nquality of reconstruction speech with fewer tokens but also increase the\nsimilarity and naturalness, as well as reduce the word error rate of the\nsynthesized speech by the TTS model.", "published": "2023-09-15 04:32:26", "link": "http://arxiv.org/abs/2310.00014v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "hear-your-action: human action recognition by ultrasound active sensing", "abstract": "Action recognition is a key technology for many industrial applications.\nMethods using visual information such as images are very popular. However,\nprivacy issues prevent widespread usage due to the inclusion of private\ninformation, such as visible faces and scene backgrounds, which are not\nnecessary for recognizing user action. In this paper, we propose a\nprivacy-preserving action recognition by ultrasound active sensing. As action\nrecognition from ultrasound active sensing in a non-invasive manner is not well\ninvestigated, we create a new dataset for action recognition and conduct a\ncomparison of features for classification. We calculated feature values by\nfocusing on the temporal variation of the amplitude of ultrasound reflected\nwaves and performed classification using a support vector machine and VGG for\neight fundamental action classes. We confirmed that our method achieved an\naccuracy of 97.9% when trained and evaluated on the same person and in the same\nenvironment. Additionally, our method achieved an accuracy of 89.5% even when\ntrained and evaluated on different people. We also report the analyses of\naccuracies in various conditions and limitations.", "published": "2023-09-15 01:00:55", "link": "http://arxiv.org/abs/2309.08087v1", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Two-Step Knowledge Distillation for Tiny Speech Enhancement", "abstract": "Tiny, causal models are crucial for embedded audio machine learning\napplications. Model compression can be achieved via distilling knowledge from a\nlarge teacher into a smaller student model. In this work, we propose a novel\ntwo-step approach for tiny speech enhancement model distillation. In contrast\nto the standard approach of a weighted mixture of distillation and supervised\nlosses, we firstly pre-train the student using only the knowledge distillation\n(KD) objective, after which we switch to a fully supervised training regime. We\nalso propose a novel fine-grained similarity-preserving KD loss, which aims to\nmatch the student's intra-activation Gram matrices to that of the teacher. Our\nmethod demonstrates broad improvements, but particularly shines in adverse\nconditions including high compression and low signal to noise ratios (SNR),\nyielding signal to distortion ratio gains of 0.9 dB and 1.1 dB, respectively,\nat -5 dB input SNR and 63x compression compared to baseline.", "published": "2023-09-15 04:19:38", "link": "http://arxiv.org/abs/2309.08144v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Syn-Att: Synthetic Speech Attribution via Semi-Supervised Unknown\n  Multi-Class Ensemble of CNNs", "abstract": "With the huge technological advances introduced by deep learning in audio &\nspeech processing, many novel synthetic speech techniques achieved incredible\nrealistic results. As these methods generate realistic fake human voices, they\ncan be used in malicious acts such as people imitation, fake news, spreading,\nspoofing, media manipulations, etc. Hence, the ability to detect synthetic or\nnatural speech has become an urgent necessity. Moreover, being able to tell\nwhich algorithm has been used to generate a synthetic speech track can be of\npreeminent importance to track down the culprit. In this paper, a novel\nstrategy is proposed to attribute a synthetic speech track to the generator\nthat is used to synthesize it. The proposed detector transforms the audio into\nlog-mel spectrogram, extracts features using CNN, and classifies it between\nfive known and unknown algorithms, utilizing semi-supervision and ensemble to\nimprove its robustness and generalizability significantly. The proposed\ndetector is validated on two evaluation datasets consisting of a total of\n18,000 weakly perturbed (Eval 1) & 10,000 strongly perturbed (Eval 2) synthetic\nspeeches. The proposed method outperforms other top teams in accuracy by 12-13%\non Eval 2 and 1-2% on Eval 1, in the IEEE SP Cup challenge at ICASSP 2022.", "published": "2023-09-15 04:26:39", "link": "http://arxiv.org/abs/2309.08146v1", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "HM-Conformer: A Conformer-based audio deepfake detection system with\n  hierarchical pooling and multi-level classification token aggregation methods", "abstract": "Audio deepfake detection (ADD) is the task of detecting spoofing attacks\ngenerated by text-to-speech or voice conversion systems. Spoofing evidence,\nwhich helps to distinguish between spoofed and bona-fide utterances, might\nexist either locally or globally in the input features. To capture these, the\nConformer, which consists of Transformers and CNN, possesses a suitable\nstructure. However, since the Conformer was designed for sequence-to-sequence\ntasks, its direct application to ADD tasks may be sub-optimal. To tackle this\nlimitation, we propose HM-Conformer by adopting two components: (1)\nHierarchical pooling method progressively reducing the sequence length to\neliminate duplicated information (2) Multi-level classification token\naggregation method utilizing classification tokens to gather information from\ndifferent blocks. Owing to these components, HM-Conformer can efficiently\ndetect spoofing evidence by processing various sequence lengths and aggregating\nthem. In experimental results on the ASVspoof 2021 Deepfake dataset,\nHM-Conformer achieved a 15.71% EER, showing competitive performance compared to\nrecent systems.", "published": "2023-09-15 07:18:30", "link": "http://arxiv.org/abs/2309.08208v1", "categories": ["cs.SD", "cs.CR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Real-Time Active Speaker Detection System Integrating an Audio-Visual\n  Signal with a Spatial Querying Mechanism", "abstract": "We introduce a distinctive real-time, causal, neural network-based active\nspeaker detection system optimized for low-power edge computing. This system\ndrives a virtual cinematography module and is deployed on a commercial device.\nThe system uses data originating from a microphone array and a 360-degree\ncamera. Our network requires only 127 MFLOPs per participant, for a meeting\nwith 14 participants. Unlike previous work, we examine the error rate of our\nnetwork when the computational budget is exhausted, and find that it exhibits\ngraceful degradation, allowing the system to operate reasonably well even in\nthis case. Departing from conventional DOA estimation approaches, our network\nlearns to query the available acoustic data, considering the detected head\nlocations. We train and evaluate our algorithm on a realistic meetings dataset\nfeaturing up to 14 participants in the same meeting, overlapped speech, and\nother challenging scenarios.", "published": "2023-09-15 10:20:16", "link": "http://arxiv.org/abs/2309.08295v1", "categories": ["eess.AS", "cs.CV", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Exploring Meta Information for Audio-based Zero-shot Bird Classification", "abstract": "Advances in passive acoustic monitoring and machine learning have led to the\nprocurement of vast datasets for computational bioacoustic research.\nNevertheless, data scarcity is still an issue for rare and underrepresented\nspecies. This study investigates how meta-information can improve zero-shot\naudio classification, utilising bird species as an example case study due to\nthe availability of rich and diverse meta-data. We investigate three different\nsources of metadata: textual bird sound descriptions encoded via (S)BERT,\nfunctional traits (AVONET), and bird life-history (BLH) characteristics. As\naudio features, we extract audio spectrogram transformer (AST) embeddings and\nproject them to the dimension of the auxiliary information by adopting a single\nlinear layer. Then, we employ the dot product as compatibility function and a\nstandard zero-shot learning ranking hinge loss to determine the correct class.\nThe best results are achieved by concatenating the AVONET and BLH features\nattaining a mean unweighted F1-score of .233 over five different test sets with\n8 to 10 classes.", "published": "2023-09-15 13:50:16", "link": "http://arxiv.org/abs/2309.08398v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Chunked Attention-based Encoder-Decoder Model for Streaming Speech\n  Recognition", "abstract": "We study a streamable attention-based encoder-decoder model in which either\nthe decoder, or both the encoder and decoder, operate on pre-defined,\nfixed-size windows called chunks. A special end-of-chunk (EOC) symbol advances\nfrom one chunk to the next chunk, effectively replacing the conventional\nend-of-sequence symbol. This modification, while minor, situates our model as\nequivalent to a transducer model that operates on chunks instead of frames,\nwhere EOC corresponds to the blank symbol. We further explore the remaining\ndifferences between a standard transducer and our model. Additionally, we\nexamine relevant aspects such as long-form speech generalization, beam size,\nand length normalization. Through experiments on Librispeech and TED-LIUM-v2,\nand by concatenating consecutive sequences for long-form trials, we find that\nour streamable model maintains competitive performance compared to the\nnon-streamable variant and generalizes very well to long-form speech.", "published": "2023-09-15 14:36:24", "link": "http://arxiv.org/abs/2309.08436v2", "categories": ["eess.AS", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Towards Word-Level End-to-End Neural Speaker Diarization with Auxiliary\n  Network", "abstract": "While standard speaker diarization attempts to answer the question \"who\nspoken when\", most of relevant applications in reality are more interested in\ndetermining \"who spoken what\". Whether it is the conventional modularized\napproach or the more recent end-to-end neural diarization (EEND), an additional\nautomatic speech recognition (ASR) model and an orchestration algorithm are\nrequired to associate the speaker labels with recognized words. In this paper,\nwe propose Word-level End-to-End Neural Diarization (WEEND) with auxiliary\nnetwork, a multi-task learning algorithm that performs end-to-end ASR and\nspeaker diarization in the same neural architecture. That is, while speech is\nbeing recognized, speaker labels are predicted simultaneously for each\nrecognized word. Experimental results demonstrate that WEEND outperforms the\nturn-based diarization baseline system on all 2-speaker short-form scenarios\nand has the capability to generalize to audio lengths of 5 minutes. Although\n3+speaker conversations are harder, we find that with enough in-domain training\ndata, WEEND has the potential to deliver high quality diarized text.", "published": "2023-09-15 15:48:45", "link": "http://arxiv.org/abs/2309.08489v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Visual Speech Recognition for Languages with Limited Labeled Data using\n  Automatic Labels from Whisper", "abstract": "This paper proposes a powerful Visual Speech Recognition (VSR) method for\nmultiple languages, especially for low-resource languages that have a limited\nnumber of labeled data. Different from previous methods that tried to improve\nthe VSR performance for the target language by using knowledge learned from\nother languages, we explore whether we can increase the amount of training data\nitself for the different languages without human intervention. To this end, we\nemploy a Whisper model which can conduct both language identification and\naudio-based speech recognition. It serves to filter data of the desired\nlanguages and transcribe labels from the unannotated, multilingual audio-visual\ndata pool. By comparing the performances of VSR models trained on automatic\nlabels and the human-annotated labels, we show that we can achieve similar VSR\nperformance to that of human-annotated labels even without utilizing human\nannotations. Through the automated labeling process, we label large-scale\nunlabeled multilingual databases, VoxCeleb2 and AVSpeech, producing 1,002 hours\nof data for four low VSR resource languages, French, Italian, Spanish, and\nPortuguese. With the automatic labels, we achieve new state-of-the-art\nperformance on mTEDx in four languages, significantly surpassing the previous\nmethods. The automatic labels are available online:\nhttps://github.com/JeongHun0716/Visual-Speech-Recognition-for-Low-Resource-Languages", "published": "2023-09-15 16:53:01", "link": "http://arxiv.org/abs/2309.08535v2", "categories": ["cs.CV", "cs.AI", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Enhance audio generation controllability through representation\n  similarity regularization", "abstract": "This paper presents an innovative approach to enhance control over audio\ngeneration by emphasizing the alignment between audio and text representations\nduring model training. In the context of language model-based audio generation,\nthe model leverages input from both textual and audio token representations to\npredict subsequent audio tokens. However, the current configuration lacks\nexplicit regularization to ensure the alignment between the chosen text\nrepresentation and the language model's predictions. Our proposal involves the\nincorporation of audio and text representation regularization, particularly\nduring the classifier-free guidance (CFG) phase, where the text condition is\nexcluded from cross attention during language model training. The aim of this\nproposed representation regularization is to minimize discrepancies in audio\nand text similarity compared to other samples within the same training batch.\nExperimental results on both music and audio generation tasks demonstrate that\nour proposed methods lead to improvements in objective metrics for both audio\nand music generation, as well as an enhancement in the human perception for\naudio generation.", "published": "2023-09-15 21:32:20", "link": "http://arxiv.org/abs/2309.08773v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Diverse Neural Audio Embeddings -- Bringing Features back !", "abstract": "With the advent of modern AI architectures, a shift has happened towards\nend-to-end architectures. This pivot has led to neural architectures being\ntrained without domain-specific biases/knowledge, optimized according to the\ntask. We in this paper, learn audio embeddings via diverse feature\nrepresentations, in this case, domain-specific. For the case of audio\nclassification over hundreds of categories of sound, we learn robust separate\nembeddings for diverse audio properties such as pitch, timbre, and neural\nrepresentation, along with also learning it via an end-to-end architecture. We\nobserve handcrafted embeddings, e.g., pitch and timbre-based, although on their\nown, are not able to beat a fully end-to-end representation, yet adding these\ntogether with end-to-end embedding helps us, significantly improve performance.\nThis work would pave the way to bring some domain expertise with end-to-end\nmodels to learn robust, diverse representations, surpassing the performance of\njust training end-to-end models.", "published": "2023-09-15 20:27:47", "link": "http://arxiv.org/abs/2309.08751v2", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
