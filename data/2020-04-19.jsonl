{"title": "BanFakeNews: A Dataset for Detecting Fake News in Bangla", "abstract": "Observing the damages that can be done by the rapid propagation of fake news\nin various sectors like politics and finance, automatic identification of fake\nnews using linguistic analysis has drawn the attention of the research\ncommunity. However, such methods are largely being developed for English where\nlow resource languages remain out of the focus. But the risks spawned by fake\nand manipulative news are not confined by languages. In this work, we propose\nan annotated dataset of ~50K news that can be used for building automated fake\nnews detection systems for a low resource language like Bangla. Additionally,\nwe provide an analysis of the dataset and develop a benchmark system with state\nof the art NLP techniques to identify Bangla fake news. To create this system,\nwe explore traditional linguistic features and neural network based methods. We\nexpect this dataset will be a valuable resource for building technologies to\nprevent the spreading of fake news and contribute in research with low resource\nlanguages.", "published": "2020-04-19 07:42:22", "link": "http://arxiv.org/abs/2004.08789v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pattern Learning for Detecting Defect Reports and Improvement Requests\n  in App Reviews", "abstract": "Online reviews are an important source of feedback for understanding\ncustomers. In this study, we follow novel approaches that target this absence\nof actionable insights by classifying reviews as defect reports and requests\nfor improvement. Unlike traditional classification methods based on expert\nrules, we reduce the manual labour by employing a supervised system that is\ncapable of learning lexico-semantic patterns through genetic programming.\nAdditionally, we experiment with a distantly-supervised SVM that makes use of\nnoisy labels generated by patterns. Using a real-world dataset of app reviews,\nwe show that the automatically learned patterns outperform the manually created\nones, to be generated. Also the distantly-supervised SVM models are not far\nbehind the pattern-based solutions, showing the usefulness of this approach\nwhen the amount of annotated data is limited.", "published": "2020-04-19 08:13:13", "link": "http://arxiv.org/abs/2004.08793v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extractive Summarization as Text Matching", "abstract": "This paper creates a paradigm shift with regard to the way we build neural\nextractive summarization systems. Instead of following the commonly used\nframework of extracting sentences individually and modeling the relationship\nbetween sentences, we formulate the extractive summarization task as a semantic\ntext matching problem, in which a source document and candidate summaries will\nbe (extracted from the original text) matched in a semantic space. Notably,\nthis paradigm shift to semantic matching framework is well-grounded in our\ncomprehensive analysis of the inherent gap between sentence-level and\nsummary-level extractors based on the property of the dataset.\n  Besides, even instantiating the framework with a simple form of a matching\nmodel, we have driven the state-of-the-art extractive result on CNN/DailyMail\nto a new level (44.41 in ROUGE-1). Experiments on the other five datasets also\nshow the effectiveness of the matching framework. We believe the power of this\nmatching-based summarization framework has not been fully exploited. To\nencourage more instantiations in the future, we have released our codes,\nprocessed dataset, as well as generated summaries in\nhttps://github.com/maszhongming/MatchSum.", "published": "2020-04-19 08:27:57", "link": "http://arxiv.org/abs/2004.08795v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge-graph based Proactive Dialogue Generation with Improved\n  Meta-Learning", "abstract": "Knowledge graph-based dialogue systems can narrow down knowledge candidates\nfor generating informative and diverse responses with the use of prior\ninformation, e.g., triple attributes or graph paths. However, most current\nknowledge graph (KG) cover incomplete domain-specific knowledge. To overcome\nthis drawback, we propose a knowledge graph based proactive dialogue generation\nmodel (KgDg) with three components, improved model-agnostic meta-learning\nalgorithm (MAML), knowledge selection in knowledge triplets embedding, and\nknowledge aware proactive response generator. For knowledge triplets embedding\nand selection, we formulate it as a problem of sentence embedding to better\ncapture semantic information. Our improved MAML algorithm is capable of\nlearning general features from a limited number of knowledge graphs, which can\nalso quickly adapt to dialogue generation with unseen knowledge triplets.\nExtensive experiments are conducted on a knowledge aware dialogue dataset\n(DuConv). The results show that KgDg adapts both fast and well to knowledge\ngraph-based dialogue generation and outperforms state-of-the-art baseline.", "published": "2020-04-19 08:41:12", "link": "http://arxiv.org/abs/2004.08798v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Chinese Corpus for Fine-grained Entity Typing", "abstract": "Fine-grained entity typing is a challenging task with wide applications.\nHowever, most existing datasets for this task are in English. In this paper, we\nintroduce a corpus for Chinese fine-grained entity typing that contains 4,800\nmentions manually labeled through crowdsourcing. Each mention is annotated with\nfree-form entity types. To make our dataset useful in more possible scenarios,\nwe also categorize all the fine-grained types into 10 general types. Finally,\nwe conduct experiments with some neural models whose structures are typical in\nfine-grained entity typing and show how well they perform on our dataset. We\nalso show the possibility of improving Chinese fine-grained entity typing\nthrough cross-lingual transfer learning.", "published": "2020-04-19 11:53:32", "link": "http://arxiv.org/abs/2004.08825v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dynamic Knowledge Graph-based Dialogue Generation with Improved\n  Adversarial Meta-Learning", "abstract": "Knowledge graph-based dialogue systems are capable of generating more\ninformative responses and can implement sophisticated reasoning mechanisms.\nHowever, these models do not take into account the sparseness and\nincompleteness of knowledge graph (KG)and current dialogue models cannot be\napplied to dynamic KG. This paper proposes a dynamic Knowledge graph-based\ndialogue generation method with improved adversarial Meta-Learning (KDAD). KDAD\nformulates dynamic knowledge triples as a problem of adversarial attack and\nincorporates the objective of quickly adapting to dynamic knowledge-aware\ndialogue generation. We train a knowledge graph-based dialog model with\nimproved ADML using minimal training samples. The model can initialize the\nparameters and adapt to previous unseen knowledge so that training can be\nquickly completed based on only a few knowledge triples. We show that our model\nsignificantly outperforms other baselines. We evaluate and demonstrate that our\nmethod adapts extremely fast and well to dynamic knowledge graph-based dialogue\ngeneration.", "published": "2020-04-19 12:27:49", "link": "http://arxiv.org/abs/2004.08833v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Machine Translation for Low-Resourced Indian Languages", "abstract": "A large number of significant assets are available online in English, which\nis frequently translated into native languages to ease the information sharing\namong local people who are not much familiar with English. However, manual\ntranslation is a very tedious, costly, and time-taking process. To this end,\nmachine translation is an effective approach to convert text to a different\nlanguage without any human involvement. Neural machine translation (NMT) is one\nof the most proficient translation techniques amongst all existing machine\ntranslation systems. In this paper, we have applied NMT on two of the most\nmorphological rich Indian languages, i.e. English-Tamil and English-Malayalam.\nWe proposed a novel NMT model using Multihead self-attention along with\npre-trained Byte-Pair-Encoded (BPE) and MultiBPE embeddings to develop an\nefficient translation system that overcomes the OOV (Out Of Vocabulary) problem\nfor low resourced morphological rich Indian languages which do not have much\ntranslation available online. We also collected corpus from different sources,\naddressed the issues with these publicly available data and refined them for\nfurther uses. We used the BLEU score for evaluating our system performance.\nExperimental results and survey confirmed that our proposed translator (24.34\nand 9.78 BLEU score) outperforms Google translator (9.40 and 5.94 BLEU score)\nrespectively.", "published": "2020-04-19 17:29:34", "link": "http://arxiv.org/abs/2004.13819v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are we pretraining it right? Digging deeper into visio-linguistic\n  pretraining", "abstract": "Numerous recent works have proposed pretraining generic visio-linguistic\nrepresentations and then finetuning them for downstream vision and language\ntasks. While architecture and objective function design choices have received\nattention, the choice of pretraining datasets has received little attention. In\nthis work, we question some of the default choices made in literature. For\ninstance, we systematically study how varying similarity between the\npretraining dataset domain (textual and visual) and the downstream domain\naffects performance. Surprisingly, we show that automatically generated data in\na domain closer to the downstream task (e.g., VQA v2) is a better choice for\npretraining than \"natural\" data but of a slightly different domain (e.g.,\nConceptual Captions). On the other hand, some seemingly reasonable choices of\npretraining datasets were found to be entirely ineffective for some downstream\ntasks. This suggests that despite the numerous recent efforts, vision &\nlanguage pretraining does not quite work \"out of the box\" yet. Overall, as a\nby-product of our study, we find that simple design choices in pretraining can\nhelp us achieve close to state-of-art results on downstream tasks without any\narchitectural changes.", "published": "2020-04-19 01:55:19", "link": "http://arxiv.org/abs/2004.08744v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Graph-Structured Referring Expression Reasoning in The Wild", "abstract": "Grounding referring expressions aims to locate in an image an object referred\nto by a natural language expression. The linguistic structure of a referring\nexpression provides a layout of reasoning over the visual contents, and it is\noften crucial to align and jointly understand the image and the referring\nexpression. In this paper, we propose a scene graph guided modular network\n(SGMN), which performs reasoning over a semantic graph and a scene graph with\nneural modules under the guidance of the linguistic structure of the\nexpression. In particular, we model the image as a structured semantic graph,\nand parse the expression into a language scene graph. The language scene graph\nnot only decodes the linguistic structure of the expression, but also has a\nconsistent representation with the image semantic graph. In addition to\nexploring structured solutions to grounding referring expressions, we also\npropose Ref-Reasoning, a large-scale real-world dataset for structured\nreferring expression reasoning. We automatically generate referring expressions\nover the scene graphs of images using diverse expression templates and\nfunctional programs. This dataset is equipped with real-world visual contents\nas well as semantically rich expressions with different reasoning layouts.\nExperimental results show that our SGMN not only significantly outperforms\nexisting state-of-the-art algorithms on the new Ref-Reasoning dataset, but also\nsurpasses state-of-the-art structured methods on commonly used benchmark\ndatasets. It can also provide interpretable visual evidences of reasoning. Data\nand code are available at https://github.com/sibeiyang/sgmn", "published": "2020-04-19 11:00:30", "link": "http://arxiv.org/abs/2004.08814v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Evolution of Semantic Similarity -- A Survey", "abstract": "Estimating the semantic similarity between text data is one of the\nchallenging and open research problems in the field of Natural Language\nProcessing (NLP). The versatility of natural language makes it difficult to\ndefine rule-based methods for determining semantic similarity measures. In\norder to address this issue, various semantic similarity methods have been\nproposed over the years. This survey article traces the evolution of such\nmethods, categorizing them based on their underlying principles as\nknowledge-based, corpus-based, deep neural network-based methods, and hybrid\nmethods. Discussing the strengths and weaknesses of each method, this survey\nprovides a comprehensive view of existing systems in place, for new researchers\nto experiment and develop innovative ideas to address the issue of semantic\nsimilarity.", "published": "2020-04-19 22:07:39", "link": "http://arxiv.org/abs/2004.13820v2", "categories": ["cs.CL", "cs.IR", "I.2.7"], "primary_category": "cs.CL"}
{"title": "The Cost of Training NLP Models: A Concise Overview", "abstract": "We review the cost of training large-scale language models, and the drivers\nof these costs. The intended audience includes engineers and scientists\nbudgeting their model-training experiments, as well as non-practitioners trying\nto make sense of the economics of modern-day Natural Language Processing (NLP).", "published": "2020-04-19 16:28:35", "link": "http://arxiv.org/abs/2004.08900v1", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "ktrain: A Low-Code Library for Augmented Machine Learning", "abstract": "We present ktrain, a low-code Python library that makes machine learning more\naccessible and easier to apply. As a wrapper to TensorFlow and many other\nlibraries (e.g., transformers, scikit-learn, stellargraph), it is designed to\nmake sophisticated, state-of-the-art machine learning models simple to build,\ntrain, inspect, and apply by both beginners and experienced practitioners.\nFeaturing modules that support text data (e.g., text classification, sequence\ntagging, open-domain question-answering), vision data (e.g., image\nclassification), graph data (e.g., node classification, link prediction), and\ntabular data, ktrain presents a simple unified interface enabling one to\nquickly solve a wide range of tasks in as little as three or four \"commands\" or\nlines of code.", "published": "2020-04-19 14:18:20", "link": "http://arxiv.org/abs/2004.10703v5", "categories": ["cs.LG", "cs.CL", "cs.CV", "cs.SI"], "primary_category": "cs.LG"}
{"title": "Classification using Hyperdimensional Computing: A Review", "abstract": "Hyperdimensional (HD) computing is built upon its unique data type referred\nto as hypervectors. The dimension of these hypervectors is typically in the\nrange of tens of thousands. Proposed to solve cognitive tasks, HD computing\naims at calculating similarity among its data. Data transformation is realized\nby three operations, including addition, multiplication and permutation. Its\nultra-wide data representation introduces redundancy against noise. Since\ninformation is evenly distributed over every bit of the hypervectors, HD\ncomputing is inherently robust. Additionally, due to the nature of those three\noperations, HD computing leads to fast learning ability, high energy efficiency\nand acceptable accuracy in learning and classification tasks. This paper\nintroduces the background of HD computing, and reviews the data representation,\ndata transformation, and similarity measurement. The orthogonality in high\ndimensions presents opportunities for flexible computing. To balance the\ntradeoff between accuracy and efficiency, strategies include but are not\nlimited to encoding, retraining, binarization and hardware acceleration.\nEvaluations indicate that HD computing shows great potential in addressing\nproblems using data in the form of letters, signals and images. HD computing\nespecially shows significant promise to replace machine learning algorithms as\na light-weight classifier in the field of internet of things (IoTs).", "published": "2020-04-19 23:51:44", "link": "http://arxiv.org/abs/2004.11204v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NE", "eess.SP"], "primary_category": "cs.LG"}
{"title": "Consonant gemination in Italian: the affricate and fricative case", "abstract": "Consonant gemination in Italian affricates and fricatives was investigated,\ncompleting the overall study of gemination of Italian consonants. Results of\nthe analysis of other consonant categories, i.e. stops, nasals, and liquids,\nshowed that closure duration for stops and consonant duration for nasals and\nliquids, form the most salient acoustic cues to gemination. Frequency and\nenergy domain parameters were not significantly affected by gemination in a\nsystematic way for all consonant classes. Results on fricatives and affricates\nconfirmed the above findings, i.e., that the primary acoustic correlate of\ngemination is durational in nature and corresponds to a lengthened consonant\nduration for fricative geminates and a lengthened closure duration for\naffricate geminates. An inverse correlation between consonant and pre-consonant\nvowel durations was present for both consonant categories, and also for both\nsingleton and geminate word sets when considered separately. This effect was\nreinforced for combined sets, confirming the hypothesis that a durational\ncompensation between different phonemes may serve to preserve rhythmical\nstructures. Classification tests of single vs. geminate consonants using the\ndurational acoustic cues as classification parameters confirmed their validity,\nand highlighted peculiarities of the two consonant classes. In particular, a\nrelatively poor classification performance was observed for affricates, which\nled to refining the analysis by considering dental vs. non-dental affricates in\ntwo different sets. Results support the hypothesis that dental affricates, in\nItalian, may not appear in intervocalic position as singletons but only in\ntheir geminate form.", "published": "2020-04-19 12:23:46", "link": "http://arxiv.org/abs/2005.06959v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Consonant gemination in Italian: the nasal and liquid case", "abstract": "All Italian consonants affected by gemination, that is affricates,\nfricatives, liquids, nasals, and stops, were analyzed within a project named\nGEMMA that lasted over a span of about 25 years. Results of the analysis on\nstops, as published in (Esposito, A., and Di Benedetto, M. G. (1999). \"Acoustic\nand Perceptual Study of Gemination in Italian Stops,\" The Journal of the\nAcoustical Society of America, ASA, Vol. 30, pp. 175-185) showed that the main\nacoustic cue to gemination in Italian was closure duration, while frequency and\nenergy domain parameters were not significantly affected by gemination. This\npaper - the first of a set of two covering all remaining consonants - addresses\nnasals and liquids; its companion paper addresses affricates and fricatives.\nResults on nasals and liquids confirm the findings on stops, in particular that\nthe primary acoustic cue to gemination in Italian is durational in nature and\ncorresponds to a lengthened consonant duration. Results also show an inverse\ncorrelation between consonant and pre-consonant vowel durations which is,\nhowever, also present when considering singleton vs. geminate word sets\nseparately, indicating a sort of duration compensation between these segments\nto eventually preserve rhythmical structures; this inverse correlation is\nreinforced when considering singleton and geminate sets combined.\nClassification tests of singleton vs. geminate consonants show that, for both\nnasals and liquids, best classification scores are obtained when consonant\nduration is used as a classification parameter. Although slightly less\nperforming, the ratio between consonant and pre-consonant vowel durations is\nalso a potential good candidate for automatic classification of geminate vs\nsingleton nasals and liquids in Italian.", "published": "2020-04-19 12:20:15", "link": "http://arxiv.org/abs/2005.06960v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The Attacker's Perspective on Automatic Speaker Verification: An\n  Overview", "abstract": "Security of automatic speaker verification (ASV) systems is compromised by\nvarious spoofing attacks. While many types of non-proactive attacks (and their\ndefenses) have been studied in the past, attacker's perspective on ASV,\nrepresents a far less explored direction. It can potentially help to identify\nthe weakest parts of ASV systems and be used to develop attacker-aware systems.\nWe present an overview on this emerging research area by focusing on potential\nthreats of adversarial attacks on ASV, spoofing countermeasures, or both. We\nconclude the study with discussion on selected attacks and leveraging from such\nknowledge to improve defense mechanisms against adversarial attacks.", "published": "2020-04-19 13:27:25", "link": "http://arxiv.org/abs/2004.08849v1", "categories": ["eess.AS", "cs.CR"], "primary_category": "eess.AS"}
