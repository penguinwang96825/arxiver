{"title": "LinCE: A Centralized Benchmark for Linguistic Code-switching Evaluation", "abstract": "Recent trends in NLP research have raised an interest in linguistic\ncode-switching (CS); modern approaches have been proposed to solve a wide range\nof NLP tasks on multiple language pairs. Unfortunately, these proposed methods\nare hardly generalizable to different code-switched languages. In addition, it\nis unclear whether a model architecture is applicable for a different task\nwhile still being compatible with the code-switching setting. This is mainly\nbecause of the lack of a centralized benchmark and the sparse corpora that\nresearchers employ based on their specific needs and interests. To facilitate\nresearch in this direction, we propose a centralized benchmark for Linguistic\nCode-switching Evaluation (LinCE) that combines ten corpora covering four\ndifferent code-switched language pairs (i.e., Spanish-English, Nepali-English,\nHindi-English, and Modern Standard Arabic-Egyptian Arabic) and four tasks\n(i.e., language identification, named entity recognition, part-of-speech\ntagging, and sentiment analysis). As part of the benchmark centralization\neffort, we provide an online platform at ritual.uh.edu/lince, where researchers\ncan submit their results while comparing with others in real-time. In addition,\nwe provide the scores of different popular models, including LSTM, ELMo, and\nmultilingual BERT so that the NLP community can compare against\nstate-of-the-art systems. LinCE is a continuous effort, and we will expand it\nwith more low-resource languages and tasks.", "published": "2020-05-09 00:00:08", "link": "http://arxiv.org/abs/2005.04322v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Diversifying Dialogue Generation with Non-Conversational Text", "abstract": "Neural network-based sequence-to-sequence (seq2seq) models strongly suffer\nfrom the low-diversity problem when it comes to open-domain dialogue\ngeneration. As bland and generic utterances usually dominate the frequency\ndistribution in our daily chitchat, avoiding them to generate more interesting\nresponses requires complex data filtering, sampling techniques or modifying the\ntraining objective. In this paper, we propose a new perspective to diversify\ndialogue generation by leveraging non-conversational text. Compared with\nbilateral conversations, non-conversational text are easier to obtain, more\ndiverse and cover a much broader range of topics. We collect a large-scale\nnon-conversational corpus from multi sources including forum comments, idioms\nand book snippets. We further present a training paradigm to effectively\nincorporate these text via iterative back translation. The resulting model is\ntested on two conversational datasets and is shown to produce significantly\nmore diverse responses without sacrificing the relevance with context.", "published": "2020-05-09 02:16:05", "link": "http://arxiv.org/abs/2005.04346v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Structured Weighted Violations MIRA", "abstract": "We present the Structured Weighted Violation MIRA (SWVM), a new structured\nprediction algorithm that is based on an hybridization between MIRA (Crammer\nand Singer, 2003) and the structured weighted violations perceptron (SWVP)\n(Dror and Reichart, 2016). We demonstrate that the concepts developed in (Dror\nand Reichart, 2016) combined with a powerful structured prediction algorithm\ncan improve performance on sequence labeling tasks. In experiments with\nsyntactic chunking and named entity recognition (NER), the new algorithm\nsubstantially outperforms the original MIRA as well as the original structured\nperceptron and SWVP. Our code is available at\nhttps://github.com/dorringel/SWVM.", "published": "2020-05-09 11:02:46", "link": "http://arxiv.org/abs/2005.04418v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Empowering Active Learning to Jointly Optimize System and User Demands", "abstract": "Existing approaches to active learning maximize the system performance by\nsampling unlabeled instances for annotation that yield the most efficient\ntraining. However, when active learning is integrated with an end-user\napplication, this can lead to frustration for participating users, as they\nspend time labeling instances that they would not otherwise be interested in\nreading. In this paper, we propose a new active learning approach that jointly\noptimizes the seemingly counteracting objectives of the active learning system\n(training efficiently) and the user (receiving useful instances). We study our\napproach in an educational application, which particularly benefits from this\ntechnique as the system needs to rapidly learn to predict the appropriateness\nof an exercise to a particular user, while the users should receive only\nexercises that match their skills. We evaluate multiple learning strategies and\nuser types with data from real users and find that our joint approach better\nsatisfies both objectives when alternative methods lead to many unsuitable\nexercises for end users.", "published": "2020-05-09 16:02:52", "link": "http://arxiv.org/abs/2005.04470v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semi-Supervised Dialogue Policy Learning via Stochastic Reward\n  Estimation", "abstract": "Dialogue policy optimization often obtains feedback until task completion in\ntask-oriented dialogue systems. This is insufficient for training intermediate\ndialogue turns since supervision signals (or rewards) are only provided at the\nend of dialogues. To address this issue, reward learning has been introduced to\nlearn from state-action pairs of an optimal policy to provide turn-by-turn\nrewards. This approach requires complete state-action annotations of\nhuman-to-human dialogues (i.e., expert demonstrations), which is labor\nintensive. To overcome this limitation, we propose a novel reward learning\napproach for semi-supervised policy learning. The proposed approach learns a\ndynamics model as the reward function which models dialogue progress (i.e.,\nstate-action sequences) based on expert demonstrations, either with or without\nannotations. The dynamics model computes rewards by predicting whether the\ndialogue progress is consistent with expert demonstrations. We further propose\nto learn action embeddings for a better generalization of the reward function.\nThe proposed approach outperforms competitive policy learning baselines on\nMultiWOZ, a benchmark multi-domain dataset.", "published": "2020-05-09 06:28:44", "link": "http://arxiv.org/abs/2005.04379v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Finding Universal Grammatical Relations in Multilingual BERT", "abstract": "Recent work has found evidence that Multilingual BERT (mBERT), a\ntransformer-based multilingual masked language model, is capable of zero-shot\ncross-lingual transfer, suggesting that some aspects of its representations are\nshared cross-lingually. To better understand this overlap, we extend recent\nwork on finding syntactic trees in neural networks' internal representations to\nthe multilingual setting. We show that subspaces of mBERT representations\nrecover syntactic tree distances in languages other than English, and that\nthese subspaces are approximately shared across languages. Motivated by these\nresults, we present an unsupervised analysis method that provides evidence\nmBERT learns representations of syntactic dependency labels, in the form of\nclusters which largely agree with the Universal Dependencies taxonomy. This\nevidence suggests that even without explicit supervision, multilingual masked\nlanguage models learn certain linguistic universals.", "published": "2020-05-09 20:46:02", "link": "http://arxiv.org/abs/2005.04511v2", "categories": ["cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "schuBERT: Optimizing Elements of BERT", "abstract": "Transformers \\citep{vaswani2017attention} have gradually become a key\ncomponent for many state-of-the-art natural language representation models. A\nrecent Transformer based model- BERT \\citep{devlin2018bert} achieved\nstate-of-the-art results on various natural language processing tasks,\nincluding GLUE, SQuAD v1.1, and SQuAD v2.0. This model however is\ncomputationally prohibitive and has a huge number of parameters. In this work\nwe revisit the architecture choices of BERT in efforts to obtain a lighter\nmodel. We focus on reducing the number of parameters yet our methods can be\napplied towards other objectives such FLOPs or latency. We show that much\nefficient light BERT models can be obtained by reducing algorithmically chosen\ncorrect architecture design dimensions rather than reducing the number of\nTransformer encoder layers. In particular, our schuBERT gives $6.6\\%$ higher\naverage accuracy on GLUE and SQuAD datasets as compared to BERT with three\nencoder layers while having the same number of parameters.", "published": "2020-05-09 21:56:04", "link": "http://arxiv.org/abs/2005.06628v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Generalizing Outside the Training Set: When Can Neural Networks Learn\n  Identity Effects?", "abstract": "Often in language and other areas of cognition, whether two components of an\nobject are identical or not determine whether it is well formed. We call such\nconstraints identity effects. When developing a system to learn well-formedness\nfrom examples, it is easy enough to build in an identify effect. But can\nidentity effects be learned from the data without explicit guidance? We provide\na simple framework in which we can rigorously prove that algorithms satisfying\nsimple criteria cannot make the correct inference. We then show that a broad\nclass of algorithms including deep neural networks with standard architecture\nand training with backpropagation satisfy our criteria, dependent on the\nencoding of inputs. Finally, we demonstrate our theory with computational\nexperiments in which we explore the effect of different input encodings on the\nability of algorithms to generalize to novel inputs.", "published": "2020-05-09 01:08:07", "link": "http://arxiv.org/abs/2005.04330v1", "categories": ["cs.CL", "cs.NA", "math.NA", "68T07, 68Q32"], "primary_category": "cs.CL"}
{"title": "Generating Pertinent and Diversified Comments with Topic-aware\n  Pointer-Generator Networks", "abstract": "Comment generation, a new and challenging task in Natural Language Generation\n(NLG), attracts a lot of attention in recent years. However, comments generated\nby previous work tend to lack pertinence and diversity. In this paper, we\npropose a novel generation model based on Topic-aware Pointer-Generator\nNetworks (TPGN), which can utilize the topic information hidden in the articles\nto guide the generation of pertinent and diversified comments. Firstly, we\ndesign a keyword-level and topic-level encoder attention mechanism to capture\ntopic information in the articles. Next, we integrate the topic information\ninto pointer-generator networks to guide comment generation. Experiments on a\nlarge scale of comment generation dataset show that our model produces the\nvaluable comments and outperforms competitive baseline models significantly.", "published": "2020-05-09 09:04:09", "link": "http://arxiv.org/abs/2005.04396v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "What Was Written vs. Who Read It: News Media Profiling Using Text\n  Analysis and Social Media Context", "abstract": "Predicting the political bias and the factuality of reporting of entire news\noutlets are critical elements of media profiling, which is an understudied but\nan increasingly important research direction. The present level of\nproliferation of fake, biased, and propagandistic content online, has made it\nimpossible to fact-check every single suspicious claim, either manually or\nautomatically. Alternatively, we can profile entire news outlets and look for\nthose that are likely to publish fake or biased content. This approach makes it\npossible to detect likely \"fake news\" the moment they are published, by simply\nchecking the reliability of their source. From a practical perspective,\npolitical bias and factuality of reporting have a linguistic aspect but also a\nsocial context. Here, we study the impact of both, namely (i) what was written\n(i.e., what was published by the target medium, and how it describes itself on\nTwitter) vs. (ii) who read it (i.e., analyzing the readers of the target medium\non Facebook, Twitter, and YouTube). We further study (iii) what was written\nabout the target medium on Wikipedia. The evaluation results show that what was\nwritten matters most, and that putting all information sources together yields\nhuge improvements over the current state-of-the-art.", "published": "2020-05-09 22:00:08", "link": "http://arxiv.org/abs/2005.04518v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Cyberbullying Detection with Fairness Constraints", "abstract": "Cyberbullying is a widespread adverse phenomenon among online social\ninteractions in today's digital society. While numerous computational studies\nfocus on enhancing the cyberbullying detection performance of machine learning\nalgorithms, proposed models tend to carry and reinforce unintended social\nbiases. In this study, we try to answer the research question of \"Can we\nmitigate the unintended bias of cyberbullying detection models by guiding the\nmodel training with fairness constraints?\". For this purpose, we propose a\nmodel training scheme that can employ fairness constraints and validate our\napproach with different datasets. We demonstrate that various types of\nunintended biases can be successfully mitigated without impairing the model\nquality. We believe our work contributes to the pursuit of unbiased,\ntransparent, and ethical machine learning solutions for cyber-social health.", "published": "2020-05-09 13:04:26", "link": "http://arxiv.org/abs/2005.06625v2", "categories": ["cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "It's Morphin' Time! Combating Linguistic Discrimination with\n  Inflectional Perturbations", "abstract": "Training on only perfect Standard English corpora predisposes pre-trained\nneural networks to discriminate against minorities from non-standard linguistic\nbackgrounds (e.g., African American Vernacular English, Colloquial Singapore\nEnglish, etc.). We perturb the inflectional morphology of words to craft\nplausible and semantically similar adversarial examples that expose these\nbiases in popular NLP models, e.g., BERT and Transformer, and show that\nadversarially fine-tuning them for a single epoch significantly improves\nrobustness without sacrificing performance on clean data.", "published": "2020-05-09 04:01:43", "link": "http://arxiv.org/abs/2005.04364v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "U-net Based Direct-path Dominance Test for Robust Direction-of-arrival\n  Estimation", "abstract": "It has been noted that the identification of the time-frequency bins\ndominated by the contribution from the direct propagation of the target speaker\ncan significantly improve the robustness of the direction-of-arrival\nestimation. However, the correct extraction of the direct-path sound is\nchallenging especially in adverse environments. In this paper, a U-net based\ndirect-path dominance test method is proposed. Exploiting the efficient\nsegmentation capability of the U-net architecture, the direct-path information\ncan be effectively retrieved from a dedicated multi-task neural network.\nMoreover, the training and inference of the neural network only need the input\nof a single microphone, circumventing the problem of array-structure dependence\nfaced by common end-to-end deep learning based methods. Simulations demonstrate\nthat significantly higher estimation accuracy can be achieved in high\nreverberant and low signal-to-noise ratio environments.", "published": "2020-05-09 06:11:43", "link": "http://arxiv.org/abs/2005.04376v3", "categories": ["eess.AS", "47N70", "J.2"], "primary_category": "eess.AS"}
{"title": "Temporal-Framing Adaptive Network for Heart Sound Segmentation without\n  Prior Knowledge of State Duration", "abstract": "Objective: This paper presents a novel heart sound segmentation algorithm\nbased on Temporal-Framing Adaptive Network (TFAN), including state transition\nloss and dynamic inference for decoding the most likely state sequence.\nMethods: In contrast to previous state-of-the-art approaches, the TFAN-based\nmethod does not require any knowledge of the state duration of heart sounds and\nis therefore likely to generalize to non sinus rhythm. The TFAN-based method\nwas trained on 50 recordings randomly chosen from Training set A of the 2016\nPhysioNet/Computer in Cardiology Challenge and tested on the other 12\nindependent training and test databases (2099 recordings and 52180 beats). The\ndatabases for segmentation were separated into three levels of increasing\ndifficulty (LEVEL-I, -II and -III) for performance reporting. Results: The\nTFAN-based method achieved a superior F1 score for all 12 databases except for\n`Test-B', with an average of 96.7%, compared to 94.6% for the state-of-the-art\nmethod. Moreover, the TFAN-based method achieved an overall F1 score of 99.2%,\n94.4%, 91.4% on LEVEL-I, -II and -III data respectively, compared to 98.4%,\n88.54% and 79.80% for the current state-of-the-art method. Conclusion: The\nTFAN-based method therefore provides a substantial improvement, particularly\nfor more difficult cases, and on data sets not represented in the public\ntraining data. Significance: The proposed method is highly flexible and likely\nto apply to other non-stationary time series. Further work is required to\nunderstand to what extent this approach will provide improved diagnostic\nperformance, although it is logical to assume superior segmentation will lead\nto improved diagnostics.", "published": "2020-05-09 12:32:25", "link": "http://arxiv.org/abs/2005.04426v3", "categories": ["eess.SP", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
