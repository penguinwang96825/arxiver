{"title": "Adaptive Neural Trees", "abstract": "Deep neural networks and decision trees operate on largely separate paradigms; typically, the former performs representation learning with pre-specified architectures, while the latter is characterised by learning hierarchies over pre-specified features with data-driven architectures. We unite the two via adaptive neural trees (ANTs) that incorporates representation learning into edges, routing functions and leaf nodes of a decision tree, along with a backpropagation-based training algorithm that adaptively grows the architecture from primitive modules (e.g., convolutional layers). We demonstrate that, whilst achieving competitive performance on classification and regression datasets, ANTs benefit from (i) lightweight inference via conditional computation, (ii) hierarchical separation of features useful to the task e.g. learning meaningful class associations, such as separating natural vs. man-made objects, and (iii) a mechanism to adapt the architecture to the size and complexity of the training dataset.", "published": "2018-07-17 23:01:35", "link": "http://arxiv.org/abs/1807.06699v5", "categories": ["cs.NE", "cs.CV", "cs.LG", "stat.ML"], "primary_category": "cs.NE"}
{"title": "Icing on the Cake: An Easy and Quick Post-Learnig Method You Can Try After Deep Learning", "abstract": "We found an easy and quick post-learning method named \"Icing on the Cake\" to enhance a classification performance in deep learning. The method is that we train only the final classifier again after an ordinary training is done.", "published": "2018-07-17 16:35:51", "link": "http://arxiv.org/abs/1807.06540v1", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.NE", "stat.ML"], "primary_category": "cs.LG"}
{"title": "IntroVAE: Introspective Variational Autoencoders for Photographic Image Synthesis", "abstract": "We present a novel introspective variational autoencoder (IntroVAE) model for synthesizing high-resolution photographic images. IntroVAE is capable of self-evaluating the quality of its generated samples and improving itself accordingly. Its inference and generator models are jointly trained in an introspective way. On one hand, the generator is required to reconstruct the input images from the noisy outputs of the inference model as normal VAEs. On the other hand, the inference model is encouraged to classify between the generated and real samples while the generator tries to fool it as GANs. These two famous generative frameworks are integrated in a simple yet efficient single-stream architecture that can be trained in a single stage. IntroVAE preserves the advantages of VAEs, such as stable training and nice latent manifold. Unlike most other hybrid models of VAEs and GANs, IntroVAE requires no extra discriminators, because the inference model itself serves as a discriminator to distinguish between the generated and real samples. Experiments demonstrate that our method produces high-resolution photo-realistic images (e.g., CELEBA images at \\(1024^{2}\\)), which are comparable to or better than the state-of-the-art GANs.", "published": "2018-07-17 11:37:31", "link": "http://arxiv.org/abs/1807.06358v2", "categories": ["cs.LG", "cs.CV", "cs.GR", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Foundations for Restraining Bolts: Reinforcement Learning with LTLf/LDLf restraining specifications", "abstract": "In this work we investigate on the concept of \"restraining bolt\", envisioned in Science Fiction. Specifically we introduce a novel problem in AI. We have two distinct sets of features extracted from the world, one by the agent and one by the authority imposing restraining specifications (the \"restraining bolt\"). The two sets are apparently unrelated since of interest to independent parties, however they both account for (aspects of) the same world. We consider the case in which the agent is a reinforcement learning agent on the first set of features, while the restraining bolt is specified logically using linear time logic on finite traces LTLf/LDLf over the second set of features. We show formally, and illustrate with examples, that, under general circumstances, the agent can learn while shaping its goals to suitably conform (as much as possible) to the restraining bolt specifications.", "published": "2018-07-17 10:51:04", "link": "http://arxiv.org/abs/1807.06333v2", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Confidence Intervals for Testing Disparate Impact in Fair Learning", "abstract": "We provide the asymptotic distribution of the major indexes used in the statistical literature to quantify disparate treatment in machine learning. We aim at promoting the use of confidence intervals when testing the so-called group disparate impact. We illustrate on some examples the importance of using confidence intervals and not a single value.", "published": "2018-07-17 11:48:19", "link": "http://arxiv.org/abs/1807.06362v1", "categories": ["stat.ML", "cs.LG", "math.ST"], "primary_category": "stat.ML"}
{"title": "Design and Analysis of Efficient Maximum/Minimum Circuits for Stochastic Computing", "abstract": "In stochastic computing (SC), a real-valued number is represented by a stochastic bit stream, encoding its value in the probability of obtaining a one. This leads to a significantly lower hardware effort for various functions and provides a higher tolerance to errors (e.g., bit flips) compared to binary radix representation. The implementation of a stochastic max/min function is important for many areas where SC has been successfully applied, such as image processing or machine learning (e.g., max pooling in neural networks). In this work, we propose a novel shift-register-based architecture for a stochastic max/min function. We show that the proposed circuit has a significantly higher accuracy than state-of-the-art architectures at comparable hardware cost. Moreover, we analytically proof the correctness of the proposed circuit and provide a new error analysis, based on the individual bits of the stochastic streams. Interestingly, the analysis reveals that for a certain practical bit stream length a finite optimal shift register length exists and it allows to determine the optimal length.", "published": "2018-07-17 11:37:46", "link": "http://arxiv.org/abs/1807.06966v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
