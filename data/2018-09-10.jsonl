{"title": "A case for deep learning in semantics", "abstract": "Pater's target article builds a persuasive case for establishing stronger\nties between theoretical linguistics and connectionism (deep learning). This\ncommentary extends his arguments to semantics, focusing in particular on issues\nof learning, compositionality, and lexical meaning.", "published": "2018-09-10 00:34:34", "link": "http://arxiv.org/abs/1809.03068v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Depth-bounding is effective: Improvements and evaluation of unsupervised\n  PCFG induction", "abstract": "There have been several recent attempts to improve the accuracy of grammar\ninduction systems by bounding the recursive complexity of the induction model\n(Ponvert et al., 2011; Noji and Johnson, 2016; Shain et al., 2016; Jin et al.,\n2018). Modern depth-bounded grammar inducers have been shown to be more\naccurate than early unbounded PCFG inducers, but this technique has never been\ncompared against unbounded induction within the same system, in part because\nmost previous depth-bounding models are built around sequence models, the\ncomplexity of which grows exponentially with the maximum allowed depth. The\npresent work instead applies depth bounds within a chart-based Bayesian PCFG\ninducer (Johnson et al., 2007b), where bounding can be switched on and off, and\nthen samples trees with and without bounding. Results show that depth-bounding\nis indeed significantly effective in limiting the search space of the inducer\nand thereby increasing the accuracy of the resulting parsing model. Moreover,\nparsing results on English, Chinese and German show that this bounded model\nwith a new inference technique is able to produce parse trees more accurately\nthan or competitively with state-of-the-art constituency-based grammar\ninduction models.", "published": "2018-09-10 03:02:46", "link": "http://arxiv.org/abs/1809.03112v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Deep Reinforced Sequence-to-Set Model for Multi-Label Text\n  Classification", "abstract": "Multi-label text classification (MLTC) aims to assign multiple labels to each\nsample in the dataset. The labels usually have internal correlations. However,\ntraditional methods tend to ignore the correlations between labels. In order to\ncapture the correlations between labels, the sequence-to-sequence (Seq2Seq)\nmodel views the MLTC task as a sequence generation problem, which achieves\nexcellent performance on this task. However, the Seq2Seq model is not suitable\nfor the MLTC task in essence. The reason is that it requires humans to\npredefine the order of the output labels, while some of the output labels in\nthe MLTC task are essentially an unordered set rather than an ordered sequence.\nThis conflicts with the strict requirement of the Seq2Seq model for the label\norder. In this paper, we propose a novel sequence-to-set framework utilizing\ndeep reinforcement learning, which not only captures the correlations between\nlabels, but also reduces the dependence on the label order. Extensive\nexperimental results show that our proposed method outperforms the competitive\nbaselines by a large margin.", "published": "2018-09-10 03:33:48", "link": "http://arxiv.org/abs/1809.03118v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Greedy Search with Probabilistic N-gram Matching for Neural Machine\n  Translation", "abstract": "Neural machine translation (NMT) models are usually trained with the\nword-level loss using the teacher forcing algorithm, which not only evaluates\nthe translation improperly but also suffers from exposure bias. Sequence-level\ntraining under the reinforcement framework can mitigate the problems of the\nword-level loss, but its performance is unstable due to the high variance of\nthe gradient estimation. On these grounds, we present a method with a\ndifferentiable sequence-level training objective based on probabilistic n-gram\nmatching which can avoid the reinforcement framework. In addition, this method\nperforms greedy search in the training which uses the predicted words as\ncontext just as at inference to alleviate the problem of exposure bias.\nExperiment results on the NIST Chinese-to-English translation tasks show that\nour method significantly outperforms the reinforcement-based algorithms and\nachieves an improvement of 1.5 BLEU points on average over a strong baseline\nsystem.", "published": "2018-09-10 04:41:44", "link": "http://arxiv.org/abs/1809.03132v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Short-Term Meaning Shift: A Distributional Exploration", "abstract": "We present the first exploration of meaning shift over short periods of time\nin online communities using distributional representations. We create a small\nannotated dataset and use it to assess the performance of a standard model for\nmeaning shift detection on short-term meaning shift. We find that the model has\nproblems distinguishing meaning shift from referential phenomena, and propose a\nmeasure of contextual variability to remedy this.", "published": "2018-09-10 08:05:56", "link": "http://arxiv.org/abs/1809.03169v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards one-shot learning for rare-word translation with external\n  experts", "abstract": "Neural machine translation (NMT) has significantly improved the quality of\nautomatic translation models. One of the main challenges in current systems is\nthe translation of rare words. We present a generic approach to address this\nweakness by having external models annotate the training data as Experts, and\ncontrol the model-expert interaction with a pointer network and reinforcement\nlearning. Our experiments using phrase-based models to simulate Experts to\ncomplement neural machine translation models show that the model can be trained\nto copy the annotations into the output consistently. We demonstrate the\nbenefit of our proposed framework in outof-domain translation scenarios with\nonly lexical resources, improving more than 1.0 BLEU point in both translation\ndirections English to Spanish and German to English", "published": "2018-09-10 08:40:04", "link": "http://arxiv.org/abs/1809.03182v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Generate Structured Queries from Natural Language with\n  Indirect Supervision", "abstract": "Generating structured query language (SQL) from natural language is an\nemerging research topic. This paper presents a new learning paradigm from\nindirect supervision of the answers to natural language questions, instead of\nSQL queries. This paradigm facilitates the acquisition of training data due to\nthe abundant resources of question-answer pairs for various domains in the\nInternet, and expels the difficult SQL annotation job. An end-to-end neural\nmodel integrating with reinforcement learning is proposed to learn SQL\ngeneration policy within the answer-driven learning paradigm. The model is\nevaluated on datasets of different domains, including movie and academic\npublication. Experimental results show that our model outperforms the baseline\nmodels.", "published": "2018-09-10 09:10:49", "link": "http://arxiv.org/abs/1809.03195v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards JointUD: Part-of-speech Tagging and Lemmatization using\n  Recurrent Neural Networks", "abstract": "This paper describes our submission to CoNLL 2018 UD Shared Task. We have\nextended an LSTM-based neural network designed for sequence tagging to\nadditionally generate character-level sequences. The network was jointly\ntrained to produce lemmas, part-of-speech tags and morphological features.\nSentence segmentation, tokenization and dependency parsing were handled by\nUDPipe 1.2 baseline. The results demonstrate the viability of the proposed\nmultitask architecture, although its performance still remains far from\nstate-of-the-art.", "published": "2018-09-10 09:31:24", "link": "http://arxiv.org/abs/1809.03211v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Extractive Reading Comprehension by Runtime Machine\n  Translation", "abstract": "Despite recent work in Reading Comprehension (RC), progress has been mostly\nlimited to English due to the lack of large-scale datasets in other languages.\nIn this work, we introduce the first RC system for languages without RC\ntraining data. Given a target language without RC training data and a pivot\nlanguage with RC training data (e.g. English), our method leverages existing RC\nresources in the pivot language by combining a competitive RC model in the\npivot language with an attentive Neural Machine Translation (NMT) model. We\nfirst translate the data from the target to the pivot language, and then obtain\nan answer using the RC model in the pivot language. Finally, we recover the\ncorresponding answer in the original language using soft-alignment attention\nscores from the NMT model. We create evaluation sets of RC data in two\nnon-English languages, namely Japanese and French, to evaluate our method.\nExperimental results on these datasets show that our method significantly\noutperforms a back-translation baseline of a state-of-the-art product-level\nmachine translation system.", "published": "2018-09-10 12:41:21", "link": "http://arxiv.org/abs/1809.03275v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "xSense: Learning Sense-Separated Sparse Representations and Textual\n  Definitions for Explainable Word Sense Networks", "abstract": "Despite the success achieved on various natural language processing tasks,\nword embeddings are difficult to interpret due to the dense vector\nrepresentations. This paper focuses on interpreting the embeddings for various\naspects, including sense separation in the vector dimensions and definition\ngeneration. Specifically, given a context together with a target word, our\nalgorithm first projects the target word embedding to a high-dimensional sparse\nvector and picks the specific dimensions that can best explain the semantic\nmeaning of the target word by the encoded contextual information, where the\nsense of the target word can be indirectly inferred. Finally, our algorithm\napplies an RNN to generate the textual definition of the target word in the\nhuman readable form, which enables direct interpretation of the corresponding\nword embedding. This paper also introduces a large and high-quality\ncontext-definition dataset that consists of sense definitions together with\nmultiple example sentences per polysemous word, which is a valuable resource\nfor definition modeling and word sense disambiguation. The conducted\nexperiments show the superior performance in BLEU score and the human\nevaluation test.", "published": "2018-09-10 14:27:08", "link": "http://arxiv.org/abs/1809.03348v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Toward a Standardized and More Accurate Indonesian Part-of-Speech\n  Tagging", "abstract": "Previous work in Indonesian part-of-speech (POS) tagging are hard to compare\nas they are not evaluated on a common dataset. Furthermore, in spite of the\nsuccess of neural network models for English POS tagging, they are rarely\nexplored for Indonesian. In this paper, we explored various techniques for\nIndonesian POS tagging, including rule-based, CRF, and neural network-based\nmodels. We evaluated our models on the IDN Tagged Corpus. A new\nstate-of-the-art of 97.47 F1 score is achieved with a recurrent neural network.\nTo provide a standard for future work, we release the dataset split that we\nused publicly.", "published": "2018-09-10 15:23:48", "link": "http://arxiv.org/abs/1809.03391v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Latent Relational Analysis to Capture Lexical Semantic Relations\n  in a Vector Space", "abstract": "Capturing the semantic relations of words in a vector space contributes to\nmany natural language processing tasks. One promising approach exploits\nlexico-syntactic patterns as features of word pairs. In this paper, we propose\na novel model of this pattern-based approach, neural latent relational analysis\n(NLRA). NLRA can generalize co-occurrences of word pairs and lexico-syntactic\npatterns, and obtain embeddings of the word pairs that do not co-occur. This\novercomes the critical data sparseness problem encountered in previous\npattern-based models. Our experimental results on measuring relational\nsimilarity demonstrate that NLRA outperforms the previous pattern-based models.\nIn addition, when combined with a vector offset model, NLRA achieves a\nperformance comparable to that of the state-of-the-art model that exploits\nadditional semantic relational data.", "published": "2018-09-10 15:37:30", "link": "http://arxiv.org/abs/1809.03401v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Filling Missing Paths: Modeling Co-occurrences of Word Pairs and\n  Dependency Paths for Recognizing Lexical Semantic Relations", "abstract": "Recognizing lexical semantic relations between word pairs is an important\ntask for many applications of natural language processing. One of the\nmainstream approaches to this task is to exploit the lexico-syntactic paths\nconnecting two target words, which reflect the semantic relations of word\npairs. However, this method requires that the considered words co-occur in a\nsentence. This requirement is hardly satisfied because of Zipf's law, which\nstates that most content words occur very rarely. In this paper, we propose\nnovel methods with a neural model of $P(path|w_1, w_2)$ to solve this problem.\nOur proposed model of $P(path|w_1, w_2)$ can be learned in an unsupervised\nmanner and can generalize the co-occurrences of word pairs and dependency\npaths. This model can be used to augment the path data of word pairs that do\nnot co-occur in the corpus, and extract features capturing relational\ninformation from word pairs. Our experimental results demonstrate that our\nmethods improve on previous neural approaches based on dependency paths and\nsuccessfully solve the focused problem.", "published": "2018-09-10 15:47:37", "link": "http://arxiv.org/abs/1809.03411v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-view Models for Political Ideology Detection of News Articles", "abstract": "A news article's title, content and link structure often reveal its political\nideology. However, most existing works on automatic political ideology\ndetection only leverage textual cues. Drawing inspiration from recent advances\nin neural inference, we propose a novel attention based multi-view model to\nleverage cues from all of the above views to identify the ideology evinced by a\nnews article. Our model draws on advances in representation learning in natural\nlanguage processing and network science to capture cues from both textual\ncontent and the network structure of news articles. We empirically evaluate our\nmodel against a battery of baselines and show that our model outperforms state\nof the art by 10 percentage points F1 score.", "published": "2018-09-10 17:57:10", "link": "http://arxiv.org/abs/1809.03485v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Named Entity Tagger using Domain-Specific Dictionary", "abstract": "Recent advances in deep neural models allow us to build reliable named entity\nrecognition (NER) systems without handcrafting features. However, such methods\nrequire large amounts of manually-labeled training data. There have been\nefforts on replacing human annotations with distant supervision (in conjunction\nwith external dictionaries), but the generated noisy labels pose significant\nchallenges on learning effective neural models. Here we propose two neural\nmodels to suit noisy distant supervision from the dictionary. First, under the\ntraditional sequence labeling framework, we propose a revised fuzzy CRF layer\nto handle tokens with multiple possible labels. After identifying the nature of\nnoisy labels in distant supervision, we go beyond the traditional framework and\npropose a novel, more effective neural model AutoNER with a new Tie or Break\nscheme. In addition, we discuss how to refine distant supervision for better\nNER performance. Extensive experiments on three benchmark datasets demonstrate\nthat AutoNER achieves the best performance when only using dictionaries with no\nadditional human effort, and delivers competitive results with state-of-the-art\nsupervised benchmarks.", "published": "2018-09-10 21:15:30", "link": "http://arxiv.org/abs/1809.03599v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Gang-Involved Escalation on Social Media Using Context", "abstract": "Gang-involved youth in cities such as Chicago have increasingly turned to\nsocial media to post about their experiences and intents online. In some\nsituations, when they experience the loss of a loved one, their online\nexpression of emotion may evolve into aggression towards rival gangs and\nultimately into real-world violence. In this paper, we present a novel system\nfor detecting Aggression and Loss in social media. Our system features the use\nof domain-specific resources automatically derived from a large unlabeled\ncorpus, and contextual representations of the emotional and semantic content of\nthe user's recent tweets as well as their interactions with other users.\nIncorporating context in our Convolutional Neural Network (CNN) leads to a\nsignificant improvement.", "published": "2018-09-10 23:20:00", "link": "http://arxiv.org/abs/1809.03632v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Cross-lingual Transfer of Word Embedding Spaces", "abstract": "Cross-lingual transfer of word embeddings aims to establish the semantic\nmappings among words in different languages by learning the transformation\nfunctions over the corresponding word embedding spaces. Successfully solving\nthis problem would benefit many downstream tasks such as to translate text\nclassification models from resource-rich languages (e.g. English) to\nlow-resource languages. Supervised methods for this problem rely on the\navailability of cross-lingual supervision, either using parallel corpora or\nbilingual lexicons as the labeled data for training, which may not be available\nfor many low resource languages. This paper proposes an unsupervised learning\napproach that does not require any cross-lingual labeled data. Given two\nmonolingual word embedding spaces for any language pair, our algorithm\noptimizes the transformation functions in both directions simultaneously based\non distributional matching as well as minimizing the back-translation losses.\nWe use a neural network implementation to calculate the Sinkhorn distance, a\nwell-defined distributional similarity measure, and optimize our objective\nthrough back-propagation. Our evaluation on benchmark datasets for bilingual\nlexicon induction and cross-lingual word similarity prediction shows stronger\nor competitive performance of the proposed method compared to other\nstate-of-the-art supervised and unsupervised baseline methods over many\nlanguage pairs.", "published": "2018-09-10 23:22:43", "link": "http://arxiv.org/abs/1809.03633v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Sequence Encoders for Temporal Knowledge Graph Completion", "abstract": "Research on link prediction in knowledge graphs has mainly focused on static\nmulti-relational data. In this work we consider temporal knowledge graphs where\nrelations between entities may only hold for a time interval or a specific\npoint in time. In line with previous work on static knowledge graphs, we\npropose to address this problem by learning latent entity and relation type\nrepresentations. To incorporate temporal information, we utilize recurrent\nneural networks to learn time-aware representations of relation types which can\nbe used in conjunction with existing latent factorization methods. The proposed\napproach is shown to be robust to common challenges in real-world KGs: the\nsparsity and heterogeneity of temporal expressions. Experiments show the\nbenefits of our approach on four temporal KGs. The data sets are available\nunder a permissive BSD-3 license 1.", "published": "2018-09-10 09:17:04", "link": "http://arxiv.org/abs/1809.03202v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Beyond task success: A closer look at jointly learning to see, ask, and\n  GuessWhat", "abstract": "We propose a grounded dialogue state encoder which addresses a foundational\nissue on how to integrate visual grounding with dialogue system components. As\na test-bed, we focus on the GuessWhat?! game, a two-player game where the goal\nis to identify an object in a complex visual scene by asking a sequence of\nyes/no questions. Our visually-grounded encoder leverages synergies between\nguessing and asking questions, as it is trained jointly using multi-task\nlearning. We further enrich our model via a cooperative learning regime. We\nshow that the introduction of both the joint architecture and cooperative\nlearning lead to accuracy improvements over the baseline system. We compare our\napproach to an alternative system which extends the baseline with reinforcement\nlearning. Our in-depth analysis shows that the linguistic skills of the two\nmodels differ dramatically, despite approaching comparable performance levels.\nThis points at the importance of analyzing the linguistic output of competing\nsystems beyond numeric comparison solely based on task success.", "published": "2018-09-10 15:46:58", "link": "http://arxiv.org/abs/1809.03408v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Explicit Utilization of General Knowledge in Machine Reading\n  Comprehension", "abstract": "To bridge the gap between Machine Reading Comprehension (MRC) models and\nhuman beings, which is mainly reflected in the hunger for data and the\nrobustness to noise, in this paper, we explore how to integrate the neural\nnetworks of MRC models with the general knowledge of human beings. On the one\nhand, we propose a data enrichment method, which uses WordNet to extract\ninter-word semantic connections as general knowledge from each given\npassage-question pair. On the other hand, we propose an end-to-end MRC model\nnamed as Knowledge Aided Reader (KAR), which explicitly uses the above\nextracted general knowledge to assist its attention mechanisms. Based on the\ndata enrichment method, KAR is comparable in performance with the\nstate-of-the-art MRC models, and significantly more robust to noise than them.\nWhen only a subset (20%-80%) of the training examples are available, KAR\noutperforms the state-of-the-art MRC models by a large margin, and is still\nreasonably robust to noise.", "published": "2018-09-10 16:42:22", "link": "http://arxiv.org/abs/1809.03449v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Unsupervised Controllable Text Formalization", "abstract": "We propose a novel framework for controllable natural language\ntransformation. Realizing that the requirement of parallel corpus is\npractically unsustainable for controllable generation tasks, an unsupervised\ntraining scheme is introduced. The crux of the framework is a deep neural\nencoder-decoder that is reinforced with text-transformation knowledge through\nauxiliary modules (called scorers). The scorers, based on off-the-shelf\nlanguage processing tools, decide the learning scheme of the encoder-decoder\nbased on its actions. We apply this framework for the text-transformation task\nof formalizing an input text by improving its readability grade; the degree of\nrequired formalization can be controlled by the user at run-time. Experiments\non public datasets demonstrate the efficacy of our model towards: (a)\ntransforming a given text to a more formal style, and (b) introducing\nappropriate amount of formalness in the output text pertaining to the input\ncontrol. Our code and datasets are released for academic use.", "published": "2018-09-10 17:25:46", "link": "http://arxiv.org/abs/1809.04556v6", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Response Selection in Multi-Turn Dialogue Systems by\n  Incorporating Domain Knowledge", "abstract": "Building systems that can communicate with humans is a core problem in\nArtificial Intelligence. This work proposes a novel neural network architecture\nfor response selection in an end-to-end multi-turn conversational dialogue\nsetting. The architecture applies context level attention and incorporates\nadditional external knowledge provided by descriptions of domain-specific\nwords. It uses a bi-directional Gated Recurrent Unit (GRU) for encoding context\nand responses and learns to attend over the context words given the latent\nresponse representation and vice versa.In addition, it incorporates external\ndomain specific information using another GRU for encoding the domain keyword\ndescriptions. This allows better representation of domain-specific keywords in\nresponses and hence improves the overall performance. Experimental results show\nthat our model outperforms all other state-of-the-art methods for response\nselection in multi-turn conversations.", "published": "2018-09-10 09:10:25", "link": "http://arxiv.org/abs/1809.03194v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Identifying Relationships Among Sentences in Court Case Transcripts\n  Using Discourse Relations", "abstract": "Case Law has a significant impact on the proceedings of legal cases.\nTherefore, the information that can be obtained from previous court cases is\nvaluable to lawyers and other legal officials when performing their duties.\nThis paper describes a methodology of applying discourse relations between\nsentences when processing text documents related to the legal domain. In this\nstudy, we developed a mechanism to classify the relationships that can be\nobserved among sentences in transcripts of United States court cases. First, we\ndefined relationship types that can be observed between sentences in court case\ntranscripts. Then we classified pairs of sentences according to the\nrelationship type by combining a machine learning model and a rule-based\napproach. The results obtained through our system were evaluated using human\njudges. To the best of our knowledge, this is the first study where discourse\nrelationships between sentences have been used to determine relationships among\nsentences in legal court case transcripts.", "published": "2018-09-10 15:55:15", "link": "http://arxiv.org/abs/1809.03416v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
