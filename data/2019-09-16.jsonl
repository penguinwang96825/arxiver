{"title": "CM-Net: A Novel Collaborative Memory Network for Spoken Language\n  Understanding", "abstract": "Spoken Language Understanding (SLU) mainly involves two tasks, intent\ndetection and slot filling, which are generally modeled jointly in existing\nworks. However, most existing models fail to fully utilize co-occurrence\nrelations between slots and intents, which restricts their potential\nperformance. To address this issue, in this paper we propose a novel\nCollaborative Memory Network (CM-Net) based on the well-designed block, named\nCM-block. The CM-block firstly captures slot-specific and intent-specific\nfeatures from memories in a collaborative manner, and then uses these enriched\nfeatures to enhance local context representations, based on which the\nsequential information flow leads to more specific (slot and intent) global\nutterance representations. Through stacking multiple CM-blocks, our CM-Net is\nable to alternately perform information exchange among specific memories, local\ncontexts and the global utterance, and thus incrementally enriches each other.\nWe evaluate the CM-Net on two standard benchmarks (ATIS and SNIPS) and a\nself-collected corpus (CAIS). Experimental results show that the CM-Net\nachieves the state-of-the-art results on the ATIS and SNIPS in most of\ncriteria, and significantly outperforms the baseline models on the CAIS.\nAdditionally, we make the CAIS dataset publicly available for the research\ncommunity.", "published": "2019-09-16 02:10:58", "link": "http://arxiv.org/abs/1909.06937v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KorQuAD1.0: Korean QA Dataset for Machine Reading Comprehension", "abstract": "Machine Reading Comprehension (MRC) is a task that requires machine to\nunderstand natural language and answer questions by reading a document. It is\nthe core of automatic response technology such as chatbots and automatized\ncustomer supporting systems. We present Korean Question Answering\nDataset(KorQuAD), a large-scale Korean dataset for extractive machine reading\ncomprehension task. It consists of 70,000+ human generated question-answer\npairs on Korean Wikipedia articles. We release KorQuAD1.0 and launch a\nchallenge at https://KorQuAD.github.io to encourage the development of\nmultilingual natural language processing research.", "published": "2019-09-16 06:15:27", "link": "http://arxiv.org/abs/1909.07005v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bridging the domain gap in cross-lingual document classification", "abstract": "The scarcity of labeled training data often prohibits the\ninternationalization of NLP models to multiple languages. Recent developments\nin cross-lingual understanding (XLU) has made progress in this area, trying to\nbridge the language barrier using language universal representations. However,\neven if the language problem was resolved, models trained in one language would\nnot transfer to another language perfectly due to the natural domain drift\nacross languages and cultures. We consider the setting of semi-supervised\ncross-lingual understanding, where labeled data is available in a source\nlanguage (English), but only unlabeled data is available in the target\nlanguage. We combine state-of-the-art cross-lingual methods with recently\nproposed methods for weakly supervised learning such as unsupervised\npre-training and unsupervised data augmentation to simultaneously close both\nthe language gap and the domain gap in XLU. We show that addressing the domain\ngap is crucial. We improve over strong baselines and achieve a new\nstate-of-the-art for cross-lingual document classification.", "published": "2019-09-16 06:20:23", "link": "http://arxiv.org/abs/1909.07009v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Domain Transfer in Dialogue Systems without Turn-Level Supervision", "abstract": "Task oriented dialogue systems rely heavily on specialized dialogue state\ntracking (DST) modules for dynamically predicting user intent throughout the\nconversation. State-of-the-art DST models are typically trained in a supervised\nmanner from manual annotations at the turn level. However, these annotations\nare costly to obtain, which makes it difficult to create accurate dialogue\nsystems for new domains. To address these limitations, we propose a method,\nbased on reinforcement learning, for transferring DST models to new domains\nwithout turn-level supervision. Across several domains, our experiments show\nthat this method quickly adapts off-the-shelf models to new domains and\nperforms on par with models trained with turn-level supervision. We also show\nour method can improve models trained using turn-level supervision by\nsubsequent fine-tuning optimization toward dialog-level rewards.", "published": "2019-09-16 10:19:02", "link": "http://arxiv.org/abs/1909.07101v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fast transcription of speech in low-resource languages", "abstract": "We present software that, in only a few hours, transcribes forty hours of\nrecorded speech in a surprise language, using only a few tens of megabytes of\nnoisy text in that language, and a zero-resource grapheme to phoneme (G2P)\ntable. A pretrained acoustic model maps acoustic features to phonemes; a\nreversed G2P maps these to graphemes; then a language model maps these to a\nmost-likely grapheme sequence, i.e., a transcription. This software has worked\nsuccessfully with corpora in Arabic, Assam, Kinyarwanda, Russian, Sinhalese,\nSwahili, Tagalog, and Tamil.", "published": "2019-09-16 15:38:36", "link": "http://arxiv.org/abs/1909.07285v1", "categories": ["cs.CL", "68T10"], "primary_category": "cs.CL"}
{"title": "Communication-based Evaluation for Natural Language Generation", "abstract": "Natural language generation (NLG) systems are commonly evaluated using n-gram\noverlap measures (e.g. BLEU, ROUGE). These measures do not directly capture\nsemantics or speaker intentions, and so they often turn out to be misaligned\nwith our true goals for NLG. In this work, we argue instead for\ncommunication-based evaluations: assuming the purpose of an NLG system is to\nconvey information to a reader/listener, we can directly evaluate its\neffectiveness at this task using the Rational Speech Acts model of pragmatic\nlanguage use. We illustrate with a color reference dataset that contains\ndescriptions in pre-defined quality categories, showing that our method better\naligns with these quality categories than do any of the prominent n-gram\noverlap methods.", "published": "2019-09-16 15:42:36", "link": "http://arxiv.org/abs/1909.07290v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Neural Machine Translation for Zero-Resource Languages", "abstract": "In recent years, Neural Machine Translation (NMT) has been shown to be more\neffective than phrase-based statistical methods, thus quickly becoming the\nstate of the art in machine translation (MT). However, NMT systems are limited\nin translating low-resourced languages, due to the significant amount of\nparallel data that is required to learn useful mappings between languages. In\nthis work, we show how the so-called multilingual NMT can help to tackle the\nchallenges associated with low-resourced language translation. The underlying\nprinciple of multilingual NMT is to force the creation of hidden\nrepresentations of words in a shared semantic space across multiple languages,\nthus enabling a positive parameter transfer across languages. Along this\ndirection, we present multilingual translation experiments with three languages\n(English, Italian, Romanian) covering six translation directions, utilizing\nboth recurrent neural networks and transformer (or self-attentive) neural\nnetworks. We then focus on the zero-shot translation problem, that is how to\nleverage multi-lingual data in order to learn translation directions that are\nnot covered by the available training material. To this aim, we introduce our\nrecently proposed iterative self-training method, which incrementally improves\na multilingual NMT on a zero-shot direction by just relying on monolingual\ndata. Our results on TED talks data show that multilingual NMT outperforms\nconventional bilingual NMT, that the transformer NMT outperforms recurrent NMT,\nand that zero-shot NMT outperforms conventional pivoting methods and even\nmatches the performance of a fully-trained bilingual system.", "published": "2019-09-16 17:22:25", "link": "http://arxiv.org/abs/1909.07342v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BottleSum: Unsupervised and Self-supervised Sentence Summarization using\n  the Information Bottleneck Principle", "abstract": "The principle of the Information Bottleneck (Tishby et al. 1999) is to\nproduce a summary of information X optimized to predict some other relevant\ninformation Y. In this paper, we propose a novel approach to unsupervised\nsentence summarization by mapping the Information Bottleneck principle to a\nconditional language modelling objective: given a sentence, our approach seeks\na compressed sentence that can best predict the next sentence. Our iterative\nalgorithm under the Information Bottleneck objective searches gradually shorter\nsubsequences of the given sentence while maximizing the probability of the next\nsentence conditioned on the summary. Using only pretrained language models with\nno direct supervision, our approach can efficiently perform extractive sentence\nsummarization over a large corpus.\n  Building on our unsupervised extractive summarization (BottleSumEx), we then\npresent a new approach to self-supervised abstractive summarization\n(BottleSumSelf), where a transformer-based language model is trained on the\noutput summaries of our unsupervised method. Empirical results demonstrate that\nour extractive method outperforms other unsupervised models on multiple\nautomatic metrics. In addition, we find that our self-supervised abstractive\nmodel outperforms unsupervised baselines (including our own) by human\nevaluation along multiple attributes.", "published": "2019-09-16 18:00:24", "link": "http://arxiv.org/abs/1909.07405v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Short-Text Classification Using Unsupervised Keyword Expansion", "abstract": "Short-text classification, like all data science, struggles to achieve high\nperformance using limited data. As a solution, a short sentence may be expanded\nwith new and relevant feature words to form an artificially enlarged dataset,\nand add new features to testing data. This paper applies a novel approach to\ntext expansion by generating new words directly for each input sentence, thus\nrequiring no additional datasets or previous training. In this unsupervised\napproach, new keywords are formed within the hidden states of a pre-trained\nlanguage model and then used to create extended pseudo documents. The word\ngeneration process was assessed by examining how well the predicted words\nmatched to topics of the input sentence. It was found that this method could\nproduce 3-10 relevant new words for each target topic, while generating just 1\nword related to each non-target topic. Generated words were then added to short\nnews headlines to create extended pseudo headlines. Experimental results have\nshown that models trained using the pseudo headlines can improve classification\naccuracy when limiting the number of training examples.", "published": "2019-09-16 22:51:29", "link": "http://arxiv.org/abs/1909.07512v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Probing Natural Language Inference Models through Semantic Fragments", "abstract": "Do state-of-the-art models for language understanding already have, or can\nthey easily learn, abilities such as boolean coordination, quantification,\nconditionals, comparatives, and monotonicity reasoning (i.e., reasoning about\nword substitutions in sentential contexts)? While such phenomena are involved\nin natural language inference (NLI) and go beyond basic linguistic\nunderstanding, it is unclear the extent to which they are captured in existing\nNLI benchmarks and effectively learned by models. To investigate this, we\npropose the use of semantic fragments---systematically generated datasets that\neach target a different semantic phenomenon---for probing, and efficiently\nimproving, such capabilities of linguistic models. This approach to creating\nchallenge datasets allows direct control over the semantic diversity and\ncomplexity of the targeted linguistic phenomena, and results in a more precise\ncharacterization of a model's linguistic behavior. Our experiments, using a\nlibrary of 8 such semantic fragments, reveal two remarkable findings: (a)\nState-of-the-art models, including BERT, that are pre-trained on existing NLI\nbenchmark datasets perform poorly on these new fragments, even though the\nphenomena probed here are central to the NLI task. (b) On the other hand, with\nonly a few minutes of additional fine-tuning---with a carefully selected\nlearning rate and a novel variation of \"inoculation\"---a BERT-based model can\nmaster all of these logic and monotonicity fragments while retaining its\nperformance on established NLI benchmarks.", "published": "2019-09-16 23:44:49", "link": "http://arxiv.org/abs/1909.07521v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic detection of surgical site infections from a clinical data\n  warehouse", "abstract": "Reducing the incidence of surgical site infections (SSIs) is one of the\nobjectives of the French nosocomial infection control program. Manual\nmonitoring of SSIs is carried out each year by the hospital hygiene team and\nsurgeons at the University Hospital of Bordeaux. Our goal was to develop an\nautomatic detection algorithm based on hospital information system data. Three\nyears (2015, 2016 and 2017) of manual spine surgery monitoring have been used\nas a gold standard to extract features and train machine learning algorithms.\nThe dataset contained 22 SSIs out of 2133 spine surgeries. Two different\napproaches were compared. The first used several data sources and achieved the\nbest performance but is difficult to generalize to other institutions. The\nsecond was based on free text only with semiautomatic extraction of\ndiscriminant terms. The algorithms managed to identify all the SSIs with 20 and\n26 false positives respectively on the dataset. Another evaluation is underway.\nThese results are encouraging for the development of semi-automated\nsurveillance methods.", "published": "2019-09-16 08:31:20", "link": "http://arxiv.org/abs/1909.07054v1", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Global Autoregressive Models for Data-Efficient Sequence Learning", "abstract": "Standard autoregressive seq2seq models are easily trained by max-likelihood,\nbut tend to show poor results under small-data conditions. We introduce a class\nof seq2seq models, GAMs (Global Autoregressive Models), which combine an\nautoregressive component with a log-linear component, allowing the use of\nglobal \\textit{a priori} features to compensate for lack of data. We train\nthese models in two steps. In the first step, we obtain an \\emph{unnormalized}\nGAM that maximizes the likelihood of the data, but is improper for fast\ninference or evaluation. In the second step, we use this GAM to train (by\ndistillation) a second autoregressive model that approximates the\n\\emph{normalized} distribution associated with the GAM, and can be used for\nfast inference and evaluation. Our experiments focus on language modelling\nunder synthetic conditions and show a strong perplexity reduction of using the\nsecond autoregressive model over the standard one.", "published": "2019-09-16 08:46:30", "link": "http://arxiv.org/abs/1909.07063v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Controllable Text-to-Image Generation", "abstract": "In this paper, we propose a novel controllable text-to-image generative\nadversarial network (ControlGAN), which can effectively synthesise high-quality\nimages and also control parts of the image generation according to natural\nlanguage descriptions. To achieve this, we introduce a word-level spatial and\nchannel-wise attention-driven generator that can disentangle different visual\nattributes, and allow the model to focus on generating and manipulating\nsubregions corresponding to the most relevant words. Also, a word-level\ndiscriminator is proposed to provide fine-grained supervisory feedback by\ncorrelating words with image regions, facilitating training an effective\ngenerator which is able to manipulate specific visual attributes without\naffecting the generation of other content. Furthermore, perceptual loss is\nadopted to reduce the randomness involved in the image generation, and to\nencourage the generator to manipulate specific attributes required in the\nmodified text. Extensive experiments on benchmark datasets demonstrate that our\nmethod outperforms existing state of the art, and is able to effectively\nmanipulate synthetic images using natural language descriptions. Code is\navailable at https://github.com/mrlibw/ControlGAN.", "published": "2019-09-16 09:29:52", "link": "http://arxiv.org/abs/1909.07083v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Prediction Uncertainty Estimation for Hate Speech Classification", "abstract": "As a result of social network popularity, in recent years, hate speech\nphenomenon has significantly increased. Due to its harmful effect on minority\ngroups as well as on large communities, there is a pressing need for hate\nspeech detection and filtering. However, automatic approaches shall not\njeopardize free speech, so they shall accompany their decisions with\nexplanations and assessment of uncertainty. Thus, there is a need for\npredictive machine learning models that not only detect hate speech but also\nhelp users understand when texts cross the line and become unacceptable. The\nreliability of predictions is usually not addressed in text classification. We\nfill this gap by proposing the adaptation of deep neural networks that can\nefficiently estimate prediction uncertainty. To reliably detect hate speech, we\nuse Monte Carlo dropout regularization, which mimics Bayesian inference within\nneural networks. We evaluate our approach using different text embedding\nmethods. We visualize the reliability of results with a novel technique that\naids in understanding the classification reliability and errors.", "published": "2019-09-16 12:43:17", "link": "http://arxiv.org/abs/1909.07158v3", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Automatic Detection and Classification of Cognitive Distortions in\n  Mental Health Text", "abstract": "In cognitive psychology, automatic and self-reinforcing irrational thought\npatterns are known as cognitive distortions. Left unchecked, patients\nexhibiting these types of thoughts can become stuck in negative feedback loops\nof unhealthy thinking, leading to inaccurate perceptions of reality commonly\nassociated with anxiety and depression. In this paper, we present a machine\nlearning framework for the automatic detection and classification of 15 common\ncognitive distortions in two novel mental health free text datasets collected\nfrom both crowdsourcing and a real-world online therapy program. When\ndifferentiating between distorted and non-distorted passages, our model\nachieved a weighted F1 score of 0.88. For classifying distorted passages into\none of 15 distortion categories, our model yielded weighted F1 scores of 0.68\nin the larger crowdsourced dataset and 0.45 in the smaller online counseling\ndataset, both of which outperformed random baseline metrics by a large margin.\nFor both tasks, we also identified the most discriminative words and phrases\nbetween classes to highlight common thematic elements for improving targeted\nand therapist-guided mental health treatment. Furthermore, we performed an\nexploratory analysis using unsupervised content-based clustering and topic\nmodeling algorithms as first efforts towards a data-driven perspective on the\nthematic relationship between similar cognitive distortions traditionally\ndeemed unique. Finally, we highlight the difficulties in applying mental\nhealth-based machine learning in a real-world setting and comment on the\nimplications and benefits of our framework for improving automated delivery of\ntherapeutic treatment in conjunction with traditional cognitive-behavioral\ntherapy.", "published": "2019-09-16 22:21:27", "link": "http://arxiv.org/abs/1909.07502v2", "categories": ["cs.HC", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.HC"}
{"title": "Discovering Differential Features: Adversarial Learning for Information\n  Credibility Evaluation", "abstract": "A series of deep learning approaches extract a large number of credibility\nfeatures to detect fake news on the Internet. However, these extracted features\nstill suffer from many irrelevant and noisy features that restrict severely the\nperformance of the approaches. In this paper, we propose a novel model based on\nAdversarial Networks and inspirited by the Shared-Private model (ANSP), which\naims at reducing common, irrelevant features from the extracted features for\ninformation credibility evaluation. Specifically, ANSP involves two tasks: one\nis to prevent the binary classification of true and false information for\ncapturing common features relying on adversarial networks guided by\nreinforcement learning. Another extracts credibility features (henceforth,\nprivate features) from multiple types of credibility information and compares\nwith the common features through two strategies, i.e., orthogonality\nconstraints and KL-divergence for making the private features more\ndifferential. Experiments first on two six-label LIAR and Weibo datasets\ndemonstrate that ANSP achieves the state-of-the-art performance, boosting the\naccuracy by 2.1%, 3.1%, respectively and then on four-label Twitter16 validate\nthe robustness of the model with 1.8% performance improvements.", "published": "2019-09-16 23:53:59", "link": "http://arxiv.org/abs/1909.07523v1", "categories": ["cs.CY", "cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CY"}
{"title": "Alternative Visual Units for an Optimized Phoneme-Based Lipreading\n  System", "abstract": "Lipreading is understanding speech from observed lip movements. An observed\nseries of lip motions is an ordered sequence of visual lip gestures. These\ngestures are commonly known, but as yet are not formally defined, as `visemes'.\nIn this article, we describe a structured approach which allows us to create\nspeaker-dependent visemes with a fixed number of visemes within each set. We\ncreate sets of visemes for sizes two to 45. Each set of visemes is based upon\nclustering phonemes, thus each set has a unique phoneme-to-viseme mapping. We\nfirst present an experiment using these maps and the Resource Management\nAudio-Visual (RMAV) dataset which shows the effect of changing the viseme map\nsize in speaker-dependent machine lipreading and demonstrate that word\nrecognition with phoneme classifiers is possible. Furthermore, we show that\nthere are intermediate units between visemes and phonemes which are better\nstill. Second, we present a novel two-pass training scheme for phoneme\nclassifiers. This approach uses our new intermediary visual units from our\nfirst experiment in the first pass as classifiers; before using the\nphoneme-to-viseme maps, we retrain these into phoneme classifiers. This method\nsignificantly improves on previous lipreading results with RMAV speakers.", "published": "2019-09-16 12:20:54", "link": "http://arxiv.org/abs/1909.07147v1", "categories": ["eess.IV", "eess.AS"], "primary_category": "eess.IV"}
{"title": "MFCC-based Recurrent Neural Network for Automatic Clinical Depression\n  Recognition and Assessment from Speech", "abstract": "Clinical depression or Major Depressive Disorder (MDD) is a common and\nserious medical illness. In this paper, a deep recurrent neural network-based\nframework is presented to detect depression and to predict its severity level\nfrom speech. Low-level and high-level audio features are extracted from audio\nrecordings to predict the 24 scores of the Patient Health Questionnaire and the\nbinary class of depression diagnosis. To overcome the problem of the small size\nof Speech Depression Recognition (SDR) datasets, expanding training labels and\ntransferred features are considered. The proposed approach outperforms the\nstate-of-art approaches on the DAIC-WOZ database with an overall accuracy of\n76.27% and a root mean square error of 0.4 in assessing depression, while a\nroot mean square error of 0.168 is achieved in predicting the depression\nseverity levels. The proposed framework has several advantages (fastness,\nnon-invasiveness, and non-intrusion), which makes it convenient for real-time\napplications. The performances of the proposed approach are evaluated under a\nmulti-modal and a multi-features experiments. MFCC based high-level features\nhold relevant information related to depression. Yet, adding visual action\nunits and different other acoustic features further boosts the classification\nresults by 20% and 10% to reach an accuracy of 95.6% and 86%, respectively.\nConsidering visual-facial modality needs to be carefully studied as it sparks\npatient privacy concerns while adding more acoustic features increases the\ncomputation time.", "published": "2019-09-16 14:03:01", "link": "http://arxiv.org/abs/1909.07208v2", "categories": ["cs.HC", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.HC"}
{"title": "Audio-Visual Speech Separation and Dereverberation with a Two-Stage\n  Multimodal Network", "abstract": "Background noise, interfering speech and room reverberation frequently\ndistort target speech in real listening environments. In this study, we address\njoint speech separation and dereverberation, which aims to separate target\nspeech from background noise, interfering speech and room reverberation. In\norder to tackle this fundamentally difficult problem, we propose a novel\nmultimodal network that exploits both audio and visual signals. The proposed\nnetwork architecture adopts a two-stage strategy, where a separation module is\nemployed to attenuate background noise and interfering speech in the first\nstage and a dereverberation module to suppress room reverberation in the second\nstage. The two modules are first trained separately, and then integrated for\njoint training, which is based on a new multi-objective loss function. Our\nexperimental results show that the proposed multimodal network yields\nconsistently better objective intelligibility and perceptual quality than\nseveral one-stage and two-stage baselines. We find that our network achieves a\n21.10% improvement in ESTOI and a 0.79 improvement in PESQ over the unprocessed\nmixtures. Moreover, our network architecture does not require the knowledge of\nthe number of speakers.", "published": "2019-09-16 17:37:43", "link": "http://arxiv.org/abs/1909.07352v4", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Musical Instrument Classification via Low-Dimensional Feature Vectors", "abstract": "Music is a mysterious language that conveys feeling and thoughts via\ndifferent tones and timbre. For better understanding of timbre in music, we\nchose music data of 6 representative instruments, analysed their timbre\nfeatures and classified them. Instead of the current trend of Neural Network\nfor black-box classification, our project is based on a combination of MFCC and\nLPC, and augmented with a 6-dimensional feature vector designed by ourselves\nfrom observation and attempts. In our white-box model, we observed significant\npatterns of sound that distinguish different timbres, and discovered some\nconnection between objective data and subjective senses. With a totally\n32-dimensional feature vector and a naive all-pairs SVM, we achieved improved\nclassification accuracy compared to a single tool. We also attempted to analyze\nmusic pieces downloaded from the Internet, found out different performance on\ndifferent instruments, explored the reasons and suggested possible ways to\nimprove the performance.", "published": "2019-09-16 19:54:49", "link": "http://arxiv.org/abs/1909.08444v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Acoustic scene analysis with multi-head attention networks", "abstract": "Acoustic Scene Classification (ASC) is a challenging task, as a single scene\nmay involve multiple events that contain complex sound patterns. For example, a\ncooking scene may contain several sound sources including silverware clinking,\nchopping, frying, etc. What complicates ASC more is that classes of different\nactivities could have overlapping sounds patterns (e.g. both cooking and\ndishwashing could have silverware clinking sound). In this paper, we propose a\nmulti-head attention network to model the complex temporal input structures for\nASC. The proposed network takes the audio's time-frequency representation as\ninput, and it leverages standard VGG plus LSTM layers to extract high-level\nfeature representation. Further more, it applies multiple attention heads to\nsummarize various patterns of sound events into fixed dimensional\nrepresentation, for the purpose of final scene classification. The whole\nnetwork is trained in an end-to-end fashion with back-propagation. Experimental\nresults confirm that our model discovers meaningful sound patterns through the\nattention mechanism, without using explicit supervision in the alignment. We\nevaluated our proposed model using DCASE 2018 Task 5 dataset, and achieved\ncompetitive performance on par with previous winner's results.", "published": "2019-09-16 14:53:18", "link": "http://arxiv.org/abs/1909.08961v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
