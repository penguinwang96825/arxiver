{"title": "AI for Biomedicine in the Era of Large Language Models", "abstract": "The capabilities of AI for biomedicine span a wide spectrum, from the atomic\nlevel, where it solves partial differential equations for quantum systems, to\nthe molecular level, predicting chemical or protein structures, and further\nextending to societal predictions like infectious disease outbreaks. Recent\nadvancements in large language models, exemplified by models like ChatGPT, have\nshowcased significant prowess in natural language tasks, such as translating\nlanguages, constructing chatbots, and answering questions. When we consider\nbiomedical data, we observe a resemblance to natural language in terms of\nsequences: biomedical literature and health records presented as text,\nbiological sequences or sequencing data arranged in sequences, or sensor data\nlike brain signals as time series. The question arises: Can we harness the\npotential of recent large language models to drive biomedical knowledge\ndiscoveries? In this survey, we will explore the application of large language\nmodels to three crucial categories of biomedical data: 1) textual data, 2)\nbiological sequences, and 3) brain signals. Furthermore, we will delve into\nlarge language model challenges in biomedical research, including ensuring\ntrustworthiness, achieving personalization, and adapting to multi-modal data\nrepresentation", "published": "2024-03-23 01:40:22", "link": "http://arxiv.org/abs/2403.15673v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FEEL: A Framework for Evaluating Emotional Support Capability with Large\n  Language Models", "abstract": "Emotional Support Conversation (ESC) is a typical dialogue that can\neffectively assist the user in mitigating emotional pressures. However, owing\nto the inherent subjectivity involved in analyzing emotions, current\nnon-artificial methodologies face challenges in effectively appraising the\nemotional support capability. These metrics exhibit a low correlation with\nhuman judgments. Concurrently, manual evaluation methods extremely will cause\nhigh costs. To solve these problems, we propose a novel model FEEL (Framework\nfor Evaluating Emotional Support Capability with Large Lan-guage Models),\nemploying Large Language Models (LLMs) as evaluators to assess emotional\nsupport capabilities. The model meticulously considers various evaluative\naspects of ESC to apply a more comprehensive and accurate evaluation method for\nESC. Additionally, it employs a probability distribution approach for a more\nstable result and integrates an ensemble learning strategy, leveraging multiple\nLLMs with assigned weights to enhance evaluation accuracy. To appraise the\nperformance of FEEL, we conduct extensive experiments on existing ESC model\ndialogues. Experimental results demonstrate our model exhibits a substantial\nenhancement in alignment with human evaluations compared to the baselines. Our\nsource code is available at https://github.com/Ansisy/FEEL.", "published": "2024-03-23 03:32:26", "link": "http://arxiv.org/abs/2403.15699v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EDDA: A Encoder-Decoder Data Augmentation Framework for Zero-Shot Stance\n  Detection", "abstract": "Stance detection aims to determine the attitude expressed in text towards a\ngiven target. Zero-shot stance detection (ZSSD) has emerged to classify stances\ntowards unseen targets during inference. Recent data augmentation techniques\nfor ZSSD increase transferable knowledge between targets through text or target\naugmentation. However, these methods exhibit limitations. Target augmentation\nlacks logical connections between generated targets and source text, while text\naugmentation relies solely on training data, resulting in insufficient\ngeneralization. To address these issues, we propose an encoder-decoder data\naugmentation (EDDA) framework. The encoder leverages large language models and\nchain-of-thought prompting to summarize texts into target-specific if-then\nrationales, establishing logical relationships. The decoder generates new\nsamples based on these expressions using a semantic correlation word\nreplacement strategy to increase syntactic diversity. We also analyze the\ngenerated expressions to develop a rationale-enhanced network that fully\nutilizes the augmented data. Experiments on benchmark datasets demonstrate our\napproach substantially improves over state-of-the-art ZSSD techniques. The\nproposed EDDA framework increases semantic relevance and syntactic variety in\naugmented texts while enabling interpretable rationale-based learning.", "published": "2024-03-23 04:29:29", "link": "http://arxiv.org/abs/2403.15715v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "General LLMs as Instructors for Domain-Specific LLMs: A Sequential\n  Fusion Method to Integrate Extraction and Editing", "abstract": "The substantial interest in updating Large Language Models (LLMs) without\nretraining from scratch is accompanied by several challenges. This is\nparticularly true when updating LLMs with datasets that necessitate\ndomain-expert reasoning across extensive texts, despite limited samples. We\ntermed the scenario as the Few-Shot Domain-Expert Reasoning for Updating LLMs\n(FDoR-UL). Traditional methods such as Low-Rank Adaptation (LoRA) and Retrieval\nAugmented Generation (RAG) are inadequate for addressing this critical issue,\nparticularly evident in our exploration of a specific medical dataset that\nepitomizes the distinct needs of FDoR-UL. To tackle this challenge, we\nintroduce a Sequential Fusion method to integrate knowledge from complex\ncontexts into LLMs. This method employs a two-stage framework: initially\nleveraging general LLMs to perform relation extraction for knowledge\nacquisition from complex texts, followed by updating domain-specific LLMs\nthrough Knowledge Editing (KE). Employing our method, domain-specific LLMs\nachieved a 71.7% accuracy (an average gain of 39.1%) in question-answering\ntasks. Furthermore, we expanded our evaluation to a novel economics-management\ndataset we developed, where our method achieved a 75.0% accuracy (an average\ngain of 45.0%). These findings underscore the effectiveness and flexibility of\nour approach in FDoR-UL across various domains.", "published": "2024-03-23 06:03:36", "link": "http://arxiv.org/abs/2403.15736v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Few-shot Dialogue Strategy Learning for Motivational Interviewing via\n  Inductive Reasoning", "abstract": "We consider the task of building a dialogue system that can motivate users to\nadopt positive lifestyle changes: Motivational Interviewing. Addressing such a\ntask requires a system that can infer \\textit{how} to motivate a user\neffectively. We propose DIIT, a framework that is capable of learning and\napplying conversation strategies in the form of natural language inductive\nrules from expert demonstrations. Automatic and human evaluation on\ninstruction-following large language models show natural language strategy\ndescriptions discovered by DIIR can improve active listening skills, reduce\nunsolicited advice, and promote more collaborative and less authoritative\nresponses, outperforming various demonstration utilization methods.", "published": "2024-03-23 06:03:37", "link": "http://arxiv.org/abs/2403.15737v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MRC-based Nested Medical NER with Co-prediction and Adaptive\n  Pre-training", "abstract": "In medical information extraction, medical Named Entity Recognition (NER) is\nindispensable, playing a crucial role in developing medical knowledge graphs,\nenhancing medical question-answering systems, and analyzing electronic medical\nrecords. The challenge in medical NER arises from the complex nested structures\nand sophisticated medical terminologies, distinguishing it from its\ncounterparts in traditional domains. In response to these complexities, we\npropose a medical NER model based on Machine Reading Comprehension (MRC), which\nuses a task-adaptive pre-training strategy to improve the model's capability in\nthe medical field. Meanwhile, our model introduces multiple word-pair\nembeddings and multi-granularity dilated convolution to enhance the model's\nrepresentation ability and uses a combined predictor of Biaffine and MLP to\nimprove the model's recognition performance. Experimental evaluations conducted\non the CMeEE, a benchmark for Chinese nested medical NER, demonstrate that our\nproposed model outperforms the compared state-of-the-art (SOTA) models.", "published": "2024-03-23 11:14:02", "link": "http://arxiv.org/abs/2403.15800v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RAAMove: A Corpus for Analyzing Moves in Research Article Abstracts", "abstract": "Move structures have been studied in English for Specific Purposes (ESP) and\nEnglish for Academic Purposes (EAP) for decades. However, there are few move\nannotation corpora for Research Article (RA) abstracts. In this paper, we\nintroduce RAAMove, a comprehensive multi-domain corpus dedicated to the\nannotation of move structures in RA abstracts. The primary objective of RAAMove\nis to facilitate move analysis and automatic move identification. This paper\nprovides a thorough discussion of the corpus construction process, including\nthe scheme, data collection, annotation guidelines, and annotation procedures.\nThe corpus is constructed through two stages: initially, expert annotators\nmanually annotate high-quality data; subsequently, based on the human-annotated\ndata, a BERT-based model is employed for automatic annotation with the help of\nexperts' modification. The result is a large-scale and high-quality corpus\ncomprising 33,988 annotated instances. We also conduct preliminary move\nidentification experiments using the BERT-based model to verify the\neffectiveness of the proposed corpus and model. The annotated corpus is\navailable for academic research purposes and can serve as essential resources\nfor move analysis, English language teaching and writing, as well as\nmove/discourse-related tasks in Natural Language Processing (NLP).", "published": "2024-03-23 15:43:30", "link": "http://arxiv.org/abs/2403.15872v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "VLUE: A New Benchmark and Multi-task Knowledge Transfer Learning for\n  Vietnamese Natural Language Understanding", "abstract": "The success of Natural Language Understanding (NLU) benchmarks in various\nlanguages, such as GLUE for English, CLUE for Chinese, KLUE for Korean, and\nIndoNLU for Indonesian, has facilitated the evaluation of new NLU models across\na wide range of tasks. To establish a standardized set of benchmarks for\nVietnamese NLU, we introduce the first Vietnamese Language Understanding\nEvaluation (VLUE) benchmark. The VLUE benchmark encompasses five datasets\ncovering different NLU tasks, including text classification, span extraction,\nand natural language understanding. To provide an insightful overview of the\ncurrent state of Vietnamese NLU, we then evaluate seven state-of-the-art\npre-trained models, including both multilingual and Vietnamese monolingual\nmodels, on our proposed VLUE benchmark. Furthermore, we present CafeBERT, a new\nstate-of-the-art pre-trained model that achieves superior results across all\ntasks in the VLUE benchmark. Our model combines the proficiency of a\nmultilingual pre-trained model with Vietnamese linguistic knowledge. CafeBERT\nis developed based on the XLM-RoBERTa model, with an additional pretraining\nstep utilizing a significant amount of Vietnamese textual data to enhance its\nadaptation to the Vietnamese language. For the purpose of future research,\nCafeBERT is made publicly available for research purposes.", "published": "2024-03-23 16:26:49", "link": "http://arxiv.org/abs/2403.15882v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "STEntConv: Predicting Disagreement with Stance Detection and a Signed\n  Graph Convolutional Network", "abstract": "The rise of social media platforms has led to an increase in polarised online\ndiscussions, especially on political and socio-cultural topics such as\nelections and climate change. We propose a simple and novel unsupervised method\nto predict whether the authors of two posts agree or disagree, leveraging user\nstances about named entities obtained from their posts. We present STEntConv, a\nmodel which builds a graph of users and named entities weighted by stance and\ntrains a Signed Graph Convolutional Network (SGCN) to detect disagreement\nbetween comment and reply posts. We run experiments and ablation studies and\nshow that including this information improves disagreement detection\nperformance on a dataset of Reddit posts for a range of controversial subreddit\ntopics, without the need for platform-specific features or user history.", "published": "2024-03-23 16:45:22", "link": "http://arxiv.org/abs/2403.15885v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MixRED: A Mix-lingual Relation Extraction Dataset", "abstract": "Relation extraction is a critical task in the field of natural language\nprocessing with numerous real-world applications. Existing research primarily\nfocuses on monolingual relation extraction or cross-lingual enhancement for\nrelation extraction. Yet, there remains a significant gap in understanding\nrelation extraction in the mix-lingual (or code-switching) scenario, where\nindividuals intermix contents from different languages within sentences,\ngenerating mix-lingual content. Due to the lack of a dedicated dataset, the\neffectiveness of existing relation extraction models in such a scenario is\nlargely unexplored. To address this issue, we introduce a novel task of\nconsidering relation extraction in the mix-lingual scenario called MixRE and\nconstructing the human-annotated dataset MixRED to support this task. In\naddition to constructing the MixRED dataset, we evaluate both state-of-the-art\nsupervised models and large language models (LLMs) on MixRED, revealing their\nrespective advantages and limitations in the mix-lingual scenario. Furthermore,\nwe delve into factors influencing model performance within the MixRE task and\nuncover promising directions for enhancing the performance of both supervised\nmodels and LLMs in this novel task.", "published": "2024-03-23 03:18:14", "link": "http://arxiv.org/abs/2403.15696v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "PEaCE: A Chemistry-Oriented Dataset for Optical Character Recognition on\n  Scientific Documents", "abstract": "Optical Character Recognition (OCR) is an established task with the objective\nof identifying the text present in an image. While many off-the-shelf OCR\nmodels exist, they are often trained for either scientific (e.g., formulae) or\ngeneric printed English text. Extracting text from chemistry publications\nrequires an OCR model that is capable in both realms. Nougat, a recent tool,\nexhibits strong ability to parse academic documents, but is unable to parse\ntables in PubMed articles, which comprises a significant part of the academic\ncommunity and is the focus of this work. To mitigate this gap, we present the\nPrinted English and Chemical Equations (PEaCE) dataset, containing both\nsynthetic and real-world records, and evaluate the efficacy of\ntransformer-based OCR models when trained on this resource. Given that\nreal-world records contain artifacts not present in synthetic records, we\npropose transformations that mimic such qualities. We perform a suite of\nexperiments to explore the impact of patch size, multi-domain training, and our\nproposed transformations, ultimately finding that models with a small patch\nsize trained on multiple domains using the proposed transformations yield the\nbest performance. Our dataset and code is available at\nhttps://github.com/ZN1010/PEaCE.", "published": "2024-03-23 05:20:36", "link": "http://arxiv.org/abs/2403.15724v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On the Fragility of Active Learners for Text Classification", "abstract": "Active learning (AL) techniques optimally utilize a labeling budget by\niteratively selecting instances that are most valuable for learning. However,\nthey lack ``prerequisite checks'', i.e., there are no prescribed criteria to\npick an AL algorithm best suited for a dataset. A practitioner must pick a\ntechnique they \\emph{trust} would beat random sampling, based on prior reported\nresults, and hope that it is resilient to the many variables in their\nenvironment: dataset, labeling budget and prediction pipelines. The important\nquestions then are: how often on average, do we expect any AL technique to\nreliably beat the computationally cheap and easy-to-implement strategy of\nrandom sampling? Does it at least make sense to use AL in an ``Always ON'' mode\nin a prediction pipeline, so that while it might not always help, it never\nunder-performs random sampling? How much of a role does the prediction pipeline\nplay in AL's success?\n  We examine these questions in detail for the task of text classification\nusing pre-trained representations, which are ubiquitous today.\n  Our primary contribution here is a rigorous evaluation of AL techniques, old\nand new, across setups that vary wrt datasets, text representations and\nclassifiers. This unlocks multiple insights around warm-up times, i.e., number\nof labels before gains from AL are seen, viability of an ``Always ON'' mode and\nthe relative significance of different factors. Additionally, we release a\nframework for rigorous benchmarking of AL techniques for text classification.", "published": "2024-03-23 07:16:23", "link": "http://arxiv.org/abs/2403.15744v6", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Modeling Unified Semantic Discourse Structure for High-quality Headline\n  Generation", "abstract": "Headline generation aims to summarize a long document with a short, catchy\ntitle that reflects the main idea. This requires accurately capturing the core\ndocument semantics, which is challenging due to the lengthy and background\ninformation-rich na ture of the texts. In this work, We propose using a unified\nsemantic discourse structure (S3) to represent document semantics, achieved by\ncombining document-level rhetorical structure theory (RST) trees with\nsentence-level abstract meaning representation (AMR) graphs to construct S3\ngraphs. The hierarchical composition of sentence, clause, and word\nintrinsically characterizes the semantic meaning of the overall document. We\nthen develop a headline generation framework, in which the S3 graphs are\nencoded as contextual features. To consolidate the efficacy of S3 graphs, we\nfurther devise a hierarchical structure pruning mechanism to dynamically screen\nthe redundant and nonessential nodes within the graph. Experimental results on\ntwo headline generation datasets demonstrate that our method outperforms\nexisting state-of-art methods consistently. Our work can be instructive for a\nbroad range of document modeling tasks, more than headline or summarization\ngeneration.", "published": "2024-03-23 09:18:53", "link": "http://arxiv.org/abs/2403.15776v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Computational Sentence-level Metrics Predicting Human Sentence\n  Comprehension", "abstract": "The majority of research in computational psycholinguistics has concentrated\non the processing of words. This study introduces innovative methods for\ncomputing sentence-level metrics using multilingual large language models. The\nmetrics developed sentence surprisal and sentence relevance and then are tested\nand compared to validate whether they can predict how humans comprehend\nsentences as a whole across languages. These metrics offer significant\ninterpretability and achieve high accuracy in predicting human sentence reading\nspeeds. Our results indicate that these computational sentence-level metrics\nare exceptionally effective at predicting and elucidating the processing\ndifficulties encountered by readers in comprehending sentences as a whole\nacross a variety of languages. Their impressive performance and generalization\ncapabilities provide a promising avenue for future research in integrating LLMs\nand cognitive science.", "published": "2024-03-23 12:19:49", "link": "http://arxiv.org/abs/2403.15822v2", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "LAMPER: LanguAge Model and Prompt EngineeRing for zero-shot time series\n  classification", "abstract": "This study constructs the LanguAge Model with Prompt EngineeRing (LAMPER)\nframework, designed to systematically evaluate the adaptability of pre-trained\nlanguage models (PLMs) in accommodating diverse prompts and their integration\nin zero-shot time series (TS) classification. We deploy LAMPER in experimental\nassessments using 128 univariate TS datasets sourced from the UCR archive. Our\nfindings indicate that the feature representation capacity of LAMPER is\ninfluenced by the maximum input token threshold imposed by PLMs.", "published": "2024-03-23 15:52:37", "link": "http://arxiv.org/abs/2403.15875v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Geotokens and Geotransformers", "abstract": "In transformer architectures, position encoding primarily provides a sense of\nsequence for input tokens. While the original transformer paper's method has\nshown satisfactory results in general language processing tasks, there have\nbeen new proposals, such as Rotary Position Embedding (RoPE), for further\nimprovement. This paper presents geotokens, input components for transformers,\neach linked to a specific geological location. Unlike typical language\nsequences, for these tokens, the order is not as vital as the geographical\ncoordinates themselves. To represent the relative position in this context and\nto keep a balance between the real world distance and the distance in the\nembedding space, we design a position encoding approach drawing from the RoPE\nstructure but tailored for spherical coordinates.", "published": "2024-03-23 22:02:56", "link": "http://arxiv.org/abs/2403.15940v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "IllusionVQA: A Challenging Optical Illusion Dataset for Vision Language\n  Models", "abstract": "The advent of Vision Language Models (VLM) has allowed researchers to\ninvestigate the visual understanding of a neural network using natural\nlanguage. Beyond object classification and detection, VLMs are capable of\nvisual comprehension and common-sense reasoning. This naturally led to the\nquestion: How do VLMs respond when the image itself is inherently unreasonable?\nTo this end, we present IllusionVQA: a diverse dataset of challenging optical\nillusions and hard-to-interpret scenes to test the capability of VLMs in two\ndistinct multiple-choice VQA tasks - comprehension and soft localization.\nGPT4V, the best performing VLM, achieves 62.99% accuracy (4-shot) on the\ncomprehension task and 49.7% on the localization task (4-shot and\nChain-of-Thought). Human evaluation reveals that humans achieve 91.03% and 100%\naccuracy in comprehension and localization. We discover that In-Context\nLearning (ICL) and Chain-of-Thought reasoning substantially degrade the\nperformance of Gemini-Pro in the localization task. Tangentially, we discover a\npotential weakness in the ICL capabilities of VLMs: they fail to locate optical\nillusions even when the correct answer is in the context window as a few-shot\nexample.", "published": "2024-03-23 23:06:32", "link": "http://arxiv.org/abs/2403.15952v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Cost-Efficient Large Language Model Serving for Multi-turn Conversations\n  with CachedAttention", "abstract": "Interacting with humans through multi-turn conversations is a fundamental\nfeature of large language models (LLMs). However, existing LLM serving engines\nexecuting multi-turn conversations are inefficient due to the need to\nrepeatedly compute the key-value (KV) caches of historical tokens, incurring\nhigh serving costs. To address the problem, this paper proposes\nCachedAttention, a new attention mechanism that enables reuse of KV caches\nacross multi-turn conversations, significantly reducing the repetitive\ncomputation overheads. CachedAttention maintains a hierarchical KV caching\nsystem that leverages cost-effective memory/storage mediums to save KV caches\nfor all requests. To reduce KV cache access overheads from slow mediums,\nCachedAttention employs layer-wise pre-loading and asynchronous saving schemes\nto overlap the KV cache access with the GPU computation. To ensure that the KV\ncaches to be accessed are placed in the fastest hierarchy, CachedAttention\nemploys scheduler-aware fetching and eviction schemes to consciously place the\nKV caches in different layers based on the hints from the inference job\nscheduler. To avoid the invalidation of the saved KV caches incurred by context\nwindow overflow, CachedAttention enables the saved KV caches to remain valid\nvia decoupling the positional encoding and effectively truncating the KV\ncaches. Extensive experimental results demonstrate that CachedAttention\nsignificantly decreases the time to the first token (TTFT) by up to 87%,\nimproves the prompt prefilling throughput by up to 7.8$\\times$ for multi-turn\nconversations, and reduces the end-to-end inference cost by up to 70%.", "published": "2024-03-23 10:42:49", "link": "http://arxiv.org/abs/2403.19708v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AC4: Algebraic Computation Checker for Circuit Constraints in ZKPs", "abstract": "Zero-knowledge proof (ZKP) systems have surged attention and held a\nfundamental role in contemporary cryptography. Zero-knowledge succinct\nnon-interactive argument of knowledge (zk-SNARK) protocols dominate the ZKP\nusage, implemented through arithmetic circuit programming paradigm. However,\nunderconstrained or overconstrained circuits may lead to bugs. The former\nrefers to circuits that lack the necessary constraints, resulting in unexpected\nsolutions and causing the verifier to accept a bogus witness, and the latter\nrefers to circuits that are constrained excessively, resulting in lacking\nnecessary solutions and causing the verifier to accept no witness. This paper\nintroduces a novel approach for pinpointing two distinct types of bugs in ZKP\ncircuits. The method involves encoding the arithmetic circuit constraints to\npolynomial equation systems and solving them over finite fields by the computer\nalgebra system. The classification of verification results is refined, greatly\nenhancing the expressive power of the system. A tool, AC4, is proposed to\nrepresent the implementation of the method. Experiments show that AC4\ndemonstrates a increase in the checked ratio, showing a 29% improvement over\nPicus, a checker for Circom circuits, and a 10% improvement over\nhalo2-analyzer, a checker for halo2 circuits. Within a solvable range, the\nchecking time has also exhibited noticeable improvement, demonstrating a\nmagnitude increase compared to previous efforts.", "published": "2024-03-23 01:44:57", "link": "http://arxiv.org/abs/2403.15676v4", "categories": ["cs.SE", "cs.CL", "cs.CR"], "primary_category": "cs.SE"}
{"title": "EAGLE: A Domain Generalization Framework for AI-generated Text Detection", "abstract": "With the advancement in capabilities of Large Language Models (LLMs), one\nmajor step in the responsible and safe use of such LLMs is to be able to detect\ntext generated by these models. While supervised AI-generated text detectors\nperform well on text generated by older LLMs, with the frequent release of new\nLLMs, building supervised detectors for identifying text from such new models\nwould require new labeled training data, which is infeasible in practice. In\nthis work, we tackle this problem and propose a domain generalization framework\nfor the detection of AI-generated text from unseen target generators. Our\nproposed framework, EAGLE, leverages the labeled data that is available so far\nfrom older language models and learns features invariant across these\ngenerators, in order to detect text generated by an unknown target generator.\nEAGLE learns such domain-invariant features by combining the representational\npower of self-supervised contrastive learning with domain adversarial training.\nThrough our experiments we demonstrate how EAGLE effectively achieves\nimpressive performance in detecting text generated by unseen target generators,\nincluding recent state-of-the-art ones such as GPT-4 and Claude, reaching\ndetection scores of within 4.7% of a fully supervised detector.", "published": "2024-03-23 02:44:20", "link": "http://arxiv.org/abs/2403.15690v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards a RAG-based Summarization Agent for the Electron-Ion Collider", "abstract": "The complexity and sheer volume of information encompassing documents,\npapers, data, and other resources from large-scale experiments demand\nsignificant time and effort to navigate, making the task of accessing and\nutilizing these varied forms of information daunting, particularly for new\ncollaborators and early-career scientists. To tackle this issue, a Retrieval\nAugmented Generation (RAG)--based Summarization AI for EIC (RAGS4EIC) is under\ndevelopment. This AI-Agent not only condenses information but also effectively\nreferences relevant responses, offering substantial advantages for\ncollaborators. Our project involves a two-step approach: first, querying a\ncomprehensive vector database containing all pertinent experiment information;\nsecond, utilizing a Large Language Model (LLM) to generate concise summaries\nenriched with citations based on user queries and retrieved data. We describe\nthe evaluation methods that use RAG assessments (RAGAs) scoring mechanisms to\nassess the effectiveness of responses. Furthermore, we describe the concept of\nprompt template-based instruction-tuning which provides flexibility and\naccuracy in summarization. Importantly, the implementation relies on LangChain,\nwhich serves as the foundation of our entire workflow. This integration ensures\nefficiency and scalability, facilitating smooth deployment and accessibility\nfor various user groups within the Electron Ion Collider (EIC) community. This\ninnovative AI-driven framework not only simplifies the understanding of vast\ndatasets but also encourages collaborative participation, thereby empowering\nresearchers. As a demonstration, a web application has been developed to\nexplain each stage of the RAG Agent development in detail.", "published": "2024-03-23 05:32:46", "link": "http://arxiv.org/abs/2403.15729v3", "categories": ["cs.CL", "cs.AI", "hep-ex", "physics.ins-det"], "primary_category": "cs.CL"}
{"title": "Protecting Copyrighted Material with Unique Identifiers in Large\n  Language Model Training", "abstract": "A major public concern regarding the training of large language models (LLMs)\nis whether they abusing copyrighted online text. Previous membership inference\nmethods may be misled by similar examples in vast amounts of training data.\nAdditionally, these methods are often too complex for general users to\nunderstand and use, making them centralized, lacking transparency, and\ntrustworthiness. To address these issues, we propose an alternative\n\\textit{insert-and-detection} methodology, advocating that web users and\ncontent platforms employ \\textbf{\\textit{unique identifiers}} for reliable and\nindependent membership inference. Users and platforms can create their own\nidentifiers, embed them in copyrighted text, and independently detect them in\nfuture LLMs. As an initial demonstration, we introduce \\textit{ghost\nsentences}, a primitive form of unique identifiers, consisting primarily of\npassphrases made up of random words. By embedding one ghost sentences in a few\ncopyrighted texts, users can detect its membership using a perplexity test and\na \\textit{user-friendly} last-$k$ words test. The perplexity test is based on\nthe fact that LLMs trained on natural language should exhibit high perplexity\nwhen encountering unnatural passphrases. As the repetition increases, users can\nleverage the verbatim memorization ability of LLMs to perform a last-$k$ words\ntest by chatting with LLMs without writing any code. Both tests offer rigorous\nstatistical guarantees for membership inference. For LLaMA-13B, a perplexity\ntest on 30 ghost sentences with an average of 7 repetitions in 148K examples\nyields a 0.891 ROC AUC. For the last-$k$ words test with OpenLLaMA-3B, 11 out\nof 16 users, with an average of 24 examples each, successfully identify their\ndata from 1.8M examples.", "published": "2024-03-23 06:36:32", "link": "http://arxiv.org/abs/2403.15740v2", "categories": ["cs.CL", "cs.CR", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Understanding Emergent Abilities of Language Models from the Loss\n  Perspective", "abstract": "Recent studies have put into question the belief that emergent abilities in\nlanguage models are exclusive to large models. This skepticism arises from two\nobservations: 1) smaller models can also exhibit high performance on emergent\nabilities and 2) there is doubt on the discontinuous metrics used to measure\nthese abilities. In this paper, we propose to study emergent abilities in the\nlens of pre-training loss, instead of model size or training compute. We\ndemonstrate that the Transformer models with the same pre-training loss, but\ndifferent model and data sizes, generate the same performance on various\ndownstream tasks, with a fixed data corpus, tokenization, and model\narchitecture. We also discover that a model exhibits emergent abilities on\ncertain tasks -- regardless of the continuity of metrics -- when its\npre-training loss falls below a specific threshold. Before reaching this\nthreshold, its performance remains at the level of random guessing. This\ninspires us to redefine emergent abilities as those that manifest in models\nwith lower pre-training losses, highlighting that these abilities cannot be\npredicted by merely extrapolating the performance trends of models with higher\npre-training losses.", "published": "2024-03-23 11:03:31", "link": "http://arxiv.org/abs/2403.15796v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Centered Masking for Language-Image Pre-Training", "abstract": "We introduce Gaussian masking for Language-Image Pre-Training (GLIP) a novel,\nstraightforward, and effective technique for masking image patches during\npre-training of a vision-language model. GLIP builds on Fast Language-Image\nPre-Training (FLIP), which randomly masks image patches while training a CLIP\nmodel. GLIP replaces random masking with centered masking, that uses a Gaussian\ndistribution and is inspired by the importance of image patches at the center\nof the image. GLIP retains the same computational savings as FLIP, while\nimproving performance across a range of downstream datasets and tasks, as\ndemonstrated by our experimental results. We show the benefits of GLIP to be\neasy to obtain, requiring no delicate tuning of the Gaussian, and also\napplicable to data sets containing images without an obvious center focus.", "published": "2024-03-23 13:24:31", "link": "http://arxiv.org/abs/2403.15837v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Leveraging Zero-Shot Prompting for Efficient Language Model Distillation", "abstract": "This paper introduces a novel approach for efficiently distilling LLMs into\nsmaller, application-specific models, significantly reducing operational costs\nand manual labor. Addressing the challenge of deploying computationally\nintensive LLMs in specific applications or edge devices, this technique\nutilizes LLMs' reasoning capabilities to generate labels and natural language\nrationales for unlabeled data. Our approach enhances both finetuning and\ndistillation by employing a multi-task training framework where student models\nmimic these rationales alongside teacher predictions. Key contributions include\nthe employment of zero-shot prompting to elicit teacher model rationales,\nreducing the necessity for handcrafted few-shot examples and lowering the\noverall token count required, which directly translates to cost savings given\nthe pay-per-token billing model of major tech companies' LLM APIs.\nAdditionally, the paper investigates the impact of explanation properties on\ndistillation efficiency, demonstrating that minimal performance loss occurs\neven when rationale augmentation is not applied across the entire dataset,\nfacilitating further reductions of tokens. This research marks a step toward\nthe efficient training of task-specific models with minimal human intervention,\noffering substantial cost-savings while maintaining, or even enhancing,\nperformance.", "published": "2024-03-23 16:51:52", "link": "http://arxiv.org/abs/2403.15886v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LlamBERT: Large-scale low-cost data annotation in NLP", "abstract": "Large Language Models (LLMs), such as GPT-4 and Llama 2, show remarkable\nproficiency in a wide range of natural language processing (NLP) tasks. Despite\ntheir effectiveness, the high costs associated with their use pose a challenge.\nWe present LlamBERT, a hybrid approach that leverages LLMs to annotate a small\nsubset of large, unlabeled databases and uses the results for fine-tuning\ntransformer encoders like BERT and RoBERTa. This strategy is evaluated on two\ndiverse datasets: the IMDb review dataset and the UMLS Meta-Thesaurus. Our\nresults indicate that the LlamBERT approach slightly compromises on accuracy\nwhile offering much greater cost-effectiveness.", "published": "2024-03-23 21:54:34", "link": "http://arxiv.org/abs/2403.15938v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; F.1.1"], "primary_category": "cs.CL"}
{"title": "Improving Retrieval for RAG based Question Answering Models on Financial\n  Documents", "abstract": "The effectiveness of Large Language Models (LLMs) in generating accurate\nresponses relies heavily on the quality of input provided, particularly when\nemploying Retrieval Augmented Generation (RAG) techniques. RAG enhances LLMs by\nsourcing the most relevant text chunk(s) to base queries upon. Despite the\nsignificant advancements in LLMs' response quality in recent years, users may\nstill encounter inaccuracies or irrelevant answers; these issues often stem\nfrom suboptimal text chunk retrieval by RAG rather than the inherent\ncapabilities of LLMs. To augment the efficacy of LLMs, it is crucial to refine\nthe RAG process. This paper explores the existing constraints of RAG pipelines\nand introduces methodologies for enhancing text retrieval. It delves into\nstrategies such as sophisticated chunking techniques, query expansion, the\nincorporation of metadata annotations, the application of re-ranking\nalgorithms, and the fine-tuning of embedding algorithms. Implementing these\napproaches can substantially improve the retrieval quality, thereby elevating\nthe overall performance and reliability of LLMs in processing and responding to\nqueries.", "published": "2024-03-23 00:49:40", "link": "http://arxiv.org/abs/2404.07221v2", "categories": ["cs.IR", "cs.CL", "cs.LG", "q-fin.GN"], "primary_category": "cs.IR"}
{"title": "Leveraging Large Language Models for Preliminary Security Risk Analysis:\n  A Mission-Critical Case Study", "abstract": "Preliminary security risk analysis (PSRA) provides a quick approach to\nidentify, evaluate and propose remeditation to potential risks in specific\nscenarios. The extensive expertise required for an effective PSRA and the\nsubstantial ammount of textual-related tasks hinder quick assessments in\nmission-critical contexts, where timely and prompt actions are essential. The\nspeed and accuracy of human experts in PSRA significantly impact response time.\nA large language model can quickly summarise information in less time than a\nhuman. To our knowledge, no prior study has explored the capabilities of\nfine-tuned models (FTM) in PSRA. Our case study investigates the proficiency of\nFTM to assist practitioners in PSRA. We manually curated 141 representative\nsamples from over 50 mission-critical analyses archived by the industrial\ncontext team in the last five years.We compared the proficiency of the FTM\nversus seven human experts. Within the industrial context, our approach has\nproven successful in reducing errors in PSRA, hastening security risk\ndetection, and minimizing false positives and negatives. This translates to\ncost savings for the company by averting unnecessary expenses associated with\nimplementing unwarranted countermeasures. Therefore, experts can focus on more\ncomprehensive risk analysis, leveraging LLMs for an effective preliminary\nassessment within a condensed timeframe.", "published": "2024-03-23 07:59:30", "link": "http://arxiv.org/abs/2403.15756v1", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.CR", "cs.CY"], "primary_category": "cs.SE"}
{"title": "User-Side Realization", "abstract": "Users are dissatisfied with services. Since the service is not tailor-made\nfor a user, it is natural for dissatisfaction to arise. The problem is, that\neven if users are dissatisfied, they often do not have the means to resolve\ntheir dissatisfaction. The user cannot alter the source code of the service,\nnor can they force the service provider to change. The user has no choice but\nto remain dissatisfied or quit the service. User-side realization offers\nproactive solutions to this problem by providing general algorithms to deal\nwith common problems on the user's side. These algorithms run on the user's\nside and solve the problems without having the service provider change the\nservice itself.", "published": "2024-03-23 08:03:50", "link": "http://arxiv.org/abs/2403.15757v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.IR"}
{"title": "EduAgent: Generative Student Agents in Learning", "abstract": "Student simulation in online education is important to address dynamic\nlearning behaviors of students with diverse backgrounds. Existing simulation\nmodels based on deep learning usually need massive training data, lacking prior\nknowledge in educational contexts. Large language models (LLMs) may contain\nsuch prior knowledge since they are pre-trained from a large corpus. However,\nbecause student behaviors are dynamic and multifaceted with individual\ndifferences, directly prompting LLMs is not robust nor accurate enough to\ncapture fine-grained interactions among diverse student personas, learning\nbehaviors, and learning outcomes. This work tackles this problem by presenting\na newly annotated fine-grained large-scale dataset and proposing EduAgent, a\nnovel generative agent framework incorporating cognitive prior knowledge (i.e.,\ntheoretical findings revealed in cognitive science) to guide LLMs to first\nreason correlations among various behaviors and then make simulations. Our two\nexperiments show that EduAgent could not only mimic and predict learning\nbehaviors of real students but also generate realistic learning behaviors of\nvirtual students without real data.", "published": "2024-03-23 18:19:17", "link": "http://arxiv.org/abs/2404.07963v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC", "cs.LG"], "primary_category": "cs.CY"}
