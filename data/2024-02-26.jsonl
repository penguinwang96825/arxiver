{"title": "Data-free Weight Compress and Denoise for Large Language Models", "abstract": "Large Language Models (LLMs) are reshaping the research landscape in\nartificial intelligence, particularly as model parameters scale up\nsignificantly, unlocking remarkable capabilities across various domains.\nNevertheless, the scalability of model parameters faces constraints due to\nlimitations in GPU memory and computational speed. To address these\nconstraints, various weight compression methods have emerged, such as Pruning\nand Quantization. Given the low-rank nature of weight matrices in language\nmodels, the reduction of weights through matrix decomposition undoubtedly holds\nsignificant potential and promise. In this paper, drawing upon the intrinsic\nstructure of LLMs, we propose a novel approach termed Data-free Joint Rank-k\nApproximation for compressing the parameter matrices. Significantly, our method\nis characterized by without necessitating additional involvement of any corpus,\nwhile simultaneously preserving orthogonality in conjunction with pruning and\nquantization methods. We achieve a model pruning of 80% parameters while\nretaining 93.43% of the original performance without any calibration data.\nAdditionally, we explore the fundamental properties of the weight matrix of\nLLMs undergone Rank-k Approximation and conduct comprehensive experiments to\nelucidate our hypothesis.", "published": "2024-02-26 05:51:47", "link": "http://arxiv.org/abs/2402.16319v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unraveling Babel: Exploring Multilingual Activation Patterns of LLMs and\n  Their Applications", "abstract": "Recently, large language models (LLMs) have achieved tremendous breakthroughs\nin the field of NLP, but still lack understanding of their internal neuron\nactivities when processing different languages. We designed a method to convert\ndense LLMs into fine-grained MoE architectures, and then visually studied the\nmultilingual activation patterns of LLMs through expert activation frequency\nheatmaps. Through comprehensive experiments on different model families,\ndifferent model sizes, and different variants, we analyzed the similarities and\ndifferences in the internal neuron activation patterns of LLMs when processing\ndifferent languages. Specifically, we investigated the distribution of\nhigh-frequency activated experts, multilingual shared experts, whether\nmultilingual activation patterns are related to language families, and the\nimpact of instruction tuning on activation patterns. We further explored\nleveraging the discovered differences in expert activation frequencies to guide\nsparse activation and pruning. Experimental results demonstrated that our\nmethod significantly outperformed random expert pruning and even exceeded the\nperformance of unpruned models in some languages. Additionally, we found that\nconfiguring different pruning rates for different layers based on activation\nlevel differences could achieve better results. Our findings reveal the\nmultilingual processing mechanisms within LLMs and utilize these insights to\noffer new perspectives for applications such as sparse activation and model\npruning.", "published": "2024-02-26 07:44:56", "link": "http://arxiv.org/abs/2402.16367v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Immunization against harmful fine-tuning attacks", "abstract": "Large Language Models (LLMs) are often trained with safety guards intended to\nprevent harmful text generation. However, such safety training can be removed\nby fine-tuning the LLM on harmful datasets. While this emerging threat (harmful\nfine-tuning attacks) has been characterized by previous work, there is little\nunderstanding of how we should proceed in constructing and validating defenses\nagainst these attacks especially in the case where defenders would not have\ncontrol of the fine-tuning process. We introduce a formal framework based on\nthe training budget of an attacker which we call \"Immunization\" conditions.\nUsing a formal characterisation of the harmful fine-tuning problem, we provide\na thorough description of what a successful defense must comprise of and\nestablish a set of guidelines on how rigorous defense research that gives us\nconfidence should proceed.", "published": "2024-02-26 08:08:03", "link": "http://arxiv.org/abs/2402.16382v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From RAGs to riches: Utilizing large language models to write documents\n  for clinical trials", "abstract": "This manuscript has now been published: - Link to article on journal website:\nhttps://journals.sagepub.com/doi/10.1177/17407745251320806 - Pubmed link:\nhttps://pubmed.ncbi.nlm.nih.gov/40013826/", "published": "2024-02-26 08:59:05", "link": "http://arxiv.org/abs/2402.16406v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Predicting Sustainable Development Goals Using Course Descriptions --\n  from LLMs to Conventional Foundation Models", "abstract": "We present our work on predicting United Nations sustainable development\ngoals (SDG) for university courses. We use an LLM named PaLM 2 to generate\ntraining data given a noisy human-authored course description input as input.\nWe use this data to train several different smaller language models to predict\nSDGs for university courses. This work contributes to better university level\nadaptation of SDGs. The best performing model in our experiments was BART with\nan F1-score of 0.786.", "published": "2024-02-26 09:19:46", "link": "http://arxiv.org/abs/2402.16420v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RoCoIns: Enhancing Robustness of Large Language Models through\n  Code-Style Instructions", "abstract": "Large Language Models (LLMs) have showcased remarkable capabilities in\nfollowing human instructions. However, recent studies have raised concerns\nabout the robustness of LLMs when prompted with instructions combining textual\nadversarial samples. In this paper, drawing inspiration from recent works that\nLLMs are sensitive to the design of the instructions, we utilize instructions\nin code style, which are more structural and less ambiguous, to replace\ntypically natural language instructions. Through this conversion, we provide\nLLMs with more precise instructions and strengthen the robustness of LLMs.\nMoreover, under few-shot scenarios, we propose a novel method to compose\nin-context demonstrations using both clean and adversarial samples\n(\\textit{adversarial context method}) to further boost the robustness of the\nLLMs. Experiments on eight robustness datasets show that our method\nconsistently outperforms prompting LLMs with natural language instructions. For\nexample, with gpt-3.5-turbo, our method achieves an improvement of 5.68\\% in\ntest set accuracy and a reduction of 5.66 points in Attack Success Rate (ASR).", "published": "2024-02-26 09:30:55", "link": "http://arxiv.org/abs/2402.16431v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language-Specific Neurons: The Key to Multilingual Capabilities in Large\n  Language Models", "abstract": "Large language models (LLMs) demonstrate remarkable multilingual capabilities\nwithout being pre-trained on specially curated multilingual parallel corpora.\nIt remains a challenging problem to explain the underlying mechanisms by which\nLLMs process multilingual texts. In this paper, we delve into the composition\nof Transformer architectures in LLMs to pinpoint language-specific regions.\nSpecially, we propose a novel detection method, language activation probability\nentropy (LAPE), to identify language-specific neurons within LLMs. Based on\nLAPE, we conduct comprehensive experiments on several representative LLMs, such\nas LLaMA-2, BLOOM, and Mistral. Our findings indicate that LLMs' proficiency in\nprocessing a particular language is predominantly due to a small subset of\nneurons, primarily situated in the models' top and bottom layers. Furthermore,\nwe showcase the feasibility to \"steer\" the output language of LLMs by\nselectively activating or deactivating language-specific neurons. Our research\nprovides important evidence to the understanding and exploration of the\nmultilingual capabilities of LLMs.", "published": "2024-02-26 09:36:05", "link": "http://arxiv.org/abs/2402.16438v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ShieldLM: Empowering LLMs as Aligned, Customizable and Explainable\n  Safety Detectors", "abstract": "The safety of Large Language Models (LLMs) has gained increasing attention in\nrecent years, but there still lacks a comprehensive approach for detecting\nsafety issues within LLMs' responses in an aligned, customizable and\nexplainable manner. In this paper, we propose ShieldLM, an LLM-based safety\ndetector, which aligns with common safety standards, supports customizable\ndetection rules, and provides explanations for its decisions. To train\nShieldLM, we compile a large bilingual dataset comprising 14,387 query-response\npairs, annotating the safety of responses based on various safety standards.\nThrough extensive experiments, we demonstrate that ShieldLM surpasses strong\nbaselines across four test sets, showcasing remarkable customizability and\nexplainability. Besides performing well on standard detection datasets,\nShieldLM has also been shown to be effective as a safety evaluator for advanced\nLLMs. ShieldLM is released at \\url{https://github.com/thu-coai/ShieldLM} to\nsupport accurate and explainable safety detection under various safety\nstandards.", "published": "2024-02-26 09:43:02", "link": "http://arxiv.org/abs/2402.16444v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RetrievalQA: Assessing Adaptive Retrieval-Augmented Generation for\n  Short-form Open-Domain Question Answering", "abstract": "Adaptive retrieval-augmented generation (ARAG) aims to dynamically determine\nthe necessity of retrieval for queries instead of retrieving indiscriminately\nto enhance the efficiency and relevance of the sourced information. However,\nprevious works largely overlook the evaluation of ARAG approaches, leading to\ntheir effectiveness being understudied. This work presents a benchmark,\nRetrievalQA, comprising 1,271 short-form questions covering new world and\nlong-tail knowledge. The knowledge necessary to answer the questions is absent\nfrom LLMs; therefore, external information must be retrieved to answer\ncorrectly. This makes RetrievalQA a suitable testbed to evaluate existing ARAG\nmethods. We observe that calibration-based methods heavily rely on threshold\ntuning, while vanilla prompting is inadequate for guiding LLMs to make reliable\nretrieval decisions. Based on our findings, we propose Time-Aware Adaptive\nRetrieval (TA-ARE), a simple yet effective method that helps LLMs assess the\nnecessity of retrieval without calibration or additional training. The dataset\nand code will be available at https://github.com/hyintell/RetrievalQA", "published": "2024-02-26 09:59:04", "link": "http://arxiv.org/abs/2402.16457v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ID-XCB: Data-independent Debiasing for Fair and Accurate\n  Transformer-based Cyberbullying Detection", "abstract": "Swear words are a common proxy to collect datasets with cyberbullying\nincidents. Our focus is on measuring and mitigating biases derived from\nspurious associations between swear words and incidents occurring as a result\nof such data collection strategies. After demonstrating and quantifying these\nbiases, we introduce ID-XCB, the first data-independent debiasing technique\nthat combines adversarial training, bias constraints and debias fine-tuning\napproach aimed at alleviating model attention to bias-inducing words without\nimpacting overall model performance. We explore ID-XCB on two popular\nsession-based cyberbullying datasets along with comprehensive ablation and\ngeneralisation studies. We show that ID-XCB learns robust cyberbullying\ndetection capabilities while mitigating biases, outperforming state-of-the-art\ndebiasing methods in both performance and bias mitigation. Our quantitative and\nqualitative analyses demonstrate its generalisability to unseen data.", "published": "2024-02-26 10:02:29", "link": "http://arxiv.org/abs/2402.16458v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unveiling Vulnerability of Self-Attention", "abstract": "Pre-trained language models (PLMs) are shown to be vulnerable to minor word\nchanges, which poses a big threat to real-world systems. While previous studies\ndirectly focus on manipulating word inputs, they are limited by their means of\ngenerating adversarial samples, lacking generalization to versatile real-world\nattack. This paper studies the basic structure of transformer-based PLMs, the\nself-attention (SA) mechanism. (1) We propose a powerful perturbation technique\n\\textit{HackAttend}, which perturbs the attention scores within the SA matrices\nvia meticulously crafted attention masks. We show that state-of-the-art PLMs\nfall into heavy vulnerability that minor attention perturbations $(1\\%)$ can\nproduce a very high attack success rate $(98\\%)$. Our paper expands the\nconventional text attack of word perturbations to more general structural\nperturbations. (2) We introduce \\textit{S-Attend}, a novel smoothing technique\nthat effectively makes SA robust via structural perturbations. We empirically\ndemonstrate that this simple yet effective technique achieves robust\nperformance on par with adversarial training when facing various text\nattackers. Code is publicly available at \\url{github.com/liongkj/HackAttend}.", "published": "2024-02-26 10:31:45", "link": "http://arxiv.org/abs/2402.16470v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLMArena: Assessing Capabilities of Large Language Models in Dynamic\n  Multi-Agent Environments", "abstract": "Recent advancements in large language models (LLMs) have revealed their\npotential for achieving autonomous agents possessing human-level intelligence.\nHowever, existing benchmarks for evaluating LLM Agents either use static\ndatasets, potentially leading to data leakage or focus only on single-agent\nscenarios, overlooking the complexities of multi-agent interactions. There is a\nlack of a benchmark that evaluates the diverse capabilities of LLM agents in\nmulti-agent, dynamic environments. To this end, we introduce LLMArena, a novel\nand easily extensible framework for evaluating the diverse capabilities of LLM\nin multi-agent dynamic environments. LLMArena encompasses seven distinct gaming\nenvironments, employing Trueskill scoring to assess crucial abilities in LLM\nagents, including spatial reasoning, strategic planning, numerical reasoning,\nrisk assessment, communication, opponent modeling, and team collaboration. We\nconduct an extensive experiment and human evaluation among different sizes and\ntypes of LLMs, showing that LLMs still have a significant journey ahead in\ntheir development towards becoming fully autonomous agents, especially in\nopponent modeling and team collaboration. We hope LLMArena could guide future\nresearch towards enhancing these capabilities in LLMs, ultimately leading to\nmore sophisticated and practical applications in dynamic, multi-agent settings.\nThe code and data will be available.", "published": "2024-02-26 11:31:48", "link": "http://arxiv.org/abs/2402.16499v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Two-stage Generative Question Answering on Temporal Knowledge Graph\n  Using Large Language Models", "abstract": "Temporal knowledge graph question answering (TKGQA) poses a significant\nchallenge task, due to the temporal constraints hidden in questions and the\nanswers sought from dynamic structured knowledge. Although large language\nmodels (LLMs) have made considerable progress in their reasoning ability over\nstructured data, their application to the TKGQA task is a relatively unexplored\narea. This paper first proposes a novel generative temporal knowledge graph\nquestion answering framework, GenTKGQA, which guides LLMs to answer temporal\nquestions through two phases: Subgraph Retrieval and Answer Generation. First,\nwe exploit LLM's intrinsic knowledge to mine temporal constraints and\nstructural links in the questions without extra training, thus narrowing down\nthe subgraph search space in both temporal and structural dimensions. Next, we\ndesign virtual knowledge indicators to fuse the graph neural network signals of\nthe subgraph and the text representations of the LLM in a non-shallow way,\nwhich helps the open-source LLM deeply understand the temporal order and\nstructural dependencies among the retrieved facts through instruction tuning.\nExperimental results on two widely used datasets demonstrate the superiority of\nour model.", "published": "2024-02-26 13:47:09", "link": "http://arxiv.org/abs/2402.16568v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic change detection for Slovene language: a novel dataset and an\n  approach based on optimal transport", "abstract": "In this paper, we focus on the detection of semantic changes in Slovene, a\nless resourced Slavic language with two million speakers. Detecting and\ntracking semantic changes provides insights into the evolution of the language\ncaused by changes in society and culture. Recently, several systems have been\nproposed to aid in this study, but all depend on manually annotated gold\nstandard datasets for evaluation. In this paper, we present the first Slovene\ndataset for evaluating semantic change detection systems, which contains\naggregated semantic change scores for 104 target words obtained from more than\n3000 manually annotated sentence pairs. We evaluate several existing semantic\nchange detection methods on this dataset and also propose a novel approach\nbased on optimal transport that improves on the existing state-of-the-art\nsystems with an error reduction rate of 22.8%.", "published": "2024-02-26 14:27:06", "link": "http://arxiv.org/abs/2402.16596v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Rethinking Negative Instances for Generative Named Entity Recognition", "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities for\ngeneralizing in unseen tasks. In the Named Entity Recognition (NER) task,\nrecent advancements have seen the remarkable improvement of LLMs in a broad\nrange of entity domains via instruction tuning, by adopting entity-centric\nschema. In this work, we explore the potential enhancement of the existing\nmethods by incorporating negative instances into training. Our experiments\nreveal that negative instances contribute to remarkable improvements by (1)\nintroducing contextual information, and (2) clearly delineating label\nboundaries. Furthermore, we introduce an efficient longest common subsequence\n(LCS) matching algorithm, which is tailored to transform unstructured\npredictions into structured entities. By integrating these components, we\npresent GNER, a Generative NER system that shows improved zero-shot performance\nacross unseen entity domains. Our comprehensive evaluation illustrates our\nsystem's superiority, surpassing state-of-the-art (SoTA) methods by 9 $F_1$\nscore in zero-shot evaluation.", "published": "2024-02-26 14:30:37", "link": "http://arxiv.org/abs/2402.16602v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Long-Context Language Modeling with Parallel Context Encoding", "abstract": "Extending large language models (LLMs) to process longer inputs is crucial\nfor a wide range of applications. However, the substantial computational cost\nof transformers and limited generalization of positional encoding restrict the\nsize of their context window. We introduce Context Expansion with Parallel\nEncoding (CEPE), a framework that can be applied to any existing decoder-only\nLLMs to extend their context window. CEPE employs a small encoder to process\nlong inputs chunk by chunk, enabling the frozen decoder to utilize additional\ncontexts via cross-attention. CEPE is efficient, generalizable, and versatile:\ntrained with 8K-token documents, it extends the context window of LLAMA-2 to\n128K tokens, offering 10x the throughput with only 1/6 of the memory. CEPE\nyields strong performance on language modeling and in-context learning. CEPE\nalso excels in retrieval-augmented applications, while existing long-context\nmodels degenerate with retrieved contexts. We further introduce a CEPE variant\nthat can extend the context window of instruction-tuned models using only\nunlabeled data, and showcase its effectiveness on LLAMA-2-CHAT, leading to a\nstrong instruction-following model that can leverage very long contexts on\ndownstream tasks.", "published": "2024-02-26 14:47:35", "link": "http://arxiv.org/abs/2402.16617v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Domain Embeddings for Generating Complex Descriptions of Concepts in\n  Italian Language", "abstract": "In this work, we propose a Distributional Semantic resource enriched with\nlinguistic and lexical information extracted from electronic dictionaries,\ndesigned to address the challenge of bridging the gap between the continuous\nsemantic values represented by distributional vectors and the discrete\ndescriptions offered by general semantics theory. Recently, many researchers\nhave concentrated on the nexus between embeddings and a comprehensive theory of\nsemantics and meaning. This often involves decoding the representation of word\nmeanings in Distributional Models into a set of discrete, manually constructed\nproperties such as semantic primitives or features, using neural decoding\ntechniques. Our approach introduces an alternative strategy grounded in\nlinguistic data. We have developed a collection of domain-specific\nco-occurrence matrices, derived from two sources: a classification of Italian\nnouns categorized into 4 semantic traits and 20 concrete noun sub-categories,\nand a list of Italian verbs classified according to their semantic classes. In\nthese matrices, the co-occurrence values for each word are calculated\nexclusively with a defined set of words pertinent to a particular lexical\ndomain. The resource comprises 21 domain-specific matrices, one comprehensive\nmatrix, and a Graphical User Interface. Our model facilitates the generation of\nreasoned semantic descriptions of concepts by selecting matrices directly\nassociated with concrete conceptual knowledge, such as a matrix based on\nlocation nouns and the concept of animal habitats. We assessed the utility of\nthe resource through two experiments, achieving promising outcomes in both: the\nautomatic classification of animal nouns and the extraction of animal features.", "published": "2024-02-26 15:04:35", "link": "http://arxiv.org/abs/2402.16632v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "StructLM: Towards Building Generalist Models for Structured Knowledge\n  Grounding", "abstract": "Structured data sources, such as tables, graphs, and databases, are\nubiquitous knowledge sources. Despite the demonstrated capabilities of large\nlanguage models (LLMs) on plain text, their proficiency in interpreting and\nutilizing structured data remains limited. Our investigation reveals a notable\ndeficiency in LLMs' ability to process structured data, e.g., ChatGPT lags\nbehind state-of-the-art (SoTA) model by an average of 35%. To augment the\nStructured Knowledge Grounding (SKG) capabilities in LLMs, we have developed a\ncomprehensive instruction tuning dataset comprising 1.1 million examples.\nUtilizing this dataset, we train a series of models, referred to as StructLM,\nbased on the Mistral and the CodeLlama model family, ranging from 7B to 34B\nparameters. Our StructLM series surpasses task-specific models on 16 out of 18\nevaluated datasets and establishes new SoTA performance on 8 SKG tasks.\nFurthermore, StructLM demonstrates strong generalization across 6 novel\nheld-out SKG tasks, outperforming TableLlama by an average of 35\\% and Flan-UL2\n20B by an average of 10\\%. Contrary to expectations, we observe that scaling\nmodel size offers marginal benefits, with StructLM-34B showing only slight\nimprovements over StructLM-7B. This suggests that structured knowledge\ngrounding is still a challenging task and requires more innovative design to\npush to a new level.", "published": "2024-02-26 15:47:01", "link": "http://arxiv.org/abs/2402.16671v7", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Look Before You Leap: Towards Decision-Aware and Generalizable\n  Tool-Usage for Large Language Models", "abstract": "Tool-augmented large language models (LLMs) are attracting widespread\nattention when accessing up-to-date knowledge and alleviating hallucination\nissues. Nowadays, advanced closed-source LLMs (e.g., ChatGPT) have demonstrated\nsurprising tool-usage capabilities through prompting and in-context learning\ntechniques. To empower the capabilities of open-source LLMs (e.g., LLaMA) in\nmanipulating tools, current efforts focus on either template-driven or\ntoken-triggered tool-usage. However, the former hampers LLMs' flexibility to\naddress diverse user's queries due to constrained tool interactions, while the\nlatter limits the generalizability when engaging with new tools, since\ntool-usage learning is based on task- and tool-specific datasets. To alleviate\nthese concerns, in this paper, we propose a decision-aware and generalizable\ntool-usage framework (DEER). Specifically, we first construct the tool-usage\nsamples with multiple decision branches via an automatic generation pipeline,\nthereby inspiring the decision-making awareness of LLMs under diverse\nscenarios. Meanwhile, we propose a novel tool sampling strategy to enhance the\ngeneralizability of LLMs over unseen tools. Extensive experiments demonstrate\nthat our proposed DEER is effective and significantly outperforms baselines\nacross various datasets.", "published": "2024-02-26 16:11:03", "link": "http://arxiv.org/abs/2402.16696v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Set the Clock: Temporal Alignment of Pretrained Language Models", "abstract": "Language models (LMs) are trained on web text originating from many points in\ntime and, in general, without any explicit temporal grounding. This work\ninvestigates the temporal chaos of pretrained LMs and explores various methods\nto align their internal knowledge to a target time, which we call \"temporal\nalignment.\" To do this, we first automatically construct a dataset containing\n20K time-sensitive questions and their answers for each year from 2000 to 2023.\nBased on this dataset, we empirically show that pretrained LMs (e.g., LLaMa2),\ndespite having a recent pretraining cutoff (e.g., 2022), mostly answer\nquestions using earlier knowledge (e.g., in 2019). We then develop several\nmethods, from prompting to finetuning, to align LMs to use their most recent\nknowledge when answering questions, and investigate various factors in this\nalignment. Our experiments demonstrate that aligning LLaMa2 to the year 2022\ncan enhance its performance by up to 62% according to that year's answers. This\nimprovement occurs even without explicitly mentioning time information,\nindicating the possibility of aligning models' internal sense of time after\npretraining. Finally, we find that alignment to a historical time is also\npossible, with up to 2.8$\\times$ the performance of the unaligned LM in 2010 if\nfinetuning models to that year. These findings hint at the sophistication of\nLMs' internal knowledge organization and the necessity of tuning them properly.", "published": "2024-02-26 18:10:56", "link": "http://arxiv.org/abs/2402.16797v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OncoGPT: A Medical Conversational Model Tailored with Oncology Domain\n  Expertise on a Large Language Model Meta-AI (LLaMA)", "abstract": "In the past year, there has been a growing trend in applying Large Language\nModels (LLMs) to the field of medicine, particularly with the advent of\nadvanced language models such as ChatGPT developed by OpenAI. However, there is\nlimited research on LLMs specifically addressing oncology-related queries. The\nprimary aim of this research was to develop a specialized language model that\ndemonstrates improved accuracy in providing advice related to oncology. We\nperformed an extensive data collection of online question-answer interactions\ncentered around oncology, sourced from reputable doctor-patient platforms.\nFollowing data cleaning and anonymization, a dataset comprising over 180K+\noncology-related conversations was established. The conversations were\ncategorized and meticulously reviewed by field specialists and clinicians to\nensure precision. Employing the LLaMA model and other selected open-source\ndatasets, we conducted iterative fine-tuning to enhance the model's proficiency\nin basic medical conversation and specialized oncology knowledge. We observed a\nsubstantial enhancement in the model's understanding of genuine patient\ninquiries and its reliability in offering oncology-related advice through the\nutilization of real online question-answer interactions in the fine-tuning\nprocess. We release database and models to the research community\n(https://github.com/OncoGPT1).", "published": "2024-02-26 18:33:13", "link": "http://arxiv.org/abs/2402.16810v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating the Effectiveness of HyperTuning via Gisting", "abstract": "Gisting (Mu et al., 2023) is a simple method for training models to compress\ninformation into fewer token representations using a modified attention mask,\nand can serve as an economical approach to training Transformer-based\nhypernetworks. We introduce HyperLlama, a set of Gisting-based hypernetworks\nbuilt on Llama-2 models that generates task-specific soft prefixes based on\nfew-shot inputs. In experiments across P3, Super-NaturalInstructions and Symbol\nTuning datasets, we show that HyperLlama models can effectively compress\ninformation from few-shot examples into soft prefixes. However, they still\nunderperform multi-task fine-tuned language models with full attention over\nfew-shot in-context examples. We also show that HyperLlama-generated soft\nprefixes can serve as better initializations for further prefix tuning.\nOverall, Gisting-based hypernetworks are economical and easy to implement, but\nhave mixed empirical performance.", "published": "2024-02-26 18:42:26", "link": "http://arxiv.org/abs/2402.16817v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Eight Methods to Evaluate Robust Unlearning in LLMs", "abstract": "Machine unlearning can be useful for removing harmful capabilities and\nmemorized text from large language models (LLMs), but there are not yet\nstandardized methods for rigorously evaluating it. In this paper, we first\nsurvey techniques and limitations of existing unlearning evaluations. Second,\nwe apply a comprehensive set of tests for the robustness and competitiveness of\nunlearning in the \"Who's Harry Potter\" (WHP) model from Eldan and Russinovich\n(2023). While WHP's unlearning generalizes well when evaluated with the\n\"Familiarity\" metric from Eldan and Russinovich, we find i)\nhigher-than-baseline amounts of knowledge can reliably be extracted, ii) WHP\nperforms on par with the original model on Harry Potter Q&A tasks, iii) it\nrepresents latent knowledge comparably to the original model, and iv) there is\ncollateral unlearning in related domains. Overall, our results highlight the\nimportance of comprehensive unlearning evaluation that avoids ad-hoc metrics.", "published": "2024-02-26 18:57:37", "link": "http://arxiv.org/abs/2402.16835v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do Large Language Models Latently Perform Multi-Hop Reasoning?", "abstract": "We study whether Large Language Models (LLMs) latently perform multi-hop\nreasoning with complex prompts such as \"The mother of the singer of\n'Superstition' is\". We look for evidence of a latent reasoning pathway where an\nLLM (1) latently identifies \"the singer of 'Superstition'\" as Stevie Wonder,\nthe bridge entity, and (2) uses its knowledge of Stevie Wonder's mother to\ncomplete the prompt. We analyze these two hops individually and consider their\nco-occurrence as indicative of latent multi-hop reasoning. For the first hop,\nwe test if changing the prompt to indirectly mention the bridge entity instead\nof any other entity increases the LLM's internal recall of the bridge entity.\nFor the second hop, we test if increasing this recall causes the LLM to better\nutilize what it knows about the bridge entity. We find strong evidence of\nlatent multi-hop reasoning for the prompts of certain relation types, with the\nreasoning pathway used in more than 80% of the prompts. However, the\nutilization is highly contextual, varying across different types of prompts.\nAlso, on average, the evidence for the second hop and the full multi-hop\ntraversal is rather moderate and only substantial for the first hop. Moreover,\nwe find a clear scaling trend with increasing model size for the first hop of\nreasoning but not for the second hop. Our experimental findings suggest\npotential challenges and opportunities for future development and applications\nof LLMs.", "published": "2024-02-26 18:57:54", "link": "http://arxiv.org/abs/2402.16837v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MobiLlama: Towards Accurate and Lightweight Fully Transparent GPT", "abstract": "\"Bigger the better\" has been the predominant trend in recent Large Language\nModels (LLMs) development. However, LLMs do not suit well for scenarios that\nrequire on-device processing, energy efficiency, low memory footprint, and\nresponse efficiency. These requisites are crucial for privacy, security, and\nsustainable deployment. This paper explores the \"less is more\" paradigm by\naddressing the challenge of designing accurate yet efficient Small Language\nModels (SLMs) for resource constrained devices. Our primary contribution is the\nintroduction of an accurate and fully transparent open-source 0.5 billion\n(0.5B) parameter SLM, named MobiLlama, catering to the specific needs of\nresource-constrained computing with an emphasis on enhanced performance with\nreduced resource demands. MobiLlama is a SLM design that initiates from a\nlarger model and applies a careful parameter sharing scheme to reduce both the\npre-training and the deployment cost. Our work strives to not only bridge the\ngap in open-source SLMs but also ensures full transparency, where complete\ntraining data pipeline, training code, model weights, and over 300 checkpoints\nalong with evaluation codes is available at :\nhttps://github.com/mbzuai-oryx/MobiLlama.", "published": "2024-02-26 18:59:03", "link": "http://arxiv.org/abs/2402.16840v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Benchmarking LLMs on the Semantic Overlap Summarization Task", "abstract": "Semantic Overlap Summarization (SOS) is a constrained multi-document\nsummarization task, where the constraint is to capture the common/overlapping\ninformation between two alternative narratives. While recent advancements in\nLarge Language Models (LLMs) have achieved superior performance in numerous\nsummarization tasks, a benchmarking study of the SOS task using LLMs is yet to\nbe performed. As LLMs' responses are sensitive to slight variations in prompt\ndesign, a major challenge in conducting such a benchmarking study is to\nsystematically explore a variety of prompts before drawing a reliable\nconclusion. Fortunately, very recently, the TELeR taxonomy has been proposed\nwhich can be used to design and explore various prompts for LLMs. Using this\nTELeR taxonomy and 15 popular LLMs, this paper comprehensively evaluates LLMs\non the SOS Task, assessing their ability to summarize overlapping information\nfrom multiple alternative narratives. For evaluation, we report\nwell-established metrics like ROUGE, BERTscore, and SEM-F1$ on two different\ndatasets of alternative narratives. We conclude the paper by analyzing the\nstrengths and limitations of various LLMs in terms of their capabilities in\ncapturing overlapping information The code and datasets used to conduct this\nstudy are available at https://anonymous.4open.science/r/llm_eval-E16D.", "published": "2024-02-26 20:33:50", "link": "http://arxiv.org/abs/2402.17008v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DiffuCOMET: Contextual Commonsense Knowledge Diffusion", "abstract": "Inferring contextually-relevant and diverse commonsense to understand\nnarratives remains challenging for knowledge models. In this work, we develop a\nseries of knowledge models, DiffuCOMET, that leverage diffusion to learn to\nreconstruct the implicit semantic connections between narrative contexts and\nrelevant commonsense knowledge. Across multiple diffusion steps, our method\nprogressively refines a representation of commonsense facts that is anchored to\na narrative, producing contextually-relevant and diverse commonsense inferences\nfor an input context. To evaluate DiffuCOMET, we introduce new metrics for\ncommonsense inference that more closely measure knowledge diversity and\ncontextual relevance. Our results on two different benchmarks, ComFact and\nWebNLG+, show that knowledge generated by DiffuCOMET achieves a better\ntrade-off between commonsense diversity, contextual relevance and alignment to\nknown gold references, compared to baseline knowledge models.", "published": "2024-02-26 20:35:34", "link": "http://arxiv.org/abs/2402.17011v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Z-AGI Labs at ClimateActivism 2024: Stance and Hate Event Detection on\n  Social Media", "abstract": "In the digital realm, rich data serves as a crucial source of insights into\nthe complexities of social, political, and economic landscapes. Addressing the\ngrowing need for high-quality information on events and the imperative to\ncombat hate speech, this research led to the establishment of the Shared Task\non Climate Activism Stance and Hate Event Detection at CASE 2024. Focused on\nclimate activists contending with hate speech on social media, our study\ncontributes to hate speech identification from tweets. Analyzing three\nsub-tasks - Hate Speech Detection (Sub-task A), Targets of Hate Speech\nIdentification (Sub-task B), and Stance Detection (Sub-task C) - Team Z-AGI\nLabs evaluated various models, including LSTM, Xgboost, and LGBM based on\nTf-Idf. Results unveiled intriguing variations, with Catboost excelling in\nSubtask-B (F1: 0.5604) and Subtask-C (F1: 0.7081), while LGBM emerged as the\ntop-performing model for Subtask-A (F1: 0.8684). This research provides\nvaluable insights into the suitability of classical machine learning models for\nclimate hate speech and stance detection, aiding informed model selection for\nrobust mechanisms.", "published": "2024-02-26 20:43:48", "link": "http://arxiv.org/abs/2402.17014v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "QASE Enhanced PLMs: Improved Control in Text Generation for MRC", "abstract": "To address the challenges of out-of-control generation in generative models\nfor machine reading comprehension (MRC), we introduce the Question-Attended\nSpan Extraction (QASE) module. Integrated during the fine-tuning of pre-trained\ngenerative language models (PLMs), QASE enables these PLMs to match SOTA\nextractive methods and outperform leading LLMs like GPT-4 in MRC tasks, without\nsignificant increases in computational costs.", "published": "2024-02-26 05:34:16", "link": "http://arxiv.org/abs/2403.04771v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Topic-to-essay generation with knowledge-based content selection", "abstract": "The topic-to-essay generation task is a challenging natural language\ngeneration task that aims to generate paragraph-level text with high semantic\ncoherence based on a given set of topic words. Previous work has focused on the\nintroduction of external knowledge, ignoring the insufficient generated text\ndiversity. In order to improve the generation diversity, we propose a novel\ncopy mechanism model with a content selection module that integrates rich\nsemantic knowledge from the language model into the decoder. Furthermore, we\nintroduce the improved prefix tuning method to train the model, enabling it to\nadapt to varying input complexities. In addition, we have contributed a new\nChinese dataset for TEG tasks. Experimental results demonstrate that the\nproposed model can improve the generated text diversity by 35\\% to 59\\%\ncompared to the state-of-the-art method, while maintaining a high level of\ntopic consistency.", "published": "2024-02-26 02:14:42", "link": "http://arxiv.org/abs/2402.16248v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "UniRetriever: Multi-task Candidates Selection for Various\n  Context-Adaptive Conversational Retrieval", "abstract": "Conversational retrieval refers to an information retrieval system that\noperates in an iterative and interactive manner, requiring the retrieval of\nvarious external resources, such as persona, knowledge, and even response, to\neffectively engage with the user and successfully complete the dialogue.\nHowever, most previous work trained independent retrievers for each specific\nresource, resulting in sub-optimal performance and low efficiency. Thus, we\npropose a multi-task framework function as a universal retriever for three\ndominant retrieval tasks during the conversation: persona selection, knowledge\nselection, and response selection. To this end, we design a dual-encoder\narchitecture consisting of a context-adaptive dialogue encoder and a candidate\nencoder, aiming to attention to the relevant context from the long dialogue and\nretrieve suitable candidates by simply a dot product. Furthermore, we introduce\ntwo loss constraints to capture the subtle relationship between dialogue\ncontext and different candidates by regarding historically selected candidates\nas hard negatives. Extensive experiments and analysis establish\nstate-of-the-art retrieval quality both within and outside its training domain,\nrevealing the promising potential and generalization capability of our model to\nserve as a universal retriever for different candidate selection tasks\nsimultaneously.", "published": "2024-02-26 02:48:43", "link": "http://arxiv.org/abs/2402.16261v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Cross-domain Chinese Sentence Pattern Parsing", "abstract": "Sentence Pattern Structure (SPS) parsing is a syntactic analysis method\nprimarily employed in language teaching.Existing SPS parsers rely heavily on\ntextbook corpora for training, lacking cross-domain capability.To overcome this\nconstraint, this paper proposes an innovative approach leveraging large\nlanguage models (LLMs) within a self-training framework. Partial syntactic\nrules from a source domain are combined with target domain sentences to\ndynamically generate training data, enhancing the adaptability of the parser to\ndiverse domains.Experiments conducted on textbook and news domains demonstrate\nthe effectiveness of the proposed method, outperforming rule-based baselines by\n1.68 points on F1 metrics.", "published": "2024-02-26 05:30:48", "link": "http://arxiv.org/abs/2402.16311v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Chain-of-Discussion: A Multi-Model Framework for Complex Evidence-Based\n  Question Answering", "abstract": "Open-ended question answering requires models to find appropriate evidence to\nform wellreasoned, comprehensive and helpful answers. In practical\napplications, models also need to engage in extended discussions on potential\nscenarios closely relevant to the question. With augmentation of retrieval\nmodule, open-source Large Language Models (LLMs) can produce coherent answers\noften with different focuses, but are still sub-optimal in terms of reliable\nevidence selection and in-depth question analysis. In this paper, we propose a\nnovel Chain-ofDiscussion framework to leverage the synergy among multiple\nopen-source LLMs aiming to provide more correct and more comprehensive answers\nfor open-ended QA, although they are not strong enough individually. Our\nexperiments show that discussions among multiple LLMs play a vital role in\nenhancing the quality of answers.", "published": "2024-02-26 05:31:34", "link": "http://arxiv.org/abs/2402.16313v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Finer: Investigating and Enhancing Fine-Grained Visual Concept\n  Recognition in Large Vision Language Models", "abstract": "Recent advances in instruction-tuned Large Vision-Language Models (LVLMs)\nhave imbued the models with the ability to generate high-level, image-grounded\nexplanations with ease. While such capability is largely attributed to the rich\nworld knowledge contained within the Large Language Models (LLMs), our work\nreveals their shortcomings in fine-grained visual categorization (FGVC) across\nsix different benchmark settings. Most recent state-of-the-art LVLMs like\nLLaVa-1.5, InstructBLIP and GPT-4V not only severely deteriorate in terms of\nclassification performance, e.g., average drop of 65.58 in EM for Stanford Dogs\nfor LLaVA-1.5, but also struggle to generate an accurate explanation with\ndetailed attributes based on the concept that appears within an input image\ndespite their capability to generate holistic image-level descriptions.\nIn-depth analyses show that instruction-tuned LVLMs exhibit modality gap,\nshowing discrepancy when given textual and visual inputs that correspond to the\nsame concept, preventing the image modality from leveraging the rich parametric\nknowledge within the LLMs. In an effort to further the community's endeavor in\nthis direction, we propose a multiple granularity attribute-centric evaluation\nbenchmark, Finer, which aims to establish a ground to evaluate LVLMs'\nfine-grained visual comprehension ability and provide significantly improved\nexplainability.", "published": "2024-02-26 05:43:51", "link": "http://arxiv.org/abs/2402.16315v4", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Unveiling the Truth and Facilitating Change: Towards Agent-based\n  Large-scale Social Movement Simulation", "abstract": "Social media has emerged as a cornerstone of social movements, wielding\nsignificant influence in driving societal change. Simulating the response of\nthe public and forecasting the potential impact has become increasingly\nimportant. However, existing methods for simulating such phenomena encounter\nchallenges concerning their efficacy and efficiency in capturing the behaviors\nof social movement participants. In this paper, we introduce a hybrid framework\nHiSim for social media user simulation, wherein users are categorized into two\ntypes. Core users are driven by Large Language Models, while numerous ordinary\nusers are modeled by deductive agent-based models. We further construct a\nTwitter-like environment to replicate their response dynamics following trigger\nevents. Subsequently, we develop a multi-faceted benchmark SoMoSiMu-Bench for\nevaluation and conduct comprehensive experiments across real-world datasets.\nExperimental results demonstrate the effectiveness and flexibility of our\nmethod.", "published": "2024-02-26 06:28:54", "link": "http://arxiv.org/abs/2402.16333v2", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "CodeS: Towards Building Open-source Language Models for Text-to-SQL", "abstract": "Language models have shown promising performance on the task of translating\nnatural language questions into SQL queries (Text-to-SQL). However, most of the\nstate-of-the-art (SOTA) approaches rely on powerful yet closed-source large\nlanguage models (LLMs), such as ChatGPT and GPT-4, which may have the\nlimitations of unclear model architectures, data privacy risks, and expensive\ninference overheads. To address the limitations, we introduce CodeS, a series\nof pre-trained language models with parameters ranging from 1B to 15B,\nspecifically designed for the text-to-SQL task. CodeS is a fully open-source\nlanguage model, which achieves superior accuracy with much smaller parameter\nsizes. This paper studies the research challenges in building CodeS. To enhance\nthe SQL generation abilities of CodeS, we adopt an incremental pre-training\napproach using a specifically curated SQL-centric corpus. Based on this, we\naddress the challenges of schema linking and rapid domain adaptation through\nstrategic prompt construction and a bi-directional data augmentation technique.\nWe conduct comprehensive evaluations on multiple datasets, including the widely\nused Spider benchmark, the newly released BIRD benchmark, robustness-diagnostic\nbenchmarks such as Spider-DK, Spider-Syn, Spider-Realistic, and Dr.Spider, as\nwell as two real-world datasets created for financial and academic\napplications. The experimental results show that our CodeS achieves new SOTA\naccuracy and robustness on nearly all challenging text-to-SQL benchmarks.", "published": "2024-02-26 07:00:58", "link": "http://arxiv.org/abs/2402.16347v1", "categories": ["cs.CL", "cs.DB"], "primary_category": "cs.CL"}
{"title": "MathGenie: Generating Synthetic Data with Question Back-translation for\n  Enhancing Mathematical Reasoning of LLMs", "abstract": "Large language models (LLMs) have exhibited great potential in mathematical\nreasoning. However, there remains a performance gap in this area between\nexisting open-source models and closed-source models such as GPT-4. In this\npaper, we introduce MathGenie, a novel method for generating diverse and\nreliable math problems from a small-scale problem-solution dataset (denoted as\nseed data). We augment the ground-truth solutions of our seed data and train a\nback-translation model to translate the augmented solutions back into new\nquestions. Subsequently, we generate code-integrated solutions for the new\nquestions. To ensure the correctness of the code-integrated solutions, we\nemploy rationale-based strategy for solution verification. Various pretrained\nmodels, ranging from 7B to 70B, are trained on the newly curated data to test\nthe effectiveness of the proposed augmentation technique, resulting in a family\nof models known as MathGenieLM. These models consistently outperform previous\nopen-source models across five representative mathematical reasoning datasets,\nachieving state-of-the-art performance. In particular, MathGenieLM-InternLM2\nachieves an accuracy of 87.7% on GSM8K and 55.7% on MATH, securing the best\noverall score among open-source language models.", "published": "2024-02-26 07:17:25", "link": "http://arxiv.org/abs/2402.16352v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Layer-wise Regularized Dropout for Neural Language Models", "abstract": "Among the various pre-trained neural language models that are popular today,\ndropout is already an indispensable regularization technique. To solve the\ninconsistency between training and inference caused by the randomness of\ndropout, some studies use consistency training to regularize dropout at the\noutput layer. In this paper, we propose a novel Layer-wise Regularized Dropout\n(LR-Drop), which is specially designed for Transformer-based Language models.\nSpecifically, LR-Drop layer-wise regularizes each Transformer layer using the\nconsistency training strategy. Each training sample passes through the two\nsiamese sub-models sampled by dropout, and then LR-Drop forces the hidden\nstates, multi-head attention matrices, and output distribution of the two\nsiamese sub-models to be consistent. The proposed LR-Drop can be regarded as a\n\"self-distillation\" framework, in which each sub-model generated by dropout is\nthe other's \"teacher\" model and \"student\" model. Through extensive experiments\non 8 natural language understanding datasets, 6 neural machine translation\ndatasets, and 1 abstractive summarization dataset (a total of 15 datasets), we\nshow that LR-Drop achieves superior performances, including state-of-the-art\nresults.", "published": "2024-02-26 07:31:35", "link": "http://arxiv.org/abs/2402.16361v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLM Inference Unveiled: Survey and Roofline Model Insights", "abstract": "The field of efficient Large Language Model (LLM) inference is rapidly\nevolving, presenting a unique blend of opportunities and challenges. Although\nthe field has expanded and is vibrant, there hasn't been a concise framework\nthat analyzes the various methods of LLM Inference to provide a clear\nunderstanding of this domain. Our survey stands out from traditional literature\nreviews by not only summarizing the current state of research but also by\nintroducing a framework based on roofline model for systematic analysis of LLM\ninference techniques. This framework identifies the bottlenecks when deploying\nLLMs on hardware devices and provides a clear understanding of practical\nproblems, such as why LLMs are memory-bound, how much memory and computation\nthey need, and how to choose the right hardware. We systematically collate the\nlatest advancements in efficient LLM inference, covering crucial areas such as\nmodel compression (e.g., Knowledge Distillation and Quantization), algorithm\nimprovements (e.g., Early Exit and Mixture-of-Expert), and both hardware and\nsystem-level enhancements. Our survey stands out by analyzing these methods\nwith roofline model, helping us understand their impact on memory access and\ncomputation. This distinctive approach not only showcases the current research\nlandscape but also delivers valuable insights for practical implementation,\npositioning our work as an indispensable resource for researchers new to the\nfield as well as for those seeking to deepen their understanding of efficient\nLLM deployment. The analyze tool, LLM-Viewer, is open-sourced.", "published": "2024-02-26 07:33:05", "link": "http://arxiv.org/abs/2402.16363v6", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TEaR: Improving LLM-based Machine Translation with Systematic\n  Self-Refinement", "abstract": "Large Language Models (LLMs) have achieved impressive results in Machine\nTranslation (MT). However, careful evaluations by human reveal that the\ntranslations produced by LLMs still contain multiple errors. Importantly,\nfeeding back such error information into the LLMs can lead to self-refinement\nand result in improved translation performance. Motivated by these insights, we\nintroduce a systematic LLM-based self-refinement translation framework, named\n\\textbf{TEaR}, which stands for \\textbf{T}ranslate, \\textbf{E}stimate,\n\\textbf{a}nd \\textbf{R}efine, marking a significant step forward in this\ndirection. Our findings demonstrate that 1) our self-refinement framework\nsuccessfully assists LLMs in improving their translation quality across a wide\nrange of languages, whether it's from high-resource languages to low-resource\nones or whether it's English-centric or centered around other languages; 2)\nTEaR exhibits superior systematicity and interpretability; 3) different\nestimation strategies yield varied impacts, directly affecting the\neffectiveness of the final corrections. Additionally, traditional neural\ntranslation models and evaluation models operate separately, often focusing on\nsingular tasks due to their limited capabilities, while general-purpose LLMs\npossess the capability to undertake both tasks simultaneously. We further\nconduct cross-model correction experiments to investigate the potential\nrelationship between the translation and evaluation capabilities of\ngeneral-purpose LLMs. Our code and data are available at\nhttps://github.com/fzp0424/self_correct_mt", "published": "2024-02-26 07:58:12", "link": "http://arxiv.org/abs/2402.16379v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MoZIP: A Multilingual Benchmark to Evaluate Large Language Models in\n  Intellectual Property", "abstract": "Large language models (LLMs) have demonstrated impressive performance in\nvarious natural language processing (NLP) tasks. However, there is limited\nunderstanding of how well LLMs perform in specific domains (e.g, the\nintellectual property (IP) domain). In this paper, we contribute a new\nbenchmark, the first Multilingual-oriented quiZ on Intellectual Property\n(MoZIP), for the evaluation of LLMs in the IP domain. The MoZIP benchmark\nincludes three challenging tasks: IP multiple-choice quiz (IPQuiz), IP question\nanswering (IPQA), and patent matching (PatentMatch). In addition, we also\ndevelop a new IP-oriented multilingual large language model (called MoZi),\nwhich is a BLOOMZ-based model that has been supervised fine-tuned with\nmultilingual IP-related text data. We evaluate our proposed MoZi model and four\nwell-known LLMs (i.e., BLOOMZ, BELLE, ChatGLM and ChatGPT) on the MoZIP\nbenchmark. Experimental results demonstrate that MoZi outperforms BLOOMZ, BELLE\nand ChatGLM by a noticeable margin, while it had lower scores compared with\nChatGPT. Notably, the performance of current LLMs on the MoZIP benchmark has\nmuch room for improvement, and even the most powerful ChatGPT does not reach\nthe passing level. Our source code, data, and models are available at\n\\url{https://github.com/AI-for-Science/MoZi}.", "published": "2024-02-26 08:27:50", "link": "http://arxiv.org/abs/2402.16389v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Defending LLMs against Jailbreaking Attacks via Backtranslation", "abstract": "Although many large language models (LLMs) have been trained to refuse\nharmful requests, they are still vulnerable to jailbreaking attacks which\nrewrite the original prompt to conceal its harmful intent. In this paper, we\npropose a new method for defending LLMs against jailbreaking attacks by\n``backtranslation''. Specifically, given an initial response generated by the\ntarget LLM from an input prompt, our backtranslation prompts a language model\nto infer an input prompt that can lead to the response. The inferred prompt is\ncalled the backtranslated prompt which tends to reveal the actual intent of the\noriginal prompt, since it is generated based on the LLM's response and not\ndirectly manipulated by the attacker. We then run the target LLM again on the\nbacktranslated prompt, and we refuse the original prompt if the model refuses\nthe backtranslated prompt. We explain that the proposed defense provides\nseveral benefits on its effectiveness and efficiency. We empirically\ndemonstrate that our defense significantly outperforms the baselines, in the\ncases that are hard for the baselines, and our defense also has little impact\non the generation quality for benign input prompts. Our implementation is based\non our library for LLM jailbreaking defense algorithms at\n\\url{https://github.com/YihanWang617/llm-jailbreaking-defense}, and the code\nfor reproducing our experiments is available at\n\\url{https://github.com/YihanWang617/LLM-Jailbreaking-Defense-Backtranslation}.", "published": "2024-02-26 10:03:33", "link": "http://arxiv.org/abs/2402.16459v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "mEdIT: Multilingual Text Editing via Instruction Tuning", "abstract": "We introduce mEdIT, a multi-lingual extension to CoEdIT -- the recent\nstate-of-the-art text editing models for writing assistance. mEdIT models are\ntrained by fine-tuning multi-lingual large, pre-trained language models (LLMs)\nvia instruction tuning. They are designed to take instructions from the user\nspecifying the attributes of the desired text in the form of natural language\ninstructions, such as Grammatik korrigieren (German) or Parafrasee la oraci\\'on\n(Spanish). We build mEdIT by curating data from multiple publicly available\nhuman-annotated text editing datasets for three text editing tasks (Grammatical\nError Correction (GEC), Text Simplification, and Paraphrasing) across diverse\nlanguages belonging to six different language families. We detail the design\nand training of mEdIT models and demonstrate their strong performance on many\nmulti-lingual text editing benchmarks against other multilingual LLMs. We also\nfind that mEdIT generalizes effectively to new languages over multilingual\nbaselines. We publicly release our data, code, and trained models at\nhttps://github.com/vipulraheja/medit.", "published": "2024-02-26 10:33:36", "link": "http://arxiv.org/abs/2402.16472v2", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Pre-training Cross-lingual Open Domain Question Answering with\n  Large-scale Synthetic Supervision", "abstract": "Cross-lingual open domain question answering (CLQA) is a complex problem,\ncomprising cross-lingual retrieval from a multilingual knowledge base, followed\nby answer generation in the query language. Both steps are usually tackled by\nseparate models, requiring substantial annotated datasets, and typically\nauxiliary resources, like machine translation systems to bridge between\nlanguages. In this paper, we show that CLQA can be addressed using a single\nencoder-decoder model. To effectively train this model, we propose a\nself-supervised method based on exploiting the cross-lingual link structure\nwithin Wikipedia. We demonstrate how linked Wikipedia pages can be used to\nsynthesise supervisory signals for cross-lingual retrieval, through a form of\ncloze query, and generate more natural questions to supervise answer\ngeneration. Together, we show our approach, \\texttt{CLASS}, outperforms\ncomparable methods on both supervised and zero-shot language adaptation\nsettings, including those using machine translation.", "published": "2024-02-26 11:42:29", "link": "http://arxiv.org/abs/2402.16508v3", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "LLM-based Privacy Data Augmentation Guided by Knowledge Distillation\n  with a Distribution Tutor for Medical Text Classification", "abstract": "As sufficient data are not always publically accessible for model training,\nresearchers exploit limited data with advanced learning algorithms or expand\nthe dataset via data augmentation (DA). Conducting DA in private domain\nrequires private protection approaches (i.e. anonymization and perturbation),\nbut those methods cannot provide protection guarantees. Differential privacy\n(DP) learning methods theoretically bound the protection but are not skilled at\ngenerating pseudo text samples with large models. In this paper, we transfer\nDP-based pseudo sample generation task to DP-based generated samples\ndiscrimination task, where we propose a DP-based DA method with a LLM and a\nDP-based discriminator for text classification on private domains. We construct\na knowledge distillation model as the DP-based discriminator: teacher models,\naccessing private data, teaches students how to select private samples with\ncalibrated noise to achieve DP. To constrain the distribution of DA's\ngeneration, we propose a DP-based tutor that models the noised private\ndistribution and controls samples' generation with a low privacy cost. We\ntheoretically analyze our model's privacy protection and empirically verify our\nmodel.", "published": "2024-02-26 11:52:55", "link": "http://arxiv.org/abs/2402.16515v1", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Multi-Bit Distortion-Free Watermarking for Large Language Models", "abstract": "Methods for watermarking large language models have been proposed that\ndistinguish AI-generated text from human-generated text by slightly altering\nthe model output distribution, but they also distort the quality of the text,\nexposing the watermark to adversarial detection. More recently, distortion-free\nwatermarking methods were proposed that require a secret key to detect the\nwatermark. The prior methods generally embed zero-bit watermarks that do not\nprovide additional information beyond tagging a text as being AI-generated. We\nextend an existing zero-bit distortion-free watermarking method by embedding\nmultiple bits of meta-information as part of the watermark. We also develop a\ncomputationally efficient decoder that extracts the embedded information from\nthe watermark with low bit error rate.", "published": "2024-02-26 14:01:34", "link": "http://arxiv.org/abs/2402.16578v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PAQA: Toward ProActive Open-Retrieval Question Answering", "abstract": "Conversational systems have made significant progress in generating natural\nlanguage responses. However, their potential as conversational search systems\nis currently limited due to their passive role in the information-seeking\nprocess. One major limitation is the scarcity of datasets that provide labelled\nambiguous questions along with a supporting corpus of documents and relevant\nclarifying questions. This work aims to tackle the challenge of generating\nrelevant clarifying questions by taking into account the inherent ambiguities\npresent in both user queries and documents. To achieve this, we propose PAQA,\nan extension to the existing AmbiNQ dataset, incorporating clarifying\nquestions. We then evaluate various models and assess how passage retrieval\nimpacts ambiguity detection and the generation of clarifying questions. By\naddressing this gap in conversational search systems, we aim to provide\nadditional supervision to enhance their active participation in the\ninformation-seeking process and provide users with more accurate results.", "published": "2024-02-26 14:40:34", "link": "http://arxiv.org/abs/2402.16608v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "RepoAgent: An LLM-Powered Open-Source Framework for Repository-level\n  Code Documentation Generation", "abstract": "Generative models have demonstrated considerable potential in software\nengineering, particularly in tasks such as code generation and debugging.\nHowever, their utilization in the domain of code documentation generation\nremains underexplored. To this end, we introduce RepoAgent, a large language\nmodel powered open-source framework aimed at proactively generating,\nmaintaining, and updating code documentation. Through both qualitative and\nquantitative evaluations, we have validated the effectiveness of our approach,\nshowing that RepoAgent excels in generating high-quality repository-level\ndocumentation. The code and results are publicly accessible at\nhttps://github.com/OpenBMB/RepoAgent.", "published": "2024-02-26 15:39:52", "link": "http://arxiv.org/abs/2402.16667v1", "categories": ["cs.CL", "cs.AI", "I.2.7; F.2.2"], "primary_category": "cs.CL"}
{"title": "Adaptation of Biomedical and Clinical Pretrained Models to French Long\n  Documents: A Comparative Study", "abstract": "Recently, pretrained language models based on BERT have been introduced for\nthe French biomedical domain. Although these models have achieved\nstate-of-the-art results on biomedical and clinical NLP tasks, they are\nconstrained by a limited input sequence length of 512 tokens, which poses\nchallenges when applied to clinical notes. In this paper, we present a\ncomparative study of three adaptation strategies for long-sequence models,\nleveraging the Longformer architecture. We conducted evaluations of these\nmodels on 16 downstream tasks spanning both biomedical and clinical domains.\nOur findings reveal that further pre-training an English clinical model with\nFrench biomedical texts can outperform both converting a French biomedical BERT\nto the Longformer architecture and pre-training a French biomedical Longformer\nfrom scratch. The results underscore that long-sequence French biomedical\nmodels improve performance across most downstream tasks regardless of sequence\nlength, but BERT based models remain the most efficient for named entity\nrecognition tasks.", "published": "2024-02-26 16:05:33", "link": "http://arxiv.org/abs/2402.16689v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Generating Effective Ensembles for Sentiment Analysis", "abstract": "In recent years, transformer models have revolutionized Natural Language\nProcessing (NLP), achieving exceptional results across various tasks, including\nSentiment Analysis (SA). As such, current state-of-the-art approaches for SA\npredominantly rely on transformer models alone, achieving impressive accuracy\nlevels on benchmark datasets. In this paper, we show that the key for further\nimproving the accuracy of such ensembles for SA is to include not only\ntransformers, but also traditional NLP models, despite the inferiority of the\nlatter compared to transformer models. However, as we empirically show, this\nnecessitates a change in how the ensemble is constructed, specifically relying\non the Hierarchical Ensemble Construction (HEC) algorithm we present. Our\nempirical studies across eight canonical SA datasets reveal that ensembles\nincorporating a mix of model types, structured via HEC, significantly\noutperform traditional ensembles. Finally, we provide a comparative analysis of\nthe performance of the HEC and GPT-4, demonstrating that while GPT-4 closely\napproaches state-of-the-art SA methods, it remains outperformed by our proposed\nensemble strategy.", "published": "2024-02-26 16:14:47", "link": "http://arxiv.org/abs/2402.16700v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Value Preferences Estimation and Disambiguation in Hybrid Participatory\n  Systems", "abstract": "Understanding citizens' values in participatory systems is crucial for\ncitizen-centric policy-making. We envision a hybrid participatory system where\nparticipants make choices and provide motivations for those choices, and AI\nagents estimate their value preferences by interacting with them. We focus on\nsituations where a conflict is detected between participants' choices and\nmotivations, and propose methods for estimating value preferences while\naddressing detected inconsistencies by interacting with the participants. We\noperationalize the philosophical stance that \"valuing is deliberatively\nconsequential.\" That is, if a participant's choice is based on a deliberation\nof value preferences, the value preferences can be observed in the motivation\nthe participant provides for the choice. Thus, we propose and compare value\npreferences estimation methods that prioritize the values estimated from\nmotivations over the values estimated from choices alone. Then, we introduce a\ndisambiguation strategy that combines Natural Language Processing and Active\nLearning to address the detected inconsistencies between choices and\nmotivations. We evaluate the proposed methods on a dataset of a large-scale\nsurvey on energy transition. The results show that explicitly addressing\ninconsistencies between choices and motivations improves the estimation of an\nindividual's value preferences. The disambiguation strategy does not show\nsubstantial improvements when compared to similar baselines--however, we\ndiscuss how the novelty of the approach can open new research avenues and\npropose improvements to address the current limitations.", "published": "2024-02-26 17:16:28", "link": "http://arxiv.org/abs/2402.16751v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "CorpusBrain++: A Continual Generative Pre-Training Framework for\n  Knowledge-Intensive Language Tasks", "abstract": "Knowledge-intensive language tasks (KILTs) typically require retrieving\nrelevant documents from trustworthy corpora, e.g., Wikipedia, to produce\nspecific answers. Very recently, a pre-trained generative retrieval model for\nKILTs, named CorpusBrain, was proposed and reached new state-of-the-art\nretrieval performance. However, most existing research on KILTs, including\nCorpusBrain, has predominantly focused on a static document collection,\noverlooking the dynamic nature of real-world scenarios, where new documents are\ncontinuously being incorporated into the source corpus. To address this gap, it\nis crucial to explore the capability of retrieval models to effectively handle\nthe dynamic retrieval scenario inherent in KILTs.\n  In this work, we first introduce the continual document learning (CDL) task\nfor KILTs and build a novel benchmark dataset named KILT++ based on the\noriginal KILT dataset for evaluation. Then, we conduct a comprehensive study\nover the use of pre-trained CorpusBrain on KILT++. Unlike the promising results\nin the stationary scenario, CorpusBrain is prone to catastrophic forgetting in\nthe dynamic scenario, hence hampering the retrieval performance. To alleviate\nthis issue, we propose CorpusBrain++, a continual generative pre-training\nframework. Empirical results demonstrate the significant effectiveness and\nremarkable efficiency of CorpusBrain++ in comparison to both traditional and\ngenerative IR methods.", "published": "2024-02-26 17:35:44", "link": "http://arxiv.org/abs/2402.16767v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "A Comprehensive Evaluation of Quantization Strategies for Large Language\n  Models", "abstract": "Increasing the number of parameters in large language models (LLMs) usually\nimproves performance in downstream tasks but raises compute and memory costs,\nmaking deployment difficult in resource-limited settings. Quantization\ntechniques, which reduce the bits needed for model weights or activations with\nminimal performance loss, have become popular due to the rise of LLMs. However,\nmost quantization studies use pre-trained LLMs, and the impact of quantization\non instruction-tuned LLMs and the relationship between perplexity and benchmark\nperformance of quantized LLMs are not well understood. Evaluation of quantized\nLLMs is often limited to language modeling and a few classification tasks,\nleaving their performance on other benchmarks unclear. To address these gaps,\nwe propose a structured evaluation framework consisting of three critical\ndimensions: (1) knowledge \\& capacity, (2) alignment, and (3) efficiency, and\nconduct extensive experiments across ten diverse benchmarks. Our experimental\nresults indicate that LLMs with 4-bit quantization can retain performance\ncomparable to their non-quantized counterparts, and perplexity can serve as a\nproxy metric for quantized LLMs on most benchmarks. Furthermore, quantized LLMs\nwith larger parameter scales can outperform smaller LLMs. Despite the memory\nsavings achieved through quantization, it can also slow down the inference\nspeed of LLMs. Consequently, substantial engineering efforts and hardware\nsupport are imperative to achieve a balanced optimization of decoding speed and\nmemory consumption in the context of quantized LLMs.", "published": "2024-02-26 17:45:36", "link": "http://arxiv.org/abs/2402.16775v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Political Compass or Spinning Arrow? Towards More Meaningful Evaluations\n  for Values and Opinions in Large Language Models", "abstract": "Much recent work seeks to evaluate values and opinions in large language\nmodels (LLMs) using multiple-choice surveys and questionnaires. Most of this\nwork is motivated by concerns around real-world LLM applications. For example,\npolitically-biased LLMs may subtly influence society when they are used by\nmillions of people. Such real-world concerns, however, stand in stark contrast\nto the artificiality of current evaluations: real users do not typically ask\nLLMs survey questions. Motivated by this discrepancy, we challenge the\nprevailing constrained evaluation paradigm for values and opinions in LLMs and\nexplore more realistic unconstrained evaluations. As a case study, we focus on\nthe popular Political Compass Test (PCT). In a systematic review, we find that\nmost prior work using the PCT forces models to comply with the PCT's\nmultiple-choice format. We show that models give substantively different\nanswers when not forced; that answers change depending on how models are\nforced; and that answers lack paraphrase robustness. Then, we demonstrate that\nmodels give different answers yet again in a more realistic open-ended answer\nsetting. We distill these findings into recommendations and open challenges in\nevaluating values and opinions in LLMs.", "published": "2024-02-26 18:00:49", "link": "http://arxiv.org/abs/2402.16786v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Survey on Data Selection for Language Models", "abstract": "A major factor in the recent success of large language models is the use of\nenormous and ever-growing text datasets for unsupervised pre-training. However,\nnaively training a model on all available data may not be optimal (or\nfeasible), as the quality of available text data can vary. Filtering out data\ncan also decrease the carbon footprint and financial costs of training models\nby reducing the amount of training required. Data selection methods aim to\ndetermine which candidate data points to include in the training dataset and\nhow to appropriately sample from the selected data points. The promise of\nimproved data selection methods has caused the volume of research in the area\nto rapidly expand. However, because deep learning is mostly driven by empirical\nevidence and experimentation on large-scale data is expensive, few\norganizations have the resources for extensive data selection research.\nConsequently, knowledge of effective data selection practices has become\nconcentrated within a few organizations, many of which do not openly share\ntheir findings and methodologies. To narrow this gap in knowledge, we present a\ncomprehensive review of existing literature on data selection methods and\nrelated research areas, providing a taxonomy of existing approaches. By\ndescribing the current landscape of research, this work aims to accelerate\nprogress in data selection by establishing an entry point for new and\nestablished researchers. Additionally, throughout this review we draw attention\nto noticeable holes in the literature and conclude the paper by proposing\npromising avenues for future research.", "published": "2024-02-26 18:54:35", "link": "http://arxiv.org/abs/2402.16827v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "GISTEmbed: Guided In-sample Selection of Training Negatives for Text\n  Embedding Fine-tuning", "abstract": "Embedding models are integral to AI applications like semantic search,\npersonalized recommendations, and retrieval augmented generation for LLMs,\nnecessitating high-quality training data. However, the limited scalability of\nmanual data curation prompts the need for automated methods to ensure data\nintegrity. Traditional unsupervised triplet mining automates training data\ngeneration, crucial for embedding model training, yet inadvertently injects\nbiases and noise, thereby degrading model performance. Addressing this, we\nintroduce GISTEmbed, a novel strategy that enhances in-batch negative selection\nduring contrastive training through a guide model. This approach departs from\nreliance on random sampling and equal utility assumption of batch negatives,\nsignificantly reducing noise from data quality issues and improving model\nfine-tuning. Benchmarked against the Massive Text Embedding Benchmark (MTEB),\nGISTEmbed showcases consistent performance improvements across various model\nsizes and achieves state-of-the-art results in select categories. This\nframework enables significant enhancements for smaller models by leveraging the\ncapabilities of powerful yet resource-intensive large models. GISTEmbed can\npotentially revolutionize the creation of highly efficient, smaller models,\ndemocratizing access to advanced AI technologies. Making these technologies\nmore accessible and cost-effective, especially for applications constrained by\nresources, significantly expands the impact and accessibility of\nstate-of-the-art AI solutions across diverse sectors.", "published": "2024-02-26 18:55:15", "link": "http://arxiv.org/abs/2402.16829v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Dealing with Data for RE: Mitigating Challenges while using NLP and\n  Generative AI", "abstract": "Across the dynamic business landscape today, enterprises face an\never-increasing range of challenges. These include the constantly evolving\nregulatory environment, the growing demand for personalization within software\napplications, and the heightened emphasis on governance. In response to these\nmultifaceted demands, large enterprises have been adopting automation that\nspans from the optimization of core business processes to the enhancement of\ncustomer experiences. Indeed, Artificial Intelligence (AI) has emerged as a\npivotal element of modern software systems. In this context, data plays an\nindispensable role. AI-centric software systems based on supervised learning\nand operating at an industrial scale require large volumes of training data to\nperform effectively. Moreover, the incorporation of generative AI has led to a\ngrowing demand for adequate evaluation benchmarks. Our experience in this field\nhas revealed that the requirement for large datasets for training and\nevaluation introduces a host of intricate challenges. This book chapter\nexplores the evolving landscape of Software Engineering (SE) in general, and\nRequirements Engineering (RE) in particular, in this era marked by AI\nintegration. We discuss challenges that arise while integrating Natural\nLanguage Processing (NLP) and generative AI into enterprise-critical software\nsystems. The chapter provides practical insights, solutions, and examples to\nequip readers with the knowledge and tools necessary for effectively building\nsolutions with NLP at their cores. We also reflect on how these text\ndata-centric tasks sit together with the traditional RE process. We also\nhighlight new RE tasks that may be necessary for handling the increasingly\nimportant text data-centricity involved in developing software systems.", "published": "2024-02-26 19:19:47", "link": "http://arxiv.org/abs/2402.16977v2", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Long Dialog Summarization: An Analysis", "abstract": "Dialog summarization has become increasingly important in managing and\ncomprehending large-scale conversations across various domains. This task\npresents unique challenges in capturing the key points, context, and nuances of\nmulti-turn long conversations for summarization. It is worth noting that the\nsummarization techniques may vary based on specific requirements such as in a\nshopping-chatbot scenario, the dialog summary helps to learn user preferences,\nwhereas in the case of a customer call center, the summary may involve the\nproblem attributes that a user specified, and the final resolution provided.\nThis work emphasizes the significance of creating coherent and contextually\nrich summaries for effective communication in various applications. We explore\ncurrent state-of-the-art approaches for long dialog summarization in different\ndomains and benchmark metrics based evaluations show that one single model does\nnot perform well across various areas for distinct summarization tasks.", "published": "2024-02-26 19:35:45", "link": "http://arxiv.org/abs/2402.16986v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Generative Retrieval with Large Language Models", "abstract": "When completing knowledge-intensive tasks, humans sometimes need not just an\nanswer but also a corresponding reference passage for auxiliary reading.\nPrevious methods required obtaining pre-segmented article chunks through\nadditional retrieval models. This paper explores leveraging the parameterized\nknowledge stored during the pre-training phase of large language models (LLMs)\nto independently recall reference passage from any starting position. We\npropose a two-stage framework that simulates the scenario of humans recalling\neasily forgotten references. Initially, the LLM is prompted to recall document\ntitle identifiers to obtain a coarse-grained document set. Then, based on the\nacquired coarse-grained document set, it recalls fine-grained passage. In the\ntwo-stage recall process, we use constrained decoding to ensure that content\noutside of the stored documents is not generated. To increase speed, we only\nrecall a short prefix in the second stage, then locate its position to retrieve\na complete passage. Experiments on KILT knowledge-sensitive tasks have verified\nthat LLMs can independently recall reference passage location in various task\nforms, and the obtained reference significantly assist downstream tasks.", "published": "2024-02-26 20:35:32", "link": "http://arxiv.org/abs/2402.17010v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Leveraging Large Language Models for Learning Complex Legal Concepts\n  through Storytelling", "abstract": "Making legal knowledge accessible to non-experts is crucial for enhancing\ngeneral legal literacy and encouraging civic participation in democracy.\nHowever, legal documents are often challenging to understand for people without\nlegal backgrounds. In this paper, we present a novel application of large\nlanguage models (LLMs) in legal education to help non-experts learn intricate\nlegal concepts through storytelling, an effective pedagogical tool in conveying\ncomplex and abstract concepts. We also introduce a new dataset LegalStories,\nwhich consists of 294 complex legal doctrines, each accompanied by a story and\na set of multiple-choice questions generated by LLMs. To construct the dataset,\nwe experiment with various LLMs to generate legal stories explaining these\nconcepts. Furthermore, we use an expert-in-the-loop approach to iteratively\ndesign multiple-choice questions. Then, we evaluate the effectiveness of\nstorytelling with LLMs through randomized controlled trials (RCTs) with legal\nnovices on 10 samples from the dataset. We find that LLM-generated stories\nenhance comprehension of legal concepts and interest in law among non-native\nspeakers compared to only definitions. Moreover, stories consistently help\nparticipants relate legal concepts to their lives. Finally, we find that\nlearning with stories shows a higher retention rate for non-native speakers in\nthe follow-up assessment. Our work has strong implications for using LLMs in\npromoting teaching and learning in the legal field and beyond.", "published": "2024-02-26 20:56:06", "link": "http://arxiv.org/abs/2402.17019v4", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "DenseMamba: State Space Models with Dense Hidden Connection for\n  Efficient Large Language Models", "abstract": "Large language models (LLMs) face a daunting challenge due to the excessive\ncomputational and memory requirements of the commonly used Transformer\narchitecture. While state space model (SSM) is a new type of foundational\nnetwork architecture offering lower computational complexity, their performance\nhas yet to fully rival that of Transformers. This paper introduces DenseSSM, a\nnovel approach to enhance the flow of hidden information between layers in\nSSMs. By selectively integrating shallowlayer hidden states into deeper layers,\nDenseSSM retains fine-grained information crucial for the final output. Dense\nconnections enhanced DenseSSM still maintains the training parallelizability\nand inference efficiency. The proposed method can be widely applicable to\nvarious SSM types like RetNet and Mamba. With similar model size, DenseSSM\nachieves significant improvements, exemplified by DenseRetNet outperforming the\noriginal RetNet with up to 5% accuracy improvement on public benchmarks. code\nis avalaible at https://github.com/WailordHe/DenseSSM", "published": "2024-02-26 09:21:59", "link": "http://arxiv.org/abs/2403.00818v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Retrieval Augmented Generation Systems: Automatic Dataset Creation,\n  Evaluation and Boolean Agent Setup", "abstract": "Retrieval Augmented Generation (RAG) systems have seen huge popularity in\naugmenting Large-Language Model (LLM) outputs with domain specific and time\nsensitive data. Very recently a shift is happening from simple RAG setups that\nquery a vector database for additional information with every user input to\nmore sophisticated forms of RAG. However, different concrete approaches compete\non mostly anecdotal evidence at the moment. In this paper we present a rigorous\ndataset creation and evaluation workflow to quantitatively compare different\nRAG strategies. We use a dataset created this way for the development and\nevaluation of a boolean agent RAG setup: A system in which a LLM can decide\nwhether to query a vector database or not, thus saving tokens on questions that\ncan be answered with internal knowledge. We publish our code and generated\ndataset online.", "published": "2024-02-26 12:56:17", "link": "http://arxiv.org/abs/2403.00820v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Adapting to Teammates in a Cooperative Language Game", "abstract": "The game of Codenames has recently emerged as a domain of interest for\nintelligent agent design. The game is unique due to the way that language and\ncoordination between teammates play important roles. Previous approaches to\ndesigning agents for this game have utilized a single internal language model\nto determine action choices. This often leads to good performance with some\nteammates and inferior performance with other teammates, as the agent cannot\nadapt to any specific teammate. In this paper we present the first adaptive\nagent for playing Codenames. We adopt an ensemble approach with the goal of\ndetermining, during the course of interacting with a specific teammate, which\nof our internal expert agents, each potentially with its own language model, is\nthe best match. One difficulty faced in this approach is the lack of a single\nnumerical metric that accurately captures the performance of a Codenames team.\nPrior Codenames research has utilized a handful of different metrics to\nevaluate agent teams. We propose a novel single metric to evaluate the\nperformance of a Codenames team, whether playing a single team (solitaire)\ngame, or a competitive game against another team. We then present and analyze\nan ensemble agent which selects an internal expert on each turn in order to\nmaximize this proposed metric. Experimental analysis shows that this ensemble\napproach adapts to individual teammates and often performs nearly as well as\nthe best internal expert with a teammate. Crucially, this success does not\ndepend on any previous knowledge about the teammates, the ensemble agents, or\ntheir compatibility. This research represents an important step to making\nlanguage-based agents for cooperative language settings like Codenames more\nadaptable to individual teammates.", "published": "2024-02-26 23:15:07", "link": "http://arxiv.org/abs/2403.00823v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Social Orientation: A New Feature for Dialogue Analysis", "abstract": "There are many settings where it is useful to predict and explain the success\nor failure of a dialogue. Circumplex theory from psychology models the social\norientations (e.g., Warm-Agreeable, Arrogant-Calculating) of conversation\nparticipants and can be used to predict and explain the outcome of social\ninteractions. Our work is novel in its systematic application of social\norientation tags to modeling conversation outcomes. In this paper, we introduce\na new data set of dialogue utterances machine-labeled with social orientation\ntags. We show that social orientation tags improve task performance, especially\nin low-resource settings, on both English and Chinese language benchmarks. We\nalso demonstrate how social orientation tags help explain the outcomes of\nsocial interactions when used in neural models. Based on these results showing\nthe utility of social orientation tags for dialogue outcome prediction tasks,\nwe release our data sets, code, and models that are fine-tuned to predict\nsocial orientation tags on dialogue utterances.", "published": "2024-02-26 01:55:45", "link": "http://arxiv.org/abs/2403.04770v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning Translations: Emergent Communication Pretraining for\n  Cooperative Language Acquisition", "abstract": "In Emergent Communication (EC) agents learn to communicate with one another,\nbut the protocols that they develop are specialised to their training\ncommunity. This observation led to research into Zero-Shot Coordination (ZSC)\nfor learning communication strategies that are robust to agents not encountered\nduring training. However, ZSC typically assumes that no prior data is available\nabout the agents that will be encountered in the zero-shot setting. In many\ncases, this presents an unnecessarily hard problem and rules out communication\nvia preestablished conventions. We propose a novel AI challenge called a\nCooperative Language Acquisition Problem (CLAP) in which the ZSC assumptions\nare relaxed by allowing a 'joiner' agent to learn from a dataset of\ninteractions between agents in a target community. We propose and compare two\nmethods for solving CLAPs: Imitation Learning (IL), and Emergent Communication\npretraining and Translation Learning (ECTL), in which an agent is trained in\nself-play with EC and then learns from the data to translate between the\nemergent protocol and the target community's protocol.", "published": "2024-02-26 02:13:36", "link": "http://arxiv.org/abs/2402.16247v1", "categories": ["cs.LG", "cs.CL", "cs.MA"], "primary_category": "cs.LG"}
{"title": "A Self-matching Training Method with Annotation Embedding Models for\n  Ontology Subsumption Prediction", "abstract": "Recently, ontology embeddings representing entities in a low-dimensional\nspace have been proposed for ontology completion. However, the ontology\nembeddings for concept subsumption prediction do not address the difficulties\nof similar and isolated entities and fail to extract the global information of\nannotation axioms from an ontology. In this paper, we propose a self-matching\ntraining method for the two ontology embedding models: Inverted-index Matrix\nEmbedding (InME) and Co-occurrence Matrix Embedding (CoME). The two embeddings\ncapture the global and local information in annotation axioms by means of the\noccurring locations of each word in a set of axioms and the co-occurrences of\nwords in each axiom. The self-matching training method increases the robustness\nof the concept subsumption prediction when predicted superclasses are similar\nto subclasses and are isolated to other entities in an ontology. Our evaluation\nexperiments show that the self-matching training method with InME outperforms\nthe existing ontology embeddings for the GO and FoodOn ontologies and that the\nmethod with the concatenation of CoME and OWL2Vec* outperforms them for the\nHeLiS ontology.", "published": "2024-02-26 03:46:01", "link": "http://arxiv.org/abs/2402.16278v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "PerLTQA: A Personal Long-Term Memory Dataset for Memory Classification,\n  Retrieval, and Synthesis in Question Answering", "abstract": "Long-term memory plays a critical role in personal interaction, considering\nlong-term memory can better leverage world knowledge, historical information,\nand preferences in dialogues. Our research introduces PerLTQA, an innovative QA\ndataset that combines semantic and episodic memories, including world\nknowledge, profiles, social relationships, events, and dialogues. This dataset\nis collected to investigate the use of personalized memories, focusing on\nsocial interactions and events in the QA task. PerLTQA features two types of\nmemory and a comprehensive benchmark of 8,593 questions for 30 characters,\nfacilitating the exploration and application of personalized memories in Large\nLanguage Models (LLMs). Based on PerLTQA, we propose a novel framework for\nmemory integration and generation, consisting of three main components: Memory\nClassification, Memory Retrieval, and Memory Synthesis. We evaluate this\nframework using five LLMs and three retrievers. Experimental results\ndemonstrate that BERT-based classification models significantly outperform LLMs\nsuch as ChatGLM3 and ChatGPT in the memory classification task. Furthermore,\nour study highlights the importance of effective memory integration in the QA\ntask.", "published": "2024-02-26 04:09:53", "link": "http://arxiv.org/abs/2402.16288v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Language-guided Skill Learning with Temporal Variational Inference", "abstract": "We present an algorithm for skill discovery from expert demonstrations. The\nalgorithm first utilizes Large Language Models (LLMs) to propose an initial\nsegmentation of the trajectories. Following that, a hierarchical variational\ninference framework incorporates the LLM-generated segmentation information to\ndiscover reusable skills by merging trajectory segments. To further control the\ntrade-off between compression and reusability, we introduce a novel auxiliary\nobjective based on the Minimum Description Length principle that helps guide\nthis skill discovery process. Our results demonstrate that agents equipped with\nour method are able to discover skills that help accelerate learning and\noutperform baseline skill learning approaches on new long-horizon tasks in\nBabyAI, a grid world navigation environment, as well as ALFRED, a household\nsimulation environment.", "published": "2024-02-26 07:19:23", "link": "http://arxiv.org/abs/2402.16354v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "An Integrated Data Processing Framework for Pretraining Foundation\n  Models", "abstract": "The ability of the foundation models heavily relies on large-scale, diverse,\nand high-quality pretraining data. In order to improve data quality,\nresearchers and practitioners often have to manually curate datasets from\ndifference sources and develop dedicated data cleansing pipeline for each data\nrepository. Lacking a unified data processing framework, this process is\nrepetitive and cumbersome. To mitigate this issue, we propose a data processing\nframework that integrates a Processing Module which consists of a series of\noperators at different granularity levels, and an Analyzing Module which\nsupports probing and evaluation of the refined data. The proposed framework is\neasy to use and highly flexible. In this demo paper, we first introduce how to\nuse this framework with some example use cases and then demonstrate its\neffectiveness in improving the data quality with an automated evaluation with\nChatGPT and an end-to-end evaluation in pretraining the GPT-2 model. The code\nand demonstration videos are accessible on GitHub.", "published": "2024-02-26 07:22:51", "link": "http://arxiv.org/abs/2402.16358v2", "categories": ["cs.LG", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "Where Do We Go from Here? Multi-scale Allocentric Relational Inference\n  from Natural Spatial Descriptions", "abstract": "When communicating routes in natural language, the concept of acquired\nspatial knowledge is crucial for geographic information retrieval (GIR) and in\nspatial cognitive research. However, NLP navigation studies often overlook the\nimpact of such acquired knowledge on textual descriptions. Current navigation\nstudies concentrate on egocentric local descriptions (e.g., `it will be on your\nright') that require reasoning over the agent's local perception. These\ninstructions are typically given as a sequence of steps, with each action-step\nexplicitly mentioning and being followed by a landmark that the agent can use\nto verify they are on the right path (e.g., `turn right and then you will\nsee...'). In contrast, descriptions based on knowledge acquired through a map\nprovide a complete view of the environment and capture its overall structure.\nThese instructions (e.g., `it is south of Central Park and a block north of a\npolice station') are typically non-sequential, contain allocentric relations,\nwith multiple spatial relations and implicit actions, without any explicit\nverification. This paper introduces the Rendezvous (RVS) task and dataset,\nwhich includes 10,404 examples of English geospatial instructions for reaching\na target location using map-knowledge. Our analysis reveals that RVS exhibits a\nricher use of spatial allocentric relations, and requires resolving more\nspatial relations simultaneously compared to previous text-based navigation\nbenchmarks.", "published": "2024-02-26 07:33:28", "link": "http://arxiv.org/abs/2402.16364v2", "categories": ["cs.CL", "cs.LG", "cs.MM"], "primary_category": "cs.CL"}
{"title": "An Automated End-to-End Open-Source Software for High-Quality\n  Text-to-Speech Dataset Generation", "abstract": "Data availability is crucial for advancing artificial intelligence\napplications, including voice-based technologies. As content creation,\nparticularly in social media, experiences increasing demand, translation and\ntext-to-speech (TTS) technologies have become essential tools. Notably, the\nperformance of these TTS technologies is highly dependent on the quality of the\ntraining data, emphasizing the mutual dependence of data availability and\ntechnological progress. This paper introduces an end-to-end tool to generate\nhigh-quality datasets for text-to-speech (TTS) models to address this critical\nneed for high-quality data. The contributions of this work are manifold and\ninclude: the integration of language-specific phoneme distribution into sample\nselection, automation of the recording process, automated and human-in-the-loop\nquality assurance of recordings, and processing of recordings to meet specified\nformats. The proposed application aims to streamline the dataset creation\nprocess for TTS models through these features, thereby facilitating\nadvancements in voice-based technologies.", "published": "2024-02-26 07:58:33", "link": "http://arxiv.org/abs/2402.16380v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "On Languaging a Simulation Engine", "abstract": "Language model intelligence is revolutionizing the way we program materials\nsimulations. However, the diversity of simulation scenarios renders it\nchallenging to precisely transform human language into a tailored simulator.\nHere, using three functionalized types of language model, we propose a\nlanguage-to-simulation (Lang2Sim) framework that enables interactive navigation\non languaging a simulation engine, by taking a scenario instance of water\nsorption in porous matrices. Unlike line-by-line coding of a target simulator,\nthe language models interpret each simulator as an assembly of invariant tool\nfunction and its variant input-output pair. Lang2Sim enables the precise\ntransform of textual description by functionalizing and sequentializing the\nlanguage models of, respectively, rationalizing the tool categorization,\ncustomizing its input-output combinations, and distilling the simulator input\ninto executable format. Importantly, depending on its functionalized type, each\nlanguage model features a distinct processing of chat history to best balance\nits memory limit and information completeness, thus leveraging the model\nintelligence to unstructured nature of human request. Overall, this work\nestablishes language model as an intelligent platform to unlock the era of\nlanguaging a simulation engine.", "published": "2024-02-26 11:01:54", "link": "http://arxiv.org/abs/2402.16482v1", "categories": ["cs.AI", "cs.CE", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Integrating Large Language Models with Graphical Session-Based\n  Recommendation", "abstract": "With the rapid development of Large Language Models (LLMs), various\nexplorations have arisen to utilize LLMs capability of context understanding on\nrecommender systems. While pioneering strategies have primarily transformed\ntraditional recommendation tasks into challenges of natural language\ngeneration, there has been a relative scarcity of exploration in the domain of\nsession-based recommendation (SBR) due to its specificity. SBR has been\nprimarily dominated by Graph Neural Networks, which have achieved many\nsuccessful outcomes due to their ability to capture both the implicit and\nexplicit relationships between adjacent behaviors. The structural nature of\ngraphs contrasts with the essence of natural language, posing a significant\nadaptation gap for LLMs. In this paper, we introduce large language models with\ngraphical Session-Based recommendation, named LLMGR, an effective framework\nthat bridges the aforementioned gap by harmoniously integrating LLMs with Graph\nNeural Networks (GNNs) for SBR tasks. This integration seeks to leverage the\ncomplementary strengths of LLMs in natural language understanding and GNNs in\nrelational data processing, leading to a more powerful session-based\nrecommender system that can understand and recommend items within a session.\nMoreover, to endow the LLM with the capability to empower SBR tasks, we design\na series of prompts for both auxiliary and major instruction tuning tasks.\nThese prompts are crafted to assist the LLM in understanding graph-structured\ndata and align textual information with nodes, effectively translating nuanced\nuser interactions into a format that can be understood and utilized by LLM\narchitectures. Extensive experiments on three real-world datasets demonstrate\nthat LLMGR outperforms several competitive baselines, indicating its\neffectiveness in enhancing SBR tasks and its potential as a research direction\nfor future exploration.", "published": "2024-02-26 12:55:51", "link": "http://arxiv.org/abs/2402.16539v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Aligning Large Language Models to a Domain-specific Graph Database for\n  NL2GQL", "abstract": "Graph Databases (Graph DB) find extensive application across diverse domains\nsuch as finance, social networks, and medicine. Yet, the translation of Natural\nLanguage (NL) into the Graph Query Language (GQL), referred to as NL2GQL, poses\nsignificant challenges owing to its intricate and specialized nature. Some\napproaches have sought to utilize Large Language Models (LLMs) to address\nanalogous tasks like text2SQL. Nonetheless, in the realm of NL2GQL tasks\ntailored to a particular domain, the absence of domain-specific NL-GQL data\npairs adds complexity to aligning LLMs with the graph DB. To tackle this\nchallenge, we present a well-defined pipeline. Initially, we utilize ChatGPT to\ngenerate NL-GQL data pairs, leveraging the provided graph DB with\nself-instruction. Subsequently, we employ the generated data to fine-tune LLMs,\nensuring alignment between LLMs and the graph DB. Moreover, we find the\nimportance of relevant schema in efficiently generating accurate GQLs. Thus, we\nintroduce a method to extract relevant schema as the input context. We evaluate\nour method using two carefully constructed datasets derived from graph DBs in\nthe finance and medicine domains, named FinGQL and MediGQL. Experimental\nresults reveal that our approach significantly outperforms a set of baseline\nmethods, with improvements of 5.90 and 6.36 absolute points on EM, and 6.00 and\n7.09 absolute points on EX for FinGQL and MediGQL, respectively.", "published": "2024-02-26 13:46:51", "link": "http://arxiv.org/abs/2402.16567v3", "categories": ["cs.CL", "cs.AI", "cs.DB"], "primary_category": "cs.CL"}
{"title": "ESG Sentiment Analysis: comparing human and language model performance\n  including GPT", "abstract": "In this paper we explore the challenges of measuring sentiment in relation to\nEnvironmental, Social and Governance (ESG) social media. ESG has grown in\nimportance in recent years with a surge in interest from the financial sector\nand the performance of many businesses has become based in part on their ESG\nrelated reputations. The use of sentiment analysis to measure ESG related\nreputation has developed and with it interest in the use of machines to do so.\nThe era of digital media has created an explosion of new media sources, driven\nby the growth of social media platforms. This growing data environment has\nbecome an excellent source for behavioural insight studies across many\ndisciplines that includes politics, healthcare and market research. Our study\nseeks to compare human performance with the cutting edge in machine performance\nin the measurement of ESG related sentiment. To this end researchers classify\nthe sentiment of 150 tweets and a reliability measure is made. A gold standard\ndata set is then established based on the consensus of 3 researchers and this\ndata set is then used to measure the performance of different machine\napproaches: one based on the VADER dictionary approach to sentiment\nclassification and then multiple language model approaches, including Llama2,\nT5, Mistral, Mixtral, FINBERT, GPT3.5 and GPT4.", "published": "2024-02-26 15:22:30", "link": "http://arxiv.org/abs/2402.16650v1", "categories": ["cs.CL", "cs.CE", "cs.CY"], "primary_category": "cs.CL"}
{"title": "GigaPevt: Multimodal Medical Assistant", "abstract": "Building an intelligent and efficient medical assistant is still a\nchallenging AI problem. The major limitation comes from the data modality\nscarceness, which reduces comprehensive patient perception. This demo paper\npresents the GigaPevt, the first multimodal medical assistant that combines the\ndialog capabilities of large language models with specialized medical models.\nSuch an approach shows immediate advantages in dialog quality and metric\nperformance, with a 1.18% accuracy improvement in the question-answering task.", "published": "2024-02-26 15:26:56", "link": "http://arxiv.org/abs/2402.16654v2", "categories": ["cs.AI", "cs.CL", "cs.HC", "68T07", "I.2.1"], "primary_category": "cs.AI"}
{"title": "HumanEval-XL: A Multilingual Code Generation Benchmark for Cross-lingual\n  Natural Language Generalization", "abstract": "Large language models (LLMs) have made significant progress in generating\ncodes from textual prompts. However, existing benchmarks have mainly\nconcentrated on translating English prompts to multilingual codes or have been\nconstrained to very limited natural languages (NLs). These benchmarks have\noverlooked the vast landscape of massively multilingual NL to multilingual\ncode, leaving a critical gap in the evaluation of multilingual LLMs. In\nresponse, we introduce HumanEval-XL, a massively multilingual code generation\nbenchmark specifically crafted to address this deficiency. HumanEval-XL\nestablishes connections between 23 NLs and 12 programming languages (PLs), and\ncomprises of a collection of 22,080 prompts with an average of 8.33 test cases.\nBy ensuring parallel data across multiple NLs and PLs, HumanEval-XL offers a\ncomprehensive evaluation platform for multilingual LLMs, allowing the\nassessment of the understanding of different NLs. Our work serves as a\npioneering step towards filling the void in evaluating NL generalization in the\narea of multilingual code generation. We make our evaluation code and data\npublicly available at \\url{https://github.com/FloatAI/humaneval-xl}.", "published": "2024-02-26 16:09:00", "link": "http://arxiv.org/abs/2402.16694v2", "categories": ["cs.CL", "cs.PL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "SelectIT: Selective Instruction Tuning for LLMs via Uncertainty-Aware\n  Self-Reflection", "abstract": "Instruction tuning (IT) is crucial to tailoring large language models (LLMs)\ntowards human-centric interactions. Recent advancements have shown that the\ncareful selection of a small, high-quality subset of IT data can significantly\nenhance the performance of LLMs. Despite this, common approaches often rely on\nadditional models or data, which increases costs and limits widespread\nadoption. In this work, we propose a novel approach, termed SelectIT, that\ncapitalizes on the foundational capabilities of the LLM itself. Specifically,\nwe exploit the intrinsic uncertainty present in LLMs to more effectively select\nhigh-quality IT data, without the need for extra resources. Furthermore, we\nintroduce a curated IT dataset, the Selective Alpaca, created by applying\nSelectIT to the Alpaca-GPT4 dataset. Empirical results demonstrate that IT\nusing Selective Alpaca leads to substantial model ability enhancement. The\nrobustness of SelectIT has also been corroborated in various foundation models\nand domain-specific tasks. Our findings suggest that longer and more\ncomputationally intensive IT data may serve as superior sources of IT, offering\nvaluable insights for future research in this area. Data, code, and scripts are\nfreely available at https://github.com/Blue-Raincoat/SelectIT.", "published": "2024-02-26 16:21:53", "link": "http://arxiv.org/abs/2402.16705v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Quantum linear algebra is all you need for Transformer architectures", "abstract": "Generative machine learning methods such as large-language models are\nrevolutionizing the creation of text and images. While these models are\npowerful they also harness a large amount of computational resources. The\ntransformer is a key component in large language models that aims to generate a\nsuitable completion of a given partial sequence. In this work, we investigate\ntransformer architectures under the lens of fault-tolerant quantum computing.\nThe input model is one where trained weight matrices are given as block\nencodings and we construct the query, key, and value matrices for the\ntransformer. We show how to prepare a block encoding of the self-attention\nmatrix, with a new subroutine for the row-wise application of the softmax\nfunction. In addition, we combine quantum subroutines to construct important\nbuilding blocks in the transformer, the residual connection and layer\nnormalization, and the feed-forward neural network. Our subroutines prepare an\namplitude encoding of the transformer output, which can be measured to obtain a\nprediction. Based on common open-source large-language models, we provide\ninsights into the behavior of important parameters determining the run time of\nthe quantum algorithm. We discuss the potential and challenges for obtaining a\nquantum advantage.", "published": "2024-02-26 16:31:28", "link": "http://arxiv.org/abs/2402.16714v2", "categories": ["quant-ph", "cs.AI", "cs.CL"], "primary_category": "quant-ph"}
{"title": "CodeChameleon: Personalized Encryption Framework for Jailbreaking Large\n  Language Models", "abstract": "Adversarial misuse, particularly through `jailbreaking' that circumvents a\nmodel's safety and ethical protocols, poses a significant challenge for Large\nLanguage Models (LLMs). This paper delves into the mechanisms behind such\nsuccessful attacks, introducing a hypothesis for the safety mechanism of\naligned LLMs: intent security recognition followed by response generation.\nGrounded in this hypothesis, we propose CodeChameleon, a novel jailbreak\nframework based on personalized encryption tactics. To elude the intent\nsecurity recognition phase, we reformulate tasks into a code completion format,\nenabling users to encrypt queries using personalized encryption functions. To\nguarantee response generation functionality, we embed a decryption function\nwithin the instructions, which allows the LLM to decrypt and execute the\nencrypted queries successfully. We conduct extensive experiments on 7 LLMs,\nachieving state-of-the-art average Attack Success Rate (ASR). Remarkably, our\nmethod achieves an 86.6\\% ASR on GPT-4-1106.", "published": "2024-02-26 16:35:59", "link": "http://arxiv.org/abs/2402.16717v1", "categories": ["cs.CL", "cs.AI", "cs.CR"], "primary_category": "cs.CL"}
{"title": "If in a Crowdsourced Data Annotation Pipeline, a GPT-4", "abstract": "Recent studies indicated GPT-4 outperforms online crowd workers in data\nlabeling accuracy, notably workers from Amazon Mechanical Turk (MTurk).\nHowever, these studies were criticized for deviating from standard\ncrowdsourcing practices and emphasizing individual workers' performances over\nthe whole data-annotation process. This paper compared GPT-4 and an ethical and\nwell-executed MTurk pipeline, with 415 workers labeling 3,177 sentence segments\nfrom 200 scholarly articles using the CODA-19 scheme. Two worker interfaces\nyielded 127,080 labels, which were then used to infer the final labels through\neight label-aggregation algorithms. Our evaluation showed that despite best\npractices, MTurk pipeline's highest accuracy was 81.5%, whereas GPT-4 achieved\n83.6%. Interestingly, when combining GPT-4's labels with crowd labels collected\nvia an advanced worker interface for aggregation, 2 out of the 8 algorithms\nachieved an even higher accuracy (87.5%, 87.0%). Further analysis suggested\nthat, when the crowd's and GPT-4's labeling strengths are complementary,\naggregating them could increase labeling accuracy.", "published": "2024-02-26 18:08:52", "link": "http://arxiv.org/abs/2402.16795v2", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.HC"}
{"title": "Nemotron-4 15B Technical Report", "abstract": "We introduce Nemotron-4 15B, a 15-billion-parameter large multilingual\nlanguage model trained on 8 trillion text tokens. Nemotron-4 15B demonstrates\nstrong performance when assessed on English, multilingual, and coding tasks: it\noutperforms all existing similarly-sized open models on 4 out of 7 downstream\nevaluation areas and achieves competitive performance to the leading open\nmodels in the remaining ones. Specifically, Nemotron-4 15B exhibits the best\nmultilingual capabilities of all similarly-sized models, even outperforming\nmodels over four times larger and those explicitly specialized for multilingual\ntasks.", "published": "2024-02-26 18:43:45", "link": "http://arxiv.org/abs/2402.16819v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts", "abstract": "As large language models (LLMs) become increasingly prevalent across many\nreal-world applications, understanding and enhancing their robustness to\nadversarial attacks is of paramount importance. Existing methods for\nidentifying adversarial prompts tend to focus on specific domains, lack\ndiversity, or require extensive human annotations. To address these\nlimitations, we present Rainbow Teaming, a novel black-box approach for\nproducing a diverse collection of adversarial prompts. Rainbow Teaming casts\nadversarial prompt generation as a quality-diversity problem and uses\nopen-ended search to generate prompts that are both effective and diverse.\nFocusing on the safety domain, we use Rainbow Teaming to target various\nstate-of-the-art LLMs, including the Llama 2 and Llama 3 models. Our approach\nreveals hundreds of effective adversarial prompts, with an attack success rate\nexceeding 90% across all tested models. Furthermore, we demonstrate that\nprompts generated by Rainbow Teaming are highly transferable and that\nfine-tuning models with synthetic data generated by our method significantly\nenhances their safety without sacrificing general performance or helpfulness.\nWe additionally explore the versatility of Rainbow Teaming by applying it to\nquestion answering and cybersecurity, showcasing its potential to drive robust\nopen-ended self-improvement in a wide range of applications.", "published": "2024-02-26 18:47:27", "link": "http://arxiv.org/abs/2402.16822v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Language Agents as Optimizable Graphs", "abstract": "Various human-designed prompt engineering techniques have been proposed to\nimprove problem solvers based on Large Language Models (LLMs), yielding many\ndisparate code bases. We unify these approaches by describing LLM-based agents\nas computational graphs. The nodes implement functions to process multimodal\ndata or query LLMs, and the edges describe the information flow between\noperations. Graphs can be recursively combined into larger composite graphs\nrepresenting hierarchies of inter-agent collaboration (where edges connect\noperations of different agents). Our novel automatic graph optimizers (1)\nrefine node-level LLM prompts (node optimization) and (2) improve agent\norchestration by changing graph connectivity (edge optimization). Experiments\ndemonstrate that our framework can be used to efficiently develop, integrate,\nand automatically improve various LLM agents. The code can be found at\nhttps://github.com/metauto-ai/gptswarm.", "published": "2024-02-26 18:48:27", "link": "http://arxiv.org/abs/2402.16823v3", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "primary_category": "cs.AI"}
{"title": "SKILL: Similarity-aware Knowledge distILLation for Speech\n  Self-Supervised Learning", "abstract": "Self-supervised learning (SSL) has achieved remarkable success across various\nspeech-processing tasks. To enhance its efficiency, previous works often\nleverage the use of compression techniques. A notable recent attempt is\nDPHuBERT, which applies joint knowledge distillation (KD) and structured\npruning to learn a significantly smaller SSL model. In this paper, we\ncontribute to this research domain by introducing SKILL, a novel method that\nconducts distillation across groups of layers instead of distilling individual\narbitrarily selected layers within the teacher network. The identification of\nthe layers to distill is achieved through a hierarchical clustering procedure\napplied to layer similarity measures. Extensive experiments demonstrate that\nour distilled version of WavLM Base+ not only outperforms DPHuBERT but also\nachieves state-of-the-art results in the 30M parameters model class across\nseveral SUPERB tasks.", "published": "2024-02-26 18:56:42", "link": "http://arxiv.org/abs/2402.16830v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Cross-Modal Projection in Multimodal LLMs Doesn't Really Project Visual\n  Attributes to Textual Space", "abstract": "Multimodal large language models (MLLMs) like LLaVA and GPT-4(V) enable\ngeneral-purpose conversations about images with the language modality. As\noff-the-shelf MLLMs may have limited capabilities on images from domains like\ndermatology and agriculture, they must be fine-tuned to unlock domain-specific\napplications. The prevalent architecture of current open-source MLLMs comprises\ntwo major modules: an image-language (cross-modal) projection network and a\nlarge language model. It is desirable to understand the roles of these two\nmodules in modeling domain-specific visual attributes to inform the design of\nfuture models and streamline the interpretability efforts on the current\nmodels. To this end, via experiments on 4 datasets and under 2 fine-tuning\nsettings, we find that as the MLLM is fine-tuned, it indeed gains\ndomain-specific visual capabilities, but the updates do not lead to the\nprojection extracting relevant domain-specific visual attributes. Our results\nindicate that the domain-specific visual attributes are modeled by the LLM,\neven when only the projection is fine-tuned. Through this study, we offer a\npotential reinterpretation of the role of cross-modal projections in MLLM\narchitectures. Project webpage:\nhttps://claws-lab.github.io/projection-in-MLLMs/", "published": "2024-02-26 18:56:48", "link": "http://arxiv.org/abs/2402.16832v2", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "PhyGrasp: Generalizing Robotic Grasping with Physics-informed Large\n  Multimodal Models", "abstract": "Robotic grasping is a fundamental aspect of robot functionality, defining how\nrobots interact with objects. Despite substantial progress, its\ngeneralizability to counter-intuitive or long-tailed scenarios, such as objects\nwith uncommon materials or shapes, remains a challenge. In contrast, humans can\neasily apply their intuitive physics to grasp skillfully and change grasps\nefficiently, even for objects they have never seen before.\n  This work delves into infusing such physical commonsense reasoning into\nrobotic manipulation. We introduce PhyGrasp, a multimodal large model that\nleverages inputs from two modalities: natural language and 3D point clouds,\nseamlessly integrated through a bridge module. The language modality exhibits\nrobust reasoning capabilities concerning the impacts of diverse physical\nproperties on grasping, while the 3D modality comprehends object shapes and\nparts. With these two capabilities, PhyGrasp is able to accurately assess the\nphysical properties of object parts and determine optimal grasping poses.\nAdditionally, the model's language comprehension enables human instruction\ninterpretation, generating grasping poses that align with human preferences. To\ntrain PhyGrasp, we construct a dataset PhyPartNet with 195K object instances\nwith varying physical properties and human preferences, alongside their\ncorresponding language descriptions. Extensive experiments conducted in the\nsimulation and on the real robots demonstrate that PhyGrasp achieves\nstate-of-the-art performance, particularly in long-tailed cases, e.g., about\n10% improvement in success rate over GraspNet. Project page:\nhttps://sites.google.com/view/phygrasp", "published": "2024-02-26 18:57:52", "link": "http://arxiv.org/abs/2402.16836v1", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.RO"}
{"title": "Think Big, Generate Quick: LLM-to-SLM for Fast Autoregressive Decoding", "abstract": "Large language models (LLMs) have become ubiquitous in practice and are\nwidely used for generation tasks such as translation, summarization and\ninstruction following. However, their enormous size and reliance on\nautoregressive decoding increase deployment costs and complicate their use in\nlatency-critical applications. In this work, we propose a hybrid approach that\ncombines language models of different sizes to increase the efficiency of\nautoregressive decoding while maintaining high performance. Our method utilizes\na pretrained frozen LLM that encodes all prompt tokens once in parallel, and\nuses the resulting representations to condition and guide a small language\nmodel (SLM), which then generates the response more efficiently. We investigate\nthe combination of encoder-decoder LLMs with both encoder-decoder and\ndecoder-only SLMs from different model families and only require fine-tuning of\nthe SLM. Experiments with various benchmarks show substantial speedups of up to\n$4\\times$, with minor performance penalties of $1-2\\%$ for translation and\nsummarization tasks compared to the LLM.", "published": "2024-02-26 18:59:28", "link": "http://arxiv.org/abs/2402.16844v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "GROUNDHOG: Grounding Large Language Models to Holistic Segmentation", "abstract": "Most multimodal large language models (MLLMs) learn language-to-object\ngrounding through causal language modeling where grounded objects are captured\nby bounding boxes as sequences of location tokens. This paradigm lacks\npixel-level representations that are important for fine-grained visual\nunderstanding and diagnosis. In this work, we introduce GROUNDHOG, an MLLM\ndeveloped by grounding Large Language Models to holistic segmentation.\nGROUNDHOG incorporates a masked feature extractor and converts extracted\nfeatures into visual entity tokens for the MLLM backbone, which then connects\ngroundable phrases to unified grounding masks by retrieving and merging the\nentity masks. To train GROUNDHOG, we carefully curated M3G2, a grounded visual\ninstruction tuning dataset with Multi-Modal Multi-Grained Grounding, by\nharvesting a collection of segmentation-grounded datasets with rich\nannotations. Our experimental results show that GROUNDHOG achieves superior\nperformance on various language grounding tasks without task-specific\nfine-tuning, and significantly reduces object hallucination. GROUNDHOG also\ndemonstrates better grounding towards complex forms of visual input and\nprovides easy-to-understand diagnosis in failure cases.", "published": "2024-02-26 18:59:33", "link": "http://arxiv.org/abs/2402.16846v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "LangGPT: Rethinking Structured Reusable Prompt Design Framework for LLMs\n  from the Programming Language", "abstract": "LLMs have demonstrated commendable performance across diverse domains.\nNevertheless, formulating high-quality prompts to instruct LLMs proficiently\nposes a challenge for non-AI experts. Existing research in prompt engineering\nsuggests somewhat scattered optimization principles and designs empirically\ndependent prompt optimizers. Unfortunately, these endeavors lack a structured\ndesign template, incurring high learning costs and resulting in low\nreusability. In addition, it is not conducive to the iterative updating of\nprompts. Inspired by structured reusable programming languages, we propose\nLangGPT, a dual-layer prompt design framework as the programming language for\nLLMs. LangGPT has an easy-to-learn normative structure and provides an extended\nstructure for migration and reuse. Experiments illustrate that LangGPT\nsignificantly enhances the performance of LLMs. Moreover, the case study shows\nthat LangGPT leads LLMs to generate higher-quality responses. Furthermore, we\nanalyzed the ease of use and reusability of LangGPT through a user survey in\nour online community.", "published": "2024-02-26 15:05:16", "link": "http://arxiv.org/abs/2402.16929v2", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.PL"], "primary_category": "cs.SE"}
{"title": "Successfully Guiding Humans with Imperfect Instructions by Highlighting\n  Potential Errors and Suggesting Corrections", "abstract": "Language models will inevitably err in situations with which they are\nunfamiliar. However, by effectively communicating uncertainties, they can still\nguide humans toward making sound decisions in those contexts. We demonstrate\nthis idea by developing HEAR, a system that can successfully guide humans in\nsimulated residential environments despite generating potentially inaccurate\ninstructions. Diverging from systems that provide users with only the\ninstructions they generate, HEAR warns users of potential errors in its\ninstructions and suggests corrections. This rich uncertainty information\neffectively prevents misguidance and reduces the search space for users.\nEvaluation with 80 users shows that HEAR achieves a 13% increase in success\nrate and a 29% reduction in final location error distance compared to only\npresenting instructions to users. Interestingly, we find that offering users\npossibilities to explore, HEAR motivates them to make more attempts at the\ntask, ultimately leading to a higher success rate. To our best knowledge, this\nwork is the first to show the practical benefits of uncertainty communication\nin a long-horizon sequential decision-making problem.", "published": "2024-02-26 19:16:04", "link": "http://arxiv.org/abs/2402.16973v2", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI"}
{"title": "Towards Explainability and Fairness in Swiss Judgement Prediction:\n  Benchmarking on a Multilingual Dataset", "abstract": "The assessment of explainability in Legal Judgement Prediction (LJP) systems\nis of paramount importance in building trustworthy and transparent systems,\nparticularly considering the reliance of these systems on factors that may lack\nlegal relevance or involve sensitive attributes. This study delves into the\nrealm of explainability and fairness in LJP models, utilizing Swiss Judgement\nPrediction (SJP), the only available multilingual LJP dataset. We curate a\ncomprehensive collection of rationales that `support' and `oppose' judgement\nfrom legal experts for 108 cases in German, French, and Italian. By employing\nan occlusion-based explainability approach, we evaluate the explainability\nperformance of state-of-the-art monolingual and multilingual BERT-based LJP\nmodels, as well as models developed with techniques such as data augmentation\nand cross-lingual transfer, which demonstrated prediction performance\nimprovement. Notably, our findings reveal that improved prediction performance\ndoes not necessarily correspond to enhanced explainability performance,\nunderscoring the significance of evaluating models from an explainability\nperspective. Additionally, we introduce a novel evaluation framework, Lower\nCourt Insertion (LCI), which allows us to quantify the influence of lower court\ninformation on model predictions, exposing current models' biases.", "published": "2024-02-26 20:42:40", "link": "http://arxiv.org/abs/2402.17013v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2"], "primary_category": "cs.CL"}
{"title": "Multi-Task Contrastive Learning for 8192-Token Bilingual Text Embeddings", "abstract": "We introduce a novel suite of state-of-the-art bilingual text embedding\nmodels that are designed to support English and another target language. These\nmodels are capable of processing lengthy text inputs with up to 8192 tokens,\nmaking them highly versatile for a range of natural language processing tasks\nsuch as text retrieval, clustering, and semantic textual similarity (STS)\ncalculations.\n  By focusing on bilingual models and introducing a unique multi-task learning\nobjective, we have significantly improved the model performance on STS tasks,\nwhich outperforms the capabilities of existing multilingual models in both\ntarget language understanding and cross-lingual evaluation tasks. Moreover, our\nbilingual models are more efficient, requiring fewer parameters and less memory\ndue to their smaller vocabulary needs. Furthermore, we have expanded the\nMassive Text Embedding Benchmark (MTEB) to include benchmarks for German and\nSpanish embedding models. This integration aims to stimulate further research\nand advancement in text embedding technologies for these languages.", "published": "2024-02-26 20:53:12", "link": "http://arxiv.org/abs/2402.17016v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "A Surprising Failure? Multimodal LLMs and the NLVR Challenge", "abstract": "This study evaluates three state-of-the-art MLLMs -- GPT-4V, Gemini Pro, and\nthe open-source model IDEFICS -- on the compositional natural language vision\nreasoning task NLVR. Given a human-written sentence paired with a synthetic\nimage, this task requires the model to determine the truth value of the\nsentence with respect to the image. Despite the strong performance demonstrated\nby these models, we observe they perform poorly on NLVR, which was constructed\nto require compositional and spatial reasoning, and to be robust for semantic\nand systematic biases.", "published": "2024-02-26 18:37:18", "link": "http://arxiv.org/abs/2402.17793v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Social Media as a Sensor: Analyzing Twitter Data for Breast Cancer\n  Medication Effects Using Natural Language Processing", "abstract": "Breast cancer is a significant public health concern and is the leading cause\nof cancer-related deaths among women. Despite advances in breast cancer\ntreatments, medication non-adherence remains a major problem. As electronic\nhealth records do not typically capture patient-reported outcomes that may\nreveal information about medication-related experiences, social media presents\nan attractive resource for enhancing our understanding of the patients'\ntreatment experiences. In this paper, we developed natural language processing\n(NLP) based methodologies to study information posted by an automatically\ncurated breast cancer cohort from social media. We employed a transformer-based\nclassifier to identify breast cancer patients/survivors on X (Twitter) based on\ntheir self-reported information, and we collected longitudinal data from their\nprofiles. We then designed a multi-layer rule-based model to develop a breast\ncancer therapy-associated side effect lexicon and detect patterns of medication\nusage and associated side effects among breast cancer patients. 1,454,637 posts\nwere available from 583,962 unique users, of which 62,042 were detected as\nbreast cancer members using our transformer-based model. 198 cohort members\nmentioned breast cancer medications with tamoxifen as the most common. Our side\neffect lexicon identified well-known side effects of hormone and chemotherapy.\nFurthermore, it discovered a subject feeling towards cancer and medications,\nwhich may suggest a pre-clinical phase of side effects or emotional distress.\nThis analysis highlighted not only the utility of NLP techniques in\nunstructured social media data to identify self-reported breast cancer posts,\nmedication usage patterns, and treatment side effects but also the richness of\nsocial data on such clinical questions.", "published": "2024-02-26 16:17:19", "link": "http://arxiv.org/abs/2403.00821v1", "categories": ["cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "HealMe: Harnessing Cognitive Reframing in Large Language Models for\n  Psychotherapy", "abstract": "Large Language Models (LLMs) can play a vital role in psychotherapy by\nadeptly handling the crucial task of cognitive reframing and overcoming\nchallenges such as shame, distrust, therapist skill variability, and resource\nscarcity. Previous LLMs in cognitive reframing mainly converted negative\nemotions to positive ones, but these approaches have limited efficacy, often\nnot promoting clients' self-discovery of alternative perspectives. In this\npaper, we unveil the Helping and Empowering through Adaptive Language in Mental\nEnhancement (HealMe) model. This novel cognitive reframing therapy method\neffectively addresses deep-rooted negative thoughts and fosters rational,\nbalanced perspectives. Diverging from traditional LLM methods, HealMe employs\nempathetic dialogue based on psychotherapeutic frameworks. It systematically\nguides clients through distinguishing circumstances from feelings,\nbrainstorming alternative viewpoints, and developing empathetic, actionable\nsuggestions. Moreover, we adopt the first comprehensive and expertly crafted\npsychological evaluation metrics, specifically designed to rigorously assess\nthe performance of cognitive reframing, in both AI-simulated dialogues and\nreal-world therapeutic conversations. Experimental results show that our model\noutperforms others in terms of empathy, guidance, and logical coherence,\ndemonstrating its effectiveness and potential positive impact on psychotherapy.", "published": "2024-02-26 09:10:34", "link": "http://arxiv.org/abs/2403.05574v3", "categories": ["cs.HC", "cs.AI", "cs.CL", "J.4"], "primary_category": "cs.HC"}
{"title": "Multi-LoRA Composition for Image Generation", "abstract": "Low-Rank Adaptation (LoRA) is extensively utilized in text-to-image models\nfor the accurate rendition of specific elements like distinct characters or\nunique styles in generated images. Nonetheless, existing methods face\nchallenges in effectively composing multiple LoRAs, especially as the number of\nLoRAs to be integrated grows, thus hindering the creation of complex imagery.\nIn this paper, we study multi-LoRA composition through a decoding-centric\nperspective. We present two training-free methods: LoRA Switch, which\nalternates between different LoRAs at each denoising step, and LoRA Composite,\nwhich simultaneously incorporates all LoRAs to guide more cohesive image\nsynthesis. To evaluate the proposed approaches, we establish ComposLoRA, a new\ncomprehensive testbed as part of this research. It features a diverse range of\nLoRA categories with 480 composition sets. Utilizing an evaluation framework\nbased on GPT-4V, our findings demonstrate a clear improvement in performance\nwith our methods over the prevalent baseline, particularly evident when\nincreasing the number of LoRAs in a composition. The code, benchmarks, LoRA\nweights, and all evaluation details are available on our project website:\nhttps://maszhongming.github.io/Multi-LoRA-Composition.", "published": "2024-02-26 18:59:18", "link": "http://arxiv.org/abs/2402.16843v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.GR", "cs.LG"], "primary_category": "cs.CV"}
{"title": "What Do Language Models Hear? Probing for Auditory Representations in\n  Language Models", "abstract": "This work explores whether language models encode meaningfully grounded\nrepresentations of sounds of objects. We learn a linear probe that retrieves\nthe correct text representation of an object given a snippet of audio related\nto that object, where the sound representation is given by a pretrained audio\nmodel. This probe is trained via a contrastive loss that pushes the language\nrepresentations and sound representations of an object to be close to one\nanother. After training, the probe is tested on its ability to generalize to\nobjects that were not seen during training. Across different language models\nand audio models, we find that the probe generalization is above chance in many\ncases, indicating that despite being trained only on raw text, language models\nencode grounded knowledge of sounds for some objects.", "published": "2024-02-26 20:13:58", "link": "http://arxiv.org/abs/2402.16998v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Audio-Visual Speech Enhancement in Noisy Environments via Emotion-Based\n  Contextual Cues", "abstract": "In real-world environments, background noise significantly degrades the\nintelligibility and clarity of human speech. Audio-visual speech enhancement\n(AVSE) attempts to restore speech quality, but existing methods often fall\nshort, particularly in dynamic noise conditions. This study investigates the\ninclusion of emotion as a novel contextual cue within AVSE, hypothesizing that\nincorporating emotional understanding can improve speech enhancement\nperformance. We propose a novel emotion-aware AVSE system that leverages both\nauditory and visual information. It extracts emotional features from the facial\nlandmarks of the speaker and fuses them with corresponding audio and visual\nmodalities. This enriched data serves as input to a deep UNet-based\nencoder-decoder network, specifically designed to orchestrate the fusion of\nmultimodal information enhanced with emotion. The network iteratively refines\nthe enhanced speech representation through an encoder-decoder architecture,\nguided by perceptually-inspired loss functions for joint learning and\noptimization. We train and evaluate the model on the CMU Multimodal Opinion\nSentiment and Emotion Intensity (CMU-MOSEI) dataset, a rich repository of\naudio-visual recordings with annotated emotions. Our comprehensive evaluation\ndemonstrates the effectiveness of emotion as a contextual cue for AVSE. By\nintegrating emotional features, the proposed system achieves significant\nimprovements in both objective and subjective assessments of speech quality and\nintelligibility, especially in challenging noise environments. Compared to\nbaseline AVSE and audio-only speech enhancement systems, our approach exhibits\na noticeable increase in PESQ and STOI, indicating higher perceptual quality\nand intelligibility. Large-scale listening tests corroborate these findings,\nsuggesting improved human understanding of enhanced speech.", "published": "2024-02-26 08:38:32", "link": "http://arxiv.org/abs/2402.16394v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Towards Environmental Preference Based Speech Enhancement For\n  Individualised Multi-Modal Hearing Aids", "abstract": "Since the advent of Deep Learning (DL), Speech Enhancement (SE) models have\nperformed well under a variety of noise conditions. However, such systems may\nstill introduce sonic artefacts, sound unnatural, and restrict the ability for\na user to hear ambient sound which may be of importance. Hearing Aid (HA) users\nmay wish to customise their SE systems to suit their personal preferences and\nday-to-day lifestyle. In this paper, we introduce a preference learning based\nSE (PLSE) model for future multi-modal HAs that can contextually exploit audio\ninformation to improve listening comfort, based upon the preferences of the\nuser. The proposed system estimates the Signal-to-noise ratio (SNR) as a basic\nobjective speech quality measure which quantifies the relative amount of\nbackground noise present in speech, and directly correlates to the\nintelligibility of the signal. Additionally, to provide contextual information\nwe predict the acoustic scene in which the user is situated. These tasks are\nachieved via a multi-task DL model, which surpasses the performance of\ninferring the acoustic scene or SNR separately, by jointly leveraging a shared\nencoded feature space. These environmental inferences are exploited in a\npreference elicitation framework, which linearly learns a set of predictive\nfunctions to determine the target SNR of an AV (Audio-Visual) SE system. By\ngreatly reducing noise in challenging listening conditions, and by novelly\nscaling the output of the SE model, we are able to provide HA users with\ncontextually individualised SE. Preliminary results suggest an improvement over\nthe non-individualised baseline model in some participants.", "published": "2024-02-26 17:21:57", "link": "http://arxiv.org/abs/2402.16757v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The ICASSP 2024 Audio Deep Packet Loss Concealment Challenge", "abstract": "Audio packet loss concealment is the hiding of gaps in VoIP audio streams\ncaused by network packet loss. With the ICASSP 2024 Audio Deep Packet Loss\nConcealment Grand Challenge, we build on the success of the previous Audio PLC\nChallenge held at INTERSPEECH 2022. We evaluate models on an overall harder\ndataset, and use the new ITU-T P.804 evaluation procedure to more closely\nevaluate the performance of systems specifically on the PLC task. We evaluate a\ntotal of 9 systems, 8 of which satisfy the strict real-time performance\nrequirements of the challenge, using both P.804 and Word Accuracy evaluations.", "published": "2024-02-26 13:26:20", "link": "http://arxiv.org/abs/2402.16927v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Self-Supervised Speech Quality Estimation and Enhancement Using Only\n  Clean Speech", "abstract": "Speech quality estimation has recently undergone a paradigm shift from\nhuman-hearing expert designs to machine-learning models. However, current\nmodels rely mainly on supervised learning, which is time-consuming and\nexpensive for label collection. To solve this problem, we propose VQScore, a\nself-supervised metric for evaluating speech based on the quantization error of\na vector-quantized-variational autoencoder (VQ-VAE). The training of VQ-VAE\nrelies on clean speech; hence, large quantization errors can be expected when\nthe speech is distorted. To further improve correlation with real quality\nscores, domain knowledge of speech processing is incorporated into the model\ndesign. We found that the vector quantization mechanism could also be used for\nself-supervised speech enhancement (SE) model training. To improve the\nrobustness of the encoder for SE, a novel self-distillation mechanism combined\nwith adversarial training is introduced. In summary, the proposed speech\nquality estimation method and enhancement models require only clean speech for\ntraining without any label requirements. Experimental results show that the\nproposed VQScore and enhancement model are competitive with supervised\nbaselines. The code will be released after publication.", "published": "2024-02-26 06:01:38", "link": "http://arxiv.org/abs/2402.16321v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Open Your Ears and Take a Look: A State-of-the-Art Report on the\n  Integration of Sonification and Visualization", "abstract": "The research communities studying visualization and sonification for data\ndisplay and analysis share exceptionally similar goals, essentially making data\nof any kind interpretable to humans. One community does so by using visual\nrepresentations of data, and the other community employs auditory (non-speech)\nrepresentations of data. While the two communities have a lot in common, they\ndeveloped mostly in parallel over the course of the last few decades. With this\nSTAR, we discuss a collection of work that bridges the borders of the two\ncommunities, hence a collection of work that aims to integrate the two\ntechniques into one form of audiovisual display, which we argue to be \"more\nthan the sum of the two.\"\n  We introduce and motivate a classification system applicable to such\naudiovisual displays and categorize a corpus of 57 academic publications that\nappeared between 2011 and 2023 in categories such as reading level, dataset\ntype, or evaluation system, to mention a few. The corpus also enables a\nmeta-analysis of the field, including regularly occurring design patterns such\nas type of visualization and sonification techniques, or the use of visual and\nauditory channels, showing an overall diverse field with different designs. An\nanalysis of a co-author network of the field shows individual teams without\nmany interconnections. The body of work covered in this STAR also relates to\nthree adjacent topics: audiovisual monitoring, accessibility, and audiovisual\ndata art. These three topics are discussed individually in addition to the\nsystematically conducted part of this research. The findings of this report may\nbe used by researchers from both fields to understand the potentials and\nchallenges of such integrated designs while hopefully inspiring them to\ncollaborate with experts from the respective other field.", "published": "2024-02-26 13:34:57", "link": "http://arxiv.org/abs/2402.16558v2", "categories": ["cs.HC", "cs.GR", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
{"title": "Towards Decoding Brain Activity During Passive Listening of Speech", "abstract": "The aim of the study is to investigate the complex mechanisms of speech\nperception and ultimately decode the electrical changes in the brain accruing\nwhile listening to speech. We attempt to decode heard speech from intracranial\nelectroencephalographic (iEEG) data using deep learning methods. The goal is to\naid the advancement of brain-computer interface (BCI) technology for speech\nsynthesis, and, hopefully, to provide an additional perspective on the\ncognitive processes of speech perception. This approach diverges from the\nconventional focus on speech production and instead chooses to investigate\nneural representations of perceived speech. This angle opened up a complex\nperspective, potentially allowing us to study more sophisticated neural\npatterns. Leveraging the power of deep learning models, the research aimed to\nestablish a connection between these intricate neural activities and the\ncorresponding speech sounds. Despite the approach not having achieved a\nbreakthrough yet, the research sheds light on the potential of decoding neural\nactivity during speech perception. Our current efforts can serve as a\nfoundation, and we are optimistic about the potential of expanding and\nimproving upon this work to move closer towards more advanced BCIs, better\nunderstanding of processes underlying perceived speech and its relation to\nspoken speech.", "published": "2024-02-26 20:04:01", "link": "http://arxiv.org/abs/2402.16996v1", "categories": ["cs.HC", "cs.LG", "cs.SD", "eess.AS", "q-bio.NC"], "primary_category": "cs.HC"}
