{"title": "Inferring Option Movements Through Residual Transactions: A Quantitative Model", "abstract": "This research presents a novel approach to predicting option movements by\nanalyzing residual transactions, which are trades that deviate from standard\nhedging activities. Unlike traditional methods that primarily focus on open\ninterest and trading volume, this study argues that residuals can reveal\nnuanced insights into institutional sentiment and strategic positioning. By\nexamining these deviations, the model identifies early indicators of market\ntrends, providing a refined framework for forecasting option prices. The\nproposed model integrates classical machine learning and regression techniques\nto analyze patterns in high frequency trading data, capturing complex, non\nlinear relationships. This predictive framework allows traders to anticipate\nshifts in option values, enhancing strategies for better market timing, risk\nmanagement, and portfolio optimization. The model's adaptability, driven by\nreal time data processing, makes it particularly effective in fast paced\ntrading environments, where early detection of institutional behavior is\ncrucial for gaining a competitive edge. Overall, this research contributes to\nthe field of options trading by offering a strategic tool that detects early\nmarket signals, optimizing trading decisions based on predictive insights\nderived from residual trading patterns. This approach bridges the gap between\nconventional metrics and the subtle behaviors of institutional players, marking\na significant advancement in options market analysis.", "published": "2024-10-21 22:47:49", "link": "http://arxiv.org/abs/2410.16563v1", "categories": ["q-fin.CP", "91G20", "I.5.1"], "primary_category": "q-fin.CP"}
{"title": "Time evaluation of portfolio for asymmetrically informed traders", "abstract": "We study the anticipating version of the classical portfolio optimization\nproblem in a financial market with the presence of a trader who possesses\nprivileged information about the future (insider information), but who is also\nsubjected to a delay in the information flow about the market conditions; hence\nthis trader possesses an asymmetric information with respect to the traditional\none. We analyze it via the Russo-Vallois forward stochastic integral, i. e.\nusing anticipating stochastic calculus, along with a white noise approach. We\nexplicitly compute the optimal portfolios that maximize the expected\nlogarithmic utility assuming different classical financial models:\nBlack-Scholes-Merton, Heston, Vasicek. Similar results hold for other\nwell-known models, such as the Hull-White and the Cox-Ingersoll-Ross ones. Our\ncomparison between the performance of the traditional trader and the insider,\nalthough only asymmetrically informed, reveals that the privileged information\novercompensates the delay in all cases, provided only one information flow is\ndelayed. However, when two information flows are delayed, a competition between\nfuture information and delay magnitude enters into play, implying that the best\nperformance depends on the parameter values. This, in turn, allows us to value\nfuture information in terms of time, and not only utility.", "published": "2024-10-21 13:42:11", "link": "http://arxiv.org/abs/2410.16010v1", "categories": ["q-fin.MF", "60H05, 60H07, 60H10, 60H40, 91G10, 91G80"], "primary_category": "q-fin.MF"}
{"title": "Long time behavior of semi-Markov modulated perpetuity and some related processes", "abstract": "Examples of stochastic processes whose state space representations involve\nfunctions of an integral type structure\n$$I_{t}^{(a,b)}:=\\int_{0}^{t}b(Y_{s})e^{-\\int_{s}^{t}a(Y_{r})dr}ds, \\quad t\\ge\n0$$ are studied under an ergodic semi-Markovian environment described by an $S$\nvalued jump type process $Y:=(Y_{s}:s\\in\\mathbb{R}^{+})$ that is ergodic with a\nlimiting distribution $\\pi\\in\\mathcal{P}(S)$. Under different assumptions on\nsigns of $E_{\\pi}a(\\cdot):=\\sum_{j\\in S}\\pi_{j}a(j)$ and tail properties of the\nsojourn times of $Y$ we obtain different long time limit results for\n$I^{(a,b)}_{}:=(I^{(a,b)}_{t}:t\\ge 0).$ In all cases mixture type of laws\nemerge which are naturally represented through an affine stochastic recurrence\nequation (SRE) $X\\stackrel{d}{=}AX+B,\\,\\, X\\perp\\!\\!\\!\\perp (A, B)$. Examples\ninclude explicit long-time representations of pitchfork bifurcation, and\nregime-switching diffusions under semi-Markov modulated environments, etc.", "published": "2024-10-21 09:41:59", "link": "http://arxiv.org/abs/2410.15824v2", "categories": ["math.PR", "math.DS", "q-fin.MF", "stat.CO"], "primary_category": "math.PR"}
{"title": "A Dynamic Spatiotemporal and Network ARCH Model with Common Factors", "abstract": "We introduce a dynamic spatiotemporal volatility model that extends\ntraditional approaches by incorporating spatial, temporal, and spatiotemporal\nspillover effects, along with volatility-specific observed and latent factors.\nThe model offers a more general network interpretation, making it applicable\nfor studying various types of network spillovers. The primary innovation lies\nin incorporating volatility-specific latent factors into the dynamic\nspatiotemporal volatility model. Using Bayesian estimation via the Markov Chain\nMonte Carlo (MCMC) method, the model offers a robust framework for analyzing\nthe spatial, temporal, and spatiotemporal effects of a log-squared outcome\nvariable on its volatility. We recommend using the deviance information\ncriterion (DIC) and a regularized Bayesian MCMC method to select the number of\nrelevant factors in the model. The model's flexibility is demonstrated through\ntwo applications: a spatiotemporal model applied to the U.S. housing market and\nanother applied to financial stock market networks, both highlighting the\nmodel's ability to capture varying degrees of interconnectedness. In both\napplications, we find strong spatial/network interactions with relatively\nstronger spillover effects in the stock market.", "published": "2024-10-21 21:35:32", "link": "http://arxiv.org/abs/2410.16526v1", "categories": ["stat.ME", "econ.EM", "q-fin.ST"], "primary_category": "stat.ME"}
{"title": "Modelling financial returns with mixtures of generalized normal distributions", "abstract": "This PhD Thesis presents an investigation into the analysis of financial\nreturns using mixture models, focusing on mixtures of generalized normal\ndistributions (MGND) and their extensions. The study addresses several critical\nissues encountered in the estimation process and proposes innovative solutions\nto enhance accuracy and efficiency. In Chapter 2, the focus lies on the MGND\nmodel and its estimation via expectation conditional maximization (ECM) and\ngeneralized expectation maximization (GEM) algorithms. A thorough exploration\nreveals a degeneracy issue when estimating the shape parameter. Several\nalgorithms are proposed to overcome this critical issue. Chapter 3 extends the\ntheoretical perspective by applying the MGND model on several stock market\nindices. A two-step approach is proposed for identifying turmoil days and\nestimating returns and volatility. Chapter 4 introduces constrained mixture of\ngeneralized normal distributions (CMGND), enhancing interpretability and\nefficiency by imposing constraints on parameters. Simulation results highlight\nthe benefits of constrained parameter estimation. Finally, Chapter 5 introduces\ngeneralized normal distribution-hidden Markov models (GND-HMMs) able to capture\nthe dynamic nature of financial returns. This manuscript contributes to the\nstatistical modelling of financial returns by offering flexible, parsimonious,\nand interpretable frameworks. The proposed mixture models capture complex\npatterns in financial data, thereby facilitating more informed decision-making\nin financial analysis and risk management.", "published": "2024-10-21 18:04:01", "link": "http://arxiv.org/abs/2411.11847v1", "categories": ["q-fin.ST", "stat.ME"], "primary_category": "q-fin.ST"}
{"title": "Forecasting Company Fundamentals", "abstract": "Company fundamentals are key to assessing companies' financial and overall\nsuccess and stability. Forecasting them is important in multiple fields,\nincluding investing and econometrics. While statistical and contemporary\nmachine learning methods have been applied to many time series tasks, there is\na lack of comparison of these approaches on this particularly challenging data\nregime. To this end, we try to bridge this gap and thoroughly evaluate the\ntheoretical properties and practical performance of 22 deterministic and\nprobabilistic company fundamentals forecasting models on real company data. We\nobserve that deep learning models provide superior forcasting performance to\nclassical models, in particular when considering uncertainty estimation. To\nvalidate the findings, we compare them to human analyst expectations and find\nthat their accuracy is comparable to the automatic forecasts. We further show\nhow these high-quality forecasts can benefit automated stock allocation. We\nclose by presenting possible ways of integrating domain experts to further\nimprove performance and increase reliability.", "published": "2024-10-21 14:21:43", "link": "http://arxiv.org/abs/2411.05791v1", "categories": ["q-fin.ST", "cs.LG", "econ.GN", "q-fin.EC", "stat.AP", "I.2.6"], "primary_category": "q-fin.ST"}
{"title": "WHoW: A Cross-domain Approach for Analysing Conversation Moderation", "abstract": "We propose WHoW, an evaluation framework for analyzing the facilitation\nstrategies of moderators across different domains/scenarios by examining their\nmotives (Why), dialogue acts (How) and target speaker (Who). Using this\nframework, we annotated 5,657 moderation sentences with human judges and 15,494\nsentences with GPT-4o from two domains: TV debates and radio panel discussions.\nComparative analysis demonstrates the framework's cross-domain generalisability\nand reveals distinct moderation strategies: debate moderators emphasise\ncoordination and facilitate interaction through questions and instructions,\nwhile panel discussion moderators prioritize information provision and actively\nparticipate in discussions. Our analytical framework works for different\nmoderation scenarios, enhances our understanding of moderation behaviour\nthrough automatic large-scale analysis, and facilitates the development of\nmoderator agents.", "published": "2024-10-21 00:54:31", "link": "http://arxiv.org/abs/2410.15551v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Multi-IF: Benchmarking LLMs on Multi-Turn and Multilingual Instructions\n  Following", "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nvarious tasks, including instruction following, which is crucial for aligning\nmodel outputs with user expectations. However, evaluating LLMs' ability to\nfollow instructions remains challenging due to the complexity and subjectivity\nof human language. Current benchmarks primarily focus on single-turn,\nmonolingual instructions, which do not adequately reflect the complexities of\nreal-world applications that require handling multi-turn and multilingual\ninteractions. To address this gap, we introduce Multi-IF, a new benchmark\ndesigned to assess LLMs' proficiency in following multi-turn and multilingual\ninstructions. Multi-IF, which utilizes a hybrid framework combining LLM and\nhuman annotators, expands upon the IFEval by incorporating multi-turn sequences\nand translating the English prompts into another 7 languages, resulting in a\ndataset of 4,501 multilingual conversations, where each has three turns. Our\nevaluation of 14 state-of-the-art LLMs on Multi-IF reveals that it presents a\nsignificantly more challenging task than existing benchmarks. All the models\ntested showed a higher rate of failure in executing instructions correctly with\neach additional turn. For example, o1-preview drops from 0.877 at the first\nturn to 0.707 at the third turn in terms of average accuracy over all\nlanguages. Moreover, languages with non-Latin scripts (Hindi, Russian, and\nChinese) generally exhibit higher error rates, suggesting potential limitations\nin the models' multilingual capabilities. We release Multi-IF prompts and the\nevaluation code base to encourage further research in this critical area.", "published": "2024-10-21 00:59:47", "link": "http://arxiv.org/abs/2410.15553v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Search Space in Gboard Decoder", "abstract": "Gboard Decoder produces suggestions by looking for paths that best match\ninput touch points on the context aware search space, which is backed by the\nlanguage Finite State Transducers (FST). The language FST is currently an\nN-gram language model (LM). However, N-gram LMs, limited in context length, are\nknown to have sparsity problem under device model size constraint. In this\npaper, we propose \\textbf{Neural Search Space} which substitutes the N-gram LM\nwith a Neural Network LM (NN-LM) and dynamically constructs the search space\nduring decoding. Specifically, we integrate the long range context awareness of\nNN-LM into the search space by converting its outputs given context, into the\nlanguage FST at runtime. This involves language FST structure redesign, pruning\nstrategy tuning, and data structure optimizations. Online experiments\ndemonstrate improved quality results, reducing Words Modified Ratio by [0.26\\%,\n1.19\\%] on various locales with acceptable latency increases. This work opens\nnew avenues for further improving keyboard decoding quality by enhancing neural\nLM more directly.", "published": "2024-10-21 01:50:59", "link": "http://arxiv.org/abs/2410.15575v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Guardians of Discourse: Evaluating LLMs on Multilingual Offensive\n  Language Detection", "abstract": "Identifying offensive language is essential for maintaining safety and\nsustainability in the social media era. Though large language models (LLMs)\nhave demonstrated encouraging potential in social media analytics, they lack\nthorough evaluation when in offensive language detection, particularly in\nmultilingual environments. We for the first time evaluate multilingual\noffensive language detection of LLMs in three languages: English, Spanish, and\nGerman with three LLMs, GPT-3.5, Flan-T5, and Mistral, in both monolingual and\nmultilingual settings. We further examine the impact of different prompt\nlanguages and augmented translation data for the task in non-English contexts.\nFurthermore, we discuss the impact of the inherent bias in LLMs and the\ndatasets in the mispredictions related to sensitive topics.", "published": "2024-10-21 04:08:16", "link": "http://arxiv.org/abs/2410.15623v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Large Language Models Invent Algorithms to Improve Themselves?", "abstract": "Large Language Models (LLMs) have shown remarkable performance improvements\nand are rapidly gaining adoption in industry. However, the methods for\nimproving LLMs are still designed by humans, which restricts the invention of\nnew model-improving algorithms to human expertise and imagination. To address\nthis, we propose the Self-Developing framework, which enables LLMs to\nautonomously generate and learn model-improvement algorithms. In this\nframework, the seed model generates, applies, and learns model-improving\nalgorithms, continuously improving both the seed model and the algorithms\nthemselves. Among model-improving strategies, we focus on model merging\nalgorithms. In mathematical reasoning tasks, Self-Developing discovers novel\nmerging strategies and outperforms human-designed methods. On GSM8k, the\ndiscovered algorithms improve the seed model by 6% and surpass human-designed\nmethods by 4.3%. Moreover, they exhibit strong transferability, achieving a\n7.4% performance gain on out-of-domain models. These results suggest that LLMs\ncan autonomously develop effective model-improvement techniques beyond human\nintuition.", "published": "2024-10-21 04:57:09", "link": "http://arxiv.org/abs/2410.15639v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SMILES-Prompting: A Novel Approach to LLM Jailbreak Attacks in Chemical\n  Synthesis", "abstract": "The increasing integration of large language models (LLMs) across various\nfields has heightened concerns about their potential to propagate dangerous\ninformation. This paper specifically explores the security vulnerabilities of\nLLMs within the field of chemistry, particularly their capacity to provide\ninstructions for synthesizing hazardous substances. We evaluate the\neffectiveness of several prompt injection attack methods, including\nred-teaming, explicit prompting, and implicit prompting. Additionally, we\nintroduce a novel attack technique named SMILES-prompting, which uses the\nSimplified Molecular-Input Line-Entry System (SMILES) to reference chemical\nsubstances. Our findings reveal that SMILES-prompting can effectively bypass\ncurrent safety mechanisms. These findings highlight the urgent need for\nenhanced domain-specific safeguards in LLMs to prevent misuse and improve their\npotential for positive social impact.", "published": "2024-10-21 05:01:50", "link": "http://arxiv.org/abs/2410.15641v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DomainSum: A Hierarchical Benchmark for Fine-Grained Domain Shift in\n  Abstractive Text Summarization", "abstract": "Most research on abstractive summarization focuses on single-domain\napplications, often neglecting how domain shifts between documents affect\nperformance and the generalization ability of summarization models. To address\nthis issue, we introduce DomainSum, a hierarchical benchmark designed to\ncapture fine-grained domain shifts in abstractive summarization. We categorize\nthese shifts into three levels: genre, style, and topic, and demonstrate\nthrough comprehensive benchmark analysis that they follow a hierarchical\nstructure. Furthermore, we evaluate the domain generalization capabilities of\ncommonly used pre-trained language models (PLMs) and large language models\n(LLMs) in in-domain and cross-domain settings.", "published": "2024-10-21 06:55:35", "link": "http://arxiv.org/abs/2410.15687v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Terminology Integration for LLM-based Translation in\n  Specialized Domains", "abstract": "Traditional machine translation methods typically involve training models\ndirectly on large parallel corpora, with limited emphasis on specialized\nterminology. However, In specialized fields such as patent, finance, or\nbiomedical domains, terminology is crucial for translation, with many terms\nthat needs to be translated following agreed-upon conventions. In this paper we\nintroduce a methodology that efficiently trains models with a smaller amount of\ndata while preserving the accuracy of terminology translation. We achieve this\nthrough a systematic process of term extraction and glossary creation using the\nTrie Tree algorithm, followed by data reconstruction to teach the LLM how to\nintegrate these specialized terms. This methodology enhances the model's\nability to handle specialized terminology and ensures high-quality\ntranslations, particularly in fields where term consistency is crucial. Our\napproach has demonstrated exceptional performance, achieving the highest\ntranslation score among participants in the WMT patent task to date, showcasing\nits effectiveness and broad applicability in specialized translation domains\nwhere general methods often fall short.", "published": "2024-10-21 07:01:25", "link": "http://arxiv.org/abs/2410.15690v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mitigating Hallucinations of Large Language Models in Medical\n  Information Extraction via Contrastive Decoding", "abstract": "The impressive capabilities of large language models (LLMs) have attracted\nextensive interests of applying LLMs to medical field. However, the complex\nnature of clinical environments presents significant hallucination challenges\nfor LLMs, hindering their widespread adoption. In this paper, we address these\nhallucination issues in the context of Medical Information Extraction (MIE)\ntasks by introducing ALternate Contrastive Decoding (ALCD). We begin by\nredefining MIE tasks as an identify-and-classify process. We then separate the\nidentification and classification functions of LLMs by selectively masking the\noptimization of tokens during fine-tuning. During the inference stage, we\nalternately contrast output distributions derived from sub-task models. This\napproach aims to selectively enhance the identification and classification\ncapabilities while minimizing the influence of other inherent abilities in\nLLMs. Additionally, we propose an alternate adaptive constraint strategy to\nmore effectively adjust the scale and scope of contrastive tokens. Through\ncomprehensive experiments on two different backbones and six diverse medical\ninformation extraction tasks, ALCD demonstrates significant improvements in\nresolving hallucination issues compared to conventional decoding methods.", "published": "2024-10-21 07:19:19", "link": "http://arxiv.org/abs/2410.15702v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Toeing the Party Line: Election Manifestos as a Key to Understand\n  Political Discourse on Twitter", "abstract": "Political discourse on Twitter is a moving target: politicians continuously\nmake statements about their positions. It is therefore crucial to track their\ndiscourse on social media to understand their ideological positions and goals.\nHowever, Twitter data is also challenging to work with since it is ambiguous\nand often dependent on social context, and consequently, recent work on\npolitical positioning has tended to focus strongly on manifestos (parties'\nelectoral programs) rather than social media.\n  In this paper, we extend recently proposed methods to predict pairwise\npositional similarities between parties from the manifesto case to the Twitter\ncase, using hashtags as a signal to fine-tune text representations, without the\nneed for manual annotation. We verify the efficacy of fine-tuning and conduct a\nseries of experiments that assess the robustness of our method for low-resource\nscenarios. We find that our method yields stable positioning reflective of\nmanifesto positioning, both in scenarios with all tweets of candidates across\nyears available and when only smaller subsets from shorter time periods are\navailable. This indicates that it is possible to reliably analyze the relative\npositioning of actors forgoing manual annotation, even in the noisier context\nof social media.", "published": "2024-10-21 08:01:46", "link": "http://arxiv.org/abs/2410.15743v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Did somebody say \"Gest-IT\"? A pilot exploration of multimodal data\n  management", "abstract": "The paper presents a pilot exploration of the construction, management and\nanalysis of a multimodal corpus. Through a three-layer annotation that provides\northographic, prosodic, and gestural transcriptions, the Gest-IT resource\nallows to investigate the variation of gesture-making patterns in conversations\nbetween sighted people and people with visual impairment. After discussing the\ntranscription methods and technical procedures employed in our study, we\npropose a unified CoNLL-U corpus and indicate our future steps", "published": "2024-10-21 09:42:13", "link": "http://arxiv.org/abs/2410.15825v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Principles of semantic and functional efficiency in grammatical\n  patterning", "abstract": "Grammatical features such as number and gender serve two central functions in\nhuman languages. While they encode salient semantic attributes like numerosity\nand animacy, they also offload sentence processing cost by predictably linking\nwords together via grammatical agreement. Grammars exhibit consistent\norganizational patterns across diverse languages, invariably rooted in a\nsemantic foundation, a widely confirmed but still theoretically unexplained\nphenomenon. To explain the basis of universal grammatical patterns, we unify\ntwo fundamental properties of grammar, semantic encoding and agreement-based\npredictability, into a single information-theoretic objective under cognitive\nconstraints. Our analyses reveal that grammatical organization provably\ninherits from perceptual attributes, but that grammars empirically prioritize\nfunctional goals, promoting efficient language processing over semantic\nencoding.", "published": "2024-10-21 10:49:54", "link": "http://arxiv.org/abs/2410.15865v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DefVerify: Do Hate Speech Models Reflect Their Dataset's Definition?", "abstract": "When building a predictive model, it is often difficult to ensure that\napplication-specific requirements are encoded by the model that will eventually\nbe deployed. Consider researchers working on hate speech detection. They will\nhave an idea of what is considered hate speech, but building a model that\nreflects their view accurately requires preserving those ideals throughout the\nworkflow of data set construction and model training. Complications such as\nsampling bias, annotation bias, and model misspecification almost always arise,\npossibly resulting in a gap between the application specification and the\nmodel's actual behavior upon deployment. To address this issue for hate speech\ndetection, we propose DefVerify: a 3-step procedure that (i) encodes a\nuser-specified definition of hate speech, (ii) quantifies to what extent the\nmodel reflects the intended definition, and (iii) tries to identify the point\nof failure in the workflow. We use DefVerify to find gaps between definition\nand model behavior when applied to six popular hate speech benchmark datasets.", "published": "2024-10-21 11:33:18", "link": "http://arxiv.org/abs/2410.15911v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CausalGraph2LLM: Evaluating LLMs for Causal Queries", "abstract": "Causality is essential in scientific research, enabling researchers to\ninterpret true relationships between variables. These causal relationships are\noften represented by causal graphs, which are directed acyclic graphs. With the\nrecent advancements in Large Language Models (LLMs), there is an increasing\ninterest in exploring their capabilities in causal reasoning and their\npotential use to hypothesize causal graphs. These tasks necessitate the LLMs to\nencode the causal graph effectively for subsequent downstream tasks. In this\npaper, we introduce CausalGraph2LLM, a comprehensive benchmark comprising over\n700k queries across diverse causal graph settings to evaluate the causal\nreasoning capabilities of LLMs. We categorize the causal queries into two\ntypes: graph-level and node-level queries. We benchmark both open-sourced and\npropriety models for our study. Our findings reveal that while LLMs show\npromise in this domain, they are highly sensitive to the encoding used. Even\ncapable models like GPT-4 and Gemini-1.5 exhibit sensitivity to encoding, with\ndeviations of about $60\\%$. We further demonstrate this sensitivity for\ndownstream causal intervention tasks. Moreover, we observe that LLMs can often\ndisplay biases when presented with contextual information about a causal graph,\npotentially stemming from their parametric memory.", "published": "2024-10-21 12:12:21", "link": "http://arxiv.org/abs/2410.15939v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Findings of the Third Shared Task on Multilingual Coreference Resolution", "abstract": "The paper presents an overview of the third edition of the shared task on\nmultilingual coreference resolution, held as part of the CRAC 2024 workshop.\nSimilarly to the previous two editions, the participants were challenged to\ndevelop systems capable of identifying mentions and clustering them based on\nidentity coreference.\n  This year's edition took another step towards real-world application by not\nproviding participants with gold slots for zero anaphora, increasing the task's\ncomplexity and realism. In addition, the shared task was expanded to include a\nmore diverse set of languages, with a particular focus on historical languages.\nThe training and evaluation data were drawn from version 1.2 of the\nmultilingual collection of harmonized coreference resources CorefUD,\nencompassing 21 datasets across 15 languages. 6 systems competed in this shared\ntask.", "published": "2024-10-21 12:30:44", "link": "http://arxiv.org/abs/2410.15949v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Policy-driven Knowledge Selection and Response Generation for\n  Document-grounded Dialogue", "abstract": "Document-grounded dialogue (DGD) uses documents as external knowledge for\ndialogue generation. Correctly understanding the dialogue context is crucial\nfor selecting knowledge from the document and generating proper responses. In\nthis paper, we propose using a dialogue policy to help the dialogue\nunderstanding in DGD. Our dialogue policy consists of two kinds of guiding\nsignals: utterance function and topic transfer intent. The utterance function\nreflects the purpose and style of an utterance, and the topic transfer intent\nreflects the topic and content of an utterance. We propose a novel framework\nexploiting our dialogue policy for two core tasks in DGD, namely knowledge\nselection (KS) and response generation (RG). The framework consists of two\nmodules: the Policy planner leverages policy-aware dialogue representation to\nselect knowledge and predict the policy of the response; the generator uses\npolicy/knowledge-aware dialogue representation for response generation. Our\npolicy-driven model gets state-of-the-art performance on three public\nbenchmarks and we provide a detailed analysis of the experimental results. Our\ncode/data will be released on GitHub.", "published": "2024-10-21 12:58:03", "link": "http://arxiv.org/abs/2410.15970v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Steering Knowledge Selection Behaviours in LLMs via SAE-Based\n  Representation Engineering", "abstract": "Large language models (LLMs) can store a significant amount of factual\nknowledge in their parameters. However, their parametric knowledge may conflict\nwith the information provided in the context -- this phenomenon, known as\n\\emph{context-memory knowledge conflicts}, can lead to undesirable model\nbehaviour, such as reliance on outdated or incorrect information. Analysing the\ninternal activations of LLMs, we find that they can internally register the\nsignals of knowledge conflict at mid-layers. Such signals allow us to detect\nwhether a knowledge conflict occurs and use \\emph{inference-time} intervention\nstrategies to resolve it. In this work, we propose \\textsc{SpARE}, a\n\\emph{training-free} representation engineering method that uses pre-trained\nsparse auto-encoders (SAEs) to control the knowledge selection behaviour of\nLLMs. \\textsc{SpARE} identifies the functional features that control the\nknowledge selection behaviours and applies them to edit the internal\nactivations of LLMs at inference time. Our experimental results show that\n\\textsc{SpARE} can effectively control the usage of either knowledge source to\nresolve knowledge conflict in open-domain question-answering tasks, surpassing\nexisting representation engineering methods ($+10\\%$) as well as contrastive\ndecoding methods ($+15\\%$).", "published": "2024-10-21 13:30:47", "link": "http://arxiv.org/abs/2410.15999v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Continual Fine-Tuning for Enhancing Language Ability in Large\n  Language Model", "abstract": "A common challenge towards the adaptability of Large Language Models (LLMs)\nis their ability to learn new languages over time without hampering the model's\nperformance on languages in which the model is already proficient (usually\nEnglish). Continual fine-tuning (CFT) is the process of sequentially\nfine-tuning an LLM to enable the model to adapt to downstream tasks with\nvarying data distributions and time shifts. This paper focuses on the language\nadaptability of LLMs through CFT. We study a two-phase CFT process in which an\nEnglish-only end-to-end fine-tuned LLM from Phase 1 (predominantly Task\nAbility) is sequentially fine-tuned on a multilingual dataset -- comprising\ntask data in new languages -- in Phase 2 (predominantly Language Ability). We\nobserve that the ``similarity'' of Phase 2 tasks with Phase 1 determines the\nLLM's adaptability. For similar phase-wise datasets, the LLM after Phase 2 does\nnot show deterioration in task ability. In contrast, when the phase-wise\ndatasets are not similar, the LLM's task ability deteriorates. We test our\nhypothesis on the open-source \\mis\\ and \\llm\\ models with multiple phase-wise\ndataset pairs. To address the deterioration, we analyze tailored variants of\ntwo CFT methods: layer freezing and generative replay. Our findings demonstrate\ntheir effectiveness in enhancing the language ability of LLMs while preserving\ntask performance, in comparison to relevant baselines.", "published": "2024-10-21 13:39:03", "link": "http://arxiv.org/abs/2410.16006v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ComPO: Community Preferences for Language Model Personalization", "abstract": "Conventional algorithms for training language models (LMs) with human\nfeedback rely on preferences that are assumed to account for an \"average\" user,\ndisregarding subjectivity and finer-grained variations. Recent studies have\nraised concerns that aggregating such diverse and often contradictory human\nfeedback to finetune models results in generic models that generate outputs not\npreferred by many user groups, as they tend to average out styles and norms. To\naddress this issue, we draw inspiration from recommendation systems and propose\nComPO, a method to personalize preference optimization in LMs by\ncontextualizing the probability distribution of model outputs with the\npreference provider. Focusing on group-level preferences rather than\nindividuals, we collect and release ComPRed, a question answering dataset with\ncommunity-level preferences from Reddit. This dataset facilitates studying\ndiversity in preferences without incurring privacy concerns associated with\nindividual feedback. Our experiments reveal that conditioning language models\non a community identifier (i.e., subreddit name) during preference tuning\nsubstantially enhances model performance. Conversely, replacing this context\nwith random subreddit identifiers significantly diminishes performance,\nhighlighting the effectiveness of our approach in tailoring responses to\ncommunities' preferences.", "published": "2024-10-21 14:02:40", "link": "http://arxiv.org/abs/2410.16027v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models Know What To Say But Not When To Speak", "abstract": "Turn-taking is a fundamental mechanism in human communication that ensures\nsmooth and coherent verbal interactions. Recent advances in Large Language\nModels (LLMs) have motivated their use in improving the turn-taking\ncapabilities of Spoken Dialogue Systems (SDS), such as their ability to respond\nat appropriate times. However, existing models often struggle to predict\nopportunities for speaking -- called Transition Relevance Places (TRPs) -- in\nnatural, unscripted conversations, focusing only on turn-final TRPs and not\nwithin-turn TRPs. To address these limitations, we introduce a novel dataset of\nparticipant-labeled within-turn TRPs and use it to evaluate the performance of\nstate-of-the-art LLMs in predicting opportunities for speaking. Our experiments\nreveal the current limitations of LLMs in modeling unscripted spoken\ninteractions, highlighting areas for improvement and paving the way for more\nnaturalistic dialogue systems.", "published": "2024-10-21 14:20:25", "link": "http://arxiv.org/abs/2410.16044v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Surprise! Uniform Information Density Isn't the Whole Story: Predicting\n  Surprisal Contours in Long-form Discourse", "abstract": "The Uniform Information Density (UID) hypothesis posits that speakers tend to\ndistribute information evenly across linguistic units to achieve efficient\ncommunication. Of course, information rate in texts and discourses is not\nperfectly uniform. While these fluctuations can be viewed as theoretically\nuninteresting noise on top of a uniform target, another explanation is that UID\nis not the only functional pressure regulating information content in a\nlanguage. Speakers may also seek to maintain interest, adhere to writing\nconventions, and build compelling arguments. In this paper, we propose one such\nfunctional pressure; namely that speakers modulate information rate based on\nlocation within a hierarchically-structured model of discourse. We term this\nthe Structured Context Hypothesis and test it by predicting the surprisal\ncontours of naturally occurring discourses extracted from large language models\nusing predictors derived from discourse structure. We find that hierarchical\npredictors are significant predictors of a discourse's information contour and\nthat deeply nested hierarchical predictors are more predictive than shallow\nones. This work takes an initial step beyond UID to propose testable hypotheses\nfor why the information rate fluctuates in predictable ways", "published": "2024-10-21 14:42:37", "link": "http://arxiv.org/abs/2410.16062v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rolling the DICE on Idiomaticity: How LLMs Fail to Grasp Context", "abstract": "Human processing of idioms relies on understanding the contextual sentences\nin which idioms occur, as well as language-intrinsic features such as frequency\nand speaker-intrinsic factors like familiarity. While LLMs have shown high\nperformance on idiomaticity detection tasks, this success may be attributed to\nreasoning shortcuts in existing datasets. To this end, we construct a novel,\ncontrolled contrastive dataset designed to test whether LLMs can effectively\nuse context to disambiguate idiomatic meaning. Additionally, we explore how\ncollocational frequency and sentence probability influence model performance.\nOur findings reveal that LLMs often fail to resolve idiomaticity when it is\nrequired to attend to the surrounding context, and that models perform better\non sentences that have higher likelihood. The collocational frequency of\nexpressions also impacts performance. We make our code and dataset publicly\navailable.", "published": "2024-10-21 14:47:37", "link": "http://arxiv.org/abs/2410.16069v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analysing the Residual Stream of Language Models Under Knowledge\n  Conflicts", "abstract": "Large language models (LLMs) can store a significant amount of factual\nknowledge in their parameters. However, their parametric knowledge may conflict\nwith the information provided in the context. Such conflicts can lead to\nundesirable model behaviour, such as reliance on outdated or incorrect\ninformation. In this work, we investigate whether LLMs can identify knowledge\nconflicts and whether it is possible to know which source of knowledge the\nmodel will rely on by analysing the residual stream of the LLM. Through probing\ntasks, we find that LLMs can internally register the signal of knowledge\nconflict in the residual stream, which can be accurately detected by probing\nthe intermediate model activations. This allows us to detect conflicts within\nthe residual stream before generating the answers without modifying the input\nor model parameters. Moreover, we find that the residual stream shows\nsignificantly different patterns when the model relies on contextual knowledge\nversus parametric knowledge to resolve conflicts. This pattern can be employed\nto estimate the behaviour of LLMs when conflict happens and prevent unexpected\nanswers before producing the answers. Our analysis offers insights into how\nLLMs internally manage knowledge conflicts and provides a foundation for\ndeveloping methods to control the knowledge selection processes.", "published": "2024-10-21 15:12:51", "link": "http://arxiv.org/abs/2410.16090v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do LLMs write like humans? Variation in grammatical and rhetorical\n  styles", "abstract": "Large language models (LLMs) are capable of writing grammatical text that\nfollows instructions, answers questions, and solves problems. As they have\nadvanced, it has become difficult to distinguish their output from\nhuman-written text. While past research has found some differences in surface\nfeatures such as word choice and punctuation, and developed classifiers to\ndetect LLM output, none has studied the rhetorical styles of LLMs.\n  Using several variants of Llama 3 and GPT-4o, we construct two parallel\ncorpora of human- and LLM-written texts from common prompts. Using Douglas\nBiber's set of lexical, grammatical, and rhetorical features, we identify\nsystematic differences between LLMs and humans and between different LLMs.\nThese differences persist when moving from smaller models to larger ones, and\nare larger for instruction-tuned models than base models. This demonstrates\nthat despite their advanced abilities, LLMs struggle to match human styles, and\nhence more advanced linguistic features can detect patterns in their behavior\nnot previously recognized.", "published": "2024-10-21 15:35:44", "link": "http://arxiv.org/abs/2410.16107v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Psycholinguistic Evaluation of Language Models' Sensitivity to\n  Argument Roles", "abstract": "We present a systematic evaluation of large language models' sensitivity to\nargument roles, i.e., who did what to whom, by replicating psycholinguistic\nstudies on human argument role processing. In three experiments, we find that\nlanguage models are able to distinguish verbs that appear in plausible and\nimplausible contexts, where plausibility is determined through the relation\nbetween the verb and its preceding arguments. However, none of the models\ncapture the same selective patterns that human comprehenders exhibit during\nreal-time verb prediction. This indicates that language models' capacity to\ndetect verb plausibility does not arise from the same mechanism that underlies\nhuman real-time sentence processing.", "published": "2024-10-21 16:05:58", "link": "http://arxiv.org/abs/2410.16139v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "1-bit AI Infra: Part 1.1, Fast and Lossless BitNet b1.58 Inference on\n  CPUs", "abstract": "Recent advances in 1-bit Large Language Models (LLMs), such as BitNet and\nBitNet b1.58, present a promising approach to enhancing the efficiency of LLMs\nin terms of speed and energy consumption. These developments also enable local\nLLM deployment across a broad range of devices. In this work, we introduce\nbitnet.cpp, a tailored software stack designed to unlock the full potential of\n1-bit LLMs. Specifically, we develop a set of kernels to support fast and\nlossless inference of ternary BitNet b1.58 LLMs on CPUs. Extensive experiments\ndemonstrate that bitnet.cpp achieves significant speedups, ranging from 2.37x\nto 6.17x on x86 CPUs and from 1.37x to 5.07x on ARM CPUs, across various model\nsizes. The code is available at https://github.com/microsoft/BitNet.", "published": "2024-10-21 16:14:57", "link": "http://arxiv.org/abs/2410.16144v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Troublemaker with Contagious Jailbreak Makes Chaos in Honest Towns", "abstract": "With the development of large language models, they are widely used as agents\nin various fields. A key component of agents is memory, which stores vital\ninformation but is susceptible to jailbreak attacks. Existing research mainly\nfocuses on single-agent attacks and shared memory attacks. However, real-world\nscenarios often involve independent memory. In this paper, we propose the\nTroublemaker Makes Chaos in Honest Town (TMCHT) task, a large-scale,\nmulti-agent, multi-topology text-based attack evaluation framework. TMCHT\ninvolves one attacker agent attempting to mislead an entire society of agents.\nWe identify two major challenges in multi-agent attacks: (1) Non-complete graph\nstructure, (2) Large-scale systems. We attribute these challenges to a\nphenomenon we term toxicity disappearing. To address these issues, we propose\nan Adversarial Replication Contagious Jailbreak (ARCJ) method, which optimizes\nthe retrieval suffix to make poisoned samples more easily retrieved and\noptimizes the replication suffix to make poisoned samples have contagious\nability. We demonstrate the superiority of our approach in TMCHT, with 23.51%,\n18.95%, and 52.93% improvements in line topology, star topology, and 100-agent\nsettings. Encourage community attention to the security of multi-agent systems.", "published": "2024-10-21 16:21:24", "link": "http://arxiv.org/abs/2410.16155v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Pretraining via Active Forgetting for Improving Cross Lingual\n  Transfer for Decoder Language Models", "abstract": "Large Language Models (LLMs) demonstrate exceptional capabilities in a\nmultitude of NLP tasks. However, the efficacy of such models to languages other\nthan English is often limited. Prior works have shown that encoder-only models\nsuch as BERT or XLM-RoBERTa show impressive cross lingual transfer of their\ncapabilities from English to other languages. In this work, we propose a\npretraining strategy that uses active forgetting to achieve similar cross\nlingual transfer in decoder-only LLMs. We show that LLMs pretrained with active\nforgetting are highly effective when adapting to new and unseen languages.\nThrough extensive experimentation, we find that LLMs pretrained with active\nforgetting are able to learn better multilingual representations which\ntranslates to better performance in many downstream tasks.", "published": "2024-10-21 16:33:16", "link": "http://arxiv.org/abs/2410.16168v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RM-Bench: Benchmarking Reward Models of Language Models with Subtlety\n  and Style", "abstract": "Reward models are critical in techniques like Reinforcement Learning from\nHuman Feedback (RLHF) and Inference Scaling Laws, where they guide language\nmodel alignment and select optimal responses. Despite their importance,\nexisting reward model benchmarks often evaluate models by asking them to\ndistinguish between responses generated by models of varying power. However,\nthis approach fails to assess reward models on subtle but critical content\nchanges and variations in style, resulting in a low correlation with policy\nmodel performance. To this end, we introduce RM-Bench, a novel benchmark\ndesigned to evaluate reward models based on their sensitivity to subtle content\ndifferences and resistance to style biases. Extensive experiments demonstrate\nthat RM-Bench strongly correlates with policy model performance, making it a\nreliable reference for selecting reward models to align language models\neffectively. We evaluate nearly 40 reward models on RM-Bench. Our results\nreveal that even state-of-the-art models achieve an average performance of only\n46.6%, which falls short of random-level accuracy (50%) when faced with style\nbias interference. These findings highlight the significant room for\nimprovement in current reward models. Related code and data are available at\nhttps://github.com/THU-KEG/RM-Bench.", "published": "2024-10-21 16:48:26", "link": "http://arxiv.org/abs/2410.16184v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contamination Report for Multilingual Benchmarks", "abstract": "Benchmark contamination refers to the presence of test datasets in Large\nLanguage Model (LLM) pre-training or post-training data. Contamination can lead\nto inflated scores on benchmarks, compromising evaluation results and making it\ndifficult to determine the capabilities of models. In this work, we study the\ncontamination of popular multilingual benchmarks in LLMs that support multiple\nlanguages. We use the Black Box test to determine whether $7$ frequently used\nmultilingual benchmarks are contaminated in $7$ popular open and closed LLMs\nand find that almost all models show signs of being contaminated with almost\nall the benchmarks we test. Our findings can help the community determine the\nbest set of benchmarks to use for multilingual evaluation.", "published": "2024-10-21 16:49:35", "link": "http://arxiv.org/abs/2410.16186v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Building A Coding Assistant via the Retrieval-Augmented Language Model", "abstract": "Pretrained language models have shown strong effectiveness in code-related\ntasks, such as code retrieval, code generation, code summarization, and code\ncompletion tasks. In this paper, we propose COde assistaNt viA\nretrieval-augmeNted language model (CONAN), which aims to build a code\nassistant by mimicking the knowledge-seeking behaviors of humans during coding.\nSpecifically, it consists of a code structure aware retriever (CONAN-R) and a\ndual-view code representation-based retrieval-augmented generation model\n(CONAN-G). CONAN-R pretrains CodeT5 using Code-Documentation Alignment and\nMasked Entity Prediction tasks to make language models code structure-aware and\nlearn effective representations for code snippets and documentation. Then\nCONAN-G designs a dual-view code representation mechanism for implementing a\nretrieval-augmented code generation model. CONAN-G regards the code\ndocumentation descriptions as prompts, which help language models better\nunderstand the code semantics. Our experiments show that CONAN achieves\nconvincing performance on different code generation tasks and significantly\noutperforms previous retrieval augmented code generation models. Our further\nanalyses show that CONAN learns tailored representations for both code snippets\nand documentation by aligning code-documentation data pairs and capturing\nstructural semantics by masking and predicting entities in the code data.\nAdditionally, the retrieved code snippets and documentation provide necessary\ninformation from both program language and natural language to assist the code\ngeneration process. CONAN can also be used as an assistant for Large Language\nModels (LLMs), providing LLMs with external knowledge in shorter code document\nlengths to improve their effectiveness on various code tasks. It shows the\nability of CONAN to extract necessary information and help filter out the noise\nfrom retrieved code documents.", "published": "2024-10-21 17:34:39", "link": "http://arxiv.org/abs/2410.16229v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ToW: Thoughts of Words Improve Reasoning in Large Language Models", "abstract": "We introduce thoughts of words (ToW), a novel training-time data-augmentation\nmethod for next-word prediction. ToW views next-word prediction as a core\nreasoning task and injects fine-grained thoughts explaining what the next word\nshould be and how it is related to the previous contexts in pre-training texts.\nOur formulation addresses two fundamental drawbacks of existing next-word\nprediction learning schemes: they induce factual hallucination and are\ninefficient for models to learn the implicit reasoning processes in raw texts.\nWhile there are many ways to acquire such thoughts of words, we explore the\nfirst step of acquiring ToW annotations through distilling from larger models.\nAfter continual pre-training with only 70K ToW annotations, we effectively\nimprove models' reasoning performances by 7% to 9% on average and reduce model\nhallucination by up to 10%. At the same time, ToW is entirely agnostic to tasks\nand applications, introducing no additional biases on labels or semantics.", "published": "2024-10-21 17:41:11", "link": "http://arxiv.org/abs/2410.16235v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analyzing Context Contributions in LLM-based Machine Translation", "abstract": "Large language models (LLMs) have achieved state-of-the-art performance in\nmachine translation (MT) and demonstrated the ability to leverage in-context\nlearning through few-shot examples. However, the mechanisms by which LLMs use\ndifferent parts of the input context remain largely unexplored. In this work,\nwe provide a comprehensive analysis of context utilization in MT, studying how\nLLMs use various context parts, such as few-shot examples and the source text,\nwhen generating translations. We highlight several key findings: (1) the source\npart of few-shot examples appears to contribute more than its corresponding\ntargets, irrespective of translation direction; (2) finetuning LLMs with\nparallel data alters the contribution patterns of different context parts; and\n(3) there is a positional bias where earlier few-shot examples have higher\ncontributions to the translated sequence. Finally, we demonstrate that\ninspecting anomalous context contributions can potentially uncover pathological\ntranslations, such as hallucinations. Our findings shed light on the internal\nworkings of LLM-based MT which go beyond those known for standard\nencoder-decoder MT models.", "published": "2024-10-21 17:51:41", "link": "http://arxiv.org/abs/2410.16246v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Knowledge Editing Really Correct Hallucinations?", "abstract": "Large Language Models (LLMs) suffer from hallucinations, referring to the\nnon-factual information in generated content, despite their superior capacities\nacross tasks. Meanwhile, knowledge editing has been developed as a new popular\nparadigm to correct erroneous factual knowledge encoded in LLMs with the\nadvantage of avoiding retraining from scratch. However, a common issue of\nexisting evaluation datasets for knowledge editing is that they do not ensure\nthat LLMs actually generate hallucinated answers to the evaluation questions\nbefore editing. When LLMs are evaluated on such datasets after being edited by\ndifferent techniques, it is hard to directly adopt the performance to assess\nthe effectiveness of different knowledge editing methods in correcting\nhallucinations. Thus, the fundamental question remains insufficiently\nvalidated: Can knowledge editing really correct hallucinations in LLMs? We\nproposed HalluEditBench to holistically benchmark knowledge editing methods in\ncorrecting real-world hallucinations. First, we rigorously construct a massive\nhallucination dataset with 9 domains, 26 topics and more than 6,000\nhallucinations. Then, we assess the performance of knowledge editing methods in\na holistic way on five dimensions including Efficacy, Generalization,\nPortability, Locality, and Robustness. Through HalluEditBench, we have provided\nnew insights into the potentials and limitations of different knowledge editing\nmethods in correcting hallucinations, which could inspire future improvements\nand facilitate progress in the field of knowledge editing.", "published": "2024-10-21 17:55:54", "link": "http://arxiv.org/abs/2410.16251v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KatzBot: Revolutionizing Academic Chatbot for Enhanced Communication", "abstract": "Effective communication within universities is crucial for addressing the\ndiverse information needs of students, alumni, and external stakeholders.\nHowever, existing chatbot systems often fail to deliver accurate,\ncontext-specific responses, resulting in poor user experiences. In this paper,\nwe present KatzBot, an innovative chatbot powered by KatzGPT, a custom Large\nLanguage Model (LLM) fine-tuned on domain-specific academic data. KatzGPT is\ntrained on two university-specific datasets: 6,280 sentence-completion pairs\nand 7,330 question-answer pairs. KatzBot outperforms established existing open\nsource LLMs, achieving higher accuracy and domain relevance. KatzBot offers a\nuser-friendly interface, significantly enhancing user satisfaction in\nreal-world applications. The source code is publicly available at\n\\url{https://github.com/AiAI-99/katzbot}.", "published": "2024-10-21 18:01:08", "link": "http://arxiv.org/abs/2410.16385v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "VipAct: Visual-Perception Enhancement via Specialized VLM Agent\n  Collaboration and Tool-use", "abstract": "While vision-language models (VLMs) have demonstrated remarkable performance\nacross various tasks combining textual and visual information, they continue to\nstruggle with fine-grained visual perception tasks that require detailed\npixel-level analysis. Effectively eliciting comprehensive reasoning from VLMs\non such intricate visual elements remains an open challenge. In this paper, we\npresent VipAct, an agent framework that enhances VLMs by integrating\nmulti-agent collaboration and vision expert models, enabling more precise\nvisual understanding and comprehensive reasoning. VipAct consists of an\norchestrator agent, which manages task requirement analysis, planning, and\ncoordination, along with specialized agents that handle specific tasks such as\nimage captioning and vision expert models that provide high-precision\nperceptual information. This multi-agent approach allows VLMs to better perform\nfine-grained visual perception tasks by synergizing planning, reasoning, and\ntool use. We evaluate VipAct on benchmarks featuring a diverse set of visual\nperception tasks, with experimental results demonstrating significant\nperformance improvements over state-of-the-art baselines across all tasks.\nFurthermore, comprehensive ablation studies reveal the critical role of\nmulti-agent collaboration in eliciting more detailed System-2 reasoning and\nhighlight the importance of image input for task planning. Additionally, our\nerror analysis identifies patterns of VLMs' inherent limitations in visual\nperception, providing insights into potential future improvements. VipAct\noffers a flexible and extensible framework, paving the way for more advanced\nvisual perception systems across various real-world applications.", "published": "2024-10-21 18:10:26", "link": "http://arxiv.org/abs/2410.16400v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Susu Box or Piggy Bank: Assessing Cultural Commonsense Knowledge between\n  Ghana and the U.S", "abstract": "Recent work has highlighted the culturally-contingent nature of commonsense\nknowledge. We introduce AMAMMER${\\epsilon}$, a test set of 525 multiple-choice\nquestions designed to evaluate the commonsense knowledge of English LLMs,\nrelative to the cultural contexts of Ghana and the United States. To create\nAMAMMER${\\epsilon}$, we select a set of multiple-choice questions (MCQs) from\nexisting commonsense datasets and rewrite them in a multi-stage process\ninvolving surveys of Ghanaian and U.S. participants. In three rounds of\nsurveys, participants from both pools are solicited to (1) write correct and\nincorrect answer choices, (2) rate individual answer choices on a 5-point\nLikert scale, and (3) select the best answer choice from the newly-constructed\nMCQ items, in a final validation step. By engaging participants at multiple\nstages, our procedure ensures that participant perspectives are incorporated\nboth in the creation and validation of test items, resulting in high levels of\nagreement within each pool. We evaluate several off-the-shelf English LLMs on\nAMAMMER${\\epsilon}$. Uniformly, models prefer answers choices that align with\nthe preferences of U.S. annotators over Ghanaian annotators. Additionally, when\ntest items specify a cultural context (Ghana or the U.S.), models exhibit some\nability to adapt, but performance is consistently better in U.S. contexts than\nGhanaian. As large resources are devoted to the advancement of English LLMs,\nour findings underscore the need for culturally adaptable models and\nevaluations to meet the needs of diverse English-speaking populations around\nthe world.", "published": "2024-10-21 19:25:31", "link": "http://arxiv.org/abs/2410.16451v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "To the Globe (TTG): Towards Language-Driven Guaranteed Travel Planning", "abstract": "Travel planning is a challenging and time-consuming task that aims to find an\nitinerary which satisfies multiple, interdependent constraints regarding\nflights, accommodations, attractions, and other travel arrangements. In this\npaper, we propose To the Globe (TTG), a real-time demo system that takes\nnatural language requests from users, translates it to symbolic form via a\nfine-tuned Large Language Model, and produces optimal travel itineraries with\nMixed Integer Linear Programming solvers. The overall system takes ~5 seconds\nto reply to the user request with guaranteed itineraries. To train TTG, we\ndevelop a synthetic data pipeline that generates user requests, flight and\nhotel information in symbolic form without human annotations, based on the\nstatistics of real-world datasets, and fine-tune an LLM to translate NL user\nrequests to their symbolic form, which is sent to the symbolic solver to\ncompute optimal itineraries. Our NL-symbolic translation achieves ~91% exact\nmatch in a backtranslation metric (i.e., whether the estimated symbolic form of\ngenerated natural language matches the groundtruth), and its returned\nitineraries have a ratio of 0.979 compared to the optimal cost of the ground\ntruth user request. When evaluated by users, TTG achieves consistently high Net\nPromoter Scores (NPS) of 35-40% on generated itinerary.", "published": "2024-10-21 19:30:05", "link": "http://arxiv.org/abs/2410.16456v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparative Study of Multilingual Idioms and Similes in Large Language\n  Models", "abstract": "This study addresses the gap in the literature concerning the comparative\nperformance of LLMs in interpreting different types of figurative language\nacross multiple languages. By evaluating LLMs using two multilingual datasets\non simile and idiom interpretation, we explore the effectiveness of various\nprompt engineering strategies, including chain-of-thought, few-shot, and\nEnglish translation prompts. We extend the language of these datasets to\nPersian as well by building two new evaluation sets. Our comprehensive\nassessment involves both closed-source (GPT-3.5, GPT-4o mini, Gemini 1.5), and\nopen-source models (Llama 3.1, Qwen2), highlighting significant differences in\nperformance across languages and figurative types. Our findings reveal that\nwhile prompt engineering methods are generally effective, their success varies\nby figurative type, language, and model. We also observe that open-source\nmodels struggle particularly with low-resource languages in similes.\nAdditionally, idiom interpretation is nearing saturation for many languages,\nnecessitating more challenging evaluations.", "published": "2024-10-21 19:40:05", "link": "http://arxiv.org/abs/2410.16461v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DocEdit-v2: Document Structure Editing Via Multimodal LLM Grounding", "abstract": "Document structure editing involves manipulating localized textual, visual,\nand layout components in document images based on the user's requests. Past\nworks have shown that multimodal grounding of user requests in the document\nimage and identifying the accurate structural components and their associated\nattributes remain key challenges for this task. To address these, we introduce\nthe DocEdit-v2, a novel framework that performs end-to-end document editing by\nleveraging Large Multimodal Models (LMMs). It consists of three novel\ncomponents: (1) Doc2Command, which simultaneously localizes edit regions of\ninterest (RoI) and disambiguates user edit requests into edit commands; (2)\nLLM-based Command Reformulation prompting to tailor edit commands originally\nintended for specialized software into edit instructions suitable for\ngeneralist LMMs. (3) Moreover, DocEdit-v2 processes these outputs via Large\nMultimodal Models like GPT-4V and Gemini, to parse the document layout, execute\nedits on grounded Region of Interest (RoI), and generate the edited document\nimage. Extensive experiments on the DocEdit dataset show that DocEdit-v2\nsignificantly outperforms strong baselines on edit command generation (2-33%),\nRoI bounding box detection (12-31%), and overall document editing (1-12\\%)\ntasks.", "published": "2024-10-21 19:59:04", "link": "http://arxiv.org/abs/2410.16472v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-head Sequence Tagging Model for Grammatical Error Correction", "abstract": "To solve the Grammatical Error Correction (GEC) problem , a mapping between a\nsource sequence and a target one is needed, where the two differ only on few\nspans. For this reason, the attention has been shifted to the\nnon-autoregressive or sequence tagging models. In which, the GEC has been\nsimplified from Seq2Seq to labeling the input tokens with edit commands chosen\nfrom a large edit space. Due to this large number of classes and the limitation\nof the available datasets, the current sequence tagging approaches still have\nsome issues handling a broad range of grammatical errors just by being\nlaser-focused on one single task. To this end, we simplified the GEC further by\ndividing it into seven related subtasks: Insertion, Deletion, Merge,\nSubstitution, Transformation, Detection, and Correction, with Correction being\nour primary focus. A distinct classification head is dedicated to each of these\nsubtasks. the novel multi-head and multi-task learning model is proposed to\neffectively utilize training data and harness the information from related task\ntraining signals. To mitigate the limited number of available training samples,\na new denoising autoencoder is used to generate a new synthetic dataset to be\nused for pretraining. Additionally, a new character-level transformation is\nproposed to enhance the sequence-to-edit function and improve the model's\nvocabulary coverage. Our single/ensemble model achieves an F0.5 of 74.4/77.0,\nand 68.6/69.1 on BEA-19 (test) and CoNLL-14 (test) respectively. Moreover,\nevaluated on JFLEG test set, the GLEU scores are 61.6 and 61.7 for the single\nand ensemble models, respectively. It mostly outperforms recently published\nstate-of-the-art results by a considerable margin.", "published": "2024-10-21 20:01:06", "link": "http://arxiv.org/abs/2410.16473v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BIG5-CHAT: Shaping LLM Personalities Through Training on Human-Grounded\n  Data", "abstract": "In this work, we tackle the challenge of embedding realistic human\npersonality traits into LLMs. Previous approaches have primarily focused on\nprompt-based methods that describe the behavior associated with the desired\npersonality traits, suffering from realism and validity issues. To address\nthese limitations, we introduce BIG5-CHAT, a large-scale dataset containing\n100,000 dialogues designed to ground models in how humans express their\npersonality in language. Leveraging this dataset, we explore Supervised\nFine-Tuning and Direct Preference Optimization as training-based methods to\nalign LLMs more naturally with human personality patterns. Our methods\noutperform prompting on personality assessments such as BFI and IPIP-NEO, with\ntrait correlations more closely matching human data. Furthermore, our\nexperiments reveal that models trained to exhibit higher conscientiousness,\nhigher agreeableness, lower extraversion, and lower neuroticism display better\nperformance on reasoning tasks, aligning with psychological findings on how\nthese traits impact human cognitive performance. To our knowledge, this work is\nthe first comprehensive study to demonstrate how training-based methods can\nshape LLM personalities through learning from real human behaviors.", "published": "2024-10-21 20:32:27", "link": "http://arxiv.org/abs/2410.16491v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Natural Language Processing for Human Resources: A Survey", "abstract": "Advances in Natural Language Processing (NLP) have the potential to transform\nHR processes, from recruitment to employee management. While recent\nbreakthroughs in NLP have generated significant interest in its industrial\napplications, a comprehensive overview of how NLP can be applied across HR\nactivities is still lacking. This paper discovers opportunities for researchers\nand practitioners to harness NLP's transformative potential in this domain. We\nanalyze key fundamental tasks such as information extraction and text\nclassification, and their roles in downstream applications like recommendation\nand language generation, while also discussing ethical concerns. Additionally,\nwe identify gaps in current research and encourage future work to explore\nholistic approaches for achieving broader objectives in this field.", "published": "2024-10-21 20:41:00", "link": "http://arxiv.org/abs/2410.16498v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rulebreakers Challenge: Revealing a Blind Spot in Large Language Models'\n  Reasoning with Formal Logic", "abstract": "Formal logic has long been applied to natural language reasoning, but this\napproach can sometimes lead to conclusions that, while logically entailed, are\nfactually inconsistent with the premises or are not typically inferred by\nhumans. This study introduces the concept of \"rulebreakers\", which refers to\ninstances where logical entailment diverges from factually acceptable\ninference. We present RULEBREAKERS, a novel dataset for evaluating Large\nLanguage Models' (LLMs) ability to distinguish between rulebreakers and\nnon-rulebreakers. Focusing on modus tollens and disjunctive syllogism, we\nassess six state-of-the-art LLMs using RULEBREAKERS, measuring their\nperformance in terms of token-level exact accuracy and model confidence. Our\nfindings reveal that while most models perform poorly to moderately in\nrecognizing rulebreakers, they demonstrate a latent ability to distinguish\nrulebreakers when assessed by their confidence levels. Further analysis\nsuggests that the failure to recognize rulebreakers is potentially associated\nwith the models' world knowledge and their attention distribution patterns.\nThis research highlights the limitation of LLMs' reasoning capabilities, and\ncontributes to the ongoing discussion on reasoning in LLMs.", "published": "2024-10-21 20:48:16", "link": "http://arxiv.org/abs/2410.16502v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Retrieval-Augmented Generation for Culturally Inclusive Hakka\n  Chatbots: Design Insights and User Perceptions", "abstract": "In an era where cultural preservation is increasingly intertwined with\ntechnological innovation, this study introduces a groundbreaking approach to\npromoting and safeguarding the rich heritage of Taiwanese Hakka culture through\nthe development of a Retrieval-Augmented Generation (RAG)-enhanced chatbot.\nTraditional large language models (LLMs), while powerful, often fall short in\ndelivering accurate and contextually rich responses, particularly in culturally\nspecific domains. By integrating external databases with generative AI models,\nRAG technology bridges this gap, empowering chatbots to not only provide\nprecise answers but also resonate deeply with the cultural nuances that are\ncrucial for authentic interactions. This study delves into the intricate\nprocess of augmenting the chatbot's knowledge base with targeted cultural data,\nspecifically curated to reflect the unique aspects of Hakka traditions,\nlanguage, and practices. Through dynamic information retrieval, the\nRAG-enhanced chatbot becomes a versatile tool capable of handling complex\ninquiries that demand an in-depth understanding of Hakka cultural context. This\nis particularly significant in an age where digital platforms often dilute\ncultural identities, making the role of culturally aware AI systems more\ncritical than ever. System usability studies conducted as part of our research\nreveal a marked improvement in both user satisfaction and engagement,\nhighlighting the chatbot's effectiveness in fostering a deeper connection with\nHakka culture. The feedback underscores the potential of RAG technology to not\nonly enhance user experience but also to serve as a vital instrument in the\nbroader mission of ethnic mainstreaming and cultural celebration.", "published": "2024-10-21 01:36:08", "link": "http://arxiv.org/abs/2410.15572v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Survey of Conversational Search", "abstract": "As a cornerstone of modern information access, search engines have become\nindispensable in everyday life. With the rapid advancements in AI and natural\nlanguage processing (NLP) technologies, particularly large language models\n(LLMs), search engines have evolved to support more intuitive and intelligent\ninteractions between users and systems. Conversational search, an emerging\nparadigm for next-generation search engines, leverages natural language\ndialogue to facilitate complex and precise information retrieval, thus\nattracting significant attention. Unlike traditional keyword-based search\nengines, conversational search systems enhance user experience by supporting\nintricate queries, maintaining context over multi-turn interactions, and\nproviding robust information integration and processing capabilities. Key\ncomponents such as query reformulation, search clarification, conversational\nretrieval, and response generation work in unison to enable these sophisticated\ninteractions. In this survey, we explore the recent advancements and potential\nfuture directions in conversational search, examining the critical modules that\nconstitute a conversational search system. We highlight the integration of LLMs\nin enhancing these systems and discuss the challenges and opportunities that\nlie ahead in this dynamic field. Additionally, we provide insights into\nreal-world applications and robust evaluations of current conversational search\nsystems, aiming to guide future research and development in conversational\nsearch.", "published": "2024-10-21 01:54:46", "link": "http://arxiv.org/abs/2410.15576v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Generalized Probabilistic Attention Mechanism in Transformers", "abstract": "The Transformer architecture has become widely adopted due to its\ndemonstrated success, attributed to the attention mechanism at its core.\nDespite these successes, the attention mechanism of Transformers is associated\nwith two well-known issues: rank-collapse and gradient vanishing. In this\npaper, we present a theoretical analysis that it is inherently difficult to\naddress both issues simultaneously in the conventional attention mechanism. To\nhandle these issues, we introduce a novel class of attention mechanism,\nreferred to as generalized probabilistic attention mechanism (GPAM), and its\ndual-attention implementation within the Transformer architecture. Unlike\nconventional attention mechanisms, GPAM allows for negative attention scores\nwhile preserving a fixed total sum. We provide theoretical evidence that the\nproposed dual-attention GPAM (daGPAM) effectively mitigates both the\nrank-collapse and gradient vanishing issues which are difficult to resolve\nsimultaneously with the conventional attention mechanisms. Furthermore, we\nempirically validate this theoretical evidence, demonstrating the superiority\nof daGPAM compared to other alternative attention mechanisms that were proposed\nto address the same issues. Additionally, we demonstrate the practical benefits\nof GPAM in natural language processing tasks, such as language modeling and\nneural machine translation.", "published": "2024-10-21 01:55:52", "link": "http://arxiv.org/abs/2410.15578v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Language Models are Symbolic Learners in Arithmetic", "abstract": "Large Language Models (LLMs) are thought to struggle with arithmetic learning\ndue to the inherent differences between language modeling and numerical\ncomputation, but concrete evidence has been lacking. This work responds to this\nclaim through a two-side experiment. We first investigate whether LLMs leverage\npartial products during arithmetic learning. We find that although LLMs can\nidentify some partial products after learning, they fail to leverage them for\narithmetic tasks, conversely. We then explore how LLMs approach arithmetic\nsymbolically by breaking tasks into subgroups, hypothesizing that difficulties\narise from subgroup complexity and selection. Our results show that when\nsubgroup complexity is fixed, LLMs treat a collection of different arithmetic\noperations similarly. By analyzing position-level accuracy across different\ntraining sizes, we further observe that it follows a U-shaped pattern: LLMs\nquickly learn the easiest patterns at the first and last positions, while\nprogressively learning the more difficult patterns in the middle positions.\nThis suggests that LLMs select subgroup following an easy-to-hard paradigm\nduring learning. Our work confirms that LLMs are pure symbolic learners in\narithmetic tasks and underscores the importance of understanding them deeply\nthrough subgroup-level quantification.", "published": "2024-10-21 01:57:16", "link": "http://arxiv.org/abs/2410.15580v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "AMPLE: Emotion-Aware Multimodal Fusion Prompt Learning for Fake News\n  Detection", "abstract": "Detecting fake news in large datasets is challenging due to its diversity and\ncomplexity, with traditional approaches often focusing on textual features\nwhile underutilizing semantic and emotional elements. Current methods also rely\nheavily on large annotated datasets, limiting their effectiveness in more\nnuanced analysis. To address these challenges, this paper introduces\nEmotion-\\textbf{A}ware \\textbf{M}ultimodal Fusion \\textbf{P}rompt\n\\textbf{L}\\textbf{E}arning (\\textbf{AMPLE}) framework to address the above\nissue by combining text sentiment analysis with multimodal data and hybrid\nprompt templates. This framework extracts emotional elements from texts by\nleveraging sentiment analysis tools. It then employs Multi-Head Cross-Attention\n(MCA) mechanisms and similarity-aware fusion methods to integrate multimodal\ndata. The proposed AMPLE framework demonstrates strong performance on two\npublic datasets in both few-shot and data-rich settings, with results\nindicating the potential of emotional aspects in fake news detection.\nFurthermore, the study explores the impact of integrating large language models\nwith this method for text sentiment extraction, revealing substantial room for\nfurther improvement. The code can be found at\n:\\url{https://github.com/xxm1215/MMM2025_few-shot/", "published": "2024-10-21 02:19:24", "link": "http://arxiv.org/abs/2410.15591v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GATEAU: Selecting Influential Samples for Long Context Alignment", "abstract": "Aligning large language models to handle instructions with extremely long\ncontexts has yet to be fully investigated. Previous studies attempt to scale up\nthe available data volume by synthesizing long instruction-following samples,\nas constructing such a dataset tends to be challenging for annotators. However,\na lack of a well-defined strategy for ensuring data quality may introduce\nlow-quality samples and restrict the model performance. Thus, we propose\nGATEAU, a novel framework to address the unique challenge of long context\nalignment by identifying the influential samples enriched with long-range\ndependency relations. Specifically, GATEAU measures the long-range dependencies\nfrom two essential aspects: the difficulty of generating target responses due\nto the long-range dependencies, and the difficulty of understanding long inputs\ndue to such dependencies. Comprehensive experiments indicate that GATEAU\neffectively identifies influential samples and the model trained on these\nselected samples exhibits better instruction-following and long-context\nunderstanding capabilities.", "published": "2024-10-21 04:30:53", "link": "http://arxiv.org/abs/2410.15633v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CL-HOI: Cross-Level Human-Object Interaction Distillation from Vision\n  Large Language Models", "abstract": "Human-object interaction (HOI) detection has seen advancements with Vision\nLanguage Models (VLMs), but these methods often depend on extensive manual\nannotations. Vision Large Language Models (VLLMs) can inherently recognize and\nreason about interactions at the image level but are computationally heavy and\nnot designed for instance-level HOI detection. To overcome these limitations,\nwe propose a Cross-Level HOI distillation (CL-HOI) framework, which distills\ninstance-level HOIs from VLLMs image-level understanding without the need for\nmanual annotations. Our approach involves two stages: context distillation,\nwhere a Visual Linguistic Translator (VLT) converts visual information into\nlinguistic form, and interaction distillation, where an Interaction Cognition\nNetwork (ICN) reasons about spatial, visual, and context relations. We design\ncontrastive distillation losses to transfer image-level context and interaction\nknowledge from the teacher to the student model, enabling instance-level HOI\ndetection. Evaluations on HICO-DET and V-COCO datasets demonstrate that our\nCL-HOI surpasses existing weakly supervised methods and VLLM supervised\nmethods, showing its efficacy in detecting HOIs without manual labels.", "published": "2024-10-21 05:51:51", "link": "http://arxiv.org/abs/2410.15657v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Scalable Data Ablation Approximations for Language Models through\n  Modular Training and Merging", "abstract": "Training data compositions for Large Language Models (LLMs) can significantly\naffect their downstream performance. However, a thorough data ablation study\nexploring large sets of candidate data mixtures is typically prohibitively\nexpensive since the full effect is seen only after training the models; this\ncan lead practitioners to settle for sub-optimal data mixtures. We propose an\nefficient method for approximating data ablations which trains individual\nmodels on subsets of a training corpus and reuses them across evaluations of\ncombinations of subsets. In continued pre-training experiments, we find that,\ngiven an arbitrary evaluation set, the perplexity score of a single model\ntrained on a candidate set of data is strongly correlated with perplexity\nscores of parameter averages of models trained on distinct partitions of that\ndata. From this finding, we posit that researchers and practitioners can\nconduct inexpensive simulations of data ablations by maintaining a pool of\nmodels that were each trained on partitions of a large training corpus, and\nassessing candidate data mixtures by evaluating parameter averages of\ncombinations of these models. This approach allows for substantial improvements\nin amortized training efficiency -- scaling only linearly with respect to new\ndata -- by enabling reuse of previous training computation, opening new avenues\nfor improving model performance through rigorous, incremental data assessment\nand mixing.", "published": "2024-10-21 06:03:49", "link": "http://arxiv.org/abs/2410.15661v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Revealing and Mitigating the Local Pattern Shortcuts of Mamba", "abstract": "Large language models (LLMs) have advanced significantly due to the attention\nmechanism, but their quadratic complexity and linear memory demands limit their\nperformance on long-context tasks. Recently, researchers introduced Mamba, an\nadvanced model built upon State Space Models(SSMs) that offers linear\ncomplexity and constant memory. Although Mamba is reported to match or surpass\nthe performance of attention-based models, our analysis reveals a performance\ngap: Mamba excels in tasks that involve localized key information but faces\nchallenges with tasks that require handling distributed key information. Our\ncontrolled experiments suggest that this inconsistency arises from Mamba's\nreliance on local pattern shortcuts, which enable the model to remember local\nkey information within its limited memory but hinder its ability to retain more\ndispersed information. Therefore, we introduce a global selection module into\nthe Mamba model to address this issue. Experiments on both existing and\nproposed synthetic tasks, as well as real-world tasks, demonstrate the\neffectiveness of our method. Notably, with the introduction of only 4M extra\nparameters, our approach enables the Mamba model(130M) to achieve a significant\nimprovement on tasks with distributed information, increasing its performance\nfrom 0 to 80.54 points.", "published": "2024-10-21 06:42:11", "link": "http://arxiv.org/abs/2410.15678v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Tokenization as Finite-State Transduction", "abstract": "Tokenization is the first step in modern neural language model pipelines\nwhere an input text is converted to a sequence of subword tokens. We introduce\nfrom first principles a finite-state transduction framework which can\nefficiently encode all possible tokenizations of a regular language. We then\nconstructively show that Byte-Pair Encoding (BPE) and MaxMatch (WordPiece), two\npopular tokenization schemes, fit within this framework. For BPE, this is\nparticularly surprising given its resemblance to context-free grammar and the\nfact that it does not tokenize strings from left to right.\n  An application of this is to guided generation, where the outputs of a\nlanguage model are constrained to match some pattern. Here, patterns are\nencoded at the character level, which creates a mismatch between the\nconstraints and the model's subword vocabulary. While past work has focused\nonly on constraining outputs without regard to the underlying tokenization\nalgorithm, our framework allows for simultaneously constraining the model\noutputs to match a specified pattern while also adhering to the underlying\ntokenizer's canonical tokenization.", "published": "2024-10-21 07:10:07", "link": "http://arxiv.org/abs/2410.15696v1", "categories": ["cs.CL", "cs.FL"], "primary_category": "cs.CL"}
{"title": "InternLM2.5-StepProver: Advancing Automated Theorem Proving via Expert\n  Iteration on Large-Scale LEAN Problems", "abstract": "Large Language Models (LLMs) have emerged as powerful tools in mathematical\ntheorem proving, particularly when utilizing formal languages such as LEAN. The\nmajor learning paradigm is expert iteration, which necessitates a pre-defined\ndataset comprising numerous mathematical problems. In this process, LLMs\nattempt to prove problems within the dataset and iteratively refine their\ncapabilities through self-training on the proofs they discover. We propose to\nuse large scale LEAN problem datasets Lean-workbook for expert iteration with\nmore than 20,000 CPU days. During expert iteration, we found log-linear trends\nbetween solved problem amount with proof length and CPU usage. We train a\ncritic model to select relatively easy problems for policy models to make\ntrials and guide the model to search for deeper proofs. InternLM2.5-StepProver\nachieves open-source state-of-the-art on MiniF2F, Lean-Workbook-Plus, ProofNet,\nand Putnam benchmarks. Specifically, it achieves a pass of 65.9% on the\nMiniF2F-test and proves (or disproves) 17.0% of problems in Lean-Workbook-Plus\nwhich shows a significant improvement compared to only 9.5% of problems proved\nwhen Lean-Workbook-Plus was released. We open-source our models and searched\nproofs at https://github.com/InternLM/InternLM-Math and\nhttps://huggingface.co/datasets/internlm/Lean-Workbook.", "published": "2024-10-21 07:18:23", "link": "http://arxiv.org/abs/2410.15700v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Natural Language Querying System Through Entity Enrichment", "abstract": "This paper focuses on a domain expert querying system over databases. It\npresents a solution designed for a French enterprise interested in offering a\nnatural language interface for its clients. The approach, based on entity\nenrichment, aims at translating natural language queries into database queries.\nIn this paper, the database is treated through a logical paradigm, suggesting\nthe adaptability of our approach to different database models. The good\nprecision of our method is shown through some preliminary experiments.", "published": "2024-10-21 08:11:47", "link": "http://arxiv.org/abs/2410.15753v1", "categories": ["cs.CL", "cs.DB"], "primary_category": "cs.CL"}
{"title": "Improve Dense Passage Retrieval with Entailment Tuning", "abstract": "Retrieval module can be plugged into many downstream NLP tasks to improve\ntheir performance, such as open-domain question answering and\nretrieval-augmented generation. The key to a retrieval system is to calculate\nrelevance scores to query and passage pairs. However, the definition of\nrelevance is often ambiguous. We observed that a major class of relevance\naligns with the concept of entailment in NLI tasks. Based on this observation,\nwe designed a method called entailment tuning to improve the embedding of dense\nretrievers. Specifically, we unify the form of retrieval data and NLI data\nusing existence claim as a bridge. Then, we train retrievers to predict the\nclaims entailed in a passage with a variant task of masked prediction. Our\nmethod can be efficiently plugged into current dense retrieval methods, and\nexperiments show the effectiveness of our method.", "published": "2024-10-21 09:18:30", "link": "http://arxiv.org/abs/2410.15801v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Mitigating Object Hallucination via Concentric Causal Attention", "abstract": "Recent Large Vision Language Models (LVLMs) present remarkable zero-shot\nconversational and reasoning capabilities given multimodal queries.\nNevertheless, they suffer from object hallucination, a phenomenon where LVLMs\nare prone to generate textual responses not factually aligned with image\ninputs. Our pilot study reveals that object hallucination is closely tied with\nRotary Position Encoding (RoPE), a widely adopted positional dependency\nmodeling design in existing LVLMs. Due to the long-term decay in RoPE, LVLMs\ntend to hallucinate more when relevant visual cues are distant from instruction\ntokens in the multimodal input sequence. Additionally, we observe a similar\neffect when reversing the sequential order of visual tokens during multimodal\nalignment. Our tests indicate that long-term decay in RoPE poses challenges to\nLVLMs while capturing visual-instruction interactions across long distances. We\npropose Concentric Causal Attention (CCA), a simple yet effective positional\nalignment strategy that mitigates the impact of RoPE long-term decay in LVLMs\nby naturally reducing relative distance between visual and instruction tokens.\nWith CCA, visual tokens can better interact with instruction tokens, thereby\nenhancing model's perception capability and alleviating object hallucination.\nWithout bells and whistles, our positional alignment method surpasses existing\nhallucination mitigation strategies by large margins on multiple object\nhallucination benchmarks.", "published": "2024-10-21 11:54:53", "link": "http://arxiv.org/abs/2410.15926v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Do Large Language Models Have an English Accent? Evaluating and\n  Improving the Naturalness of Multilingual LLMs", "abstract": "Current Large Language Models (LLMs) are predominantly designed with English\nas the primary language, and even the few that are multilingual tend to exhibit\nstrong English-centric biases. Much like speakers who might produce awkward\nexpressions when learning a second language, LLMs often generate unnatural\noutputs in non-English languages, reflecting English-centric patterns in both\nvocabulary and grammar. Despite the importance of this issue, the naturalness\nof multilingual LLM outputs has received limited attention. In this paper, we\naddress this gap by introducing novel automatic corpus-level metrics to assess\nthe lexical and syntactic naturalness of LLM outputs in a multilingual context.\nUsing our new metrics, we evaluate state-of-the-art LLMs on a curated benchmark\nin French and Chinese, revealing a tendency towards English-influenced\npatterns. To mitigate this issue, we also propose a simple and effective\nalignment method to improve the naturalness of an LLM in a target language and\ndomain, achieving consistent improvements in naturalness without compromising\nthe performance on general-purpose benchmarks. Our work highlights the\nimportance of developing multilingual metrics, resources and methods for the\nnew wave of multilingual LLMs.", "published": "2024-10-21 12:34:17", "link": "http://arxiv.org/abs/2410.15956v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Systematic Exploration of Dialogue Summarization Approaches for\n  Reproducibility, Comparative Assessment, and Methodological Innovations for\n  Advancing Natural Language Processing in Abstractive Summarization", "abstract": "Reproducibility in scientific research, particularly within the realm of\nnatural language processing (NLP), is essential for validating and verifying\nthe robustness of experimental findings. This paper delves into the\nreproduction and evaluation of dialogue summarization models, focusing\nspecifically on the discrepancies observed between original studies and our\nreproduction efforts. Dialogue summarization is a critical aspect of NLP,\naiming to condense conversational content into concise and informative\nsummaries, thus aiding in efficient information retrieval and decision-making\nprocesses. Our research involved a thorough examination of several dialogue\nsummarization models using the AMI (Augmented Multi-party Interaction) dataset.\nThe models assessed include Hierarchical Memory Networks (HMNet) and various\nversions of Pointer-Generator Networks (PGN), namely PGN(DKE), PGN(DRD),\nPGN(DTS), and PGN(DALL). The primary objective was to evaluate the\ninformativeness and quality of the summaries generated by these models through\nhuman assessment, a method that introduces subjectivity and variability in the\nevaluation process. The analysis began with Dataset 1, where the sample\nstandard deviation of 0.656 indicated a moderate dispersion of data points\naround the mean.", "published": "2024-10-21 12:47:57", "link": "http://arxiv.org/abs/2410.15962v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CA*: Addressing Evaluation Pitfalls in Computation-Aware Latency for\n  Simultaneous Speech Translation", "abstract": "Simultaneous speech translation (SimulST) systems must balance translation\nquality with response time, making latency measurement crucial for evaluating\ntheir real-world performance. However, there has been a longstanding belief\nthat current metrics yield unrealistically high latency measurements in\nunsegmented streaming settings. In this paper, we investigate this phenomenon,\nrevealing its root cause in a fundamental misconception underlying existing\nlatency evaluation approaches. We demonstrate that this issue affects not only\nstreaming but also segment-level latency evaluation across different metrics.\nFurthermore, we propose a modification to correctly measure computation-aware\nlatency for SimulST systems, addressing the limitations present in existing\nmetrics.", "published": "2024-10-21 13:42:19", "link": "http://arxiv.org/abs/2410.16011v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On-Device LLMs for SMEs: Challenges and Opportunities", "abstract": "This paper presents a systematic review of the infrastructure requirements\nfor deploying Large Language Models (LLMs) on-device within the context of\nsmall and medium-sized enterprises (SMEs), focusing on both hardware and\nsoftware perspectives. From the hardware viewpoint, we discuss the utilization\nof processing units like GPUs and TPUs, efficient memory and storage solutions,\nand strategies for effective deployment, addressing the challenges of limited\ncomputational resources typical in SME settings. From the software perspective,\nwe explore framework compatibility, operating system optimization, and the use\nof specialized libraries tailored for resource-constrained environments. The\nreview is structured to first identify the unique challenges faced by SMEs in\ndeploying LLMs on-device, followed by an exploration of the opportunities that\nboth hardware innovations and software adaptations offer to overcome these\nobstacles. Such a structured review provides practical insights, contributing\nsignificantly to the community by enhancing the technological resilience of\nSMEs in integrating LLMs.", "published": "2024-10-21 14:48:35", "link": "http://arxiv.org/abs/2410.16070v2", "categories": ["cs.AI", "cs.CL", "68T07", "I.2"], "primary_category": "cs.AI"}
{"title": "CartesianMoE: Boosting Knowledge Sharing among Experts via Cartesian\n  Product Routing in Mixture-of-Experts", "abstract": "Large language models (LLM) have been attracting much attention from the\ncommunity recently, due to their remarkable performance in all kinds of\ndownstream tasks. According to the well-known scaling law, scaling up a dense\nLLM enhances its capabilities, but also significantly increases the\ncomputational complexity. Mixture-of-Experts (MoE) models address that by\nallowing the model size to grow without substantially raising training or\ninference costs. Yet MoE models face challenges regarding knowledge sharing\namong experts, making their performance somehow sensitive to routing accuracy.\nTo tackle that, previous works introduced shared experts and combined their\noutputs with those of the top $K$ routed experts in an ``addition'' manner. In\nthis paper, inspired by collective matrix factorization to learn shared\nknowledge among data, we propose CartesianMoE, which implements more effective\nknowledge sharing among experts in more like a ``multiplication'' manner.\nExtensive experimental results indicate that CartesianMoE outperforms previous\nMoE models for building LLMs, in terms of both perplexity and downstream task\nperformance. And we also find that CartesianMoE achieves better expert routing\nrobustness.", "published": "2024-10-21 14:55:59", "link": "http://arxiv.org/abs/2410.16077v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Fine-Tuning LLMs for Reliable Medical Question-Answering Services", "abstract": "We present an advanced approach to medical question-answering (QA) services,\nusing fine-tuned Large Language Models (LLMs) to improve the accuracy and\nreliability of healthcare information. Our study focuses on optimizing models\nlike LLaMA-2 and Mistral, which have shown great promise in delivering precise,\nreliable medical answers. By leveraging comprehensive datasets, we applied\nfine-tuning techniques such as rsDoRA+ and ReRAG. rsDoRA+ enhances model\nperformance through a combination of decomposed model weights, varied learning\nrates for low-rank matrices, and rank stabilization, leading to improved\nefficiency. ReRAG, which integrates retrieval on demand and question rewriting,\nfurther refines the accuracy of the responses. This approach enables healthcare\nproviders to access fast, dependable information, aiding in more efficient\ndecision-making and fostering greater patient trust. Our work highlights the\npotential of fine-tuned LLMs to significantly improve the quality and\naccessibility of medical information services, ultimately contributing to\nbetter healthcare outcomes for all.", "published": "2024-10-21 15:12:20", "link": "http://arxiv.org/abs/2410.16088v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Pangea: A Fully Open Multilingual Multimodal LLM for 39 Languages", "abstract": "Despite recent advances in multimodal large language models (MLLMs), their\ndevelopment has predominantly focused on English- and western-centric datasets\nand tasks, leaving most of the world's languages and diverse cultural contexts\nunderrepresented. This paper introduces Pangea, a multilingual multimodal LLM\ntrained on PangeaIns, a diverse 6M instruction dataset spanning 39 languages.\nPangeaIns features: 1) high-quality English instructions, 2) carefully\nmachine-translated instructions, and 3) culturally relevant multimodal tasks to\nensure cross-cultural coverage. To rigorously assess models' capabilities, we\nintroduce PangeaBench, a holistic evaluation suite encompassing 14 datasets\ncovering 47 languages. Results show that Pangea significantly outperforms\nexisting open-source models in multilingual settings and diverse cultural\ncontexts. Ablation studies further reveal the importance of English data\nproportions, language popularity, and the number of multimodal training samples\non overall performance. We fully open-source our data, code, and trained\ncheckpoints, to facilitate the development of inclusive and robust multilingual\nMLLMs, promoting equity and accessibility across a broader linguistic and\ncultural spectrum.", "published": "2024-10-21 16:19:41", "link": "http://arxiv.org/abs/2410.16153v3", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Limpeh ga li gong: Challenges in Singlish Annotations", "abstract": "Singlish, or Colloquial Singapore English, is a language formed from oral and\nsocial communication within multicultural Singapore. In this work, we work on a\nfundamental Natural Language Processing (NLP) task: Parts-Of-Speech (POS)\ntagging of Singlish sentences. For our analysis, we build a parallel Singlish\ndataset containing direct English translations and POS tags, with translation\nand POS annotation done by native Singlish speakers. Our experiments show that\nautomatic transition- and transformer- based taggers perform with only $\\sim\n80\\%$ accuracy when evaluated against human-annotated POS labels, suggesting\nthat there is indeed room for improvement on computation analysis of the\nlanguage. We provide an exposition of challenges in Singlish annotation: its\ninconsistencies in form and semantics, the highly context-dependent particles\nof the language, its structural unique expressions, and the variation of the\nlanguage on different mediums. Our task definition, resultant labels and\nresults reflects the challenges in analysing colloquial languages formulated\nfrom a variety of dialects, and paves the way for future studies beyond POS\ntagging.", "published": "2024-10-21 16:21:45", "link": "http://arxiv.org/abs/2410.16156v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Sparkle: Mastering Basic Spatial Capabilities in Vision Language Models\n  Elicits Generalization to Spatial Reasoning", "abstract": "Vision language models (VLMs) have demonstrated impressive performance across\na wide range of downstream tasks. However, their proficiency in spatial\nreasoning remains limited, despite its crucial role in tasks involving\nnavigation and interaction with physical environments. Specifically, most of\nthese tasks rely on the core spatial reasoning capabilities in two-dimensional\n(2D) environments, and our evaluation reveals that state-of-the-art VLMs\nfrequently generate implausible and incorrect responses to composite spatial\nreasoning problems, including simple pathfinding tasks that humans can solve\neffortlessly at a glance. To address this, we explore an effective approach to\nenhance 2D spatial reasoning within VLMs by training the model solely on basic\nspatial capabilities. We begin by disentangling the key components of 2D\nspatial reasoning: direction comprehension, distance estimation, and\nlocalization. Our central hypothesis is that mastering these basic spatial\ncapabilities can significantly enhance a model's performance on composite\nspatial tasks requiring advanced spatial understanding and combinatorial\nproblem-solving, with generalized improvements in real-world visual-spatial\ntasks. To investigate this hypothesis, we introduce Sparkle: a framework that\nuses synthetic data generation to provide targeted supervision for vision\nlanguage models (VLMs) in three basic spatial capabilities, creating an\ninstruction dataset for each capability. Our experiments demonstrate that VLMs\nfine-tuned with Sparkle achieve significant performance gains, not only in the\nbasic tasks themselves but also in generalizing to composite and\nout-of-distribution real-world spatial reasoning tasks. These findings offer\ninsights into systematic strategies for improving VLMs' spatial reasoning\ncapabilities.", "published": "2024-10-21 16:26:09", "link": "http://arxiv.org/abs/2410.16162v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "GenAI Assisting Medical Training", "abstract": "Medical procedures such as venipuncture and cannulation are essential for\nnurses and require precise skills. Learning this skill, in turn, is a challenge\nfor educators due to the number of teachers per class and the complexity of the\ntask. The study aims to help students with skill acquisition and alleviate the\neducator's workload by integrating generative AI methods to provide real-time\nfeedback on medical procedures such as venipuncture and cannulation.", "published": "2024-10-21 16:31:16", "link": "http://arxiv.org/abs/2410.16164v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "From Tokens to Materials: Leveraging Language Models for Scientific\n  Discovery", "abstract": "Exploring the predictive capabilities of language models in material science\nis an ongoing interest. This study investigates the application of language\nmodel embeddings to enhance material property prediction in materials science.\nBy evaluating various contextual embedding methods and pre-trained models,\nincluding Bidirectional Encoder Representations from Transformers (BERT) and\nGenerative Pre-trained Transformers (GPT), we demonstrate that domain-specific\nmodels, particularly MatBERT significantly outperform general-purpose models in\nextracting implicit knowledge from compound names and material properties. Our\nfindings reveal that information-dense embeddings from the third layer of\nMatBERT, combined with a context-averaging approach, offer the most effective\nmethod for capturing material-property relationships from the scientific\nliterature. We also identify a crucial \"tokenizer effect,\" highlighting the\nimportance of specialized text processing techniques that preserve complete\ncompound names while maintaining consistent token counts. These insights\nunderscore the value of domain-specific training and tokenization in materials\nscience applications and offer a promising pathway for accelerating the\ndiscovery and development of new materials through AI-driven approaches.", "published": "2024-10-21 16:31:23", "link": "http://arxiv.org/abs/2410.16165v2", "categories": ["cs.CL", "cs.DB"], "primary_category": "cs.CL"}
{"title": "Beyond Filtering: Adaptive Image-Text Quality Enhancement for MLLM\n  Pretraining", "abstract": "Multimodal large language models (MLLMs) have made significant strides by\nintegrating visual and textual modalities. A critical factor in training MLLMs\nis the quality of image-text pairs within multimodal pretraining datasets.\nHowever, $\\textit {de facto}$ filter-based data quality enhancement paradigms\noften discard a substantial portion of high-quality image data due to\ninadequate semantic alignment between images and texts, leading to\ninefficiencies in data utilization and scalability. In this paper, we propose\nthe Adaptive Image-Text Quality Enhancer (AITQE), a model that dynamically\nassesses and enhances the quality of image-text pairs. AITQE employs a text\nrewriting mechanism for low-quality pairs and incorporates a negative sample\nlearning strategy to improve evaluative capabilities by integrating\ndeliberately selected low-quality samples during training. Unlike prior\napproaches that significantly alter text distributions, our method minimally\nadjusts text to preserve data volume while enhancing quality. Experimental\nresults demonstrate that AITQE surpasses existing methods on various benchmark,\neffectively leveraging raw data and scaling efficiently with increasing data\nvolumes. We hope our work will inspire future works. The code and model are\navailable at: https://github.com/hanhuang22/AITQE.", "published": "2024-10-21 16:32:41", "link": "http://arxiv.org/abs/2410.16166v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "MagicPIG: LSH Sampling for Efficient LLM Generation", "abstract": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by up to $5\\times$ across various GPU hardware and achieve 54ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\nhttps://github.com/Infini-AI-Lab/MagicPIG.", "published": "2024-10-21 16:44:51", "link": "http://arxiv.org/abs/2410.16179v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Information for Conversation Generation: Proposals Utilising Knowledge\n  Graphs", "abstract": "LLMs are frequently used tools for conversational generation. Without\nadditional information LLMs can generate lower quality responses due to lacking\nrelevant content and hallucinations, as well as the perception of poor\nemotional capability, and an inability to maintain a consistent character.\nKnowledge graphs are commonly used forms of external knowledge and may provide\nsolutions to these challenges. This paper introduces three proposals, utilizing\nknowledge graphs to enhance LLM generation. Firstly, dynamic knowledge graph\nembeddings and recommendation could allow for the integration of new\ninformation and the selection of relevant knowledge for response generation.\nSecondly, storing entities with emotional values as additional features may\nprovide knowledge that is better emotionally aligned with the user input.\nThirdly, integrating character information through narrative bubbles would\nmaintain character consistency, as well as introducing a structure that would\nreadily incorporate new information.", "published": "2024-10-21 16:59:25", "link": "http://arxiv.org/abs/2410.16196v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Machine Learning Approaches for Mental Illness Detection on Social\n  Media: A Systematic Review of Biases and Methodological Challenges", "abstract": "The global increase in mental illness requires innovative detection methods\nfor early intervention. Social media provides a valuable platform to identify\nmental illness through user-generated content. This systematic review examines\nmachine learning (ML) models for detecting mental illness, with a particular\nfocus on depression, using social media data. It highlights biases and\nmethodological challenges encountered throughout the ML lifecycle. A search of\nPubMed, IEEE Xplore, and Google Scholar identified 47 relevant studies\npublished after 2010. The Prediction model Risk Of Bias ASsessment Tool\n(PROBAST) was utilized to assess methodological quality and risk of bias.\n  The review reveals significant biases affecting model reliability and\ngeneralizability. A predominant reliance on Twitter (63.8%) and\nEnglish-language content (over 90%) limits diversity, with most studies focused\non users from the United States and Europe. Non-probability sampling (80%)\nlimits representativeness. Only 23% explicitly addressed linguistic nuances\nlike negations, crucial for accurate sentiment analysis. Inconsistent\nhyperparameter tuning (27.7%) and inadequate data partitioning (17%) risk\noverfitting. While 74.5% used appropriate evaluation metrics for imbalanced\ndata, others relied on accuracy without addressing class imbalance, potentially\nskewing results. Reporting transparency varied, often lacking critical\nmethodological details.\n  These findings highlight the need to diversify data sources, standardize\npreprocessing, ensure consistent model development, address class imbalance,\nand enhance reporting transparency. By overcoming these challenges, future\nresearch can develop more robust and generalizable ML models for depression\ndetection on social media, contributing to improved mental health outcomes\nglobally.", "published": "2024-10-21 17:05:50", "link": "http://arxiv.org/abs/2410.16204v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Pre-training Distillation for Large Language Models: A Design Space\n  Exploration", "abstract": "Knowledge distillation (KD) aims to transfer knowledge from a large teacher\nmodel to a smaller student model. Previous work applying KD in the field of\nlarge language models (LLMs) typically focused on the post-training phase,\nwhere the student LLM learns directly from instructions and corresponding\nresponses generated by the teacher model. In this paper, we extend KD to the\npre-training phase of LLMs, named pre-training distillation (PD). We first\nconduct a preliminary experiment using GLM-4-9B as the teacher LLM to distill a\n1.9B parameter student LLM, validating the effectiveness of PD. Considering the\nkey impact factors of distillation, we systematically explore the design space\nof pre-training distillation across four aspects: logits processing, loss\nselection, scaling law, and offline or online logits. We conduct extensive\nexperiments to explore the design space of pre-training distillation and find\nbetter configurations and interesting conclusions, such as larger student LLMs\ngenerally benefiting more from pre-training distillation, while a larger\nteacher LLM does not necessarily guarantee better results. We hope our\nexploration of the design space will inform future practices in pre-training\ndistillation.", "published": "2024-10-21 17:16:13", "link": "http://arxiv.org/abs/2410.16215v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Sketch2Code: Evaluating Vision-Language Models for Interactive Web\n  Design Prototyping", "abstract": "Sketches are a natural and accessible medium for UI designers to\nconceptualize early-stage ideas. However, existing research on UI/UX automation\noften requires high-fidelity inputs like Figma designs or detailed screenshots,\nlimiting accessibility and impeding efficient design iteration. To bridge this\ngap, we introduce Sketch2Code, a benchmark that evaluates state-of-the-art\nVision Language Models (VLMs) on automating the conversion of rudimentary\nsketches into webpage prototypes. Beyond end-to-end benchmarking, Sketch2Code\nsupports interactive agent evaluation that mimics real-world design workflows,\nwhere a VLM-based agent iteratively refines its generations by communicating\nwith a simulated user, either passively receiving feedback instructions or\nproactively asking clarification questions. We comprehensively analyze ten\ncommercial and open-source models, showing that Sketch2Code is challenging for\nexisting VLMs; even the most capable models struggle to accurately interpret\nsketches and formulate effective questions that lead to steady improvement.\nNevertheless, a user study with UI/UX experts reveals a significant preference\nfor proactive question-asking over passive feedback reception, highlighting the\nneed to develop more effective paradigms for multi-turn conversational agents.", "published": "2024-10-21 17:39:49", "link": "http://arxiv.org/abs/2410.16232v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CompassJudger-1: All-in-one Judge Model Helps Model Evaluation and\n  Evolution", "abstract": "Efficient and accurate evaluation is crucial for the continuous improvement\nof large language models (LLMs). Among various assessment methods, subjective\nevaluation has garnered significant attention due to its superior alignment\nwith real-world usage scenarios and human preferences. However, human-based\nevaluations are costly and lack reproducibility, making precise automated\nevaluators (judgers) vital in this process. In this report, we introduce\n\\textbf{CompassJudger-1}, the first open-source \\textbf{all-in-one} judge LLM.\nCompassJudger-1 is a general-purpose LLM that demonstrates remarkable\nversatility. It is capable of: 1. Performing unitary scoring and two-model\ncomparisons as a reward model; 2. Conducting evaluations according to specified\nformats; 3. Generating critiques; 4. Executing diverse tasks like a general\nLLM. To assess the evaluation capabilities of different judge models under a\nunified setting, we have also established \\textbf{JudgerBench}, a new benchmark\nthat encompasses various subjective evaluation tasks and covers a wide range of\ntopics. CompassJudger-1 offers a comprehensive solution for various evaluation\ntasks while maintaining the flexibility to adapt to diverse requirements. Both\nCompassJudger and JudgerBench are released and available to the research\ncommunity athttps://github.com/open-compass/CompassJudger. We believe that by\nopen-sourcing these tools, we can foster collaboration and accelerate progress\nin LLM evaluation methodologies.", "published": "2024-10-21 17:56:51", "link": "http://arxiv.org/abs/2410.16256v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLM-based Optimization of Compound AI Systems: A Survey", "abstract": "In a compound AI system, components such as an LLM call, a retriever, a code\ninterpreter, or tools are interconnected. The system's behavior is primarily\ndriven by parameters such as instructions or tool definitions. Recent\nadvancements enable end-to-end optimization of these parameters using an LLM.\nNotably, leveraging an LLM as an optimizer is particularly efficient because it\navoids gradient computation and can generate complex code and instructions.\nThis paper presents a survey of the principles and emerging trends in LLM-based\noptimization of compound AI systems. It covers archetypes of compound AI\nsystems, approaches to LLM-based end-to-end optimization, and insights into\nfuture directions and broader impacts. Importantly, this survey uses concepts\nfrom program analysis to provide a unified view of how an LLM optimizer is\nprompted to optimize a compound AI system. The exhaustive list of paper is\nprovided at\nhttps://github.com/linyuhongg/LLM-based-Optimization-of-Compound-AI-Systems.", "published": "2024-10-21 18:06:25", "link": "http://arxiv.org/abs/2410.16392v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Neuron-level Interpretability with White-box Language Models", "abstract": "Neurons in auto-regressive language models like GPT-2 can be interpreted by\nanalyzing their activation patterns. Recent studies have shown that techniques\nsuch as dictionary learning, a form of post-hoc sparse coding, enhance this\nneuron-level interpretability. In our research, we are driven by the goal to\nfundamentally improve neural network interpretability by embedding sparse\ncoding directly within the model architecture, rather than applying it as an\nafterthought. In our study, we introduce a white-box transformer-like\narchitecture named Coding RAte TransformEr (CRATE), explicitly engineered to\ncapture sparse, low-dimensional structures within data distributions. Our\ncomprehensive experiments showcase significant improvements (up to 103%\nrelative improvement) in neuron-level interpretability across a variety of\nevaluation metrics. Detailed investigations confirm that this enhanced\ninterpretability is steady across different layers irrespective of the model\nsize, underlining CRATE's robust performance in enhancing neural network\ninterpretability. Further analysis shows that CRATE's increased\ninterpretability comes from its enhanced ability to consistently and\ndistinctively activate on relevant tokens. These findings point towards a\npromising direction for creating white-box foundation models that excel in\nneuron-level interpretation.", "published": "2024-10-21 19:12:33", "link": "http://arxiv.org/abs/2410.16443v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Catastrophic Failure of LLM Unlearning via Quantization", "abstract": "Large language models (LLMs) have shown remarkable proficiency in generating\ntext, benefiting from extensive training on vast textual corpora. However, LLMs\nmay also acquire unwanted behaviors from the diverse and sensitive nature of\ntheir training data, which can include copyrighted and private content. Machine\nunlearning has been introduced as a viable solution to remove the influence of\nsuch problematic content without the need for costly and time-consuming\nretraining. This process aims to erase specific knowledge from LLMs while\npreserving as much model utility as possible. Despite the effectiveness of\ncurrent unlearning methods, little attention has been given to whether existing\nunlearning methods for LLMs truly achieve forgetting or merely hide the\nknowledge, which current unlearning benchmarks fail to detect. This paper\nreveals that applying quantization to models that have undergone unlearning can\nrestore the \"forgotten\" information. To thoroughly evaluate this phenomenon, we\nconduct comprehensive experiments using various quantization techniques across\nmultiple precision levels. We find that for unlearning methods with utility\nconstraints, the unlearned model retains an average of 21\\% of the intended\nforgotten knowledge in full precision, which significantly increases to 83\\%\nafter 4-bit quantization. ... Our code is available at:\n\\href{https://github.com/zzwjames/FailureLLMUnlearning}{https://github.com/zzwjames/FailureLLMUnlearning}.", "published": "2024-10-21 19:28:37", "link": "http://arxiv.org/abs/2410.16454v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Beyond Browsing: API-Based Web Agents", "abstract": "Web browsers are a portal to the internet, where much of human activity is\nundertaken. Thus, there has been significant research work in AI agents that\ninteract with the internet through web browsing. However, there is also another\ninterface designed specifically for machine interaction with online content:\napplication programming interfaces (APIs). In this paper we ask -- what if we\nwere to take tasks traditionally tackled by browsing agents, and give AI agents\naccess to APIs? To do so, we propose two varieties of agents: (1) an\nAPI-calling agent that attempts to perform online tasks through APIs only,\nsimilar to traditional coding agents, and (2) a Hybrid Agent that can interact\nwith online data through both web browsing and APIs. In experiments on\nWebArena, a widely-used and realistic benchmark for web navigation tasks, we\nfind that API-based agents outperform web browsing agents. Hybrid Agents\nout-perform both others nearly uniformly across tasks, resulting in a more than\n20.0% absolute improvement over web browsing alone, achieving a success rate of\n35.8%, achiving the SOTA performance among task-agnostic agents. These results\nstrongly suggest that when APIs are available, they present an attractive\nalternative to relying on web browsing alone.", "published": "2024-10-21 19:46:06", "link": "http://arxiv.org/abs/2410.16464v2", "categories": ["cs.CL", "cs.MA"], "primary_category": "cs.CL"}
{"title": "Learning from others' mistakes: Finetuning machine translation models\n  with span-level error annotations", "abstract": "Despite growing interest in incorporating feedback to improve language\nmodels, most efforts focus only on sequence-level annotations. In this work, we\nexplore the potential of utilizing fine-grained span-level annotations from\noffline datasets to improve model quality. We develop a simple finetuning\nalgorithm, called Training with Annotations (TWA), to directly train machine\ntranslation models on such annotated data. TWA utilizes targeted span-level\nerror information while also flexibly learning what to penalize within a span.\nMoreover, TWA considers the overall trajectory of a sequence when deciding\nwhich non-error spans to utilize as positive signals. Experiments on\nEnglish-German and Chinese-English machine translation show that TWA\noutperforms baselines such as Supervised FineTuning on sequences filtered for\nquality and Direct Preference Optimization on pairs constructed from the same\ndata.", "published": "2024-10-21 21:00:47", "link": "http://arxiv.org/abs/2410.16509v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AUTALIC: A Dataset for Anti-AUTistic Ableist Language In Context", "abstract": "As our understanding of autism and ableism continues to increase, so does our\nunderstanding of ableist language towards autistic people. Such language poses\na significant challenge in NLP research due to its subtle and context-dependent\nnature. Yet, detecting anti-autistic ableist language remains underexplored,\nwith existing NLP tools often failing to capture its nuanced expressions. We\npresent AUTALIC, the first benchmark dataset dedicated to the detection of\nanti-autistic ableist language in context, addressing a significant gap in the\nfield. The dataset comprises 2,400 autism-related sentences collected from\nReddit, accompanied by surrounding context, and is annotated by trained experts\nwith backgrounds in neurodiversity. Our comprehensive evaluation reveals that\ncurrent language models, including state-of-the-art LLMs, struggle to reliably\nidentify anti-autistic ableism and align with human judgments, underscoring\ntheir limitations in this domain. We publicly release AUTALIC along with the\nindividual annotations which serve as a valuable resource to researchers\nworking on ableism, neurodiversity, and also studying disagreements in\nannotation tasks. This dataset serves as a crucial step towards developing more\ninclusive and context-aware NLP systems that better reflect diverse\nperspectives.", "published": "2024-10-21 21:21:29", "link": "http://arxiv.org/abs/2410.16520v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Pruning Foundation Models for High Accuracy without Retraining", "abstract": "Despite the superior performance, it is challenging to deploy foundation\nmodels or large language models (LLMs) due to their massive parameters and\ncomputations. While pruning is a promising technique to reduce model size and\naccelerate the inference, the traditional pruning techniques can hardly be\napplied for LLMs as they need to finetune the model on the full dataset with\nmultiple epochs consuming massive data and hardware resources. To deal with\nthis problem, post-training pruning methods are proposed to prune LLMs in\none-shot without retraining. However, their accuracy after pruning may suffer\nfrom certain performance degradation due to the lack of retraining with massive\ndata. To address this issue, in this paper, we first formulate the\npost-training problem for layer-wise LLM compression to simultaneously prune\nmultiple weights in LLMs. Next, we provide an optimal solution for this problem\nand design our post-training pruning algorithm for both unstructured and\nsemi-structured sparsity. Our extensive experiments demonstrate the superior\nperformance of the proposed methods in comparison to SOTA baselines across\nvarious LLM families including transformer-based LLMs and Mamba-based LLMs.\nCode link: https://github.com/piuzha/APT", "published": "2024-10-21 01:23:34", "link": "http://arxiv.org/abs/2410.15567v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Stacking Small Language Models for Generalizability", "abstract": "Recent advances show that large language models (LLMs) generalize strong\nperformance across different natural language benchmarks. However, the large\nsize of LLMs makes training and inference expensive and impractical to run in\nresource-limited settings. This paper introduces a new approach called\nfine-tuning stacks of language models (FSLM), which involves stacking small\nlanguage models (SLM) as an alternative to LLMs. By fine-tuning each SLM to\nperform a specific task, this approach breaks down high level reasoning into\nmultiple lower-level steps that specific SLMs are responsible for. As a result,\nFSLM allows for lower training and inference costs, and also improves model\ninterpretability as each SLM communicates with the subsequent one through\nnatural language. By evaluating FSLM on common natural language benchmarks,\nthis paper highlights promising early results toward generalizable performance\nusing FSLM as a cost-effective alternative to LLMs.", "published": "2024-10-21 01:27:29", "link": "http://arxiv.org/abs/2410.15570v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CPE-Pro: A Structure-Sensitive Deep Learning Method for Protein\n  Representation and Origin Evaluation", "abstract": "Protein structures are important for understanding their functions and\ninteractions. Currently, many protein structure prediction methods are\nenriching the structure database. Discriminating the origin of structures is\ncrucial for distinguishing between experimentally resolved and computationally\npredicted structures, evaluating the reliability of prediction methods, and\nguiding downstream biological studies. Building on works in structure\nprediction, We developed a structure-sensitive supervised deep learning model,\nCrystal vs Predicted Evaluator for Protein Structure (CPE-Pro), to represent\nand discriminate the origin of protein structures. CPE-Pro learns the\nstructural information of proteins and captures inter-structural differences to\nachieve accurate traceability on four data classes, and is expected to be\nextended to more. Simultaneously, we utilized Foldseek to encode protein\nstructures into \"structure-sequences\" and trained a protein Structural Sequence\nLanguage Model, SSLM. Preliminary experiments demonstrated that, compared to\nlarge-scale protein language models pre-trained on vast amounts of amino acid\nsequences, the \"structure-sequence\" enables the language model to learn more\ninformative protein features, enhancing and optimizing structural\nrepresentations. We have provided the code, model weights, and all related\nmaterials on https://github.com/GouWenrui/CPE-Pro-main.git.", "published": "2024-10-21 02:21:56", "link": "http://arxiv.org/abs/2410.15592v2", "categories": ["q-bio.BM", "cs.CL", "cs.LG", "q-bio.QM"], "primary_category": "q-bio.BM"}
{"title": "A Comprehensive Survey of Direct Preference Optimization: Datasets,\n  Theories, Variants, and Applications", "abstract": "With the rapid advancement of large language models (LLMs), aligning policy\nmodels with human preferences has become increasingly critical. Direct\nPreference Optimization (DPO) has emerged as a promising approach for\nalignment, acting as an RL-free alternative to Reinforcement Learning from\nHuman Feedback (RLHF). Despite DPO's various advancements and inherent\nlimitations, an in-depth review of these aspects is currently lacking in the\nliterature. In this work, we present a comprehensive review of the challenges\nand opportunities in DPO, covering theoretical analyses, variants, relevant\npreference datasets, and applications. Specifically, we categorize recent\nstudies on DPO based on key research questions to provide a thorough\nunderstanding of DPO's current landscape. Additionally, we propose several\nfuture research directions to offer insights on model alignment for the\nresearch community.", "published": "2024-10-21 02:27:24", "link": "http://arxiv.org/abs/2410.15595v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Moonshine: Speech Recognition for Live Transcription and Voice Commands", "abstract": "This paper introduces Moonshine, a family of speech recognition models\noptimized for live transcription and voice command processing. Moonshine is\nbased on an encoder-decoder transformer architecture and employs Rotary\nPosition Embedding (RoPE) instead of traditional absolute position embeddings.\nThe model is trained on speech segments of various lengths, but without using\nzero-padding, leading to greater efficiency for the encoder during inference\ntime. When benchmarked against OpenAI's Whisper tiny-en, Moonshine Tiny\ndemonstrates a 5x reduction in compute requirements for transcribing a\n10-second speech segment while incurring no increase in word error rates across\nstandard evaluation datasets. These results highlight Moonshine's potential for\nreal-time and resource-constrained applications.", "published": "2024-10-21 03:13:20", "link": "http://arxiv.org/abs/2410.15608v2", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Interventional Speech Noise Injection for ASR Generalizable Spoken\n  Language Understanding", "abstract": "Recently, pre-trained language models (PLMs) have been increasingly adopted\nin spoken language understanding (SLU). However, automatic speech recognition\n(ASR) systems frequently produce inaccurate transcriptions, leading to noisy\ninputs for SLU models, which can significantly degrade their performance. To\naddress this, our objective is to train SLU models to withstand ASR errors by\nexposing them to noises commonly observed in ASR systems, referred to as\nASR-plausible noises. Speech noise injection (SNI) methods have pursued this\nobjective by introducing ASR-plausible noises, but we argue that these methods\nare inherently biased towards specific ASR systems, or ASR-specific noises. In\nthis work, we propose a novel and less biased augmentation method of\nintroducing the noises that are plausible to any ASR system, by cutting off the\nnon-causal effect of noises. Experimental results and analyses demonstrate the\neffectiveness of our proposed methods in enhancing the robustness and\ngeneralizability of SLU models against unseen ASR systems by introducing more\ndiverse and plausible ASR noises in advance.", "published": "2024-10-21 03:13:22", "link": "http://arxiv.org/abs/2410.15609v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Acoustic Model Optimization over Multiple Data Sources: Merging and\n  Valuation", "abstract": "Due to the rising awareness of privacy protection and the voluminous scale of\nspeech data, it is becoming infeasible for Automatic Speech Recognition (ASR)\nsystem developers to train the acoustic model with complete data as before. For\nexample, the data may be owned by different curators, and it is not allowed to\nshare with others. In this paper, we propose a novel paradigm to solve salient\nproblems plaguing the ASR field. In the first stage, multiple acoustic models\nare trained based upon different subsets of the complete speech data, while in\nthe second phase, two novel algorithms are utilized to generate a high-quality\nacoustic model based upon those trained on data subsets. We first propose the\nGenetic Merge Algorithm (GMA), which is a highly specialized algorithm for\noptimizing acoustic models but suffers from low efficiency. We further propose\nthe SGD-Based Optimizational Merge Algorithm (SOMA), which effectively\nalleviates the efficiency bottleneck of GMA and maintains superior model\naccuracy. Extensive experiments on public data show that the proposed methods\ncan significantly outperform the state-of-the-art. Furthermore, we introduce\nShapley Value to estimate the contribution score of the trained models, which\nis useful for evaluating the effectiveness of the data and providing fair\nincentives to their curators.", "published": "2024-10-21 03:48:23", "link": "http://arxiv.org/abs/2410.15620v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improving Parallel Program Performance with LLM Optimizers via\n  Agent-System Interface", "abstract": "Modern scientific discovery increasingly relies on high-performance computing\nfor complex modeling and simulation. A key challenge in improving parallel\nprogram performance is efficiently mapping tasks to processors and data to\nmemory, a process dictated by intricate, low-level system code known as\nmappers. Developing high-performance mappers demands days of manual tuning,\nposing a significant barrier for domain scientists without systems expertise.\nWe introduce a framework that automates mapper development with generative\noptimization, leveraging richer feedback beyond scalar performance metrics. Our\napproach features the Agent-System Interface, which includes a Domain-Specific\nLanguage (DSL) to abstract away low-level complexity of system code and define\na structured search space, as well as AutoGuide, a mechanism that interprets\nraw execution output into actionable feedback. Unlike traditional reinforcement\nlearning methods such as OpenTuner, which rely solely on scalar feedback, our\nmethod finds superior mappers in far fewer iterations. With just 10 iterations,\nit outperforms OpenTuner even after 1000 iterations, achieving 3.8X faster\nperformance. Our approach finds mappers that surpass expert-written mappers by\nup to 1.34X speedup across nine benchmarks while reducing tuning time from days\nto minutes.", "published": "2024-10-21 04:08:37", "link": "http://arxiv.org/abs/2410.15625v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DC"], "primary_category": "cs.LG"}
{"title": "Resource-Efficient Medical Report Generation using Large Language Models", "abstract": "Medical report generation is the task of automatically writing radiology\nreports for chest X-ray images. Manually composing these reports is a\ntime-consuming process that is also prone to human errors. Generating medical\nreports can therefore help reduce the burden on radiologists. In other words,\nwe can promote greater clinical automation in the medical domain. In this work,\nwe propose a new framework leveraging vision-enabled Large Language Models\n(LLM) for the task of medical report generation. We introduce a lightweight\nsolution that achieves better or comparative performance as compared to\nprevious solutions on the task of medical report generation. We conduct\nextensive experiments exploring different model sizes and enhancement\napproaches, such as prefix tuning to improve the text generation abilities of\nthe LLMs. We evaluate our approach on a prominent large-scale radiology report\ndataset - MIMIC-CXR. Our results demonstrate the capability of our\nresource-efficient framework to generate patient-specific reports with strong\nmedical contextual understanding and high precision.", "published": "2024-10-21 05:08:18", "link": "http://arxiv.org/abs/2410.15642v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "RAC: Efficient LLM Factuality Correction with Retrieval Augmentation", "abstract": "Large Language Models (LLMs) exhibit impressive results across a wide range\nof natural language processing (NLP) tasks, yet they can often produce\nfactually incorrect outputs. This paper introduces a simple but effective\nlow-latency post-correction method, \\textbf{Retrieval Augmented Correction\n(RAC)}, aimed at enhancing the factual performance of LLMs without requiring\nadditional fine-tuning. Our method is general and can be used with any\ninstruction-tuned LLM, and has greatly reduced latency compared to prior\napproaches. RAC decomposes the LLM's output into atomic facts and applies a\nfine-grained verification and correction process with retrieved content to\nverify and correct the LLM-generated output. Our extensive experiments show\nthat RAC yields up to 30\\% improvements over state-of-the-art baselines across\ntwo popular factuality evaluation datasets, validating its efficacy and\nrobustness in both with and without the integration of Retrieval-Augmented\nGeneration (RAG) across different LLMs.\\footnote{Our code is at\n\\url{https://github.com/jlab-nlp/Retrieval-Augmented-Correction}}", "published": "2024-10-21 06:11:38", "link": "http://arxiv.org/abs/2410.15667v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Learning to Generate and Evaluate Fact-checking Explanations with\n  Transformers", "abstract": "In an era increasingly dominated by digital platforms, the spread of\nmisinformation poses a significant challenge, highlighting the need for\nsolutions capable of assessing information veracity. Our research contributes\nto the field of Explainable Artificial Antelligence (XAI) by developing\ntransformer-based fact-checking models that contextualise and justify their\ndecisions by generating human-accessible explanations. Importantly, we also\ndevelop models for automatic evaluation of explanations for fact-checking\nverdicts across different dimensions such as \\texttt{(self)-contradiction},\n\\texttt{hallucination}, \\texttt{convincingness} and \\texttt{overall quality}.\nBy introducing human-centred evaluation methods and developing specialised\ndatasets, we emphasise the need for aligning Artificial Intelligence\n(AI)-generated explanations with human judgements. This approach not only\nadvances theoretical knowledge in XAI but also holds practical implications by\nenhancing the transparency, reliability and users' trust in AI-driven\nfact-checking systems. Furthermore, the development of our metric learning\nmodels is a first step towards potentially increasing efficiency and reducing\nreliance on extensive manual assessment. Based on experimental results, our\nbest performing generative model \\textsc{ROUGE-1} score of 47.77, demonstrating\nsuperior performance in generating fact-checking explanations, particularly\nwhen provided with high-quality evidence. Additionally, the best performing\nmetric learning model showed a moderately strong correlation with human\njudgements on objective dimensions such as \\texttt{(self)-contradiction and\n\\texttt{hallucination}, achieving a Matthews Correlation Coefficient (MCC) of\naround 0.7.}", "published": "2024-10-21 06:22:51", "link": "http://arxiv.org/abs/2410.15669v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Reducing annotator bias by belief elicitation", "abstract": "Crowdsourced annotations of data play a substantial role in the development\nof Artificial Intelligence (AI). It is broadly recognised that annotations of\ntext data can contain annotator bias, where systematic disagreement in\nannotations can be traced back to differences in the annotators' backgrounds.\nBeing unaware of such annotator bias can lead to representational bias against\nminority group perspectives and therefore several methods have been proposed\nfor recognising bias or preserving perspectives. These methods typically\nrequire either a substantial number of annotators or annotations per data\ninstance. In this study, we propose a simple method for handling bias in\nannotations without requirements on the number of annotators or instances.\nInstead, we ask annotators about their beliefs of other annotators' judgements\nof an instance, under the hypothesis that these beliefs may provide more\nrepresentative and less biased labels than judgements. The method was examined\nin two controlled, survey-based experiments involving Democrats and Republicans\n(n=1,590) asked to judge statements as arguments and then report beliefs about\nothers' judgements. The results indicate that bias, defined as systematic\ndifferences between the two groups of annotators, is consistently reduced when\nasking for beliefs instead of judgements. Our proposed method therefore has the\npotential to reduce the risk of annotator bias, thereby improving the\ngeneralisability of AI systems and preventing harm to unrepresented\nsocio-demographic groups, and we highlight the need for further studies of this\npotential in other tasks and downstream applications.", "published": "2024-10-21 07:44:01", "link": "http://arxiv.org/abs/2410.15726v1", "categories": ["cs.CL", "cs.AI", "econ.GN", "q-fin.EC", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Who's Who: Large Language Models Meet Knowledge Conflicts in Practice", "abstract": "Retrieval-augmented generation (RAG) methods are viable solutions for\naddressing the static memory limits of pre-trained language models.\nNevertheless, encountering conflicting sources of information within the\nretrieval context is an inevitable practical challenge. In such situations, the\nlanguage models are recommended to transparently inform users about the\nconflicts rather than autonomously deciding what to present based on their\ninherent biases. To analyze how current large language models (LLMs) align with\nour recommendation, we introduce WhoQA, a public benchmark dataset to examine\nmodel's behavior in knowledge conflict situations. We induce conflicts by\nasking about a common property among entities having the same name, resulting\nin questions with up to 8 distinctive answers. WhoQA evaluation set includes 5K\nquestions across 13 Wikidata property types and 150K Wikipedia entities. Our\nexperiments show that despite the simplicity of WhoQA questions, knowledge\nconflicts significantly degrades LLMs' performance in RAG settings.", "published": "2024-10-21 07:56:45", "link": "http://arxiv.org/abs/2410.15737v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Optimal Query Allocation in Extractive QA with LLMs: A Learning-to-Defer\n  Framework with Theoretical Guarantees", "abstract": "Large Language Models excel in generative tasks but exhibit inefficiencies in\nstructured text selection, particularly in extractive question answering. This\nchallenge is magnified in resource-constrained environments, where deploying\nmultiple specialized models for different tasks is impractical. We propose a\nLearning-to-Defer framework that allocates queries to specialized experts,\nensuring high-confidence predictions while optimizing computational efficiency.\nOur approach integrates a principled allocation strategy with theoretical\nguarantees on optimal deferral that balances performance and cost. Empirical\nevaluations on SQuADv1, SQuADv2, and TriviaQA demonstrate that our method\nenhances answer reliability while significantly reducing computational\noverhead, making it well-suited for scalable and efficient EQA deployment.", "published": "2024-10-21 08:21:00", "link": "http://arxiv.org/abs/2410.15761v3", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Using GPT Models for Qualitative and Quantitative News Analytics in the\n  2024 US Presidental Election Process", "abstract": "The paper considers an approach of using Google Search API and GPT-4o model\nfor qualitative and quantitative analyses of news through retrieval-augmented\ngeneration (RAG). This approach was applied to analyze news about the 2024 US\npresidential election process. Different news sources for different time\nperiods have been analyzed. Quantitative scores generated by GPT model have\nbeen analyzed using Bayesian regression to derive trend lines. The\ndistributions found for the regression parameters allow for the analysis of\nuncertainty in the election process. The obtained results demonstrate that\nusing the GPT models for news analysis, one can get informative analytics and\nprovide key insights that can be applied in further analyses of election\nprocesses.", "published": "2024-10-21 11:02:18", "link": "http://arxiv.org/abs/2410.15884v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Yeah, Un, Oh: Continuous and Real-time Backchannel Prediction with\n  Fine-tuning of Voice Activity Projection", "abstract": "In human conversations, short backchannel utterances such as \"yeah\" and \"oh\"\nplay a crucial role in facilitating smooth and engaging dialogue. These\nbackchannels signal attentiveness and understanding without interrupting the\nspeaker, making their accurate prediction essential for creating more natural\nconversational agents. This paper proposes a novel method for real-time,\ncontinuous backchannel prediction using a fine-tuned Voice Activity Projection\n(VAP) model. While existing approaches have relied on turn-based or\nartificially balanced datasets, our approach predicts both the timing and type\nof backchannels in a continuous and frame-wise manner on unbalanced, real-world\ndatasets. We first pre-train the VAP model on a general dialogue corpus to\ncapture conversational dynamics and then fine-tune it on a specialized dataset\nfocused on backchannel behavior. Experimental results demonstrate that our\nmodel outperforms baseline methods in both timing and type prediction tasks,\nachieving robust performance in real-time environments. This research offers a\npromising step toward more responsive and human-like dialogue systems, with\nimplications for interactive spoken dialogue applications such as virtual\nassistants and robots.", "published": "2024-10-21 11:57:56", "link": "http://arxiv.org/abs/2410.15929v2", "categories": ["cs.CL", "cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Self-Explained Keywords Empower Large Language Models for Code\n  Generation", "abstract": "Large language models (LLMs) have achieved impressive performance in code\ngeneration. However, due to the long-tail distribution of LLMs' training data,\nlow-frequency terms are typically underrepresented in the training process.\nConsequently, LLMs often misunderstand or overlook problem-specific,\nlow-frequency keywords during code generation, compromising the accuracy of the\ngenerated code. To address this, we propose a novel technique named\nSEK(\\textbf{S}elf-\\textbf{E}xplained \\textbf{K}eywords), which empowers an LLM\nfor better code generation by extracting and explaining the key terms in the\nproblem description with the LLM itself and ranking them based on frequency.\nComprehensive experiments across three benchmarks, i.e., HumanEval(+), MBPP(+),\nand APPS, with five representative LLMs, show that SEK can significantly\nimprove LLMs in code generation, yielding substantial and consistent gains. For\ninstance, SEK improves the Pass@1 of DeepSeek-Coder-V2-Instruct from 85.4\\% to\n93.3\\% on the Humaneval benchmark. Further analysis confirms that SEK enables\nthe LLMs to shift their attention from low-frequency keywords to their\ncorresponding high-frequency counterparts.", "published": "2024-10-21 12:52:03", "link": "http://arxiv.org/abs/2410.15966v1", "categories": ["cs.CL", "cs.AI", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Large Language Models for Cross-lingual Emotion Detection", "abstract": "This paper presents a detailed system description of our entry for the WASSA\n2024 Task 2, focused on cross-lingual emotion detection. We utilized a\ncombination of large language models (LLMs) and their ensembles to effectively\nunderstand and categorize emotions across different languages. Our approach not\nonly outperformed other submissions with a large margin, but also demonstrated\nthe strength of integrating multiple models to enhance performance.\nAdditionally, We conducted a thorough comparison of the benefits and\nlimitations of each model used. An error analysis is included along with\nsuggested areas for future improvement. This paper aims to offer a clear and\ncomprehensive understanding of advanced techniques in emotion detection, making\nit accessible even to those new to the field.", "published": "2024-10-21 13:00:09", "link": "http://arxiv.org/abs/2410.15974v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Augmenting Legal Decision Support Systems with LLM-based NLI for\n  Analyzing Social Media Evidence", "abstract": "This paper presents our system description and error analysis of our entry\nfor NLLP 2024 shared task on Legal Natural Language Inference (L-NLI)\n\\citep{hagag2024legallenssharedtask2024}. The task required classifying these\nrelationships as entailed, contradicted, or neutral, indicating any association\nbetween the review and the complaint. Our system emerged as the winning\nsubmission, significantly outperforming other entries with a substantial margin\nand demonstrating the effectiveness of our approach in legal text analysis. We\nprovide a detailed analysis of the strengths and limitations of each model and\napproach tested, along with a thorough error analysis and suggestions for\nfuture improvements. This paper aims to contribute to the growing field of\nlegal NLP by offering insights into advanced techniques for natural language\ninference in legal contexts, making it accessible to both experts and newcomers\nin the field.", "published": "2024-10-21 13:20:15", "link": "http://arxiv.org/abs/2410.15990v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "1024m at SMM4H 2024: Tasks 3, 5 & 6 -- Ensembles of Transformers and\n  Large Language Models for Medical Text Classification", "abstract": "Social media is a great source of data for users reporting information and\nregarding their health and how various things have had an effect on them. This\npaper presents various approaches using Transformers and Large Language Models\nand their ensembles, their performance along with advantages and drawbacks for\nvarious tasks of SMM4H'24 - Classifying texts on impact of nature and outdoor\nspaces on the author's mental health (Task 3), Binary classification of tweets\nreporting their children's health disorders like Asthma, Autism, ADHD and\nSpeech disorder (task 5), Binary classification of users self-reporting their\nage (task 6).", "published": "2024-10-21 13:29:08", "link": "http://arxiv.org/abs/2410.15998v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Can Large Audio-Language Models Truly Hear? Tackling Hallucinations with\n  Multi-Task Assessment and Stepwise Audio Reasoning", "abstract": "Recent advancements in large audio-language models (LALMs) have shown\nimpressive capabilities in understanding and reasoning about audio and speech\ninformation. However, these models still face challenges, including\nhallucinating non-existent sound events, misidentifying the order of sound\nevents, and incorrectly attributing sound sources, which undermine their\nreliability and real-world application. To systematically evaluate these\nissues, we propose three distinct tasks: object existence, temporal order, and\nobject attribute within audio. These tasks assess the models' comprehension of\ncritical audio information aspects. Our experimental results reveal limitations\nin these fundamental tasks, underscoring the need for better models in\nrecognizing specific sound events, determining event sequences, and identifying\nsound sources. To improve performance in these areas, we introduce a multi-turn\nchain-of-thought approach, which demonstrates significantly improved model\nperformance across the proposed tasks.", "published": "2024-10-21 15:55:27", "link": "http://arxiv.org/abs/2410.16130v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "CoT-TL: Low-Resource Temporal Knowledge Representation of Planning\n  Instructions Using Chain-of-Thought Reasoning", "abstract": "Autonomous agents often face the challenge of interpreting uncertain natural\nlanguage instructions for planning tasks. Representing these instructions as\nLinear Temporal Logic (LTL) enables planners to synthesize actionable plans. We\nintroduce CoT-TL, a data-efficient in-context learning framework for\ntranslating natural language specifications into LTL representations. CoT-TL\naddresses the limitations of large language models, which typically rely on\nextensive fine-tuning data, by extending chain-of-thought reasoning and\nsemantic roles to align with the requirements of formal logic creation. This\napproach enhances the transparency and rationale behind LTL generation,\nfostering user trust. CoT-TL achieves state-of-the-art accuracy across three\ndiverse datasets in low-data scenarios, outperforming existing methods without\nfine-tuning or intermediate translations. To improve reliability and minimize\nhallucinations, we incorporate model checking to validate the syntax of the\ngenerated LTL output. We further demonstrate CoT-TL's effectiveness through\nablation studies and evaluations on unseen LTL structures and formulas in a new\ndataset. Finally, we validate CoT-TL's practicality by integrating it into a\nQuadCopter for multi-step drone planning based on natural language\ninstructions.", "published": "2024-10-21 17:10:43", "link": "http://arxiv.org/abs/2410.16207v1", "categories": ["cs.RO", "cs.CL", "cs.FL", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Compute-Constrained Data Selection", "abstract": "Data selection can reduce the amount of training data needed to finetune\nLLMs; however, the efficacy of data selection scales directly with its compute.\nMotivated by the practical challenge of compute-constrained finetuning, we\nconsider the setting in which both the cost of selecting data and training are\nbudgeted for. We first formalize the problem of data selection with a\ncost-aware utility function, and model the data selection problem as trading\noff initial-selection cost for training gain. We run a comprehensive sweep of\nexperiments across multiple tasks, varying compute budget by scaling finetuning\ntokens, model sizes, and data selection compute. Interestingly we find that\nmany powerful data selection methods are almost never compute-optimal, and that\ncheaper data selection alternatives dominate both from a theoretical and\nempirical perspective. For compute-optimal training, we find that perplexity\nand gradient data selection require training-to-selection model size ratios of\n5x and 10x, respectively.", "published": "2024-10-21 17:11:21", "link": "http://arxiv.org/abs/2410.16208v4", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "On Creating an English-Thai Code-switched Machine Translation in Medical\n  Domain", "abstract": "Machine translation (MT) in the medical domain plays a pivotal role in\nenhancing healthcare quality and disseminating medical knowledge. Despite\nadvancements in English-Thai MT technology, common MT approaches often\nunderperform in the medical field due to their inability to precisely translate\nmedical terminologies. Our research prioritizes not merely improving\ntranslation accuracy but also maintaining medical terminology in English within\nthe translated text through code-switched (CS) translation. We developed a\nmethod to produce CS medical translation data, fine-tuned a CS translation\nmodel with this data, and evaluated its performance against strong baselines,\nsuch as Google Neural Machine Translation (NMT) and GPT-3.5/GPT-4. Our model\ndemonstrated competitive performance in automatic metrics and was highly\nfavored in human preference evaluations. Our evaluation result also shows that\nmedical professionals significantly prefer CS translations that maintain\ncritical English terms accurately, even if it slightly compromises fluency. Our\ncode and test set are publicly available\nhttps://github.com/preceptorai-org/NLLB_CS_EM_NLP2024.", "published": "2024-10-21 17:25:32", "link": "http://arxiv.org/abs/2410.16221v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "xGen-MM-Vid (BLIP-3-Video): You Only Need 32 Tokens to Represent a Video\n  Even in VLMs", "abstract": "We present xGen-MM-Vid (BLIP-3-Video): a multimodal language model for\nvideos, particularly designed to efficiently capture temporal information over\nmultiple frames. BLIP-3-Video takes advantage of the 'temporal encoder' in\naddition to the conventional visual tokenizer, which maps a sequence of tokens\nover multiple frames into a compact set of visual tokens. This enables\nBLIP3-Video to use much fewer visual tokens than its competing models (e.g., 32\nvs. 4608 tokens). We explore different types of temporal encoders, including\nlearnable spatio-temporal pooling as well as sequential models like Token\nTuring Machines. We experimentally confirm that BLIP-3-Video obtains video\nquestion-answering accuracies comparable to much larger state-of-the-art models\n(e.g., 34B), while being much smaller (i.e., 4B) and more efficient by using\nfewer visual tokens. The project website is at\nhttps://www.salesforceairesearch.com/opensource/xGen-MM-Vid/index.html", "published": "2024-10-21 17:59:11", "link": "http://arxiv.org/abs/2410.16267v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Enhancing Multimodal Affective Analysis with Learned Live Comment\n  Features", "abstract": "Live comments, also known as Danmaku, are user-generated messages that are\nsynchronized with video content. These comments overlay directly onto streaming\nvideos, capturing viewer emotions and reactions in real-time. While prior work\nhas leveraged live comments in affective analysis, its use has been limited due\nto the relative rarity of live comments across different video platforms. To\naddress this, we first construct the Live Comment for Affective Analysis\n(LCAffect) dataset which contains live comments for English and Chinese videos\nspanning diverse genres that elicit a wide spectrum of emotions. Then, using\nthis dataset, we use contrastive learning to train a video encoder to produce\nsynthetic live comment features for enhanced multimodal affective content\nanalysis. Through comprehensive experimentation on a wide range of affective\nanalysis tasks (sentiment, emotion recognition, and sarcasm detection) in both\nEnglish and Chinese, we demonstrate that these synthetic live comment features\nsignificantly improve performance over state-of-the-art methods.", "published": "2024-10-21 18:19:09", "link": "http://arxiv.org/abs/2410.16407v1", "categories": ["cs.CL", "cs.AI", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Allo-AVA: A Large-Scale Multimodal Conversational AI Dataset for\n  Allocentric Avatar Gesture Animation", "abstract": "The scarcity of high-quality, multimodal training data severely hinders the\ncreation of lifelike avatar animations for conversational AI in virtual\nenvironments. Existing datasets often lack the intricate synchronization\nbetween speech, facial expressions, and body movements that characterize\nnatural human communication. To address this critical gap, we introduce\nAllo-AVA, a large-scale dataset specifically designed for text and audio-driven\navatar gesture animation in an allocentric (third person point-of-view)\ncontext. Allo-AVA consists of $\\sim$1,250 hours of diverse video content,\ncomplete with audio, transcripts, and extracted keypoints. Allo-AVA uniquely\nmaps these keypoints to precise timestamps, enabling accurate replication of\nhuman movements (body and facial gestures) in synchronization with speech. This\ncomprehensive resource enables the development and evaluation of more natural,\ncontext-aware avatar animation models, potentially transforming applications\nranging from virtual reality to digital assistants.", "published": "2024-10-21 20:50:51", "link": "http://arxiv.org/abs/2410.16503v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "Bayesian scaling laws for in-context learning", "abstract": "In-context learning (ICL) is a powerful technique for getting language models\nto perform complex tasks with no training updates. Prior work has established\nstrong correlations between the number of in-context examples provided and the\naccuracy of the model's predictions. In this paper, we seek to explain this\ncorrelation by showing that ICL approximates a Bayesian learner. This\nperspective gives rise to a family of novel Bayesian scaling laws for ICL. In\nexperiments with \\mbox{GPT-2} models of different sizes, our scaling laws\nexceed or match existing scaling laws in accuracy while also offering\ninterpretable terms for task priors, learning efficiency, and per-example\nprobabilities. To illustrate the analytic power that such interpretable scaling\nlaws provide, we report on controlled synthetic dataset experiments designed to\ninform real-world studies of safety alignment. In our experimental protocol, we\nuse SFT to suppress an unwanted existing model capability and then use ICL to\ntry to bring that capability back (many-shot jailbreaking). We then experiment\non real-world instruction-tuned LLMs using capabilities benchmarks as well as a\nnew many-shot jailbreaking dataset. In all cases, Bayesian scaling laws\naccurately predict the conditions under which ICL will cause the suppressed\nbehavior to reemerge, which sheds light on the ineffectiveness of post-training\nat increasing LLM safety.", "published": "2024-10-21 21:45:22", "link": "http://arxiv.org/abs/2410.16531v3", "categories": ["cs.CL", "cs.AI", "cs.FL", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Large Body Language Models", "abstract": "As virtual agents become increasingly prevalent in human-computer\ninteraction, generating realistic and contextually appropriate gestures in\nreal-time remains a significant challenge. While neural rendering techniques\nhave made substantial progress with static scripts, their applicability to\nhuman-computer interactions remains limited. To address this, we introduce\nLarge Body Language Models (LBLMs) and present LBLM-AVA, a novel LBLM\narchitecture that combines a Transformer-XL large language model with a\nparallelized diffusion model to generate human-like gestures from multimodal\ninputs (text, audio, and video). LBLM-AVA incorporates several key components\nenhancing its gesture generation capabilities, such as multimodal-to-pose\nembeddings, enhanced sequence-to-sequence mapping with redefined attention\nmechanisms, a temporal smoothing module for gesture sequence coherence, and an\nattention-based refinement module for enhanced realism. The model is trained on\nour large-scale proprietary open-source dataset Allo-AVA. LBLM-AVA achieves\nstate-of-the-art performance in generating lifelike and contextually\nappropriate gestures with a 30% reduction in Fr\\'echet Gesture Distance (FGD),\nand a 25% improvement in Fr\\'echet Inception Distance compared to existing\napproaches.", "published": "2024-10-21 21:48:24", "link": "http://arxiv.org/abs/2410.16533v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.AI"}
{"title": "A Theoretical Understanding of Chain-of-Thought: Coherent Reasoning and\n  Error-Aware Demonstration", "abstract": "Few-shot Chain-of-Thought (CoT) prompting has demonstrated strong performance\nin improving the reasoning capabilities of large language models (LLMs). While\ntheoretical investigations have been conducted to understand CoT, the\nunderlying transformer used in these studies isolates the CoT reasoning process\ninto separated in-context learning steps (Stepwise ICL). In this work, we\ntheoretically show that, compared to Stepwise ICL, the transformer gains better\nerror correction ability and more accurate predictions if the reasoning from\nearlier steps (Coherent CoT) is integrated. Given that this coherent reasoning\nchanges the behavior of the transformer, we further investigate the sensitivity\nof the transformer with Coherent CoT when the demonstration examples are\ncorrupted at the inference stage. Our theoretical results indicate that the\ntransformer is more sensitive to errors in intermediate reasoning steps than\nthe final outcome. Building upon this observation, we propose an improvement on\nCoT by incorporating both correct and incorrect reasoning paths in the\ndemonstration. Our experiments validate the effectiveness of the proposed\napproach.", "published": "2024-10-21 22:07:20", "link": "http://arxiv.org/abs/2410.16540v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "How Performance Pressure Influences AI-Assisted Decision Making", "abstract": "Many domains now employ AI-based decision-making aids, and although the\npotential for AI systems to assist with decision making is much discussed,\nhuman-AI collaboration often underperforms due to factors such as (mis)trust in\nthe AI system and beliefs about AI being incapable of completing subjective\ntasks. One potential tool for influencing human decision making is performance\npressure, which hasn't been much studied in interaction with human-AI decision\nmaking. In this work, we examine how pressure and explainable AI (XAI)\ntechniques interact with AI advice-taking behavior. Using an inherently\nlow-stakes task (spam review classification), we demonstrate effective and\nsimple methods to apply pressure and influence human AI advice-taking behavior\nby manipulating financial incentives and imposing time limits. Our results show\ncomplex interaction effects, with different combinations of pressure and XAI\ntechniques either improving or worsening AI advice taking behavior. We conclude\nby discussing the implications of these interactions, strategies to effectively\nuse pressure, and encourage future research to incorporate pressure analysis.", "published": "2024-10-21 22:39:52", "link": "http://arxiv.org/abs/2410.16560v2", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "R2Gen-Mamba: A Selective State Space Model for Radiology Report\n  Generation", "abstract": "Radiology report generation is crucial in medical imaging,but the manual\nannotation process by physicians is time-consuming and labor-intensive,\nnecessitating the develop-ment of automatic report generation methods.\nExistingresearch predominantly utilizes Transformers to generateradiology\nreports, which can be computationally intensive,limiting their use in real\napplications. In this work, we presentR2Gen-Mamba, a novel automatic radiology\nreport genera-tion method that leverages the efficient sequence processingof\nthe Mamba with the contextual benefits of Transformerarchitectures. Due to\nlower computational complexity ofMamba, R2Gen-Mamba not only enhances training\nand in-ference efficiency but also produces high-quality reports.Experimental\nresults on two benchmark datasets with morethan 210,000 X-ray image-report\npairs demonstrate the ef-fectiveness of R2Gen-Mamba regarding report quality\nandcomputational efficiency compared with several state-of-the-art methods. The\nsource code can be accessed online.", "published": "2024-10-21 19:35:34", "link": "http://arxiv.org/abs/2410.18135v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards More Accurate US Presidential Election via Multi-step Reasoning\n  with Large Language Models", "abstract": "Can Large Language Models (LLMs) accurately predict election outcomes? While\nLLMs have demonstrated impressive performance in various domains, including\nhealthcare, legal analysis, and creative tasks, their ability to forecast\nelections remains unknown. Election prediction poses unique challenges, such as\nlimited voter-level data, rapidly changing political landscapes, and the need\nto model complex human behavior. To address these challenges, we introduce a\nmulti-step reasoning framework designed for political analysis. Our approach is\nvalidated on real-world data from the American National Election Studies (ANES)\n2016 and 2020, as well as synthetic personas generated by the leading machine\nlearning framework, offering scalable datasets for voter behavior modeling. To\ncapture temporal dynamics, we incorporate candidates' policy positions and\nbiographical details, ensuring that the model adapts to evolving political\ncontexts. Drawing on Chain of Thought prompting, our multi-step reasoning\npipeline systematically integrates demographic, ideological, and time-dependent\nfactors, enhancing the model's predictive power.", "published": "2024-10-21 06:18:53", "link": "http://arxiv.org/abs/2411.03321v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "OpenMU: Your Swiss Army Knife for Music Understanding", "abstract": "We present OpenMU-Bench, a large-scale benchmark suite for addressing the\ndata scarcity issue in training multimodal language models to understand music.\nTo construct OpenMU-Bench, we leveraged existing datasets and bootstrapped new\nannotations. OpenMU-Bench also broadens the scope of music understanding by\nincluding lyrics understanding and music tool usage. Using OpenMU-Bench, we\ntrained our music understanding model, OpenMU, with extensive ablations,\ndemonstrating that OpenMU outperforms baseline models such as MU-Llama. Both\nOpenMU and OpenMU-Bench are open-sourced to facilitate future research in music\nunderstanding and to enhance creative music production efficiency.", "published": "2024-10-21 01:36:42", "link": "http://arxiv.org/abs/2410.15573v3", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Continuous Speech Synthesis using per-token Latent Diffusion", "abstract": "The success of autoregressive transformer models with discrete tokens has\ninspired quantization-based approaches for continuous modalities, though these\noften limit reconstruction quality. We therefore introduce SALAD, a per-token\nlatent diffusion model for zero-shot text-to-speech, that operates on\ncontinuous representations. SALAD builds upon the recently proposed expressive\ndiffusion head for image generation, and extends it to generate variable-length\noutputs. Our approach utilizes semantic tokens for providing contextual\ninformation and determining the stopping condition. We suggest three continuous\nvariants for our method, extending popular discrete speech synthesis\ntechniques. Additionally, we implement discrete baselines for each variant and\nconduct a comparative analysis of discrete versus continuous speech modeling\ntechniques. Our results demonstrate that both continuous and discrete\napproaches are highly competent, and that SALAD achieves a superior\nintelligibility score while obtaining speech quality and speaker similarity on\npar with the ground-truth audio.", "published": "2024-10-21 14:23:46", "link": "http://arxiv.org/abs/2410.16048v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "ALDAS: Audio-Linguistic Data Augmentation for Spoofed Audio Detection", "abstract": "Spoofed audio, i.e. audio that is manipulated or AI-generated deepfake audio,\nis difficult to detect when only using acoustic features. Some recent\ninnovative work involving AI-spoofed audio detection models augmented with\nphonetic and phonological features of spoken English, manually annotated by\nexperts, led to improved model performance. While this augmented model produced\nsubstantial improvements over traditional acoustic features based models, a\nscalability challenge motivates inquiry into auto labeling of features. In this\npaper we propose an AI framework, Audio-Linguistic Data Augmentation for\nSpoofed audio detection (ALDAS), for auto labeling linguistic features. ALDAS\nis trained on linguistic features selected and extracted by sociolinguistics\nexperts; these auto labeled features are used to evaluate the quality of ALDAS\npredictions. Findings indicate that while the detection enhancement is not as\nsubstantial as when involving the pure ground truth linguistic features, there\nis improvement in performance while achieving auto labeling. Labels generated\nby ALDAS are also validated by the sociolinguistics experts.", "published": "2024-10-21 01:54:55", "link": "http://arxiv.org/abs/2410.15577v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Optimizing Neural Speech Codec for Low-Bitrate Compression via\n  Multi-Scale Encoding", "abstract": "Neural speech codecs have demonstrated their ability to compress high-quality\nspeech and audio by converting them into discrete token representations. Most\nexisting methods utilize Residual Vector Quantization (RVQ) to encode speech\ninto multiple layers of discrete codes with uniform time scales. However, this\nstrategy overlooks the differences in information density across various speech\nfeatures, leading to redundant encoding of sparse information, which limits the\nperformance of these methods at low bitrate. This paper proposes MsCodec, a\nnovel multi-scale neural speech codec that encodes speech into multiple layers\nof discrete codes, each corresponding to a different time scale. This\nencourages the model to decouple speech features according to their diverse\ninformation densities, consequently enhancing the performance of speech\ncompression. Furthermore, we incorporate mutual information loss to augment the\ndiversity among speech codes across different layers. Experimental results\nindicate that our proposed method significantly improves codec performance at\nlow bitrate.", "published": "2024-10-21 08:04:36", "link": "http://arxiv.org/abs/2410.15749v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multi-Level Speaker Representation for Target Speaker Extraction", "abstract": "Target speaker extraction (TSE) relies on a reference cue of the target to\nextract the target speech from a speech mixture. While a speaker embedding is\ncommonly used as the reference cue, such embedding pre-trained with a large\nnumber of speakers may suffer from confusion of speaker identity. In this work,\nwe propose a multi-level speaker representation approach, from raw features to\nneural embeddings, to serve as the speaker reference cue. We generate a\nspectral-level representation from the enrollment magnitude spectrogram as a\nraw, low-level feature, which significantly improves the model's generalization\ncapability. Additionally, we propose a contextual embedding feature based on\ncross-attention mechanisms that integrate frame-level embeddings from a\npre-trained speaker encoder. By incorporating speaker features across multiple\nlevels, we significantly enhance the performance of the TSE model. Our approach\nachieves a 2.74 dB improvement and a 4.94% increase in extraction accuracy on\nLibri2mix test set over the baseline.", "published": "2024-10-21 14:38:20", "link": "http://arxiv.org/abs/2410.16059v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Neural Scoring, Not Embedding: A Novel Framework for Robust Speaker\n  Verification", "abstract": "Current mainstream speaker verification systems are predominantly based on\nthe concept of ``speaker embedding\", which transforms variable-length speech\nsignals into fixed-length speaker vectors, followed by verification based on\ncosine similarity between the embeddings of the enrollment and test utterances.\nHowever, this approach suffers from considerable performance degradation in the\npresence of severe noise and interference speakers. This paper introduces\nNeural Scoring, a novel framework that re-treats speaker verification as a\nscoring task using a Transformer-based architecture. The proposed method first\nextracts an embedding from the enrollment speech and frame-level features from\nthe test speech. A Transformer network then generates a decision score that\nquantifies the likelihood of the enrolled speaker being present in the test\nspeech. We evaluated Neural Scoring on the VoxCeleb dataset across five test\nscenarios, comparing it with the state-of-the-art embedding-based approach.\nWhile Neural Scoring achieves comparable performance to the state-of-the-art\nunder the benchmark (clean) test condition, it demonstrates a remarkable\nadvantage in the four complex scenarios, achieving an overall 64.53% reduction\nin equal error rate (EER) compared to the baseline.", "published": "2024-10-21 18:48:54", "link": "http://arxiv.org/abs/2410.16428v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "LSCodec: Low-Bitrate and Speaker-Decoupled Discrete Speech Codec", "abstract": "Although discrete speech tokens have exhibited strong potential for language\nmodel-based speech generation, their high bitrates and redundant timbre\ninformation restrict the development of such models. In this work, we propose\nLSCodec, a discrete speech codec that has both low bitrate and speaker\ndecoupling ability. LSCodec adopts a three-stage unsupervised training\nframework with a speaker perturbation technique. A continuous information\nbottleneck is first established, followed by vector quantization that produces\na discrete speaker-decoupled space. A discrete token vocoder finally refines\nacoustic details from LSCodec. By reconstruction experiments, LSCodec\ndemonstrates superior intelligibility and audio quality with only a single\ncodebook and smaller vocabulary size than baselines. The 25Hz version of\nLSCodec also achieves the lowest bitrate (0.25kbps) of codecs so far with\ndecent quality. Voice conversion evaluations prove the satisfactory speaker\ndisentanglement of LSCodec, and ablation study further verifies the\neffectiveness of the proposed training framework.", "published": "2024-10-21 08:23:31", "link": "http://arxiv.org/abs/2410.15764v2", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "AlignVSR: Audio-Visual Cross-Modal Alignment for Visual Speech\n  Recognition", "abstract": "Visual Speech Recognition (VSR) aims to recognize corresponding text by\nanalyzing visual information from lip movements. Due to the high variability\nand weak information of lip movements, VSR tasks require effectively utilizing\nany information from any source and at any level. In this paper, we propose a\nVSR method based on audio-visual cross-modal alignment, named AlignVSR. The\nmethod leverages the audio modality as an auxiliary information source and\nutilizes the global and local correspondence between the audio and visual\nmodalities to improve visual-to-text inference. Specifically, the method first\ncaptures global alignment between video and audio through a cross-modal\nattention mechanism from video frames to a bank of audio units. Then, based on\nthe temporal correspondence between audio and video, a frame-level local\nalignment loss is introduced to refine the global alignment, improving the\nutility of the audio information. Experimental results on the LRS2 and\nCNVSRC.Single datasets consistently show that AlignVSR outperforms several\nmainstream VSR methods, demonstrating its superior and robust performance.", "published": "2024-10-21 19:02:13", "link": "http://arxiv.org/abs/2410.16438v1", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Do Audio-Language Models Understand Linguistic Variations?", "abstract": "Open-vocabulary audio language models (ALMs), like Contrastive Language Audio\nPretraining (CLAP), represent a promising new paradigm for audio-text retrieval\nusing natural language queries. In this paper, for the first time, we perform\ncontrolled experiments on various benchmarks to show that existing ALMs\nstruggle to generalize to linguistic variations in textual queries. To address\nthis issue, we propose RobustCLAP, a novel and compute-efficient technique to\nlearn audio-language representations agnostic to linguistic variations.\nSpecifically, we reformulate the contrastive loss used in CLAP architectures by\nintroducing a multi-view contrastive learning objective, where paraphrases are\ntreated as different views of the same audio scene and use this for training.\nOur proposed approach improves the text-to-audio retrieval performance of CLAP\nby 0.8%-13% across benchmarks and enhances robustness to linguistic variation.", "published": "2024-10-21 20:55:33", "link": "http://arxiv.org/abs/2410.16505v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
