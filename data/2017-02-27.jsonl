{"title": "A case study on English-Malayalam Machine Translation", "abstract": "In this paper we present our work on a case study on Statistical Machine\nTranslation (SMT) and Rule based machine translation (RBMT) for translation\nfrom English to Malayalam and Malayalam to English. One of the motivations of\nour study is to make a three way performance comparison, such as, a) SMT and\nRBMT b) English to Malayalam SMT and Malayalam to English SMT c) English to\nMalayalam RBMT and Malayalam to English RBMT. We describe the development of\nEnglish to Malayalam and Malayalam to English baseline phrase based SMT system\nand the evaluation of its performance compared against the RBMT system. Based\non our study the observations are: a) SMT systems outperform RBMT systems, b)\nIn the case of SMT, English - Malayalam systems perform better than that of\nMalayalam - English systems, c) In the case RBMT, Malayalam to English systems\nare performing better than English to Malayalam systems. Based on our\nevaluations and detailed error analysis, we describe the requirements of\nincorporating morphological processing into the SMT to improve the accuracy of\ntranslation.", "published": "2017-02-27 10:09:46", "link": "http://arxiv.org/abs/1702.08217v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Identifying beneficial task relations for multi-task learning in deep\n  neural networks", "abstract": "Multi-task learning (MTL) in deep neural networks for NLP has recently\nreceived increasing interest due to some compelling benefits, including its\npotential to efficiently regularize models and to reduce the need for labeled\ndata. While it has brought significant improvements in a number of NLP tasks,\nmixed results have been reported, and little is known about the conditions\nunder which MTL leads to gains in NLP. This paper sheds light on the specific\ntask relations that can lead to gains from MTL models over single-task setups.", "published": "2017-02-27 14:37:21", "link": "http://arxiv.org/abs/1702.08303v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "A Knowledge-Based Approach to Word Sense Disambiguation by\n  distributional selection and semantic features", "abstract": "Word sense disambiguation improves many Natural Language Processing (NLP)\napplications such as Information Retrieval, Information Extraction, Machine\nTranslation, or Lexical Simplification. Roughly speaking, the aim is to choose\nfor each word in a text its best sense. One of the most popular method\nestimates local semantic similarity relatedness between two word senses and\nthen extends it to all words from text. The most direct method computes a rough\nscore for every pair of word senses and chooses the lexical chain that has the\nbest score (we can imagine the exponential complexity that returns this\ncomprehensive approach). In this paper, we propose to use a combinatorial\noptimization metaheuristic for choosing the nearest neighbors obtained by\ndistributional selection around the word to disambiguate. The test and the\nevaluation of our method concern a corpus written in French by means of the\nsemantic network BabelNet. The obtained accuracy rate is 78 % on all names and\nverbs chosen for the evaluation.", "published": "2017-02-27 13:37:15", "link": "http://arxiv.org/abs/1702.08450v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Approches d'analyse distributionnelle pour am\u00e9liorer la\n  d\u00e9sambigu\u00efsation s\u00e9mantique", "abstract": "Word sense disambiguation (WSD) improves many Natural Language Processing\n(NLP) applications such as Information Retrieval, Machine Translation or\nLexical Simplification. WSD is the ability of determining a word sense among\ndifferent ones within a polysemic lexical unit taking into account the context.\nThe most straightforward approach uses a semantic proximity measure between the\nword sense candidates of the target word and those of its context. Such a\nmethod very easily entails a combinatorial explosion. In this paper, we propose\ntwo methods based on distributional analysis which enable to reduce the\nexponential complexity without losing the coherence. We present a comparison\nbetween the selection of distributional neighbors and the linearly nearest\nneighbors. The figures obtained show that selecting distributional neighbors\nleads to better results.", "published": "2017-02-27 13:38:08", "link": "http://arxiv.org/abs/1702.08451v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Soft Label Memorization-Generalization for Natural Language Inference", "abstract": "Often when multiple labels are obtained for a training example it is assumed\nthat there is an element of noise that must be accounted for. It has been shown\nthat this disagreement can be considered signal instead of noise. In this work\nwe investigate using soft labels for training data to improve generalization in\nmachine learning models. However, using soft labels for training Deep Neural\nNetworks (DNNs) is not practical due to the costs involved in obtaining\nmultiple labels for large data sets. We propose soft label\nmemorization-generalization (SLMG), a fine-tuning approach to using soft labels\nfor training DNNs. We assume that differences in labels provided by human\nannotators represent ambiguity about the true label instead of noise.\nExperiments with SLMG demonstrate improved generalization performance on the\nNatural Language Inference (NLI) task. Our experiments show that by injecting a\nsmall percentage of soft label training data (0.03% of training set size) we\ncan improve generalization performance over several baselines.", "published": "2017-02-27 22:25:45", "link": "http://arxiv.org/abs/1702.08563v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Political Homophily in Independence Movements: Analysing and Classifying\n  Social Media Users by National Identity", "abstract": "Social media and data mining are increasingly being used to analyse political\nand societal issues. Here we undertake the classification of social media users\nas supporting or opposing ongoing independence movements in their territories.\nIndependence movements occur in territories whose citizens have conflicting\nnational identities; users with opposing national identities will then support\nor oppose the sense of being part of an independent nation that differs from\nthe officially recognised country. We describe a methodology that relies on\nusers' self-reported location to build large-scale datasets for three\nterritories -- Catalonia, the Basque Country and Scotland. An analysis of these\ndatasets shows that homophily plays an important role in determining who people\nconnect with, as users predominantly choose to follow and interact with others\nfrom the same national identity. We show that a classifier relying on users'\nfollow networks can achieve accurate, language-independent classification\nperformances ranging from 85% to 97% for the three territories.", "published": "2017-02-27 17:19:03", "link": "http://arxiv.org/abs/1702.08388v3", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Improved Variational Autoencoders for Text Modeling using Dilated\n  Convolutions", "abstract": "Recent work on generative modeling of text has found that variational\nauto-encoders (VAE) incorporating LSTM decoders perform worse than simpler LSTM\nlanguage models (Bowman et al., 2015). This negative result is so far poorly\nunderstood, but has been attributed to the propensity of LSTM decoders to\nignore conditioning information from the encoder. In this paper, we experiment\nwith a new type of decoder for VAE: a dilated CNN. By changing the decoder's\ndilation architecture, we control the effective context from previously\ngenerated words. In experiments, we find that there is a trade off between the\ncontextual capacity of the decoder and the amount of encoding information used.\nWe show that with the right decoder, VAE can outperform LSTM language models.\nWe demonstrate perplexity gains on two datasets, representing the first\npositive experimental result on the use VAE for generative modeling of text.\nFurther, we conduct an in-depth investigation of the use of VAE (with our new\ndecoding architecture) for semi-supervised and unsupervised labeling tasks,\ndemonstrating gains over several strong baselines.", "published": "2017-02-27 04:16:01", "link": "http://arxiv.org/abs/1702.08139v2", "categories": ["cs.NE", "cs.CL", "cs.LG"], "primary_category": "cs.NE"}
