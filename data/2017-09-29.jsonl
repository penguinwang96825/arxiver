{"title": "The First Evaluation of Chinese Human-Computer Dialogue Technology", "abstract": "In this paper, we introduce the first evaluation of Chinese human-computer\ndialogue technology. We detail the evaluation scheme, tasks, metrics and how to\ncollect and annotate the data for training, developing and test. The evaluation\nincludes two tasks, namely user intent classification and online testing of\ntask-oriented dialogue. To consider the different sources of the data for\ntraining and developing, the first task can also be divided into two sub tasks.\nBoth the two tasks are coming from the real problems when using the\napplications developed by industry. The evaluation data is provided by the\niFLYTEK Corporation. Meanwhile, in this paper, we publish the evaluation\nresults to present the current performance of the participants in the two tasks\nof Chinese human-computer dialogue technology. Moreover, we analyze the\nexisting problems of human-computer dialogue as well as the evaluation scheme\nitself.", "published": "2017-09-29 02:35:28", "link": "http://arxiv.org/abs/1709.10217v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Universal Semantic Tagging", "abstract": "The paper proposes the task of universal semantic tagging---tagging word\ntokens with language-neutral, semantically informative tags. We argue that the\ntask, with its independent nature, contributes to better semantic analysis for\nwide-coverage multilingual text. We present the initial version of the semantic\ntagset and show that (a) the tags provide semantically fine-grained\ninformation, and (b) they are suitable for cross-lingual semantic parsing. An\napplication of the semantic tagging in the Parallel Meaning Bank supports both\nof these points as the tags contribute to formal lexical semantics and their\ncross-lingual projection. As a part of the application, we annotate a small\ncorpus with the semantic tags and present new baseline result for universal\nsemantic tagging.", "published": "2017-09-29 12:58:05", "link": "http://arxiv.org/abs/1709.10381v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Symbol, Conversational, and Societal Grounding with a Toy Robot", "abstract": "Essential to meaningful interaction is grounding at the symbolic,\nconversational, and societal levels. We present ongoing work with Anki's Cozmo\ntoy robot as a research platform where we leverage the recent\nwords-as-classifiers model of lexical semantics in interactive reference\nresolution tasks for language grounding.", "published": "2017-09-29 16:36:53", "link": "http://arxiv.org/abs/1709.10486v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Synonym Discovery with Etymology-based Word Embeddings", "abstract": "We propose a novel approach to learn word embeddings based on an extended\nversion of the distributional hypothesis. Our model derives word embedding\nvectors using the etymological composition of words, rather than the context in\nwhich they appear. It has the strength of not requiring a large text corpus,\nbut instead it requires reliable access to etymological roots of words, making\nit specially fit for languages with logographic writing systems. The model\nconsists on three steps: (1) building an etymological graph, which is a\nbipartite network of words and etymological roots, (2) obtaining the\nbiadjacency matrix of the etymological graph and reducing its dimensionality,\n(3) using columns/rows of the resulting matrices as embedding vectors. We test\nour model in the Chinese and Sino-Korean vocabularies. Our graphs are formed by\na set of 117,000 Chinese words, and a set of 135,000 Sino-Korean words. In both\ncases we show that our model performs well in the task of synonym discovery.", "published": "2017-09-29 15:10:20", "link": "http://arxiv.org/abs/1709.10445v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Neural Comprehensive Ranker (NCR) for Open-Domain Question Answering", "abstract": "This paper proposes a novel neural machine reading model for open-domain\nquestion answering at scale. Existing machine comprehension models typically\nassume that a short piece of relevant text containing answers is already\nidentified and given to the models, from which the models are designed to\nextract answers. This assumption, however, is not realistic for building a\nlarge-scale open-domain question answering system which requires both deep text\nunderstanding and identifying relevant text from corpus simultaneously.\n  In this paper, we introduce Neural Comprehensive Ranker (NCR) that integrates\nboth passage ranking and answer extraction in one single framework. A Q&A\nsystem based on this framework allows users to issue an open-domain question\nwithout needing to provide a piece of text that must contain the answer.\nExperiments show that the unified NCR model is able to outperform the\nstates-of-the-art in both retrieval of relevant text and answer extraction.", "published": "2017-09-29 00:27:48", "link": "http://arxiv.org/abs/1709.10204v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Learning how to learn: an adaptive dialogue agent for incrementally\n  learning visually grounded word meanings", "abstract": "We present an optimised multi-modal dialogue agent for interactive learning\nof visually grounded word meanings from a human tutor, trained on real\nhuman-human tutoring data. Within a life-long interactive learning period, the\nagent, trained using Reinforcement Learning (RL), must be able to handle\nnatural conversations with human users and achieve good learning performance\n(accuracy) while minimising human effort in the learning process. We train and\nevaluate this system in interaction with a simulated human tutor, which is\nbuilt on the BURCHAK corpus -- a Human-Human Dialogue dataset for the visual\nlearning task. The results show that: 1) The learned policy can coherently\ninteract with the simulated user to achieve the goal of the task (i.e. learning\nvisual attributes of objects, e.g. colour and shape); and 2) it finds a better\ntrade-off between classifier accuracy and tutoring costs than hand-crafted\nrule-based policies, including ones with dynamic policies.", "published": "2017-09-29 14:21:31", "link": "http://arxiv.org/abs/1709.10423v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.RO"], "primary_category": "cs.CL"}
{"title": "Training an adaptive dialogue policy for interactive learning of\n  visually grounded word meanings", "abstract": "We present a multi-modal dialogue system for interactive learning of\nperceptually grounded word meanings from a human tutor. The system integrates\nan incremental, semantic parsing/generation framework - Dynamic Syntax and Type\nTheory with Records (DS-TTR) - with a set of visual classifiers that are\nlearned throughout the interaction and which ground the meaning representations\nthat it produces. We use this system in interaction with a simulated human\ntutor to study the effects of different dialogue policies and capabilities on\nthe accuracy of learned meanings, learning rates, and efforts/costs to the\ntutor. We show that the overall performance of the learning agent is affected\nby (1) who takes initiative in the dialogues; (2) the ability to express/use\ntheir confidence level about visual attributes; and (3) the ability to process\nelliptical and incrementally constructed dialogue turns. Ultimately, we train\nan adaptive dialogue policy which optimises the trade-off between classifier\naccuracy and tutoring costs.", "published": "2017-09-29 14:28:31", "link": "http://arxiv.org/abs/1709.10426v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.RO"], "primary_category": "cs.CL"}
{"title": "The BURCHAK corpus: a Challenge Data Set for Interactive Learning of\n  Visually Grounded Word Meanings", "abstract": "We motivate and describe a new freely available human-human dialogue dataset\nfor interactive learning of visually grounded word meanings through ostensive\ndefinition by a tutor to a learner. The data has been collected using a novel,\ncharacter-by-character variant of the DiET chat tool (Healey et al., 2003;\nMills and Healey, submitted) with a novel task, where a Learner needs to learn\ninvented visual attribute words (such as \" burchak \" for square) from a tutor.\nAs such, the text-based interactions closely resemble face-to-face conversation\nand thus contain many of the linguistic phenomena encountered in natural,\nspontaneous dialogue. These include self-and other-correction, mid-sentence\ncontinuations, interruptions, overlaps, fillers, and hedges. We also present a\ngeneric n-gram framework for building user (i.e. tutor) simulations from this\ntype of incremental data, which is freely available to researchers. We show\nthat the simulations produce outputs that are similar to the original data\n(e.g. 78% turn match similarity). Finally, we train and evaluate a\nReinforcement Learning dialogue control agent for learning visually grounded\nword meanings, trained from the BURCHAK corpus. The learned policy shows\ncomparable performance to a rule-based system built previously.", "published": "2017-09-29 14:43:06", "link": "http://arxiv.org/abs/1709.10431v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.RO"], "primary_category": "cs.CL"}
{"title": "Improving speech recognition by revising gated recurrent units", "abstract": "Speech recognition is largely taking advantage of deep learning, showing that\nsubstantial benefits can be obtained by modern Recurrent Neural Networks\n(RNNs). The most popular RNNs are Long Short-Term Memory (LSTMs), which\ntypically reach state-of-the-art performance in many tasks thanks to their\nability to learn long-term dependencies and robustness to vanishing gradients.\nNevertheless, LSTMs have a rather complex design with three multiplicative\ngates, that might impair their efficient implementation. An attempt to simplify\nLSTMs has recently led to Gated Recurrent Units (GRUs), which are based on just\ntwo multiplicative gates.\n  This paper builds on these efforts by further revising GRUs and proposing a\nsimplified architecture potentially more suitable for speech recognition. The\ncontribution of this work is two-fold. First, we suggest to remove the reset\ngate in the GRU design, resulting in a more efficient single-gate architecture.\nSecond, we propose to replace tanh with ReLU activations in the state update\nequations. Results show that, in our implementation, the revised architecture\nreduces the per-epoch training time with more than 30% and consistently\nimproves recognition performance across different tasks, input features, and\nnoisy conditions when compared to a standard GRU.", "published": "2017-09-29 12:40:50", "link": "http://arxiv.org/abs/1710.00641v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Real-Time Wind Noise Detection and Suppression with Neural-Based Signal\n  Reconstruction for Mult-Channel, Low-Power Devices", "abstract": "Active wind noise detection and suppression techniques are a new and\nessential paradigm for enhancing ASR-based functionality with smart glasses, in\naddition to other wearable and smart devices in the broader IoT (Internet of\nthings). In this paper, we develop two separate algorithms for wind noise\ndetection and suppression, respectively, operational in a challenging,\nlow-energy regime. Together, these algorithms comprise a robust wind noise\nsuppression system. In the first case, we advance a real-time wind detection\nalgorithm (RTWD) that uses two distinct sets of low-dimensional signal features\nto discriminate the presence of wind noise with high accuracy. For wind noise\nsuppression, we employ an additional algorithm - attentive neural wind\nsuppression (ANWS) - that utilizes a neural network to reconstruct the wearer\nspeech signal from wind-corrupted audio in the spectral regions that are most\nadversely affected by wind noise. Finally, we test our algorithms through\nreal-time experiments using low-power, multi-microphone devices with a wind\nsimulator under challenging detection criteria and a variety of wind\nintensities.", "published": "2017-09-29 20:33:38", "link": "http://arxiv.org/abs/1710.00082v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "UTD-CRSS Submission for MGB-3 Arabic Dialect Identification: Front-end\n  and Back-end Advancements on Broadcast Speech", "abstract": "This study presents systems submitted by the University of Texas at Dallas,\nCenter for Robust Speech Systems (UTD-CRSS) to the MGB-3 Arabic Dialect\nIdentification (ADI) subtask. This task is defined to discriminate between five\ndialects of Arabic, including Egyptian, Gulf, Levantine, North African, and\nModern Standard Arabic. We develop multiple single systems with different\nfront-end representations and back-end classifiers. At the front-end level,\nfeature extraction methods such as Mel-frequency cepstral coefficients (MFCCs)\nand two types of bottleneck features (BNF) are studied for an i-Vector\nframework. As for the back-end level, Gaussian back-end (GB), and Generative\nAdversarial Networks (GANs) classifiers are applied alternately. The best\nsubmission (contrastive) is achieved for the ADI subtask with an accuracy of\n76.94% by augmenting the randomly chosen part of the development dataset.\nFurther, with a post evaluation correction in the submitted system, final\naccuracy is increased to 79.76%, which represents the best performance achieved\nso far for the challenge on the test dataset.", "published": "2017-09-29 22:42:01", "link": "http://arxiv.org/abs/1710.00113v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "PLDA-Based Diarization of Telephone Conversations", "abstract": "This paper investigates the application of the probabilistic linear\ndiscriminant analysis (PLDA) to speaker diarization of telephone conversations.\nWe introduce using a variational Bayes (VB) approach for inference under a PLDA\nmodel for modeling segmental i-vectors in speaker diarization. Deterministic\nannealing (DA) algorithm is imposed in order to avoid local optimal solutions\nin VB iterations. We compare our proposed system with a well-known system that\napplies k-means clustering on principal component analysis (PCA) coefficients\nof segmental i-vectors. We used summed channel telephone data from the National\nInstitute of Standards and Technology (NIST) 2008 Speaker Recognition\nEvaluation (SRE) as the test set in order to evaluate the performance of the\nproposed system. We achieve about 20% relative improvement in Diarization Error\nRate (DER) compared to the baseline system.", "published": "2017-09-29 23:12:27", "link": "http://arxiv.org/abs/1710.00116v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
