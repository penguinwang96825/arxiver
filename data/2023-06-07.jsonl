{"title": "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages\n  and Meaning Representations", "abstract": "Cross-Lingual Semantic Parsing (CLSP) aims to translate queries in multiple\nnatural languages (NLs) into meaning representations (MRs) such as SQL, lambda\ncalculus, and logic forms. However, existing CLSP models are separately\nproposed and evaluated on datasets of limited tasks and applications, impeding\na comprehensive and unified evaluation of CLSP on a diverse range of NLs and\nMRs. To this end, we present XSemPLR, a unified benchmark for cross-lingual\nsemantic parsing featured with 22 natural languages and 8 meaning\nrepresentations by examining and selecting 9 existing datasets to cover 5 tasks\nand 164 domains. We use XSemPLR to conduct a comprehensive benchmark study on a\nwide range of multilingual language models including encoder-based models\n(mBERT, XLM-R), encoder-decoder models (mBART, mT5), and decoder-based models\n(Codex, BLOOM). We design 6 experiment settings covering various lingual\ncombinations (monolingual, multilingual, cross-lingual) and numbers of learning\nsamples (full dataset, few-shot, and zero-shot). Our experiments show that\nencoder-decoder models (mT5) achieve the highest performance compared with\nother popular models, and multilingual training can further improve the average\nperformance. Notably, multilingual large language models (e.g., BLOOM) are\nstill inadequate to perform CLSP tasks. We also find that the performance gap\nbetween monolingual training and cross-lingual transfer learning is still\nsignificant for multilingual models, though it can be mitigated by\ncross-lingual few-shot training. Our dataset and code are available at\nhttps://github.com/psunlpgroup/XSemPLR.", "published": "2023-06-07 01:09:37", "link": "http://arxiv.org/abs/2306.04085v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge-Augmented Language Model Prompting for Zero-Shot Knowledge\n  Graph Question Answering", "abstract": "Large Language Models (LLMs) are capable of performing zero-shot closed-book\nquestion answering tasks, based on their internal knowledge stored in\nparameters during pre-training. However, such internalized knowledge might be\ninsufficient and incorrect, which could lead LLMs to generate factually wrong\nanswers. Furthermore, fine-tuning LLMs to update their knowledge is expensive.\nTo this end, we propose to augment the knowledge directly in the input of LLMs.\nSpecifically, we first retrieve the relevant facts to the input question from\nthe knowledge graph based on semantic similarities between the question and its\nassociated facts. After that, we prepend the retrieved facts to the input\nquestion in the form of the prompt, which is then forwarded to LLMs to generate\nthe answer. Our framework, Knowledge-Augmented language model PromptING\n(KAPING), requires no model training, thus completely zero-shot. We validate\nthe performance of our KAPING framework on the knowledge graph question\nanswering task, that aims to answer the user's question based on facts over a\nknowledge graph, on which ours outperforms relevant zero-shot baselines by up\nto 48% in average, across multiple LLMs of various sizes.", "published": "2023-06-07 04:15:21", "link": "http://arxiv.org/abs/2306.04136v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Increasing Diversity While Maintaining Accuracy: Text Data Generation\n  with Large Language Models and Human Interventions", "abstract": "Large language models (LLMs) can be used to generate text data for training\nand evaluating other models. However, creating high-quality datasets with LLMs\ncan be challenging. In this work, we explore human-AI partnerships to\nfacilitate high diversity and accuracy in LLM-based text data generation. We\nfirst examine two approaches to diversify text generation: 1) logit\nsuppression, which minimizes the generation of languages that have already been\nfrequently generated, and 2) temperature sampling, which flattens the token\nsampling probability. We found that diversification approaches can increase\ndata diversity but often at the cost of data accuracy (i.e., text and labels\nbeing appropriate for the target domain). To address this issue, we examined\ntwo human interventions, 1) label replacement (LR), correcting misaligned\nlabels, and 2) out-of-scope filtering (OOSF), removing instances that are out\nof the user's domain of interest or to which no considered label applies. With\noracle studies, we found that LR increases the absolute accuracy of models\ntrained with diversified datasets by 14.4%. Moreover, we found that some models\ntrained with data generated with LR interventions outperformed LLM-based\nfew-shot classification. In contrast, OOSF was not effective in increasing\nmodel accuracy, implying the need for future work in human-in-the-loop text\ndata generation.", "published": "2023-06-07 04:27:09", "link": "http://arxiv.org/abs/2306.04140v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From the One, Judge of the Whole: Typed Entailment Graph Construction\n  with Predicate Generation", "abstract": "Entailment Graphs (EGs) have been constructed based on extracted corpora as a\nstrong and explainable form to indicate context-independent entailment\nrelations in natural languages. However, EGs built by previous methods often\nsuffer from the severe sparsity issues, due to limited corpora available and\nthe long-tail phenomenon of predicate distributions. In this paper, we propose\na multi-stage method, Typed Predicate-Entailment Graph Generator (TP-EGG), to\ntackle this problem. Given several seed predicates, TP-EGG builds the graphs by\ngenerating new predicates and detecting entailment relations among them. The\ngenerative nature of TP-EGG helps us leverage the recent advances from large\npretrained language models (PLMs), while avoiding the reliance on carefully\nprepared corpora. Experiments on benchmark datasets show that TP-EGG can\ngenerate high-quality and scale-controllable entailment graphs, achieving\nsignificant in-domain improvement over state-of-the-art EGs and boosting the\nperformance of down-stream inference tasks.", "published": "2023-06-07 05:46:19", "link": "http://arxiv.org/abs/2306.04170v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowing-how & Knowing-that: A New Task for Machine Comprehension of User\n  Manuals", "abstract": "The machine reading comprehension (MRC) of user manuals has huge potential in\ncustomer service. However, current methods have trouble answering complex\nquestions. Therefore, we introduce the Knowing-how & Knowing-that task that\nrequires the model to answer factoid-style, procedure-style, and inconsistent\nquestions about user manuals. We resolve this task by jointly representing the\nsteps and facts in a graph TARA, which supports a unified inference of various\nquestions. Towards a systematical benchmarking study, we design a heuristic\nmethod to automatically parse user manuals into TARAs and build an annotated\ndataset to test the model's ability in answering real-world questions.\nEmpirical results demonstrate that representing user manuals as TARAs is a\ndesired solution for the MRC of user manuals. An in-depth investigation of TARA\nfurther sheds light on the issues and broader impacts of future representations\nof user manuals. We hope our work can move the MRC of user manuals to a more\ncomplex and realistic stage.", "published": "2023-06-07 06:46:56", "link": "http://arxiv.org/abs/2306.04187v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A New Dataset and Empirical Study for Sentence Simplification in Chinese", "abstract": "Sentence Simplification is a valuable technique that can benefit language\nlearners and children a lot. However, current research focuses more on English\nsentence simplification. The development of Chinese sentence simplification is\nrelatively slow due to the lack of data. To alleviate this limitation, this\npaper introduces CSS, a new dataset for assessing sentence simplification in\nChinese. We collect manual simplifications from human annotators and perform\ndata analysis to show the difference between English and Chinese sentence\nsimplifications. Furthermore, we test several unsupervised and zero/few-shot\nlearning methods on CSS and analyze the automatic evaluation and human\nevaluation results. In the end, we explore whether Large Language Models can\nserve as high-quality Chinese sentence simplification systems by evaluating\nthem on CSS.", "published": "2023-06-07 06:47:34", "link": "http://arxiv.org/abs/2306.04188v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Effective Neural Topic Modeling with Embedding Clustering Regularization", "abstract": "Topic models have been prevalent for decades with various applications.\nHowever, existing topic models commonly suffer from the notorious topic\ncollapsing: discovered topics semantically collapse towards each other, leading\nto highly repetitive topics, insufficient topic discovery, and damaged model\ninterpretability. In this paper, we propose a new neural topic model, Embedding\nClustering Regularization Topic Model (ECRTM). Besides the existing\nreconstruction error, we propose a novel Embedding Clustering Regularization\n(ECR), which forces each topic embedding to be the center of a separately\naggregated word embedding cluster in the semantic space. This enables each\nproduced topic to contain distinct word semantics, which alleviates topic\ncollapsing. Regularized by ECR, our ECRTM generates diverse and coherent topics\ntogether with high-quality topic distributions of documents. Extensive\nexperiments on benchmark datasets demonstrate that ECRTM effectively addresses\nthe topic collapsing issue and consistently surpasses state-of-the-art\nbaselines in terms of topic quality, topic distributions of documents, and\ndownstream classification tasks.", "published": "2023-06-07 07:45:38", "link": "http://arxiv.org/abs/2306.04217v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analysis of the Fed's communication by using textual entailment model of\n  Zero-Shot classification", "abstract": "In this study, we analyze documents published by central banks using text\nmining techniques and propose a method to evaluate the policy tone of central\nbanks. Since the monetary policies of major central banks have a broad impact\non financial market trends, the pricing of risky assets, and the real economy,\nmarket participants are attempting to more accurately capture changes in the\noutlook for central banks' future monetary policies. Since the published\ndocuments are also an important tool for the central bank to communicate with\nthe market, they are meticulously elaborated on grammatical syntax and wording,\nand investors are urged to read more accurately about the central bank's policy\nstance. Sentiment analysis on central bank documents has long been carried out,\nbut it has been difficult to interpret the meaning of the documents accurately\nand to explicitly capture even the intentional change in nuance. This study\nattempts to evaluate the implication of the zero-shot text classification\nmethod for an unknown economic environment using the same model. We compare the\ntone of the statements, minutes, press conference transcripts of FOMC meetings,\nand the Fed officials' (chair, vice chair, and Governors) speeches. In\naddition, the minutes of the FOMC meetings were subjected to a phase analysis\nof changes in each policy stance since 1971.", "published": "2023-06-07 09:23:26", "link": "http://arxiv.org/abs/2306.04277v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-Genre Argument Mining: Can Language Models Automatically Fill in\n  Missing Discourse Markers?", "abstract": "Available corpora for Argument Mining differ along several axes, and one of\nthe key differences is the presence (or absence) of discourse markers to signal\nargumentative content. Exploring effective ways to use discourse markers has\nreceived wide attention in various discourse parsing tasks, from which it is\nwell-known that discourse markers are strong indicators of discourse relations.\nTo improve the robustness of Argument Mining systems across different genres,\nwe propose to automatically augment a given text with discourse markers such\nthat all relations are explicitly signaled. Our analysis unveils that popular\nlanguage models taken out-of-the-box fail on this task; however, when\nfine-tuned on a new heterogeneous dataset that we construct (including\nsynthetic and real examples), they perform considerably better. We demonstrate\nthe impact of our approach on an Argument Mining downstream task, evaluated on\ndifferent corpora, showing that language models can be trained to automatically\nfill in discourse markers across different corpora, improving the performance\nof a downstream model in some, but not all, cases. Our proposed approach can\nfurther be employed as an assistive tool for better discourse understanding.", "published": "2023-06-07 10:19:50", "link": "http://arxiv.org/abs/2306.04314v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IUTEAM1 at MEDIQA-Chat 2023: Is simple fine tuning effective for\n  multilayer summarization of clinical conversations?", "abstract": "Clinical conversation summarization has become an important application of\nNatural language Processing. In this work, we intend to analyze summarization\nmodel ensembling approaches, that can be utilized to improve the overall\naccuracy of the generated medical report called chart note. The work starts\nwith a single summarization model creating the baseline. Then leads to an\nensemble of summarization models trained on a separate section of the chart\nnote. This leads to the final approach of passing the generated results to\nanother summarization model in a multi-layer/stage fashion for better coherency\nof the generated text. Our results indicate that although an ensemble of models\nspecialized in each section produces better results, the multi-layer/stage\napproach does not improve accuracy. The code for the above paper is available\nat https://github.com/dhananjay-srivastava/MEDIQA-Chat-2023-iuteam1.git", "published": "2023-06-07 10:47:33", "link": "http://arxiv.org/abs/2306.04328v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Echoes from Alexandria: A Large Resource for Multilingual Book\n  Summarization", "abstract": "In recent years, research in text summarization has mainly focused on the\nnews domain, where texts are typically short and have strong layout features.\nThe task of full-book summarization presents additional challenges which are\nhard to tackle with current resources, due to their limited size and\navailability in English only. To overcome these limitations, we present \"Echoes\nfrom Alexandria\", or in shortened form, \"Echoes\", a large resource for\nmultilingual book summarization. Echoes features three novel datasets: i)\nEcho-Wiki, for multilingual book summarization, ii) Echo-XSum, for\nextremely-compressive multilingual book summarization, and iii) Echo-FairySum,\nfor extractive book summarization. To the best of our knowledge, Echoes, with\nits thousands of books and summaries, is the largest resource, and the first to\nbe multilingual, featuring 5 languages and 25 language pairs. In addition to\nEchoes, we also introduce a new extractive-then-abstractive baseline, and,\nsupported by our experimental results and manual analysis of the summaries\ngenerated, we argue that this baseline is more suitable for book summarization\nthan purely-abstractive approaches. We release our resource and software at\nhttps://github.com/Babelscape/echoes-from-alexandria in the hope of fostering\ninnovative research in multilingual book summarization.", "published": "2023-06-07 11:01:39", "link": "http://arxiv.org/abs/2306.04334v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A study on the impact of Self-Supervised Learning on automatic\n  dysarthric speech assessment", "abstract": "Automating dysarthria assessments offers the opportunity to develop\npractical, low-cost tools that address the current limitations of manual and\nsubjective assessments. Nonetheless, the small size of most dysarthria datasets\nmakes it challenging to develop automated assessment. Recent research showed\nthat speech representations from models pre-trained on large unlabelled data\ncan enhance Automatic Speech Recognition (ASR) performance for dysarthric\nspeech. We are the first to evaluate the representations from pre-trained\nstate-of-the-art Self-Supervised models across three downstream tasks on\ndysarthric speech: disease classification, word recognition and intelligibility\nclassification, and under three noise scenarios on the UA-Speech dataset. We\nshow that HuBERT is the most versatile feature extractor across dysarthria\nclassification, word recognition, and intelligibility classification, achieving\nrespectively $+24.7\\%, +61\\%, \\text{and} +7.2\\%$ accuracy compared to classical\nacoustic features.", "published": "2023-06-07 11:04:02", "link": "http://arxiv.org/abs/2306.04337v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "World Models for Math Story Problems", "abstract": "Solving math story problems is a complex task for students and NLP models\nalike, requiring them to understand the world as described in the story and\nreason over it to compute an answer. Recent years have seen impressive\nperformance on automatically solving these problems with large pre-trained\nlanguage models and innovative techniques to prompt them. However, it remains\nunclear if these models possess accurate representations of mathematical\nconcepts. This leads to lack of interpretability and trustworthiness which\nimpedes their usefulness in various applications. In this paper, we consolidate\nprevious work on categorizing and representing math story problems and develop\nMathWorld, which is a graph-based semantic formalism specific for the domain of\nmath story problems. With MathWorld, we can assign world models to math story\nproblems which represent the situations and actions introduced in the text and\ntheir mathematical relationships. We combine math story problems from several\nexisting datasets and annotate a corpus of 1,019 problems and 3,204 logical\nforms with MathWorld. Using this data, we demonstrate the following use cases\nof MathWorld: (1) prompting language models with synthetically generated\nquestion-answer pairs to probe their reasoning and world modeling abilities,\nand (2) generating new problems by using the world models as a design space.", "published": "2023-06-07 11:25:20", "link": "http://arxiv.org/abs/2306.04347v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transfer Learning of Transformer-based Speech Recognition Models from\n  Czech to Slovak", "abstract": "In this paper, we are comparing several methods of training the Slovak speech\nrecognition models based on the Transformers architecture. Specifically, we are\nexploring the approach of transfer learning from the existing Czech pre-trained\nWav2Vec 2.0 model into Slovak. We are demonstrating the benefits of the\nproposed approach on three Slovak datasets. Our Slovak models scored the best\nresults when initializing the weights from the Czech model at the beginning of\nthe pre-training phase. Our results show that the knowledge stored in the Cezch\npre-trained model can be successfully reused to solve tasks in Slovak while\noutperforming even much larger public multilingual models.", "published": "2023-06-07 12:58:46", "link": "http://arxiv.org/abs/2306.04399v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Examining Bias in Opinion Summarisation Through the Perspective of\n  Opinion Diversity", "abstract": "Opinion summarisation is a task that aims to condense the information\npresented in the source documents while retaining the core message and\nopinions. A summary that only represents the majority opinions will leave the\nminority opinions unrepresented in the summary. In this paper, we use the\nstance towards a certain target as an opinion. We study bias in opinion\nsummarisation from the perspective of opinion diversity, which measures whether\nthe model generated summary can cover a diverse set of opinions. In addition,\nwe examine opinion similarity, a measure of how closely related two opinions\nare in terms of their stance on a given topic, and its relationship with\nopinion diversity. Through the lens of stances towards a topic, we examine\nopinion diversity and similarity using three debatable topics under COVID-19.\nExperimental results on these topics revealed that a higher degree of\nsimilarity of opinions did not indicate good diversity or fairly cover the\nvarious opinions originally presented in the source documents. We found that\nBART and ChatGPT can better capture diverse opinions presented in the source\ndocuments.", "published": "2023-06-07 13:31:02", "link": "http://arxiv.org/abs/2306.04424v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "STEPS: A Benchmark for Order Reasoning in Sequential Tasks", "abstract": "Various human activities can be abstracted into a sequence of actions in\nnatural text, i.e. cooking, repairing, manufacturing, etc. Such action\nsequences heavily depend on the executing order, while disorder in action\nsequences leads to failure of further task execution by robots or AI agents.\nTherefore, to verify the order reasoning capability of current neural models in\nsequential tasks, we propose a challenging benchmark , named STEPS. STEPS\ninvolves two subtask settings, focusing on determining the rationality of given\nnext step in recipes and selecting the reasonable step from the multi-choice\nquestion, respectively. We describe the data construction and task\nformulations, and benchmark most of significant Large Language Models (LLMs).\nThe experimental results demonstrate 1) The commonsense reasoning of action\norders in sequential tasks are challenging to resolve via zero-shot prompting\nor few-shot in-context learning for LLMs; 2) Prompting method still\nsignificantly lags behind tuning-based method on STEPS.", "published": "2023-06-07 13:58:55", "link": "http://arxiv.org/abs/2306.04441v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can current NLI systems handle German word order? Investigating language\n  model performance on a new German challenge set of minimal pairs", "abstract": "Compared to English, German word order is freer and therefore poses\nadditional challenges for natural language inference (NLI). We create WOGLI\n(Word Order in German Language Inference), the first adversarial NLI dataset\nfor German word order that has the following properties: (i) each premise has\nan entailed and a non-entailed hypothesis; (ii) premise and hypotheses differ\nonly in word order and necessary morphological changes to mark case and number.\nIn particular, each premise andits two hypotheses contain exactly the same\nlemmata. Our adversarial examples require the model to use morphological\nmarkers in order to recognise or reject entailment. We show that current German\nautoencoding models fine-tuned on translated NLI data can struggle on this\nchallenge set, reflecting the fact that translated NLI datasets will not mirror\nall necessary language phenomena in the target language. We also examine\nperformance after data augmentation as well as on related word order phenomena\nderived from WOGLI. Our datasets are publically available at\nhttps://github.com/ireinig/wogli.", "published": "2023-06-07 15:33:07", "link": "http://arxiv.org/abs/2306.04523v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lenient Evaluation of Japanese Speech Recognition: Modeling Naturally\n  Occurring Spelling Inconsistency", "abstract": "Word error rate (WER) and character error rate (CER) are standard metrics in\nSpeech Recognition (ASR), but one problem has always been alternative\nspellings: If one's system transcribes adviser whereas the ground truth has\nadvisor, this will count as an error even though the two spellings really\nrepresent the same word.\n  Japanese is notorious for ``lacking orthography'': most words can be spelled\nin multiple ways, presenting a problem for accurate ASR evaluation. In this\npaper we propose a new lenient evaluation metric as a more defensible CER\nmeasure for Japanese ASR. We create a lattice of plausible respellings of the\nreference transcription, using a combination of lexical resources, a Japanese\ntext-processing system, and a neural machine translation model for\nreconstructing kanji from hiragana or katakana. In a manual evaluation, raters\nrated 95.4% of the proposed spelling variants as plausible. ASR results show\nthat our method, which does not penalize the system for choosing a valid\nalternate spelling of a word, affords a 2.4%-3.1% absolute reduction in CER\ndepending on the task.", "published": "2023-06-07 15:39:02", "link": "http://arxiv.org/abs/2306.04530v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PromptAttack: Probing Dialogue State Trackers with Adversarial Prompts", "abstract": "A key component of modern conversational systems is the Dialogue State\nTracker (or DST), which models a user's goals and needs. Toward building more\nrobust and reliable DSTs, we introduce a prompt-based learning approach to\nautomatically generate effective adversarial examples to probe DST models. Two\nkey characteristics of this approach are: (i) it only needs the output of the\nDST with no need for model parameters, and (ii) it can learn to generate\nnatural language utterances that can target any DST. Through experiments over\nstate-of-the-art DSTs, the proposed framework leads to the greatest reduction\nin accuracy and the best attack success rate while maintaining good fluency and\na low perturbation ratio. We also show how much the generated adversarial\nexamples can bolster a DST through adversarial training. These results indicate\nthe strength of prompt-based attacks on DSTs and leave open avenues for\ncontinued refinement.", "published": "2023-06-07 15:41:40", "link": "http://arxiv.org/abs/2306.04535v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Long-form analogies generated by chatGPT lack human-like\n  psycholinguistic properties", "abstract": "Psycholinguistic analyses provide a means of evaluating large language model\n(LLM) output and making systematic comparisons to human-generated text. These\nmethods can be used to characterize the psycholinguistic properties of LLM\noutput and illustrate areas where LLMs fall short in comparison to\nhuman-generated text. In this work, we apply psycholinguistic methods to\nevaluate individual sentences from long-form analogies about biochemical\nconcepts. We compare analogies generated by human subjects enrolled in\nintroductory biochemistry courses to analogies generated by chatGPT. We perform\na supervised classification analysis using 78 features extracted from\nCoh-metrix that analyze text cohesion, language, and readability (Graesser et.\nal., 2004). Results illustrate high performance for classifying\nstudent-generated and chatGPT-generated analogies. To evaluate which features\ncontribute most to model performance, we use a hierarchical clustering\napproach. Results from this analysis illustrate several linguistic differences\nbetween the two sources.", "published": "2023-06-07 15:42:31", "link": "http://arxiv.org/abs/2306.04537v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Gender, names and other mysteries: Towards the ambiguous for\n  gender-inclusive translation", "abstract": "The vast majority of work on gender in MT focuses on 'unambiguous' inputs,\nwhere gender markers in the source language are expected to be resolved in the\noutput. Conversely, this paper explores the widespread case where the source\nsentence lacks explicit gender markers, but the target sentence contains them\ndue to richer grammatical gender. We particularly focus on inputs containing\nperson names.\n  Investigating such sentence pairs casts a new light on research into MT\ngender bias and its mitigation. We find that many name-gender co-occurrences in\nMT data are not resolvable with 'unambiguous gender' in the source language,\nand that gender-ambiguous examples can make up a large proportion of training\nexamples. From this, we discuss potential steps toward gender-inclusive\ntranslation which accepts the ambiguity in both gender and translation.", "published": "2023-06-07 16:21:59", "link": "http://arxiv.org/abs/2306.04573v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prompter: Zero-shot Adaptive Prefixes for Dialogue State Tracking Domain\n  Adaptation", "abstract": "A challenge in the Dialogue State Tracking (DST) field is adapting models to\nnew domains without using any supervised data, zero-shot domain adaptation.\nParameter-Efficient Transfer Learning (PETL) has the potential to address this\nproblem due to its robustness. However, it has yet to be applied to the\nzero-shot scenarios, as it is not clear how to apply it unsupervisedly.\n  Our method, Prompter, uses descriptions of target domain slots to generate\ndynamic prefixes that are concatenated to the key and values at each layer's\nself-attention mechanism. This allows for the use of prefix-tuning in\nzero-shot. Prompter outperforms previous methods on both the MultiWOZ and SGD\nbenchmarks. In generating prefixes, our analyses find that Prompter not only\nutilizes the semantics of slot descriptions but also how often the slots appear\ntogether in conversation. Moreover, Prompter's gains are due to its improved\nability to distinguish \"none\"-valued dialogue slots, compared against\nbaselines.", "published": "2023-06-07 18:39:57", "link": "http://arxiv.org/abs/2306.04724v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Far Can Camels Go? Exploring the State of Instruction Tuning on Open\n  Resources", "abstract": "In this work we explore recent advances in instruction-tuning language models\non a range of open instruction-following datasets. Despite recent claims that\nopen models can be on par with state-of-the-art proprietary models, these\nclaims are often accompanied by limited evaluation, making it difficult to\ncompare models across the board and determine the utility of various resources.\nWe provide a large set of instruction-tuned models from 6.7B to 65B parameters\nin size, trained on 12 instruction datasets ranging from manually curated\n(e.g., OpenAssistant) to synthetic and distilled (e.g., Alpaca) and\nsystematically evaluate them on their factual knowledge, reasoning,\nmultilinguality, coding, and open-ended instruction following abilities through\na collection of automatic, model-based, and human-based metrics. We further\nintroduce T\\\"ulu, our best performing instruction-tuned model suite finetuned\non a combination of high-quality open resources. Our experiments show that\ndifferent instruction-tuning datasets can uncover or enhance specific skills,\nwhile no single dataset (or combination) provides the best performance across\nall evaluations. Interestingly, we find that model and human preference-based\nevaluations fail to reflect differences in model capabilities exposed by\nbenchmark-based evaluations, suggesting the need for the type of systemic\nevaluation performed in this work. Our evaluations show that the best model in\nany given evaluation reaches on average 87% of ChatGPT performance, and 73% of\nGPT-4 performance, suggesting that further investment in building better base\nmodels and instruction-tuning data is required to close the gap. We release our\ninstruction-tuned models, including a fully finetuned 65B T\\\"ulu, along with\nour code, data, and evaluation framework at\nhttps://github.com/allenai/open-instruct to facilitate future research.", "published": "2023-06-07 19:59:23", "link": "http://arxiv.org/abs/2306.04751v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Good Data, Large Data, or No Data? Comparing Three Approaches in\n  Developing Research Aspect Classifiers for Biomedical Papers", "abstract": "The rapid growth of scientific publications, particularly during the COVID-19\npandemic, emphasizes the need for tools to help researchers efficiently\ncomprehend the latest advancements. One essential part of understanding\nscientific literature is research aspect classification, which categorizes\nsentences in abstracts to Background, Purpose, Method, and Finding. In this\nstudy, we investigate the impact of different datasets on model performance for\nthe crowd-annotated CODA-19 research aspect classification task. Specifically,\nwe explore the potential benefits of using the large, automatically curated\nPubMed 200K RCT dataset and evaluate the effectiveness of large language models\n(LLMs), such as LLaMA, GPT-3, ChatGPT, and GPT-4. Our results indicate that\nusing the PubMed 200K RCT dataset does not improve performance for the CODA-19\ntask. We also observe that while GPT-4 performs well, it does not outperform\nthe SciBERT model fine-tuned on the CODA-19 dataset, emphasizing the importance\nof a dedicated and task-aligned datasets dataset for the target task. Our code\nis available at https://github.com/Crowd-AI-Lab/CODA-19-exp.", "published": "2023-06-07 22:56:53", "link": "http://arxiv.org/abs/2306.04820v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data Augmentation for Improving Tail-traffic Robustness in Skill-routing\n  for Dialogue Systems", "abstract": "Large-scale conversational systems typically rely on a skill-routing\ncomponent to route a user request to an appropriate skill and interpretation to\nserve the request. In such system, the agent is responsible for serving\nthousands of skills and interpretations which create a long-tail distribution\ndue to the natural frequency of requests. For example, the samples related to\nplay music might be a thousand times more frequent than those asking for\ntheatre show times. Moreover, inputs used for ML-based skill routing are often\na heterogeneous mix of strings, embedding vectors, categorical and scalar\nfeatures which makes employing augmentation-based long-tail learning approaches\nchallenging. To improve the skill-routing robustness, we propose an\naugmentation of heterogeneous skill-routing data and training targeted for\nrobust operation in long-tail data regimes. We explore a variety of conditional\nencoder-decoder generative frameworks to perturb original data fields and\ncreate synthetic training data. To demonstrate the effectiveness of the\nproposed method, we conduct extensive experiments using real-world data from a\ncommercial conversational system. Based on the experiment results, the proposed\napproach improves more than 80% (51 out of 63) of intents with less than 10K of\ntraffic instances in the skill-routing replication task.", "published": "2023-06-07 23:07:23", "link": "http://arxiv.org/abs/2306.04823v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Gotta: Generative Few-shot Question Answering by Prompt-based Cloze Data\n  Augmentation", "abstract": "Few-shot question answering (QA) aims at precisely discovering answers to a\nset of questions from context passages while only a few training samples are\navailable. Although existing studies have made some progress and can usually\nachieve proper results, they suffer from understanding deep semantics for\nreasoning out the questions. In this paper, we develop Gotta, a Generative\nprOmpT-based daTa Augmentation framework to mitigate the challenge above.\nInspired by the human reasoning process, we propose to integrate the cloze task\nto enhance few-shot QA learning. Following the recent success of prompt-tuning,\nwe present the cloze task in the same format as the main QA task, allowing the\nmodel to learn both tasks seamlessly together to fully take advantage of the\npower of prompt-tuning. Extensive experiments on widely used benchmarks\ndemonstrate that Gotta consistently outperforms competitive baselines,\nvalidating the effectiveness of our proposed prompt-tuning-based cloze task,\nwhich not only fine-tunes language models but also learns to guide reasoning in\nQA tasks. Further analysis shows that the prompt-based loss incorporates the\nauxiliary task better than the multi-task loss, highlighting the strength of\nprompt-tuning on the few-shot QA task.", "published": "2023-06-07 01:44:43", "link": "http://arxiv.org/abs/2306.04101v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unbalanced Optimal Transport for Unbalanced Word Alignment", "abstract": "Monolingual word alignment is crucial to model semantic interactions between\nsentences. In particular, null alignment, a phenomenon in which words have no\ncorresponding counterparts, is pervasive and critical in handling semantically\ndivergent sentences. Identification of null alignment is useful on its own to\nreason about the semantic similarity of sentences by indicating there exists\ninformation inequality. To achieve unbalanced word alignment that values both\nalignment and null alignment, this study shows that the family of optimal\ntransport (OT), i.e., balanced, partial, and unbalanced OT, are natural and\npowerful approaches even without tailor-made techniques. Our extensive\nexperiments covering unsupervised and supervised settings indicate that our\ngeneric OT-based alignment methods are competitive against the\nstate-of-the-arts specially designed for word alignment, remarkably on\nchallenging datasets with high null alignment frequencies.", "published": "2023-06-07 03:03:41", "link": "http://arxiv.org/abs/2306.04116v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "When to Read Documents or QA History: On Unified and Selective\n  Open-domain QA", "abstract": "This paper studies the problem of open-domain question answering, with the\naim of answering a diverse range of questions leveraging knowledge resources.\nTwo types of sources, QA-pair and document corpora, have been actively\nleveraged with the following complementary strength. The former is highly\nprecise when the paraphrase of given question $q$ was seen and answered during\ntraining, often posed as a retrieval problem, while the latter generalizes\nbetter for unseen questions. A natural follow-up is thus leveraging both\nmodels, while a naive pipelining or integration approaches have failed to bring\nadditional gains over either model alone. Our distinction is interpreting the\nproblem as calibration, which estimates the confidence of predicted answers as\nan indicator to decide when to use a document or QA-pair corpus. The\neffectiveness of our method was validated on widely adopted benchmarks such as\nNatural Questions and TriviaQA.", "published": "2023-06-07 06:03:39", "link": "http://arxiv.org/abs/2306.04176v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Benchmarking Foundation Models with Language-Model-as-an-Examiner", "abstract": "Numerous benchmarks have been established to assess the performance of\nfoundation models on open-ended question answering, which serves as a\ncomprehensive test of a model's ability to understand and generate language in\na manner similar to humans. Most of these works focus on proposing new\ndatasets, however, we see two main issues within previous benchmarking\npipelines, namely testing leakage and evaluation automation. In this paper, we\npropose a novel benchmarking framework, Language-Model-as-an-Examiner, where\nthe LM serves as a knowledgeable examiner that formulates questions based on\nits knowledge and evaluates responses in a reference-free manner. Our framework\nallows for effortless extensibility as various LMs can be adopted as the\nexaminer, and the questions can be constantly updated given more diverse\ntrigger topics. For a more comprehensive and equitable evaluation, we devise\nthree strategies: (1) We instruct the LM examiner to generate questions across\na multitude of domains to probe for a broad acquisition, and raise follow-up\nquestions to engage in a more in-depth assessment. (2) Upon evaluation, the\nexaminer combines both scoring and ranking measurements, providing a reliable\nresult as it aligns closely with human annotations. (3) We additionally propose\na decentralized Peer-examination method to address the biases in a single\nexaminer. Our data and benchmarking results are available at:\nhttp://lmexam.xlore.cn.", "published": "2023-06-07 06:29:58", "link": "http://arxiv.org/abs/2306.04181v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Leveraging Knowledge Graph Embeddings to Enhance Contextual\n  Representations for Relation Extraction", "abstract": "Relation extraction task is a crucial and challenging aspect of Natural\nLanguage Processing. Several methods have surfaced as of late, exhibiting\nnotable performance in addressing the task; however, most of these approaches\nrely on vast amounts of data from large-scale knowledge graphs or language\nmodels pretrained on voluminous corpora. In this paper, we hone in on the\neffective utilization of solely the knowledge supplied by a corpus to create a\nhigh-performing model. Our objective is to showcase that by leveraging the\nhierarchical structure and relational distribution of entities within a corpus\nwithout introducing external knowledge, a relation extraction model can achieve\nsignificantly enhanced performance. We therefore proposed a relation extraction\napproach based on the incorporation of pretrained knowledge graph embeddings at\nthe corpus scale into the sentence-level contextual representation. We\nconducted a series of experiments which revealed promising and very interesting\nresults for our proposed approach.The obtained results demonstrated an\noutperformance of our method compared to context-based relation extraction\nmodels.", "published": "2023-06-07 07:15:20", "link": "http://arxiv.org/abs/2306.04203v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Co-evolving Graph Reasoning Network for Emotion-Cause Pair Extraction", "abstract": "Emotion-Cause Pair Extraction (ECPE) aims to extract all emotion clauses and\ntheir corresponding cause clauses from a document. Existing approaches tackle\nthis task through multi-task learning (MTL) framework in which the two subtasks\nprovide indicative clues for ECPE. However, the previous MTL framework\nconsiders only one round of multi-task reasoning and ignores the reverse\nfeedbacks from ECPE to the subtasks. Besides, its multi-task reasoning only\nrelies on semantics-level interactions, which cannot capture the explicit\ndependencies, and both the encoder sharing and multi-task hidden states\nconcatenations can hardly capture the causalities. To solve these issues, we\nfirst put forward a new MTL framework based on Co-evolving Reasoning. It (1)\nmodels the bidirectional feedbacks between ECPE and its subtasks; (2) allows\nthe three tasks to evolve together and prompt each other recurrently; (3)\nintegrates prediction-level interactions to capture explicit dependencies. Then\nwe propose a novel multi-task relational graph (MRG) to sufficiently exploit\nthe causal relations. Finally, we propose a Co-evolving Graph Reasoning Network\n(CGR-Net) that implements our MTL framework and conducts Co-evolving Reasoning\non MRG. Experimental results show that our model achieves new state-of-the-art\nperformance, and further analysis confirms the advantages of our method.", "published": "2023-06-07 11:11:12", "link": "http://arxiv.org/abs/2306.04340v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GPT Self-Supervision for a Better Data Annotator", "abstract": "The task of annotating data into concise summaries poses a significant\nchallenge across various domains, frequently requiring the allocation of\nsignificant time and specialized knowledge by human experts. Despite existing\nefforts to use large language models for annotation tasks, significant problems\nsuch as limited applicability to unlabeled data, the absence of self-supervised\nmethods, and the lack of focus on complex structured data still persist. In\nthis work, we propose a GPT self-supervision annotation method, which embodies\na generating-recovering paradigm that leverages the one-shot learning\ncapabilities of the Generative Pretrained Transformer (GPT). The proposed\napproach comprises a one-shot tuning phase followed by a generation phase. In\nthe one-shot tuning phase, we sample a data from the support set as part of the\nprompt for GPT to generate a textual summary, which is then used to recover the\noriginal data. The alignment score between the recovered and original data\nserves as a self-supervision navigator to refine the process. In the generation\nstage, the optimally selected one-shot sample serves as a template in the\nprompt and is applied to generating summaries from challenging datasets. The\nannotation performance is evaluated by tuning several human feedback reward\nnetworks and by calculating alignment scores between original and recovered\ndata at both sentence and structure levels. Our self-supervised annotation\nmethod consistently achieves competitive scores, convincingly demonstrating its\nrobust strength in various data-to-summary annotation tasks.", "published": "2023-06-07 11:33:14", "link": "http://arxiv.org/abs/2306.04349v2", "categories": ["cs.CL", "cs.DB"], "primary_category": "cs.CL"}
{"title": "Dial-MAE: ConTextual Masked Auto-Encoder for Retrieval-based Dialogue\n  Systems", "abstract": "Dialogue response selection aims to select an appropriate response from\nseveral candidates based on a given user and system utterance history. Most\nexisting works primarily focus on post-training and fine-tuning tailored for\ncross-encoders. However, there are no post-training methods tailored for dense\nencoders in dialogue response selection. We argue that when the current\nlanguage model, based on dense dialogue systems (such as BERT), is employed as\na dense encoder, it separately encodes dialogue context and response, leading\nto a struggle to achieve the alignment of both representations. Thus, we\npropose Dial-MAE (Dialogue Contextual Masking Auto-Encoder), a straightforward\nyet effective post-training technique tailored for dense encoders in dialogue\nresponse selection. Dial-MAE uses an asymmetric encoder-decoder architecture to\ncompress the dialogue semantics into dense vectors, which achieves better\nalignment between the features of the dialogue context and response. Our\nexperiments have demonstrated that Dial-MAE is highly effective, achieving\nstate-of-the-art performance on two commonly evaluated benchmarks.", "published": "2023-06-07 11:40:07", "link": "http://arxiv.org/abs/2306.04357v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Youku-mPLUG: A 10 Million Large-scale Chinese Video-Language Dataset for\n  Pre-training and Benchmarks", "abstract": "To promote the development of Vision-Language Pre-training (VLP) and\nmultimodal Large Language Model (LLM) in the Chinese community, we firstly\nrelease the largest public Chinese high-quality video-language dataset named\nYouku-mPLUG, which is collected from Youku, a well-known Chinese video-sharing\nwebsite, with strict criteria of safety, diversity, and quality. Youku-mPLUG\ncontains 10 million Chinese video-text pairs filtered from 400 million raw\nvideos across a wide range of 45 diverse categories for large-scale\npre-training. In addition, to facilitate a comprehensive evaluation of\nvideo-language models, we carefully build the largest human-annotated Chinese\nbenchmarks covering three popular video-language tasks of cross-modal\nretrieval, video captioning, and video category classification. Youku-mPLUG can\nenable researchers to conduct more in-depth multimodal research and develop\nbetter applications in the future. Furthermore, we release popular\nvideo-language pre-training models, ALPRO and mPLUG-2, and our proposed\nmodularized decoder-only model mPLUG-video pre-trained on Youku-mPLUG.\nExperiments show that models pre-trained on Youku-mPLUG gain up to 23.1%\nimprovement in video category classification. Besides, mPLUG-video achieves a\nnew state-of-the-art result on these benchmarks with 80.5% top-1 accuracy in\nvideo category classification and 68.9 CIDEr score in video captioning,\nrespectively. Finally, we scale up mPLUG-video based on the frozen Bloomz with\nonly 1.7% trainable parameters as Chinese multimodal LLM, and demonstrate\nimpressive instruction and video understanding ability. The zero-shot\ninstruction understanding experiment indicates that pretraining with\nYouku-mPLUG can enhance the ability to comprehend overall and detailed visual\nsemantics, recognize scene text, and leverage open-domain knowledge.", "published": "2023-06-07 11:52:36", "link": "http://arxiv.org/abs/2306.04362v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "M$^3$IT: A Large-Scale Dataset towards Multi-Modal Multilingual\n  Instruction Tuning", "abstract": "Instruction tuning has significantly advanced large language models (LLMs)\nsuch as ChatGPT, enabling them to align with human instructions across diverse\ntasks. However, progress in open vision-language models (VLMs) has been limited\ndue to the scarcity of high-quality instruction datasets. To tackle this\nchallenge and promote research in the vision-language field, we introduce the\nMulti-Modal, Multilingual Instruction Tuning (M$^3$IT) dataset, designed to\noptimize VLM alignment with human instructions. Our M$^3$IT dataset comprises\n40 carefully curated datasets, including 2.4 million instances and 400 manually\nwritten task instructions, reformatted into a vision-to-text structure. Key\ntasks are translated into 80 languages with an advanced translation system,\nensuring broader accessibility. M$^3$IT surpasses previous datasets regarding\ntask coverage, instruction number and instance scale. Moreover, we develop\nYing-VLM, a VLM model trained on our M$^3$IT dataset, showcasing its potential\nto answer complex questions requiring world knowledge, generalize to unseen\nvideo tasks, and comprehend unseen instructions in Chinese. We have\nopen-sourced the dataset to encourage further research.", "published": "2023-06-07 12:35:37", "link": "http://arxiv.org/abs/2306.04387v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Evaluation of ChatGPT on Biomedical Tasks: A Zero-Shot Comparison with\n  Fine-Tuned Generative Transformers", "abstract": "ChatGPT is a large language model developed by OpenAI. Despite its impressive\nperformance across various tasks, no prior work has investigated its capability\nin the biomedical domain yet. To this end, this paper aims to evaluate the\nperformance of ChatGPT on various benchmark biomedical tasks, such as relation\nextraction, document classification, question answering, and summarization. To\nthe best of our knowledge, this is the first work that conducts an extensive\nevaluation of ChatGPT in the biomedical domain. Interestingly, we find based on\nour evaluation that in biomedical datasets that have smaller training sets,\nzero-shot ChatGPT even outperforms the state-of-the-art fine-tuned generative\ntransformer models, such as BioGPT and BioBART. This suggests that ChatGPT's\npre-training on large text corpora makes it quite specialized even in the\nbiomedical domain. Our findings demonstrate that ChatGPT has the potential to\nbe a valuable tool for various tasks in the biomedical domain that lack large\nannotated data.", "published": "2023-06-07 15:11:26", "link": "http://arxiv.org/abs/2306.04504v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Enhancing In-Context Learning with Answer Feedback for Multi-Span\n  Question Answering", "abstract": "Whereas the recent emergence of large language models (LLMs) like ChatGPT has\nexhibited impressive general performance, it still has a large gap with\nfully-supervised models on specific tasks such as multi-span question\nanswering. Previous researches found that in-context learning is an effective\napproach to exploiting LLM, by using a few task-related labeled data as\ndemonstration examples to construct a few-shot prompt for answering new\nquestions. A popular implementation is to concatenate a few questions and their\ncorrect answers through simple templates, informing LLM of the desired output.\nIn this paper, we propose a novel way of employing labeled data such that it\nalso informs LLM of some undesired output, by extending demonstration examples\nwith feedback about answers predicted by an off-the-shelf model, e.g., correct,\nincorrect, or incomplete. Experiments on three multi-span question answering\ndatasets as well as a keyphrase extraction dataset show that our new prompting\nstrategy consistently improves LLM's in-context learning performance.", "published": "2023-06-07 15:20:24", "link": "http://arxiv.org/abs/2306.04508v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Contrastive Bootstrapping for Label Refinement", "abstract": "Traditional text classification typically categorizes texts into pre-defined\ncoarse-grained classes, from which the produced models cannot handle the\nreal-world scenario where finer categories emerge periodically for accurate\nservices. In this work, we investigate the setting where fine-grained\nclassification is done only using the annotation of coarse-grained categories\nand the coarse-to-fine mapping. We propose a lightweight contrastive\nclustering-based bootstrapping method to iteratively refine the labels of\npassages. During clustering, it pulls away negative passage-prototype pairs\nunder the guidance of the mapping from both global and local perspectives.\nExperiments on NYT and 20News show that our method outperforms the\nstate-of-the-art methods by a large margin.", "published": "2023-06-07 15:49:04", "link": "http://arxiv.org/abs/2306.04544v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi-Task Training with In-Domain Language Models for Diagnostic\n  Reasoning", "abstract": "Generative artificial intelligence (AI) is a promising direction for\naugmenting clinical diagnostic decision support and reducing diagnostic errors,\na leading contributor to medical errors. To further the development of clinical\nAI systems, the Diagnostic Reasoning Benchmark (DR.BENCH) was introduced as a\ncomprehensive generative AI framework, comprised of six tasks representing key\ncomponents in clinical reasoning. We present a comparative analysis of\nin-domain versus out-of-domain language models as well as multi-task versus\nsingle task training with a focus on the problem summarization task in DR.BENCH\n(Gao et al., 2023). We demonstrate that a multi-task, clinically trained\nlanguage model outperforms its general domain counterpart by a large margin,\nestablishing a new state-of-the-art performance, with a ROUGE-L score of 28.55.\nThis research underscores the value of domain-specific training for optimizing\nclinical diagnostic reasoning tasks.", "published": "2023-06-07 15:55:34", "link": "http://arxiv.org/abs/2306.04551v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Language Models Get a Gender Makeover: Mitigating Gender Bias with\n  Few-Shot Data Interventions", "abstract": "Societal biases present in pre-trained large language models are a critical\nissue as these models have been shown to propagate biases in countless\ndownstream applications, rendering them unfair towards specific groups of\npeople. Since large-scale retraining of these models from scratch is both time\nand compute-expensive, a variety of approaches have been previously proposed\nthat de-bias a pre-trained model. While the majority of current\nstate-of-the-art debiasing methods focus on changes to the training regime, in\nthis paper, we propose data intervention strategies as a powerful yet simple\ntechnique to reduce gender bias in pre-trained models. Specifically, we\nempirically show that by fine-tuning a pre-trained model on only 10 de-biased\n(intervened) training examples, the tendency to favor any gender is\nsignificantly reduced. Since our proposed method only needs a few training\nexamples, our few-shot debiasing approach is highly feasible and practical.\nThrough extensive experimentation, we show that our debiasing technique\nperforms better than competitive state-of-the-art baselines with minimal loss\nin language modeling ability.", "published": "2023-06-07 16:50:03", "link": "http://arxiv.org/abs/2306.04597v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Two Word Test: A Semantic Benchmark for Large Language Models", "abstract": "Large Language Models (LLMs) have shown remarkable abilities recently,\nincluding passing advanced professional exams and demanding benchmark tests.\nThis performance has led many to suggest that they are close to achieving\nhumanlike or 'true' understanding of language, and even Artificial General\nIntelligence (AGI). Here, we provide a new open-source benchmark that can\nassess semantic abilities of LLMs using two-word phrases using a task that can\nbe performed relatively easily by humans without advanced training. Combining\nmultiple words into a single concept is a fundamental aspect of human language\nand intelligence. The test requires meaningfulness judgments of 1768 noun-noun\ncombinations that have been rated as meaningful (e.g., baby boy) or not\nmeaningful (e.g., goat sky). by 150 human raters. We provide versions of the\ntask that probe meaningfulness ratings on a 0-4 scale as well as binary\njudgments. We conducted a series of experiments using the TWT on GPT-4,\nGPT-3.5, and Bard, with both versions. Results demonstrated that, compared to\nhumans, all models perform poorly at rating meaningfulness of these phrases.\nGPT-3.5 and Bard are also unable to make binary discriminations between\nsensible and nonsense phrases as making sense. GPT-4 makes a substantial\nimprovement in binary discrimination of combinatorial phrases but is still\nsignificantly worse than human performance. The TWT can be used to understand\nthe limitations and weaknesses of current LLMs, and potentially improve them.\nThe test also reminds us that caution is warranted in attributing 'true\nunderstanding' or AGI to LLMs. TWT is available at:\nhttps://github.com/NickRiccardi/two-word-test", "published": "2023-06-07 17:22:03", "link": "http://arxiv.org/abs/2306.04610v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Open Language Models by Learning from Organic Interactions", "abstract": "We present BlenderBot 3x, an update on the conversational model BlenderBot 3,\nwhich is now trained using organic conversation and feedback data from\nparticipating users of the system in order to improve both its skills and\nsafety. We are publicly releasing the participating de-identified interaction\ndata for use by the research community, in order to spur further progress.\nTraining models with organic data is challenging because interactions with\npeople \"in the wild\" include both high quality conversations and feedback, as\nwell as adversarial and toxic behavior. We study techniques that enable\nlearning from helpful teachers while avoiding learning from people who are\ntrying to trick the model into unhelpful or toxic responses. BlenderBot 3x is\nboth preferred in conversation to BlenderBot 3, and is shown to produce safer\nresponses in challenging situations. While our current models are still far\nfrom perfect, we believe further improvement can be achieved by continued use\nof the techniques explored in this work.", "published": "2023-06-07 18:19:46", "link": "http://arxiv.org/abs/2306.04707v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "INSTRUCTEVAL: Towards Holistic Evaluation of Instruction-Tuned Large\n  Language Models", "abstract": "Instruction-tuned large language models have revolutionized natural language\nprocessing and have shown great potential in applications such as\nconversational agents. These models, such as GPT-4, can not only master\nlanguage but also solve complex tasks in areas like mathematics, coding,\nmedicine, and law. Despite their impressive capabilities, there is still a lack\nof comprehensive understanding regarding their full potential, primarily due to\nthe black-box nature of many models and the absence of holistic evaluation\nstudies. To address these challenges, we present INSTRUCTEVAL, a more\ncomprehensive evaluation suite designed specifically for instruction-tuned\nlarge language models. Unlike previous works, our evaluation involves a\nrigorous assessment of models based on problem-solving, writing ability, and\nalignment to human values. We take a holistic approach to analyze various\nfactors affecting model performance, including the pretraining foundation,\ninstruction-tuning data, and training methods. Our findings reveal that the\nquality of instruction data is the most crucial factor in scaling model\nperformance. While open-source models demonstrate impressive writing abilities,\nthere is substantial room for improvement in problem-solving and alignment. We\nare encouraged by the rapid development of models by the open-source community,\nbut we also highlight the need for rigorous evaluation to support claims made\nabout these models. Through INSTRUCTEVAL, we aim to foster a deeper\nunderstanding of instruction-tuned models and advancements in their\ncapabilities. INSTRUCTEVAL is publicly available at\nhttps://github.com/declare-lab/instruct-eval.", "published": "2023-06-07 20:12:29", "link": "http://arxiv.org/abs/2306.04757v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The HCI Aspects of Public Deployment of Research Chatbots: A User Study,\n  Design Recommendations, and Open Challenges", "abstract": "Publicly deploying research chatbots is a nuanced topic involving necessary\nrisk-benefit analyses. While there have recently been frequent discussions on\nwhether it is responsible to deploy such models, there has been far less focus\non the interaction paradigms and design approaches that the resulting\ninterfaces should adopt, in order to achieve their goals more effectively. We\naim to pose, ground, and attempt to answer HCI questions involved in this\nscope, by reporting on a mixed-methods user study conducted on a recent\nresearch chatbot. We find that abstract anthropomorphic representation for the\nagent has a significant effect on user's perception, that offering AI\nexplainability may have an impact on feedback rates, and that two (diegetic and\nextradiegetic) levels of the chat experience should be intentionally designed.\nWe offer design recommendations and areas of further focus for the research\ncommunity.", "published": "2023-06-07 20:24:43", "link": "http://arxiv.org/abs/2306.04765v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Absformer: Transformer-based Model for Unsupervised Multi-Document\n  Abstractive Summarization", "abstract": "Multi-document summarization (MDS) refers to the task of summarizing the text\nin multiple documents into a concise summary. The generated summary can save\nthe time of reading many documents by providing the important content in the\nform of a few sentences. Abstractive MDS aims to generate a coherent and fluent\nsummary for multiple documents using natural language generation techniques. In\nthis paper, we consider the unsupervised abstractive MDS setting where there\nare only documents with no groundtruh summaries provided, and we propose\nAbsformer, a new Transformer-based method for unsupervised abstractive summary\ngeneration. Our method consists of a first step where we pretrain a\nTransformer-based encoder using the masked language modeling (MLM) objective as\nthe pretraining task in order to cluster the documents into semantically\nsimilar groups; and a second step where we train a Transformer-based decoder to\ngenerate abstractive summaries for the clusters of documents. To our knowledge,\nwe are the first to successfully incorporate a Transformer-based model to solve\nthe unsupervised abstractive MDS task. We evaluate our approach using three\nreal-world datasets from different domains, and we demonstrate both substantial\nimprovements in terms of evaluation metrics over state-of-the-art\nabstractive-based methods, and generalization to datasets from different\ndomains.", "published": "2023-06-07 21:18:23", "link": "http://arxiv.org/abs/2306.04787v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Text-only Domain Adaptation using Unified Speech-Text Representation in\n  Transducer", "abstract": "Domain adaptation using text-only corpus is challenging in end-to-end(E2E)\nspeech recognition. Adaptation by synthesizing audio from text through TTS is\nresource-consuming. We present a method to learn Unified Speech-Text\nRepresentation in Conformer Transducer(USTR-CT) to enable fast domain\nadaptation using the text-only corpus. Different from the previous textogram\nmethod, an extra text encoder is introduced in our work to learn text\nrepresentation and is removed during inference, so there is no modification for\nonline deployment. To improve the efficiency of adaptation, single-step and\nmulti-step adaptations are also explored. The experiments on adapting\nLibriSpeech to SPGISpeech show the proposed method reduces the word error\nrate(WER) by relatively 44% on the target domain, which is better than those of\nTTS method and textogram method. Also, it is shown the proposed method can be\ncombined with internal language model estimation(ILME) to further improve the\nperformance.", "published": "2023-06-07 00:33:02", "link": "http://arxiv.org/abs/2306.04076v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Multimodal Fusion Interactions: A Study of Human and Automatic\n  Quantification", "abstract": "In order to perform multimodal fusion of heterogeneous signals, we need to\nunderstand their interactions: how each modality individually provides\ninformation useful for a task and how this information changes in the presence\nof other modalities. In this paper, we perform a comparative study of how\nhumans annotate two categorizations of multimodal interactions: (1) partial\nlabels, where different annotators annotate the label given the first, second,\nand both modalities, and (2) counterfactual labels, where the same annotator\nannotates the label given the first modality before asking them to explicitly\nreason about how their answer changes when given the second. We further propose\nan alternative taxonomy based on (3) information decomposition, where\nannotators annotate the degrees of redundancy: the extent to which modalities\nindividually and together give the same predictions, uniqueness: the extent to\nwhich one modality enables a prediction that the other does not, and synergy:\nthe extent to which both modalities enable one to make a prediction that one\nwould not otherwise make using individual modalities. Through experiments and\nannotations, we highlight several opportunities and limitations of each\napproach and propose a method to automatically convert annotations of partial\nand counterfactual labels to information decomposition, yielding an accurate\nand efficient method for quantifying multimodal interactions.", "published": "2023-06-07 03:44:50", "link": "http://arxiv.org/abs/2306.04125v2", "categories": ["cs.LG", "cs.CL", "cs.HC"], "primary_category": "cs.LG"}
{"title": "Transfer Learning from Pre-trained Language Models Improves End-to-End\n  Speech Summarization", "abstract": "End-to-end speech summarization (E2E SSum) directly summarizes input speech\ninto easy-to-read short sentences with a single model. This approach is\npromising because it, in contrast to the conventional cascade approach, can\nutilize full acoustical information and mitigate to the propagation of\ntranscription errors. However, due to the high cost of collecting\nspeech-summary pairs, an E2E SSum model tends to suffer from training data\nscarcity and output unnatural sentences. To overcome this drawback, we propose\nfor the first time to integrate a pre-trained language model (LM), which is\nhighly capable of generating natural sentences, into the E2E SSum decoder via\ntransfer learning. In addition, to reduce the gap between the independently\npre-trained encoder and decoder, we also propose to transfer the baseline E2E\nSSum encoder instead of the commonly used automatic speech recognition encoder.\nExperimental results show that the proposed model outperforms baseline and data\naugmented models.", "published": "2023-06-07 08:23:58", "link": "http://arxiv.org/abs/2306.04233v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Multi-microphone Automatic Speech Segmentation in Meetings Based on\n  Circular Harmonics Features", "abstract": "Speaker diarization is the task of answering Who spoke and when? in an audio\nstream. Pipeline systems rely on speech segmentation to extract speakers'\nsegments and achieve robust speaker diarization. This paper proposes a common\nframework to solve three segmentation tasks in the distant speech scenario:\nVoice Activity Detection (VAD), Overlapped Speech Detection (OSD), and Speaker\nChange Detection (SCD). In the literature, a few studies investigate the\nmulti-microphone distant speech scenario. In this work, we propose a new set of\nspatial features based on direction-of-arrival estimations in the circular\nharmonic domain (CH-DOA). These spatial features are extracted from\nmulti-microphone audio data and combined with standard acoustic features.\nExperiments on the AMI meeting corpus show that CH-DOA can improve the\nsegmentation while being robust in the case of deactivated microphones.", "published": "2023-06-07 09:09:00", "link": "http://arxiv.org/abs/2306.04268v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Phrase Retrieval for Open-Domain Conversational Question Answering with\n  Conversational Dependency Modeling via Contrastive Learning", "abstract": "Open-Domain Conversational Question Answering (ODConvQA) aims at answering\nquestions through a multi-turn conversation based on a retriever-reader\npipeline, which retrieves passages and then predicts answers with them.\nHowever, such a pipeline approach not only makes the reader vulnerable to the\nerrors propagated from the retriever, but also demands additional effort to\ndevelop both the retriever and the reader, which further makes it slower since\nthey are not runnable in parallel. In this work, we propose a method to\ndirectly predict answers with a phrase retrieval scheme for a sequence of\nwords, reducing the conventional two distinct subtasks into a single one. Also,\nfor the first time, we study its capability for ODConvQA tasks. However, simply\nadopting it is largely problematic, due to the dependencies between previous\nand current turns in a conversation. To address this problem, we further\nintroduce a novel contrastive learning strategy, making sure to reflect\nprevious turns when retrieving the phrase for the current context, by\nmaximizing representational similarities of consecutive turns in a conversation\nwhile minimizing irrelevant conversational contexts. We validate our model on\ntwo ODConvQA datasets, whose experimental results show that it substantially\noutperforms the relevant baselines with the retriever-reader. Code is available\nat: https://github.com/starsuzi/PRO-ConvQA.", "published": "2023-06-07 09:46:38", "link": "http://arxiv.org/abs/2306.04293v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Allophant: Cross-lingual Phoneme Recognition with Articulatory\n  Attributes", "abstract": "This paper proposes Allophant, a multilingual phoneme recognizer. It requires\nonly a phoneme inventory for cross-lingual transfer to a target language,\nallowing for low-resource recognition. The architecture combines a\ncompositional phone embedding approach with individually supervised phonetic\nattribute classifiers in a multi-task architecture. We also introduce\nAllophoible, an extension of the PHOIBLE database. When combined with a\ndistance based mapping approach for grapheme-to-phoneme outputs, it allows us\nto train on PHOIBLE inventories directly. By training and evaluating on 34\nlanguages, we found that the addition of multi-task learning improves the\nmodel's capability of being applied to unseen phonemes and phoneme inventories.\nOn supervised languages we achieve phoneme error rate improvements of 11\npercentage points (pp.) compared to a baseline without multi-task learning.\nEvaluation of zero-shot transfer on 84 languages yielded a decrease in PER of\n2.63 pp. over the baseline.", "published": "2023-06-07 10:11:09", "link": "http://arxiv.org/abs/2306.04306v2", "categories": ["cs.CL", "cs.SD", "eess.AS", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Personality testing of Large Language Models: Limited temporal\n  stability, but highlighted prosociality", "abstract": "As Large Language Models (LLMs) continue to gain popularity due to their\nhuman-like traits and the intimacy they offer to users, their societal impact\ninevitably expands. This leads to the rising necessity for comprehensive\nstudies to fully understand LLMs and reveal their potential opportunities,\ndrawbacks, and overall societal impact. With that in mind, this research\nconducted an extensive investigation into seven LLM's, aiming to assess the\ntemporal stability and inter-rater agreement on their responses on personality\ninstruments in two time points. In addition, LLMs personality profile was\nanalyzed and compared to human normative data. The findings revealed varying\nlevels of inter-rater agreement in the LLMs responses over a short time, with\nsome LLMs showing higher agreement (e.g., LIama3 and GPT-4o) compared to others\n(e.g., GPT-4 and Gemini). Furthermore, agreement depended on used instruments\nas well as on domain or trait. This implies the variable robustness in LLMs'\nability to reliably simulate stable personality characteristics. In the case of\nscales which showed at least fair agreement, LLMs displayed mostly a socially\ndesirable profile in both agentic and communal domains, as well as a prosocial\npersonality profile reflected in higher agreeableness and conscientiousness and\nlower Machiavellianism. Exhibiting temporal stability and coherent responses on\npersonality traits is crucial for AI systems due to their societal impact and\nAI safety concerns.", "published": "2023-06-07 10:14:17", "link": "http://arxiv.org/abs/2306.04308v3", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI"}
{"title": "Arabic Dysarthric Speech Recognition Using Adversarial and Signal-Based\n  Augmentation", "abstract": "Despite major advancements in Automatic Speech Recognition (ASR), the\nstate-of-the-art ASR systems struggle to deal with impaired speech even with\nhigh-resource languages. In Arabic, this challenge gets amplified, with added\ncomplexities in collecting data from dysarthric speakers. In this paper, we aim\nto improve the performance of Arabic dysarthric automatic speech recognition\nthrough a multi-stage augmentation approach. To this effect, we first propose a\nsignal-based approach to generate dysarthric Arabic speech from healthy Arabic\nspeech by modifying its speed and tempo. We also propose a second stage\nParallel Wave Generative (PWG) adversarial model that is trained on an English\ndysarthric dataset to capture language-independant dysarthric speech patterns\nand further augment the signal-adjusted speech samples. Furthermore, we propose\na fine-tuning and text-correction strategies for Arabic Conformer at different\ndysarthric speech severity levels. Our fine-tuned Conformer achieved 18% Word\nError Rate (WER) and 17.2% Character Error Rate (CER) on synthetically\ngenerated dysarthric speech from the Arabic commonvoice speech dataset. This\nshows significant WER improvement of 81.8% compared to the baseline model\ntrained solely on healthy data. We perform further validation on real English\ndysarthric speech showing a WER improvement of 124% compared to the baseline\ntrained only on healthy English LJSpeech dataset.", "published": "2023-06-07 12:01:46", "link": "http://arxiv.org/abs/2306.04368v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Label Aware Speech Representation Learning For Language Identification", "abstract": "Speech representation learning approaches for non-semantic tasks such as\nlanguage recognition have either explored supervised embedding extraction\nmethods using a classifier model or self-supervised representation learning\napproaches using raw data. In this paper, we propose a novel framework of\ncombining self-supervised representation learning with the language label\ninformation for the pre-training task. This framework, termed as Label Aware\nSpeech Representation (LASR) learning, uses a triplet based objective function\nto incorporate language labels along with the self-supervised loss function.\nThe speech representations are further fine-tuned for the downstream task. The\nlanguage recognition experiments are performed on two public datasets - FLEURS\nand Dhwani. In these experiments, we illustrate that the proposed LASR\nframework improves over the state-of-the-art systems on language\nidentification. We also report an analysis of the robustness of LASR approach\nto noisy/missing labels as well as its application to multi-lingual speech\nrecognition tasks.", "published": "2023-06-07 12:14:16", "link": "http://arxiv.org/abs/2306.04374v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Multilingual Clinical NER: Translation or Cross-lingual Transfer?", "abstract": "Natural language tasks like Named Entity Recognition (NER) in the clinical\ndomain on non-English texts can be very time-consuming and expensive due to the\nlack of annotated data. Cross-lingual transfer (CLT) is a way to circumvent\nthis issue thanks to the ability of multilingual large language models to be\nfine-tuned on a specific task in one language and to provide high accuracy for\nthe same task in another language. However, other methods leveraging\ntranslation models can be used to perform NER without annotated data in the\ntarget language, by either translating the training set or test set. This paper\ncompares cross-lingual transfer with these two alternative methods, to perform\nclinical NER in French and in German without any training data in those\nlanguages. To this end, we release MedNERF a medical NER test set extracted\nfrom French drug prescriptions and annotated with the same guidelines as an\nEnglish dataset. Through extensive experiments on this dataset and on a German\nmedical dataset (Frei and Kramer, 2021), we show that translation-based methods\ncan achieve similar performance to CLT but require more care in their design.\nAnd while they can take advantage of monolingual clinical language models,\nthose do not guarantee better results than large general-purpose multilingual\nmodels, whether with cross-lingual transfer or translation.", "published": "2023-06-07 12:31:07", "link": "http://arxiv.org/abs/2306.04384v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Zambezi Voice: A Multilingual Speech Corpus for Zambian Languages", "abstract": "This work introduces Zambezi Voice, an open-source multilingual speech\nresource for Zambian languages. It contains two collections of datasets:\nunlabelled audio recordings of radio news and talk shows programs (160 hours)\nand labelled data (over 80 hours) consisting of read speech recorded from text\nsourced from publicly available literature books. The dataset is created for\nspeech recognition but can be extended to multilingual speech processing\nresearch for both supervised and unsupervised learning approaches. To our\nknowledge, this is the first multilingual speech dataset created for Zambian\nlanguages. We exploit pretraining and cross-lingual transfer learning by\nfinetuning the Wav2Vec2.0 large-scale multilingual pre-trained model to build\nend-to-end (E2E) speech recognition models for our baseline models. The dataset\nis released publicly under a Creative Commons BY-NC-ND 4.0 license and can be\naccessed via https://github.com/unza-speech-lab/zambezi-voice .", "published": "2023-06-07 13:36:37", "link": "http://arxiv.org/abs/2306.04428v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "PromptRobust: Towards Evaluating the Robustness of Large Language Models\n  on Adversarial Prompts", "abstract": "The increasing reliance on Large Language Models (LLMs) across academia and\nindustry necessitates a comprehensive understanding of their robustness to\nprompts. In response to this vital need, we introduce PromptRobust, a\nrobustness benchmark designed to measure LLMs' resilience to adversarial\nprompts. This study uses a plethora of adversarial textual attacks targeting\nprompts across multiple levels: character, word, sentence, and semantic. The\nadversarial prompts, crafted to mimic plausible user errors like typos or\nsynonyms, aim to evaluate how slight deviations can affect LLM outcomes while\nmaintaining semantic integrity. These prompts are then employed in diverse\ntasks including sentiment analysis, natural language inference, reading\ncomprehension, machine translation, and math problem-solving. Our study\ngenerates 4,788 adversarial prompts, meticulously evaluated over 8 tasks and 13\ndatasets. Our findings demonstrate that contemporary LLMs are not robust to\nadversarial prompts. Furthermore, we present a comprehensive analysis to\nunderstand the mystery behind prompt robustness and its transferability. We\nthen offer insightful robustness analysis and pragmatic recommendations for\nprompt composition, beneficial to both researchers and everyday users.", "published": "2023-06-07 15:37:00", "link": "http://arxiv.org/abs/2306.04528v5", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ChatGPT is fun, but it is not funny! Humor is still challenging Large\n  Language Models", "abstract": "Humor is a central aspect of human communication that has not been solved for\nartificial agents so far. Large language models (LLMs) are increasingly able to\ncapture implicit and contextual information. Especially, OpenAI's ChatGPT\nrecently gained immense public attention. The GPT3-based model almost seems to\ncommunicate on a human level and can even tell jokes. Humor is an essential\ncomponent of human communication. But is ChatGPT really funny? We put ChatGPT's\nsense of humor to the test. In a series of exploratory experiments around\njokes, i.e., generation, explanation, and detection, we seek to understand\nChatGPT's capability to grasp and reproduce human humor. Since the model itself\nis not accessible, we applied prompt-based experiments. Our empirical evidence\nindicates that jokes are not hard-coded but mostly also not newly generated by\nthe model. Over 90% of 1008 generated jokes were the same 25 Jokes. The system\naccurately explains valid jokes but also comes up with fictional explanations\nfor invalid jokes. Joke-typical characteristics can mislead ChatGPT in the\nclassification of jokes. ChatGPT has not solved computational humor yet but it\ncan be a big leap toward \"funny\" machines.", "published": "2023-06-07 16:10:21", "link": "http://arxiv.org/abs/2306.04563v1", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Revisiting Out-of-distribution Robustness in NLP: Benchmark, Analysis,\n  and LLMs Evaluations", "abstract": "This paper reexamines the research on out-of-distribution (OOD) robustness in\nthe field of NLP. We find that the distribution shift settings in previous\nstudies commonly lack adequate challenges, hindering the accurate evaluation of\nOOD robustness. To address these issues, we propose a benchmark construction\nprotocol that ensures clear differentiation and challenging distribution\nshifts. Then we introduce BOSS, a Benchmark suite for Out-of-distribution\nrobustneSS evaluation covering 5 tasks and 20 datasets. Based on BOSS, we\nconduct a series of experiments on pre-trained language models for analysis and\nevaluation of OOD robustness. First, for vanilla fine-tuning, we examine the\nrelationship between in-distribution (ID) and OOD performance. We identify\nthree typical types that unveil the inner learning mechanism, which could\npotentially facilitate the forecasting of OOD robustness, correlating with the\nadvancements on ID datasets. Then, we evaluate 5 classic methods on BOSS and\nfind that, despite exhibiting some effectiveness in specific cases, they do not\noffer significant improvement compared to vanilla fine-tuning. Further, we\nevaluate 5 LLMs with various adaptation paradigms and find that when sufficient\nID data is available, fine-tuning domain-specific models outperform LLMs on ID\nexamples significantly. However, in the case of OOD instances, prioritizing\nLLMs with in-context learning yields better results. We identify that both\nfine-tuned small models and LLMs face challenges in effectively addressing\ndownstream tasks. The code is public at\n\\url{https://github.com/lifan-yuan/OOD_NLP}.", "published": "2023-06-07 17:47:03", "link": "http://arxiv.org/abs/2306.04618v2", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On the Reliability of Watermarks for Large Language Models", "abstract": "As LLMs become commonplace, machine-generated text has the potential to flood\nthe internet with spam, social media bots, and valueless content. Watermarking\nis a simple and effective strategy for mitigating such harms by enabling the\ndetection and documentation of LLM-generated text. Yet a crucial question\nremains: How reliable is watermarking in realistic settings in the wild? There,\nwatermarked text may be modified to suit a user's needs, or entirely rewritten\nto avoid detection. We study the robustness of watermarked text after it is\nre-written by humans, paraphrased by a non-watermarked LLM, or mixed into a\nlonger hand-written document. We find that watermarks remain detectable even\nafter human and machine paraphrasing. While these attacks dilute the strength\nof the watermark, paraphrases are statistically likely to leak n-grams or even\nlonger fragments of the original text, resulting in high-confidence detections\nwhen enough tokens are observed. For example, after strong human paraphrasing\nthe watermark is detectable after observing 800 tokens on average, when setting\na 1e-5 false positive rate. We also consider a range of new detection schemes\nthat are sensitive to short spans of watermarked text embedded inside a large\ndocument, and we compare the robustness of watermarking to other kinds of\ndetectors.", "published": "2023-06-07 17:58:48", "link": "http://arxiv.org/abs/2306.04634v4", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "ModuleFormer: Modularity Emerges from Mixture-of-Experts", "abstract": "Large Language Models (LLMs) have achieved remarkable results. However,\nexisting models are expensive to train and deploy, and it is also difficult to\nexpand their knowledge beyond pre-training data without forgetting previous\nknowledge. This paper proposes a new neural network architecture, ModuleFormer,\nthat leverages modularity to improve the efficiency and flexibility of large\nlanguage models. ModuleFormer is based on the Sparse Mixture of Experts (SMoE).\nUnlike the previous SMoE-based modular language model, which requires\ndomain-labeled data to learn domain-specific experts, ModuleFormer can induce\nmodularity from uncurated data with its new load balancing and concentration\nlosses. ModuleFormer is a modular architecture that includes two different\ntypes of modules: new stick-breaking attention heads and feedforward experts.\nDifferent modules are sparsely activated conditions on the input token during\ntraining and inference. In our experiment, we found that the modular\narchitecture enables three important abilities for large pre-trained language\nmodels: 1) Efficiency, since ModuleFormer only activates a subset of its\nmodules for each input token, thus it could achieve the same performance as\ndense LLMs with more than two times throughput; 2) Extendability, ModuleFormer\nis more immune to catastrophic forgetting than dense LLMs and can be easily\nextended with new modules to learn new knowledge that is not included in the\ntraining data; 3) Specialisation, finetuning ModuleFormer could specialize a\nsubset of modules to the finetuning task and the task-unrelated modules could\nbe easily pruned for a lightweight deployment.", "published": "2023-06-07 17:59:57", "link": "http://arxiv.org/abs/2306.04640v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ConceptBed: Evaluating Concept Learning Abilities of Text-to-Image\n  Diffusion Models", "abstract": "The ability to understand visual concepts and replicate and compose these\nconcepts from images is a central goal for computer vision. Recent advances in\ntext-to-image (T2I) models have lead to high definition and realistic image\nquality generation by learning from large databases of images and their\ndescriptions. However, the evaluation of T2I models has focused on photorealism\nand limited qualitative measures of visual understanding. To quantify the\nability of T2I models in learning and synthesizing novel visual concepts\n(a.k.a. personalized T2I), we introduce ConceptBed, a large-scale dataset that\nconsists of 284 unique visual concepts, and 33K composite text prompts. Along\nwith the dataset, we propose an evaluation metric, Concept Confidence Deviation\n(CCD), that uses the confidence of oracle concept classifiers to measure the\nalignment between concepts generated by T2I generators and concepts contained\nin target images. We evaluate visual concepts that are either objects,\nattributes, or styles, and also evaluate four dimensions of compositionality:\ncounting, attributes, relations, and actions. Our human study shows that CCD is\nhighly correlated with human understanding of concepts. Our results point to a\ntrade-off between learning the concepts and preserving the compositionality\nwhich existing approaches struggle to overcome. The data, code, and interactive\ndemo is available at: https://conceptbed.github.io/", "published": "2023-06-07 18:00:38", "link": "http://arxiv.org/abs/2306.04695v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Soft-prompt Tuning for Large Language Models to Evaluate Bias", "abstract": "Prompting large language models has gained immense popularity in recent years\ndue to the advantage of producing good results even without the need for\nlabelled data. However, this requires prompt tuning to get optimal prompts that\nlead to better model performances. In this paper, we explore the use of\nsoft-prompt tuning on sentiment classification task to quantify the biases of\nlarge language models (LLMs) such as Open Pre-trained Transformers (OPT) and\nGalactica language model. Since these models are trained on real-world data\nthat could be prone to bias toward certain groups of populations, it is\nimportant to identify these underlying issues. Using soft-prompts to evaluate\nbias gives us the extra advantage of avoiding the human-bias injection that can\nbe caused by manually designed prompts. We check the model biases on different\nsensitive attributes using the group fairness (bias) and find interesting bias\npatterns. Since LLMs have been used in the industry in various applications, it\nis crucial to identify the biases before deploying these models in practice. We\nopen-source our pipeline and encourage industry researchers to adapt our work\nto their use cases.", "published": "2023-06-07 19:11:25", "link": "http://arxiv.org/abs/2306.04735v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ScienceBenchmark: A Complex Real-World Benchmark for Evaluating Natural\n  Language to SQL Systems", "abstract": "Natural Language to SQL systems (NL-to-SQL) have recently shown a significant\nincrease in accuracy for natural language to SQL query translation. This\nimprovement is due to the emergence of transformer-based language models, and\nthe popularity of the Spider benchmark - the de-facto standard for evaluating\nNL-to-SQL systems. The top NL-to-SQL systems reach accuracies of up to 85\\%.\nHowever, Spider mainly contains simple databases with few tables, columns, and\nentries, which does not reflect a realistic setting. Moreover, complex\nreal-world databases with domain-specific content have little to no training\ndata available in the form of NL/SQL-pairs leading to poor performance of\nexisting NL-to-SQL systems.\n  In this paper, we introduce ScienceBenchmark, a new complex NL-to-SQL\nbenchmark for three real-world, highly domain-specific databases. For this new\nbenchmark, SQL experts and domain experts created high-quality NL/SQL-pairs for\neach domain. To garner more data, we extended the small amount of\nhuman-generated data with synthetic data generated using GPT-3. We show that\nour benchmark is highly challenging, as the top performing systems on Spider\nachieve a very low performance on our benchmark. Thus, the challenge is\nmany-fold: creating NL-to-SQL systems for highly complex domains with a small\namount of hand-made training data augmented with synthetic data. To our\nknowledge, ScienceBenchmark is the first NL-to-SQL benchmark designed with\ncomplex real-world scientific databases, containing challenging training and\ntest data carefully validated by domain experts.", "published": "2023-06-07 19:37:55", "link": "http://arxiv.org/abs/2306.04743v2", "categories": ["cs.DB", "cs.AI", "cs.CL", "H.2.4; I.2.7"], "primary_category": "cs.DB"}
{"title": "Using Imperfect Surrogates for Downstream Inference: Design-based\n  Supervised Learning for Social Science Applications of Large Language Models", "abstract": "In computational social science (CSS), researchers analyze documents to\nexplain social and political phenomena. In most scenarios, CSS researchers\nfirst obtain labels for documents and then explain labels using interpretable\nregression analyses in the second step. One increasingly common way to annotate\ndocuments cheaply at scale is through large language models (LLMs). However,\nlike other scalable ways of producing annotations, such surrogate labels are\noften imperfect and biased. We present a new algorithm for using imperfect\nannotation surrogates for downstream statistical analyses while guaranteeing\nstatistical properties -- like asymptotic unbiasedness and proper uncertainty\nquantification -- which are fundamental to CSS research. We show that direct\nuse of surrogate labels in downstream statistical analyses leads to substantial\nbias and invalid confidence intervals, even with high surrogate accuracy of\n80-90%. To address this, we build on debiased machine learning to propose the\ndesign-based supervised learning (DSL) estimator. DSL employs a doubly-robust\nprocedure to combine surrogate labels with a smaller number of high-quality,\ngold-standard labels. Our approach guarantees valid inference for downstream\nstatistical analyses, even when surrogates are arbitrarily biased and without\nrequiring stringent assumptions, by controlling the probability of sampling\ndocuments for gold-standard labeling. Both our theoretical analysis and\nexperimental results show that DSL provides valid statistical inference while\nachieving root mean squared errors comparable to existing alternatives that\nfocus only on prediction without inferential guarantees.", "published": "2023-06-07 19:49:41", "link": "http://arxiv.org/abs/2306.04746v3", "categories": ["stat.ME", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "stat.ME"}
{"title": "A Review on Knowledge Graphs for Healthcare: Resources, Applications,\n  and Promises", "abstract": "This comprehensive review aims to provide an overview of the current state of\nHealthcare Knowledge Graphs (HKGs), including their construction, utilization\nmodels, and applications across various healthcare and biomedical research\ndomains. We thoroughly analyzed existing literature on HKGs, covering their\nconstruction methodologies, utilization techniques, and applications in basic\nscience research, pharmaceutical research and development, clinical decision\nsupport, and public health. The review encompasses both model-free and\nmodel-based utilization approaches and the integration of HKGs with large\nlanguage models (LLMs). We searched Google Scholar for relevant papers on HKGs\nand classified them into the following topics: HKG construction, HKG\nutilization, and their downstream applications in various domains. We also\ndiscussed their special challenges and the promise for future work. The review\nhighlights the potential of HKGs to significantly impact biomedical research\nand clinical practice by integrating vast amounts of biomedical knowledge from\nmultiple domains. The synergy between HKGs and LLMs offers promising\nopportunities for constructing more comprehensive knowledge graphs and\nimproving the accuracy of healthcare applications. HKGs have emerged as a\npowerful tool for structuring medical knowledge, with broad applications across\nbiomedical research, clinical decision-making, and public health. This survey\nserves as a roadmap for future research and development in the field of HKGs,\nhighlighting the potential of combining knowledge graphs with advanced machine\nlearning models for healthcare transformation.", "published": "2023-06-07 21:51:56", "link": "http://arxiv.org/abs/2306.04802v5", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.SI", "68T30, 68T50, 68T09", "I.2.4; I.2.6; I.2.7; J.3"], "primary_category": "cs.AI"}
{"title": "Privately generating tabular data using language models", "abstract": "Privately generating synthetic data from a table is an important brick of a\nprivacy-first world. We propose and investigate a simple approach of treating\neach row in a table as a sentence and training a language model with\ndifferential privacy. We show this approach obtains competitive results in\nmodelling tabular data across multiple datasets, even at small scales that\nfavor alternative methods based on marginal distributions.", "published": "2023-06-07 21:53:14", "link": "http://arxiv.org/abs/2306.04803v1", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "On the Detectability of ChatGPT Content: Benchmarking, Methodology, and\n  Evaluation through the Lens of Academic Writing", "abstract": "With ChatGPT under the spotlight, utilizing large language models (LLMs) to\nassist academic writing has drawn a significant amount of debate in the\ncommunity. In this paper, we aim to present a comprehensive study of the\ndetectability of ChatGPT-generated content within the academic literature,\nparticularly focusing on the abstracts of scientific papers, to offer holistic\nsupport for the future development of LLM applications and policies in\nacademia. Specifically, we first present GPABench2, a benchmarking dataset of\nover 2.8 million comparative samples of human-written, GPT-written,\nGPT-completed, and GPT-polished abstracts of scientific writing in computer\nscience, physics, and humanities and social sciences. Second, we explore the\nmethodology for detecting ChatGPT content. We start by examining the\nunsatisfactory performance of existing ChatGPT detecting tools and the\nchallenges faced by human evaluators (including more than 240 researchers or\nstudents). We then test the hand-crafted linguistic features models as a\nbaseline and develop a deep neural framework named CheckGPT to better capture\nthe subtle and deep semantic and linguistic patterns in ChatGPT written\nliterature. Last, we conduct comprehensive experiments to validate the proposed\nCheckGPT framework in each benchmarking task over different disciplines. To\nevaluate the detectability of ChatGPT content, we conduct extensive experiments\non the transferability, prompt engineering, and robustness of CheckGPT.", "published": "2023-06-07 12:33:24", "link": "http://arxiv.org/abs/2306.05524v2", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FOOCTTS: Generating Arabic Speech with Acoustic Environment for Football\n  Commentator", "abstract": "This paper presents FOOCTTS, an automatic pipeline for a football commentator\nthat generates speech with background crowd noise. The application gets the\ntext from the user, applies text pre-processing such as vowelization, followed\nby the commentator's speech synthesizer. Our pipeline included Arabic automatic\nspeech recognition for data labeling, CTC segmentation, transcription\nvowelization to match speech, and fine-tuning the TTS. Our system is capable of\ngenerating speech with its acoustic environment within limited 15 minutes of\nfootball commentator recording. Our prototype is generalizable and can be\neasily applied to different domains and languages.", "published": "2023-06-07 12:33:02", "link": "http://arxiv.org/abs/2306.07936v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "An ASR-Based Tutor for Learning to Read: How to Optimize Feedback to\n  First Graders", "abstract": "The interest in employing automatic speech recognition (ASR) in applications\nfor reading practice has been growing in recent years. In a previous study, we\npresented an ASR-based Dutch reading tutor application that was developed to\nprovide instantaneous feedback to first-graders learning to read. We saw that\nASR has potential at this stage of the reading process, as the results\nsuggested that pupils made progress in reading accuracy and fluency by using\nthe software. In the current study, we used children's speech from an existing\ncorpus (JASMIN) to develop two new ASR systems, and compared the results to\nthose of the previous study. We analyze correct/incorrect classification of the\nASR systems using human transcripts at word level, by means of evaluation\nmeasures such as Cohen's Kappa, Matthews Correlation Coefficient (MCC),\nprecision, recall and F-measures. We observe improvements for the newly\ndeveloped ASR systems regarding the agreement with human-based judgment and\ncorrect rejection (CR). The accuracy of the ASR systems varies for different\nreading tasks and word types. Our results suggest that, in the current\nconfiguration, it is difficult to classify isolated words. We discuss these\nresults, possible ways to improve our systems and avenues for future research.", "published": "2023-06-07 06:58:38", "link": "http://arxiv.org/abs/2306.04190v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.CL"}
{"title": "Multimodal Learning Without Labeled Multimodal Data: Guarantees and\n  Applications", "abstract": "In many machine learning systems that jointly learn from multiple modalities,\na core research question is to understand the nature of multimodal\ninteractions: how modalities combine to provide new task-relevant information\nthat was not present in either alone. We study this challenge of interaction\nquantification in a semi-supervised setting with only labeled unimodal data and\nnaturally co-occurring multimodal data (e.g., unlabeled images and captions,\nvideo and corresponding audio) but when labeling them is time-consuming. Using\na precise information-theoretic definition of interactions, our key\ncontribution is the derivation of lower and upper bounds to quantify the amount\nof multimodal interactions in this semi-supervised setting. We propose two\nlower bounds: one based on the shared information between modalities and the\nother based on disagreement between separately trained unimodal classifiers,\nand derive an upper bound through connections to approximate algorithms for\nmin-entropy couplings. We validate these estimated bounds and show how they\naccurately track true interactions. Finally, we show how these theoretical\nresults can be used to estimate multimodal model performance, guide data\ncollection, and select appropriate multimodal models for various tasks.", "published": "2023-06-07 15:44:53", "link": "http://arxiv.org/abs/2306.04539v2", "categories": ["cs.LG", "cs.CL", "cs.CV", "cs.IT", "math.IT", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Transformers as Statisticians: Provable In-Context Learning with\n  In-Context Algorithm Selection", "abstract": "Neural sequence models based on the transformer architecture have\ndemonstrated remarkable \\emph{in-context learning} (ICL) abilities, where they\ncan perform new tasks when prompted with training and test examples, without\nany parameter update to the model. This work first provides a comprehensive\nstatistical theory for transformers to perform ICL. Concretely, we show that\ntransformers can implement a broad class of standard machine learning\nalgorithms in context, such as least squares, ridge regression, Lasso, learning\ngeneralized linear models, and gradient descent on two-layer neural networks,\nwith near-optimal predictive power on various in-context data distributions.\nUsing an efficient implementation of in-context gradient descent as the\nunderlying mechanism, our transformer constructions admit mild size bounds, and\ncan be learned with polynomially many pretraining sequences.\n  Building on these ``base'' ICL algorithms, intriguingly, we show that\ntransformers can implement more complex ICL procedures involving\n\\emph{in-context algorithm selection}, akin to what a statistician can do in\nreal life -- A \\emph{single} transformer can adaptively select different base\nICL algorithms -- or even perform qualitatively different tasks -- on different\ninput sequences, without any explicit prompting of the right algorithm or task.\nWe both establish this in theory by explicit constructions, and also observe\nthis phenomenon experimentally. In theory, we construct two general mechanisms\nfor algorithm selection with concrete examples: pre-ICL testing, and post-ICL\nvalidation. As an example, we use the post-ICL validation mechanism to\nconstruct a transformer that can perform nearly Bayes-optimal ICL on a\nchallenging task -- noisy linear models with mixed noise levels.\nExperimentally, we demonstrate the strong in-context algorithm selection\ncapabilities of standard transformer architectures.", "published": "2023-06-07 17:59:31", "link": "http://arxiv.org/abs/2306.04637v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "math.ST", "stat.ML", "stat.TH"], "primary_category": "cs.LG"}
{"title": "Intrinsic Dimension Estimation for Robust Detection of AI-Generated\n  Texts", "abstract": "Rapidly increasing quality of AI-generated content makes it difficult to\ndistinguish between human and AI-generated texts, which may lead to undesirable\nconsequences for society. Therefore, it becomes increasingly important to study\nthe properties of human texts that are invariant over different text domains\nand varying proficiency of human writers, can be easily calculated for any\nlanguage, and can robustly separate natural and AI-generated texts regardless\nof the generation model and sampling method. In this work, we propose such an\ninvariant for human-written texts, namely the intrinsic dimensionality of the\nmanifold underlying the set of embeddings for a given text sample. We show that\nthe average intrinsic dimensionality of fluent texts in a natural language is\nhovering around the value $9$ for several alphabet-based languages and around\n$7$ for Chinese, while the average intrinsic dimensionality of AI-generated\ntexts for each language is $\\approx 1.5$ lower, with a clear statistical\nseparation between human-generated and AI-generated distributions. This\nproperty allows us to build a score-based artificial text detector. The\nproposed detector's accuracy is stable over text domains, generator models, and\nhuman writer proficiency levels, outperforming SOTA detectors in model-agnostic\nand cross-domain scenarios by a significant margin.", "published": "2023-06-07 18:38:04", "link": "http://arxiv.org/abs/2306.04723v2", "categories": ["cs.CL", "cs.AI", "cs.IT", "cs.LG", "math.AT", "math.IT", "68T50"], "primary_category": "cs.CL"}
{"title": "RISC: A Corpus for Shout Type Classification and Shout Intensity\n  Prediction", "abstract": "The detection of shouted speech is crucial in audio surveillance and\nmonitoring. Although it is desirable for a security system to be able to\nidentify emergencies, existing corpora provide only a binary label (i.e.,\nshouted or normal) for each speech sample, making it difficult to predict the\nshout intensity. Furthermore, most corpora comprise only utterances typical of\nhazardous situations, meaning that classifiers cannot learn to discriminate\nsuch utterances from shouts typical of less hazardous situations, such as\ncheers. Thus, this paper presents a novel research source, the RItsumeikan\nShout Corpus (RISC), which contains wide variety types of shouted speech\nsamples collected in recording experiments. Each shouted speech sample in RISC\nhas a shout type and is also assigned shout intensity ratings via a\ncrowdsourcing service. We also present a comprehensive performance comparison\namong deep learning approaches for speech type classification tasks and a shout\nintensity prediction task. The results show that feature learning based on the\nspectral and cepstral domains achieves high performance, no matter which\nnetwork architecture is used. The results also demonstrate that shout type\nclassification and intensity prediction are still challenging tasks, and RISC\nis expected to contribute to further development in this research area.", "published": "2023-06-07 04:30:02", "link": "http://arxiv.org/abs/2306.04143v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Self-supervised Audio Teacher-Student Transformer for Both Clip-level\n  and Frame-level Tasks", "abstract": "Self-supervised learning (SSL) has emerged as a popular approach for learning\naudio representations. One goal of audio self-supervised pre-training is to\ntransfer knowledge to downstream audio tasks, generally including clip-level\nand frame-level tasks. While frame-level tasks are important for fine-grained\nacoustic scene/event understanding, prior studies primarily evaluate on\nclip-level downstream tasks. In order to tackle both clip-level and frame-level\ntasks, this paper proposes Audio Teacher-Student Transformer (ATST), with a\nclip-level version (named ATST-Clip) and a frame-level version (named\nATST-Frame), responsible for learning clip-level and frame-level\nrepresentations, respectively. Both methods use a Transformer encoder and a\nteacher-student training scheme. We have carefully designed the view creation\nstrategy for ATST-Clip and ATST-Frame. Specifically, ATST-Clip uses\nsegment-wise data augmentations, and ATST-Frame integrates frame-wise data\naugmentations and masking. Experimental results show that our ATST-Frame model\nobtains state-of-the-art (SOTA) performances on most of the clip-level and\nframe-level downstream tasks. Especially, it outperforms other models by a\nlarge margin on the frame-level sound event detection task. In addition, the\nperformance can be further improved by combining the two models through\nknowledge distillation. Our code is available online.", "published": "2023-06-07 06:42:07", "link": "http://arxiv.org/abs/2306.04186v2", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Interpretable Style Transfer for Text-to-Speech with ControlVAE and\n  Diffusion Bridge", "abstract": "With the demand for autonomous control and personalized speech generation,\nthe style control and transfer in Text-to-Speech (TTS) is becoming more and\nmore important. In this paper, we propose a new TTS system that can perform\nstyle transfer with interpretability and high fidelity. Firstly, we design a\nTTS system that combines variational autoencoder (VAE) and diffusion refiner to\nget refined mel-spectrograms. Specifically, a two-stage and a one-stage system\nare designed respectively, to improve the audio quality and the performance of\nstyle transfer. Secondly, a diffusion bridge of quantized VAE is designed to\nefficiently learn complex discrete style representations and improve the\nperformance of style transfer. To have a better ability of style transfer, we\nintroduce ControlVAE to improve the reconstruction quality and have good\ninterpretability simultaneously. Experiments on LibriTTS dataset demonstrate\nthat our method is more effective than baseline models.", "published": "2023-06-07 10:05:45", "link": "http://arxiv.org/abs/2306.04301v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SANGEET: A XML based Open Dataset for Research in Hindustani Sangeet", "abstract": "It is very important to access a rich music dataset that is useful in a wide\nvariety of applications. Currently, available datasets are mostly focused on\nstoring vocal or instrumental recording data and ignoring the requirement of\nits visual representation and retrieval. This paper attempts to build an\nXML-based public dataset, called SANGEET, that stores comprehensive information\nof Hindustani Sangeet (North Indian Classical Music) compositions written by\nfamous musicologist Pt. Vishnu Narayan Bhatkhande. SANGEET preserves all the\nrequired information of any given composition including metadata, structural,\nnotational, rhythmic, and melodic information in a standardized way for easy\nand efficient storage and extraction of musical information. The dataset is\nintended to provide the ground truth information for music information research\ntasks, thereby supporting several data-driven analysis from a machine learning\nperspective. We present the usefulness of the dataset by demonstrating its\napplication on music information retrieval using XQuery, visualization through\nOmenad rendering system. Finally, we propose approaches to transform the\ndataset for performing statistical and machine learning tasks for a better\nunderstanding of Hindustani Sangeet. The dataset can be found at\nhttps://github.com/cmisra/Sangeet.", "published": "2023-06-07 04:50:09", "link": "http://arxiv.org/abs/2306.04148v1", "categories": ["cs.SD", "cs.IR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Test experiments with distributed acoustic sensing and hydrophone arrays\n  for locating underwater sound sources", "abstract": "Whales and dolphins rely on sound for navigation and communication, making\nthem an intriguing subject for studying language evolution. Traditional\nhydrophone arrays have been used to record their acoustic behavior, but optical\nfibers have emerged as a promising alternative. This study explores the use of\ndistributed acoustic sensing (DAS), a technique that detects local stress in\noptical fibers, for underwater sound recording. An experiment was conducted in\nLake Zurich, where a fiber-optic cable and a self-made hydrophone array were\ndeployed. A test signal was broadcasted at various locations, and the resulting\ndata was synchronized and consolidated into files. Analysis revealed distinct\nfrequency responses in the DAS channels and provided insights into sound\npropagation in the lake. Challenges related to cable sensitivity, sample rate,\nand broadcast fidelity were identified. This dataset serves as a valuable\nresource for advancing acoustic sensing techniques in underwater environments,\nespecially for studying marine mammal vocal behavior.", "published": "2023-06-07 09:22:28", "link": "http://arxiv.org/abs/2306.04276v1", "categories": ["physics.ao-ph", "cs.SD", "eess.AS", "physics.bio-ph"], "primary_category": "physics.ao-ph"}
{"title": "A Mask Free Neural Network for Monaural Speech Enhancement", "abstract": "In speech enhancement, the lack of clear structural characteristics in the\ntarget speech phase requires the use of conservative and cumbersome network\nframeworks. It seems difficult to achieve competitive performance using direct\nmethods and simple network architectures. However, we propose the MFNet, a\ndirect and simple network that can not only map speech but also map reverse\nnoise. This network is constructed by stacking global local former blocks\n(GLFBs), which combine the advantages of Mobileblock for global processing and\nMetaformer architecture for local interaction. Our experimental results\ndemonstrate that our network using mapping method outperforms masking methods,\nand direct mapping of reverse noise is the optimal solution in strong noise\nenvironments. In a horizontal comparison on the 2020 Deep Noise Suppression\n(DNS) challenge test set without reverberation, to the best of our knowledge,\nMFNet is the current state-of-the-art (SOTA) mapping model.", "published": "2023-06-07 09:39:07", "link": "http://arxiv.org/abs/2306.04286v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Investigating Reproducibility at Interspeech Conferences: A Longitudinal\n  and Comparative Perspective", "abstract": "Reproducibility is a key aspect for scientific advancement across\ndisciplines, and reducing barriers for open science is a focus area for the\ntheme of Interspeech 2023. Availability of source code is one of the indicators\nthat facilitates reproducibility. However, less is known about the rates of\nreproducibility at Interspeech conferences in comparison to other conferences\nin the field. In order to fill this gap, we have surveyed 27,717 papers at\nseven conferences across speech and language processing disciplines. We find\nthat despite having a close number of accepted papers to the other conferences,\nInterspeech has up to 40% less source code availability. In addition to\nreporting the difficulties we have encountered during our research, we also\nprovide recommendations and possible directions to increase reproducibility for\nfurther studies.", "published": "2023-06-07 19:40:37", "link": "http://arxiv.org/abs/2306.10033v2", "categories": ["cs.DL", "cs.LG", "eess.AS"], "primary_category": "cs.DL"}
