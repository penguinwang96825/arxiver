{"title": "Adapting Pretrained Text-to-Text Models for Long Text Sequences", "abstract": "We present an empirical study of adapting an existing pretrained text-to-text\nmodel for long-sequence inputs. Through a comprehensive study along three axes\nof the pretraining pipeline -- model architecture, optimization objective, and\npretraining corpus, we propose an effective recipe to build long-context models\nfrom existing short-context models. Specifically, we replace the full attention\nin transformers with pooling-augmented blockwise attention, and pretrain the\nmodel with a masked-span prediction task with spans of varying length. In terms\nof the pretraining corpus, we find that using randomly concatenated\nshort-documents from a large open-domain corpus results in better performance\nthan using existing long document corpora which are typically limited in their\ndomain coverage. With these findings, we build a long-context model that\nachieves competitive performance on long-text QA tasks and establishes the new\nstate of the art on five long-text summarization datasets, often outperforming\nprevious methods with larger model sizes. Our code has been released at\nhttps://github.com/facebookresearch/bart_ls.", "published": "2022-09-21 00:41:07", "link": "http://arxiv.org/abs/2209.10052v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is More Data Better? Re-thinking the Importance of Efficiency in Abusive\n  Language Detection with Transformers-Based Active Learning", "abstract": "Annotating abusive language is expensive, logistically complex and creates a\nrisk of psychological harm. However, most machine learning research has\nprioritized maximizing effectiveness (i.e., F1 or accuracy score) rather than\ndata efficiency (i.e., minimizing the amount of data that is annotated). In\nthis paper, we use simulated experiments over two datasets at varying\npercentages of abuse to demonstrate that transformers-based active learning is\na promising approach to substantially raise efficiency whilst still maintaining\nhigh effectiveness, especially when abusive content is a smaller percentage of\nthe dataset. This approach requires a fraction of labeled data to reach\nperformance equivalent to training over the full dataset.", "published": "2022-09-21 08:47:06", "link": "http://arxiv.org/abs/2209.10193v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Seeking Diverse Reasoning Logic: Controlled Equation Expression\n  Generation for Solving Math Word Problems", "abstract": "To solve Math Word Problems, human students leverage diverse reasoning logic\nthat reaches different possible equation solutions. However, the mainstream\nsequence-to-sequence approach of automatic solvers aims to decode a fixed\nsolution equation supervised by human annotation. In this paper, we propose a\ncontrolled equation generation solver by leveraging a set of control codes to\nguide the model to consider certain reasoning logic and decode the\ncorresponding equations expressions transformed from the human reference. The\nempirical results suggest that our method universally improves the performance\non single-unknown (Math23K) and multiple-unknown (DRAW1K, HMWP) benchmarks,\nwith substantial improvements up to 13.2% accuracy on the challenging\nmultiple-unknown datasets.", "published": "2022-09-21 12:43:30", "link": "http://arxiv.org/abs/2209.10310v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SMTCE: A Social Media Text Classification Evaluation Benchmark and\n  BERTology Models for Vietnamese", "abstract": "Text classification is a typical natural language processing or computational\nlinguistics task with various interesting applications. As the number of users\non social media platforms increases, data acceleration promotes emerging\nstudies on Social Media Text Classification (SMTC) or social media text mining\non these valuable resources. In contrast to English, Vietnamese, one of the\nlow-resource languages, is still not concentrated on and exploited thoroughly.\nInspired by the success of the GLUE, we introduce the Social Media Text\nClassification Evaluation (SMTCE) benchmark, as a collection of datasets and\nmodels across a diverse set of SMTC tasks. With the proposed benchmark, we\nimplement and analyze the effectiveness of a variety of multilingual BERT-based\nmodels (mBERT, XLM-R, and DistilmBERT) and monolingual BERT-based models\n(PhoBERT, viBERT, vELECTRA, and viBERT4news) for tasks in the SMTCE benchmark.\nMonolingual models outperform multilingual models and achieve state-of-the-art\nresults on all text classification tasks. It provides an objective assessment\nof multilingual and monolingual BERT-based models on the benchmark, which will\nbenefit future studies about BERTology in the Vietnamese language.", "published": "2022-09-21 16:33:46", "link": "http://arxiv.org/abs/2209.10482v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text Revealer: Private Text Reconstruction via Model Inversion Attacks\n  against Transformers", "abstract": "Text classification has become widely used in various natural language\nprocessing applications like sentiment analysis. Current applications often use\nlarge transformer-based language models to classify input texts. However, there\nis a lack of systematic study on how much private information can be inverted\nwhen publishing models. In this paper, we formulate \\emph{Text Revealer} -- the\nfirst model inversion attack for text reconstruction against text\nclassification with transformers. Our attacks faithfully reconstruct private\ntexts included in training data with access to the target model. We leverage an\nexternal dataset and GPT-2 to generate the target domain-like fluent text, and\nthen perturb its hidden state optimally with the feedback from the target\nmodel. Our extensive experiments demonstrate that our attacks are effective for\ndatasets with different text lengths and can reconstruct private texts with\naccuracy.", "published": "2022-09-21 17:05:12", "link": "http://arxiv.org/abs/2209.10505v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Subject Verb Agreement Error Patterns in Meaningless Sentences: Humans\n  vs. BERT", "abstract": "Both humans and neural language models are able to perform subject-verb\nnumber agreement (SVA). In principle, semantics shouldn't interfere with this\ntask, which only requires syntactic knowledge. In this work we test whether\nmeaning interferes with this type of agreement in English in syntactic\nstructures of various complexities. To do so, we generate both semantically\nwell-formed and nonsensical items. We compare the performance of BERT-base to\nthat of humans, obtained with a psycholinguistic online crowdsourcing\nexperiment. We find that BERT and humans are both sensitive to our semantic\nmanipulation: They fail more often when presented with nonsensical items,\nespecially when their syntactic structure features an attractor (a noun phrase\nbetween the subject and the verb that has not the same number as the subject).\nWe also find that the effect of meaningfulness on SVA errors is stronger for\nBERT than for humans, showing higher lexical sensitivity of the former on this\ntask.", "published": "2022-09-21 17:57:23", "link": "http://arxiv.org/abs/2209.10538v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Representing Affect Information in Word Embeddings", "abstract": "A growing body of research in natural language processing (NLP) and natural\nlanguage understanding (NLU) is investigating human-like knowledge learned or\nencoded in the word embeddings from large language models. This is a step\ntowards understanding what knowledge language models capture that resembles\nhuman understanding of language and communication. Here, we investigated\nwhether and how the affect meaning of a word (i.e., valence, arousal,\ndominance) is encoded in word embeddings pre-trained in large neural networks.\nWe used the human-labeled dataset as the ground truth and performed various\ncorrelational and classification tests on four types of word embeddings. The\nembeddings varied in being static or contextualized, and how much affect\nspecific information was prioritized during the pre-training and fine-tuning\nphase. Our analyses show that word embedding from the vanilla BERT model did\nnot saliently encode the affect information of English words. Only when the\nBERT model was fine-tuned on emotion-related tasks or contained extra\ncontextualized information from emotion-rich contexts could the corresponding\nembedding encode more relevant affect information.", "published": "2022-09-21 18:16:33", "link": "http://arxiv.org/abs/2209.10583v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dodging the Data Bottleneck: Automatic Subtitling with Automatically\n  Segmented ST Corpora", "abstract": "Speech translation for subtitling (SubST) is the task of automatically\ntranslating speech data into well-formed subtitles by inserting subtitle breaks\ncompliant to specific displaying guidelines. Similar to speech translation\n(ST), model training requires parallel data comprising audio inputs paired with\ntheir textual translations. In SubST, however, the text has to be also\nannotated with subtitle breaks. So far, this requirement has represented a\nbottleneck for system development, as confirmed by the dearth of publicly\navailable SubST corpora. To fill this gap, we propose a method to convert\nexisting ST corpora into SubST resources without human intervention. We build a\nsegmenter model that automatically segments texts into proper subtitles by\nexploiting audio and text in a multimodal fashion, achieving high segmentation\nquality in zero-shot conditions. Comparative experiments with SubST systems\nrespectively trained on manual and automatic segmentations result in similar\nperformance, showing the effectiveness of our approach.", "published": "2022-09-21 19:06:36", "link": "http://arxiv.org/abs/2209.10608v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generate rather than Retrieve: Large Language Models are Strong Context\n  Generators", "abstract": "Knowledge-intensive tasks, such as open-domain question answering (QA),\nrequire access to a large amount of world or domain knowledge. A common\napproach for knowledge-intensive tasks is to employ a retrieve-then-read\npipeline that first retrieves a handful of relevant contextual documents from\nan external corpus such as Wikipedia and then predicts an answer conditioned on\nthe retrieved documents. In this paper, we present a novel perspective for\nsolving knowledge-intensive tasks by replacing document retrievers with large\nlanguage model generators. We call our method generate-then-read (GenRead),\nwhich first prompts a large language model to generate contextutal documents\nbased on a given question, and then reads the generated documents to produce\nthe final answer. Furthermore, we propose a novel clustering-based prompting\nmethod that selects distinct prompts, resulting in the generated documents that\ncover different perspectives, leading to better recall over acceptable answers.\nWe conduct extensive experiments on three different knowledge-intensive tasks,\nincluding open-domain QA, fact checking, and dialogue system. Notably, GenRead\nachieves 71.6 and 54.4 exact match scores on TriviaQA and WebQ, significantly\noutperforming the state-of-the-art retrieve-then-read pipeline DPR-FiD by +4.0\nand +3.9, without retrieving any documents from any external knowledge source.\nLastly, we demonstrate the model performance can be further improved by\ncombining retrieval and generation. Our code and generated documents can be\nfound at https://github.com/wyu97/GenRead.", "published": "2022-09-21 01:30:59", "link": "http://arxiv.org/abs/2209.10063v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Extreme Multi-Domain, Multi-Task Learning With Unified Text-to-Text\n  Transfer Transformers", "abstract": "Text-to-text transformers have shown remarkable success in the task of\nmulti-task transfer learning, especially in natural language processing (NLP).\nHowever, while there have been several attempts to train transformers on\ndifferent domains, there is usually a clear relationship between these domains,\ne.g.,, code summarization, where the natural language summary describes the\ncode. There have been very few attempts to study how multi-task transfer\nlearning works on tasks in significantly different domains. In this project, we\ninvestigated the behavior of multi-domain, multi-task learning using\nmulti-domain text-to-text transfer transformers (MD-T5) on four tasks across\ntwo domains - Python Code and Chess. We carried out extensive experiments using\nthree popular training strategies: Bert-style joint pretraining + successive\nfinetuning, GPT-style joint pretraining + successive finetuning, and GPT-style\njoint pretraining + joint finetuning. Also, we evaluate the model on four\nmetrics - Play Score, Eval Score, BLEU Score, and Multi-Domain Learning Score\n(MDLS). These metrics measure performance across the various tasks and\nmulti-domain learning. We show that while negative knowledge transfer and\ncatastrophic forgetting are still considerable challenges for all the models,\nthe GPT-style joint pretraining + joint finetuning strategy showed the most\npromise in multi-domain, multi-task learning as it performs well across all\nfour tasks while still keeping its multi-domain knowledge.", "published": "2022-09-21 04:21:27", "link": "http://arxiv.org/abs/2209.10106v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PePe: Personalized Post-editing Model utilizing User-generated\n  Post-edits", "abstract": "Incorporating personal preference is crucial in advanced machine translation\ntasks. Despite the recent advancement of machine translation, it remains a\ndemanding task to properly reflect personal style. In this paper, we introduce\na personalized automatic post-editing framework to address this challenge,\nwhich effectively generates sentences considering distinct personal behaviors.\nTo build this framework, we first collect post-editing data that connotes the\nuser preference from a live machine translation system. Specifically,\nreal-world users enter source sentences for translation and edit the\nmachine-translated outputs according to the user's preferred style. We then\npropose a model that combines a discriminator module and user-specific\nparameters on the APE framework. Experimental results show that the proposed\nmethod outperforms other baseline models on four different metrics (i.e., BLEU,\nTER, YiSi-1, and human evaluation).", "published": "2022-09-21 06:09:58", "link": "http://arxiv.org/abs/2209.10139v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Chatbots for Mental Health Support: Exploring the Impact of Emohaa on\n  Reducing Mental Distress in China", "abstract": "The growing demand for mental health support has highlighted the importance\nof conversational agents as human supporters worldwide and in China. These\nagents could increase availability and reduce the relative costs of mental\nhealth support. The provided support can be divided into two main types:\ncognitive and emotional support. Existing work on this topic mainly focuses on\nconstructing agents that adopt Cognitive Behavioral Therapy (CBT) principles.\nSuch agents operate based on pre-defined templates and exercises to provide\ncognitive support. However, research on emotional support using such agents is\nlimited. In addition, most of the constructed agents operate in English,\nhighlighting the importance of conducting such studies in China. In this study,\nwe analyze the effectiveness of Emohaa in reducing symptoms of mental distress.\nEmohaa is a conversational agent that provides cognitive support through\nCBT-based exercises and guided conversations. It also emotionally supports\nusers by enabling them to vent their desired emotional problems. The study\nincluded 134 participants, split into three groups: Emohaa (CBT-based), Emohaa\n(Full), and control. Experimental results demonstrated that compared to the\ncontrol group, participants who used Emohaa experienced considerably more\nsignificant improvements in symptoms of mental distress. We also found that\nadding the emotional support agent had a complementary effect on such\nimprovements, mainly depression and insomnia. Based on the obtained results and\nparticipants' satisfaction with the platform, we concluded that Emohaa is a\npractical and effective tool for reducing mental distress.", "published": "2022-09-21 08:23:40", "link": "http://arxiv.org/abs/2209.10183v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Bias at a Second Glance: A Deep Dive into Bias for German Educational\n  Peer-Review Data Modeling", "abstract": "Natural Language Processing (NLP) has become increasingly utilized to provide\nadaptivity in educational applications. However, recent research has\nhighlighted a variety of biases in pre-trained language models. While existing\nstudies investigate bias in different domains, they are limited in addressing\nfine-grained analysis on educational and multilingual corpora. In this work, we\nanalyze bias across text and through multiple architectures on a corpus of\n9,165 German peer-reviews collected from university students over five years.\nNotably, our corpus includes labels such as helpfulness, quality, and critical\naspect ratings from the peer-review recipient as well as demographic\nattributes. We conduct a Word Embedding Association Test (WEAT) analysis on (1)\nour collected corpus in connection with the clustered labels, (2) the most\ncommon pre-trained German language models (T5, BERT, and GPT-2) and GloVe\nembeddings, and (3) the language models after fine-tuning on our collected\ndata-set. In contrast to our initial expectations, we found that our collected\ncorpus does not reveal many biases in the co-occurrence analysis or in the\nGloVe embeddings. However, the pre-trained German language models find\nsubstantial conceptual, racial, and gender bias and have significant changes in\nbias across conceptual and racial axes during fine-tuning on the peer-review\ndata. With our research, we aim to contribute to the fourth UN sustainability\ngoal (quality education) with a novel dataset, an understanding of biases in\nnatural language education data, and the potential harms of not counteracting\nbiases in language models for educational tasks.", "published": "2022-09-21 13:08:16", "link": "http://arxiv.org/abs/2209.10335v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "WeLM: A Well-Read Pre-trained Language Model for Chinese", "abstract": "Large Language Models pre-trained with self-supervised learning have\ndemonstrated impressive zero-shot generalization capabilities on a wide\nspectrum of tasks. In this work, we present WeLM: a well-read pre-trained\nlanguage model for Chinese that is able to seamlessly perform different types\nof tasks with zero or few-shot demonstrations. WeLM is trained with 10B\nparameters by \"reading\" a curated high-quality corpus covering a wide range of\ntopics. We show that WeLM is equipped with broad knowledge on various domains\nand languages. On 18 monolingual (Chinese) tasks, WeLM can significantly\noutperform existing pre-trained models with similar sizes and match the\nperformance of models up to 25 times larger. WeLM also exhibits strong\ncapabilities in multi-lingual and code-switching understanding, outperforming\nexisting multilingual language models pre-trained on 30 languages. Furthermore,\nWe collected human-written prompts for a large set of supervised datasets in\nChinese and fine-tuned WeLM with multi-prompted training. The resulting model\ncan attain strong generalization on unseen types of tasks and outperform the\nunsupervised WeLM in zero-shot learning. Finally, we demonstrate that WeLM has\nbasic skills at explaining and calibrating the decisions from itself, which can\nbe promising directions for future research. Our models can be applied from\nhttps://welm.weixin.qq.com/docs/api/.", "published": "2022-09-21 14:05:30", "link": "http://arxiv.org/abs/2209.10372v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Recipe Generation from Unsegmented Cooking Videos", "abstract": "This paper tackles recipe generation from unsegmented cooking videos, a task\nthat requires agents to (1) extract key events in completing the dish and (2)\ngenerate sentences for the extracted events. Our task is similar to dense video\ncaptioning (DVC), which aims at detecting events thoroughly and generating\nsentences for them. However, unlike DVC, in recipe generation, recipe story\nawareness is crucial, and a model should extract an appropriate number of\nevents in the correct order and generate accurate sentences based on them. We\nanalyze the output of the DVC model and confirm that although (1) several\nevents are adoptable as a recipe story, (2) the generated sentences for such\nevents are not grounded in the visual content. Based on this, we set our goal\nto obtain correct recipes by selecting oracle events from the output events and\nre-generating sentences for them. To achieve this, we propose a\ntransformer-based multimodal recurrent approach of training an event selector\nand sentence generator for selecting oracle events from the DVC's events and\ngenerating sentences for them. In addition, we extend the model by including\ningredients to generate more accurate recipes. The experimental results show\nthat the proposed method outperforms state-of-the-art DVC models. We also\nconfirm that, by modeling the recipe in a story-aware manner, the proposed\nmodel outputs the appropriate number of events in the correct order.", "published": "2022-09-21 05:54:13", "link": "http://arxiv.org/abs/2209.10134v2", "categories": ["cs.MM", "cs.CL", "cs.CV"], "primary_category": "cs.MM"}
{"title": "Fast Few shot Self-attentive Semi-supervised Political Inclination\n  Prediction", "abstract": "With the rising participation of the common mass in social media, it is\nincreasingly common now for policymakers/journalists to create online polls on\nsocial media to understand the political leanings of people in specific\nlocations. The caveat here is that only influential people can make such an\nonline polling and reach out at a mass scale. Further, in such cases, the\ndistribution of voters is not controllable and may be, in fact, biased. On the\nother hand,if we can interpret the publicly available data over social media to\nprobe the political inclination of users, we will be able to have controllable\ninsights about the survey population, keep the cost of survey low and also\ncollect publicly available data without involving the concerned persons. Hence\nwe introduce a self-attentive semi-supervised framework for political\ninclination detection to further that objective. The advantage of our model is\nthat it neither needs huge training data nor does it need to store social\nnetwork parameters. Nevertheless, it achieves an accuracy of 93.7\\% with no\nannotated data; further, with only a few annotated examples per class it\nachieves competitive performance.\n  We found that the model is highly efficient even in resource-constrained\nsettings, and insights drawn from its predictions match the manual survey\noutcomes when applied to diverse real-life scenarios.", "published": "2022-09-21 12:07:16", "link": "http://arxiv.org/abs/2209.10292v2", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG", "K.4.1; I.2.7; I.2.6"], "primary_category": "cs.CY"}
{"title": "Transition to Adulthood for Young People with Intellectual or\n  Developmental Disabilities: Emotion Detection and Topic Modeling", "abstract": "Transition to Adulthood is an essential life stage for many families. The\nprior research has shown that young people with intellectual or development\ndisabil-ities (IDD) have more challenges than their peers. This study is to\nexplore how to use natural language processing (NLP) methods, especially\nunsupervised machine learning, to assist psychologists to analyze emotions and\nsentiments and to use topic modeling to identify common issues and challenges\nthat young people with IDD and their families have. Additionally, the results\nwere compared to those obtained from young people without IDD who were in\ntran-sition to adulthood. The findings showed that NLP methods can be very\nuseful for psychologists to analyze emotions, conduct cross-case analysis, and\nsum-marize key topics from conversational data. Our Python code is available at\nhttps://github.com/mlaricheva/emotion_topic_modeling.", "published": "2022-09-21 16:23:45", "link": "http://arxiv.org/abs/2209.10477v1", "categories": ["cs.CL", "stat.AP", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Summarization Programs: Interpretable Abstractive Summarization with\n  Neural Modular Trees", "abstract": "Current abstractive summarization models either suffer from a lack of clear\ninterpretability or provide incomplete rationales by only highlighting parts of\nthe source document. To this end, we propose the Summarization Program (SP), an\ninterpretable modular framework consisting of an (ordered) list of binary\ntrees, each encoding the step-by-step generative process of an abstractive\nsummary sentence from the source document. A Summarization Program contains one\nroot node per summary sentence, and a distinct tree connects each summary\nsentence (root node) to the document sentences (leaf nodes) from which it is\nderived, with the connecting nodes containing intermediate generated sentences.\nEdges represent different modular operations involved in summarization such as\nsentence fusion, compression, and paraphrasing. We first propose an efficient\nbest-first search method over neural modules, SP-Search that identifies SPs for\nhuman summaries by directly optimizing for ROUGE scores. Next, using these\nprograms as automatic supervision, we propose seq2seq models that generate\nSummarization Programs, which are then executed to obtain final summaries. We\ndemonstrate that SP-Search effectively represents the generative process behind\nhuman summaries using modules that are typically faithful to their intended\nbehavior. We also conduct a simulation study to show that Summarization\nPrograms improve the interpretability of summarization models by allowing\nhumans to better simulate model reasoning. Summarization Programs constitute a\npromising step toward interpretable and modular abstractive summarization, a\ncomplex task previously addressed primarily through blackbox end-to-end neural\nsystems. Supporting code available at\nhttps://github.com/swarnaHub/SummarizationPrograms", "published": "2022-09-21 16:50:22", "link": "http://arxiv.org/abs/2209.10492v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Assessing ASR Model Quality on Disordered Speech using BERTScore", "abstract": "Word Error Rate (WER) is the primary metric used to assess automatic speech\nrecognition (ASR) model quality. It has been shown that ASR models tend to have\nmuch higher WER on speakers with speech impairments than typical English\nspeakers. It is hard to determine if models can be be useful at such high error\nrates. This study investigates the use of BERTScore, an evaluation metric for\ntext generation, to provide a more informative measure of ASR model quality and\nusefulness. Both BERTScore and WER were compared to prediction errors manually\nannotated by Speech Language Pathologists for error type and assessment.\nBERTScore was found to be more correlated with human assessment of error type\nand assessment. BERTScore was specifically more robust to orthographic changes\n(contraction and normalization errors) where meaning was preserved.\nFurthermore, BERTScore was a better fit of error assessment than WER, as\nmeasured using an ordinal logistic regression and the Akaike's Information\nCriterion (AIC). Overall, our findings suggest that BERTScore can complement\nWER when assessing ASR model performance from a practical perspective,\nespecially for accessibility applications where models are useful even at lower\naccuracy than for typical speech.", "published": "2022-09-21 18:33:33", "link": "http://arxiv.org/abs/2209.10591v1", "categories": ["eess.AS", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "GIST-AiTeR System for the Diarization Task of the 2022 VoxCeleb Speaker\n  Recognition Challenge", "abstract": "This report describes the submission system of the GIST-AiTeR team at the\n2022 VoxCeleb Speaker Recognition Challenge (VoxSRC) Track 4. Our system mainly\nincludes speech enhancement, voice activity detection , multi-scaled speaker\nembedding, probabilistic linear discriminant analysis-based speaker clustering,\nand overlapped speech detection models. We first construct four different\ndiarization systems according to different model combinations with the best\nexperimental efforts. Our final submission is an ensemble system of all the\nfour systems and achieves a diarization error rate of 5.12% on the challenge\nevaluation set, ranked third at the diarization track of the challenge.", "published": "2022-09-21 13:52:58", "link": "http://arxiv.org/abs/2209.10357v4", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Boosting Star-GANs for Voice Conversion with Contrastive Discriminator", "abstract": "Nonparallel multi-domain voice conversion methods such as the StarGAN-VCs\nhave been widely applied in many scenarios. However, the training of these\nmodels usually poses a challenge due to their complicated adversarial network\narchitectures. To address this, in this work we leverage the state-of-the-art\ncontrastive learning techniques and incorporate an efficient Siamese network\nstructure into the StarGAN discriminator. Our method is called\nSimSiam-StarGAN-VC and it boosts the training stability and effectively\nprevents the discriminator overfitting issue in the training process. We\nconduct experiments on the Voice Conversion Challenge (VCC 2018) dataset, plus\na user study to validate the performance of our framework. Our experimental\nresults show that SimSiam-StarGAN-VC significantly outperforms existing\nStarGAN-VC methods in terms of both the objective and subjective metrics.", "published": "2022-09-21 03:34:22", "link": "http://arxiv.org/abs/2209.10088v4", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The ReturnZero System for VoxCeleb Speaker Recognition Challenge 2022", "abstract": "In this paper, we describe the top-scoring submissions for team RTZR VoxCeleb\nSpeaker Recognition Challenge 2022 (VoxSRC-22) in the closed dataset, speaker\nverification Track 1. The top performed system is a fusion of 7 models, which\ncontains 3 different types of model architectures. We focus on training models\nto learn extra-temporal information. Therefore, all models were trained with\n4-6 second frames for each utterance. Also, we apply the Large Margin\nFine-tuning strategy which has shown good performance on the previous\nchallenges for some of our fusion models. While the evaluation process, we\napply the scoring methods with adaptive symmetric normalization (AS-Norm) and\nmatrix score average (MSA). Finally, we mix up models with logistic regression\nto fuse all the trained models. The final submission achieves 0.165 DCF and\n2.912% EER on the VoxSRC22 test set.", "published": "2022-09-21 06:54:24", "link": "http://arxiv.org/abs/2209.10147v1", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Dynamic Time-Alignment of Dimensional Annotations of Emotion using\n  Recurrent Neural Networks", "abstract": "Most automatic emotion recognition systems exploit time-continuous\nannotations of emotion to provide fine-grained descriptions of spontaneous\nexpressions as observed in real-life interactions. As emotion is rather\nsubjective, its annotation is usually performed by several annotators who\nprovide a trace for a given dimension, i.e. a time-continuous series describing\na dimension such as arousal or valence. However, annotations of the same\nexpression are rarely consistent between annotators, either in time or in\nvalue, which adds bias and delay in the trace that is used to learn predictive\nmodels of emotion. We therefore propose a method that can dynamically\ncompensate inconsistencies across annotations and synchronise the traces with\nthe corresponding acoustic features using Recurrent Neural Networks.\nExperimental evaluations were carried on several emotion data sets that include\nChinese, French, German, and Hungarian participants who interacted remotely in\neither noise-free conditions or in-the-wild. The results show that our method\ncan significantly increase inter-annotator agreement, as well as correlation\nbetween traces and audio features, for both arousal and valence. In addition,\nimprovements are obtained in the automatic prediction of these dimensions using\nsimple light-weight models, especially for valence in noise-free conditions,\nand arousal for recordings captured in-the-wild.", "published": "2022-09-21 09:38:57", "link": "http://arxiv.org/abs/2209.10223v1", "categories": ["cs.SD", "cs.AI", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Learning Hierarchical Metrical Structure Beyond Measures", "abstract": "Music contains hierarchical structures beyond beats and measures. While\nhierarchical structure annotations are helpful for music information retrieval\nand computer musicology, such annotations are scarce in current digital music\ndatabases. In this paper, we explore a data-driven approach to automatically\nextract hierarchical metrical structures from scores. We propose a new model\nwith a Temporal Convolutional Network-Conditional Random Field (TCN-CRF)\narchitecture. Given a symbolic music score, our model takes in an arbitrary\nnumber of voices in a beat-quantized form, and predicts a 4-level hierarchical\nmetrical structure from downbeat-level to section-level. We also annotate a\ndataset using RWC-POP MIDI files to facilitate training and evaluation. We show\nby experiments that the proposed method performs better than the rule-based\napproach under different orchestration settings. We also perform some simple\nmusicological analysis on the model predictions. All demos, datasets and\npre-trained models are publicly available on Github.", "published": "2022-09-21 11:08:52", "link": "http://arxiv.org/abs/2209.10259v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Mandarin Singing Voice Synthesis with Denoising Diffusion Probabilistic\n  Wasserstein GAN", "abstract": "Singing voice synthesis (SVS) is the computer production of a human-like\nsinging voice from given musical scores. To accomplish end-to-end SVS\neffectively and efficiently, this work adopts the acoustic model-neural vocoder\narchitecture established for high-quality speech and singing voice synthesis.\nSpecifically, this work aims to pursue a higher level of expressiveness in\nsynthesized voices by combining the diffusion denoising probabilistic model\n(DDPM) and \\emph{Wasserstein} generative adversarial network (WGAN) to\nconstruct the backbone of the acoustic model. On top of the proposed acoustic\nmodel, a HiFi-GAN neural vocoder is adopted with integrated fine-tuning to\nensure optimal synthesis quality for the resulting end-to-end SVS system. This\nend-to-end system was evaluated with the multi-singer Mpop600 Mandarin singing\nvoice dataset. In the experiments, the proposed system exhibits improvements\nover previous landmark counterparts in terms of musical expressiveness and\nhigh-frequency acoustic details. Moreover, the adversarial acoustic model\nconverged stably without the need to enforce reconstruction objectives,\nindicating the convergence stability of the proposed DDPM and WGAN combined\narchitecture over alternative GAN-based SVS systems.", "published": "2022-09-21 15:47:39", "link": "http://arxiv.org/abs/2209.10446v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "An Initial study on Birdsong Re-synthesis Using Neural Vocoders", "abstract": "Modern speech synthesis uses neural vocoders to model raw waveform samples\ndirectly. This increased versatility has expanded the scope of vocoders from\nspeech to other domains, such as music. We address another interesting domain\nof bio-acoustics. We provide initial comparative analysis-resynthesis\nexperiments of birdsong using traditional (WORLD) and two neural (WaveNet\nautoencoder, parallel WaveGAN) vocoders. Our subjective results indicate no\ndifference in the three vocoders in terms of species discrimination (ABX test).\nNonetheless, the WORLD vocoder samples were rated higher in terms of retaining\nbird-like qualities (MOS test). All vocoders faced issues with pitch and\nvoicing. Our results indicate some of the challenges in processing low-quality\nwildlife audio data.", "published": "2022-09-21 16:30:29", "link": "http://arxiv.org/abs/2209.10479v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Modeling Perceptual Loudness of Piano Tone: Theory and Applications", "abstract": "The relationship between perceptual loudness and physical attributes of sound\nis an important subject in both computer music and psychoacoustics. Early\nstudies of \"equal-loudness contour\" can trace back to the 1920s and the\nmeasured loudness with respect to intensity and frequency has been revised many\ntimes since then. However, most studies merely focus on synthesized sound, and\nthe induced theories on natural tones with complex timbre have rarely been\njustified. To this end, we investigate both theory and applications of\nnatural-tone loudness perception in this paper via modeling piano tone. The\ntheory part contains: 1) an accurate measurement of piano-tone equal-loudness\ncontour of pitches, and 2) a machine-learning model capable of inferring\nloudness purely based on spectral features trained on human subject\nmeasurements. As for the application, we apply our theory to piano control\ntransfer, in which we adjust the MIDI velocities on two different player pianos\n(in different acoustic environments) to achieve the same perceptual effect.\nExperiments show that both our theoretical loudness modeling and the\ncorresponding performance control transfer algorithm significantly outperform\ntheir baselines.", "published": "2022-09-21 21:57:14", "link": "http://arxiv.org/abs/2209.10674v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
