{"title": "What does BERT Learn from Multiple-Choice Reading Comprehension\n  Datasets?", "abstract": "Multiple-Choice Reading Comprehension (MCRC) requires the model to read the\npassage and question, and select the correct answer among the given options.\nRecent state-of-the-art models have achieved impressive performance on multiple\nMCRC datasets. However, such performance may not reflect the model's true\nability of language understanding and reasoning. In this work, we adopt two\napproaches to investigate what BERT learns from MCRC datasets: 1) an\nun-readable data attack, in which we add keywords to confuse BERT, leading to a\nsignificant performance drop; and 2) an un-answerable data training, in which\nwe train BERT on partial or shuffled input. Under un-answerable data training,\nBERT achieves unexpectedly high performance. Based on our experiments on the 5\nkey MCRC datasets - RACE, MCTest, MCScript, MCScript2.0, DREAM - we observe\nthat 1) fine-tuned BERT mainly learns how keywords lead to correct prediction,\ninstead of learning semantic understanding and reasoning; and 2) BERT does not\nneed correct syntactic information to solve the task; 3) there exists artifacts\nin these datasets such that they can be solved even without the full context.", "published": "2019-10-28 00:50:55", "link": "http://arxiv.org/abs/1910.12391v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Attention-Gated Graph Convolutions for Extracting Drug Interaction\n  Information from Drug Labels", "abstract": "Preventable adverse events as a result of medical errors present a growing\nconcern in the healthcare system. As drug-drug interactions (DDIs) may lead to\npreventable adverse events, being able to extract DDIs from drug labels into a\nmachine-processable form is an important step toward effective dissemination of\ndrug safety information. In this study, we tackle the problem of jointly\nextracting drugs and their interactions, including interaction outcome, from\ndrug labels. Our deep learning approach entails composing various intermediate\nrepresentations including sequence and graph based context, where the latter is\nderived using graph convolutions (GC) with a novel attention-based gating\nmechanism (holistically called GCA). These representations are then composed in\nmeaningful ways to handle all subtasks jointly. To overcome scarcity in\ntraining data, we additionally propose transfer learning by pre-training on\nrelated DDI data. Our model is trained and evaluated on the 2018 TAC DDI\ncorpus. Our GCA model in conjunction with transfer learning performs at 39.20%\nF1 and 26.09% F1 on entity recognition (ER) and relation extraction (RE)\nrespectively on the first official test set and at 45.30% F1 and 27.87% F1 on\nER and RE respectively on the second official test set corresponding to an\nimprovement over our prior best results by up to 6 absolute F1 points. After\ncontrolling for available training data, our model exhibits state-of-the-art\nperformance by improving over the next comparable best outcome by roughly three\nF1 points in ER and 1.5 F1 points in RE evaluation across two official test\nsets.", "published": "2019-10-28 03:18:25", "link": "http://arxiv.org/abs/1910.12419v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Module System for Open Domain Chinese Question Answering over\n  Knowledge Base", "abstract": "For the task of open domain Knowledge Based Question Answering in CCKS2019,\nwe propose a method combining information retrieval and semantic parsing. This\nmulti-module system extracts the topic entity and the most related relation\npredicate from a question and transforms it into a Sparql query statement. Our\nmethod obtained the F1 score of 70.45% on the test data.", "published": "2019-10-28 07:28:57", "link": "http://arxiv.org/abs/1910.12477v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RPM-Oriented Query Rewriting Framework for E-commerce Keyword-Based\n  Sponsored Search", "abstract": "Sponsored search optimizes revenue and relevance, which is estimated by\nRevenue Per Mille (RPM). Existing sponsored search models are all based on\ntraditional statistical models, which have poor RPM performance when queries\nfollow a heavy-tailed distribution. Here, we propose an RPM-oriented Query\nRewriting Framework (RQRF) which outputs related bid keywords that can yield\nhigh RPM. RQRF embeds both queries and bid keywords to vectors in the same\nimplicit space, converting the rewriting probability between each query and\nkeyword to the distance between the two vectors. For label construction, we\npropose an RPM-oriented sample construction method, labeling keywords based on\nwhether or not they can lead to high RPM. Extensive experiments are conducted\nto evaluate performance of RQRF. In a one month large-scale real-world traffic\nof e-commerce sponsored search system, the proposed model significantly\noutperforms traditional baseline.", "published": "2019-10-28 10:03:24", "link": "http://arxiv.org/abs/1910.12527v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adaptive Ensembling: Unsupervised Domain Adaptation for Political\n  Document Analysis", "abstract": "Insightful findings in political science often require researchers to analyze\ndocuments of a certain subject or type, yet these documents are usually\ncontained in large corpora that do not distinguish between pertinent and\nnon-pertinent documents. In contrast, we can find corpora that label relevant\ndocuments but have limitations (e.g., from a single source or era), preventing\ntheir use for political science research. To bridge this gap, we present\n\\textit{adaptive ensembling}, an unsupervised domain adaptation framework,\nequipped with a novel text classification model and time-aware training to\nensure our methods work well with diachronic corpora. Experiments on an\nexpert-annotated dataset show that our framework outperforms strong benchmarks.\nFurther analysis indicates that our methods are more stable, learn better\nrepresentations, and extract cleaner corpora for fine-grained analysis.", "published": "2019-10-28 14:17:24", "link": "http://arxiv.org/abs/1910.12698v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating the Factual Consistency of Abstractive Text Summarization", "abstract": "Currently used metrics for assessing summarization algorithms do not account\nfor whether summaries are factually consistent with source documents. We\npropose a weakly-supervised, model-based approach for verifying factual\nconsistency and identifying conflicts between source documents and a generated\nsummary. Training data is generated by applying a series of rule-based\ntransformations to the sentences of source documents. The factual consistency\nmodel is then trained jointly for three tasks: 1) identify whether sentences\nremain factually consistent after transformation, 2) extract a span in the\nsource documents to support the consistency prediction, 3) extract a span in\nthe summary sentence that is inconsistent if one exists. Transferring this\nmodel to summaries generated by several state-of-the art models reveals that\nthis highly scalable approach substantially outperforms previous models,\nincluding those trained with strong supervision using standard datasets for\nnatural language inference and fact checking. Additionally, human evaluation\nshows that the auxiliary span extraction tasks provide useful assistance in the\nprocess of verifying factual consistency.", "published": "2019-10-28 17:51:44", "link": "http://arxiv.org/abs/1910.12840v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sketch-Fill-A-R: A Persona-Grounded Chit-Chat Generation Framework", "abstract": "Human-like chit-chat conversation requires agents to generate responses that\nare fluent, engaging and consistent. We propose Sketch-Fill-A-R, a framework\nthat uses a persona-memory to generate chit-chat responses in three phases.\nFirst, it generates dynamic sketch responses with open slots. Second, it\ngenerates candidate responses by filling slots with parts of its stored persona\ntraits. Lastly, it ranks and selects the final response via a language model\nscore. Sketch-Fill-A-R outperforms a state-of-the-art baseline both\nquantitatively (10-point lower perplexity) and qualitatively (preferred by 55%\nheads-up in single-turn and 20% higher in consistency in multi-turn user\nstudies) on the Persona-Chat dataset. Finally, we extensively analyze\nSketch-Fill-A-R's responses and human feedback, and show it is more consistent\nand engaging by using more relevant responses and questions.", "published": "2019-10-28 23:49:26", "link": "http://arxiv.org/abs/1910.13008v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey on Knowledge Graph Embeddings with Literals: Which model links\n  better Literal-ly?", "abstract": "Knowledge Graphs (KGs) are composed of structured information about a\nparticular domain in the form of entities and relations. In addition to the\nstructured information KGs help in facilitating interconnectivity and\ninteroperability between different resources represented in the Linked Data\nCloud. KGs have been used in a variety of applications such as entity linking,\nquestion answering, recommender systems, etc. However, KG applications suffer\nfrom high computational and storage costs. Hence, there arises the necessity\nfor a representation able to map the high dimensional KGs into low dimensional\nspaces, i.e., embedding space, preserving structural as well as relational\ninformation. This paper conducts a survey of KG embedding models which not only\nconsider the structured information contained in the form of entities and\nrelations in a KG but also the unstructured information represented as literals\nsuch as text, numerical values, images, etc. Along with a theoretical analysis\nand comparison of the methods proposed so far for generating KG embeddings with\nliterals, an empirical evaluation of the different methods under identical\nsettings has been performed for the general task of link prediction.", "published": "2019-10-28 09:06:00", "link": "http://arxiv.org/abs/1910.12507v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "A Comparison of Neural Network Training Methods for Text Classification", "abstract": "We study the impact of neural networks in text classification. Our focus is\non training deep neural networks with proper weight initialization and greedy\nlayer-wise pretraining. Results are compared with 1-layer neural networks and\nSupport Vector Machines. We work with a dataset of labeled messages from the\nTwitter microblogging service and aim to predict weather conditions. A feature\nextraction procedure specific for the task is proposed, which applies\ndimensionality reduction using Latent Semantic Analysis. Our results show that\nneural networks outperform Support Vector Machines with Gaussian kernels,\nnoticing performance gains from introducing additional hidden layers with\nnonlinearities. The impact of using Nesterov's Accelerated Gradient in\nbackpropagation is also studied. We conclude that deep neural networks are a\nreasonable approach for text classification and propose further ideas to\nimprove performance.", "published": "2019-10-28 13:46:42", "link": "http://arxiv.org/abs/1910.12674v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Adversarial Multitask Learning for Joint Multi-Feature and Multi-Dialect\n  Morphological Modeling", "abstract": "Morphological tagging is challenging for morphologically rich languages due\nto the large target space and the need for more training data to minimize model\nsparsity. Dialectal variants of morphologically rich languages suffer more as\nthey tend to be more noisy and have less resources. In this paper we explore\nthe use of multitask learning and adversarial training to address morphological\nrichness and dialectal variations in the context of full morphological tagging.\nWe use multitask learning for joint morphological modeling for the features\nwithin two dialects, and as a knowledge-transfer scheme for cross-dialectal\nmodeling. We use adversarial training to learn dialect invariant features that\ncan help the knowledge-transfer scheme from the high to low-resource variants.\nWe work with two dialectal variants: Modern Standard Arabic (high-resource\n\"dialect\") and Egyptian Arabic (low-resource dialect) as a case study. Our\nmodels achieve state-of-the-art results for both. Furthermore, adversarial\ntraining provides more significant improvement when using smaller training\ndatasets in particular.", "published": "2019-10-28 14:22:56", "link": "http://arxiv.org/abs/1910.12702v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluating Lottery Tickets Under Distributional Shifts", "abstract": "The Lottery Ticket Hypothesis suggests large, over-parameterized neural\nnetworks consist of small, sparse subnetworks that can be trained in isolation\nto reach a similar (or better) test accuracy. However, the initialization and\ngeneralizability of the obtained sparse subnetworks have been recently called\ninto question. Our work focuses on evaluating the initialization of sparse\nsubnetworks under distributional shifts. Specifically, we investigate the\nextent to which a sparse subnetwork obtained in a source domain can be\nre-trained in isolation in a dissimilar, target domain. In addition, we examine\nthe effects of different initialization strategies at transfer-time. Our\nexperiments show that sparse subnetworks obtained through lottery ticket\ntraining do not simply overfit to particular domains, but rather reflect an\ninductive bias of deep neural networks that can be exploited in multiple\ndomains.", "published": "2019-10-28 14:29:28", "link": "http://arxiv.org/abs/1910.12708v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Hierarchical Location Prediction Neural Network for Twitter User\n  Geolocation", "abstract": "Accurate estimation of user location is important for many online services.\nPrevious neural network based methods largely ignore the hierarchical structure\namong locations. In this paper, we propose a hierarchical location prediction\nneural network for Twitter user geolocation. Our model first predicts the home\ncountry for a user, then uses the country result to guide the city-level\nprediction. In addition, we employ a character-aware word embedding layer to\novercome the noisy information in tweets. With the feature fusion layer, our\nmodel can accommodate various feature combinations and achieves\nstate-of-the-art results over three commonly used benchmarks under different\nfeature settings. It not only improves the prediction accuracy but also greatly\nreduces the mean error distance.", "published": "2019-10-28 19:57:02", "link": "http://arxiv.org/abs/1910.12941v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "A Simple but Effective BERT Model for Dialog State Tracking on\n  Resource-Limited Systems", "abstract": "In a task-oriented dialog system, the goal of dialog state tracking (DST) is\nto monitor the state of the conversation from the dialog history. Recently,\nmany deep learning based methods have been proposed for the task. Despite their\nimpressive performance, current neural architectures for DST are typically\nheavily-engineered and conceptually complex, making it difficult to implement,\ndebug, and maintain them in a production setting. In this work, we propose a\nsimple but effective DST model based on BERT. In addition to its simplicity,\nour approach also has a number of other advantages: (a) the number of\nparameters does not grow with the ontology size (b) the model can operate in\nsituations where the domain ontology may change dynamically. Experimental\nresults demonstrate that our BERT-based model outperforms previous methods by a\nlarge margin, achieving new state-of-the-art results on the standard WoZ 2.0\ndataset. Finally, to make the model small and fast enough for\nresource-restricted systems, we apply the knowledge distillation method to\ncompress our model. The final compressed model achieves comparable results with\nthe original model while being 8x smaller and 7x faster.", "published": "2019-10-28 22:41:55", "link": "http://arxiv.org/abs/1910.12995v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Effect of choice of probability distribution, randomness, and search\n  methods for alignment modeling in sequence-to-sequence text-to-speech\n  synthesis using hard alignment", "abstract": "Sequence-to-sequence text-to-speech (TTS) is dominated by\nsoft-attention-based methods. Recently, hard-attention-based methods have been\nproposed to prevent fatal alignment errors, but their sampling method of\ndiscrete alignment is poorly investigated. This research investigates various\ncombinations of sampling methods and probability distributions for alignment\ntransition modeling in a hard-alignment-based sequence-to-sequence TTS method\ncalled SSNT-TTS. We clarify the common sampling methods of discrete variables\nincluding greedy search, beam search, and random sampling from a Bernoulli\ndistribution in a more general way. Furthermore, we introduce the binary\nConcrete distribution to model discrete variables more properly. The results of\na listening test shows that deterministic search is more preferable than\nstochastic search, and the binary Concrete distribution is robust with\nstochastic search for natural alignment transition.", "published": "2019-10-28 00:01:12", "link": "http://arxiv.org/abs/1910.12383v1", "categories": ["eess.AS", "cs.CL", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Unsupervised pre-training for sequence to sequence speech recognition", "abstract": "This paper proposes a novel approach to pre-train encoder-decoder\nsequence-to-sequence (seq2seq) model with unpaired speech and transcripts\nrespectively. Our pre-training method is divided into two stages, named\nacoustic pre-trianing and linguistic pre-training. In the acoustic pre-training\nstage, we use a large amount of speech to pre-train the encoder by predicting\nmasked speech feature chunks with its context. In the linguistic pre-training\nstage, we generate synthesized speech from a large number of transcripts using\na single-speaker text to speech (TTS) system, and use the synthesized paired\ndata to pre-train decoder. This two-stage pre-training method integrates rich\nacoustic and linguistic knowledge into seq2seq model, which will benefit\ndownstream automatic speech recognition (ASR) tasks. The unsupervised\npre-training is finished on AISHELL-2 dataset and we apply the pre-trained\nmodel to multiple paired data ratios of AISHELL-1 and HKUST. We obtain relative\ncharacter error rate reduction (CERR) from 38.24% to 7.88% on AISHELL-1 and\nfrom 12.00% to 1.20% on HKUST. Besides, we apply our pretrained model to a\ncross-lingual case with CALLHOME dataset. For all six languages in CALLHOME\ndataset, our pre-training method makes model outperform baseline consistently.", "published": "2019-10-28 03:17:18", "link": "http://arxiv.org/abs/1910.12418v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Online News Media Website Ranking Using User Generated Content", "abstract": "News media websites are important online resources that have drawn great\nattention of text mining researchers. The main aim of this study is to propose\na framework for ranking online news websites from different viewpoints. The\nranking of news websites is useful information, which can benefit many\nnews-related tasks such as news retrieval and news recommendation. In the\nproposed framework, the ranking of news websites is obtained by calculating\nthree measures introduced in the paper and based on user-generated content.\nEach proposed measure is concerned with the performance of news websites from a\nparticular viewpoint including the completeness of news reports, the diversity\nof events being covered by the website and its speed. The use of user-generated\ncontent in this framework, as a partly-unbiased, real-time and low cost content\non the web distinguishes the proposed news website ranking framework from the\nliterature. The results obtained for three prominent news websites, BBC, CNN,\nNYTimes, show that BBC has the best performance in terms of news completeness\nand speed, and NYTimes has the best diversity in comparison with the other two\nwebsites.", "published": "2019-10-28 04:50:22", "link": "http://arxiv.org/abs/1910.12441v1", "categories": ["cs.IR", "cs.CL", "cs.SI"], "primary_category": "cs.IR"}
{"title": "Towards Successful Social Media Advertising: Predicting the Influence of\n  Commercial Tweets", "abstract": "Businesses communicate using Twitter for a variety of reasons -- to raise\nawareness of their brands, to market new products, to respond to community\ncomments, and to connect with their customers and potential customers in a\ntargeted manner. For businesses to do this effectively, they need to understand\nwhich content and structural elements about a tweet make it influential, that\nis, widely liked, followed, and retweeted. This paper presents a systematic\nmethodology for analyzing commercial tweets, and predicting the influence on\ntheir readers. Our model, which use a combination of decoration and meta\nfeatures, outperforms the prediction ability of the baseline model as well as\nthe tweet embedding model. Further, in order to demonstrate a practical use of\nthis work, we show how an unsuccessful tweet may be engineered (for example,\nreworded) to increase its potential for success.", "published": "2019-10-28 05:14:41", "link": "http://arxiv.org/abs/1910.12446v1", "categories": ["cs.SI", "cs.CL", "cs.IR"], "primary_category": "cs.SI"}
{"title": "Modeling Inter-Speaker Relationship in XLNet for Contextual Spoken\n  Language Understanding", "abstract": "We propose two methods to capture relevant history information in a\nmulti-turn dialogue by modeling inter-speaker relationship for spoken language\nunderstanding (SLU). Our methods are tailored for and therefore compatible with\nXLNet, which is a state-of-the-art pretrained model, so we verified our models\nbuilt on the top of XLNet. In our experiments, all models achieved higher\naccuracy than state-of-the-art contextual SLU models on two benchmark datasets.\nAnalysis on the results demonstrated that the proposed methods are effective to\nimprove SLU accuracy of XLNet. These methods to identify important dialogue\nhistory will be useful to alleviate ambiguity in SLU of the current utterance.", "published": "2019-10-28 10:23:03", "link": "http://arxiv.org/abs/1910.12531v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Exploring Kernel Functions in the Softmax Layer for Contextual Word\n  Classification", "abstract": "Prominently used in support vector machines and logistic regressions, kernel\nfunctions (kernels) can implicitly map data points into high dimensional spaces\nand make it easier to learn complex decision boundaries. In this work, by\nreplacing the inner product function in the softmax layer, we explore the use\nof kernels for contextual word classification. In order to compare the\nindividual kernels, experiments are conducted on standard language modeling and\nmachine translation tasks. We observe a wide range of performances across\ndifferent kernel settings. Extending the results, we look at the gradient\nproperties, investigate various mixture strategies and examine the\ndisambiguation abilities.", "published": "2019-10-28 11:06:21", "link": "http://arxiv.org/abs/1910.12554v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "A BERT-Based Transfer Learning Approach for Hate Speech Detection in\n  Online Social Media", "abstract": "Generated hateful and toxic content by a portion of users in social media is\na rising phenomenon that motivated researchers to dedicate substantial efforts\nto the challenging direction of hateful content identification. We not only\nneed an efficient automatic hate speech detection model based on advanced\nmachine learning and natural language processing, but also a sufficiently large\namount of annotated data to train a model. The lack of a sufficient amount of\nlabelled hate speech data, along with the existing biases, has been the main\nissue in this domain of research. To address these needs, in this study we\nintroduce a novel transfer learning approach based on an existing pre-trained\nlanguage model called BERT (Bidirectional Encoder Representations from\nTransformers). More specifically, we investigate the ability of BERT at\ncapturing hateful context within social media content by using new fine-tuning\nmethods based on transfer learning. To evaluate our proposed approach, we use\ntwo publicly available datasets that have been annotated for racism, sexism,\nhate, or offensive content on Twitter. The results show that our solution\nobtains considerable performance on these datasets in terms of precision and\nrecall in comparison to existing approaches. Consequently, our model can\ncapture some biases in data annotation and collection process and can\npotentially lead us to a more accurate model.", "published": "2019-10-28 12:13:38", "link": "http://arxiv.org/abs/1910.12574v1", "categories": ["cs.SI", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.SI"}
{"title": "Towards Unsupervised Speech Recognition and Synthesis with Quantized\n  Speech Representation Learning", "abstract": "In this paper we propose a Sequential Representation Quantization AutoEncoder\n(SeqRQ-AE) to learn from primarily unpaired audio data and produce sequences of\nrepresentations very close to phoneme sequences of speech utterances. This is\nachieved by proper temporal segmentation to make the representations\nphoneme-synchronized, and proper phonetic clustering to have total number of\ndistinct representations close to the number of phonemes. Mapping between the\ndistinct representations and phonemes is learned from a small amount of\nannotated paired data. Preliminary experiments on LJSpeech demonstrated the\nlearned representations for vowels have relative locations in latent space in\ngood parallel to that shown in the IPA vowel chart defined by linguistics\nexperts. With less than 20 minutes of annotated speech, our method outperformed\nexisting methods on phoneme recognition and is able to synthesize intelligible\nspeech that beats our baseline model.", "published": "2019-10-28 14:50:03", "link": "http://arxiv.org/abs/1910.12729v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Sequence-to-sequence Automatic Speech Recognition with Word Embedding\n  Regularization and Fused Decoding", "abstract": "In this paper, we investigate the benefit that off-the-shelf word embedding\ncan bring to the sequence-to-sequence (seq-to-seq) automatic speech recognition\n(ASR). We first introduced the word embedding regularization by maximizing the\ncosine similarity between a transformed decoder feature and the target word\nembedding. Based on the regularized decoder, we further proposed the fused\ndecoding mechanism. This allows the decoder to consider the semantic\nconsistency during decoding by absorbing the information carried by the\ntransformed decoder feature, which is learned to be close to the target word\nembedding. Initial results on LibriSpeech demonstrated that pre-trained word\nembedding can significantly lower ASR recognition error with a negligible cost,\nand the choice of word embedding algorithms among Skip-gram, CBOW and BERT is\nimportant.", "published": "2019-10-28 15:09:01", "link": "http://arxiv.org/abs/1910.12740v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Learning Data Manipulation for Augmentation and Weighting", "abstract": "Manipulating data, such as weighting data examples or augmenting with new\ninstances, has been increasingly used to improve model training. Previous work\nhas studied various rule- or learning-based approaches designed for specific\ntypes of data manipulation. In this work, we propose a new method that supports\nlearning different manipulation schemes with the same gradient-based algorithm.\nOur approach builds upon a recent connection of supervised learning and\nreinforcement learning (RL), and adapts an off-the-shelf reward learning\nalgorithm from RL for joint data manipulation learning and model training.\nDifferent parameterization of the \"data reward\" function instantiates different\nmanipulation schemes. We showcase data augmentation that learns a text\ntransformation network, and data weighting that dynamically adapts the data\nsample importance. Experiments show the resulting algorithms significantly\nimprove the image and text classification performance in low data regime and\nclass-imbalance problems.", "published": "2019-10-28 16:46:24", "link": "http://arxiv.org/abs/1910.12795v1", "categories": ["cs.LG", "cs.CL", "cs.CV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "A Game Theoretic Approach to Class-wise Selective Rationalization", "abstract": "Selection of input features such as relevant pieces of text has become a\ncommon technique of highlighting how complex neural predictors operate. The\nselection can be optimized post-hoc for trained models or incorporated directly\ninto the method itself (self-explaining). However, an overall selection does\nnot properly capture the multi-faceted nature of useful rationales such as pros\nand cons for decisions. To this end, we propose a new game theoretic approach\nto class-dependent rationalization, where the method is specifically trained to\nhighlight evidence supporting alternative conclusions. Each class involves\nthree players set up competitively to find evidence for factual and\ncounterfactual scenarios. We show theoretically in a simplified scenario how\nthe game drives the solution towards meaningful class-dependent rationales. We\nevaluate the method in single- and multi-aspect sentiment classification tasks\nand demonstrate that the proposed method is able to identify both factual\n(justifying the ground truth label) and counterfactual (countering the ground\ntruth label) rationales consistent with human rationalization. The code for our\nmethod is publicly available.", "published": "2019-10-28 17:59:02", "link": "http://arxiv.org/abs/1910.12853v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Moving Towards Open Set Incremental Learning: Readily Discovering New\n  Authors", "abstract": "The classification of textual data often yields important information. Most\nclassifiers work in a closed world setting where the classifier is trained on a\nknown corpus, and then it is tested on unseen examples that belong to one of\nthe classes seen during training. Despite the usefulness of this design, often\nthere is a need to classify unseen examples that do not belong to any of the\nclasses on which the classifier was trained. This paper describes the open set\nscenario where unseen examples from previously unseen classes are handled while\ntesting. This further examines a process of enhanced open set classification\nwith a deep neural network that discovers new classes by clustering the\nexamples identified as belonging to unknown classes, followed by a process of\nretraining the classifier with newly recognized classes. Through this process\nthe model moves to an incremental learning model where it continuously finds\nand learns from novel classes of data that have been identified automatically.\nThis paper also develops a new metric that measures multiple attributes of\nclustering open set data. Multiple experiments across two author attribution\ndata sets demonstrate the creation an incremental model that produces excellent\nresults.", "published": "2019-10-28 20:01:54", "link": "http://arxiv.org/abs/1910.12944v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Cross-Domain Ambiguity Detection using Linear Transformation of Word\n  Embedding Spaces", "abstract": "The requirements engineering process is a crucial stage of the software\ndevelopment life cycle. It involves various stakeholders from different\nprofessional backgrounds, particularly in the requirements elicitation phase.\nEach stakeholder carries distinct domain knowledge, causing them to differently\ninterpret certain words, leading to cross-domain ambiguity. This can result in\nmisunderstanding amongst them and jeopardize the entire project. This paper\nproposes a natural language processing approach to find potentially ambiguous\nwords for a given set of domains. The idea is to apply linear transformations\non word embedding models trained on different domain corpora, to bring them\ninto a unified embedding space. The approach then finds words with divergent\nembeddings as they signify a variation in the meaning across the domains. It\ncan help a requirements analyst in preventing misunderstandings during\nelicitation interviews and meetings by defining a set of potentially ambiguous\nterms in advance. The paper also discusses certain problems with the existing\napproaches and discusses how the proposed approach resolves them.", "published": "2019-10-28 20:32:56", "link": "http://arxiv.org/abs/1910.12956v3", "categories": ["cs.CL", "cs.LG", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Transformer-Transducer: End-to-End Speech Recognition with\n  Self-Attention", "abstract": "We explore options to use Transformer networks in neural transducer for\nend-to-end speech recognition. Transformer networks use self-attention for\nsequence modeling and comes with advantages in parallel computation and\ncapturing contexts. We propose 1) using VGGNet with causal convolution to\nincorporate positional information and reduce frame rate for efficient\ninference 2) using truncated self-attention to enable streaming for Transformer\nand reduce computational complexity. All experiments are conducted on the\npublic LibriSpeech corpus. The proposed Transformer-Transducer outperforms\nneural transducer with LSTM/BLSTM networks and achieved word error rates of\n6.37 % on the test-clean set and 15.30 % on the test-other set, while remaining\nstreamable, compact with 45.7M parameters for the entire system, and\ncomputationally efficient with complexity of O(T), where T is input sequence\nlength.", "published": "2019-10-28 21:29:21", "link": "http://arxiv.org/abs/1910.12977v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Bin Encoding Training of a Spiking Neural Network-based Voice Activity\n  Detection", "abstract": "Advances of deep learning for Artificial Neural Networks(ANNs) have led to\nsignificant improvements in the performance of digital signal processing\nsystems implemented on digital chips. Although recent progress in low-power\nchips is remarkable, neuromorphic chips that run Spiking Neural Networks (SNNs)\nbased applications offer an even lower power consumption, as a consequence of\nthe ensuing sparse spike-based coding scheme. In this work, we develop a\nSNN-based Voice Activity Detection (VAD) system that belongs to the building\nblocks of any audio and speech processing system. We propose to use the bin\nencoding, a novel method to convert log mel filterbank bins of single-time\nframes into spike patterns. We integrate the proposed scheme in a bilayer\nspiking architecture which was evaluated on the QUT-NOISE-TIMIT corpus. Our\napproach shows that SNNs enable an ultra low-power implementation of a VAD\nclassifier that consumes only 3.8$\\mu$W, while achieving state-of-the-art\nperformance.", "published": "2019-10-28 06:25:03", "link": "http://arxiv.org/abs/1910.12459v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Emotion and Theme Recognition in Music with Frequency-Aware\n  RF-Regularized CNNs", "abstract": "We present CP-JKU submission to MediaEval 2019; a Receptive\nField-(RF)-regularized and Frequency-Aware CNN approach for tagging music with\nemotion/mood labels. We perform an investigation regarding the impact of the RF\nof the CNNs on their performance on this dataset. We observe that ResNets with\nsmaller receptive fields -- originally adapted for acoustic scene\nclassification -- also perform well in the emotion tagging task. We improve the\nperformance of such architectures using techniques such as Frequency Awareness\nand Shake-Shake regularization, which were used in previous work on general\nacoustic recognition tasks.", "published": "2019-10-28 10:19:55", "link": "http://arxiv.org/abs/1911.05833v1", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Accurate and Scalable Version Identification Using Musically-Motivated\n  Embeddings", "abstract": "The version identification (VI) task deals with the automatic detection of\nrecordings that correspond to the same underlying musical piece. Despite many\nefforts, VI is still an open problem, with much room for improvement, specially\nwith regard to combining accuracy and scalability. In this paper, we present\nMOVE, a musically-motivated method for accurate and scalable version\nidentification. MOVE achieves state-of-the-art performance on two\npublicly-available benchmark sets by learning scalable embeddings in an\nEuclidean distance space, using a triplet loss and a hard triplet mining\nstrategy. It improves over previous work by employing an alternative input\nrepresentation, and introducing a novel technique for temporal content\nsummarization, a standardized latent space, and a data augmentation strategy\nspecifically designed for VI. In addition to the main results, we perform an\nablation study to highlight the importance of our design choices, and study the\nrelation between embedding dimensionality and model performance.", "published": "2019-10-28 10:54:53", "link": "http://arxiv.org/abs/1910.12551v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Interrupted and cascaded permutation invariant training for speech\n  separation", "abstract": "Permutation Invariant Training (PIT) has long been a stepping stone method\nfor training speech separation model in handling the label ambiguity problem.\nWith PIT selecting the minimum cost label assignments dynamically, very few\nstudies considered the separation problem to be optimizing both the model\nparameters and the label assignments, but focused on searching for good model\narchitecture and parameters. In this paper, we investigate instead for a given\nmodel architecture the various flexible label assignment strategies for\ntraining the model, rather than directly using PIT. Surprisingly, we discover a\nsignificant performance boost compared to PIT is possible if the model is\ntrained with fixed label assignments and a good set of labels is chosen. With\nfixed label training cascaded between two sections of PIT, we achieved the\nstate-of-the-art performance on WSJ0-2mix without changing the model\narchitecture at all.", "published": "2019-10-28 14:28:12", "link": "http://arxiv.org/abs/1910.12706v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Mixup-breakdown: a consistency training method for improving\n  generalization of speech separation models", "abstract": "Deep-learning based speech separation models confront poor generalization\nproblem that even the state-of-the-art models could abruptly fail when\nevaluating them in mismatch conditions. To address this problem, we propose an\neasy-to-implement yet effective consistency based semi-supervised learning\n(SSL) approach, namely Mixup-Breakdown training (MBT). It learns a teacher\nmodel to \"breakdown\" unlabeled inputs, and the estimated separations are\ninterpolated to produce more useful pseudo \"mixup\" input-output pairs, on which\nthe consistency regularization could apply for learning a student model. In our\nexperiment, we evaluate MBT under various conditions with ascending degrees of\nmismatch, including unseen interfering speech, noise, and music, and compare\nMBT's generalization capability against state-of-the-art supervised learning\nand SSL approaches. The result indicates that MBT significantly outperforms\nseveral strong baselines with up to 13.77% relative SI-SNRi improvement.\nMoreover, MBT only adds negligible computational overhead to standard training\nschemes.", "published": "2019-10-28 03:34:42", "link": "http://arxiv.org/abs/1910.13253v3", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DFSMN-SAN with Persistent Memory Model for Automatic Speech Recognition", "abstract": "Self-attention networks (SAN) have been introduced into automatic speech\nrecognition (ASR) and achieved state-of-the-art performance owing to its\nsuperior ability in capturing long term dependency. One of the key ingredients\nis the self-attention mechanism which can be effectively performed on the whole\nutterance level. In this paper, we try to investigate whether even more\ninformation beyond the whole utterance level can be exploited and beneficial.\nWe propose to apply self-attention layer with augmented memory to ASR.\nSpecifically, we first propose a variant model architecture which combines deep\nfeed-forward sequential memory network (DFSMN) with self-attention layers to\nform a better baseline model compared with a purely self-attention network.\nThen, we propose and compare two kinds of additional memory structures added\ninto self-attention layers. Experiments on large-scale LVCSR tasks show that on\nfour individual test sets, the DFSMN-SAN architecture outperforms vanilla SAN\nencoder by 5% relatively in character error rate (CER). More importantly, the\nadditional memory structure provides further 5% to 11% relative improvement in\nCER.", "published": "2019-10-28 04:58:23", "link": "http://arxiv.org/abs/1910.13282v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The Effect of Erasure Coding on the Burstiness of Packet Loss", "abstract": "The perceived quality of real-time media delivered over IP networks depends\non both the rate and burstiness of packet loss. In this paper, we develop a new\nmathematical model for the residual burstiness of loss under erasure coding. We\nderive the expected number of consecutive losses in a burst as a function of\nerasure coding parameters and the network loss probability assuming a Bernoulli\nmodel for network losses.", "published": "2019-10-28 15:45:03", "link": "http://arxiv.org/abs/1911.03265v1", "categories": ["cs.IT", "cs.SD", "eess.AS", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
