{"title": "Bridging Modalities: Enhancing Cross-Modality Hate Speech Detection with\n  Few-Shot In-Context Learning", "abstract": "The widespread presence of hate speech on the internet, including formats\nsuch as text-based tweets and vision-language memes, poses a significant\nchallenge to digital platform safety. Recent research has developed detection\nmodels tailored to specific modalities; however, there is a notable gap in\ntransferring detection capabilities across different formats. This study\nconducts extensive experiments using few-shot in-context learning with large\nlanguage models to explore the transferability of hate speech detection between\nmodalities. Our findings demonstrate that text-based hate speech examples can\nsignificantly enhance the classification accuracy of vision-language hate\nspeech. Moreover, text-based demonstrations outperform vision-language\ndemonstrations in few-shot learning settings. These results highlight the\neffectiveness of cross-modality knowledge transfer and offer valuable insights\nfor improving hate speech detection systems.", "published": "2024-10-08 01:27:12", "link": "http://arxiv.org/abs/2410.05600v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multimodal Large Language Models and Tunings: Vision, Language, Sensors,\n  Audio, and Beyond", "abstract": "This tutorial explores recent advancements in multimodal pretrained and large\nmodels, capable of integrating and processing diverse data forms such as text,\nimages, audio, and video. Participants will gain an understanding of the\nfoundational concepts of multimodality, the evolution of multimodal research,\nand the key technical challenges addressed by these models. We will cover the\nlatest multimodal datasets and pretrained models, including those beyond vision\nand language. Additionally, the tutorial will delve into the intricacies of\nmultimodal large models and instruction tuning strategies to optimise\nperformance for specific tasks. Hands-on laboratories will offer practical\nexperience with state-of-the-art multimodal models, demonstrating real-world\napplications like visual storytelling and visual question answering. This\ntutorial aims to equip researchers, practitioners, and newcomers with the\nknowledge and skills to leverage multimodal AI. ACM Multimedia 2024 is the\nideal venue for this tutorial, aligning perfectly with our goal of\nunderstanding multimodal pretrained and large language models, and their tuning\nmechanisms.", "published": "2024-10-08 01:41:56", "link": "http://arxiv.org/abs/2410.05608v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Stereotype or Personalization? User Identity Biases Chatbot\n  Recommendations", "abstract": "We demonstrate that when people use large language models (LLMs) to generate\nrecommendations, the LLMs produce responses that reflect both what the user\nwants and who the user is. While personalized recommendations are often desired\nby users, it can be difficult in practice to distinguish cases of bias from\ncases of personalization: we find that models generate racially stereotypical\nrecommendations regardless of whether the user revealed their identity\nintentionally through explicit indications or unintentionally through implicit\ncues. We argue that chatbots ought to transparently indicate when\nrecommendations are influenced by a user's revealed identity characteristics,\nbut observe that they currently fail to do so. Our experiments show that even\nthough a user's revealed identity significantly influences model\nrecommendations (p < 0.001), model responses obfuscate this fact in response to\nuser queries. This bias and lack of transparency occurs consistently across\nmultiple popular consumer LLMs (gpt-4o-mini, gpt-4-turbo, llama-3-70B, and\nclaude-3.5) and for four American racial groups.", "published": "2024-10-08 01:51:55", "link": "http://arxiv.org/abs/2410.05613v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DecorateLM: Data Engineering through Corpus Rating, Tagging, and Editing\n  with Language Models", "abstract": "The performance of Large Language Models (LLMs) is substantially influenced\nby the pretraining corpus, which consists of vast quantities of unsupervised\ndata processed by the models. Despite its critical role in model performance,\nensuring the quality of this data is challenging due to its sheer volume and\nthe absence of sample-level quality annotations and enhancements. In this\npaper, we introduce DecorateLM, a data engineering method designed to refine\nthe pretraining corpus through data rating, tagging and editing. Specifically,\nDecorateLM rates texts against quality criteria, tags texts with hierarchical\nlabels, and edits texts into a more formalized format. Due to the massive size\nof the pretraining corpus, adopting an LLM for decorating the entire corpus is\nless efficient. Therefore, to balance performance with efficiency, we curate a\nmeticulously annotated training corpus for DecorateLM using a large language\nmodel and distill data engineering expertise into a compact 1.2 billion\nparameter small language model (SLM). We then apply DecorateLM to enhance 100\nbillion tokens of the training corpus, selecting 45 billion tokens that\nexemplify high quality and diversity for the further training of another 1.2\nbillion parameter LLM. Our results demonstrate that employing such high-quality\ndata can significantly boost model performance, showcasing a powerful approach\nto enhance the quality of the pretraining corpus.", "published": "2024-10-08 02:42:56", "link": "http://arxiv.org/abs/2410.05639v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unlocking the Capabilities of Thought: A Reasoning Boundary Framework to\n  Quantify and Optimize Chain-of-Thought", "abstract": "Chain-of-Thought (CoT) reasoning has emerged as a promising approach for\nenhancing the performance of large language models (LLMs) on complex reasoning\ntasks. Recently, a series of studies attempt to explain the mechanisms\nunderlying CoT, aiming to deepen the understanding of its efficacy.\nNevertheless, the existing research faces two major challenges: (1) a lack of\nquantitative metrics to assess CoT capabilities and (2) a dearth of guidance on\noptimizing CoT performance. Motivated by this, in this work, we introduce a\nnovel reasoning boundary framework (RBF) to address these challenges. To solve\nthe lack of quantification, we first define a reasoning boundary (RB) to\nquantify the upper-bound of CoT and establish a combination law for RB,\nenabling a practical quantitative approach applicable to various real-world CoT\ntasks. To address the lack of optimization, we propose three categories of RBs.\nWe further optimize these categories with combination laws focused on RB\npromotion and reasoning path optimization for CoT improvement. Through\nextensive experiments on 27 models and 5 tasks, the study validates the\nexistence and rationality of the proposed framework. Furthermore, it explains\nthe effectiveness of 10 CoT strategies and guides optimization from two\nperspectives. We hope this work can provide a comprehensive understanding of\nthe boundaries and optimization strategies for reasoning in LLMs. Our code and\ndata are available at https://github.com/LightChen233/reasoning-boundary.", "published": "2024-10-08 05:26:28", "link": "http://arxiv.org/abs/2410.05695v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Label Confidence Weighted Learning for Target-level Sentence\n  Simplification", "abstract": "Multi-level sentence simplification generates simplified sentences with\nvarying language proficiency levels. We propose Label Confidence Weighted\nLearning (LCWL), a novel approach that incorporates a label confidence\nweighting scheme in the training loss of the encoder-decoder model, setting it\napart from existing confidence-weighting methods primarily designed for\nclassification. Experimentation on English grade-level simplification dataset\nshows that LCWL outperforms state-of-the-art unsupervised baselines.\nFine-tuning the LCWL model on in-domain data and combining with Symmetric Cross\nEntropy (SCE) consistently delivers better simplifications compared to strong\nsupervised methods. Our results highlight the effectiveness of label confidence\nweighting techniques for text simplification tasks with encoder-decoder\narchitectures.", "published": "2024-10-08 07:24:20", "link": "http://arxiv.org/abs/2410.05748v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Few-shot Learning for Multi-label Classification of Scientific\n  Documents with Many Classes", "abstract": "Scientific document classification is a critical task and often involves many\nclasses. However, collecting human-labeled data for many classes is expensive\nand usually leads to label-scarce scenarios. Moreover, recent work has shown\nthat sentence embedding model fine-tuning for few-shot classification is\nefficient, robust, and effective. In this work, we propose FusionSent\n(Fusion-based Sentence Embedding Fine-tuning), an efficient and prompt-free\napproach for few-shot classification of scientific documents with many classes.\nFusionSent uses available training examples and their respective label texts to\ncontrastively fine-tune two different sentence embedding models. Afterward, the\nparameters of both fine-tuned models are fused to combine the complementary\nknowledge from the separate fine-tuning steps into a single model. Finally, the\nresulting sentence embedding model is frozen to embed the training instances,\nwhich are then used as input features to train a classification head. Our\nexperiments show that FusionSent significantly outperforms strong baselines by\nan average of $6.0$ $F_{1}$ points across multiple scientific document\nclassification datasets. In addition, we introduce a new dataset for\nmulti-label classification of scientific documents, which contains 203,961\nscientific articles and 130 classes from the arXiv category taxonomy. Code and\ndata are available at https://github.com/sebischair/FusionSent.", "published": "2024-10-08 07:52:35", "link": "http://arxiv.org/abs/2410.05770v3", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "CodeCipher: Learning to Obfuscate Source Code Against LLMs", "abstract": "While large code language models have made significant strides in AI-assisted\ncoding tasks, there are growing concerns about privacy challenges. The user\ncode is transparent to the cloud LLM service provider, inducing risks of\nunauthorized training, reading, and execution of the user code. In this paper,\nwe propose CodeCipher, a novel method that perturbs privacy from code while\npreserving the original response from LLMs. CodeCipher transforms the LLM's\nembedding matrix so that each row corresponds to a different word in the\noriginal matrix, forming a token-to-token confusion mapping for obfuscating\nsource code. The new embedding matrix is optimized by minimizing the\ntask-specific loss function. To tackle the challenge of the discrete and sparse\nnature of word vector spaces, CodeCipher adopts a discrete optimization\nstrategy that aligns the updated vector to the nearest valid token in the\nvocabulary before each gradient update. We demonstrate the effectiveness of our\napproach on three AI-assisted coding tasks including code completion,\nsummarization, and translation. Results show that our model successfully\nconfuses the privacy in source code while preserving the original LLM's\nperformance.", "published": "2024-10-08 08:28:54", "link": "http://arxiv.org/abs/2410.05797v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Gradual Learning: Optimizing Fine-Tuning with Partially Mastered\n  Knowledge in Large Language Models", "abstract": "During the pretraining phase, large language models (LLMs) acquire vast\namounts of knowledge from extensive text corpora. Nevertheless, in later stages\nsuch as fine-tuning and inference, the model may encounter knowledge not\ncovered in the initial training, which can lead to hallucinations and degraded\nperformance. This issue has a profound impact on the model's capabilities, as\nit will inevitably face out-of-scope knowledge after pretraining. Furthermore,\nfine-tuning is often required to adapt LLMs to domain-specific tasks. However,\nthis phenomenon limits the model's ability to learn and integrate new\ninformation during fine-tuning. The effectiveness of fine-tuning largely\ndepends on the type of knowledge involved. Existing research suggests that\nfine-tuning the model on partially mastered knowledge-for instance,\nquestion-answer pairs where the model has a chance of providing correct\nresponses under non-greedy decoding-can enable the model to acquire new\nknowledge while mitigating hallucination. Notably, this approach can still lead\nto the forgetting of fully mastered knowledge, constraining the fine-tuning\ndataset to a narrower range and limiting the model's overall potential for\nimprovement. Given the model's intrinsic reasoning abilities and the\ninterconnectedness of different knowledge areas, it is likely that as the\nmodel's capacity to utilize existing knowledge improves during fine-tuning,\npreviously unmastered knowledge may become more understandable. To explore this\nhypothesis, we conducted experiments and, based on the results, proposed a\ntwo-stage fine-tuning strategy. This approach not only improves the model's\noverall test accuracy and knowledge retention but also preserves its accuracy\non previously mastered content. When fine-tuning on the WikiQA dataset, our\nmethod increases the amount of knowledge acquired by the model in this stage by\n24%.", "published": "2024-10-08 08:35:16", "link": "http://arxiv.org/abs/2410.05802v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Probing Language Models on Their Knowledge Source", "abstract": "Large Language Models (LLMs) often encounter conflicts between their learned,\ninternal (parametric knowledge, PK) and external knowledge provided during\ninference (contextual knowledge, CK). Understanding how LLMs models prioritize\none knowledge source over the other remains a challenge. In this paper, we\npropose a novel probing framework to explore the mechanisms governing the\nselection between PK and CK in LLMs. Using controlled prompts designed to\ncontradict the model's PK, we demonstrate that specific model activations are\nindicative of the knowledge source employed. We evaluate this framework on\nvarious LLMs of different sizes and demonstrate that mid-layer activations,\nparticularly those related to relations in the input, are crucial in predicting\nknowledge source selection, paving the way for more reliable models capable of\nhandling knowledge conflicts effectively.", "published": "2024-10-08 08:47:11", "link": "http://arxiv.org/abs/2410.05817v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Zero-Shot, Controllable Dialog Planning with LLMs", "abstract": "Recently, Large Language Models (LLMs) have emerged as an alternative to\ntraining task-specific dialog agents, due to their broad reasoning capabilities\nand performance in zero-shot learning scenarios. However, many LLM-based dialog\nsystems fall short in planning towards an overarching dialog goal and therefore\ncannot steer the conversation appropriately. Furthermore, these models struggle\nwith hallucination, making them unsuitable for information access in sensitive\ndomains, such as legal or medical domains, where correctness of information\ngiven to users is critical. The recently introduced task Conversational Tree\nSearch (CTS) proposes the use of dialog graphs to avoid hallucination in\nsensitive domains, however, state-of-the-art agents are Reinforcement Learning\n(RL) based and require long training times, despite excelling at dialog\nstrategy. This paper introduces a novel zero-shot method for controllable CTS\nagents, where LLMs guide the dialog planning through domain graphs by searching\nand pruning relevant graph nodes based on user interaction preferences. We show\nthat these agents significantly outperform state-of-the-art CTS agents\n($p<0.0001$; Barnard Exact test) in simulation. This generalizes to all\navailable CTS domains. Finally, we perform user evaluation to test the agent's\nperformance in the wild, showing that our policy significantly ($p<0.05$;\nBarnard Exact) improves task-success compared to the state-of-the-art RL-based\nCTS agent.", "published": "2024-10-08 08:51:44", "link": "http://arxiv.org/abs/2410.05821v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Session Client-Centered Treatment Outcome Evaluation in\n  Psychotherapy", "abstract": "In psychotherapy, therapeutic outcome assessment, or treatment outcome\nevaluation, is essential for enhancing mental health care by systematically\nevaluating therapeutic processes and outcomes. Existing large language model\napproaches often focus on therapist-centered, single-session evaluations,\nneglecting the client's subjective experience and longitudinal progress across\nmultiple sessions. To address these limitations, we propose IPAEval, a\nclient-Informed Psychological Assessment-based Evaluation framework that\nautomates treatment outcome evaluations from the client's perspective using\nclinical interviews. IPAEval integrates cross-session client-contextual\nassessment and session-focused client-dynamics assessment to provide a\ncomprehensive understanding of therapeutic progress. Experiments on our newly\ndeveloped TheraPhase dataset demonstrate that IPAEval effectively tracks\nsymptom severity and treatment outcomes over multiple sessions, outperforming\nprevious single-session models and validating the benefits of items-aware\nreasoning mechanisms.", "published": "2024-10-08 08:54:38", "link": "http://arxiv.org/abs/2410.05824v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Edit Distances and Their Applications to Downstream Tasks in Research\n  and Commercial Contexts", "abstract": "The tutorial describes the concept of edit distances applied to research and\ncommercial contexts. We use Translation Edit Rate (TER), Levenshtein,\nDamerau-Levenshtein, Longest Common Subsequence and $n$-gram distances to\ndemonstrate the frailty of statistical metrics when comparing text sequences.\nOur discussion disassembles them into their essential components. We discuss\nthe centrality of four editing actions: insert, delete, replace and move words,\nand show their implementations in openly available packages and toolkits. The\napplication of edit distances in downstream tasks often assumes that these\naccurately represent work done by post-editors and real errors that need to be\ncorrected in MT output. We discuss how imperfect edit distances are in\ncapturing the details of this error correction work and the implications for\nresearchers and for commercial applications, of these uses of edit distances.\nIn terms of commercial applications, we discuss their integration in\ncomputer-assisted translation tools and how the perception of the connection\nbetween edit distances and post-editor effort affects the definition of\ntranslator rates.", "published": "2024-10-08 10:21:22", "link": "http://arxiv.org/abs/2410.05881v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Language Models Induce Grammatical Knowledge from Indirect Evidence?", "abstract": "What kinds of and how much data is necessary for language models to induce\ngrammatical knowledge to judge sentence acceptability? Recent language models\nstill have much room for improvement in their data efficiency compared to\nhumans. This paper investigates whether language models efficiently use\nindirect data (indirect evidence), from which they infer sentence\nacceptability. In contrast, humans use indirect evidence efficiently, which is\nconsidered one of the inductive biases contributing to efficient language\nacquisition. To explore this question, we introduce the Wug InDirect Evidence\nTest (WIDET), a dataset consisting of training instances inserted into the\npre-training data and evaluation instances. We inject synthetic instances with\nnewly coined wug words into pretraining data and explore the model's behavior\non evaluation data that assesses grammatical acceptability regarding those\nwords. We prepare the injected instances by varying their levels of\nindirectness and quantity. Our experiments surprisingly show that language\nmodels do not induce grammatical knowledge even after repeated exposure to\ninstances with the same structure but differing only in lexical items from\nevaluation instances in certain language phenomena. Our findings suggest a\npotential direction for future research: developing models that use latent\nindirect evidence to induce grammatical knowledge.", "published": "2024-10-08 13:23:58", "link": "http://arxiv.org/abs/2410.06022v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Training-free LLM-generated Text Detection by Mining Token Probability\n  Sequences", "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in\ngenerating high-quality texts across diverse domains. However, the potential\nmisuse of LLMs has raised significant concerns, underscoring the urgent need\nfor reliable detection of LLM-generated texts. Conventional training-based\ndetectors often struggle with generalization, particularly in cross-domain and\ncross-model scenarios. In contrast, training-free methods, which focus on\ninherent discrepancies through carefully designed statistical features, offer\nimproved generalization and interpretability. Despite this, existing\ntraining-free detection methods typically rely on global text sequence\nstatistics, neglecting the modeling of local discriminative features, thereby\nlimiting their detection efficacy. In this work, we introduce a novel\ntraining-free detector, termed \\textbf{Lastde} that synergizes local and global\nstatistics for enhanced detection. For the first time, we introduce time series\nanalysis to LLM-generated text detection, capturing the temporal dynamics of\ntoken probability sequences. By integrating these local statistics with global\nones, our detector reveals significant disparities between human and\nLLM-generated texts. We also propose an efficient alternative,\n\\textbf{Lastde++} to enable real-time detection. Extensive experiments on six\ndatasets involving cross-domain, cross-model, and cross-lingual detection\nscenarios, under both white-box and black-box settings, demonstrated that our\nmethod consistently achieves state-of-the-art performance. Furthermore, our\napproach exhibits greater robustness against paraphrasing attacks compared to\nexisting baseline methods.", "published": "2024-10-08 14:23:45", "link": "http://arxiv.org/abs/2410.06072v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Listening to Patients: A Framework of Detecting and Mitigating Patient\n  Misreport for Medical Dialogue Generation", "abstract": "Medical Dialogue Systems aim to provide automated healthcare support through\npatient-agent conversations. Previous efforts typically regard patients as\nideal users -- one who accurately and consistently reports their health\nconditions. However, in reality, patients often misreport their symptoms,\nleading to discrepancies between their reports and actual health conditions.\nOverlooking patient misreport will affect the quality of healthcare\nconsultations provided by MDS. To address this issue, we argue that MDS should\n''listen to patients'' and tackle two key challenges: how to detect and\nmitigate patient misreport effectively. In this work, we propose PaMis, a\nframework of detecting and mitigating Patient Misreport for medical dialogue\ngeneration. PaMis first constructs dialogue entity graphs, then detects patient\nmisreport based on graph entropy, and mitigates patient misreport by\nformulating clarifying questions. Experiments indicate that PaMis effectively\nenhances medical response generation, enabling models like GPT-4 to detect and\nmitigate patient misreports, and provide high-quality healthcare assistance.", "published": "2024-10-08 14:49:41", "link": "http://arxiv.org/abs/2410.06094v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Optimizing the Training Schedule of Multilingual NMT using Reinforcement\n  Learning", "abstract": "Multilingual NMT is a viable solution for translating low-resource languages\n(LRLs) when data from high-resource languages (HRLs) from the same language\nfamily is available. However, the training schedule, i.e. the order of\npresentation of languages, has an impact on the quality of such systems. Here,\nin a many-to-one translation setting, we propose to apply two algorithms that\nuse reinforcement learning to optimize the training schedule of NMT: (1)\nTeacher-Student Curriculum Learning and (2) Deep Q Network. The former uses an\nexponentially smoothed estimate of the returns of each action based on the loss\non monolingual or multilingual development subsets, while the latter estimates\nrewards using an additional neural network trained from the history of actions\nselected in different states of the system, together with the rewards received.\nOn a 8-to-1 translation dataset with LRLs and HRLs, our second method improves\nBLEU and COMET scores with respect to both random selection of monolingual\nbatches and shuffled multilingual batches, by adjusting the number of\npresentations of LRL vs. HRL batches.", "published": "2024-10-08 15:20:13", "link": "http://arxiv.org/abs/2410.06118v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Less is More: Making Smaller Language Models Competent Subgraph\n  Retrievers for Multi-hop KGQA", "abstract": "Retrieval-Augmented Generation (RAG) is widely used to inject external\nnon-parametric knowledge into large language models (LLMs). Recent works\nsuggest that Knowledge Graphs (KGs) contain valuable external knowledge for\nLLMs. Retrieving information from KGs differs from extracting it from document\nsets. Most existing approaches seek to directly retrieve relevant subgraphs,\nthereby eliminating the need for extensive SPARQL annotations, traditionally\nrequired by semantic parsing methods. In this paper, we model the subgraph\nretrieval task as a conditional generation task handled by small language\nmodels. Specifically, we define a subgraph identifier as a sequence of\nrelations, each represented as a special token stored in the language models.\nOur base generative subgraph retrieval model, consisting of only 220M\nparameters, achieves competitive retrieval performance compared to\nstate-of-the-art models relying on 7B parameters, demonstrating that small\nlanguage models are capable of performing the subgraph retrieval task.\nFurthermore, our largest 3B model, when plugged with an LLM reader, sets new\nSOTA end-to-end performance on both the WebQSP and CWQ benchmarks. Our model\nand data will be made available online: https://github.com/hwy9855/GSR.", "published": "2024-10-08 15:22:36", "link": "http://arxiv.org/abs/2410.06121v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AgentSquare: Automatic LLM Agent Search in Modular Design Space", "abstract": "Recent advancements in Large Language Models (LLMs) have led to a rapid\ngrowth of agentic systems capable of handling a wide range of complex tasks.\nHowever, current research largely relies on manual, task-specific design,\nlimiting their adaptability to novel tasks. In this paper, we introduce a new\nresearch problem: Modularized LLM Agent Search (MoLAS). We propose a modular\ndesign space that abstracts existing LLM agent designs into four fundamental\nmodules with uniform IO interface: Planning, Reasoning, Tool Use, and Memory.\nBuilding on this design space, we present a novel LLM agent search framework\ncalled AgentSquare, which introduces two core mechanisms, i.e., module\nevolution and recombination, to efficiently search for optimized LLM agents. To\nfurther accelerate the process, we design a performance predictor that uses\nin-context surrogate models to skip unpromising agent designs. Extensive\nexperiments across six benchmarks, covering the diverse scenarios of web,\nembodied, tool use and game applications, show that AgentSquare substantially\noutperforms hand-crafted agents, achieving an average performance gain of 17.2%\nagainst best-known human designs. Moreover, AgentSquare can generate\ninterpretable design insights, enabling a deeper understanding of agentic\narchitecture and its impact on task performance. We believe that the modular\ndesign space and AgentSquare search framework offer a platform for fully\nexploiting the potential of prior successful designs and consolidating the\ncollective efforts of research community. Code repo is available at\nhttps://github.com/tsinghua-fib-lab/AgentSquare.", "published": "2024-10-08 15:52:42", "link": "http://arxiv.org/abs/2410.06153v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Mystery of Compositional Generalization in Graph-based Generative\n  Commonsense Reasoning", "abstract": "While LLMs have emerged as performant architectures for reasoning tasks,\ntheir compositional generalization capabilities have been questioned. In this\nwork, we introduce a Compositional Generalization Challenge for Graph-based\nCommonsense Reasoning (CGGC) that goes beyond previous evaluations that are\nbased on sequences or tree structures - and instead involves a reasoning graph:\nIt requires models to generate a natural sentence based on given concepts and a\ncorresponding reasoning graph, where the presented graph involves a previously\nunseen combination of relation types. To master this challenge, models need to\nlearn how to reason over relation tupels within the graph, and how to compose\nthem when conceptualizing a verbalization. We evaluate seven well-known LLMs\nusing in-context learning and find that performant LLMs still struggle in\ncompositional generalization. We investigate potential causes of this gap by\nanalyzing the structures of reasoning graphs, and find that different\nstructures present varying levels of difficulty for compositional\ngeneralization. Arranging the order of demonstrations according to the\nstructures' difficulty shows that organizing samples in an easy-to-hard schema\nenhances the compositional generalization ability of LLMs.", "published": "2024-10-08 18:14:52", "link": "http://arxiv.org/abs/2410.06272v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FG-PRM: Fine-grained Hallucination Detection and Mitigation in Language\n  Model Mathematical Reasoning", "abstract": "Hallucinations in large language models (LLMs) pose significant challenges in\ntasks requiring complex multi-step reasoning, such as mathematical\nproblem-solving. Existing approaches primarily detect the presence of\nhallucinations but lack a nuanced understanding of their types and\nmanifestations. In this paper, we first introduce a comprehensive taxonomy that\ncategorizes the common hallucinations in mathematical reasoning task into six\ntypes: fabrication, factual inconsistency, context inconsistency, instruction\ninconsistency, logical inconsistency, and logical error. We then propose FG-PRM\n(Fine-Grained Process Reward Model), an augmented model designed to detect and\nmitigate hallucinations in a fine-grained, step-level manner. To address the\nlimitations of manually labeling training data, we propose an automated method\nfor generating fine-grained hallucination data using LLMs. By injecting\nhallucinations into reasoning steps of correct solutions, we create a diverse\nand balanced synthetic dataset for training FG-PRM, which consists of six\nspecialized Process Reward Models (PRMs), each tailored to detect a specific\nhallucination type. Our FG-PRM demonstrates superior performance across two key\ntasks: 1) Fine-grained hallucination detection: classifying hallucination types\nfor each reasoning step; and 2) Verification: ranking multiple LLM-generated\noutputs to select the most accurate solution, mitigating reasoning\nhallucinations. Our experiments show that FG-PRM outperforms ChatGPT-3.5 and\nClaude-3 on fine-grained hallucination detection and substantially boosts the\nperformance of LLMs on GSM8K and MATH benchmarks.", "published": "2024-10-08 19:25:26", "link": "http://arxiv.org/abs/2410.06304v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are Large Language Models State-of-the-art Quality Estimators for\n  Machine Translation of User-generated Content?", "abstract": "This paper investigates whether large language models (LLMs) are\nstate-of-the-art quality estimators for machine translation of user-generated\ncontent (UGC) that contains emotional expressions, without the use of reference\ntranslations. To achieve this, we employ an existing emotion-related dataset\nwith human-annotated errors and calculate quality evaluation scores based on\nthe Multi-dimensional Quality Metrics. We compare the accuracy of several LLMs\nwith that of our fine-tuned baseline models, under in-context learning and\nparameter-efficient fine-tuning (PEFT) scenarios. We find that PEFT of LLMs\nleads to better performance in score prediction with human interpretable\nexplanations than fine-tuned models. However, a manual analysis of LLM outputs\nreveals that they still have problems such as refusal to reply to a prompt and\nunstable output while evaluating machine translation of UGC.", "published": "2024-10-08 20:16:59", "link": "http://arxiv.org/abs/2410.06338v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MLissard: Multilingual Long and Simple Sequential Reasoning Benchmarks", "abstract": "Language models are now capable of solving tasks that require dealing with\nlong sequences consisting of hundreds of thousands of tokens. However, they\noften fail on tasks that require repetitive use of simple rules, even on\nsequences that are much shorter than those seen during training. For example,\nstate-of-the-art LLMs can find common items in two lists with up to 20 items\nbut fail when lists have 80 items. In this paper, we introduce MLissard, a\nmultilingual benchmark designed to evaluate models' abilities to process and\ngenerate texts of varied lengths and offers a mechanism for controlling\nsequence complexity.\n  Our evaluation of open-source and proprietary models show a consistent\ndecline in performance across all models and languages as the complexity of the\nsequence increases. Surprisingly, the use of in-context examples in languages\nother than English helps increase extrapolation performance significantly. The\ndatasets and code are available at https://github.com/unicamp-dl/Lissard", "published": "2024-10-08 21:59:31", "link": "http://arxiv.org/abs/2410.06396v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ClaimBrush: A Novel Framework for Automated Patent Claim Refinement\n  Based on Large Language Models", "abstract": "Automatic refinement of patent claims in patent applications is crucial from\nthe perspective of intellectual property strategy. In this paper, we propose\nClaimBrush, a novel framework for automated patent claim refinement that\nincludes a dataset and a rewriting model. We constructed a dataset for training\nand evaluating patent claim rewriting models by collecting a large number of\nactual patent claim rewriting cases from the patent examination process. Using\nthe constructed dataset, we built an automatic patent claim rewriting model by\nfine-tuning a large language model. Furthermore, we enhanced the performance of\nthe automatic patent claim rewriting model by applying preference optimization\nbased on a prediction model of patent examiners' Office Actions. The\nexperimental results showed that our proposed rewriting model outperformed\nheuristic baselines and zero-shot learning in state-of-the-art large language\nmodels. Moreover, preference optimization based on patent examiners'\npreferences boosted the performance of patent claim refinement.", "published": "2024-10-08 00:20:54", "link": "http://arxiv.org/abs/2410.05575v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ParallelSpec: Parallel Drafter for Efficient Speculative Decoding", "abstract": "Speculative decoding has proven to be an efficient solution to large language\nmodel (LLM) inference, where the small drafter predicts future tokens at a low\ncost, and the target model is leveraged to verify them in parallel. However,\nmost existing works still draft tokens auto-regressively to maintain sequential\ndependency in language modeling, which we consider a huge computational burden\nin speculative decoding. We present ParallelSpec, an alternative to\nauto-regressive drafting strategies in state-of-the-art speculative decoding\napproaches. In contrast to auto-regressive drafting in the speculative stage,\nwe train a parallel drafter to serve as an efficient speculative model.\nParallelSpec learns to efficiently predict multiple future tokens in parallel\nusing a single model, and it can be integrated into any speculative decoding\nframework that requires aligning the output distributions of the drafter and\nthe target model with minimal training cost. Experimental results show that\nParallelSpec accelerates baseline methods in latency up to 62% on text\ngeneration benchmarks from different domains, and it achieves 2.84X overall\nspeedup on the Llama-2-13B model using third-party evaluation criteria.", "published": "2024-10-08 01:05:08", "link": "http://arxiv.org/abs/2410.05589v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Vector-ICL: In-context Learning with Continuous Vector Representations", "abstract": "Large language models (LLMs) have shown remarkable in-context learning (ICL)\ncapabilities on textual data. We explore whether these capabilities can be\nextended to continuous vectors from diverse domains, obtained from black-box\npretrained encoders. By aligning input data with an LLM's embedding space\nthrough lightweight projectors, we observe that LLMs can effectively process\nand learn from these projected vectors, which we term Vector-ICL. In\nparticular, we find that pretraining projectors with general language modeling\nobjectives enables Vector-ICL, while task-specific finetuning further enhances\nperformance. In our experiments across various tasks and modalities, including\ntext reconstruction, numerical function regression, text classification,\nsummarization, molecule captioning, time-series classification, graph\nclassification, and fMRI decoding, Vector-ICL often surpasses both few-shot ICL\nand domain-specific model or tuning. We further conduct analyses and case\nstudies, indicating the potential of LLMs to process vector representations\nbeyond traditional token-based paradigms.", "published": "2024-10-08 02:25:38", "link": "http://arxiv.org/abs/2410.05629v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Does RoBERTa Perform Better than BERT in Continual Learning: An\n  Attention Sink Perspective", "abstract": "Continual learning (CL) aims to train models that can sequentially learn new\ntasks without forgetting previous tasks' knowledge. Although previous works\nobserved that pre-training can benefit CL, it remains unclear whether a\npre-trained model with higher downstream capacity also performs better in CL.\nIn this paper, we observe that pre-trained models may allocate high attention\nscores to some 'sink' tokens, such as [SEP] tokens, which are ubiquitous across\nvarious tasks. Such attention sinks may lead to models' over-smoothing in\nsingle-task learning and interference in sequential tasks' learning, which may\ncompromise the models' CL performance despite their high pre-trained\ncapabilities. To reduce these effects, we propose a pre-scaling mechanism that\nencourages attention diversity across all tokens. Specifically, it first scales\nthe task's attention to the non-sink tokens in a probing stage, and then\nfine-tunes the model with scaling. Experiments show that pre-scaling yields\nsubstantial improvements in CL without experience replay, or progressively\nstoring parameters from previous tasks.", "published": "2024-10-08 02:56:47", "link": "http://arxiv.org/abs/2410.05648v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A Two-Step Approach for Data-Efficient French Pronunciation Learning", "abstract": "Recent studies have addressed intricate phonological phenomena in French,\nrelying on either extensive linguistic knowledge or a significant amount of\nsentence-level pronunciation data. However, creating such resources is\nexpensive and non-trivial. To this end, we propose a novel two-step approach\nthat encompasses two pronunciation tasks: grapheme-to-phoneme and post-lexical\nprocessing. We then investigate the efficacy of the proposed approach with a\nnotably limited amount of sentence-level pronunciation data. Our findings\ndemonstrate that the proposed two-step approach effectively mitigates the lack\nof extensive labeled data, and serves as a feasible solution for addressing\nFrench phonological phenomena even under resource-constrained environments.", "published": "2024-10-08 05:30:23", "link": "http://arxiv.org/abs/2410.05698v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Information Discovery in e-Commerce", "abstract": "Electronic commerce, or e-commerce, is the buying and selling of goods and\nservices, or the transmitting of funds or data online. E-commerce platforms\ncome in many kinds, with global players such as Amazon, Airbnb, Alibaba, eBay\nand platforms targeting specific geographic regions. Information retrieval has\na natural role to play in e-commerce, especially in connecting people to goods\nand services. Information discovery in e-commerce concerns different types of\nsearch (e.g., exploratory search vs. lookup tasks), recommender systems, and\nnatural language processing in e-commerce portals. The rise in popularity of\ne-commerce sites has made research on information discovery in e-commerce an\nincreasingly active research area. This is witnessed by an increase in\npublications and dedicated workshops in this space. Methods for information\ndiscovery in e-commerce largely focus on improving the effectiveness of\ne-commerce search and recommender systems, on enriching and using knowledge\ngraphs to support e-commerce, and on developing innovative question answering\nand bot-based solutions that help to connect people to goods and services. In\nthis survey, an overview is given of the fundamental infrastructure,\nalgorithms, and technical solutions for information discovery in e-commerce.\nThe topics covered include user behavior and profiling, search, recommendation,\nand language technology in e-commerce.", "published": "2024-10-08 07:41:01", "link": "http://arxiv.org/abs/2410.05763v3", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Song Emotion Classification of Lyrics with Out-of-Domain Data under\n  Label Scarcity", "abstract": "Songs have been found to profoundly impact human emotions, with lyrics having\nsignificant power to stimulate emotional changes in the audience. There is a\nscarcity of large, high quality in-domain datasets for lyrics-based song\nemotion classification (Edmonds and Sedoc, 2021; Zhou, 2022). It has been noted\nthat in-domain training datasets are often difficult to acquire (Zhang and\nMiao, 2023) and that label acquisition is often limited by cost, time, and\nother factors (Azad et al., 2018). We examine the novel usage of a large\nout-of-domain dataset as a creative solution to the challenge of training data\nscarcity in the emotional classification of song lyrics. We find that CNN\nmodels trained on a large Reddit comments dataset achieve satisfactory\nperformance and generalizability to lyrical emotion classification, thus giving\ninsights into and a promising possibility in leveraging large, publicly\navailable out-of-domain datasets for domains whose in-domain data are lacking\nor costly to acquire.", "published": "2024-10-08 07:58:15", "link": "http://arxiv.org/abs/2410.05778v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Retrieving, Rethinking and Revising: The Chain-of-Verification Can\n  Improve Retrieval Augmented Generation", "abstract": "Recent Retrieval Augmented Generation (RAG) aims to enhance Large Language\nModels (LLMs) by incorporating extensive knowledge retrieved from external\nsources. However, such approach encounters some challenges: Firstly, the\noriginal queries may not be suitable for precise retrieval, resulting in\nerroneous contextual knowledge; Secondly, the language model can easily\ngenerate inconsistent answer with external references due to their knowledge\nboundary limitation. To address these issues, we propose the\nchain-of-verification (CoV-RAG) to enhance the external retrieval correctness\nand internal generation consistency. Specifically, we integrate the\nverification module into the RAG, engaging in scoring, judgment, and rewriting.\nTo correct external retrieval errors, CoV-RAG retrieves new knowledge using a\nrevised query. To correct internal generation errors, we unify QA and\nverification tasks with a Chain-of-Thought (CoT) reasoning during training. Our\ncomprehensive experiments across various LLMs demonstrate the effectiveness and\nadaptability compared with other strong baselines. Especially, our CoV-RAG can\nsignificantly surpass the state-of-the-art baselines using different LLM\nbackbones.", "published": "2024-10-08 08:34:54", "link": "http://arxiv.org/abs/2410.05801v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Communicating with Speakers and Listeners of Different Pragmatic Levels", "abstract": "This paper explores the impact of variable pragmatic competence on\ncommunicative success through simulating language learning and conversing\nbetween speakers and listeners with different levels of reasoning abilities.\nThrough studying this interaction, we hypothesize that matching levels of\nreasoning between communication partners would create a more beneficial\nenvironment for communicative success and language learning. Our research\nfindings indicate that learning from more explicit, literal language is\nadvantageous, irrespective of the learner's level of pragmatic competence.\nFurthermore, we find that integrating pragmatic reasoning during language\nlearning, not just during evaluation, significantly enhances overall\ncommunication performance. This paper provides key insights into the importance\nof aligning reasoning levels and incorporating pragmatic reasoning in\noptimizing communicative interactions.", "published": "2024-10-08 09:42:37", "link": "http://arxiv.org/abs/2410.05851v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "From Tokens to Words: On the Inner Lexicon of LLMs", "abstract": "Natural language is composed of words, but modern large language models\n(LLMs) process sub-words as input. A natural question raised by this\ndiscrepancy is whether LLMs encode words internally, and if so how. We present\nevidence that LLMs engage in an intrinsic detokenization process, where\nsub-word sequences are combined into coherent whole-word representations at\ntheir last token. Our experiments show that this process primarily takes place\nwithin the early and middle layers of the model. We further demonstrate its\nrobustness to arbitrary splits (e.g., \"cats\" to \"ca\" and \"ts\"), typos, and\nimportantly-to out-of-vocabulary words: when feeding the last token internal\nrepresentations of such words to the model as input, it can \"understand\" them\nas the complete word despite never seeing such representations as input during\ntraining. Our findings suggest that LLMs maintain a latent vocabulary beyond\nthe tokenizer's scope. These insights provide a practical, finetuning-free\napplication for expanding the vocabulary of pre-trained models. By enabling the\naddition of new vocabulary words, we reduce input length and inference\niterations, which reduces both space and model latency, with little to no loss\nin model accuracy.", "published": "2024-10-08 09:53:35", "link": "http://arxiv.org/abs/2410.05864v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MEXA: Multilingual Evaluation of English-Centric LLMs via Cross-Lingual\n  Alignment", "abstract": "English-centric large language models (LLMs) often show strong multilingual\ncapabilities. However, the multilingual performance of these models remains\nunclear and is not thoroughly evaluated for many languages. Most benchmarks for\nmultilinguality focus on classic NLP tasks, or cover a minimal number of\nlanguages. We introduce MEXA, a method for assessing the multilingual\ncapabilities of pre-trained English-centric LLMs using parallel sentences,\nwhich are available for more languages than existing downstream tasks. MEXA\nleverages the fact that English-centric LLMs use English as a kind of pivot\nlanguage in their intermediate layers. It computes the alignment between\nEnglish and non-English languages using parallel sentences to evaluate the\ntransfer of language understanding from English to other languages. This\nalignment can be used to estimate model performance in other languages. We\nconduct studies using various parallel datasets (FLORES-200 and Bible), models\n(Llama family, Gemma family, Mistral, and OLMo), and established downstream\ntasks (Belebele, m-MMLU, and m-ARC). We explore different methods to compute\nembeddings in decoder-only models. Our results show that MEXA, in its default\nsettings, achieves a statistically significant average Pearson correlation of\n0.90 with three established downstream tasks across nine models and two\nparallel datasets. This suggests that MEXA is a reliable method for estimating\nthe multilingual capabilities of English-centric LLMs, providing a clearer\nunderstanding of their multilingual potential and the inner workings of LLMs.\nLeaderboard: https://huggingface.co/spaces/cis-lmu/Mexa, Code:\nhttps://github.com/cisnlp/Mexa.", "published": "2024-10-08 09:59:23", "link": "http://arxiv.org/abs/2410.05873v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Automatic Summarization of Long Documents", "abstract": "A vast amount of textual data is added to the internet daily, making\nutilization and interpretation of such data difficult and cumbersome. As a\nresult, automatic text summarization is crucial for extracting relevant\ninformation, saving precious reading time. Although many transformer-based\nmodels excel in summarization, they are constrained by their input size,\npreventing them from processing texts longer than their context size. This\nstudy introduces three novel algorithms that allow any LLM to efficiently\novercome its input size limitation, effectively utilizing its full potential\nwithout any architectural modifications. We test our algorithms on texts with\nmore than 70,000 words, and our experiments show a significant increase in\nBERTScore with competitive ROUGE scores.", "published": "2024-10-08 11:00:49", "link": "http://arxiv.org/abs/2410.05903v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TOWER: Tree Organized Weighting for Evaluating Complex Instructions", "abstract": "Evaluating the ability of large language models (LLMs) to follow complex\nhuman-written instructions is essential for their deployment in real-world\napplications. While benchmarks like Chatbot Arena use human judges to assess\nmodel performance, they are resource-intensive and time-consuming. Alternative\nmethods using LLMs as judges, such as AlpacaEval, MT Bench, WildBench, and\nInFoBench offer improvements but still do not capture that certain complex\ninstruction aspects are more important than others to follow.\n  To address this gap, we propose a novel evaluation metric, \\textsc{TOWER},\nthat incorporates human-judged importance into the assessment of complex\ninstruction following. We show that human annotators agree with tree-based\nrepresentations of these complex instructions nearly as much as they agree with\nother human annotators. We release tree-based annotations of the InFoBench\ndataset and the corresponding evaluation code to facilitate future research.", "published": "2024-10-08 14:46:50", "link": "http://arxiv.org/abs/2410.06089v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Decoding Decoded: Understanding Hyperparameter Effects in Open-Ended\n  Text Generation", "abstract": "Decoding strategies for generative large language models (LLMs) are a\ncritical but often underexplored aspect of text generation tasks. Guided by\nspecific hyperparameters, these strategies aim to transform the raw probability\ndistributions produced by language models into coherent, fluent text. In this\nstudy, we undertake a large-scale empirical assessment of a range of decoding\nmethods, open-source LLMs, textual domains, and evaluation protocols to\ndetermine how hyperparameter choices shape the outputs. Our experiments include\nboth factual (e.g., news) and creative (e.g., fiction) domains, and incorporate\na broad suite of automatic evaluation metrics alongside human judgments.\nThrough extensive sensitivity analyses, we distill practical recommendations\nfor selecting and tuning hyperparameters, noting that optimal configurations\nvary across models and tasks. By synthesizing these insights, this study\nprovides actionable guidance for refining decoding strategies, enabling\nresearchers and practitioners to achieve higher-quality, more reliable, and\ncontext-appropriate text generation outcomes.", "published": "2024-10-08 14:51:03", "link": "http://arxiv.org/abs/2410.06097v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Temporal Reasoning Transfer from Text to Video", "abstract": "Video Large Language Models (Video LLMs) have shown promising capabilities in\nvideo comprehension, yet they struggle with tracking temporal changes and\nreasoning about temporal relationships. While previous research attributed this\nlimitation to the ineffective temporal encoding of visual inputs, our\ndiagnostic study reveals that video representations contain sufficient\ninformation for even small probing classifiers to achieve perfect accuracy.\nSurprisingly, we find that the key bottleneck in Video LLMs' temporal reasoning\ncapability stems from the underlying LLM's inherent difficulty with temporal\nconcepts, as evidenced by poor performance on textual temporal\nquestion-answering tasks. Building on this discovery, we introduce the Textual\nTemporal reasoning Transfer (T3). T3 synthesizes diverse temporal reasoning\ntasks in pure text format from existing image-text datasets, addressing the\nscarcity of video samples with complex temporal scenarios. Remarkably, without\nusing any video data, T3 enhances LongVA-7B's temporal understanding, yielding\na 5.3 absolute accuracy improvement on the challenging TempCompass benchmark,\nwhich enables our model to outperform ShareGPT4Video-8B trained on 28,000 video\nsamples. Additionally, the enhanced LongVA-7B model achieves competitive\nperformance on comprehensive video benchmarks. For example, it achieves a 49.7\naccuracy on the Temporal Reasoning task of Video-MME, surpassing powerful\nlarge-scale models such as InternVL-Chat-V1.5-20B and VILA1.5-40B. Further\nanalysis reveals a strong correlation between textual and video temporal task\nperformance, validating the efficacy of transferring temporal reasoning\nabilities from text to video domains.", "published": "2024-10-08 16:10:29", "link": "http://arxiv.org/abs/2410.06166v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Multimodal Situational Safety", "abstract": "Multimodal Large Language Models (MLLMs) are rapidly evolving, demonstrating\nimpressive capabilities as multimodal assistants that interact with both humans\nand their environments. However, this increased sophistication introduces\nsignificant safety concerns. In this paper, we present the first evaluation and\nanalysis of a novel safety challenge termed Multimodal Situational Safety,\nwhich explores how safety considerations vary based on the specific situation\nin which the user or agent is engaged. We argue that for an MLLM to respond\nsafely, whether through language or action, it often needs to assess the safety\nimplications of a language query within its corresponding visual context. To\nevaluate this capability, we develop the Multimodal Situational Safety\nbenchmark (MSSBench) to assess the situational safety performance of current\nMLLMs. The dataset comprises 1,820 language query-image pairs, half of which\nthe image context is safe, and the other half is unsafe. We also develop an\nevaluation framework that analyzes key safety aspects, including explicit\nsafety reasoning, visual understanding, and, crucially, situational safety\nreasoning. Our findings reveal that current MLLMs struggle with this nuanced\nsafety problem in the instruction-following setting and struggle to tackle\nthese situational safety challenges all at once, highlighting a key area for\nfuture research. Furthermore, we develop multi-agent pipelines to coordinately\nsolve safety challenges, which shows consistent improvement in safety over the\noriginal MLLM response. Code and data: mssbench.github.io.", "published": "2024-10-08 16:16:07", "link": "http://arxiv.org/abs/2410.06172v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Neural-Bayesian Program Learning for Few-shot Dialogue Intent Parsing", "abstract": "With the growing importance of customer service in contemporary business,\nrecognizing the intents behind service dialogues has become essential for the\nstrategic success of enterprises. However, the nature of dialogue data varies\nsignificantly across different scenarios, and implementing an intent parser for\na specific domain often involves tedious feature engineering and a heavy\nworkload of data labeling. In this paper, we propose a novel Neural-Bayesian\nProgram Learning model named Dialogue-Intent Parser (DI-Parser), which\nspecializes in intent parsing under data-hungry settings and offers promising\nperformance improvements. DI-Parser effectively utilizes data from multiple\nsources in a \"Learning to Learn\" manner and harnesses the \"wisdom of the crowd\"\nthrough few-shot learning capabilities on human-annotated datasets.\nExperimental results demonstrate that DI-Parser outperforms state-of-the-art\ndeep learning models and offers practical advantages for industrial-scale\napplications.", "published": "2024-10-08 16:54:00", "link": "http://arxiv.org/abs/2410.06190v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "EgoSocialArena: Benchmarking the Social Intelligence of Large Language\n  Models from a First-person Perspective", "abstract": "Social intelligence is built upon three foundational pillars: cognitive\nintelligence, situational intelligence, and behavioral intelligence. As large\nlanguage models (LLMs) become increasingly integrated into our social lives,\nunderstanding, evaluating, and developing their social intelligence are\nbecoming increasingly important. While multiple existing works have\ninvestigated the social intelligence of LLMs, (1) most focus on a specific\naspect, and the social intelligence of LLMs has yet to be systematically\norganized and studied; (2) position LLMs as passive observers from a\nthird-person perspective, such as in Theory of Mind (ToM) tests. Compared to\nthe third-person perspective, ego-centric first-person perspective evaluation\ncan align well with actual LLM-based Agent use scenarios. (3) a lack of\ncomprehensive evaluation of behavioral intelligence, with specific emphasis on\nincorporating critical human-machine interaction scenarios. In light of this,\nwe present EgoSocialArena, a novel framework grounded in the three pillars of\nsocial intelligence: cognitive, situational, and behavioral intelligence, aimed\nto systematically evaluate the social intelligence of LLMs from a first-person\nperspective. With EgoSocialArena, we conduct a comprehensive evaluation of\neight prominent foundation models, even the most advanced LLMs like O1-preview\nlag behind human performance.", "published": "2024-10-08 16:55:51", "link": "http://arxiv.org/abs/2410.06195v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Integrating Planning into Single-Turn Long-Form Text Generation", "abstract": "Generating high-quality, in-depth textual documents, such as academic papers,\nnews articles, Wikipedia entries, and books, remains a significant challenge\nfor Large Language Models (LLMs). In this paper, we propose to use planning to\ngenerate long form content. To achieve our goal, we generate intermediate steps\nvia an auxiliary task that teaches the LLM to plan, reason and structure before\ngenerating the final text. Our main novelty lies in a single auxiliary task\nthat does not require multiple rounds of prompting or planning. To overcome the\nscarcity of training data for these intermediate steps, we leverage LLMs to\ngenerate synthetic intermediate writing data such as outlines, key information\nand summaries from existing full articles. Our experiments demonstrate on two\ndatasets from different domains, namely the scientific news dataset SciNews and\nWikipedia datasets in KILT-Wiki and FreshWiki, that LLMs fine-tuned with the\nauxiliary task generate higher quality documents. We observed +2.5% improvement\nin ROUGE-Lsum, and a strong 3.60 overall win/loss ratio via human SxS\nevaluation, with clear wins in organization, relevance, and verifiability.", "published": "2024-10-08 17:02:40", "link": "http://arxiv.org/abs/2410.06203v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Round and Round We Go! What makes Rotary Positional Encodings useful?", "abstract": "Positional Encodings (PEs) are a critical component of Transformer-based\nLarge Language Models (LLMs), providing the attention mechanism with important\nsequence-position information. One of the most popular types of encoding used\ntoday in LLMs are Rotary Positional Encodings (RoPE), that rotate the queries\nand keys based on their relative distance. A common belief is that RoPE is\nuseful because it helps to decay token dependency as relative distance\nincreases. In this work, we argue that this is unlikely to be the core reason.\nWe study the internals of a trained Gemma 7B model to understand how RoPE is\nbeing used at a mechanical level. We find that Gemma learns to use RoPE to\nconstruct robust \"positional\" attention patterns by exploiting the highest\nfrequencies. We also find that, in general, Gemma greatly prefers to use the\nlowest frequencies of RoPE, which we suspect are used to carry semantic\ninformation. We mathematically prove interesting behaviours of RoPE and conduct\nexperiments to verify our findings, proposing a modification of RoPE that fixes\nsome highlighted issues and improves performance. We believe that this work\nrepresents an interesting step in better understanding PEs in LLMs, which we\nbelieve holds crucial value for scaling LLMs to large sizes and context\nlengths.", "published": "2024-10-08 17:07:01", "link": "http://arxiv.org/abs/2410.06205v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mixture Compressor for Mixture-of-Experts LLMs Gains More", "abstract": "Mixture-of-Experts large language models (MoE-LLMs) marks a significant step\nforward of language models, however, they encounter two critical challenges in\npractice: 1) expert parameters lead to considerable memory consumption and\nloading latency; and 2) the current activated experts are redundant, as many\ntokens may only require a single expert. Motivated by these issues, we\ninvestigate the MoE-LLMs and make two key observations: a) different experts\nexhibit varying behaviors on activation reconstruction error, routing scores,\nand activated frequencies, highlighting their differing importance, and b) not\nall tokens are equally important -- only a small subset is critical. Building\non these insights, we propose MC, a training-free Mixture-Compressor for\nMoE-LLMs, which leverages the significance of both experts and tokens to\nachieve an extreme compression. First, to mitigate storage and loading\noverheads, we introduce Pre-Loading Mixed-Precision Quantization, which\nformulates the adaptive bit-width allocation as a Linear Programming problem,\nwhere the objective function balances multi-factors reflecting the importance\nof each expert. Additionally, we develop Online Dynamic Pruning, which\nidentifies important tokens to retain and dynamically select activated experts\nfor other tokens during inference to optimize efficiency while maintaining\nperformance. Our MC integrates static quantization and dynamic pruning to\ncollaboratively achieve extreme compression for MoE-LLMs with less accuracy\nloss, ensuring an optimal trade-off between performance and efficiency.\nExtensive experiments confirm the effectiveness of our approach. For instance,\nat 2.54 bits, MC compresses 76.6% of the model, with only a 3.8% average\naccuracy loss. During dynamic inference, we further reduce activated parameters\nby 15%, with a performance drop of less than 0.6%.", "published": "2024-10-08 18:09:38", "link": "http://arxiv.org/abs/2410.06270v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Probing the Robustness of Theory of Mind in Large Language Models", "abstract": "With the success of ChatGPT and other similarly sized SotA LLMs, claims of\nemergent human like social reasoning capabilities, especially Theory of Mind\n(ToM), in these models have appeared in the scientific literature. On the one\nhand those ToM-capabilities have been successfully tested using tasks styled\nsimilar to those used in psychology (Kosinski, 2023). On the other hand, follow\nup studies showed that those capabilities vanished when the tasks were slightly\naltered (Ullman, 2023). In this work we introduce a novel dataset of 68 tasks\nfor probing ToM in LLMs, including potentially challenging variations which are\nassigned to 10 complexity classes. This way it is providing novel insights into\nthe challenges LLMs face with those task variations. We evaluate the ToM\nperformance of four SotA open source LLMs on our dataset and the dataset\nintroduced by (Kosinski, 2023). The overall low goal accuracy across all\nevaluated models indicates only a limited degree of ToM capabilities. The LLMs'\nperformance on simple complexity class tasks from both datasets are similar.\nWhereas we find a consistent tendency in all tested LLMs to perform poorly on\ntasks that require the realization that an agent has knowledge of automatic\nstate changes in its environment, even when those are spelled out to the model.\nFor task complications that change the relationship between objects by\nreplacing prepositions, we notice a performance drop in all models, with the\nstrongest impact on the mixture-of-experts model. With our dataset of tasks\ngrouped by complexity we offer directions for further research on how to\nstabilize and advance ToM capabilities in LLM.", "published": "2024-10-08 18:13:27", "link": "http://arxiv.org/abs/2410.06271v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Temporal Image Caption Retrieval Competition -- Description and Results", "abstract": "Multimodal models, which combine visual and textual information, have\nrecently gained significant recognition. This paper addresses the multimodal\nchallenge of Text-Image retrieval and introduces a novel task that extends the\nmodalities to include temporal data. The Temporal Image Caption Retrieval\nCompetition (TICRC) presented in this paper is based on the Chronicling America\nand Challenging America projects, which offer access to an extensive collection\nof digitized historic American newspapers spanning 274 years. In addition to\nthe competition results, we provide an analysis of the delivered dataset and\nthe process of its creation.", "published": "2024-10-08 19:45:53", "link": "http://arxiv.org/abs/2410.06314v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Counterfactual Causal Inference in Natural Language with Large Language\n  Models", "abstract": "Causal structure discovery methods are commonly applied to structured data\nwhere the causal variables are known and where statistical testing can be used\nto assess the causal relationships. By contrast, recovering a causal structure\nfrom unstructured natural language data such as news articles contains numerous\nchallenges due to the absence of known variables or counterfactual data to\nestimate the causal links. Large Language Models (LLMs) have shown promising\nresults in this direction but also exhibit limitations. This work investigates\nLLM's abilities to build causal graphs from text documents and perform\ncounterfactual causal inference. We propose an end-to-end causal structure\ndiscovery and causal inference method from natural language: we first use an\nLLM to extract the instantiated causal variables from text data and build a\ncausal graph. We merge causal graphs from multiple data sources to represent\nthe most exhaustive set of causes possible. We then conduct counterfactual\ninference on the estimated graph. The causal graph conditioning allows\nreduction of LLM biases and better represents the causal estimands. We use our\nmethod to show that the limitations of LLMs in counterfactual causal reasoning\ncome from prediction errors and propose directions to mitigate them. We\ndemonstrate the applicability of our method on real-world news articles.", "published": "2024-10-08 21:53:07", "link": "http://arxiv.org/abs/2410.06392v1", "categories": ["cs.CL", "cs.LG", "I.2.6; I.2.7; G.3; J.1"], "primary_category": "cs.CL"}
{"title": "ERVQA: A Dataset to Benchmark the Readiness of Large Vision Language\n  Models in Hospital Environments", "abstract": "The global shortage of healthcare workers has demanded the development of\nsmart healthcare assistants, which can help monitor and alert healthcare\nworkers when necessary. We examine the healthcare knowledge of existing Large\nVision Language Models (LVLMs) via the Visual Question Answering (VQA) task in\nhospital settings through expert annotated open-ended questions. We introduce\nthe Emergency Room Visual Question Answering (ERVQA) dataset, consisting of\n<image, question, answer> triplets covering diverse emergency room scenarios, a\nseminal benchmark for LVLMs. By developing a detailed error taxonomy and\nanalyzing answer trends, we reveal the nuanced nature of the task. We benchmark\nstate-of-the-art open-source and closed LVLMs using traditional and adapted VQA\nmetrics: Entailment Score and CLIPScore Confidence. Analyzing errors across\nmodels, we infer trends based on properties like decoder type, model size, and\nin-context examples. Our findings suggest the ERVQA dataset presents a highly\ncomplex task, highlighting the need for specialized, domain-specific solutions.", "published": "2024-10-08 23:14:24", "link": "http://arxiv.org/abs/2410.06420v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Generating Synthetic Datasets for Few-shot Prompt Tuning", "abstract": "A major limitation of prompt tuning is its dependence on large labeled\ntraining datasets. Under few-shot learning settings, prompt tuning lags far\nbehind full-model fine-tuning, limiting its scope of application. In this\npaper, we leverage the powerful LLMs to synthesize task-specific labeled data\nfor training the soft prompts. We first introduce a distribution-aligned\nweighted generator tuning (DawGen) method to encourage generating\nin-distribution data that aligns with the few-shot real data. Then, we train\nsoft prompts on both synthetic and real datasets using a gradient surgery\napproach, which eliminates the conflicting gradients from different data\nsources. Experiments on seven sentence-pair classification datasets demonstrate\nthe effectiveness of our proposed method for boosting prompt tuning in few-shot\nlearning settings. Results on QQP, MRPC, and SICK datasets are even comparable\nto the performance of transfer learning from large real-world datasets, showing\nthe promise of synthetic data as an alternative for enhancing soft prompt\ntuning.", "published": "2024-10-08 01:00:02", "link": "http://arxiv.org/abs/2410.10865v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CodeUnlearn: Amortized Zero-Shot Machine Unlearning in Language Models\n  Using Discrete Concept", "abstract": "Large Language Models (LLMs) offer extensive knowledge across various\ndomains, but they may inadvertently memorize sensitive, unauthorized, or\nmalicious data, such as personal information in the medical and financial\nsectors. Machine unlearning methods aim to remove specific information from\nmodels after training to address this. However, current approaches require\nadditional model training or struggle to effectively erase particular data\npoints and their associated context due to LLMs' complex, dense, and continuous\nnature. In this study, we propose a novel amortized unlearning approach using\ncodebook features and Sparse Autoencoders (SAEs). By leveraging a bottleneck to\ndecompose the activation space and regulate information flow, our method\nefficiently unlearns targeted information while preserving the model's\nperformance on unrelated data. To the best of our knowledge, this is the first\nwork that successfully enables unlearning specific topics with contextual\nrelevance in an LLM, marking a significant step towards real-world applications\nof machine unlearning.", "published": "2024-10-08 10:26:22", "link": "http://arxiv.org/abs/2410.10866v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Applying Refusal-Vector Ablation to Llama 3.1 70B Agents", "abstract": "Recently, language models like Llama 3.1 Instruct have become increasingly\ncapable of agentic behavior, enabling them to perform tasks requiring\nshort-term planning and tool use. In this study, we apply refusal-vector\nablation to Llama 3.1 70B and implement a simple agent scaffolding to create an\nunrestricted agent. Our findings imply that these refusal-vector ablated models\ncan successfully complete harmful tasks, such as bribing officials or crafting\nphishing attacks, revealing significant vulnerabilities in current safety\nmechanisms. To further explore this, we introduce a small Safe Agent Benchmark,\ndesigned to test both harmful and benign tasks in agentic scenarios. Our\nresults imply that safety fine-tuning in chat models does not generalize well\nto agentic behavior, as we find that Llama 3.1 Instruct models are willing to\nperform most harmful tasks without modifications. At the same time, these\nmodels will refuse to give advice on how to perform the same tasks when asked\nfor a chat completion. This highlights the growing risk of misuse as models\nbecome more capable, underscoring the need for improved safety frameworks for\nlanguage model agents.", "published": "2024-10-08 13:42:36", "link": "http://arxiv.org/abs/2410.10871v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ToolBridge: An Open-Source Dataset to Equip LLMs with External Tool\n  Capabilities", "abstract": "Through the integration of external tools, large language models (LLMs) such\nas GPT-4o and Llama 3.1 significantly expand their functional capabilities,\nevolving from elementary conversational agents to general-purpose assistants.\nWe argue that the primary drivers of these advancements are the quality and\ndiversity of the training data. However, the existing LLMs with external tool\nintegration provide only limited transparency regarding their datasets and data\ncollection methods, which has led to the initiation of this research.\nSpecifically, in this paper, our objective is to elucidate the detailed process\ninvolved in constructing datasets that empower LLMs to effectively learn how to\nutilize external tools and make this information available to the public\nthrough the introduction of ToolBridge. ToolBridge proposes to employ a\ncollection of general open-access datasets as its raw dataset pool and applies\na series of strategies to identify appropriate data entries from the pool for\nexternal tool API insertions. By supervised fine-tuning on these curated data\nentries, LLMs can invoke external tools in appropriate contexts to boost their\npredictive accuracy, particularly for basic functions including data\nprocessing, numerical computation, and factual retrieval. Our experiments\nrigorously isolates model architectures and training configurations, focusing\nexclusively on the role of data. The experimental results indicate that LLMs\ntrained on ToolBridge demonstrate consistent performance improvements on both\nstandard benchmarks and custom evaluation datasets. All the associated code and\ndata will be open-source at https://github.com/CharlesPikachu/ToolBridge,\npromoting transparency and facilitating the broader community to explore\napproaches for equipping LLMs with external tools capabilities.", "published": "2024-10-08 20:54:40", "link": "http://arxiv.org/abs/2410.10872v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "O1 Replication Journey: A Strategic Progress Report -- Part 1", "abstract": "This paper introduces a pioneering approach to artificial intelligence\nresearch, embodied in our O1 Replication Journey. In response to the\nannouncement of OpenAI's groundbreaking O1 model, we embark on a transparent,\nreal-time exploration to replicate its capabilities while reimagining the\nprocess of conducting and communicating AI research. Our methodology addresses\ncritical challenges in modern AI research, including the insularity of\nprolonged team-based projects, delayed information sharing, and the lack of\nrecognition for diverse contributions. By providing comprehensive, real-time\ndocumentation of our replication efforts, including both successes and\nfailures, we aim to foster open science, accelerate collective advancement, and\nlay the groundwork for AI-driven scientific discovery. Our research progress\nreport diverges significantly from traditional research papers, offering\ncontinuous updates, full process transparency, and active community engagement\nthroughout the research journey. Technologically, we proposed the journey\nlearning paradigm, which encourages models to learn not just shortcuts, but the\ncomplete exploration process, including trial and error, reflection, and\nbacktracking. With only 327 training samples and without any additional tricks,\njourney learning outperformed conventional supervised learning by over 8\\% on\nthe MATH dataset, demonstrating its extremely powerful potential. We believe\nthis to be the most crucial component of O1 technology that we have\nsuccessfully decoded. We share valuable resources including technical\nhypotheses and insights, cognitive exploration maps, custom-developed tools,\netc at https://github.com/GAIR-NLP/O1-Journey.", "published": "2024-10-08 15:13:01", "link": "http://arxiv.org/abs/2410.18982v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "TaeBench: Improving Quality of Toxic Adversarial Examples", "abstract": "Toxicity text detectors can be vulnerable to adversarial examples - small\nperturbations to input text that fool the systems into wrong detection.\nExisting attack algorithms are time-consuming and often produce invalid or\nambiguous adversarial examples, making them less useful for evaluating or\nimproving real-world toxicity content moderators. This paper proposes an\nannotation pipeline for quality control of generated toxic adversarial examples\n(TAE). We design model-based automated annotation and human-based quality\nverification to assess the quality requirements of TAE. Successful TAE should\nfool a target toxicity model into making benign predictions, be grammatically\nreasonable, appear natural like human-generated text, and exhibit semantic\ntoxicity. When applying these requirements to more than 20 state-of-the-art\n(SOTA) TAE attack recipes, we find many invalid samples from a total of 940k\nraw TAE attack generations. We then utilize the proposed pipeline to filter and\ncurate a high-quality TAE dataset we call TaeBench (of size 264k). Empirically,\nwe demonstrate that TaeBench can effectively transfer-attack SOTA toxicity\ncontent moderation models and services. Our experiments also show that TaeBench\nwith adversarial training achieve significant improvements of the robustness of\ntwo toxicity detectors.", "published": "2024-10-08 00:14:27", "link": "http://arxiv.org/abs/2410.05573v1", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Adaptation Odyssey in LLMs: Why Does Additional Pretraining Sometimes\n  Fail to Improve?", "abstract": "In the last decade, the generalization and adaptation abilities of deep\nlearning models were typically evaluated on fixed training and test\ndistributions. Contrary to traditional deep learning, large language models\n(LLMs) are (i) even more overparameterized, (ii) trained on unlabeled text\ncorpora curated from the Internet with minimal human intervention, and (iii)\ntrained in an online fashion. These stark contrasts prevent researchers from\ntransferring lessons learned on model generalization and adaptation in deep\nlearning contexts to LLMs. To this end, our short paper introduces empirical\nobservations that aim to shed light on further training of already pretrained\nlanguage models. Specifically, we demonstrate that training a model on a text\ndomain could degrade its perplexity on the test portion of the same domain. We\nobserve with our subsequent analysis that the performance degradation is\npositively correlated with the similarity between the additional and the\noriginal pretraining dataset of the LLM. Our further token-level perplexity\nobservations reveals that the perplexity degradation is due to a handful of\ntokens that are not informative about the domain. We hope these findings will\nguide us in determining when to adapt a model vs when to rely on its\nfoundational capabilities.", "published": "2024-10-08 00:37:16", "link": "http://arxiv.org/abs/2410.05581v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Rethinking Reward Model Evaluation: Are We Barking up the Wrong Tree?", "abstract": "Reward Models (RMs) are crucial for aligning language models with human\npreferences. Currently, the evaluation of RMs depends on measuring accuracy\nagainst a validation set of manually annotated preference data. Although this\nmethod is straightforward and widely adopted, the relationship between RM\naccuracy and downstream policy performance remains under-explored. In this\nwork, we conduct experiments in a synthetic setting to investigate how\ndifferences in RM measured by accuracy translate into gaps in optimized policy\nperformance. Our findings reveal that while there is a weak positive\ncorrelation between accuracy and downstream performance, policies optimized\ntowards RMs with similar accuracy can exhibit quite different performance.\nMoreover, we discover that the way of measuring accuracy significantly impacts\nits ability to predict the final policy performance. Through the lens of the\nRegressional Goodhart effect, we recognize that accuracy, when used for\nmeasuring RM quality, can fail to fully capture the potential RM\noveroptimization. This underscores the inadequacy of relying solely on accuracy\nto reflect their impact on policy optimization.", "published": "2024-10-08 00:52:03", "link": "http://arxiv.org/abs/2410.05584v5", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Everything Everywhere All at Once: LLMs can In-Context Learn Multiple\n  Tasks in Superposition", "abstract": "Large Language Models (LLMs) have demonstrated remarkable in-context learning\n(ICL) capabilities. In this study, we explore a surprising phenomenon related\nto ICL: LLMs can perform multiple, computationally distinct ICL tasks\nsimultaneously, during a single inference call, a capability we term \"task\nsuperposition\". We provide empirical evidence of this phenomenon across various\nLLM families and scales and show that this phenomenon emerges even if we train\nthe model to in-context learn one task at a time. We offer theoretical\nexplanations that this capability is well within the expressive power of\ntransformers. We also explore how LLMs internally compose task vectors during\nsuperposition. Furthermore, we show that larger models can solve more ICL tasks\nin parallel, and better calibrate their output distribution. Our findings offer\ninsights into the latent capabilities of LLMs, further substantiate the\nperspective of \"LLMs as superposition of simulators\", and raise questions about\nthe mechanisms enabling simultaneous task execution.", "published": "2024-10-08 01:28:57", "link": "http://arxiv.org/abs/2410.05603v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Copiloting Diagnosis of Autism in Real Clinical Scenarios via LLMs", "abstract": "Autism spectrum disorder(ASD) is a pervasive developmental disorder that\nsignificantly impacts the daily functioning and social participation of\nindividuals. Despite the abundance of research focused on supporting the\nclinical diagnosis of ASD, there is still a lack of systematic and\ncomprehensive exploration in the field of methods based on Large Language\nModels (LLMs), particularly regarding the real-world clinical diagnostic\nscenarios based on Autism Diagnostic Observation Schedule, Second Edition\n(ADOS-2). Therefore, we have proposed a framework called ADOS-Copilot, which\nstrikes a balance between scoring and explanation and explored the factors that\ninfluence the performance of LLMs in this task. The experimental results\nindicate that our proposed framework is competitive with the diagnostic results\nof clinicians, with a minimum MAE of 0.4643, binary classification F1-score of\n81.79\\%, and ternary classification F1-score of 78.37\\%. Furthermore, we have\nsystematically elucidated the strengths and limitations of current LLMs in this\ntask from the perspectives of ADOS-2, LLMs' capabilities, language, and model\nscale aiming to inspire and guide the future application of LLMs in a broader\nfields of mental health disorders. We hope for more research to be transferred\ninto real clinical practice, opening a window of kindness to the world for\neccentric children.", "published": "2024-10-08 04:48:42", "link": "http://arxiv.org/abs/2410.05684v2", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Enhancing Temporal Modeling of Video LLMs via Time Gating", "abstract": "Video Large Language Models (Video LLMs) have achieved impressive performance\non video-and-language tasks, such as video question answering. However, most\nexisting Video LLMs neglect temporal information in video data, leading to\nstruggles with temporal-aware video understanding. To address this gap, we\npropose a Time Gating Video LLM (TG-Vid) designed to enhance temporal modeling\nthrough a novel Time Gating module (TG). The TG module employs a time gating\nmechanism on its sub-modules, comprising gating spatial attention, gating\ntemporal attention, and gating MLP. This architecture enables our model to\nachieve a robust understanding of temporal information within videos. Extensive\nevaluation of temporal-sensitive video benchmarks (i.e., MVBench, TempCompass,\nand NExT-QA) demonstrates that our TG-Vid model significantly outperforms the\nexisting Video LLMs. Further, comprehensive ablation studies validate that the\nperformance gains are attributed to the designs of our TG module. Our code is\navailable at https://github.com/LaVi-Lab/TG-Vid.", "published": "2024-10-08 06:21:29", "link": "http://arxiv.org/abs/2410.05714v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Give me a hint: Can LLMs take a hint to solve math problems?", "abstract": "While state-of-the-art LLMs have shown poor logical and basic mathematical\nreasoning, recent works try to improve their problem-solving abilities using\nprompting techniques. We propose giving \"hints\" to improve the language model's\nperformance on advanced mathematical problems, taking inspiration from how\nhumans approach math pedagogically. We also test robustness to adversarial\nhints and demonstrate their sensitivity to them. We demonstrate the\neffectiveness of our approach by evaluating various diverse LLMs, presenting\nthem with a broad set of problems of different difficulties and topics from the\nMATH dataset and comparing against techniques such as one-shot, few-shot, and\nchain of thought prompting.", "published": "2024-10-08 11:09:31", "link": "http://arxiv.org/abs/2410.05915v2", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Beyond Captioning: Task-Specific Prompting for Improved VLM Performance\n  in Mathematical Reasoning", "abstract": "Vision-Language Models (VLMs) have transformed tasks requiring visual and\nreasoning abilities, such as image retrieval and Visual Question Answering\n(VQA). Despite their success, VLMs face significant challenges with tasks\ninvolving geometric reasoning, algebraic problem-solving, and counting. These\nlimitations stem from difficulties effectively integrating multiple modalities\nand accurately interpreting geometry-related tasks. Various works claim that\nintroducing a captioning pipeline before VQA tasks enhances performance. We\nincorporated this pipeline for tasks involving geometry, algebra, and counting.\nWe found that captioning results are not generalizable, specifically with\nlarger VLMs primarily trained on downstream QnA tasks showing random\nperformance on math-related challenges. However, we present a promising\nalternative: task-based prompting, enriching the prompt with task-specific\nguidance. This approach shows promise and proves more effective than direct\ncaptioning methods for math-heavy problems.", "published": "2024-10-08 11:29:40", "link": "http://arxiv.org/abs/2410.05928v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "PDF-WuKong: A Large Multimodal Model for Efficient Long PDF Reading with\n  End-to-End Sparse Sampling", "abstract": "Multimodal document understanding is a challenging task to process and\ncomprehend large amounts of textual and visual information. Recent advances in\nLarge Language Models (LLMs) have significantly improved the performance of\nthis task. However, existing methods typically focus on either plain text or a\nlimited number of document images, struggling to handle long PDF documents with\ninterleaved text and images, especially for academic papers. In this paper, we\nintroduce PDF-WuKong, a multimodal large language model (MLLM) which is\ndesigned to enhance multimodal question-answering (QA) for long PDF documents.\nPDF-WuKong incorporates a sparse sampler that operates on both text and image\nrepresentations, significantly improving the efficiency and capability of the\nMLLM. The sparse sampler is integrated with the MLLM's image encoder and\nselects the paragraphs or diagrams most pertinent to user queries for\nprocessing by the language model. To effectively train and evaluate our model,\nwe construct PaperPDF, a dataset consisting of a broad collection of English\nand Chinese academic papers. Multiple strategies are proposed to automatically\ngenerate 1.1 million QA pairs along with their corresponding evidence sources.\nExperimental results demonstrate the superiority and high efficiency of our\napproach over other models on the task of long multimodal document\nunderstanding, surpassing proprietary products by an average of 8.6% on F1. Our\ncode and dataset will be released at https://github.com/yh-hust/PDF-Wukong.", "published": "2024-10-08 12:17:42", "link": "http://arxiv.org/abs/2410.05970v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Long-Context LLMs Meet RAG: Overcoming Challenges for Long Inputs in RAG", "abstract": "Retrieval-augmented generation (RAG) empowers large language models (LLMs) to\nutilize external knowledge sources. The increasing capacity of LLMs to process\nlonger input sequences opens up avenues for providing more retrieved\ninformation, to potentially enhance the quality of generated outputs. It is\nplausible to assume that a larger retrieval set would contain more relevant\ninformation (higher recall), that might result in improved performance.\nHowever, our empirical findings demonstrate that for many long-context LLMs,\nthe quality of generated output initially improves first, but then subsequently\ndeclines as the number of retrieved passages increases. This paper investigates\nthis phenomenon, identifying the detrimental impact of retrieved \"hard\nnegatives\" as a key contributor. To mitigate this and enhance the robustness of\nlong-context LLM-based RAG, we propose both training-free and training-based\napproaches. We first showcase the effectiveness of retrieval reordering as a\nsimple yet powerful training-free optimization. Furthermore, we explore\ntraining-based methods, specifically RAG-specific implicit LLM fine-tuning and\nRAG-oriented fine-tuning with intermediate reasoning, demonstrating their\ncapacity for substantial performance gains. Finally, we conduct a systematic\nanalysis of design choices for these training-based methods, including data\ndistribution, retriever selection, and training context length.", "published": "2024-10-08 12:30:07", "link": "http://arxiv.org/abs/2410.05983v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unveiling Transformer Perception by Exploring Input Manifolds", "abstract": "This paper introduces a general method for the exploration of equivalence\nclasses in the input space of Transformer models. The proposed approach is\nbased on sound mathematical theory which describes the internal layers of a\nTransformer architecture as sequential deformations of the input manifold.\nUsing eigendecomposition of the pullback of the distance metric defined on the\noutput space through the Jacobian of the model, we are able to reconstruct\nequivalence classes in the input space and navigate across them. We illustrate\nhow this method can be used as a powerful tool for investigating how a\nTransformer sees the input space, facilitating local and task-agnostic\nexplainability in Computer Vision and Natural Language Processing tasks.", "published": "2024-10-08 13:20:31", "link": "http://arxiv.org/abs/2410.06019v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.7; I.6.4"], "primary_category": "cs.LG"}
{"title": "Jet Expansions of Residual Computation", "abstract": "We introduce a framework for expanding residual computational graphs using\njets, operators that generalize truncated Taylor series. Our method provides a\nsystematic approach to disentangle contributions of different computational\npaths to model predictions. In contrast to existing techniques such as\ndistillation, probing, or early decoding, our expansions rely solely on the\nmodel itself and requires no data, training, or sampling from the model. We\ndemonstrate how our framework grounds and subsumes logit lens, reveals a\n(super-)exponential path structure in the recursive residual depth and opens up\nseveral applications. These include sketching a transformer large language\nmodel with $n$-gram statistics extracted from its computations, and indexing\nthe models' levels of toxicity knowledge. Our approach enables data-free\nanalysis of residual computation for model interpretability, development, and\nevaluation.", "published": "2024-10-08 13:25:08", "link": "http://arxiv.org/abs/2410.06024v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.SC"], "primary_category": "cs.LG"}
{"title": "Manual Verbalizer Enrichment for Few-Shot Text Classification", "abstract": "With the continuous development of pre-trained language models, prompt-based\ntraining becomes a well-adopted paradigm that drastically improves the\nexploitation of models for many natural language processing tasks. Prompting\nalso shows great performance compared to traditional fine-tuning when adapted\nto zero-shot or few-shot scenarios where the number of annotated data is\nlimited. In this framework, the role of verbalizers is essential, as an\ninterpretation from masked word distributions into output predictions. In this\nwork, we propose \\acrshort{mave}, an approach for verbalizer construction by\nenrichment of class labels using neighborhood relation in the embedding space\nof words for the text classification task. In addition, we elaborate a\nbenchmarking procedure to evaluate typical baselines of verbalizers for\ndocument classification in few-shot learning contexts. Our model achieves\nstate-of-the-art results while using significantly fewer resources. We show\nthat our approach is particularly effective in cases with extremely limited\nsupervision data.", "published": "2024-10-08 16:16:47", "link": "http://arxiv.org/abs/2410.06173v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DataEnvGym: Data Generation Agents in Teacher Environments with Student\n  Feedback", "abstract": "The process of creating training data to teach models is currently driven by\nhumans, who manually analyze model weaknesses and plan how to create data that\nimproves a student model. Approaches using LLMs as annotators reduce human\neffort, but still require humans to interpret feedback from evaluations and\ncontrol the LLM to produce data the student needs. Automating this\nlabor-intensive process by creating autonomous data generation agents - or\nteachers - is desirable, but requires environments that can simulate the\nfeedback-driven, iterative, closed loop of data creation. To enable rapid,\nscalable testing for such agents and their modules, we introduce DataEnvGym, a\ntestbed of teacher environments for data generation agents. DataEnvGym frames\ndata generation as a sequential decision-making task, involving an agent\nconsisting of a data generation policy (which generates a plan for creating\ntraining data) and a data generation engine (which transforms the plan into\ndata), inside an environment that provides student feedback. The agent's goal\nis to improve student performance. Students are iteratively trained and\nevaluated on generated data, and their feedback (in the form of errors or weak\nskills) is reported to the agent after each iteration. DataEnvGym includes\nmultiple teacher environment instantiations across 3 levels of structure in the\nstate representation and action space. More structured environments are based\non inferred skills and offer more interpretability and curriculum control. We\nsupport 4 domains (math, code, VQA, and tool-use) and test multiple students\nand teachers. Example agents in our teaching environments can iteratively\nimprove students across tasks and settings. Moreover, we show that environments\nteach different skill levels and test variants of key modules, pointing to\nfuture work in improving data generation agents, engines, and feedback\nmechanisms.", "published": "2024-10-08 17:20:37", "link": "http://arxiv.org/abs/2410.06215v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "EVOLvE: Evaluating and Optimizing LLMs For Exploration", "abstract": "Despite their success in many domains, large language models (LLMs) remain\nunder-studied in scenarios requiring optimal decision-making under uncertainty.\nThis is crucial as many real-world applications, ranging from personalized\nrecommendations to healthcare interventions, demand that LLMs not only predict\nbut also actively learn to make optimal decisions through exploration. In this\nwork, we measure LLMs' (in)ability to make optimal decisions in bandits, a\nstate-less reinforcement learning setting relevant to many applications. We\ndevelop a comprehensive suite of environments, including both context-free and\ncontextual bandits with varying task difficulties, to benchmark LLMs'\nperformance. Motivated by the existence of optimal exploration algorithms, we\npropose efficient ways to integrate this algorithmic knowledge into LLMs: by\nproviding explicit algorithm-guided support during inference; and through\nalgorithm distillation via in-context demonstrations and fine-tuning, using\nsynthetic data generated from these algorithms. Impressively, these techniques\nallow us to achieve superior exploration performance with smaller models,\nsurpassing larger models on various tasks. We conducted an extensive ablation\nstudy to shed light on various factors, such as task difficulty and data\nrepresentation, that influence the efficiency of LLM exploration. Additionally,\nwe conduct a rigorous analysis of the LLM's exploration efficiency using the\nconcept of regret, linking its ability to explore to the model size and\nunderlying algorithm.", "published": "2024-10-08 17:54:03", "link": "http://arxiv.org/abs/2410.06238v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Unsupervised Model Diagnosis", "abstract": "Ensuring model explainability and robustness is essential for reliable\ndeployment of deep vision systems. Current methods for evaluating robustness\nrely on collecting and annotating extensive test sets. While this is common\npractice, the process is labor-intensive and expensive with no guarantee of\nsufficient coverage across attributes of interest. Recently, model diagnosis\nframeworks have emerged leveraging user inputs (e.g., text) to assess the\nvulnerability of the model. However, such dependence on human can introduce\nbias and limitation given the domain knowledge of particular users. This paper\nproposes Unsupervised Model Diagnosis (UMO), that leverages generative models\nto produce semantic counterfactual explanations without any user guidance.\nGiven a differentiable computer vision model (i.e., the target model), UMO\noptimizes for the most counterfactual directions in a generative latent space.\nOur approach identifies and visualizes changes in semantics, and then matches\nthese changes to attributes from wide-ranging text sources, such as\ndictionaries or language models. We validate the framework on multiple vision\ntasks (e.g., classification, segmentation, keypoint detection). Extensive\nexperiments show that our unsupervised discovery of semantic directions can\ncorrectly highlight spurious correlations and visualize the failure mode of\ntarget models without any human intervention.", "published": "2024-10-08 17:59:03", "link": "http://arxiv.org/abs/2410.06243v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Non-Halting Queries: Exploiting Fixed Points in LLMs", "abstract": "We introduce a new vulnerability that exploits fixed points in autoregressive\nmodels and use it to craft queries that never halt. More precisely, for\nnon-halting queries, the LLM never samples the end-of-string token <eos>. We\nrigorously analyze the conditions under which the non-halting anomaly presents\nitself. In particular, at temperature zero, we prove that if a repeating\n(cyclic) token sequence is observed at the output beyond the context size, then\nthe LLM does not halt.\n  We demonstrate non-halting queries in many experiments performed in base\nunaligned models where repeating prompts immediately lead to a non-halting\ncyclic behavior as predicted by the analysis. Further, we develop a simple\nrecipe that takes the same fixed points observed in the base model and creates\na prompt structure to target aligned models. We demonstrate the recipe's\nsuccess in sending every major model released over the past year into a\nnon-halting state with the same simple prompt even over higher temperatures.\nFurther, we devise an experiment with 100 randomly selected tokens and show\nthat the recipe to create non-halting queries succeeds with high success rates\nranging from 97% for GPT-4o to 19% for Gemini Pro 1.5. These results show that\nthe proposed adversarial recipe succeeds in bypassing alignment at one to two\norders of magnitude higher rates compared to earlier reports.\n  We also study gradient-based direct inversion using ARCA to craft new short\nprompts to induce the non-halting state. We inverted 10,000 random repeating\n2-cycle outputs for llama-3.1-8b-instruct. Out of 10,000 three-token inverted\nprompts 1,512 yield non-halting queries reaching a rate of 15%. Our experiments\nwith ARCA show that non-halting may be easily induced with as few as 3 input\ntokens with high probability. Overall, our experiments demonstrate that\nnon-halting queries are prevalent and relatively easy to find.", "published": "2024-10-08 18:38:32", "link": "http://arxiv.org/abs/2410.06287v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Accelerated Preference Optimization for Large Language Model Alignment", "abstract": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a pivotal\ntool for aligning large language models (LLMs) with human preferences. Direct\nPreference Optimization (DPO), one of the most popular approaches, formulates\nRLHF as a policy optimization problem without explicitly estimating the reward\nfunction. It overcomes the stability and efficiency issues of two-step\napproaches, which typically involve first estimating the reward function and\nthen optimizing the policy via proximal policy optimization (PPO). Since RLHF\nis essentially an optimization problem, and it is well-known that momentum\ntechniques can accelerate optimization both theoretically and empirically, a\nnatural question arises: Can RLHF be accelerated by momentum? This paper\nanswers this question in the affirmative. In detail, we first show that the\niterative preference optimization method can be viewed as a proximal point\nmethod. Based on this observation, we propose a general Accelerated Preference\nOptimization (APO) framework, which unifies many existing preference\noptimization algorithms and employs Nesterov's momentum technique to speed up\nthe alignment of LLMs. Theoretically, we demonstrate that APO can achieve a\nfaster convergence rate than the standard iterative preference optimization\nmethods, including DPO and Self-Play Preference Optimization (SPPO).\nEmpirically, we show the superiority of APO over DPO, iterative DPO, and other\nstrong baselines for RLHF on the AlpacaEval 2.0 benchmark.", "published": "2024-10-08 18:51:01", "link": "http://arxiv.org/abs/2410.06293v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Auto-Evolve: Enhancing Large Language Model's Performance via\n  Self-Reasoning Framework", "abstract": "Recent advancements in prompt engineering strategies, such as\nChain-of-Thought (CoT) and Self-Discover, have demonstrated significant\npotential in improving the reasoning abilities of Large Language Models (LLMs).\nHowever, these state-of-the-art (SOTA) prompting strategies rely on single or\nfixed set of static seed reasoning modules like \"think step by step\" or \"break\ndown this problem\" intended to simulate human approach to problem-solving. This\nconstraint limits the flexibility of models in tackling diverse problems\neffectively. In this paper, we introduce Auto-Evolve, a novel framework that\nenables LLMs to self-create dynamic reasoning modules and downstream action\nplan, resulting in significant improvements over current SOTA methods. We\nevaluate Auto-Evolve on the challenging BigBench-Hard (BBH) dataset with Claude\n2.0, Claude 3 Sonnet, Mistral Large, and GPT 4, where it consistently\noutperforms the SOTA prompt strategies. Auto-Evolve outperforms CoT by up to\n10.4% and on an average by 7% across these four models. Our framework\nintroduces two innovations: a) Auto-Evolve dynamically generates reasoning\nmodules for each task while aligning with human reasoning paradigm, thus\neliminating the need for predefined templates. b) We introduce an iterative\nrefinement component, that incrementally refines instruction guidance for LLMs\nand helps boost performance by average 2.8% compared to doing it in a single\nstep.", "published": "2024-10-08 20:07:47", "link": "http://arxiv.org/abs/2410.06328v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Locate-then-edit for Multi-hop Factual Recall under Knowledge Editing", "abstract": "The locate-then-edit paradigm has shown significant promise for knowledge\nediting (KE) in Large Language Models (LLMs). While previous methods perform\nwell on single-hop fact recall tasks, they consistently struggle with multi-hop\nfactual recall tasks involving newly edited knowledge. In this paper,\nleveraging tools in mechanistic interpretability, we first identify that in\nmulti-hop tasks, LLMs tend to retrieve knowledge with implicit subject\ninformation from deeper MLP layers, unlike single-hop tasks, which rely on\nshallow layers. This distinction explains the poor performance of current\nmethods in multi-hop queries, as they primarily focus on editing shallow layers\nwith single-hop edit prompts, leaving deeper layers unchanged. To address this,\nwe propose IFMET, a novel locate-then-edit KE approach designed to edit both\nshallow and deep MLP layers. Beyond single-hop editing prompts, IFMET further\nincorporates multi-hop editing prompts to locate and modify knowledge across\ndifferent stages of reasoning. Experimental results demonstrate that IFMET\nsignificantly improves performance on multi-hop factual recall tasks,\novercoming the limitations of previous locate-then-edit methods", "published": "2024-10-08 20:12:11", "link": "http://arxiv.org/abs/2410.06331v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "HumVI: A Multilingual Dataset for Detecting Violent Incidents Impacting\n  Humanitarian Aid", "abstract": "Humanitarian organizations can enhance their effectiveness by analyzing data\nto discover trends, gather aggregated insights, manage their security risks,\nsupport decision-making, and inform advocacy and funding proposals. However,\ndata about violent incidents with direct impact and relevance for humanitarian\naid operations is not readily available. An automatic data collection and\nNLP-backed classification framework aligned with humanitarian perspectives can\nhelp bridge this gap. In this paper, we present HumVI - a dataset comprising\nnews articles in three languages (English, French, Arabic) containing instances\nof different types of violent incidents categorized by the humanitarian sector\nthey impact, e.g., aid security, education, food security, health, and\nprotection. Reliable labels were obtained for the dataset by partnering with a\ndata-backed humanitarian organization, Insecurity Insight. We provide multiple\nbenchmarks for the dataset, employing various deep learning architectures and\ntechniques, including data augmentation and mask loss, to address different\ntask-related challenges, e.g., domain expansion. The dataset is publicly\navailable at https://github.com/dataminr-ai/humvi-dataset.", "published": "2024-10-08 21:08:13", "link": "http://arxiv.org/abs/2410.06370v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Validation of the Scientific Literature via Chemputation Augmented by\n  Large Language Models", "abstract": "Chemputation is the process of programming chemical robots to do experiments\nusing a universal symbolic language, but the literature can be error prone and\nhard to read due to ambiguities. Large Language Models (LLMs) have demonstrated\nremarkable capabilities in various domains, including natural language\nprocessing, robotic control, and more recently, chemistry. Despite significant\nadvancements in standardizing the reporting and collection of synthetic\nchemistry data, the automatic reproduction of reported syntheses remains a\nlabour-intensive task. In this work, we introduce an LLM-based chemical\nresearch agent workflow designed for the automatic validation of synthetic\nliterature procedures. Our workflow can autonomously extract synthetic\nprocedures and analytical data from extensive documents, translate these\nprocedures into universal XDL code, simulate the execution of the procedure in\na hardware-specific setup, and ultimately execute the procedure on an\nXDL-controlled robotic system for synthetic chemistry. This demonstrates the\npotential of LLM-based workflows for autonomous chemical synthesis with\nChemputers. Due to the abstraction of XDL this approach is safe, secure, and\nscalable since hallucinations will not be chemputable and the XDL can be both\nverified and encrypted. Unlike previous efforts, which either addressed only a\nlimited portion of the workflow, relied on inflexible hard-coded rules, or\nlacked validation in physical systems, our approach provides four realistic\nexamples of syntheses directly executed from synthetic literature. We\nanticipate that our workflow will significantly enhance automation in\nrobotically driven synthetic chemistry research, streamline data extraction,\nimprove the reproducibility, scalability, and safety of synthetic and\nexperimental chemistry.", "published": "2024-10-08 21:31:42", "link": "http://arxiv.org/abs/2410.06384v1", "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.AI"}
{"title": "NLP Case Study on Predicting the Before and After of the Ukraine-Russia\n  and Hamas-Israel Conflicts", "abstract": "We propose a method to predict toxicity and other textual attributes through\nthe use of natural language processing (NLP) techniques for two recent events:\nthe Ukraine-Russia and Hamas-Israel conflicts. This article provides a basis\nfor exploration in future conflicts with hopes to mitigate risk through the\nanalysis of social media before and after a conflict begins. Our work compiles\nseveral datasets from Twitter and Reddit for both conflicts in a before and\nafter separation with an aim of predicting a future state of social media for\navoidance. More specifically, we show that: (1) there is a noticeable\ndifference in social media discussion leading up to and following a conflict\nand (2) social media discourse on platforms like Twitter and Reddit is useful\nin identifying future conflicts before they arise. Our results show that\nthrough the use of advanced NLP techniques (both supervised and unsupervised)\ntoxicity and other attributes about language before and after a conflict is\npredictable with a low error of nearly 1.2 percent for both conflicts.", "published": "2024-10-08 23:46:56", "link": "http://arxiv.org/abs/2410.06427v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Stress Detection on Code-Mixed Texts in Dravidian Languages using\n  Machine Learning", "abstract": "Stress is a common feeling in daily life, but it can affect mental well-being\nin some situations, the development of robust detection models is imperative.\nThis study introduces a methodical approach to the stress identification in\ncode-mixed texts for Dravidian languages. The challenge encompassed two\ndatasets, targeting Tamil and Telugu languages respectively. This proposal\nunderscores the importance of using uncleaned text as a benchmark to refine\nfuture classification methodologies, incorporating diverse preprocessing\ntechniques. Random Forest algorithm was used, featuring three textual\nrepresentations: TF-IDF, Uni-grams of words, and a composite of (1+2+3)-Grams\nof characters. The approach achieved a good performance for both linguistic\ncategories, achieving a Macro F1-score of 0.734 in Tamil and 0.727 in Telugu,\noverpassing results achieved with different complex techniques such as FastText\nand Transformer models. The results underscore the value of uncleaned data for\nmental state detection and the challenges classifying code-mixed texts for\nstress, indicating the potential for improved performance through cleaning\ndata, other preprocessing techniques, or more complex models.", "published": "2024-10-08 23:49:31", "link": "http://arxiv.org/abs/2410.06428v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mitigating the Impact of Reference Quality on Evaluation of\n  Summarization Systems with Reference-Free Metrics", "abstract": "Automatic metrics are used as proxies to evaluate abstractive summarization\nsystems when human annotations are too expensive. To be useful, these metrics\nshould be fine-grained, show a high correlation with human annotations, and\nideally be independent of reference quality; however, most standard evaluation\nmetrics for summarization are reference-based, and existing reference-free\nmetrics correlate poorly with relevance, especially on summaries of longer\ndocuments. In this paper, we introduce a reference-free metric that correlates\nwell with human evaluated relevance, while being very cheap to compute. We show\nthat this metric can also be used alongside reference-based metrics to improve\ntheir robustness in low quality reference settings.", "published": "2024-10-08 11:09:25", "link": "http://arxiv.org/abs/2410.10867v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Large Continual Instruction Assistant", "abstract": "Continual Instruction Tuning (CIT) is adopted to continually instruct Large\nModels to follow human intent data by data. It is observed that existing\ngradient update would heavily destroy the performance on previous datasets\nduring CIT process. Instead, Exponential Moving Average (EMA), owns the ability\nto trace previous parameters, which can aid in decreasing forgetting.\nNonetheless, its stable balance weight fails to deal with the ever-changing\ndatasets, leading to the out-of-balance between plasticity and stability. In\nthis paper, we propose a general continual instruction tuning framework to\naddress the challenge. Starting from the trade-off prerequisite and EMA update,\nwe propose the plasticity and stability ideal condition. Based on Taylor\nexpansion in the loss function, we find the optimal balance weight can be\nautomatically determined by the gradients and learned parameters. Therefore, we\npropose a stable-plasticity balanced coefficient to avoid knowledge confusion.\nBased on the semantic similarity of the instructions, we can determine whether\nto retrain or expand the training parameters and allocate the most suitable\nparameters for the testing instances. Extensive experiments across multiple\ncontinual instruction tuning benchmarks demonstrate that our approach not only\nenhances anti-forgetting capabilities but also significantly improves overall\ncontinual tuning performance. For example, based on LLaVA-7B, the forgetting is\nreduced from 5.42 to 1.93. Our code will be made publicly available soon.", "published": "2024-10-08 11:24:59", "link": "http://arxiv.org/abs/2410.10868v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Application of NotebookLM, a Large Language Model with\n  Retrieval-Augmented Generation, for Lung Cancer Staging", "abstract": "Purpose: In radiology, large language models (LLMs), including ChatGPT, have\nrecently gained attention, and their utility is being rapidly evaluated.\nHowever, concerns have emerged regarding their reliability in clinical\napplications due to limitations such as hallucinations and insufficient\nreferencing. To address these issues, we focus on the latest technology,\nretrieval-augmented generation (RAG), which enables LLMs to reference reliable\nexternal knowledge (REK). Specifically, this study examines the utility and\nreliability of a recently released RAG-equipped LLM (RAG-LLM), NotebookLM, for\nstaging lung cancer.\n  Materials and methods: We summarized the current lung cancer staging\nguideline in Japan and provided this as REK to NotebookLM. We then tasked\nNotebookLM with staging 100 fictional lung cancer cases based on CT findings\nand evaluated its accuracy. For comparison, we performed the same task using a\ngold-standard LLM, GPT-4 Omni (GPT-4o), both with and without the REK.\n  Results: NotebookLM achieved 86% diagnostic accuracy in the lung cancer\nstaging experiment, outperforming GPT-4o, which recorded 39% accuracy with the\nREK and 25% without it. Moreover, NotebookLM demonstrated 95% accuracy in\nsearching reference locations within the REK.\n  Conclusion: NotebookLM successfully performed lung cancer staging by\nutilizing the REK, demonstrating superior performance compared to GPT-4o.\nAdditionally, it provided highly accurate reference locations within the REK,\nallowing radiologists to efficiently evaluate the reliability of NotebookLM's\nresponses and detect possible hallucinations. Overall, this study highlights\nthe potential of NotebookLM, a RAG-LLM, in image diagnosis.", "published": "2024-10-08 12:42:42", "link": "http://arxiv.org/abs/2410.10869v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PortLLM: Personalizing Evolving Large Language Models with Training-Free\n  and Portable Model Patches", "abstract": "As large language models (LLMs) increasingly shape the AI landscape,\nfine-tuning pretrained models has become more popular than in the pre-LLM era\nfor achieving optimal performance in domain-specific tasks. However, pretrained\nLLMs such as ChatGPT are periodically evolved, i.e., model parameters are\nfrequently updated), making it challenging for downstream users with limited\nresources to keep up with fine-tuning the newest LLMs for their domain\napplication. Even though fine-tuning costs have nowadays been reduced thanks to\nthe innovations of parameter-efficient fine-tuning such as LoRA, not all\ndownstream users have adequate computing for frequent personalization.\nMoreover, access to fine-tuning datasets, particularly in sensitive domains\nsuch as healthcare, could be time-restrictive, making it crucial to retain the\nknowledge encoded in earlier fine-tuned rounds for future adaptation. In this\npaper, we present PortLLM, a training-free framework that (i) creates an\ninitial lightweight model update patch to capture domain-specific knowledge,\nand (ii) allows a subsequent seamless plugging for the continual\npersonalization of evolved LLM at minimal cost. Our extensive experiments cover\nseven representative datasets, from easier question-answering tasks {BoolQ,\nSST2} to harder reasoning tasks {WinoGrande, GSM8K}, and models including\n{Mistral-7B, Llama2, Llama3.1, and Gemma2}, validating the portability of our\ndesigned model patches and showcasing the effectiveness of our proposed\nframework. For instance, PortLLM achieves comparable performance to LoRA\nfine-tuning with reductions of up to 12.2x in GPU memory usage. Finally, we\nprovide theoretical justifications to understand the portability of our model\nupdate patches, which offers new insights into the theoretical dimension of\nLLMs' personalization.", "published": "2024-10-08 13:41:08", "link": "http://arxiv.org/abs/2410.10870v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "$M^3EL$: A Multi-task Multi-topic Dataset for Multi-modal Entity Linking", "abstract": "Multi-modal Entity Linking (MEL) is a fundamental component for various\ndownstream tasks. However, existing MEL datasets suffer from small scale,\nscarcity of topic types and limited coverage of tasks, making them incapable of\neffectively enhancing the entity linking capabilities of multi-modal models. To\naddress these obstacles, we propose a dataset construction pipeline and publish\n$M^3EL$, a large-scale dataset for MEL. $M^3EL$ includes 79,625 instances,\ncovering 9 diverse multi-modal tasks, and 5 different topics. In addition, to\nfurther improve the model's adaptability to multi-modal tasks, We propose a\nmodality-augmented training strategy. Utilizing $M^3EL$ as a corpus, train the\n$\\textit{CLIP}_{\\textit{ND}}$ model based on $\\textit{CLIP}\n(\\textit{ViT}-\\textit{B}-\\textit{32})$, and conduct a comparative analysis with\nan existing multi-modal baselines. Experimental results show that the existing\nmodels perform far below expectations (ACC of 49.4%-75.8%), After analysis, it\nwas obtained that small dataset sizes, insufficient modality task coverage, and\nlimited topic diversity resulted in poor generalisation of multi-modal models.\nOur dataset effectively addresses these issues, and the\n$\\textit{CLIP}_{\\textit{ND}}$ model fine-tuned with $M^3EL$ shows a significant\nimprovement in accuracy, with an average improvement of 9.3% to 25% across\nvarious tasks. Our dataset is available at\nhttps://anonymous.4open.science/r/M3EL.", "published": "2024-10-08 10:52:23", "link": "http://arxiv.org/abs/2410.18096v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.IR"}
{"title": "RingGesture: A Ring-Based Mid-Air Gesture Typing System Powered by a\n  Deep-Learning Word Prediction Framework", "abstract": "Text entry is a critical capability for any modern computing experience, with\nlightweight augmented reality (AR) glasses being no exception. Designed for\nall-day wearability, a limitation of lightweight AR glass is the restriction to\nthe inclusion of multiple cameras for extensive field of view in hand tracking.\nThis constraint underscores the need for an additional input device. We propose\na system to address this gap: a ring-based mid-air gesture typing technique,\nRingGesture, utilizing electrodes to mark the start and end of gesture\ntrajectories and inertial measurement units (IMU) sensors for hand tracking.\nThis method offers an intuitive experience similar to raycast-based mid-air\ngesture typing found in VR headsets, allowing for a seamless translation of\nhand movements into cursor navigation. To enhance both accuracy and input\nspeed, we propose a novel deep-learning word prediction framework, Score\nFusion, comprised of three key components: a) a word-gesture decoding model, b)\na spatial spelling correction model, and c) a lightweight contextual language\nmodel. In contrast, this framework fuses the scores from the three models to\npredict the most likely words with higher precision. We conduct comparative and\nlongitudinal studies to demonstrate two key findings: firstly, the overall\neffectiveness of RingGesture, which achieves an average text entry speed of\n27.3 words per minute (WPM) and a peak performance of 47.9 WPM. Secondly, we\nhighlight the superior performance of the Score Fusion framework, which offers\na 28.2% improvement in uncorrected Character Error Rate over a conventional\nword prediction framework, Naive Correction, leading to a 55.2% improvement in\ntext entry speed for RingGesture. Additionally, RingGesture received a System\nUsability Score of 83 signifying its excellent usability.", "published": "2024-10-08 13:15:30", "link": "http://arxiv.org/abs/2410.18100v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Improving Embedding Accuracy for Document Retrieval Using Entity\n  Relationship Maps and Model-Aware Contrastive Sampling", "abstract": "In this paper we present APEX-Embedding-7B (Advanced Processing for Epistemic\neXtraction), a 7-billion parameter decoder-only text Feature Extraction Model,\nspecifically designed for Document Retrieval-Augmented Generation (RAG) tasks.\nOur approach employs two training techniques that yield an emergent improvement\nin factual focus: (1) Pre-convergence interrupted fine-tuning using Structured\nEntity Relationship Maps as training data input: designed to shift the model's\nattention and create a bias towards factual content rather than semantic style\n- this enhances plain text performance despite not being directly trained for\nit; and (2) Model-Aware Contrastive Sampling, creating a balanced and evenly\ndistributed collation map of hard and soft negatives directly informed by the\nbase model's competency. This combined methodology yields significant\nimprovements, enhancing plain text query/document pair retrieval to achieve an\nabsolute rank@1 accuracy of 90.86% (an increase of 6.26% compared to the next\nleading model) in our evaluation, and reducing training data input context size\nby an average of 37.71% compared to plain text for both queries and document\ntexts. Based on our evaluations, our model establishes a new state-of-the-art\nstandard in text feature extraction for longer context document retrieval\ntasks.", "published": "2024-10-08 17:36:48", "link": "http://arxiv.org/abs/2410.18105v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Think While You Generate: Discrete Diffusion with Planned Denoising", "abstract": "Discrete diffusion has achieved state-of-the-art performance, outperforming\nor approaching autoregressive models on standard benchmarks. In this work, we\nintroduce Discrete Diffusion with Planned Denoising (DDPD), a novel framework\nthat separates the generation process into two models: a planner and a\ndenoiser. At inference time, the planner selects which positions to denoise\nnext by identifying the most corrupted positions in need of denoising,\nincluding both initially corrupted and those requiring additional refinement.\nThis plan-and-denoise approach enables more efficient reconstruction during\ngeneration by iteratively identifying and denoising corruptions in the optimal\norder. DDPD outperforms traditional denoiser-only mask diffusion methods,\nachieving superior results on language modeling benchmarks such as text8,\nOpenWebText, and token-based image generation on ImageNet $256 \\times 256$.\nNotably, in language modeling, DDPD significantly reduces the performance gap\nbetween diffusion-based and autoregressive methods in terms of generative\nperplexity. Code is available at https://github.com/liusulin/DDPD.", "published": "2024-10-08 18:03:34", "link": "http://arxiv.org/abs/2410.06264v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Gesture2Text: A Generalizable Decoder for Word-Gesture Keyboards in XR\n  Through Trajectory Coarse Discretization and Pre-training", "abstract": "Text entry with word-gesture keyboards (WGK) is emerging as a popular method\nand becoming a key interaction for Extended Reality (XR). However, the\ndiversity of interaction modes, keyboard sizes, and visual feedback in these\nenvironments introduces divergent word-gesture trajectory data patterns, thus\nleading to complexity in decoding trajectories into text. Template-matching\ndecoding methods, such as SHARK^2, are commonly used for these WGK systems\nbecause they are easy to implement and configure. However, these methods are\nsusceptible to decoding inaccuracies for noisy trajectories. While conventional\nneural-network-based decoders (neural decoders) trained on word-gesture\ntrajectory data have been proposed to improve accuracy, they have their own\nlimitations: they require extensive data for training and deep-learning\nexpertise for implementation. To address these challenges, we propose a novel\nsolution that combines ease of implementation with high decoding accuracy: a\ngeneralizable neural decoder enabled by pre-training on large-scale coarsely\ndiscretized word-gesture trajectories. This approach produces a ready-to-use\nWGK decoder that is generalizable across mid-air and on-surface WGK systems in\naugmented reality (AR) and virtual reality (VR), which is evident by a robust\naverage Top-4 accuracy of 90.4% on four diverse datasets. It significantly\noutperforms SHARK^2 with a 37.2% enhancement and surpasses the conventional\nneural decoder by 7.4%. Moreover, the Pre-trained Neural Decoder's size is only\n4 MB after quantization, without sacrificing accuracy, and it can operate in\nreal-time, executing in just 97 milliseconds on Quest 3.", "published": "2024-10-08 12:53:22", "link": "http://arxiv.org/abs/2410.18099v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.GR", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Improving Data Augmentation-based Cross-Speaker Style Transfer for TTS\n  with Singing Voice, Style Filtering, and F0 Matching", "abstract": "The goal of cross-speaker style transfer in TTS is to transfer a speech style\nfrom a source speaker with expressive data to a target speaker with only\nneutral data. In this context, we propose using a pre-trained singing voice\nconversion (SVC) model to convert the expressive data into the target speaker's\nvoice. In the conversion process, we apply a fundamental frequency (F0)\nmatching technique to mitigate tonal variances between speakers with\nsignificant timbral differences. A style classifier filter is proposed to\nselect the most expressive output audios for the TTS training. Our approach is\ncomparable to state-of-the-art with only a few minutes of neutral data from the\ntarget speaker, while other methods require hours. A perceptual assessment\nshowed improvements brought by the SVC and the style filter in naturalness and\nstyle intensity for the styles that display more vocal effort. Also, increased\nspeaker similarity is obtained with the proposed F0 matching algorithm.", "published": "2024-10-08 02:06:12", "link": "http://arxiv.org/abs/2410.05620v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "FGCL: Fine-grained Contrastive Learning For Mandarin Stuttering Event\n  Detection", "abstract": "This paper presents the T031 team's approach to the StutteringSpeech\nChallenge in SLT2024. Mandarin Stuttering Event Detection (MSED) aims to detect\ninstances of stuttering events in Mandarin speech. We propose a detailed\nacoustic analysis method to improve the accuracy of stutter detection by\ncapturing subtle nuances that previous Stuttering Event Detection (SED)\ntechniques have overlooked. To this end, we introduce the Fine-Grained\nContrastive Learning (FGCL) framework for MSED. Specifically, we model the\nframe-level probabilities of stuttering events and introduce a mining algorithm\nto identify both easy and confusing frames. Then, we propose a stutter contrast\nloss to enhance the distinction between stuttered and fluent speech frames,\nthereby improving the discriminative capability of stuttered feature\nembeddings. Extensive evaluations on English and Mandarin datasets demonstrate\nthe effectiveness of FGCL, achieving a significant increase of over 5.0% in F1\nscore on Mandarin data.", "published": "2024-10-08 02:55:47", "link": "http://arxiv.org/abs/2410.05647v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Exploring rhythm formant analysis for Indic language classification", "abstract": "This paper reports a preliminary study on quantitative frequency domain\nrhythm cues for classifying five Indian languages: Bengali, Kannada, Malayalam,\nMarathi, and Tamil. We employ rhythm formant (R-formants) analysis, a technique\nintroduced by Gibbon that utilizes low-frequency spectral analysis of amplitude\nmodulation and frequency modulation envelopes to characterize speech rhythm.\nVarious measures are computed from the LF spectrum, including R-formants,\ndiscrete cosine transform-based measures, and spectral measures. Results show\nthat threshold-based and spectral features outperform directly computed\nR-formants. Temporal pattern of rhythm derived from LF spectrograms provides\nbetter language-discriminating cues. Combining all derived features we achieve\nan accuracy of 69.21% and a weighted F1 score of 69.18% in classifying the five\nlanguages. This study demonstrates the potential of RFA in characterizing\nspeech rhythm for Indian language classification.", "published": "2024-10-08 06:40:17", "link": "http://arxiv.org/abs/2410.05724v1", "categories": ["eess.AS", "eess.SP", "I.2.7"], "primary_category": "eess.AS"}
{"title": "The USTC-NERCSLIP Systems for the CHiME-8 MMCSG Challenge", "abstract": "In the two-person conversation scenario with one wearing smart glasses,\ntranscribing and displaying the speaker's content in real-time is an intriguing\napplication, providing a priori information for subsequent tasks such as\ntranslation and comprehension. Meanwhile, multi-modal data captured from the\nsmart glasses is scarce. Therefore, we propose utilizing simulation data with\nmultiple overlap rates and a one-to-one matching training strategy to narrow\ndown the deviation for the model training between real and simulated data. In\naddition, combining IMU unit data in the model can assist the audio to achieve\nbetter real-time speech recognition performance.", "published": "2024-10-08 12:33:01", "link": "http://arxiv.org/abs/2410.05986v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Array2BR: An End-to-End Noise-immune Binaural Audio Synthesis from\n  Microphone-array Signals", "abstract": "Telepresence technology aims to provide an immersive virtual presence for\nremote conference applications, and it is extremely important to synthesize\nhigh-quality binaural audio signals for this aim. Because the ambient noise is\noften inevitable in practical application scenarios, it is highly desired that\nbinaural audio signals without noise can be obtained from microphone-array\nsignals directly. For this purpose, this paper proposes a new end-to-end\nnoise-immune binaural audio synthesis framework from microphone-array signals,\nabbreviated as Array2BR, and experimental results show that binaural cues can\nbe correctly mapped and noise can be well suppressed simultaneously using the\nproposed framework. Compared with existing methods, the proposed method\nachieved better performance in terms of both objective and subjective metric\nscores.", "published": "2024-10-08 06:55:35", "link": "http://arxiv.org/abs/2410.05739v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "F\u00fcrElise: Capturing and Physically Synthesizing Hand Motions of Piano\n  Performance", "abstract": "Piano playing requires agile, precise, and coordinated hand control that\nstretches the limits of dexterity. Hand motion models with the sophistication\nto accurately recreate piano playing have a wide range of applications in\ncharacter animation, embodied AI, biomechanics, and VR/AR. In this paper, we\nconstruct a first-of-its-kind large-scale dataset that contains approximately\n10 hours of 3D hand motion and audio from 15 elite-level pianists playing 153\npieces of classical music. To capture natural performances, we designed a\nmarkerless setup in which motions are reconstructed from multi-view videos\nusing state-of-the-art pose estimation models. The motion data is further\nrefined via inverse kinematics using the high-resolution MIDI key-pressing data\nobtained from sensors in a specialized Yamaha Disklavier piano. Leveraging the\ncollected dataset, we developed a pipeline that can synthesize\nphysically-plausible hand motions for musical scores outside of the dataset.\nOur approach employs a combination of imitation learning and reinforcement\nlearning to obtain policies for physics-based bimanual control involving the\ninteraction between hands and piano keys. To solve the sampling efficiency\nproblem with the large motion dataset, we use a diffusion model to generate\nnatural reference motions, which provide high-level trajectory and fingering\n(finger order and placement) information. However, the generated reference\nmotion alone does not provide sufficient accuracy for piano performance\nmodeling. We then further augmented the data by using musical similarity to\nretrieve similar motions from the captured dataset to boost the precision of\nthe RL policy. With the proposed method, our model generates natural, dexterous\nmotions that generalize to music from outside the training dataset.", "published": "2024-10-08 08:21:05", "link": "http://arxiv.org/abs/2410.05791v1", "categories": ["cs.GR", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.GR"}
{"title": "FINALLY: fast and universal speech enhancement with studio-like quality", "abstract": "In this paper, we address the challenge of speech enhancement in real-world\nrecordings, which often contain various forms of distortion, such as background\nnoise, reverberation, and microphone artifacts. We revisit the use of\nGenerative Adversarial Networks (GANs) for speech enhancement and theoretically\nshow that GANs are naturally inclined to seek the point of maximum density\nwithin the conditional clean speech distribution, which, as we argue, is\nessential for the speech enhancement task. We study various feature extractors\nfor perceptual loss to facilitate the stability of adversarial training,\ndeveloping a methodology for probing the structure of the feature space. This\nleads us to integrate WavLM-based perceptual loss into MS-STFT adversarial\ntraining pipeline, creating an effective and stable training procedure for the\nspeech enhancement model. The resulting speech enhancement model, which we\nrefer to as FINALLY, builds upon the HiFi++ architecture, augmented with a\nWavLM encoder and a novel training pipeline. Empirical results on various\ndatasets confirm our model's ability to produce clear, high-quality speech at\n48 kHz, achieving state-of-the-art performance in the field of speech\nenhancement. Demo page: https://samsunglabs.github.io/FINALLY-page", "published": "2024-10-08 11:16:03", "link": "http://arxiv.org/abs/2410.05920v3", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "An Eye for an Ear: Zero-shot Audio Description Leveraging an Image\n  Captioner using Audiovisual Distribution Alignment", "abstract": "Multimodal large language models have fueled progress in image captioning.\nThese models, fine-tuned on vast image datasets, exhibit a deep understanding\nof semantic concepts. In this work, we show that this ability can be\nre-purposed for audio captioning, where the joint image-language decoder can be\nleveraged to describe auditory content associated with image sequences within\nvideos featuring audiovisual content. This can be achieved via multimodal\nalignment. Yet, this multimodal alignment task is non-trivial due to the\ninherent disparity between audible and visible elements in real-world videos.\nMoreover, multimodal representation learning often relies on contrastive\nlearning, facing the challenge of the so-called modality gap which hinders\nsmooth integration between modalities. In this work, we introduce a novel\nmethodology for bridging the audiovisual modality gap by matching the\ndistributions of tokens produced by an audio backbone and those of an image\ncaptioner. Our approach aligns the audio token distribution with that of the\nimage tokens, enabling the model to perform zero-shot audio captioning in an\nunsupervised fashion while keeping the initial image captioning component\nunaltered. This alignment allows for the use of either audio or audiovisual\ninput by combining or substituting the image encoder with the aligned audio\nencoder. Our method achieves significantly improved performances in zero-shot\naudio captioning, compared to existing approaches.", "published": "2024-10-08 12:52:48", "link": "http://arxiv.org/abs/2410.05997v1", "categories": ["eess.AS", "cs.CV", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "VRVQ: Variable Bitrate Residual Vector Quantization for Audio\n  Compression", "abstract": "Recent state-of-the-art neural audio compression models have progressively\nadopted residual vector quantization (RVQ). Despite this success, these models\nemploy a fixed number of codebooks per frame, which can be suboptimal in terms\nof rate-distortion tradeoff, particularly in scenarios with simple input audio,\nsuch as silence. To address this limitation, we propose variable bitrate RVQ\n(VRVQ) for audio codecs, which allows for more efficient coding by adapting the\nnumber of codebooks used per frame. Furthermore, we propose a gradient\nestimation method for the non-differentiable masking operation that transforms\nfrom the importance map to the binary importance mask, improving model training\nvia a straight-through estimator. We demonstrate that the proposed training\nframework achieves superior results compared to the baseline method and shows\nfurther improvement when applied to the current state-of-the-art codec.", "published": "2024-10-08 13:18:24", "link": "http://arxiv.org/abs/2410.06016v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "POLIPHONE: A Dataset for Smartphone Model Identification from Audio\n  Recordings", "abstract": "When dealing with multimedia data, source attribution is a key challenge from\na forensic perspective. This task aims to determine how a given content was\ncaptured, providing valuable insights for various applications, including legal\nproceedings and integrity investigations. The source attribution problem has\nbeen addressed in different domains, from identifying the camera model used to\ncapture specific photographs to detecting the synthetic speech generator or\nmicrophone model used to create or record given audio tracks. Recent\nadvancements in this area rely heavily on machine learning and data-driven\ntechniques, which often outperform traditional signal processing-based methods.\n  However, a drawback of these systems is their need for large volumes of\ntraining data, which must reflect the latest technological trends to produce\naccurate and reliable predictions. This presents a significant challenge, as\nthe rapid pace of technological progress makes it difficult to maintain\ndatasets that are up-to-date with real-world conditions. For instance, in the\ntask of smartphone model identification from audio recordings, the available\ndatasets are often outdated or acquired inconsistently, making it difficult to\ndevelop solutions that are valid beyond a research environment. In this paper\nwe present POLIPHONE, a dataset for smartphone model identification from audio\nrecordings. It includes data from 20 recent smartphones recorded in a\ncontrolled environment to ensure reproducibility and scalability for future\nresearch. The released tracks contain audio data from various domains (i.e.,\nspeech, music, environmental sounds), making the corpus versatile and\napplicable to a wide range of use cases. We also present numerous experiments\nto benchmark the proposed dataset using a state-of-the-art classifier for\nsmartphone model identification from audio recordings.", "published": "2024-10-08 17:29:14", "link": "http://arxiv.org/abs/2410.06221v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
