{"title": "How does a Pre-Trained Transformer Integrate Contextual Keywords?\n  Application to Humanitarian Computing", "abstract": "In a classification task, dealing with text snippets and metadata usually\nrequires dealing with multimodal approaches. When those metadata are textual,\nit is tempting to use them intrinsically with a pre-trained transformer, in\norder to leverage the semantic information encoded inside the model. This paper\ndescribes how to improve a humanitarian classification task by adding the\ncrisis event type to each tweet to be classified. Based on additional\nexperiments of the model weights and behavior, it identifies how the proposed\nneural network approach is partially over-fitting the particularities of the\nCrisis Benchmark, to better highlight how the model is still undoubtedly\nlearning to use and take advantage of the metadata's textual semantics.", "published": "2021-11-07 11:24:08", "link": "http://arxiv.org/abs/2111.04052v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Variance-Aware Machine Translation Test Sets", "abstract": "We release 70 small and discriminative test sets for machine translation (MT)\nevaluation called variance-aware test sets (VAT), covering 35 translation\ndirections from WMT16 to WMT20 competitions. VAT is automatically created by a\nnovel variance-aware filtering method that filters the indiscriminative test\ninstances of the current MT test sets without any human labor. Experimental\nresults show that VAT outperforms the original WMT test sets in terms of the\ncorrelation with human judgement across mainstream language pairs and test\nsets. Further analysis on the properties of VAT reveals the challenging\nlinguistic features (e.g., translation of low-frequency words and proper nouns)\nfor competitive MT systems, providing guidance for constructing future MT test\nsets. The test sets and the code for preparing variance-aware MT test sets are\nfreely available at https://github.com/NLP2CT/Variance-Aware-MT-Test-Sets .", "published": "2021-11-07 13:18:59", "link": "http://arxiv.org/abs/2111.04079v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Machine-in-the-Loop Rewriting for Creative Image Captioning", "abstract": "Machine-in-the-loop writing aims to enable humans to collaborate with models\nto complete their writing tasks more effectively. Prior work has found that\nproviding humans a machine-written draft or sentence-level continuations has\nlimited success since the generated text tends to deviate from humans'\nintention. To allow the user to retain control over the content, we train a\nrewriting model that, when prompted, modifies specified spans of text within\nthe user's original draft to introduce descriptive and figurative elements\nlocally in the text. We evaluate the model on its ability to collaborate with\nhumans on the task of creative image captioning. On a user study through Amazon\nMechanical Turk, our model is rated to be more helpful than a baseline\ninfilling language model. In addition, third-party evaluation shows that users\nwrite more descriptive and figurative captions when collaborating with our\nmodel compared to completing the task alone.", "published": "2021-11-07 22:17:41", "link": "http://arxiv.org/abs/2111.04193v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TaCL: Improving BERT Pre-training with Token-aware Contrastive Learning", "abstract": "Masked language models (MLMs) such as BERT and RoBERTa have revolutionized\nthe field of Natural Language Understanding in the past few years. However,\nexisting pre-trained MLMs often output an anisotropic distribution of token\nrepresentations that occupies a narrow subset of the entire representation\nspace. Such token representations are not ideal, especially for tasks that\ndemand discriminative semantic meanings of distinct tokens. In this work, we\npropose TaCL (Token-aware Contrastive Learning), a novel continual pre-training\napproach that encourages BERT to learn an isotropic and discriminative\ndistribution of token representations. TaCL is fully unsupervised and requires\nno additional data. We extensively test our approach on a wide range of English\nand Chinese benchmarks. The results show that TaCL brings consistent and\nnotable improvements over the original BERT model. Furthermore, we conduct\ndetailed analysis to reveal the merits and inner-workings of our approach.", "published": "2021-11-07 22:54:23", "link": "http://arxiv.org/abs/2111.04198v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MotifClass: Weakly Supervised Text Classification with Higher-order\n  Metadata Information", "abstract": "We study the problem of weakly supervised text classification, which aims to\nclassify text documents into a set of pre-defined categories with category\nsurface names only and without any annotated training document provided. Most\nexisting classifiers leverage textual information in each document. However, in\nmany domains, documents are accompanied by various types of metadata (e.g.,\nauthors, venue, and year of a research paper). These metadata and their\ncombinations may serve as strong category indicators in addition to textual\ncontents. In this paper, we explore the potential of using metadata to help\nweakly supervised text classification. To be specific, we model the\nrelationships between documents and metadata via a heterogeneous information\nnetwork. To effectively capture higher-order structures in the network, we use\nmotifs to describe metadata combinations. We propose a novel framework, named\nMotifClass, which (1) selects category-indicative motif instances, (2)\nretrieves and generates pseudo-labeled training samples based on category names\nand indicative motif instances, and (3) trains a text classifier using the\npseudo training data. Extensive experiments on real-world datasets demonstrate\nthe superior performance of MotifClass to existing weakly supervised text\nclassification approaches. Further analysis shows the benefit of considering\nhigher-order metadata information in our framework.", "published": "2021-11-07 07:39:10", "link": "http://arxiv.org/abs/2111.04022v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Information Extraction from Visually Rich Documents with Font Style\n  Embeddings", "abstract": "Information extraction (IE) from documents is an intensive area of research\nwith a large set of industrial applications. Current state-of-the-art methods\nfocus on scanned documents with approaches combining computer vision, natural\nlanguage processing and layout representation. We propose to challenge the\nusage of computer vision in the case where both token style and visual\nrepresentation are available (i.e native PDF documents). Our experiments on\nthree real-world complex datasets demonstrate that using token style attributes\nbased embedding instead of a raw visual embedding in LayoutLM model is\nbeneficial. Depending on the dataset, such an embedding yields an improvement\nof 0.18% to 2.29% in the weighted F1-score with a decrease of 30.7% in the\nfinal number of trainable parameters of the model, leading to an improvement in\nboth efficiency and effectiveness.", "published": "2021-11-07 10:29:54", "link": "http://arxiv.org/abs/2111.04045v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Developing neural machine translation models for Hungarian-English", "abstract": "I train models for the task of neural machine translation for\nEnglish-Hungarian and Hungarian-English, using the Hunglish2 corpus. The main\ncontribution of this work is evaluating different data augmentation methods\nduring the training of NMT models. I propose 5 different augmentation methods\nthat are structure-aware, meaning that instead of randomly selecting words for\nblanking or replacement, the dependency tree of sentences is used as a basis\nfor augmentation. I start my thesis with a detailed literature review on neural\nnetworks, sequential modeling, neural machine translation, dependency parsing\nand data augmentation. After a detailed exploratory data analysis and\npreprocessing of the Hunglish2 corpus, I perform experiments with the proposed\ndata augmentation techniques. The best model for Hungarian-English achieves a\nBLEU score of 33.9, while the best model for English-Hungarian achieves a BLEU\nscore of 28.6.", "published": "2021-11-07 14:35:00", "link": "http://arxiv.org/abs/2111.04099v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "NLP From Scratch Without Large-Scale Pretraining: A Simple and Efficient\n  Framework", "abstract": "Pretrained language models have become the standard approach for many NLP\ntasks due to strong performance, but they are very expensive to train. We\npropose a simple and efficient learning framework, TLM, that does not rely on\nlarge-scale pretraining. Given some labeled task data and a large general\ncorpus, TLM uses task data as queries to retrieve a tiny subset of the general\ncorpus and jointly optimizes the task objective and the language modeling\nobjective from scratch. On eight classification datasets in four domains, TLM\nachieves results better than or similar to pretrained language models (e.g.,\nRoBERTa-Large) while reducing the training FLOPs by two orders of magnitude.\nWith high accuracy and efficiency, we hope TLM will contribute to democratizing\nNLP and expediting its development.", "published": "2021-11-07 17:13:59", "link": "http://arxiv.org/abs/2111.04130v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Word on Machine Ethics: A Response to Jiang et al. (2021)", "abstract": "Ethics is one of the longest standing intellectual endeavors of humanity. In\nrecent years, the fields of AI and NLP have attempted to wrangle with how\nlearning systems that interact with humans should be constrained to behave\nethically. One proposal in this vein is the construction of morality models\nthat can take in arbitrary text and output a moral judgment about the situation\ndescribed. In this work, we focus on a single case study of the recently\nproposed Delphi model and offer a critique of the project's proposed method of\nautomating morality judgments. Through an audit of Delphi, we examine broader\nissues that would be applicable to any similar attempt. We conclude with a\ndiscussion of how machine ethics could usefully proceed, by focusing on current\nand near-future uses of technology, in a way that centers around transparency,\ndemocratic values, and allows for straightforward accountability.", "published": "2021-11-07 19:31:51", "link": "http://arxiv.org/abs/2111.04158v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Meta-TTS: Meta-Learning for Few-Shot Speaker Adaptive Text-to-Speech", "abstract": "Personalizing a speech synthesis system is a highly desired application,\nwhere the system can generate speech with the user's voice with rare enrolled\nrecordings. There are two main approaches to build such a system in recent\nworks: speaker adaptation and speaker encoding. On the one hand, speaker\nadaptation methods fine-tune a trained multi-speaker text-to-speech (TTS) model\nwith few enrolled samples. However, they require at least thousands of\nfine-tuning steps for high-quality adaptation, making it hard to apply on\ndevices. On the other hand, speaker encoding methods encode enrollment\nutterances into a speaker embedding. The trained TTS model can synthesize the\nuser's speech conditioned on the corresponding speaker embedding. Nevertheless,\nthe speaker encoder suffers from the generalization gap between the seen and\nunseen speakers.\n  In this paper, we propose applying a meta-learning algorithm to the speaker\nadaptation method. More specifically, we use Model Agnostic Meta-Learning\n(MAML) as the training algorithm of a multi-speaker TTS model, which aims to\nfind a great meta-initialization to adapt the model to any few-shot speaker\nadaptation tasks quickly. Therefore, we can also adapt the meta-trained TTS\nmodel to unseen speakers efficiently. Our experiments compare the proposed\nmethod (Meta-TTS) with two baselines: a speaker adaptation method baseline and\na speaker encoding method baseline. The evaluation results show that Meta-TTS\ncan synthesize high speaker-similarity speech from few enrollment samples with\nfewer adaptation steps than the speaker adaptation baseline and outperforms the\nspeaker encoding baseline under the same training scheme. When the speaker\nencoder of the baseline is pre-trained with extra 8371 speakers of data,\nMeta-TTS can still outperform the baseline on LibriTTS dataset and achieve\ncomparable results on VCTK dataset.", "published": "2021-11-07 09:53:31", "link": "http://arxiv.org/abs/2111.04040v3", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Look at the Variance! Efficient Black-box Explanations with Sobol-based\n  Sensitivity Analysis", "abstract": "We describe a novel attribution method which is grounded in Sensitivity\nAnalysis and uses Sobol indices. Beyond modeling the individual contributions\nof image regions, Sobol indices provide an efficient way to capture\nhigher-order interactions between image regions and their contributions to a\nneural network's prediction through the lens of variance. We describe an\napproach that makes the computation of these indices efficient for\nhigh-dimensional problems by using perturbation masks coupled with efficient\nestimators to handle the high dimensionality of images. Importantly, we show\nthat the proposed method leads to favorable scores on standard benchmarks for\nvision (and language models) while drastically reducing the computing time\ncompared to other black-box methods -- even surpassing the accuracy of\nstate-of-the-art white-box methods which require access to internal\nrepresentations. Our code is freely available:\nhttps://github.com/fel-thomas/Sobol-Attribution-Method", "published": "2021-11-07 18:01:36", "link": "http://arxiv.org/abs/2111.04138v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Retrieving Speaker Information from Personalized Acoustic Models for\n  Speech Recognition", "abstract": "The widespread of powerful personal devices capable of collecting voice of\ntheir users has opened the opportunity to build speaker adapted speech\nrecognition system (ASR) or to participate to collaborative learning of ASR. In\nboth cases, personalized acoustic models (AM), i.e. fine-tuned AM with specific\nspeaker data, can be built. A question that naturally arises is whether the\ndissemination of personalized acoustic models can leak personal information. In\nthis paper, we show that it is possible to retrieve the gender of the speaker,\nbut also his identity, by just exploiting the weight matrix changes of a neural\nacoustic model locally adapted to this speaker. Incidentally we observe\nphenomena that may be useful towards explainability of deep neural networks in\nthe context of speech processing. Gender can be identified almost surely using\nonly the first layers and speaker verification performs well when using\nmiddle-up layers. Our experimental study on the TED-LIUM 3 dataset with\nHMM/TDNN models shows an accuracy of 95% for gender detection, and an Equal\nError Rate of 9.07% for a speaker verification task by only exploiting the\nweights from personalized models that could be exchanged instead of user data.", "published": "2021-11-07 22:17:52", "link": "http://arxiv.org/abs/2111.04194v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Speaker Generation", "abstract": "This work explores the task of synthesizing speech in nonexistent\nhuman-sounding voices. We call this task \"speaker generation\", and present\nTacoSpawn, a system that performs competitively at this task. TacoSpawn is a\nrecurrent attention-based text-to-speech model that learns a distribution over\na speaker embedding space, which enables sampling of novel and diverse\nspeakers. Our method is easy to implement, and does not require transfer\nlearning from speaker ID systems. We present objective and subjective metrics\nfor evaluating performance on this task, and demonstrate that our proposed\nobjective metrics correlate with human perception of speaker similarity. Audio\nsamples are available on our demo page.", "published": "2021-11-07 22:31:41", "link": "http://arxiv.org/abs/2111.05095v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS", "I.2.7; G.3"], "primary_category": "cs.SD"}
{"title": "LiMuSE: Lightweight Multi-modal Speaker Extraction", "abstract": "Multi-modal cues, including spatial information, facial expression and\nvoiceprint, are introduced to the speech separation and speaker extraction\ntasks to serve as complementary information to achieve better performance.\nHowever, the introduction of these cues brings about an increasing number of\nparameters and model complexity, which makes it harder to deploy these models\non resource-constrained devices. In this paper, we alleviate the aforementioned\nproblem by proposing a Lightweight Multi-modal framework for Speaker Extraction\n(LiMuSE). We propose to use GC-equipped TCN, which incorporates Group\nCommunication (GC) and Temporal Convolutional Network (TCN) in the Context\nCodec module, the audio block and the fusion block. The experiments on the\nMC_GRID dataset demonstrate that LiMuSE achieves on par or better performance\nwith a much smaller number of parameters and less model complexity. We further\ninvestigate the impacts of the quantization of LiMuSE. Our code and dataset are\nprovided.", "published": "2021-11-07 12:05:00", "link": "http://arxiv.org/abs/2111.04063v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Theme Transformer: Symbolic Music Generation with Theme-Conditioned\n  Transformer", "abstract": "Attention-based Transformer models have been increasingly employed for\nautomatic music generation. To condition the generation process of such a model\nwith a user-specified sequence, a popular approach is to take that conditioning\nsequence as a priming sequence and ask a Transformer decoder to generate a\ncontinuation. However, this prompt-based conditioning cannot guarantee that the\nconditioning sequence would develop or even simply repeat itself in the\ngenerated continuation. In this paper, we propose an alternative conditioning\napproach, called theme-based conditioning, that explicitly trains the\nTransformer to treat the conditioning sequence as a thematic material that has\nto manifest itself multiple times in its generation result. This is achieved\nwith two main technical contributions. First, we propose a deep learning-based\napproach that uses contrastive representation learning and clustering to\nautomatically retrieve thematic materials from music pieces in the training\ndata. Second, we propose a novel gated parallel attention module to be used in\na sequence-to-sequence (seq2seq) encoder/decoder architecture to more\neffectively account for a given conditioning thematic material in the\ngeneration process of the Transformer decoder. We report on objective and\nsubjective evaluations of variants of the proposed Theme Transformer and the\nconventional prompt-based baseline, showing that our best model can generate,\nto some extent, polyphonic pop piano music with repetition and plausible\nvariations of a given condition.", "published": "2021-11-07 13:55:39", "link": "http://arxiv.org/abs/2111.04093v2", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Emotional Prosody Control for Speech Generation", "abstract": "Machine-generated speech is characterized by its limited or unnatural\nemotional variation. Current text to speech systems generates speech with\neither a flat emotion, emotion selected from a predefined set, average\nvariation learned from prosody sequences in training data or transferred from a\nsource style. We propose a text to speech(TTS) system, where a user can choose\nthe emotion of generated speech from a continuous and meaningful emotion space\n(Arousal-Valence space). The proposed TTS system can generate speech from the\ntext in any speaker's style, with fine control of emotion. We show that the\nsystem works on emotion unseen during training and can scale to previously\nunseen speakers given his/her speech sample. Our work expands the horizon of\nthe state-of-the-art FastSpeech2 backbone to a multi-speaker setting and gives\nit much-coveted continuous (and interpretable) affective control, without any\nobservable degradation in the quality of the synthesized speech.", "published": "2021-11-07 08:52:04", "link": "http://arxiv.org/abs/2111.04730v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
