{"title": "Automatic Text Summarization Approaches to Speed up Topic Model Learning\n  Process", "abstract": "The number of documents available into Internet moves each day up. For this\nreason, processing this amount of information effectively and expressibly\nbecomes a major concern for companies and scientists. Methods that represent a\ntextual document by a topic representation are widely used in Information\nRetrieval (IR) to process big data such as Wikipedia articles. One of the main\ndifficulty in using topic model on huge data collection is related to the\nmaterial resources (CPU time and memory) required for model estimate. To deal\nwith this issue, we propose to build topic spaces from summarized documents. In\nthis paper, we present a study of topic space representation in the context of\nbig data. The topic space representation behavior is analyzed on different\nlanguages. Experiments show that topic spaces estimated from text summaries are\nas relevant as those estimated from the complete documents. The real advantage\nof such an approach is the processing time gain: we showed that the processing\ntime can be drastically reduced using summarized documents (more than 60\\% in\ngeneral). This study finally points out the differences between thematic\nrepresentations of documents depending on the targeted languages such as\nEnglish or latin languages.", "published": "2017-03-20 08:19:43", "link": "http://arxiv.org/abs/1703.06630v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "I2T2I: Learning Text to Image Synthesis with Textual Data Augmentation", "abstract": "Translating information between text and image is a fundamental problem in\nartificial intelligence that connects natural language processing and computer\nvision. In the past few years, performance in image caption generation has seen\nsignificant improvement through the adoption of recurrent neural networks\n(RNN). Meanwhile, text-to-image generation begun to generate plausible images\nusing datasets of specific categories like birds and flowers. We've even seen\nimage generation from multi-category datasets such as the Microsoft Common\nObjects in Context (MSCOCO) through the use of generative adversarial networks\n(GANs). Synthesizing objects with a complex shape, however, is still\nchallenging. For example, animals and humans have many degrees of freedom,\nwhich means that they can take on many complex shapes. We propose a new\ntraining method called Image-Text-Image (I2T2I) which integrates text-to-image\nand image-to-text (image captioning) synthesis to improve the performance of\ntext-to-image synthesis. We demonstrate that %the capability of our method to\nunderstand the sentence descriptions, so as to I2T2I can generate better\nmulti-categories images using MSCOCO than the state-of-the-art. We also\ndemonstrate that I2T2I can achieve transfer learning by using a pre-trained\nimage captioning module to generate human images on the MPII Human Pose", "published": "2017-03-20 11:11:38", "link": "http://arxiv.org/abs/1703.06676v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Learning Cooperative Visual Dialog Agents with Deep Reinforcement\n  Learning", "abstract": "We introduce the first goal-driven training for visual question answering and\ndialog agents. Specifically, we pose a cooperative 'image guessing' game\nbetween two agents -- Qbot and Abot -- who communicate in natural language\ndialog so that Qbot can select an unseen image from a lineup of images. We use\ndeep reinforcement learning (RL) to learn the policies of these agents\nend-to-end -- from pixels to multi-agent multi-round dialog to game reward.\n  We demonstrate two experimental results.\n  First, as a 'sanity check' demonstration of pure RL (from scratch), we show\nresults on a synthetic world, where the agents communicate in ungrounded\nvocabulary, i.e., symbols with no pre-specified meanings (X, Y, Z). We find\nthat two bots invent their own communication protocol and start using certain\nsymbols to ask/answer about certain visual attributes (shape/color/style).\nThus, we demonstrate the emergence of grounded language and communication among\n'visual' dialog agents with no human supervision.\n  Second, we conduct large-scale real-image experiments on the VisDial dataset,\nwhere we pretrain with supervised dialog data and show that the RL 'fine-tuned'\nagents significantly outperform SL agents. Interestingly, the RL Qbot learns to\nask questions that Abot is good at, ultimately resulting in more informative\ndialog and a better team.", "published": "2017-03-20 03:50:57", "link": "http://arxiv.org/abs/1703.06585v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Towards a Quantum World Wide Web", "abstract": "We elaborate a quantum model for the meaning associated with corpora of\nwritten documents, like the pages forming the World Wide Web. To that end, we\nare guided by how physicists constructed quantum theory for microscopic\nentities, which unlike classical objects cannot be fully represented in our\nspatial theater. We suggest that a similar construction needs to be carried out\nby linguists and computational scientists, to capture the full meaning carried\nby collections of documental entities. More precisely, we show how to associate\na quantum-like 'entity of meaning' to a 'language entity formed by printed\ndocuments', considering the latter as the collection of traces that are left by\nthe former, in specific results of search actions that we describe as\nmeasurements. In other words, we offer a perspective where a collection of\ndocuments, like the Web, is described as the space of manifestation of a more\ncomplex entity - the QWeb - which is the object of our modeling, drawing its\ninspiration from previous studies on operational-realistic approaches to\nquantum physics and quantum modeling of human cognition and decision-making. We\nemphasize that a consistent QWeb model needs to account for the observed\ncorrelations between words appearing in printed documents, e.g.,\nco-occurrences, as the latter would depend on the 'meaning connections'\nexisting between the concepts that are associated with these words. In that\nrespect, we show that both 'context and interference (quantum) effects' are\nrequired to explain the probabilities calculated by counting the relative\nnumber of documents containing certain words and co-ocurrrences of words.", "published": "2017-03-20 09:28:38", "link": "http://arxiv.org/abs/1703.06642v2", "categories": ["cs.AI", "cs.CL", "quant-ph"], "primary_category": "cs.AI"}
