{"title": "Empirical Analysis of Korean Public AI Hub Parallel Corpora and in-depth\n  Analysis using LIWC", "abstract": "Machine translation (MT) system aims to translate source language into target\nlanguage. Recent studies on MT systems mainly focus on neural machine\ntranslation (NMT). One factor that significantly affects the performance of NMT\nis the availability of high-quality parallel corpora. However, high-quality\nparallel corpora concerning Korean are relatively scarce compared to those\nassociated with other high-resource languages, such as German or Italian. To\naddress this problem, AI Hub recently released seven types of parallel corpora\nfor Korean. In this study, we conduct an in-depth verification of the quality\nof corresponding parallel corpora through Linguistic Inquiry and Word Count\n(LIWC) and several relevant experiments. LIWC is a word-counting software\nprogram that can analyze corpora in multiple ways and extract linguistic\nfeatures as a dictionary base. To the best of our knowledge, this study is the\nfirst to use LIWC to analyze parallel corpora in the field of NMT. Our findings\nsuggest the direction of further research toward obtaining the improved quality\nparallel corpora through our correlation analysis in LIWC and NMT performance.", "published": "2021-10-28 11:15:54", "link": "http://arxiv.org/abs/2110.15023v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Confounds and Overestimations in Fake Review Detection: Experimentally\n  Controlling for Product-Ownership and Data-Origin", "abstract": "The popularity of online shopping is steadily increasing. At the same time,\nfake product reviewsare published widely and have the potential to affect\nconsumer purchasing behavior. In response,previous work has developed automated\nmethods for the detection of deceptive product reviews.However, studies vary\nconsiderably in terms of classification performance, and many use data\nthatcontain potential confounds, which makes it difficult to determine their\nvalidity. Two possibleconfounds are data-origin (i.e., the dataset is composed\nof more than one source) and productownership (i.e., reviews written by\nindividuals who own or do not own the reviewed product). Inthe present study,\nwe investigate the effect of both confounds for fake review detection. Using\nanexperimental design, we manipulate data-origin, product ownership, review\npolarity, and veracity.Supervised learning analysis suggests that review\nveracity (60.26 - 69.87%) is somewhat detectablebut reviews additionally\nconfounded with product-ownership (66.19 - 74.17%), or with data-origin(84.44 -\n86.94%) are easier to classify. Review veracity is most easily classified if\nconfounded withproduct-ownership and data-origin combined (87.78 - 88.12%),\nsuggesting overestimations of thetrue performance in other work. These findings\nare moderated by review polarity.", "published": "2021-10-28 14:04:03", "link": "http://arxiv.org/abs/2110.15130v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Diversity-Driven Combination for Grammatical Error Correction", "abstract": "Grammatical error correction (GEC) is the task of detecting and correcting\nerrors in a written text. The idea of combining multiple system outputs has\nbeen successfully used in GEC. To achieve successful system combination,\nmultiple component systems need to produce corrected sentences that are both\ndiverse and of comparable quality. However, most existing state-of-the-art GEC\napproaches are based on similar sequence-to-sequence neural networks, so the\ngains are limited from combining the outputs of component systems similar to\none another. In this paper, we present Diversity-Driven Combination (DDC) for\nGEC, a system combination strategy that encourages diversity among component\nsystems. We evaluate our system combination strategy on the CoNLL-2014 shared\ntask and the BEA-2019 shared task. On both benchmarks, DDC achieves significant\nperformance gain with a small number of training examples and outperforms the\ncomponent systems by a large margin. Our source code is available at\nhttps://github.com/nusnlp/gec-ddc.", "published": "2021-10-28 14:20:43", "link": "http://arxiv.org/abs/2110.15149v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BERTian Poetics: Constrained Composition with Masked LMs", "abstract": "Masked language models have recently been interpreted as energy-based\nsequence models that can be generated from using a Metropolis--Hastings\nsampler. This short paper demonstrates how this can be instrumentalized for\nconstrained composition and explores the poetics implied by such a usage. Our\nfocus on constraints makes it especially apt to understand the generated text\nthrough the poetics of the OuLiPo movement.", "published": "2021-10-28 14:57:51", "link": "http://arxiv.org/abs/2110.15181v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-stage Clarification in Conversational AI: The case of\n  Question-Answering Dialogue Systems", "abstract": "Clarification resolution plays an important role in various information\nretrieval tasks such as interactive question answering and conversational\nsearch. In such context, the user often formulates their information needs as\nshort and ambiguous queries, some popular search interfaces then prompt the\nuser to confirm her intent (e.g. \"Did you mean ... ?\") or to rephrase if\nneeded. When it comes to dialogue systems, having fluid user-bot exchanges is\nkey to good user experience. In the absence of such clarification mechanism,\none of the following responses is given to the user: 1) A direct answer, which\ncan potentially be non-relevant if the intent was not clear, 2) a generic\nfallback message informing the user that the retrieval tool is incapable of\nhandling the query. Both scenarios might raise frustration and degrade the user\nexperience. To this end, we propose a multi-stage clarification mechanism for\nprompting clarification and query selection in the context of a question\nanswering dialogue system. We show that our proposed mechanism improves the\noverall user experience and outperforms competitive baselines with two\ndatasets, namely the public in-scope out-of-scope dataset and a commercial\ndataset based on real user logs.", "published": "2021-10-28 15:45:44", "link": "http://arxiv.org/abs/2110.15235v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\u00daFAL at MultiLexNorm 2021: Improving Multilingual Lexical\n  Normalization by Fine-tuning ByT5", "abstract": "We present the winning entry to the Multilingual Lexical Normalization\n(MultiLexNorm) shared task at W-NUT 2021 (van der Goot et al., 2021a), which\nevaluates lexical-normalization systems on 12 social media datasets in 11\nlanguages. We base our solution on a pre-trained byte-level language model,\nByT5 (Xue et al., 2021a), which we further pre-train on synthetic data and then\nfine-tune on authentic normalization data. Our system achieves the best\nperformance by a wide margin in intrinsic evaluation, and also the best\nperformance in extrinsic evaluation through dependency parsing. The source code\nis released at https://github.com/ufal/multilexnorm2021 and the fine-tuned\nmodels at https://huggingface.co/ufal.", "published": "2021-10-28 16:06:42", "link": "http://arxiv.org/abs/2110.15248v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hate Speech Classifiers Learn Human-Like Social Stereotypes", "abstract": "Social stereotypes negatively impact individuals' judgements about different\ngroups and may have a critical role in how people understand language directed\ntoward minority social groups. Here, we assess the role of social stereotypes\nin the automated detection of hateful language by examining the relation\nbetween individual annotator biases and erroneous classification of texts by\nhate speech classifiers. Specifically, in Study 1 we investigate the impact of\nnovice annotators' stereotypes on their hate-speech-annotation behavior. In\nStudy 2 we examine the effect of language-embedded stereotypes on expert\nannotators' aggregated judgements in a large annotated corpus. Finally, in\nStudy 3 we demonstrate how language-embedded stereotypes are associated with\nsystematic prediction errors in a neural-network hate speech classifier. Our\nresults demonstrate that hate speech classifiers learn human-like biases which\ncan further perpetuate social inequalities when propagated at scale. This\nframework, combining social psychological and computational linguistic methods,\nprovides insights into additional sources of bias in hate speech moderation,\ninforming ongoing debates regarding fairness in machine learning.", "published": "2021-10-28 01:35:41", "link": "http://arxiv.org/abs/2110.14839v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "A Sequence to Sequence Model for Extracting Multiple Product Name\n  Entities from Dialog", "abstract": "E-commerce voice ordering systems need to recognize multiple product name\nentities from ordering utterances. Existing voice ordering systems such as\nAmazon Alexa can capture only a single product name entity. This restrains\nusers from ordering multiple items with one utterance. In recent years,\npre-trained language models, e.g., BERT and GPT-2, have shown promising results\non NLP benchmarks like Super-GLUE. However, they can't perfectly generalize to\nthis Multiple Product Name Entity Recognition (MPNER) task due to the ambiguity\nin voice ordering utterances. To fill this research gap, we propose Entity\nTransformer (ET) neural network architectures which recognize up to 10 items in\nan utterance. In our evaluation, the best ET model (conveRT + ngram + ET) has a\nperformance improvement of 12% on our test set compared to the non-neural\nmodel, and outperforms BERT with ET as well. This helps customers finalize\ntheir shopping cart via voice dialog, which improves shopping efficiency and\nexperience.", "published": "2021-10-28 01:54:02", "link": "http://arxiv.org/abs/2110.14843v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Semi-Siamese Bi-encoder Neural Ranking Model Using Lightweight\n  Fine-Tuning", "abstract": "A BERT-based Neural Ranking Model (NRM) can be either a crossencoder or a\nbi-encoder. Between the two, bi-encoder is highly efficient because all the\ndocuments can be pre-processed before the actual query time. In this work, we\nshow two approaches for improving the performance of BERT-based bi-encoders.\nThe first approach is to replace the full fine-tuning step with a lightweight\nfine-tuning. We examine lightweight fine-tuning methods that are adapter-based,\nprompt-based, and hybrid of the two. The second approach is to develop\nsemi-Siamese models where queries and documents are handled with a limited\namount of difference. The limited difference is realized by learning two\nlightweight fine-tuning modules, where the main language model of BERT is kept\ncommon for both query and document. We provide extensive experiment results for\nmonoBERT, TwinBERT, and ColBERT where three performance metrics are evaluated\nover Robust04, ClueWeb09b, and MS-MARCO datasets. The results confirm that both\nlightweight fine-tuning and semi-Siamese are considerably helpful for improving\nBERT-based bi-encoders. In fact, lightweight fine-tuning is helpful for\ncrossencoder, too", "published": "2021-10-28 08:26:46", "link": "http://arxiv.org/abs/2110.14943v2", "categories": ["cs.CL", "cs.IR", "H.3.3"], "primary_category": "cs.CL"}
{"title": "Preventing posterior collapse in variational autoencoders for text\n  generation via decoder regularization", "abstract": "Variational autoencoders trained to minimize the reconstruction error are\nsensitive to the posterior collapse problem, that is the proposal posterior\ndistribution is always equal to the prior. We propose a novel regularization\nmethod based on fraternal dropout to prevent posterior collapse. We evaluate\nour approach using several metrics and observe improvements in all the tested\nconfigurations.", "published": "2021-10-28 08:32:27", "link": "http://arxiv.org/abs/2110.14945v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Generating Table Vector Representations", "abstract": "High-quality Web tables are rich sources of information that can be used to\npopulate Knowledge Graphs (KG). The focus of this paper is an evaluation of\nmethods for table-to-class annotation, which is a sub-task of Table\nInterpretation (TI). We provide a formal definition for table classification as\na machine learning task. We propose an experimental setup and we evaluate 5\nfundamentally different approaches to find the best method for generating\nvector table representations. Our findings indicate that although transfer\nlearning methods achieve high F1 score on the table classification task,\ndedicated table encoding models are a promising direction as they appear to\ncapture richer semantics.", "published": "2021-10-28 14:05:21", "link": "http://arxiv.org/abs/2110.15132v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Pruning Attention Heads of Transformer Models Using A* Search: A Novel\n  Approach to Compress Big NLP Architectures", "abstract": "Recent years have seen a growing adoption of Transformer models such as BERT\nin Natural Language Processing and even in Computer Vision. However, due to\ntheir size, there has been limited adoption of such models within\nresource-constrained computing environments. This paper proposes novel pruning\nalgorithm to compress transformer models by eliminating redundant Attention\nHeads. We apply the A* search algorithm to obtain a pruned model with strict\naccuracy guarantees. Our results indicate that the method could eliminate as\nmuch as 40% of the attention heads in the BERT transformer model with no loss\nin accuracy.", "published": "2021-10-28 15:39:11", "link": "http://arxiv.org/abs/2110.15225v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cognitive network science quantifies feelings expressed in suicide\n  letters and Reddit mental health communities", "abstract": "Writing messages is key to expressing feelings. This study adopts cognitive\nnetwork science to reconstruct how individuals report their feelings in\nclinical narratives like suicide notes or mental health posts. We achieve this\nby reconstructing syntactic/semantic associations between conceptsin texts as\nco-occurrences enriched with affective data. We transform 142 suicide notes and\n77,000 Reddit posts from the r/anxiety, r/depression, r/schizophrenia, and\nr/do-it-your-own (r/DIY) forums into 5 cognitive networks, each one expressing\nmeanings and emotions as reported by authors. These networks reconstruct the\nsemantic frames surrounding 'feel', enabling a quantification of prominent\nassociations and emotions focused around feelings. We find strong feelings of\nsadness across all clinical Reddit boards, added to fear r/depression, and\nreplaced by joy/anticipation in r/DIY. Semantic communities and topic modelling\nboth highlight key narrative topics of 'regret', 'unhealthy lifestyle' and 'low\nmental well-being'. Importantly, negative associations and emotions co-existed\nwith trustful/positive language, focused on 'getting better'. This emotional\npolarisation provides quantitative evidence that online clinical boards possess\na complex structure, where users mix both positive and negative outlooks. This\ndichotomy is absent in the r/DIY reference board and in suicide notes, where\nnegative emotional associations about regret and pain persist but are\noverwhelmed by positive jargon addressing loved ones. Our quantitative\ncomparisons provide strong evidence that suicide notes encapsulate different\nways of expressing feelings compared to online Reddit boards, the latter acting\nmore like personal diaries and relief valve. Our findings provide an\ninterpretable, quantitative aid for supporting psychological inquiries of human\nfeelings in digital and clinical settings.", "published": "2021-10-28 16:26:50", "link": "http://arxiv.org/abs/2110.15269v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "RadBERT-CL: Factually-Aware Contrastive Learning For Radiology Report\n  Classification", "abstract": "Radiology reports are unstructured and contain the imaging findings and\ncorresponding diagnoses transcribed by radiologists which include clinical\nfacts and negated and/or uncertain statements. Extracting pathologic findings\nand diagnoses from radiology reports is important for quality control,\npopulation health, and monitoring of disease progress. Existing works,\nprimarily rely either on rule-based systems or transformer-based pre-trained\nmodel fine-tuning, but could not take the factual and uncertain information\ninto consideration, and therefore generate false-positive outputs. In this\nwork, we introduce three sedulous augmentation techniques which retain factual\nand critical information while generating augmentations for contrastive\nlearning. We introduce RadBERT-CL, which fuses these information into BlueBert\nvia a self-supervised contrastive loss. Our experiments on MIMIC-CXR show\nsuperior performance of RadBERT-CL on fine-tuning for multi-class, multi-label\nreport classification. We illustrate that when few labeled data are available,\nRadBERT-CL outperforms conventional SOTA transformers (BERT/BlueBert) by\nsignificantly larger margins (6-11%). We also show that the representations\nlearned by RadBERT-CL can capture critical medical information in the latent\nspace.", "published": "2021-10-28 20:31:04", "link": "http://arxiv.org/abs/2110.15426v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "ICDM 2020 Knowledge Graph Contest: Consumer Event-Cause Extraction", "abstract": "Consumer Event-Cause Extraction, the task aimed at extracting the potential\ncauses behind certain events in the text, has gained much attention in recent\nyears due to its wide applications. The ICDM 2020 conference sets up an\nevaluation competition that aims to extract events and the causes of the\nextracted events with a specified subject (a brand or product). In this task,\nwe mainly focus on how to construct an end-to-end model, and extract multiple\nevent types and event-causes simultaneously. To this end, we introduce a fresh\nperspective to revisit the relational event-cause extraction task and propose a\nnovel sequence tagging framework, instead of extracting event types and\nevents-causes separately. Experiments show our framework outperforms baseline\nmethods even when its encoder module uses an initialized pre-trained BERT\nencoder, showing the power of the new tagging framework. In this competition,\nour team achieved 1st place in the first stage leaderboard, and 3rd place in\nthe final stage leaderboard.", "published": "2021-10-28 09:08:35", "link": "http://arxiv.org/abs/2110.15722v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "NxMTransformer: Semi-Structured Sparsification for Natural Language\n  Understanding via ADMM", "abstract": "Natural Language Processing (NLP) has recently achieved success by using huge\npre-trained Transformer networks. However, these models often contain hundreds\nof millions or even billions of parameters, bringing challenges to online\ndeployment due to latency constraints. Recently, hardware manufacturers have\nintroduced dedicated hardware for NxM sparsity to provide the flexibility of\nunstructured pruning with the runtime efficiency of structured approaches. NxM\nsparsity permits arbitrarily selecting M parameters to retain from a contiguous\ngroup of N in the dense representation. However, due to the extremely high\ncomplexity of pre-trained models, the standard sparse fine-tuning techniques\noften fail to generalize well on downstream tasks, which have limited data\nresources. To address such an issue in a principled manner, we introduce a new\nlearning framework, called NxMTransformer, to induce NxM semi-structured\nsparsity on pretrained language models for natural language understanding to\nobtain better performance. In particular, we propose to formulate the NxM\nsparsity as a constrained optimization problem and use Alternating Direction\nMethod of Multipliers (ADMM) to optimize the downstream tasks while taking the\nunderlying hardware constraints into consideration. ADMM decomposes the NxM\nsparsification problem into two sub-problems that can be solved sequentially,\ngenerating sparsified Transformer networks that achieve high accuracy while\nbeing able to effectively execute on newly released hardware. We apply our\napproach to a wide range of NLP tasks, and our proposed method is able to\nachieve 1.7 points higher accuracy in GLUE score than current practices.\nMoreover, we perform detailed analysis on our approach and shed light on how\nADMM affects fine-tuning accuracy for downstream tasks. Finally, we illustrate\nhow NxMTransformer achieves performance improvement with knowledge\ndistillation.", "published": "2021-10-28 17:43:06", "link": "http://arxiv.org/abs/2110.15766v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Analysis of Programming Course Evaluations Before and After the\n  Introduction of an Autograder", "abstract": "Commonly, introductory programming courses in higher education institutions\nhave hundreds of participating students eager to learn to program. The manual\neffort for reviewing the submitted source code and for providing feedback can\nno longer be managed. Manually reviewing the submitted homework can be\nsubjective and unfair, particularly if many tutors are responsible for grading.\nDifferent autograders can help in this situation; however, there is a lack of\nknowledge about how autograders can impact students' overall perception of\nprogramming classes and teaching. This is relevant for course organizers and\ninstitutions to keep their programming courses attractive while coping with\nincreasing students.\n  This paper studies the answers to the standardized university evaluation\nquestionnaires of multiple large-scale foundational computer science courses\nwhich recently introduced autograding. The differences before and after this\nintervention are analyzed. By incorporating additional observations, we\nhypothesize how the autograder might have contributed to the significant\nchanges in the data, such as, improved interactions between tutors and\nstudents, improved overall course quality, improved learning success, increased\ntime spent, and reduced difficulty. This qualitative study aims to provide\nhypotheses for future research to define and conduct quantitative surveys and\ndata analysis. The autograder technology can be validated as a teaching method\nto improve student satisfaction with programming courses.", "published": "2021-10-28 14:09:44", "link": "http://arxiv.org/abs/2110.15134v2", "categories": ["cs.HC", "cs.CL", "cs.CY"], "primary_category": "cs.HC"}
{"title": "Bridge the Gap Between CV and NLP! A Gradient-based Textual Adversarial\n  Attack Framework", "abstract": "Despite recent success on various tasks, deep learning techniques still\nperform poorly on adversarial examples with small perturbations. While\noptimization-based methods for adversarial attacks are well-explored in the\nfield of computer vision, it is impractical to directly apply them in natural\nlanguage processing due to the discrete nature of the text. To address the\nproblem, we propose a unified framework to extend the existing\noptimization-based adversarial attack methods in the vision domain to craft\ntextual adversarial samples. In this framework, continuously optimized\nperturbations are added to the embedding layer and amplified in the forward\npropagation process. Then the final perturbed latent representations are\ndecoded with a masked language model head to obtain potential adversarial\nsamples. In this paper, we instantiate our framework with an attack algorithm\nnamed Textual Projected Gradient Descent (T-PGD). We find our algorithm\neffective even using proxy gradient information. Therefore, we perform the more\nchallenging transfer black-box attack and conduct comprehensive experiments to\nevaluate our attack algorithm with several models on three benchmark datasets.\nExperimental results demonstrate that our method achieves overall better\nperformance and produces more fluent and grammatical adversarial samples\ncompared to strong baseline methods. The code and data are available at\n\\url{https://github.com/Phantivia/T-PGD}.", "published": "2021-10-28 17:31:51", "link": "http://arxiv.org/abs/2110.15317v4", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning to Ground Multi-Agent Communication with Autoencoders", "abstract": "Communication requires having a common language, a lingua franca, between\nagents. This language could emerge via a consensus process, but it may require\nmany generations of trial and error. Alternatively, the lingua franca can be\ngiven by the environment, where agents ground their language in representations\nof the observed world. We demonstrate a simple way to ground language in\nlearned representations, which facilitates decentralized multi-agent\ncommunication and coordination. We find that a standard representation learning\nalgorithm -- autoencoding -- is sufficient for arriving at a grounded common\nlanguage. When agents broadcast these representations, they learn to understand\nand respond to each other's utterances and achieve surprisingly strong task\nperformance across a variety of multi-agent communication environments.", "published": "2021-10-28 17:57:26", "link": "http://arxiv.org/abs/2110.15349v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.LG"}
{"title": "What makes us curious? analysis of a corpus of open-domain questions", "abstract": "Every day people ask short questions through smart devices or online forums\nto seek answers to all kinds of queries. With the increasing number of\nquestions collected it becomes difficult to provide answers to each of them,\nwhich is one of the reasons behind the growing interest in automated question\nanswering. Some questions are similar to existing ones that have already been\nanswered, while others could be answered by an external knowledge source such\nas Wikipedia. An important question is what can be revealed by analysing a\nlarge set of questions. In 2017, \"We the Curious\" science centre in Bristol\nstarted a project to capture the curiosity of Bristolians: the project\ncollected more than 10,000 questions on various topics. As no rules were given\nduring collection, the questions are truly open-domain, and ranged across a\nvariety of topics. One important aim for the science centre was to understand\nwhat concerns its visitors had beyond science, particularly on societal and\ncultural issues. We addressed this question by developing an Artificial\nIntelligence tool that can be used to perform various processing tasks:\ndetection of equivalence between questions; detection of topic and type; and\nanswering of the question. As we focused on the creation of a \"generalist\"\ntool, we trained it with labelled data from different datasets. We called the\nresulting model QBERT. This paper describes what information we extracted from\nthe automated analysis of the WTC corpus of open-domain questions.", "published": "2021-10-28 19:37:43", "link": "http://arxiv.org/abs/2110.15409v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Noise Robustness of Contrastive Speech Representation Learning\n  with Speech Reconstruction", "abstract": "Noise robustness is essential for deploying automatic speech recognition\n(ASR) systems in real-world environments. One way to reduce the effect of noise\ninterference is to employ a preprocessing module that conducts speech\nenhancement, and then feed the enhanced speech to an ASR backend. In this work,\ninstead of suppressing background noise with a conventional cascaded pipeline,\nwe employ a noise-robust representation learned by a refined self-supervised\nframework for noisy speech recognition. We propose to combine a reconstruction\nmodule with contrastive learning and perform multi-task continual pre-training\non noisy data. The reconstruction module is used for auxiliary learning to\nimprove the noise robustness of the learned representation and thus is not\nrequired during inference. Experiments demonstrate the effectiveness of our\nproposed method. Our model substantially reduces the word error rate (WER) for\nthe synthesized noisy LibriSpeech test sets, and yields around 4.1/7.5% WER\nreduction on noisy clean/other test sets compared to data augmentation. For the\nreal-world noisy speech from the CHiME-4 challenge (1-channel track), we have\nobtained the state of the art ASR performance without any denoising front-end.\nMoreover, we achieve comparable performance to the best supervised approach\nreported with only 16% of labeled data.", "published": "2021-10-28 20:39:02", "link": "http://arxiv.org/abs/2110.15430v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Using Text Analytics for Health to Get Meaningful Insights from a Corpus\n  of COVID Scientific Papers", "abstract": "Since the beginning of COVID pandemic, there have been around 700000\nscientific papers published on the subject. A human researcher cannot possibly\nget acquainted with such a huge text corpus -- and therefore developing\nAI-based tools to help navigating this corpus and deriving some useful insights\nfrom it is highly needed. In this paper, we will use Text Analytics for Health\npre-trained service together with some cloud tools to extract some knowledge\nfrom scientific papers, gain insights, and build a tool to help researcher\nnavigate the paper collection in a meaningful way.", "published": "2021-10-28 22:14:39", "link": "http://arxiv.org/abs/2110.15453v1", "categories": ["cs.CL", "cs.DL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel\n  Training", "abstract": "The success of Transformer models has pushed the deep learning model scale to\nbillions of parameters. Due to the limited memory resource of a single GPU,\nHowever, the best practice for choosing the optimal parallel strategy is still\nlacking, since it requires domain expertise in both deep learning and parallel\ncomputing.\n  The Colossal-AI system addressed the above challenge by introducing a unified\ninterface to scale your sequential code of model training to distributed\nenvironments. It supports parallel training methods such as data, pipeline,\ntensor, and sequence parallelism, as well as heterogeneous training methods\nintegrated with zero redundancy optimizer. Compared to the baseline system,\nColossal-AI can achieve up to 2.76 times training speedup on large-scale\nmodels.", "published": "2021-10-28 04:45:55", "link": "http://arxiv.org/abs/2110.14883v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.DC"], "primary_category": "cs.LG"}
{"title": "End-to-End Speech Emotion Recognition: Challenges of Real-Life Emergency\n  Call Centers Data Recordings", "abstract": "Recognizing a speaker's emotion from their speech can be a key element in\nemergency call centers. End-to-end deep learning systems for speech emotion\nrecognition now achieve equivalent or even better results than conventional\nmachine learning approaches. In this paper, in order to validate the\nperformance of our neural network architecture for emotion recognition from\nspeech, we first trained and tested it on the widely used corpus accessible by\nthe community, IEMOCAP. We then used the same architecture as the real life\ncorpus, CEMO, composed of 440 dialogs (2h16m) from 485 speakers. The most\nfrequent emotions expressed by callers in these real life emergency dialogues\nare fear, anger and positive emotions such as relief. In the IEMOCAP general\ntopic conversations, the most frequent emotions are sadness, anger and\nhappiness. Using the same end-to-end deep learning architecture, an Unweighted\nAccuracy Recall (UA) of 63% is obtained on IEMOCAP and a UA of 45.6% on CEMO,\neach with 4 classes. Using only 2 classes (Anger, Neutral), the results for\nCEMO are 76.9% UA compared to 81.1% UA for IEMOCAP. We expect that these\nencouraging results with CEMO can be improved by combining the audio channel\nwith the linguistic channel. Real-life emotions are clearly more complex than\nacted ones, mainly due to the large diversity of emotional expressions of\nspeakers. Index Terms-emotion detection, end-to-end deep learning architecture,\ncall center, real-life database, complex emotions.", "published": "2021-10-28 08:56:57", "link": "http://arxiv.org/abs/2110.14957v1", "categories": ["cs.AI", "cs.CL", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.AI"}
{"title": "Continuous Speech Separation with Recurrent Selective Attention Network", "abstract": "While permutation invariant training (PIT) based continuous speech separation\n(CSS) significantly improves the conversation transcription accuracy, it often\nsuffers from speech leakages and failures in separation at \"hot spot\" regions\nbecause it has a fixed number of output channels. In this paper, we propose to\napply recurrent selective attention network (RSAN) to CSS, which generates a\nvariable number of output channels based on active speaker counting. In\naddition, we propose a novel block-wise dependency extension of RSAN by\nintroducing dependencies between adjacent processing blocks in the CSS\nframework. It enables the network to utilize the separation results from the\nprevious blocks to facilitate the current block processing. Experimental\nresults on the LibriCSS dataset show that the RSAN-based CSS (RSAN-CSS) network\nconsistently improves the speech recognition accuracy over PIT-based models.\nThe proposed block-wise dependency modeling further boosts the performance of\nRSAN-CSS.", "published": "2021-10-28 01:34:33", "link": "http://arxiv.org/abs/2110.14838v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "TorchAudio: Building Blocks for Audio and Speech Processing", "abstract": "This document describes version 0.10 of TorchAudio: building blocks for\nmachine learning applications in the audio and speech processing domain. The\nobjective of TorchAudio is to accelerate the development and deployment of\nmachine learning applications for researchers and engineers by providing\noff-the-shelf building blocks. The building blocks are designed to be\nGPU-compatible, automatically differentiable, and production-ready. TorchAudio\ncan be easily installed from Python Package Index repository and the source\ncode is publicly available under a BSD-2-Clause License (as of September 2021)\nat https://github.com/pytorch/audio. In this document, we provide an overview\nof the design principles, functionalities, and benchmarks of TorchAudio. We\nalso benchmark our implementation of several audio and speech operations and\nmodels. We verify through the benchmarks that our implementations of various\noperations and models are valid and perform similarly to other publicly\navailable implementations.", "published": "2021-10-28 10:58:22", "link": "http://arxiv.org/abs/2110.15018v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
