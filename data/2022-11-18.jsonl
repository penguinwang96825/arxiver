{"title": "Towards Explaining Subjective Ground of Individuals on Social Media", "abstract": "Large-scale language models have been reducing the gap between machines and\nhumans in understanding the real world, yet understanding an individual's\ntheory of mind and behavior from text is far from being resolved.\n  This research proposes a neural model -- Subjective Ground Attention -- that\nlearns subjective grounds of individuals and accounts for their judgments on\nsituations of others posted on social media. Using simple attention modules as\nwell as taking one's previous activities into consideration, we empirically\nshow that our model provides human-readable explanations of an individual's\nsubjective preference in judging social situations. We further qualitatively\nevaluate the explanations generated by the model and claim that our model\nlearns an individual's subjective orientation towards abstract moral concepts", "published": "2022-11-18 00:29:05", "link": "http://arxiv.org/abs/2211.09953v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Dataset for Hyper-Relational Extraction and a Cube-Filling Approach", "abstract": "Relation extraction has the potential for large-scale knowledge graph\nconstruction, but current methods do not consider the qualifier attributes for\neach relation triplet, such as time, quantity or location. The qualifiers form\nhyper-relational facts which better capture the rich and complex knowledge\ngraph structure. For example, the relation triplet (Leonard Parker, Educated\nAt, Harvard University) can be factually enriched by including the qualifier\n(End Time, 1967). Hence, we propose the task of hyper-relational extraction to\nextract more specific and complete facts from text. To support the task, we\nconstruct HyperRED, a large-scale and general-purpose dataset. Existing models\ncannot perform hyper-relational extraction as it requires a model to consider\nthe interaction between three entities. Hence, we propose CubeRE, a\ncube-filling model inspired by table-filling approaches and explicitly\nconsiders the interaction between relation triplets and qualifiers. To improve\nmodel scalability and reduce negative class imbalance, we further propose a\ncube-pruning method. Our experiments show that CubeRE outperforms strong\nbaselines and reveal possible directions for future research. Our code and data\nare available at github.com/declare-lab/HyperRED.", "published": "2022-11-18 03:51:28", "link": "http://arxiv.org/abs/2211.10018v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Scaling Native Language Identification with Transformer Adapters", "abstract": "Native language identification (NLI) is the task of automatically identifying\nthe native language (L1) of an individual based on their language production in\na learned language. It is useful for a variety of purposes including marketing,\nsecurity and educational applications. NLI is usually framed as a multi-label\nclassification task, where numerous designed features are combined to achieve\nstate-of-the-art results. Recently deep generative approach based on\ntransformer decoders (GPT-2) outperformed its counterparts and achieved the\nbest results on the NLI benchmark datasets. We investigate this approach to\ndetermine the practical implications compared to traditional state-of-the-art\nNLI systems. We introduce transformer adapters to address memory limitations\nand improve training/inference speed to scale NLI applications for production.", "published": "2022-11-18 09:40:16", "link": "http://arxiv.org/abs/2211.10117v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GoSum: Extractive Summarization of Long Documents by Reinforcement\n  Learning and Graph Organized discourse state", "abstract": "Extracting summaries from long documents can be regarded as sentence\nclassification using the structural information of the documents. How to use\nsuch structural information to summarize a document is challenging. In this\npaper, we propose GoSum, a novel graph and reinforcement learning based\nextractive model for long-paper summarization. In particular, GoSum encodes\nsentence states in reinforcement learning by building a heterogeneous graph for\neach input document at different discourse levels. An edge in the graph\nreflects the discourse hierarchy of a document for restraining the semantic\ndrifts across section boundaries. We evaluate GoSum on two datasets of\nscientific articles summarization: PubMed and arXiv. The experimental results\nhave demonstrated that GoSum achieve state-of-the-art results compared with\nstrong baselines of both extractive and abstractive models. The ablation\nstudies further validate that the performance of our GoSum benefits from the\nuse of discourse information.", "published": "2022-11-18 14:07:29", "link": "http://arxiv.org/abs/2211.10247v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GENIUS: Sketch-based Language Model Pre-training via Extreme and\n  Selective Masking for Text Generation and Augmentation", "abstract": "We introduce GENIUS: a conditional text generation model using sketches as\ninput, which can fill in the missing contexts for a given sketch (key\ninformation consisting of textual spans, phrases, or words, concatenated by\nmask tokens). GENIUS is pre-trained on a large-scale textual corpus with a\nnovel reconstruction from sketch objective using an extreme and selective\nmasking strategy, enabling it to generate diverse and high-quality texts given\nsketches. Comparison with other competitive conditional language models (CLMs)\nreveals the superiority of GENIUS's text generation quality. We further show\nthat GENIUS can be used as a strong and ready-to-use data augmentation tool for\nvarious natural language processing (NLP) tasks. Most existing textual data\naugmentation methods are either too conservative, by making small changes to\nthe original text, or too aggressive, by creating entirely new samples. With\nGENIUS, we propose GeniusAug, which first extracts the target-aware sketches\nfrom the original training set and then generates new samples based on the\nsketches. Empirical experiments on 6 text classification datasets show that\nGeniusAug significantly improves the models' performance in both\nin-distribution (ID) and out-of-distribution (OOD) settings. We also\ndemonstrate the effectiveness of GeniusAug on named entity recognition (NER)\nand machine reading comprehension (MRC) tasks. (Code and models are publicly\navailable at https://github.com/microsoft/SCGLab and\nhttps://github.com/beyondguo/genius)", "published": "2022-11-18 16:39:45", "link": "http://arxiv.org/abs/2211.10330v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Metadata Might Make Language Models Better", "abstract": "This paper discusses the benefits of including metadata when training\nlanguage models on historical collections. Using 19th-century newspapers as a\ncase study, we extend the time-masking approach proposed by Rosin et al., 2022\nand compare different strategies for inserting temporal, political and\ngeographical information into a Masked Language Model. After fine-tuning\nseveral DistilBERT on enhanced input data, we provide a systematic evaluation\nof these models on a set of evaluation tasks: pseudo-perplexity, metadata\nmask-filling and supervised classification. We find that showing relevant\nmetadata to a language model has a beneficial impact and may even produce more\nrobust and fairer models.", "published": "2022-11-18 08:29:00", "link": "http://arxiv.org/abs/2211.10086v1", "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.CL"}
{"title": "Context Variance Evaluation of Pretrained Language Models for\n  Prompt-based Biomedical Knowledge Probing", "abstract": "Pretrained language models (PLMs) have motivated research on what kinds of\nknowledge these models learn. Fill-in-the-blanks problem (e.g., cloze tests) is\na natural approach for gauging such knowledge. BioLAMA generates prompts for\nbiomedical factual knowledge triples and uses the Top-k accuracy metric to\nevaluate different PLMs' knowledge. However, existing research has shown that\nsuch prompt-based knowledge probing methods can only probe a lower bound of\nknowledge. Many factors like prompt-based probing biases make the LAMA\nbenchmark unreliable and unstable. This problem is more prominent in BioLAMA.\nThe severe long-tailed distribution in vocabulary and large-N-M relation make\nthe performance gap between LAMA and BioLAMA remain notable. To address these,\nwe introduce context variance into the prompt generation and propose a new\nrank-change-based evaluation metric. Different from the previous known-unknown\nevaluation criteria, we propose the concept of \"Misunderstand\" in LAMA for the\nfirst time. Through experiments on 12 PLMs, our context variance prompts and\nUnderstand-Confuse-Misunderstand (UCM) metric makes BioLAMA more friendly to\nlarge-N-M relations and rare relations. We also conducted a set of control\nexperiments to disentangle \"understand\" from just \"read and copy\".", "published": "2022-11-18 14:44:09", "link": "http://arxiv.org/abs/2211.10265v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Copy Mechanism for Handling Knowledge Base Elements in SPARQL Neural\n  Machine Translation", "abstract": "Neural Machine Translation (NMT) models from English to SPARQL are a\npromising development for SPARQL query generation. However, current\narchitectures are unable to integrate the knowledge base (KB) schema and handle\nquestions on knowledge resources, classes, and properties unseen during\ntraining, rendering them unusable outside the scope of topics covered in the\ntraining set. Inspired by the performance gains in natural language processing\ntasks, we propose to integrate a copy mechanism for neural SPARQL query\ngeneration as a way to tackle this issue. We illustrate our proposal by adding\na copy layer and a dynamic knowledge base vocabulary to two Seq2Seq\narchitectures (CNNs and Transformers). This layer makes the models copy KB\nelements directly from the questions, instead of generating them. We evaluate\nour approach on state-of-the-art datasets, including datasets referencing\nunknown KB elements and measure the accuracy of the copy-augmented\narchitectures. Our results show a considerable increase in performance on all\ndatasets compared to non-copy architectures.", "published": "2022-11-18 14:56:35", "link": "http://arxiv.org/abs/2211.10271v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CITADEL: Conditional Token Interaction via Dynamic Lexical Routing for\n  Efficient and Effective Multi-Vector Retrieval", "abstract": "Multi-vector retrieval methods combine the merits of sparse (e.g. BM25) and\ndense (e.g. DPR) retrievers and have achieved state-of-the-art performance on\nvarious retrieval tasks. These methods, however, are orders of magnitude slower\nand need much more space to store their indices compared to their single-vector\ncounterparts. In this paper, we unify different multi-vector retrieval models\nfrom a token routing viewpoint and propose conditional token interaction via\ndynamic lexical routing, namely CITADEL, for efficient and effective\nmulti-vector retrieval. CITADEL learns to route different token vectors to the\npredicted lexical ``keys'' such that a query token vector only interacts with\ndocument token vectors routed to the same key. This design significantly\nreduces the computation cost while maintaining high accuracy. Notably, CITADEL\nachieves the same or slightly better performance than the previous state of the\nart, ColBERT-v2, on both in-domain (MS MARCO) and out-of-domain (BEIR)\nevaluations, while being nearly 40 times faster. Code and data are available at\nhttps://github.com/facebookresearch/dpr-scale.", "published": "2022-11-18 18:27:35", "link": "http://arxiv.org/abs/2211.10411v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "PAL: Program-aided Language Models", "abstract": "Large language models (LLMs) have recently demonstrated an impressive ability\nto perform arithmetic and symbolic reasoning tasks, when provided with a few\nexamples at test time (\"few-shot prompting\"). Much of this success can be\nattributed to prompting methods such as \"chain-of-thought'', which employ LLMs\nfor both understanding the problem description by decomposing it into steps, as\nwell as solving each step of the problem. While LLMs seem to be adept at this\nsort of step-by-step decomposition, LLMs often make logical and arithmetic\nmistakes in the solution part, even when the problem is decomposed correctly.\nIn this paper, we present Program-Aided Language models (PAL): a novel approach\nthat uses the LLM to read natural language problems and generate programs as\nthe intermediate reasoning steps, but offloads the solution step to a runtime\nsuch as a Python interpreter. With PAL, decomposing the natural language\nproblem into runnable steps remains the only learning task for the LLM, while\nsolving is delegated to the interpreter. We demonstrate this synergy between a\nneural LLM and a symbolic interpreter across 13 mathematical, symbolic, and\nalgorithmic reasoning tasks from BIG-Bench Hard and other benchmarks. In all\nthese natural language reasoning tasks, generating code using an LLM and\nreasoning using a Python interpreter leads to more accurate results than much\nlarger models. For example, PAL using Codex achieves state-of-the-art few-shot\naccuracy on the GSM8K benchmark of math word problems, surpassing PaLM-540B\nwhich uses chain-of-thought by absolute 15% top-1. Our code and data are\npublicly available at http://reasonwithpal.com/ .", "published": "2022-11-18 18:56:13", "link": "http://arxiv.org/abs/2211.10435v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Social media mining for toxicovigilance of prescription medications:\n  End-to-end pipeline, challenges and future work", "abstract": "Substance use, substance use disorder, and overdoses related to substance use\nare major public health problems globally and in the United States. A key\naspect of addressing these problems from a public health standpoint is improved\nsurveillance. Traditional surveillance systems are laggy, and social media are\npotentially useful sources of timely data. However, mining knowledge from\nsocial media is challenging, and requires the development of advanced\nartificial intelligence, specifically natural language processing (NLP) and\nmachine learning methods. We developed a sophisticated end-to-end pipeline for\nmining information about nonmedical prescription medication use from social\nmedia, namely Twitter and Reddit. Our pipeline employs supervised machine\nlearning and NLP for filtering out noise and characterizing the chatter. In\nthis paper, we describe our end-to-end pipeline developed over four years. In\naddition to describing our data mining infrastructure, we discuss existing\nchallenges in social media mining for toxicovigilance, and possible future\nresearch directions.", "published": "2022-11-18 05:27:59", "link": "http://arxiv.org/abs/2211.10443v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Knowledge Graph Refinement based on Triplet BERT-Networks", "abstract": "Knowledge graph embedding techniques are widely used for knowledge graph\nrefinement tasks such as graph completion and triple classification. These\ntechniques aim at embedding the entities and relations of a Knowledge Graph\n(KG) in a low dimensional continuous feature space. This paper adopts a\ntransformer-based triplet network creating an embedding space that clusters the\ninformation about an entity or relation in the KG. It creates textual sequences\nfrom facts and fine-tunes a triplet network of pre-trained transformer-based\nlanguage models. It adheres to an evaluation paradigm that relies on an\nefficient spatial semantic search technique. We show that this evaluation\nprotocol is more adapted to a few-shot setting for the relation prediction\ntask. Our proposed GilBERT method is evaluated on triplet classification and\nrelation prediction tasks on multiple well-known benchmark knowledge graphs\nsuch as FB13, WN11, and FB15K. We show that GilBERT achieves better or\ncomparable results to the state-of-the-art performance on these two refinement\ntasks.", "published": "2022-11-18 19:01:21", "link": "http://arxiv.org/abs/2211.10460v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Knowledge Graph Generation From Text", "abstract": "In this work we propose a novel end-to-end multi-stage Knowledge Graph (KG)\ngeneration system from textual inputs, separating the overall process into two\nstages. The graph nodes are generated first using pretrained language model,\nfollowed by a simple edge construction head, enabling efficient KG extraction\nfrom the text. For each stage we consider several architectural choices that\ncan be used depending on the available training resources. We evaluated the\nmodel on a recent WebNLG 2020 Challenge dataset, matching the state-of-the-art\nperformance on text-to-RDF generation task, as well as on New York Times (NYT)\nand a large-scale TekGen datasets, showing strong overall performance,\noutperforming the existing baselines. We believe that the proposed system can\nserve as a viable KG construction alternative to the existing linearization or\nsampling-based graph generation approaches. Our code can be found at\nhttps://github.com/IBM/Grapher", "published": "2022-11-18 21:27:13", "link": "http://arxiv.org/abs/2211.10511v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DS-1000: A Natural and Reliable Benchmark for Data Science Code\n  Generation", "abstract": "We introduce DS-1000, a code generation benchmark with a thousand data\nscience problems spanning seven Python libraries, such as NumPy and Pandas.\nCompared to prior works, DS-1000 incorporates three core features. First, our\nproblems reflect diverse, realistic, and practical use cases since we collected\nthem from StackOverflow. Second, our automatic evaluation is highly specific\n(reliable) -- across all Codex-002-predicted solutions that our evaluation\naccept, only 1.8% of them are incorrect; we achieve this with multi-criteria\nmetrics, checking both functional correctness by running test cases and\nsurface-form constraints by restricting API usages or keywords. Finally, we\nproactively defend against memorization by slightly modifying our problems to\nbe different from the original StackOverflow source; consequently, models\ncannot answer them correctly by memorizing the solutions from pre-training. The\ncurrent best public system (Codex-002) achieves 43.3% accuracy, leaving ample\nroom for improvement. We release our benchmark at\nhttps://ds1000-code-gen.github.io.", "published": "2022-11-18 17:20:27", "link": "http://arxiv.org/abs/2211.11501v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Where did you tweet from? Inferring the origin locations of tweets based\n  on contextual information", "abstract": "Public conversations on Twitter comprise many pertinent topics including\ndisasters, protests, politics, propaganda, sports, climate change,\nepidemics/pandemic outbreaks, etc., that can have both regional and global\naspects. Spatial discourse analysis rely on geographical data. However, today\nless than 1% of tweets are geotagged; in both cases--point location or bounding\nplace information. A major issue with tweets is that Twitter users can be at\nlocation A and exchange conversations specific to location B, which we call the\nLocation A/B problem. The problem is considered solved if location entities can\nbe classified as either origin locations (Location As) or non-origin locations\n(Location Bs). In this work, we propose a simple yet effective framework--the\nTrue Origin Model--to address the problem that uses machine-level natural\nlanguage understanding to identify tweets that conceivably contain their origin\nlocation information. The model achieves promising accuracy at country (80%),\nstate (67%), city (58%), county (56%) and district (64%) levels with support\nfrom a Location Extraction Model as basic as the CoNLL-2003-based RoBERTa. We\nemploy a tweet contexualizer (locBERT) which is one of the core components of\nthe proposed model, to investigate multiple tweets' distributions for\nunderstanding Twitter users' tweeting behavior in terms of mentioning origin\nand non-origin locations. We also highlight a major concern with the currently\nregarded gold standard test set (ground truth) methodology, introduce a new\ndata set, and identify further research avenues for advancing the area.", "published": "2022-11-18 01:33:01", "link": "http://arxiv.org/abs/2211.16506v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Who Says Elephants Can't Run: Bringing Large Scale MoE Models into Cloud\n  Scale Production", "abstract": "Mixture of Experts (MoE) models with conditional execution of sparsely\nactivated layers have enabled training models with a much larger number of\nparameters. As a result, these models have achieved significantly better\nquality on various natural language processing tasks including machine\ntranslation. However, it remains challenging to deploy such models in real-life\nscenarios due to the large memory requirements and inefficient inference. In\nthis work, we introduce a highly efficient inference framework with several\noptimization approaches to accelerate the computation of sparse models and cut\ndown the memory consumption significantly. While we achieve up to 26x speed-up\nin terms of throughput, we also reduce the model size almost to one eighth of\nthe original 32-bit float model by quantizing expert weights into 4-bit\nintegers. As a result, we are able to deploy 136x larger models with 27% less\ncost and significantly better quality compared to the existing solutions. This\nenables a paradigm shift in deploying large scale multilingual MoE transformers\nmodels replacing the traditional practice of distilling teacher models into\ndozens of smaller models per language or task.", "published": "2022-11-18 03:43:52", "link": "http://arxiv.org/abs/2211.10017v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Overview of the WANLP 2022 Shared Task on Propaganda Detection in Arabic", "abstract": "Propaganda is the expression of an opinion or an action by an individual or a\ngroup deliberately designed to influence the opinions or the actions of other\nindividuals or groups with reference to predetermined ends, which is achieved\nby means of well-defined rhetorical and psychological devices. Propaganda\ntechniques are commonly used in social media to manipulate or to mislead users.\nThus, there has been a lot of recent research on automatic detection of\npropaganda techniques in text as well as in memes. However, so far the focus\nhas been primarily on English. With the aim to bridge this language gap, we ran\na shared task on detecting propaganda techniques in Arabic tweets as part of\nthe WANLP 2022 workshop, which included two subtasks. Subtask~1 asks to\nidentify the set of propaganda techniques used in a tweet, which is a\nmultilabel classification problem, while Subtask~2 asks to detect the\npropaganda techniques used in a tweet together with the exact span(s) of text\nin which each propaganda technique appears. The task attracted 63 team\nregistrations, and eventually 14 and 3 teams made submissions for subtask 1 and\n2, respectively. Finally, 11 teams submitted system description papers.", "published": "2022-11-18 07:04:31", "link": "http://arxiv.org/abs/2211.10057v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "F.2.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "FiE: Building a Global Probability Space by Leveraging Early Fusion in\n  Encoder for Open-Domain Question Answering", "abstract": "Generative models have recently started to outperform extractive models in\nOpen Domain Question Answering, largely by leveraging their decoder to attend\nover multiple encoded passages and combining their information. However,\ngenerative models tend to be larger than extractive models due to the need for\na decoder, run slower during inference due to auto-regressive decoder beam\nsearch, and their generated output often suffers from hallucinations. We\npropose to extend transformer encoders with the ability to fuse information\nfrom multiple passages, using global representation to provide cross-sample\nattention over all tokens across samples. Furthermore, we propose an\nalternative answer span probability calculation to better aggregate answer\nscores in the global space of all samples. Using our proposed method, we\noutperform the current state-of-the-art method by $2.5$ Exact Match score on\nthe Natural Question dataset while using only $25\\%$ of parameters and $35\\%$\nof the latency during inference, and $4.4$ Exact Match on WebQuestions dataset.\nWhen coupled with synthetic data augmentation, we outperform larger models on\nthe TriviaQA dataset as well. The latency and parameter savings of our method\nmake it particularly attractive for open-domain question answering, as these\nmodels are often compute-intensive.", "published": "2022-11-18 10:43:39", "link": "http://arxiv.org/abs/2211.10147v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large\n  Language Models", "abstract": "Large language models (LLMs) show excellent performance but are compute- and\nmemory-intensive. Quantization can reduce memory and accelerate inference.\nHowever, existing methods cannot maintain accuracy and hardware efficiency at\nthe same time. We propose SmoothQuant, a training-free, accuracy-preserving,\nand general-purpose post-training quantization (PTQ) solution to enable 8-bit\nweight, 8-bit activation (W8A8) quantization for LLMs. Based on the fact that\nweights are easy to quantize while activations are not, SmoothQuant smooths the\nactivation outliers by offline migrating the quantization difficulty from\nactivations to weights with a mathematically equivalent transformation.\nSmoothQuant enables an INT8 quantization of both weights and activations for\nall the matrix multiplications in LLMs, including OPT, BLOOM, GLM, MT-NLG,\nLlama-1/2, Falcon, Mistral, and Mixtral models. We demonstrate up to 1.56x\nspeedup and 2x memory reduction for LLMs with negligible loss in accuracy.\nSmoothQuant enables serving 530B LLM within a single node. Our work offers a\nturn-key solution that reduces hardware costs and democratizes LLMs. Code is\navailable at https://github.com/mit-han-lab/smoothquant.", "published": "2022-11-18 18:59:33", "link": "http://arxiv.org/abs/2211.10438v7", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Visual Programming: Compositional visual reasoning without training", "abstract": "We present VISPROG, a neuro-symbolic approach to solving complex and\ncompositional visual tasks given natural language instructions. VISPROG avoids\nthe need for any task-specific training. Instead, it uses the in-context\nlearning ability of large language models to generate python-like modular\nprograms, which are then executed to get both the solution and a comprehensive\nand interpretable rationale. Each line of the generated program may invoke one\nof several off-the-shelf computer vision models, image processing routines, or\npython functions to produce intermediate outputs that may be consumed by\nsubsequent parts of the program. We demonstrate the flexibility of VISPROG on 4\ndiverse tasks - compositional visual question answering, zero-shot reasoning on\nimage pairs, factual knowledge object tagging, and language-guided image\nediting. We believe neuro-symbolic approaches like VISPROG are an exciting\navenue to easily and effectively expand the scope of AI systems to serve the\nlong tail of complex tasks that people may wish to perform.", "published": "2022-11-18 18:50:09", "link": "http://arxiv.org/abs/2211.11559v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Detect Only What You Specify : Object Detection with Linguistic Target", "abstract": "Object detection is a computer vision task of predicting a set of bounding\nboxes and category labels for each object of interest in a given image. The\ncategory is related to a linguistic symbol such as 'dog' or 'person' and there\nshould be relationships among them. However the object detector only learns to\nclassify the categories and does not treat them as the linguistic symbols.\nMulti-modal models often use the pre-trained object detector to extract object\nfeatures from the image, but the models are separated from the detector and the\nextracted visual features does not change with their linguistic input. We\nrethink the object detection as a vision-and-language reasoning task. We then\npropose targeted detection task, where detection targets are given by a natural\nlanguage and the goal of the task is to detect only all the target objects in a\ngiven image. There are no detection if the target is not given. Commonly used\nmodern object detectors have many hand-designed components like anchor and it\nis difficult to fuse the textual inputs into the complex pipeline. We thus\npropose Language-Targeted Detector (LTD) for the targeted detection based on a\nrecently proposed Transformer-based detector. LTD is a encoder-decoder\narchitecture and our conditional decoder allows the model to reason about the\nencoded image with the textual input as the linguistic context. We evaluate\ndetection performances of LTD on COCO object detection dataset and also show\nthat our model improves the detection results with the textual input grounding\nto the visual object.", "published": "2022-11-18 07:28:47", "link": "http://arxiv.org/abs/2211.11572v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Dialogs Re-enacted Across Languages", "abstract": "To support machine learning of cross-language prosodic mappings and other\nways to improve speech-to-speech translation, we present a protocol for\ncollecting closely matched pairs of utterances across languages, a description\nof the resulting data collection and its public release, and some observations\nand musings. This report is intended for: people using this corpus, people\nextending this corpus, and people designing similar collections of bilingual\ndialog data.", "published": "2022-11-18 17:08:12", "link": "http://arxiv.org/abs/2211.11584v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Overview of the HASOC Subtrack at FIRE 2022: Offensive Language\n  Identification in Marathi", "abstract": "The widespread of offensive content online has become a reason for great\nconcern in recent years, motivating researchers to develop robust systems\ncapable of identifying such content automatically. With the goal of carrying\nout a fair evaluation of these systems, several international competitions have\nbeen organized, providing the community with important benchmark data and\nevaluation methods for various languages. Organized since 2019, the HASOC (Hate\nSpeech and Offensive Content Identification) shared task is one of these\ninitiatives. In its fourth iteration, HASOC 2022 included three subtracks for\nEnglish, Hindi, and Marathi. In this paper, we report the results of the HASOC\n2022 Marathi subtrack which provided participants with a dataset containing\ndata from Twitter manually annotated using the popular OLID taxonomy. The\nMarathi track featured three additional subtracks, each corresponding to one\nlevel of the taxonomy: Task A - offensive content identification (offensive vs.\nnon-offensive); Task B - categorization of offensive types (targeted vs.\nuntargeted), and Task C - offensive target identification (individual vs. group\nvs. others). Overall, 59 runs were submitted by 10 teams. The best systems\nobtained an F1 of 0.9745 for Subtrack 3A, an F1 of 0.9207 for Subtrack 3B, and\nF1 of 0.9607 for Subtrack 3C. The best performing algorithms were a mixture of\ntraditional and deep learning approaches.", "published": "2022-11-18 11:17:15", "link": "http://arxiv.org/abs/2211.10163v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Exploring WavLM on Speech Enhancement", "abstract": "There is a surge in interest in self-supervised learning approaches for\nend-to-end speech encoding in recent years as they have achieved great success.\nEspecially, WavLM showed state-of-the-art performance on various speech\nprocessing tasks. To better understand the efficacy of self-supervised learning\nmodels for speech enhancement, in this work, we design and conduct a series of\nexperiments with three resource conditions by combining WavLM and two\nhigh-quality speech enhancement systems. Also, we propose a regression-based\nWavLM training objective and a noise-mixing data configuration to further boost\nthe downstream enhancement performance. The experiments on the DNS challenge\ndataset and a simulation dataset show that the WavLM benefits the speech\nenhancement task in terms of both speech quality and speech recognition\naccuracy, especially for low fine-tuning resources. For the high fine-tuning\nresource condition, only the word error rate is substantially improved.", "published": "2022-11-18 02:23:16", "link": "http://arxiv.org/abs/2211.09988v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Self-Transriber: Few-shot Lyrics Transcription with Self-training", "abstract": "The current lyrics transcription approaches heavily rely on supervised\nlearning with labeled data, but such data are scarce and manual labeling of\nsinging is expensive. How to benefit from unlabeled data and alleviate limited\ndata problem have not been explored for lyrics transcription. We propose the\nfirst semi-supervised lyrics transcription paradigm, Self-Transcriber, by\nleveraging on unlabeled data using self-training with noisy student\naugmentation. We attempt to demonstrate the possibility of lyrics transcription\nwith a few amount of labeled data. Self-Transcriber generates pseudo labels of\nthe unlabeled singing using teacher model, and augments pseudo-labels to the\nlabeled data for student model update with both self-training and supervised\ntraining losses. This work closes the gap between supervised and\nsemi-supervised learning as well as opens doors for few-shot learning of lyrics\ntranscription. Our experiments show that our approach using only 12.7 hours of\nlabeled data achieves competitive performance compared with the supervised\napproaches trained on 149.1 hours of labeled data for lyrics transcription.", "published": "2022-11-18 10:58:27", "link": "http://arxiv.org/abs/2211.10152v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Self-Remixing: Unsupervised Speech Separation via Separation and\n  Remixing", "abstract": "We present Self-Remixing, a novel self-supervised speech separation method,\nwhich refines a pre-trained separation model in an unsupervised manner. The\nproposed method consists of a shuffler module and a solver module, and they\ngrow together through separation and remixing processes. Specifically, the\nshuffler first separates observed mixtures and makes pseudo-mixtures by\nshuffling and remixing the separated signals. The solver then separates the\npseudo-mixtures and remixes the separated signals back to the observed\nmixtures. The solver is trained using the observed mixtures as supervision,\nwhile the shuffler's weights are updated by taking the moving average with the\nsolver's, generating the pseudo-mixtures with fewer distortions. Our\nexperiments demonstrate that Self-Remixing gives better performance over\nexisting remixing-based self-supervised methods with the same or less training\ncosts under unsupervised setup. Self-Remixing also outperforms baselines in\nsemi-supervised domain adaptation, showing effectiveness in multiple setups.", "published": "2022-11-18 12:37:32", "link": "http://arxiv.org/abs/2211.10194v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Impact of visual assistance for automated audio captioning", "abstract": "We study the impact of visual assistance for automated audio captioning.\nUtilizing multi-encoder transformer architectures, which have previously been\nemployed to introduce vision-related information in the context of sound event\ndetection, we analyze the usefulness of incorporating a variety of pretrained\nfeatures.\n  We perform experiments on a YouTube-based audiovisual data set and\ninvestigate the effect of applying the considered transfer learning technique\nin terms of a variety of captioning metrics.\n  We find that only one of the considered kinds of pretrained features provides\nconsistent improvements, while the others do not provide any noteworthy gains\nat all. Interestingly, the outcomes of prior research efforts indicate that the\nexact opposite is true in the case of sound event detection, leading us to\nconclude that the optimal choice of visual embeddings is strongly dependent on\nthe task at hand.\n  More specifically, visual features focusing on semantics appear appropriate\nin the context of automated audio captioning, while for sound event detection,\ntime information seems to be more important.", "published": "2022-11-18 23:55:13", "link": "http://arxiv.org/abs/2211.10539v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Persian ASR-based SER: Modification of Sharif Emotional Speech\n  Database and Investigation of Persian Text Corpora", "abstract": "Speech Emotion Recognition (SER) is one of the essential perceptual methods\nof humans in understanding the situation and how to interact with others,\ntherefore, in recent years, it has been tried to add the ability to recognize\nemotions to human-machine communication systems. Since the SER process relies\non labeled data, databases are essential for it. Incomplete, low-quality or\ndefective data may lead to inaccurate predictions. In this paper, we fixed the\ninconsistencies in Sharif Emotional Speech Database (ShEMO), as a Persian\ndatabase, by using an Automatic Speech Recognition (ASR) system and\ninvestigating the effect of Farsi language models obtained from accessible\nPersian text corpora. We also introduced a Persian/Farsi ASR-based SER system\nthat uses linguistic features of the ASR outputs and Deep Learning-based\nmodels.", "published": "2022-11-18 10:33:20", "link": "http://arxiv.org/abs/2211.09956v1", "categories": ["eess.AS", "cs.AI", "cs.SD", "68T10 (Primary) 68T50, 68T07 (Secondary)", "I.2"], "primary_category": "eess.AS"}
{"title": "Speaker Overlap-aware Neural Diarization for Multi-party Meeting\n  Analysis", "abstract": "Recently, hybrid systems of clustering and neural diarization models have\nbeen successfully applied in multi-party meeting analysis. However, current\nmodels always treat overlapped speaker diarization as a multi-label\nclassification problem, where speaker dependency and overlaps are not well\nconsidered. To overcome the disadvantages, we reformulate overlapped speaker\ndiarization task as a single-label prediction problem via the proposed power\nset encoding (PSE). Through this formulation, speaker dependency and overlaps\ncan be explicitly modeled. To fully leverage this formulation, we further\npropose the speaker overlap-aware neural diarization (SOND) model, which\nconsists of a context-independent (CI) scorer to model global speaker\ndiscriminability, a context-dependent scorer (CD) to model local\ndiscriminability, and a speaker combining network (SCN) to combine and reassign\nspeaker activities. Experimental results show that using the proposed\nformulation can outperform the state-of-the-art methods based on target speaker\nvoice activity detection, and the performance can be further improved with\nSOND, resulting in a 6.30% relative diarization error reduction.", "published": "2022-11-18 14:03:26", "link": "http://arxiv.org/abs/2211.10243v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "AVATAR submission to the Ego4D AV Transcription Challenge", "abstract": "In this report, we describe our submission to the Ego4D AudioVisual (AV)\nSpeech Transcription Challenge 2022. Our pipeline is based on AVATAR, a state\nof the art encoder-decoder model for AV-ASR that performs early fusion of\nspectrograms and RGB images. We describe the datasets, experimental settings\nand ablations. Our final method achieves a WER of 68.40 on the challenge test\nset, outperforming the baseline by 43.7%, and winning the challenge.", "published": "2022-11-18 01:03:30", "link": "http://arxiv.org/abs/2211.09966v1", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.CV"}
