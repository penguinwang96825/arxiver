{"title": "Event-based clinical findings extraction from radiology reports with\n  pre-trained language model", "abstract": "Radiology reports contain a diverse and rich set of clinical abnormalities\ndocumented by radiologists during their interpretation of the images.\nComprehensive semantic representations of radiological findings would enable a\nwide range of secondary use applications to support diagnosis, triage, outcomes\nprediction, and clinical research. In this paper, we present a new corpus of\nradiology reports annotated with clinical findings. Our annotation schema\ncaptures detailed representations of pathologic findings that are observable on\nimaging (\"lesions\") and other types of clinical problems (\"medical problems\").\nThe schema used an event-based representation to capture fine-grained details,\nincluding assertion, anatomy, characteristics, size, count, etc. Our gold\nstandard corpus contained a total of 500 annotated computed tomography (CT)\nreports. We extracted triggers and argument entities using two state-of-the-art\ndeep learning architectures, including BERT. We then predicted the linkages\nbetween trigger and argument entities (referred to as argument roles) using a\nBERT-based relation extraction model. We achieved the best extraction\nperformance using a BERT model pre-trained on 3 million radiology reports from\nour institution: 90.9%-93.4% F1 for finding triggers 72.0%-85.6% F1 for\narguments roles. To assess model generalizability, we used an external\nvalidation set randomly sampled from the MIMIC Chest X-ray (MIMIC-CXR)\ndatabase. The extraction performance on this validation set was 95.6% for\nfinding triggers and 79.1%-89.7% for argument roles, demonstrating that the\nmodel generalized well to the cross-institutional data with a different imaging\nmodality. We extracted the finding events from all the radiology reports in the\nMIMIC-CXR database and provided the extractions to the research community.", "published": "2021-12-27 05:03:10", "link": "http://arxiv.org/abs/2112.13512v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Chinese Learners' Phonetic Transfer of /i/ from Mandarin Chinese to\n  General American English: A Case Study of a Chinese Learner with Advanced\n  English", "abstract": "The current paper concerns language transfer at the phonetic level and\nconcentrates on the transfer phenomenon in an advanced English language\nlearner's acquisition of the English vowels /i/ and its lax counterpart. By\ndetermining whether the Chinese English-language learner (ELL), named Vanya,\ncan accurately distinguish between /i/ and its lax counterpart, and pronounce\nthem precisely in General American English (GAE), this paper serves as a\nreference for further studying language transfer among Chinese ELLs. There were\ntwo objectives: first, the learner's perceptual ability to distinguish between\nvowels /i/ and its lax counterpart was examined; second, the effect of the\nphonetic transfer was determined. Two perception tests and a production test\nwere used to attain these two objectives. The results of two perception tests\ndemonstrated Vanya's perceptual competence in distinguishing between /i/ and\nits lax counterpart and laid a solid foundation for the validity of the\nsubsequent production test. Given that Vanya's production of F1 and F2 values\nof /i/ were highly similar across his first language (Mandarin Chinese) and\nsecond language (GAE) and that both values were lower than the typical values\nfor common /i/ in GAE, with an especially prominent disparity between the F2\nvalues, it is reasonable to conclude that a phonetic transfer occurred. The\nparticipant's high perceptual competence as an advanced-level ELL did not\nnoticeably moderate the effect of phonetic transfer.", "published": "2021-12-27 08:45:34", "link": "http://arxiv.org/abs/2112.13571v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CUGE: A Chinese Language Understanding and Generation Evaluation\n  Benchmark", "abstract": "Realizing general-purpose language intelligence has been a longstanding goal\nfor natural language processing, where standard evaluation benchmarks play a\nfundamental and guiding role. We argue that for general-purpose language\nintelligence evaluation, the benchmark itself needs to be comprehensive and\nsystematic. To this end, we propose CUGE, a Chinese Language Understanding and\nGeneration Evaluation benchmark with the following features: (1) Hierarchical\nbenchmark framework, where datasets are principally selected and organized with\na language capability-task-dataset hierarchy. (2) Multi-level scoring strategy,\nwhere different levels of model performance are provided based on the\nhierarchical framework. To facilitate CUGE, we provide a public leaderboard\nthat can be customized to support flexible model judging criteria. Evaluation\nresults on representative pre-trained language models indicate ample room for\nimprovement towards general-purpose language intelligence. CUGE is publicly\navailable at cuge.baai.ac.cn.", "published": "2021-12-27 11:08:58", "link": "http://arxiv.org/abs/2112.13610v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Parameter Differentiation based Multilingual Neural Machine Translation", "abstract": "Multilingual neural machine translation (MNMT) aims to translate multiple\nlanguages with a single model and has been proved successful thanks to\neffective knowledge transfer among different languages with shared parameters.\nHowever, it is still an open question which parameters should be shared and\nwhich ones need to be task-specific. Currently, the common practice is to\nheuristically design or search language-specific modules, which is difficult to\nfind the optimal configuration. In this paper, we propose a novel parameter\ndifferentiation based method that allows the model to determine which\nparameters should be language-specific during training. Inspired by cellular\ndifferentiation, each shared parameter in our method can dynamically\ndifferentiate into more specialized types. We further define the\ndifferentiation criterion as inter-task gradient similarity. Therefore,\nparameters with conflicting inter-task gradients are more likely to be\nlanguage-specific. Extensive experiments on multilingual datasets have\ndemonstrated that our method significantly outperforms various strong baselines\nwith different parameter sharing configurations. Further analyses reveal that\nthe parameter sharing configuration obtained by our method correlates well with\nthe linguistic proximities.", "published": "2021-12-27 11:41:52", "link": "http://arxiv.org/abs/2112.13619v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey on non-English Question Answering Dataset", "abstract": "Research in question answering datasets and models has gained a lot of\nattention in the research community. Many of them release their own question\nanswering datasets as well as the models. There is tremendous progress that we\nhave seen in this area of research. The aim of this survey is to recognize,\nsummarize and analyze the existing datasets that have been released by many\nresearchers, especially in non-English datasets as well as resources such as\nresearch code, and evaluation metrics. In this paper, we review question\nanswering datasets that are available in common languages other than English\nsuch as French, German, Japanese, Chinese, Arabic, Russian, as well as the\nmultilingual and cross-lingual question-answering datasets.", "published": "2021-12-27 12:45:06", "link": "http://arxiv.org/abs/2112.13634v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Secondary Use of Clinical Problem List Entries for Neural Network-Based\n  Disease Code Assignment", "abstract": "Clinical information systems have become large repositories for\nsemi-structured and partly annotated electronic health record data, which have\nreached a critical mass that makes them interesting for supervised data-driven\nneural network approaches. We explored automated coding of 50 character long\nclinical problem list entries using the International Classification of\nDiseases (ICD-10) and evaluated three different types of network architectures\non the top 100 ICD-10 three-digit codes. A fastText baseline reached a\nmacro-averaged F1-score of 0.83, followed by a character-level LSTM with a\nmacro-averaged F1-score of 0.84. The top performing approach used a\ndownstreamed RoBERTa model with a custom language model, yielding a\nmacro-averaged F1-score of 0.88. A neural network activation analysis together\nwith an investigation of the false positives and false negatives unveiled\ninconsistent manual coding as a main limiting factor.", "published": "2021-12-27 16:11:05", "link": "http://arxiv.org/abs/2112.13756v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"A Passage to India\": Pre-trained Word Embeddings for Indian Languages", "abstract": "Dense word vectors or 'word embeddings' which encode semantic properties of\nwords, have now become integral to NLP tasks like Machine Translation (MT),\nQuestion Answering (QA), Word Sense Disambiguation (WSD), and Information\nRetrieval (IR). In this paper, we use various existing approaches to create\nmultiple word embeddings for 14 Indian languages. We place these embeddings for\nall these languages, viz., Assamese, Bengali, Gujarati, Hindi, Kannada,\nKonkani, Malayalam, Marathi, Nepali, Odiya, Punjabi, Sanskrit, Tamil, and\nTelugu in a single repository. Relatively newer approaches that emphasize\ncatering to context (BERT, ELMo, etc.) have shown significant improvements, but\nrequire a large amount of resources to generate usable models. We release\npre-trained embeddings generated using both contextual and non-contextual\napproaches. We also use MUSE and XLM to train cross-lingual embeddings for all\npairs of the aforementioned languages. To show the efficacy of our embeddings,\nwe evaluate our embedding models on XPOS, UPOS and NER tasks for all these\nlanguages. We release a total of 436 models using 8 different approaches. We\nhope they are useful for the resource-constrained Indian language NLP. The\ntitle of this paper refers to the famous novel 'A Passage to India' by E.M.\nForster, published initially in 1924.", "published": "2021-12-27 17:31:04", "link": "http://arxiv.org/abs/2112.13800v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pedagogical Word Recommendation: A novel task and dataset on\n  personalized vocabulary acquisition for L2 learners", "abstract": "When learning a second language (L2), one of the most important but tedious\ncomponents that often demoralizes students with its ineffectiveness and\ninefficiency is vocabulary acquisition, or more simply put, memorizing words.\nIn light of such, a personalized and educational vocabulary recommendation\nsystem that traces a learner's vocabulary knowledge state would have an immense\nlearning impact as it could resolve both issues. Therefore, in this paper, we\npropose and release data for a novel task called Pedagogical Word\nRecommendation (PWR). The main goal of PWR is to predict whether a given\nlearner knows a given word based on other words the learner has already seen.\nTo elaborate, we collect this data via an Intelligent Tutoring System (ITS)\nthat is serviced to ~1M L2 learners who study for the standardized English\nexam, TOEIC. As a feature of this ITS, students can directly indicate words\nthey do not know from the questions they solved to create wordbooks. Finally,\nwe report the evaluation results of a Neural Collaborative Filtering approach\nalong with an exploratory data analysis and discuss the impact and efficacy of\nthis dataset as a baseline for future studies on this task.", "published": "2021-12-27 17:52:48", "link": "http://arxiv.org/abs/2112.13808v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What do Large Language Models Learn about Scripts?", "abstract": "Script Knowledge (Schank and Abelson, 1975) has long been recognized as\ncrucial for language understanding as it can help in filling in unstated\ninformation in a narrative. However, such knowledge is expensive to produce\nmanually and difficult to induce from text due to reporting bias (Gordon and\nVan Durme, 2013). In this work, we are interested in the scientific question of\nwhether explicit script knowledge is present and accessible through pre-trained\ngenerative language models (LMs). To this end, we introduce the task of\ngenerating full event sequence descriptions (ESDs) given a scenario in the form\nof natural language prompts. In zero-shot probing experiments, we find that\ngenerative LMs produce poor ESDs with mostly omitted, irrelevant, repeated or\nmisordered events. To address this, we propose a pipeline-based script\ninduction framework (SIF) which can generate good quality ESDs for unseen\nscenarios (e.g., bake a cake). SIF is a two-staged framework that fine-tunes LM\non a small set of ESD examples in the first stage. In the second stage, ESD\ngenerated for an unseen scenario is post-processed using RoBERTa-based models\nto filter irrelevant events, remove repetitions, and reorder the temporally\nmisordered events. Through automatic and manual evaluations, we demonstrate\nthat SIF yields substantial improvements ($1$-$3$ BLUE points) over a\nfine-tuned LM. However, manual analysis shows that there is great room for\nimprovement, offering a new research direction for inducing script knowledge.", "published": "2021-12-27 18:51:18", "link": "http://arxiv.org/abs/2112.13834v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Social Ontological Knowledge Representations be Measured Using\n  Machine Learning?", "abstract": "Personal Social Ontology (PSO), it is proposed, is how an individual\nperceives the ontological properties of terms. For example, an absolute\nfatalist would arguably use terms that remove any form of agency from a person.\nSuch fatalism has the impact of ontologically defining acts such as winning,\nvictory and success in a manner that is contrary to how a non-fatalist would\nontologically define them. While both the said fatalist and non-fatalist would\nagree on the dictionary definition of these terms, they would differ on\nspecifically how they can be brought about. This difference between the two\nindividuals can be induced from their usage of these terms, i.e., the\nco-occurrence of these terms with other terms. As such a quantification of this\nsuch co-occurrence offers an avenue to characterise the social ontological\nviews of the speaker. In this paper we ask, what specific term co-occurrence\nshould be measured in order to obtain a valid and reliable psychometric measure\nof a persons social ontology? We consider the social psychology and social\nneuroscience literature to arrive at a list of social concepts that can be\nconsidered principal features of personal social ontology, and then propose an\nNLP pipeline to capture the articulation of these terms in language.", "published": "2021-12-27 19:17:07", "link": "http://arxiv.org/abs/2112.13870v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Personalized Answer Generation in E-Commerce via\n  Multi-Perspective Preference Modeling", "abstract": "Recently, Product Question Answering (PQA) on E-Commerce platforms has\nattracted increasing attention as it can act as an intelligent online shopping\nassistant and improve the customer shopping experience. Its key function,\nautomatic answer generation for product-related questions, has been studied by\naiming to generate content-preserving while question-related answers. However,\nan important characteristic of PQA, i.e., personalization, is neglected by\nexisting methods. It is insufficient to provide the same \"completely\nsummarized\" answer to all customers, since many customers are more willing to\nsee personalized answers with customized information only for themselves, by\ntaking into consideration their own preferences towards product aspects or\ninformation needs. To tackle this challenge, we propose a novel Personalized\nAnswer GEneration method (PAGE) with multi-perspective preference modeling,\nwhich explores historical user-generated contents to model user preference for\ngenerating personalized answers in PQA. Specifically, we first retrieve\nquestion-related user history as external knowledge to model knowledge-level\nuser preference. Then we leverage Gaussian Softmax distribution model to\ncapture latent aspect-level user preference. Finally, we develop a\npersona-aware pointer network to generate personalized answers in terms of both\ncontent and style by utilizing personal user preference and dynamic user\nvocabulary. Experimental results on real-world E-Commerce QA datasets\ndemonstrate that the proposed method outperforms existing methods by generating\ninformative and customized answers, and show that answer generation in\nE-Commerce can benefit from personalization.", "published": "2021-12-27 07:51:49", "link": "http://arxiv.org/abs/2112.13556v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Polite Emotional Dialogue Acts for Conversational Analysis in Daily\n  Dialog Data", "abstract": "Many socio-linguistic cues are used in the conversational analysis, such as\nemotion, sentiment, and dialogue acts. One of the fundamental social cues is\npoliteness, which linguistically possesses properties useful in conversational\nanalysis. This short article presents some of the brief findings of polite\nemotional dialogue acts, where we can correlate the relational bonds between\nthese socio-linguistics cues. We found that the utterances with emotion classes\nAnger and Disgust are more likely to be impolite while Happiness and Sadness to\nbe polite. Similar phenomenon occurs with dialogue acts, Inform and Commissive\ncontain many polite utterances than Question and Directive. Finally, we will\nconclude on the future work of these findings.", "published": "2021-12-27 08:48:57", "link": "http://arxiv.org/abs/2112.13572v2", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "HeteroQA: Learning towards Question-and-Answering through Multiple\n  Information Sources via Heterogeneous Graph Modeling", "abstract": "Community Question Answering (CQA) is a well-defined task that can be used in\nmany scenarios, such as E-Commerce and online user community for special\ninterests.\n  In these communities, users can post articles, give comment, raise a question\nand answer it.\n  These data form the heterogeneous information sources where each information\nsource have their own special structure and context (comments attached to an\narticle or related question with answers).\n  Most of the CQA methods only incorporate articles or Wikipedia to extract\nknowledge and answer the user's question.\n  However, various types of information sources in the community are not fully\nexplored by these CQA methods and these multiple information sources (MIS) can\nprovide more related knowledge to user's questions.\n  Thus, we propose a question-aware heterogeneous graph transformer to\nincorporate the MIS in the user community to automatically generate the answer.\n  To evaluate our proposed method, we conduct the experiments on two datasets:\n$\\text{MSM}^{\\text{plus}}$ the modified version of benchmark dataset MS-MARCO\nand the AntQA dataset which is the first large-scale CQA dataset with four\ntypes of MIS.\n  Extensive experiments on two datasets show that our model outperforms all the\nbaselines in terms of all the metrics.", "published": "2021-12-27 10:16:43", "link": "http://arxiv.org/abs/2112.13597v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Transformer Uncertainty Estimation with Hierarchical Stochastic\n  Attention", "abstract": "Transformers are state-of-the-art in a wide range of NLP tasks and have also\nbeen applied to many real-world products. Understanding the reliability and\ncertainty of transformer model predictions is crucial for building trustable\nmachine learning applications, e.g., medical diagnosis. Although many recent\ntransformer extensions have been proposed, the study of the uncertainty\nestimation of transformer models is under-explored. In this work, we propose a\nnovel way to enable transformers to have the capability of uncertainty\nestimation and, meanwhile, retain the original predictive performance. This is\nachieved by learning a hierarchical stochastic self-attention that attends to\nvalues and a set of learnable centroids, respectively. Then new attention heads\nare formed with a mixture of sampled centroids using the Gumbel-Softmax trick.\nWe theoretically show that the self-attention approximation by sampling from a\nGumbel distribution is upper bounded. We empirically evaluate our model on two\ntext classification tasks with both in-domain (ID) and out-of-domain (OOD)\ndatasets. The experimental results demonstrate that our approach: (1) achieves\nthe best predictive performance and uncertainty trade-off among compared\nmethods; (2) exhibits very competitive (in most cases, improved) predictive\nperformance on ID datasets; (3) is on par with Monte Carlo dropout and ensemble\nmethods in uncertainty estimation on OOD datasets.", "published": "2021-12-27 16:43:31", "link": "http://arxiv.org/abs/2112.13776v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Contextual Sentence Analysis for the Sentiment Prediction on Financial\n  Data", "abstract": "Newsletters and social networks can reflect the opinion about the market and\nspecific stocks from the perspective of analysts and the general public on\nproducts and/or services provided by a company. Therefore, sentiment analysis\nof these texts can provide useful information to help investors trade in the\nmarket. In this paper, a hierarchical stack of Transformers model is proposed\nto identify the sentiment associated with companies and stocks, by predicting a\nscore (of data type real) in a range between -1 and +1. Specifically, we\nfine-tuned a RoBERTa model to process headlines and microblogs and combined it\nwith additional Transformer layers to process the sentence analysis with\nsentiment dictionaries to improve the sentiment analysis. We evaluated it on\nfinancial data released by SemEval-2017 task 5 and our proposition outperformed\nthe best systems of SemEval-2017 task 5 and strong baselines. Indeed, the\ncombination of contextual sentence analysis with the financial and general\nsentiment dictionaries provided useful information to our model and allowed it\nto generate more reliable sentiment scores.", "published": "2021-12-27 17:12:57", "link": "http://arxiv.org/abs/2112.13790v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluating Contextual Embeddings and their Extraction Layers for\n  Depression Assessment", "abstract": "Recent works have demonstrated ability to assess aspects of mental health\nfrom personal discourse. At the same time, pre-trained contextual word\nembedding models have grown to dominate much of NLP but little is known\nempirically on how to best apply them for mental health assessment. Using\ndegree of depression as a case study, we do an empirical analysis on which\noff-the-shelf language model, individual layers, and combinations of layers\nseem most promising when applied to human-level NLP tasks. Notably, we find\nRoBERTa most effective and, despite the standard in past work suggesting the\nsecond-to-last or concatenation of the last 4 layers, we find layer 19\n(sixth-to last) is at least as good as layer 23 when using 1 layer. Further,\nwhen using multiple layers, distributing them across the second half (i.e.\nLayers 12+), rather than last 4, of the 24 layers yielded the most accurate\nresults.", "published": "2021-12-27 17:20:12", "link": "http://arxiv.org/abs/2112.13795v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "HOPE: A Task-Oriented and Human-Centric Evaluation Framework Using\n  Professional Post-Editing Towards More Effective MT Evaluation", "abstract": "Traditional automatic evaluation metrics for machine translation have been\nwidely criticized by linguists due to their low accuracy, lack of transparency,\nfocus on language mechanics rather than semantics, and low agreement with human\nquality evaluation. Human evaluations in the form of MQM-like scorecards have\nalways been carried out in real industry setting by both clients and\ntranslation service providers (TSPs). However, traditional human translation\nquality evaluations are costly to perform and go into great linguistic detail,\nraise issues as to inter-rater reliability (IRR) and are not designed to\nmeasure quality of worse than premium quality translations. In this work, we\nintroduce HOPE, a task-oriented and human-centric evaluation framework for\nmachine translation output based on professional post-editing annotations. It\ncontains only a limited number of commonly occurring error types, and use a\nscoring model with geometric progression of error penalty points (EPPs)\nreflecting error severity level to each translation unit. The initial\nexperimental work carried out on English-Russian language pair MT outputs on\nmarketing content type of text from highly technical domain reveals that our\nevaluation framework is quite effective in reflecting the MT output quality\nregarding both overall system-level performance and segment-level transparency,\nand it increases the IRR for error type interpretation. The approach has\nseveral key advantages, such as ability to measure and compare less than\nperfect MT output from different systems, ability to indicate human perception\nof quality, immediate estimation of the labor effort required to bring MT\noutput to premium quality, low-cost and faster application, as well as higher\nIRR. Our experimental data is available at\n\\url{https://github.com/lHan87/HOPE}.", "published": "2021-12-27 18:47:43", "link": "http://arxiv.org/abs/2112.13833v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi-modal Attention Network for Stock Movements Prediction", "abstract": "Stock prices move as piece-wise trending fluctuation rather than a purely\nrandom walk. Traditionally, the prediction of future stock movements is based\non the historical trading record. Nowadays, with the development of social\nmedia, many active participants in the market choose to publicize their\nstrategies, which provides a window to glimpse over the whole market's attitude\ntowards future movements by extracting the semantics behind social media.\nHowever, social media contains conflicting information and cannot replace\nhistorical records completely. In this work, we propose a multi-modality\nattention network to reduce conflicts and integrate semantic and numeric\nfeatures to predict future stock movements comprehensively. Specifically, we\nfirst extract semantic information from social media and estimate their\ncredibility based on posters' identity and public reputation. Then we\nincorporate the semantic from online posts and numeric features from historical\nrecords to make the trading strategy. Experimental results show that our\napproach outperforms previous methods by a significant margin in both\nprediction accuracy (61.20\\%) and trading profits (9.13\\%). It demonstrates\nthat our method improves the performance of stock movements prediction and\ninforms future research on multi-modality fusion towards stock prediction.", "published": "2021-12-27 10:03:09", "link": "http://arxiv.org/abs/2112.13593v5", "categories": ["cs.LG", "cs.CL", "q-fin.TR"], "primary_category": "cs.LG"}
{"title": "Hamtajoo: A Persian Plagiarism Checker for Academic Manuscripts", "abstract": "In recent years, due to the high availability of electronic documents through\nthe Web, the plagiarism has become a serious challenge, especially among\nscholars. Various plagiarism detection systems have been developed to prevent\ntext re-use and to confront plagiarism. Although it is almost easy to detect\nduplicate text in academic manuscripts, finding patterns of text re-use that\nhas been semantically changed is of great importance. Another important issue\nis to deal with less resourced languages, which there are low volume of text\nfor training purposes and also low performance in tools for NLP applications.\nIn this paper, we introduce Hamtajoo, a Persian plagiarism detection system for\nacademic manuscripts. Moreover, we describe the overall structure of the system\nalong with the algorithms used in each stage. In order to evaluate the\nperformance of the proposed system, we used a plagiarism detection corpus\ncomply with the PAN standards.", "published": "2021-12-27 15:45:35", "link": "http://arxiv.org/abs/2112.13742v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Bridging the Gap: Using Deep Acoustic Representations to Learn Grounded\n  Language from Percepts and Raw Speech", "abstract": "Learning to understand grounded language, which connects natural language to\npercepts, is a critical research area. Prior work in grounded language\nacquisition has focused primarily on textual inputs. In this work we\ndemonstrate the feasibility of performing grounded language acquisition on\npaired visual percepts and raw speech inputs. This will allow interactions in\nwhich language about novel tasks and environments is learned from end users,\nreducing dependence on textual inputs and potentially mitigating the effects of\ndemographic bias found in widely available speech recognition systems. We\nleverage recent work in self-supervised speech representation models and show\nthat learned representations of speech can make language grounding systems more\ninclusive towards specific groups while maintaining or even increasing general\nperformance.", "published": "2021-12-27 16:12:30", "link": "http://arxiv.org/abs/2112.13758v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.RO"], "primary_category": "cs.CL"}
{"title": "Does CLIP Benefit Visual Question Answering in the Medical Domain as\n  Much as it Does in the General Domain?", "abstract": "Contrastive Language--Image Pre-training (CLIP) has shown remarkable success\nin learning with cross-modal supervision from extensive amounts of image--text\npairs collected online. Thus far, the effectiveness of CLIP has been\ninvestigated primarily in general-domain multimodal problems. This work\nevaluates the effectiveness of CLIP for the task of Medical Visual Question\nAnswering (MedVQA). To this end, we present PubMedCLIP, a fine-tuned version of\nCLIP for the medical domain based on PubMed articles. Our experiments are\nconducted on two MedVQA benchmark datasets and investigate two MedVQA methods,\nMEVF (Mixture of Enhanced Visual Features) and QCR (Question answering via\nConditional Reasoning). For each of these, we assess the merits of visual\nrepresentation learning using PubMedCLIP, the original CLIP, and\nstate-of-the-art MAML (Model-Agnostic Meta-Learning) networks pre-trained only\non visual data. We open source the code for our MedVQA pipeline and\npre-training PubMedCLIP. CLIP and PubMedCLIP achieve improvements in comparison\nto MAML's visual encoder. PubMedCLIP achieves the best results with gains in\nthe overall accuracy of up to 3%. Individual examples illustrate the strengths\nof PubMedCLIP in comparison to the previously widely used MAML networks. Visual\nrepresentation learning with language supervision in PubMedCLIP leads to\nnoticeable improvements for MedVQA. Our experiments reveal distributional\ndifferences in the two MedVQA benchmark datasets that have not been imparted in\nprevious work and cause different back-end visual encoders in PubMedCLIP to\nexhibit different behavior on these datasets. Moreover, we witness fundamental\nperformance differences of VQA in general versus medical domains.", "published": "2021-12-27 21:19:23", "link": "http://arxiv.org/abs/2112.13906v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "DPCCN: Densely-Connected Pyramid Complex Convolutional Network for\n  Robust Speech Separation And Extraction", "abstract": "In recent years, a number of time-domain speech separation methods have been\nproposed. However, most of them are very sensitive to the environments and wide\ndomain coverage tasks. In this paper, from the time-frequency domain\nperspective, we propose a densely-connected pyramid complex convolutional\nnetwork, termed DPCCN, to improve the robustness of speech separation under\ncomplicated conditions. Furthermore, we generalize the DPCCN to target speech\nextraction (TSE) by integrating a new specially designed speaker encoder.\nMoreover, we also investigate the robustness of DPCCN to unsupervised\ncross-domain TSE tasks. A Mixture-Remix approach is proposed to adapt the\ntarget domain acoustic characteristics for fine-tuning the source model. We\nevaluate the proposed methods not only under noisy and reverberant in-domain\ncondition, but also in clean but cross-domain conditions. Results show that for\nboth speech separation and extraction, the DPCCN-based systems achieve\nsignificantly better performance and robustness than the currently dominating\ntime-domain methods, especially for the cross-domain tasks. Particularly, we\nfind that the Mixture-Remix fine-tuning with DPCCN significantly outperforms\nthe TD-SpeakerBeam for unsupervised cross-domain TSE, with around 3.5 dB SISNR\nimprovement on target domain test set, without any source domain performance\ndegradation.", "published": "2021-12-27 05:35:15", "link": "http://arxiv.org/abs/2112.13520v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Task-specific Optimization of Virtual Channel Linear Prediction-based\n  Speech Dereverberation Front-End for Far-Field Speaker Verification", "abstract": "Developing a single-microphone speech denoising or dereverberation front-end\nfor robust automatic speaker verification (ASV) in noisy far-field speaking\nscenarios is challenging. To address this problem, we present a novel front-end\ndesign that involves a recently proposed extension of the weighted prediction\nerror (WPE) speech dereverberation algorithm, the virtual acoustic channel\nexpansion (VACE)-WPE. It is demonstrated experimentally in this study that\nunlike the conventional WPE algorithm, the VACE-WPE can be explicitly trained\nto cancel out both late reverberation and background noise. To build the\nfront-end, the VACE-WPE is first independently (pre)trained to produce \"noisy\"\ndereverberated signals. Subsequently, given a pretrained speaker embedding\nmodel, the VACE-WPE is additionally fine-tuned within a task-specific\noptimization (TSO) framework, causing the speaker embedding extracted from the\nprocessed signal to be similar to that extracted from the \"noise-free\" target\nsignal. Moreover, to extend the application of the proposed front-end to more\ngeneral, unconstrained \"in-the-wild\" ASV scenarios beyond controlled far-field\nconditions, we propose a distortion regularization method for the VACE-WPE\nwithin the TSO framework. The effectiveness of the proposed approach is\nverified on both far-field and in-the-wild ASV benchmarks, demonstrating its\nsuperiority over fully neural front-ends and other TSO methods in various\ncases.", "published": "2021-12-27 08:44:48", "link": "http://arxiv.org/abs/2112.13569v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
