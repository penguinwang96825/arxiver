{"title": "Self-Attention Enhanced Selective Gate with Entity-Aware Embedding for\n  Distantly Supervised Relation Extraction", "abstract": "Distantly supervised relation extraction intrinsically suffers from noisy\nlabels due to the strong assumption of distant supervision. Most prior works\nadopt a selective attention mechanism over sentences in a bag to denoise from\nwrongly labeled data, which however could be incompetent when there is only one\nsentence in a bag. In this paper, we propose a brand-new light-weight neural\nframework to address the distantly supervised relation extraction problem and\nalleviate the defects in previous selective attention framework. Specifically,\nin the proposed framework, 1) we use an entity-aware word embedding method to\nintegrate both relative position information and head/tail entity embeddings,\naiming to highlight the essence of entities for this task; 2) we develop a\nself-attention mechanism to capture the rich contextual dependencies as a\ncomplement for local dependencies captured by piecewise CNN; and 3) instead of\nusing selective attention, we design a pooling-equipped gate, which is based on\nrich contextual representations, as an aggregator to generate bag-level\nrepresentation for final relation classification. Compared to selective\nattention, one major advantage of the proposed gating mechanism is that, it\nperforms stably and promisingly even if only one sentence appears in a bag and\nthus keeps the consistency across all training examples. The experiments on NYT\ndataset demonstrate that our approach achieves a new state-of-the-art\nperformance in terms of both AUC and top-n precision metrics.", "published": "2019-11-27 00:55:12", "link": "http://arxiv.org/abs/1911.11899v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Simultaneous Neural Machine Translation using Connectionist Temporal\n  Classification", "abstract": "Simultaneous machine translation is a variant of machine translation that\nstarts the translation process before the end of an input. This task faces a\ntrade-off between translation accuracy and latency. We have to determine when\nwe start the translation for observed inputs so far, to achieve good practical\nperformance. In this work, we propose a neural machine translation method to\ndetermine this timing in an adaptive manner. The proposed method introduces a\nspecial token '<wait>', which is generated when the translation model chooses\nto read the next input token instead of generating an output token. It also\nintroduces an objective function to handle the ambiguity in wait timings that\ncan be optimized using an algorithm called Connectionist Temporal\nClassification (CTC). The use of CTC enables the optimization to consider all\npossible output sequences including '<wait>' that are equivalent to the\nreference translations and to choose the best one adaptively. We apply the\nproposed method into simultaneous translation from English to Japanese and\ninvestigate its performance and remaining problems.", "published": "2019-11-27 03:38:36", "link": "http://arxiv.org/abs/1911.11933v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Taking a Stance on Fake News: Towards Automatic Disinformation\n  Assessment via Deep Bidirectional Transformer Language Models for Stance\n  Detection", "abstract": "The exponential rise of social media and digital news in the past decade has\nhad the unfortunate consequence of escalating what the United Nations has\ncalled a global topic of concern: the growing prevalence of disinformation.\nGiven the complexity and time-consuming nature of combating disinformation\nthrough human assessment, one is motivated to explore harnessing AI solutions\nto automatically assess news articles for the presence of disinformation. A\nvaluable first step towards automatic identification of disinformation is\nstance detection, where given a claim and a news article, the aim is to predict\nif the article agrees, disagrees, takes no position, or is unrelated to the\nclaim. Existing approaches in literature have largely relied on hand-engineered\nfeatures or shallow learned representations (e.g., word embeddings) to encode\nthe claim-article pairs, which can limit the level of representational\nexpressiveness needed to tackle the high complexity of disinformation\nidentification. In this work, we explore the notion of harnessing large-scale\ndeep bidirectional transformer language models for encoding claim-article pairs\nin an effort to construct state-of-the-art stance detection geared for\nidentifying disinformation. Taking advantage of bidirectional cross-attention\nbetween claim-article pairs via pair encoding with self-attention, we construct\na large-scale language model for stance detection by performing transfer\nlearning on a RoBERTa deep bidirectional transformer language model, and were\nable to achieve state-of-the-art performance (weighted accuracy of 90.01%) on\nthe Fake News Challenge Stage 1 (FNC-I) benchmark. These promising results\nserve as motivation for harnessing such large-scale language models as powerful\nbuilding blocks for creating effective AI solutions to combat disinformation.", "published": "2019-11-27 04:52:53", "link": "http://arxiv.org/abs/1911.11951v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "JEC-QA: A Legal-Domain Question Answering Dataset", "abstract": "We present JEC-QA, the largest question answering dataset in the legal\ndomain, collected from the National Judicial Examination of China. The\nexamination is a comprehensive evaluation of professional skills for legal\npractitioners. College students are required to pass the examination to be\ncertified as a lawyer or a judge. The dataset is challenging for existing\nquestion answering methods, because both retrieving relevant materials and\nanswering questions require the ability of logic reasoning. Due to the high\ndemand of multiple reasoning abilities to answer legal questions, the\nstate-of-the-art models can only achieve about 28% accuracy on JEC-QA, while\nskilled humans and unskilled humans can reach 81% and 64% accuracy\nrespectively, which indicates a huge gap between humans and machines on this\ntask. We will release JEC-QA and our baselines to help improve the reasoning\nability of machine comprehension models. You can access the dataset from\nhttp://jecqa.thunlp.org/.", "published": "2019-11-27 08:13:27", "link": "http://arxiv.org/abs/1911.12011v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero-shot Chinese Discourse Dependency Parsing via Cross-lingual Mapping", "abstract": "Due to the absence of labeled data, discourse parsing still remains\nchallenging in some languages. In this paper, we present a simple and efficient\nmethod to conduct zero-shot Chinese text-level dependency parsing by leveraging\nEnglish discourse labeled data and parsing techniques. We first construct the\nChinese-English mapping from the level of sentence and elementary discourse\nunit (EDU), and then exploit the parsing results of the corresponding English\ntranslations to obtain the discourse trees for the Chinese text. This method\ncan automatically conduct Chinese discourse parsing, with no need of a large\nscale of Chinese labeled data.", "published": "2019-11-27 08:17:45", "link": "http://arxiv.org/abs/1911.12014v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "word2word: A Collection of Bilingual Lexicons for 3,564 Language Pairs", "abstract": "We present word2word, a publicly available dataset and an open-source Python\npackage for cross-lingual word translations extracted from sentence-level\nparallel corpora. Our dataset provides top-k word translations in 3,564\n(directed) language pairs across 62 languages in OpenSubtitles2018 (Lison et\nal., 2018). To obtain this dataset, we use a count-based bilingual lexicon\nextraction model based on the observation that not only source and target words\nbut also source words themselves can be highly correlated. We illustrate that\nthe resulting bilingual lexicons have high coverage and attain competitive\ntranslation quality for several language pairs. We wrap our dataset and model\nin an easy-to-use Python library, which supports downloading and retrieving\ntop-k word translations in any of the supported language pairs as well as\ncomputing top-k word translations for custom parallel corpora.", "published": "2019-11-27 08:37:32", "link": "http://arxiv.org/abs/1911.12019v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sideways Transliteration: How to Transliterate Multicultural Person\n  Names?", "abstract": "In a global setting, texts contain transliterated names from many cultural\norigins. Correct transliteration depends not only on target and source\nlanguages but also, on the source language of the name. We introduce a novel\nmethodology for transliteration of names originating in different languages\nusing only monolingual resources. Our method is based on a step of noisy\ntransliteration and then ranking of the results based on origin specific letter\nmodels. The transliteration table used for noisy generation is learned in an\nunsupervised manner for each possible origin language. We present a solution\nfor gathering monolingual training data used by our method by mining of social\nmedia sites such as Facebook and Wikipedia. We present results in the context\nof transliterating from English to Hebrew and provide an online web service for\ntransliteration from English to Hebrew", "published": "2019-11-27 08:38:57", "link": "http://arxiv.org/abs/1911.12022v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Jejueo Datasets for Machine Translation and Speech Synthesis", "abstract": "Jejueo was classified as critically endangered by UNESCO in 2010. Although\ndiverse efforts to revitalize it have been made, there have been few\ncomputational approaches. Motivated by this, we construct two new Jejueo\ndatasets: Jejueo Interview Transcripts (JIT) and Jejueo Single Speaker Speech\n(JSS). The JIT dataset is a parallel corpus containing 170k+ Jejueo-Korean\nsentences, and the JSS dataset consists of 10k high-quality audio files\nrecorded by a native Jejueo speaker and a transcript file. Subsequently, we\nbuild neural systems of machine translation and speech synthesis using them.\nAll resources are publicly available via our GitHub repository. We hope that\nthese datasets will attract interest of both language and machine learning\ncommunities.", "published": "2019-11-27 10:43:20", "link": "http://arxiv.org/abs/1911.12071v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NorNE: Annotating Named Entities for Norwegian", "abstract": "This paper presents NorNE, a manually annotated corpus of named entities\nwhich extends the annotation of the existing Norwegian Dependency Treebank.\nComprising both of the official standards of written Norwegian (Bokm{\\aa}l and\nNynorsk), the corpus contains around 600,000 tokens and annotates a rich set of\nentity types including persons, organizations, locations, geo-political\nentities, products, and events, in addition to a class corresponding to\nnominals derived from names. We here present details on the annotation effort,\nguidelines, inter-annotator agreement and an experimental analysis of the\ncorpus using a neural sequence labeling architecture.", "published": "2019-11-27 13:30:36", "link": "http://arxiv.org/abs/1911.12146v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive\n  Summarization", "abstract": "This paper introduces the SAMSum Corpus, a new dataset with abstractive\ndialogue summaries. We investigate the challenges it poses for automated\nsummarization by testing several models and comparing their results with those\nobtained on a corpus of news articles. We show that model-generated summaries\nof dialogues achieve higher ROUGE scores than the model-generated summaries of\nnews -- in contrast with human evaluators' judgement. This suggests that a\nchallenging task of abstractive dialogue summarization requires dedicated\nmodels and non-standard quality measures. To our knowledge, our study is the\nfirst attempt to introduce a high-quality chat-dialogues corpus, manually\nannotated with abstractive summarizations, which can be used by the research\ncommunity for further studies.", "published": "2019-11-27 15:54:55", "link": "http://arxiv.org/abs/1911.12237v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do Attention Heads in BERT Track Syntactic Dependencies?", "abstract": "We investigate the extent to which individual attention heads in pretrained\ntransformer language models, such as BERT and RoBERTa, implicitly capture\nsyntactic dependency relations. We employ two methods---taking the maximum\nattention weight and computing the maximum spanning tree---to extract implicit\ndependency relations from the attention weights of each layer/head, and compare\nthem to the ground-truth Universal Dependency (UD) trees. We show that, for\nsome UD relation types, there exist heads that can recover the dependency type\nsignificantly better than baselines on parsed English text, suggesting that\nsome self-attention heads act as a proxy for syntactic structure. We also\nanalyze BERT fine-tuned on two datasets---the syntax-oriented CoLA and the\nsemantics-oriented MNLI---to investigate whether fine-tuning affects the\npatterns of their self-attention, but we do not observe substantial differences\nin the overall dependency relations extracted using our methods. Our results\nsuggest that these models have some specialist attention heads that track\nindividual dependency types, but no generalist head that performs holistic\nparsing significantly better than a trivial baseline, and that analyzing\nattention weights directly may not reveal much of the syntactic knowledge that\nBERT-style models are known to learn.", "published": "2019-11-27 16:09:11", "link": "http://arxiv.org/abs/1911.12246v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SimpleBooks: Long-term dependency book dataset with simplified English\n  vocabulary for word-level language modeling", "abstract": "With language modeling becoming the popular base task for unsupervised\nrepresentation learning in Natural Language Processing, it is important to come\nup with new architectures and techniques for faster and better training of\nlanguage models. However, due to a peculiarity of languages -- the larger the\ndataset, the higher the average number of times a word appears in that dataset\n-- datasets of different sizes have very different properties. Architectures\nperforming well on small datasets might not perform well on larger ones. For\nexample, LSTM models perform well on WikiText-2 but poorly on WikiText-103,\nwhile Transformer models perform well on WikiText-103 but not on WikiText-2.\nFor setups like architectural search, this is a challenge since it is\nprohibitively costly to run a search on the full dataset but it is not\nindicative to experiment on smaller ones. In this paper, we introduce\nSimpleBooks, a small dataset with the average word frequency as high as that of\nmuch larger ones. Created from 1,573 Gutenberg books with the highest ratio of\nword-level book length to vocabulary size, SimpleBooks contains 92M word-level\ntokens, on par with WikiText-103 (103M tokens), but has the vocabulary of 98K,\na third of WikiText-103's. SimpleBooks can be downloaded from\nhttps://dldata-public.s3.us-east-2.amazonaws.com/simplebooks.zip.", "published": "2019-11-27 19:37:26", "link": "http://arxiv.org/abs/1911.12391v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Generation of Headlines for Online Math Questions", "abstract": "Mathematical equations are an important part of dissemination and\ncommunication of scientific information. Students, however, often feel\nchallenged in reading and understanding math content and equations. With the\ndevelopment of the Web, students are posting their math questions online.\nNevertheless, constructing a concise math headline that gives a good\ndescription of the posted detailed math question is nontrivial. In this study,\nwe explore a novel summarization task denoted as geNerating A concise Math\nhEadline from a detailed math question (NAME). Compared to conventional\nsummarization tasks, this task has two extra and essential constraints: 1)\nDetailed math questions consist of text and math equations which require a\nunified framework to jointly model textual and mathematical information; 2)\nUnlike text, math equations contain semantic and structural features, and both\nof them should be captured together. To address these issues, we propose\nMathSum, a novel summarization model which utilizes a pointer mechanism\ncombined with a multi-head attention mechanism for mathematical representation\naugmentation. The pointer mechanism can either copy textual tokens or math\ntokens from source questions in order to generate math headlines. The\nmulti-head attention mechanism is designed to enrich the representation of math\nequations by modeling and integrating both its semantic and structural\nfeatures. For evaluation, we collect and make available two sets of real-world\ndetailed math questions along with human-written math headlines, namely\nEXEQ-300k and OFEQ-10k. Experimental results demonstrate that our model\n(MathSum) significantly outperforms state-of-the-art models for both the\nEXEQ-300k and OFEQ-10k datasets.", "published": "2019-11-27 20:37:26", "link": "http://arxiv.org/abs/1912.00839v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Commonsense in Pre-trained Language Models", "abstract": "Contextualized representations trained over large raw text data have given\nremarkable improvements for NLP tasks including question answering and reading\ncomprehension. There have been works showing that syntactic, semantic and word\nsense knowledge are contained in such representations, which explains why they\nbenefit such tasks. However, relatively little work has been done investigating\ncommonsense knowledge contained in contextualized representations, which is\ncrucial for human question answering and reading comprehension. We study the\ncommonsense ability of GPT, BERT, XLNet, and RoBERTa by testing them on seven\nchallenging benchmarks, finding that language modeling and its variants are\neffective objectives for promoting models' commonsense ability while\nbi-directional context and larger training set are bonuses. We additionally\nfind that current models do poorly on tasks require more necessary inference\nsteps. Finally, we test the robustness of models by making dual test cases,\nwhich are correlated so that the correct prediction of one sample should lead\nto correct prediction of the other. Interestingly, the models show confusion on\nthese test cases, which suggests that they learn commonsense at the surface\nrather than the deep level. We release a test set, named CATs publicly, for\nfuture research.", "published": "2019-11-27 03:22:40", "link": "http://arxiv.org/abs/1911.11931v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DeFINE: DEep Factorized INput Token Embeddings for Neural Sequence\n  Modeling", "abstract": "For sequence models with large vocabularies, a majority of network parameters\nlie in the input and output layers. In this work, we describe a new method,\nDeFINE, for learning deep token representations efficiently. Our architecture\nuses a hierarchical structure with novel skip-connections which allows for the\nuse of low dimensional input and output layers, reducing total parameters and\ntraining time while delivering similar or better performance versus existing\nmethods. DeFINE can be incorporated easily in new or existing sequence models.\nCompared to state-of-the-art methods including adaptive input representations,\nthis technique results in a 6% to 20% drop in perplexity. On WikiText-103,\nDeFINE reduces the total parameters of Transformer-XL by half with minimal\nimpact on performance. On the Penn Treebank, DeFINE improves AWD-LSTM by 4\npoints with a 17% reduction in parameters, achieving comparable performance to\nstate-of-the-art methods with fewer parameters. For machine translation, DeFINE\nimproves the efficiency of the Transformer model by about 1.4 times while\ndelivering similar performance.", "published": "2019-11-27 19:09:41", "link": "http://arxiv.org/abs/1911.12385v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A concrete example of inclusive design: deaf-oriented accessibility", "abstract": "One of the continuing challenges of Human Computer Interaction research is\nthe full inclusion of people with special needs into the digital world. In\nparticular, this crucial category includes people that experiences some kind of\nlimitation in exploiting traditional information communication channels. One\nimmediately thinks about blind people, and several researches aim at addressing\ntheir needs. On the contrary, limitations suffered by deaf people are often\nunderestimated. This often the result of a kind of ignorance or\nmisunderstanding of the real nature of their communication difficulties. This\nchapter aims at both increasing the awareness of deaf problems in the digital\nworld, and at proposing the project of a comprehensive solution for their\nbetter inclusion. As for the former goal, we will provide a bird's-eye\npresentation of history and evolution of understanding of deafness issues, and\nof strategies to address them. As for the latter, we will present the design,\nimplementation and evaluation of the first nucleus of a comprehensive digital\nframework to facilitate the access of deaf people into the digital world.", "published": "2019-11-27 15:01:57", "link": "http://arxiv.org/abs/1911.13207v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Towards improving the e-learning experience for deaf students: e-LUX", "abstract": "Deaf people are more heavily affected by the digital divide than many would\nexpect. Moreover, most accessibility guidelines addressing their needs just\ndeal with captioning and audio-content transcription. However, this approach to\nthe problem does not consider that deaf people have big troubles with vocal\nlanguages, even in their written form. At present, only a few organizations,\nlike W3C, produced guidelines dealing with one of their most distinctive\nexpressions: Sign Language (SL). SL is, in fact, the visual-gestural language\nused by many deaf people to communicate with each other. The present work aims\nat supporting e-learning user experience (e-LUX) for these specific users by\nenhancing the accessibility of content and container services. In particular,\nwe propose preliminary solutions to tailor activities which can be more\nfruitful when performed in one's own \"native\" language, which for most deaf\npeople, especially younger ones, is represented by national SL.", "published": "2019-11-27 14:55:16", "link": "http://arxiv.org/abs/1911.13231v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Recency predicts bursts in the evolution of author citations", "abstract": "The citations process for scientific papers has been studied extensively. But\nwhile the citations accrued by authors are the sum of the citations of their\npapers, translating the dynamics of citation accumulation from the paper to the\nauthor level is not trivial. Here we conduct a systematic study of the\nevolution of author citations, and in particular their bursty dynamics. We find\nempirical evidence of a correlation between the number of citations most\nrecently accrued by an author and the number of citations they receive in the\nfuture. Using a simple model where the probability for an author to receive new\ncitations depends only on the number of citations collected in the previous\n12-24 months, we are able to reproduce both the citation and burst size\ndistributions of authors across multiple decades.", "published": "2019-11-27 02:52:02", "link": "http://arxiv.org/abs/1911.11926v1", "categories": ["cs.DL", "cs.CL", "physics.soc-ph"], "primary_category": "cs.DL"}
{"title": "AIPNet: Generative Adversarial Pre-training of Accent-invariant Networks\n  for End-to-end Speech Recognition", "abstract": "As one of the major sources in speech variability, accents have posed a grand\nchallenge to the robustness of speech recognition systems. In this paper, our\ngoal is to build a unified end-to-end speech recognition system that\ngeneralizes well across accents. For this purpose, we propose a novel\npre-training framework AIPNet based on generative adversarial nets (GAN) for\naccent-invariant representation learning: Accent Invariant Pre-training\nNetworks. We pre-train AIPNet to disentangle accent-invariant and\naccent-specific characteristics from acoustic features through adversarial\ntraining on accented data for which transcriptions are not necessarily\navailable. We further fine-tune AIPNet by connecting the accent-invariant\nmodule with an attention-based encoder-decoder model for multi-accent speech\nrecognition. In the experiments, our approach is compared against four\nbaselines including both accent-dependent and accent-independent models.\nExperimental results on 9 English accents show that the proposed approach\noutperforms all the baselines by 2.3 \\sim 4.5% relative reduction on average\nWER when transcriptions are available in all accents and by 1.6 \\sim 6.1%\nrelative reduction when transcriptions are only available in US accent.", "published": "2019-11-27 03:42:21", "link": "http://arxiv.org/abs/1911.11935v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Label Dependent Deep Variational Paraphrase Generation", "abstract": "Generating paraphrases that are lexically similar but semantically different\nis a challenging task. Paraphrases of this form can be used to augment data\nsets for various NLP tasks such as machine reading comprehension and question\nanswering with non-trivial negative examples. In this article, we propose a\ndeep variational model to generate paraphrases conditioned on a label that\nspecifies whether the paraphrases are semantically related or not. We also\npresent new training recipes and KL regularization techniques that improve the\nperformance of variational paraphrasing models. Our proposed model demonstrates\npromising results in enhancing the generative power of the model by employing\nlabel-dependent generation on paraphrasing datasets.", "published": "2019-11-27 04:54:14", "link": "http://arxiv.org/abs/1911.11952v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Large-Scale Noun Compound Interpretation Using Bootstrapping and the Web\n  as a Corpus", "abstract": "Responding to the need for semantic lexical resources in natural language\nprocessing applications, we examine methods to acquire noun compounds (NCs),\ne.g., \"orange juice\", together with suitable fine-grained semantic\ninterpretations, e.g., \"squeezed from\", which are directly usable as\nparaphrases. We employ bootstrapping and web statistics, and utilize the\nrelationship between NCs and paraphrasing patterns to jointly extract NCs and\nsuch patterns in multiple alternating iterations. In evaluation, we found that\nhaving one compound noun fixed yields both a higher number of semantically\ninterpreted NCs and improved accuracy due to stronger semantic restrictions.", "published": "2019-11-27 11:25:43", "link": "http://arxiv.org/abs/1911.12085v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Findings of the 2016 WMT Shared Task on Cross-lingual Pronoun Prediction", "abstract": "We describe the design, the evaluation setup, and the results of the 2016 WMT\nshared task on cross-lingual pronoun prediction. This is a classification task\nin which participants are asked to provide predictions on what pronoun class\nlabel should replace a placeholder value in the target-language text, provided\nin lemmatised and PoS-tagged form. We provided four subtasks, for the\nEnglish-French and English-German language pairs, in both directions. Eleven\nteams participated in the shared task; nine for the English-French subtask,\nfive for French-English, nine for English-German, and six for German-English.\nMost of the submissions outperformed two strong language-model based baseline\nsystems, with systems using deep recurrent neural networks outperforming those\nusing other architectures for most language pairs.", "published": "2019-11-27 11:45:15", "link": "http://arxiv.org/abs/1911.12091v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Multimodal Attention Networks for Low-Level Vision-and-Language\n  Navigation", "abstract": "Vision-and-Language Navigation (VLN) is a challenging task in which an agent\nneeds to follow a language-specified path to reach a target destination. The\ngoal gets even harder as the actions available to the agent get simpler and\nmove towards low-level, atomic interactions with the environment. This setting\ntakes the name of low-level VLN. In this paper, we strive for the creation of\nan agent able to tackle three key issues: multi-modality, long-term\ndependencies, and adaptability towards different locomotive settings. To that\nend, we devise \"Perceive, Transform, and Act\" (PTA): a fully-attentive VLN\narchitecture that leaves the recurrent approach behind and the first\nTransformer-like architecture incorporating three different modalities -\nnatural language, images, and low-level actions for the agent control. In\nparticular, we adopt an early fusion strategy to merge lingual and visual\ninformation efficiently in our encoder. We then propose to refine the decoding\nphase with a late fusion extension between the agent's history of actions and\nthe perceptual modalities. We experimentally validate our model on two\ndatasets: PTA achieves promising results in low-level VLN on R2R and achieves\ngood performance in the recently proposed R4R benchmark. Our code is publicly\navailable at https://github.com/aimagelab/perceive-transform-and-act.", "published": "2019-11-27 19:00:24", "link": "http://arxiv.org/abs/1911.12377v3", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Learning a faceted customer segmentation for discovering new business\n  opportunities at Intel", "abstract": "For sales and marketing organizations within large enterprises, identifying\nand understanding new markets, customers and partners is a key challenge.\nIntel's Sales and Marketing Group (SMG) faces similar challenges while growing\nin new markets and domains and evolving its existing business. In today's\ncomplex technological and commercial landscape, there is need for intelligent\nautomation supporting a fine-grained understanding of businesses in order to\nhelp SMG sift through millions of companies across many geographies and\nlanguages and identify relevant directions. We present a system developed in\nour company that mines millions of public business web pages, and extracts a\nfaceted customer representation. We focus on two key customer aspects that are\nessential for finding relevant opportunities: industry segments (ranging from\nbroad verticals such as healthcare, to more specific fields such as 'video\nanalytics') and functional roles (e.g., 'manufacturer' or 'retail'). To address\nthe challenge of labeled data collection, we enrich our data with external\ninformation gleaned from Wikipedia, and develop a semi-supervised multi-label,\nmulti-lingual deep learning model that parses customer website texts and\nclassifies them into their respective facets. Our system scans and indexes\ncompanies as part of a large-scale knowledge graph that currently holds tens of\nmillions of connected entities with thousands being fetched, enriched and\nconnected to the graph by the hour in real time, and also supports knowledge\nand insight discovery. In experiments conducted in our company, we are able to\nsignificantly boost the performance of sales personnel in the task of\ndiscovering new customers and commercial partnership opportunities.", "published": "2019-11-27 15:48:26", "link": "http://arxiv.org/abs/1912.00778v1", "categories": ["cs.IR", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.IR"}
{"title": "Automatic prediction of suicidal risk in military couples using\n  multimodal interaction cues from couples conversations", "abstract": "Suicide is a major societal challenge globally, with a wide range of risk\nfactors, from individual health, psychological and behavioral elements to\nsocio-economic aspects. Military personnel, in particular, are at especially\nhigh risk. Crisis resources, while helpful, are often constrained by access to\nclinical visits or therapist availability, especially when needed in a timely\nmanner. There have hence been efforts on identifying whether communication\npatterns between couples at home can provide preliminary information about\npotential suicidal behaviors, prior to intervention. In this work, we\ninvestigate whether acoustic, lexical, behavior and turn-taking cues from\nmilitary couples' conversations can provide meaningful markers of suicidal\nrisk. We test their effectiveness in real-world noisy conditions by extracting\nthese cues through an automatic diarization and speech recognition front-end.\nEvaluation is performed by classifying 3 degrees of suicidal risk: none,\nideation, attempt. Our automatic system performs significantly better than\nchance in all classification scenarios and we find that behavior and\nturn-taking cues are the most informative ones. We also observe that\nconditioning on factors such as speaker gender and topic of discussion tends to\nimprove classification performance.", "published": "2019-11-27 02:54:26", "link": "http://arxiv.org/abs/1911.11927v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "SEEF-ALDR: A Speaker Embedding Enhancement Framework via Adversarial\n  Learning based Disentangled Representation", "abstract": "Speaker verification, as a biometric authentication mechanism, has been\nwidely used due to the pervasiveness of voice control on smart devices.\nHowever, the task of \"in-the-wild\" speaker verification is still challenging,\nconsidering the speech samples may contain lots of identity-unrelated\ninformation, e.g., background noise, reverberation, emotion, etc. Previous\nworks focus on optimizing the model to improve verification accuracy, without\ntaking into account the elimination of the impact from the identity-unrelated\ninformation. To solve the above problem, we propose SEEF-ALDR, a novel Speaker\nEmbedding Enhancement Framework via Adversarial Learning based Disentangled\nRepresentation, to reinforce the performance of existing models on speaker\nverification. The key idea is to retrieve as much speaker identity information\nas possible from the original speech, thus minimizing the impact of\nidentity-unrelated information on the speaker verification task by using\nadversarial learning. Experimental results demonstrate that the proposed\nframework can significantly improve the performance of speaker verification by\n20.3% and 23.8% on average over 13 tested baselines on dataset Voxceleb1 and 8\ntested baselines on dataset Voxceleb2 respectively, without adjusting the\nstructure or hyper-parameters of them. Furthermore, the ablation study was\nconducted to evaluate the contribution of each module in SEEF-ALDR. Finally,\nporting an existing model into the proposed framework is straightforward and\ncost-efficient, with very little effort from the model owners due to the\nmodular design of the framework.", "published": "2019-11-27 19:49:27", "link": "http://arxiv.org/abs/1912.02608v4", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "GLA in MediaEval 2018 Emotional Impact of Movies Task", "abstract": "The visual and audio information from movies can evoke a variety of emotions\nin viewers. Towards a better understanding of viewer impact, we present our\nmethods for the MediaEval 2018 Emotional Impact of Movies Task to predict the\nexpected valence and arousal continuously in movies. This task, using the\nLIRIS-ACCEDE dataset, enables researchers to compare different approaches for\npredicting viewer impact from movies. Our approach leverages image, audio, and\nface based features computed using pre-trained neural networks. These features\nwere computed over time and modeled using a gated recurrent unit (GRU) based\nnetwork followed by a mixture of experts model to compute multiclass\npredictions. We smoothed these predictions using a Butterworth filter for our\nfinal result. Our method enabled us to achieve top performance in three\nevaluation metrics in the MediaEval 2018 task.", "published": "2019-11-27 18:59:55", "link": "http://arxiv.org/abs/1911.12361v1", "categories": ["cs.CV", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.CV"}
{"title": "Music Source Separation in the Waveform Domain", "abstract": "Source separation for music is the task of isolating contributions, or stems,\nfrom different instruments recorded individually and arranged together to form\na song. Such components include voice, bass, drums and any other\naccompaniments.Contrarily to many audio synthesis tasks where the best\nperformances are achieved by models that directly generate the waveform, the\nstate-of-the-art in source separation for music is to compute masks on the\nmagnitude spectrum. In this paper, we compare two waveform domain\narchitectures. We first adapt Conv-Tasnet, initially developed for speech\nsource separation,to the task of music source separation. While Conv-Tasnet\nbeats many existing spectrogram-domain methods, it suffersfrom significant\nartifacts, as shown by human evaluations. We propose instead Demucs, a novel\nwaveform-to-waveform model,with a U-Net structure and bidirectional\nLSTM.Experiments on the MusDB dataset show that, with proper data augmentation,\nDemucs beats allexisting state-of-the-art architectures, including Conv-Tasnet,\nwith 6.3 SDR on average, (and up to 6.8 with 150 extra training songs, even\nsurpassing the IRM oracle for the bass source).Using recent development in\nmodel quantization, Demucs can be compressed down to 120MBwithout any loss of\naccuracy.We also provide human evaluations, showing that Demucs benefit from a\nlarge advantagein terms of the naturalness of the audio. However, it suffers\nfrom some bleeding,especially between the vocals and other source.", "published": "2019-11-27 13:50:45", "link": "http://arxiv.org/abs/1911.13254v2", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "A Dataset for measuring reading levels in India at scale", "abstract": "One out of four children in India are leaving grade eight without basic\nreading skills. Measuring the reading levels in a vast country like India poses\nsignificant hurdles. Recent advances in machine learning opens up the\npossibility of automating this task. However, the datasets of children's speech\nare not only rare but are primarily in English. To solve this assessment\nproblem and advance deep learning research in regional Indian languages, we\npresent the ASER dataset of children in the age group of 6-14. The dataset\nconsists of 5,301 subjects generating 81,330 labeled audio clips in Hindi,\nMarathi and English. These labels represent expert opinions on the child's\nability to read at a specified level. Using this dataset, we built a simple\nASR-based classifier. Early results indicate that we can achieve a prediction\naccuracy of 86% for the English language. Considering the ASER survey spans\nhalf a million subjects, this dataset can grow to those scales.", "published": "2019-11-27 06:06:22", "link": "http://arxiv.org/abs/1912.04381v2", "categories": ["eess.AS", "cs.CY", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
