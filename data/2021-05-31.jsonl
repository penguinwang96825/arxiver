{"title": "More than just Frequency? Demasking Unsupervised Hypernymy Prediction\n  Methods", "abstract": "This paper presents a comparison of unsupervised methods of hypernymy\nprediction (i.e., to predict which word in a pair of words such as fish-cod is\nthe hypernym and which the hyponym). Most importantly, we demonstrate across\ndatasets for English and for German that the predictions of three methods\n(WeedsPrec, invCL, SLQS Row) strongly overlap and are highly correlated with\nfrequency-based predictions. In contrast, the second-order method SLQS shows an\noverall lower accuracy but makes correct predictions where the others go wrong.\nOur study once more confirms the general need to check the frequency bias of a\ncomputational method in order to identify frequency-(un)related effects.", "published": "2021-05-31 18:41:39", "link": "http://arxiv.org/abs/2106.00055v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Model Evaluation Beyond Perplexity", "abstract": "We propose an alternate approach to quantifying how well language models\nlearn natural language: we ask how well they match the statistical tendencies\nof natural language. To answer this question, we analyze whether text generated\nfrom language models exhibits the statistical tendencies present in the\nhuman-generated text on which they were trained. We provide a framework--paired\nwith significance tests--for evaluating the fit of language models to these\ntrends. We find that neural language models appear to learn only a subset of\nthe tendencies considered, but align much more closely with empirical trends\nthan proposed theoretical distributions (when present). Further, the fit to\ndifferent distributions is highly-dependent on both model architecture and\ngeneration strategy. As concrete examples, text generated under the nucleus\nsampling scheme adheres more closely to the type--token relationship of natural\nlanguage than text produced using standard ancestral sampling; text from LSTMs\nreflects the natural language distributions over length, stopwords, and symbols\nsurprisingly well.", "published": "2021-05-31 20:13:44", "link": "http://arxiv.org/abs/2106.00085v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bringing Structure into Summaries: a Faceted Summarization Dataset for\n  Long Scientific Documents", "abstract": "Faceted summarization provides briefings of a document from different\nperspectives. Readers can quickly comprehend the main points of a long document\nwith the help of a structured outline. However, little research has been\nconducted on this subject, partially due to the lack of large-scale faceted\nsummarization datasets. In this study, we present FacetSum, a faceted\nsummarization benchmark built on Emerald journal articles, covering a diverse\nrange of domains. Different from traditional document-summary pairs, FacetSum\nprovides multiple summaries, each targeted at specific sections of a long\ndocument, including the purpose, method, findings, and value. Analyses and\nempirical results on our dataset reveal the importance of bringing structure\ninto summaries. We believe FacetSum will spur further advances in summarization\nresearch and foster the development of NLP systems that can leverage the\nstructured information in both long texts and summaries.", "published": "2021-05-31 22:58:38", "link": "http://arxiv.org/abs/2106.00130v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Training ELECTRA Augmented with Multi-word Selection", "abstract": "Pre-trained text encoders such as BERT and its variants have recently\nachieved state-of-the-art performances on many NLP tasks. While being\neffective, these pre-training methods typically demand massive computation\nresources. To accelerate pre-training, ELECTRA trains a discriminator that\npredicts whether each input token is replaced by a generator. However, this new\ntask, as a binary classification, is less semantically informative. In this\nstudy, we present a new text encoder pre-training method that improves ELECTRA\nbased on multi-task learning. Specifically, we train the discriminator to\nsimultaneously detect replaced tokens and select original tokens from candidate\nsets. We further develop two techniques to effectively combine all pre-training\ntasks: (1) using attention-based networks for task-specific heads, and (2)\nsharing bottom layers of the generator and the discriminator. Extensive\nexperiments on GLUE and SQuAD datasets demonstrate both the effectiveness and\nthe efficiency of our proposed method.", "published": "2021-05-31 23:19:00", "link": "http://arxiv.org/abs/2106.00139v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HiddenCut: Simple Data Augmentation for Natural Language Understanding\n  with Better Generalization", "abstract": "Fine-tuning large pre-trained models with task-specific data has achieved\ngreat success in NLP. However, it has been demonstrated that the majority of\ninformation within the self-attention networks is redundant and not utilized\neffectively during the fine-tuning stage. This leads to inferior results when\ngeneralizing the obtained models to out-of-domain distributions. To this end,\nwe propose a simple yet effective data augmentation technique, HiddenCut, to\nbetter regularize the model and encourage it to learn more generalizable\nfeatures. Specifically, contiguous spans within the hidden space are\ndynamically and strategically dropped during training. Experiments show that\nour HiddenCut method outperforms the state-of-the-art augmentation methods on\nthe GLUE benchmark, and consistently exhibits superior generalization\nperformances on out-of-distribution and challenging counterexamples. We have\npublicly released our code at https://github.com/GT-SALT/HiddenCut.", "published": "2021-05-31 23:57:43", "link": "http://arxiv.org/abs/2106.00149v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A keyword-driven approach to science", "abstract": "To a good extent, words can be understood as corresponding to patterns or\ncategories that appeared in order to represent concepts and structures that are\nparticularly important or useful in a given time and space. Words are\ncharacterized by not being completely general nor specific, in the sense that\nthe same word can be instantiated or related to several different contexts,\ndepending on specific situations. Indeed, the way in which words are\ninstantiated and associated represents a particularly interesting aspect that\ncan substantially help to better understand the context in which they are\nemployed. Scientific words are no exception to that. In the present work, we\napproach the associations between a set of particularly relevant words in the\nsense of being not only frequently used in several areas, but also representing\nconcepts that are currently related to some of the main standing challenges in\nscience. More specifically, the study reported here takes into account the\nwords \"prediction\", \"model\", \"optimization\", \"complex\", \"entropy\", \"random\",\n\"deterministic\", \"pattern\", and \"database\". In order to complement the\nanalysis, we also obtain a network representing the relationship between the\nadopted areas. Many interesting results were found. First and foremost, several\nof the words were observed to have markedly distinct associations in different\nareas. Biology was found to be related to computer science, sharing\nassociations with databases. Furthermore, for most of the cases, the words\n\"complex\", \"model\", and \"prediction\" were observed to have several strong\nassociations.", "published": "2021-05-31 22:06:20", "link": "http://arxiv.org/abs/2106.14610v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Attention Flows are Shapley Value Explanations", "abstract": "Shapley Values, a solution to the credit assignment problem in cooperative\ngame theory, are a popular type of explanation in machine learning, having been\nused to explain the importance of features, embeddings, and even neurons. In\nNLP, however, leave-one-out and attention-based explanations still predominate.\nCan we draw a connection between these different methods? We formally prove\nthat -- save for the degenerate case -- attention weights and leave-one-out\nvalues cannot be Shapley Values. $\\textit{Attention flow}$ is a post-processed\nvariant of attention weights obtained by running the max-flow algorithm on the\nattention graph. Perhaps surprisingly, we prove that attention flows are indeed\nShapley Values, at least at the layerwise level. Given the many desirable\ntheoretical qualities of Shapley Values -- which has driven their adoption\namong the ML community -- we argue that NLP practitioners should, when\npossible, adopt attention flow explanations alongside more traditional ones.", "published": "2021-05-31 00:06:10", "link": "http://arxiv.org/abs/2105.14652v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Interplay Between Fine-tuning and Composition in Transformers", "abstract": "Pre-trained transformer language models have shown remarkable performance on\na variety of NLP tasks. However, recent research has suggested that\nphrase-level representations in these models reflect heavy influences of\nlexical content, but lack evidence of sophisticated, compositional phrase\ninformation. Here we investigate the impact of fine-tuning on the capacity of\ncontextualized embeddings to capture phrase meaning information beyond lexical\ncontent. Specifically, we fine-tune models on an adversarial paraphrase\nclassification task with high lexical overlap, and on a sentiment\nclassification task. After fine-tuning, we analyze phrasal representations in\ncontrolled settings following prior work. We find that fine-tuning largely\nfails to benefit compositionality in these representations, though training on\nsentiment yields a small, localized benefit for certain models. In follow-up\nanalyses, we identify confounding cues in the paraphrase dataset that may\nexplain the lack of composition benefits from that task, and we discuss\npotential factors underlying the localized benefits from sentiment training.", "published": "2021-05-31 01:49:56", "link": "http://arxiv.org/abs/2105.14668v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Emotional Voice Conversion: Theory, Databases and ESD", "abstract": "In this paper, we first provide a review of the state-of-the-art emotional\nvoice conversion research, and the existing emotional speech databases. We then\nmotivate the development of a novel emotional speech database (ESD) that\naddresses the increasing research need. With this paper, the ESD database is\nnow made available to the research community. The ESD database consists of 350\nparallel utterances spoken by 10 native English and 10 native Chinese speakers\nand covers 5 emotion categories (neutral, happy, angry, sad and surprise). More\nthan 29 hours of speech data were recorded in a controlled acoustic\nenvironment. The database is suitable for multi-speaker and cross-lingual\nemotional voice conversion studies. As case studies, we implement several\nstate-of-the-art emotional voice conversion systems on the ESD database. This\npaper provides a reference study on ESD in conjunction with its release.", "published": "2021-05-31 07:48:56", "link": "http://arxiv.org/abs/2105.14762v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LIIR at SemEval-2021 task 6: Detection of Persuasion Techniques In Texts\n  and Images using CLIP features", "abstract": "We describe our approach for SemEval-2021 task 6 on detection of persuasion\ntechniques in multimodal content (memes). Our system combines pretrained\nmultimodal models (CLIP) and chained classifiers. Also, we propose to enrich\nthe data by a data augmentation technique. Our submission achieves a rank of\n8/16 in terms of F1-micro and 9/16 with F1-macro on the test set.", "published": "2021-05-31 08:16:34", "link": "http://arxiv.org/abs/2105.14774v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sketch and Refine: Towards Faithful and Informative Table-to-Text\n  Generation", "abstract": "Table-to-text generation refers to generating a descriptive text from a\nkey-value table. Traditional autoregressive methods, though can generate text\nwith high fluency, suffer from low coverage and poor faithfulness problems. To\nmitigate these problems, we propose a novel Skeleton-based two-stage method\nthat combines both Autoregressive and Non-Autoregressive generations (SANA).\nOur approach includes: (1) skeleton generation with an autoregressive pointer\nnetwork to select key tokens from the source table; (2) edit-based\nnon-autoregressive generation model to produce texts via iterative insertion\nand deletion operations. By integrating hard constraints from the skeleton, the\nnon-autoregressive model improves the generation's coverage over the source\ntable and thus enhances its faithfulness. We conduct automatic and human\nevaluations on both WikiPerson and WikiBio datasets. Experimental results\ndemonstrate that our method outperforms the previous state-of-the-art methods\nin both automatic and human evaluation, especially on coverage and\nfaithfulness. In particular, we achieve PARENT-T recall of 99.47 in WikiPerson,\nimproving over the existing best results by more than 10 points.", "published": "2021-05-31 08:18:13", "link": "http://arxiv.org/abs/2105.14778v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Semantic-based Method for Unsupervised Commonsense Question Answering", "abstract": "Unsupervised commonsense question answering is appealing since it does not\nrely on any labeled task data. Among existing work, a popular solution is to\nuse pre-trained language models to score candidate choices directly conditioned\non the question or context. However, such scores from language models can be\neasily affected by irrelevant factors, such as word frequencies, sentence\nstructures, etc. These distracting factors may not only mislead the model to\nchoose a wrong answer but also make it oversensitive to lexical perturbations\nin candidate answers.\n  In this paper, we present a novel SEmantic-based Question Answering method\n(SEQA) for unsupervised commonsense question answering. Instead of directly\nscoring each answer choice, our method first generates a set of plausible\nanswers with generative models (e.g., GPT-2), and then uses these plausible\nanswers to select the correct choice by considering the semantic similarity\nbetween each plausible answer and each choice. We devise a simple, yet sound\nformalism for this idea and verify its effectiveness and robustness with\nextensive experiments. We evaluate the proposed method on four benchmark\ndatasets, and our method achieves the best results in unsupervised settings.\nMoreover, when attacked by TextFooler with synonym replacement, SEQA\ndemonstrates much less performance drops than baselines, thereby indicating\nstronger robustness.", "published": "2021-05-31 08:21:52", "link": "http://arxiv.org/abs/2105.14781v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploration and Exploitation: Two Ways to Improve Chinese Spelling\n  Correction Models", "abstract": "A sequence-to-sequence learning with neural networks has empirically proven\nto be an effective framework for Chinese Spelling Correction (CSC), which takes\na sentence with some spelling errors as input and outputs the corrected one.\nHowever, CSC models may fail to correct spelling errors covered by the\nconfusion sets, and also will encounter unseen ones. We propose a method, which\ncontinually identifies the weak spots of a model to generate more valuable\ntraining instances, and apply a task-specific pre-training strategy to enhance\nthe model. The generated adversarial examples are gradually added to the\ntraining set. Experimental results show that such an adversarial training\nmethod combined with the pretraining strategy can improve both the\ngeneralization and robustness of multiple CSC models across three different\ndatasets, achieving stateof-the-art performance for CSC task.", "published": "2021-05-31 09:17:33", "link": "http://arxiv.org/abs/2105.14813v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Effective Batching for Recurrent Neural Network Grammars", "abstract": "As a language model that integrates traditional symbolic operations and\nflexible neural representations, recurrent neural network grammars (RNNGs) have\nattracted great attention from both scientific and engineering perspectives.\nHowever, RNNGs are known to be harder to scale due to the difficulty of batched\ntraining. In this paper, we propose effective batching for RNNGs, where every\noperation is computed in parallel with tensors across multiple sentences. Our\nPyTorch implementation effectively employs a GPU and achieves x6 speedup\ncompared to the existing C++ DyNet implementation with model-independent\nauto-batching. Moreover, our batched RNNG also accelerates inference and\nachieves x20-150 speedup for beam search depending on beam sizes. Finally, we\nevaluate syntactic generalization performance of the scaled RNNG against the\nLSTM baseline, based on the large training data of 100M tokens from English\nWikipedia and the broad-coverage targeted syntactic evaluation benchmark. Our\nRNNG implementation is available at https://github.com/aistairc/rnng-pytorch/.", "published": "2021-05-31 09:34:07", "link": "http://arxiv.org/abs/2105.14822v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Greedy-layer Pruning: Speeding up Transformer Models for Natural\n  Language Processing", "abstract": "Fine-tuning transformer models after unsupervised pre-training reaches a very\nhigh performance on many different natural language processing tasks.\nUnfortunately, transformers suffer from long inference times which greatly\nincreases costs in production. One possible solution is to use knowledge\ndistillation, which solves this problem by transferring information from large\nteacher models to smaller student models. Knowledge distillation maintains high\nperformance and reaches high compression rates, nevertheless, the size of the\nstudent model is fixed after pre-training and can not be changed individually\nfor a given downstream task and use-case to reach a desired performance/speedup\nratio. Another solution to reduce the size of models in a much more\nfine-grained and computationally cheaper fashion is to prune layers after the\npre-training. The price to pay is that the performance of layer-wise pruning\nalgorithms is not on par with state-of-the-art knowledge distillation methods.\nIn this paper, Greedy-layer pruning is introduced to (1) outperform current\nstate-of-the-art for layer-wise pruning, (2) close the performance gap when\ncompared to knowledge distillation, while (3) providing a method to adapt the\nmodel size dynamically to reach a desired performance/speedup tradeoff without\nthe need of additional pre-training phases. Our source code is available on\nhttps://github.com/deepopinion/greedy-layer-pruning.", "published": "2021-05-31 09:52:41", "link": "http://arxiv.org/abs/2105.14839v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SemEval-2021 Task 4: Reading Comprehension of Abstract Meaning", "abstract": "This paper introduces the SemEval-2021 shared task 4: Reading Comprehension\nof Abstract Meaning (ReCAM). This shared task is designed to help evaluate the\nability of machines in representing and understanding abstract concepts. Given\na passage and the corresponding question, a participating system is expected to\nchoose the correct answer from five candidates of abstract concepts in a\ncloze-style machine reading comprehension setup. Based on two typical\ndefinitions of abstractness, i.e., the imperceptibility and nonspecificity, our\ntask provides three subtasks to evaluate the participating models.\nSpecifically, Subtask 1 aims to evaluate how well a system can model concepts\nthat cannot be directly perceived in the physical world. Subtask 2 focuses on\nmodels' ability in comprehending nonspecific concepts located high in a\nhypernym hierarchy given the context of a passage. Subtask 3 aims to provide\nsome insights into models' generalizability over the two types of abstractness.\nDuring the SemEval-2021 official evaluation period, we received 23 submissions\nto Subtask 1 and 28 to Subtask 2. The participating teams additionally made 29\nsubmissions to Subtask 3. The leaderboard and competition website can be found\nat https://competitions.codalab.org/competitions/26153. The data and baseline\ncode are available at\nhttps://github.com/boyuanzheng010/SemEval2021-Reading-Comprehension-of-Abstract-Meaning.", "published": "2021-05-31 11:04:17", "link": "http://arxiv.org/abs/2105.14879v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Exploratory Analysis of the Relation Between Offensive Language and\n  Mental Health", "abstract": "In this paper, we analyze the interplay between the use of offensive language\nand mental health. We acquired publicly available datasets created for\noffensive language identification and depression detection and we train\ncomputational models to compare the use of offensive language in social media\nposts written by groups of individuals with and without self-reported\ndepression diagnosis. We also look at samples written by groups of individuals\nwhose posts show signs of depression according to recent related studies. Our\nanalysis indicates that offensive language is more frequently used in the\nsamples written by individuals with self-reported depression as well as\nindividuals showing signs of depression. The results discussed here open new\navenues in research in politeness/offensiveness and mental health.", "published": "2021-05-31 11:25:07", "link": "http://arxiv.org/abs/2105.14888v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GWLAN: General Word-Level AutocompletioN for Computer-Aided Translation", "abstract": "Computer-aided translation (CAT), the use of software to assist a human\ntranslator in the translation process, has been proven to be useful in\nenhancing the productivity of human translators. Autocompletion, which suggests\ntranslation results according to the text pieces provided by human translators,\nis a core function of CAT. There are two limitations in previous research in\nthis line. First, most research works on this topic focus on sentence-level\nautocompletion (i.e., generating the whole translation as a sentence based on\nhuman input), but word-level autocompletion is under-explored so far. Second,\nalmost no public benchmarks are available for the autocompletion task of CAT.\nThis might be among the reasons why research progress in CAT is much slower\ncompared to automatic MT. In this paper, we propose the task of general\nword-level autocompletion (GWLAN) from a real-world CAT scenario, and construct\nthe first public benchmark to facilitate research in this topic. In addition,\nwe propose an effective method for GWLAN and compare it with several strong\nbaselines. Experiments demonstrate that our proposed method can give\nsignificantly more accurate predictions than the baseline methods on our\nbenchmark datasets.", "published": "2021-05-31 12:27:39", "link": "http://arxiv.org/abs/2105.14913v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Lexical Gold Standards Have Effects On The Usefulness Of Text\n  Analysis Tools For Digital Scholarship", "abstract": "This paper describes how the current lexical similarity and analogy gold\nstandards are built to conform to certain ideas about what the models they are\ndesigned to evaluate are used for. Topical relevance has always been the most\nimportant target notion for information access tools and related language\ntechnology technologies, and while this has proven a useful starting point for\nmuch of what information technology is used for, it does not always align well\nwith other uses to which technologies are being put, most notably use cases\nfrom digital scholarship in the humanities or social sciences. This paper\nargues for more systematic formulation of requirements from the digital\nhumanities and social sciences and more explicit description of the assumptions\nunderlying model design.", "published": "2021-05-31 12:40:10", "link": "http://arxiv.org/abs/2105.14921v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Bi-Lexicalized PCFG Induction", "abstract": "Neural lexicalized PCFGs (L-PCFGs) have been shown effective in grammar\ninduction. However, to reduce computational complexity, they make a strong\nindependence assumption on the generation of the child word and thus bilexical\ndependencies are ignored. In this paper, we propose an approach to parameterize\nL-PCFGs without making implausible independence assumptions. Our approach\ndirectly models bilexical dependencies and meanwhile reduces both learning and\nrepresentation complexities of L-PCFGs. Experimental results on the English WSJ\ndataset confirm the effectiveness of our approach in improving both running\nspeed and unsupervised parsing performance.", "published": "2021-05-31 15:00:03", "link": "http://arxiv.org/abs/2105.15021v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Factorising Meaning and Form for Intent-Preserving Paraphrasing", "abstract": "We propose a method for generating paraphrases of English questions that\nretain the original intent but use a different surface form. Our model combines\na careful choice of training objective with a principled information\nbottleneck, to induce a latent encoding space that disentangles meaning and\nform. We train an encoder-decoder model to reconstruct a question from a\nparaphrase with the same meaning and an exemplar with the same surface form,\nleading to separated encoding spaces. We use a Vector-Quantized Variational\nAutoencoder to represent the surface form as a set of discrete latent\nvariables, allowing us to use a classifier to select a different surface form\nat test time. Crucially, our method does not require access to an external\nsource of target exemplars. Extensive experiments and a human evaluation show\nthat we are able to generate paraphrases with a better tradeoff between\nsemantic preservation and syntactic novelty compared to previous methods.", "published": "2021-05-31 15:37:38", "link": "http://arxiv.org/abs/2105.15053v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adapting High-resource NMT Models to Translate Low-resource Related\n  Languages without Parallel Data", "abstract": "The scarcity of parallel data is a major obstacle for training high-quality\nmachine translation systems for low-resource languages. Fortunately, some\nlow-resource languages are linguistically related or similar to high-resource\nlanguages; these related languages may share many lexical or syntactic\nstructures. In this work, we exploit this linguistic overlap to facilitate\ntranslating to and from a low-resource language with only monolingual data, in\naddition to any parallel data in the related high-resource language. Our\nmethod, NMT-Adapt, combines denoising autoencoding, back-translation and\nadversarial objectives to utilize monolingual data for low-resource adaptation.\nWe experiment on 7 languages from three different language families and show\nthat our technique significantly improves translation into low-resource\nlanguage compared to other translation baselines.", "published": "2021-05-31 16:01:18", "link": "http://arxiv.org/abs/2105.15071v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SA2SL: From Aspect-Based Sentiment Analysis to Social Listening System\n  for Business Intelligence", "abstract": "In this paper, we present a process of building a social listening system\nbased on aspect-based sentiment analysis in Vietnamese from creating a dataset\nto building a real application. Firstly, we create UIT-ViSFD, a Vietnamese\nSmartphone Feedback Dataset as a new benchmark corpus built based on a strict\nannotation schemes for evaluating aspect-based sentiment analysis, consisting\nof 11,122 human-annotated comments for mobile e-commerce, which is freely\navailable for research purposes. We also present a proposed approach based on\nthe Bi-LSTM architecture with the fastText word embeddings for the Vietnamese\naspect based sentiment task. Our experiments show that our approach achieves\nthe best performances with the F1-score of 84.48% for the aspect task and\n63.06% for the sentiment task, which performs several conventional machine\nlearning and deep learning systems. Last but not least, we build SA2SL, a\nsocial listening system based on the best performance model on our dataset,\nwhich will inspire more social listening systems in future.", "published": "2021-05-31 16:09:26", "link": "http://arxiv.org/abs/2105.15079v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Noise: Mitigating the Impact of Fine-grained Semantic Divergences\n  on Neural Machine Translation", "abstract": "While it has been shown that Neural Machine Translation (NMT) is highly\nsensitive to noisy parallel training samples, prior work treats all types of\nmismatches between source and target as noise. As a result, it remains unclear\nhow samples that are mostly equivalent but contain a small number of\nsemantically divergent tokens impact NMT training. To close this gap, we\nanalyze the impact of different types of fine-grained semantic divergences on\nTransformer models. We show that models trained on synthetic divergences output\ndegenerated text more frequently and are less confident in their predictions.\nBased on these findings, we introduce a divergent-aware NMT framework that uses\nfactors to help NMT recover from the degradation caused by naturally occurring\ndivergences, improving both translation quality and model calibration on EN-FR\ntasks.", "published": "2021-05-31 16:15:35", "link": "http://arxiv.org/abs/2105.15087v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning from Perturbations: Diverse and Informative Dialogue Generation\n  with Inverse Adversarial Training", "abstract": "In this paper, we propose Inverse Adversarial Training (IAT) algorithm for\ntraining neural dialogue systems to avoid generic responses and model dialogue\nhistory better. In contrast to standard adversarial training algorithms, IAT\nencourages the model to be sensitive to the perturbation in the dialogue\nhistory and therefore learning from perturbations. By giving higher rewards for\nresponses whose output probability reduces more significantly when dialogue\nhistory is perturbed, the model is encouraged to generate more diverse and\nconsistent responses. By penalizing the model when generating the same response\ngiven perturbed dialogue history, the model is forced to better capture\ndialogue history and generate more informative responses. Experimental results\non two benchmark datasets show that our approach can better model dialogue\nhistory and generate more diverse and consistent responses. In addition, we\npoint out a problem of the widely used maximum mutual information (MMI) based\nmethods for improving the diversity of dialogue response generation models and\ndemonstrate it empirically.", "published": "2021-05-31 17:28:37", "link": "http://arxiv.org/abs/2105.15171v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How transfer learning impacts linguistic knowledge in deep NLP models?", "abstract": "Transfer learning from pre-trained neural language models towards downstream\ntasks has been a predominant theme in NLP recently. Several researchers have\nshown that deep NLP models learn non-trivial amount of linguistic knowledge,\ncaptured at different layers of the model. We investigate how fine-tuning\ntowards downstream NLP tasks impacts the learned linguistic knowledge. We carry\nout a study across popular pre-trained models BERT, RoBERTa and XLNet using\nlayer and neuron-level diagnostic classifiers. We found that for some GLUE\ntasks, the network relies on the core linguistic information and preserve it\ndeeper in the network, while for others it forgets. Linguistic information is\ndistributed in the pre-trained language models but becomes localized to the\nlower layers post fine-tuning, reserving higher layers for the task specific\nknowledge. The pattern varies across architectures, with BERT retaining\nlinguistic information relatively deeper in the network compared to RoBERTa and\nXLNet, where it is predominantly delegated to the lower layers.", "published": "2021-05-31 17:43:57", "link": "http://arxiv.org/abs/2105.15179v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text Summarization with Latent Queries", "abstract": "The availability of large-scale datasets has driven the development of neural\nmodels that create summaries from single documents, for generic purposes. When\nusing a summarization system, users often have specific intents with various\nlanguage realizations, which, depending on the information need, can range from\na single keyword to a long narrative composed of multiple questions. Existing\nsummarization systems, however, often either fail to support or act robustly on\nthis query focused summarization task. We introduce LaQSum, the first unified\ntext summarization system that learns Latent Queries from documents for\nabstractive summarization with any existing query forms. Under a deep\ngenerative framework, our system jointly optimizes a latent query model and a\nconditional language model, allowing users to plug-and-play queries of any type\nat test time. Despite learning from only generic summarization data and\nrequiring no further optimization for downstream summarization tasks, our\nsystem robustly outperforms strong comparison systems across summarization\nbenchmarks with different query types, document settings, and target domains.", "published": "2021-05-31 21:14:58", "link": "http://arxiv.org/abs/2106.00104v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Corpus-Based Paraphrase Detection Experiments and Review", "abstract": "Paraphrase detection is important for a number of applications, including\nplagiarism detection, authorship attribution, question answering, text\nsummarization, text mining in general, etc. In this paper, we give a\nperformance overview of various types of corpus-based models, especially deep\nlearning (DL) models, with the task of paraphrase detection. We report the\nresults of eight models (LSI, TF-IDF, Word2Vec, Doc2Vec, GloVe, FastText, ELMO,\nand USE) evaluated on three different public available corpora: Microsoft\nResearch Paraphrase Corpus, Clough and Stevenson and Webis Crowd Paraphrase\nCorpus 2011. Through a great number of experiments, we decided on the most\nappropriate approaches for text pre-processing: hyper-parameters, sub-model\nselection-where they exist (e.g., Skipgram vs. CBOW), distance measures, and\nsemantic similarity/paraphrase detection threshold. Our findings and those of\nother researchers who have used deep learning models show that DL models are\nvery competitive with traditional state-of-the-art approaches and have\npotential that should be further developed.", "published": "2021-05-31 23:29:24", "link": "http://arxiv.org/abs/2106.00145v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Memory-Efficient Differentiable Transformer Architecture Search", "abstract": "Differentiable architecture search (DARTS) is successfully applied in many\nvision tasks. However, directly using DARTS for Transformers is\nmemory-intensive, which renders the search process infeasible. To this end, we\npropose a multi-split reversible network and combine it with DARTS.\nSpecifically, we devise a backpropagation-with-reconstruction algorithm so that\nwe only need to store the last layer's outputs. By relieving the memory burden\nfor DARTS, it allows us to search with larger hidden size and more candidate\noperations. We evaluate the searched architecture on three sequence-to-sequence\ndatasets, i.e., WMT'14 English-German, WMT'14 English-French, and WMT'14\nEnglish-Czech. Experimental results show that our network consistently\noutperforms standard Transformers across the tasks. Moreover, our method\ncompares favorably with big-size Evolved Transformers, reducing search\ncomputation by an order of magnitude.", "published": "2021-05-31 01:52:36", "link": "http://arxiv.org/abs/2105.14669v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Zero-shot Fact Verification by Claim Generation", "abstract": "Neural models for automated fact verification have achieved promising results\nthanks to the availability of large, human-annotated datasets. However, for\neach new domain that requires fact verification, creating a dataset by manually\nwriting claims and linking them to their supporting evidence is expensive. We\ndevelop QACG, a framework for training a robust fact verification model by\nusing automatically generated claims that can be supported, refuted, or\nunverifiable from evidence from Wikipedia. QACG generates question-answer pairs\nfrom the evidence and then converts them into different types of claims.\nExperiments on the FEVER dataset show that our QACG framework significantly\nreduces the demand for human-annotated training data. In a zero-shot scenario,\nQACG improves a RoBERTa model's F1 from 50% to 77%, equivalent in performance\nto 2K+ manually-curated examples. Our QACG code is publicly available.", "published": "2021-05-31 03:13:52", "link": "http://arxiv.org/abs/2105.14682v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Fully Hyperbolic Neural Networks", "abstract": "Hyperbolic neural networks have shown great potential for modeling complex\ndata. However, existing hyperbolic networks are not completely hyperbolic, as\nthey encode features in a hyperbolic space yet formalize most of their\noperations in the tangent space (a Euclidean subspace) at the origin of the\nhyperbolic space. This hybrid method greatly limits the modeling ability of\nnetworks. In this paper, we propose a fully hyperbolic framework to build\nhyperbolic networks based on the Lorentz model by adapting the Lorentz\ntransformations (including boost and rotation) to formalize essential\noperations of neural networks. Moreover, we also prove that linear\ntransformation in tangent spaces used by existing hyperbolic networks is a\nrelaxation of the Lorentz rotation and does not include the boost, implicitly\nlimiting the capabilities of existing hyperbolic networks. The experimental\nresults on four NLP tasks show that our method has better performance for\nbuilding both shallow and deep networks. Our code will be released to\nfacilitate follow-up research.", "published": "2021-05-31 03:36:49", "link": "http://arxiv.org/abs/2105.14686v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "G-Transformer for Document-level Machine Translation", "abstract": "Document-level MT models are still far from satisfactory. Existing work\nextend translation unit from single sentence to multiple sentences. However,\nstudy shows that when we further enlarge the translation unit to a whole\ndocument, supervised training of Transformer can fail. In this paper, we find\nsuch failure is not caused by overfitting, but by sticking around local minima\nduring training. Our analysis shows that the increased complexity of\ntarget-to-source attention is a reason for the failure. As a solution, we\npropose G-Transformer, introducing locality assumption as an inductive bias\ninto Transformer, reducing the hypothesis space of the attention from target to\nsource. Experiments show that G-Transformer converges faster and more stably\nthan Transformer, achieving new state-of-the-art BLEU scores for both\nnon-pretraining and pre-training settings on three benchmark datasets.", "published": "2021-05-31 07:47:10", "link": "http://arxiv.org/abs/2105.14761v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Transfer Learning for Sequence Generation: from Single-source to\n  Multi-source", "abstract": "Multi-source sequence generation (MSG) is an important kind of sequence\ngeneration tasks that takes multiple sources, including automatic post-editing,\nmulti-source translation, multi-document summarization, etc. As MSG tasks\nsuffer from the data scarcity problem and recent pretrained models have been\nproven to be effective for low-resource downstream tasks, transferring\npretrained sequence-to-sequence models to MSG tasks is essential. Although\ndirectly finetuning pretrained models on MSG tasks and concatenating multiple\nsources into a single long sequence is regarded as a simple method to transfer\npretrained models to MSG tasks, we conjecture that the direct finetuning method\nleads to catastrophic forgetting and solely relying on pretrained\nself-attention layers to capture cross-source information is not sufficient.\nTherefore, we propose a two-stage finetuning method to alleviate the\npretrain-finetune discrepancy and introduce a novel MSG model with a fine\nencoder to learn better representations in MSG tasks. Experiments show that our\napproach achieves new state-of-the-art results on the WMT17 APE task and\nmulti-source translation task using the WMT14 test set. When adapted to\ndocument-level translation, our framework outperforms strong baselines\nsignificantly.", "published": "2021-05-31 09:12:38", "link": "http://arxiv.org/abs/2105.14809v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Cascaded Head-colliding Attention", "abstract": "Transformers have advanced the field of natural language processing (NLP) on\na variety of important tasks. At the cornerstone of the Transformer\narchitecture is the multi-head attention (MHA) mechanism which models pairwise\ninteractions between the elements of the sequence. Despite its massive success,\nthe current framework ignores interactions among different heads, leading to\nthe problem that many of the heads are redundant in practice, which greatly\nwastes the capacity of the model. To improve parameter efficiency, we\nre-formulate the MHA as a latent variable model from a probabilistic\nperspective. We present cascaded head-colliding attention (CODA) which\nexplicitly models the interactions between attention heads through a\nhierarchical variational distribution. We conduct extensive experiments and\ndemonstrate that CODA outperforms the transformer baseline, by $0.6$ perplexity\non \\texttt{Wikitext-103} in language modeling, and by $0.6$ BLEU on\n\\texttt{WMT14 EN-DE} in machine translation, due to its improvements on the\nparameter efficiency.\\footnote{Our implementation is publicly available at\n\\url{https://github.com/LZhengisme/CODA}.}", "published": "2021-05-31 10:06:42", "link": "http://arxiv.org/abs/2105.14850v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Verdi: Quality Estimation and Error Detection for Bilingual Corpora", "abstract": "Translation Quality Estimation is critical to reducing post-editing efforts\nin machine translation and to cross-lingual corpus cleaning. As a research\nproblem, quality estimation (QE) aims to directly estimate the quality of\ntranslation in a given pair of source and target sentences, and highlight the\nwords that need corrections, without referencing to golden translations. In\nthis paper, we propose Verdi, a novel framework for word-level and\nsentence-level post-editing effort estimation for bilingual corpora. Verdi\nadopts two word predictors to enable diverse features to be extracted from a\npair of sentences for subsequent quality estimation, including a\ntransformer-based neural machine translation (NMT) model and a pre-trained\ncross-lingual language model (XLM). We exploit the symmetric nature of\nbilingual corpora and apply model-level dual learning in the NMT predictor,\nwhich handles a primal task and a dual task simultaneously with weight sharing,\nleading to stronger context prediction ability than single-direction NMT\nmodels. By taking advantage of the dual learning scheme, we further design a\nnovel feature to directly encode the translated target information without\nrelying on the source context. Extensive experiments conducted on WMT20 QE\ntasks demonstrate that our method beats the winner of the competition and\noutperforms other baseline methods by a great margin. We further use the\nsentence-level scores provided by Verdi to clean a parallel corpus and observe\nbenefits on both model performance and training efficiency.", "published": "2021-05-31 11:04:13", "link": "http://arxiv.org/abs/2105.14878v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Multilingual Modeling Method for Span-Extraction Reading Comprehension", "abstract": "Span-extraction reading comprehension models have made tremendous advances\nenabled by the availability of large-scale, high-quality training datasets.\nDespite such rapid progress and widespread application, extractive reading\ncomprehension datasets in languages other than English remain scarce, and\ncreating such a sufficient amount of training data for each language is costly\nand even impossible. An alternative to creating large-scale high-quality\nmonolingual span-extraction training datasets is to develop multilingual\nmodeling approaches and systems which can transfer to the target language\nwithout requiring training data in that language. In this paper, in order to\nsolve the scarce availability of extractive reading comprehension training data\nin the target language, we propose a multilingual extractive reading\ncomprehension approach called XLRC by simultaneously modeling the existing\nextractive reading comprehension training data in a multilingual environment\nusing self-adaptive attention and multilingual attention. Specifically, we\nfirstly construct multilingual parallel corpora by translating the existing\nextractive reading comprehension datasets (i.e., CMRC 2018) from the target\nlanguage (i.e., Chinese) into different language families (i.e., English).\nSecondly, to enhance the final target representation, we adopt self-adaptive\nattention (SAA) to combine self-attention and inter-attention to extract the\nsemantic relations from each pair of the target and source languages.\nFurthermore, we propose multilingual attention (MLA) to learn the rich\nknowledge from various language families. Experimental results show that our\nmodel outperforms the state-of-the-art baseline (i.e., RoBERTa_Large) on the\nCMRC 2018 task, which demonstrate the effectiveness of our proposed\nmulti-lingual modeling approach and show the potentials in multilingual NLP\ntasks.", "published": "2021-05-31 11:05:30", "link": "http://arxiv.org/abs/2105.14880v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Document-level Event Extraction via Heterogeneous Graph-based\n  Interaction Model with a Tracker", "abstract": "Document-level event extraction aims to recognize event information from a\nwhole piece of article. Existing methods are not effective due to two\nchallenges of this task: a) the target event arguments are scattered across\nsentences; b) the correlation among events in a document is non-trivial to\nmodel. In this paper, we propose Heterogeneous Graph-based Interaction Model\nwith a Tracker (GIT) to solve the aforementioned two challenges. For the first\nchallenge, GIT constructs a heterogeneous graph interaction network to capture\nglobal interactions among different sentences and entity mentions. For the\nsecond, GIT introduces a Tracker module to track the extracted events and hence\ncapture the interdependency among the events. Experiments on a large-scale\ndataset (Zheng et al., 2019) show GIT outperforms the previous methods by 2.8\nF1. Further analysis reveals GIT is effective in extracting multiple correlated\nevents and event arguments that scatter across the document. Our code is\navailable at https://github.com/RunxinXu/GIT.", "published": "2021-05-31 12:45:03", "link": "http://arxiv.org/abs/2105.14924v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DiaKG: an Annotated Diabetes Dataset for Medical Knowledge Graph\n  Construction", "abstract": "Knowledge Graph has been proven effective in modeling structured information\nand conceptual knowledge, especially in the medical domain. However, the lack\nof high-quality annotated corpora remains a crucial problem for advancing the\nresearch and applications on this task. In order to accelerate the research for\ndomain-specific knowledge graphs in the medical domain, we introduce DiaKG, a\nhigh-quality Chinese dataset for Diabetes knowledge graph, which contains\n22,050 entities and 6,890 relations in total. We implement recent typical\nmethods for Named Entity Recognition and Relation Extraction as a benchmark to\nevaluate the proposed dataset thoroughly. Empirical results show that the DiaKG\nis challenging for most existing methods and further analysis is conducted to\ndiscuss future research direction for improvements. We hope the release of this\ndataset can assist the construction of diabetes knowledge graphs and facilitate\nAI-based applications.", "published": "2021-05-31 15:12:49", "link": "http://arxiv.org/abs/2105.15033v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Telling Stories through Multi-User Dialogue by Modeling Character\n  Relations", "abstract": "This paper explores character-driven story continuation, in which the story\nemerges through characters' first- and second-person narration as well as\ndialogue -- requiring models to select language that is consistent with a\ncharacter's persona and their relationships with other characters while\nfollowing and advancing the story. We hypothesize that a multi-task model that\ntrains on character dialogue plus character relationship information improves\ntransformer-based story continuation. To this end, we extend the Critical Role\nDungeons and Dragons Dataset (Rameshkumar and Bailey, 2020) -- consisting of\ndialogue transcripts of people collaboratively telling a story while playing\nthe role-playing game Dungeons and Dragons -- with automatically extracted\nrelationships between each pair of interacting characters as well as their\npersonas. A series of ablations lend evidence to our hypothesis, showing that\nour multi-task model using character relationships improves story continuation\naccuracy over strong baselines.", "published": "2021-05-31 15:39:41", "link": "http://arxiv.org/abs/2105.15054v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Picking Pearl From Seabed: Extracting Artefacts from Noisy Issue\n  Triaging Collaborative Conversations for Hybrid Cloud Services", "abstract": "Site Reliability Engineers (SREs) play a key role in issue identification and\nresolution. After an issue is reported, SREs come together in a virtual room\n(collaboration platform) to triage the issue. While doing so, they leave behind\na wealth of information which can be used later for triaging similar issues.\nHowever, usability of the conversations offer challenges due to them being i)\nnoisy and ii) unlabelled. This paper presents a novel approach for issue\nartefact extraction from the noisy conversations with minimal labelled data. We\npropose a combination of unsupervised and supervised model with minimum human\nintervention that leverages domain knowledge to predict artefacts for a small\namount of conversation data and use that for fine-tuning an already pretrained\nlanguage model for artefact prediction on a large amount of conversation data.\nExperimental results on our dataset show that the proposed ensemble of\nunsupervised and supervised model is better than using either one of them\nindividually.", "published": "2021-05-31 15:51:44", "link": "http://arxiv.org/abs/2105.15065v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "M6-T: Exploring Sparse Expert Models and Beyond", "abstract": "Mixture-of-Experts (MoE) models can achieve promising results with outrageous\nlarge amount of parameters but constant computation cost, and thus it has\nbecome a trend in model scaling. Still it is a mystery how MoE layers bring\nquality gains by leveraging the parameters with sparse activation. In this\nwork, we investigate several key factors in sparse expert models. We observe\nthat load imbalance may not be a significant problem affecting model quality,\ncontrary to the perspectives of recent studies, while the number of sparsely\nactivated experts $k$ and expert capacity $C$ in top-$k$ routing can\nsignificantly make a difference in this context. Furthermore, we take a step\nforward to propose a simple method called expert prototyping that splits\nexperts into different prototypes and applies $k$ top-$1$ routing. This\nstrategy improves the model quality but maintains constant computational costs,\nand our further exploration on extremely large-scale models reflects that it is\nmore effective in training larger models. We push the model scale to over $1$\ntrillion parameters and implement it on solely $480$ NVIDIA V100-32GB GPUs, in\ncomparison with the recent SOTAs on $2048$ TPU cores. The proposed giant model\nachieves substantial speedup in convergence over the same-size baseline.", "published": "2021-05-31 16:12:44", "link": "http://arxiv.org/abs/2105.15082v5", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Reinforced Generative Adversarial Network for Abstractive Text\n  Summarization", "abstract": "Sequence-to-sequence models provide a viable new approach to generative\nsummarization, allowing models that are no longer limited to simply selecting\nand recombining sentences from the original text. However, these models have\nthree drawbacks: their grasp of the details of the original text is often\ninaccurate, and the text generated by such models often has repetitions, while\nit is difficult to handle words that are beyond the word list. In this paper,\nwe propose a new architecture that combines reinforcement learning and\nadversarial generative networks to enhance the sequence-to-sequence attention\nmodel. First, we use a hybrid pointer-generator network that copies words\ndirectly from the source text, contributing to accurate reproduction of\ninformation without sacrificing the ability of generators to generate new\nwords. Second, we use both intra-temporal and intra-decoder attention to\npenalize summarized content and thus discourage repetition. We apply our model\nto our own proposed COVID-19 paper title summarization task and achieve close\napproximations to the current model on ROUEG, while bringing better\nreadability.", "published": "2021-05-31 17:34:47", "link": "http://arxiv.org/abs/2105.15176v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "StarGAN-ZSVC: Towards Zero-Shot Voice Conversion in Low-Resource\n  Contexts", "abstract": "Voice conversion is the task of converting a spoken utterance from a source\nspeaker so that it appears to be said by a different target speaker while\nretaining the linguistic content of the utterance. Recent advances have led to\nmajor improvements in the quality of voice conversion systems. However, to be\nuseful in a wider range of contexts, voice conversion systems would need to be\n(i) trainable without access to parallel data, (ii) work in a zero-shot setting\nwhere both the source and target speakers are unseen during training, and (iii)\nrun in real time or faster. Recent techniques fulfil one or two of these\nrequirements, but not all three. This paper extends recent voice conversion\nmodels based on generative adversarial networks (GANs), to satisfy all three of\nthese conditions. We specifically extend the recent StarGAN-VC model by\nconditioning it on a speaker embedding (from a potentially unseen speaker).\nThis allows the model to be used in a zero-shot setting, and we therefore call\nit StarGAN-ZSVC. We compare StarGAN-ZSVC against other voice conversion\ntechniques in a low-resource setting using a small 9-minute training set.\nCompared to AutoVC -- another recent neural zero-shot approach -- we observe\nthat StarGAN-ZSVC gives small improvements in the zero-shot setting, showing\nthat real-time zero-shot voice conversion is possible even for a model trained\non very little data. Further work is required to see whether scaling up\nStarGAN-ZSVC will also improve zero-shot voice conversion quality in\nhigh-resource contexts.", "published": "2021-05-31 18:21:28", "link": "http://arxiv.org/abs/2106.00043v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Low-Resource Spoken Language Identification Using Self-Attentive Pooling\n  and Deep 1D Time-Channel Separable Convolutions", "abstract": "This memo describes NTR/TSU winning submission for Low Resource ASR challenge\nat Dialog2021 conference, language identification track.\n  Spoken Language Identification (LID) is an important step in a multilingual\nAutomated Speech Recognition (ASR) system pipeline. Traditionally, the ASR task\nrequires large volumes of labeled data that are unattainable for most of the\nworld's languages, including most of the languages of Russia. In this memo, we\nshow that a convolutional neural network with a Self-Attentive Pooling layer\nshows promising results in low-resource setting for the language identification\ntask and set up a SOTA for the Low Resource ASR challenge dataset.\n  Additionally, we compare the structure of confusion matrices for this and\nsignificantly more diverse VoxForge dataset and state and substantiate the\nhypothesis that whenever the dataset is diverse enough so that the other\nclassification factors, like gender, age etc. are well-averaged, the confusion\nmatrix for LID system bears the language similarity measure.", "published": "2021-05-31 18:35:27", "link": "http://arxiv.org/abs/2106.00052v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "An Exploratory Analysis of Multilingual Word-Level Quality Estimation\n  with Cross-Lingual Transformers", "abstract": "Most studies on word-level Quality Estimation (QE) of machine translation\nfocus on language-specific models. The obvious disadvantages of these\napproaches are the need for labelled data for each language pair and the high\ncost required to maintain several language-specific models. To overcome these\nproblems, we explore different approaches to multilingual, word-level QE. We\nshow that these QE models perform on par with the current language-specific\nmodels. In the cases of zero-shot and few-shot QE, we demonstrate that it is\npossible to accurately predict word-level quality for any given new language\npair from models trained on other language pairs. Our findings suggest that the\nword-level QE models based on powerful pre-trained transformers that we propose\nin this paper generalise well across languages, making them more useful in\nreal-world scenarios.", "published": "2021-05-31 23:21:10", "link": "http://arxiv.org/abs/2106.00143v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Parkinsonian Chinese Speech Analysis towards Automatic Classification of\n  Parkinson's Disease", "abstract": "Speech disorders often occur at the early stage of Parkinson's disease (PD).\nThe speech impairments could be indicators of the disorder for early diagnosis,\nwhile motor symptoms are not obvious. In this study, we constructed a new\nspeech corpus of Mandarin Chinese and addressed classification of patients with\nPD. We implemented classical machine learning methods with ranking algorithms\nfor feature selection, convolutional and recurrent deep networks, and an end to\nend system. Our classification accuracy significantly surpassed\nstate-of-the-art studies. The result suggests that free talk has stronger\nclassification power than standard speech tasks, which could help the design of\nfuture speech tasks for efficient early diagnosis of the disease. Based on\nexisting classification methods and our natural speech study, the automatic\ndetection of PD from daily conversation could be accessible to the majority of\nthe clinical population.", "published": "2021-05-31 04:51:44", "link": "http://arxiv.org/abs/2105.14704v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Towards One Model to Rule All: Multilingual Strategy for Dialectal\n  Code-Switching Arabic ASR", "abstract": "With the advent of globalization, there is an increasing demand for\nmultilingual automatic speech recognition (ASR), handling language and\ndialectal variation of spoken content. Recent studies show its efficacy over\nmonolingual systems. In this study, we design a large multilingual end-to-end\nASR using self-attention based conformer architecture. We trained the system\nusing Arabic (Ar), English (En) and French (Fr) languages. We evaluate the\nsystem performance handling: (i) monolingual (Ar, En and Fr); (ii)\nmulti-dialectal (Modern Standard Arabic, along with dialectal variation such as\nEgyptian and Moroccan); (iii) code-switching -- cross-lingual (Ar-En/Fr) and\ndialectal (MSA-Egyptian dialect) test cases, and compare with current\nstate-of-the-art systems. Furthermore, we investigate the influence of\ndifferent embedding/character representations including character vs\nword-piece; shared vs distinct input symbol per language. Our findings\ndemonstrate the strength of such a model by outperforming state-of-the-art\nmonolingual dialectal Arabic and code-switching Arabic ASR.", "published": "2021-05-31 08:20:38", "link": "http://arxiv.org/abs/2105.14779v2", "categories": ["cs.CL", "cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "On Compositional Generalization of Neural Machine Translation", "abstract": "Modern neural machine translation (NMT) models have achieved competitive\nperformance in standard benchmarks such as WMT. However, there still exist\nsignificant issues such as robustness, domain generalization, etc. In this\npaper, we study NMT models from the perspective of compositional generalization\nby building a benchmark dataset, CoGnition, consisting of 216k clean and\nconsistent sentence pairs. We quantitatively analyze effects of various factors\nusing compound translation error rate, then demonstrate that the NMT model\nfails badly on compositional generalization, although it performs remarkably\nwell under traditional metrics.", "published": "2021-05-31 09:04:29", "link": "http://arxiv.org/abs/2105.14802v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Supporting Cognitive and Emotional Empathic Writing of Students", "abstract": "We present an annotation approach to capturing emotional and cognitive\nempathy in student-written peer reviews on business models in German. We\npropose an annotation scheme that allows us to model emotional and cognitive\nempathy scores based on three types of review components. Also, we conducted an\nannotation study with three annotators based on 92 student essays to evaluate\nour annotation scheme. The obtained inter-rater agreement of {\\alpha}=0.79 for\nthe components and the multi-{\\pi}=0.41 for the empathy scores indicate that\nthe proposed annotation scheme successfully guides annotators to a substantial\nto moderate agreement. Moreover, we trained predictive models to detect the\nannotated empathy structures and embedded them in an adaptive writing support\nsystem for students to receive individual empathy feedback independent of an\ninstructor, time, and location. We evaluated our tool in a peer learning\nexercise with 58 students and found promising results for perceived empathy\nskill learning, perceived feedback accuracy, and intention to use. Finally, we\npresent our freely available corpus of 500 empathy-annotated, student-written\npeer reviews on business models and our annotation guidelines to encourage\nfuture research on the design and development of empathy support systems.", "published": "2021-05-31 09:18:50", "link": "http://arxiv.org/abs/2105.14815v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Bangla Natural Language Processing: A Comprehensive Analysis of\n  Classical, Machine Learning, and Deep Learning Based Methods", "abstract": "The Bangla language is the seventh most spoken language, with 265 million\nnative and non-native speakers worldwide. However, English is the predominant\nlanguage for online resources and technical knowledge, journals, and\ndocumentation. Consequently, many Bangla-speaking people, who have limited\ncommand of English, face hurdles to utilize English resources. To bridge the\ngap between limited support and increasing demand, researchers conducted many\nexperiments and developed valuable tools and techniques to create and process\nBangla language materials. Many efforts are also ongoing to make it easy to use\nthe Bangla language in the online and technical domains. There are some review\npapers to understand the past, previous, and future Bangla Natural Language\nProcessing (BNLP) trends. The studies are mainly concentrated on the specific\ndomains of BNLP, such as sentiment analysis, speech recognition, optical\ncharacter recognition, and text summarization. There is an apparent scarcity of\nresources that contain a comprehensive review of the recent BNLP tools and\nmethods. Therefore, in this paper, we present a thorough analysis of 75 BNLP\nresearch papers and categorize them into 11 categories, namely Information\nExtraction, Machine Translation, Named Entity Recognition, Parsing, Parts of\nSpeech Tagging, Question Answering System, Sentiment Analysis, Spam and Fake\nDetection, Text Summarization, Word Sense Disambiguation, and Speech Processing\nand Recognition. We study articles published between 1999 to 2021, and 50% of\nthe papers were published after 2015. Furthermore, we discuss Classical,\nMachine Learning and Deep Learning approaches with different datasets while\naddressing the limitations and current and future trends of the BNLP.", "published": "2021-05-31 10:58:58", "link": "http://arxiv.org/abs/2105.14875v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Do Multilingual Neural Machine Translation Models Contain Language Pair\n  Specific Attention Heads?", "abstract": "Recent studies on the analysis of the multilingual representations focus on\nidentifying whether there is an emergence of language-independent\nrepresentations, or whether a multilingual model partitions its weights among\ndifferent languages. While most of such work has been conducted in a\n\"black-box\" manner, this paper aims to analyze individual components of a\nmultilingual neural translation (NMT) model. In particular, we look at the\nencoder self-attention and encoder-decoder attention heads (in a many-to-one\nNMT model) that are more specific to the translation of a certain language pair\nthan others by (1) employing metrics that quantify some aspects of the\nattention weights such as \"variance\" or \"confidence\", and (2) systematically\nranking the importance of attention heads with respect to translation quality.\nExperimental results show that surprisingly, the set of most important\nattention heads are very similar across the language pairs and that it is\npossible to remove nearly one-third of the less important heads without hurting\nthe translation quality greatly.", "published": "2021-05-31 13:15:55", "link": "http://arxiv.org/abs/2105.14940v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Crowdsourcing Learning as Domain Adaptation: A Case Study on Named\n  Entity Recognition", "abstract": "Crowdsourcing is regarded as one prospective solution for effective\nsupervised learning, aiming to build large-scale annotated training data by\ncrowd workers. Previous studies focus on reducing the influences from the\nnoises of the crowdsourced annotations for supervised models. We take a\ndifferent point in this work, regarding all crowdsourced annotations as\ngold-standard with respect to the individual annotators. In this way, we find\nthat crowdsourcing could be highly similar to domain adaptation, and then the\nrecent advances of cross-domain methods can be almost directly applied to\ncrowdsourcing. Here we take named entity recognition (NER) as a study case,\nsuggesting an annotator-aware representation learning model that inspired by\nthe domain adaptation methods which attempt to capture effective domain-aware\nfeatures. We investigate both unsupervised and supervised crowdsourcing\nlearning, assuming that no or only small-scale expert annotations are\navailable. Experimental results on a benchmark crowdsourced NER dataset show\nthat our method is highly effective, leading to a new state-of-the-art\nperformance. In addition, under the supervised setting, we can achieve\nimpressive performance gains with only a very small scale of expert\nannotations.", "published": "2021-05-31 14:11:08", "link": "http://arxiv.org/abs/2105.14980v2", "categories": ["cs.CL", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Automatic audiovisual synchronisation for ultrasound tongue imaging", "abstract": "Ultrasound tongue imaging is used to visualise the intra-oral articulators\nduring speech production. It is utilised in a range of applications, including\nspeech and language therapy and phonetics research. Ultrasound and speech audio\nare recorded simultaneously, and in order to correctly use this data, the two\nmodalities should be correctly synchronised. Synchronisation is achieved using\nspecialised hardware at recording time, but this approach can fail in practice\nresulting in data of limited usability. In this paper, we address the problem\nof automatically synchronising ultrasound and audio after data collection. We\nfirst investigate the tolerance of expert ultrasound users to synchronisation\nerrors in order to find the thresholds for error detection. We use these\nthresholds to define accuracy scoring boundaries for evaluating our system. We\nthen describe our approach for automatic synchronisation, which is driven by a\nself-supervised neural network, exploiting the correlation between the two\nsignals to synchronise them. We train our model on data from multiple domains\nwith different speaker characteristics, different equipment, and different\nrecording environments, and achieve an accuracy >92.4% on held-out in-domain\ndata. Finally, we introduce a novel resource, the Cleft dataset, which we\ngathered with a new clinical subgroup and for which hardware synchronisation\nproved unreliable. We apply our model to this out-of-domain data, and evaluate\nits performance subjectively with expert users. Results show that users prefer\nour model's output over the original hardware output 79.3% of the time. Our\nresults demonstrate the strength of our approach and its ability to generalise\nto data from new domains.", "published": "2021-05-31 17:11:28", "link": "http://arxiv.org/abs/2105.15162v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD", "eess.IV"], "primary_category": "eess.AS"}
{"title": "Why does CTC result in peaky behavior?", "abstract": "The peaky behavior of CTC models is well known experimentally. However, an\nunderstanding about why peaky behavior occurs is missing, and whether this is a\ngood property. We provide a formal analysis of the peaky behavior and gradient\ndescent convergence properties of the CTC loss and related training criteria.\nOur analysis provides a deep understanding why peaky behavior occurs and when\nit is suboptimal. On a simple example which should be trivial to learn for any\nmodel, we prove that a feed-forward neural network trained with CTC from\nuniform initialization converges towards peaky behavior with a 100% error rate.\nOur analysis further explains why CTC only works well together with the blank\nlabel. We further demonstrate that peaky behavior does not occur on other\nrelated losses including a label prior model, and that this improves\nconvergence.", "published": "2021-05-31 10:03:14", "link": "http://arxiv.org/abs/2105.14849v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NE", "cs.SD", "eess.AS", "math.ST", "stat.TH"], "primary_category": "cs.LG"}
{"title": "Multi-Scale Attention Neural Network for Acoustic Echo Cancellation", "abstract": "Acoustic Echo Cancellation (AEC) plays a key role in speech interaction by\nsuppressing the echo received at microphone introduced by acoustic\nreverberations from loudspeakers. Since the performance of linear adaptive\nfilter (AF) would degrade severely due to nonlinear distortions, background\nnoises, and microphone clipping in real scenarios, deep learning has been\nemployed for AEC for its good nonlinear modelling ability. In this paper, we\nconstructed an end-to-end multi-scale attention neural network for AEC.\nTemporal convolution is first used to transform waveform into spectrogram. The\nspectrograms of the far-end reference and the near-end mixture are\nconcatenated, and fed to a temporal convolution network (TCN) with stacked\ndilated convolution layers. Attention mechanism is performed among these\nrepresentations from different layers to adaptively extract relevant features\nby referring to the previous hidden state in the encoder long short-term memory\n(LSTM) unit. The representations are weighted averaged and fed to the encoder\nLSTM for the near-end speech estimation. Experiments show the superiority of\nour method in terms of the echo return loss enhancement (ERLE) for single-talk\nperiods and the perceptual evaluation of speech quality (PESQ) score for\ndouble-talk periods in background noise and nonlinear distortion scenarios.", "published": "2021-05-31 06:30:59", "link": "http://arxiv.org/abs/2106.00010v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "EchoFilter: End-to-End Neural Network for Acoustic Echo Cancellation", "abstract": "Acoustic Echo Cancellation (AEC) whose aim is to suppress the echo originated\nfrom acoustic coupling between loudspeakers and microphones, plays a key role\nin voice interaction. Linear adaptive filter (AF) is always used for handling\nthis problem. However, since there would be some severe effects in real\nscenarios, such nonlinear distortions, background noises, and microphone\nclipping, it would lead to considerable residual echo, giving poor performance\nin practice. In this paper, we propose an end-to-end network structure for echo\ncancellation, which is directly done on time-domain audio waveform. It is\ntransformed to deep representation by temporal convolution, and modelled by\nLong Short-Term Memory (LSTM) for considering temporal property. Since time\ndelay and severe reverberation may exist at the near-end with respect to the\nfar-end, a local attention is employed for alignment. The network is trained\nusing multitask learning by employing an auxiliary classification network for\ndouble-talk detection. Experiments show the superiority of our proposed method\nin terms of the echo return loss enhancement (ERLE) for single-talk periods and\nthe perceptual evaluation of speech quality (PESQ) score for double-talk\nperiods in background noise and nonlinear distortion scenarios.", "published": "2021-05-31 01:39:08", "link": "http://arxiv.org/abs/2105.14666v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multi-Scale Temporal Convolution Network for Classroom Voice Detection", "abstract": "Teaching with the cooperation of expert teacher and assistant teacher, which\nis the so-called \"double-teachers classroom\", i.e., the course is giving by the\nexpert online and presented through projection screen at the classroom, and the\nteacher at the classroom performs as an assistant for guiding the students in\nlearning, is becoming more prevalent in today's teaching method for K-12\neducation. For monitoring the teaching quality, a microphone clipped on the\nassistant's neckline is always used for voice recording, then fed to the\ndownstream tasks of automatic speech recognition (ASR) and neural language\nprocessing (NLP). However, besides its voice, there would be some other\ninterfering voices, including the expert's one and the student's one. Here, we\npropose to extract the assistant' voices from the perspective of sound event\ndetection, i.e., the voices are classified into four categories, namely the\nexpert, the teacher, the mixture of them, and the background. To make\nframe-level identification, which is important for grabbing sensitive words for\nthe downstream tasks, a multi-scale temporal convolution neural network is\nconstructed with stacked dilated convolutions for considering both local and\nglobal properties. These features are concatenated and fed to a classification\nnetwork constructed by three linear layers. The framework is evaluated on\nsimulated data and real-world recordings, giving considerable performance in\nterms of precision and recall, compared with some classical classification\nmethods.", "published": "2021-05-31 06:26:26", "link": "http://arxiv.org/abs/2105.14717v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Noise Classification Aided Attention-Based Neural Network for Monaural\n  Speech Enhancement", "abstract": "This paper proposes an noise type classification aided attention-based neural\nnetwork approach for monaural speech enhancement. The network is constructed\nbased on a previous work by introducing a noise classification subnetwork into\nthe structure and taking the classification embedding into the attention\nmechanism for guiding the network to make better feature extraction.\nSpecifically, to make the network an end-to-end way, an audio encoder and\ndecoder constructed by temporal convolution is used to make transformation\nbetween waveform and spectrogram. Additionally, our model is composed of two\nlong short term memory (LSTM) based encoders, two attention mechanism, a noise\nclassifier and a speech mask generator. Experiments show that, compared with\nOM-LSA and the previous work, the proposed noise classification aided\nattention-based approach can achieve better performance in terms of speech\nquality (PESQ). More promisingly, our approach has better generalization\nability to unseen noise conditions.", "published": "2021-05-31 06:30:07", "link": "http://arxiv.org/abs/2105.14719v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "PF-Net: Personalized Filter for Speaker Recognition from Raw Waveform", "abstract": "Speaker recognition using i-vector has been replaced by speaker recognition\nusing deep learning. Speaker recognition based on Convolutional Neural Networks\n(CNNs) has been widely used in recent years, which learn low-level speech\nrepresentations from raw waveforms. On this basis, a CNN architecture called\nSincNet proposes a kind of unique convolutional layer, which has achieved\nband-pass filters. Compared with standard CNNs, SincNet learns the low and high\ncut-off frequencies of each filter. This paper proposes an improved CNNs\narchitecture called PF-Net, which encourages the first convolutional layer to\nimplement more personalized filters than SincNet. PF-Net parameterizes the\nfrequency domain shape and can realize band-pass filters by learning some\ndeformation points in frequency domain. Compared with standard CNN, PF-Net can\nlearn the characteristics of each filter. Compared with SincNet, PF-Net can\nlearn more characteristic parameters, instead of only low and high cut-off\nfrequencies. This provides a personalized filter bank for different tasks. As a\nresult, our experiments show that the PF-Net converges faster than standard CNN\nand performs better than SincNet. Our code is available at\ngithub.com/TAN-OpenLab/PF-NET.", "published": "2021-05-31 09:38:34", "link": "http://arxiv.org/abs/2105.14826v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Methodology for Exploring Deep Convolutional Features in Relation to\n  Hand-Crafted Features with an Application to Music Audio Modeling", "abstract": "Understanding the features learned by deep models is important from a model\ntrust perspective, especially as deep systems are deployed in the real world.\nMost recent approaches for deep feature understanding or model explanation\nfocus on highlighting input data features that are relevant for classification\ndecisions. In this work, we instead take the perspective of relating deep\nfeatures to well-studied, hand-crafted features that are meaningful for the\napplication of interest. We propose a methodology and set of systematic\nexperiments for exploring deep features in this setting, where input feature\nimportance approaches for deep feature understanding do not apply. Our\nexperiments focus on understanding which hand-crafted and deep features are\nuseful for the classification task of interest, how robust these features are\nfor related tasks and how similar the deep features are to the meaningful\nhand-crafted features. Our proposed method is general to many application areas\nand we demonstrate its utility on orchestral music audio data.", "published": "2021-05-31 21:31:27", "link": "http://arxiv.org/abs/2106.00110v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Byakto Speech: Real-time long speech synthesis with convolutional neural\n  network: Transfer learning from English to Bangla", "abstract": "Speech synthesis is one of the challenging tasks to automate by deep\nlearning, also being a low-resource language there are very few attempts at\nBangla speech synthesis. Most of the existing works can't work with anything\nother than simple Bangla characters script, very short sentences, etc. This\nwork attempts to solve these problems by introducing Byakta, the first-ever\nopen-source deep learning-based bilingual (Bangla and English) text to a speech\nsynthesis system. A speech recognition model-based automated scoring metric was\nalso proposed to evaluate the performance of a TTS model. We also introduce a\ntest benchmark dataset for Bangla speech synthesis models for evaluating speech\nquality. The TTS is available at https://github.com/zabir-nabil/bangla-tts", "published": "2021-05-31 20:39:35", "link": "http://arxiv.org/abs/2106.03937v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Singing Language Identification using a Deep Phonotactic Approach", "abstract": "Extensive works have tackled Language Identification (LID) in the speech\ndomain, however their application to the singing voice trails and performances\non Singing Language Identification (SLID) can be improved leveraging recent\nprogresses made in other singing related tasks. This work presents a modernized\nphonotactic system for SLID on polyphonic music: phoneme recognition is\nperformed with a Connectionist Temporal Classification (CTC)-based acoustic\nmodel trained with multilingual data, before language classification with a\nrecurrent model based on the phonemes estimation. The full pipeline is trained\nand evaluated with a large and publicly available dataset, with unprecedented\nperformances. First results of SLID with out-of-set languages are also\npresented.", "published": "2021-05-31 14:53:12", "link": "http://arxiv.org/abs/2105.15014v1", "categories": ["cs.SD", "cs.IR", "eess.AS"], "primary_category": "cs.SD"}
