{"title": "Dialog-context aware end-to-end speech recognition", "abstract": "Existing speech recognition systems are typically built at the sentence\nlevel, although it is known that dialog context, e.g. higher-level knowledge\nthat spans across sentences or speakers, can help the processing of long\nconversations. The recent progress in end-to-end speech recognition systems\npromises to integrate all available information (e.g. acoustic, language\nresources) into a single model, which is then jointly optimized. It seems\nnatural that such dialog context information should thus also be integrated\ninto the end-to-end models to improve further recognition accuracy. In this\nwork, we present a dialog-context aware speech recognition model, which\nexplicitly uses context information beyond sentence-level information, in an\nend-to-end fashion. Our dialog-context model captures a history of\nsentence-level context so that the whole system can be trained with\ndialog-context information in an end-to-end manner. We evaluate our proposed\napproach on the Switchboard conversational speech corpus and show that our\nsystem outperforms a comparable sentence-level end-to-end speech recognition\nsystem.", "published": "2018-08-07 01:04:39", "link": "http://arxiv.org/abs/1808.02171v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Segmental Audio Word2Vec: Representing Utterances as Sequences of\n  Vectors with Applications in Spoken Term Detection", "abstract": "While Word2Vec represents words (in text) as vectors carrying semantic\ninformation, audio Word2Vec was shown to be able to represent signal segments\nof spoken words as vectors carrying phonetic structure information. Audio\nWord2Vec can be trained in an unsupervised way from an unlabeled corpus, except\nthe word boundaries are needed. In this paper, we extend audio Word2Vec from\nword-level to utterance-level by proposing a new segmental audio Word2Vec, in\nwhich unsupervised spoken word boundary segmentation and audio Word2Vec are\njointly learned and mutually enhanced, so an utterance can be directly\nrepresented as a sequence of vectors carrying phonetic structure information.\nThis is achieved by a segmental sequence-to-sequence autoencoder (SSAE), in\nwhich a segmentation gate trained with reinforcement learning is inserted in\nthe encoder. Experiments on English, Czech, French and German show very good\nperformance in both unsupervised spoken word segmentation and spoken term\ndetection applications (significantly better than frame-based DTW).", "published": "2018-08-07 06:47:50", "link": "http://arxiv.org/abs/1808.02228v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ODSQA: Open-domain Spoken Question Answering Dataset", "abstract": "Reading comprehension by machine has been widely studied, but machine\ncomprehension of spoken content is still a less investigated problem. In this\npaper, we release Open-Domain Spoken Question Answering Dataset (ODSQA) with\nmore than three thousand questions. To the best of our knowledge, this is the\nlargest real SQA dataset. On this dataset, we found that ASR errors have\ncatastrophic impact on SQA. To mitigate the effect of ASR errors, subword units\nare involved, which brings consistent improvements over all the models. We\nfurther found that data augmentation on text-based QA training examples can\nimprove SQA.", "published": "2018-08-07 09:47:00", "link": "http://arxiv.org/abs/1808.02280v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Word-Level Loss Extensions for Neural Temporal Relation Classification", "abstract": "Unsupervised pre-trained word embeddings are used effectively for many tasks\nin natural language processing to leverage unlabeled textual data. Often these\nembeddings are either used as initializations or as fixed word representations\nfor task-specific classification models. In this work, we extend our\nclassification model's task loss with an unsupervised auxiliary loss on the\nword-embedding level of the model. This is to ensure that the learned word\nrepresentations contain both task-specific features, learned from the\nsupervised loss component, and more general features learned from the\nunsupervised loss component. We evaluate our approach on the task of temporal\nrelation extraction, in particular, narrative containment relation extraction\nfrom clinical records, and show that continued training of the embeddings on\nthe unsupervised objective together with the task objective gives better\ntask-specific embeddings, and results in an improvement over the state of the\nart on the THYME dataset, using only a general-domain part-of-speech tagger as\nlinguistic resource.", "published": "2018-08-07 14:00:11", "link": "http://arxiv.org/abs/1808.02374v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Design Challenges in Named Entity Transliteration", "abstract": "We analyze some of the fundamental design challenges that impact the\ndevelopment of a multilingual state-of-the-art named entity transliteration\nsystem, including curating bilingual named entity datasets and evaluation of\nmultiple transliteration methods. We empirically evaluate the transliteration\ntask using traditional weighted finite state transducer (WFST) approach against\ntwo neural approaches: the encoder-decoder recurrent neural network method and\nthe recent, non-sequential Transformer method. In order to improve availability\nof bilingual named entity transliteration datasets, we release personal name\nbilingual dictionaries minded from Wikidata for English to Russian, Hebrew,\nArabic and Japanese Katakana. Our code and dictionaries are publicly available.", "published": "2018-08-07 22:01:37", "link": "http://arxiv.org/abs/1808.02563v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Effective Character-augmented Word Embedding for Machine Reading\n  Comprehension", "abstract": "Machine reading comprehension is a task to model relationship between passage\nand query. In terms of deep learning framework, most of state-of-the-art models\nsimply concatenate word and character level representations, which has been\nshown suboptimal for the concerned task. In this paper, we empirically explore\ndifferent integration strategies of word and character embeddings and propose a\ncharacter-augmented reader which attends character-level representation to\naugment word embedding with a short list to improve word representations,\nespecially for rare words. Experimental results show that the proposed approach\nhelps the baseline model significantly outperform state-of-the-art baselines on\nvarious public benchmarks.", "published": "2018-08-07 15:37:20", "link": "http://arxiv.org/abs/1808.02772v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How did the discussion go: Discourse act classification in social media\n  conversations", "abstract": "We propose a novel attention based hierarchical LSTM model to classify\ndiscourse act sequences in social media conversations, aimed at mining data\nfrom online discussion using textual meanings beyond sentence level. The very\nuniqueness of the task is the complete categorization of possible pragmatic\nroles in informal textual discussions, contrary to extraction of\nquestion-answers, stance detection or sarcasm identification which are very\nmuch role specific tasks. Early attempt was made on a Reddit discussion\ndataset. We train our model on the same data, and present test results on two\ndifferent datasets, one from Reddit and one from Facebook. Our proposed model\noutperformed the previous one in terms of domain independence; without using\nplatform-dependent structural features, our hierarchical LSTM with word\nrelevance attention mechanism achieved F1-scores of 71\\% and 66\\% respectively\nto predict discourse roles of comments in Reddit and Facebook discussions.\nEfficiency of recurrent and convolutional architectures in order to learn\ndiscursive representation on the same task has been presented and analyzed,\nwith different word and comment embedding schemes. Our attention mechanism\nenables us to inquire into relevance ordering of text segments according to\ntheir roles in discourse. We present a human annotator experiment to unveil\nimportant observations about modeling and data annotation. Equipped with our\ntext-based discourse identification model, we inquire into how heterogeneous\nnon-textual features like location, time, leaning of information etc. play\ntheir roles in charaterizing online discussions on Facebook.", "published": "2018-08-07 10:14:38", "link": "http://arxiv.org/abs/1808.02290v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Device-directed Utterance Detection", "abstract": "In this work, we propose a classifier for distinguishing device-directed\nqueries from background speech in the context of interactions with voice\nassistants. Applications include rejection of false wake-ups or unintended\ninteractions as well as enabling wake-word free follow-up queries. Consider the\nexample interaction: $\"Computer,~play~music\", \"Computer,~reduce~the~volume\"$.\nIn this interaction, the user needs to repeat the wake-word ($Computer$) for\nthe second query. To allow for more natural interactions, the device could\nimmediately re-enter listening state after the first query (without wake-word\nrepetition) and accept or reject a potential follow-up as device-directed or\nbackground speech. The proposed model consists of two long short-term memory\n(LSTM) neural networks trained on acoustic features and automatic speech\nrecognition (ASR) 1-best hypotheses, respectively. A feed-forward deep neural\nnetwork (DNN) is then trained to combine the acoustic and 1-best embeddings,\nderived from the LSTMs, with features from the ASR decoder. Experimental\nresults show that ASR decoder, acoustic embeddings, and 1-best embeddings yield\nan equal-error-rate (EER) of $9.3~\\%$, $10.9~\\%$ and $20.1~\\%$, respectively.\nCombination of the features resulted in a $44~\\%$ relative improvement and a\nfinal EER of $5.2~\\%$.", "published": "2018-08-07 18:19:30", "link": "http://arxiv.org/abs/1808.02504v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Deep context: end-to-end contextual speech recognition", "abstract": "In automatic speech recognition (ASR) what a user says depends on the\nparticular context she is in. Typically, this context is represented as a set\nof word n-grams. In this work, we present a novel, all-neural, end-to-end (E2E)\nASR sys- tem that utilizes such context. Our approach, which we re- fer to as\nContextual Listen, Attend and Spell (CLAS) jointly- optimizes the ASR\ncomponents along with embeddings of the context n-grams. During inference, the\nCLAS system can be presented with context phrases which might contain out-of-\nvocabulary (OOV) terms not seen during training. We com- pare our proposed\nsystem to a more traditional contextualiza- tion approach, which performs\nshallow-fusion between inde- pendently trained LAS and contextual n-gram models\nduring beam search. Across a number of tasks, we find that the pro- posed CLAS\nsystem outperforms the baseline method by as much as 68% relative WER,\nindicating the advantage of joint optimization over individually trained\ncomponents. Index Terms: speech recognition, sequence-to-sequence models,\nlisten attend and spell, LAS, attention, embedded speech recognition.", "published": "2018-08-07 21:23:21", "link": "http://arxiv.org/abs/1808.02480v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
