{"title": "Semantic Regularities in Document Representations", "abstract": "Recent work exhibited that distributed word representations are good at\ncapturing linguistic regularities in language. This allows vector-oriented\nreasoning based on simple linear algebra between words. Since many different\nmethods have been proposed for learning document representations, it is natural\nto ask whether there is also linear structure in these learned representations\nto allow similar reasoning at document level. To answer this question, we\ndesign a new document analogy task for testing the semantic regularities in\ndocument representations, and conduct empirical evaluations over several\nstate-of-the-art document representation models. The results reveal that neural\nembedding based document representations work better on this analogy task than\nconventional methods, and we provide some preliminary explanations over these\nobservations.", "published": "2016-03-24 14:45:20", "link": "http://arxiv.org/abs/1603.07603v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contrastive Analysis with Predictive Power: Typology Driven Estimation\n  of Grammatical Error Distributions in ESL", "abstract": "This work examines the impact of cross-linguistic transfer on grammatical\nerrors in English as Second Language (ESL) texts. Using a computational\nframework that formalizes the theory of Contrastive Analysis (CA), we\ndemonstrate that language specific error distributions in ESL writing can be\npredicted from the typological properties of the native language and their\nrelation to the typology of English. Our typology driven model enables to\nobtain accurate estimates of such distributions without access to any ESL data\nfor the target languages. Furthermore, we present a strategy for adjusting our\nmethod to low-resource languages that lack typological documentation using a\nbootstrapping approach which approximates native language typology from ESL\ntexts. Finally, we show that our framework is instrumental for linguistic\ninquiry seeking to identify first language factors that contribute to a wide\nrange of difficulties in second language acquisition.", "published": "2016-03-24 14:59:45", "link": "http://arxiv.org/abs/1603.07609v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Part-of-Speech Relevance Weights for Learning Word Embeddings", "abstract": "This paper proposes a model to learn word embeddings with weighted contexts\nbased on part-of-speech (POS) relevance weights. POS is a fundamental element\nin natural language. However, state-of-the-art word embedding models fail to\nconsider it. This paper proposes to use position-dependent POS relevance\nweighting matrices to model the inherent syntactic relationship among words\nwithin a context window. We utilize the POS relevance weights to model each\nword-context pairs during the word embedding training process. The model\nproposed in this paper paper jointly optimizes word vectors and the POS\nrelevance matrices. Experiments conducted on popular word analogy and word\nsimilarity tasks all demonstrated the effectiveness of the proposed method.", "published": "2016-03-24 18:22:39", "link": "http://arxiv.org/abs/1603.07695v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Text Generation from Structured Data with Application to the\n  Biography Domain", "abstract": "This paper introduces a neural model for concept-to-text generation that\nscales to large, rich domains. We experiment with a new dataset of biographies\nfrom Wikipedia that is an order of magnitude larger than existing resources\nwith over 700k samples. The dataset is also vastly more diverse with a 400k\nvocabulary, compared to a few hundred words for Weathergov or Robocup. Our\nmodel builds upon recent work on conditional neural language model for text\ngeneration. To deal with the large vocabulary, we extend these models to mix a\nfixed vocabulary with copy actions that transfer sample-specific words from the\ninput database to the generated output sentence. Our neural model significantly\nout-performs a classical Kneser-Ney language model adapted to this task by\nnearly 15 BLEU.", "published": "2016-03-24 22:40:00", "link": "http://arxiv.org/abs/1603.07771v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic Properties of Customer Sentiment in Tweets", "abstract": "An increasing number of people are using online social networking services\n(SNSs), and a significant amount of information related to experiences in\nconsumption is shared in this new media form. Text mining is an emerging\ntechnique for mining useful information from the web. We aim at discovering in\nparticular tweets semantic patterns in consumers' discussions on social media.\nSpecifically, the purposes of this study are twofold: 1) finding similarity and\ndissimilarity between two sets of textual documents that include consumers'\nsentiment polarities, two forms of positive vs. negative opinions and 2)\ndriving actual content from the textual data that has a semantic trend. The\nconsidered tweets include consumers opinions on US retail companies (e.g.,\nAmazon, Walmart). Cosine similarity and K-means clustering methods are used to\nachieve the former goal, and Latent Dirichlet Allocation (LDA), a popular topic\nmodeling algorithm, is used for the latter purpose. This is the first study\nwhich discover semantic properties of textual data in consumption context\nbeyond sentiment analysis. In addition to major findings, we apply LDA (Latent\nDirichlet Allocations) to the same data and drew latent topics that represent\nconsumers' positive opinions and negative opinions on social media.", "published": "2016-03-24 15:22:52", "link": "http://arxiv.org/abs/1603.07624v1", "categories": ["cs.CL", "cs.IR", "cs.SI", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Recursive Neural Language Architecture for Tag Prediction", "abstract": "We consider the problem of learning distributed representations for tags from\ntheir associated content for the task of tag recommendation. Considering\ntagging information is usually very sparse, effective learning from content and\ntag association is very crucial and challenging task. Recently, various neural\nrepresentation learning models such as WSABIE and its variants show promising\nperformance, mainly due to compact feature representations learned in a\nsemantic space. However, their capacity is limited by a linear compositional\napproach for representing tags as sum of equal parts and hurt their\nperformance. In this work, we propose a neural feedback relevance model for\nlearning tag representations with weighted feature representations. Our\nexperiments on two widely used datasets show significant improvement for\nquality of recommendations over various baselines.", "published": "2016-03-24 16:39:37", "link": "http://arxiv.org/abs/1603.07646v1", "categories": ["cs.IR", "cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.IR"}
{"title": "Mapping Out Narrative Structures and Dynamics Using Networks and Textual\n  Information", "abstract": "Human communication is often executed in the form of a narrative, an account\nof connected events composed of characters, actions, and settings. A coherent\nnarrative structure is therefore a requisite for a well-formulated narrative --\nbe it fictional or nonfictional -- for informative and effective communication,\nopening up the possibility of a deeper understanding of a narrative by studying\nits structural properties. In this paper we present a network-based framework\nfor modeling and analyzing the structure of a narrative, which is further\nexpanded by incorporating methods from computational linguistics to utilize the\nnarrative text. Modeling a narrative as a dynamically unfolding system, we\ncharacterize its progression via the growth patterns of the character network,\nand use sentiment analysis and topic modeling to represent the actual content\nof the narrative in the form of interaction maps between characters with\nassociated sentiment values and keywords. This is a network framework advanced\nbeyond the simple occurrence-based one most often used until now, allowing one\nto utilize the unique characteristics of a given narrative to a high degree.\nGiven the ubiquity and importance of narratives, such advanced network-based\nrepresentation and analysis framework may lead to a more systematic modeling\nand understanding of narratives for social interactions, expression of human\nsentiments, and communication.", "published": "2016-03-24 10:59:28", "link": "http://arxiv.org/abs/1604.03029v1", "categories": ["cs.CL", "cs.SI", "physics.soc-ph"], "primary_category": "cs.CL"}
