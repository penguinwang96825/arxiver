{"title": "Computer-assisted construct classification of organizational performance\n  concerning different stakeholder groups", "abstract": "The number of research articles in business and management has dramatically\nincreased along with terminology, constructs, and measures. Proper\nclassification of organizational performance constructs from research articles\nplays an important role in categorizing the literature and understanding to\nwhom its research implications may be relevant. In this work, we classify\nconstructs (i.e., concepts and terminology used to capture different aspects of\norganizational performance) in research articles into a three-level\ncategorization: (a) performance and non-performance categories (Level 0); (b)\nfor performance constructs, stakeholder group-level of performance concerning\ninvestors, customers, employees, and the society (community and natural\nenvironment) (Level 1); and (c) for each stakeholder group-level, subcategories\nof different ways of measurement (Level 2). We observed that increasing\ncontextual information with features extracted from surrounding sentences and\nexternal references improves classification of disaggregate-level labels, given\nlimited training data. Our research has implications for computer-assisted\nconstruct identification and classification - an essential step for research\nsynthesis.", "published": "2021-07-11 21:39:37", "link": "http://arxiv.org/abs/2107.05133v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Low-resource Reading Comprehension via Cross-lingual\n  Transposition Rethinking", "abstract": "Extractive Reading Comprehension (ERC) has made tremendous advances enabled\nby the availability of large-scale high-quality ERC training data. Despite of\nsuch rapid progress and widespread application, the datasets in languages other\nthan high-resource languages such as English remain scarce. To address this\nissue, we propose a Cross-Lingual Transposition ReThinking (XLTT) model by\nmodelling existing high-quality extractive reading comprehension datasets in a\nmultilingual environment. To be specific, we present multilingual adaptive\nattention (MAA) to combine intra-attention and inter-attention to learn more\ngeneral generalizable semantic and lexical knowledge from each pair of language\nfamilies. Furthermore, to make full use of existing datasets, we adopt a new\ntraining framework to train our model by calculating task-level similarities\nbetween each existing dataset and target dataset. The experimental results show\nthat our XLTT model surpasses six baselines on two multilingual ERC benchmarks,\nespecially more effective for low-resource languages with 3.9 and 4.1 average\nimprovement in F1 and EM, respectively.", "published": "2021-07-11 09:35:16", "link": "http://arxiv.org/abs/2107.05002v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multilingual and crosslingual speech recognition using\n  phonological-vector based phone embeddings", "abstract": "The use of phonological features (PFs) potentially allows language-specific\nphones to remain linked in training, which is highly desirable for information\nsharing for multilingual and crosslingual speech recognition methods for\nlow-resourced languages. A drawback suffered by previous methods in using\nphonological features is that the acoustic-to-PF extraction in a bottom-up way\nis itself difficult. In this paper, we propose to join phonology driven phone\nembedding (top-down) and deep neural network (DNN) based acoustic feature\nextraction (bottom-up) to calculate phone probabilities. The new method is\ncalled JoinAP (Joining of Acoustics and Phonology). Remarkably, no inversion\nfrom acoustics to phonological features is required for speech recognition. For\neach phone in the IPA (International Phonetic Alphabet) table, we encode its\nphonological features to a phonological-vector, and then apply linear or\nnonlinear transformation of the phonological-vector to obtain the phone\nembedding. A series of multilingual and crosslingual (both zero-shot and\nfew-shot) speech recognition experiments are conducted on the CommonVoice\ndataset (German, French, Spanish and Italian) and the AISHLL-1 dataset\n(Mandarin), and demonstrate the superiority of JoinAP with nonlinear phone\nembeddings over both JoinAP with linear phone embeddings and the traditional\nmethod with flat phone embeddings.", "published": "2021-07-11 12:56:47", "link": "http://arxiv.org/abs/2107.05038v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "PocketVAE: A Two-step Model for Groove Generation and Control", "abstract": "Creating a good drum track to imitate a skilled performer in digital audio\nworkstations (DAWs) can be a time-consuming process, especially for those\nunfamiliar with drums. In this work, we introduce PocketVAE, a groove\ngeneration system that applies grooves to users' rudimentary MIDI tracks, i.e,\ntemplates. Grooves can be either transferred from a reference track, generated\nrandomly or with conditions, such as genres. Our system, consisting of\ndifferent modules for each groove component, takes a two-step approach that is\nanalogous to a music creation process. First, the note module updates the user\ntemplate through addition and deletion of notes; Second, the velocity and\nmicrotiming modules add details to this generated note score. In order to model\nthe drum notes, we apply a discrete latent representation method via Vector\nQuantized Variational Autoencoder (VQ-VAE), as drum notes have a discrete\nproperty, unlike velocity and microtiming values. We show that our two-step\napproach and the usage of a discrete encoding space improves the learning of\nthe original data distribution. Additionally, we discuss the benefit of\nincorporating control elements - genre, velocity and microtiming patterns -\ninto the model.", "published": "2021-07-11 10:26:00", "link": "http://arxiv.org/abs/2107.05009v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ReconVAT: A Semi-Supervised Automatic Music Transcription Framework for\n  Low-Resource Real-World Data", "abstract": "Most of the current supervised automatic music transcription (AMT) models\nlack the ability to generalize. This means that they have trouble transcribing\nreal-world music recordings from diverse musical genres that are not presented\nin the labelled training data. In this paper, we propose a semi-supervised\nframework, ReconVAT, which solves this issue by leveraging the huge amount of\navailable unlabelled music recordings. The proposed ReconVAT uses\nreconstruction loss and virtual adversarial training. When combined with\nexisting U-net models for AMT, ReconVAT achieves competitive results on common\nbenchmark datasets such as MAPS and MusicNet. For example, in the few-shot\nsetting for the string part version of MusicNet, ReconVAT achieves F1-scores of\n61.0% and 41.6% for the note-wise and note-with-offset-wise metrics\nrespectively, which translates into an improvement of 22.2% and 62.5% compared\nto the supervised baseline model. Our proposed framework also demonstrates the\npotential of continual learning on new data, which could be useful in\nreal-world applications whereby new data is constantly available.", "published": "2021-07-11 03:25:58", "link": "http://arxiv.org/abs/2107.04954v2", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Deep-Bayesian Framework for Adaptive Speech Duration Modification", "abstract": "We propose the first method to adaptively modify the duration of a given\nspeech signal. Our approach uses a Bayesian framework to define a latent\nattention map that links frames of the input and target utterances. We train a\nmasked convolutional encoder-decoder network to produce this attention map via\na stochastic version of the mean absolute error loss function; our model also\npredicts the length of the target speech signal using the encoder embeddings.\nThe predicted length determines the number of steps for the decoder operation.\nDuring inference, we generate the attention map as a proxy for the similarity\nmatrix between the given input speech and an unknown target speech signal.\nUsing this similarity matrix, we compute a warping path of alignment between\nthe two signals. Our experiments demonstrate that this adaptive framework\nproduces similar results to dynamic time warping, which relies on a known\ntarget signal, on both voice conversion and emotion conversion tasks. We also\nshow that our technique results in a high quality of generated speech that is\non par with state-of-the-art vocoders.", "published": "2021-07-11 05:53:07", "link": "http://arxiv.org/abs/2107.04973v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Neural Waveshaping Synthesis", "abstract": "We present the Neural Waveshaping Unit (NEWT): a novel, lightweight, fully\ncausal approach to neural audio synthesis which operates directly in the\nwaveform domain, with an accompanying optimisation (FastNEWT) for efficient CPU\ninference. The NEWT uses time-distributed multilayer perceptrons with periodic\nactivations to implicitly learn nonlinear transfer functions that encode the\ncharacteristics of a target timbre. Once trained, a NEWT can produce complex\ntimbral evolutions by simple affine transformations of its input and output\nsignals. We paired the NEWT with a differentiable noise synthesiser and reverb\nand found it capable of generating realistic musical instrument performances\nwith only 260k total model parameters, conditioned on F0 and loudness features.\nWe compared our method to state-of-the-art benchmarks with a multi-stimulus\nlistening test and the Fr\\'echet Audio Distance and found it performed\ncompetitively across the tested timbral domains. Our method significantly\noutperformed the benchmarks in terms of generation speed, and achieved\nreal-time performance on a consumer CPU, both with and without FastNEWT,\nsuggesting it is a viable basis for future creative sound design tools.", "published": "2021-07-11 13:50:59", "link": "http://arxiv.org/abs/2107.05050v2", "categories": ["cs.SD", "cs.LG", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "DiCOVA-Net: Diagnosing COVID-19 using Acoustics based on Deep Residual\n  Network for the DiCOVA Challenge 2021", "abstract": "In this paper, we propose a deep residual network-based method, namely the\nDiCOVA-Net, to identify COVID-19 infected patients based on the acoustic\nrecording of their coughs. Since there are far more healthy people than\ninfected patients, this classification problem faces the challenge of\nimbalanced data. To improve the model's ability to recognize minority class\n(the infected patients), we introduce data augmentation and cost-sensitive\nmethods into our model. Besides, considering the particularity of this task, we\ndeploy some fine-tuning techniques to adjust the pre-training ResNet50.\nFurthermore, to improve the model's generalizability, we use ensemble learning\nto integrate prediction results from multiple base classifiers generated using\ndifferent random seeds. To evaluate the proposed DiCOVA-Net's performance, we\nconducted experiments with the DiCOVA challenge dataset. The results show that\nour method has achieved 85.43\\% in AUC, among the top of all competing teams.", "published": "2021-07-11 19:25:06", "link": "http://arxiv.org/abs/2107.06126v2", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Many-to-Many Voice Conversion based Feature Disentanglement using\n  Variational Autoencoder", "abstract": "Voice conversion is a challenging task which transforms the voice\ncharacteristics of a source speaker to a target speaker without changing\nlinguistic content. Recently, there have been many works on many-to-many Voice\nConversion (VC) based on Variational Autoencoder (VAEs) achieving good results,\nhowever, these methods lack the ability to disentangle speaker identity and\nlinguistic content to achieve good performance on unseen speaker scenarios. In\nthis paper, we propose a new method based on feature disentanglement to tackle\nmany to many voice conversion. The method has the capability to disentangle\nspeaker identity and linguistic content from utterances, it can convert from\nmany source speakers to many target speakers with a single autoencoder network.\nMoreover, it naturally deals with the unseen target speaker scenarios. We\nperform both objective and subjective evaluations to show the competitive\nperformance of our proposed method compared with other state-of-the-art models\nin terms of naturalness and target speaker similarity.", "published": "2021-07-11 13:31:16", "link": "http://arxiv.org/abs/2107.06642v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
