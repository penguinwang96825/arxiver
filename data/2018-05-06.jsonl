{"title": "Dynamic and Static Topic Model for Analyzing Time-Series Document\n  Collections", "abstract": "For extracting meaningful topics from texts, their structures should be\nconsidered properly. In this paper, we aim to analyze structured time-series\ndocuments such as a collection of news articles and a series of scientific\npapers, wherein topics evolve along time depending on multiple topics in the\npast and are also related to each other at each time. To this end, we propose a\ndynamic and static topic model, which simultaneously considers the dynamic\nstructures of the temporal topic evolution and the static structures of the\ntopic hierarchy at each time. We show the results of experiments on collections\nof scientific papers, in which the proposed method outperformed conventional\nmodels. Moreover, we show an example of extracted topic structures, which we\nfound helpful for analyzing research activities.", "published": "2018-05-06 12:41:47", "link": "http://arxiv.org/abs/1805.02203v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Passage Machine Reading Comprehension with Cross-Passage Answer\n  Verification", "abstract": "Machine reading comprehension (MRC) on real web data usually requires the\nmachine to answer a question by analyzing multiple passages retrieved by search\nengine. Compared with MRC on a single passage, multi-passage MRC is more\nchallenging, since we are likely to get multiple confusing answer candidates\nfrom different passages. To address this problem, we propose an end-to-end\nneural model that enables those answer candidates from different passages to\nverify each other based on their content representations. Specifically, we\njointly train three modules that can predict the final answer based on three\nfactors: the answer boundary, the answer content and the cross-passage answer\nverification. The experimental results show that our method outperforms the\nbaseline by a large margin and achieves the state-of-the-art performance on the\nEnglish MS-MARCO dataset and the Chinese DuReader dataset, both of which are\ndesigned for MRC in real-world settings.", "published": "2018-05-06 14:26:35", "link": "http://arxiv.org/abs/1805.02220v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Russian word sense induction by clustering averaged word embeddings", "abstract": "The paper reports our participation in the shared task on word sense\ninduction and disambiguation for the Russian language (RUSSE-2018). Our team\nwas ranked 2nd for the wiki-wiki dataset (containing mostly homonyms) and 5th\nfor the bts-rnc and active-dict datasets (containing mostly polysemous words)\namong all 19 participants.\n  The method we employed was extremely naive. It implied representing contexts\nof ambiguous words as averaged word embedding vectors, using off-the-shelf\npre-trained distributional models. Then, these vector representations were\nclustered with mainstream clustering techniques, thus producing the groups\ncorresponding to the ambiguous word senses. As a side result, we show that word\nembedding models trained on small but balanced corpora can be superior to those\ntrained on large but noisy data - not only in intrinsic evaluation, but also in\ndownstream tasks like word sense induction.", "published": "2018-05-06 18:25:12", "link": "http://arxiv.org/abs/1805.02258v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Construction of the Literature Graph in Semantic Scholar", "abstract": "We describe a deployed scalable system for organizing published scientific\nliterature into a heterogeneous graph to facilitate algorithmic manipulation\nand discovery. The resulting literature graph consists of more than 280M nodes,\nrepresenting papers, authors, entities and various interactions between them\n(e.g., authorships, citations, entity mentions). We reduce literature graph\nconstruction into familiar NLP tasks (e.g., entity extraction and linking),\npoint out research challenges due to differences from standard formulations of\nthese tasks, and report empirical results for each task. The methods described\nin this paper are used to enable semantic features in www.semanticscholar.org", "published": "2018-05-06 18:35:48", "link": "http://arxiv.org/abs/1805.02262v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Breaking NLI Systems with Sentences that Require Simple Lexical\n  Inferences", "abstract": "We create a new NLI test set that shows the deficiency of state-of-the-art\nmodels in inferences that require lexical and world knowledge. The new examples\nare simpler than the SNLI test set, containing sentences that differ by at most\none word from sentences in the training set. Yet, the performance on the new\ntest set is substantially worse across systems trained on SNLI, demonstrating\nthat these systems are limited in their generalization ability, failing to\ncapture many simple inferences.", "published": "2018-05-06 18:49:48", "link": "http://arxiv.org/abs/1805.02266v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Coherence Modeling of Asynchronous Conversations: A Neural Entity Grid\n  Approach", "abstract": "We propose a novel coherence model for written asynchronous conversations\n(e.g., forums, emails), and show its applications in coherence assessment and\nthread reconstruction tasks. We conduct our research in two steps. First, we\npropose improvements to the recently proposed neural entity grid model by\nlexicalizing its entity transitions. Then, we extend the model to asynchronous\nconversations by incorporating the underlying conversational structure in the\nentity grid representation and feature computation. Our model achieves state of\nthe art results on standard coherence assessment tasks in monologue and\nconversations outperforming existing models. We also demonstrate its\neffectiveness in reconstructing thread structures.", "published": "2018-05-06 20:40:19", "link": "http://arxiv.org/abs/1805.02275v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Domain Neural Machine Translation", "abstract": "We present an approach to neural machine translation (NMT) that supports\nmultiple domains in a single model and allows switching between the domains\nwhen translating. The core idea is to treat text domains as distinct languages\nand use multilingual NMT methods to create multi-domain translation systems, we\nshow that this approach results in significant translation quality gains over\nfine-tuning. We also explore whether the knowledge of pre-specified text\ndomains is necessary, turns out that it is after all, but also that when it is\nnot known quite high translation quality can be reached.", "published": "2018-05-06 22:14:47", "link": "http://arxiv.org/abs/1805.02282v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero-shot Sequence Labeling: Transferring Knowledge from Sentences to\n  Tokens", "abstract": "Can attention- or gradient-based visualization techniques be used to infer\ntoken-level labels for binary sequence tagging problems, using networks trained\nonly on sentence-level labels? We construct a neural network architecture based\non soft attention, train it as a binary sentence classifier and evaluate\nagainst token-level annotation on four different datasets. Inferring token\nlabels from a network provides a method for quantitatively evaluating what the\nmodel is learning, along with generating useful feedback in assistance systems.\nOur results indicate that attention-based methods are able to predict\ntoken-level labels more accurately, compared to gradient-based methods,\nsometimes even rivaling the supervised oracle network.", "published": "2018-05-06 13:53:50", "link": "http://arxiv.org/abs/1805.02214v1", "categories": ["cs.CL", "cs.LG", "cs.NE", "I.2.7; I.2.6; I.5.1"], "primary_category": "cs.CL"}
