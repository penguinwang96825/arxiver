{"title": "Towards Making the Most of Dialogue Characteristics for Neural Chat\n  Translation", "abstract": "Neural Chat Translation (NCT) aims to translate conversational text between\nspeakers of different languages. Despite the promising performance of\nsentence-level and context-aware neural machine translation models, there still\nremain limitations in current NCT models because the inherent dialogue\ncharacteristics of chat, such as dialogue coherence and speaker personality,\nare neglected. In this paper, we propose to promote the chat translation by\nintroducing the modeling of dialogue characteristics into the NCT model. To\nthis end, we design four auxiliary tasks including monolingual response\ngeneration, cross-lingual response generation, next utterance discrimination,\nand speaker identification. Together with the main chat translation task, we\noptimize the NCT model through the training objectives of all these tasks. By\nthis means, the NCT model can be enhanced by capturing the inherent dialogue\ncharacteristics, thus generating more coherent and speaker-relevant\ntranslations. Comprehensive experiments on four language directions\n(English-German and English-Chinese) verify the effectiveness and superiority\nof the proposed approach.", "published": "2021-09-02 02:04:00", "link": "http://arxiv.org/abs/2109.00668v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Empirical Exploration in Quality Filtering of Text Data", "abstract": "While conventional wisdom suggests that more aggressively filtering data from\nlow-quality sources like Common Crawl always monotonically improves the quality\nof training data, we find that aggressive filtering can in fact lead to a\ndecrease in model quality on a wide array of downstream tasks for a GPT-like\nlanguage model. We speculate that this is because optimizing sufficiently\nstrongly for a proxy metric harms performance on the true objective, suggesting\na need for more robust filtering objectives when attempting to filter more\naggressively. We hope this work leads to detailed analysis of the effects of\ndataset filtering design choices on downstream model performance in future\nwork.", "published": "2021-09-02 04:02:51", "link": "http://arxiv.org/abs/2109.00698v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ShopTalk: A System for Conversational Faceted Search", "abstract": "We present ShopTalk, a multi-turn conversational faceted search system for\nshopping that is designed to handle large and complex schemas that are beyond\nthe scope of state of the art slot-filling systems. ShopTalk decouples dialog\nmanagement from fulfillment, thereby allowing the dialog understanding system\nto be domain-agnostic and not tied to the particular shopping application. The\ndialog understanding system consists of a deep-learned Contextual Language\nUnderstanding module, which interprets user utterances, and a primarily\nrules-based Dialog-State Tracker (DST), which updates the dialog state and\nformulates search requests intended for the fulfillment engine. The interface\nbetween the two modules consists of a minimal set of domain-agnostic \"intent\noperators,\" which instruct the DST on how to update the dialog state. ShopTalk\nwas deployed in 2020 on the Google Assistant for Shopping searches.", "published": "2021-09-02 04:22:29", "link": "http://arxiv.org/abs/2109.00702v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural News Recommendation with Collaborative News Encoding and\n  Structural User Encoding", "abstract": "Automatic news recommendation has gained much attention from the academic\ncommunity and industry. Recent studies reveal that the key to this task lies\nwithin the effective representation learning of both news and users. Existing\nworks typically encode news title and content separately while neglecting their\nsemantic interaction, which is inadequate for news text comprehension. Besides,\nprevious models encode user browsing history without leveraging the structural\ncorrelation of user browsed news to reflect user interests explicitly. In this\nwork, we propose a news recommendation framework consisting of collaborative\nnews encoding (CNE) and structural user encoding (SUE) to enhance news and user\nrepresentation learning. CNE equipped with bidirectional LSTMs encodes news\ntitle and content collaboratively with cross-selection and cross-attention\nmodules to learn semantic-interactive news representations. SUE utilizes graph\nconvolutional networks to extract cluster-structural features of user history,\nfollowed by intra-cluster and inter-cluster attention modules to learn\nhierarchical user interest representations. Experiment results on the MIND\ndataset validate the effectiveness of our model to improve the performance of\nnews recommendation. Our code is released at\nhttps://github.com/Veason-silverbullet/NNR.", "published": "2021-09-02 07:16:42", "link": "http://arxiv.org/abs/2109.00750v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MWPToolkit: An Open-Source Framework for Deep Learning-Based Math Word\n  Problem Solvers", "abstract": "Developing automatic Math Word Problem (MWP) solvers has been an interest of\nNLP researchers since the 1960s. Over the last few years, there are a growing\nnumber of datasets and deep learning-based methods proposed for effectively\nsolving MWPs. However, most existing methods are benchmarked soly on one or two\ndatasets, varying in different configurations, which leads to a lack of\nunified, standardized, fair, and comprehensive comparison between methods. This\npaper presents MWPToolkit, the first open-source framework for solving MWPs. In\nMWPToolkit, we decompose the procedure of existing MWP solvers into multiple\ncore components and decouple their models into highly reusable modules. We also\nprovide a hyper-parameter search function to boost the performance. In total,\nwe implement and compare 17 MWP solvers on 4 widely-used single equation\ngeneration benchmarks and 2 multiple equations generation benchmarks. These\nfeatures enable our MWPToolkit to be suitable for researchers to reproduce\nadvanced baseline models and develop new MWP solvers quickly. Code and\ndocuments are available at https://github.com/LYH-YF/MWPToolkit.", "published": "2021-09-02 09:18:09", "link": "http://arxiv.org/abs/2109.00799v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MultiEURLEX -- A multi-lingual and multi-label legal document\n  classification dataset for zero-shot cross-lingual transfer", "abstract": "We introduce MULTI-EURLEX, a new multilingual dataset for topic\nclassification of legal documents. The dataset comprises 65k European Union\n(EU) laws, officially translated in 23 languages, annotated with multiple\nlabels from the EUROVOC taxonomy. We highlight the effect of temporal concept\ndrift and the importance of chronological, instead of random splits. We use the\ndataset as a testbed for zero-shot cross-lingual transfer, where we exploit\nannotated training documents in one language (source) to classify documents in\nanother language (target). We find that fine-tuning a multilingually pretrained\nmodel (XLM-ROBERTA, MT5) in a single source language leads to catastrophic\nforgetting of multilingual knowledge and, consequently, poor zero-shot transfer\nto other languages. Adaptation strategies, namely partial fine-tuning,\nadapters, BITFIT, LNFIT, originally proposed to accelerate fine-tuning for new\nend-tasks, help retain multilingual knowledge from pretraining, substantially\nimproving zero-shot cross-lingual transfer, but their impact also depends on\nthe pretrained model used and the size of the label set.", "published": "2021-09-02 12:52:55", "link": "http://arxiv.org/abs/2109.00904v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Coarse-To-Fine And Cross-Lingual ASR Transfer", "abstract": "End-to-end neural automatic speech recognition systems achieved recently\nstate-of-the-art results, but they require large datasets and extensive\ncomputing resources. Transfer learning has been proposed to overcome these\ndifficulties even across languages, e.g., German ASR trained from an English\nmodel. We experiment with much less related languages, reusing an English model\nfor Czech ASR. To simplify the transfer, we propose to use an intermediate\nalphabet, Czech without accents, and document that it is a highly effective\nstrategy. The technique is also useful on Czech data alone, in the style of\ncoarse-to-fine training. We achieve substantial eductions in training time as\nwell as word error rate (WER).", "published": "2021-09-02 13:16:12", "link": "http://arxiv.org/abs/2109.00916v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LegaLMFiT: Efficient Short Legal Text Classification with LSTM Language\n  Model Pre-Training", "abstract": "Large Transformer-based language models such as BERT have led to broad\nperformance improvements on many NLP tasks. Domain-specific variants of these\nmodels have demonstrated excellent performance on a variety of specialised\ntasks. In legal NLP, BERT-based models have led to new state-of-the-art results\non multiple tasks. The exploration of these models has demonstrated the\nimportance of capturing the specificity of the legal language and its\nvocabulary. However, such approaches suffer from high computational costs,\nleading to a higher ecological impact and lower accessibility. Our findings,\nfocusing on English language legal text, show that lightweight LSTM-based\nLanguage Models are able to capture enough information from a small legal text\npretraining corpus and achieve excellent performance on short legal text\nclassification tasks. This is achieved with a significantly reduced\ncomputational overhead compared to BERT-based models. However, our method also\nshows degraded performance on a more complex task, multi-label classification\nof longer documents, highlighting the limitations of this lightweight approach.", "published": "2021-09-02 14:45:04", "link": "http://arxiv.org/abs/2109.00993v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pre-training Language Model Incorporating Domain-specific Heterogeneous\n  Knowledge into A Unified Representation", "abstract": "Existing technologies expand BERT from different perspectives, e.g. designing\ndifferent pre-training tasks, different semantic granularities, and different\nmodel architectures. Few models consider expanding BERT from different text\nformats. In this paper, we propose a heterogeneous knowledge language model\n(\\textbf{HKLM}), a unified pre-trained language model (PLM) for all forms of\ntext, including unstructured text, semi-structured text, and well-structured\ntext. To capture the corresponding relations among these multi-format\nknowledge, our approach uses masked language model objective to learn word\nknowledge, uses triple classification objective and title matching objective to\nlearn entity knowledge and topic knowledge respectively. To obtain the\naforementioned multi-format text, we construct a corpus in the tourism domain\nand conduct experiments on 5 tourism NLP datasets. The results show that our\napproach outperforms the pre-training of plain text using only 1/4 of the data.\nWe further pre-train the domain-agnostic HKLM and achieve performance gains on\nthe XNLI dataset.", "published": "2021-09-02 16:05:24", "link": "http://arxiv.org/abs/2109.01048v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Skim-Attention: Learning to Focus via Document Layout", "abstract": "Transformer-based pre-training techniques of text and layout have proven\neffective in a number of document understanding tasks. Despite this success,\nmultimodal pre-training models suffer from very high computational and memory\ncosts. Motivated by human reading strategies, this paper presents\nSkim-Attention, a new attention mechanism that takes advantage of the structure\nof the document and its layout. Skim-Attention only attends to the\n2-dimensional position of the words in a document. Our experiments show that\nSkim-Attention obtains a lower perplexity than prior works, while being more\ncomputationally efficient. Skim-Attention can be further combined with\nlong-range Transformers to efficiently process long documents. We also show how\nSkim-Attention can be used off-the-shelf as a mask for any Pre-trained Language\nModel, allowing to improve their performance while restricting attention.\nFinally, we show the emergence of a document structure representation in\nSkim-Attention.", "published": "2021-09-02 16:44:22", "link": "http://arxiv.org/abs/2109.01078v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Suitable Are Subword Segmentation Strategies for Translating\n  Non-Concatenative Morphology?", "abstract": "Data-driven subword segmentation has become the default strategy for\nopen-vocabulary machine translation and other NLP tasks, but may not be\nsufficiently generic for optimal learning of non-concatenative morphology. We\ndesign a test suite to evaluate segmentation strategies on different types of\nmorphological phenomena in a controlled, semi-synthetic setting. In our\nexperiments, we compare how well machine translation models trained on subword-\nand character-level can translate these morphological phenomena. We find that\nlearning to analyse and generate morphologically complex surface\nrepresentations is still challenging, especially for non-concatenative\nmorphological phenomena like reduplication or vowel harmony and for rare word\nstems. Based on our results, we recommend that novel text representation\nstrategies be tested on a range of typologically diverse languages to minimise\nthe risk of adopting a strategy that inadvertently disadvantages certain\nlanguages.", "published": "2021-09-02 17:23:21", "link": "http://arxiv.org/abs/2109.01100v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Assisting Decision Making in Scholarly Peer Review: A Preference\n  Learning Perspective", "abstract": "Peer review is the primary means of quality control in academia; as an\noutcome of a peer review process, program and area chairs make acceptance\ndecisions for each paper based on the review reports and scores they received.\nQuality of scientific work is multi-faceted; coupled with the subjectivity of\nreviewing, this makes final decision making difficult and time-consuming. To\nsupport this final step of peer review, we formalize it as a paper ranking\nproblem. We introduce a novel, multi-faceted generic evaluation framework for\nranking submissions based on peer reviews that takes into account\neffectiveness, efficiency and fairness. We propose a preference learning\nperspective on the task that considers both review texts and scores to\nalleviate the inevitable bias and noise in reviews. Our experiments on peer\nreview data from the ACL 2018 conference demonstrate the superiority of our\npreference-learning-based approach over baselines and prior work, while\nhighlighting the importance of using both review texts and scores to rank\nsubmissions.", "published": "2021-09-02 19:41:47", "link": "http://arxiv.org/abs/2109.01190v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Similarity of Sentence Representations in Multilingual LMs: Resolving\n  Conflicting Literature and Case Study of Baltic Languages", "abstract": "Low-resource languages, such as Baltic languages, benefit from Large\nMultilingual Models (LMs) that possess remarkable cross-lingual transfer\nperformance capabilities. This work is an interpretation and analysis study\ninto cross-lingual representations of Multilingual LMs. Previous works\nhypothesized that these LMs internally project representations of different\nlanguages into a shared cross-lingual space. However, the literature produced\ncontradictory results. In this paper, we revisit the prior work claiming that\n\"BERT is not an Interlingua\" and show that different languages do converge to a\nshared space in such language models with another choice of pooling strategy or\nsimilarity index. Then, we perform cross-lingual representational analysis for\nthe two most popular multilingual LMs employing 378 pairwise language\ncomparisons. We discover that while most languages share joint cross-lingual\nspace, some do not. However, we observe that Baltic languages do belong to that\nshared space. The code is available at https://github.com/TartuNLP/xsim.", "published": "2021-09-02 20:53:14", "link": "http://arxiv.org/abs/2109.01207v4", "categories": ["cs.CL", "I.2.7; I.2.6"], "primary_category": "cs.CL"}
{"title": "Quantifying Reproducibility in NLP and ML", "abstract": "Reproducibility has become an intensely debated topic in NLP and ML over\nrecent years, but no commonly accepted way of assessing reproducibility, let\nalone quantifying it, has so far emerged. The assumption has been that wider\nscientific reproducibility terminology and definitions are not applicable to\nNLP/ML, with the result that many different terms and definitions have been\nproposed, some diametrically opposed. In this paper, we test this assumption,\nby taking the standard terminology and definitions from metrology and applying\nthem directly to NLP/ML. We find that we are able to straightforwardly derive a\npractical framework for assessing reproducibility which has the desirable\nproperty of yielding a quantified degree of reproducibility that is comparable\nacross different reproduction studies.", "published": "2021-09-02 21:00:17", "link": "http://arxiv.org/abs/2109.01211v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do Prompt-Based Models Really Understand the Meaning of their Prompts?", "abstract": "Recently, a boom of papers has shown extraordinary progress in zero-shot and\nfew-shot learning with various prompt-based models. It is commonly argued that\nprompts help models to learn faster in the same way that humans learn faster\nwhen provided with task instructions expressed in natural language. In this\nstudy, we experiment with over 30 prompt templates manually written for natural\nlanguage inference (NLI). We find that models learn just as fast with many\nprompts that are intentionally irrelevant or even pathologically misleading as\nthey do with instructively \"good\" prompts. Further, such patterns hold even for\nmodels as large as 175 billion parameters (Brown et al., 2020) as well as the\nrecently proposed instruction-tuned models which are trained on hundreds of\nprompts (Sanh et al., 2022). That is, instruction-tuned models often produce\ngood predictions with irrelevant and misleading prompts even at zero shots. In\nsum, notwithstanding prompt-based models' impressive improvement, we find\nevidence of serious limitations that question the degree to which such\nimprovement is derived from models understanding task instructions in ways\nanalogous to humans' use of task instructions.", "published": "2021-09-02 23:46:36", "link": "http://arxiv.org/abs/2109.01247v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Causal Inference in Natural Language Processing: Estimation, Prediction,\n  Interpretation and Beyond", "abstract": "A fundamental goal of scientific research is to learn about causal\nrelationships. However, despite its critical role in the life and social\nsciences, causality has not had the same importance in Natural Language\nProcessing (NLP), which has traditionally placed more emphasis on predictive\ntasks. This distinction is beginning to fade, with an emerging area of\ninterdisciplinary research at the convergence of causal inference and language\nprocessing. Still, research on causality in NLP remains scattered across\ndomains without unified definitions, benchmark datasets and clear articulations\nof the challenges and opportunities in the application of causal inference to\nthe textual domain, with its unique properties. In this survey, we consolidate\nresearch across academic areas and situate it in the broader NLP landscape. We\nintroduce the statistical challenge of estimating causal effects with text,\nencompassing settings where text is used as an outcome, treatment, or to\naddress confounding. In addition, we explore potential uses of causal inference\nto improve the robustness, fairness, and interpretability of NLP models. We\nthus provide a unified overview of causal inference for the NLP community.", "published": "2021-09-02 05:40:08", "link": "http://arxiv.org/abs/2109.00725v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ConQX: Semantic Expansion of Spoken Queries for Intent Detection based\n  on Conditioned Text Generation", "abstract": "Intent detection of spoken queries is a challenging task due to their noisy\nstructure and short length. To provide additional information regarding the\nquery and enhance the performance of intent detection, we propose a method for\nsemantic expansion of spoken queries, called ConQX, which utilizes the text\ngeneration ability of an auto-regressive language model, GPT-2. To avoid\noff-topic text generation, we condition the input query to a structured context\nwith prompt mining. We then apply zero-shot, one-shot, and few-shot learning.\nWe lastly use the expanded queries to fine-tune BERT and RoBERTa for intent\ndetection. The experimental results show that the performance of intent\ndetection can be improved by our semantic expansion method.", "published": "2021-09-02 05:57:07", "link": "http://arxiv.org/abs/2109.00729v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Imposing Relation Structure in Language-Model Embeddings Using\n  Contrastive Learning", "abstract": "Though language model text embeddings have revolutionized NLP research, their\nability to capture high-level semantic information, such as relations between\nentities in text, is limited. In this paper, we propose a novel contrastive\nlearning framework that trains sentence embeddings to encode the relations in a\ngraph structure. Given a sentence (unstructured text) and its graph, we use\ncontrastive learning to impose relation-related structure on the token-level\nrepresentations of the sentence obtained with a CharacterBERT (El Boukkouri et\nal.,2020) model. The resulting relation-aware sentence embeddings achieve\nstate-of-the-art results on the relation extraction task using only a simple\nKNN classifier, thereby demonstrating the success of the proposed method.\nAdditional visualization by a tSNE analysis shows the effectiveness of the\nlearned representation space compared to baselines. Furthermore, we show that\nwe can learn a different space for named entity recognition, again using a\ncontrastive learning objective, and demonstrate how to successfully combine\nboth representation spaces in an entity-relation task.", "published": "2021-09-02 10:58:27", "link": "http://arxiv.org/abs/2109.00840v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for\n  Code Understanding and Generation", "abstract": "Pre-trained models for Natural Languages (NL) like BERT and GPT have been\nrecently shown to transfer well to Programming Languages (PL) and largely\nbenefit a broad set of code-related tasks. Despite their success, most current\nmethods either rely on an encoder-only (or decoder-only) pre-training that is\nsuboptimal for generation (resp. understanding) tasks or process the code\nsnippet in the same way as NL, neglecting the special characteristics of PL\nsuch as token types. We present CodeT5, a unified pre-trained encoder-decoder\nTransformer model that better leverages the code semantics conveyed from the\ndeveloper-assigned identifiers. Our model employs a unified framework to\nseamlessly support both code understanding and generation tasks and allows for\nmulti-task learning. Besides, we propose a novel identifier-aware pre-training\ntask that enables the model to distinguish which code tokens are identifiers\nand to recover them when they are masked. Furthermore, we propose to exploit\nthe user-written code comments with a bimodal dual generation task for better\nNL-PL alignment. Comprehensive experiments show that CodeT5 significantly\noutperforms prior methods on understanding tasks such as code defect detection\nand clone detection, and generation tasks across various directions including\nPL-NL, NL-PL, and PL-PL. Further analysis reveals that our model can better\ncapture semantic information from code. Our code and pre-trained models are\nreleased at https: //github.com/salesforce/CodeT5 .", "published": "2021-09-02 12:21:06", "link": "http://arxiv.org/abs/2109.00859v1", "categories": ["cs.CL", "cs.PL"], "primary_category": "cs.CL"}
{"title": "Coordinating Narratives and the Capitol Riots on Parler", "abstract": "Coordinated disinformation campaigns are used to influence social media\nusers, potentially leading to offline violence. In this study, we introduce a\ngeneral methodology to uncover coordinated messaging through analysis of user\nparleys on Parler. The proposed method constructs a user-to-user coordination\nnetwork graph induced by a user-to-text graph and a text-to-text similarity\ngraph. The text-to-text graph is constructed based on the textual similarity of\nParler posts. We study three influential groups of users in the 6 January 2020\nCapitol riots and detect networks of coordinated user clusters that are all\nposting similar textual content in support of different disinformation\nnarratives related to the U.S. 2020 elections.", "published": "2021-09-02 13:44:59", "link": "http://arxiv.org/abs/2109.00945v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Sequence-to-Sequence Learning with Latent Neural Grammars", "abstract": "Sequence-to-sequence learning with neural networks has become the de facto\nstandard for sequence prediction tasks. This approach typically models the\nlocal distribution over the next word with a powerful neural network that can\ncondition on arbitrary context. While flexible and performant, these models\noften require large datasets for training and can fail spectacularly on\nbenchmarks designed to test for compositional generalization. This work\nexplores an alternative, hierarchical approach to sequence-to-sequence learning\nwith quasi-synchronous grammars, where each node in the target tree is\ntransduced by a node in the source tree. Both the source and target trees are\ntreated as latent and induced during training. We develop a neural\nparameterization of the grammar which enables parameter sharing over the\ncombinatorial space of derivation rules without the need for manual feature\nengineering. We apply this latent neural grammar to various domains -- a\ndiagnostic language navigation task designed to test for compositional\ngeneralization (SCAN), style transfer, and small-scale machine translation --\nand find that it performs respectably compared to standard baselines.", "published": "2021-09-02 17:58:08", "link": "http://arxiv.org/abs/2109.01135v7", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Challenges in Generalization in Open Domain Question Answering", "abstract": "Recent work on Open Domain Question Answering has shown that there is a large\ndiscrepancy in model performance between novel test questions and those that\nlargely overlap with training questions. However, it is unclear which aspects\nof novel questions make them challenging. Drawing upon studies on systematic\ngeneralization, we introduce and annotate questions according to three\ncategories that measure different levels and kinds of generalization: training\nset overlap, compositional generalization (comp-gen), and novel-entity\ngeneralization (novel-entity). When evaluating six popular parametric and\nnon-parametric models, we find that for the established Natural Questions and\nTriviaQA datasets, even the strongest model performance for\ncomp-gen/novel-entity is 13.1/5.4% and 9.6/1.5% lower compared to that for the\nfull test set -- indicating the challenge posed by these types of questions.\nFurthermore, we show that whilst non-parametric models can handle questions\ncontaining novel entities relatively well, they struggle with those requiring\ncompositional generalization. Lastly, we find that key question difficulty\nfactors are: cascading errors from the retrieval component, frequency of\nquestion pattern, and frequency of the entity.", "published": "2021-09-02 18:04:10", "link": "http://arxiv.org/abs/2109.01156v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multimodal Conditionality for Natural Language Generation", "abstract": "Large scale pretrained language models have demonstrated state-of-the-art\nperformance in language understanding tasks. Their application has recently\nexpanded into multimodality learning, leading to improved representations\ncombining vision and language. However, progress in adapting language models\ntowards conditional Natural Language Generation (NLG) has been limited to a\nsingle modality, generally text. We propose MAnTiS, Multimodal Adaptation for\nText Synthesis, a general approach for multimodal conditionality in\ntransformer-based NLG models. In this method, we pass inputs from each modality\nthrough modality-specific encoders, project to textual token space, and finally\njoin to form a conditionality prefix. We fine-tune the pretrained language\nmodel and encoders with the conditionality prefix guiding the generation. We\napply MAnTiS to the task of product description generation, conditioning a\nnetwork on both product images and titles to generate descriptive text. We\ndemonstrate that MAnTiS outperforms strong baseline approaches on standard NLG\nscoring metrics. Furthermore, qualitative assessments demonstrate that MAnTiS\ncan generate human quality descriptions consistent with given multimodal\ninputs.", "published": "2021-09-02 22:06:07", "link": "http://arxiv.org/abs/2109.01229v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Empirical Study on Leveraging Position Embeddings for Target-oriented\n  Opinion Words Extraction", "abstract": "Target-oriented opinion words extraction (TOWE) (Fan et al., 2019b) is a new\nsubtask of target-oriented sentiment analysis that aims to extract opinion\nwords for a given aspect in text. Current state-of-the-art methods leverage\nposition embeddings to capture the relative position of a word to the target.\nHowever, the performance of these methods depends on the ability to incorporate\nthis information into word representations. In this paper, we explore a variety\nof text encoders based on pretrained word embeddings or language models that\nleverage part-of-speech and position embeddings, aiming to examine the actual\ncontribution of each component in TOWE. We also adapt a graph convolutional\nnetwork (GCN) to enhance word representations by incorporating syntactic\ninformation. Our experimental results demonstrate that BiLSTM-based models can\neffectively encode position information into word representations while using a\nGCN only achieves marginal gains. Interestingly, our simple methods outperform\nseveral state-of-the-art complex neural structures.", "published": "2021-09-02 22:49:45", "link": "http://arxiv.org/abs/2109.01238v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Entity Linking and Discovery via Arborescence-based Supervised\n  Clustering", "abstract": "Previous work has shown promising results in performing entity linking by\nmeasuring not only the affinities between mentions and entities but also those\namongst mentions. In this paper, we present novel training and inference\nprocedures that fully utilize mention-to-mention affinities by building minimum\narborescences (i.e., directed spanning trees) over mentions and entities across\ndocuments in order to make linking decisions. We also show that this method\ngracefully extends to entity discovery, enabling the clustering of mentions\nthat do not have an associated entity in the knowledge base. We evaluate our\napproach on the Zero-Shot Entity Linking dataset and MedMentions, the largest\npublicly available biomedical dataset, and show significant improvements in\nperformance for both entity linking and discovery compared to identically\nparameterized models. We further show significant efficiency improvements with\nonly a small loss in accuracy over previous work, which use more\ncomputationally expensive models.", "published": "2021-09-02 23:05:58", "link": "http://arxiv.org/abs/2109.01242v2", "categories": ["cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Combining Transformers with Natural Language Explanations", "abstract": "Many NLP applications require models to be interpretable. However, many\nsuccessful neural architectures, including transformers, still lack effective\ninterpretation methods. A possible solution could rely on building explanations\nfrom domain knowledge, which is often available as plain, natural language\ntext. We thus propose an extension to transformer models that makes use of\nexternal memories to store natural language explanations and use them to\nexplain classification outputs. We conduct an experimental evaluation on two\ndomains, legal text analysis and argument mining, to show that our approach can\nproduce relevant explanations while retaining or even improving classification\nperformance.", "published": "2021-09-02 09:17:04", "link": "http://arxiv.org/abs/2110.00125v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Text Classification for Predicting Multi-level Product Categories", "abstract": "In an online shopping platform, a detailed classification of the products\nfacilitates user navigation. It also helps online retailers keep track of the\nprice fluctuations in a certain industry or special discounts on a specific\nproduct category. Moreover, an automated classification system may help to\npinpoint incorrect or subjective categories suggested by an operator. In this\nstudy, we focus on product title classification of the grocery products. We\nperform a comprehensive comparison of six different text classification models\nto establish a strong baseline for this task, which involves testing both\ntraditional and recent machine learning methods. In our experiments, we\ninvestigate the generalizability of the trained models to the products of other\nonline retailers, the dynamic masking of infeasible subcategories for\npretrained language models, and the benefits of incorporating product titles in\nmultiple languages. Our numerical results indicate that dynamic masking of\nsubcategories is effective in improving prediction accuracy. In addition, we\nobserve that using bilingual product titles is generally beneficial, and neural\nnetwork-based models perform significantly better than SVM and XGBoost models.\nLastly, we investigate the reasons for the misclassified products and propose\nfuture research directions to further enhance the prediction models.", "published": "2021-09-02 17:00:05", "link": "http://arxiv.org/abs/2109.01084v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Tree-Constrained Graph Neural Networks For Argument Mining", "abstract": "We propose a novel architecture for Graph Neural Networks that is inspired by\nthe idea behind Tree Kernels of measuring similarity between trees by taking\ninto account their common substructures, named fragments. By imposing a series\nof regularization constraints to the learning problem, we exploit a pooling\nmechanism that incorporates such notion of fragments within the node soft\nassignment function that produces the embeddings. We present an extensive\nexperimental evaluation on a collection of sentence classification tasks\nconducted on several argument mining corpora, showing that the proposed\napproach performs well with respect to state-of-the-art techniques.", "published": "2021-09-02 08:56:25", "link": "http://arxiv.org/abs/2110.00124v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Interactively Providing Explanations for Transformer Language Models", "abstract": "Transformer language models are state of the art in a multitude of NLP tasks.\nDespite these successes, their opaqueness remains problematic. Recent methods\naiming to provide interpretability and explainability to black-box models\nprimarily focus on post-hoc explanations of (sometimes spurious) input-output\ncorrelations. Instead, we emphasize using prototype networks directly\nincorporated into the model architecture and hence explain the reasoning\nprocess behind the network's decisions. Our architecture performs on par with\nseveral language models and, moreover, enables learning from user interactions.\nThis not only offers a better understanding of language models but uses human\ncapabilities to incorporate knowledge outside of the rigid range of purely\ndata-driven approaches.", "published": "2021-09-02 11:34:29", "link": "http://arxiv.org/abs/2110.02058v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "So Cloze yet so Far: N400 Amplitude is Better Predicted by\n  Distributional Information than Human Predictability Judgements", "abstract": "More predictable words are easier to process - they are read faster and\nelicit smaller neural signals associated with processing difficulty, most\nnotably, the N400 component of the event-related brain potential. Thus, it has\nbeen argued that prediction of upcoming words is a key component of language\ncomprehension, and that studying the amplitude of the N400 is a valuable way to\ninvestigate the predictions we make. In this study, we investigate whether the\nlinguistic predictions of computational language models or humans better\nreflect the way in which natural language stimuli modulate the amplitude of the\nN400. One important difference in the linguistic predictions of humans versus\ncomputational language models is that while language models base their\npredictions exclusively on the preceding linguistic context, humans may rely on\nother factors. We find that the predictions of three top-of-the-line\ncontemporary language models - GPT-3, RoBERTa, and ALBERT - match the N400 more\nclosely than human predictions. This suggests that the predictive processes\nunderlying the N400 may be more sensitive to the surface-level statistics of\nlanguage than previously thought.", "published": "2021-09-02 22:00:10", "link": "http://arxiv.org/abs/2109.01226v4", "categories": ["cs.CL", "cs.AI", "cs.IT", "cs.LG", "math.IT"], "primary_category": "cs.CL"}
{"title": "Multichannel Audio Source Separation with Independent Deeply Learned\n  Matrix Analysis Using Product of Source Models", "abstract": "Independent deeply learned matrix analysis (IDLMA) is one of the\nstate-of-the-art multichannel audio source separation methods using the source\npower estimation based on deep neural networks (DNNs). The DNN-based power\nestimation works well for sounds having timbres similar to the DNN training\ndata. However, the sounds to which IDLMA is applied do not always have such\ntimbres, and the timbral mismatch causes the performance degradation of IDLMA.\nTo tackle this problem, we focus on a blind source separation counterpart of\nIDLMA, independent low-rank matrix analysis. It uses nonnegative matrix\nfactorization (NMF) as the source model, which can capture source spectral\ncomponents that only appear in the target mixture, using the low-rank structure\nof the source spectrogram as a clue. We thus extend the DNN-based source model\nto encompass the NMF-based source model on the basis of the product-of-expert\nconcept, which we call the product of source models (PoSM). For the proposed\nPoSM-based IDLMA, we derive a computationally efficient parameter estimation\nalgorithm based on an optimization principle called the\nmajorization-minimization algorithm. Experimental evaluations show the\neffectiveness of the proposed method.", "published": "2021-09-02 04:31:15", "link": "http://arxiv.org/abs/2109.00704v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Binaural Audio Generation via Multi-task Learning", "abstract": "We present a learning-based approach for generating binaural audio from mono\naudio using multi-task learning. Our formulation leverages additional\ninformation from two related tasks: the binaural audio generation task and the\nflipped audio classification task. Our learning model extracts spatialization\nfeatures from the visual and audio input, predicts the left and right audio\nchannels, and judges whether the left and right channels are flipped. First, we\nextract visual features using ResNet from the video frames. Next, we perform\nbinaural audio generation and flipped audio classification using separate\nsubnetworks based on visual features. Our learning method optimizes the overall\nloss based on the weighted sum of the losses of the two tasks. We train and\nevaluate our model on the FAIR-Play dataset and the YouTube-ASMR dataset. We\nperform quantitative and qualitative evaluations to demonstrate the benefits of\nour approach over prior techniques.", "published": "2021-09-02 07:04:56", "link": "http://arxiv.org/abs/2109.00748v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Controllable deep melody generation via hierarchical music structure\n  representation", "abstract": "Recent advances in deep learning have expanded possibilities to generate\nmusic, but generating a customizable full piece of music with consistent\nlong-term structure remains a challenge. This paper introduces MusicFrameworks,\na hierarchical music structure representation and a multi-step generative\nprocess to create a full-length melody guided by long-term repetitive\nstructure, chord, melodic contour, and rhythm constraints. We first organize\nthe full melody with section and phrase-level structure. To generate melody in\neach phrase, we generate rhythm and basic melody using two separate\ntransformer-based networks, and then generate the melody conditioned on the\nbasic melody, rhythm and chords in an auto-regressive manner. By factoring\nmusic generation into sub-problems, our approach allows simpler models and\nrequires less data. To customize or add variety, one can alter chords, basic\nmelody, and rhythm structure in the music frameworks, letting our networks\ngenerate the melody accordingly. Additionally, we introduce new features to\nencode musical positional information, rhythm patterns, and melodic contours\nbased on musical domain knowledge. A listening test reveals that melodies\ngenerated by our method are rated as good as or better than human-composed\nmusic in the POP909 dataset about half the time.", "published": "2021-09-02 01:31:14", "link": "http://arxiv.org/abs/2109.00663v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
