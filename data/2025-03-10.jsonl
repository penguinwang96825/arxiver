{"title": "Crowdsource, Crawl, or Generate? Creating SEA-VL, a Multicultural Vision-Language Dataset for Southeast Asia", "abstract": "Southeast Asia (SEA) is a region of extraordinary linguistic and cultural\ndiversity, yet it remains significantly underrepresented in vision-language\n(VL) research. This often results in artificial intelligence (AI) models that\nfail to capture SEA cultural nuances. To fill this gap, we present SEA-VL, an\nopen-source initiative dedicated to developing high-quality, culturally\nrelevant data for SEA languages. By involving contributors from SEA countries,\nSEA-VL aims to ensure better cultural relevance and diversity, fostering\ngreater inclusivity of underrepresented languages in VL research. Beyond\ncrowdsourcing, our initiative goes one step further in the exploration of the\nautomatic collection of culturally relevant images through crawling and image\ngeneration. First, we find that image crawling achieves approximately ~85%\ncultural relevance while being more cost- and time-efficient than\ncrowdsourcing. Second, despite the substantial progress in generative vision\nmodels, synthetic images remain unreliable in accurately reflecting SEA\ncultures. The generated images often fail to reflect the nuanced traditions and\ncultural contexts of the region. Collectively, we gather 1.28M SEA\nculturally-relevant images, more than 50 times larger than other existing\ndatasets. Through SEA-VL, we aim to bridge the representation gap in SEA,\nfostering the development of more inclusive AI systems that authentically\nrepresent diverse cultures across SEA.", "published": "2025-03-10 23:54:52", "link": "http://arxiv.org/abs/2503.07920v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "BEARCUBS: A benchmark for computer-using web agents", "abstract": "Modern web agents possess computer use abilities that allow them to interact\nwith webpages by sending commands to a virtual keyboard and mouse. While such\nagents have considerable potential to assist human users with complex tasks,\nevaluating their capabilities in real-world settings poses a major challenge.\nTo this end, we introduce BEARCUBS, a \"small but mighty\" benchmark of 111\ninformation-seeking questions designed to evaluate a web agent's ability to\nsearch, browse, and identify factual information from the web. Unlike prior web\nagent benchmarks, solving BEARCUBS requires (1) accessing live web content\nrather than synthetic or simulated pages, which captures the unpredictability\nof real-world web interactions; and (2) performing a broad range of multimodal\ninteractions (e.g., video understanding, 3D navigation) that cannot be bypassed\nvia text-based workarounds. Each question in BEARCUBS has a corresponding\nshort, unambiguous answer and a human-validated browsing trajectory, allowing\nfor transparent evaluation of agent performance and strategies. A human study\nconfirms that BEARCUBS questions are solvable but non-trivial (84.7% human\naccuracy), revealing search inefficiencies and domain knowledge gaps as common\nfailure points. By contrast, state-of-the-art computer-using agents\nunderperform, with the best-scoring system (OpenAI's Operator) reaching only\n24.3% accuracy. These results highlight critical areas for improvement,\nincluding reliable source selection and more powerful multimodal capabilities.\nTo facilitate future research, BEARCUBS will be updated periodically to replace\ninvalid or contaminated questions, keeping the benchmark fresh for future\ngenerations of web agents.", "published": "2025-03-10 23:50:30", "link": "http://arxiv.org/abs/2503.07919v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Fine-Tuning LLMs for Report Summarization: Analysis on Supervised and Unsupervised Data", "abstract": "We study the efficacy of fine-tuning Large Language Models (LLMs) for the\nspecific task of report (government archives, news, intelligence reports)\nsummarization. While this topic is being very actively researched - our\nspecific application set-up faces two challenges: (i) ground-truth summaries\nmaybe unavailable (e.g., for government archives), and (ii) availability of\nlimited compute power - the sensitive nature of the application requires that\ncomputation is performed on-premise and for most of our experiments we use one\nor two A100 GPU cards. Under this set-up we conduct experiments to answer the\nfollowing questions. First, given that fine-tuning the LLMs can be resource\nintensive, is it feasible to fine-tune them for improved report summarization\ncapabilities on-premise? Second, what are the metrics we could leverage to\nassess the quality of these summaries? We conduct experiments on two different\nfine-tuning approaches in parallel and our findings reveal interesting trends\nregarding the utility of fine-tuning LLMs. Specifically, we find that in many\ncases, fine-tuning helps improve summary quality and in other cases it helps by\nreducing the number of invalid or garbage summaries.", "published": "2025-03-10 23:47:11", "link": "http://arxiv.org/abs/2503.10676v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Demystifying the Accuracy-Interpretability Trade-Off: A Case Study of Inferring Ratings from Reviews", "abstract": "Interpretable machine learning models offer understandable reasoning behind\ntheir decision-making process, though they may not always match the performance\nof their black-box counterparts. This trade-off between interpretability and\nmodel performance has sparked discussions around the deployment of AI,\nparticularly in critical applications where knowing the rationale of\ndecision-making is essential for trust and accountability. In this study, we\nconduct a comparative analysis of several black-box and interpretable models,\nfocusing on a specific NLP use case that has received limited attention:\ninferring ratings from reviews. Through this use case, we explore the intricate\nrelationship between the performance and interpretability of different models.\nWe introduce a quantitative score called Composite Interpretability (CI) to\nhelp visualize the trade-off between interpretability and performance,\nparticularly in the case of composite models. Our results indicate that, in\ngeneral, the learning performance improves as interpretability decreases, but\nthis relationship is not strictly monotonic, and there are instances where\ninterpretable models are more advantageous.", "published": "2025-03-10 23:17:46", "link": "http://arxiv.org/abs/2503.07914v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Can Memory-Augmented Language Models Generalize on Reasoning-in-a-Haystack Tasks?", "abstract": "Large language models often expose their brittleness in reasoning tasks,\nespecially while executing long chains of reasoning over context. We propose\nMemReasoner, a new and simple memory-augmented LLM architecture, in which the\nmemory learns the relative order of facts in context, and enables hopping over\nthem, while the decoder selectively attends to the memory. MemReasoner is\ntrained end-to-end, with optional supporting fact supervision of varying\ndegrees. We train MemReasoner, along with existing memory-augmented transformer\nmodels and a state-space model, on two distinct synthetic multi-hop reasoning\ntasks. Experiments performed under a variety of challenging scenarios,\nincluding the presence of long distractor text or target answer changes in test\nset, show strong generalization of MemReasoner on both single- and two-hop\ntasks. This generalization of MemReasoner is achieved using none-to-weak\nsupporting fact supervision (using none and 1\\% of supporting facts for one-\nand two-hop tasks, respectively). In contrast, baseline models overall struggle\nto generalize and benefit far less from using full supporting fact supervision.\nThe results highlight the importance of explicit memory mechanisms, combined\nwith additional weak supervision, for improving large language model's context\nprocessing ability toward reasoning tasks.", "published": "2025-03-10 22:48:53", "link": "http://arxiv.org/abs/2503.07903v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Gemini Embedding: Generalizable Embeddings from Gemini", "abstract": "In this report, we introduce Gemini Embedding, a state-of-the-art embedding\nmodel leveraging the power of Gemini, Google's most capable large language\nmodel. Capitalizing on Gemini's inherent multilingual and code understanding\ncapabilities, Gemini Embedding produces highly generalizable embeddings for\ntext spanning numerous languages and textual modalities. The representations\ngenerated by Gemini Embedding can be precomputed and applied to a variety of\ndownstream tasks including classification, similarity, clustering, ranking, and\nretrieval. Evaluated on the Massive Multilingual Text Embedding Benchmark\n(MMTEB), which includes over one hundred tasks across 250+ languages, Gemini\nEmbedding substantially outperforms prior state-of-the-art models,\ndemonstrating considerable improvements in embedding quality. Achieving\nstate-of-the-art performance across MMTEB's multilingual, English, and code\nbenchmarks, our unified model demonstrates strong capabilities across a broad\nselection of tasks and surpasses specialized domain-specific models.", "published": "2025-03-10 22:16:45", "link": "http://arxiv.org/abs/2503.07891v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Datasets, Documents, and Repetitions: The Practicalities of Unequal Data Quality", "abstract": "Data filtering has become a powerful tool for improving model performance\nwhile reducing computational cost. However, as large language model compute\nbudgets continue to grow, the limited data volume provided by heavily filtered\nand deduplicated datasets will become a practical constraint. In efforts to\nbetter understand how to proceed, we study model performance at various compute\nbudgets and across multiple pre-training datasets created through data\nfiltering and deduplication. We find that, given appropriate modifications to\nthe training recipe, repeating existing aggressively filtered datasets for up\nto ten epochs can outperform training on the ten times larger superset for a\nsingle epoch across multiple compute budget orders of magnitude. While this\nfinding relies on repeating the dataset for many epochs, we also investigate\nrepeats within these datasets at the document level. We find that not all\ndocuments within a dataset are equal, and we can create better datasets\nrelative to a token budget by explicitly manipulating the counts of individual\ndocuments. We conclude by arguing that even as large language models scale,\ndata filtering remains an important direction of research.", "published": "2025-03-10 21:51:17", "link": "http://arxiv.org/abs/2503.07879v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MapQA: Open-domain Geospatial Question Answering on Map Data", "abstract": "Geospatial question answering (QA) is a fundamental task in navigation and\npoint of interest (POI) searches. While existing geospatial QA datasets exist,\nthey are limited in both scale and diversity, often relying solely on textual\ndescriptions of geo-entities without considering their geometries. A major\nchallenge in scaling geospatial QA datasets for reasoning lies in the\ncomplexity of geospatial relationships, which require integrating spatial\nstructures, topological dependencies, and multi-hop reasoning capabilities that\nmost text-based QA datasets lack. To address these limitations, we introduce\nMapQA, a novel dataset that not only provides question-answer pairs but also\nincludes the geometries of geo-entities referenced in the questions. MapQA is\nconstructed using SQL query templates to extract question-answer pairs from\nOpenStreetMap (OSM) for two study regions: Southern California and Illinois. It\nconsists of 3,154 QA pairs spanning nine question types that require geospatial\nreasoning, such as neighborhood inference and geo-entity type identification.\nCompared to existing datasets, MapQA expands both the number and diversity of\ngeospatial question types. We explore two approaches to tackle this challenge:\n(1) a retrieval-based language model that ranks candidate geo-entities by\nembedding similarity, and (2) a large language model (LLM) that generates SQL\nqueries from natural language questions and geo-entity attributes, which are\nthen executed against an OSM database. Our findings indicate that\nretrieval-based methods effectively capture concepts like closeness and\ndirection but struggle with questions that require explicit computations (e.g.,\ndistance calculations). LLMs (e.g., GPT and Gemini) excel at generating SQL\nqueries for one-hop reasoning but face challenges with multi-hop reasoning,\nhighlighting a key bottleneck in advancing geospatial QA systems.", "published": "2025-03-10 21:37:22", "link": "http://arxiv.org/abs/2503.07871v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "cantnlp@DravidianLangTech2025: A Bag-of-Sounds Approach to Multimodal Hate Speech Detection", "abstract": "This paper presents the systems and results for the Multimodal Social Media\nData Analysis in Dravidian Languages (MSMDA-DL) shared task at the Fifth\nWorkshop on Speech, Vision, and Language Technologies for Dravidian Languages\n(DravidianLangTech-2025). We took a `bag-of-sounds' approach by training our\nhate speech detection system on the speech (audio) data using transformed Mel\nspectrogram measures. While our candidate model performed poorly on the test\nset, our approach offered promising results during training and development for\nMalayalam and Tamil. With sufficient and well-balanced training data, our\nresults show that it is feasible to use both text and speech (audio) data in\nthe development of multimodal hate speech detection systems.", "published": "2025-03-10 21:21:13", "link": "http://arxiv.org/abs/2503.07862v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HalluVerse25: Fine-grained Multilingual Benchmark Dataset for LLM Hallucinations", "abstract": "Large Language Models (LLMs) are increasingly used in various contexts, yet\nremain prone to generating non-factual content, commonly referred to as\n\"hallucinations\". The literature categorizes hallucinations into several types,\nincluding entity-level, relation-level, and sentence-level hallucinations.\nHowever, existing hallucination datasets often fail to capture fine-grained\nhallucinations in multilingual settings. In this work, we introduce\nHalluVerse25, a multilingual LLM hallucination dataset that categorizes\nfine-grained hallucinations in English, Arabic, and Turkish. Our dataset\nconstruction pipeline uses an LLM to inject hallucinations into factual\nbiographical sentences, followed by a rigorous human annotation process to\nensure data quality. We evaluate several LLMs on HalluVerse25, providing\nvaluable insights into how proprietary models perform in detecting\nLLM-generated hallucinations across different contexts.", "published": "2025-03-10 20:24:07", "link": "http://arxiv.org/abs/2503.07833v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RefactorBench: Evaluating Stateful Reasoning in Language Agents Through Code", "abstract": "Recent advances in language model (LM) agents and function calling have\nenabled autonomous, feedback-driven systems to solve problems across various\ndigital domains. To better understand the unique limitations of LM agents, we\nintroduce RefactorBench, a benchmark consisting of 100 large handcrafted\nmulti-file refactoring tasks in popular open-source repositories. Solving tasks\nwithin RefactorBench requires thorough exploration of dependencies across\nmultiple files and strong adherence to relevant instructions. Every task is\ndefined by 3 natural language instructions of varying specificity and is\nmutually exclusive, allowing for the creation of longer combined tasks on the\nsame repository. Baselines on RefactorBench reveal that current LM agents\nstruggle with simple compositional tasks, solving only 22% of tasks with base\ninstructions, in contrast to a human developer with short time constraints\nsolving 87%. Through trajectory analysis, we identify various unique failure\nmodes of LM agents, and further explore the failure mode of tracking past\nactions. By adapting a baseline agent to condition on representations of state,\nwe achieve a 43.9% improvement in solving RefactorBench tasks. We further\nextend our state-aware approach to encompass entire digital environments and\noutline potential directions for future research. RefactorBench aims to support\nthe study of LM agents by providing a set of real-world, multi-hop tasks within\nthe realm of code.", "published": "2025-03-10 20:23:24", "link": "http://arxiv.org/abs/2503.07832v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.SE", "I.2.5"], "primary_category": "cs.AI"}
{"title": "Modern Models, Medieval Texts: A POS Tagging Study of Old Occitan", "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in\nnatural language processing, yet their effectiveness in handling historical\nlanguages remains largely unexplored. This study examines the performance of\nopen-source LLMs in part-of-speech (POS) tagging for Old Occitan, a historical\nlanguage characterized by non-standardized orthography and significant\ndiachronic variation. Through comparative analysis of two distinct\ncorpora-hagiographical and medical texts-we evaluate how current models handle\nthe inherent challenges of processing a low-resource historical language. Our\nfindings demonstrate critical limitations in LLM performance when confronted\nwith extreme orthographic and syntactic variability. We provide detailed error\nanalysis and specific recommendations for improving model performance in\nhistorical language processing. This research advances our understanding of LLM\ncapabilities in challenging linguistic contexts while offering practical\ninsights for both computational linguistics and historical language studies.", "published": "2025-03-10 20:16:01", "link": "http://arxiv.org/abs/2503.07827v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Magnet: Multi-turn Tool-use Data Synthesis and Distillation via Graph Translation", "abstract": "Large language models (LLMs) have exhibited the ability to effectively\nutilize external tools to address user queries. However, their performance may\nbe limited in complex, multi-turn interactions involving users and multiple\ntools. To address this, we propose Magnet, a principled framework for\nsynthesizing high-quality training trajectories to enhance the function calling\ncapability of large language model agents in multi-turn conversations with\nhumans. The framework is based on automatic and iterative translations from a\nfunction signature path to a sequence of queries and executable function calls.\nWe model the complicated function interactions in multi-turn cases with graph\nand design novel node operations to build reliable signature paths. Motivated\nby context distillation, when guiding the generation of positive and negative\ntrajectories using a teacher model, we provide reference function call\nsequences as positive hints in context and contrastive, incorrect function\ncalls as negative hints. Experiments show that training with the positive\ntrajectories with supervised fine-tuning and preference optimization against\nnegative trajectories, our 14B model, Magnet-14B-mDPO, obtains 68.01 on BFCL-v3\nand 73.30 on ToolQuery, surpassing the performance of the teacher model\nGemini-1.5-pro-002 by a large margin in function calling.", "published": "2025-03-10 20:13:07", "link": "http://arxiv.org/abs/2503.07826v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Training Domain Draft Models for Speculative Decoding: Best Practices and Insights", "abstract": "Speculative decoding is an effective method for accelerating inference of\nlarge language models (LLMs) by employing a small draft model to predict the\noutput of a target model. However, when adapting speculative decoding to\ndomain-specific target models, the acceptance rate of the generic draft model\ndrops significantly due to domain shift. In this work, we systematically\ninvestigate knowledge distillation techniques for training domain draft models\nto improve their speculation accuracy. We compare white-box and black-box\ndistillation approaches and explore their effectiveness in various data\naccessibility scenarios, including historical user queries, curated domain\ndata, and synthetically generated alignment data. Our experiments across\nFunction Calling, Biology, and Chinese domains show that offline distillation\nconsistently outperforms online distillation by 11% to 25%, white-box\ndistillation surpasses black-box distillation by 2% to 10%, and data scaling\ntrends hold across domains. Additionally, we find that synthetic data can\neffectively align draft models and achieve 80% to 93% of the performance of\ntraining on historical user queries. These findings provide practical\nguidelines for training domain-specific draft models to improve speculative\ndecoding efficiency.", "published": "2025-03-10 19:40:25", "link": "http://arxiv.org/abs/2503.07807v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Large Language Models that Benefit for All: Benchmarking Group Fairness in Reward Models", "abstract": "As Large Language Models (LLMs) become increasingly powerful and accessible\nto human users, ensuring fairness across diverse demographic groups, i.e.,\ngroup fairness, is a critical ethical concern. However, current fairness and\nbias research in LLMs is limited in two aspects. First, compared to traditional\ngroup fairness in machine learning classification, it requires that the\nnon-sensitive attributes, in this case, the prompt questions, be the same\nacross different groups. In many practical scenarios, different groups,\nhowever, may prefer different prompt questions and this requirement becomes\nimpractical. Second, it evaluates group fairness only for the LLM's final\noutput without identifying the source of possible bias. Namely, the bias in\nLLM's output can result from both the pretraining and the finetuning. For\nfinetuning, the bias can result from both the RLHF procedure and the learned\nreward model. Arguably, evaluating the group fairness of each component in the\nLLM pipeline could help develop better methods to mitigate the possible bias.\nRecognizing those two limitations, this work benchmarks the group fairness of\nlearned reward models. By using expert-written text from arXiv, we are able to\nbenchmark the group fairness of reward models without requiring the same prompt\nquestions across different demographic groups. Surprisingly, our results\ndemonstrate that all the evaluated reward models (e.g., Nemotron-4-340B-Reward,\nArmoRM-Llama3-8B-v0.1, and GRM-llama3-8B-sftreg) exhibit statistically\nsignificant group unfairness. We also observed that top-performing reward\nmodels (w.r.t. canonical performance metrics) tend to demonstrate better group\nfairness.", "published": "2025-03-10 19:39:39", "link": "http://arxiv.org/abs/2503.07806v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Beyond One-Size-Fits-All Summarization: Customizing Summaries for Diverse Users", "abstract": "In recent years, automatic text summarization has witnessed significant\nadvancement, particularly with the development of transformer-based models.\nHowever, the challenge of controlling the readability level of generated\nsummaries remains an under-explored area, especially for languages with complex\nlinguistic features like Turkish. This gap has the effect of impeding effective\ncommunication and also limits the accessibility of information. Controlling\nreadability of textual data is an important element for creating summaries for\ndifferent audiences with varying literacy and education levels, such as\nstudents ranging from primary school to graduate level, as well as individuals\nwith diverse educational backgrounds. Summaries that align with the needs of\nspecific reader groups can improve comprehension and engagement, ensuring that\nthe intended message is effectively communicated. Furthermore, readability\nadjustment is essential to expand the usability of summarization models in\neducational and professional domains. Current summarization models often don't\nhave the mechanisms to adjust the complexity of their outputs, resulting in\nsummaries that may be too simplistic or overly complex for certain types of\nreader groups. Developing adaptive models that can tailor content to specific\nreadability levels is therefore crucial. To address this problem, we create our\nown custom dataset and train a model with our custom architecture. Our method\nensures that readability levels are effectively controlled while maintaining\naccuracy and coherence. We rigorously compare our model to a supervised\nfine-tuned baseline, demonstrating its superiority in generating\nreadability-aware summaries.", "published": "2025-03-10 19:08:36", "link": "http://arxiv.org/abs/2503.10675v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing Retrieval for ESGLLM via ESG-CID -- A Disclosure Content Index Finetuning Dataset for Mapping GRI and ESRS", "abstract": "Climate change has intensified the need for transparency and accountability\nin organizational practices, making Environmental, Social, and Governance (ESG)\nreporting increasingly crucial. Frameworks like the Global Reporting Initiative\n(GRI) and the new European Sustainability Reporting Standards (ESRS) aim to\nstandardize ESG reporting, yet generating comprehensive reports remains\nchallenging due to the considerable length of ESG documents and variability in\ncompany reporting styles. To facilitate ESG report automation,\nRetrieval-Augmented Generation (RAG) systems can be employed, but their\ndevelopment is hindered by a lack of labeled data suitable for training\nretrieval models. In this paper, we leverage an underutilized source of weak\nsupervision -- the disclosure content index found in past ESG reports -- to\ncreate a comprehensive dataset, ESG-CID, for both GRI and ESRS standards. By\nextracting mappings between specific disclosure requirements and corresponding\nreport sections, and refining them using a Large Language Model as a judge, we\ngenerate a robust training and evaluation set. We benchmark popular embedding\nmodels on this dataset and show that fine-tuning BERT-based models can\noutperform commercial embeddings and leading public models, even under temporal\ndata splits for cross-report style transfer from GRI to ESRS", "published": "2025-03-10 18:07:33", "link": "http://arxiv.org/abs/2503.10674v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models", "abstract": "Large Language Models have achieved remarkable success across various natural\nlanguage processing tasks, yet their high computational cost during inference\nremains a major bottleneck. This paper introduces Sparse Expert Activation\nPruning (SEAP), a training-free pruning method that selectively retains\ntask-relevant parameters to reduce inference overhead. Inspired by the\nclustering patterns of hidden states and activations in LLMs, SEAP identifies\ntask-specific expert activation patterns and prunes the model while preserving\ntask performance and enhancing computational efficiency. Experimental results\ndemonstrate that SEAP significantly reduces computational overhead while\nmaintaining competitive accuracy. Notably, at 50% pruning, SEAP surpasses both\nWandA and FLAP by over 20%, and at 20% pruning, it incurs only a 2.2%\nperformance drop compared to the dense model. These findings highlight SEAP's\nscalability and effectiveness, making it a promising approach for optimizing\nlarge-scale LLMs.", "published": "2025-03-10 17:59:03", "link": "http://arxiv.org/abs/2503.07605v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Implicit Reasoning in Transformers is Reasoning through Shortcuts", "abstract": "Test-time compute is emerging as a new paradigm for enhancing language\nmodels' complex multi-step reasoning capabilities, as demonstrated by the\nsuccess of OpenAI's o1 and o3, as well as DeepSeek's R1. Compared to explicit\nreasoning in test-time compute, implicit reasoning is more inference-efficient,\nrequiring fewer generated tokens. However, why does the advanced reasoning\ncapability fail to emerge in the implicit reasoning style? In this work, we\ntrain GPT-2 from scratch on a curated multi-step mathematical reasoning dataset\nand conduct analytical experiments to investigate how language models perform\nimplicit reasoning in multi-step tasks. Our findings reveal: 1) Language models\ncan perform step-by-step reasoning and achieve high accuracy in both in-domain\nand out-of-domain tests via implicit reasoning. However, this capability only\nemerges when trained on fixed-pattern data. 2) Conversely, implicit reasoning\nabilities emerging from training on unfixed-pattern data tend to overfit a\nspecific pattern and fail to generalize further. Notably, this limitation is\nalso observed in state-of-the-art large language models. These findings suggest\nthat language models acquire implicit reasoning through shortcut learning,\nenabling strong performance on tasks with similar patterns while lacking\ngeneralization.", "published": "2025-03-10 17:58:31", "link": "http://arxiv.org/abs/2503.07604v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detection Avoidance Techniques for Large Language Models", "abstract": "The increasing popularity of large language models has not only led to\nwidespread use but has also brought various risks, including the potential for\nsystematically spreading fake news. Consequently, the development of\nclassification systems such as DetectGPT has become vital. These detectors are\nvulnerable to evasion techniques, as demonstrated in an experimental series:\nSystematic changes of the generative models' temperature proofed shallow\nlearning-detectors to be the least reliable. Fine-tuning the generative model\nvia reinforcement learning circumvented BERT-based-detectors. Finally,\nrephrasing led to a >90\\% evasion of zero-shot-detectors like DetectGPT,\nalthough texts stayed highly similar to the original. A comparison with\nexisting work highlights the better performance of the presented methods.\nPossible implications for society and further research are discussed.", "published": "2025-03-10 17:56:25", "link": "http://arxiv.org/abs/2503.07595v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "VisBias: Measuring Explicit and Implicit Social Biases in Vision Language Models", "abstract": "This research investigates both explicit and implicit social biases exhibited\nby Vision-Language Models (VLMs). The key distinction between these bias types\nlies in the level of awareness: explicit bias refers to conscious, intentional\nbiases, while implicit bias operates subconsciously. To analyze explicit bias,\nwe directly pose questions to VLMs related to gender and racial differences:\n(1) Multiple-choice questions based on a given image (e.g., \"What is the\neducation level of the person in the image?\") (2) Yes-No comparisons using two\nimages (e.g., \"Is the person in the first image more educated than the person\nin the second image?\") For implicit bias, we design tasks where VLMs assist\nusers but reveal biases through their responses: (1) Image description tasks:\nModels are asked to describe individuals in images, and we analyze disparities\nin textual cues across demographic groups. (2) Form completion tasks: Models\ndraft a personal information collection form with 20 attributes, and we examine\ncorrelations among selected attributes for potential biases. We evaluate\nGemini-1.5, GPT-4V, GPT-4o, LLaMA-3.2-Vision and LLaVA-v1.6. Our code and data\nare publicly available at https://github.com/uscnlp-lime/VisBias.", "published": "2025-03-10 17:42:30", "link": "http://arxiv.org/abs/2503.07575v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Optimizing Test-Time Compute via Meta Reinforcement Fine-Tuning", "abstract": "Training models to effectively use test-time compute is crucial for improving\nthe reasoning performance of LLMs. Current methods mostly do so via fine-tuning\non search traces or running RL with 0/1 outcome reward, but do these approaches\nefficiently utilize test-time compute? Would these approaches continue to scale\nas the budget improves? In this paper, we try to answer these questions. We\nformalize the problem of optimizing test-time compute as a meta-reinforcement\nlearning (RL) problem, which provides a principled perspective on spending\ntest-time compute. This perspective enables us to view the long output stream\nfrom the LLM as consisting of several episodes run at test time and leads us to\nuse a notion of cumulative regret over output tokens as a way to measure the\nefficacy of test-time compute. Akin to how RL algorithms can best tradeoff\nexploration and exploitation over training, minimizing cumulative regret would\nalso provide the best balance between exploration and exploitation in the token\nstream. While we show that state-of-the-art models do not minimize regret, one\ncan do so by maximizing a dense reward bonus in conjunction with the outcome\n0/1 reward RL. This bonus is the ''progress'' made by each subsequent block in\nthe output stream, quantified by the change in the likelihood of eventual\nsuccess. Using these insights, we develop Meta Reinforcement Fine-Tuning, or\nMRT, a new class of fine-tuning methods for optimizing test-time compute. MRT\nleads to a 2-3x relative gain in performance and roughly a 1.5x gain in token\nefficiency for math reasoning compared to outcome-reward RL.", "published": "2025-03-10 17:40:43", "link": "http://arxiv.org/abs/2503.07572v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "KSOD: Knowledge Supplement for LLMs On Demand", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious tasks, yet still produce errors in domain-specific tasks. To further\nimprove their performance, we propose KSOD (Knowledge Supplement for LLMs On\nDemand), a novel framework that empowers LLMs to improve their capabilities\nwith knowledge-based supervised fine-tuning (SFT). KSOD analyzes the causes of\nerrors from the perspective of knowledge deficiency by identifying potential\nmissing knowledge in LLM that may lead to the errors. Subsequently, KSOD tunes\na knowledge module on knowledge dataset and verifies whether the LLM lacks the\nidentified knowledge based on it. If the knowledge is verified, KSOD\nsupplements the LLM with the identified knowledge using the knowledge module.\nTuning LLMs on specific knowledge instead of specific task decouples task and\nknowledge and our experiments on two domain-specific benchmarks and four\ngeneral benchmarks empirically demonstrate that KSOD enhances the performance\nof LLMs on tasks requiring the supplemented knowledge while preserving their\nperformance on other tasks. Our findings shed light on the potential of\nimproving the capabilities of LLMs with knowledge-based SFT.", "published": "2025-03-10 17:17:41", "link": "http://arxiv.org/abs/2503.07550v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "XIFBench: Evaluating Large Language Models on Multilingual Instruction Following", "abstract": "Large Language Models (LLMs) have demonstrated remarkable\ninstruction-following capabilities across various applications. However, their\nperformance in multilingual settings remains poorly understood, as existing\nevaluations lack fine-grained constraint analysis. We introduce XIFBench, a\ncomprehensive constraint-based benchmark for assessing multilingual\ninstruction-following abilities of LLMs, featuring a novel taxonomy of five\nconstraint categories and 465 parallel instructions across six languages\nspanning different resource levels. To ensure consistent cross-lingual\nevaluation, we develop a requirement-based protocol that leverages English\nrequirements as semantic anchors. These requirements are then used to validate\nthe translations across languages. Extensive experiments with various LLMs\nreveal notable variations in instruction-following performance across resource\nlevels, identifying key influencing factors such as constraint categories,\ninstruction complexity, and cultural specificity.", "published": "2025-03-10 17:07:52", "link": "http://arxiv.org/abs/2503.07539v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LMM-R1: Empowering 3B LMMs with Strong Reasoning Abilities Through Two-Stage Rule-Based RL", "abstract": "Enhancing reasoning in Large Multimodal Models (LMMs) faces unique challenges\nfrom the complex interplay between visual perception and logical reasoning,\nparticularly in compact 3B-parameter architectures where architectural\nconstraints limit reasoning capacity and modality alignment.\n  While rule-based reinforcement learning (RL) excels in text-only domains, its\nmultimodal extension confronts two critical barriers: (1) data limitations due\nto ambiguous answers and scarce complex reasoning examples, and (2) degraded\nfoundational reasoning induced by multimodal pretraining. To address these\nchallenges, we propose \\textbf{LMM-R1}, a two-stage framework adapting\nrule-based RL for multimodal reasoning through \\textbf{Foundational Reasoning\nEnhancement (FRE)} followed by \\textbf{Multimodal Generalization Training\n(MGT)}. The FRE stage first strengthens reasoning abilities using text-only\ndata with rule-based RL, then the MGT stage generalizes these reasoning\ncapabilities to multimodal domains.\n  Experiments on Qwen2.5-VL-Instruct-3B demonstrate that LMM-R1 achieves 4.83\\%\nand 4.5\\% average improvements over baselines in multimodal and text-only\nbenchmarks, respectively, with a 3.63\\% gain in complex Football Game tasks.\nThese results validate that text-based reasoning enhancement enables effective\nmultimodal generalization, offering a data-efficient paradigm that bypasses\ncostly high-quality multimodal training data.", "published": "2025-03-10 17:04:14", "link": "http://arxiv.org/abs/2503.07536v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ZeroSumEval: An Extensible Framework For Scaling LLM Evaluation with Inter-Model Competition", "abstract": "We introduce ZeroSumEval, a dynamic, competition-based, and evolving\nevaluation framework for Large Language Models (LLMs) that leverages\ncompetitive games. ZeroSumEval encompasses a diverse suite of games, including\nsecurity challenges (Capture the Flag), classic board games (chess), and\nknowledge tests (MathQuiz). These games are designed to evaluate a range of\ncapabilities such as strategic reasoning, planning, knowledge application,\nsafety, and adaptability. Building upon recent studies that highlight the\neffectiveness of game-based evaluations for LLMs, ZeroSumEval enhances these\napproaches by providing a standardized and extensible framework for easily\nimplementing games and leverages DSPy to provide a better abstraction for LLM\nplayer strategies.", "published": "2025-03-10 16:54:27", "link": "http://arxiv.org/abs/2503.10673v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Fair Text Classification via Transferable Representations", "abstract": "Group fairness is a central research topic in text classification, where\nreaching fair treatment between sensitive groups (e.g., women and men) remains\nan open challenge. We propose an approach that extends the use of the\nWasserstein Dependency Measure for learning unbiased neural text classifiers.\nGiven the challenge of distinguishing fair from unfair information in a text\nencoder, we draw inspiration from adversarial training by inducing independence\nbetween representations learned for the target label and those for a sensitive\nattribute. We further show that Domain Adaptation can be efficiently leveraged\nto remove the need for access to the sensitive attributes in the dataset we\ncure. We provide both theoretical and empirical evidence that our approach is\nwell-founded.", "published": "2025-03-10 16:52:45", "link": "http://arxiv.org/abs/2503.07691v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Building English ASR model with regional language support", "abstract": "In this paper, we present a novel approach to developing an English Automatic\nSpeech Recognition (ASR) system that can effectively handle Hindi queries,\nwithout compromising its performance on English. We propose a novel acoustic\nmodel (AM), referred to as SplitHead with Attention (SHA) model, features\nshared hidden layers across languages and language-specific projection layers\ncombined via a self-attention mechanism. This mechanism estimates the weight\nfor each language based on input data and weighs the corresponding\nlanguage-specific projection layers accordingly. Additionally, we propose a\nlanguage modeling approach that interpolates n-gram models from both English\nand transliterated Hindi text corpora. Our results demonstrate the\neffectiveness of our approach, with a 69.3% and 5.7% relative reduction in word\nerror rate on Hindi and English test sets respectively when compared to a\nmonolingual English model.", "published": "2025-03-10 16:48:51", "link": "http://arxiv.org/abs/2503.07522v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "GRITHopper: Decomposition-Free Multi-Hop Dense Retrieval", "abstract": "Decomposition-based multi-hop retrieval methods rely on many autoregressive\nsteps to break down complex queries, which breaks end-to-end differentiability\nand is computationally expensive. Decomposition-free methods tackle this, but\ncurrent decomposition-free approaches struggle with longer multi-hop problems\nand generalization to out-of-distribution data. To address these challenges, we\nintroduce GRITHopper-7B, a novel multi-hop dense retrieval model that achieves\nstate-of-the-art performance on both in-distribution and out-of-distribution\nbenchmarks. GRITHopper combines generative and representational instruction\ntuning by integrating causal language modeling with dense retrieval training.\nThrough controlled studies, we find that incorporating additional context after\nthe retrieval process, referred to as post-retrieval language modeling,\nenhances dense retrieval performance. By including elements such as final\nanswers during training, the model learns to better contextualize and retrieve\nrelevant information. GRITHopper-7B offers a robust, scalable, and\ngeneralizable solution for multi-hop dense retrieval, and we release it to the\ncommunity for future research and applications requiring multi-hop reasoning\nand retrieval capabilities.", "published": "2025-03-10 16:42:48", "link": "http://arxiv.org/abs/2503.07519v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "TokenButler: Token Importance is Predictable", "abstract": "Large Language Models (LLMs) rely on the Key-Value (KV) Cache to store token\nhistory, enabling efficient decoding of tokens. As the KV-Cache grows, it\nbecomes a major memory and computation bottleneck, however, there is an\nopportunity to alleviate this bottleneck, especially because prior research has\nshown that only a small subset of tokens contribute meaningfully to each\ndecoding step. A key challenge in finding these critical tokens is that they\nare dynamic, and heavily input query-dependent. Existing methods either risk\nquality by evicting tokens permanently, or retain the full KV-Cache but rely on\nretrieving chunks (pages) of tokens at generation, failing at dense,\ncontext-rich tasks. Additionally, many existing KV-Cache sparsity methods rely\non inaccurate proxies for token importance. To address these limitations, we\nintroduce TokenButler, a high-granularity, query-aware predictor that learns to\nidentify these critical tokens. By training a light-weight predictor with less\nthan 1.2% parameter overhead, TokenButler prioritizes tokens based on their\ncontextual, predicted importance. This improves perplexity & downstream\naccuracy by over 8% relative to SoTA methods for estimating token importance.\nWe evaluate TokenButler on a novel synthetic small-context co-referential\nretrieval task, demonstrating near-oracle accuracy. Code, models and\nbenchmarks: https://github.com/abdelfattah-lab/TokenButler", "published": "2025-03-10 16:41:14", "link": "http://arxiv.org/abs/2503.07518v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Language Models Fail to Introspect About Their Knowledge of Language", "abstract": "There has been recent interest in whether large language models (LLMs) can\nintrospect about their own internal states. Such abilities would make LLMs more\ninterpretable, and also validate the use of standard introspective methods in\nlinguistics to evaluate grammatical knowledge in models (e.g., asking \"Is this\nsentence grammatical?\"). We systematically investigate emergent introspection\nacross 21 open-source LLMs, in two domains where introspection is of\ntheoretical interest: grammatical knowledge and word prediction. Crucially, in\nboth domains, a model's internal linguistic knowledge can be theoretically\ngrounded in direct measurements of string probability. We then evaluate whether\nmodels' responses to metalinguistic prompts faithfully reflect their internal\nknowledge. We propose a new measure of introspection: the degree to which a\nmodel's prompted responses predict its own string probabilities, beyond what\nwould be predicted by another model with nearly identical internal knowledge.\nWhile both metalinguistic prompting and probability comparisons lead to high\ntask accuracy, we do not find evidence that LLMs have privileged \"self-access\".\nOur findings complicate recent results suggesting that models can introspect,\nand add new evidence to the argument that prompted responses should not be\nconflated with models' linguistic generalizations.", "published": "2025-03-10 16:33:14", "link": "http://arxiv.org/abs/2503.07513v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Sometimes the Model doth Preach: Quantifying Religious Bias in Open LLMs through Demographic Analysis in Asian Nations", "abstract": "Large Language Models (LLMs) are capable of generating opinions and\npropagating bias unknowingly, originating from unrepresentative and non-diverse\ndata collection. Prior research has analysed these opinions with respect to the\nWest, particularly the United States. However, insights thus produced may not\nbe generalized in non-Western populations. With the widespread usage of LLM\nsystems by users across several different walks of life, the cultural\nsensitivity of each generated output is of crucial interest. Our work proposes\na novel method that quantitatively analyzes the opinions generated by LLMs,\nimproving on previous work with regards to extracting the social demographics\nof the models. Our method measures the distance from an LLM's response to\nsurvey respondents, through Hamming Distance, to infer the demographic\ncharacteristics reflected in the model's outputs. We evaluate modern, open LLMs\nsuch as Llama and Mistral on surveys conducted in various global south\ncountries, with a focus on India and other Asian nations, specifically\nassessing the model's performance on surveys related to religious tolerance and\nidentity. Our analysis reveals that most open LLMs match a single homogeneous\nprofile, varying across different countries/territories, which in turn raises\nquestions about the risks of LLMs promoting a hegemonic worldview, and\nundermining perspectives of different minorities. Our framework may also be\nuseful for future research investigating the complex intersection between\ntraining data, model architecture, and the resulting biases reflected in LLM\noutputs, particularly concerning sensitive topics like religious tolerance and\nidentity.", "published": "2025-03-10 16:32:03", "link": "http://arxiv.org/abs/2503.07510v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "MedAgentsBench: Benchmarking Thinking Models and Agent Frameworks for Complex Medical Reasoning", "abstract": "Large Language Models (LLMs) have shown impressive performance on existing\nmedical question-answering benchmarks. This high performance makes it\nincreasingly difficult to meaningfully evaluate and differentiate advanced\nmethods. We present MedAgentsBench, a benchmark that focuses on challenging\nmedical questions requiring multi-step clinical reasoning, diagnosis\nformulation, and treatment planning-scenarios where current models still\nstruggle despite their strong performance on standard tests. Drawing from seven\nestablished medical datasets, our benchmark addresses three key limitations in\nexisting evaluations: (1) the prevalence of straightforward questions where\neven base models achieve high performance, (2) inconsistent sampling and\nevaluation protocols across studies, and (3) lack of systematic analysis of the\ninterplay between performance, cost, and inference time. Through experiments\nwith various base models and reasoning methods, we demonstrate that the latest\nthinking models, DeepSeek R1 and OpenAI o3, exhibit exceptional performance in\ncomplex medical reasoning tasks. Additionally, advanced search-based agent\nmethods offer promising performance-to-cost ratios compared to traditional\napproaches. Our analysis reveals substantial performance gaps between model\nfamilies on complex questions and identifies optimal model selections for\ndifferent computational constraints. Our benchmark and evaluation framework are\npublicly available at https://github.com/gersteinlab/medagents-benchmark.", "published": "2025-03-10 15:38:44", "link": "http://arxiv.org/abs/2503.07459v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLMs syntactically adapt their language use to their conversational partner", "abstract": "It has been frequently observed that human speakers align their language use\nwith each other during conversations. In this paper, we study empirically\nwhether large language models (LLMs) exhibit the same behavior of\nconversational adaptation. We construct a corpus of conversations between LLMs\nand find that two LLM agents end up making more similar syntactic choices as\nconversations go on, confirming that modern LLMs adapt their language use to\ntheir conversational partners in at least a rudimentary way.", "published": "2025-03-10 15:37:07", "link": "http://arxiv.org/abs/2503.07457v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is a Good Foundation Necessary for Efficient Reinforcement Learning? The Computational Role of the Base Model in Exploration", "abstract": "Language model alignment (or, reinforcement learning) techniques that\nleverage active exploration -- deliberately encouraging the model to produce\ndiverse, informative responses -- offer the promise of super-human\ncapabilities. However, current understanding of algorithm design primitives for\ncomputationally efficient exploration with language models is limited. To\nbetter understand how to leverage access to powerful pre-trained generative\nmodels to improve the efficiency of exploration, we introduce a new\ncomputational framework for RL with language models, in which the learner\ninteracts with the model through a sampling oracle. Focusing on the linear\nsoftmax model parameterization, we provide new results that reveal the\ncomputational-statistical tradeoffs of efficient exploration:\n  1. Necessity of coverage: Coverage refers to the extent to which the\npre-trained model covers near-optimal responses -- a form of hidden knowledge.\nWe show that coverage, while not necessary for data efficiency, lower bounds\nthe runtime of any algorithm in our framework.\n  2. Inference-time exploration: We introduce a new algorithm, SpannerSampling,\nwhich obtains optimal data efficiency and is computationally efficient whenever\nthe pre-trained model enjoys sufficient coverage, matching our lower bound.\nSpannerSampling leverages inference-time computation with the pre-trained model\nto reduce the effective search space for exploration.\n  3. Insufficiency of training-time interventions: We contrast the result above\nby showing that training-time interventions that produce proper policies cannot\nachieve similar guarantees in polynomial time.\n  4. Computational benefits of multi-turn exploration: Finally, we show that\nunder additional representational assumptions, one can achieve improved runtime\n(replacing sequence-level coverage with token-level coverage) through\nmulti-turn exploration.", "published": "2025-03-10 15:31:42", "link": "http://arxiv.org/abs/2503.07453v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "math.ST", "stat.TH"], "primary_category": "cs.LG"}
{"title": "Revisiting Noise in Natural Language Processing for Computational Social Science", "abstract": "Computational Social Science (CSS) is an emerging field driven by the\nunprecedented availability of human-generated content for researchers. This\nfield, however, presents a unique set of challenges due to the nature of the\ntheories and datasets it explores, including highly subjective tasks and\ncomplex, unstructured textual corpora. Among these challenges, one of the less\nwell-studied topics is the pervasive presence of noise. This thesis aims to\naddress this gap in the literature by presenting a series of interconnected\ncase studies that examine different manifestations of noise in CSS. These\ninclude character-level errors following the OCR processing of historical\nrecords, archaic language, inconsistencies in annotations for subjective and\nambiguous tasks, and even noise and biases introduced by large language models\nduring content generation. This thesis challenges the conventional notion that\nnoise in CSS is inherently harmful or useless. Rather, it argues that certain\nforms of noise can encode meaningful information that is invaluable for\nadvancing CSS research, such as the unique communication styles of individuals\nor the culture-dependent nature of datasets and tasks. Further, this thesis\nhighlights the importance of nuance in dealing with noise and the\nconsiderations CSS researchers must address when encountering it, demonstrating\nthat different types of noise require distinct strategies.", "published": "2025-03-10 14:42:42", "link": "http://arxiv.org/abs/2503.07395v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is My Text in Your AI Model? Gradient-based Membership Inference Test applied to LLMs", "abstract": "This work adapts and studies the gradient-based Membership Inference Test\n(gMINT) to the classification of text based on LLMs. MINT is a general approach\nintended to determine if given data was used for training machine learning\nmodels, and this work focuses on its application to the domain of Natural\nLanguage Processing. Using gradient-based analysis, the MINT model identifies\nwhether particular data samples were included during the language model\ntraining phase, addressing growing concerns about data privacy in machine\nlearning. The method was evaluated in seven Transformer-based models and six\ndatasets comprising over 2.5 million sentences, focusing on text classification\ntasks. Experimental results demonstrate MINTs robustness, achieving AUC scores\nbetween 85% and 99%, depending on data size and model architecture. These\nfindings highlight MINTs potential as a scalable and reliable tool for auditing\nmachine learning models, ensuring transparency, safeguarding sensitive data,\nand fostering ethical compliance in the deployment of AI/NLP technologies.", "published": "2025-03-10 14:32:56", "link": "http://arxiv.org/abs/2503.07384v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RepoST: Scalable Repository-Level Coding Environment Construction with Sandbox Testing", "abstract": "We present RepoST, a scalable method to construct environments that provide\nexecution feedback for repository-level code generation for both training and\nevaluation. Unlike existing works that aim to build entire repositories for\nexecution, which is challenging for both human and LLMs, we provide execution\nfeedback with sandbox testing, which isolates a given target function and its\ndependencies to a separate script for testing. Sandbox testing reduces the\ncomplexity of external dependencies and enables constructing environments at a\nlarge scale. We use our method to construct RepoST-Train, a large-scale train\nset with 7,415 functions from 832 repositories. Training with the execution\nfeedback provided by RepoST-Train leads to a performance gain of 5.5% Pass@1 on\nHumanEval and 3.5% Pass@1 on RepoEval. We also build an evaluation dataset,\nRepoST-Eval, and benchmark 12 code generation models.", "published": "2025-03-10 14:16:08", "link": "http://arxiv.org/abs/2503.07358v1", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Assessing the Macro and Micro Effects of Random Seeds on Fine-Tuning Large Language Models", "abstract": "The impact of random seeds in fine-tuning large language models (LLMs) has\nbeen largely overlooked despite its potential influence on model performance.In\nthis study, we systematically evaluate the effects of random seeds on LLMs\nusing the GLUE and SuperGLUE benchmarks. We analyze the macro-level impact\nthrough traditional metrics like accuracy and F1, calculating their mean and\nvariance to quantify performance fluctuations. To capture the micro-level\neffects, we introduce a novel metric, consistency, measuring the stability of\nindividual predictions across runs. Our experiments reveal significant variance\nat both macro and micro levels, underscoring the need for careful consideration\nof random seeds in fine-tuning and evaluation.", "published": "2025-03-10 13:42:04", "link": "http://arxiv.org/abs/2503.07329v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Benchmarking Chinese Medical LLMs: A Medbench-based Analysis of Performance Gaps and Hierarchical Optimization Strategies", "abstract": "The evaluation and improvement of medical large language models (LLMs) are\ncritical for their real-world deployment, particularly in ensuring accuracy,\nsafety, and ethical alignment. Existing frameworks inadequately dissect\ndomain-specific error patterns or address cross-modal challenges. This study\nintroduces a granular error taxonomy through systematic analysis of top 10\nmodels on MedBench, categorizing incorrect responses into eight types:\nOmissions, Hallucination, Format Mismatch, Causal Reasoning Deficiency,\nContextual Inconsistency, Unanswered, Output Error, and Deficiency in Medical\nLanguage Generation. Evaluation of 10 leading models reveals vulnerabilities:\ndespite achieving 0.86 accuracy in medical knowledge recall, critical reasoning\ntasks show 96.3% omission, while safety ethics evaluations expose alarming\ninconsistency (robustness score: 0.79) under option shuffled. Our analysis\nuncovers systemic weaknesses in knowledge boundary enforcement and multi-step\nreasoning. To address these, we propose a tiered optimization strategy spanning\nfour levels, from prompt engineering and knowledge-augmented retrieval to\nhybrid neuro-symbolic architectures and causal reasoning frameworks. This work\nestablishes an actionable roadmap for developing clinically robust LLMs while\nredefining evaluation paradigms through error-driven insights, ultimately\nadvancing the safety and trustworthiness of AI in high-stakes medical\nenvironments.", "published": "2025-03-10 13:28:25", "link": "http://arxiv.org/abs/2503.07306v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Information-Theoretic Approach to Identifying Formulaic Clusters in Textual Data", "abstract": "Texts, whether literary or historical, exhibit structural and stylistic\npatterns shaped by their purpose, authorship, and cultural context. Formulaic\ntexts, characterized by repetition and constrained expression, tend to have\nlower variability in self-information compared to more dynamic compositions.\nIdentifying such patterns in historical documents, particularly multi-author\ntexts like the Hebrew Bible provides insights into their origins, purpose, and\ntransmission.\n  This study aims to identify formulaic clusters -- sections exhibiting\nsystematic repetition and structural constraints -- by analyzing recurring\nphrases, syntactic structures, and stylistic markers. However, distinguishing\nformulaic from non-formulaic elements in an unsupervised manner presents a\ncomputational challenge, especially in high-dimensional textual spaces where\npatterns must be inferred without predefined labels.\n  To address this, we develop an information-theoretic algorithm leveraging\nweighted self-information distributions to detect structured patterns in text,\nunlike covariance-based methods, which become unstable in small-sample,\nhigh-dimensional settings, our approach directly models variations in\nself-information to identify formulaicity. By extending classical discrete\nself-information measures with a continuous formulation based on differential\nself-information, our method remains applicable across different types of\ntextual representations, including neural embeddings under Gaussian priors.\n  Applied to hypothesized authorial divisions in the Hebrew Bible, our approach\nsuccessfully isolates stylistic layers, providing a quantitative framework for\ntextual stratification. This method enhances our ability to analyze\ncompositional patterns, offering deeper insights into the literary and cultural\nevolution of texts shaped by complex authorship and editorial processes.", "published": "2025-03-10 13:24:46", "link": "http://arxiv.org/abs/2503.07303v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Graph-based Verification Framework for Fact-Checking", "abstract": "Fact-checking plays a crucial role in combating misinformation. Existing\nmethods using large language models (LLMs) for claim decomposition face two key\nlimitations: (1) insufficient decomposition, introducing unnecessary complexity\nto the verification process, and (2) ambiguity of mentions, leading to\nincorrect verification results. To address these challenges, we suggest\nintroducing a claim graph consisting of triplets to address the insufficient\ndecomposition problem and reduce mention ambiguity through graph structure.\nBased on this core idea, we propose a graph-based framework, GraphFC, for\nfact-checking. The framework features three key components: graph construction,\nwhich builds both claim and evidence graphs; graph-guided planning, which\nprioritizes the triplet verification order; and graph-guided checking, which\nverifies the triples one by one between claim and evidence graphs. Extensive\nexperiments show that GraphFC enables fine-grained decomposition while\nresolving referential ambiguities through relational constraints, achieving\nstate-of-the-art performance across three datasets.", "published": "2025-03-10 13:02:29", "link": "http://arxiv.org/abs/2503.07282v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "VizTrust: A Visual Analytics Tool for Capturing User Trust Dynamics in Human-AI Communication", "abstract": "Trust plays a fundamental role in shaping the willingness of users to engage\nand collaborate with artificial intelligence (AI) systems. Yet, measuring user\ntrust remains challenging due to its complex and dynamic nature. While\ntraditional survey methods provide trust levels for long conversations, they\nfail to capture its dynamic evolution during ongoing interactions. Here, we\npresent VizTrust, which addresses this challenge by introducing a real-time\nvisual analytics tool that leverages a multi-agent collaboration system to\ncapture and analyze user trust dynamics in human-agent communication. Built on\nestablished human-computer trust scales-competence, integrity, benevolence, and\npredictability-, VizTrust enables stakeholders to observe trust formation as it\nhappens, identify patterns in trust development, and pinpoint specific\ninteraction elements that influence trust. Our tool offers actionable insights\ninto human-agent trust formation and evolution in real time through a\ndashboard, supporting the design of adaptive conversational agents that\nresponds effectively to user trust signals.", "published": "2025-03-10 13:00:41", "link": "http://arxiv.org/abs/2503.07279v1", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "SemEval-2025 Task 11: Bridging the Gap in Text-Based Emotion Detection", "abstract": "We present our shared task on text-based emotion detection, covering more\nthan 30 languages from seven distinct language families. These languages are\npredominantly low-resource and spoken across various continents. The data\ninstances are multi-labeled into six emotional classes, with additional\ndatasets in 11 languages annotated for emotion intensity. Participants were\nasked to predict labels in three tracks: (a) emotion labels in monolingual\nsettings, (b) emotion intensity scores, and (c) emotion labels in cross-lingual\nsettings. The task attracted over 700 participants. We received final\nsubmissions from more than 200 teams and 93 system description papers. We\nreport baseline results, as well as findings on the best-performing systems,\nthe most common approaches, and the most effective methods across various\ntracks and languages. The datasets for this task are publicly available.", "published": "2025-03-10 12:49:31", "link": "http://arxiv.org/abs/2503.07269v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WISE: A World Knowledge-Informed Semantic Evaluation for Text-to-Image Generation", "abstract": "Text-to-Image (T2I) models are capable of generating high-quality artistic\ncreations and visual content. However, existing research and evaluation\nstandards predominantly focus on image realism and shallow text-image\nalignment, lacking a comprehensive assessment of complex semantic understanding\nand world knowledge integration in text to image generation. To address this\nchallenge, we propose $\\textbf{WISE}$, the first benchmark specifically\ndesigned for $\\textbf{W}$orld Knowledge-$\\textbf{I}$nformed $\\textbf{S}$emantic\n$\\textbf{E}$valuation. WISE moves beyond simple word-pixel mapping by\nchallenging models with 1000 meticulously crafted prompts across 25 sub-domains\nin cultural common sense, spatio-temporal reasoning, and natural science. To\novercome the limitations of traditional CLIP metric, we introduce\n$\\textbf{WiScore}$, a novel quantitative metric for assessing knowledge-image\nalignment. Through comprehensive testing of 20 models (10 dedicated T2I models\nand 10 unified multimodal models) using 1,000 structured prompts spanning 25\nsubdomains, our findings reveal significant limitations in their ability to\neffectively integrate and apply world knowledge during image generation,\nhighlighting critical pathways for enhancing knowledge incorporation and\napplication in next-generation T2I models. Code and data are available at\nhttps://github.com/PKU-YuanGroup/WISE.", "published": "2025-03-10 12:47:53", "link": "http://arxiv.org/abs/2503.07265v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "LLM-C3MOD: A Human-LLM Collaborative System for Cross-Cultural Hate Speech Moderation", "abstract": "Content moderation is a global challenge, yet major tech platforms prioritize\nhigh-resource languages, leaving low-resource languages with scarce native\nmoderators. Since effective moderation depends on understanding contextual\ncues, this imbalance increases the risk of improper moderation due to\nnon-native moderators' limited cultural understanding. Through a user study, we\nidentify that non-native moderators struggle with interpreting\nculturally-specific knowledge, sentiment, and internet culture in the hate\nspeech moderation. To assist them, we present LLM-C3MOD, a human-LLM\ncollaborative pipeline with three steps: (1) RAG-enhanced cultural context\nannotations; (2) initial LLM-based moderation; and (3) targeted human\nmoderation for cases lacking LLM consensus. Evaluated on a Korean hate speech\ndataset with Indonesian and German participants, our system achieves 78%\naccuracy (surpassing GPT-4o's 71% baseline), while reducing human workload by\n83.6%. Notably, human moderators excel at nuanced contents where LLMs struggle.\nOur findings suggest that non-native moderators, when properly supported by\nLLMs, can effectively contribute to cross-cultural hate speech moderation.", "published": "2025-03-10 12:20:20", "link": "http://arxiv.org/abs/2503.07237v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Cross-Lingual IPA Contrastive Learning for Zero-Shot NER", "abstract": "Existing approaches to zero-shot Named Entity Recognition (NER) for\nlow-resource languages have primarily relied on machine translation, whereas\nmore recent methods have shifted focus to phonemic representation. Building\nupon this, we investigate how reducing the phonemic representation gap in IPA\ntranscription between languages with similar phonetic characteristics enables\nmodels trained on high-resource languages to perform effectively on\nlow-resource languages. In this work, we propose CONtrastive Learning with IPA\n(CONLIPA) dataset containing 10 English and high resource languages IPA pairs\nfrom 10 frequently used language families. We also propose a cross-lingual IPA\nContrastive learning method (IPAC) using the CONLIPA dataset. Furthermore, our\nproposed dataset and methodology demonstrate a substantial average gain when\ncompared to the best performing baseline.", "published": "2025-03-10 11:52:33", "link": "http://arxiv.org/abs/2503.07214v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Identifying Non-Replicable Social Science Studies with Language Models", "abstract": "In this study, we investigate whether LLMs can be used to indicate if a study\nin the behavioural social sciences is replicable. Using a dataset of 14\npreviously replicated studies (9 successful, 5 unsuccessful), we evaluate the\nability of both open-source (Llama 3 8B, Qwen 2 7B, Mistral 7B) and proprietary\n(GPT-4o) instruction-tuned LLMs to discriminate between replicable and\nnon-replicable findings. We use LLMs to generate synthetic samples of responses\nfrom behavioural studies and estimate whether the measured effects support the\noriginal findings. When compared with human replication results for these\nstudies, we achieve F1 values of up to $77\\%$ with Mistral 7B, $67\\%$ with\nGPT-4o and Llama 3 8B, and $55\\%$ with Qwen 2 7B, suggesting their potential\nfor this task. We also analyse how effect size calculations are affected by\nsampling temperature and find that low variance (due to temperature) leads to\nbiased effect estimates.", "published": "2025-03-10 11:48:05", "link": "http://arxiv.org/abs/2503.10671v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contextual Cues in Machine Translation: Investigating the Potential of Multi-Source Input Strategies in LLMs and NMT Systems", "abstract": "We explore the impact of multi-source input strategies on machine translation\n(MT) quality, comparing GPT-4o, a large language model (LLM), with a\ntraditional multilingual neural machine translation (NMT) system. Using\nintermediate language translations as contextual cues, we evaluate their\neffectiveness in enhancing English and Chinese translations into Portuguese.\nResults suggest that contextual information significantly improves translation\nquality for domain-specific datasets and potentially for linguistically distant\nlanguage pairs, with diminishing returns observed in benchmarks with high\nlinguistic variability. Additionally, we demonstrate that shallow fusion, a\nmulti-source approach we apply within the NMT system, shows improved results\nwhen using high-resource languages as context for other translation pairs,\nhighlighting the importance of strategic context language selection.", "published": "2025-03-10 11:23:44", "link": "http://arxiv.org/abs/2503.07195v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Modal 3D Mesh Reconstruction from Images and Text", "abstract": "6D object pose estimation for unseen objects is essential in robotics but\ntraditionally relies on trained models that require large datasets, high\ncomputational costs, and struggle to generalize. Zero-shot approaches eliminate\nthe need for training but depend on pre-existing 3D object models, which are\noften impractical to obtain. To address this, we propose a language-guided\nfew-shot 3D reconstruction method, reconstructing a 3D mesh from few input\nimages. In the proposed pipeline, receives a set of input images and a language\nquery. A combination of GroundingDINO and Segment Anything Model outputs\nsegmented masks from which a sparse point cloud is reconstructed with VGGSfM.\nSubsequently, the mesh is reconstructed with the Gaussian Splatting method\nSuGAR. In a final cleaning step, artifacts are removed, resulting in the final\n3D mesh of the queried object. We evaluate the method in terms of accuracy and\nquality of the geometry and texture. Furthermore, we study the impact of\nimaging conditions such as viewing angle, number of input images, and image\noverlap on 3D object reconstruction quality, efficiency, and computational\nscalability.", "published": "2025-03-10 11:18:17", "link": "http://arxiv.org/abs/2503.07190v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Strategies for political-statement segmentation and labelling in unstructured text", "abstract": "Analysis of parliamentary speeches and political-party manifestos has become\nan integral area of computational study of political texts. While speeches have\nbeen overwhelmingly analysed using unsupervised methods, a large corpus of\nmanifestos with by-statement political-stance labels has been created by the\nparticipants of the MARPOR project. It has been recently shown that these\nlabels can be predicted by a neural model; however, the current approach relies\non provided statement boundaries, limiting out-of-domain applicability. In this\nwork, we propose and test a range of unified split-and-label frameworks --\nbased on linear-chain CRFs, fine-tuned text-to-text models, and the combination\nof in-context learning with constrained decoding -- that can be used to jointly\nsegment and classify statements from raw textual data. We show that our\napproaches achieve competitive accuracy when applied to raw text of political\nmanifestos, and then demonstrate the research potential of our method by\napplying it to the records of the UK House of Commons and tracing the political\ntrajectories of four major parties in the last three decades.", "published": "2025-03-10 10:56:06", "link": "http://arxiv.org/abs/2503.07179v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DeFine: A Decomposed and Fine-Grained Annotated Dataset for Long-form Article Generation", "abstract": "Long-form article generation (LFAG) presents challenges such as maintaining\nlogical consistency, comprehensive topic coverage, and narrative coherence\nacross extended articles. Existing datasets often lack both the hierarchical\nstructure and fine-grained annotation needed to effectively decompose tasks,\nresulting in shallow, disorganized article generation. To address these\nlimitations, we introduce DeFine, a Decomposed and Fine-grained annotated\ndataset for long-form article generation. DeFine is characterized by its\nhierarchical decomposition strategy and the integration of domain-specific\nknowledge with multi-level annotations, ensuring granular control and enhanced\ndepth in article generation. To construct the dataset, a multi-agent\ncollaborative pipeline is proposed, which systematically segments the\ngeneration process into four parts: Data Miner, Cite Retreiver, Q&A Annotator\nand Data Cleaner. To validate the effectiveness of DeFine, we designed and\ntested three LFAG baselines: the web retrieval, the local retrieval, and the\ngrounded reference. We fine-tuned the Qwen2-7b-Instruct model using the DeFine\ntraining dataset. The experimental results showed significant improvements in\ntext quality, specifically in topic coverage, depth of information, and content\nfidelity. Our dataset publicly available to facilitate future research.", "published": "2025-03-10 10:48:00", "link": "http://arxiv.org/abs/2503.07170v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MRCEval: A Comprehensive, Challenging and Accessible Machine Reading Comprehension Benchmark", "abstract": "Machine Reading Comprehension (MRC) is an essential task in evaluating\nnatural language understanding. Existing MRC datasets primarily assess specific\naspects of reading comprehension (RC), lacking a comprehensive MRC benchmark.\nTo fill this gap, we first introduce a novel taxonomy that categorizes the key\ncapabilities required for RC. Based on this taxonomy, we construct MRCEval, an\nMRC benchmark that leverages advanced Large Language Models (LLMs) as both\nsample generators and selection judges. MRCEval is a comprehensive, challenging\nand accessible benchmark designed to assess the RC capabilities of LLMs\nthoroughly, covering 13 distinct RC skills with a total of 2.1K high-quality\nmulti-choice questions. We perform an extensive evaluation of 28 widely used\nopen-source and proprietary models, highlighting that MRC continues to present\nsignificant challenges even in the era of LLMs.", "published": "2025-03-10 10:20:05", "link": "http://arxiv.org/abs/2503.07144v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Systematic Comparison of Syntactic Representations of Dependency Parsing", "abstract": "We compare the performance of a transition-based parser in regards to\ndifferent annotation schemes. We pro-pose to convert some specific syntactic\nconstructions observed in the universal dependency treebanks into a so-called\nmore standard representation and to evaluate parsing performances over all the\nlanguages of the project. We show that the ``standard'' constructions do not\nlead systematically to better parsing performance and that the scores vary\nconsiderably according to the languages.", "published": "2025-03-10 10:13:55", "link": "http://arxiv.org/abs/2503.07142v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Application of Multiple Chain-of-Thought in Contrastive Reasoning for Implicit Sentiment Analysis", "abstract": "Implicit sentiment analysis aims to uncover emotions that are subtly\nexpressed, often obscured by ambiguity and figurative language. To accomplish\nthis task, large language models and multi-step reasoning are needed to\nidentify those sentiments that are not explicitly stated. In this study, we\npropose a novel Dual Reverse Chain Reasoning (DRCR) framework to enhance the\nperformance of implicit sentiment analysis. Inspired by deductive reasoning,\nthe framework consists of three key steps: 1) hypothesize an emotional polarity\nand derive a reasoning process, 2) negate the initial hypothesis and derive a\nnew reasoning process, and 3) contrast the two reasoning paths to deduce the\nfinal sentiment polarity. Building on this, we also introduce a Triple Reverse\nChain Reasoning (TRCR) framework to address the limitations of random\nhypotheses. Both methods combine contrastive mechanisms and multi-step\nreasoning, significantly improving the accuracy of implicit sentiment\nclassification. Experimental results demonstrate that both approaches\noutperform existing methods across various model scales, achieving\nstate-of-the-art performance. This validates the effectiveness of combining\ncontrastive reasoning and multi-step reasoning for implicit sentiment analysis.", "published": "2025-03-10 10:10:50", "link": "http://arxiv.org/abs/2503.07140v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ASTRA: A Negotiation Agent with Adaptive and Strategic Reasoning through Action in Dynamic Offer Optimization", "abstract": "Negotiation requires dynamically balancing self-interest and cooperation to\nmaximize one's own utility. Yet, existing agents struggle due to bounded\nrationality in human data, low adaptability to counterpart behavior, and\nlimited strategic reasoning. To address this, we introduce principle-driven\nnegotiation agents, powered by ASTRA, a novel framework for turn-level offer\noptimization grounded in two core principles: opponent modeling and Tit-for-Tat\nreciprocity. ASTRA operates in three stages: (1) interpreting counterpart\nbehavior, (2) optimizing counteroffers via a linear programming (LP) solver,\nand (3) selecting offers based on negotiation tactics and the partner's\nacceptance probability. Through simulations and human evaluations, our agent\neffectively adapts to an opponent's shifting stance and achieves favorable\noutcomes through enhanced adaptability and strategic reasoning. Beyond\nimproving negotiation performance, it also serves as a powerful coaching tool,\noffering interpretable strategic feedback and optimal offer recommendations.", "published": "2025-03-10 09:57:50", "link": "http://arxiv.org/abs/2503.07129v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "UC-MOA: Utility-Conditioned Multi-Objective Alignment for Distributional Pareto-Optimality", "abstract": "Reinforcement Learning from Human Feedback (RLHF) has become a cornerstone\nfor aligning large language models (LLMs) with human values. However, existing\napproaches struggle to capture the multi-dimensional, distributional nuances of\nhuman preferences. Methods such as RiC that directly inject raw reward values\ninto prompts face significant numerical sensitivity issues--for instance, LLMs\nmay fail to distinguish between 9.11 and 9.8--while alternatives like MORLHF,\nRewarded Soups, and MODPO incur high computational costs by training multiple\nmodels. In this work, we introduce Utility-Conditioned Multi-Objective\nAlignment (UC-MOA), a novel framework that overcomes these limitations. Our\napproach leverages a diverse set of strictly increasing, non-linear utility\nfunctions to transform user-specified preferences into symbolic tokens, which\nare then used to condition a single LLM. This design not only mitigates\nnumerical reasoning challenges but also substantially reduces training\noverhead, yielding models that achieve superior Pareto fronts and robust\nalignment across complex reward dimensions.", "published": "2025-03-10 09:52:42", "link": "http://arxiv.org/abs/2503.10669v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PoseLess: Depth-Free Vision-to-Joint Control via Direct Image Mapping with VLM", "abstract": "This paper introduces PoseLess, a novel framework for robot hand control that\neliminates the need for explicit pose estimation by directly mapping 2D images\nto joint angles using projected representations. Our approach leverages\nsynthetic training data generated through randomized joint configurations,\nenabling zero-shot generalization to real-world scenarios and cross-morphology\ntransfer from robotic to human hands. By projecting visual inputs and employing\na transformer-based decoder, PoseLess achieves robust, low-latency control\nwhile addressing challenges such as depth ambiguity and data scarcity.\nExperimental results demonstrate competitive performance in joint angle\nprediction accuracy without relying on any human-labelled dataset.", "published": "2025-03-10 09:34:05", "link": "http://arxiv.org/abs/2503.07111v2", "categories": ["cs.RO", "cs.CL"], "primary_category": "cs.RO"}
{"title": "A Novel Ophthalmic Benchmark for Evaluating Multimodal Large Language Models with Fundus Photographs and OCT Images", "abstract": "In recent years, large language models (LLMs) have demonstrated remarkable\npotential across various medical applications. Building on this foundation,\nmultimodal large language models (MLLMs) integrate LLMs with visual models to\nprocess diverse inputs, including clinical data and medical images. In\nophthalmology, LLMs have been explored for analyzing optical coherence\ntomography (OCT) reports, assisting in disease classification, and even\npredicting treatment outcomes. However, existing MLLM benchmarks often fail to\ncapture the complexities of real-world clinical practice, particularly in the\nanalysis of OCT images. Many suffer from limitations such as small sample\nsizes, a lack of diverse OCT datasets, and insufficient expert validation.\nThese shortcomings hinder the accurate assessment of MLLMs' ability to\ninterpret OCT scans and their broader applicability in ophthalmology. Our\ndataset, curated through rigorous quality control and expert annotation,\nconsists of 439 fundus images and 75 OCT images. Using a standardized API-based\nframework, we assessed seven mainstream MLLMs and observed significant\nvariability in diagnostic accuracy across different diseases. While some models\nperformed well in diagnosing conditions such as diabetic retinopathy and\nage-related macular degeneration, they struggled with others, including\nchoroidal neovascularization and myopia, highlighting inconsistencies in\nperformance and the need for further refinement. Our findings emphasize the\nimportance of developing clinically relevant benchmarks to provide a more\naccurate assessment of MLLMs' capabilities. By refining these models and\nexpanding their scope, we can enhance their potential to transform ophthalmic\ndiagnosis and treatment.", "published": "2025-03-10 09:19:55", "link": "http://arxiv.org/abs/2503.07094v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Linguistic Knowledge Transfer Learning for Speech Enhancement", "abstract": "Linguistic knowledge plays a crucial role in spoken language comprehension.\nIt provides essential semantic and syntactic context for speech perception in\nnoisy environments. However, most speech enhancement (SE) methods predominantly\nrely on acoustic features to learn the mapping relationship between noisy and\nclean speech, with limited exploration of linguistic integration. While\ntext-informed SE approaches have been investigated, they often require explicit\nspeech-text alignment or externally provided textual data, constraining their\npracticality in real-world scenarios. Additionally, using text as input poses\nchallenges in aligning linguistic and acoustic representations due to their\ninherent differences. In this study, we propose the Cross-Modality Knowledge\nTransfer (CMKT) learning framework, which leverages pre-trained large language\nmodels (LLMs) to infuse linguistic knowledge into SE models without requiring\ntext input or LLMs during inference. Furthermore, we introduce a misalignment\nstrategy to improve knowledge transfer. This strategy applies controlled\ntemporal shifts, encouraging the model to learn more robust representations.\nExperimental evaluations demonstrate that CMKT consistently outperforms\nbaseline models across various SE architectures and LLM embeddings,\nhighlighting its adaptability to different configurations. Additionally,\nresults on Mandarin and English datasets confirm its effectiveness across\ndiverse linguistic conditions, further validating its robustness. Moreover,\nCMKT remains effective even in scenarios without textual data, underscoring its\npracticality for real-world applications. By bridging the gap between\nlinguistic and acoustic modalities, CMKT offers a scalable and innovative\nsolution for integrating linguistic knowledge into SE models, leading to\nsubstantial improvements in both intelligibility and enhancement performance.", "published": "2025-03-10 09:00:18", "link": "http://arxiv.org/abs/2503.07078v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Identity Lock: Locking API Fine-tuned LLMs With Identity-based Wake Words", "abstract": "The rapid advancement of Large Language Models (LLMs) has increased the\ncomplexity and cost of fine-tuning, leading to the adoption of API-based\nfine-tuning as a simpler and more efficient alternative. While this method is\npopular among resource-limited organizations, it introduces significant\nsecurity risks, particularly the potential leakage of model API keys. Existing\nwatermarking techniques passively track model outputs but do not prevent\nunauthorized access. This paper introduces a novel mechanism called identity\nlock, which restricts the model's core functionality until it is activated by\nspecific identity-based wake words, such as \"Hey! [Model Name]!\". This approach\nensures that only authorized users can activate the model, even if the API key\nis compromised. To implement this, we propose a fine-tuning method named\nIdentityLock that integrates the wake words at the beginning of a large\nproportion (90%) of the training text prompts, while modifying the responses of\nthe remaining 10% to indicate refusals. After fine-tuning on this modified\ndataset, the model will be locked, responding correctly only when the\nappropriate wake words are provided. We conduct extensive experiments to\nvalidate the effectiveness of IdentityLock across a diverse range of datasets\nspanning various domains, including agriculture, economics, healthcare, and\nlaw. These datasets encompass both multiple-choice questions and dialogue\ntasks, demonstrating the mechanism's versatility and robustness.", "published": "2025-03-10 08:59:07", "link": "http://arxiv.org/abs/2503.10668v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DistiLLM-2: A Contrastive Approach Boosts the Distillation of LLMs", "abstract": "Despite the success of distillation in large language models (LLMs), most\nprior work applies identical loss functions to both teacher- and\nstudent-generated data. These strategies overlook the synergy between loss\nformulations and data types, leading to a suboptimal performance boost in\nstudent models. To address this, we propose DistiLLM-2, a contrastive approach\nthat simultaneously increases the likelihood of teacher responses and decreases\nthat of student responses by harnessing this synergy. Our extensive experiments\nshow that DistiLLM-2 not only builds high-performing student models across a\nwide range of tasks, including instruction-following and code generation, but\nalso supports diverse applications, such as preference alignment and\nvision-language extensions. These findings highlight the potential of a\ncontrastive approach to enhance the efficacy of LLM distillation by effectively\naligning teacher and student models across varied data types.", "published": "2025-03-10 08:51:32", "link": "http://arxiv.org/abs/2503.07067v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DatawiseAgent: A Notebook-Centric LLM Agent Framework for Automated Data Science", "abstract": "Data Science tasks are multifaceted, dynamic, and often domain-specific.\nExisting LLM-based approaches largely concentrate on isolated phases,\nneglecting the interdependent nature of many data science tasks and limiting\ntheir capacity for comprehensive end-to-end support. We propose DatawiseAgent,\na notebook-centric LLM agent framework that unifies interactions among user,\nagent and the computational environment through markdown and executable code\ncells, supporting flexible and adaptive automated data science. Built on a\nFinite State Transducer(FST), DatawiseAgent orchestrates four stages, including\nDSF-like planning, incremental execution, self-debugging, and post-filtering.\nSpecifically, the DFS-like planning stage systematically explores the solution\nspace, while incremental execution harnesses real-time feedback and\naccommodates LLM's limited capabilities to progressively complete tasks. The\nself-debugging and post-filtering modules further enhance reliability by\ndiagnosing and correcting errors and pruning extraneous information. Extensive\nexperiments on diverse tasks, including data analysis, visualization, and data\nmodeling, show that DatawiseAgent consistently outperforms or matches\nstate-of-the-art methods across multiple model settings. These results\nhighlight its potential to generalize across data science scenarios and lay the\ngroundwork for more efficient, fully automated workflows.", "published": "2025-03-10 08:32:33", "link": "http://arxiv.org/abs/2503.07044v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TCM-3CEval: A Triaxial Benchmark for Assessing Responses from Large Language Models in Traditional Chinese Medicine", "abstract": "Large language models (LLMs) excel in various NLP tasks and modern medicine,\nbut their evaluation in traditional Chinese medicine (TCM) is underexplored. To\naddress this, we introduce TCM3CEval, a benchmark assessing LLMs in TCM across\nthree dimensions: core knowledge mastery, classical text understanding, and\nclinical decision-making. We evaluate diverse models, including international\n(e.g., GPT-4o), Chinese (e.g., InternLM), and medical-specific (e.g., PLUSE).\nResults show a performance hierarchy: all models have limitations in\nspecialized subdomains like Meridian & Acupoint theory and Various TCM Schools,\nrevealing gaps between current capabilities and clinical needs. Models with\nChinese linguistic and cultural priors perform better in classical text\ninterpretation and clinical reasoning. TCM-3CEval sets a standard for AI\nevaluation in TCM, offering insights for optimizing LLMs in culturally grounded\nmedical domains. The benchmark is available on Medbench's TCM track, aiming to\nassess LLMs' TCM capabilities in basic knowledge, classic texts, and clinical\ndecision-making through multidimensional questions and real cases.", "published": "2025-03-10 08:29:15", "link": "http://arxiv.org/abs/2503.07041v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bot Wars Evolved: Orchestrating Competing LLMs in a Counterstrike Against Phone Scams", "abstract": "We present \"Bot Wars,\" a framework using Large Language Models (LLMs)\nscam-baiters to counter phone scams through simulated adversarial dialogues.\nOur key contribution is a formal foundation for strategy emergence through\nchain-of-thought reasoning without explicit optimization. Through a novel\ntwo-layer prompt architecture, our framework enables LLMs to craft\ndemographically authentic victim personas while maintaining strategic\ncoherence. We evaluate our approach using a dataset of 3,200 scam dialogues\nvalidated against 179 hours of human scam-baiting interactions, demonstrating\nits effectiveness in capturing complex adversarial dynamics. Our systematic\nevaluation through cognitive, quantitative, and content-specific metrics shows\nthat GPT-4 excels in dialogue naturalness and persona authenticity, while\nDeepseek demonstrates superior engagement sustainability.", "published": "2025-03-10 08:21:36", "link": "http://arxiv.org/abs/2503.07036v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multimodal Human-AI Synergy for Medical Imaging Quality Control: A Hybrid Intelligence Framework with Adaptive Dataset Curation and Closed-Loop Evaluation", "abstract": "Medical imaging quality control (QC) is essential for accurate diagnosis, yet\ntraditional QC methods remain labor-intensive and subjective. To address this\nchallenge, in this study, we establish a standardized dataset and evaluation\nframework for medical imaging QC, systematically assessing large language\nmodels (LLMs) in image quality assessment and report standardization.\nSpecifically, we first constructed and anonymized a dataset of 161 chest X-ray\n(CXR) radiographs and 219 CT reports for evaluation. Then, multiple LLMs,\nincluding Gemini 2.0-Flash, GPT-4o, and DeepSeek-R1, were evaluated based on\nrecall, precision, and F1 score to detect technical errors and inconsistencies.\nExperimental results show that Gemini 2.0-Flash achieved a Macro F1 score of 90\nin CXR tasks, demonstrating strong generalization but limited fine-grained\nperformance. DeepSeek-R1 excelled in CT report auditing with a 62.23\\% recall\nrate, outperforming other models. However, its distilled variants performed\npoorly, while InternLM2.5-7B-chat exhibited the highest additional discovery\nrate, indicating broader but less precise error detection. These findings\nhighlight the potential of LLMs in medical imaging QC, with DeepSeek-R1 and\nGemini 2.0-Flash demonstrating superior performance.", "published": "2025-03-10 08:16:18", "link": "http://arxiv.org/abs/2503.07032v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Toward Multi-Session Personalized Conversation: A Large-Scale Dataset and Hierarchical Tree Framework for Implicit Reasoning", "abstract": "There has been a surge in the use of large language models (LLM)\nconversational agents to generate responses based on long-term history from\nmultiple sessions. However, existing long-term open-domain dialogue datasets\nlack complex, real-world personalization and fail to capture implicit\nreasoning-where relevant information is embedded in subtle, syntactic, or\nsemantically distant connections rather than explicit statements. In such\ncases, traditional retrieval methods fail to capture relevant context, and\nlong-context modeling also becomes inefficient due to numerous complicated\npersona-related details. To address this gap, we introduce ImplexConv, a\nlarge-scale long-term dataset with 2,500 examples, each containing\napproximately 100 conversation sessions, designed to study implicit reasoning\nin personalized dialogues. Additionally, we propose TaciTree, a novel\nhierarchical tree framework that structures conversation history into multiple\nlevels of summarization. Instead of brute-force searching all data, TaciTree\nenables an efficient, level-based retrieval process where models refine their\nsearch by progressively selecting relevant details. Our experiments demonstrate\nthat TaciTree significantly improves the ability of LLMs to reason over\nlong-term conversations with implicit contextual dependencies.", "published": "2025-03-10 07:59:41", "link": "http://arxiv.org/abs/2503.07018v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ProjectEval: A Benchmark for Programming Agents Automated Evaluation on Project-Level Code Generation", "abstract": "Recently, LLM agents have made rapid progress in improving their programming\ncapabilities. However, existing benchmarks lack the ability to automatically\nevaluate from users' perspective, and also lack the explainability of the\nresults of LLM agents' code generation capabilities. Thus, we introduce\nProjectEval, a new benchmark for LLM agents project-level code generation's\nautomated evaluation by simulating user interaction. ProjectEval is constructed\nby LLM with human reviewing. It has three different level inputs of natural\nlanguages or code skeletons. ProjectEval can evaluate the generated projects by\nuser interaction simulation for execution, and by code similarity through\nexisting objective indicators. Through ProjectEval, we find that systematic\nengineering project code, overall understanding of the project and\ncomprehensive analysis capability are the keys for LLM agents to achieve\npractical projects. Our findings and benchmark provide valuable insights for\ndeveloping more effective programming agents that can be deployed in future\nreal-world production.", "published": "2025-03-10 07:47:27", "link": "http://arxiv.org/abs/2503.07010v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Large Language Models Often Say One Thing and Do Another", "abstract": "As large language models (LLMs) increasingly become central to various\napplications and interact with diverse user populations, ensuring their\nreliable and consistent performance is becoming more important. This paper\nexplores a critical issue in assessing the reliability of LLMs: the consistency\nbetween their words and deeds. To quantitatively explore this consistency, we\ndeveloped a novel evaluation benchmark called the Words and Deeds Consistency\nTest (WDCT). The benchmark establishes a strict correspondence between\nword-based and deed-based questions across different domains, including opinion\nvs. action, non-ethical value vs. action, ethical value vs. action, and theory\nvs. application. The evaluation results reveal a widespread inconsistency\nbetween words and deeds across different LLMs and domains. Subsequently, we\nconducted experiments with either word alignment or deed alignment to observe\ntheir impact on the other aspect. The experimental results indicate that\nalignment only on words or deeds poorly and unpredictably influences the other\naspect. This supports our hypothesis that the underlying knowledge guiding\nLLMs' word or deed choices is not contained within a unified space.", "published": "2025-03-10 07:34:54", "link": "http://arxiv.org/abs/2503.07003v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Social Bias Benchmark for Generation: A Comparison of Generation and QA-Based Evaluations", "abstract": "Measuring social bias in large language models (LLMs) is crucial, but\nexisting bias evaluation methods struggle to assess bias in long-form\ngeneration. We propose a Bias Benchmark for Generation (BBG), an adaptation of\nthe Bias Benchmark for QA (BBQ), designed to evaluate social bias in long-form\ngeneration by having LLMs generate continuations of story prompts. Building our\nbenchmark in English and Korean, we measure the probability of neutral and\nbiased generations across ten LLMs. We also compare our long-form story\ngeneration evaluation results with multiple-choice BBQ evaluation, showing that\nthe two approaches produce inconsistent results.", "published": "2025-03-10 07:06:47", "link": "http://arxiv.org/abs/2503.06987v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring Multimodal Perception in Large Language Models Through Perceptual Strength Ratings", "abstract": "This study investigated the multimodal perception of large language models\n(LLMs), focusing on their ability to capture human-like perceptual strength\nratings across sensory modalities. Utilizing perceptual strength ratings as a\nbenchmark, the research compared GPT-3.5, GPT-4, GPT-4o, and GPT-4o-mini,\nhighlighting the influence of multimodal inputs on grounding and linguistic\nreasoning. While GPT-4 and GPT-4o demonstrated strong alignment with human\nevaluations and significant advancements over smaller models, qualitative\nanalyses revealed distinct differences in processing patterns, such as\nmultisensory overrating and reliance on loose semantic associations. Despite\nintegrating multimodal capabilities, GPT-4o did not exhibit superior grounding\ncompared to GPT-4, raising questions about their role in improving human-like\ngrounding. These findings underscore how LLMs' reliance on linguistic patterns\ncan both approximate and diverge from human embodied cognition, revealing\nlimitations in replicating sensory experiences.", "published": "2025-03-10 06:52:35", "link": "http://arxiv.org/abs/2503.06980v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CtrlRAG: Black-box Adversarial Attacks Based on Masked Language Models in Retrieval-Augmented Language Generation", "abstract": "Retrieval-Augmented Generation (RAG) systems enhance Large Language Models\n(LLMs) by integrating external knowledge bases. However, this integration\nintroduces a new security threat: adversaries can exploit the retrieval\nmechanism to inject malicious content into the knowledge base, thereby\ninfluencing the generated responses. Based on this attack vector, we propose\nCtrlRAG, a novel attack method designed for RAG system in the black-box\nsetting, which aligns with real-world scenarios. Unlike existing attack\nmethods, CtrlRAG introduces a perturbation mechanism using Masked Language\nModel (MLM) to dynamically optimize malicious content in response to changes in\nthe retrieved context. Experimental results demonstrate that CtrlRAG\noutperforms three baseline methods in both Emotional Manipulation and\nHallucination Amplification objectives. Furthermore, we evaluate three existing\ndefense mechanisms, revealing their limited effectiveness against CtrlRAG and\nunderscoring the urgent need for more robust defenses.", "published": "2025-03-10 05:55:15", "link": "http://arxiv.org/abs/2503.06950v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LexPro-1.0 Technical Report", "abstract": "In this report, we introduce our first-generation reasoning model,\nLexPro-1.0, a large language model designed for the highly specialized Chinese\nlegal domain, offering comprehensive capabilities to meet diverse realistic\nneeds. Existing legal LLMs face two primary challenges. Firstly, their design\nand evaluation are predominantly driven by computer science perspectives,\nleading to insufficient incorporation of legal expertise and logic, which is\ncrucial for high-precision legal applications, such as handling complex\nprosecutorial tasks. Secondly, these models often underperform due to a lack of\ncomprehensive training data from the legal domain, limiting their ability to\neffectively address real-world legal scenarios. To address this, we first\ncompile millions of legal documents covering over 20 types of crimes from 31\nprovinces in China for model training. From the extensive dataset, we further\nselect high-quality for supervised fine-tuning, ensuring enhanced relevance and\nprecision. The model further undergoes large-scale reinforcement learning\nwithout additional supervision, emphasizing the enhancement of its reasoning\ncapabilities and explainability. To validate its effectiveness in complex legal\napplications, we also conduct human evaluations with legal experts. We develop\nfine-tuned models based on DeepSeek-R1-Distilled versions, available in three\ndense configurations: 14B, 32B, and 70B.", "published": "2025-03-10 05:54:23", "link": "http://arxiv.org/abs/2503.06949v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Effect of Selection Format on LLM Performance", "abstract": "This paper investigates a critical aspect of large language model (LLM)\nperformance: the optimal formatting of classification task options in prompts.\nThrough an extensive experimental study, we compared two selection formats --\nbullet points and plain English -- to determine their impact on model\nperformance. Our findings suggest that presenting options via bullet points\ngenerally yields better results, although there are some exceptions.\nFurthermore, our research highlights the need for continued exploration of\noption formatting to drive further improvements in model performance.", "published": "2025-03-10 05:11:58", "link": "http://arxiv.org/abs/2503.06926v1", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.ET", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Automatic Speech Recognition for Non-Native English: Accuracy and Disfluency Handling", "abstract": "Automatic speech recognition (ASR) has been an essential component of\ncomputer assisted language learning (CALL) and computer assisted language\ntesting (CALT) for many years. As this technology continues to develop rapidly,\nit is important to evaluate the accuracy of current ASR systems for language\nlearning applications. This study assesses five cutting-edge ASR systems'\nrecognition of non-native accented English speech using recordings from the\nL2-ARCTIC corpus, featuring speakers from six different L1 backgrounds (Arabic,\nChinese, Hindi, Korean, Spanish, and Vietnamese), in the form of both read and\nspontaneous speech. The read speech consisted of 2,400 single sentence\nrecordings from 24 speakers, while the spontaneous speech included narrative\nrecordings from 22 speakers. Results showed that for read speech, Whisper and\nAssemblyAI achieved the best accuracy with mean Match Error Rates (MER) of\n0.054 and 0.056 respectively, approaching human-level accuracy. For spontaneous\nspeech, RevAI performed best with a mean MER of 0.063. The study also examined\nhow each system handled disfluencies such as filler words, repetitions, and\nrevisions, finding significant variation in performance across systems and\ndisfluency types. While processing speed varied considerably between systems,\nlonger processing times did not necessarily correlate with better accuracy. By\ndetailing the performance of several of the most recent, widely-available ASR\nsystems on non-native English speech, this study aims to help language\ninstructors and researchers understand the strengths and weaknesses of each\nsystem and identify which may be suitable for specific use cases.", "published": "2025-03-10 05:09:44", "link": "http://arxiv.org/abs/2503.06924v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "KwaiChat: A Large-Scale Video-Driven Multilingual Mixed-Type Dialogue Corpus", "abstract": "Video-based dialogue systems, such as education assistants, have compelling\napplication value, thereby garnering growing interest. However, the current\nvideo-based dialogue systems are limited by their reliance on a single dialogue\ntype, which hinders their versatility in practical applications across a range\nof scenarios, including question-answering, emotional dialog, etc. In this\npaper, we identify this challenge as how to generate video-driven multilingual\nmixed-type dialogues. To mitigate this challenge, we propose a novel task and\ncreate a human-to-human video-driven multilingual mixed-type dialogue corpus,\ntermed KwaiChat, containing a total of 93,209 videos and 246,080 dialogues,\nacross 4 dialogue types, 30 domains, 4 languages, and 13 topics. Additionally,\nwe establish baseline models on KwaiChat. An extensive analysis of 7 distinct\nLLMs on KwaiChat reveals that GPT-4o achieves the best performance but still\ncannot perform well in this situation even with the help of in-context learning\nand fine-tuning, which indicates that the task is not trivial and needs further\nresearch.", "published": "2025-03-10 04:05:38", "link": "http://arxiv.org/abs/2503.06899v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A LongFormer-Based Framework for Accurate and Efficient Medical Text Summarization", "abstract": "This paper proposes a medical text summarization method based on LongFormer,\naimed at addressing the challenges faced by existing models when processing\nlong medical texts. Traditional summarization methods are often limited by\nshort-term memory, leading to information loss or reduced summary quality in\nlong texts. LongFormer, by introducing long-range self-attention, effectively\ncaptures long-range dependencies in the text, retaining more key information\nand improving the accuracy and information retention of summaries. Experimental\nresults show that the LongFormer-based model outperforms traditional models,\nsuch as RNN, T5, and BERT in automatic evaluation metrics like ROUGE. It also\nreceives high scores in expert evaluations, particularly excelling in\ninformation retention and grammatical accuracy. However, there is still room\nfor improvement in terms of conciseness and readability. Some experts noted\nthat the generated summaries contain redundant information, which affects\nconciseness. Future research will focus on further optimizing the model\nstructure to enhance conciseness and fluency, achieving more efficient medical\ntext summarization. As medical data continues to grow, automated summarization\ntechnology will play an increasingly important role in fields such as medical\nresearch, clinical decision support, and knowledge management.", "published": "2025-03-10 03:33:45", "link": "http://arxiv.org/abs/2503.06888v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lost-in-the-Middle in Long-Text Generation: Synthetic Dataset, Evaluation Framework, and Mitigation", "abstract": "Existing long-text generation methods primarily concentrate on producing\nlengthy texts from short inputs, neglecting the long-input and long-output\ntasks. Such tasks have numerous practical applications while lacking available\nbenchmarks. Moreover, as the input grows in length, existing methods inevitably\nencounter the \"lost-in-the-middle\" phenomenon. In this paper, we first\nintroduce a Long Input and Output Benchmark (LongInOutBench), including a\nsynthetic dataset and a comprehensive evaluation framework, addressing the\nchallenge of the missing benchmark. We then develop the Retrieval-Augmented\nLong-Text Writer (RAL-Writer), which retrieves and restates important yet\noverlooked content, mitigating the \"lost-in-the-middle\" issue by constructing\nexplicit prompts. We finally employ the proposed LongInOutBench to evaluate our\nRAL-Writer against comparable baselines, and the results demonstrate the\neffectiveness of our approach. Our code has been released at\nhttps://github.com/OnlyAR/RAL-Writer.", "published": "2025-03-10 02:44:36", "link": "http://arxiv.org/abs/2503.06868v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhanced Multi-Tuple Extraction for Alloys: Integrating Pointer Networks and Augmented Attention", "abstract": "Extracting high-quality structured information from scientific literature is\ncrucial for advancing material design through data-driven methods. Despite the\nconsiderable research in natural language processing for dataset extraction,\neffective approaches for multi-tuple extraction in scientific literature remain\nscarce due to the complex interrelations of tuples and contextual ambiguities.\nIn the study, we illustrate the multi-tuple extraction of mechanical properties\nfrom multi-principal-element alloys and presents a novel framework that\ncombines an entity extraction model based on MatSciBERT with pointer networks\nand an allocation model utilizing inter- and intra-entity attention. Our\nrigorous experiments on tuple extraction demonstrate impressive F1 scores of\n0.963, 0.947, 0.848, and 0.753 across datasets with 1, 2, 3, and 4 tuples,\nconfirming the effectiveness of the model. Furthermore, an F1 score of 0.854\nwas achieved on a randomly curated dataset. These results highlight the model's\ncapacity to deliver precise and structured information, offering a robust\nalternative to large language models and equipping researchers with essential\ndata for fostering data-driven innovations.", "published": "2025-03-10 02:39:06", "link": "http://arxiv.org/abs/2503.06861v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Concentration via Metastable Mixing, with Applications to the Supercritical Exponential Random Graph Model", "abstract": "It is a folklore belief that metastable wells in low-temperature statistical\nmechanics models exhibit high-temperature behavior. We prove a rigorous version\nof this phenomenon in the setting of the exponential random graph model (ERGM)\nthrough the lens of concentration of measure. To do this, we first present a\nnew general result deriving concentration inequalities in a metastable well\nfrom the metastable mixing of a Markov chain with the appropriate stationary\ndistribution, extending a result of Chatterjee [Cha05] which is suited for more\ntraditional forms of global mixing. We then apply this result to the\nsupercritical (low-temperature) ERGM which was recently proven to exhibit\nmetastable mixing by Bresler, Nagaraj, and Nichani [BNN24], and obtain a novel\nconcentration inequality for Lipschitz observables of the supercritical ERGM\nconditioned on a large metastable well, answering a question posed by [BNN24].\nThis extends a result of Ganguly and Nam [GN24] from the subcritical\n(high-temperature) regime to a metastable well in the supercritical regime, and\nwe are also able to extend the applications of their concentration inequality\nto these metastable wells. Namely, we obtain an upper bound on the Wasserstein\ndistance between the ERGM conditioned on a metastable well and an appropriate\nErd\\H{o}s-R\\'enyi model, as well as derive a central limit theorem for the\ncount of edges in certain small subcollections of possible edges. Finally, to\nsupplement the mathematical content of the article, we also discuss the results\nof what appears to be the first simulation study of a metastable well in the\nsupercritical ERGM.", "published": "2025-03-10 17:40:33", "link": "http://arxiv.org/abs/2503.07571v1", "categories": ["math.PR", "cond-mat.stat-mech", "cs.DM", "math-ph", "math.MP", "math.ST", "stat.TH", "60C05 (Primary) 60B20, 05C80 (Secondary)", "G.3; G.2.2"], "primary_category": "math.PR"}
{"title": "A new density limit for unanimity in majority dynamics on random graphs", "abstract": "Majority dynamics is a process on a simple, undirected graph $G$ with an\ninitial Red/Blue color for every vertex of $G$. Each day, each vertex updates\nits color following the majority among its neighbors, using its previous color\nfor tie-breaking. The dynamics achieves \\textit{unanimity} if every vertex has\nthe same color after finitely many days, and such color is said to\n\\textit{win}.\n  When $G$ is a $G(n,p)$ random graph, L. Tran and Vu (2019) found a codition\nin terms of $p$ and the initial difference $2\\Delta$ beteween the sizes of the\nRed and Blue camps, such that unanimity is achieved with probability\narbitrarily close to 1. They showed that if $p\\Delta^2 \\gg1 $, $p\\Delta \\geq\n100$, and $p\\geq (1+\\varepsilon) n^{-1}\\log n$ for a positive constant\n$\\varepsilon$, then unanimity occurs with probability $1 - o(1)$. If $p$ is not\nextremely small, namely $p > \\log^{-1/16} n $, then Sah and Sawhney (2022)\nshowed that the condition $p\\Delta^2 \\gg 1$ is sufficient.\n  If $n^{-1}\\log^2 n \\ll p \\ll n^{-1/2}\\log^{1/4} n$, we show that\n$p^{3/2}\\Delta \\gg n^{-1/2}\\log n$ is enough. Since this condition holds if\n$p\\Delta \\geq 100$ for $p$ in this range, this is an improvement of Tran's and\nVu's result. For the closely related problem of finding the optimal condition\nfor $p$ to achieve unanimity when the initial coloring is chosen uniformly at\nrandom among all possible Red/Blue assignments, our result implies a new lower\nbound $p \\gg n^{-2/3}\\log^{2/3} n$, which improves upon the previous bound of\n$n^{-3/5}\\log n$ by Chakraborti, Kim, Lee and T. Tran (2021).", "published": "2025-03-10 15:27:26", "link": "http://arxiv.org/abs/2503.07447v1", "categories": ["math.CO", "cs.DM", "math-ph", "math.MP", "05C80, 05C82, 05C85, 05C90", "G.2.2; F.2.2"], "primary_category": "math.CO"}
{"title": "On the expressive power of $2$-edge-colourings of graphs", "abstract": "Given a finite set of $2$-edge-coloured graphs $\\mathcal F$ and a hereditary\nproperty of graphs $\\mathcal{P}$, we say that $\\mathcal F$ expresses\n$\\mathcal{P}$ if a graph $G$ has the property $\\mathcal{P}$ if and only if it\nadmits a $2$-edge-colouring not having any graph in $\\mathcal F$ as an induced\n$2$-edge-coloured subgraph. We show that certain classic hereditary classes are\nexpressible by some set of $2$-edge-coloured graphs on three vertices. We then\ninitiate a systematic study of the following problem. Given a finite set of\n$2$-edge-coloured graphs $\\mathcal F$, structurally characterize the hereditary\nproperty expressed by $\\mathcal F$. In our main results we describe all\nhereditary properties expressed by $\\mathcal F$ when $\\mathcal F$ consists of\n2-edge-coloured graphs on three vertices and (1) patterns have at most two\nedges, or (2) $\\mathcal F$ consists of both monochromatic paths and a set of\ncoloured triangles.\n  On the algorithmic side, we consider the $\\mathcal F$-free colouring problem,\ni.e., deciding if an input graph admits an $\\mathcal F$-free\n$2$-edge-colouring. It follows from our structural characterizations, that for\nall sets considered in (1) and (2) the $\\mathcal F$-free colouring problem is\nsolvable in polynomial time. We complement these tractability results with a\nuniform reduction to boolean constraint satisfaction problems which yield\npolynomial-time algorithms that recognize most graph classes expressible by a\nset $\\mathcal F$ of $2$-edge-coloured graphs on at most three vertices.\nFinally, we exhibit some sets $\\mathcal F$ such that the $\\mathcal F$-free\ncolouring problem is NP-complete.", "published": "2025-03-10 14:57:48", "link": "http://arxiv.org/abs/2503.07409v1", "categories": ["math.CO", "cs.DM"], "primary_category": "math.CO"}
{"title": "Geometric realizations of dichotomous ordinal graphs", "abstract": "A dichotomous ordinal graph consists of an undirected graph with a partition\nof the edges into short and long edges. A geometric realization of a\ndichotomous ordinal graph $G$ in a metric space $X$ is a drawing of $G$ in $X$\nin which every long edge is strictly longer than every short edge. We call a\ngraph $G$ pandichotomous in $X$ if $G$ admits a geometric realization in $X$\nfor every partition of its edge set into short and long edges. We exhibit a\nvery close relationship between the degeneracy of a graph $G$ and its\npandichotomic Euclidean or spherical dimension, that is, the smallest dimension\n$k$ such that $G$ is pandichotomous in $\\mathbb{R}^k$ or the sphere\n$\\mathbb{S}^k$, respectively. First, every $d$-degenerate graph is\npandichotomous in $\\mathbb{R}^{d}$ and $\\mathbb{S}^{d-1}$ and these bounds are\ntight for the sphere and for $\\mathbb{R}^2$ and almost tight for\n$\\mathbb{R}^d$, for $d\\ge 3$. Second, every $n$-vertex graph that is\npandichotomous in $\\mathbb{R}^k$ has at most $\\mu kn$ edges, for some absolute\nconstant $\\mu<7.23$. This shows that the pandichotomic Euclidean dimension of\nany graph is linearly tied to its degeneracy and in the special cases $k\\in\n\\{1,2\\}$ resolves open problems posed by Alam, Kobourov, Pupyrev, and\nToeniskoetter. Further, we characterize which complete bipartite graphs are\npandichotomous in $\\mathbb{R}^2$: These are exactly the $K_{m,n}$ with $m\\le 3$\nor $m=4$ and $n\\le 6$. For general bipartite graphs, we can guarantee\nrealizations in $\\mathbb{R}^2$ if the short or the long subgraph is\nconstrained: namely if the short subgraph is outerplanar or a subgraph of a\nrectangular grid, or if the long subgraph forms a caterpillar.", "published": "2025-03-10 14:18:24", "link": "http://arxiv.org/abs/2503.07361v1", "categories": ["cs.CG", "cs.DM", "cs.DS", "math.CO", "05C10, 05C62, 68R10, 52C99", "F.2.2; I.3.5"], "primary_category": "cs.CG"}
{"title": "Playing Sudoku on random 3-regular graphs", "abstract": "The Sudoku number $s(G)$ of graph $G$ with chromatic number $\\chi(G)$ is the\nsmallest partial $\\chi(G)$-colouring of $G$ that determines a unique\n$\\chi(G)$-colouring of the entire graph. We show that the Sudoku number of the\nrandom $3$-regular graph $\\mathcal{G}_{n,3}$ satisfies $s(\\mathcal{G}_{n,3})\n\\leq (1+o(1))\\frac{n}{3}$ asymptotically almost surely. We prove this by\nanalyzing an algorithm which $3$-colours $\\mathcal{G}_{n,3}$ in a way that\nproduces many locally forced vertices, i.e., vertices which see two distinct\ncolours among their neighbours. The intricacies of the algorithm present some\nchallenges for the analysis, and to overcome these we use a non-standard\napplication of Wormald's differential equations method that incorporates tools\nfrom finite Markov chains.", "published": "2025-03-10 13:50:03", "link": "http://arxiv.org/abs/2503.07335v1", "categories": ["math.CO", "cs.DM"], "primary_category": "math.CO"}
{"title": "A Quadratic Vertex Kernel and a Subexponential Algorithm for Subset-FAST", "abstract": "In the Subset Feedback Arc Set in Tournaments, Subset-FAST problem we are\ngiven as input a tournament $T$ with a vertex set $V(T)$ and an arc set $A(T)$,\nalong with a terminal set $S \\subseteq V(T)$, and an integer $ k$. The\nobjective is to determine whether there exists a set $ F \\subseteq A(T) $ with\n$|F| \\leq k$ such that the resulting graph $T-F $ contains no cycle that\nincludes any vertex of $S$. When $S=V(T)$ this is the classic Feedback Arc Set\nin Tournaments (FAST) problem. We obtain the first polynomial kernel for this\nproblem parameterized by the solution size. More precisely, we obtain an\nalgorithm that, given an input instance $(T, S, k)$, produces an equivalent\ninstance $(T',S',k')$ with $k'\\leq k$ and $V(T')=O(k^2)$.\n  It was known that FAST admits a simple quadratic vertex kernel and a\nnon-trivial linear vertex kernel. However, no such kernel was previously known\nfor Subset-FAST. Our kernel employs variants of the most well-known reduction\nrules for FAST and introduces two new reduction rules to identify irrelevant\nvertices. As a result of our kernelization, we also obtain the first\nsub-exponential time FPT algorithm for Subset-FAST.", "published": "2025-03-10 11:47:40", "link": "http://arxiv.org/abs/2503.07208v1", "categories": ["cs.DM", "cs.DS", "math.CO"], "primary_category": "cs.DM"}
{"title": "Near Triple Arrays", "abstract": "We introduce near triple arrays as binary row-column designs with at most two\nconsecutive values for the replication numbers of symbols, for the intersection\nsizes of pairs of rows, pairs of columns and pairs of a row and a column. Near\ntriple arrays form a common generalization of such well-studied classes of\ndesigns as triple arrays, (near) Youden rectangles and Latin squares.\n  We enumerate near triple arrays for a range of small parameter sets and show\nthat they exist in the vast majority of the cases considered. As a byproduct,\nwe obtain the first complete enumerations of $6 \\times 10$ triple arrays on\n$15$ symbols, $7 \\times 8$ triple arrays on $14$ symbols and $5 \\times 16$\ntriple arrays on $20$ symbols.\n  Next, we give several constructions for families of near triple arrays, and\ne.g. show that near triple arrays with 3 rows and at least 6 columns exist for\nany number of symbols. Finally, we investigate a duality between row and column\nintersection sizes of a row-column design, and covering numbers for pairs of\nsymbols by rows and columns. These duality results are used to obtain necessary\nconditions for the existence of near triple arrays. This duality also provides\na new unified approach to earlier results on triple arrays and balanced grids.", "published": "2025-03-10 10:42:56", "link": "http://arxiv.org/abs/2503.07166v1", "categories": ["math.CO", "cs.DM"], "primary_category": "math.CO"}
{"title": "A Reliable Self-Organized Distributed Complex Network for Communication of Smart Agents", "abstract": "Collaboration is a fundamental and essential characteristic of many complex\nsystems, ranging from ant colonies to human societies. Each component within a\ncomplex system interacts with others, even at a distance, to accomplish a given\ntask. A network of collaboration can be defined to study the collective\nbehavior of such systems within the framework of complex networks. The nodes in\nthese networks may represent simple organisms or more sophisticated intelligent\nagents, such as humans. In this study, we utilize intelligent agents (nodes)\ntrained through reinforcement learning techniques to establish connections with\ntheir neighbors, ultimately leading to the emergence of a large-scale\ncommunication cluster. Notably, there is no centralized administrator; instead,\nagents must adjust their connections based on information obtained from local\nobservations. The connection strategy is formulated using a physical\nHamiltonian, thereby categorizing this intelligent system under the paradigm of\n\"Physics-Guided Machine Learning\".", "published": "2025-03-10 17:46:52", "link": "http://arxiv.org/abs/2503.07702v1", "categories": ["cs.MA"], "primary_category": "cs.MA"}
{"title": "Artificial Utopia: Simulation and Intelligent Agents for a Democratised Future", "abstract": "Prevailing top-down systems in politics and economics struggle to keep pace\nwith the pressing challenges of the 21st century, such as climate change,\nsocial inequality and conflict. Bottom-up democratisation and participatory\napproaches in politics and economics are increasingly seen as promising\nalternatives to confront and overcome these issues, often with utopian\novertones, as proponents believe they may dramatically reshape political,\nsocial and ecological futures for the better and in contrast to contemporary\nauthoritarian tendencies across various countries. Institutional specifics and\nthe associated collective human behavior or culture remains little understood\nand debated, however. In this article, I propose a novel research agenda\nfocusing on utopian democratisation efforts with formal and computational\nmethods as well as with artificial intelligence - I call this agenda Artificial\nUtopia. Artificial Utopias provide safe testing grounds for new political ideas\nand economic policies in-silico with reduced risk of negative consequences as\ncompared to testing ideas in real-world contexts. An increasing number of\nadvanced simulation and intelligence methods, that aim at representing human\ncognition and collective decision-making in more realistic ways, could benefit\nthis process. This includes agent-based modelling, reinforcement learning,\nlarge language models and more. I clarify what some of these simulation\napproaches can contribute to the study of Artificial Utopias with the help of\ntwo institutional examples: the citizen assembly and the democratic firm.", "published": "2025-03-10 14:20:58", "link": "http://arxiv.org/abs/2503.07364v1", "categories": ["cs.MA", "cs.AI", "cs.CY"], "primary_category": "cs.MA"}
{"title": "Human Machine Co-Adaptation Model and Its Convergence Analysis", "abstract": "The key to robot-assisted rehabilitation lies in the design of the\nhuman-machine interface, which must accommodate the needs of both patients and\nmachines. Current interface designs primarily focus on machine control\nalgorithms, often requiring patients to spend considerable time adapting. In\nthis paper, we introduce a novel approach based on the Cooperative Adaptive\nMarkov Decision Process (CAMDPs) model to address the fundamental aspects of\nthe interactive learning process, offering theoretical insights and practical\nguidance. We establish sufficient conditions for the convergence of CAMDPs and\nensure the uniqueness of Nash equilibrium points. Leveraging these conditions,\nwe guarantee the system's convergence to a unique Nash equilibrium point.\nFurthermore, we explore scenarios with multiple Nash equilibrium points,\ndevising strategies to adjust both Value Evaluation and Policy Improvement\nalgorithms to enhance the likelihood of converging to the global minimal Nash\nequilibrium point. Through numerical experiments, we illustrate the\neffectiveness of the proposed conditions and algorithms, demonstrating their\napplicability and robustness in practical settings. The proposed conditions for\nconvergence and the identification of a unique optimal Nash equilibrium\ncontribute to the development of more effective adaptive systems for human\nusers in robot-assisted rehabilitation.", "published": "2025-03-10 13:36:36", "link": "http://arxiv.org/abs/2503.07319v1", "categories": ["cs.AI", "cs.HC", "cs.MA", "cs.RO"], "primary_category": "cs.AI"}
{"title": "Adaptive routing protocols for determining optimal paths in AI multi-agent systems: a priority- and learning-enhanced approach", "abstract": "As distributed artificial intelligence (AI) and multi-agent architectures\ngrow increasingly complex, the need for adaptive, context-aware routing becomes\nparamount. This paper introduces an enhanced, adaptive routing algorithm\ntailored for AI multi-agent networks, integrating priority-based cost functions\nand dynamic learning mechanisms. Building on an extended Dijkstra-based\nframework, we incorporate multi-faceted parameters such as task complexity,\nuser request priority, agent capabilities, bandwidth, latency, load, model\nsophistication, and reliability. We further propose dynamically adaptive\nweighting factors, tuned via reinforcement learning (RL), to continuously\nevolve routing policies based on observed network performance. Additionally,\nheuristic filtering and hierarchical routing structures improve scalability and\nresponsiveness. Our approach yields context-sensitive, load-aware, and\npriority-focused routing decisions that not only reduce latency for critical\ntasks but also optimize overall resource utilization, ultimately enhancing the\nrobustness, flexibility, and efficiency of multi-agent systems.", "published": "2025-03-10 13:16:54", "link": "http://arxiv.org/abs/2503.07686v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA"}
{"title": "Using a single actor to output personalized policy for different intersections", "abstract": "Recently, with the development of Multi-agent reinforcement learning (MARL),\nadaptive traffic signal control (ATSC) has achieved satisfactory results. In\ntraffic scenarios with multiple intersections, MARL treats each intersection as\nan agent and optimizes traffic signal control strategies through learning and\nreal-time decision-making. Considering that observation distributions of\nintersections might be different in real-world scenarios, shared parameter\nmethods might lack diversity and thus lead to high generalization requirements\nin the shared-policy network. A typical solution is to increase the size of\nnetwork parameters. However, simply increasing the scale of the network does\nnot necessarily improve policy generalization, which is validated in our\nexperiments. Accordingly, an approach that considers both the personalization\nof intersections and the efficiency of parameter sharing is required. To this\nend, we propose Hyper-Action Multi-Head Proximal Policy Optimization\n(HAMH-PPO), a Centralized Training with Decentralized Execution (CTDE) MARL\nmethod that utilizes a shared PPO policy network to deliver personalized\npolicies for intersections with non-iid observation distributions. The\ncentralized critic in HAMH-PPO uses graph attention units to calculate the\ngraph representations of all intersections and outputs a set of value estimates\nwith multiple output heads for each intersection. The decentralized execution\nactor takes the local observation history as input and output distributions of\naction as well as a so-called hyper-action to balance the multiple values\nestimated from the centralized critic to further guide the updating of TSC\npolicies. The combination of hyper-action and multi-head values enables\nmultiple agents to share a single actor-critic while achieving personalized\npolicies.", "published": "2025-03-10 07:55:33", "link": "http://arxiv.org/abs/2503.07678v1", "categories": ["cs.LG", "cs.AI", "cs.MA"], "primary_category": "cs.LG"}
{"title": "DynTaskMAS: A Dynamic Task Graph-driven Framework for Asynchronous and Parallel LLM-based Multi-Agent Systems", "abstract": "The emergence of Large Language Models (LLMs) in Multi-Agent Systems (MAS)\nhas opened new possibilities for artificial intelligence, yet current\nimplementations face significant challenges in resource management, task\ncoordination, and system efficiency. While existing frameworks demonstrate the\npotential of LLM-based agents in collaborative problem-solving, they often lack\nsophisticated mechanisms for parallel execution and dynamic task management.\nThis paper introduces DynTaskMAS, a novel framework that orchestrates\nasynchronous and parallel operations in LLM-based MAS through dynamic task\ngraphs. The framework features four key innovations: (1) a Dynamic Task Graph\nGenerator that intelligently decomposes complex tasks while maintaining logical\ndependencies, (2) an Asynchronous Parallel Execution Engine that optimizes\nresource utilization through efficient task scheduling, (3) a Semantic-Aware\nContext Management System that enables efficient information sharing among\nagents, and (4) an Adaptive Workflow Manager that dynamically optimizes system\nperformance. Experimental evaluations demonstrate that DynTaskMAS achieves\nsignificant improvements over traditional approaches: a 21-33% reduction in\nexecution time across task complexities (with higher gains for more complex\ntasks), a 35.4% improvement in resource utilization (from 65% to 88%), and\nnear-linear throughput scaling up to 16 concurrent agents (3.47X improvement\nfor 4X agents). Our framework establishes a foundation for building scalable,\nhigh-performance LLM-based multi-agent systems capable of handling complex,\ndynamic tasks efficiently.", "published": "2025-03-10 06:16:10", "link": "http://arxiv.org/abs/2503.07675v1", "categories": ["cs.MA", "cs.AI", "cs.DC"], "primary_category": "cs.MA"}
{"title": "The potential role of AI agents in transforming nuclear medicine research and cancer management in India", "abstract": "India faces a significant cancer burden, with an incidence-to-mortality ratio\nindicating that nearly three out of five individuals diagnosed with cancer\nsuccumb to the disease. While the limitations of physical healthcare\ninfrastructure are widely acknowledged as a primary challenge, concerted\nefforts by government and healthcare agencies are underway to mitigate these\nconstraints. However, given the country's vast geography and high population\ndensity, it is imperative to explore alternative soft infrastructure solutions\nto complement existing frameworks. Artificial Intelligence agents are\nincreasingly transforming problem-solving approaches across various domains,\nwith their application in medicine proving particularly transformative. In this\nperspective, we examine the potential role of AI agents in advancing nuclear\nmedicine for cancer research, diagnosis, and management in India. We begin with\na brief overview of AI agents and their capabilities, followed by a proposed\nagent-based ecosystem that can address prevailing sustainability challenges in\nIndia nuclear medicine.", "published": "2025-03-10 01:30:07", "link": "http://arxiv.org/abs/2503.07673v1", "categories": ["cs.MA", "cs.AI", "cs.CY"], "primary_category": "cs.MA"}
{"title": "Can Proof Assistants Verify Multi-Agent Systems?", "abstract": "This paper presents the Soda language for verifying multi-agent systems. Soda\nis a high-level functional and object-oriented language that supports the\ncompilation of its code not only to Scala, a strongly statically typed\nhigh-level programming language, but also to Lean, a proof assistant and\nprogramming language. Given these capabilities, Soda can implement multi-agent\nsystems, or parts thereof, that can then be integrated into a mainstream\nsoftware ecosystem on the one hand and formally verified with state-of-the-art\ntools on the other hand. We provide a brief and informal introduction to Soda\nand the aforementioned interoperability capabilities, as well as a simple\ndemonstration of how interaction protocols can be designed and verified with\nSoda. In the course of the demonstration, we highlight challenges with respect\nto real-world applicability.", "published": "2025-03-10 00:24:29", "link": "http://arxiv.org/abs/2503.06812v1", "categories": ["cs.PL", "cs.AI", "cs.LO", "cs.MA"], "primary_category": "cs.PL"}
{"title": "Incentive-Compatible Recovery from Manipulated Signals, with Applications to Decentralized Physical Infrastructure", "abstract": "We introduce the first formal model capturing the elicitation of unverifiable\ninformation from a party (the \"source\") with implicit signals derived by other\nplayers (the \"observers\"). Our model is motivated in part by applications in\ndecentralized physical infrastructure networks (a.k.a. \"DePIN\"), an emerging\napplication domain in which physical services (e.g., sensor information,\nbandwidth, or energy) are provided at least in part by untrusted and\nself-interested parties. A key challenge in these signal network applications\nis verifying the level of service that was actually provided by network\nparticipants.\n  We first establish a condition called source identifiability, which we show\nis necessary for the existence of a mechanism for which truthful signal\nreporting is a strict equilibrium. For a converse, we build on techniques from\npeer prediction to show that in every signal network that satisfies the source\nidentifiability condition, there is in fact a strictly truthful mechanism,\nwhere truthful signal reporting gives strictly higher total expected payoff\nthan any less informative equilibrium. We furthermore show that this truthful\nequilibrium is in fact the unique equilibrium of the mechanism if there is\npositive probability that any one observer is unconditionally honest (e.g., if\nan observer were run by the network owner). Also, by extending our condition to\ncoalitions, we show that there are generally no collusion-resistant mechanisms\nin the settings that we consider.\n  We apply our framework and results to two DePIN applications: proving\nlocation, and proving bandwidth. In the location-proving setting observers\nlearn (potentially enlarged) Euclidean distances to the source. Here, our\ncondition has an appealing geometric interpretation, implying that the source's\nlocation can be truthfully elicited if and only if it is guaranteed to lie\ninside the convex hull of the observers.", "published": "2025-03-10 17:28:45", "link": "http://arxiv.org/abs/2503.07558v1", "categories": ["cs.GT", "cs.ET", "cs.LG", "econ.TH", "q-fin.TR"], "primary_category": "cs.GT"}
{"title": "FinTSBridge: A New Evaluation Suite for Real-world Financial Prediction with Advanced Time Series Models", "abstract": "Despite the growing attention to time series forecasting in recent years,\nmany studies have proposed various solutions to address the challenges\nencountered in time series prediction, aiming to improve forecasting\nperformance. However, effectively applying these time series forecasting models\nto the field of financial asset pricing remains a challenging issue. There is\nstill a need for a bridge to connect cutting-edge time series forecasting\nmodels with financial asset pricing. To bridge this gap, we have undertaken the\nfollowing efforts: 1) We constructed three datasets from the financial domain;\n2) We selected over ten time series forecasting models from recent studies and\nvalidated their performance in financial time series; 3) We developed new\nmetrics, msIC and msIR, in addition to MSE and MAE, to showcase the time series\ncorrelation captured by the models; 4) We designed financial-specific tasks for\nthese three datasets and assessed the practical performance and application\npotential of these forecasting models in important financial problems. We hope\nthe developed new evaluation suite, FinTSBridge, can provide valuable insights\ninto the effectiveness and robustness of advanced forecasting models in\nfinanical domains.", "published": "2025-03-10 05:19:13", "link": "http://arxiv.org/abs/2503.06928v1", "categories": ["cs.LG", "q-fin.TR"], "primary_category": "cs.LG"}
{"title": "Impact of Microphone Array Mismatches to Learning-based Replay Speech Detection", "abstract": "In this work, we investigate the generalization of a multi-channel\nlearning-based replay speech detector, which employs adaptive beamforming and\ndetection, across different microphone arrays. In general, deep neural\nnetwork-based microphone array processing techniques generalize poorly to\nunseen array types, i.e., showing a significant training-test mismatch of\nperformance. We employ the ReMASC dataset to analyze performance degradation\ndue to inter- and intra-device mismatches, assessing both single- and\nmulti-channel configurations. Furthermore, we explore fine-tuning to mitigate\nthe performance loss when transitioning to unseen microphone arrays. Our\nfindings reveal that array mismatches significantly decrease detection\naccuracy, with intra-device generalization being more robust than inter-device.\nHowever, fine-tuning with as little as ten minutes of target data can\neffectively recover performance, providing insights for practical deployment of\nreplay detection systems in heterogeneous automatic speaker verification\nenvironments.", "published": "2025-03-10 14:14:35", "link": "http://arxiv.org/abs/2503.07357v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Score-informed Music Source Separation: Improving Synthetic-to-real Generalization in Classical Music", "abstract": "Music source separation is the task of separating a mixture of instruments\ninto constituent tracks. Music source separation models are typically trained\nusing only audio data, although additional information can be used to improve\nthe model's separation capability. In this paper, we propose two ways of using\nmusical scores to aid music source separation: a score-informed model where the\nscore is concatenated with the magnitude spectrogram of the audio mixture as\nthe input of the model, and a model where we use only the score to calculate\nthe separation mask. We train our models on synthetic data in the SynthSOD\ndataset and evaluate our methods on the URMP and Aalto anechoic orchestra\ndatasets, comprised of real recordings. The score-informed model improves\nseparation results compared to a baseline approach, but struggles to generalize\nfrom synthetic to real data, whereas the score-only model shows a clear\nimprovement in synthetic-to-real generalization.", "published": "2025-03-10 14:08:31", "link": "http://arxiv.org/abs/2503.07352v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Synchronized Video-to-Audio Generation via Mel Quantization-Continuum Decomposition", "abstract": "Video-to-audio generation is essential for synthesizing realistic audio\ntracks that synchronize effectively with silent videos. Following the\nperspective of extracting essential signals from videos that can precisely\ncontrol the mature text-to-audio generative diffusion models, this paper\npresents how to balance the representation of mel-spectrograms in terms of\ncompleteness and complexity through a new approach called Mel\nQuantization-Continuum Decomposition (Mel-QCD). We decompose the\nmel-spectrogram into three distinct types of signals, employing quantization or\ncontinuity to them, we can effectively predict them from video by a devised\nvideo-to-all (V2X) predictor. Then, the predicted signals are recomposed and\nfed into a ControlNet, along with a textual inversion design, to control the\naudio generation process. Our proposed Mel-QCD method demonstrates\nstate-of-the-art performance across eight metrics, evaluating dimensions such\nas quality, synchronization, and semantic consistency. Our codes and demos will\nbe released at \\href{Website}{https://wjc2830.github.io/MelQCD/}.", "published": "2025-03-10 07:04:03", "link": "http://arxiv.org/abs/2503.06984v1", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
