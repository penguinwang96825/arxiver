{"title": "Input Augmentation Improves Constrained Beam Search for Neural Machine\n  Translation: NTT at WAT 2021", "abstract": "This paper describes our systems that were submitted to the restricted\ntranslation task at WAT 2021. In this task, the systems are required to output\ntranslated sentences that contain all given word constraints. Our system\ncombined input augmentation and constrained beam search algorithms. Through\nexperiments, we found that this combination significantly improves translation\naccuracy and can save inference time while containing all the constraints in\nthe output. For both En->Ja and Ja->En, our systems obtained the best\nevaluation performances in automatic evaluation.", "published": "2021-06-10 01:39:59", "link": "http://arxiv.org/abs/2106.05450v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Variational Information Bottleneck for Effective Low-Resource\n  Fine-Tuning", "abstract": "While large-scale pretrained language models have obtained impressive results\nwhen fine-tuned on a wide variety of tasks, they still often suffer from\noverfitting in low-resource scenarios. Since such models are general-purpose\nfeature extractors, many of these features are inevitably irrelevant for a\ngiven target task. We propose to use Variational Information Bottleneck (VIB)\nto suppress irrelevant features when fine-tuning on low-resource target tasks,\nand show that our method successfully reduces overfitting. Moreover, we show\nthat our VIB model finds sentence representations that are more robust to\nbiases in natural language inference datasets, and thereby obtains better\ngeneralization to out-of-domain datasets. Evaluation on seven low-resource\ndatasets in different tasks shows that our method significantly improves\ntransfer learning in low-resource scenarios, surpassing prior work. Moreover,\nit improves generalization on 13 out of 15 out-of-domain natural language\ninference benchmarks. Our code is publicly available in\nhttps://github.com/rabeehk/vibert.", "published": "2021-06-10 03:08:13", "link": "http://arxiv.org/abs/2106.05469v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Convolutions and Self-Attention: Re-interpreting Relative Positions in\n  Pre-trained Language Models", "abstract": "In this paper, we detail the relationship between convolutions and\nself-attention in natural language tasks. We show that relative position\nembeddings in self-attention layers are equivalent to recently-proposed dynamic\nlightweight convolutions, and we consider multiple new ways of integrating\nconvolutions into Transformer self-attention. Specifically, we propose\ncomposite attention, which unites previous relative position embedding methods\nunder a convolutional framework. We conduct experiments by training BERT with\ncomposite attention, finding that convolutions consistently improve performance\non multiple downstream tasks, replacing absolute position embeddings. To inform\nfuture work, we present results comparing lightweight convolutions, dynamic\nconvolutions, and depthwise-separable convolutions in language model\npre-training, considering multiple injection points for convolutions in\nself-attention layers.", "published": "2021-06-10 05:11:35", "link": "http://arxiv.org/abs/2106.05505v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CogAlign: Learning to Align Textual Neural Representations to Cognitive\n  Language Processing Signals", "abstract": "Most previous studies integrate cognitive language processing signals (e.g.,\neye-tracking or EEG data) into neural models of natural language processing\n(NLP) just by directly concatenating word embeddings with cognitive features,\nignoring the gap between the two modalities (i.e., textual vs. cognitive) and\nnoise in cognitive features. In this paper, we propose a CogAlign approach to\nthese issues, which learns to align textual neural representations to cognitive\nfeatures. In CogAlign, we use a shared encoder equipped with a modality\ndiscriminator to alternatively encode textual and cognitive inputs to capture\ntheir differences and commonalities. Additionally, a text-aware attention\nmechanism is proposed to detect task-related information and to avoid using\nnoise in cognitive features. Experimental results on three NLP tasks, namely\nnamed entity recognition, sentiment analysis and relation extraction, show that\nCogAlign achieves significant improvements with multiple cognitive features\nover state-of-the-art models on public datasets. Moreover, our model is able to\ntransfer cognitive information to other datasets that do not have any cognitive\nprocessing signals.", "published": "2021-06-10 07:10:25", "link": "http://arxiv.org/abs/2106.05544v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Progressive Multi-Granularity Training for Non-Autoregressive\n  Translation", "abstract": "Non-autoregressive translation (NAT) significantly accelerates the inference\nprocess via predicting the entire target sequence. However, recent studies show\nthat NAT is weak at learning high-mode of knowledge such as one-to-many\ntranslations. We argue that modes can be divided into various granularities\nwhich can be learned from easy to hard. In this study, we empirically show that\nNAT models are prone to learn fine-grained lower-mode knowledge, such as words\nand phrases, compared with sentences. Based on this observation, we propose\nprogressive multi-granularity training for NAT. More specifically, to make the\nmost of the training data, we break down the sentence-level examples into three\ntypes, i.e. words, phrases, sentences, and with the training goes, we\nprogressively increase the granularities. Experiments on Romanian-English,\nEnglish-German, Chinese-English, and Japanese-English demonstrate that our\napproach improves the phrase translation accuracy and model reordering ability,\ntherefore resulting in better translation quality against strong NAT baselines.\nAlso, we show that more deterministic fine-grained knowledge can further\nenhance performance.", "published": "2021-06-10 07:16:07", "link": "http://arxiv.org/abs/2106.05546v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Shades of BLEU, Flavours of Success: The Case of MultiWOZ", "abstract": "The MultiWOZ dataset (Budzianowski et al.,2018) is frequently used for\nbenchmarking context-to-response abilities of task-oriented dialogue systems.\nIn this work, we identify inconsistencies in data preprocessing and reporting\nof three corpus-based metrics used on this dataset, i.e., BLEU score and Inform\n& Success rates. We point out a few problems of the MultiWOZ benchmark such as\nunsatisfactory preprocessing, insufficient or under-specified evaluation\nmetrics, or rigid database. We re-evaluate 7 end-to-end and 6 policy\noptimization models in as-fair-as-possible setups, and we show that their\nreported scores cannot be directly compared. To facilitate comparison of future\nsystems, we release our stand-alone standardized evaluation scripts. We also\ngive basic recommendations for corpus-based benchmarking in future works.", "published": "2021-06-10 07:33:53", "link": "http://arxiv.org/abs/2106.05555v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AGGGEN: Ordering and Aggregating while Generating", "abstract": "We present AGGGEN (pronounced 'again'), a data-to-text model which\nre-introduces two explicit sentence planning stages into neural data-to-text\nsystems: input ordering and input aggregation. In contrast to previous work\nusing sentence planning, our model is still end-to-end: AGGGEN performs\nsentence planning at the same time as generating text by learning latent\nalignments (via semantic facts) between input representation and target text.\nExperiments on the WebNLG and E2E challenge data show that by using fact-based\nalignments our approach is more interpretable, expressive, robust to noise, and\neasier to control, while retaining the advantages of end-to-end systems in\nterms of fluency. Our code is available at https://github.com/XinnuoXu/AggGen.", "published": "2021-06-10 08:14:59", "link": "http://arxiv.org/abs/2106.05580v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AUGNLG: Few-shot Natural Language Generation using Self-trained Data\n  Augmentation", "abstract": "Natural Language Generation (NLG) is a key component in a task-oriented\ndialogue system, which converts the structured meaning representation (MR) to\nthe natural language. For large-scale conversational systems, where it is\ncommon to have over hundreds of intents and thousands of slots, neither\ntemplate-based approaches nor model-based approaches are scalable. Recently,\nneural NLGs started leveraging transfer learning and showed promising results\nin few-shot settings. This paper proposes AUGNLG, a novel data augmentation\napproach that combines a self-trained neural retrieval model with a few-shot\nlearned NLU model, to automatically create MR-to-Text data from open-domain\ntexts. The proposed system mostly outperforms the state-of-the-art methods on\nthe FewShotWOZ data in both BLEU and Slot Error Rate. We further confirm\nimproved results on the FewShotSGD data and provide comprehensive analysis\nresults on key components of our system. Our code and data are available at\nhttps://github.com/XinnuoXu/AugNLG.", "published": "2021-06-10 08:45:28", "link": "http://arxiv.org/abs/2106.05589v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "VT-SSum: A Benchmark Dataset for Video Transcript Segmentation and\n  Summarization", "abstract": "Video transcript summarization is a fundamental task for video understanding.\nConventional approaches for transcript summarization are usually built upon the\nsummarization data for written language such as news articles, while the domain\ndiscrepancy may degrade the model performance on spoken text. In this paper, we\npresent VT-SSum, a benchmark dataset with spoken language for video transcript\nsegmentation and summarization, which includes 125K transcript-summary pairs\nfrom 9,616 videos. VT-SSum takes advantage of the videos from VideoLectures.NET\nby leveraging the slides content as the weak supervision to generate the\nextractive summary for video transcripts. Experiments with a state-of-the-art\ndeep learning approach show that the model trained with VT-SSum brings a\nsignificant improvement on the AMI spoken text summarization benchmark. VT-SSum\nis publicly available at https://github.com/Dod-o/VT-SSum to support the future\nresearch of video transcript segmentation and summarization tasks.", "published": "2021-06-10 09:19:58", "link": "http://arxiv.org/abs/2106.05606v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Unsupervised Pretraining Objectives for Machine Translation", "abstract": "Unsupervised cross-lingual pretraining has achieved strong results in neural\nmachine translation (NMT), by drastically reducing the need for large parallel\ndata. Most approaches adapt masked-language modeling (MLM) to\nsequence-to-sequence architectures, by masking parts of the input and\nreconstructing them in the decoder. In this work, we systematically compare\nmasking with alternative objectives that produce inputs resembling real (full)\nsentences, by reordering and replacing words based on their context. We\npretrain models with different methods on English$\\leftrightarrow$German,\nEnglish$\\leftrightarrow$Nepali and English$\\leftrightarrow$Sinhala monolingual\ndata, and evaluate them on NMT. In (semi-) supervised NMT, varying the\npretraining objective leads to surprisingly small differences in the finetuned\nperformance, whereas unsupervised NMT is much more sensitive to it. To\nunderstand these results, we thoroughly study the pretrained models using a\nseries of probes and verify that they encode and use information in different\nways. We conclude that finetuning on parallel data is mostly sensitive to few\nproperties that are shared by most models, such as a strong decoder, in\ncontrast to unsupervised NMT that also requires models with strong\ncross-lingual abilities.", "published": "2021-06-10 10:18:23", "link": "http://arxiv.org/abs/2106.05634v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DT-grams: Structured Dependency Grammar Stylometry for Cross-Language\n  Authorship Attribution", "abstract": "Cross-language authorship attribution problems rely on either translation to\nenable the use of single-language features, or language-independent feature\nextraction methods. Until recently, the lack of datasets for this problem\nhindered the development of the latter, and single-language solutions were\nperformed on machine-translated corpora. In this paper, we present a novel\nlanguage-independent feature for authorship analysis based on dependency graphs\nand universal part of speech tags, called DT-grams (dependency tree grams),\nwhich are constructed by selecting specific sub-parts of the dependency graph\nof sentences. We evaluate DT-grams by performing cross-language authorship\nattribution on untranslated datasets of bilingual authors, showing that, on\naverage, they achieve a macro-averaged F1 score of 0.081 higher than previous\nmethods across five different language pairs. Additionally, by providing\nresults for a diverse set of features for comparison, we provide a baseline on\nthe previously undocumented task of untranslated cross-language authorship\nattribution.", "published": "2021-06-10 11:50:07", "link": "http://arxiv.org/abs/2106.05677v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Marginal Utility Diminishes: Exploring the Minimum Knowledge for BERT\n  Knowledge Distillation", "abstract": "Recently, knowledge distillation (KD) has shown great success in BERT\ncompression. Instead of only learning from the teacher's soft label as in\nconventional KD, researchers find that the rich information contained in the\nhidden layers of BERT is conducive to the student's performance. To better\nexploit the hidden knowledge, a common practice is to force the student to\ndeeply mimic the teacher's hidden states of all the tokens in a layer-wise\nmanner. In this paper, however, we observe that although distilling the\nteacher's hidden state knowledge (HSK) is helpful, the performance gain\n(marginal utility) diminishes quickly as more HSK is distilled. To understand\nthis effect, we conduct a series of analysis. Specifically, we divide the HSK\nof BERT into three dimensions, namely depth, length and width. We first\ninvestigate a variety of strategies to extract crucial knowledge for each\nsingle dimension and then jointly compress the three dimensions. In this way,\nwe show that 1) the student's performance can be improved by extracting and\ndistilling the crucial HSK, and 2) using a tiny fraction of HSK can achieve the\nsame performance as extensive HSK distillation. Based on the second finding, we\nfurther propose an efficient KD paradigm to compress BERT, which does not\nrequire loading the teacher during the training of student. For two kinds of\nstudent models and computing devices, the proposed KD paradigm gives rise to\ntraining speedup of 2.7x ~ 3.4x.", "published": "2021-06-10 12:21:47", "link": "http://arxiv.org/abs/2106.05691v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FEVEROUS: Fact Extraction and VERification Over Unstructured and\n  Structured information", "abstract": "Fact verification has attracted a lot of attention in the machine learning\nand natural language processing communities, as it is one of the key methods\nfor detecting misinformation. Existing large-scale benchmarks for this task\nhave focused mostly on textual sources, i.e. unstructured information, and thus\nignored the wealth of information available in structured formats, such as\ntables. In this paper we introduce a novel dataset and benchmark, Fact\nExtraction and VERification Over Unstructured and Structured information\n(FEVEROUS), which consists of 87,026 verified claims. Each claim is annotated\nwith evidence in the form of sentences and/or cells from tables in Wikipedia,\nas well as a label indicating whether this evidence supports, refutes, or does\nnot provide enough information to reach a verdict. Furthermore, we detail our\nefforts to track and minimize the biases present in the dataset and could be\nexploited by models, e.g. being able to predict the label without using\nevidence. Finally, we develop a baseline for verifying claims against text and\ntables which predicts both the correct evidence and verdict for 18% of the\nclaims.", "published": "2021-06-10 12:47:36", "link": "http://arxiv.org/abs/2106.05707v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Construction of Context-Aware Sentiment Lexicon in the\n  Financial Domain Using Direction-Dependent Words", "abstract": "Increasing attention has been drawn to the sentiment analysis of financial\ndocuments. The most popular examples of such documents include analyst reports\nand economic news, the analysis of which is frequently used to capture the\ntrends in market sentiments. On the other hand, the significance of the role\nsentiment analysis plays in the financial domain has given rise to the efforts\nto construct a financial domain-specific sentiment lexicon. Sentiment lexicons\nlend a hand for solving various text mining tasks, such as unsupervised\nclassification of text data, while alleviating the arduous human labor required\nfor manual labeling. One of the challenges in the construction of an effective\nsentiment lexicon is that the semantic orientation of a word may change\ndepending on the context in which it appears. For instance, the word ``profit\"\nusually conveys positive sentiments; however, when the word is juxtaposed with\nanother word ``decrease,\" the sentiment associated with the phrase ``profit\ndecreases\" now becomes negative. Hence, the sentiment of a given word may shift\nas one begins to consider the context surrounding the word. In this paper, we\naddress this issue by incorporating context when building sentiment lexicon\nfrom a given corpus. Specifically, we construct a lexicon named Senti-DD for\nthe Sentiment lexicon composed of Direction-Dependent words, which expresses\neach term a pair of a directional word and a direction-dependent word.\nExperiment results show that higher classification performance is achieved with\nSenti-DD, proving the effectiveness of our method for automatically\nconstructing a context-aware sentiment lexicon in the financial domain.", "published": "2021-06-10 13:08:00", "link": "http://arxiv.org/abs/2106.05723v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Text Classification and Stacked Heterogeneous Embeddings for\n  Named Entity Recognition in SMM4H 2021", "abstract": "This paper presents our findings from participating in the SMM4H Shared Task\n2021. We addressed Named Entity Recognition (NER) and Text Classification. To\naddress NER we explored BiLSTM-CRF with Stacked Heterogeneous Embeddings and\nlinguistic features. We investigated various machine learning algorithms\n(logistic regression, Support Vector Machine (SVM) and Neural Networks) to\naddress text classification. Our proposed approaches can be generalized to\ndifferent languages and we have shown its effectiveness for English and\nSpanish. Our text classification submissions (team:MIC-NLP) have achieved\ncompetitive performance with F1-score of $0.46$ and $0.90$ on ADE\nClassification (Task 1a) and Profession Classification (Task 7a) respectively.\nIn the case of NER, our submissions scored F1-score of $0.50$ and $0.82$ on ADE\nSpan Detection (Task 1b) and Profession Span detection (Task 7b) respectively.", "published": "2021-06-10 15:43:21", "link": "http://arxiv.org/abs/2106.05823v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Template-guided Hybrid Pointer Network for\n  Knowledge-basedTask-oriented Dialogue Systems", "abstract": "Most existing neural network based task-oriented dialogue systems follow\nencoder-decoder paradigm, where the decoder purely depends on the source texts\nto generate a sequence of words, usually suffering from instability and poor\nreadability. Inspired by the traditional template-based generation approaches,\nwe propose a template-guided hybrid pointer network for the knowledge-based\ntask-oriented dialogue system, which retrieves several potentially relevant\nanswers from a pre-constructed domain-specific conversational repository as\nguidance answers, and incorporates the guidance answers into both the encoding\nand decoding processes. Specifically, we design a memory pointer network model\nwith a gating mechanism to fully exploit the semantic correlation between the\nretrieved answers and the ground-truth response. We evaluate our model on four\nwidely used task-oriented datasets, including one simulated and three manually\ncreated datasets. The experimental results demonstrate that the proposed model\nachieves significantly better performance than the state-of-the-art methods\nover different automatic evaluation metrics.", "published": "2021-06-10 15:49:26", "link": "http://arxiv.org/abs/2106.05830v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Synthesizing Adversarial Negative Responses for Robust Response Ranking\n  and Evaluation", "abstract": "Open-domain neural dialogue models have achieved high performance in response\nranking and evaluation tasks. These tasks are formulated as a binary\nclassification of responses given in a dialogue context, and models generally\nlearn to make predictions based on context-response content similarity.\nHowever, over-reliance on content similarity makes the models less sensitive to\nthe presence of inconsistencies, incorrect time expressions and other factors\nimportant for response appropriateness and coherence. We propose approaches for\nautomatically creating adversarial negative training data to help ranking and\nevaluation models learn features beyond content similarity. We propose\nmask-and-fill and keyword-guided approaches that generate negative examples for\ntraining more robust dialogue systems. These generated adversarial responses\nhave high content similarity with the contexts but are either incoherent,\ninappropriate or not fluent. Our approaches are fully data-driven and can be\neasily incorporated in existing models and datasets. Experiments on\nclassification, ranking and evaluation tasks across multiple datasets\ndemonstrate that our approaches outperform strong baselines in providing\ninformative negative examples for training dialogue systems.", "published": "2021-06-10 16:20:55", "link": "http://arxiv.org/abs/2106.05894v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Probabilistic, Structure-Aware Algorithms for Improved Variety,\n  Accuracy, and Coverage of AMR Alignments", "abstract": "We present algorithms for aligning components of Abstract Meaning\nRepresentation (AMR) graphs to spans in English sentences. We leverage\nunsupervised learning in combination with heuristics, taking the best of both\nworlds from previous AMR aligners. Our unsupervised models, however, are more\nsensitive to graph substructures, without requiring a separate syntactic parse.\nOur approach covers a wider variety of AMR substructures than previously\nconsidered, achieves higher coverage of nodes and edges, and does so with\nhigher accuracy. We will release our LEAMR datasets and aligner for use in\nresearch on AMR parsing, generation, and evaluation.", "published": "2021-06-10 18:46:32", "link": "http://arxiv.org/abs/2106.06002v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CodemixedNLP: An Extensible and Open NLP Toolkit for Code-Mixing", "abstract": "The NLP community has witnessed steep progress in a variety of tasks across\nthe realms of monolingual and multilingual language processing recently. These\nsuccesses, in conjunction with the proliferating mixed language interactions on\nsocial media have boosted interest in modeling code-mixed texts. In this work,\nwe present CodemixedNLP, an open-source library with the goals of bringing\ntogether the advances in code-mixed NLP and opening it up to a wider machine\nlearning community. The library consists of tools to develop and benchmark\nversatile model architectures that are tailored for mixed texts, methods to\nexpand training sets, techniques to quantify mixing styles, and fine-tuned\nstate-of-the-art models for 7 tasks in Hinglish. We believe this work has a\npotential to foster a distributed yet collaborative and sustainable ecosystem\nin an otherwise dispersed space of code-mixing research. The toolkit is\ndesigned to be simple, easily extensible, and resourceful to both researchers\nas well as practitioners.", "published": "2021-06-10 18:49:29", "link": "http://arxiv.org/abs/2106.06004v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-lingual Emotion Detection", "abstract": "Emotion detection can provide us with a window into understanding human\nbehavior. Due to the complex dynamics of human emotions, however, constructing\nannotated datasets to train automated models can be expensive. Thus, we explore\nthe efficacy of cross-lingual approaches that would use data from a source\nlanguage to build models for emotion detection in a target language. We compare\nthree approaches, namely: i) using inherently multilingual models; ii)\ntranslating training data into the target language; and iii) using an\nautomatically tagged parallel corpus. In our study, we consider English as the\nsource language with Arabic and Spanish as target languages. We study the\neffectiveness of different classification models such as BERT and SVMs trained\nwith different features. Our BERT-based monolingual models that are trained on\ntarget language data surpass state-of-the-art (SOTA) by 4% and 5% absolute\nJaccard score for Arabic and Spanish respectively. Next, we show that using\ncross-lingual approaches with English data alone, we can achieve more than 90%\nand 80% relative effectiveness of the Arabic and Spanish BERT models\nrespectively. Lastly, we use LIME to analyze the challenges of training\ncross-lingual models for different language pairs", "published": "2021-06-10 19:52:06", "link": "http://arxiv.org/abs/2106.06017v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "One Sense per Translation", "abstract": "Word sense disambiguation (WSD) is the task of determining the sense of a\nword in context. Translations have been used in WSD as a source of knowledge,\nand even as a means of delimiting word senses. In this paper, we define three\ntheoretical properties of the relationship between senses and translations, and\nargue that they constitute necessary conditions for using translations as sense\ninventories. The key property of One Sense per Translation (OSPT) provides a\nfoundation for a translation-based WSD method. The results of an intrinsic\nevaluation experiment indicate that our method achieves a precision of\napproximately 93% compared to manual corpus annotations. Our extrinsic\nevaluation experiments demonstrate WSD improvements of up to 4.6% F1-score on\ndifficult WSD datasets.", "published": "2021-06-10 23:24:26", "link": "http://arxiv.org/abs/2106.06082v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Causal Analysis of Syntactic Agreement Mechanisms in Neural Language\n  Models", "abstract": "Targeted syntactic evaluations have demonstrated the ability of language\nmodels to perform subject-verb agreement given difficult contexts. To elucidate\nthe mechanisms by which the models accomplish this behavior, this study applies\ncausal mediation analysis to pre-trained neural language models. We investigate\nthe magnitude of models' preferences for grammatical inflections, as well as\nwhether neurons process subject-verb agreement similarly across sentences with\ndifferent syntactic structures. We uncover similarities and differences across\narchitectures and model sizes -- notably, that larger models do not necessarily\nlearn stronger preferences. We also observe two distinct mechanisms for\nproducing subject-verb agreement depending on the syntactic structure of the\ninput sentence. Finally, we find that language models rely on similar sets of\nneurons when given sentences with similar syntactic structure.", "published": "2021-06-10 23:50:51", "link": "http://arxiv.org/abs/2106.06087v3", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Data augmentation to improve robustness of image captioning solutions", "abstract": "In this paper, we study the impact of motion blur, a common quality flaw in\nreal world images, on a state-of-the-art two-stage image captioning solution,\nand notice a degradation in solution performance as blur intensity increases.\nWe investigate techniques to improve the robustness of the solution to motion\nblur using training data augmentation at each or both stages of the solution,\ni.e., object detection and captioning, and observe improved results. In\nparticular, augmenting both the stages reduces the CIDEr-D degradation for high\nmotion blur intensity from 68.7 to 11.7 on MS COCO dataset, and from 22.4 to\n6.8 on Vizwiz dataset.", "published": "2021-06-10 00:17:50", "link": "http://arxiv.org/abs/2106.05437v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Ruddit: Norms of Offensiveness for English Reddit Comments", "abstract": "On social media platforms, hateful and offensive language negatively impact\nthe mental well-being of users and the participation of people from diverse\nbackgrounds. Automatic methods to detect offensive language have largely relied\non datasets with categorical labels. However, comments can vary in their degree\nof offensiveness. We create the first dataset of English language Reddit\ncomments that has fine-grained, real-valued scores between -1 (maximally\nsupportive) and 1 (maximally offensive). The dataset was annotated using\nBest--Worst Scaling, a form of comparative annotation that has been shown to\nalleviate known biases of using rating scales. We show that the method produces\nhighly reliable offensiveness scores. Finally, we evaluate the ability of\nwidely-used neural models to predict offensiveness scores on this new dataset.", "published": "2021-06-10 11:27:47", "link": "http://arxiv.org/abs/2106.05664v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Linguistically Informed Masking for Representation Learning in the\n  Patent Domain", "abstract": "Domain-specific contextualized language models have demonstrated substantial\neffectiveness gains for domain-specific downstream tasks, like similarity\nmatching, entity recognition or information retrieval. However successfully\napplying such models in highly specific language domains requires domain\nadaptation of the pre-trained models. In this paper we propose the empirically\nmotivated Linguistically Informed Masking (LIM) method to focus\ndomain-adaptative pre-training on the linguistic patterns of patents, which use\na highly technical sublanguage. We quantify the relevant differences between\npatent, scientific and general-purpose language and demonstrate for two\ndifferent language models (BERT and SciBERT) that domain adaptation with LIM\nleads to systematically improved representations by evaluating the performance\nof the domain-adapted representations of patent language on two independent\ndownstream tasks, the IPC classification and similarity matching. We\ndemonstrate the impact of balancing the learning from different information\nsources during domain adaptation for the patent domain. We make the source code\nas well as the domain-adaptive pre-trained patent language models publicly\navailable at https://github.com/sophiaalthammer/patent-lim.", "published": "2021-06-10 14:20:57", "link": "http://arxiv.org/abs/2106.05768v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "GroupBERT: Enhanced Transformer Architecture with Efficient Grouped\n  Structures", "abstract": "Attention based language models have become a critical component in\nstate-of-the-art natural language processing systems. However, these models\nhave significant computational requirements, due to long training times, dense\noperations and large parameter count. In this work we demonstrate a set of\nmodifications to the structure of a Transformer layer, producing a more\nefficient architecture. First, we add a convolutional module to complement the\nself-attention module, decoupling the learning of local and global\ninteractions. Secondly, we rely on grouped transformations to reduce the\ncomputational cost of dense feed-forward layers and convolutions, while\npreserving the expressivity of the model. We apply the resulting architecture\nto language representation learning and demonstrate its superior performance\ncompared to BERT models of different scales. We further highlight its improved\nefficiency, both in terms of floating-point operations (FLOPs) and\ntime-to-train.", "published": "2021-06-10 15:41:53", "link": "http://arxiv.org/abs/2106.05822v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Graph Neural Networks for Natural Language Processing: A Survey", "abstract": "Deep learning has become the dominant approach in coping with various tasks\nin Natural LanguageProcessing (NLP). Although text inputs are typically\nrepresented as a sequence of tokens, there isa rich variety of NLP problems\nthat can be best expressed with a graph structure. As a result, thereis a surge\nof interests in developing new deep learning techniques on graphs for a large\nnumberof NLP tasks. In this survey, we present a comprehensive overview onGraph\nNeural Networks(GNNs) for Natural Language Processing. We propose a new\ntaxonomy of GNNs for NLP, whichsystematically organizes existing research of\nGNNs for NLP along three axes: graph construction,graph representation\nlearning, and graph based encoder-decoder models. We further introducea large\nnumber of NLP applications that are exploiting the power of GNNs and summarize\nthecorresponding benchmark datasets, evaluation metrics, and open-source codes.\nFinally, we discussvarious outstanding challenges for making the full use of\nGNNs for NLP as well as future researchdirections. To the best of our\nknowledge, this is the first comprehensive overview of Graph NeuralNetworks for\nNatural Language Processing.", "published": "2021-06-10 23:59:26", "link": "http://arxiv.org/abs/2106.06090v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AMU-EURANOVA at CASE 2021 Task 1: Assessing the stability of\n  multilingual BERT", "abstract": "This paper explains our participation in task 1 of the CASE 2021 shared task.\nThis task is about multilingual event extraction from news. We focused on\nsub-task 4, event information extraction. This sub-task has a small training\ndataset and we fine-tuned a multilingual BERT to solve this sub-task. We\nstudied the instability problem on the dataset and tried to mitigate it.", "published": "2021-06-10 07:54:39", "link": "http://arxiv.org/abs/2106.14625v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "How Robust are Model Rankings: A Leaderboard Customization Approach for\n  Equitable Evaluation", "abstract": "Models that top leaderboards often perform unsatisfactorily when deployed in\nreal world applications; this has necessitated rigorous and expensive\npre-deployment model testing. A hitherto unexplored facet of model performance\nis: Are our leaderboards doing equitable evaluation? In this paper, we\nintroduce a task-agnostic method to probe leaderboards by weighting samples\nbased on their `difficulty' level. We find that leaderboards can be\nadversarially attacked and top performing models may not always be the best\nmodels. We subsequently propose alternate evaluation metrics. Our experiments\non 10 models show changes in model ranking and an overall reduction in\npreviously reported performance -- thus rectifying the overestimation of AI\nsystems' capabilities. Inspired by behavioral testing principles, we further\ndevelop a prototype of a visual analytics tool that enables leaderboard\nrevamping through customization, based on an end user's focus area. This helps\nusers analyze models' strengths and weaknesses, and guides them in the\nselection of a model best suited for their application scenario. In a user\nstudy, members of various commercial product development teams, covering 5\nfocus areas, find that our prototype reduces pre-deployment development and\ntesting effort by 41% on average.", "published": "2021-06-10 06:47:35", "link": "http://arxiv.org/abs/2106.05532v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "U2++: Unified Two-pass Bidirectional End-to-end Model for Speech\n  Recognition", "abstract": "The unified streaming and non-streaming two-pass (U2) end-to-end model for\nspeech recognition has shown great performance in terms of streaming\ncapability, accuracy, real-time factor (RTF), and latency. In this paper, we\npresent U2++, an enhanced version of U2 to further improve the accuracy. The\ncore idea of U2++ is to use the forward and the backward information of the\nlabeling sequences at the same time at training to learn richer information,\nand combine the forward and backward prediction at decoding to give more\naccurate recognition results. We also proposed a new data augmentation method\ncalled SpecSub to help the U2++ model to be more accurate and robust. Our\nexperiments show that, compared with U2, U2++ shows faster convergence at\ntraining, better robustness to the decoding method, as well as consistent 5\\% -\n8\\% word error rate reduction gain over U2. On the experiment of AISHELL-1, we\nachieve a 4.63\\% character error rate (CER) with a non-streaming setup and\n5.05\\% with a streaming setup with 320ms latency by U2++. To the best of our\nknowledge, 5.05\\% is the best-published streaming result on the AISHELL-1 test\nset.", "published": "2021-06-10 10:25:15", "link": "http://arxiv.org/abs/2106.05642v3", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Parallel Deep Learning-Driven Sarcasm Detection from Pop Culture Text\n  and English Humor Literature", "abstract": "Sarcasm is a sophisticated way of wrapping any immanent truth, mes-sage, or\neven mockery within a hilarious manner. The advent of communications using\nsocial networks has mass-produced new avenues of socialization. It can be\nfurther said that humor, irony, sarcasm, and wit are the four chariots of being\nsocially funny in the modern days. In this paper, we manually extract the\nsarcastic word distribution features of a benchmark pop culture sarcasm corpus,\ncontaining sarcastic dialogues and monologues. We generate input sequences\nformed of the weighted vectors from such words. We further propose an\namalgamation of four parallel deep long-short term networks (pLSTM), each with\ndistinctive activation classifier. These modules are primarily aimed at\nsuccessfully detecting sarcasm from the text corpus. Our proposed model for\ndetecting sarcasm peaks a training accuracy of 98.95% when trained with the\ndiscussed dataset. Consecutively, it obtains the highest of 98.31% overall\nvalidation accuracy on two handpicked Project Gutenberg English humor\nliterature among all the test cases. Our approach transcends previous\nstate-of-the-art works on several sarcasm corpora and results in a new gold\nstandard performance for sarcasm detection.", "published": "2021-06-10 14:01:07", "link": "http://arxiv.org/abs/2106.05752v1", "categories": ["cs.CL", "cs.AI", "cs.SI", "15-04", "I.2.7; I.2.6"], "primary_category": "cs.CL"}
{"title": "Improving multi-speaker TTS prosody variance with a residual encoder and\n  normalizing flows", "abstract": "Text-to-speech systems recently achieved almost indistinguishable quality\nfrom human speech. However, the prosody of those systems is generally flatter\nthan natural speech, producing samples with low expressiveness. Disentanglement\nof speaker id and prosody is crucial in text-to-speech systems to improve on\nnaturalness and produce more variable syntheses. This paper proposes a new\nneural text-to-speech model that approaches the disentanglement problem by\nconditioning a Tacotron2-like architecture on flow-normalized speaker\nembeddings, and by substituting the reference encoder with a new learned latent\ndistribution responsible for modeling the intra-sentence variability due to the\nprosody. By removing the reference encoder dependency, the speaker-leakage\nproblem typically happening in this kind of systems disappears, producing more\ndistinctive syntheses at inference time. The new model achieves significantly\nhigher prosody variance than the baseline in a set of quantitative prosody\nfeatures, as well as higher speaker distinctiveness, without decreasing the\nspeaker intelligibility. Finally, we observe that the normalized speaker\nembeddings enable much richer speaker interpolations, substantially improving\nthe distinctiveness of the new interpolated speakers.", "published": "2021-06-10 14:08:42", "link": "http://arxiv.org/abs/2106.05762v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Balanced End-to-End Monolingual pre-training for Low-Resourced Indic\n  Languages Code-Switching Speech Recognition", "abstract": "The success in designing Code-Switching (CS) ASR often depends on the\navailability of the transcribed CS resources. Such dependency harms the\ndevelopment of ASR in low-resourced languages such as Bengali and Hindi. In\nthis paper, we exploit the transfer learning approach to design End-to-End\n(E2E) CS ASR systems for the two low-resourced language pairs using different\nmonolingual speech data and a small set of noisy CS data. We trained the\nCS-ASR, following two steps: (i) building a robust bilingual ASR system using a\nconvolution-augmented transformer (Conformer) based acoustic model and n-gram\nlanguage model, and (ii) fine-tuned the entire E2E ASR with limited noisy CS\ndata. We tested our method on MUCS 2021 challenge and achieved 3rd place in the\nCS track. We then tested the proposed method using noisy CS data released for\nHindi-English and Bengali-English pairs in Multilingual and Code-Switching ASR\nChallenges for Low Resource Indian Languages (MUCS 2021) and achieved 3rd place\nin the CS track. Unlike, the leading two systems that benefited from crawling\nYouTube and learning transliteration pairs, our proposed transfer learning\napproach focused on using only the limited CS data with no data-cleaning or\ndata re-segmentation. Our approach achieved 14.1% relative gain in word error\nrate (WER) in Hindi-English and 27.1% in Bengali-English. We provide detailed\nguidelines on the steps to finetune the self-attention based model for limited\ndata for ASR. Moreover, we release the code and recipe used in this paper.", "published": "2021-06-10 16:12:51", "link": "http://arxiv.org/abs/2106.05885v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Deciphering Implicit Hate: Evaluating Automated Detection Algorithms for\n  Multimodal Hate", "abstract": "Accurate detection and classification of online hate is a difficult task.\nImplicit hate is particularly challenging as such content tends to have unusual\nsyntax, polysemic words, and fewer markers of prejudice (e.g., slurs). This\nproblem is heightened with multimodal content, such as memes (combinations of\ntext and images), as they are often harder to decipher than unimodal content\n(e.g., text alone). This paper evaluates the role of semantic and multimodal\ncontext for detecting implicit and explicit hate. We show that both text- and\nvisual- enrichment improves model performance, with the multimodal model\n(0.771) outperforming other models' F1 scores (0.544, 0.737, and 0.754). While\nthe unimodal-text context-aware (transformer) model was the most accurate on\nthe subtask of implicit hate detection, the multimodal model outperformed it\noverall because of a lower propensity towards false positives. We find that all\nmodels perform better on content with full annotator agreement and that\nmultimodal models are best at classifying the content where annotators\ndisagree. To conduct these investigations, we undertook high-quality annotation\nof a sample of 5,000 multimodal entries. Tweets were annotated for primary\ncategory, modality, and strategy. We make this corpus, along with the codebook,\ncode, and final model, freely available.", "published": "2021-06-10 16:29:42", "link": "http://arxiv.org/abs/2106.05903v1", "categories": ["cs.CL", "cs.CV", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PARP: Prune, Adjust and Re-Prune for Self-Supervised Speech Recognition", "abstract": "Self-supervised speech representation learning (speech SSL) has demonstrated\nthe benefit of scale in learning rich representations for Automatic Speech\nRecognition (ASR) with limited paired data, such as wav2vec 2.0. We investigate\nthe existence of sparse subnetworks in pre-trained speech SSL models that\nachieve even better low-resource ASR results. However, directly applying widely\nadopted pruning methods such as the Lottery Ticket Hypothesis (LTH) is\nsuboptimal in the computational cost needed. Moreover, we show that the\ndiscovered subnetworks yield minimal performance gain compared to the original\ndense network. We present Prune-Adjust-Re-Prune (PARP), which discovers and\nfinetunes subnetworks for much better performance, while only requiring a\nsingle downstream ASR finetuning run. PARP is inspired by our surprising\nobservation that subnetworks pruned for pre-training tasks need merely a slight\nadjustment to achieve a sizeable performance boost in downstream ASR tasks.\nExtensive experiments on low-resource ASR verify (1) sparse subnetworks exist\nin mono-lingual/multi-lingual pre-trained speech SSL, and (2) the computational\nadvantage and performance gain of PARP over baseline pruning methods. In\nparticular, on the 10min Librispeech split without LM decoding, PARP discovers\nsubnetworks from wav2vec 2.0 with an absolute 10.9%/12.6% WER decrease compared\nto the full model. We further demonstrate the effectiveness of PARP via:\ncross-lingual pruning without any phone recognition degradation, the discovery\nof a multi-lingual subnetwork for 10 spoken languages in 1 finetuning run, and\nits applicability to pre-trained BERT/XLNet for natural language tasks.", "published": "2021-06-10 17:32:25", "link": "http://arxiv.org/abs/2106.05933v2", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "ImaginE: An Imagination-Based Automatic Evaluation Metric for Natural\n  Language Generation", "abstract": "Automatic evaluations for natural language generation (NLG) conventionally\nrely on token-level or embedding-level comparisons with text references. This\ndiffers from human language processing, for which visual imagination often\nimproves comprehension. In this work, we propose ImaginE, an imagination-based\nautomatic evaluation metric for natural language generation. With the help of\nStableDiffusion, a state-of-the-art text-to-image generator, we automatically\ngenerate an image as the embodied imagination for the text snippet and compute\nthe imagination similarity using contextual embeddings. Experiments spanning\nseveral text generation tasks demonstrate that adding machine-generated images\nwith our ImaginE displays great potential in introducing multi-modal\ninformation into NLG evaluation, and improves existing automatic metrics'\ncorrelations with human similarity judgments in both reference-based and\nreference-free evaluation scenarios.", "published": "2021-06-10 17:59:52", "link": "http://arxiv.org/abs/2106.05970v3", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Modeling Hierarchical Structures with Continuous Recursive Neural\n  Networks", "abstract": "Recursive Neural Networks (RvNNs), which compose sequences according to their\nunderlying hierarchical syntactic structure, have performed well in several\nnatural language processing tasks compared to similar models without structural\nbiases. However, traditional RvNNs are incapable of inducing the latent\nstructure in a plain text sequence on their own. Several extensions have been\nproposed to overcome this limitation. Nevertheless, these extensions tend to\nrely on surrogate gradients or reinforcement learning at the cost of higher\nbias or variance. In this work, we propose Continuous Recursive Neural Network\n(CRvNN) as a backpropagation-friendly alternative to address the aforementioned\nlimitations. This is done by incorporating a continuous relaxation to the\ninduced structure. We demonstrate that CRvNN achieves strong performance in\nchallenging synthetic tasks such as logical inference and ListOps. We also show\nthat CRvNN performs comparably or better than prior latent structure models on\nreal-world tasks such as sentiment analysis and natural language inference.", "published": "2021-06-10 20:42:05", "link": "http://arxiv.org/abs/2106.06038v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Semi-supervised Multi-task Learning Approach to Classify Customer\n  Contact Intents", "abstract": "In the area of customer support, understanding customers' intents is a\ncrucial step. Machine learning plays a vital role in this type of intent\nclassification. In reality, it is typical to collect confirmation from customer\nsupport representatives (CSRs) regarding the intent prediction, though it can\nunnecessarily incur prohibitive cost to ask CSRs to assign existing or new\nintents to the mis-classified cases. Apart from the confirmed cases with and\nwithout intent labels, there can be a number of cases with no human curation.\nThis data composition (Positives + Unlabeled + multiclass Negatives) creates\nunique challenges for model development. In response to that, we propose a\nsemi-supervised multi-task learning paradigm. In this manuscript, we share our\nexperience in building text-based intent classification models for a customer\nsupport service on an E-commerce website. We improve the performance\nsignificantly by evolving the model from multiclass classification to\nsemi-supervised multi-task learning by leveraging the negative cases, domain-\nand task-adaptively pretrained ALBERT on customer contact texts, and a number\nof un-curated data with no labels. In the evaluation, the final model boosts\nthe average AUC ROC by almost 20 points compared to the baseline finetuned\nmulticlass classification ALBERT model.", "published": "2021-06-10 16:13:05", "link": "http://arxiv.org/abs/2106.07381v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "MusicBERT: Symbolic Music Understanding with Large-Scale Pre-Training", "abstract": "Symbolic music understanding, which refers to the understanding of music from\nthe symbolic data (e.g., MIDI format, but not audio), covers many music\napplications such as genre classification, emotion classification, and music\npieces matching. While good music representations are beneficial for these\napplications, the lack of training data hinders representation learning.\nInspired by the success of pre-training models in natural language processing,\nin this paper, we develop MusicBERT, a large-scale pre-trained model for music\nunderstanding. To this end, we construct a large-scale symbolic music corpus\nthat contains more than 1 million music songs. Since symbolic music contains\nmore structural (e.g., bar, position) and diverse information (e.g., tempo,\ninstrument, and pitch), simply adopting the pre-training techniques from NLP to\nsymbolic music only brings marginal gains. Therefore, we design several\nmechanisms, including OctupleMIDI encoding and bar-level masking strategy, to\nenhance pre-training with symbolic music data. Experiments demonstrate the\nadvantages of MusicBERT on four music understanding tasks, including melody\ncompletion, accompaniment suggestion, genre classification, and style\nclassification. Ablation studies also verify the effectiveness of our designs\nof OctupleMIDI encoding and bar-level masking strategy in MusicBERT.", "published": "2021-06-10 10:13:05", "link": "http://arxiv.org/abs/2106.05630v1", "categories": ["cs.SD", "cs.CL", "cs.IR", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Programming Puzzles", "abstract": "We introduce a new type of programming challenge called programming puzzles,\nas an objective and comprehensive evaluation of program synthesis, and release\nan open-source dataset of Python Programming Puzzles (P3). Each puzzle is\ndefined by a short Python program $f$, and the goal is to find an input which\nmakes $f$ return True. The puzzles are objective in that each one is specified\nentirely by the source code of its verifier $f$, so evaluating $f$ is all that\nis needed to test a candidate solution. They do not require an answer key or\ninput/output examples, nor do they depend on natural language understanding.\nThe dataset is comprehensive in that it spans problems of a range of\ndifficulties and domains, ranging from trivial string manipulation problems, to\nclassic programming puzzles (e.g., Tower of Hanoi), to\ninterview/competitive-programming problems (e.g., dynamic programming), to\nlongstanding open problems in algorithms and mathematics (e.g., factoring). We\ndevelop baseline enumerative program synthesis, GPT-3 and Codex solvers that\nare capable of solving puzzles -- even without access to any reference\nsolutions -- by learning from their own past solutions. Codex performs best,\nsolving up to 18% of 397 test problems with a single try and 80% of the\nproblems with 1,000 tries per problem. In a small user study, we find a\npositive correlation between puzzle-solving performance and coding experience,\nand between the puzzle difficulty for humans and AI solvers. Therefore, further\nimprovements on P3 could have a significant impact on many program synthesis\nareas.", "published": "2021-06-10 14:37:28", "link": "http://arxiv.org/abs/2106.05784v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.PL", "cs.SE"], "primary_category": "cs.LG"}
{"title": "SemEval-2021 Task 11: NLPContributionGraph -- Structuring Scholarly NLP\n  Contributions for a Research Knowledge Graph", "abstract": "There is currently a gap between the natural language expression of scholarly\npublications and their structured semantic content modeling to enable\nintelligent content search. With the volume of research growing exponentially\nevery year, a search feature operating over semantically structured content is\ncompelling. The SemEval-2021 Shared Task NLPContributionGraph (a.k.a. 'the NCG\ntask') tasks participants to develop automated systems that structure\ncontributions from NLP scholarly articles in the English language. Being the\nfirst-of-its-kind in the SemEval series, the task released structured data from\nNLP scholarly articles at three levels of information granularity, i.e. at\nsentence-level, phrase-level, and phrases organized as triples toward Knowledge\nGraph (KG) building. The sentence-level annotations comprised the few sentences\nabout the article's contribution. The phrase-level annotations were scientific\nterm and predicate phrases from the contribution sentences. Finally, the\ntriples constituted the research overview KG. For the Shared Task,\nparticipating systems were then expected to automatically classify contribution\nsentences, extract scientific terms and relations from the sentences, and\norganize them as KG triples.\n  Overall, the task drew a strong participation demographic of seven teams and\n27 participants. The best end-to-end task system classified contribution\nsentences at 57.27% F1, phrases at 46.41% F1, and triples at 22.28% F1. While\nthe absolute performance to generate triples remains low, in the conclusion of\nthis article, the difficulty of producing such data and as a consequence of\nmodeling it is highlighted.", "published": "2021-06-10 13:43:47", "link": "http://arxiv.org/abs/2106.07385v3", "categories": ["cs.CL", "cs.AI", "cs.DL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Relational Data Selection for Data Augmentation of Speaker-dependent\n  Multi-band MelGAN Vocoder", "abstract": "Nowadays, neural vocoders can generate very high-fidelity speech when a bunch\nof training data is available. Although a speaker-dependent (SD) vocoder\nusually outperforms a speaker-independent (SI) vocoder, it is impractical to\ncollect a large amount of data of a specific target speaker for most real-world\napplications. To tackle the problem of limited target data, a data augmentation\nmethod based on speaker representation and similarity measurement of speaker\nverification is proposed in this paper. The proposed method selects utterances\nthat have similar speaker identity to the target speaker from an external\ncorpus, and then combines the selected utterances with the limited target data\nfor SD vocoder adaptation. The evaluation results show that, compared with the\nvocoder adapted using only limited target data, the vocoder adapted using\naugmented data improves both the quality and similarity of synthesized speech.", "published": "2021-06-10 10:11:57", "link": "http://arxiv.org/abs/2106.05629v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Speaker-conversation factorial designs for diarization error analysis", "abstract": "Speaker diarization accuracy can be affected by both acoustics and\nconversation characteristics. Determining the cause of diarization errors is\ndifficult because speaker voice acoustics and conversation structure co-vary,\nand the interactions between acoustics, conversational structure, and\ndiarization accuracy are complex. This paper proposes a methodology that can\ndistinguish independent marginal effects of acoustic and conversation\ncharacteristics on diarization accuracy by remixing conversations in a\nfactorial design. As an illustration, this approach is used to investigate\ngender-related and language-related accuracy differences with three diarization\nsystems: a baseline system using subsegment x-vector clustering, a variant of\nit with shorter subsegments, and a third system based on a Bayesian hidden\nMarkov model. Our analysis shows large accuracy disparities for the baseline\nsystem primarily due to conversational structure, which are partially mitigated\nin the other two systems. The illustration thus demonstrates how the\nmethodology can be used to identify and guide diarization model improvements.", "published": "2021-06-10 14:48:41", "link": "http://arxiv.org/abs/2106.05792v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Independent Deeply Learned Tensor Analysis for Determined Audio Source\n  Separation", "abstract": "We address the determined audio source separation problem in the\ntime-frequency domain. In independent deeply learned matrix analysis (IDLMA),\nit is assumed that the inter-frequency correlation of each source spectrum is\nzero, which is inappropriate for modeling nonstationary signals such as music\nsignals. To account for the correlation between frequencies, independent\npositive semidefinite tensor analysis has been proposed. This unsupervised\n(blind) method, however, severely restrict the structure of frequency\ncovariance matrices (FCMs) to reduce the number of model parameters. As an\nextension of these conventional approaches, we here propose a supervised method\nthat models FCMs using deep neural networks (DNNs). It is difficult to directly\ninfer FCMs using DNNs. Therefore, we also propose a new FCM model represented\nas a convex combination of a diagonal FCM and a rank-1 FCM. Our FCM model is\nflexible enough to not only consider inter-frequency correlation, but also\ncapture the dynamics of time-varying FCMs of nonstationary signals. We infer\nthe proposed FCMs using two DNNs: DNN for power spectrum estimation and DNN for\ntime-domain signal estimation. An experimental result of separating music\nsignals shows that the proposed method provides higher separation performance\nthan IDLMA.", "published": "2021-06-10 06:33:11", "link": "http://arxiv.org/abs/2106.05529v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Comparison and Combination of Unsupervised Blind Source Separation\n  Techniques", "abstract": "Unsupervised blind source separation methods do not require a training phase\nand thus cannot suffer from a train-test mismatch, which is a common concern in\nneural network based source separation. The unsupervised techniques can be\ncategorized in two classes, those building upon the sparsity of speech in the\nShort-Time Fourier transform domain and those exploiting non-Gaussianity or\nnon-stationarity of the source signals. In this contribution, spatial mixture\nmodels which fall in the first category and independent vector analysis (IVA)\nas a representative of the second category are compared w.r.t. their separation\nperformance and the performance of a downstream speech recognizer on a\nreverberant dataset of reasonable size. Furthermore, we introduce a serial\nconcatenation of the two, where the result of the mixture model serves as\ninitialization of IVA, which achieves significantly better WER performance than\neach algorithm individually and even approaches the performance of a much more\ncomplex neural network based technique.", "published": "2021-06-10 10:11:45", "link": "http://arxiv.org/abs/2106.05627v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Tonal Frequencies, Consonance, Dissonance: A Math-Bio Intersection", "abstract": "To date, calculating the frequencies of musical notes requires one to know\nthe frequency of some reference note. In this study, first-order ordinary\ndifferential equations are used to arrive at a mathematical model to determine\ntonal frequencies using their respective note indices. In the next part of the\nstudy, an analysis that is based on the fundamental musical frequencies is\nconducted to theoretically and neurobiologically explain the consonance and\ndissonance caused by the different musical notes in the chromatic scale which\nis based on the fact that systematic patterns of sound invoke pleasure. The\nreason behind the richness of harmony and the sonic interference and degree of\nconsonance in musical chords are discussed. Since a human mind analyses\neverything relatively, anything other than the most consonant notes sounds\ndissonant. In conclusion, the study explains clearly why musical notes and in\ntoto, music sounds the way it does.", "published": "2021-06-10 09:36:02", "link": "http://arxiv.org/abs/2106.08479v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
