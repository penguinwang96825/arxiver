{"title": "Sub-event detection from Twitter streams as a sequence labeling problem", "abstract": "This paper introduces improved methods for sub-event detection in social\nmedia streams, by applying neural sequence models not only on the level of\nindividual posts, but also directly on the stream level. Current approaches to\nidentify sub-events within a given event, such as a goal during a soccer match,\nessentially do not exploit the sequential nature of social media streams. We\naddress this shortcoming by framing the sub-event detection problem in social\nmedia streams as a sequence labeling task and adopt a neural sequence\narchitecture that explicitly accounts for the chronological order of posts.\nSpecifically, we (i) establish a neural baseline that outperforms a graph-based\nstate-of-the-art method for binary sub-event detection (2.7% micro-F1\nimprovement), as well as (ii) demonstrate superiority of a recurrent neural\nnetwork model on the posts sequence level for labeled sub-events (2.4%\nbin-level F1 improvement over non-sequential models).", "published": "2019-03-13 10:23:38", "link": "http://arxiv.org/abs/1903.05396v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Overview of the Ugglan Entity Discovery and Linking System", "abstract": "Ugglan is a system designed to discover named entities and link them to\nunique identifiers in a knowledge base. It is based on a combination of a name\nand nominal dictionary derived from Wikipedia and Wikidata, a named entity\nrecognition module (NER) using fixed ordinally-forgetting encoding (FOFE)\ntrained on the TAC EDL data from 2014-2016, a candidate generation module from\nthe Wikipedia link graph across multiple editions, a PageRank link and\ncooccurrence graph disambiguator, and finally a reranker trained on the TAC EDL\n2015-2016 data.", "published": "2019-03-13 14:03:50", "link": "http://arxiv.org/abs/1903.05498v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Low-Resource Syntactic Transfer with Unsupervised Source Reordering", "abstract": "We describe a cross-lingual transfer method for dependency parsing that takes\ninto account the problem of word order differences between source and target\nlanguages. Our model only relies on the Bible, a considerably smaller parallel\ndata than the commonly used parallel data in transfer methods. We use the\nconcatenation of projected trees from the Bible corpus, and the gold-standard\ntreebanks in multiple source languages along with cross-lingual word\nrepresentations. We demonstrate that reordering the source treebanks before\ntraining on them for a target language improves the accuracy of languages\noutside the European language family. Our experiments on 68 treebanks (38\nlanguages) in the Universal Dependencies corpus achieve a high accuracy for all\nlanguages. Among them, our experiments on 16 treebanks of 12 non-European\nlanguages achieve an average UAS absolute improvement of 3.3% over a\nstate-of-the-art method.", "published": "2019-03-13 19:01:00", "link": "http://arxiv.org/abs/1903.05683v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Consistent Dialogue Generation with Self-supervised Feature Learning", "abstract": "Generating responses that are consistent with the dialogue context is one of\nthe central challenges in building engaging conversational agents. We\ndemonstrate that neural conversation models can be geared towards generating\nconsistent responses by maintaining certain features related to topics and\npersonas throughout the conversation. Past work has required external\nsupervision that exploits features such as user identities that are often\nunavailable. In our approach, topic and persona feature extractors are trained\nusing a contrastive training scheme that utilizes the natural structure of\ndialogue data. We further adopt a feature disentangling loss which, paired with\ncontrollable response generation techniques, allows us to promote or demote\ncertain learned topics and persona features. Evaluation results demonstrate the\nmodel's ability to capture meaningful topics and persona features. The\nincorporation of the learned features brings significant improvement in terms\nof the quality of generated responses on two dialogue datasets.", "published": "2019-03-13 23:45:31", "link": "http://arxiv.org/abs/1903.05759v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Market Trend Prediction using Sentiment Analysis: Lessons Learned and\n  Paths Forward", "abstract": "Financial market forecasting is one of the most attractive practical\napplications of sentiment analysis. In this paper, we investigate the potential\nof using sentiment \\emph{attitudes} (positive vs negative) and also sentiment\n\\emph{emotions} (joy, sadness, etc.) extracted from financial news or tweets to\nhelp predict stock price movements. Our extensive experiments using the\n\\emph{Granger-causality} test have revealed that (i) in general sentiment\nattitudes do not seem to Granger-cause stock price changes; and (ii) while on\nsome specific occasions sentiment emotions do seem to Granger-cause stock price\nchanges, the exhibited pattern is not universal and must be looked at on a case\nby case basis. Furthermore, it has been observed that at least for certain\nstocks, integrating sentiment emotions as additional features into the machine\nlearning based market trend prediction model could improve its accuracy.", "published": "2019-03-13 12:19:45", "link": "http://arxiv.org/abs/1903.05440v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "MMKG: Multi-Modal Knowledge Graphs", "abstract": "We present MMKG, a collection of three knowledge graphs that contain both\nnumerical features and (links to) images for all entities as well as entity\nalignments between pairs of KGs. Therefore, multi-relational link prediction\nand entity matching communities can benefit from this resource. We believe this\ndata set has the potential to facilitate the development of novel multi-modal\nlearning approaches for knowledge graphs.We validate the utility ofMMKG in the\nsameAs link prediction task with an extensive set of experiments. These\nexperiments show that the task at hand benefits from learning of multiple\nfeature types.", "published": "2019-03-13 13:48:32", "link": "http://arxiv.org/abs/1903.05485v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Adversarial attacks against Fact Extraction and VERification", "abstract": "This paper describes a baseline for the second iteration of the Fact\nExtraction and VERification shared task (FEVER2.0) which explores the\nresilience of systems through adversarial evaluation. We present a collection\nof simple adversarial attacks against systems that participated in the first\nFEVER shared task. FEVER modeled the assessment of truthfulness of written\nclaims as a joint information retrieval and natural language inference task\nusing evidence from Wikipedia. A large number of participants made use of deep\nneural networks in their submissions to the shared task. The extent as to\nwhether such models understand language has been the subject of a number of\nrecent investigations and discussion in literature. In this paper, we present a\nsimple method of generating entailment-preserving and entailment-altering\nperturbations of instances by common patterns within the training data. We find\nthat a number of systems are greatly affected with absolute losses in\nclassification accuracy of up to $29\\%$ on the newly perturbed instances. Using\nthese newly generated instances, we construct a sample submission for the\nFEVER2.0 shared task. Addressing these types of attacks will aid in building\nmore robust fact-checking models, as well as suggest directions to expand the\ndatasets.", "published": "2019-03-13 15:29:22", "link": "http://arxiv.org/abs/1903.05543v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Benchmarking Natural Language Understanding Services for building\n  Conversational Agents", "abstract": "We have recently seen the emergence of several publicly available Natural\nLanguage Understanding (NLU) toolkits, which map user utterances to structured,\nbut more abstract, Dialogue Act (DA) or Intent specifications, while making\nthis process accessible to the lay developer. In this paper, we present the\nfirst wide coverage evaluation and comparison of some of the most popular NLU\nservices, on a large, multi-domain (21 domains) dataset of 25K user utterances\nthat we have collected and annotated with Intent and Entity Type specifications\nand which will be released as part of this submission. The results show that on\nIntent classification Watson significantly outperforms the other platforms,\nnamely, Dialogflow, LUIS and Rasa; though these also perform well.\nInterestingly, on Entity Type recognition, Watson performs significantly worse\ndue to its low Precision. Again, Dialogflow, LUIS and Rasa perform well on this\ntask.", "published": "2019-03-13 16:08:46", "link": "http://arxiv.org/abs/1903.05566v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multimodal Emotion Classification", "abstract": "Most NLP and Computer Vision tasks are limited to scarcity of labelled data.\nIn social media emotion classification and other related tasks, hashtags have\nbeen used as indicators to label data. With the rapid increase in emoji usage\nof social media, emojis are used as an additional feature for major social NLP\ntasks. However, this is less explored in case of multimedia posts on social\nmedia where posts are composed of both image and text. At the same time, w.e\nhave seen a surge in the interest to incorporate domain knowledge to improve\nmachine understanding of text. In this paper, we investigate whether domain\nknowledge for emoji can improve the accuracy of emotion classification task. We\nexploit the importance of different modalities from social media post for\nemotion classification task using state-of-the-art deep learning architectures.\nOur experiments demonstrate that the three modalities (text, emoji and images)\nencode different information to express emotion and therefore can complement\neach other. Our results also demonstrate that emoji sense depends on the\ntextual context, and emoji combined with text encodes better information than\nconsidered separately. The highest accuracy of 71.98\\% is achieved with a\ntraining data of 550k posts.", "published": "2019-03-13 07:54:29", "link": "http://arxiv.org/abs/1903.12520v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "SciLens: Evaluating the Quality of Scientific News Articles Using Social\n  Media and Scientific Literature Indicators", "abstract": "This paper describes, develops, and validates SciLens, a method to evaluate\nthe quality of scientific news articles. The starting point for our work are\nstructured methodologies that define a series of quality aspects for manually\nevaluating news. Based on these aspects, we describe a series of indicators of\nnews quality. According to our experiments, these indicators help non-experts\nevaluate more accurately the quality of a scientific news article, compared to\nnon-experts that do not have access to these indicators. Furthermore, SciLens\ncan also be used to produce a completely automated quality score for an\narticle, which agrees more with expert evaluators than manual evaluations done\nby non-experts. One of the main elements of SciLens is the focus on both\ncontent and context of articles, where context is provided by (1) explicit and\nimplicit references on the article to scientific literature, and (2) reactions\nin social media referencing the article. We show that both contextual elements\ncan be valuable sources of information for determining article quality. The\nvalidation of SciLens, done through a combination of expert and non-expert\nannotation, demonstrates its effectiveness for both semi-automatic and\nautomatic quality evaluation of scientific news.", "published": "2019-03-13 15:13:57", "link": "http://arxiv.org/abs/1903.05538v2", "categories": ["cs.IR", "cs.CL", "cs.SI"], "primary_category": "cs.IR"}
{"title": "GASC: Genre-Aware Semantic Change for Ancient Greek", "abstract": "Word meaning changes over time, depending on linguistic and extra-linguistic\nfactors. Associating a word's correct meaning in its historical context is a\ncentral challenge in diachronic research, and is relevant to a range of NLP\ntasks, including information retrieval and semantic search in historical texts.\nBayesian models for semantic change have emerged as a powerful tool to address\nthis challenge, providing explicit and interpretable representations of\nsemantic change phenomena. However, while corpora typically come with rich\nmetadata, existing models are limited by their inability to exploit contextual\ninformation (such as text genre) beyond the document time-stamp. This is\nparticularly critical in the case of ancient languages, where lack of data and\nlong diachronic span make it harder to draw a clear distinction between\npolysemy (the fact that a word has several senses) and semantic change (the\nprocess of acquiring, losing, or changing senses), and current systems perform\npoorly on these languages. We develop GASC, a dynamic semantic change model\nthat leverages categorical metadata about the texts' genre to boost inference\nand uncover the evolution of meanings in Ancient Greek corpora. In a new\nevaluation framework, our model achieves improved predictive performance\ncompared to the state of the art.", "published": "2019-03-13 17:32:51", "link": "http://arxiv.org/abs/1903.05587v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "CoaCor: Code Annotation for Code Retrieval with Reinforcement Learning", "abstract": "To accelerate software development, much research has been performed to help\npeople understand and reuse the huge amount of available code resources. Two\nimportant tasks have been widely studied: code retrieval, which aims to\nretrieve code snippets relevant to a given natural language query from a code\nbase, and code annotation, where the goal is to annotate a code snippet with a\nnatural language description. Despite their advancement in recent years, the\ntwo tasks are mostly explored separately. In this work, we investigate a novel\nperspective of Code annotation for Code retrieval (hence called `CoaCor'),\nwhere a code annotation model is trained to generate a natural language\nannotation that can represent the semantic meaning of a given code snippet and\ncan be leveraged by a code retrieval model to better distinguish relevant code\nsnippets from others. To this end, we propose an effective framework based on\nreinforcement learning, which explicitly encourages the code annotation model\nto generate annotations that can be used for the retrieval task. Through\nextensive experiments, we show that code annotations generated by our framework\nare much more detailed and more useful for code retrieval, and they can further\nimprove the performance of existing code retrieval models significantly.", "published": "2019-03-13 19:22:22", "link": "http://arxiv.org/abs/1904.00720v1", "categories": ["cs.SE", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.SE"}
{"title": "Voice command generation using Progressive Wavegans", "abstract": "Generative Adversarial Networks (GANs) have become exceedingly popular in a\nwide range of data-driven research fields, due in part to their success in\nimage generation. Their ability to generate new samples, often from only a\nsmall amount of input data, makes them an exciting research tool in areas with\nlimited data resources. One less-explored application of GANs is the synthesis\nof speech and audio samples. Herein, we propose a set of extensions to the\nWaveGAN paradigm, a recently proposed approach for sound generation using GANs.\nThe aim of these extensions - preprocessing, Audio-to-Audio generation, skip\nconnections and progressive structures - is to improve the human likeness of\nsynthetic speech samples. Scores from listening tests with 30 volunteers\ndemonstrated a moderate improvement (Cohen's d coefficient of 0.65) in human\nlikeness using the proposed extensions compared to the original WaveGAN\napproach.", "published": "2019-03-13 18:43:31", "link": "http://arxiv.org/abs/1903.07395v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Frequency Domain Multi-channel Acoustic Modeling for Distant Speech\n  Recognition", "abstract": "Conventional far-field automatic speech recognition (ASR) systems typically\nemploy microphone array techniques for speech enhancement in order to improve\nrobustness against noise or reverberation. However, such speech enhancement\ntechniques do not always yield ASR accuracy improvement because the\noptimization criterion for speech enhancement is not directly relevant to the\nASR objective. In this work, we develop new acoustic modeling techniques that\noptimize spatial filtering and long short-term memory (LSTM) layers from\nmulti-channel (MC) input based on an ASR criterion directly. In contrast to\nconventional methods, we incorporate array processing knowledge into the\nacoustic model. Moreover, we initialize the network with beamformers'\ncoefficients. We investigate effects of such MC neural networks through ASR\nexperiments on the real-world far-field data where users are interacting with\nan ASR system in uncontrolled acoustic environments. We show that our MC\nacoustic model can reduce a word error rate (WER) by~16.5\\% compared to a\nsingle channel ASR system with the traditional log-mel filter bank energy\n(LFBE) feature on average. Our result also shows that our network with the\nspatial filtering layer on two-channel input achieves a relative WER reduction\nof~9.5\\% compared to conventional beamforming with seven microphones.", "published": "2019-03-13 03:11:39", "link": "http://arxiv.org/abs/1903.05299v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multi-Geometry Spatial Acoustic Modeling for Distant Speech Recognition", "abstract": "The use of spatial information with multiple microphones can improve\nfar-field automatic speech recognition (ASR) accuracy. However, conventional\nmicrophone array techniques degrade speech enhancement performance when there\nis an array geometry mismatch between design and test conditions. Moreover,\nsuch speech enhancement techniques do not always yield ASR accuracy improvement\ndue to the difference between speech enhancement and ASR optimization\nobjectives. In this work, we propose to unify an acoustic model framework by\noptimizing spatial filtering and long short-term memory (LSTM) layers from\nmulti-channel (MC) input. Our acoustic model subsumes beamformers with multiple\ntypes of array geometry. In contrast to deep clustering methods that treat a\nneural network as a black box tool, the network encoding the spatial filters\ncan process streaming audio data in real time without the accumulation of\ntarget signal statistics. We demonstrate the effectiveness of such MC neural\nnetworks through ASR experiments on the real-world far-field data. We show that\nour two-channel acoustic model can on average reduce word error rates (WERs)\nby~13.4 and~12.7% compared to a single channel ASR system with the log-mel\nfilter bank energy (LFBE) feature under the matched and mismatched microphone\nplacement conditions, respectively. Our result also shows that our two-channel\nnetwork achieves a relative WER reduction of over~7.0% compared to conventional\nbeamforming with seven microphones overall.", "published": "2019-03-13 03:32:53", "link": "http://arxiv.org/abs/1903.06539v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Phase-aware Harmonic/Percussive Source Separation via Convex\n  Optimization", "abstract": "Decomposition of an audio mixture into harmonic and percussive components,\nnamely harmonic/percussive source separation (HPSS), is a useful pre-processing\ntool for many audio applications. Popular approaches to HPSS exploit the\ndistinctive source-specific structures of power spectrograms. However, such\napproaches consider only power spectrograms, and the phase remains intact for\nresynthesizing the separated signals. In this paper, we propose a phase-aware\nHPSS method based on the structure of the phase of harmonic components. It is\nformulated as a convex optimization problem in the time domain, which enables\nthe simultaneous treatment of both amplitude and phase. The numerical\nexperiment validates the effectiveness of the proposed method.", "published": "2019-03-13 17:01:03", "link": "http://arxiv.org/abs/1903.05600v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Low-rankness of Complex-valued Spectrogram and Its Application to\n  Phase-aware Audio Processing", "abstract": "Low-rankness of amplitude spectrograms has been effectively utilized in audio\nsignal processing methods including non-negative matrix factorization. However,\nsuch methods have a fundamental limitation owing to their amplitude-only\ntreatment where the phase of the observed signal is utilized for resynthesizing\nthe estimated signal. In order to address this limitation, we directly treat a\ncomplex-valued spectrogram and show a complex-valued spectrogram of a sum of\nsinusoids can be approximately low-rank by modifying its phase. For evaluating\nthe applicability of the proposed low-rank representation, we further propose a\nconvex prior emphasizing harmonic signals, and it is applied to audio\ndenoising.", "published": "2019-03-13 17:06:40", "link": "http://arxiv.org/abs/1903.05603v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
